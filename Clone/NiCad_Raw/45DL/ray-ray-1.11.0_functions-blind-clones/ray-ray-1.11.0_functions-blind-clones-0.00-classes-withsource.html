<html>
<head>
    <title>NiCad6 Clone Report</title>
    <style type="text/css">
        body {font-family:sans-serif;}
        table {background-color:white; border:0px; padding:0px; border-spacing:4px; width:auto; margin-left:30px; margin-right:auto;}
        td {background-color:rgba(192,212,238,0.8); border:0px; padding:8px; width:auto; vertical-align:top; border-radius:8px}
        pre {background-color:white; padding:4px;}
        a {color:darkblue;}
    </style>
</head>
<body>
<h2>NiCad6 Clone Report</h2>
<table>
<tr style="font-size:14pt">
<td><b>System:</b> &nbsp; ray-ray-1.11.0</td>
<td><b>Clone pairs:</b> &nbsp; 120</td>
<td><b>Clone classes:</b> &nbsp; 52</td>
</tr>
<tr style="font-size:12pt">
<td style="background-color:white">Clone type: &nbsp; 2</td>
<td style="background-color:white">Granularity: &nbsp; functions-blind</td>
<td style="background-color:white">Max diff threshold: &nbsp; 0%</td>
<td style="background-color:white">Clone size: &nbsp; 10 - 2500 lines</td>
<td style="background-color:white">Total functions-blind: &nbsp; 7990</td>
</tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 1:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag340')" href="javascript:;">
ray-ray-1.11.0/rllib/agents/sac/sac_tf_model.py: 128-148
</a>
<div class="mid" id="frag340" style="display:none"><pre>
    def build_policy_model(self, obs_space, num_outputs, policy_model_config,
                           name):
        """Builds the policy model used by this SAC.

        Override this method in a sub-class of SACTFModel to implement your
        own policy net. Alternatively, simply set `custom_model` within the
        top level SAC `policy_model` config key to make this default
        implementation of `build_policy_model` use your custom policy network.

        Returns:
            TFModelV2: The TFModelV2 policy sub-model.
        """
        model = ModelCatalog.get_model_v2(
            obs_space,
            self.action_space,
            num_outputs,
            policy_model_config,
            framework="tf",
            name=name)
        return model

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag361')" href="javascript:;">
ray-ray-1.11.0/rllib/agents/sac/sac_torch_model.py: 136-156
</a>
<div class="mid" id="frag361" style="display:none"><pre>
    def build_policy_model(self, obs_space, num_outputs, policy_model_config,
                           name):
        """Builds the policy model used by this SAC.

        Override this method in a sub-class of SACTFModel to implement your
        own policy net. Alternatively, simply set `custom_model` within the
        top level SAC `policy_model` config key to make this default
        implementation of `build_policy_model` use your custom policy network.

        Returns:
            TorchModelV2: The TorchModelV2 policy sub-model.
        """
        model = ModelCatalog.get_model_v2(
            obs_space,
            self.action_space,
            num_outputs,
            policy_model_config,
            framework="torch",
            name=name)
        return model

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 2:</b> &nbsp; 2 fragments, nominal size 29 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag341')" href="javascript:;">
ray-ray-1.11.0/rllib/agents/sac/sac_tf_model.py: 149-189
</a>
<div class="mid" id="frag341" style="display:none"><pre>
    def build_q_model(self, obs_space, action_space, num_outputs,
                      q_model_config, name):
        """Builds one of the (twin) Q-nets used by this SAC.

        Override this method in a sub-class of SACTFModel to implement your
        own Q-nets. Alternatively, simply set `custom_model` within the
        top level SAC `Q_model` config key to make this default implementation
        of `build_q_model` use your custom Q-nets.

        Returns:
            TFModelV2: The TFModelV2 Q-net sub-model.
        """
        self.concat_obs_and_actions = False
        if self.discrete:
            input_space = obs_space
        else:
            orig_space = getattr(obs_space, "original_space", obs_space)
            if isinstance(orig_space, Box) and len(orig_space.shape) == 1:
                input_space = Box(
                    float("-inf"),
                    float("inf"),
                    shape=(orig_space.shape[0] + action_space.shape[0], ))
                self.concat_obs_and_actions = True
            else:
                if isinstance(orig_space, gym.spaces.Tuple):
                    spaces = list(orig_space.spaces)
                elif isinstance(orig_space, gym.spaces.Dict):
                    spaces = list(orig_space.spaces.values())
                else:
                    spaces = [obs_space]
                input_space = gym.spaces.Tuple(spaces + [action_space])

        model = ModelCatalog.get_model_v2(
            input_space,
            action_space,
            num_outputs,
            q_model_config,
            framework="tf",
            name=name)
        return model

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag362')" href="javascript:;">
ray-ray-1.11.0/rllib/agents/sac/sac_torch_model.py: 157-197
</a>
<div class="mid" id="frag362" style="display:none"><pre>
    def build_q_model(self, obs_space, action_space, num_outputs,
                      q_model_config, name):
        """Builds one of the (twin) Q-nets used by this SAC.

        Override this method in a sub-class of SACTFModel to implement your
        own Q-nets. Alternatively, simply set `custom_model` within the
        top level SAC `Q_model` config key to make this default implementation
        of `build_q_model` use your custom Q-nets.

        Returns:
            TorchModelV2: The TorchModelV2 Q-net sub-model.
        """
        self.concat_obs_and_actions = False
        if self.discrete:
            input_space = obs_space
        else:
            orig_space = getattr(obs_space, "original_space", obs_space)
            if isinstance(orig_space, Box) and len(orig_space.shape) == 1:
                input_space = Box(
                    float("-inf"),
                    float("inf"),
                    shape=(orig_space.shape[0] + action_space.shape[0], ))
                self.concat_obs_and_actions = True
            else:
                if isinstance(orig_space, gym.spaces.Tuple):
                    spaces = list(orig_space.spaces)
                elif isinstance(orig_space, gym.spaces.Dict):
                    spaces = list(orig_space.spaces.values())
                else:
                    spaces = [obs_space]
                input_space = gym.spaces.Tuple(spaces + [action_space])

        model = ModelCatalog.get_model_v2(
            input_space,
            action_space,
            num_outputs,
            q_model_config,
            framework="torch",
            name=name)
        return model

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 3:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag344')" href="javascript:;">
ray-ray-1.11.0/rllib/agents/sac/sac_tf_model.py: 229-257
</a>
<div class="mid" id="frag344" style="display:none"><pre>
    def _get_q_value(self, model_out, actions, net):
        # Model outs may come as original Tuple/Dict observations, concat them
        # here if this is the case.
        if isinstance(net.obs_space, Box):
            if isinstance(model_out, (list, tuple)):
                model_out = tf.concat(model_out, axis=-1)
            elif isinstance(model_out, dict):
                model_out = tf.concat(list(model_out.values()), axis=-1)
        elif isinstance(model_out, dict):
            model_out = list(model_out.values())

        # Continuous case -&gt; concat actions to model_out.
        if actions is not None:
            if self.concat_obs_and_actions:
                input_dict = {"obs": tf.concat([model_out, actions], axis=-1)}
            else:
                # TODO(junogng) : SampleBatch doesn't support list columns yet.
                #     Use ModelInputDict.
                input_dict = {"obs": force_list(model_out) + [actions]}
        # Discrete case -&gt; return q-vals for all actions.
        else:
            input_dict = {"obs": model_out}
        # Switch on training mode (when getting Q-values, we are usually in
        # training).
        input_dict["is_training"] = True

        out, _ = net(input_dict, [], None)
        return out

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag365')" href="javascript:;">
ray-ray-1.11.0/rllib/agents/sac/sac_torch_model.py: 237-265
</a>
<div class="mid" id="frag365" style="display:none"><pre>
    def _get_q_value(self, model_out, actions, net):
        # Model outs may come as original Tuple observations, concat them
        # here if this is the case.
        if isinstance(net.obs_space, Box):
            if isinstance(model_out, (list, tuple)):
                model_out = torch.cat(model_out, dim=-1)
            elif isinstance(model_out, dict):
                model_out = torch.cat(list(model_out.values()), dim=-1)
        elif isinstance(model_out, dict):
            model_out = list(model_out.values())

        # Continuous case -&gt; concat actions to model_out.
        if actions is not None:
            if self.concat_obs_and_actions:
                input_dict = {"obs": torch.cat([model_out, actions], dim=-1)}
            else:
                # TODO(junogng) : SampleBatch doesn't support list columns yet.
                #     Use ModelInputDict.
                input_dict = {"obs": force_list(model_out) + [actions]}
        # Discrete case -&gt; return q-vals for all actions.
        else:
            input_dict = {"obs": model_out}
        # Switch on training mode (when getting Q-values, we are usually in
        # training).
        input_dict["is_training"] = True

        out, _ = net(input_dict, [], None)
        return out

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 4:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag345')" href="javascript:;">
ray-ray-1.11.0/rllib/agents/sac/sac_tf_model.py: 258-287
</a>
<div class="mid" id="frag345" style="display:none"><pre>
    def get_policy_output(self, model_out: TensorType) -&gt; TensorType:
        """Returns policy outputs, given the output of self.__call__().

        For continuous action spaces, these will be the mean/stddev
        distribution inputs for the (SquashedGaussian) action distribution.
        For discrete action spaces, these will be the logits for a categorical
        distribution.

        Args:
            model_out (TensorType): Feature outputs from the model layers
                (result of doing `self.__call__(obs)`).

        Returns:
            TensorType: Distribution inputs for sampling actions.
        """
        # Model outs may come as original Tuple/Dict observations, concat them
        # here if this is the case.
        if isinstance(self.action_model.obs_space, Box):
            if isinstance(model_out, (list, tuple)):
                model_out = tf.concat(model_out, axis=-1)
            elif isinstance(model_out, dict):
                model_out = tf.concat(
                    [
                        tf.expand_dims(val, 1) if len(val.shape) == 1 else val
                        for val in tree.flatten(model_out.values())
                    ],
                    axis=-1)
        out, _ = self.action_model({"obs": model_out}, [], None)
        return out

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag366')" href="javascript:;">
ray-ray-1.11.0/rllib/agents/sac/sac_torch_model.py: 266-295
</a>
<div class="mid" id="frag366" style="display:none"><pre>
    def get_policy_output(self, model_out: TensorType) -&gt; TensorType:
        """Returns policy outputs, given the output of self.__call__().

        For continuous action spaces, these will be the mean/stddev
        distribution inputs for the (SquashedGaussian) action distribution.
        For discrete action spaces, these will be the logits for a categorical
        distribution.

        Args:
            model_out (TensorType): Feature outputs from the model layers
                (result of doing `self.__call__(obs)`).

        Returns:
            TensorType: Distribution inputs for sampling actions.
        """
        # Model outs may come as original Tuple observations, concat them
        # here if this is the case.
        if isinstance(self.action_model.obs_space, Box):
            if isinstance(model_out, (list, tuple)):
                model_out = torch.cat(model_out, dim=-1)
            elif isinstance(model_out, dict):
                model_out = torch.cat(
                    [
                        torch.unsqueeze(val, 1) if len(val.shape) == 1 else val
                        for val in tree.flatten(model_out.values())
                    ],
                    dim=-1)
        out, _ = self.action_model({"obs": model_out}, [], None)
        return out

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 5:</b> &nbsp; 2 fragments, nominal size 37 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag394')" href="javascript:;">
ray-ray-1.11.0/rllib/agents/marwil/tests/test_marwil.py: 26-82
</a>
<div class="mid" id="frag394" style="display:none"><pre>

    def test_marwil_compilation_and_learning_from_offline_file(self):
        """Test whether a MARWILTrainer can be built with all frameworks.

        Learns from a historic-data file.
        To generate this data, first run:
        $ ./train.py --run=PPO --env=CartPole-v0 \
          --stop='{"timesteps_total": 50000}' \
          --config='{"output": "/tmp/out", "batch_mode": "complete_episodes"}'
        """
        rllib_dir = Path(__file__).parent.parent.parent.parent
        print("rllib dir={}".format(rllib_dir))
        data_file = os.path.join(rllib_dir, "tests/data/cartpole/large.json")
        print("data_file={} exists={}".format(data_file,
                                              os.path.isfile(data_file)))

        config = marwil.DEFAULT_CONFIG.copy()
        config["num_workers"] = 2
        config["evaluation_num_workers"] = 1
        config["evaluation_interval"] = 3
        config["evaluation_duration"] = 5
        config["evaluation_parallel_to_training"] = True
        # Evaluate on actual environment.
        config["evaluation_config"] = {"input": "sampler"}
        # Learn from offline data.
        config["input"] = [data_file]
        num_iterations = 350
        min_reward = 70.0

        # Test for all frameworks.
        for _ in framework_iterator(config, frameworks=("tf", "torch")):
            trainer = marwil.MARWILTrainer(config=config, env="CartPole-v0")
            learnt = False
            for i in range(num_iterations):
                results = trainer.train()
                check_train_results(results)
                print(results)

                eval_results = results.get("evaluation")
                if eval_results:
                    print("iter={} R={} ".format(
                        i, eval_results["episode_reward_mean"]))
                    # Learn until some reward is reached on an actual live env.
                    if eval_results["episode_reward_mean"] &gt; min_reward:
                        print("learnt!")
                        learnt = True
                        break

            if not learnt:
                raise ValueError(
                    "MARWILTrainer did not reach {} reward from expert "
                    "offline data!".format(min_reward))

            check_compute_single_action(
                trainer, include_prev_action_reward=True)

            trainer.stop()
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag399')" href="javascript:;">
ray-ray-1.11.0/rllib/agents/marwil/tests/test_bc.py: 22-77
</a>
<div class="mid" id="frag399" style="display:none"><pre>

    def test_bc_compilation_and_learning_from_offline_file(self):
        """Test whether a BCTrainer can be built with all frameworks.

        And learns from a historic-data file (while being evaluated on an
        actual env using evaluation_num_workers &gt; 0).
        """
        rllib_dir = Path(__file__).parent.parent.parent.parent
        print("rllib dir={}".format(rllib_dir))
        data_file = os.path.join(rllib_dir, "tests/data/cartpole/large.json")
        print("data_file={} exists={}".format(data_file,
                                              os.path.isfile(data_file)))

        config = marwil.BC_DEFAULT_CONFIG.copy()
        config["num_workers"] = 0  # Run locally.

        config["evaluation_interval"] = 3
        config["evaluation_num_workers"] = 1
        config["evaluation_duration"] = 5
        config["evaluation_parallel_to_training"] = True
        # Evaluate on actual environment.
        config["evaluation_config"] = {"input": "sampler"}
        # Learn from offline data.
        config["input"] = [data_file]
        num_iterations = 350
        min_reward = 70.0

        # Test for all frameworks.
        for _ in framework_iterator(config, frameworks=("tf", "torch")):
            trainer = marwil.BCTrainer(config=config, env="CartPole-v0")
            learnt = False
            for i in range(num_iterations):
                results = trainer.train()
                check_train_results(results)
                print(results)

                eval_results = results.get("evaluation")
                if eval_results:
                    print("iter={} R={}".format(
                        i, eval_results["episode_reward_mean"]))
                    # Learn until good reward is reached in the actual env.
                    if eval_results["episode_reward_mean"] &gt; min_reward:
                        print("learnt!")
                        learnt = True
                        break

            if not learnt:
                raise ValueError(
                    "BCTrainer did not reach {} reward from expert offline "
                    "data!".format(min_reward))

            check_compute_single_action(
                trainer, include_prev_action_reward=True)

            trainer.stop()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 6:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1158')" href="javascript:;">
ray-ray-1.11.0/rllib/utils/tests/test_check_env.py: 42-54
</a>
<div class="mid" id="frag1158" style="display:none"><pre>

    def test_sampled_observation_contained(self):
        env = RandomEnv()
        # check for observation that is out of bounds
        error = ".*A sampled observation from your env wasn't contained .*"
        env.observation_space.sample = MagicMock(return_value=5)
        with pytest.raises(ValueError, match=error):
            check_env(env)
        # check for observation that is in bounds, but the wrong type
        env.observation_space.sample = MagicMock(return_value=float(1))
        with pytest.raises(ValueError, match=error):
            check_env(env)
        del env
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1159')" href="javascript:;">
ray-ray-1.11.0/rllib/utils/tests/test_check_env.py: 55-66
</a>
<div class="mid" id="frag1159" style="display:none"><pre>

    def test_sampled_action_contained(self):
        env = RandomEnv()
        error = ".*A sampled action from your env wasn't contained .*"
        env.action_space.sample = MagicMock(return_value=5)
        with pytest.raises(ValueError, match=error):
            check_env(env)
        # check for observation that is in bounds, but the wrong type
        env.action_space.sample = MagicMock(return_value=float(1))
        with pytest.raises(ValueError, match=error):
            check_env(env)
        del env
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 7:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1535')" href="javascript:;">
ray-ray-1.11.0/rllib/tests/test_external_multi_agent_env.py: 25-37
</a>
<div class="mid" id="frag1535" style="display:none"><pre>
    def test_external_multi_agent_env_complete_episodes(self):
        agents = 4
        ev = RolloutWorker(
            env_creator=lambda _: SimpleMultiServing(BasicMultiAgent(agents)),
            policy_spec=MockPolicy,
            rollout_fragment_length=40,
            batch_mode="complete_episodes")
        for _ in range(3):
            batch = ev.sample()
            self.assertEqual(batch.count, 40)
            self.assertEqual(
                len(np.unique(batch[SampleBatch.AGENT_INDEX])), agents)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1536')" href="javascript:;">
ray-ray-1.11.0/rllib/tests/test_external_multi_agent_env.py: 38-50
</a>
<div class="mid" id="frag1536" style="display:none"><pre>
    def test_external_multi_agent_env_truncate_episodes(self):
        agents = 4
        ev = RolloutWorker(
            env_creator=lambda _: SimpleMultiServing(BasicMultiAgent(agents)),
            policy_spec=MockPolicy,
            rollout_fragment_length=40,
            batch_mode="truncate_episodes")
        for _ in range(3):
            batch = ev.sample()
            self.assertEqual(batch.count, 160)
            self.assertEqual(
                len(np.unique(batch[SampleBatch.AGENT_INDEX])), agents)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 8:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1668')" href="javascript:;">
ray-ray-1.11.0/rllib/examples/models/shared_weights_model.py: 67-84
</a>
<div class="mid" id="frag1668" style="display:none"><pre>
    def __init__(self, observation_space, action_space, num_outputs,
                 model_config, name):
        super().__init__(observation_space, action_space, num_outputs,
                         model_config, name)

        inputs = tf.keras.layers.Input(observation_space.shape)
        with tf1.variable_scope(
                tf1.VariableScope(tf1.AUTO_REUSE, "shared"),
                reuse=tf1.AUTO_REUSE,
                auxiliary_name_scope=False):
            last_layer = tf.keras.layers.Dense(
                units=64, activation=tf.nn.relu, name="fc1")(inputs)
        output = tf.keras.layers.Dense(
            units=num_outputs, activation=None, name="fc_out")(last_layer)
        vf = tf.keras.layers.Dense(
            units=1, activation=None, name="value_out")(last_layer)
        self.base_model = tf.keras.models.Model(inputs, [output, vf])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1671')" href="javascript:;">
ray-ray-1.11.0/rllib/examples/models/shared_weights_model.py: 98-117
</a>
<div class="mid" id="frag1671" style="display:none"><pre>
    def __init__(self, observation_space, action_space, num_outputs,
                 model_config, name):
        super().__init__(observation_space, action_space, num_outputs,
                         model_config, name)

        inputs = tf.keras.layers.Input(observation_space.shape)

        # Weights shared with SharedWeightsModel1.
        with tf1.variable_scope(
                tf1.VariableScope(tf1.AUTO_REUSE, "shared"),
                reuse=tf1.AUTO_REUSE,
                auxiliary_name_scope=False):
            last_layer = tf.keras.layers.Dense(
                units=64, activation=tf.nn.relu, name="fc1")(inputs)
        output = tf.keras.layers.Dense(
            units=num_outputs, activation=None, name="fc_out")(last_layer)
        vf = tf.keras.layers.Dense(
            units=1, activation=None, name="value_out")(last_layer)
        self.base_model = tf.keras.models.Model(inputs, [output, vf])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 9:</b> &nbsp; 9 fragments, nominal size 13 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1898')" href="javascript:;">
ray-ray-1.11.0/rllib/examples/env/dm_control_suite.py: 7-21
</a>
<div class="mid" id="frag1898" style="display:none"><pre>
def acrobot_swingup(from_pixels=True,
                    height=64,
                    width=64,
                    frame_skip=2,
                    channels_first=True):
    return DMCEnv(
        "acrobot",
        "swingup",
        from_pixels=from_pixels,
        height=height,
        width=width,
        frame_skip=frame_skip,
        channels_first=channels_first)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1899')" href="javascript:;">
ray-ray-1.11.0/rllib/examples/env/dm_control_suite.py: 22-36
</a>
<div class="mid" id="frag1899" style="display:none"><pre>
def walker_walk(from_pixels=True,
                height=64,
                width=64,
                frame_skip=2,
                channels_first=True):
    return DMCEnv(
        "walker",
        "walk",
        from_pixels=from_pixels,
        height=height,
        width=width,
        frame_skip=frame_skip,
        channels_first=channels_first)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1903')" href="javascript:;">
ray-ray-1.11.0/rllib/examples/env/dm_control_suite.py: 82-96
</a>
<div class="mid" id="frag1903" style="display:none"><pre>
def walker_run(from_pixels=True,
               height=64,
               width=64,
               frame_skip=2,
               channels_first=True):
    return DMCEnv(
        "walker",
        "run",
        from_pixels=from_pixels,
        height=height,
        width=width,
        frame_skip=frame_skip,
        channels_first=channels_first)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1900')" href="javascript:;">
ray-ray-1.11.0/rllib/examples/env/dm_control_suite.py: 37-51
</a>
<div class="mid" id="frag1900" style="display:none"><pre>
def hopper_hop(from_pixels=True,
               height=64,
               width=64,
               frame_skip=2,
               channels_first=True):
    return DMCEnv(
        "hopper",
        "hop",
        from_pixels=from_pixels,
        height=height,
        width=width,
        frame_skip=frame_skip,
        channels_first=channels_first)


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1904')" href="javascript:;">
ray-ray-1.11.0/rllib/examples/env/dm_control_suite.py: 97-111
</a>
<div class="mid" id="frag1904" style="display:none"><pre>
def pendulum_swingup(from_pixels=True,
                     height=64,
                     width=64,
                     frame_skip=2,
                     channels_first=True):
    return DMCEnv(
        "pendulum",
        "swingup",
        from_pixels=from_pixels,
        height=height,
        width=width,
        frame_skip=frame_skip,
        channels_first=channels_first)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1901')" href="javascript:;">
ray-ray-1.11.0/rllib/examples/env/dm_control_suite.py: 52-66
</a>
<div class="mid" id="frag1901" style="display:none"><pre>
def hopper_stand(from_pixels=True,
                 height=64,
                 width=64,
                 frame_skip=2,
                 channels_first=True):
    return DMCEnv(
        "hopper",
        "stand",
        from_pixels=from_pixels,
        height=height,
        width=width,
        frame_skip=frame_skip,
        channels_first=channels_first)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1905')" href="javascript:;">
ray-ray-1.11.0/rllib/examples/env/dm_control_suite.py: 112-126
</a>
<div class="mid" id="frag1905" style="display:none"><pre>
def cartpole_swingup(from_pixels=True,
                     height=64,
                     width=64,
                     frame_skip=2,
                     channels_first=True):
    return DMCEnv(
        "cartpole",
        "swingup",
        from_pixels=from_pixels,
        height=height,
        width=width,
        frame_skip=frame_skip,
        channels_first=channels_first)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1902')" href="javascript:;">
ray-ray-1.11.0/rllib/examples/env/dm_control_suite.py: 67-81
</a>
<div class="mid" id="frag1902" style="display:none"><pre>
def cheetah_run(from_pixels=True,
                height=64,
                width=64,
                frame_skip=2,
                channels_first=True):
    return DMCEnv(
        "cheetah",
        "run",
        from_pixels=from_pixels,
        height=height,
        width=width,
        frame_skip=frame_skip,
        channels_first=channels_first)


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1906')" href="javascript:;">
ray-ray-1.11.0/rllib/examples/env/dm_control_suite.py: 127-139
</a>
<div class="mid" id="frag1906" style="display:none"><pre>
def humanoid_walk(from_pixels=True,
                  height=64,
                  width=64,
                  frame_skip=2,
                  channels_first=True):
    return DMCEnv(
        "humanoid",
        "walk",
        from_pixels=from_pixels,
        height=height,
        width=width,
        frame_skip=frame_skip,
        channels_first=channels_first)
</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 10:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2165')" href="javascript:;">
ray-ray-1.11.0/release/lightgbm_tests/workloads/train_small.py: 42-55
</a>
<div class="mid" id="frag2165" style="display:none"><pre>
    def train():
        os.environ["TEST_OUTPUT_JSON"] = output
        os.environ["TEST_STATE_JSON"] = state
        train_ray(
            path="/data/classification.parquet",
            num_workers=4,
            num_boost_rounds=100,
            num_files=25,
            regression=False,
            use_gpu=False,
            ray_params=ray_params,
            lightgbm_params=None,
        )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2240')" href="javascript:;">
ray-ray-1.11.0/release/xgboost_tests/workloads/train_small.py: 42-55
</a>
<div class="mid" id="frag2240" style="display:none"><pre>
    def train():
        os.environ["TEST_OUTPUT_JSON"] = output
        os.environ["TEST_STATE_JSON"] = state
        train_ray(
            path="/data/classification.parquet",
            num_workers=4,
            num_boost_rounds=100,
            num_files=25,
            regression=False,
            use_gpu=False,
            ray_params=ray_params,
            xgboost_params=None,
        )

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 11:</b> &nbsp; 6 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2167')" href="javascript:;">
ray-ray-1.11.0/release/lightgbm_tests/workloads/tune_small.py: 25-37
</a>
<div class="mid" id="frag2167" style="display:none"><pre>
def train_wrapper(config, ray_params):
    train_ray(
        path="/data/classification.parquet",
        num_workers=4,
        num_boost_rounds=100,
        num_files=25,
        regression=False,
        use_gpu=False,
        ray_params=ray_params,
        lightgbm_params=config,
    )


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2168')" href="javascript:;">
ray-ray-1.11.0/release/lightgbm_tests/workloads/tune_4x32.py: 25-37
</a>
<div class="mid" id="frag2168" style="display:none"><pre>
def train_wrapper(config, ray_params):
    train_ray(
        path="/data/classification.parquet",
        num_workers=32,
        num_boost_rounds=100,
        num_files=128,
        regression=False,
        use_gpu=False,
        ray_params=ray_params,
        lightgbm_params=config,
    )


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2245')" href="javascript:;">
ray-ray-1.11.0/release/xgboost_tests/workloads/tune_32x4.py: 25-37
</a>
<div class="mid" id="frag2245" style="display:none"><pre>
def train_wrapper(config, ray_params):
    train_ray(
        path="/data/classification.parquet",
        num_workers=4,
        num_boost_rounds=100,
        num_files=64,
        regression=False,
        use_gpu=False,
        ray_params=ray_params,
        xgboost_params=config,
    )


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2169')" href="javascript:;">
ray-ray-1.11.0/release/lightgbm_tests/workloads/tune_32x4.py: 25-37
</a>
<div class="mid" id="frag2169" style="display:none"><pre>
def train_wrapper(config, ray_params):
    train_ray(
        path="/data/classification.parquet",
        num_workers=4,
        num_boost_rounds=100,
        num_files=64,
        regression=False,
        use_gpu=False,
        ray_params=ray_params,
        lightgbm_params=config,
    )


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2244')" href="javascript:;">
ray-ray-1.11.0/release/xgboost_tests/workloads/tune_4x32.py: 25-37
</a>
<div class="mid" id="frag2244" style="display:none"><pre>
def train_wrapper(config, ray_params):
    train_ray(
        path="/data/classification.parquet",
        num_workers=32,
        num_boost_rounds=100,
        num_files=128,
        regression=False,
        use_gpu=False,
        ray_params=ray_params,
        xgboost_params=config,
    )


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2243')" href="javascript:;">
ray-ray-1.11.0/release/xgboost_tests/workloads/tune_small.py: 25-37
</a>
<div class="mid" id="frag2243" style="display:none"><pre>
def train_wrapper(config, ray_params):
    train_ray(
        path="/data/classification.parquet",
        num_workers=4,
        num_boost_rounds=100,
        num_files=25,
        regression=False,
        use_gpu=False,
        ray_params=ray_params,
        xgboost_params=config,
    )


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 12:</b> &nbsp; 3 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2191')" href="javascript:;">
ray-ray-1.11.0/release/long_running_tests/workloads/actor_deaths.py: 73-84
</a>
<div class="mid" id="frag2191" style="display:none"><pre>
    def ping(self, num_pings):
        children_outputs = []
        for _ in range(num_pings):
            children_outputs += [
                child.ping.remote() for child in self.children
            ]
        try:
            ray.get(children_outputs)
        except Exception:
            # Replace the children if one of them died.
            self.__init__(len(self.children), self.death_probability)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5853')" href="javascript:;">
ray-ray-1.11.0/python/ray/tests/test_threaded_actor.py: 219-230
</a>
<div class="mid" id="frag5853" style="display:none"><pre>
        def ping(self, num_pings):
            children_outputs = []
            for _ in range(num_pings):
                children_outputs += [
                    child.ping.remote() for child in self.children
                ]
            try:
                ray.get(children_outputs)
            except Exception:
                # Replace the children if one of them died.
                self.__init__(len(self.children), self.death_probability)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2333')" href="javascript:;">
ray-ray-1.11.0/release/nightly_tests/stress_tests/test_dead_actors.py: 39-50
</a>
<div class="mid" id="frag2333" style="display:none"><pre>
    def ping(self, num_pings):
        children_outputs = []
        for _ in range(num_pings):
            children_outputs += [
                child.ping.remote() for child in self.children
            ]
        try:
            ray.get(children_outputs)
        except Exception:
            # Replace the children if one of them died.
            self.__init__(len(self.children), self.death_probability)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 13:</b> &nbsp; 3 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2225')" href="javascript:;">
ray-ray-1.11.0/release/serve_tests/workloads/serve_cluster_fault_tolerance_gcs.py: 28-39
</a>
<div class="mid" id="frag2225" style="display:none"><pre>
def request_with_retries(endpoint, timeout=3):
    start = time.time()
    while True:
        try:
            return requests.get(
                "http://127.0.0.1:8000" + endpoint, timeout=timeout)
        except requests.RequestException:
            if time.time() - start &gt; timeout:
                raise TimeoutError
            time.sleep(0.1)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7817')" href="javascript:;">
ray-ray-1.11.0/python/ray/serve/tests/test_failure.py: 12-23
</a>
<div class="mid" id="frag7817" style="display:none"><pre>
def request_with_retries(endpoint, timeout=30):
    start = time.time()
    while True:
        try:
            return requests.get(
                "http://127.0.0.1:8000" + endpoint, timeout=timeout)
        except requests.RequestException:
            if time.time() - start &gt; timeout:
                raise TimeoutError
            time.sleep(0.1)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2229')" href="javascript:;">
ray-ray-1.11.0/release/serve_tests/workloads/serve_cluster_fault_tolerance.py: 30-41
</a>
<div class="mid" id="frag2229" style="display:none"><pre>
def request_with_retries(endpoint, timeout=3):
    start = time.time()
    while True:
        try:
            return requests.get(
                "http://127.0.0.1:8000" + endpoint, timeout=timeout)
        except requests.RequestException:
            if time.time() - start &gt; timeout:
                raise TimeoutError
            time.sleep(0.1)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 14:</b> &nbsp; 3 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2378')" href="javascript:;">
ray-ray-1.11.0/doc/kubernetes/example_scripts/job_example.py: 18-32
</a>
<div class="mid" id="frag2378" style="display:none"><pre>
def wait_for_nodes(expected):
    # Wait for all nodes to join the cluster.
    while True:
        resources = ray.cluster_resources()
        node_keys = [key for key in resources if "node" in key]
        num_nodes = sum(resources[node_key] for node_key in node_keys)
        if num_nodes &lt; expected:
            print("{} nodes have joined so far, waiting for {} more.".format(
                num_nodes, expected - num_nodes))
            sys.stdout.flush()
            time.sleep(1)
        else:
            break


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2384')" href="javascript:;">
ray-ray-1.11.0/doc/kubernetes/example_scripts/run_local_example.py: 25-39
</a>
<div class="mid" id="frag2384" style="display:none"><pre>
def wait_for_nodes(expected):
    # Wait for all nodes to join the cluster.
    while True:
        resources = ray.cluster_resources()
        node_keys = [key for key in resources if "node" in key]
        num_nodes = sum(resources[node_key] for node_key in node_keys)
        if num_nodes &lt; expected:
            print("{} nodes have joined so far, waiting for {} more.".format(
                num_nodes, expected - num_nodes))
            sys.stdout.flush()
            time.sleep(1)
        else:
            break


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2381')" href="javascript:;">
ray-ray-1.11.0/doc/kubernetes/example_scripts/run_on_head.py: 17-31
</a>
<div class="mid" id="frag2381" style="display:none"><pre>
def wait_for_nodes(expected):
    # Wait for all nodes to join the cluster.
    while True:
        resources = ray.cluster_resources()
        node_keys = [key for key in resources if "node" in key]
        num_nodes = sum(resources[node_key] for node_key in node_keys)
        if num_nodes &lt; expected:
            print("{} nodes have joined so far, waiting for {} more.".format(
                num_nodes, expected - num_nodes))
            sys.stdout.flush()
            time.sleep(1)
        else:
            break


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 15:</b> &nbsp; 3 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2379')" href="javascript:;">
ray-ray-1.11.0/doc/kubernetes/example_scripts/job_example.py: 33-48
</a>
<div class="mid" id="frag2379" style="display:none"><pre>
def main():
    wait_for_nodes(3)

    # Check that objects can be transferred from each node to each other node.
    for i in range(10):
        print("Iteration {}".format(i))
        results = [
            gethostname.remote(gethostname.remote(())) for _ in range(100)
        ]
        print(Counter(ray.get(results)))
        sys.stdout.flush()

    print("Success!")
    sys.stdout.flush()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2385')" href="javascript:;">
ray-ray-1.11.0/doc/kubernetes/example_scripts/run_local_example.py: 40-55
</a>
<div class="mid" id="frag2385" style="display:none"><pre>
def main():
    wait_for_nodes(3)

    # Check that objects can be transferred from each node to each other node.
    for i in range(10):
        print("Iteration {}".format(i))
        results = [
            gethostname.remote(gethostname.remote(())) for _ in range(100)
        ]
        print(Counter(ray.get(results)))
        sys.stdout.flush()

    print("Success!")
    sys.stdout.flush()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2382')" href="javascript:;">
ray-ray-1.11.0/doc/kubernetes/example_scripts/run_on_head.py: 32-47
</a>
<div class="mid" id="frag2382" style="display:none"><pre>
def main():
    wait_for_nodes(3)

    # Check that objects can be transferred from each node to each other node.
    for i in range(10):
        print("Iteration {}".format(i))
        results = [
            gethostname.remote(gethostname.remote(())) for _ in range(100)
        ]
        print(Counter(ray.get(results)))
        sys.stdout.flush()

    print("Success!")
    sys.stdout.flush()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 16:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2601')" href="javascript:;">
ray-ray-1.11.0/ci/travis/py_dep_analysis_test.py: 40-59
</a>
<div class="mid" id="frag2601" style="display:none"><pre>
    def test_import_line_continuation(self):
        graph = pda.DepGraph()
        graph.ids["ray"] = 0

        with tempfile.TemporaryDirectory() as tmpdir:
            src_path = os.path.join(tmpdir, "continuation1.py")
            self.create_tmp_file(
                src_path, """
import ray.rllib.env.\\
    mock_env
b = 2
""")
            pda._process_file(graph, src_path, "ray")

        self.assertEqual(len(graph.ids), 2)
        print(graph.ids)
        # Shoud pick up the full module name.
        self.assertEqual(graph.ids["ray.rllib.env.mock_env"], 1)
        self.assertEqual(graph.edges[0], {1: True})

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2602')" href="javascript:;">
ray-ray-1.11.0/ci/travis/py_dep_analysis_test.py: 60-79
</a>
<div class="mid" id="frag2602" style="display:none"><pre>
    def test_import_line_continuation_parenthesis(self):
        graph = pda.DepGraph()
        graph.ids["ray"] = 0

        with tempfile.TemporaryDirectory() as tmpdir:
            src_path = os.path.join(tmpdir, "continuation1.py")
            self.create_tmp_file(
                src_path, """
from ray.rllib.env import (ClassName,
    module1, module2)
b = 2
""")
            pda._process_file(graph, src_path, "ray")

        self.assertEqual(len(graph.ids), 2)
        print(graph.ids)
        # Shoud pick up the full module name without trailing (.
        self.assertEqual(graph.ids["ray.rllib.env"], 1)
        self.assertEqual(graph.edges[0], {1: True})

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 17:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3002')" href="javascript:;">
ray-ray-1.11.0/python/ray/tune/tests/test_torch_trainable.py: 154-166
</a>
<div class="mid" id="frag3002" style="display:none"><pre>
def test_colocated_gpu(ray_4_node_gpu):  # noqa: F811
    assert ray.available_resources()["GPU"] == 8
    trainable_cls = DistributedTrainableCreator(
        _train_check_global,
        num_workers=4,
        num_gpus_per_worker=2,
        num_workers_per_host=1)
    trainable = trainable_cls()
    assert ray.available_resources().get("GPU", 0) == 0
    trainable.train()
    trainable.stop()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3194')" href="javascript:;">
ray-ray-1.11.0/python/ray/tune/tests/test_tensorflow_trainable.py: 115-127
</a>
<div class="mid" id="frag3194" style="display:none"><pre>
def test_colocated_gpu(ray_4_node_gpu):  # noqa: F811
    assert ray.available_resources()["GPU"] == 8
    trainable_cls = DistributedTrainableCreator(
        _train_check_global,
        num_workers=4,
        num_gpus_per_worker=2,
        num_workers_per_host=1)
    trainable = trainable_cls()
    assert ray.available_resources().get("GPU", 0) == 0
    trainable.train()
    trainable.stop()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 18:</b> &nbsp; 2 fragments, nominal size 36 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3365')" href="javascript:;">
ray-ray-1.11.0/python/ray/tune/tests/test_tune_restore.py: 293-335
</a>
<div class="mid" id="frag3365" style="display:none"><pre>
    def testFailResumeWithPreset(self):
        os.environ["TUNE_MAX_PENDING_TRIALS_PG"] = "1"

        search_alg = BasicVariantGenerator(points_to_evaluate=[{
            "test": -1,
            "test2": -1
        }, {
            "test": -1
        }, {
            "test2": -1
        }])

        config = dict(
            num_samples=3 + 3,  # 3 preset, 3 samples
            fail_fast=True,
            config={
                "test": tune.grid_search([1, 2, 3]),
                "test2": tune.grid_search([1, 2, 3]),
            },
            stop={"training_iteration": 2},
            local_dir=self.logdir,
            verbose=1)
        with self.assertRaises(RuntimeError):
            tune.run(
                "trainable",
                callbacks=[self.FailureInjectorCallback(5)],
                search_alg=search_alg,
                **config)

        analysis = tune.run(
            "trainable",
            resume=True,
            callbacks=[self.CheckStateCallback(expected_trials=5)],
            search_alg=search_alg,
            **config)
        assert len(analysis.trials) == 34
        test_counter = Counter([t.config["test"] for t in analysis.trials])
        assert test_counter.pop(-1) == 4
        assert all(v == 10 for v in test_counter.values())
        test2_counter = Counter([t.config["test2"] for t in analysis.trials])
        assert test2_counter.pop(-1) == 4
        assert all(v == 10 for v in test2_counter.values())

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3366')" href="javascript:;">
ray-ray-1.11.0/python/ray/tune/tests/test_tune_restore.py: 336-379
</a>
<div class="mid" id="frag3366" style="display:none"><pre>
    def testFailResumeAfterPreset(self):
        os.environ["TUNE_MAX_PENDING_TRIALS_PG"] = "1"

        search_alg = BasicVariantGenerator(points_to_evaluate=[{
            "test": -1,
            "test2": -1
        }, {
            "test": -1
        }, {
            "test2": -1
        }])

        config = dict(
            num_samples=3 + 3,  # 3 preset, 3 samples
            fail_fast=True,
            config={
                "test": tune.grid_search([1, 2, 3]),
                "test2": tune.grid_search([1, 2, 3]),
            },
            stop={"training_iteration": 2},
            local_dir=self.logdir,
            verbose=1)

        with self.assertRaises(RuntimeError):
            tune.run(
                "trainable",
                callbacks=[self.FailureInjectorCallback(15)],
                search_alg=search_alg,
                **config)

        analysis = tune.run(
            "trainable",
            resume=True,
            callbacks=[self.CheckStateCallback(expected_trials=15)],
            search_alg=search_alg,
            **config)
        assert len(analysis.trials) == 34
        test_counter = Counter([t.config["test"] for t in analysis.trials])
        assert test_counter.pop(-1) == 4
        assert all(v == 10 for v in test_counter.values())
        test2_counter = Counter([t.config["test2"] for t in analysis.trials])
        assert test2_counter.pop(-1) == 4
        assert all(v == 10 for v in test2_counter.values())

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 19:</b> &nbsp; 4 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3407')" href="javascript:;">
ray-ray-1.11.0/python/ray/tune/tests/test_searchers.py: 80-95
</a>
<div class="mid" id="frag3407" style="display:none"><pre>
    def testBayesOpt(self):
        from ray.tune.suggest.bayesopt import BayesOptSearch

        out = tune.run(
            _invalid_objective,
            # At least one nan, inf, -inf and float
            search_alg=BayesOptSearch(random_state=1234),
            config=self.config,
            metric="_metric",
            mode="max",
            num_samples=8,
            reuse_actors=False)

        best_trial = out.best_trial
        self.assertLessEqual(best_trial.config["report"], 2.0)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3412')" href="javascript:;">
ray-ray-1.11.0/python/ray/tune/tests/test_searchers.py: 176-191
</a>
<div class="mid" id="frag3412" style="display:none"><pre>
    def testHEBO(self):
        from ray.tune.suggest.hebo import HEBOSearch

        out = tune.run(
            _invalid_objective,
            # At least one nan, inf, -inf and float
            search_alg=HEBOSearch(random_state_seed=123),
            config=self.config,
            metric="_metric",
            mode="max",
            num_samples=8,
            reuse_actors=False)

        best_trial = out.best_trial
        self.assertLessEqual(best_trial.config["report"], 2.0)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3409')" href="javascript:;">
ray-ray-1.11.0/python/ray/tune/tests/test_searchers.py: 119-133
</a>
<div class="mid" id="frag3409" style="display:none"><pre>
    def testBOHB(self):
        from ray.tune.suggest.bohb import TuneBOHB

        out = tune.run(
            _invalid_objective,
            search_alg=TuneBOHB(seed=1000),
            config=self.config,
            metric="_metric",
            mode="max",
            num_samples=8,
            reuse_actors=False)

        best_trial = out.best_trial
        self.assertLessEqual(best_trial.config["report"], 2.0)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3413')" href="javascript:;">
ray-ray-1.11.0/python/ray/tune/tests/test_searchers.py: 192-207
</a>
<div class="mid" id="frag3413" style="display:none"><pre>
    def testHyperopt(self):
        from ray.tune.suggest.hyperopt import HyperOptSearch

        out = tune.run(
            _invalid_objective,
            # At least one nan, inf, -inf and float
            search_alg=HyperOptSearch(random_state_seed=1234),
            config=self.config,
            metric="_metric",
            mode="max",
            num_samples=8,
            reuse_actors=False)

        best_trial = out.best_trial
        self.assertLessEqual(best_trial.config["report"], 2.0)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 20:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3427')" href="javascript:;">
ray-ray-1.11.0/python/ray/tune/tests/test_searchers.py: 433-451
</a>
<div class="mid" id="frag3427" style="display:none"><pre>
    def testHEBO(self):
        from ray.tune.suggest.hebo import HEBOSearch

        searcher = HEBOSearch(
            space=self.space,
            metric="metric",
            mode="max",
        )

        point = {
            self.param_name: self.valid_value,
        }

        get_len_X = lambda s: len(s._opt.X)  # noqa E731
        get_len_y = lambda s: len(s._opt.y)  # noqa E731

        self.run_add_evaluated_point(point, searcher, get_len_X, get_len_y)
        self.run_add_evaluated_trials(searcher, get_len_X, get_len_y)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3428')" href="javascript:;">
ray-ray-1.11.0/python/ray/tune/tests/test_searchers.py: 452-471
</a>
<div class="mid" id="frag3428" style="display:none"><pre>
    def testSkOpt(self):
        from ray.tune.suggest.skopt import SkOptSearch

        searcher = SkOptSearch(
            space=self.space,
            metric="metric",
            mode="max",
        )

        point = {
            self.param_name: self.valid_value,
        }

        get_len_X = lambda s: len(s._skopt_opt.Xi)  # noqa E731
        get_len_y = lambda s: len(s._skopt_opt.yi)  # noqa E731

        self.run_add_evaluated_point(point, searcher, get_len_X, get_len_y)
        self.run_add_evaluated_trials(searcher, get_len_X, get_len_y)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 21:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3562')" href="javascript:;">
ray-ray-1.11.0/python/ray/tune/suggest/bohb.py: 168-182
</a>
<div class="mid" id="frag3562" style="display:none"><pre>

        bohb_config = self._bohb_config or {}
        self.bohber = BOHB(self._space, **bohb_config)

    def set_search_properties(self, metric: Optional[str], mode: Optional[str],
                              config: Dict, **spec) -&gt; bool:
        if self._space:
            return False
        space = self.convert_search_space(config)
        self._space = space

        if metric:
            self._metric = metric
        if mode:
            self._mode = mode
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3583')" href="javascript:;">
ray-ray-1.11.0/python/ray/tune/suggest/dragonfly.py: 310-323
</a>
<div class="mid" id="frag3583" style="display:none"><pre>
            logger.warning("Only non errored and non pruned points"
                           " can be added to dragonfly.")

    def set_search_properties(self, metric: Optional[str], mode: Optional[str],
                              config: Dict, **spec) -&gt; bool:
        if self._opt:
            return False
        space = self.convert_search_space(config)
        self._space = space
        if metric:
            self._metric = metric
        if mode:
            self._mode = mode

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 22:</b> &nbsp; 3 fragments, nominal size 17 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3718')" href="javascript:;">
ray-ray-1.11.0/python/ray/tune/examples/optuna_example.py: 33-54
</a>
<div class="mid" id="frag3718" style="display:none"><pre>
def run_optuna_tune(smoke_test=False):
    algo = OptunaSearch()
    algo = ConcurrencyLimiter(algo, max_concurrent=4)
    scheduler = AsyncHyperBandScheduler()
    analysis = tune.run(
        easy_objective,
        metric="mean_loss",
        mode="min",
        search_alg=algo,
        scheduler=scheduler,
        num_samples=10 if smoke_test else 100,
        config={
            "steps": 100,
            "width": tune.uniform(0, 20),
            "height": tune.uniform(-100, 100),
            # This is an ignored parameter.
            "activation": tune.choice(["relu", "tanh"])
        })

    print("Best hyperparameters found were: ", analysis.best_config)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3903')" href="javascript:;">
ray-ray-1.11.0/python/ray/tune/examples/blendsearch_example.py: 30-51
</a>
<div class="mid" id="frag3903" style="display:none"><pre>
def run_blendsearch_tune(smoke_test=False):
    algo = BlendSearch()
    algo = ConcurrencyLimiter(algo, max_concurrent=4)
    scheduler = AsyncHyperBandScheduler()
    analysis = tune.run(
        easy_objective,
        metric="mean_loss",
        mode="min",
        search_alg=algo,
        scheduler=scheduler,
        num_samples=10 if smoke_test else 100,
        config={
            "steps": 100,
            "width": tune.uniform(0, 20),
            "height": tune.uniform(-100, 100),
            # This is an ignored parameter.
            "activation": tune.choice(["relu", "tanh"])
        })

    print("Best hyperparameters found were: ", analysis.best_config)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3802')" href="javascript:;">
ray-ray-1.11.0/python/ray/tune/examples/cfo_example.py: 30-51
</a>
<div class="mid" id="frag3802" style="display:none"><pre>
def run_cfo_tune(smoke_test=False):
    algo = CFO()
    algo = ConcurrencyLimiter(algo, max_concurrent=4)
    scheduler = AsyncHyperBandScheduler()
    analysis = tune.run(
        easy_objective,
        metric="mean_loss",
        mode="min",
        search_alg=algo,
        scheduler=scheduler,
        num_samples=10 if smoke_test else 100,
        config={
            "steps": 100,
            "width": tune.uniform(0, 20),
            "height": tune.uniform(-100, 100),
            # This is an ignored parameter.
            "activation": tune.choice(["relu", "tanh"])
        })

    print("Best hyperparameters found were: ", analysis.best_config)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 23:</b> &nbsp; 3 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3773')" href="javascript:;">
ray-ray-1.11.0/python/ray/tune/examples/mnist_ptl_mini.py: 29-39
</a>
<div class="mid" id="frag3773" style="display:none"><pre>
    def forward(self, x):
        batch_size, channels, width, height = x.size()
        x = x.view(batch_size, -1)
        x = self.layer_1(x)
        x = torch.relu(x)
        x = self.layer_2(x)
        x = torch.relu(x)
        x = self.layer_3(x)
        x = torch.log_softmax(x, dim=1)
        return x

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6840')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/sgd/torch/examples/pytorch-lightning/mnist-ptl.py: 37-50
</a>
<div class="mid" id="frag6840" style="display:none"><pre>
    def forward(self, x):
        batch_size, channels, width, height = x.size()

        # (b, 1, 28, 28) -&gt; (b, 1*28*28)
        x = x.view(batch_size, -1)
        x = self.layer_1(x)
        x = torch.relu(x)
        x = self.layer_2(x)
        x = torch.relu(x)
        x = self.layer_3(x)

        x = torch.log_softmax(x, dim=1)
        return x

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3852')" href="javascript:;">
ray-ray-1.11.0/python/ray/tune/examples/mnist_pytorch_lightning.py: 48-62
</a>
<div class="mid" id="frag3852" style="display:none"><pre>

    def forward(self, x):
        batch_size, channels, width, height = x.size()
        x = x.view(batch_size, -1)

        x = self.layer_1(x)
        x = torch.relu(x)

        x = self.layer_2(x)
        x = torch.relu(x)

        x = self.layer_3(x)
        x = torch.log_softmax(x, dim=1)

        return x
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 24:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3898')" href="javascript:;">
ray-ray-1.11.0/python/ray/tune/examples/wandb_example.py: 44-60
</a>
<div class="mid" id="frag3898" style="display:none"><pre>


def tune_decorated(api_key_file):
    """Example for using the @wandb_mixin decorator with the function API"""
    analysis = tune.run(
        decorated_train_function,
        metric="loss",
        mode="min",
        config={
            "mean": tune.grid_search([1, 2, 3, 4, 5]),
            "sd": tune.uniform(0.2, 0.8),
            "wandb": {
                "api_key_file": api_key_file,
                "project": "Wandb_example"
            }
        })
    return analysis.best_config
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3900')" href="javascript:;">
ray-ray-1.11.0/python/ray/tune/examples/wandb_example.py: 69-85
</a>
<div class="mid" id="frag3900" style="display:none"><pre>


def tune_trainable(api_key_file):
    """Example for using a WandTrainableMixin with the class API"""
    analysis = tune.run(
        WandbTrainable,
        metric="loss",
        mode="min",
        config={
            "mean": tune.grid_search([1, 2, 3, 4, 5]),
            "sd": tune.uniform(0.2, 0.8),
            "wandb": {
                "api_key_file": api_key_file,
                "project": "Wandb_example"
            }
        })
    return analysis.best_config
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 25:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4431')" href="javascript:;">
ray-ray-1.11.0/python/ray/data/tests/mock_server.py: 50-67
</a>
<div class="mid" id="frag4431" style="display:none"><pre>
def stop_process(process):
    try:
        process.send_signal(signal.SIGTERM)
        process.communicate(timeout=20)
    except sp.TimeoutExpired:
        process.kill()
        outs, errors = process.communicate(timeout=20)
        exit_code = process.returncode
        msg = "Child process finished {} not in clean way: {} {}" \
            .format(exit_code, outs, errors)
        raise RuntimeError(msg)


# TODO(Clark): We should be able to use "session" scope here, but we've found
# that the s3_fs fixture ends up hanging with S3 ops timing out (or the server
# being unreachable). This appears to only be an issue when using the tmp_dir
# fixture as the S3 dir path. We should fix this since "session" scope should
# reduce a lot of the per-test overhead (2x faster execution for IO methods in
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4527')" href="javascript:;">
ray-ray-1.11.0/python/ray/workflow/tests/mock_server.py: 48-59
</a>
<div class="mid" id="frag4527" style="display:none"><pre>
def stop_process(process):
    try:
        process.send_signal(signal.SIGTERM)
        process.communicate(timeout=20)
    except sp.TimeoutExpired:
        process.kill()
        outs, errors = process.communicate(timeout=20)
        exit_code = process.returncode
        msg = "Child process finished {} not in clean way: {} {}" \
            .format(exit_code, outs, errors)
        raise RuntimeError(msg)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 26:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5030')" href="javascript:;">
ray-ray-1.11.0/python/ray/tests/test_object_spilling_2.py: 114-129
</a>
<div class="mid" id="frag5030" style="display:none"><pre>
        def create_objects(self):
            for _ in range(80):
                ref = None
                while ref is None:
                    ref = ray.put(arr)
                    self.replay_buffer.append(ref)
                # Remove the replay buffer with 60% probability.
                if random.randint(0, 9) &lt; 6:
                    self.replay_buffer.pop()

            # Do random sampling.
            for _ in range(200):
                ref = random.choice(self.replay_buffer)
                sample = ray.get(ref, timeout=0)
                assert np.array_equal(sample, arr)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5035')" href="javascript:;">
ray-ray-1.11.0/python/ray/tests/test_object_spilling_2.py: 184-199
</a>
<div class="mid" id="frag5035" style="display:none"><pre>
        def create_objects(self):
            for _ in range(80):
                ref = None
                while ref is None:
                    ref = ray.put(arr)
                    self.replay_buffer.append(ref)
                # Remove the replay buffer with 60% probability.
                if random.randint(0, 9) &lt; 6:
                    self.replay_buffer.pop()

            # Do random sampling.
            for _ in range(50):
                ref = random.choice(self.replay_buffer)
                sample = ray.get(ref, timeout=10)
                assert np.array_equal(sample, arr)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 27:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5081')" href="javascript:;">
ray-ray-1.11.0/python/ray/tests/test_reference_counting.py: 36-52
</a>
<div class="mid" id="frag5081" style="display:none"><pre>
def _fill_object_store_and_get(obj, succeed=True, object_MiB=20,
                               num_objects=5):
    for _ in range(num_objects):
        ray.put(np.zeros(object_MiB * 1024 * 1024, dtype=np.uint8))

    if type(obj) is bytes:
        obj = ray.ObjectRef(obj)

    if succeed:
        wait_for_condition(
            lambda: ray.worker.global_worker.core_worker.object_exists(obj))
    else:
        wait_for_condition(
            lambda: not ray.worker.global_worker.core_worker.object_exists(obj)
        )


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5283')" href="javascript:;">
ray-ray-1.11.0/python/ray/tests/test_reference_counting_2.py: 38-56
</a>
<div class="mid" id="frag5283" style="display:none"><pre>
def _fill_object_store_and_get(obj, succeed=True, object_MiB=20,
                               num_objects=5):
    for _ in range(num_objects):
        ray.put(np.zeros(object_MiB * 1024 * 1024, dtype=np.uint8))

    if type(obj) is bytes:
        obj = ray.ObjectRef(obj)

    if succeed:
        wait_for_condition(
            lambda: ray.worker.global_worker.core_worker.object_exists(obj))
    else:
        wait_for_condition(
            lambda: not ray.worker.global_worker.core_worker.object_exists(obj)
        )


# Test that an object containing object refs within it pins the inner IDs
# recursively and for submitted tasks.
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 28:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5481')" href="javascript:;">
ray-ray-1.11.0/python/ray/tests/test_component_failures_2.py: 77-93
</a>
<div class="mid" id="frag5481" style="display:none"><pre>
def check_components_alive(cluster, component_type, check_component_alive):
    """Check that a given component type is alive on all worker nodes."""
    worker_nodes = get_other_nodes(cluster)
    assert len(worker_nodes) &gt; 0
    for node in worker_nodes:
        process = node.all_processes[component_type][0].process
        if check_component_alive:
            assert process.poll() is None
        else:
            print("waiting for " + component_type + " with PID " +
                  str(process.pid) + "to terminate")
            process.wait()
            print("done waiting for " + component_type + " with PID " +
                  str(process.pid) + "to terminate")
            assert not process.poll() is None


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5828')" href="javascript:;">
ray-ray-1.11.0/python/ray/tests/test_multinode_failures.py: 135-151
</a>
<div class="mid" id="frag5828" style="display:none"><pre>
def check_components_alive(cluster, component_type, check_component_alive):
    """Check that a given component type is alive on all worker nodes."""
    worker_nodes = get_other_nodes(cluster)
    assert len(worker_nodes) &gt; 0
    for node in worker_nodes:
        process = node.all_processes[component_type][0].process
        if check_component_alive:
            assert process.poll() is None
        else:
            print("waiting for " + component_type + " with PID " +
                  str(process.pid) + "to terminate")
            process.wait()
            print("done waiting for " + component_type + " with PID " +
                  str(process.pid) + "to terminate")
            assert not process.poll() is None


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 29:</b> &nbsp; 2 fragments, nominal size 19 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5693')" href="javascript:;">
ray-ray-1.11.0/python/ray/tests/test_component_failures.py: 66-104
</a>
<div class="mid" id="frag5693" style="display:none"><pre>
def test_dying_driver_get(ray_start_regular):
    # Start the Ray processes.
    address_info = ray_start_regular

    @ray.remote
    def sleep_forever():
        time.sleep(10**6)

    x_id = sleep_forever.remote()

    driver = """
import ray
ray.init("{}")
ray.get(ray.ObjectRef(ray._private.utils.hex_to_binary("{}")))
""".format(address_info["address"], x_id.hex())

    p = run_string_as_driver_nonblocking(driver)
    # Make sure the driver is running.
    time.sleep(1)
    assert p.poll() is None

    # Kill the driver process.
    p.kill()
    p.wait()
    time.sleep(0.1)

    # Make sure the original task hasn't finished.
    ready_ids, _ = ray.wait([x_id], timeout=0)
    assert len(ready_ids) == 0
    # Seal the object so the store attempts to notify the worker that the
    # get has been fulfilled.
    obj = np.ones(200 * 1024, dtype=np.uint8)
    ray.worker.global_worker.put_object(obj, x_id)
    time.sleep(0.1)

    # Make sure that nothing has died.
    assert ray._private.services.remaining_processes_alive()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5699')" href="javascript:;">
ray-ray-1.11.0/python/ray/tests/test_component_failures.py: 145-183
</a>
<div class="mid" id="frag5699" style="display:none"><pre>
def test_dying_driver_wait(ray_start_regular):
    # Start the Ray processes.
    address_info = ray_start_regular

    @ray.remote
    def sleep_forever():
        time.sleep(10**6)

    x_id = sleep_forever.remote()

    driver = """
import ray
ray.init("{}")
ray.wait([ray.ObjectRef(ray._private.utils.hex_to_binary("{}"))])
""".format(address_info["address"], x_id.hex())

    p = run_string_as_driver_nonblocking(driver)
    # Make sure the driver is running.
    time.sleep(1)
    assert p.poll() is None

    # Kill the driver process.
    p.kill()
    p.wait()
    time.sleep(0.1)

    # Make sure the original task hasn't finished.
    ready_ids, _ = ray.wait([x_id], timeout=0)
    assert len(ready_ids) == 0
    # Seal the object so the store attempts to notify the worker that the
    # wait can return.
    obj = np.ones(200 * 1024, dtype=np.uint8)
    ray.worker.global_worker.put_object(obj, x_id)
    time.sleep(0.1)

    # Make sure that nothing has died.
    assert ray._private.services.remaining_processes_alive()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 30:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6550')" href="javascript:;">
ray-ray-1.11.0/python/ray/train/tests/test_tune.py: 47-66
</a>
<div class="mid" id="frag6550" style="display:none"><pre>


def torch_fashion_mnist(num_workers, use_gpu, num_samples):
    epochs = 2

    trainer = Trainer("torch", num_workers=num_workers, use_gpu=use_gpu)
    MnistTrainable = trainer.to_tune_trainable(fashion_mnist_train_func)

    analysis = tune.run(
        MnistTrainable,
        num_samples=num_samples,
        config={
            "lr": tune.loguniform(1e-4, 1e-1),
            "batch_size": tune.choice([32, 64, 128]),
            "epochs": epochs
        })

    # Check that loss decreases in each trial.
    for path, df in analysis.trial_dataframes.items():
        assert df.loc[1, "loss"] &lt; df.loc[0, "loss"]
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6552')" href="javascript:;">
ray-ray-1.11.0/python/ray/train/tests/test_tune.py: 71-89
</a>
<div class="mid" id="frag6552" style="display:none"><pre>


def tune_tensorflow_mnist(num_workers, use_gpu, num_samples):
    epochs = 2
    trainer = Trainer("tensorflow", num_workers=num_workers, use_gpu=use_gpu)
    MnistTrainable = trainer.to_tune_trainable(tensorflow_mnist_train_func)

    analysis = tune.run(
        MnistTrainable,
        num_samples=num_samples,
        config={
            "lr": tune.loguniform(1e-4, 1e-1),
            "batch_size": tune.choice([32, 64, 128]),
            "epochs": epochs
        })

    # Check that loss decreases in each trial.
    for path, df in analysis.trial_dataframes.items():
        assert df.loc[1, "loss"] &lt; df.loc[0, "loss"]
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 31:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6999')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/sgd/tests/test_torch_basic.py: 40-53
</a>
<div class="mid" id="frag6999" style="display:none"><pre>
def test_single_step(ray_start_2_cpus, use_local):  # noqa: F811
    trainer = TorchTrainer(
        training_operator_cls=Operator,
        num_workers=1,
        use_local=use_local,
        use_gpu=False)
    metrics = trainer.train(num_steps=1)
    assert metrics[BATCH_COUNT] == 1

    val_metrics = trainer.validate(num_steps=1)
    assert val_metrics[BATCH_COUNT] == 1
    trainer.shutdown()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7049')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/sgd/tests/test_ptl.py: 87-100
</a>
<div class="mid" id="frag7049" style="display:none"><pre>
@pytest.mark.parametrize("use_local", [True, False])
def test_single_step(ray_start_2_cpus, use_local):  # noqa: F811
    trainer = TorchTrainer(
        training_operator_cls=Operator,
        num_workers=1,
        use_local=use_local,
        use_gpu=False)
    metrics = trainer.train(num_steps=1)
    assert metrics[BATCH_COUNT] == 1

    val_metrics = trainer.validate(num_steps=1)
    assert val_metrics[BATCH_COUNT] == 1
    trainer.shutdown()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 32:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7202')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/collective/tests/distributed_gpu_tests/test_distributed_allgather.py: 14-30
</a>
<div class="mid" id="frag7202" style="display:none"><pre>
                         [2, 2**5, 2**10, 2**15, 2**20, [2, 2], [5, 5, 5]])
def test_allgather_different_array_size(ray_start_distributed_2_nodes_4_gpus,
                                        array_size, tensor_backend):
    world_size = 4
    actors, _ = create_collective_workers(world_size)
    init_tensors_for_gather_scatter(
        actors, array_size=array_size, tensor_backend=tensor_backend)
    results = ray.get([a.do_allgather.remote() for a in actors])
    for i in range(world_size):
        for j in range(world_size):
            if tensor_backend == "cupy":
                assert (results[i][j] == cp.ones(array_size, dtype=cp.float32)
                        * (j + 1)).all()
            else:
                assert (results[i][j] == torch.ones(
                    array_size, dtype=torch.float32).cuda() * (j + 1)).all()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7236')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/collective/tests/single_node_gpu_tests/test_allgather.py: 14-30
</a>
<div class="mid" id="frag7236" style="display:none"><pre>
                         [2, 2**5, 2**10, 2**15, 2**20, [2, 2], [5, 5, 5]])
def test_allgather_different_array_size(ray_start_single_node_2_gpus,
                                        array_size, tensor_backend):
    world_size = 2
    actors, _ = create_collective_workers(world_size)
    init_tensors_for_gather_scatter(
        actors, array_size=array_size, tensor_backend=tensor_backend)
    results = ray.get([a.do_allgather.remote() for a in actors])
    for i in range(world_size):
        for j in range(world_size):
            if tensor_backend == "cupy":
                assert (results[i][j] == cp.ones(array_size, dtype=cp.float32)
                        * (j + 1)).all()
            else:
                assert (results[i][j] == torch.ones(
                    array_size, dtype=torch.float32).cuda() * (j + 1)).all()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 33:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7205')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/collective/tests/distributed_gpu_tests/test_distributed_allgather.py: 59-71
</a>
<div class="mid" id="frag7205" style="display:none"><pre>
@pytest.mark.parametrize("shape", [10, 20, [4, 5], [1, 3, 5, 7]])
def test_unmatched_tensor_shape(ray_start_distributed_2_nodes_4_gpus, shape):
    world_size = 4
    actors, _ = create_collective_workers(world_size)
    init_tensors_for_gather_scatter(actors, array_size=10)
    list_buffer = [cp.ones(shape, dtype=cp.float32) for _ in range(world_size)]
    ray.get([a.set_list_buffer.remote(list_buffer) for a in actors])
    if shape != 10:
        with pytest.raises(RuntimeError):
            ray.get([a.do_allgather.remote() for a in actors])
    else:
        ray.get([a.do_allgather.remote() for a in actors])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7239')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/collective/tests/single_node_gpu_tests/test_allgather.py: 57-69
</a>
<div class="mid" id="frag7239" style="display:none"><pre>
@pytest.mark.parametrize("shape", [10, 20, [4, 5], [1, 3, 5, 7]])
def test_unmatched_tensor_shape(ray_start_single_node_2_gpus, shape):
    world_size = 2
    actors, _ = create_collective_workers(world_size)
    init_tensors_for_gather_scatter(actors, array_size=10)
    list_buffer = [cp.ones(shape, dtype=cp.float32) for _ in range(world_size)]
    ray.get([a.set_list_buffer.remote(list_buffer) for a in actors])
    if shape != 10:
        with pytest.raises(RuntimeError):
            ray.get([a.do_allgather.remote() for a in actors])
    else:
        ray.get([a.do_allgather.remote() for a in actors])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 34:</b> &nbsp; 2 fragments, nominal size 48 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7206')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/collective/tests/distributed_gpu_tests/test_distributed_allgather.py: 72-128
</a>
<div class="mid" id="frag7206" style="display:none"><pre>

def test_allgather_torch_cupy(ray_start_distributed_2_nodes_4_gpus):
    world_size = 4
    shape = [10, 10]
    actors, _ = create_collective_workers(world_size)

    # tensor is pytorch, list is cupy
    for i, a in enumerate(actors):
        t = torch.ones(shape, dtype=torch.float32).cuda() * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = [
            cp.ones(shape, dtype=cp.float32) for _ in range(world_size)
        ]
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_allgather.remote() for a in actors])
    for i in range(world_size):
        for j in range(world_size):
            assert (results[i][j] == cp.ones(shape, dtype=cp.float32) *
                    (j + 1)).all()

    # tensor is cupy, list is pytorch
    for i, a in enumerate(actors):
        t = cp.ones(shape, dtype=cp.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = [
            torch.ones(shape, dtype=torch.float32).cuda()
            for _ in range(world_size)
        ]
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_allgather.remote() for a in actors])
    for i in range(world_size):
        for j in range(world_size):
            assert (results[i][j] == torch.ones(
                shape, dtype=torch.float32).cuda() * (j + 1)).all()

    # some tensors in the list are pytorch, some are cupy
    for i, a in enumerate(actors):
        t = cp.ones(shape, dtype=cp.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = []
        for j in range(world_size):
            if j % 2 == 0:
                list_buffer.append(
                    torch.ones(shape, dtype=torch.float32).cuda())
            else:
                list_buffer.append(cp.ones(shape, dtype=cp.float32))
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_allgather.remote() for a in actors])
    for i in range(world_size):
        for j in range(world_size):
            if j % 2 == 0:
                assert (results[i][j] == torch.ones(
                    shape, dtype=torch.float32).cuda() * (j + 1)).all()
            else:
                assert (results[i][j] == cp.ones(shape, dtype=cp.float32) *
                        (j + 1)).all()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7240')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/collective/tests/single_node_gpu_tests/test_allgather.py: 70-126
</a>
<div class="mid" id="frag7240" style="display:none"><pre>

def test_allgather_torch_cupy(ray_start_single_node_2_gpus):
    world_size = 2
    shape = [10, 10]
    actors, _ = create_collective_workers(world_size)

    # tensor is pytorch, list is cupy
    for i, a in enumerate(actors):
        t = torch.ones(shape, dtype=torch.float32).cuda() * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = [
            cp.ones(shape, dtype=cp.float32) for _ in range(world_size)
        ]
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_allgather.remote() for a in actors])
    for i in range(world_size):
        for j in range(world_size):
            assert (results[i][j] == cp.ones(shape, dtype=cp.float32) *
                    (j + 1)).all()

    # tensor is cupy, list is pytorch
    for i, a in enumerate(actors):
        t = cp.ones(shape, dtype=cp.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = [
            torch.ones(shape, dtype=torch.float32).cuda()
            for _ in range(world_size)
        ]
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_allgather.remote() for a in actors])
    for i in range(world_size):
        for j in range(world_size):
            assert (results[i][j] == torch.ones(
                shape, dtype=torch.float32).cuda() * (j + 1)).all()

    # some tensors in the list are pytorch, some are cupy
    for i, a in enumerate(actors):
        t = cp.ones(shape, dtype=cp.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = []
        for j in range(world_size):
            if j % 2 == 0:
                list_buffer.append(
                    torch.ones(shape, dtype=torch.float32).cuda())
            else:
                list_buffer.append(cp.ones(shape, dtype=cp.float32))
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_allgather.remote() for a in actors])
    for i in range(world_size):
        for j in range(world_size):
            if j % 2 == 0:
                assert (results[i][j] == torch.ones(
                    shape, dtype=torch.float32).cuda() * (j + 1)).all()
            else:
                assert (results[i][j] == cp.ones(shape, dtype=cp.float32) *
                        (j + 1)).all()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 35:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7210')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/collective/tests/distributed_gpu_tests/test_distributed_reduce.py: 101-114
</a>
<div class="mid" id="frag7210" style="display:none"><pre>
def test_reduce_torch_cupy(ray_start_distributed_2_nodes_4_gpus, dst_rank):
    import torch
    world_size = 4
    actors, _ = create_collective_workers(world_size)
    ray.wait([actors[1].set_buffer.remote(torch.ones(10, ).cuda())])
    results = ray.get([a.do_reduce.remote(dst_rank=dst_rank) for a in actors])
    if dst_rank == 0:
        assert (results[0] == cp.ones((10, )) * world_size).all()
        assert (results[1] == torch.ones((10, )).cuda()).all()
    else:
        assert (results[0] == cp.ones((10, ))).all()
        assert (results[1] == torch.ones((10, )).cuda() * world_size).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7261')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/collective/tests/single_node_gpu_tests/test_reduce.py: 125-138
</a>
<div class="mid" id="frag7261" style="display:none"><pre>
def test_reduce_torch_cupy(ray_start_single_node_2_gpus, dst_rank):
    import torch
    world_size = 2
    actors, _ = create_collective_workers(world_size)
    ray.wait([actors[1].set_buffer.remote(torch.ones(10, ).cuda())])
    results = ray.get([a.do_reduce.remote(dst_rank=dst_rank) for a in actors])
    if dst_rank == 0:
        assert (results[0] == cp.ones((10, )) * world_size).all()
        assert (results[1] == torch.ones((10, )).cuda()).all()
    else:
        assert (results[0] == cp.ones((10, ))).all()
        assert (results[1] == torch.ones((10, )).cuda() * world_size).all()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 36:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7213')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/collective/tests/distributed_gpu_tests/test_distributed_allreduce.py: 24-38
</a>
<div class="mid" id="frag7213" style="display:none"><pre>
def test_allreduce_different_array_size(ray_start_distributed_2_nodes_4_gpus,
                                        array_size):
    world_size = 4
    actors, _ = create_collective_workers(world_size)
    ray.wait([
        a.set_buffer.remote(cp.ones(array_size, dtype=cp.float32))
        for a in actors
    ])
    results = ray.get([a.do_allreduce.remote() for a in actors])
    assert (results[0] == cp.ones(
        (array_size, ), dtype=cp.float32) * world_size).all()
    assert (results[1] == cp.ones(
        (array_size, ), dtype=cp.float32) * world_size).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7242')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/collective/tests/single_node_gpu_tests/test_allreduce.py: 21-35
</a>
<div class="mid" id="frag7242" style="display:none"><pre>
def test_allreduce_different_array_size(ray_start_single_node_2_gpus,
                                        array_size):
    world_size = 2
    actors, _ = create_collective_workers(world_size)
    ray.wait([
        a.set_buffer.remote(cp.ones(array_size, dtype=cp.float32))
        for a in actors
    ])
    results = ray.get([a.do_allreduce.remote() for a in actors])
    assert (results[0] == cp.ones(
        (array_size, ), dtype=cp.float32) * world_size).all()
    assert (results[1] == cp.ones(
        (array_size, ), dtype=cp.float32) * world_size).all()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 37:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7215')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/collective/tests/distributed_gpu_tests/test_distributed_allreduce.py: 66-82
</a>
<div class="mid" id="frag7215" style="display:none"><pre>
def test_allreduce_multiple_group(ray_start_distributed_2_nodes_4_gpus,
                                  backend="nccl",
                                  num_groups=5):
    world_size = 4
    actors, _ = create_collective_workers(world_size)
    for group_name in range(1, num_groups):
        ray.get([
            actor.init_group.remote(world_size, i, backend, str(group_name))
            for i, actor in enumerate(actors)
        ])
    for i in range(num_groups):
        group_name = "default" if i == 0 else str(i)
        results = ray.get([a.do_allreduce.remote(group_name) for a in actors])
        assert (results[0] == cp.ones(
            (10, ), dtype=cp.float32) * (world_size**(i + 1))).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7244')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/collective/tests/single_node_gpu_tests/test_allreduce.py: 63-79
</a>
<div class="mid" id="frag7244" style="display:none"><pre>
def test_allreduce_multiple_group(ray_start_single_node_2_gpus,
                                  backend="nccl",
                                  num_groups=5):
    world_size = 2
    actors, _ = create_collective_workers(world_size)
    for group_name in range(1, num_groups):
        ray.get([
            actor.init_group.remote(world_size, i, backend, str(group_name))
            for i, actor in enumerate(actors)
        ])
    for i in range(num_groups):
        group_name = "default" if i == 0 else str(i)
        results = ray.get([a.do_allreduce.remote(group_name) for a in actors])
        assert (results[0] == cp.ones(
            (10, ), dtype=cp.float32) * (world_size**(i + 1))).all()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 38:</b> &nbsp; 2 fragments, nominal size 22 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7216')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/collective/tests/distributed_gpu_tests/test_distributed_allreduce.py: 83-115
</a>
<div class="mid" id="frag7216" style="display:none"><pre>
def test_allreduce_different_op(ray_start_distributed_2_nodes_4_gpus):
    world_size = 4
    actors, _ = create_collective_workers(world_size)

    # check product
    ray.wait([
        a.set_buffer.remote(cp.ones(10, dtype=cp.float32) * (i + 2))
        for i, a in enumerate(actors)
    ])
    results = ray.get(
        [a.do_allreduce.remote(op=ReduceOp.PRODUCT) for a in actors])
    assert (results[0] == cp.ones((10, ), dtype=cp.float32) * 120).all()
    assert (results[1] == cp.ones((10, ), dtype=cp.float32) * 120).all()

    # check min
    ray.wait([
        a.set_buffer.remote(cp.ones(10, dtype=cp.float32) * (i + 2))
        for i, a in enumerate(actors)
    ])
    results = ray.get([a.do_allreduce.remote(op=ReduceOp.MIN) for a in actors])
    assert (results[0] == cp.ones((10, ), dtype=cp.float32) * 2).all()
    assert (results[1] == cp.ones((10, ), dtype=cp.float32) * 2).all()

    # check max
    ray.wait([
        a.set_buffer.remote(cp.ones(10, dtype=cp.float32) * (i + 2))
        for i, a in enumerate(actors)
    ])
    results = ray.get([a.do_allreduce.remote(op=ReduceOp.MAX) for a in actors])
    assert (results[0] == cp.ones((10, ), dtype=cp.float32) * 5).all()
    assert (results[1] == cp.ones((10, ), dtype=cp.float32) * 5).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7245')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/collective/tests/single_node_gpu_tests/test_allreduce.py: 80-112
</a>
<div class="mid" id="frag7245" style="display:none"><pre>
def test_allreduce_different_op(ray_start_single_node_2_gpus):
    world_size = 2
    actors, _ = create_collective_workers(world_size)

    # check product
    ray.wait([
        a.set_buffer.remote(cp.ones(10, dtype=cp.float32) * (i + 2))
        for i, a in enumerate(actors)
    ])
    results = ray.get(
        [a.do_allreduce.remote(op=ReduceOp.PRODUCT) for a in actors])
    assert (results[0] == cp.ones((10, ), dtype=cp.float32) * 6).all()
    assert (results[1] == cp.ones((10, ), dtype=cp.float32) * 6).all()

    # check min
    ray.wait([
        a.set_buffer.remote(cp.ones(10, dtype=cp.float32) * (i + 2))
        for i, a in enumerate(actors)
    ])
    results = ray.get([a.do_allreduce.remote(op=ReduceOp.MIN) for a in actors])
    assert (results[0] == cp.ones((10, ), dtype=cp.float32) * 2).all()
    assert (results[1] == cp.ones((10, ), dtype=cp.float32) * 2).all()

    # check max
    ray.wait([
        a.set_buffer.remote(cp.ones(10, dtype=cp.float32) * (i + 2))
        for i, a in enumerate(actors)
    ])
    results = ray.get([a.do_allreduce.remote(op=ReduceOp.MAX) for a in actors])
    assert (results[0] == cp.ones((10, ), dtype=cp.float32) * 3).all()
    assert (results[1] == cp.ones((10, ), dtype=cp.float32) * 3).all()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 39:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7218')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/collective/tests/distributed_gpu_tests/test_distributed_allreduce.py: 128-139
</a>
<div class="mid" id="frag7218" style="display:none"><pre>
def test_allreduce_torch_cupy(ray_start_distributed_2_nodes_4_gpus):
    # import torch
    world_size = 4
    actors, _ = create_collective_workers(world_size)
    ray.wait([actors[1].set_buffer.remote(torch.ones(10, ).cuda())])
    results = ray.get([a.do_allreduce.remote() for a in actors])
    assert (results[0] == cp.ones((10, )) * world_size).all()

    ray.wait([actors[0].set_buffer.remote(torch.ones(10, ))])
    ray.wait([actors[1].set_buffer.remote(cp.ones(10, ))])
    with pytest.raises(RuntimeError):
        results = ray.get([a.do_allreduce.remote() for a in actors])
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7247')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/collective/tests/single_node_gpu_tests/test_allreduce.py: 124-137
</a>
<div class="mid" id="frag7247" style="display:none"><pre>
def test_allreduce_torch_cupy(ray_start_single_node_2_gpus):
    # import torch
    world_size = 2
    actors, _ = create_collective_workers(world_size)
    ray.wait([actors[1].set_buffer.remote(torch.ones(10, ).cuda())])
    results = ray.get([a.do_allreduce.remote() for a in actors])
    assert (results[0] == cp.ones((10, )) * world_size).all()

    ray.wait([actors[0].set_buffer.remote(torch.ones(10, ))])
    ray.wait([actors[1].set_buffer.remote(cp.ones(10, ))])
    with pytest.raises(RuntimeError):
        results = ray.get([a.do_allreduce.remote() for a in actors])


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 40:</b> &nbsp; 2 fragments, nominal size 70 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7221')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/collective/tests/distributed_gpu_tests/test_distributed_reducescatter.py: 43-123
</a>
<div class="mid" id="frag7221" style="display:none"><pre>

def test_reducescatter_torch_cupy(ray_start_distributed_2_nodes_4_gpus):
    world_size = 4
    shape = [10, 10]
    actors, _ = create_collective_workers(world_size)

    # tensor is pytorch, list is cupy
    for i, a in enumerate(actors):
        t = torch.ones(shape, dtype=torch.float32).cuda() * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = [
            cp.ones(shape, dtype=cp.float32) for _ in range(world_size)
        ]
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_reducescatter.remote() for a in actors])
    for i in range(world_size):
        assert (results[i] == torch.ones(shape, dtype=torch.float32).cuda() *
                world_size).all()

    # tensor is cupy, list is pytorch
    for i, a in enumerate(actors):
        t = cp.ones(shape, dtype=cp.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = [
            torch.ones(shape, dtype=torch.float32).cuda()
            for _ in range(world_size)
        ]
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_reducescatter.remote() for a in actors])
    for i in range(world_size):
        assert (
            results[i] == cp.ones(shape, dtype=cp.float32) * world_size).all()

    # some tensors in the list are pytorch, some are cupy
    for i, a in enumerate(actors):
        if i % 2 == 0:
            t = torch.ones(shape, dtype=torch.float32).cuda() * (i + 1)
        else:
            t = cp.ones(shape, dtype=cp.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = []
        for j in range(world_size):
            if j % 2 == 0:
                list_buffer.append(
                    torch.ones(shape, dtype=torch.float32).cuda())
            else:
                list_buffer.append(cp.ones(shape, dtype=cp.float32))
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_reducescatter.remote() for a in actors])
    for i in range(world_size):
        if i % 2 == 0:
            assert (results[i] == torch.ones(
                shape, dtype=torch.float32).cuda() * world_size).all()
        else:
            assert (results[i] == cp.ones(shape, dtype=cp.float32) *
                    world_size).all()

    # mixed case
    for i, a in enumerate(actors):
        if i % 2 == 0:
            t = torch.ones(shape, dtype=torch.float32).cuda() * (i + 1)
        else:
            t = cp.ones(shape, dtype=cp.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = []
        for j in range(world_size):
            if j % 2 == 0:
                list_buffer.append(cp.ones(shape, dtype=cp.float32))
            else:
                list_buffer.append(
                    torch.ones(shape, dtype=torch.float32).cuda())
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_reducescatter.remote() for a in actors])
    for i in range(world_size):
        if i % 2 == 0:
            assert (results[i] == torch.ones(
                shape, dtype=torch.float32).cuda() * world_size).all()
        else:
            assert (results[i] == cp.ones(shape, dtype=cp.float32) *
                    world_size).all()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7256')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/collective/tests/single_node_gpu_tests/test_reducescatter.py: 42-122
</a>
<div class="mid" id="frag7256" style="display:none"><pre>

def test_reducescatter_torch_cupy(ray_start_single_node_2_gpus):
    world_size = 2
    shape = [10, 10]
    actors, _ = create_collective_workers(world_size)

    # tensor is pytorch, list is cupy
    for i, a in enumerate(actors):
        t = torch.ones(shape, dtype=torch.float32).cuda() * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = [
            cp.ones(shape, dtype=cp.float32) for _ in range(world_size)
        ]
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_reducescatter.remote() for a in actors])
    for i in range(world_size):
        assert (results[i] == torch.ones(shape, dtype=torch.float32).cuda() *
                world_size).all()

    # tensor is cupy, list is pytorch
    for i, a in enumerate(actors):
        t = cp.ones(shape, dtype=cp.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = [
            torch.ones(shape, dtype=torch.float32).cuda()
            for _ in range(world_size)
        ]
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_reducescatter.remote() for a in actors])
    for i in range(world_size):
        assert (
            results[i] == cp.ones(shape, dtype=cp.float32) * world_size).all()

    # some tensors in the list are pytorch, some are cupy
    for i, a in enumerate(actors):
        if i % 2 == 0:
            t = torch.ones(shape, dtype=torch.float32).cuda() * (i + 1)
        else:
            t = cp.ones(shape, dtype=cp.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = []
        for j in range(world_size):
            if j % 2 == 0:
                list_buffer.append(
                    torch.ones(shape, dtype=torch.float32).cuda())
            else:
                list_buffer.append(cp.ones(shape, dtype=cp.float32))
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_reducescatter.remote() for a in actors])
    for i in range(world_size):
        if i % 2 == 0:
            assert (results[i] == torch.ones(
                shape, dtype=torch.float32).cuda() * world_size).all()
        else:
            assert (results[i] == cp.ones(shape, dtype=cp.float32) *
                    world_size).all()

    # mixed case
    for i, a in enumerate(actors):
        if i % 2 == 0:
            t = torch.ones(shape, dtype=torch.float32).cuda() * (i + 1)
        else:
            t = cp.ones(shape, dtype=cp.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = []
        for j in range(world_size):
            if j % 2 == 0:
                list_buffer.append(cp.ones(shape, dtype=cp.float32))
            else:
                list_buffer.append(
                    torch.ones(shape, dtype=torch.float32).cuda())
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_reducescatter.remote() for a in actors])
    for i in range(world_size):
        if i % 2 == 0:
            assert (results[i] == torch.ones(
                shape, dtype=torch.float32).cuda() * world_size).all()
        else:
            assert (results[i] == cp.ones(shape, dtype=cp.float32) *
                    world_size).all()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 41:</b> &nbsp; 3 fragments, nominal size 16 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7226')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/collective/tests/distributed_gpu_tests/test_distributed_basic_apis.py: 66-84
</a>
<div class="mid" id="frag7226" style="display:none"><pre>

def test_is_group_initialized(ray_start_distributed_2_nodes_4_gpus):
    world_size = 4
    actors, _ = create_collective_workers(world_size)
    # check group is_init
    actor0_is_init = ray.get(actors[0].report_is_group_initialized.remote())
    assert actor0_is_init
    actor0_is_init = ray.get(
        actors[0].report_is_group_initialized.remote("random"))
    assert not actor0_is_init
    actor0_is_init = ray.get(
        actors[0].report_is_group_initialized.remote("123"))
    assert not actor0_is_init
    actor1_is_init = ray.get(actors[0].report_is_group_initialized.remote())
    assert actor1_is_init
    actor1_is_init = ray.get(
        actors[0].report_is_group_initialized.remote("456"))
    assert not actor1_is_init

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7252')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/collective/tests/single_node_gpu_tests/test_basic_apis.py: 60-78
</a>
<div class="mid" id="frag7252" style="display:none"><pre>

def test_is_group_initialized(ray_start_single_node_2_gpus):
    world_size = 2
    actors, _ = create_collective_workers(world_size)
    # check group is_init
    actor0_is_init = ray.get(actors[0].report_is_group_initialized.remote())
    assert actor0_is_init
    actor0_is_init = ray.get(
        actors[0].report_is_group_initialized.remote("random"))
    assert not actor0_is_init
    actor0_is_init = ray.get(
        actors[0].report_is_group_initialized.remote("123"))
    assert not actor0_is_init
    actor1_is_init = ray.get(actors[0].report_is_group_initialized.remote())
    assert actor1_is_init
    actor1_is_init = ray.get(
        actors[0].report_is_group_initialized.remote("456"))
    assert not actor1_is_init

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7269')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/collective/tests/distributed_multigpu_tests/test_distributed_multigpu_basic_apis.py: 50-68
</a>
<div class="mid" id="frag7269" style="display:none"><pre>
def test_is_group_initialized(ray_start_distributed_multigpu_2_nodes_4_gpus):
    world_size = 2
    actors, _ = create_collective_multigpu_workers(world_size)
    # check group is_init
    actor0_is_init = ray.get(actors[0].report_is_group_initialized.remote())
    assert actor0_is_init
    actor0_is_init = ray.get(
        actors[0].report_is_group_initialized.remote("random"))
    assert not actor0_is_init
    actor0_is_init = ray.get(
        actors[0].report_is_group_initialized.remote("123"))
    assert not actor0_is_init
    actor1_is_init = ray.get(actors[0].report_is_group_initialized.remote())
    assert actor1_is_init
    actor1_is_init = ray.get(
        actors[0].report_is_group_initialized.remote("456"))
    assert not actor1_is_init


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 42:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7229')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/collective/tests/distributed_gpu_tests/test_distributed_broadcast.py: 31-45
</a>
<div class="mid" id="frag7229" style="display:none"><pre>
def test_broadcast_different_array_size(ray_start_distributed_2_nodes_4_gpus,
                                        array_size, src_rank):
    world_size = 4
    actors, _ = create_collective_workers(world_size)
    ray.wait([
        a.set_buffer.remote(cp.ones(array_size, dtype=cp.float32) * (i + 2))
        for i, a in enumerate(actors)
    ])
    results = ray.get(
        [a.do_broadcast.remote(src_rank=src_rank) for a in actors])
    for i in range(world_size):
        assert (results[i] == cp.ones(
            (array_size, ), dtype=cp.float32) * (src_rank + 2)).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7233')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/collective/tests/single_node_gpu_tests/test_broadcast.py: 31-45
</a>
<div class="mid" id="frag7233" style="display:none"><pre>
def test_broadcast_different_array_size(ray_start_single_node_2_gpus,
                                        array_size, src_rank):
    world_size = 2
    actors, _ = create_collective_workers(world_size)
    ray.wait([
        a.set_buffer.remote(cp.ones(array_size, dtype=cp.float32) * (i + 2))
        for i, a in enumerate(actors)
    ])
    results = ray.get(
        [a.do_broadcast.remote(src_rank=src_rank) for a in actors])
    for i in range(world_size):
        assert (results[i] == cp.ones(
            (array_size, ), dtype=cp.float32) * (src_rank + 2)).all()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 43:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7230')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/collective/tests/distributed_gpu_tests/test_distributed_broadcast.py: 47-62
</a>
<div class="mid" id="frag7230" style="display:none"><pre>
def test_broadcast_torch_cupy(ray_start_distributed_2_nodes_4_gpus, src_rank):
    import torch
    world_size = 4
    actors, _ = create_collective_workers(world_size)
    ray.wait(
        [actors[1].set_buffer.remote(torch.ones(10, ).cuda() * world_size)])
    results = ray.get(
        [a.do_broadcast.remote(src_rank=src_rank) for a in actors])
    if src_rank == 0:
        assert (results[0] == cp.ones((10, ))).all()
        assert (results[1] == torch.ones((10, )).cuda()).all()
    else:
        assert (results[0] == cp.ones((10, )) * world_size).all()
        assert (results[1] == torch.ones((10, )).cuda() * world_size).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7234')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/collective/tests/single_node_gpu_tests/test_broadcast.py: 47-62
</a>
<div class="mid" id="frag7234" style="display:none"><pre>
def test_broadcast_torch_cupy(ray_start_single_node_2_gpus, src_rank):
    import torch
    world_size = 2
    actors, _ = create_collective_workers(world_size)
    ray.wait(
        [actors[1].set_buffer.remote(torch.ones(10, ).cuda() * world_size)])
    results = ray.get(
        [a.do_broadcast.remote(src_rank=src_rank) for a in actors])
    if src_rank == 0:
        assert (results[0] == cp.ones((10, ))).all()
        assert (results[1] == torch.ones((10, )).cuda()).all()
    else:
        assert (results[0] == cp.ones((10, )) * world_size).all()
        assert (results[1] == torch.ones((10, )).cuda() * world_size).all()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 44:</b> &nbsp; 2 fragments, nominal size 24 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7253')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/collective/tests/single_node_gpu_tests/test_basic_apis.py: 79-111
</a>
<div class="mid" id="frag7253" style="display:none"><pre>

def test_destroy_group(ray_start_single_node_2_gpus):
    world_size = 2
    actors, _ = create_collective_workers(world_size)
    # Now destroy the group at actor0
    ray.wait([actors[0].destroy_group.remote()])
    actor0_is_init = ray.get(actors[0].report_is_group_initialized.remote())
    assert not actor0_is_init

    # should go well as the group `random` does not exist at all
    ray.wait([actors[0].destroy_group.remote("random")])

    actor1_is_init = ray.get(actors[1].report_is_group_initialized.remote())
    assert actor1_is_init
    ray.wait([actors[1].destroy_group.remote("random")])
    actor1_is_init = ray.get(actors[1].report_is_group_initialized.remote())
    assert actor1_is_init
    ray.wait([actors[1].destroy_group.remote("default")])
    actor1_is_init = ray.get(actors[1].report_is_group_initialized.remote())
    assert not actor1_is_init

    # Now reconstruct the group using the same name
    init_results = ray.get([
        actor.init_group.remote(world_size, i)
        for i, actor in enumerate(actors)
    ])
    for i in range(world_size):
        assert init_results[i]
    actor0_is_init = ray.get(actors[0].report_is_group_initialized.remote())
    assert actor0_is_init
    actor1_is_init = ray.get(actors[0].report_is_group_initialized.remote())
    assert actor1_is_init

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7270')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/collective/tests/distributed_multigpu_tests/test_distributed_multigpu_basic_apis.py: 69-101
</a>
<div class="mid" id="frag7270" style="display:none"><pre>
def test_destroy_group(ray_start_distributed_multigpu_2_nodes_4_gpus):
    world_size = 2
    actors, _ = create_collective_multigpu_workers(world_size)
    # Now destroy the group at actor0
    ray.wait([actors[0].destroy_group.remote()])
    actor0_is_init = ray.get(actors[0].report_is_group_initialized.remote())
    assert not actor0_is_init

    # should go well as the group `random` does not exist at all
    ray.wait([actors[0].destroy_group.remote("random")])

    actor1_is_init = ray.get(actors[1].report_is_group_initialized.remote())
    assert actor1_is_init
    ray.wait([actors[1].destroy_group.remote("random")])
    actor1_is_init = ray.get(actors[1].report_is_group_initialized.remote())
    assert actor1_is_init
    ray.wait([actors[1].destroy_group.remote("default")])
    actor1_is_init = ray.get(actors[1].report_is_group_initialized.remote())
    assert not actor1_is_init

    # Now reconstruct the group using the same name
    init_results = ray.get([
        actor.init_group.remote(world_size, i)
        for i, actor in enumerate(actors)
    ])
    for i in range(world_size):
        assert init_results[i]
    actor0_is_init = ray.get(actors[0].report_is_group_initialized.remote())
    assert actor0_is_init
    actor1_is_init = ray.get(actors[0].report_is_group_initialized.remote())
    assert actor1_is_init


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 45:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7305')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/collective/tests/single_node_cpu_tests/test_allgather.py: 61-73
</a>
<div class="mid" id="frag7305" style="display:none"><pre>
@pytest.mark.parametrize("shape", [10, 20, [4, 5], [1, 3, 5, 7]])
def test_unmatched_tensor_shape(ray_start_single_node, shape, backend):
    world_size = 2
    actors, _ = create_collective_workers(world_size, backend=backend)
    init_tensors_for_gather_scatter(actors, array_size=10)
    list_buffer = [np.ones(shape, dtype=np.float32) for _ in range(world_size)]
    ray.get([a.set_list_buffer.remote(list_buffer, copy=True) for a in actors])
    if shape != 10:
        with pytest.raises(RuntimeError):
            ray.get([a.do_allgather.remote() for a in actors])
    else:
        ray.get([a.do_allgather.remote() for a in actors])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7380')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/collective/tests/distributed_cpu_tests/test_distributed_allgather.py: 65-77
</a>
<div class="mid" id="frag7380" style="display:none"><pre>
@pytest.mark.parametrize("shape", [10, 20, [4, 5], [1, 3, 5, 7]])
def test_unmatched_tensor_shape(ray_start_distributed_2_nodes, shape, backend):
    world_size = 8
    actors, _ = create_collective_workers(world_size, backend=backend)
    init_tensors_for_gather_scatter(actors, array_size=10)
    list_buffer = [np.ones(shape, dtype=np.float32) for _ in range(world_size)]
    ray.get([a.set_list_buffer.remote(list_buffer, copy=True) for a in actors])
    if shape != 10:
        with pytest.raises(RuntimeError):
            ray.get([a.do_allgather.remote() for a in actors])
    else:
        ray.get([a.do_allgather.remote() for a in actors])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 46:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7318')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/collective/tests/single_node_cpu_tests/test_basic_apis.py: 70-88
</a>
<div class="mid" id="frag7318" style="display:none"><pre>
@pytest.mark.parametrize("backend", [Backend.GLOO])
def test_is_group_initialized(ray_start_single_node, backend):
    world_size = 2
    actors, _ = create_collective_workers(world_size, backend=backend)
    # check group is_init
    actor0_is_init = ray.get(actors[0].report_is_group_initialized.remote())
    assert actor0_is_init
    actor0_is_init = ray.get(
        actors[0].report_is_group_initialized.remote("random"))
    assert not actor0_is_init
    actor0_is_init = ray.get(
        actors[0].report_is_group_initialized.remote("123"))
    assert not actor0_is_init
    actor1_is_init = ray.get(actors[0].report_is_group_initialized.remote())
    assert actor1_is_init
    actor1_is_init = ray.get(
        actors[0].report_is_group_initialized.remote("456"))
    assert not actor1_is_init

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7401')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/collective/tests/distributed_cpu_tests/test_distributed_basic_apis.py: 73-91
</a>
<div class="mid" id="frag7401" style="display:none"><pre>
@pytest.mark.parametrize("backend", [Backend.GLOO])
def test_is_group_initialized(ray_start_distributed_2_nodes, backend):
    world_size = 8
    actors, _ = create_collective_workers(world_size, backend=backend)
    # check group is_init
    actor0_is_init = ray.get(actors[0].report_is_group_initialized.remote())
    assert actor0_is_init
    actor0_is_init = ray.get(
        actors[0].report_is_group_initialized.remote("random"))
    assert not actor0_is_init
    actor0_is_init = ray.get(
        actors[0].report_is_group_initialized.remote("123"))
    assert not actor0_is_init
    actor1_is_init = ray.get(actors[0].report_is_group_initialized.remote())
    assert actor1_is_init
    actor1_is_init = ray.get(
        actors[0].report_is_group_initialized.remote("456"))
    assert not actor1_is_init

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 47:</b> &nbsp; 2 fragments, nominal size 68 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7322')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/collective/tests/single_node_cpu_tests/test_reducescatter.py: 46-123
</a>
<div class="mid" id="frag7322" style="display:none"><pre>
@pytest.mark.parametrize("backend", [Backend.GLOO])
def test_reducescatter_torch_numpy(ray_start_single_node, backend):
    world_size = 2
    shape = [10, 10]
    actors, _ = create_collective_workers(world_size, backend=backend)

    # tensor is pytorch, list is numpy
    for i, a in enumerate(actors):
        t = torch.ones(shape, dtype=torch.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = [
            np.ones(shape, dtype=np.float32) for _ in range(world_size)
        ]
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_reducescatter.remote() for a in actors])
    for i in range(world_size):
        assert (results[i] == torch.ones(shape, dtype=torch.float32) *
                world_size).all()

    # tensor is numpy, list is pytorch
    for i, a in enumerate(actors):
        t = np.ones(shape, dtype=np.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = [
            torch.ones(shape, dtype=torch.float32) for _ in range(world_size)
        ]
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_reducescatter.remote() for a in actors])
    for i in range(world_size):
        assert (
            results[i] == np.ones(shape, dtype=np.float32) * world_size).all()

    # some tensors in the list are pytorch, some are numpy
    for i, a in enumerate(actors):
        if i % 2 == 0:
            t = torch.ones(shape, dtype=torch.float32) * (i + 1)
        else:
            t = np.ones(shape, dtype=np.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = []
        for j in range(world_size):
            if j % 2 == 0:
                list_buffer.append(torch.ones(shape, dtype=torch.float32))
            else:
                list_buffer.append(np.ones(shape, dtype=np.float32))
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_reducescatter.remote() for a in actors])
    for i in range(world_size):
        if i % 2 == 0:
            assert (results[i] == torch.ones(shape, dtype=torch.float32) *
                    world_size).all()
        else:
            assert (results[i] == np.ones(shape, dtype=np.float32) *
                    world_size).all()

    # mixed case
    for i, a in enumerate(actors):
        if i % 2 == 0:
            t = torch.ones(shape, dtype=torch.float32) * (i + 1)
        else:
            t = np.ones(shape, dtype=np.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = []
        for j in range(world_size):
            if j % 2 == 0:
                list_buffer.append(np.ones(shape, dtype=np.float32))
            else:
                list_buffer.append(torch.ones(shape, dtype=torch.float32))
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_reducescatter.remote() for a in actors])
    for i in range(world_size):
        if i % 2 == 0:
            assert (results[i] == torch.ones(shape, dtype=torch.float32) *
                    world_size).all()
        else:
            assert (results[i] == np.ones(shape, dtype=np.float32) *
                    world_size).all()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7396')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/collective/tests/distributed_cpu_tests/test_distributed_reducescatter.py: 47-124
</a>
<div class="mid" id="frag7396" style="display:none"><pre>
@pytest.mark.parametrize("backend", [Backend.GLOO])
def test_reducescatter_torch_numpy(ray_start_distributed_2_nodes, backend):
    world_size = 8
    shape = [10, 10]
    actors, _ = create_collective_workers(world_size, backend=backend)

    # tensor is pytorch, list is numpy
    for i, a in enumerate(actors):
        t = torch.ones(shape, dtype=torch.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = [
            np.ones(shape, dtype=np.float32) for _ in range(world_size)
        ]
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_reducescatter.remote() for a in actors])
    for i in range(world_size):
        assert (results[i] == torch.ones(shape, dtype=torch.float32) *
                world_size).all()

    # tensor is numpy, list is pytorch
    for i, a in enumerate(actors):
        t = np.ones(shape, dtype=np.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = [
            torch.ones(shape, dtype=torch.float32) for _ in range(world_size)
        ]
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_reducescatter.remote() for a in actors])
    for i in range(world_size):
        assert (
            results[i] == np.ones(shape, dtype=np.float32) * world_size).all()

    # some tensors in the list are pytorch, some are numpy
    for i, a in enumerate(actors):
        if i % 2 == 0:
            t = torch.ones(shape, dtype=torch.float32) * (i + 1)
        else:
            t = np.ones(shape, dtype=np.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = []
        for j in range(world_size):
            if j % 2 == 0:
                list_buffer.append(torch.ones(shape, dtype=torch.float32))
            else:
                list_buffer.append(np.ones(shape, dtype=np.float32))
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_reducescatter.remote() for a in actors])
    for i in range(world_size):
        if i % 2 == 0:
            assert (results[i] == torch.ones(shape, dtype=torch.float32) *
                    world_size).all()
        else:
            assert (results[i] == np.ones(shape, dtype=np.float32) *
                    world_size).all()

    # mixed case
    for i, a in enumerate(actors):
        if i % 2 == 0:
            t = torch.ones(shape, dtype=torch.float32) * (i + 1)
        else:
            t = np.ones(shape, dtype=np.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = []
        for j in range(world_size):
            if j % 2 == 0:
                list_buffer.append(np.ones(shape, dtype=np.float32))
            else:
                list_buffer.append(torch.ones(shape, dtype=torch.float32))
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_reducescatter.remote() for a in actors])
    for i in range(world_size):
        if i % 2 == 0:
            assert (results[i] == torch.ones(shape, dtype=torch.float32) *
                    world_size).all()
        else:
            assert (results[i] == np.ones(shape, dtype=np.float32) *
                    world_size).all()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 48:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7324')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/collective/tests/single_node_cpu_tests/test_reduce.py: 31-48
</a>
<div class="mid" id="frag7324" style="display:none"><pre>
def test_reduce_different_array_size(ray_start_single_node, array_size,
                                     dst_rank, backend):
    world_size = 2
    actors, _ = create_collective_workers(world_size, backend=backend)
    ray.wait([
        a.set_buffer.remote(np.ones(array_size, dtype=np.float32))
        for a in actors
    ])
    results = ray.get([a.do_reduce.remote(dst_rank=dst_rank) for a in actors])
    for i in range(world_size):
        if i == dst_rank:
            assert (results[i] == np.ones(
                (array_size, ), dtype=np.float32) * world_size).all()
        else:
            assert (results[i] == np.ones((array_size, ),
                                          dtype=np.float32)).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7383')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/collective/tests/distributed_cpu_tests/test_distributed_reduce.py: 31-48
</a>
<div class="mid" id="frag7383" style="display:none"><pre>
def test_reduce_different_array_size(ray_start_distributed_2_nodes, backend,
                                     array_size, dst_rank):
    world_size = 8
    actors, _ = create_collective_workers(world_size, backend=backend)
    ray.wait([
        a.set_buffer.remote(np.ones(array_size, dtype=np.float32))
        for a in actors
    ])
    results = ray.get([a.do_reduce.remote(dst_rank=dst_rank) for a in actors])
    for i in range(world_size):
        if i == dst_rank:
            assert (results[i] == np.ones(
                (array_size, ), dtype=np.float32) * world_size).all()
        else:
            assert (results[i] == np.ones((array_size, ),
                                          dtype=np.float32)).all()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 49:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7351')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/collective/tests/util.py: 100-115
</a>
<div class="mid" id="frag7351" style="display:none"><pre>
def create_collective_workers(num_workers=2,
                              group_name="default",
                              backend="nccl"):
    actors = [None] * num_workers
    for i in range(num_workers):
        actor = Worker.remote()
        ray.get([actor.init_tensors.remote()])
        actors[i] = actor
    world_size = num_workers
    init_results = ray.get([
        actor.init_group.remote(world_size, i, backend, group_name)
        for i, actor in enumerate(actors)
    ])
    return actors, init_results


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7426')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/collective/tests/cpu_util.py: 107-122
</a>
<div class="mid" id="frag7426" style="display:none"><pre>
def create_collective_workers(num_workers=2,
                              group_name="default",
                              backend="nccl"):
    actors = [None] * num_workers
    for i in range(num_workers):
        actor = Worker.remote()
        ray.get([actor.init_tensors.remote()])
        actors[i] = actor
    world_size = num_workers
    init_results = ray.get([
        actor.init_group.remote(world_size, i, backend, group_name)
        for i, actor in enumerate(actors)
    ])
    return actors, init_results


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 50:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7365')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/collective/tests/util.py: 288-305
</a>
<div class="mid" id="frag7365" style="display:none"><pre>
    def do_send_multigpu(self,
                         group_name="default",
                         dst_rank=0,
                         dst_gpu_index=0,
                         src_gpu_index=0):
        if src_gpu_index == 0:
            col.send_multigpu(self.buffer0, dst_rank, dst_gpu_index,
                              group_name)
            cp.cuda.Device(0).synchronize()
            return self.buffer0
        elif src_gpu_index == 1:
            col.send_multigpu(self.buffer1, dst_rank, dst_gpu_index,
                              group_name)
            cp.cuda.Device(1).synchronize()
            return self.buffer1
        else:
            raise RuntimeError()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7366')" href="javascript:;">
ray-ray-1.11.0/python/ray/util/collective/tests/util.py: 306-323
</a>
<div class="mid" id="frag7366" style="display:none"><pre>
    def do_recv_multigpu(self,
                         group_name="default",
                         src_rank=0,
                         src_gpu_index=0,
                         dst_gpu_index=0):
        if dst_gpu_index == 0:
            col.recv_multigpu(self.buffer0, src_rank, src_gpu_index,
                              group_name)
            cp.cuda.Device(0).synchronize()
            return self.buffer0
        elif dst_gpu_index == 1:
            col.recv_multigpu(self.buffer1, src_rank, src_gpu_index,
                              group_name)
            cp.cuda.Device(1).synchronize()
            return self.buffer1
        else:
            raise RuntimeError()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 51:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7672')" href="javascript:;">
ray-ray-1.11.0/python/ray/cloudpickle/__init__.py: 24-36
</a>
<div class="mid" id="frag7672" style="display:none"><pre>
def dump_debug(obj, *args, **kwargs):
    try:
        return dump(obj, *args, **kwargs)
    except (TypeError, PicklingError) as exc:
        if os.environ.get("RAY_PICKLE_VERBOSE_DEBUG"):
            from ray.util.check_serialize import inspect_serializability
            inspect_serializability(obj)
            raise
        else:
            msg = _warn_msg(obj, "ray.cloudpickle.dump", exc)
            raise type(exc)(msg)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7673')" href="javascript:;">
ray-ray-1.11.0/python/ray/cloudpickle/__init__.py: 37-47
</a>
<div class="mid" id="frag7673" style="display:none"><pre>
def dumps_debug(obj, *args, **kwargs):
    try:
        return dumps(obj, *args, **kwargs)
    except (TypeError, PicklingError) as exc:
        if os.environ.get("RAY_PICKLE_VERBOSE_DEBUG"):
            from ray.util.check_serialize import inspect_serializability
            inspect_serializability(obj)
            raise
        else:
            msg = _warn_msg(obj, "ray.cloudpickle.dumps", exc)
            raise type(exc)(msg)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 52:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7751')" href="javascript:;">
ray-ray-1.11.0/python/ray/serve/tests/test_autoscaling_policy.py: 48-59
</a>
<div class="mid" id="frag7751" style="display:none"><pre>
    def test_scale_up(self):
        config = AutoscalingConfig(
            min_replicas=0,
            max_replicas=100,
            target_num_ongoing_requests_per_replica=1)
        num_replicas = 10
        num_ongoing_requests = [2.0] * num_replicas
        desired_num_replicas = calculate_desired_num_replicas(
            autoscaling_config=config,
            current_num_ongoing_requests=num_ongoing_requests)
        assert 19 &lt;= desired_num_replicas &lt;= 21  # 10 * 2 = 20

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7752')" href="javascript:;">
ray-ray-1.11.0/python/ray/serve/tests/test_autoscaling_policy.py: 60-71
</a>
<div class="mid" id="frag7752" style="display:none"><pre>
    def test_scale_down(self):
        config = AutoscalingConfig(
            min_replicas=0,
            max_replicas=100,
            target_num_ongoing_requests_per_replica=1)
        num_replicas = 10
        num_ongoing_requests = [0.5] * num_replicas
        desired_num_replicas = calculate_desired_num_replicas(
            autoscaling_config=config,
            current_num_ongoing_requests=num_ongoing_requests)
        assert 4 &lt;= desired_num_replicas &lt;= 6  # 10 * 0.5 = 5

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<script language="JavaScript">
function ShowHide(divId) { 
    if(document.getElementById(divId).style.display == 'none') {
        document.getElementById(divId).style.display='block';
    } else { 
        document.getElementById(divId).style.display = 'none';
    } 
}
</script>
</body>
</html>

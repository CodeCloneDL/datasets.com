<clones>
<systeminfo processor="nicad6" system="pytorchvideo-0.1.3" granularity="functions-blind" threshold="30%" minlines="10" maxlines="2500"/>
<cloneinfo npcs="475" npairs="58"/>
<runinfo ncompares="9786" cputime="51936"/>
<classinfo nclasses="29"/>

<class classid="1" nclones="2" nlines="17" similarity="100">
<source file="systems/pytorchvideo-0.1.3/tutorials/video_classification_example/train.py" startline="296" endline="318" pcid="14">
    def train_dataloader(self):
        """
        Defines the train DataLoader that the PyTorch Lightning Trainer trains/tests with.
        """
        sampler = DistributedSampler if self.trainer.use_ddp else RandomSampler
        train_transform = self._make_transforms(mode="train")
        self.train_dataset = LimitDataset(
            pytorchvideo.data.Kinetics(
                data_path=os.path.join(self.args.data_path, "train.csv"),
                clip_sampler=pytorchvideo.data.make_clip_sampler(
                    "random", self.args.clip_duration
                ),
                video_path_prefix=self.args.video_path_prefix,
                transform=train_transform,
                video_sampler=sampler,
            )
        )
        return torch.utils.data.DataLoader(
            self.train_dataset,
            batch_size=self.args.batch_size,
            num_workers=self.args.workers,
        )

</source>
<source file="systems/pytorchvideo-0.1.3/tutorials/video_classification_example/train.py" startline="319" endline="342" pcid="15">
    def val_dataloader(self):
        """
        Defines the train DataLoader that the PyTorch Lightning Trainer trains/tests with.
        """
        sampler = DistributedSampler if self.trainer.use_ddp else RandomSampler
        val_transform = self._make_transforms(mode="val")
        self.val_dataset = LimitDataset(
            pytorchvideo.data.Kinetics(
                data_path=os.path.join(self.args.data_path, "val.csv"),
                clip_sampler=pytorchvideo.data.make_clip_sampler(
                    "uniform", self.args.clip_duration
                ),
                video_path_prefix=self.args.video_path_prefix,
                transform=val_transform,
                video_sampler=sampler,
            )
        )
        return torch.utils.data.DataLoader(
            self.val_dataset,
            batch_size=self.args.batch_size,
            num_workers=self.args.workers,
        )


</source>
</class>

<class classid="2" nclones="2" nlines="19" similarity="100">
<source file="systems/pytorchvideo-0.1.3/pytorchvideo/data/kinetics.py" startline="17" endline="70" pcid="52">
def Kinetics(
    data_path: str,
    clip_sampler: ClipSampler,
    video_sampler: Type[torch.utils.data.Sampler] = torch.utils.data.RandomSampler,
    transform: Optional[Callable[[Dict[str, Any]], Dict[str, Any]]] = None,
    video_path_prefix: str = "",
    decode_audio: bool = True,
    decoder: str = "pyav",
) -> LabeledVideoDataset:
    """
    A helper function to create ``LabeledVideoDataset`` object for the Kinetics dataset.

    Args:
        data_path (str): Path to the data. The path type defines how the data
            should be read:

            * For a file path, the file is read and each line is parsed into a
              video path and label.
            * For a directory, the directory structure defines the classes
              (i.e. each subdirectory is a class).

        clip_sampler (ClipSampler): Defines how clips should be sampled from each
                video. See the clip sampling documentation for more information.

        video_sampler (Type[torch.utils.data.Sampler]): Sampler for the internal
                video container. This defines the order videos are decoded and,
                if necessary, the distributed split.

        transform (Callable): This callable is evaluated on the clip output before
                the clip is returned. It can be used for user defined preprocessing and
                augmentations to the clips. See the ``LabeledVideoDataset`` class for clip
                output format.

        video_path_prefix (str): Path to root directory with the videos that are
                loaded in ``LabeledVideoDataset``. All the video paths before loading
                are prefixed with this path.

        decode_audio (bool): If True, also decode audio from video.

        decoder (str): Defines what type of decoder used to decode a video.

    """

    torch._C._log_api_usage_once("PYTORCHVIDEO.dataset.Kinetics")

    return labeled_video_dataset(
        data_path,
        clip_sampler,
        video_sampler,
        transform,
        video_path_prefix,
        decode_audio,
        decoder,
    )
</source>
<source file="systems/pytorchvideo-0.1.3/pytorchvideo/data/ucf101.py" startline="17" endline="70" pcid="79">
def Ucf101(
    data_path: str,
    clip_sampler: ClipSampler,
    video_sampler: Type[torch.utils.data.Sampler] = torch.utils.data.RandomSampler,
    transform: Optional[Callable[[Dict[str, Any]], Dict[str, Any]]] = None,
    video_path_prefix: str = "",
    decode_audio: bool = True,
    decoder: str = "pyav",
) -> LabeledVideoDataset:
    """
    A helper function to create ``LabeledVideoDataset`` object for the Ucf101 dataset.

    Args:
        data_path (str): Path to the data. The path type defines how the data
            should be read:

            * For a file path, the file is read and each line is parsed into a
              video path and label.
            * For a directory, the directory structure defines the classes
              (i.e. each subdirectory is a class).

        clip_sampler (ClipSampler): Defines how clips should be sampled from each
                video. See the clip sampling documentation for more information.

        video_sampler (Type[torch.utils.data.Sampler]): Sampler for the internal
                video container. This defines the order videos are decoded and,
                if necessary, the distributed split.

        transform (Callable): This callable is evaluated on the clip output before
                the clip is returned. It can be used for user defined preprocessing and
                augmentations to the clips. See the ``LabeledVideoDataset`` class for clip
                output format.

        video_path_prefix (str): Path to root directory with the videos that are
                loaded in ``LabeledVideoDataset``. All the video paths before loading
                are prefixed with this path.

        decode_audio (bool): If True, also decode audio from video.

        decoder (str): Defines what type of decoder used to decode a video.

    """

    torch._C._log_api_usage_once("PYTORCHVIDEO.dataset.Ucf101")

    return labeled_video_dataset(
        data_path,
        clip_sampler,
        video_sampler,
        transform,
        video_path_prefix,
        decode_audio,
        decoder,
    )
</source>
</class>

<class classid="3" nclones="2" nlines="33" similarity="72">
<source file="systems/pytorchvideo-0.1.3/pytorchvideo/data/ssv2.py" startline="136" endline="192" pcid="56">
    def __next__(self) -> dict:
        """
        Retrieves the next clip based on the clip sampling strategy and video sampler.

        Returns:
            A dictionary with the following format.

            .. code-block:: text

                {
                    'video': <video_tensor>,
                    'label': <index_label>,
                    'video_label': <index_label>
                    'video_index': <video_index>,
                    'clip_index': <clip_index>,
                    'aug_index': <aug_index>,
                }
        """
        if not self._video_sampler_iter:
            # Setup MultiProcessSampler here - after PyTorch DataLoader workers are spawned.
            self._video_sampler_iter = iter(MultiProcessSampler(self._video_sampler))

        if self._loaded_video:
            video, video_index = self._loaded_video
        else:
            video_index = next(self._video_sampler_iter)
            path_to_video_frames = self._path_to_videos[video_index]
            video = FrameVideo.from_frame_paths(path_to_video_frames)
            self._loaded_video = (video, video_index)

        clip_start, clip_end, clip_index, aug_index, is_last_clip = self._clip_sampler(
            self._next_clip_start_time, video.duration, {}
        )
        # Only load the clip once and reuse previously stored clip if there are multiple
        # views for augmentations to perform on the same clip.
        if aug_index == 0:
            self._loaded_clip = video.get_clip(0, video.duration, self._frame_filter)

        self._next_clip_start_time = clip_end

        if is_last_clip:
            self._loaded_video = None
            self._next_clip_start_time = 0.0

        sample_dict = {
            "video": self._loaded_clip["video"],
            "label": self._labels[video_index],
            "video_name": str(video_index),
            "video_index": video_index,
            "clip_index": clip_index,
            "aug_index": aug_index,
        }
        if self._transform is not None:
            sample_dict = self._transform(sample_dict)

        return sample_dict

</source>
<source file="systems/pytorchvideo-0.1.3/pytorchvideo/data/charades.py" startline="112" endline="178" pcid="76">
    def __next__(self) -> dict:
        """
        Retrieves the next clip based on the clip sampling strategy and video sampler.

        Returns:
            A dictionary with the following format.

            .. code-block:: text

                {
                    'video': <video_tensor>,
                    'label': <index_label>,
                    'video_label': <index_label>
                    'video_index': <video_index>,
                    'clip_index': <clip_index>,
                    'aug_index': <aug_index>,
                }
        """
        if not self._video_sampler_iter:
            # Setup MultiProcessSampler here - after PyTorch DataLoader workers are spawned.
            self._video_sampler_iter = iter(MultiProcessSampler(self._video_sampler))

        if self._loaded_video:
            video, video_index = self._loaded_video
        else:
            video_index = next(self._video_sampler_iter)
            path_to_video_frames = self._path_to_videos[video_index]
            video = FrameVideo.from_frame_paths(path_to_video_frames)
            self._loaded_video = (video, video_index)

        clip_start, clip_end, clip_index, aug_index, is_last_clip = self._clip_sampler(
            self._next_clip_start_time, video.duration, {}
        )
        # Only load the clip once and reuse previously stored clip if there are multiple
        # views for augmentations to perform on the same clip.
        if aug_index == 0:
            self._loaded_clip = video.get_clip(clip_start, clip_end, self._frame_filter)

        frames, frame_indices = (
            self._loaded_clip["video"],
            self._loaded_clip["frame_indices"],
        )
        self._next_clip_start_time = clip_end

        if is_last_clip:
            self._loaded_video = None
            self._next_clip_start_time = 0.0

        # Merge unique labels from each frame into clip label.
        labels_by_frame = [
            self._labels[video_index][i]
            for i in range(min(frame_indices), max(frame_indices) + 1)
        ]
        sample_dict = {
            "video": frames,
            "label": labels_by_frame,
            "video_label": self._video_labels[video_index],
            "video_name": str(video_index),
            "video_index": video_index,
            "clip_index": clip_index,
            "aug_index": aug_index,
        }
        if self._transform is not None:
            sample_dict = self._transform(sample_dict)

        return sample_dict

</source>
</class>

<class classid="4" nclones="2" nlines="36" similarity="91">
<source file="systems/pytorchvideo-0.1.3/pytorchvideo/data/encoded_video_torchvision.py" startline="91" endline="161" pcid="71">
    def get_clip(
        self, start_sec: float, end_sec: float
    ) -> Dict[str, Optional[torch.Tensor]]:
        """
        Retrieves frames from the encoded video at the specified start and end times
        in seconds (the video always starts at 0 seconds).

        Args:
            start_sec (float): the clip start time in seconds
            end_sec (float): the clip end time in seconds
        Returns:
            clip_data:
                A dictionary mapping the entries at "video" and "audio" to a tensors.

                "video": A tensor of the clip's RGB frames with shape:
                (channel, time, height, width). The frames are of type torch.float32 and
                in the range [0 - 255].

                "audio": A tensor of the clip's audio samples with shape:
                (samples). The samples are of type torch.float32 and
                in the range [0 - 255].

            Returns None if no video or audio found within time range.

        """
        video_frames = None
        if self._video is not None:
            video_start_pts = secs_to_pts(
                start_sec, self._video_time_base, self._video_start_pts
            )
            video_end_pts = secs_to_pts(
                end_sec, self._video_time_base, self._video_start_pts
            )
            video_frames = [
                f
                for f, pts in self._video
                if pts >= video_start_pts and pts <= video_end_pts
            ]

        audio_samples = None
        if self._decode_audio and self._audio:
            audio_start_pts = secs_to_pts(
                start_sec, self._audio_time_base, self._audio_start_pts
            )
            audio_end_pts = secs_to_pts(
                end_sec, self._audio_time_base, self._audio_start_pts
            )
            audio_samples = [
                f
                for f, pts in self._audio
                if pts >= audio_start_pts and pts <= audio_end_pts
            ]
            audio_samples = torch.cat(audio_samples, axis=0)
            audio_samples = audio_samples.to(torch.float32)

        if video_frames is None or len(video_frames) == 0:
            logger.warning(
                f"No video found within {start_sec} and {end_sec} seconds. "
                f"Video starts at time 0 and ends at {self.duration}."
            )

            video_frames = None

        if video_frames is not None:
            video_frames = thwc_to_cthw(torch.stack(video_frames)).to(torch.float32)

        return {
            "video": video_frames,
            "audio": audio_samples,
        }

</source>
<source file="systems/pytorchvideo-0.1.3/pytorchvideo/data/encoded_video_pyav.py" startline="118" endline="192" pcid="94">
    def get_clip(
        self, start_sec: float, end_sec: float
    ) -> Dict[str, Optional[torch.Tensor]]:
        """
        Retrieves frames from the encoded video at the specified start and end times
        in seconds (the video always starts at 0 seconds).

        Args:
            start_sec (float): the clip start time in seconds
            end_sec (float): the clip end time in seconds
        Returns:
            clip_data:
                A dictionary mapping the entries at "video" and "audio" to a tensors.

                "video": A tensor of the clip's RGB frames with shape:
                (channel, time, height, width). The frames are of type torch.float32 and
                in the range [0 - 255].

                "audio": A tensor of the clip's audio samples with shape:
                (samples). The samples are of type torch.float32 and
                in the range [0 - 255].

            Returns None if no video or audio found within time range.

        """
        if self._selective_decoding:
            self._video, self._audio = self._pyav_decode_video(start_sec, end_sec)

        video_frames = None
        if self._video is not None:
            video_start_pts = secs_to_pts(
                start_sec, self._video_time_base, self._video_start_pts
            )
            video_end_pts = secs_to_pts(
                end_sec, self._video_time_base, self._video_start_pts
            )

            video_frames = [
                f
                for f, pts in self._video
                if pts >= video_start_pts and pts <= video_end_pts
            ]

        audio_samples = None
        if self._has_audio and self._audio is not None:
            audio_start_pts = secs_to_pts(
                start_sec, self._audio_time_base, self._audio_start_pts
            )
            audio_end_pts = secs_to_pts(
                end_sec, self._audio_time_base, self._audio_start_pts
            )
            audio_samples = [
                f
                for f, pts in self._audio
                if pts >= audio_start_pts and pts <= audio_end_pts
            ]
            audio_samples = torch.cat(audio_samples, axis=0)
            audio_samples = audio_samples.to(torch.float32)

        if video_frames is None or len(video_frames) == 0:
            logger.debug(
                f"No video found within {start_sec} and {end_sec} seconds. "
                f"Video starts at time 0 and ends at {self.duration}."
            )

            video_frames = None

        if video_frames is not None:
            video_frames = thwc_to_cthw(torch.stack(video_frames)).to(torch.float32)

        return {
            "video": video_frames,
            "audio": audio_samples,
        }

</source>
</class>

<class classid="5" nclones="2" nlines="46" similarity="77">
<source file="systems/pytorchvideo-0.1.3/pytorchvideo/data/epic_kitchen/utils.py" startline="10" endline="85" pcid="80">
def build_frame_manifest_from_flat_directory(
    data_directory_path: str, multithreaded: bool
) -> Dict[str, VideoFrameInfo]:
    """
    Args:
        data_directory_path (str): Path or URI to EpicKitchenDataset data.
                Data at this path must be a folder of structure:
                    {
                        "{video_id}": [
                            "frame_{frame_number}.{file_extension}",
                            "frame_{frame_number}.{file_extension}",
                            "frame_{frame_number}.{file_extension}",
                        ...]
                    ...}
        multithreaded (bool):
            controls whether io operations are performed across multiple threads.

    Returns:
        Dictionary mapping video_id of available videos to the locations of their
        underlying frame files.
    """

    video_frames = {}
    video_ids = g_pathmgr.ls(str(data_directory_path))

    def add_video_frames(video_id: str, video_path: str) -> None:
        video_frame_file_names = sorted(g_pathmgr.ls(video_path))
        for frame in video_frame_file_names:
            file_extension = frame.split(".")[-1]
            frame_name = frame[: -(len(file_extension) + 1)]
            stem, path_frame_id = frame_name.split("_")
            if video_id not in video_frames:
                video_frames[video_id] = VideoFrameInfo(
                    video_id=video_id,
                    location=video_path,
                    frame_file_stem=f"{stem}_",
                    frame_string_length=len(frame_name),
                    min_frame_number=int(path_frame_id),
                    max_frame_number=int(path_frame_id),
                    file_extension=file_extension,
                )
            else:
                video_frame_info = video_frames[video_id]
                # Check that this new frame is of the same format as other frames for this video
                # and that it is the next frame in order, if so update the frame info for this
                # video to reflect there is an additional frame.
                # We don't need to check video_id or frame_file_stem as they are function of
                # video_id which is aligned within the dictionary
                assert video_frame_info.frame_string_length == len(frame_name)
                assert video_frame_info.location == video_path, (
                    f"Frames for {video_id} found in two paths: "
                    f"{video_frame_info.location} and {video_path}"
                )
                assert video_frame_info.max_frame_number + 1 == int(path_frame_id)
                assert (
                    video_frame_info.file_extension == file_extension
                ), f"Frames with two different file extensions found for video {video_id}"
                video_frames[video_id] = VideoFrameInfo(
                    video_id=video_frame_info.video_id,
                    location=video_frame_info.location,
                    frame_file_stem=video_frame_info.frame_file_stem,
                    frame_string_length=video_frame_info.frame_string_length,
                    min_frame_number=video_frame_info.min_frame_number,
                    max_frame_number=int(path_frame_id),  # Update
                    file_extension=video_frame_info.file_extension,
                )

    video_paths = [
        (video_id, f"{data_directory_path}/{video_id}") for video_id in video_ids
    ]
    # Kick off frame indexing for all participants
    optional_threaded_foreach(add_video_frames, video_paths, multithreaded)

    return video_frames


</source>
<source file="systems/pytorchvideo-0.1.3/pytorchvideo/data/epic_kitchen/utils.py" startline="86" endline="172" pcid="82">
def build_frame_manifest_from_nested_directory(
    data_directory_path: str, multithreaded: bool
) -> Dict[str, VideoFrameInfo]:
    """
    Args:
        data_directory_path (str): Path or URI to EpicKitchenDataset data.
            If this dataset is to load from the frame-based dataset:
                Data at this path must be a folder of structure:
    {
        "{participant_id}" : [
            "{participant_id}_{participant_video_id}_{frame_number}.{file_extension}",

        ...],
    ...}

        multithreaded (bool):
                controls whether io operations are performed across multiple threads.

        Returns:
            Dictionary mapping video_id of available videos to the locations of their
            underlying frame files.
    """

    participant_ids = g_pathmgr.ls(str(data_directory_path))
    video_frames = {}

    # Create function to execute in parallel that lists files available for each participant
    def add_participant_video_frames(
        participant_id: str, participant_path: str
    ) -> None:
        participant_frames = sorted(g_pathmgr.ls(str(participant_path)))
        for frame_file_name in participant_frames:
            file_extension = frame_file_name.split(".")[-1]
            frame_name = frame_file_name[: -(len(file_extension) + 1)]
            [path_participant_id, path_video_id, path_frame_id] = frame_name.split("_")
            assert path_participant_id == participant_id
            video_id = f"{path_participant_id}_{path_video_id}"
            if (
                video_id not in video_frames
            ):  # This is the first frame we have seen from video w/ video_id
                video_frames[video_id] = VideoFrameInfo(
                    video_id=video_id,
                    location=participant_path,
                    frame_file_stem=f"{video_id}_",
                    frame_string_length=len(frame_name),
                    min_frame_number=int(path_frame_id),
                    max_frame_number=int(path_frame_id),
                    file_extension=file_extension,
                )
            else:
                video_frame_info = video_frames[video_id]
                # Check that this new frame is of the same format as other frames for this video
                # and that it is the next frame in order, if so update the frame info for this
                # video to reflect there is an additional frame.
                # We don't need to check video_id or frame_file_stem as they are function of
                # video_id which is aligned within the dictionary
                assert video_frame_info.frame_string_length == len(frame_name)
                assert video_frame_info.location == participant_path, (
                    f"Frames for {video_id} found in two paths: "
                    f"{video_frame_info.location} and {participant_path}"
                )
                assert video_frame_info.max_frame_number + 1 == int(path_frame_id)
                assert (
                    video_frame_info.file_extension == file_extension
                ), f"Frames with two different file extensions found for video {video_id}"
                video_frames[video_id] = VideoFrameInfo(
                    video_id=video_frame_info.video_id,
                    location=video_frame_info.location,
                    frame_file_stem=video_frame_info.frame_file_stem,
                    frame_string_length=video_frame_info.frame_string_length,
                    min_frame_number=video_frame_info.min_frame_number,
                    max_frame_number=int(path_frame_id),  # Update
                    file_extension=video_frame_info.file_extension,
                )

    particpant_paths = [
        (participant_id, f"{data_directory_path}/{participant_id}")
        for participant_id in participant_ids
    ]
    # Kick off frame indexing for all participants
    optional_threaded_foreach(
        add_participant_video_frames, particpant_paths, multithreaded
    )

    return video_frames


</source>
</class>

<class classid="6" nclones="4" nlines="21" similarity="70">
<source file="systems/pytorchvideo-0.1.3/pytorchvideo/accelerator/deployment/mobile_cpu/transmuter/transmuter_mobile_cpu.py" startline="13" endline="42" pcid="111">
def transmute_Conv3dPwBnAct(input_module: nn.Module):
    """
    Given an input_module, transmutes it into a equivalent Conv3dPwBnAct. Returns None
    if no equivalent Conv3dPwBnAct is found, else returns an instance of equivalent
    Conv3dPwBnAct.
    Args:
        input_module (nn.Module): input module to find an equivalent Conv3dPwBnAct
    """
    if not isinstance(input_module, nn.Conv3d):
        return None
    if (
        input_module.kernel_size == (1, 1, 1)
        and input_module.groups == 1
        and input_module.stride == (1, 1, 1)
        and input_module.padding == (0, 0, 0)
        and input_module.dilation == (1, 1, 1)
    ):
        module = Conv3dPwBnAct(
            in_channels=input_module.in_channels,
            out_channels=input_module.out_channels,
            bias=False if input_module.bias is None else True,
            activation="identity",
            use_bn=False,
        )
        module.kernel.conv.load_state_dict(input_module.state_dict())
        return module
    else:
        return None


</source>
<source file="systems/pytorchvideo-0.1.3/pytorchvideo/accelerator/deployment/mobile_cpu/transmuter/transmuter_mobile_cpu.py" startline="43" endline="76" pcid="112">
def transmute_Conv3d3x3x3DwBnAct(input_module: nn.Module):
    """
    Given an input_module, transmutes it into a equivalent Conv3d3x3x3DwBnAct. Returns
    None if no equivalent Conv3d3x3x3DwBnAct is found, else returns an instance of
    equivalent Conv3d3x3x3DwBnAct.
    Args:
        input_module (nn.Module): input module to find an equivalent Conv3d3x3x3DwBnAct
    """
    if not isinstance(input_module, nn.Conv3d):
        return None
    if (
        input_module.kernel_size == (3, 3, 3)
        and input_module.in_channels == input_module.out_channels
        and input_module.groups == input_module.out_channels
        and input_module.stride[0] == 1
        and input_module.stride[1] == input_module.stride[2]
        and input_module.padding == (1, 1, 1)
        and input_module.padding_mode == "zeros"
        and input_module.dilation == (1, 1, 1)
    ):
        spatial_stride = input_module.stride[1]
        module = Conv3d3x3x3DwBnAct(
            in_channels=input_module.in_channels,
            spatial_stride=spatial_stride,
            bias=False if input_module.bias is None else True,
            activation="identity",
            use_bn=False,
        )
        module.kernel.conv.load_state_dict(input_module.state_dict())
        return module
    else:
        return None


</source>
<source file="systems/pytorchvideo-0.1.3/pytorchvideo/accelerator/deployment/mobile_cpu/transmuter/transmuter_mobile_cpu.py" startline="130" endline="161" pcid="114">
def transmute_Conv3d3x1x1BnAct(input_module: nn.Module):
    """
    Given an input_module, transmutes it into a equivalent Conv3d3x1x1BnAct.
    Returns None if no equivalent Conv3d3x1x1BnAct is found, else returns
    an instance of equivalent Conv3d3x1x1BnAct.
    Args:
        input_module (nn.Module): input module to find an equivalent Conv3d3x1x1BnAct
    """
    if not isinstance(input_module, nn.Conv3d):
        return None

    if (
        input_module.kernel_size == (3, 1, 1)
        and input_module.stride == (1, 1, 1)
        and input_module.padding == (1, 0, 0)
        and input_module.dilation == (1, 1, 1)
        and input_module.padding_mode == "zeros"
    ):
        module = Conv3d3x1x1BnAct(
            in_channels=input_module.in_channels,
            out_channels=input_module.out_channels,
            bias=False if input_module.bias is None else True,
            groups=input_module.groups,
            activation="identity",
            use_bn=False,
        )
        module.kernel.conv.load_state_dict(input_module.state_dict())
        return module
    else:
        return None


</source>
<source file="systems/pytorchvideo-0.1.3/pytorchvideo/accelerator/deployment/mobile_cpu/transmuter/transmuter_mobile_cpu.py" startline="162" endline="193" pcid="115">
def transmute_Conv3d5x1x1BnAct(input_module: nn.Module):
    """
    Given an input_module, transmutes it into a equivalent Conv3d5x1x1BnAct.
    Returns None if no equivalent Conv3d5x1x1BnAct is found, else returns
    an instance of equivalent Conv3d5x1x1BnAct.
    Args:
        input_module (nn.Module): input module to find an equivalent Conv3d5x1x1BnAct
    """
    if not isinstance(input_module, nn.Conv3d):
        return None

    if (
        input_module.kernel_size == (5, 1, 1)
        and input_module.stride == (1, 1, 1)
        and input_module.padding == (2, 0, 0)
        and input_module.dilation == (1, 1, 1)
        and input_module.padding_mode == "zeros"
    ):
        module = Conv3d5x1x1BnAct(
            in_channels=input_module.in_channels,
            out_channels=input_module.out_channels,
            bias=False if input_module.bias is None else True,
            groups=input_module.groups,
            activation="identity",
            use_bn=False,
        )
        module.kernel.conv.load_state_dict(input_module.state_dict())
        return module
    else:
        return None


</source>
</class>

<class classid="7" nclones="2" nlines="10" similarity="100">
<source file="systems/pytorchvideo-0.1.3/pytorchvideo/transforms/mix.py" startline="35" endline="53" pcid="174">
    def __init__(
        self,
        alpha: float = 1.0,
        label_smoothing: float = 0.0,
        num_classes: int = 400,
    ) -> None:
        """
        This implements MixUp for videos.

        Args:
            alpha (float): Mixup alpha value.
            label_smoothing (float): Label smoothing value.
            num_classes (int): Number of total classes.
        """
        super().__init__()
        self.mixup_beta_sampler = torch.distributions.beta.Beta(alpha, alpha)
        self.label_smoothing = label_smoothing
        self.num_classes = num_classes

</source>
<source file="systems/pytorchvideo-0.1.3/pytorchvideo/transforms/mix.py" startline="85" endline="103" pcid="176">
    def __init__(
        self,
        alpha: float = 1.0,
        label_smoothing: float = 0.0,
        num_classes: int = 400,
    ) -> None:
        """
        This implements CutMix for videos.

        Args:
            alpha (float): CutMix alpha value.
            label_smoothing (float): Label smoothing value.
            num_classes (int): Number of total classes.
        """
        super().__init__()
        self.cutmix_beta_sampler = torch.distributions.beta.Beta(alpha, alpha)
        self.label_smoothing = label_smoothing
        self.num_classes = num_classes

</source>
</class>

<class classid="8" nclones="2" nlines="14" similarity="85">
<source file="systems/pytorchvideo-0.1.3/pytorchvideo/transforms/mix.py" startline="54" endline="78" pcid="175">
    def forward(
        self, x: torch.Tensor, labels: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        The input is a batch of samples and their corresponding labels.

        Args:
            x (torch.Tensor): Input tensor. The input should be a batch of videos with
                shape (B, C, T, H, W).
            labels (torch.Tensor): Labels for input with shape (B).
        """
        assert x.size(0) > 1, "MixUp cannot be applied to a single instance."

        mixup_lambda = self.mixup_beta_sampler.sample()
        x_flipped = x.flip(0).mul_(1.0 - mixup_lambda)
        x.mul_(mixup_lambda).add_(x_flipped)
        new_labels = _mix_labels(
            labels,
            self.num_classes,
            mixup_lambda,
            self.label_smoothing,
        )
        return x, new_labels


</source>
<source file="systems/pytorchvideo-0.1.3/pytorchvideo/transforms/mix.py" startline="140" endline="164" pcid="180">
    def forward(
        self, x: torch.Tensor, labels: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        The input is a batch of samples and their corresponding labels.

        Args:
            x (torch.Tensor): Input tensor. The input should be a batch of videos with
                shape (B, C, T, H, W).
            labels (torch.Tensor): Labels for input with shape (B).
        """
        assert x.size(0) > 1, "Cutmix cannot be applied to a single instance."
        assert x.dim() == 4 or x.dim() == 5, "Please correct input shape."

        cutmix_lamda = self.cutmix_beta_sampler.sample()
        x, cutmix_lamda_corrected = self._cutmix(x, cutmix_lamda)
        new_labels = _mix_labels(
            labels,
            self.num_classes,
            cutmix_lamda_corrected,
            self.label_smoothing,
        )
        return x, new_labels


</source>
</class>

<class classid="9" nclones="3" nlines="19" similarity="78">
<source file="systems/pytorchvideo-0.1.3/pytorchvideo/layers/batch_norm.py" startline="16" endline="43" pcid="183">
    def forward(self, input):
        if get_world_size() == 1 or not self.training:
            return super().forward(input)

        B, C = input.shape[0], input.shape[1]

        mean = torch.mean(input, dim=[0, 2])
        meansqr = torch.mean(input * input, dim=[0, 2])

        assert B > 0, "SyncBatchNorm does not support zero batch size."

        vec = torch.cat([mean, meansqr], dim=0)
        vec = differentiable_all_reduce(vec) * (1.0 / dist.get_world_size())
        mean, meansqr = torch.split(vec, C)
        var = meansqr - mean * mean

        invstd = torch.rsqrt(var + self.eps)
        scale = self.weight * invstd
        bias = self.bias - mean * scale
        scale = scale.reshape(1, -1, 1)
        bias = bias.reshape(1, -1, 1)

        self.running_mean += self.momentum * (mean.detach() - self.running_mean)
        self.running_var += self.momentum * (var.detach() - self.running_var)

        return input * scale + bias


</source>
<source file="systems/pytorchvideo-0.1.3/pytorchvideo/layers/batch_norm.py" startline="95" endline="120" pcid="185">
    def forward(self, input):
        if get_world_size() == 1 or not self.training:
            return super().forward(input)

        B, C = input.shape[0], input.shape[1]

        mean = torch.mean(input, dim=[0, 2, 3, 4])
        meansqr = torch.mean(input * input, dim=[0, 2, 3, 4])

        assert B > 0, "SyncBatchNorm does not support zero batch size."

        vec = torch.cat([mean, meansqr], dim=0)
        vec = differentiable_all_reduce(vec) * (1.0 / dist.get_world_size())
        mean, meansqr = torch.split(vec, C)
        var = meansqr - mean * mean

        invstd = torch.rsqrt(var + self.eps)
        scale = self.weight * invstd
        bias = self.bias - mean * scale
        scale = scale.reshape(1, -1, 1, 1, 1)
        bias = bias.reshape(1, -1, 1, 1, 1)

        self.running_mean += self.momentum * (mean.detach() - self.running_mean)
        self.running_var += self.momentum * (var.detach() - self.running_var)

        return input * scale + bias
</source>
<source file="systems/pytorchvideo-0.1.3/pytorchvideo/layers/batch_norm.py" startline="60" endline="87" pcid="184">
    def forward(self, input):
        if get_world_size() == 1 or not self.training:
            return super().forward(input)

        B, C = input.shape[0], input.shape[1]

        mean = torch.mean(input, dim=[0, 2, 3])
        meansqr = torch.mean(input * input, dim=[0, 2, 3])

        assert B > 0, "SyncBatchNorm does not support zero batch size."

        vec = torch.cat([mean, meansqr], dim=0)
        vec = differentiable_all_reduce(vec) * (1.0 / dist.get_world_size())
        mean, meansqr = torch.split(vec, C)
        var = meansqr - mean * mean

        invstd = torch.rsqrt(var + self.eps)
        scale = self.weight * invstd
        bias = self.bias - mean * scale
        scale = scale.reshape(1, -1, 1, 1)
        bias = bias.reshape(1, -1, 1, 1)

        self.running_mean += self.momentum * (mean.detach() - self.running_mean)
        self.running_var += self.momentum * (var.detach() - self.running_var)

        return input * scale + bias


</source>
</class>

<class classid="10" nclones="2" nlines="15" similarity="93">
<source file="systems/pytorchvideo-0.1.3/tests/test_layers_positional_encoding.py" startline="71" endline="87" pcid="259">
    def test_SpatioTemporalClsPositionalEncoding(self):
        # Test with cls token.
        batch_dim = 4
        dim = 16
        video_shape = (1, 2, 4)
        video_sum = video_shape[0] * video_shape[1] * video_shape[2]
        has_cls = True
        model = SpatioTemporalClsPositionalEncoding(
            embed_dim=dim,
            patch_embed_shape=video_shape,
            has_cls=has_cls,
        )
        fake_input = torch.rand(batch_dim, video_sum, dim)
        output = model(fake_input)
        output_gt_shape = (batch_dim, video_sum + 1, dim)
        self.assertEqual(tuple(output.shape), output_gt_shape)

</source>
<source file="systems/pytorchvideo-0.1.3/tests/test_layers_positional_encoding.py" startline="88" endline="104" pcid="260">
    def test_SpatioTemporalClsPositionalEncoding_nocls(self):
        # Test without cls token.
        batch_dim = 4
        dim = 16
        video_shape = (1, 2, 4)
        video_sum = video_shape[0] * video_shape[1] * video_shape[2]
        has_cls = False
        model = SpatioTemporalClsPositionalEncoding(
            embed_dim=dim,
            patch_embed_shape=video_shape,
            has_cls=has_cls,
        )
        fake_input = torch.rand(batch_dim, video_sum, dim)
        output = model(fake_input)
        output_gt_shape = (batch_dim, video_sum, dim)
        self.assertEqual(tuple(output.shape), output_gt_shape)

</source>
</class>

<class classid="11" nclones="3" nlines="37" similarity="81">
<source file="systems/pytorchvideo-0.1.3/tests/test_models_stem.py" startline="22" endline="67" pcid="263">
    def test_create_simple_stem(self):
        """
        Test simple ResNetBasicStem (without pooling layer).
        """
        for input_dim, output_dim in itertools.product((2, 3), (4, 8, 16)):
            model = ResNetBasicStem(
                conv=nn.Conv3d(
                    input_dim,
                    output_dim,
                    kernel_size=3,
                    stride=1,
                    padding=1,
                    bias=False,
                ),
                norm=nn.BatchNorm3d(output_dim),
                activation=nn.ReLU(),
                pool=None,
            )

            # Test forwarding.
            for tensor in TestResNetBasicStem._get_inputs(input_dim):
                if tensor.shape[1] != input_dim:
                    with self.assertRaises(RuntimeError):
                        output_tensor = model(tensor)
                    continue
                else:
                    output_tensor = model(tensor)

                input_shape = tensor.shape
                output_shape = output_tensor.shape
                output_shape_gt = (
                    input_shape[0],
                    output_dim,
                    input_shape[2],
                    input_shape[3],
                    input_shape[4],
                )

                self.assertEqual(
                    output_shape,
                    output_shape_gt,
                    "Output shape {} is different from expected shape {}".format(
                        output_shape, output_shape_gt
                    ),
                )

</source>
<source file="systems/pytorchvideo-0.1.3/tests/test_models_stem.py" startline="68" endline="113" pcid="264">
    def test_create_stem_with_conv_reduced_3d(self):
        """
        Test simple ResNetBasicStem with ConvReduce3D.
        """
        for input_dim, output_dim in itertools.product((2, 3), (4, 8, 16)):
            model = ResNetBasicStem(
                conv=ConvReduce3D(
                    in_channels=input_dim,
                    out_channels=output_dim,
                    kernel_size=(3, 3),
                    stride=(1, 1),
                    padding=(1, 1),
                    bias=(False, False),
                ),
                norm=nn.BatchNorm3d(output_dim),
                activation=nn.ReLU(),
                pool=None,
            )

            # Test forwarding.
            for tensor in TestResNetBasicStem._get_inputs(input_dim):
                if tensor.shape[1] != input_dim:
                    with self.assertRaises(RuntimeError):
                        output_tensor = model(tensor)
                    continue
                else:
                    output_tensor = model(tensor)

                input_shape = tensor.shape
                output_shape = output_tensor.shape
                output_shape_gt = (
                    input_shape[0],
                    output_dim,
                    input_shape[2],
                    input_shape[3],
                    input_shape[4],
                )

                self.assertEqual(
                    output_shape,
                    output_shape_gt,
                    "Output shape {} is different from expected shape {}".format(
                        output_shape, output_shape_gt
                    ),
                )

</source>
<source file="systems/pytorchvideo-0.1.3/tests/test_models_stem.py" startline="114" endline="162" pcid="265">
    def test_create_complex_stem(self):
        """
        Test complex ResNetBasicStem.
        """
        for input_dim, output_dim in itertools.product((2, 3), (4, 8, 16)):
            model = ResNetBasicStem(
                conv=nn.Conv3d(
                    input_dim,
                    output_dim,
                    kernel_size=[3, 7, 7],
                    stride=[1, 2, 2],
                    padding=[1, 3, 3],
                    bias=False,
                ),
                norm=nn.BatchNorm3d(output_dim),
                activation=nn.ReLU(),
                pool=nn.MaxPool3d(
                    kernel_size=[1, 3, 3], stride=[1, 2, 2], padding=[0, 1, 1]
                ),
            )

            # Test forwarding.
            for input_tensor in TestResNetBasicStem._get_inputs(input_dim):
                if input_tensor.shape[1] != input_dim:
                    with self.assertRaises(Exception):
                        output_tensor = model(input_tensor)
                    continue
                else:
                    output_tensor = model(input_tensor)

                input_shape = input_tensor.shape
                output_shape = output_tensor.shape

                output_shape_gt = (
                    input_shape[0],
                    output_dim,
                    input_shape[2],
                    (((input_shape[3] - 1) // 2 + 1) - 1) // 2 + 1,
                    (((input_shape[4] - 1) // 2 + 1) - 1) // 2 + 1,
                )

                self.assertEqual(
                    output_shape,
                    output_shape_gt,
                    "Output shape {} is different from expected shape {}".format(
                        output_shape, output_shape_gt
                    ),
                )

</source>
</class>

<class classid="12" nclones="2" nlines="46" similarity="84">
<source file="systems/pytorchvideo-0.1.3/tests/test_models_stem.py" startline="163" endline="219" pcid="266">
    def test_create_stem_with_callable(self):
        """
        Test builder `create_res_basic_stem` with callable inputs.
        """
        for (pool, activation, norm) in itertools.product(
            (nn.AvgPool3d, nn.MaxPool3d, None),
            (nn.ReLU, nn.Softmax, nn.Sigmoid, None),
            (nn.BatchNorm3d, None),
        ):
            model = create_res_basic_stem(
                in_channels=3,
                out_channels=64,
                pool=pool,
                activation=activation,
                norm=norm,
            )
            model_gt = ResNetBasicStem(
                conv=nn.Conv3d(
                    3,
                    64,
                    kernel_size=[3, 7, 7],
                    stride=[1, 2, 2],
                    padding=[1, 3, 3],
                    bias=False,
                ),
                norm=None if norm is None else norm(64),
                activation=None if activation is None else activation(),
                pool=None
                if pool is None
                else pool(kernel_size=[1, 3, 3], stride=[1, 2, 2], padding=[0, 1, 1]),
            )

            model.load_state_dict(
                model_gt.state_dict(), strict=True
            )  # explicitly use strict mode.

            # Test forwarding.
            for input_tensor in TestResNetBasicStem._get_inputs():
                with torch.no_grad():
                    if input_tensor.shape[1] != 3:
                        with self.assertRaises(RuntimeError):
                            output_tensor = model(input_tensor)
                        continue
                    else:
                        output_tensor = model(input_tensor)
                        output_tensor_gt = model_gt(input_tensor)
                self.assertEqual(
                    output_tensor.shape,
                    output_tensor_gt.shape,
                    "Output shape {} is different from expected shape {}".format(
                        output_tensor.shape, output_tensor_gt.shape
                    ),
                )
                self.assertTrue(
                    np.allclose(output_tensor.numpy(), output_tensor_gt.numpy())
                )

</source>
<source file="systems/pytorchvideo-0.1.3/tests/test_models_stem.py" startline="220" endline="277" pcid="267">
    def test_create_acoustic_stem_with_callable(self):
        """
        Test builder `create_acoustic_res_basic_stem` with callable
        inputs.
        """
        for (pool, activation, norm) in itertools.product(
            (nn.AvgPool3d, nn.MaxPool3d, None),
            (nn.ReLU, nn.Softmax, nn.Sigmoid, None),
            (nn.BatchNorm3d, None),
        ):
            model = create_acoustic_res_basic_stem(
                in_channels=3,
                out_channels=64,
                pool=pool,
                activation=activation,
                norm=norm,
            )
            model_gt = ResNetBasicStem(
                conv=ConvReduce3D(
                    in_channels=3,
                    out_channels=64,
                    kernel_size=((3, 1, 1), (1, 7, 7)),
                    stride=((1, 1, 1), (1, 1, 1)),
                    padding=((1, 0, 0), (0, 3, 3)),
                    bias=(False, False),
                ),
                norm=None if norm is None else norm(64),
                activation=None if activation is None else activation(),
                pool=None
                if pool is None
                else pool(kernel_size=[1, 3, 3], stride=[1, 2, 2], padding=[0, 1, 1]),
            )

            model.load_state_dict(
                model_gt.state_dict(), strict=True
            )  # explicitly use strict mode.

            # Test forwarding.
            for input_tensor in TestResNetBasicStem._get_inputs():
                with torch.no_grad():
                    if input_tensor.shape[1] != 3:
                        with self.assertRaises(RuntimeError):
                            output_tensor = model(input_tensor)
                        continue
                    else:
                        output_tensor = model(input_tensor)
                        output_tensor_gt = model_gt(input_tensor)
                self.assertEqual(
                    output_tensor.shape,
                    output_tensor_gt.shape,
                    "Output shape {} is different from expected shape {}".format(
                        output_tensor.shape, output_tensor_gt.shape
                    ),
                )
                self.assertTrue(
                    np.allclose(output_tensor.numpy(), output_tensor_gt.numpy())
                )

</source>
</class>

<class classid="13" nclones="6" nlines="16" similarity="77">
<source file="systems/pytorchvideo-0.1.3/tests/test_models_stem.py" startline="279" endline="303" pcid="268">
    def _get_inputs(input_dim: int = 3) -> torch.tensor:
        """
        Provide different tensors as test cases.

        Yield:
            (torch.tensor): tensor as test case input.
        """
        # Prepare random tensor as test cases.
        shapes = (
            # Forward succeeded.
            (1, input_dim, 3, 7, 7),
            (1, input_dim, 5, 7, 7),
            (1, input_dim, 7, 7, 7),
            (2, input_dim, 3, 7, 7),
            (4, input_dim, 3, 7, 7),
            (8, input_dim, 3, 7, 7),
            (2, input_dim, 3, 7, 14),
            (2, input_dim, 3, 14, 7),
            (2, input_dim, 3, 14, 14),
            # Forward failed.
            (8, input_dim * 2, 3, 7, 7),
            (8, input_dim * 4, 5, 7, 7),
        )
        for shape in shapes:
            yield torch.rand(shape)
</source>
<source file="systems/pytorchvideo-0.1.3/tests/test_layers_nonlocal_net.py" startline="136" endline="159" pcid="441">
    def _get_inputs(input_dim: int = 8) -> torch.tensor:
        """
        Provide different tensors as test cases.

        Yield:
            (torch.tensor): tensor as test case input.
        """
        # Prepare random tensor as test cases.
        shapes = (
            # Forward succeeded.
            (1, input_dim, 5, 7, 7),
            (2, input_dim, 5, 7, 7),
            (4, input_dim, 5, 7, 7),
            (4, input_dim, 5, 7, 7),
            (4, input_dim, 7, 7, 7),
            (4, input_dim, 7, 7, 14),
            (4, input_dim, 7, 14, 7),
            (4, input_dim, 7, 14, 14),
            # Forward failed.
            (8, input_dim * 2, 3, 7, 7),
            (8, input_dim * 4, 5, 7, 7),
        )
        for shape in shapes:
            yield torch.rand(shape)
</source>
<source file="systems/pytorchvideo-0.1.3/tests/test_layers_convolutions.py" startline="195" endline="219" pcid="469">
    def _get_inputs(input_dim: int = 3) -> torch.tensor:
        """
        Provide different tensors as test cases.

        Yield:
            (torch.tensor): tensor as test case input.
        """
        # Prepare random tensor as test cases.
        shapes = (
            # Forward succeeded.
            (1, input_dim, 3, 7, 7),
            (1, input_dim, 5, 7, 7),
            (1, input_dim, 7, 7, 7),
            (2, input_dim, 3, 7, 7),
            (4, input_dim, 3, 7, 7),
            (8, input_dim, 3, 7, 7),
            (2, input_dim, 3, 7, 14),
            (2, input_dim, 3, 14, 7),
            (2, input_dim, 3, 14, 14),
            # Forward failed.
            (8, input_dim * 2, 3, 7, 7),
            (8, input_dim * 4, 5, 7, 7),
        )
        for shape in shapes:
            yield torch.rand(shape)
</source>
<source file="systems/pytorchvideo-0.1.3/tests/test_layers_convolutions.py" startline="98" endline="124" pcid="466">
    def _get_inputs(input_dim: int = 3) -> torch.tensor:
        """
        Provide different tensors as test cases.

        Yield:
            (torch.tensor): tensor as test case input.
        """
        # Prepare random tensor as test cases.
        shapes = (
            # Forward succeeded.
            (1, input_dim, 3, 7, 7),
            (1, input_dim, 5, 7, 7),
            (1, input_dim, 7, 7, 7),
            (2, input_dim, 3, 7, 7),
            (4, input_dim, 3, 7, 7),
            (8, input_dim, 3, 7, 7),
            (2, input_dim, 3, 7, 14),
            (2, input_dim, 3, 14, 7),
            (2, input_dim, 3, 14, 14),
            # Forward failed.
            (8, input_dim * 2, 3, 7, 7),
            (8, input_dim * 4, 5, 7, 7),
        )
        for shape in shapes:
            yield torch.rand(shape)


</source>
<source file="systems/pytorchvideo-0.1.3/tests/test_models_head.py" startline="156" endline="181" pcid="365">
    def _get_inputs(input_dim: int = 8) -> torch.tensor:
        """
        Provide different tensors as test cases.

        Yield:
            (torch.tensor): tensor as test case input.
        """
        # Prepare random tensor as test cases.
        shapes = (
            # Forward succeeded.
            (1, input_dim, 5, 7, 7),
            (2, input_dim, 5, 7, 7),
            (4, input_dim, 5, 7, 7),
            (4, input_dim, 5, 7, 7),
            (4, input_dim, 7, 7, 7),
            (4, input_dim, 7, 7, 14),
            (4, input_dim, 7, 14, 7),
            (4, input_dim, 7, 14, 14),
            # Forward failed.
            (8, input_dim * 2, 3, 7, 7),
            (8, input_dim * 4, 5, 7, 7),
        )
        for shape in shapes:
            yield torch.rand(shape)


</source>
<source file="systems/pytorchvideo-0.1.3/tests/test_models_head.py" startline="375" endline="402" pcid="372">
    def _get_inputs(input_dim: int = 8) -> torch.tensor:
        """
        Provide different tensors as test cases.

        Yield:
            (torch.tensor): tensor as test case input.
            (torch.tensor): tensor as test case bboxes.
        """
        # Prepare random tensor as test cases.
        shapes = (
            # Forward succeeded.
            (1, input_dim, 5, 7, 7),
            (2, input_dim, 5, 7, 7),
            (4, input_dim, 5, 7, 7),
            (4, input_dim, 5, 7, 7),
            (4, input_dim, 7, 7, 7),
            (4, input_dim, 7, 7, 14),
            (4, input_dim, 7, 14, 7),
            (4, input_dim, 7, 14, 14),
            # Forward failed.
            (8, input_dim * 2, 3, 7, 7),
            (8, input_dim * 4, 5, 7, 7),
        )
        for shape in shapes:
            input_tensor = torch.rand(shape)
            bboxes = [[i, 1, 2, 3, 4] for i in range(input_tensor.shape[0])]
            bboxes = torch.Tensor(bboxes)
            yield (input_tensor, bboxes)
</source>
</class>

<class classid="14" nclones="3" nlines="11" similarity="100">
<source file="systems/pytorchvideo-0.1.3/tests/test_data_json_dataset.py" startline="18" endline="30" pcid="272">
    def test_recognition_random_clip_sampler(self):
        total_duration = 0.05
        with mock_json_annotations() as (annotation_json, labels, duration):
            clip_sampler = make_clip_sampler("random", total_duration)
            dataset = json_dataset.clip_recognition_dataset(
                data_path=annotation_json,
                clip_sampler=clip_sampler,
                decode_audio=False,
            )

            self.assertEqual(dataset.num_videos, 4)
            self.assertEqual(len(list(iter(dataset))), 4)

</source>
<source file="systems/pytorchvideo-0.1.3/tests/test_data_json_dataset.py" startline="31" endline="43" pcid="273">
    def test_recognition_uniform_clip_sampler(self):
        total_duration = 0.05
        with mock_json_annotations() as (annotation_json, labels, duration):
            clip_sampler = make_clip_sampler("uniform", total_duration)
            dataset = json_dataset.clip_recognition_dataset(
                data_path=annotation_json,
                clip_sampler=clip_sampler,
                decode_audio=False,
            )

            self.assertEqual(dataset.num_videos, 4)
            self.assertEqual(len(list(iter(dataset))), 4)

</source>
<source file="systems/pytorchvideo-0.1.3/tests/test_data_json_dataset.py" startline="44" endline="57" pcid="274">
    def test_video_only_frame_video_dataset(self):
        total_duration = 2.0
        with mock_json_annotations() as (annotation_json, labels, duration):
            clip_sampler = make_clip_sampler("random", total_duration)
            dataset = json_dataset.video_only_dataset(
                data_path=annotation_json,
                clip_sampler=clip_sampler,
                decode_audio=False,
            )

            self.assertEqual(dataset.num_videos, 2)
            self.assertEqual(len(list(iter(dataset))), 2)


</source>
</class>

<class classid="15" nclones="2" nlines="32" similarity="90">
<source file="systems/pytorchvideo-0.1.3/tests/test_accelerator_models_efficient_x3d.py" startline="49" endline="87" pcid="278">
    def test_load_hubconf(self):
        path = os.path.join(
            os.path.dirname(os.path.realpath(__file__)),
            "..",
        )
        for (input_clip_length, input_crop_size, model_name) in [
            (4, 160, "efficient_x3d_xs"),
            (13, 160, "efficient_x3d_s"),
        ]:
            model = torch.hub.load(
                repo_or_dir=path,
                source="local",
                model=model_name,
                pretrained=False,
            )
            self.assertIsNotNone(model)

            # Test forwarding.
            for tensor in TestEfficientX3d._get_inputs(
                input_clip_length, input_crop_size
            ):
                if tensor.shape[1] != 3:
                    with self.assertRaises(RuntimeError):
                        out = model(tensor)
                    continue

                out = model(tensor)

                output_shape = out.shape
                output_shape_gt = (tensor.shape[0], 400)

                self.assertEqual(
                    output_shape,
                    output_shape_gt,
                    "Output shape {} is different from expected shape {}".format(
                        output_shape, output_shape_gt
                    ),
                )

</source>
<source file="systems/pytorchvideo-0.1.3/tests/test_models_x3d.py" startline="82" endline="120" pcid="359">
    def test_load_hubconf(self):
        path = os.path.join(
            os.path.dirname(os.path.realpath(__file__)),
            "..",
        )
        for (input_clip_length, input_crop_size, model_name) in [
            (4, 160, "x3d_xs"),
            (13, 160, "x3d_s"),
            (16, 224, "x3d_m"),
        ]:
            model = torch.hub.load(
                repo_or_dir=path,
                source="local",
                model=model_name,
                pretrained=False,
                head_output_with_global_average=True,
            )
            self.assertIsNotNone(model)

            # Test forwarding.
            for tensor in TestX3d._get_inputs(input_clip_length, input_crop_size):
                if tensor.shape[1] != 3:
                    with self.assertRaises(RuntimeError):
                        out = model(tensor)
                    continue

                out = model(tensor)

                output_shape = out.shape
                output_shape_gt = (tensor.shape[0], 400)

                self.assertEqual(
                    output_shape,
                    output_shape_gt,
                    "Output shape {} is different from expected shape {}".format(
                        output_shape, output_shape_gt
                    ),
                )

</source>
</class>

<class classid="16" nclones="2" nlines="35" similarity="73">
<source file="systems/pytorchvideo-0.1.3/tests/test_data_domsev_dataset.py" startline="121" endline="168" pcid="304">
    def test__len__(self, dataset_type):
        with tempfile.TemporaryDirectory(prefix=f"{TestDomsevVideoDataset}") as tempdir:
            tempdir = Path(tempdir)

            video_info_file = tempdir / "test_video_info.csv"
            save_dataclass_objs_to_headered_csv(
                list(MOCK_VIDEO_INFOS.values()), video_info_file
            )
            label_file = tempdir / "activity_video_info.csv"
            labels = []
            for label_list in self.LABELS_DATA.values():
                for label_data in label_list:
                    labels.append(label_data)
            save_dataclass_objs_to_headered_csv(labels, label_file)

            video_data_manifest_file_path = (
                tempdir / "video_data_manifest_file_path.json"
            )
            with ExitStack() as stack:
                if dataset_type == VideoDatasetType.Frame:
                    video_data_dict = get_flat_video_frames(tempdir, "jpg")
                elif dataset_type == VideoDatasetType.EncodedVideo:
                    video_data_dict = get_encoded_video_infos(tempdir, stack)

                save_dataclass_objs_to_headered_csv(
                    list(video_data_dict.values()), video_data_manifest_file_path
                )
                video_ids = list(self.LABELS_DATA)
                dataset = DomsevVideoDataset(
                    video_data_manifest_file_path=str(video_data_manifest_file_path),
                    video_info_file_path=str(video_info_file),
                    labels_file_path=str(label_file),
                    dataset_type=dataset_type,
                    clip_sampler=lambda x, y: [
                        VideoClipInfo(video_ids[i // 2], i * 2.0, i * 2.0 + 0.9)
                        for i in range(0, 7)
                    ],
                )

                self.assertEqual(len(dataset._videos), 4)
                total_labels = [
                    label_data
                    for video_labels in list(dataset._labels_per_video.values())
                    for label_data in video_labels
                ]
                self.assertEqual(len(total_labels), 6)
                self.assertEqual(len(dataset), 7)  # Num clips

</source>
<source file="systems/pytorchvideo-0.1.3/tests/test_data_epic_kitchen_dataset.py" startline="156" endline="196" pcid="316">
    def test__len__(self, dataset_type):
        with tempfile.TemporaryDirectory(prefix=f"{TestEpicKitchenDataset}") as tempdir:
            tempdir = Path(tempdir)

            video_info_file = tempdir / "test_video_info.csv"
            save_dataclass_objs_to_headered_csv(
                list(MOCK_VIDEO_INFOS.values()), video_info_file
            )
            action_file = tempdir / "action_video_info.csv"
            actions = []
            for action_list in self.ACTIONS_DATAS.values():
                for action in action_list:
                    actions.append(action)
            save_dataclass_objs_to_headered_csv(actions, action_file)

            video_data_manifest_file_path = (
                tempdir / "video_data_manifest_file_path.json"
            )
            with ExitStack() as stack:
                if dataset_type == VideoDatasetType.Frame:
                    video_data_dict = get_flat_video_frames(tempdir, "jpg")
                elif dataset_type == VideoDatasetType.EncodedVideo:
                    video_data_dict = get_encoded_video_infos(tempdir, stack)

                save_dataclass_objs_to_headered_csv(
                    list(video_data_dict.values()), video_data_manifest_file_path
                )

                dataset = EpicKitchenDataset(
                    video_info_file_path=str(video_info_file),
                    actions_file_path=str(action_file),
                    clip_sampler=lambda x, y: [
                        VideoClipInfo(str(i), i * 2.0, i * 2.0 + 0.9)
                        for i in range(0, 7)
                    ],
                    video_data_manifest_file_path=str(video_data_manifest_file_path),
                    dataset_type=dataset_type,
                )

                self.assertEqual(len(dataset), 7)

</source>
</class>

<class classid="17" nclones="2" nlines="50" similarity="96">
<source file="systems/pytorchvideo-0.1.3/tests/test_data_domsev_dataset.py" startline="170" endline="230" pcid="305">
    def test__getitem__(self, dataset_type):
        with tempfile.TemporaryDirectory(prefix=f"{TestDomsevVideoDataset}") as tempdir:
            tempdir = Path(tempdir)

            video_info_file = tempdir / "test_video_info.csv"
            save_dataclass_objs_to_headered_csv(
                list(MOCK_VIDEO_INFOS.values()), video_info_file
            )
            label_file = tempdir / "activity_video_info.csv"
            labels = []
            for label_list in self.LABELS_DATA.values():
                for label_data in label_list:
                    labels.append(label_data)
            save_dataclass_objs_to_headered_csv(labels, label_file)

            video_data_manifest_file_path = (
                tempdir / "video_data_manifest_file_path.json"
            )
            with ExitStack() as stack:
                if dataset_type == VideoDatasetType.Frame:
                    video_data_dict = get_flat_video_frames(tempdir, "jpg")
                elif dataset_type == VideoDatasetType.EncodedVideo:
                    video_data_dict = get_encoded_video_infos(tempdir, stack)

                save_dataclass_objs_to_headered_csv(
                    list(video_data_dict.values()), video_data_manifest_file_path
                )
                video_ids = list(self.LABELS_DATA)
                dataset = DomsevVideoDataset(
                    video_data_manifest_file_path=str(video_data_manifest_file_path),
                    video_info_file_path=str(video_info_file),
                    labels_file_path=str(label_file),
                    dataset_type=dataset_type,
                    clip_sampler=lambda x, y: [
                        VideoClipInfo(video_ids[i // 2], i * 2.0, i * 2.0 + 0.9)
                        for i in range(0, 7)
                    ],
                )

                get_clip_string = (
                    "pytorchvideo.data.frame_video.FrameVideo.get_clip"
                    if dataset_type == VideoDatasetType.Frame
                    else "pytorchvideo.data.encoded_video.EncodedVideo.get_clip"
                )
                with unittest.mock.patch(
                    get_clip_string,
                    return_value=({"video": torch.rand(3, 5, 10, 20), "audio": []}),
                ) as _:
                    clip_1 = dataset.__getitem__(1)
                    for i, a in enumerate(clip_1["labels"]):
                        self.assertEqual(a, self.LABELS_DATA[video_ids[0]][i])
                    self.assertEqual(clip_1["start_time"], 2.0)
                    self.assertEqual(clip_1["stop_time"], 2.9)
                    self.assertEqual(clip_1["video_id"], MOCK_VIDEO_IDS[0])

                    clip_2 = dataset.__getitem__(2)
                    for i, a in enumerate(clip_2["labels"]):
                        self.assertEqual(a, self.LABELS_DATA[video_ids[1]][i])
                    self.assertEqual(clip_2["start_time"], 4.0)
                    self.assertEqual(clip_2["stop_time"], 4.9)
                    self.assertEqual(clip_2["video_id"], MOCK_VIDEO_IDS[1])
</source>
<source file="systems/pytorchvideo-0.1.3/tests/test_data_epic_kitchen_dataset.py" startline="198" endline="258" pcid="317">
    def test__getitem__(self, dataset_type):
        with tempfile.TemporaryDirectory(prefix=f"{TestEpicKitchenDataset}") as tempdir:
            tempdir = Path(tempdir)

            video_info_file = tempdir / "test_video_info.csv"
            save_dataclass_objs_to_headered_csv(
                list(MOCK_VIDEO_INFOS.values()), video_info_file
            )
            action_file = tempdir / "action_video_info.csv"
            actions = []
            for action_list in self.ACTIONS_DATAS.values():
                for action in action_list:
                    actions.append(action)
            save_dataclass_objs_to_headered_csv(actions, action_file)

            video_data_manifest_file_path = (
                tempdir / "video_data_manifest_file_path.json"
            )
            with ExitStack() as stack:
                if dataset_type == VideoDatasetType.Frame:
                    video_data_dict = get_flat_video_frames(tempdir, "jpg")
                elif dataset_type == VideoDatasetType.EncodedVideo:
                    video_data_dict = get_encoded_video_infos(tempdir, stack)

                save_dataclass_objs_to_headered_csv(
                    list(video_data_dict.values()), video_data_manifest_file_path
                )
                video_ids = list(self.ACTIONS_DATAS)
                dataset = EpicKitchenDataset(
                    video_info_file_path=str(video_info_file),
                    actions_file_path=str(action_file),
                    clip_sampler=lambda x, y: [
                        VideoClipInfo(video_ids[i // 2], i * 2.0, i * 2.0 + 0.9)
                        for i in range(0, 7)
                    ],
                    video_data_manifest_file_path=str(video_data_manifest_file_path),
                    dataset_type=dataset_type,
                )

                get_clip_string = (
                    "pytorchvideo.data.frame_video.FrameVideo.get_clip"
                    if dataset_type == VideoDatasetType.Frame
                    else "pytorchvideo.data.encoded_video.EncodedVideo.get_clip"
                )
                with unittest.mock.patch(
                    get_clip_string,
                    return_value=({"video": torch.rand(3, 5, 10, 20), "audio": []}),
                ) as _:
                    clip_1 = dataset.__getitem__(1)
                    for i, a in enumerate(clip_1["actions"]):
                        self.assertEqual(a, self.ACTIONS_DATAS[video_ids[0]][i])
                    self.assertEqual(clip_1["start_time"], 2.0)
                    self.assertEqual(clip_1["stop_time"], 2.9)
                    self.assertEqual(clip_1["video_id"], MOCK_VIDEO_IDS[0])

                    clip_2 = dataset.__getitem__(2)
                    for i, a in enumerate(clip_2["actions"]):
                        self.assertEqual(a, self.ACTIONS_DATAS[video_ids[1]][i])
                    self.assertEqual(clip_2["start_time"], 4.0)
                    self.assertEqual(clip_2["stop_time"], 4.9)
                    self.assertEqual(clip_2["video_id"], MOCK_VIDEO_IDS[1])
</source>
</class>

<class classid="18" nclones="3" nlines="21" similarity="76">
<source file="systems/pytorchvideo-0.1.3/tests/test_data_encoded_video.py" startline="39" endline="69" pcid="325">
    def test_video_with_shorter_audio_works(self):
        num_audio_samples = 8000
        num_frames = 5
        fps = 5
        audio_rate = 8000
        with temp_encoded_video_with_audio(
            num_frames=num_frames,
            fps=fps,
            num_audio_samples=num_audio_samples,
            audio_rate=audio_rate,
        ) as (file_name, video_data, audio_data):
            test_video = EncodedVideo.from_path(file_name)

            # Duration is max of both streams, therefore, the video duration will be expected.
            self.assertEqual(test_video.duration, num_frames / fps)

            # All audio (0 - 2 seconds)
            clip = test_video.get_clip(0, test_video.duration)
            frames, audio_samples = clip["video"], clip["audio"]
            self.assertTrue(frames.equal(video_data))
            self.assertTrue(audio_samples.equal(audio_data))

            # Half frames
            clip = test_video.get_clip(0, test_video.duration / 2)
            frames, audio_samples = clip["video"], clip["audio"]

            self.assertTrue(frames.equal(video_data[:, : num_frames // 2]))
            self.assertTrue(audio_samples.equal(audio_data))

            test_video.close()

</source>
<source file="systems/pytorchvideo-0.1.3/tests/test_data_encoded_video.py" startline="70" endline="96" pcid="326">
    def test_video_with_longer_audio_works(self):
        audio_rate = 10000
        fps = 5
        num_frames = 5
        num_audio_samples = 40000
        with temp_encoded_video_with_audio(
            num_frames=num_frames,
            fps=fps,
            num_audio_samples=num_audio_samples,
            audio_rate=audio_rate,
        ) as (file_name, video_data, audio_data):
            test_video = EncodedVideo.from_path(file_name)

            # All audio
            clip = test_video.get_clip(0, test_video.duration)
            frames, audio_samples = clip["video"], clip["audio"]
            self.assertTrue(frames.equal(video_data))
            self.assertTrue(audio_samples.equal(audio_data))

            # No frames (3 - 5 seconds)
            clip = test_video.get_clip(test_video.duration + 1, test_video.duration + 2)
            frames, audio_samples = clip["video"], clip["audio"]
            self.assertEqual(frames, None)
            self.assertEqual(audio_samples, None)

            test_video.close()

</source>
<source file="systems/pytorchvideo-0.1.3/tests/test_data_encoded_video.py" startline="97" endline="117" pcid="327">
    def test_decode_audio_is_false(self):
        audio_rate = 10000
        fps = 5
        num_frames = 5
        num_audio_samples = 40000
        with temp_encoded_video_with_audio(
            num_frames=num_frames,
            fps=fps,
            num_audio_samples=num_audio_samples,
            audio_rate=audio_rate,
        ) as (file_name, video_data, audio_data):
            test_video = EncodedVideo.from_path(file_name, decode_audio=False)

            # All audio
            clip = test_video.get_clip(0, test_video.duration)
            frames, audio_samples = clip["video"], clip["audio"]
            self.assertTrue(frames.equal(video_data))
            self.assertEqual(audio_samples, None)

            test_video.close()

</source>
</class>

<class classid="19" nclones="3" nlines="82" similarity="92">
<source file="systems/pytorchvideo-0.1.3/tests/benchmark_accelerator_efficient_blocks.py" startline="27" endline="140" pcid="332">
    def test_benchmark_conv3d_pw_bn_relu(self, num_iters: int = 20) -> None:
        """
        Benchmark Conv3dPwBnAct with ReLU activation.
        Note efficient block Conv3dPwBnAct is designed for mobile cpu with qnnpack
        backend, and benchmarking on server with another backend (e.g., fbgemm) may
        have different latency result compared to running on mobile cpu with qnnpack.
        Running on x86 based server cpu with qnnpack may also have different latency as
        running on mobile cpu with qnnpack, as qnnpack is optimized for
        ARM based mobile cpu.
        Args:
            num_iters (int): number of iterations to perform benchmarking.
        """

        torch.backends.quantized.engine = "qnnpack"
        kwargs_list = [
            {
                "mode": "original",
                "input_blob_size": (1, 48, 4, 40, 40),
                "in_channels": 48,
                "out_channels": 108,
                "quantize": False,
            },
            {
                "mode": "deployable",
                "input_blob_size": (1, 48, 4, 40, 40),
                "in_channels": 48,
                "out_channels": 108,
                "quantize": False,
            },
            {
                "mode": "original",
                "input_blob_size": (1, 48, 4, 40, 40),
                "in_channels": 48,
                "out_channels": 108,
                "quantize": True,
            },
            {
                "mode": "deployable",
                "input_blob_size": (1, 48, 4, 40, 40),
                "in_channels": 48,
                "out_channels": 108,
                "quantize": True,
            },
        ]

        def _benchmark_conv3d_pw_bn_relu_forward(**kwargs) -> Callable:
            assert kwargs["mode"] in ("original", "deployable"), (
                "kwargs['mode'] must be either 'original' or 'deployable',"
                "but got {}.".format(kwargs["mode"])
            )
            input_tensor = torch.randn((kwargs["input_blob_size"]))
            conv_block = Conv3dPwBnAct(
                kwargs["in_channels"],
                kwargs["out_channels"],
                use_bn=False,  # assume BN has already been fused for forward
            )

            if kwargs["mode"] == "deployable":
                conv_block.convert(kwargs["input_blob_size"])
            conv_block.eval()
            if kwargs["quantize"] is True:
                if kwargs["mode"] == "original":  # manually fuse conv and relu
                    conv_block.kernel = torch.quantization.fuse_modules(
                        conv_block.kernel, ["conv", "act.act"]
                    )
                conv_block = nn.Sequential(
                    torch.quantization.QuantStub(),
                    conv_block,
                    torch.quantization.DeQuantStub(),
                )

                conv_block.qconfig = torch.quantization.get_default_qconfig("qnnpack")
                conv_block = torch.quantization.prepare(conv_block)
                try:
                    conv_block = torch.quantization.convert(conv_block)
                except Exception as e:
                    logging.info(
                        "benchmark_conv3d_pw_bn_relu: "
                        "catch exception '{}' with kwargs of {}".format(e, kwargs)
                    )

                    def func_to_benchmark_dummy() -> None:
                        return

                    return func_to_benchmark_dummy
            traced_model = torch.jit.trace(conv_block, input_tensor, strict=False)
            if kwargs["quantize"] is False:
                traced_model = optimize_for_mobile(traced_model)

            logging.info(f"model arch: {traced_model}")

            def func_to_benchmark() -> None:
                try:
                    _ = traced_model(input_tensor)
                except Exception as e:
                    logging.info(
                        "benchmark_conv3d_pw_bn_relu: "
                        "catch exception '{}' with kwargs of {}".format(e, kwargs)
                    )

                return

            return func_to_benchmark

        benchmark(
            _benchmark_conv3d_pw_bn_relu_forward,
            "benchmark_conv3d_pw_bn_relu",
            kwargs_list,
            num_iters=num_iters,
            warmup_iters=2,
        )

        self.assertTrue(True)

</source>
<source file="systems/pytorchvideo-0.1.3/tests/benchmark_accelerator_efficient_blocks.py" startline="141" endline="245" pcid="336">
    def test_benchmark_conv3d_3x3x3_dw_bn_relu(self, num_iters: int = 20) -> None:
        """
        Benchmark Conv3d3x3x3DwBnAct with ReLU activation.
        Note efficient block Conv3d3x3x3DwBnAct is designed for mobile cpu with qnnpack
        backend, and benchmarking on server with another backend (e.g., fbgemm) may have
        different latency result compared as running on mobile cpu.
        Args:
            num_iters (int): number of iterations to perform benchmarking.
        """
        torch.backends.quantized.engine = "qnnpack"
        kwargs_list = [
            {
                "mode": "original",
                "input_blob_size": (1, 48, 4, 40, 40),
                "in_channels": 48,
                "quantize": False,
            },
            {
                "mode": "deployable",
                "input_blob_size": (1, 48, 4, 40, 40),
                "in_channels": 48,
                "quantize": False,
            },
            {
                "mode": "original",
                "input_blob_size": (1, 48, 4, 40, 40),
                "in_channels": 48,
                "quantize": True,
            },
            {
                "mode": "deployable",
                "input_blob_size": (1, 48, 4, 40, 40),
                "in_channels": 48,
                "quantize": True,
            },
        ]

        def _benchmark_conv3d_3x3x3_dw_bn_relu_forward(**kwargs) -> Callable:
            assert kwargs["mode"] in ("original", "deployable"), (
                "kwargs['mode'] must be either 'original' or 'deployable',"
                "but got {}.".format(kwargs["mode"])
            )
            input_tensor = torch.randn((kwargs["input_blob_size"]))
            conv_block = Conv3d3x3x3DwBnAct(
                kwargs["in_channels"],
                use_bn=False,  # assume BN has already been fused for forward
            )

            if kwargs["mode"] == "deployable":
                conv_block.convert(kwargs["input_blob_size"])
            conv_block.eval()
            if kwargs["quantize"] is True:
                if kwargs["mode"] == "original":  # manually fuse conv and relu
                    conv_block.kernel = torch.quantization.fuse_modules(
                        conv_block.kernel, ["conv", "act.act"]
                    )
                conv_block = nn.Sequential(
                    torch.quantization.QuantStub(),
                    conv_block,
                    torch.quantization.DeQuantStub(),
                )

                conv_block.qconfig = torch.quantization.get_default_qconfig("qnnpack")
                conv_block = torch.quantization.prepare(conv_block)
                try:
                    conv_block = torch.quantization.convert(conv_block)
                except Exception as e:
                    logging.info(
                        "benchmark_conv3d_3x3x3_dw_bn_relu: "
                        "catch exception '{}' with kwargs of {}".format(e, kwargs)
                    )

                    def func_to_benchmark_dummy() -> None:
                        return

                    return func_to_benchmark_dummy

            traced_model = torch.jit.trace(conv_block, input_tensor, strict=False)
            if kwargs["quantize"] is False:
                traced_model = optimize_for_mobile(traced_model)

            logging.info(f"model arch: {traced_model}")

            def func_to_benchmark() -> None:
                try:
                    _ = traced_model(input_tensor)
                except Exception as e:
                    logging.info(
                        "benchmark_conv3d_3x3x3_dw_bn_relu: "
                        "catch exception '{}' with kwargs of {}".format(e, kwargs)
                    )
                return

            return func_to_benchmark

        benchmark(
            _benchmark_conv3d_3x3x3_dw_bn_relu_forward,
            "benchmark_conv3d_3x3x3_dw_bn_relu",
            kwargs_list,
            num_iters=num_iters,
            warmup_iters=2,
        )

        self.assertTrue(True)

</source>
<source file="systems/pytorchvideo-0.1.3/tests/benchmark_accelerator_efficient_blocks.py" startline="246" endline="355" pcid="340">
    def test_benchmark_x3d_bottleneck_block(self, num_iters: int = 20) -> None:
        """
        Benchmark X3dBottleneckBlock.
        Note efficient block X3dBottleneckBlock is designed for mobile cpu with qnnpack
        backend, and benchmarking on server/laptop may have different latency result
        compared to running on mobile cpu.
        Args:
            num_iters (int): number of iterations to perform benchmarking.
        """
        torch.backends.quantized.engine = "qnnpack"
        kwargs_list = [
            {
                "mode": "original",
                "input_blob_size": (1, 48, 4, 20, 20),
                "in_channels": 48,
                "mid_channels": 108,
                "out_channels": 48,
                "quantize": False,
            },
            {
                "mode": "deployable",
                "input_blob_size": (1, 48, 4, 20, 20),
                "in_channels": 48,
                "mid_channels": 108,
                "out_channels": 48,
                "quantize": False,
            },
            {
                "mode": "original",
                "input_blob_size": (1, 48, 4, 20, 20),
                "in_channels": 48,
                "mid_channels": 108,
                "out_channels": 48,
                "quantize": True,
            },
            {
                "mode": "deployable",
                "input_blob_size": (1, 48, 4, 20, 20),
                "in_channels": 48,
                "mid_channels": 108,
                "out_channels": 48,
                "quantize": True,
            },
        ]

        def _benchmark_x3d_bottleneck_forward(**kwargs) -> Callable:
            assert kwargs["mode"] in ("original", "deployable"), (
                "kwargs['mode'] must be either 'original' or 'deployable',"
                "but got {}.".format(kwargs["mode"])
            )
            input_tensor = torch.randn((kwargs["input_blob_size"]))
            conv_block = X3dBottleneckBlock(
                kwargs["in_channels"],
                kwargs["mid_channels"],
                kwargs["out_channels"],
                use_bn=(False, False, False),  # Assume BN has been fused for forward
            )

            if kwargs["mode"] == "deployable":
                conv_block.convert(kwargs["input_blob_size"])
            conv_block.eval()
            if kwargs["quantize"] is True:
                conv_block = nn.Sequential(
                    torch.quantization.QuantStub(),
                    conv_block,
                    torch.quantization.DeQuantStub(),
                )

                conv_block.qconfig = torch.quantization.get_default_qconfig("qnnpack")
                conv_block = torch.quantization.prepare(conv_block)
                try:
                    conv_block = torch.quantization.convert(conv_block)
                except Exception as e:
                    logging.info(
                        "benchmark_x3d_bottleneck_forward: "
                        "catch exception '{}' with kwargs of {}".format(e, kwargs)
                    )

                    def func_to_benchmark_dummy() -> None:
                        return

                    return func_to_benchmark_dummy

            traced_model = torch.jit.trace(conv_block, input_tensor, strict=False)
            if kwargs["quantize"] is False:
                traced_model = optimize_for_mobile(traced_model)

            logging.info(f"model arch: {traced_model}")

            def func_to_benchmark() -> None:
                try:
                    _ = traced_model(input_tensor)
                except Exception as e:
                    logging.info(
                        "benchmark_x3d_bottleneck_forward: "
                        "catch exception '{}' with kwargs of {}".format(e, kwargs)
                    )
                return

            return func_to_benchmark

        benchmark(
            _benchmark_x3d_bottleneck_forward,
            "benchmark_x3d_bottleneck_forward",
            kwargs_list,
            num_iters=num_iters,
            warmup_iters=2,
        )

        self.assertTrue(True)
</source>
</class>

<class classid="20" nclones="3" nlines="24" similarity="87">
<source file="systems/pytorchvideo-0.1.3/tests/test_models_head.py" startline="25" endline="56" pcid="362">
    def test_build_simple_head(self):
        """
        Test simple ResNetBasicHead (without dropout and activation layers).
        """
        for input_dim, output_dim in itertools.product((4, 8), (4, 8, 16)):
            model = ResNetBasicHead(
                proj=nn.Linear(input_dim, output_dim),
                pool=nn.AdaptiveAvgPool3d(1),
                output_pool=nn.AdaptiveAvgPool3d(1),
            )

            # Test forwarding.
            for input_tensor in TestHeadHelper._get_inputs(input_dim=input_dim):
                if input_tensor.shape[1] != input_dim:
                    with self.assertRaises(RuntimeError):
                        output_tensor = model(input_tensor)
                    continue
                else:
                    output_tensor = model(input_tensor)

                input_shape = input_tensor.shape
                output_shape = output_tensor.shape
                output_shape_gt = (input_shape[0], output_dim)

                self.assertEqual(
                    output_shape,
                    output_shape_gt,
                    "Output shape {} is different from expected shape {}".format(
                        output_shape, output_shape_gt
                    ),
                )

</source>
<source file="systems/pytorchvideo-0.1.3/tests/test_models_head.py" startline="187" endline="220" pcid="367">
    def test_build_simple_head(self):
        """
        Test simple ResNetRoIHead
        (without pool_spatial, roi, dropout and activation layers).
        """
        for input_dim, output_dim in itertools.product((4, 8), (4, 8, 16)):
            model = ResNetRoIHead(
                proj=nn.Linear(input_dim, output_dim),
                pool=nn.AdaptiveAvgPool3d(1),
                output_pool=nn.AdaptiveAvgPool3d(1),
            )
            bboxes = None

            # Test forwarding.
            for input_tensor in TestHeadHelper._get_inputs(input_dim=input_dim):
                if input_tensor.shape[1] != input_dim:
                    with self.assertRaises(RuntimeError):
                        output_tensor = model(input_tensor, bboxes)
                    continue
                else:
                    output_tensor = model(input_tensor, bboxes)

                input_shape = input_tensor.shape
                output_shape = output_tensor.shape
                output_shape_gt = (input_shape[0], output_dim)

                self.assertEqual(
                    output_shape,
                    output_shape_gt,
                    "Output shape {} is different from expected shape {}".format(
                        output_shape, output_shape_gt
                    ),
                )

</source>
<source file="systems/pytorchvideo-0.1.3/tests/test_models_head.py" startline="57" endline="90" pcid="363">
    def test_build_complex_head(self):
        """
        Test complex ResNetBasicHead.
        """
        for input_dim, output_dim in itertools.product((4, 8), (4, 8, 16)):
            model = ResNetBasicHead(
                proj=nn.Linear(input_dim, output_dim),
                activation=nn.Softmax(),
                pool=nn.AdaptiveAvgPool3d(1),
                dropout=nn.Dropout(0.5),
                output_pool=nn.AdaptiveAvgPool3d(1),
            )

            # Test forwarding.
            for input_tensor in TestHeadHelper._get_inputs(input_dim=input_dim):
                if input_tensor.shape[1] != input_dim:
                    with self.assertRaises(Exception):
                        output_tensor = model(input_tensor)
                    continue

                output_tensor = model(input_tensor)

                input_shape = input_tensor.shape
                output_shape = output_tensor.shape
                output_shape_gt = (input_shape[0], output_dim)

                self.assertEqual(
                    output_shape,
                    output_shape_gt,
                    "Output shape {} is different from expected shape {}".format(
                        output_shape, output_shape_gt
                    ),
                )

</source>
</class>

<class classid="21" nclones="2" nlines="50" similarity="82">
<source file="systems/pytorchvideo-0.1.3/tests/test_models_r2plus1d.py" startline="21" endline="85" pcid="374">
    def test_create_r2plus1d(self):
        """
        Test simple r2plus1d with different inputs.
        """
        for input_channel, input_clip_length, input_crop_size in itertools.product(
            (3, 2), (4, 8), (56, 64)
        ):
            stage_spatial_stride = (2, 2, 2, 2)
            stage_temporal_stride = (1, 1, 2, 2)

            total_spatial_stride = 2 * np.prod(stage_spatial_stride)
            total_temporal_stride = np.prod(stage_temporal_stride)
            head_pool_kernel_size = (
                input_clip_length // total_temporal_stride,
                input_crop_size // total_spatial_stride,
                input_crop_size // total_spatial_stride,
            )

            model = create_r2plus1d(
                input_channel=input_channel,
                model_depth=50,
                model_num_class=400,
                dropout_rate=0.0,
                norm=nn.BatchNorm3d,
                activation=nn.ReLU,
                stem_dim_out=8,
                stem_conv_kernel_size=(1, 7, 7),
                stem_conv_stride=(1, 2, 2),
                stage_conv_b_kernel_size=((3, 3, 3),) * 4,
                stage_spatial_stride=stage_spatial_stride,
                stage_temporal_stride=stage_temporal_stride,
                stage_bottleneck=(
                    create_bottleneck_block,
                    create_2plus1d_bottleneck_block,
                    create_2plus1d_bottleneck_block,
                    create_2plus1d_bottleneck_block,
                ),
                head_pool=nn.AvgPool3d,
                head_pool_kernel_size=head_pool_kernel_size,
                head_output_size=(1, 1, 1),
                head_activation=nn.Softmax,
            )

            # Test forwarding.
            for tensor in TestR2plus1d._get_inputs(
                input_channel, input_clip_length, input_crop_size
            ):
                if tensor.shape[1] != input_channel:
                    with self.assertRaises(RuntimeError):
                        out = model(tensor)
                    continue

                out = model(tensor)

                output_shape = out.shape
                output_shape_gt = (tensor.shape[0], 400)

                self.assertEqual(
                    output_shape,
                    output_shape_gt,
                    "Output shape {} is different from expected shape {}".format(
                        output_shape, output_shape_gt
                    ),
                )

</source>
<source file="systems/pytorchvideo-0.1.3/tests/test_models_csn.py" startline="19" endline="80" pcid="417">
    def test_create_csn(self):
        """
        Test simple CSN with different inputs.
        """
        for input_channel, input_clip_length, input_crop_size in itertools.product(
            (3, 2), (4, 8), (56, 64)
        ):
            stage_spatial_stride = (1, 2, 2, 2)
            stage_temporal_stride = (1, 2, 2, 1)

            total_spatial_stride = 2 * np.prod(stage_spatial_stride)
            total_temporal_stride = np.prod(stage_temporal_stride)
            head_pool_kernel_size = (
                input_clip_length // total_temporal_stride,
                input_crop_size // total_spatial_stride,
                input_crop_size // total_spatial_stride,
            )

            model = create_csn(
                input_channel=input_channel,
                model_depth=50,
                model_num_class=400,
                dropout_rate=0,
                norm=nn.BatchNorm3d,
                activation=nn.ReLU,
                stem_dim_out=8,
                stem_conv_kernel_size=(3, 7, 7),
                stem_conv_stride=(1, 2, 2),
                stage_conv_a_kernel_size=(1, 1, 1),
                stage_conv_b_kernel_size=(3, 3, 3),
                stage_conv_b_width_per_group=1,
                stage_spatial_stride=(1, 2, 2, 2),
                stage_temporal_stride=(1, 2, 2, 1),
                bottleneck=create_bottleneck_block,
                head_pool=nn.AvgPool3d,
                head_pool_kernel_size=head_pool_kernel_size,
                head_output_size=(1, 1, 1),
                head_activation=nn.Softmax,
            )

            # Test forwarding.
            for tensor in TestCSN._get_inputs(
                input_channel, input_clip_length, input_crop_size
            ):
                if tensor.shape[1] != input_channel:
                    with self.assertRaises(RuntimeError):
                        out = model(tensor)
                    continue

                out = model(tensor)

                output_shape = out.shape
                output_shape_gt = (tensor.shape[0], 400)

                self.assertEqual(
                    output_shape,
                    output_shape_gt,
                    "Output shape {} is different from expected shape {}".format(
                        output_shape, output_shape_gt
                    ),
                )

</source>
</class>

<class classid="22" nclones="2" nlines="12" similarity="75">
<source file="systems/pytorchvideo-0.1.3/tests/test_transforms.py" startline="289" endline="304" pcid="391">
    def test_uniform_crop_transform(self):
        video = thwc_to_cthw(create_dummy_video_frames(10, 30, 40)).to(
            dtype=torch.float32
        )
        test_clip = {"video": video, "aug_index": 1, "label": 0}

        transform = UniformCropVideo(20)

        actual = transform(test_clip)
        c, t, h, w = actual["video"].shape
        self.assertEqual(c, 3)
        self.assertEqual(t, 10)
        self.assertEqual(h, 20)
        self.assertEqual(w, 20)
        self.assertTrue(actual["video"].equal(video[:, :, 5:25, 10:30]))

</source>
<source file="systems/pytorchvideo-0.1.3/tests/test_transforms.py" startline="335" endline="348" pcid="395">
    def test_center_crop(self):
        video = thwc_to_cthw(create_dummy_video_frames(10, 30, 40)).to(
            dtype=torch.float32
        )
        transform = CenterCropVideo(10)

        actual = transform(video)
        c, t, h, w = actual.shape
        self.assertEqual(c, 3)
        self.assertEqual(t, 10)
        self.assertEqual(h, 10)
        self.assertEqual(w, 10)
        self.assertTrue(actual.equal(video[:, :, 10:20, 15:25]))

</source>
</class>

<class classid="23" nclones="3" nlines="69" similarity="70">
<source file="systems/pytorchvideo-0.1.3/tests/test_transforms.py" startline="439" endline="523" pcid="398">
    def test_mixup(self):
        # Test images.
        batch_size = 2
        h_size = 10
        w_size = 10
        c_size = 3
        input_images = torch.rand(batch_size, c_size, h_size, w_size)
        input_images[0, :].fill_(0)
        input_images[1, :].fill_(1)
        alpha = 1.0
        label_smoothing = 0.0
        num_classes = 5
        transform_mixup = MixUp(
            alpha=alpha,
            label_smoothing=label_smoothing,
            num_classes=num_classes,
        )
        labels = torch.arange(0, batch_size) % num_classes
        mixed_images, mixed_labels = transform_mixup(input_images, labels)
        gt_image_sum = h_size * w_size * c_size
        label_sum = batch_size

        self.assertTrue(
            np.allclose(mixed_images.sum().item(), gt_image_sum, rtol=0.001)
        )
        self.assertTrue(np.allclose(mixed_labels.sum().item(), label_sum, rtol=0.001))
        self.assertEqual(mixed_labels.size(0), batch_size)
        self.assertEqual(mixed_labels.size(1), num_classes)

        # Test videos.
        batch_size = 2
        h_size = 10
        w_size = 10
        c_size = 3
        t_size = 2
        input_video = torch.rand(batch_size, c_size, t_size, h_size, w_size)
        input_video[0, :].fill_(0)
        input_video[1, :].fill_(1)
        alpha = 1.0
        label_smoothing = 0.0
        num_classes = 5
        transform_mixup = MixUp(
            alpha=alpha,
            label_smoothing=label_smoothing,
            num_classes=num_classes,
        )
        labels = torch.arange(0, batch_size) % num_classes
        mixed_videos, mixed_labels = transform_mixup(input_video, labels)
        gt_video_sum = h_size * w_size * c_size * t_size
        label_sum = batch_size

        self.assertTrue(
            np.allclose(mixed_videos.sum().item(), gt_video_sum, rtol=0.001)
        )
        self.assertTrue(np.allclose(mixed_labels.sum().item(), label_sum, rtol=0.001))
        self.assertEqual(mixed_labels.size(0), batch_size)
        self.assertEqual(mixed_labels.size(1), num_classes)

        # Test videos with label smoothing.
        input_video = torch.rand(batch_size, c_size, t_size, h_size, w_size)
        input_video[0, :].fill_(0)
        input_video[1, :].fill_(1)
        alpha = 1.0
        label_smoothing = 0.2
        num_classes = 5
        transform_mixup = MixUp(
            alpha=alpha,
            label_smoothing=label_smoothing,
            num_classes=num_classes,
        )
        labels = torch.arange(0, batch_size) % num_classes
        mixed_videos, mixed_labels = transform_mixup(input_video, labels)
        gt_video_sum = h_size * w_size * c_size * t_size
        label_sum = batch_size
        self.assertTrue(
            np.allclose(mixed_videos.sum().item(), gt_video_sum, rtol=0.001)
        )
        self.assertTrue(np.allclose(mixed_labels.sum().item(), label_sum, rtol=0.001))
        self.assertEqual(mixed_labels.size(0), batch_size)
        self.assertEqual(mixed_labels.size(1), num_classes)

        # Check the smoothing value is in label.
        smooth_value = label_smoothing / num_classes
        self.assertTrue(smooth_value in torch.unique(mixed_labels))

</source>
<source file="systems/pytorchvideo-0.1.3/tests/test_transforms.py" startline="628" endline="693" pcid="400">
    def test_mixvideo(self):

        self.assertRaises(AssertionError, MixVideo, cutmix_prob=2.0)

        torch.manual_seed(0)
        # Test images.
        batch_size = 2
        h_size = 10
        w_size = 10
        c_size = 3
        input_images = torch.rand(batch_size, c_size, h_size, w_size)
        input_images[0, :].fill_(0)
        input_images[1, :].fill_(1)
        mixup_alpha = 1.0
        cutmix_alpha = 1.0
        label_smoothing = 0.0
        num_classes = 5
        transform_mix = MixVideo(
            mixup_alpha=mixup_alpha,
            cutmix_alpha=cutmix_alpha,
            label_smoothing=label_smoothing,
            num_classes=num_classes,
        )
        labels = torch.arange(0, batch_size) % num_classes
        mixed_images, mixed_labels = transform_mix(input_images, labels)
        gt_image_sum = h_size * w_size * c_size
        label_sum = batch_size

        self.assertTrue(
            np.allclose(mixed_images.sum().item(), gt_image_sum, rtol=0.001)
        )
        self.assertTrue(np.allclose(mixed_labels.sum().item(), label_sum, rtol=0.001))
        self.assertEqual(mixed_labels.size(0), batch_size)
        self.assertEqual(mixed_labels.size(1), num_classes)

        # Test videos.
        batch_size = 2
        h_size = 10
        w_size = 10
        c_size = 3
        t_size = 2
        input_video = torch.rand(batch_size, c_size, t_size, h_size, w_size)
        input_video[0, :].fill_(0)
        input_video[1, :].fill_(1)
        mixup_alpha = 1.0
        cutmix_alpha = 1.0
        label_smoothing = 0.0
        num_classes = 5
        transform_mix = MixVideo(
            mixup_alpha=mixup_alpha,
            cutmix_alpha=cutmix_alpha,
            label_smoothing=label_smoothing,
            num_classes=num_classes,
        )
        labels = torch.arange(0, batch_size) % num_classes
        mixed_videos, mixed_labels = transform_mix(input_video, labels)
        gt_video_sum = h_size * w_size * c_size * t_size
        label_sum = batch_size

        self.assertTrue(
            np.allclose(mixed_videos.sum().item(), gt_video_sum, rtol=0.001)
        )
        self.assertTrue(np.allclose(mixed_labels.sum().item(), label_sum, rtol=0.001))
        self.assertEqual(mixed_labels.size(0), batch_size)
        self.assertEqual(mixed_labels.size(1), num_classes)

</source>
<source file="systems/pytorchvideo-0.1.3/tests/test_transforms.py" startline="524" endline="627" pcid="399">
    def test_cutmix(self):
        torch.manual_seed(0)
        # Test images.
        batch_size = 2
        h_size = 10
        w_size = 10
        c_size = 3
        input_images = torch.rand(batch_size, c_size, h_size, w_size)
        input_images[0, :].fill_(0)
        input_images[1, :].fill_(1)
        alpha = 1.0
        label_smoothing = 0.0
        num_classes = 5
        transform_cutmix = CutMix(
            alpha=alpha,
            label_smoothing=label_smoothing,
            num_classes=num_classes,
        )
        labels = torch.arange(0, batch_size) % num_classes
        mixed_images, mixed_labels = transform_cutmix(input_images, labels)
        gt_image_sum = h_size * w_size * c_size
        label_sum = batch_size

        self.assertTrue(
            np.allclose(mixed_images.sum().item(), gt_image_sum, rtol=0.001)
        )
        self.assertTrue(np.allclose(mixed_labels.sum().item(), label_sum, rtol=0.001))
        self.assertEqual(mixed_labels.size(0), batch_size)
        self.assertEqual(mixed_labels.size(1), num_classes)

        # Test videos.
        batch_size = 2
        h_size = 10
        w_size = 10
        c_size = 3
        t_size = 2
        input_video = torch.rand(batch_size, c_size, t_size, h_size, w_size)
        input_video[0, :].fill_(0)
        input_video[1, :].fill_(1)
        alpha = 1.0
        label_smoothing = 0.0
        num_classes = 5
        transform_cutmix = CutMix(
            alpha=alpha,
            label_smoothing=label_smoothing,
            num_classes=num_classes,
        )
        labels = torch.arange(0, batch_size) % num_classes
        mixed_videos, mixed_labels = transform_cutmix(input_video, labels)
        gt_video_sum = h_size * w_size * c_size * t_size
        label_sum = batch_size

        self.assertTrue(
            np.allclose(mixed_videos.sum().item(), gt_video_sum, rtol=0.001)
        )
        self.assertTrue(np.allclose(mixed_labels.sum().item(), label_sum, rtol=0.001))
        self.assertEqual(mixed_labels.size(0), batch_size)
        self.assertEqual(mixed_labels.size(1), num_classes)

        # Test videos with label smoothing.
        input_video = torch.rand(batch_size, c_size, t_size, h_size, w_size)
        input_video[0, :].fill_(0)
        input_video[1, :].fill_(1)
        alpha = 1.0
        label_smoothing = 0.2
        num_classes = 5
        transform_cutmix = CutMix(
            alpha=alpha,
            label_smoothing=label_smoothing,
            num_classes=num_classes,
        )
        labels = torch.arange(0, batch_size) % num_classes
        mixed_videos, mixed_labels = transform_cutmix(input_video, labels)
        gt_video_sum = h_size * w_size * c_size * t_size
        label_sum = batch_size
        self.assertTrue(
            np.allclose(mixed_videos.sum().item(), gt_video_sum, rtol=0.001)
        )
        self.assertTrue(np.allclose(mixed_labels.sum().item(), label_sum, rtol=0.001))
        self.assertEqual(mixed_labels.size(0), batch_size)
        self.assertEqual(mixed_labels.size(1), num_classes)

        # Check the smoothing value is in label.
        smooth_value = label_smoothing / num_classes
        self.assertTrue(smooth_value in torch.unique(mixed_labels))

        # Check cutmixed video has both 0 and 1.
        # Run 20 times to avoid rare cases where the random box is empty.
        test_times = 20
        seen_all_value1 = False
        seen_all_value2 = False
        for _ in range(test_times):
            mixed_videos, mixed_labels = transform_cutmix(input_video, labels)
            if 0 in mixed_videos[0, :] and 1 in mixed_videos[0, :]:
                seen_all_value1 = True

            if 0 in mixed_videos[1, :] and 1 in mixed_videos[1, :]:
                seen_all_value2 = True

            if seen_all_value1 and seen_all_value2:
                break
        self.assertTrue(seen_all_value1)
        self.assertTrue(seen_all_value2)

</source>
</class>

<class classid="24" nclones="2" nlines="39" similarity="100">
<source file="systems/pytorchvideo-0.1.3/tests/test_data_epic_kitchen_utils.py" startline="40" endline="80" pcid="409">
def get_flat_video_frames(directory, file_extension):
    return {
        "P02_001": VideoFrameInfo(
            video_id="P02_001",
            location=f"{directory}/P02_001",
            frame_file_stem="frame_",
            frame_string_length=16,
            min_frame_number=1,
            max_frame_number=3000,
            file_extension=file_extension,
        ),
        "P02_002": VideoFrameInfo(
            video_id="P02_002",
            location=f"{directory}/P02_002",
            frame_file_stem="frame_",
            frame_string_length=16,
            min_frame_number=2,
            max_frame_number=3001,
            file_extension=file_extension,
        ),
        "P02_005": VideoFrameInfo(
            video_id="P02_005",
            location=f"{directory}/P02_005",
            frame_file_stem="frame_",
            frame_string_length=16,
            min_frame_number=1,
            max_frame_number=30003,
            file_extension=file_extension,
        ),
        "P07_002": VideoFrameInfo(
            video_id="P07_002",
            location=f"{directory}/P07_002",
            frame_file_stem="frame_",
            frame_string_length=16,
            min_frame_number=2,
            max_frame_number=1530,
            file_extension=file_extension,
        ),
    }


</source>
<source file="systems/pytorchvideo-0.1.3/tests/test_data_epic_kitchen_utils.py" startline="81" endline="121" pcid="410">
def get_nested_video_frames(directory, file_extension):
    return {
        "P02_001": VideoFrameInfo(
            video_id="P02_001",
            location=f"{directory}/P02",
            frame_file_stem="P02_001_",
            frame_string_length=16,
            min_frame_number=1,
            max_frame_number=3000,
            file_extension=file_extension,
        ),
        "P02_002": VideoFrameInfo(
            video_id="P02_002",
            location=f"{directory}/P02",
            frame_file_stem="P02_002_",
            frame_string_length=16,
            min_frame_number=2,
            max_frame_number=3001,
            file_extension=file_extension,
        ),
        "P02_005": VideoFrameInfo(
            video_id="P02_005",
            location=f"{directory}/P02",
            frame_file_stem="P02_005_",
            frame_string_length=16,
            min_frame_number=1,
            max_frame_number=30003,
            file_extension=file_extension,
        ),
        "P07_002": VideoFrameInfo(
            video_id="P07_002",
            location=f"{directory}/P07",
            frame_file_stem="P07_002_",
            frame_string_length=16,
            min_frame_number=2,
            max_frame_number=1530,
            file_extension=file_extension,
        ),
    }


</source>
</class>

<class classid="25" nclones="2" nlines="10" similarity="100">
<source file="systems/pytorchvideo-0.1.3/tests/test_data_epic_kitchen_utils.py" startline="126" endline="140" pcid="412">
    def test_build_frame_manifest_from_flat_directory(self, multithreading=True):
        with tempfile.TemporaryDirectory(prefix="TestEpicKitchenUtils") as tempdir:
            video_frames_expected = get_flat_video_frames(tempdir, "jpg")
            write_mock_frame_files(video_frames_expected, tempdir, "jpg")

            video_frames = build_frame_manifest_from_flat_directory(
                tempdir, multithreading
            )

            self.assertEqual(len(video_frames_expected), len(video_frames))
            for video_id in video_frames_expected:
                self.assertEqual(
                    video_frames[video_id], video_frames_expected[video_id]
                )

</source>
<source file="systems/pytorchvideo-0.1.3/tests/test_data_epic_kitchen_utils.py" startline="144" endline="157" pcid="414">
    def test_build_frame_manifest_from_nested_directory(self, multithreading=True):
        with tempfile.TemporaryDirectory(prefix="TestEpicKitchenUtils") as tempdir:
            video_frames_expected = get_nested_video_frames(tempdir, "png")
            write_mock_frame_files(video_frames_expected, tempdir, "png")

            video_frames = build_frame_manifest_from_nested_directory(
                tempdir, multithreading
            )
            self.assertEqual(len(video_frames_expected), len(video_frames))
            for video_id in video_frames_expected:
                self.assertEqual(
                    video_frames[video_id], video_frames_expected[video_id]
                )

</source>
</class>

<class classid="26" nclones="2" nlines="19" similarity="73">
<source file="systems/pytorchvideo-0.1.3/tests/test_models_csn.py" startline="99" endline="120" pcid="419">
    def test_load_hubconf(self):
        path = os.path.join(
            os.path.dirname(os.path.realpath(__file__)),
            "..",
        )
        input_channel = 3
        input_clip_length = 4
        input_crop_size = 56
        model = torch.hub.load(
            repo_or_dir=path, source="local", model="csn_r101", pretrained=False
        )
        self.assertIsNotNone(model)

        # Test forwarding.
        for tensor in TestCSN._get_inputs(
            input_channel, input_clip_length, input_crop_size
        ):
            with torch.no_grad():
                if tensor.shape[1] != input_channel:
                    with self.assertRaises(RuntimeError):
                        model(tensor)
                    continue
</source>
<source file="systems/pytorchvideo-0.1.3/tests/test_models_slowfast.py" startline="19" endline="42" pcid="456">
    def test_load_hubconf(self):
        path = os.path.join(
            os.path.dirname(os.path.realpath(__file__)),
            "..",
        )
        for model_name in ["slowfast_r50", "slowfast_r101"]:
            model = torch.hub.load(
                repo_or_dir=path, source="local", model=model_name, pretrained=False
            )
            self.assertIsNotNone(model)

            input_clip_length = 32
            input_crop_size = 224
            input_channel = 3
            # Test forwarding.
            for tensor in TestSlowFast._get_inputs(
                input_channel, input_clip_length, input_crop_size
            ):
                with torch.no_grad():
                    if tensor[0].shape[1] != input_channel:
                        with self.assertRaises(RuntimeError):
                            model(tensor)
                        continue

</source>
</class>

<class classid="27" nclones="2" nlines="57" similarity="88">
<source file="systems/pytorchvideo-0.1.3/tests/test_data_ava_dataset.py" startline="61" endline="142" pcid="433">
    def test_multiple_videos(self):
        with tempfile.NamedTemporaryFile(delete=False, suffix=".csv") as data_file:
            with temp_ava_dataset_2_videos() as (
                frame_paths_file,
                video_1,
                video_2,
                video_1_name,
                video_2_name,
            ):
                # add bounding boxes
                # video 1
                bb_1_a, bb_1_a_string = get_random_bbox()
                action_1_a, iou_1_a = 1, 0.85
                bb_1_b, bb_1_b_string = get_random_bbox()
                action_1_b, iou_1_b = 2, 0.4

                data_file.write(
                    (
                        f"{video_1_name},902,{bb_1_a_string},"
                        + f"{str(action_1_a)},{str(iou_1_a)}\n"
                    ).encode()
                )
                data_file.write(
                    (
                        f"{video_1_name},902,{bb_1_b_string},"
                        + f"{str(action_1_b)},{str(iou_1_b)}\n"
                    ).encode()
                )
                # video 2
                bb_2_a, bb_2_a_string = get_random_bbox()
                action_2_a, iou_2_a = 3, 0.95
                bb_2_b, bb_2_b_string = get_random_bbox()
                action_2_b, iou_2_b = 4, 0.9

                data_file.write(
                    (
                        f"{video_2_name},902,{bb_2_a_string},"
                        + f"{str(action_2_a)},{str(iou_2_a)}\n"
                    ).encode()
                )
                data_file.write(
                    (
                        f"{video_2_name},902,{bb_2_b_string},"
                        + f"{str(action_2_b)},{str(iou_2_b)}\n"
                    ).encode()
                )

                data_file.close()

                dataset = Ava(
                    frame_paths_file=frame_paths_file,
                    frame_labels_file=data_file.name,
                    clip_sampler=make_clip_sampler("random", 1.0),
                )

                # All videos are of the form cthw and fps is 30
                # Clip is samples at time step = 2 secs in video
                sample_1 = next(dataset)
                self.assertTrue(sample_1["video"].equal(video_1[:, 45:75, :, :]))
                self.assertTrue(
                    torch.tensor(sample_1["boxes"]).equal(
                        torch.tensor([bb_1_a, bb_1_b])
                    )
                )
                self.assertTrue(
                    torch.tensor(sample_1["labels"]).equal(
                        torch.tensor([[action_1_a], [action_1_b]])
                    )
                )
                sample_2 = next(dataset)
                self.assertTrue(sample_2["video"].equal(video_2[:, 45:75, :, :]))
                self.assertTrue(
                    torch.tensor(sample_2["boxes"]).equal(
                        torch.tensor([bb_2_a, bb_2_b])
                    )
                )
                self.assertTrue(
                    torch.tensor(sample_2["labels"]).equal(
                        torch.tensor([[action_2_a], [action_2_b]])
                    )
                )

</source>
<source file="systems/pytorchvideo-0.1.3/tests/test_data_ava_dataset.py" startline="143" endline="239" pcid="434">
    def test_multiple_videos_with_label_map(self):
        with tempfile.NamedTemporaryFile(delete=False, suffix=".csv") as label_map_file:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".csv") as data_file:
                with temp_ava_dataset_2_videos() as (
                    frame_paths_file,
                    video_1,
                    video_2,
                    video_1_name,
                    video_2_name,
                ):
                    # Create labelmap file
                    label_map = """item {
  name: "bend/bow (at the waist)"
  id: 1
}
item {
  name: "crouch/kneel"
  id: 3
}
item {
  name: "dance"
  id: 4
}"""
                    label_map_file.write(label_map.encode())
                    label_map_file.close()

                    # add bounding boxes
                    # video 1
                    bb_1_a, bb_1_a_string = get_random_bbox()
                    action_1_a, iou_1_a = 1, 0.85
                    bb_1_b, bb_1_b_string = get_random_bbox()
                    action_1_b, iou_1_b = 2, 0.4

                    data_file.write(
                        (
                            f"{video_1_name},902,{bb_1_a_string},"
                            + f"{str(action_1_a)},{str(iou_1_a)}\n"
                        ).encode()
                    )
                    data_file.write(
                        (
                            f"{video_1_name},902,{bb_1_b_string},"
                            + f"{str(action_1_b)},{str(iou_1_b)}\n"
                        ).encode()
                    )
                    # video 2
                    bb_2_a, bb_2_a_string = get_random_bbox()
                    action_2_a, iou_2_a = 3, 0.95
                    bb_2_b, bb_2_b_string = get_random_bbox()
                    action_2_b, iou_2_b = 4, 0.9

                    data_file.write(
                        (
                            f"{video_2_name},902,{bb_2_a_string},"
                            + f"{str(action_2_a)},{str(iou_2_a)}\n"
                        ).encode()
                    )
                    data_file.write(
                        (
                            f"{video_2_name},902,{bb_2_b_string},"
                            + f"{str(action_2_b)},{str(iou_2_b)}\n"
                        ).encode()
                    )

                    data_file.close()

                    dataset = Ava(
                        frame_paths_file=frame_paths_file,
                        frame_labels_file=data_file.name,
                        clip_sampler=make_clip_sampler("random", 1.0),
                        label_map_file=label_map_file.name,
                    )

                    # All videos are of the form cthw and fps is 30
                    # Clip is samples at time step = 2 secs in video
                    sample_1 = next(dataset)
                    self.assertTrue(sample_1["video"].equal(video_1[:, 45:75, :, :]))
                    self.assertTrue(
                        torch.tensor(sample_1["boxes"]).equal(torch.tensor([bb_1_a]))
                    )
                    self.assertTrue(
                        torch.tensor(sample_1["labels"]).equal(
                            torch.tensor([[action_1_a]])
                        )
                    )
                    sample_2 = next(dataset)
                    self.assertTrue(sample_2["video"].equal(video_2[:, 45:75, :, :]))
                    self.assertTrue(
                        torch.tensor(sample_2["boxes"]).equal(
                            torch.tensor([bb_2_a, bb_2_b])
                        )
                    )
                    self.assertTrue(
                        torch.tensor(sample_2["labels"]).equal(
                            torch.tensor([[action_2_a], [action_2_b]])
                        )
                    )
</source>
</class>

<class classid="28" nclones="2" nlines="23" similarity="79">
<source file="systems/pytorchvideo-0.1.3/tests/test_accelerator_efficient_blocks_mobile_cpu_conv3d.py" startline="18" endline="49" pcid="447">
    def test_Conv3dPwBnAct_equivalency(self):
        # Input tensor
        input_tensor = torch.randn(1, 3, 4, 6, 6)
        # A conv block
        l0 = Conv3dPwBnAct(3, 12)
        l1 = Conv3dPwBnAct(
            12, 3, bias=True, activation="identity"
        )  # Skip relu to avoid NaN for rel error
        seq0 = nn.Sequential(l0, l1)
        seq0.eval()
        out0 = seq0(input_tensor)
        # Replicate the conv block
        l0_1 = deepcopy(l0)
        l1_1 = deepcopy(l1)
        # Convert into deployment mode
        l0_1.convert((1, 3, 4, 6, 6))  # Input tensor size is (1,3,4,6,6)
        l1_1.convert((1, 12, 4, 6, 6))  # Input tensor size is (1,12,4,6,6)
        seq1 = nn.Sequential(l0_1, l1_1)
        out1 = seq1(input_tensor)
        # Check arithmetic equivalency
        max_err = float(torch.max(torch.abs(out0 - out1)))
        rel_err = torch.abs((out0 - out1) / out0)
        max_rel_err = float(torch.max(rel_err))

        logging.info(
            (
                "test_Conv3dPwBnAct_equivalency: "
                f"max_err {max_err}, max_rel_err {max_rel_err}"
            )
        )
        self.assertTrue(max_err < 1e-3)

</source>
<source file="systems/pytorchvideo-0.1.3/tests/test_accelerator_efficient_blocks_mobile_cpu_conv3d.py" startline="50" endline="83" pcid="448">
    def test_Conv3d3x3x3DwBnAct_equivalency(self):
        # Input tensor
        input_tensor = torch.randn(1, 3, 4, 6, 6)
        # A conv block
        l0 = Conv3dPwBnAct(3, 12)
        l1 = Conv3d3x3x3DwBnAct(12)
        l2 = Conv3dPwBnAct(
            12, 3, bias=True, activation="identity"
        )  # Skip relu to avoid NaN for relative error
        seq0 = nn.Sequential(l0, l1, l2)
        seq0.eval()
        out0 = seq0(input_tensor)
        # Replicate the conv block
        l0_1 = deepcopy(l0)
        l1_1 = deepcopy(l1)
        l2_1 = deepcopy(l2)
        # Convert into deployment mode
        l0_1.convert((1, 3, 4, 6, 6))  # Input tensor size is (1,3,4,6,6)
        l1_1.convert((1, 12, 4, 6, 6))  # Input tensor size is (1,12,4,6,6)
        l2_1.convert((1, 12, 4, 6, 6))  # Input tensor size is (1,12,4,6,6)
        seq1 = nn.Sequential(l0_1, l1_1, l2_1)
        out1 = seq1(input_tensor)
        # Check arithmetic equivalency
        max_err = float(torch.max(torch.abs(out0 - out1)))
        rel_err = torch.abs((out0 - out1) / out0)
        max_rel_err = float(torch.max(rel_err))
        logging.info(
            (
                "test_Conv3d3x3x3DwBnAct_equivalency: "
                f"max_err {max_err}, max_rel_err {max_rel_err}"
            )
        )
        self.assertTrue(max_err < 1e-3)

</source>
</class>

<class classid="29" nclones="2" nlines="19" similarity="100">
<source file="systems/pytorchvideo-0.1.3/tests/test_accelerator_efficient_blocks_mobile_cpu_conv3d.py" startline="84" endline="114" pcid="449">
    def test_Conv3d3x1x1BnAct_equivalency(self):
        for input_temporal in range(3):
            input_size = (1, 3, input_temporal + 1, 6, 6)
            # Input tensor
            input_tensor = torch.randn(input_size)
            # A conv block
            l0 = Conv3d3x1x1BnAct(3, 6)
            l0.eval()
            out0 = l0(input_tensor)
            # Replicate the conv block
            l0_1 = deepcopy(l0)
            # Convert into deployment mode
            l0_1.convert(input_size)  # Input tensor size is (1,3,4,6,6)
            out1 = l0_1(input_tensor)
            # Check output size
            assert (
                out0.size() == out1.size()
            ), f"Sizes of out0 {out0.size()} and out1 {out1.size()} are different."
            # Check arithmetic equivalency
            max_err = float(torch.max(torch.abs(out0 - out1)))
            rel_err = torch.abs((out0 - out1) / out0)
            max_rel_err = float(torch.max(rel_err))
            logging.info(
                (
                    "test_Conv3d3x1x1BnAct_equivalency: "
                    f"input tensor size: {input_size}"
                    f"max_err {max_err}, max_rel_err {max_rel_err}"
                )
            )
            self.assertTrue(max_err < 1e-3)

</source>
<source file="systems/pytorchvideo-0.1.3/tests/test_accelerator_efficient_blocks_mobile_cpu_conv3d.py" startline="115" endline="144" pcid="450">
    def test_Conv3d5x1x1BnAct_equivalency(self):
        for input_temporal in range(5):
            input_size = (1, 3, input_temporal + 1, 6, 6)
            # Input tensor
            input_tensor = torch.randn(input_size)
            # A conv block
            l0 = Conv3d5x1x1BnAct(3, 6)
            l0.eval()
            out0 = l0(input_tensor)
            # Replicate the conv block
            l0_1 = deepcopy(l0)
            # Convert into deployment mode
            l0_1.convert(input_size)  # Input tensor size is (1,3,4,6,6)
            out1 = l0_1(input_tensor)
            # Check output size
            assert (
                out0.size() == out1.size()
            ), f"Sizes of out0 {out0.size()} and out1 {out1.size()} are different."
            # Check arithmetic equivalency
            max_err = float(torch.max(torch.abs(out0 - out1)))
            rel_err = torch.abs((out0 - out1) / out0)
            max_rel_err = float(torch.max(rel_err))
            logging.info(
                (
                    "test_Conv3d5x1x1BnAct_equivalency: "
                    f"input tensor size: {input_size}"
                    f"max_err {max_err}, max_rel_err {max_rel_err}"
                )
            )
            self.assertTrue(max_err < 1e-3)
</source>
</class>

</clones>

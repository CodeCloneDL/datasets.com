<clones>
<systeminfo processor="nicad6" system="deepchem-2.6.1" granularity="functions" threshold="0%" minlines="10" maxlines="2500"/>
<cloneinfo npcs="1958" npairs="27"/>
<runinfo ncompares="24295" cputime="58654"/>
<classinfo nclasses="16"/>

<class classid="1" nclones="2" nlines="61" similarity="100">
<source file="systems/deepchem-2.6.1/contrib/torch/pytorch_graphconv.py" startline="217" endline="300" pcid="49">
    def evaluate(self, train_features,
                       test_features,
                       y_train,
                       y_test, 
                       transformer,
                       batch_size=32):
        
        self.net.eval()
        print("TRAIN:")
        
        o = []
        l = []

        train_inds = range(0, len(train_features))

        for b in range(0, len(train_features)/batch_size):
            batch_inds = [train_inds[idx] for idx in range(b*batch_size, (b+1)*batch_size)]
            
            train_x_batch = np.concatenate([np.expand_dims(train_features[idx]["x"], 0) for idx in batch_inds], axis=0)
            train_g_batch = np.concatenate([np.expand_dims(train_features[idx]["g"], 0) for idx in batch_inds], axis=0)

            xb = torch.from_numpy(train_x_batch.astype(np.float32)).cuda()
            gb = torch.from_numpy(train_g_batch.astype(np.float32)).cuda()
            
            self.input_x.resize_as_(xb).copy_(xb)
            self.input_g.resize_as_(gb).copy_(gb)
            
            input_xv = Variable(self.input_x)
            input_gv = Variable(self.input_g)

            output = self.net(input_gv, input_xv)
            
            if transformer is not None:
                o.append(transformer.inverse_transform(output.data.cpu().numpy().reshape((-1,1))).flatten())
                l.append(transformer.inverse_transform(y_train[batch_inds].reshape((-1,1))).flatten())
            else:
                o.append(output.data.cpu().numpy().reshape((-1,1)).flatten())
                l.append(y_train[batch_inds].reshape((-1,1)).flatten())

        o = np.concatenate(o)
        l = np.concatenate(l)
        print("RMSE:")
        print(np.sqrt(np.mean(np.square(l-o))))
        print("ROC AUC:")
        print(roc_auc_score(l, o))
        
        o = []
        l = []

        print("TEST:")
        test_inds = range(0, len(test_features))

        for b in range(0, len(test_features)/batch_size):
            batch_inds = [test_inds[idx] for idx in range(b*batch_size, (b+1)*batch_size)]
            
            test_x_batch = np.concatenate([np.expand_dims(test_features[idx]["x"], 0) for idx in batch_inds], axis=0)
            test_g_batch = np.concatenate([np.expand_dims(test_features[idx]["g"], 0) for idx in batch_inds], axis=0)

            xb = torch.from_numpy(test_x_batch.astype(np.float32)).cuda()
            gb = torch.from_numpy(test_g_batch.astype(np.float32)).cuda()
            
            self.input_x.resize_as_(xb).copy_(xb)
            self.input_g.resize_as_(gb).copy_(gb)
            
            input_xv = Variable(self.input_x)
            input_gv = Variable(self.input_g)

            output = self.net(input_gv, input_xv)
            
            if transformer is not None:
                o.append(transformer.inverse_transform(output.data.cpu().numpy().reshape((-1,1))).flatten())
                l.append(transformer.inverse_transform(y_test[batch_inds].reshape((-1,1))).flatten())
            else:
                o.append(output.data.cpu().numpy().reshape((-1,1)).flatten())
                l.append(y_test[batch_inds].reshape((-1,1)).flatten())

        o = np.concatenate(o)
        l = np.concatenate(l)
        print("RMSE:")
        print(np.sqrt(np.mean(np.square(l-o))))
        print("ROC AUC:")
        print(roc_auc_score(l, o))


</source>
<source file="systems/deepchem-2.6.1/contrib/torch/pytorch_graphconv.py" startline="393" endline="474" pcid="53">
    def evaluate(self, train_features,
                       test_features,
                       y_train,
                       y_test, 
                       transformer,
                       batch_size=32):
        
        self.net.eval()
        print("TRAIN:")
        
        o = []
        l = []

        train_inds = range(0, len(train_features))

        for b in range(0, len(train_features)/batch_size):
            batch_inds = [train_inds[idx] for idx in range(b*batch_size, (b+1)*batch_size)]
            
            train_x_batch = np.concatenate([np.expand_dims(train_features[idx]["x"], 0) for idx in batch_inds], axis=0)
            train_g_batch = np.concatenate([np.expand_dims(train_features[idx]["g"], 0) for idx in batch_inds], axis=0)

            xb = torch.from_numpy(train_x_batch.astype(np.float32)).cuda()
            gb = torch.from_numpy(train_g_batch.astype(np.float32)).cuda()
            
            self.input_x.resize_as_(xb).copy_(xb)
            self.input_g.resize_as_(gb).copy_(gb)
            
            input_xv = Variable(self.input_x)
            input_gv = Variable(self.input_g)

            output = self.net(input_gv, input_xv)
            
            if transformer is not None:
                o.append(transformer.inverse_transform(output.data.cpu().numpy().reshape((-1,1))).flatten())
                l.append(transformer.inverse_transform(y_train[batch_inds].reshape((-1,1))).flatten())
            else:
                o.append(output.data.cpu().numpy().reshape((-1,1)).flatten())
                l.append(y_train[batch_inds].reshape((-1,1)).flatten())

        o = np.concatenate(o)
        l = np.concatenate(l)
        print("RMSE:")
        print(np.sqrt(np.mean(np.square(l-o))))
        print("ROC AUC:")
        print(roc_auc_score(l, o))
        
        o = []
        l = []

        print("TEST:")
        test_inds = range(0, len(test_features))

        for b in range(0, len(test_features)/batch_size):
            batch_inds = [test_inds[idx] for idx in range(b*batch_size, (b+1)*batch_size)]
            
            test_x_batch = np.concatenate([np.expand_dims(test_features[idx]["x"], 0) for idx in batch_inds], axis=0)
            test_g_batch = np.concatenate([np.expand_dims(test_features[idx]["g"], 0) for idx in batch_inds], axis=0)

            xb = torch.from_numpy(test_x_batch.astype(np.float32)).cuda()
            gb = torch.from_numpy(test_g_batch.astype(np.float32)).cuda()
            
            self.input_x.resize_as_(xb).copy_(xb)
            self.input_g.resize_as_(gb).copy_(gb)
            
            input_xv = Variable(self.input_x)
            input_gv = Variable(self.input_g)

            output = self.net(input_gv, input_xv)
            
            if transformer is not None:
                o.append(transformer.inverse_transform(output.data.cpu().numpy().reshape((-1,1))).flatten())
                l.append(transformer.inverse_transform(y_test[batch_inds].reshape((-1,1))).flatten())
            else:
                o.append(output.data.cpu().numpy().reshape((-1,1)).flatten())
                l.append(y_test[batch_inds].reshape((-1,1)).flatten())

        o = np.concatenate(o)
        l = np.concatenate(l)
        print("RMSE:")
        print(np.sqrt(np.mean(np.square(l-o))))
        print("ROC AUC:")
        print(roc_auc_score(l, o))
</source>
</class>

<class classid="2" nclones="4" nlines="12" similarity="100">
<source file="systems/deepchem-2.6.1/contrib/rl/test_mcts.py" startline="29" endline="41" pcid="119">
      def step(self, action):
        if action == 37:
          self._terminated = True  # Walk away.
          return 0.0
        wheel = np.random.randint(37)
        if wheel == 0:
          if action == 0:
            return 35.0
          return -1.0
        if action != 0 and wheel % 2 == action % 2:
          return 1.0
        return -1.0

</source>
<source file="systems/deepchem-2.6.1/deepchem/rl/tests/test_rl_reload.py" startline="14" endline="26" pcid="1156">
    def step(self, action):
      if action == 37:
        self._terminated = True  # Walk away.
        return 0.0
      wheel = np.random.randint(37)
      if wheel == 0:
        if action == 0:
          return 35.0
        return -1.0
      if action != 0 and wheel % 2 == action % 2:
        return 1.0
      return -1.0

</source>
<source file="systems/deepchem-2.6.1/deepchem/rl/tests/test_ppo.py" startline="35" endline="47" pcid="1166">
      def step(self, action):
        if action == 37:
          self._terminated = True  # Walk away.
          return 0.0
        wheel = np.random.randint(37)
        if wheel == 0:
          if action == 0:
            return 35.0
          return -1.0
        if action != 0 and wheel % 2 == action % 2:
          return 1.0
        return -1.0

</source>
<source file="systems/deepchem-2.6.1/deepchem/rl/tests/test_a2c.py" startline="35" endline="47" pcid="1187">
      def step(self, action):
        if action == 37:
          self._terminated = True  # Walk away.
          return 0.0
        wheel = np.random.randint(37)
        if wheel == 0:
          if action == 0:
            return 35.0
          return -1.0
        if action != 0 and wheel % 2 == action % 2:
          return 1.0
        return -1.0

</source>
</class>

<class classid="3" nclones="2" nlines="18" similarity="100">
<source file="systems/deepchem-2.6.1/contrib/atomicconv/splits/pdbbind_temporal_split.py" startline="8" endline="42" pcid="171">
def load_pdbbind_labels(labels_file):
  """Loads pdbbind labels as dataframe

  Parameters
  ----------
  labels_file: str
    Location of PDBbind datafile.

  Returns
  -------
  contents_df: pd.DataFrame
    Dataframe containing contents of PDBbind datafile.

  """

  contents = []
  with open(labels_file) as f:
    for line in f:
      if line.startswith("#"):
        continue
      else:
        splitline = line.split()
        if len(splitline) == 8:
          contents.append(splitline)
        else:
          print("Incorrect data format")
          print(splitline)

  contents_df = pd.DataFrame(
      contents,
      columns=("PDB code", "resolution", "release year", "-logKd/Ki", "Kd/Ki",
               "ignore-this-field", "reference", "ligand name"))
  return contents_df


</source>
<source file="systems/deepchem-2.6.1/contrib/atomicconv/feat/atomicnet_pdbbind_datasets.py" startline="15" endline="49" pcid="183">
def load_pdbbind_labels(labels_file):
  """Loads pdbbind labels as dataframe

  Parameters
  ----------
  labels_file: str
    Location of PDBbind datafile.

  Returns
  -------
  contents_df: pd.DataFrame
    Dataframe containing contents of PDBbind datafile.

  """

  contents = []
  with open(labels_file) as f:
    for line in f:
      if line.startswith("#"):
        continue
      else:
        splitline = line.split()
        if len(splitline) == 8:
          contents.append(splitline)
        else:
          print("Incorrect data format")
          print(splitline)

  contents_df = pd.DataFrame(
      contents,
      columns=("PDB code", "resolution", "release year", "-logKd/Ki", "Kd/Ki",
               "ignore-this-field", "reference", "ligand name"))
  return contents_df


</source>
</class>

<class classid="4" nclones="2" nlines="12" similarity="100">
<source file="systems/deepchem-2.6.1/deepchem/splits/tests/test_splitter.py" startline="27" endline="42" pcid="220">
def load_multitask_data():
  """Load example multitask data."""
  current_dir = os.path.dirname(os.path.abspath(__file__))
  featurizer = dc.feat.CircularFingerprint(size=1024)
  tasks = [
      "task0", "task1", "task2", "task3", "task4", "task5", "task6", "task7",
      "task8", "task9", "task10", "task11", "task12", "task13", "task14",
      "task15", "task16"
  ]
  input_file = os.path.join(current_dir,
                            "../../models/tests/multitask_example.csv")
  loader = dc.data.CSVLoader(
      tasks=tasks, feature_field="smiles", featurizer=featurizer)
  return loader.create_dataset(input_file)


</source>
<source file="systems/deepchem-2.6.1/deepchem/data/tests/test_datasets.py" startline="31" endline="46" pcid="1306">
def load_multitask_data():
  """Load example multitask data."""
  current_dir = os.path.dirname(os.path.abspath(__file__))
  featurizer = dc.feat.CircularFingerprint(size=1024)
  tasks = [
      "task0", "task1", "task2", "task3", "task4", "task5", "task6", "task7",
      "task8", "task9", "task10", "task11", "task12", "task13", "task14",
      "task15", "task16"
  ]
  input_file = os.path.join(current_dir,
                            "../../models/tests/multitask_example.csv")
  loader = dc.data.CSVLoader(
      tasks=tasks, feature_field="smiles", featurizer=featurizer)
  return loader.create_dataset(input_file)


</source>
</class>

<class classid="5" nclones="2" nlines="14" similarity="100">
<source file="systems/deepchem-2.6.1/deepchem/models/chemnet_layers.py" startline="240" endline="260" pcid="409">
  def call(self, inputs):
    """Invoked when __call__ method of the layer is used."""
    conv1 = inputs
    for layer in self.conv_block1:
      conv1 = layer(conv1)

    conv2 = inputs
    for layer in self.conv_block2:
      conv2 = layer(conv2)

    concat_conv = self.concat_layer([conv1, conv2])

    conv3 = concat_conv
    for layer in self.conv_block3:
      conv3 = layer(conv3)

    output = self.add_layer([conv3, inputs])
    output = self.activation_layer(output)
    return output


</source>
<source file="systems/deepchem-2.6.1/deepchem/models/chemnet_layers.py" startline="337" endline="357" pcid="412">
  def call(self, inputs):
    """Invoked when __call__ method of the layer is used."""
    conv1 = inputs
    for layer in self.conv_block1:
      conv1 = layer(conv1)

    conv2 = inputs
    for layer in self.conv_block2:
      conv2 = layer(conv2)

    concat_conv = self.concat_layer([conv1, conv2])

    conv3 = concat_conv
    for layer in self.conv_block3:
      conv3 = layer(conv3)

    output = self.add_layer([conv3, inputs])
    output = self.activation_layer(output)
    return output


</source>
</class>

<class classid="6" nclones="2" nlines="16" similarity="100">
<source file="systems/deepchem-2.6.1/deepchem/feat/tests/test_coulomb_matrices.py" startline="16" endline="41" pcid="1043">
  def setUp(self):
    """
    Set up tests.
    """
    from rdkit import Chem
    from rdkit.Chem import AllChem
    smiles = 'CC(=O)OC1=CC=CC=C1C(=O)O'
    mol = Chem.MolFromSmiles(smiles)
    self.mol_with_no_conf = mol

    # with one conformer
    mol_with_one_conf = Chem.AddHs(mol)
    AllChem.EmbedMolecule(mol_with_one_conf, AllChem.ETKDG())
    self.mol_with_one_conf = mol_with_one_conf

    # with multiple conformers
    self.num_confs = 4
    engine = conformers.ConformerGenerator(max_conformers=self.num_confs)
    self.mol_with_multi_conf = engine.generate_conformers(mol)

    # include explicit hydrogens
    self.num_atoms = mol_with_one_conf.GetNumAtoms()
    assert self.num_atoms == 21
    assert self.mol_with_one_conf.GetNumConformers() == 1
    assert self.mol_with_multi_conf.GetNumConformers() == self.num_confs

</source>
<source file="systems/deepchem-2.6.1/deepchem/feat/tests/test_coulomb_matrices.py" startline="130" endline="155" pcid="1050">
  def setUp(self):
    """
    Set up tests.
    """
    from rdkit import Chem
    from rdkit.Chem import AllChem
    smiles = 'CC(=O)OC1=CC=CC=C1C(=O)O'
    mol = Chem.MolFromSmiles(smiles)
    self.mol_with_no_conf = mol

    # with one conformer
    mol_with_one_conf = Chem.AddHs(mol)
    AllChem.EmbedMolecule(mol_with_one_conf, AllChem.ETKDG())
    self.mol_with_one_conf = mol_with_one_conf

    # with multiple conformers
    self.num_confs = 4
    engine = conformers.ConformerGenerator(max_conformers=self.num_confs)
    self.mol_with_multi_conf = engine.generate_conformers(mol)

    # include explicit hydrogens
    self.num_atoms = mol_with_one_conf.GetNumAtoms()
    assert self.num_atoms == 21
    assert self.mol_with_one_conf.GetNumConformers() == 1
    assert self.mol_with_multi_conf.GetNumConformers() == self.num_confs

</source>
</class>

<class classid="7" nclones="2" nlines="15" similarity="100">
<source file="systems/deepchem-2.6.1/deepchem/rl/a2c.py" startline="345" endline="361" pcid="1147">
  def _predict_outputs(self, state, use_saved_states, save_states):
    """Compute a set of outputs for a state. """
    if not self._state_is_list:
      state = [state]
    if use_saved_states:
      state = state + list(self._rnn_states)
    else:
      state = state + list(self._policy.rnn_initial_states)
    inputs = [np.expand_dims(s, axis=0) for s in state]
    results = self._compute_model(inputs)
    results = [r.numpy() for r in results]
    if save_states:
      self._rnn_states = [
          np.squeeze(results[i], 0) for i in self._rnn_final_state_indices
      ]
    return results

</source>
<source file="systems/deepchem-2.6.1/deepchem/rl/ppo.py" startline="367" endline="383" pcid="1237">
  def _predict_outputs(self, state, use_saved_states, save_states):
    """Compute a set of outputs for a state. """
    if not self._state_is_list:
      state = [state]
    if use_saved_states:
      state = state + list(self._rnn_states)
    else:
      state = state + list(self._policy.rnn_initial_states)
    inputs = [np.expand_dims(s, axis=0) for s in state]
    results = self._compute_model(inputs)
    results = [r.numpy() for r in results]
    if save_states:
      self._rnn_states = [
          np.squeeze(results[i], 0) for i in self._rnn_final_state_indices
      ]
    return results

</source>
</class>

<class classid="8" nclones="2" nlines="10" similarity="100">
<source file="systems/deepchem-2.6.1/deepchem/rl/tests/test_ppo.py" startline="60" endline="76" pcid="1169">
      def create_model(self, **kwargs):

        class TestModel(tf.keras.Model):

          def __init__(self):
            super(TestModel, self).__init__(**kwargs)
            self.action = tf.Variable(np.ones(env.n_actions, np.float32))
            self.value = tf.Variable([0.0], tf.float32)

          def call(self, inputs, **kwargs):
            prob = tf.nn.softmax(tf.reshape(self.action, (-1, env.n_actions)))
            return (prob, self.value)

        return TestModel()

    # Optimize it.

</source>
<source file="systems/deepchem-2.6.1/deepchem/rl/tests/test_a2c.py" startline="60" endline="76" pcid="1190">
      def create_model(self, **kwargs):

        class TestModel(tf.keras.Model):

          def __init__(self):
            super(TestModel, self).__init__(**kwargs)
            self.action = tf.Variable(np.ones(env.n_actions, np.float32))
            self.value = tf.Variable([0.0], tf.float32)

          def call(self, inputs, **kwargs):
            prob = tf.nn.softmax(tf.reshape(self.action, (-1, env.n_actions)))
            return (prob, self.value)

        return TestModel()

    # Optimize it.

</source>
</class>

<class classid="9" nclones="2" nlines="11" similarity="100">
<source file="systems/deepchem-2.6.1/deepchem/rl/tests/test_ppo.py" startline="137" endline="151" pcid="1177">
      def create_model(self, **kwargs):
        state = Input(shape=(10,))
        rnn_state = Input(shape=(10,))
        reshaped = Reshape((1, 10))(state)
        gru, rnn_final_state = GRU(
            10, return_state=True, return_sequences=True, time_major=True)(
                reshaped, initial_state=rnn_state)
        output = Softmax()(Reshape((10,))(gru))
        value = dc.models.layers.Variable([0.0])([state])
        return tf.keras.Model(
            inputs=[state, rnn_state], outputs=[output, value, rnn_final_state])

    # We don't care about actually optimizing it, so just run a few rollouts to make
    # sure fit() doesn't crash, then check the behavior of the GRU state.

</source>
<source file="systems/deepchem-2.6.1/deepchem/rl/tests/test_a2c.py" startline="136" endline="150" pcid="1198">
      def create_model(self, **kwargs):
        state = Input(shape=(10,))
        rnn_state = Input(shape=(10,))
        reshaped = Reshape((1, 10))(state)
        gru, rnn_final_state = GRU(
            10, return_state=True, return_sequences=True, time_major=True)(
                reshaped, initial_state=rnn_state)
        output = Softmax()(Reshape((10,))(gru))
        value = dc.models.layers.Variable([0.0])([state])
        return tf.keras.Model(
            inputs=[state, rnn_state], outputs=[output, value, rnn_final_state])

    # We don't care about actually optimizing it, so just run a few rollouts to make
    # sure fit() doesn't crash, then check the behavior of the GRU state.

</source>
</class>

<class classid="10" nclones="2" nlines="12" similarity="100">
<source file="systems/deepchem-2.6.1/deepchem/rl/tests/test_ppo.py" startline="197" endline="209" pcid="1181">
      def step(self, action):
        new_state = self._state.copy()
        new_state[:2] += self.moves[action]
        self._state = new_state
        self.count += 1
        reward = 0
        if np.array_equal(new_state[:2], new_state[2:]):
          self._terminated = True
          reward = 1
        elif self.count == 1000:
          self._terminated = True
        return reward

</source>
<source file="systems/deepchem-2.6.1/deepchem/rl/tests/test_a2c.py" startline="196" endline="208" pcid="1202">
      def step(self, action):
        new_state = self._state.copy()
        new_state[:2] += self.moves[action]
        self._state = new_state
        self.count += 1
        reward = 0
        if np.array_equal(new_state[:2], new_state[2:]):
          self._terminated = True
          reward = 1
        elif self.count == 1000:
          self._terminated = True
        return reward

</source>
</class>

<class classid="11" nclones="2" nlines="15" similarity="100">
<source file="systems/deepchem-2.6.1/deepchem/rl/tests/test_ppo.py" startline="210" endline="227" pcid="1182">
      def apply_hindsight(self, states, actions, goal):
        new_states = []
        rewards = []
        goal_pos = goal[:2]
        for state, action in zip(states, actions):
          new_state = state.copy()
          new_state[2:] = goal_pos
          new_states.append(new_state)
          pos_after_action = new_state[:2] + self.moves[action]
          if np.array_equal(pos_after_action, goal_pos):
            rewards.append(1)
            break
          else:
            rewards.append(0)
        return new_states, rewards

    # A simple policy with two hidden layers.

</source>
<source file="systems/deepchem-2.6.1/deepchem/rl/tests/test_a2c.py" startline="209" endline="226" pcid="1203">
      def apply_hindsight(self, states, actions, goal):
        new_states = []
        rewards = []
        goal_pos = goal[:2]
        for state, action in zip(states, actions):
          new_state = state.copy()
          new_state[2:] = goal_pos
          new_states.append(new_state)
          pos_after_action = new_state[:2] + self.moves[action]
          if np.array_equal(pos_after_action, goal_pos):
            rewards.append(1)
            break
          else:
            rewards.append(0)
        return new_states, rewards

    # A simple policy with two hidden layers.

</source>
</class>

<class classid="12" nclones="3" nlines="11" similarity="100">
<source file="systems/deepchem-2.6.1/deepchem/data/tests/test_datasets.py" startline="281" endline="294" pcid="1317">
  def shard_generator():
    for sz in shard_sizes:
      X_b = np.random.rand(sz, 1)
      y_b = np.random.rand(sz, 1)
      w_b = np.random.rand(sz, 1)
      ids_b = np.random.rand(sz)

      all_Xs.append(X_b)
      all_ys.append(y_b)
      all_ws.append(w_b)
      all_ids.append(ids_b)

      yield X_b, y_b, w_b, ids_b

</source>
<source file="systems/deepchem-2.6.1/deepchem/data/tests/test_datasets.py" startline="563" endline="576" pcid="1331">

    def shard_generator():
      for sz in shard_sizes:
        X_b = np.random.rand(sz, 1)
        y_b = np.random.rand(sz, 1)
        w_b = np.random.rand(sz, 1)
        ids_b = np.random.rand(sz)

        all_Xs.append(X_b)
        all_ys.append(y_b)
        all_ws.append(w_b)
        all_ids.append(ids_b)

        yield X_b, y_b, w_b, ids_b
</source>
<source file="systems/deepchem-2.6.1/deepchem/data/tests/test_datasets.py" startline="443" endline="456" pcid="1327">

  def shard_generator():
    for sz in shard_sizes:
      X_b = np.random.rand(sz, 1)
      y_b = np.random.rand(sz, 1)
      w_b = np.random.rand(sz, 1)
      ids_b = np.random.rand(sz)

      all_Xs.append(X_b)
      all_ys.append(y_b)
      all_ws.append(w_b)
      all_ids.append(ids_b)

      yield X_b, y_b, w_b, ids_b
</source>
</class>

<class classid="13" nclones="2" nlines="10" similarity="100">
<source file="systems/deepchem-2.6.1/deepchem/trans/tests/test_cdf_transform.py" startline="7" endline="21" pcid="1617">
def load_gaussian_cdf_data():
  """Load example with numbers sampled from Gaussian normal distribution.
     Each feature and task is a column of values that is sampled
     from a normal distribution of mean 0, stdev 1."""
  current_dir = os.path.dirname(os.path.abspath(__file__))
  features = ["feat0", "feat1"]
  featurizer = dc.feat.UserDefinedFeaturizer(features)
  tasks = ["task0", "task1"]
  input_file = os.path.join(current_dir,
                            "../../models/tests/gaussian_cdf_example.csv")
  loader = dc.data.UserCSVLoader(
      tasks=tasks, featurizer=featurizer, id_field="id")
  return loader.create_dataset(input_file)


</source>
<source file="systems/deepchem-2.6.1/deepchem/trans/tests/test_power.py" startline="6" endline="20" pcid="1668">
def load_gaussian_cdf_data():
  """Load example with numbers sampled from Gaussian normal distribution.
     Each feature and task is a column of values that is sampled
     from a normal distribution of mean 0, stdev 1."""
  current_dir = os.path.dirname(os.path.abspath(__file__))
  features = ["feat0", "feat1"]
  featurizer = dc.feat.UserDefinedFeaturizer(features)
  tasks = ["task0", "task1"]
  input_file = os.path.join(current_dir,
                            "../../models/tests/gaussian_cdf_example.csv")
  loader = dc.data.UserCSVLoader(
      tasks=tasks, featurizer=featurizer, id_field="id")
  return loader.create_dataset(input_file)


</source>
</class>

<class classid="14" nclones="2" nlines="10" similarity="100">
<source file="systems/deepchem-2.6.1/deepchem/metalearning/tests/test_maml_reload.py" startline="12" endline="22" pcid="1792">
    def __init__(self):
      self.batch_size = 10
      self.w1 = tf.Variable(np.random.normal(size=[1, 40], scale=1.0))
      self.w2 = tf.Variable(
          np.random.normal(size=[40, 40], scale=np.sqrt(1 / 40)))
      self.w3 = tf.Variable(
          np.random.normal(size=[40, 1], scale=np.sqrt(1 / 40)))
      self.b1 = tf.Variable(np.zeros(40))
      self.b2 = tf.Variable(np.zeros(40))
      self.b3 = tf.Variable(np.zeros(1))

</source>
<source file="systems/deepchem-2.6.1/deepchem/metalearning/tests/test_maml.py" startline="26" endline="36" pcid="1799">
      def __init__(self):
        self.batch_size = 10
        self.w1 = tf.Variable(np.random.normal(size=[1, 40], scale=1.0))
        self.w2 = tf.Variable(
            np.random.normal(size=[40, 40], scale=np.sqrt(1 / 40)))
        self.w3 = tf.Variable(
            np.random.normal(size=[40, 1], scale=np.sqrt(1 / 40)))
        self.b1 = tf.Variable(np.zeros(40))
        self.b2 = tf.Variable(np.zeros(40))
        self.b3 = tf.Variable(np.zeros(1))

</source>
</class>

<class classid="15" nclones="3" nlines="10" similarity="100">
<source file="systems/deepchem-2.6.1/examples/kinase/KINASE_datasets.py" startline="15" endline="30" pcid="1922">
def remove_missing_entries(dataset):
  """Remove missing entries.

  Some of the datasets have missing entries that sneak in as zero'd out
  feature vectors. Get rid of them.
  """
  for i, (X, y, w, ids) in enumerate(dataset.itershards()):
    available_rows = X.any(axis=1)
    print("Shard %d has %d missing entries."
        % (i, np.count_nonzero(~available_rows)))
    X = X[available_rows]
    y = y[available_rows]
    w = w[available_rows]
    ids = ids[available_rows]
    dataset.set_shard(i, X, y, w, ids)

</source>
<source file="systems/deepchem-2.6.1/examples/uv/UV_datasets.py" startline="15" endline="30" pcid="1929">
def remove_missing_entries(dataset):
  """Remove missing entries.

  Some of the datasets have missing entries that sneak in as zero'd out
  feature vectors. Get rid of them.
  """
  for i, (X, y, w, ids) in enumerate(dataset.itershards()):
    available_rows = X.any(axis=1)
    print("Shard %d has %d missing entries."
        % (i, np.count_nonzero(~available_rows)))
    X = X[available_rows]
    y = y[available_rows]
    w = w[available_rows]
    ids = ids[available_rows]
    dataset.set_shard(i, X, y, w, ids)

</source>
<source file="systems/deepchem-2.6.1/examples/factors/FACTORS_datasets.py" startline="15" endline="30" pcid="1949">
def remove_missing_entries(dataset):
  """Remove missing entries.

  Some of the datasets have missing entries that sneak in as zero'd out
  feature vectors. Get rid of them.
  """
  for i, (X, y, w, ids) in enumerate(dataset.itershards()):
    available_rows = X.any(axis=1)
    print("Shard %d has %d missing entries."
        % (i, np.count_nonzero(~available_rows)))
    X = X[available_rows]
    y = y[available_rows]
    w = w[available_rows]
    ids = ids[available_rows]
    dataset.set_shard(i, X, y, w, ids)

</source>
</class>

<class classid="16" nclones="3" nlines="14" similarity="100">
<source file="systems/deepchem-2.6.1/examples/kinase/KINASE_tf_singletask.py" startline="38" endline="53" pcid="1926">
def task_model_builder(m_dir):
  return dc.models.TensorflowMultitaskRegressor(
      n_tasks=1,
      n_features=n_features,
      logdir=m_dir,
      layer_sizes=[1000] * n_layers,
      dropouts=[.25] * n_layers,
      weight_init_stddevs=[.02] * n_layers,
      bias_init_consts=[1.] * n_layers,
      learning_rate=.0003,
      penalty=.0001,
      penalty_type="l2",
      optimizer="adam",
      batch_size=100)


</source>
<source file="systems/deepchem-2.6.1/examples/factors/FACTORS_tf_singletask.py" startline="38" endline="53" pcid="1948">
def task_model_builder(m_dir):
  return dc.models.TensorflowMultitaskRegressor(
      n_tasks=1,
      n_features=n_features,
      logdir=m_dir,
      layer_sizes=[1000] * n_layers,
      dropouts=[.25] * n_layers,
      weight_init_stddevs=[.02] * n_layers,
      bias_init_consts=[1.] * n_layers,
      learning_rate=.0003,
      penalty=.0001,
      penalty_type="l2",
      optimizer="adam",
      batch_size=100)


</source>
<source file="systems/deepchem-2.6.1/examples/uv/UV_tf_singletask.py" startline="37" endline="52" pcid="1934">
def task_model_builder(m_dir):
  return dc.models.TensorflowMultitaskRegressor(
      n_tasks=1,
      n_features=n_features,
      logdir=m_dir,
      layer_sizes=[1000] * n_layers,
      dropouts=[.25] * n_layers,
      weight_init_stddevs=[.02] * n_layers,
      bias_init_consts=[1.] * n_layers,
      learning_rate=.0003,
      penalty=.0001,
      penalty_type="l2",
      optimizer="adam",
      batch_size=100)


</source>
</class>

</clones>

<clones>
<systeminfo processor="nicad6" system="texar-0.2.4" granularity="functions-blind" threshold="30%" minlines="10" maxlines="2500"/>
<cloneinfo npcs="1305" npairs="126"/>
<runinfo ncompares="65043" cputime="83755"/>
<classinfo nclasses="63"/>

<class classid="1" nclones="2" nlines="12" similarity="90">
<source file="systems/texar-0.2.4/texar/tf/agents/seq_pg_agent.py" startline="371" endline="387" pcid="66">
    def _evaluate_pg_loss(self):
        fetches = {
            "loss": self._pg_loss
        }

        feed_dict_ = None
        if not self._qvalue_inputs_fed:
            qvalues = self._get_qvalues()
            feed_dict_ = {self._qvalue_inputs: qvalues}

        vals = self._sess.partial_run(
            self._partial_run_handle, fetches, feed_dict=feed_dict_)

        self._qvalue_inputs_fed = True

        return vals['loss']

</source>
<source file="systems/texar-0.2.4/texar/tf/agents/seq_pg_agent.py" startline="388" endline="406" pcid="67">
    def _train_policy(self):
        """Updates the policy.
        """
        fetches = {
            "loss": self._train_op,
        }

        feed_dict_ = None
        if not self._qvalue_inputs_fed:
            qvalues = self._get_qvalues()
            feed_dict_ = {self._qvalue_inputs: qvalues}

        vals = self._sess.partial_run(
            self._partial_run_handle, fetches, feed_dict=feed_dict_)

        self._qvalue_inputs_fed = True

        return vals['loss']

</source>
</class>

<class classid="2" nclones="5" nlines="12" similarity="78">
<source file="systems/texar-0.2.4/texar/tf/agents/seq_pg_agent_test.py" startline="34" endline="45" pcid="74">
    def setUp(self):
        tf.test.TestCase.setUp(self)
        self._vocab_size = 4
        self._max_time = 8
        self._batch_size = 16
        self._emb_dim = 20
        self._inputs = tf.random_uniform(
            [self._batch_size, self._max_time, self._emb_dim],
            maxval=1., dtype=tf.float32)
        self._embedding = tf.random_uniform(
            [self._vocab_size, self._emb_dim], maxval=1., dtype=tf.float32)

</source>
<source file="systems/texar-0.2.4/texar/tf/modules/decoders/rnn_decoders_test.py" startline="29" endline="40" pcid="758">
    def setUp(self):
        tf.test.TestCase.setUp(self)
        self._vocab_size = 4
        self._max_time = 8
        self._batch_size = 16
        self._emb_dim = 20
        self._inputs = tf.random_uniform(
            [self._batch_size, self._max_time, self._emb_dim],
            maxval=1., dtype=tf.float32)
        self._embedding = tf.random_uniform(
            [self._vocab_size, self._emb_dim], maxval=1., dtype=tf.float32)

</source>
<source file="systems/texar-0.2.4/texar/tf/modules/decoders/rnn_decoder_helpers_test.py" startline="26" endline="40" pcid="702">

    def setUp(self):
        tf.test.TestCase.setUp(self)
        self._batch_size = 16
        self._vocab_size = 4
        self._start_tokens = [self._vocab_size - 2] * self._batch_size
        self._end_token = self._vocab_size - 1
        self._max_time = 8
        self._emb_dim = 100
        self._inputs = tf.random_uniform(
            [self._batch_size, self._max_time, self._emb_dim],
            maxval=1., dtype=tf.float32)
        self._embedding = tf.random_uniform(
            [self._vocab_size, self._emb_dim], maxval=1., dtype=tf.float32)
        self._max_seq_length = 10
</source>
<source file="systems/texar-0.2.4/texar/tf/modules/decoders/rnn_decoders_test.py" startline="223" endline="237" pcid="764">

    def setUp(self):
        tf.test.TestCase.setUp(self)
        self._vocab_size = 10
        self._max_time = 16
        self._batch_size = 8
        self._emb_dim = 20
        self._attention_dim = 256
        self._inputs = tf.random_uniform(
            [self._batch_size, self._max_time, self._emb_dim],
            maxval=1., dtype=tf.float32)
        self._embedding = tf.random_uniform(
            [self._vocab_size, self._emb_dim], maxval=1., dtype=tf.float32)
        self._encoder_output = tf.random_uniform(
            [self._batch_size, self._max_time, 64])
</source>
<source file="systems/texar-0.2.4/texar/tf/modules/decoders/beam_search_decode_test.py" startline="29" endline="45" pcid="768">
    def setUp(self):
        tf.test.TestCase.setUp(self)
        self._vocab_size = 10
        self._max_time = 16
        self._batch_size = 8
        self._emb_dim = 20
        self._cell_dim = 256
        self._attention_dim = self._cell_dim
        self._beam_width = 11
        self._inputs = tf.random_uniform(
            [self._batch_size, self._max_time, self._emb_dim],
            maxval=1., dtype=tf.float32)
        self._embedding = tf.random_uniform(
            [self._vocab_size, self._emb_dim], maxval=1., dtype=tf.float32)
        self._encoder_output = tf.random_uniform(
            [self._batch_size, self._max_time, 64])

</source>
</class>

<class classid="3" nclones="3" nlines="13" similarity="85">
<source file="systems/texar-0.2.4/texar/tf/data/tokenizers/bert_tokenizer_test.py" startline="49" endline="66" pcid="184">

    def test_pickle(self):
        tokenizer = BERTTokenizer.load(self.vocab_file)
        self.assertIsNotNone(tokenizer)

        text = u"Munich and Berlin are nice cities"
        subwords = tokenizer.map_text_to_token(text)

        with tempfile.TemporaryDirectory() as tmpdirname:
            filename = os.path.join(tmpdirname, u"tokenizer.bin")
            with open(filename, "wb") as f:
                pickle.dump(tokenizer, f)
            with open(filename, "rb") as f:
                tokenizer_new = pickle.load(f)

        subwords_loaded = tokenizer_new.map_text_to_token(text)

        self.assertListEqual(subwords, subwords_loaded)
</source>
<source file="systems/texar-0.2.4/texar/tf/data/tokenizers/xlnet_tokenizer_test.py" startline="82" endline="99" pcid="196">
                                           u'<unk>', u'.'])

    def test_pickle(self):
        tokenizer = XLNetTokenizer.load(self.tmp_dir.name)
        self.assertIsNotNone(tokenizer)

        text = u"Munich and Berlin are nice cities"
        subwords = tokenizer.map_text_to_token(text)

        with tempfile.TemporaryDirectory() as tmpdirname:
            filename = os.path.join(tmpdirname, u"tokenizer.bin")
            with open(filename, "wb") as f:
                pickle.dump(tokenizer, f)
            with open(filename, "rb") as f:
                tokenizer_new = pickle.load(f)

        subwords_loaded = tokenizer_new.map_text_to_token(text)

</source>
<source file="systems/texar-0.2.4/texar/tf/data/tokenizers/gpt2_tokenizer_test.py" startline="61" endline="79" pcid="218">
            input_bpe_tokens)

    def test_pickle(self):
        tokenizer = GPT2Tokenizer.load(self.tmp_dir.name,
                                       self.special_tokens_map)
        self.assertIsNotNone(tokenizer)

        text = u"Munich and Berlin are nice cities"
        subwords = tokenizer.map_text_to_token(text)

        with tempfile.TemporaryDirectory() as tmpdirname:
            filename = os.path.join(tmpdirname, u"tokenizer.bin")
            with open(filename, "wb") as f:
                pickle.dump(tokenizer, f)
            with open(filename, "rb") as f:
                tokenizer_new = pickle.load(f)

        subwords_loaded = tokenizer_new.map_text_to_token(text)

</source>
</class>

<class classid="4" nclones="3" nlines="10" similarity="81">
<source file="systems/texar-0.2.4/texar/tf/data/tokenizers/bert_tokenizer_test.py" startline="67" endline="80" pcid="185">

    def test_save_load(self):
        tokenizer = BERTTokenizer.load(self.vocab_file)

        before_tokens = tokenizer.map_text_to_id(
            u"He is very happy, UNwant\u00E9d,running")

        with tempfile.TemporaryDirectory() as tmpdirname:
            tokenizer.save(tmpdirname)
            tokenizer = tokenizer.load(tmpdirname)

        after_tokens = tokenizer.map_text_to_id(
            u"He is very happy, UNwant\u00E9d,running")
        self.assertListEqual(before_tokens, after_tokens)
</source>
<source file="systems/texar-0.2.4/texar/tf/data/tokenizers/gpt2_tokenizer_test.py" startline="80" endline="94" pcid="219">
        self.assertListEqual(subwords, subwords_loaded)

    def test_save_load(self):
        tokenizer = GPT2Tokenizer.load(self.tmp_dir.name,
                                       self.special_tokens_map)

        before_tokens = tokenizer.map_text_to_id(
            u"He is very happy, UNwant\u00E9d,running")

        with tempfile.TemporaryDirectory() as tmpdirname:
            tokenizer.save(tmpdirname)
            tokenizer = tokenizer.load(tmpdirname)

        after_tokens = tokenizer.map_text_to_id(
            u"He is very happy, UNwant\u00E9d,running")
</source>
<source file="systems/texar-0.2.4/texar/tf/data/tokenizers/xlnet_tokenizer_test.py" startline="100" endline="113" pcid="197">
        self.assertListEqual(subwords, subwords_loaded)

    def test_save_load(self):
        tokenizer = XLNetTokenizer.load(self.tmp_dir.name)

        before_tokens = tokenizer.map_text_to_id(
            u"He is very happy, UNwant\u00E9d,running")

        with tempfile.TemporaryDirectory() as tmpdirname:
            tokenizer.save(tmpdirname)
            tokenizer = tokenizer.load(tmpdirname)

        after_tokens = tokenizer.map_text_to_id(
            u"He is very happy, UNwant\u00E9d,running")
</source>
</class>

<class classid="5" nclones="3" nlines="13" similarity="85">
<source file="systems/texar-0.2.4/texar/tf/data/tokenizers/bert_tokenizer_test.py" startline="87" endline="105" pcid="187">

    def test_encode_decode(self):
        tokenizer = BERTTokenizer.load(self.vocab_file)

        input_text = u"UNwant\u00E9d,running"
        output_text = u"unwanted, running"

        tokens = tokenizer.map_text_to_token(input_text)
        ids = tokenizer.map_token_to_id(tokens)
        ids_2 = tokenizer.map_text_to_id(input_text)
        self.assertListEqual(ids, ids_2)

        tokens_2 = tokenizer.map_id_to_token(ids)
        text_2 = tokenizer.map_id_to_text(ids)

        self.assertEqual(text_2, output_text)

        self.assertNotEqual(len(tokens_2), 0)
        self.assertIsInstance(text_2, str)
</source>
<source file="systems/texar-0.2.4/texar/tf/data/tokenizers/gpt2_tokenizer_test.py" startline="101" endline="120" pcid="221">
        self.assertListEqual(model_list_1, model_list_2)

    def test_encode_decode(self):
        tokenizer = GPT2Tokenizer.load(self.tmp_dir.name,
                                       self.special_tokens_map)

        input_text = u"lower newer"
        output_text = u"lower<unk>newer"

        tokens = tokenizer.map_text_to_token(input_text)
        ids = tokenizer.map_token_to_id(tokens)
        ids_2 = tokenizer.map_text_to_id(input_text)
        self.assertListEqual(ids, ids_2)

        tokens_2 = tokenizer.map_id_to_token(ids)
        text_2 = tokenizer.map_id_to_text(ids)

        self.assertEqual(text_2, output_text)

        self.assertNotEqual(len(tokens_2), 0)
</source>
<source file="systems/texar-0.2.4/texar/tf/data/tokenizers/xlnet_tokenizer_test.py" startline="120" endline="138" pcid="199">
        self.assertListEqual(model_list_1, model_list_2)

    def test_encode_decode(self):
        tokenizer = XLNetTokenizer.load(self.tmp_dir.name)

        input_text = u"This is a test"
        output_text = u"This is a test"

        tokens = tokenizer.map_text_to_token(input_text)
        ids = tokenizer.map_token_to_id(tokens)
        ids_2 = tokenizer.map_text_to_id(input_text)
        self.assertListEqual(ids, ids_2)

        tokens_2 = tokenizer.map_id_to_token(ids)
        text_2 = tokenizer.map_id_to_text(ids)

        self.assertEqual(text_2, output_text)

        self.assertNotEqual(len(tokens_2), 0)
</source>
</class>

<class classid="6" nclones="3" nlines="38" similarity="94">
<source file="systems/texar-0.2.4/texar/tf/data/tokenizers/bert_tokenizer_test.py" startline="106" endline="154" pcid="188">

    def test_add_tokens(self):
        tokenizer = BERTTokenizer.load(self.vocab_file)

        vocab_size = tokenizer.vocab_size
        all_size = len(tokenizer)

        self.assertNotEqual(vocab_size, 0)
        self.assertEqual(vocab_size, all_size)

        new_toks = ["aaaaabbbbbb", "cccccccccdddddddd"]
        added_toks = tokenizer.add_tokens(new_toks)
        vocab_size_2 = tokenizer.vocab_size
        all_size_2 = len(tokenizer)

        self.assertNotEqual(vocab_size_2, 0)
        self.assertEqual(vocab_size, vocab_size_2)
        self.assertEqual(added_toks, len(new_toks))
        self.assertEqual(all_size_2, all_size + len(new_toks))

        tokens = tokenizer.map_text_to_id("aaaaabbbbbb low cccccccccdddddddd l")
        self.assertGreaterEqual(len(tokens), 4)
        self.assertGreater(tokens[0], tokenizer.vocab_size - 1)
        self.assertGreater(tokens[-2], tokenizer.vocab_size - 1)

        new_toks_2 = {'eos_token': ">>>>|||<||<<|<<",
                      'pad_token': "<<<<<|||>|>>>>|>"}
        added_toks_2 = tokenizer.add_special_tokens(new_toks_2)
        vocab_size_3 = tokenizer.vocab_size
        all_size_3 = len(tokenizer)

        self.assertNotEqual(vocab_size_3, 0)
        self.assertEqual(vocab_size, vocab_size_3)
        self.assertEqual(added_toks_2, len(new_toks_2))
        self.assertEqual(all_size_3, all_size_2 + len(new_toks_2))

        tokens = tokenizer.map_text_to_id(
            ">>>>|||<||<<|<< aaaaabbbbbb low cccccccccdddddddd "
            "<<<<<|||>|>>>>|> l")

        self.assertGreaterEqual(len(tokens), 6)
        self.assertGreater(tokens[0], tokenizer.vocab_size - 1)
        self.assertGreater(tokens[0], tokens[1])
        self.assertGreater(tokens[-2], tokenizer.vocab_size - 1)
        self.assertGreater(tokens[-2], tokens[-3])
        self.assertEqual(tokens[0],
                         tokenizer.map_token_to_id(tokenizer.eos_token))
        self.assertEqual(tokens[-2],
                         tokenizer.map_token_to_id(tokenizer.pad_token))
</source>
<source file="systems/texar-0.2.4/texar/tf/data/tokenizers/xlnet_tokenizer_test.py" startline="139" endline="187" pcid="200">
        self.assertIsInstance(text_2, str)

    def test_add_tokens(self):
        tokenizer = XLNetTokenizer.load(self.tmp_dir.name)

        vocab_size = tokenizer.vocab_size
        all_size = len(tokenizer)

        self.assertNotEqual(vocab_size, 0)
        self.assertEqual(vocab_size, all_size)

        new_toks = ["aaaaabbbbbb", "cccccccccdddddddd"]
        added_toks = tokenizer.add_tokens(new_toks)
        vocab_size_2 = tokenizer.vocab_size
        all_size_2 = len(tokenizer)

        self.assertNotEqual(vocab_size_2, 0)
        self.assertEqual(vocab_size, vocab_size_2)
        self.assertEqual(added_toks, len(new_toks))
        self.assertEqual(all_size_2, all_size + len(new_toks))

        tokens = tokenizer.map_text_to_id("aaaaabbbbbb low cccccccccdddddddd l")
        self.assertGreaterEqual(len(tokens), 4)
        self.assertGreater(tokens[0], tokenizer.vocab_size - 1)
        self.assertGreater(tokens[-2], tokenizer.vocab_size - 1)

        new_toks_2 = {'eos_token': ">>>>|||<||<<|<<",
                      'pad_token': "<<<<<|||>|>>>>|>"}
        added_toks_2 = tokenizer.add_special_tokens(new_toks_2)
        vocab_size_3 = tokenizer.vocab_size
        all_size_3 = len(tokenizer)

        self.assertNotEqual(vocab_size_3, 0)
        self.assertEqual(vocab_size, vocab_size_3)
        self.assertEqual(added_toks_2, len(new_toks_2))
        self.assertEqual(all_size_3, all_size_2 + len(new_toks_2))

        tokens = tokenizer.map_text_to_id(
            ">>>>|||<||<<|<< aaaaabbbbbb low cccccccccdddddddd "
            "<<<<<|||>|>>>>|> l")

        self.assertGreaterEqual(len(tokens), 6)
        self.assertGreater(tokens[0], tokenizer.vocab_size - 1)
        self.assertGreater(tokens[0], tokens[1])
        self.assertGreater(tokens[-2], tokenizer.vocab_size - 1)
        self.assertGreater(tokens[-2], tokens[-3])
        self.assertEqual(tokens[0],
                         tokenizer.map_token_to_id(tokenizer.eos_token))
        self.assertEqual(tokens[-2],
</source>
<source file="systems/texar-0.2.4/texar/tf/data/tokenizers/gpt2_tokenizer_test.py" startline="121" endline="170" pcid="222">
        self.assertIsInstance(text_2, str)

    def test_add_tokens(self):
        tokenizer = GPT2Tokenizer.load(self.tmp_dir.name,
                                       self.special_tokens_map)

        vocab_size = tokenizer.vocab_size
        all_size = len(tokenizer)

        self.assertNotEqual(vocab_size, 0)
        self.assertEqual(vocab_size, all_size)

        new_toks = ["aaaaabbbbbb", "cccccccccdddddddd"]
        added_toks = tokenizer.add_tokens(new_toks)
        vocab_size_2 = tokenizer.vocab_size
        all_size_2 = len(tokenizer)

        self.assertNotEqual(vocab_size_2, 0)
        self.assertEqual(vocab_size, vocab_size_2)
        self.assertEqual(added_toks, len(new_toks))
        self.assertEqual(all_size_2, all_size + len(new_toks))

        tokens = tokenizer.map_text_to_id("aaaaabbbbbb low cccccccccdddddddd l")
        self.assertGreaterEqual(len(tokens), 4)
        self.assertGreater(tokens[0], tokenizer.vocab_size - 1)
        self.assertGreater(tokens[-2], tokenizer.vocab_size - 1)

        new_toks_2 = {'eos_token': ">>>>|||<||<<|<<",
                      'pad_token': "<<<<<|||>|>>>>|>"}
        added_toks_2 = tokenizer.add_special_tokens(new_toks_2)
        vocab_size_3 = tokenizer.vocab_size
        all_size_3 = len(tokenizer)

        self.assertNotEqual(vocab_size_3, 0)
        self.assertEqual(vocab_size, vocab_size_3)
        self.assertEqual(added_toks_2, len(new_toks_2))
        self.assertEqual(all_size_3, all_size_2 + len(new_toks_2))

        tokens = tokenizer.map_text_to_id(
            ">>>>|||<||<<|<< aaaaabbbbbb low cccccccccdddddddd "
            "<<<<<|||>|>>>>|> l")

        self.assertGreaterEqual(len(tokens), 6)
        self.assertGreater(tokens[0], tokenizer.vocab_size - 1)
        self.assertGreater(tokens[0], tokens[1])
        self.assertGreater(tokens[-2], tokenizer.vocab_size - 1)
        self.assertGreater(tokens[-2], tokens[-3])
        self.assertEqual(tokens[0],
                         tokenizer.map_token_to_id(tokenizer.eos_token))
        self.assertEqual(tokens[-2],
</source>
</class>

<class classid="7" nclones="2" nlines="18" similarity="73">
<source file="systems/texar-0.2.4/texar/tf/data/tokenizers/xlnet_tokenizer_test.py" startline="188" endline="207" pcid="201">
                         tokenizer.map_token_to_id(tokenizer.pad_token))

    def test_tokenizer_lower(self):
        tokenizer = XLNetTokenizer.load(
            self.SAMPLE_VOCAB[0], configs={'do_lower_case': True,
                                           'keep_accents': False})
        tokens = tokenizer.map_text_to_token(
            u"I was born in 92000, and this is falsé.")
        self.assertListEqual(tokens, [SPIECE_UNDERLINE + u'', u'i',
                                      SPIECE_UNDERLINE + u'was',
                                      SPIECE_UNDERLINE + u'b',
                                      u'or', u'n', SPIECE_UNDERLINE + u'in',
                                      SPIECE_UNDERLINE + u'',
                                      u'9', u'2', u'0', u'0', u'0', u',',
                                      SPIECE_UNDERLINE + u'and',
                                      SPIECE_UNDERLINE + u'this',
                                      SPIECE_UNDERLINE + u'is',
                                      SPIECE_UNDERLINE + u'f', u'al', u'se',
                                      u'.'])
        self.assertListEqual(tokenizer.map_text_to_token(u"H\u00E9llo"),
</source>
<source file="systems/texar-0.2.4/texar/tf/data/tokenizers/xlnet_tokenizer_test.py" startline="208" endline="225" pcid="202">
                             [u"▁he", u"ll", u"o"])

    def test_tokenizer_no_lower(self):
        tokenizer = XLNetTokenizer.load(
            self.SAMPLE_VOCAB[0], configs={'do_lower_case': False,
                                           'keep_accents': False})
        tokens = tokenizer.map_text_to_token(
            u"I was born in 92000, and this is falsé.")
        self.assertListEqual(tokens, [SPIECE_UNDERLINE + u'I',
                                      SPIECE_UNDERLINE + u'was',
                                      SPIECE_UNDERLINE + u'b', u'or',
                                      u'n', SPIECE_UNDERLINE + u'in',
                                      SPIECE_UNDERLINE + u'',
                                      u'9', u'2', u'0', u'0', u'0', u',',
                                      SPIECE_UNDERLINE + u'and',
                                      SPIECE_UNDERLINE + u'this',
                                      SPIECE_UNDERLINE + u'is',
                                      SPIECE_UNDERLINE + u'f', u'al', u'se',
</source>
</class>

<class classid="8" nclones="2" nlines="17" similarity="76">
<source file="systems/texar-0.2.4/texar/tf/data/tokenizers/bert_tokenizer.py" startline="233" endline="323" pcid="212">
        return input_ids, segment_ids, input_mask

    @staticmethod
    def default_hparams() -> Dict[str, Any]:
        r"""Returns a dictionary of hyperparameters with default values.

        * The tokenizer is determined by the constructor argument
          :attr:`pretrained_model_name` if it's specified. In this case,
          `hparams` are ignored.
        * Otherwise, the tokenizer is determined by
          `hparams['pretrained_model_name']` if it's specified. All other
          configurations in `hparams` are ignored.
        * If the above two are `None`, the tokenizer is defined by the
          configurations in `hparams`.

        .. code-block:: python

            {
                "pretrained_model_name": "bert-base-uncased",
                "vocab_file": None,
                "max_len": 512,
                "unk_token": "[UNK]",
                "sep_token": "[SEP]",
                "pad_token": "[PAD]",
                "cls_token": "[CLS]",
                "mask_token": "[MASK]",
                "tokenize_chinese_chars": True,
                "do_lower_case": True,
                "do_basic_tokenize": True,
                "non_split_tokens": None,
                "name": "bert_tokenizer",
            }

        Here:

        `"pretrained_model_name"`: str or None
            The name of the pre-trained BERT model.

        `"vocab_file"`: str or None
            The path to a one-wordpiece-per-line vocabulary file.

        `"max_len"`: int
            The maximum sequence length that this model might ever be used with.

        `"unk_token"`: str
            Unknown token.

        `"sep_token"`: str
            Separation token.

        `"pad_token"`: str
            Padding token.

        `"cls_token"`: str
            Classification token.

        `"mask_token"`: str
            Masking token.

        `"tokenize_chinese_chars"`: bool
            Whether to tokenize Chinese characters.

        `"do_lower_case"`: bool
            Whether to lower case the input
            Only has an effect when `do_basic_tokenize=True`

        `"do_basic_tokenize"`: bool
            Whether to do basic tokenization before wordpiece.

        `"non_split_tokens"`: list
            List of tokens which will never be split during tokenization.
            Only has an effect when `do_basic_tokenize=True`

        `"name"`: str
            Name of the tokenizer.

        """
        return {
            'pretrained_model_name': 'bert-base-uncased',
            'vocab_file': None,
            'max_len': 512,
            'unk_token': '[UNK]',
            'sep_token': '[SEP]',
            'pad_token': '[PAD]',
            'cls_token': '[CLS]',
            'mask_token': '[MASK]',
            'tokenize_chinese_chars': True,
            'do_lower_case': True,
            'do_basic_tokenize': True,
            'non_split_tokens': None,
            'name': 'bert_tokenizer',
</source>
<source file="systems/texar-0.2.4/texar/tf/data/tokenizers/bert_tokenizer.py" startline="325" endline="340" pcid="213">
        }

    @classmethod
    def _transform_config(cls, pretrained_model_name: str,
                          cache_dir: str):
        r"""Returns the configuration of the pre-trained BERT tokenizer."""
        return {
            'vocab_file': None,
            'max_len': 512,
            'unk_token': '[UNK]',
            'sep_token': '[SEP]',
            'pad_token': '[PAD]',
            'cls_token': '[CLS]',
            'mask_token': '[MASK]',
            'tokenize_chinese_chars': True,
            'do_lower_case': True,
</source>
</class>

<class classid="9" nclones="2" nlines="16" similarity="81">
<source file="systems/texar-0.2.4/texar/tf/data/data/scalar_data.py" startline="171" endline="191" pcid="331">
    def _make_processor(dataset_hparams, data_spec, chained=True,
                        name_prefix=None):
        # Create data decoder
        decoder = ScalarDataDecoder(
            ScalarData._get_dtype(dataset_hparams["data_type"]),
            data_name=name_prefix)
        # Create other transformations
        data_spec.add_spec(decoder=decoder)
        # pylint: disable=protected-access
        other_trans = MonoTextData._make_other_transformations(
            dataset_hparams["other_transformations"], data_spec)

        data_spec.add_spec(name_prefix=name_prefix)

        if chained:
            chained_tran = dsutils.make_chained_transformation(
                [decoder] + other_trans)
            return chained_tran, data_spec
        else:
            return decoder, other_trans, data_spec

</source>
<source file="systems/texar-0.2.4/texar/tf/data/data/tfrecord_data.py" startline="319" endline="340" pcid="348">
    def _make_processor(dataset_hparams, data_spec, chained=True,
                        name_prefix=None):
        # Create data decoder
        decoder = TFRecordDataDecoder(
            feature_original_types=dataset_hparams.feature_original_types,
            feature_convert_types=dataset_hparams.feature_convert_types,
            image_options=dataset_hparams.image_options)
        # Create other transformations
        data_spec.add_spec(decoder=decoder)
        # pylint: disable=protected-access
        other_trans = MonoTextData._make_other_transformations(
            dataset_hparams["other_transformations"], data_spec)

        data_spec.add_spec(name_prefix=name_prefix)

        if chained:
            chained_tran = dsutils.make_chained_transformation(
                [decoder] + other_trans)
            return chained_tran, data_spec
        else:
            return decoder, other_trans, data_spec

</source>
</class>

<class classid="10" nclones="2" nlines="10" similarity="100">
<source file="systems/texar-0.2.4/texar/tf/data/data/scalar_data.py" startline="192" endline="205" pcid="332">
    def _process_dataset(self, dataset, hparams, data_spec):
        chained_tran, data_spec = self._make_processor(
            hparams["dataset"], data_spec,
            name_prefix=hparams["dataset"]["data_name"])
        num_parallel_calls = hparams["num_parallel_calls"]
        dataset = dataset.map(
            lambda *args: chained_tran(dsutils.maybe_tuple(args)),
            num_parallel_calls=num_parallel_calls)

        # Truncates data count
        dataset = dataset.take(hparams["max_dataset_size"])

        return dataset, data_spec

</source>
<source file="systems/texar-0.2.4/texar/tf/data/data/tfrecord_data.py" startline="341" endline="354" pcid="349">
    def _process_dataset(self, dataset, hparams, data_spec):
        chained_tran, data_spec = self._make_processor(
            hparams["dataset"], data_spec,
            name_prefix=hparams["dataset"]["data_name"])
        num_parallel_calls = hparams["num_parallel_calls"]
        dataset = dataset.map(
            lambda *args: chained_tran(dsutils.maybe_tuple(args)),
            num_parallel_calls=num_parallel_calls)

        # Truncates data count
        dataset = dataset.take(hparams["max_dataset_size"])

        return dataset, data_spec

</source>
</class>

<class classid="11" nclones="2" nlines="18" similarity="70">
<source file="systems/texar-0.2.4/texar/tf/data/data/scalar_data.py" startline="206" endline="232" pcid="333">
    def _make_data(self):
        dataset_hparams = self._hparams.dataset

        # Create and shuffle dataset
        dataset = MonoTextData._make_mono_text_dataset(dataset_hparams)
        dataset, dataset_size = self._shuffle_dataset(
            dataset, self._hparams, self._hparams.dataset.files)
        self._dataset_size = dataset_size

        # Processing
        # pylint: disable=protected-access
        data_spec = dsutils._DataSpec(dataset=dataset,
                                      dataset_size=self._dataset_size)
        dataset, data_spec = self._process_dataset(dataset, self._hparams,
                                                   data_spec)
        self._data_spec = data_spec
        self._decoder = data_spec.decoder  # pylint: disable=no-member

        # Batching
        dataset = self._make_batch(dataset, self._hparams)

        # Prefetching
        if self._hparams.prefetch_buffer_size > 0:
            dataset = dataset.prefetch(self._hparams.prefetch_buffer_size)

        self._dataset = dataset

</source>
<source file="systems/texar-0.2.4/texar/tf/data/data/tfrecord_data.py" startline="355" endline="382" pcid="350">
    def _make_data(self):
        dataset = self._read_TFRecord_data()
        # Create and shuffle dataset
        num_shards = self._hparams.dataset.num_shards
        shard_id = self._hparams.dataset.shard_id
        if num_shards is not None and shard_id is not None:
            dataset = dataset.shard(num_shards, shard_id)
        dataset, dataset_size = self._shuffle_dataset(
            dataset, self._hparams, self._hparams.dataset.files)
        self._dataset_size = dataset_size

        # Processing
        # pylint: disable=protected-access
        data_spec = dsutils._DataSpec(dataset=dataset,
                                      dataset_size=self._dataset_size)
        dataset, data_spec = self._process_dataset(dataset, self._hparams,
                                                   data_spec)
        self._data_spec = data_spec
        self._decoder = data_spec.decoder  # pylint: disable=no-member
        # Batching
        dataset = self._make_batch(dataset, self._hparams)
        # Prefetching
        if self._hparams.prefetch_buffer_size > 0:
            dataset = dataset.prefetch(self._hparams.prefetch_buffer_size)

        self._dataset = dataset
        self.dataset = dataset

</source>
</class>

<class classid="12" nclones="4" nlines="38" similarity="75">
<source file="systems/texar-0.2.4/texar/tf/data/data/data_iterators_test.py" startline="105" endline="149" pcid="340">
    def test_iterator_multi_datasets(self):
        """Tests iterating over multiple datasets.
        """
        train_data = tx.data.MonoTextData(self._train_hparams)
        test_data = tx.data.MonoTextData(self._test_hparams)

        iterator = tx.data.DataIterator([train_data, test_data])
        data_batch = iterator.get_next()

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            sess.run(tf.local_variables_initializer())
            sess.run(tf.tables_initializer())

            for _ in range(2):
                # Iterates over train data
                iterator.switch_to_dataset(sess, train_data.name)
                i = 0
                while True:
                    try:
                        data_batch_ = sess.run(data_batch)
                        self.assertEqual(
                            tf.compat.as_text(data_batch_['text'][0][0]),
                            str(i + 1))
                        i = (i + 1) % 1000
                    except tf.errors.OutOfRangeError:
                        print('Train data limit reached')
                        self.assertEqual(i, 0)
                        break

                # Iterates over test data
                iterator.switch_to_dataset(sess, test_data.name)
                i = 1001
                while True:
                    try:
                        data_batch_ = sess.run(data_batch)
                        self.assertEqual(
                            tf.compat.as_text(data_batch_['text'][0][0]),
                            str(i))
                        i += 1
                    except tf.errors.OutOfRangeError:
                        print('Test data limit reached')
                        self.assertEqual(i, 2001)
                        break

</source>
<source file="systems/texar-0.2.4/texar/tf/data/data/data_iterators_test.py" startline="246" endline="296" pcid="343">
    def test_train_test_feedable_data_iterator(self):
        """Tests :class:`texar.tf.data.TrainTestFeedableDataIterator`
        """
        train_data = tx.data.MonoTextData(self._train_hparams)
        test_data = tx.data.MonoTextData(self._test_hparams)

        iterator = tx.data.TrainTestFeedableDataIterator(train=train_data,
                                                         test=test_data)
        data_batch = iterator.get_next()

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            sess.run(tf.local_variables_initializer())
            sess.run(tf.tables_initializer())

            for _ in range(2):
                iterator.restart_train_dataset(sess)
                i = 0
                while True:
                    try:
                        feed_dict = {
                            iterator.handle: iterator.get_train_handle(sess)
                        }
                        data_batch_ = sess.run(data_batch, feed_dict=feed_dict)
                        self.assertEqual(
                            tf.compat.as_text(data_batch_['text'][0][0]),
                            str(i + 1))
                        i = (i + 1) % 1000
                    except tf.errors.OutOfRangeError:
                        print('Train data limit reached')
                        self.assertEqual(i, 0)
                        break

                iterator.restart_test_dataset(sess)
                i = 1001
                while True:
                    try:
                        feed_dict = {
                            iterator.handle: iterator.get_test_handle(sess)
                        }
                        data_batch_ = sess.run(data_batch, feed_dict=feed_dict)
                        self.assertEqual(
                            tf.compat.as_text(data_batch_['text'][0][0]),
                            str(i))
                        i += 1
                    except tf.errors.OutOfRangeError:
                        print('Test data limit reached')
                        self.assertEqual(i, 2001)
                        break


</source>
<source file="systems/texar-0.2.4/texar/tf/data/data/data_iterators_test.py" startline="150" endline="193" pcid="341">
    def test_train_test_data_iterator(self):
        """Tests :class:`texar.tf.data.TrainTestDataIterator`
        """
        train_data = tx.data.MonoTextData(self._train_hparams)
        test_data = tx.data.MonoTextData(self._test_hparams)

        iterator = tx.data.TrainTestDataIterator(train=train_data,
                                                 test=test_data)
        data_batch = iterator.get_next()

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            sess.run(tf.local_variables_initializer())
            sess.run(tf.tables_initializer())

            for _ in range(2):
                iterator.switch_to_train_data(sess)
                i = 0
                while True:
                    try:
                        data_batch_ = sess.run(data_batch)
                        self.assertEqual(
                            tf.compat.as_text(data_batch_['text'][0][0]),
                            str(i + 1))
                        i = (i + 1) % 1000
                    except tf.errors.OutOfRangeError:
                        print('Train data limit reached')
                        self.assertEqual(i, 0)
                        break

                iterator.switch_to_test_data(sess)
                i = 1001
                while True:
                    try:
                        data_batch_ = sess.run(data_batch)
                        self.assertEqual(
                            tf.compat.as_text(data_batch_['text'][0][0]),
                            str(i))
                        i += 1
                    except tf.errors.OutOfRangeError:
                        print('Test data limit reached')
                        self.assertEqual(i, 2001)
                        break

</source>
<source file="systems/texar-0.2.4/texar/tf/data/data/data_iterators_test.py" startline="194" endline="245" pcid="342">
    def test_feedable_iterator_multi_datasets(self):
        """Tests iterating over multiple datasets with the
        :class:`FeedableDataIterator`.
        """
        train_data = tx.data.MonoTextData(self._train_hparams)
        test_data = tx.data.MonoTextData(self._test_hparams)

        iterator = tx.data.FeedableDataIterator([train_data, test_data])
        data_batch = iterator.get_next()

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            sess.run(tf.local_variables_initializer())
            sess.run(tf.tables_initializer())

            iterator.initialize_dataset(sess)

            for _ in range(2):
                # Iterates over train data
                iterator.restart_dataset(sess, train_data.name)
                data_handle = iterator.get_handle(sess, train_data.name)
                i = 0
                while True:
                    try:
                        feed_dict = {iterator.handle: data_handle}
                        data_batch_ = sess.run(data_batch, feed_dict=feed_dict)
                        self.assertEqual(
                            tf.compat.as_text(data_batch_['text'][0][0]),
                            str(i + 1))
                        i = (i + 1) % 1000
                    except tf.errors.OutOfRangeError:
                        print('Train data limit reached')
                        self.assertEqual(i, 0)
                        break

                # Iterates over test data
                iterator.restart_dataset(sess, test_data.name)
                data_handle = iterator.get_handle(sess, test_data.name)
                i = 1001
                while True:
                    try:
                        feed_dict = {iterator.handle: data_handle}
                        data_batch_ = sess.run(data_batch, feed_dict=feed_dict)
                        self.assertEqual(
                            tf.compat.as_text(data_batch_['text'][0][0]),
                            str(i))
                        i += 1
                    except tf.errors.OutOfRangeError:
                        print('Test data limit reached')
                        self.assertEqual(i, 2001)
                        break

</source>
</class>

<class classid="13" nclones="2" nlines="29" similarity="96">
<source file="systems/texar-0.2.4/texar/tf/data/data/paired_text_data_test.py" startline="29" endline="65" pcid="364">
    def setUp(self):
        tf.test.TestCase.setUp(self)

        # Create test data
        vocab_list = ['This', 'is', 'a', 'word', '词']
        vocab_file = tempfile.NamedTemporaryFile()
        vocab_file.write('\n'.join(vocab_list).encode("utf-8"))
        vocab_file.flush()
        self._vocab_file = vocab_file
        self._vocab_size = len(vocab_list)

        src_text = ['This is a sentence from source .', '词 词 。 source']
        src_text_file = tempfile.NamedTemporaryFile()
        src_text_file.write('\n'.join(src_text).encode("utf-8"))
        src_text_file.flush()
        self._src_text_file = src_text_file

        tgt_text = ['This is a sentence from target .', '词 词 。 target']
        tgt_text_file = tempfile.NamedTemporaryFile()
        tgt_text_file.write('\n'.join(tgt_text).encode("utf-8"))
        tgt_text_file.flush()
        self._tgt_text_file = tgt_text_file

        self._hparams = {
            "num_epochs": 50,
            "batch_size": 3,
            "source_dataset": {
                "files": [self._src_text_file.name],
                "vocab_file": self._vocab_file.name,
            },
            "target_dataset": {
                "files": self._tgt_text_file.name,
                "vocab_share": True,
                "eos_token": "<TARGET_EOS>"
            }
        }

</source>
<source file="systems/texar-0.2.4/texar/tf/run/executor_test.py" startline="25" endline="60" pcid="582">
    def setUp(self):
        tf.test.TestCase.setUp(self)

        # Create data
        vocab_list = ['This', 'is', 'a', 'word', '词']
        vocab_file = tempfile.NamedTemporaryFile()
        vocab_file.write('\n'.join(vocab_list).encode("utf-8"))
        vocab_file.flush()
        self._vocab_file = vocab_file
        self._vocab_size = len(vocab_list)

        src_text = ['This is a sentence from source .', '词 词 。 source']
        src_text_file = tempfile.NamedTemporaryFile()
        src_text_file.write('\n'.join(src_text).encode("utf-8"))
        src_text_file.flush()
        self._src_text_file = src_text_file

        tgt_text = ['This is a sentence from target .', '词 词 。 target']
        tgt_text_file = tempfile.NamedTemporaryFile()
        tgt_text_file.write('\n'.join(tgt_text).encode("utf-8"))
        tgt_text_file.flush()
        self._tgt_text_file = tgt_text_file

        self._data_hparams = {
            "num_epochs": 20,
            "batch_size": 2,
            "source_dataset": {
                "files": [self._src_text_file.name],
                "vocab_file": self._vocab_file.name,
            },
            "target_dataset": {
                "files": self._tgt_text_file.name,
                "vocab_share": True,
            }
        }

</source>
</class>

<class classid="14" nclones="3" nlines="27" similarity="72">
<source file="systems/texar-0.2.4/texar/tf/losses/mle_losses.py" startline="39" endline="120" pcid="506">
def sequence_softmax_cross_entropy(labels,
                                   logits,
                                   sequence_length,
                                   average_across_batch=True,
                                   average_across_timesteps=False,
                                   sum_over_batch=False,
                                   sum_over_timesteps=True,
                                   time_major=False,
                                   stop_gradient_to_label=False,
                                   name=None):
    """Computes softmax cross entropy for each time step of sequence
    predictions.

    Args:
        labels: Target class distributions.

            - If :attr:`time_major` is `False` (default), this must be a\
            Tensor of shape `[batch_size, max_time, num_classes]`.

            - If `time_major` is `True`, this must be a Tensor of shape\
            `[max_time, batch_size, num_classes]`.

            Each row of `labels` should be a valid probability
            distribution, otherwise, the computation of the gradient will be
            incorrect.
        logits: Unscaled log probabilities. This must have the shape of
            `[max_time, batch_size, num_classes]` or
            `[batch_size, max_time, num_classes]` according to
            the value of `time_major`.
        sequence_length: A Tensor of shape `[batch_size]`. Time steps beyond
            the respective sequence lengths will have zero losses.
        average_across_timesteps (bool): If set, average the loss across
            the time dimension. Must not set `average_across_timesteps`
            and `sum_over_timesteps` at the same time.
        average_across_batch (bool): If set, average the loss across the
            batch dimension. Must not set `average_across_batch`'
            and `sum_over_batch` at the same time.
        sum_over_timesteps (bool): If set, sum the loss across the
            time dimension. Must not set `average_across_timesteps`
            and `sum_over_timesteps` at the same time.
        sum_over_batch (bool): If set, sum the loss across the
            batch dimension. Must not set `average_across_batch`
            and `sum_over_batch` at the same time.
        time_major (bool): The shape format of the inputs. If `True`,
            :attr:`labels` and :attr:`logits` must have shape
            `[max_time, batch_size, ...]`. If `False`
            (default), they must have shape `[batch_size, max_time, ...]`.
        stop_gradient_to_label (bool): If set, gradient propagation to
            :attr:`labels` will be disabled.
        name (str, optional): A name for the operation.

    Returns:
        A Tensor containing the loss, of rank 0, 1, or 2 depending on the
        arguments :attr:`{average_across}/{sum_over}_{timesteps}/{batch}`.
        For example:

        - If :attr:`sum_over_timesteps` and :attr:`average_across_batch`  \
        are `True` (default), the return Tensor is of rank 0.

        - If :attr:`average_across_batch` is `True` and other arguments are \
        `False`, the return Tensor is of shape `[max_time]`.
    """
    with tf.name_scope(name, "sequence_softmax_cross_entropy"):
        if stop_gradient_to_label:
            labels = tf.stop_gradient(labels)

        losses = tf.nn.softmax_cross_entropy_with_logits_v2(
            labels=labels, logits=logits)

        losses = mask_and_reduce(
            losses,
            sequence_length,
            rank=2,
            average_across_batch=average_across_batch,
            average_across_timesteps=average_across_timesteps,
            sum_over_batch=sum_over_batch,
            sum_over_timesteps=sum_over_timesteps,
            time_major=time_major)

        return losses


</source>
<source file="systems/texar-0.2.4/texar/tf/losses/mle_losses.py" startline="211" endline="309" pcid="508">
def sequence_sigmoid_cross_entropy(labels,
                                   logits,
                                   sequence_length,
                                   average_across_batch=True,
                                   average_across_timesteps=False,
                                   average_across_classes=True,
                                   sum_over_batch=False,
                                   sum_over_timesteps=True,
                                   sum_over_classes=False,
                                   time_major=False,
                                   stop_gradient_to_label=False,
                                   name=None):
    """Computes sigmoid cross entropy for each time step of sequence
    predictions.

    Args:
        labels: Target class distributions.

            - If :attr:`time_major` is `False` (default), this must be a\
            Tensor of shape `[batch_size, max_time(, num_classes)]`.

            - If `time_major` is `True`, this must be a Tensor of shape\
            `[max_time, batch_size(, num_classes)]`.

            Each row of `labels` should be a valid probability
            distribution, otherwise, the computation of the gradient will be
            incorrect.
        logits: Unscaled log probabilities having the same shape as with
            :attr:`labels`.
        sequence_length: A Tensor of shape `[batch_size]`. Time steps beyond
            the respective sequence lengths will have zero losses.
        average_across_timesteps (bool): If set, average the loss across
            the time dimension. Must not set `average_across_timesteps`
            and `sum_over_timesteps` at the same time.
        average_across_batch (bool): If set, average the loss across the
            batch dimension. Must not set `average_across_batch`'
            and `sum_over_batch` at the same time.
        average_across_classes (bool): If set, average the loss across the
            class dimension (if exists). Must not set
            `average_across_classes`' and `sum_over_classes` at
            the same time. Ignored if :attr:`logits` is a 2D Tensor.
        sum_over_timesteps (bool): If set, sum the loss across the
            time dimension. Must not set `average_across_timesteps`
            and `sum_over_timesteps` at the same time.
        sum_over_batch (bool): If set, sum the loss across the
            batch dimension. Must not set `average_across_batch`
            and `sum_over_batch` at the same time.
        sum_over_classes (bool): If set, sum the loss across the
            class dimension. Must not set `average_across_classes`
            and `sum_over_classes` at the same time. Ignored if
            :attr:`logits` is a 2D Tensor.
        time_major (bool): The shape format of the inputs. If `True`,
            :attr:`labels` and :attr:`logits` must have shape
            `[max_time, batch_size, ...]`. If `False`
            (default), they must have shape `[batch_size, max_time, ...]`.
        stop_gradient_to_label (bool): If set, gradient propagation to
            :attr:`labels` will be disabled.
        name (str, optional): A name for the operation.

    Returns:
        A Tensor containing the loss, of rank 0, 1, or 2 depending on the
        arguments
        :attr:`{average_across}/{sum_over}_{timesteps}/{batch}/{classes}`.
        For example, if the class dimension does not exist, and

        - If :attr:`sum_over_timesteps` and :attr:`average_across_batch`  \
        are `True` (default), the return Tensor is of rank 0.

        - If :attr:`average_across_batch` is `True` and other arguments are \
        `False`, the return Tensor is of shape `[max_time]`.
    """

    with tf.name_scope(name, "sequence_sigmoid_cross_entropy"):
        if stop_gradient_to_label:
            labels = tf.stop_gradient(labels)

        losses = tf.nn.sigmoid_cross_entropy_with_logits(
            labels=labels, logits=logits)

        rank = shapes.get_rank(logits) or shapes.get_rank(labels)
        if rank is None:
            raise ValueError(
                'Cannot determine the rank of `logits` or `labels`.')

        losses = mask_and_reduce(
            losses,
            sequence_length,
            rank=rank,
            average_across_batch=average_across_batch,
            average_across_timesteps=average_across_timesteps,
            average_across_remaining=average_across_classes,
            sum_over_batch=sum_over_batch,
            sum_over_timesteps=sum_over_timesteps,
            sum_over_remaining=sum_over_classes,
            time_major=time_major)

        return losses


</source>
<source file="systems/texar-0.2.4/texar/tf/losses/mle_losses.py" startline="121" endline="210" pcid="507">
def sequence_sparse_softmax_cross_entropy(labels,
                                          logits,
                                          sequence_length,
                                          average_across_batch=True,
                                          average_across_timesteps=False,
                                          sum_over_batch=False,
                                          sum_over_timesteps=True,
                                          time_major=False,
                                          name=None):
    """Computes sparse softmax cross entropy for each time step of sequence
    predictions.

    Args:
        labels: Target class indexes. I.e., classes are mutually exclusive
            (each entry is in exactly one class).

            - If :attr:`time_major` is `False` (default), this must be\
            a Tensor of shape `[batch_size, max_time]`.

            - If `time_major` is `True`, this must be a Tensor of shape\
            `[max_time, batch_size].`
        logits: Unscaled log probabilities. This must have the shape of
            `[max_time, batch_size, num_classes]` or
            `[batch_size, max_time, num_classes]` according to
            the value of `time_major`.
        sequence_length: A Tensor of shape `[batch_size]`. Time steps beyond
            the respective sequence lengths will have zero losses.
        average_across_timesteps (bool): If set, average the loss across
            the time dimension. Must not set `average_across_timesteps`
            and `sum_over_timesteps` at the same time.
        average_across_batch (bool): If set, average the loss across the
            batch dimension. Must not set `average_across_batch`'
            and `sum_over_batch` at the same time.
        sum_over_timesteps (bool): If set, sum the loss across the
            time dimension. Must not set `average_across_timesteps`
            and `sum_over_timesteps` at the same time.
        sum_over_batch (bool): If set, sum the loss across the
            batch dimension. Must not set `average_across_batch`
            and `sum_over_batch` at the same time.
        time_major (bool): The shape format of the inputs. If `True`,
            :attr:`labels` and :attr:`logits` must have shape
            `[max_time, batch_size, ...]`. If `False`
            (default), they must have shape `[batch_size, max_time, ...]`.
        name (str, optional): A name for the operation.

    Returns:
        A Tensor containing the loss, of rank 0, 1, or 2 depending on the
        arguments :attr:`{average_across}/{sum_over}_{timesteps}/{batch}`.
        For example:

        - If :attr:`sum_over_timesteps` and :attr:`average_across_batch`  \
        are `True` (default), the return Tensor is of rank 0.

        - If :attr:`average_across_batch` is `True` and other arguments are \
        `False`, the return Tensor is of shape `[max_time]`.

    Example:

        .. code-block:: python

            embedder = WordEmbedder(vocab_size=data.vocab.size)
            decoder = BasicRNNDecoder(vocab_size=data.vocab.size)
            outputs, _, _ = decoder(
                decoding_strategy='train_greedy',
                inputs=embedder(data_batch['text_ids']),
                sequence_length=data_batch['length']-1)

            loss = sequence_sparse_softmax_cross_entropy(
                labels=data_batch['text_ids'][:, 1:],
                logits=outputs.logits,
                sequence_length=data_batch['length']-1)

    """
    with tf.name_scope(name, "sequence_sparse_softmax_cross_entropy"):
        losses = tf.nn.sparse_softmax_cross_entropy_with_logits(
            labels=labels, logits=logits)

        losses = mask_and_reduce(
            losses,
            sequence_length,
            rank=2,
            average_across_batch=average_across_batch,
            average_across_timesteps=average_across_timesteps,
            sum_over_batch=sum_over_batch,
            sum_over_timesteps=sum_over_timesteps,
            time_major=time_major)

        return losses


</source>
</class>

<class classid="15" nclones="4" nlines="22" similarity="72">
<source file="systems/texar-0.2.4/texar/tf/losses/rewards_test.py" startline="78" endline="105" pcid="524">
            np.testing.assert_array_almost_equal(
                discounted_reward_n, r_n, decimal=6)

    def test_discount_reward_py_1d(self):
        """Tests :func:`texar.tf.losses.rewards._discount_reward_py_1d`
        """
        reward = np.ones([2], dtype=np.float64)
        sequence_length = [3, 5]

        discounted_reward_1 = _discount_reward_py_1d(
            reward, sequence_length, discount=1.)

        discounted_reward_2 = _discount_reward_py_1d(
            reward, sequence_length, discount=.1)

        r = discounted_reward_1
        for i in range(5):
            if i < 3:
                self.assertEqual(r[0, i], 1)
            else:
                self.assertEqual(r[0, i], 0)
            self.assertEqual(r[1, i], 1)

        r = discounted_reward_2
        for i in range(5):
            if i < 3:
                self.assertAlmostEqual(r[0, i], 0.1**(2 - i))
            else:
</source>
<source file="systems/texar-0.2.4/texar/tf/losses/rewards_test.py" startline="106" endline="136" pcid="525">
                self.assertAlmostEqual(r[0, i], 0)
            self.assertAlmostEqual(r[1, i], 0.1**(4 - i))

    def test_discount_reward_tensor_1d(self):
        """Tests :func:`texar.tf.losses.rewards._discount_reward_tensor_1d`
        """
        reward = tf.ones([2], dtype=tf.float64)
        sequence_length = [3, 5]

        discounted_reward_1 = _discount_reward_tensor_1d(
            reward, sequence_length, discount=1.)

        discounted_reward_2 = _discount_reward_tensor_1d(
            reward, sequence_length, discount=.1)

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())

            r = sess.run(discounted_reward_1)
            for i in range(5):
                if i < 3:
                    self.assertEqual(r[0, i], 1)
                else:
                    self.assertEqual(r[0, i], 0)
                self.assertEqual(r[1, i], 1)

            r = sess.run(discounted_reward_2)
            for i in range(5):
                if i < 3:
                    self.assertAlmostEqual(r[0, i], 0.1**(2 - i))
                else:
</source>
<source file="systems/texar-0.2.4/texar/tf/losses/rewards_test.py" startline="137" endline="164" pcid="526">
                    self.assertAlmostEqual(r[0, i], 0)
                self.assertAlmostEqual(r[1, i], 0.1**(4 - i))

    def test_discount_reward_py_2d(self):
        """Tests :func:`texar.tf.losses.rewards._discount_reward_py_2d`
        """
        reward = np.ones([2, 10], dtype=np.float64)
        sequence_length = [5, 10]

        discounted_reward_1 = _discount_reward_py_2d(
            reward, sequence_length, discount=1.)

        discounted_reward_2 = _discount_reward_py_2d(
            reward, sequence_length, discount=.1)

        r = discounted_reward_1
        for i in range(10):
            if i < 5:
                self.assertEqual(r[0, i], 5 - i)
            else:
                self.assertEqual(r[0, i], 0)
            self.assertEqual(r[1, i], 10 - i)

        r = discounted_reward_2
        for i in range(10):
            if i < 5:
                self.assertEqual(r[0, i], int(11111. / 10**i) / 10**(4 - i))
            else:
</source>
<source file="systems/texar-0.2.4/texar/tf/losses/rewards_test.py" startline="165" endline="198" pcid="527">
                self.assertEqual(r[0, i], 0)
            self.assertEqual(r[1, i], int(1111111111. / 10**i) / 10**(9 - i))

    def test_discount_reward_tensor_2d(self):
        """Tests :func:`texar.tf.losses.rewards._discount_reward_tensor_2d`
        """
        reward = tf.ones([2, 10], dtype=tf.float64)
        sequence_length = [5, 10]

        discounted_reward_1 = _discount_reward_tensor_2d(
            reward, sequence_length, discount=1.)

        discounted_reward_2 = _discount_reward_tensor_2d(
            reward, sequence_length, discount=.1)

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())

            r = sess.run(discounted_reward_1)
            for i in range(10):
                if i < 5:
                    self.assertEqual(r[0, i], 5 - i)
                else:
                    self.assertEqual(r[0, i], 0)
                self.assertEqual(r[1, i], 10 - i)

            r = sess.run(discounted_reward_2)
            for i in range(10):
                if i < 5:
                    self.assertEqual(r[0, i],
                                     int(11111. / 10**i) / 10**(4 - i))
                else:
                    self.assertEqual(r[0, i], 0)
                self.assertEqual(r[1, i],
</source>
</class>

<class classid="16" nclones="2" nlines="13" similarity="100">
<source file="systems/texar-0.2.4/texar/tf/core/layers_test.py" startline="207" endline="224" pcid="540">
    def test_max_reduce_pooling_layer(self):
        """Tests :class:`texar.tf.core.MaxReducePooling1D`.
        """
        pool_layer = layers.MaxReducePooling1D()

        inputs = tf.random_uniform(
            [self._batch_size, self._seq_length, self._emb_dim])
        output_shape = pool_layer.compute_output_shape(inputs.get_shape())
        output = pool_layer(inputs)
        output_reduce = tf.reduce_max(inputs, axis=1)
        self.assertEqual(output.get_shape(), output_shape)
        self.assertEqual(output.get_shape(), [self._batch_size, self._emb_dim])

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            output_, output_reduce_ = sess.run([output, output_reduce])
            np.testing.assert_array_equal(output_, output_reduce_)

</source>
<source file="systems/texar-0.2.4/texar/tf/core/layers_test.py" startline="225" endline="243" pcid="541">
    def test_average_reduce_pooling_layer(self):
        """Tests :class:`texar.tf.core.AverageReducePooling1D`.
        """
        pool_layer = layers.AverageReducePooling1D()

        inputs = tf.random_uniform(
            [self._batch_size, self._seq_length, self._emb_dim])
        output_shape = pool_layer.compute_output_shape(inputs.get_shape())
        output = pool_layer(inputs)
        output_reduce = tf.reduce_mean(inputs, axis=1)
        self.assertEqual(output.get_shape(), output_shape)
        self.assertEqual(output.get_shape(), [self._batch_size, self._emb_dim])

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            output_, output_reduce_ = sess.run([output, output_reduce])
            np.testing.assert_array_equal(output_, output_reduce_)


</source>
</class>

<class classid="17" nclones="2" nlines="10" similarity="90">
<source file="systems/texar-0.2.4/texar/tf/run/executor.py" startline="98" endline="109" pcid="577">
    def _get_train_spec(self, max_steps=None):
        if 'train' not in self._data_hparams:
            raise ValueError('`data_hparams` must contain field `train` for '
                             'training data config.')
        input_fn = self._model.get_input_fn(
            mode=tf.estimator.ModeKeys.TRAIN,
            hparams=self._data_hparams['train'])
        return tf.estimator.TrainSpec(
            input_fn=input_fn,
            max_steps=max_steps,
            hooks=self._train_hooks)

</source>
<source file="systems/texar-0.2.4/texar/tf/run/executor.py" startline="110" endline="121" pcid="578">
    def _get_eval_spec(self, steps):
        if 'eval' not in self._data_hparams:
            raise ValueError('`data_hparams` must contain field `eval` for '
                             'evaluation data config.')
        input_fn = self._model.get_input_fn(
            mode=tf.estimator.ModeKeys.EVAL,
            hparams=self._data_hparams['eval'])
        return tf.estimator.EvalSpec(
            input_fn=input_fn,
            steps=steps,
            hooks=self._eval_hooks)

</source>
</class>

<class classid="18" nclones="2" nlines="12" similarity="75">
<source file="systems/texar-0.2.4/texar/tf/modules/decoders/rnn_decoders.py" startline="592" endline="606" pcid="626">
    def step(self, time, inputs, state, name=None):
        wrapper_outputs, wrapper_state = self._cell(inputs, state)
        # Essentisally the same as in BasicRNNDecoder.step()
        logits = self._output_layer(wrapper_outputs)
        sample_ids = self._helper.sample(
            time=time, outputs=logits, state=wrapper_state)

        attention_scores = wrapper_state.alignments
        attention_context = wrapper_state.attention
        outputs = AttentionRNNDecoderOutput(
            logits, sample_ids, wrapper_outputs,
            attention_scores, attention_context)

        return (outputs, wrapper_state)

</source>
<source file="systems/texar-0.2.4/examples/seq2seq_exposure_bias/interpolation_decoder.py" startline="97" endline="116" pcid="1201">

    def step(self, time, inputs, state, name=None):
        # Basicly the same as in AttentionRNNDecoder except considering
        # about the different form of 'state'(decoded_ids, rnn_state)

        wrapper_outputs, wrapper_state = self._cell(inputs, state[1])
        decoded_ids = state[0]

        logits = self._output_layer(wrapper_outputs)

        sample_ids = self._helper.sample(
            time=time, outputs=logits, state=[decoded_ids, wrapper_state])

        attention_scores = wrapper_state.alignments
        attention_context = wrapper_state.attention
        outputs = AttentionRNNDecoderOutput(
            logits, sample_ids, wrapper_outputs,
            attention_scores, attention_context)

        return (outputs, wrapper_state)
</source>
</class>

<class classid="19" nclones="2" nlines="13" similarity="92">
<source file="systems/texar-0.2.4/texar/tf/modules/decoders/tf_helpers.py" startline="688" endline="705" pcid="691">
    def sample(self, time, outputs, state, name=None):
        """Gets a sample for one step."""
        del time, state  # unused by sample_fn
        # Outputs are logits, we sample instead of argmax (greedy).
        if not isinstance(outputs, ops.Tensor):
            raise TypeError("Expected outputs to be a single Tensor, got: %s" %
                            type(outputs))
        if self._softmax_temperature is None:
            logits = outputs
        else:
            logits = outputs / self._softmax_temperature

        sample_id_sampler = tfpd.Categorical(logits=logits)
        sample_ids = sample_id_sampler.sample(seed=self._seed)

        return sample_ids


</source>
<source file="systems/texar-0.2.4/texar/tf/modules/decoders/rnn_decoder_helpers.py" startline="257" endline="276" pcid="781">

    def sample(self, time, outputs, state, name=None):
        """Gets a sample for one step."""
        del time, state  # unused by sample_fn
        # Outputs are logits, we sample from the top_k candidates
        if not isinstance(outputs, tf.Tensor):
            raise TypeError("Expected outputs to be a single Tensor, got: %s" %
                            type(outputs))
        if self._softmax_temperature is None:
            logits = outputs
        else:
            logits = outputs / self._softmax_temperature

        logits = _top_k_logits(logits, k=self._top_k)

        sample_id_sampler = tfpd.Categorical(logits=logits)
        sample_ids = sample_id_sampler.sample(seed=self._seed)

        return sample_ids

</source>
</class>

<class classid="20" nclones="2" nlines="46" similarity="73">
<source file="systems/texar-0.2.4/texar/tf/modules/decoders/rnn_decoder_helpers_test.py" startline="41" endline="110" pcid="703">

    def test_softmax_embedding_helpers(self):
        """Tests softmax helpers.
        """

        def _test_fn(helper):
            _, next_inputs, _ = helper.next_inputs(
                time=1,
                outputs=tf.ones([self._batch_size, self._vocab_size]),
                state=None,
                sample_ids=tf.ones([self._batch_size, self._vocab_size]))

            self.assertEqual(helper.sample_ids_shape,
                             tf.TensorShape(self._vocab_size))
            self.assertEqual(next_inputs.get_shape(),
                             tf.TensorShape([self._batch_size, self._emb_dim]))

            # Test in an RNN decoder
            output_layer = tf.layers.Dense(self._vocab_size)
            decoder = BasicRNNDecoder(vocab_size=self._vocab_size,
                                      output_layer=output_layer)
            outputs, final_state, sequence_lengths = decoder(
                helper=helper, max_decoding_length=self._max_seq_length)

            cell_dim = decoder.hparams.rnn_cell.kwargs.num_units
            with self.test_session() as sess:
                sess.run(tf.global_variables_initializer())
                outputs_, final_state_, sequence_lengths_ = sess.run(
                    [outputs, final_state, sequence_lengths])
                max_length = max(sequence_lengths_)
                self.assertEqual(
                    outputs_.logits.shape,
                    (self._batch_size, max_length, self._vocab_size))
                self.assertEqual(
                    outputs_.sample_id.shape,
                    (self._batch_size, max_length, self._vocab_size))
                self.assertEqual(final_state_[0].shape,
                                 (self._batch_size, cell_dim))

        # SoftmaxEmbeddingHelper

        # case-(1)
        helper = SoftmaxEmbeddingHelper(
            self._embedding, self._start_tokens, self._end_token, 0.7)
        _test_fn(helper)

        # case-(2)
        embedder = WordEmbedder(self._embedding)
        helper = SoftmaxEmbeddingHelper(
            embedder, self._start_tokens, self._end_token, 0.7,
            embedding_size=self._vocab_size)
        _test_fn(helper)

        # case-(3)
        word_embedder = WordEmbedder(self._embedding)
        pos_embedder = PositionEmbedder(position_size=self._max_seq_length)

        def _emb_fn(soft_ids, times):
            return word_embedder(soft_ids=soft_ids) + pos_embedder(times)
        helper = SoftmaxEmbeddingHelper(
            _emb_fn, self._start_tokens, self._end_token, 0.7,
            embedding_size=self._vocab_size)
        _test_fn(helper)

        # GumbelSoftmaxEmbeddingHelper

        # case-(1)
        helper = GumbelSoftmaxEmbeddingHelper(
            self._embedding, self._start_tokens, self._end_token, 0.7)
        _test_fn(helper)
</source>
<source file="systems/texar-0.2.4/texar/tf/modules/decoders/rnn_decoder_helpers_test.py" startline="111" endline="169" pcid="706">

    def test_infer_helpers(self):
        """Tests inference helpers.
        """

        def _test_fn(helper):
            _, next_inputs, _ = helper.next_inputs(
                time=1,
                outputs=tf.ones([self._batch_size, self._vocab_size]),
                state=None,
                sample_ids=tf.ones([self._batch_size], dtype=tf.int32))

            self.assertEqual(helper.sample_ids_shape,
                             tf.TensorShape([]))
            self.assertEqual(next_inputs.get_shape(),
                             tf.TensorShape([self._batch_size, self._emb_dim]))

            # Test in an RNN decoder
            output_layer = tf.layers.Dense(self._vocab_size)
            decoder = BasicRNNDecoder(vocab_size=self._vocab_size,
                                      output_layer=output_layer)
            outputs, final_state, sequence_lengths = decoder(
                helper=helper, max_decoding_length=self._max_seq_length)

            cell_dim = decoder.hparams.rnn_cell.kwargs.num_units
            with self.test_session() as sess:
                sess.run(tf.global_variables_initializer())
                outputs_, final_state_, sequence_lengths_ = sess.run(
                    [outputs, final_state, sequence_lengths])
                max_length = max(sequence_lengths_)
                self.assertEqual(
                    outputs_.logits.shape,
                    (self._batch_size, max_length, self._vocab_size))
                self.assertEqual(
                    outputs_.sample_id.shape, (self._batch_size, max_length))
                self.assertEqual(final_state_[0].shape,
                                 (self._batch_size, cell_dim))

        # case-(1)
        helper = GreedyEmbeddingHelper(
            self._embedding, self._start_tokens, self._end_token)
        _test_fn(helper)

        # case-(2)
        embedder = WordEmbedder(self._embedding)
        helper = GreedyEmbeddingHelper(
            embedder, self._start_tokens, self._end_token)
        _test_fn(helper)

        # case-(3)
        word_embedder = WordEmbedder(self._embedding)
        pos_embedder = PositionEmbedder(position_size=self._max_seq_length)

        def _emb_fn(ids, times):
            return word_embedder(ids) + pos_embedder(times)
        helper = GreedyEmbeddingHelper(
            _emb_fn, self._start_tokens, self._end_token)
        _test_fn(helper)

</source>
</class>

<class classid="21" nclones="2" nlines="56" similarity="96">
<source file="systems/texar-0.2.4/texar/tf/modules/decoders/gpt2_decoder.py" startline="96" endline="262" pcid="711">
    def default_hparams():
        r"""Returns a dictionary of hyperparameters with default values.

        * The decoder arch is determined by the constructor argument
          :attr:`pretrained_model_name` if it's specified. In this case,
          `hparams` are ignored.
        * Otherwise, the encoder arch is determined by
          `hparams['pretrained_model_name']` if it's specified. All other
          configurations in `hparams` are ignored.
        * If the above two are `None`, the decoder arch is defined by the
          configurations in `hparams` and weights are randomly initialized.

        .. code-block:: python

            {
                "name": "gpt2_decoder",
                "pretrained_model_name": "gpt2-small",
                "vocab_size": 50257,
                "context_size": 1024,
                "embedding_size": 768,
                "embed": {
                    "dim": 768,
                    "name": "word_embeddings"
                },
                "position_size": 1024,
                "position_embed": {
                    "dim": 768,
                    "name": "position_embeddings"
                },

                # hparams for TransformerDecoder
                "decoder": {
                    "dim": 768,
                    "num_blocks": 12,
                    "use_gpt_config": True,
                    "embedding_dropout": 0,
                    "residual_dropout": 0,
                    "multihead_attention": {
                        "use_bias": True,
                        "num_units": 768,
                        "num_heads": 12,
                        "dropout_rate": 0.0,
                        "output_dim": 768
                    },
                    "initializer": {
                        "type": "variance_scaling_initializer",
                        "kwargs": {
                            "factor": 1.0,
                            "mode": "FAN_AVG",
                            "uniform": True
                        }
                    },
                    "poswise_feedforward": {
                        "layers": [
                            {
                                "type": "Dense",
                                "kwargs": {
                                    "activation": "gelu",
                                    "name": "intermediate",
                                    "units": 3072,
                                    "use_bias": True
                                }
                            },
                            {
                                "type": "Dense",
                                "kwargs": {
                                    "activation": None,
                                    "name": "output",
                                    "units": 3072,
                                    "use_bias": True
                                }
                            }
                        ],
                        "name": "ffn"
                    }
                },
                "name": "gpt2_decoder",
            }

        Here:

        The default parameters are values for 124M GPT2 model.

        `"pretrained_model_name"`: str or None
            The name of the pre-trained GPT2 model. If None, the model
            will be randomly initialized.

        `"embed"`: dict
            Hyperparameters for word embedding layer.

        `"vocab_size"`: int
            The vocabulary size of `inputs` in `GPT2Model`.

        `"position_embed"`: dict
            Hyperparameters for position embedding layer.

        `"position_size"`:  int
            The maximum sequence length that this model might ever be used with.

        `"name"`: str
            Name of the module.
        """
        return {
            'decoder': {
                'name': 'decoder',
                'dim': 768,
                'num_blocks': 12,
                'embedding_dropout': 0,
                'residual_dropout': 0,
                'multihead_attention': {
                    'name': 'self',
                    'use_bias': True,
                    'num_units': 768,
                    'num_heads': 12,
                    "dropout_rate": 0.0,
                    'output_dim': 768
                },
                'initializer': {
                    'type': 'variance_scaling_initializer',
                    'kwargs': {
                        'factor': 1.0,
                        'mode': 'FAN_AVG',
                        'uniform': True
                    }
                },
                'poswise_feedforward': {
                    'layers': [
                        {
                            'type': 'Dense',
                            'kwargs': {
                                'activation': 'gelu',
                                'name': 'intermediate',
                                'units': 3072,
                                'use_bias': True
                            }
                        },
                        {
                            'type': 'Dense',
                            'kwargs': {
                                'activation': None,
                                'name': 'output',
                                'units': 768,
                                'use_bias': True
                            }
                        }
                    ],
                    'name': 'ffn',
                },
            },

            'pretrained_model_name': 'gpt2-small',
            'vocab_size': 50257,
            'context_size': 1024,
            'embedding_size': 768,
            'embed': {
                'dim': 768,
                'name': 'word_embeddings'
            },
            'position_size': 1024,
            'position_embed': {
                'dim': 768,
                'name': 'position_embeddings'
            },
            'name': 'gpt2_decoder',
            '@no_typecheck': ['pretrained_model_name'],
        }

</source>
<source file="systems/texar-0.2.4/texar/tf/modules/encoders/gpt2_encoder.py" startline="93" endline="268" pcid="844">
    def default_hparams():
        r"""Returns a dictionary of hyperparameters with default values.

        * The encoder arch is determined by the constructor argument
          :attr:`pretrained_model_name` if it's specified. In this case,
          `hparams` are ignored.
        * Otherwise, the encoder arch is determined by
          `hparams['pretrained_model_name']` if it's specified. All other
          configurations in `hparams` are ignored.
        * If the above two are `None`, the encoder arch is defined by the
          configurations in `hparams` and weights are randomly initialized.

        .. code-block:: python

            {
                "pretrained_model_name": "gpt2-small",
                "vocab_size": 50257,
                "context_size": 1024,
                "embedding_size": 768,
                "embed": {
                    "dim": 768,
                    "name": "word_embeddings"
                },
                "position_size": 1024,
                "position_embed": {
                    "dim": 768,
                    "name": "position_embeddings"
                },

                "encoder": {
                    "dim": 768,
                    "num_blocks": 12,
                    "use_gpt_config": True,
                    "embedding_dropout": 0,
                    "residual_dropout": 0,
                    "multihead_attention": {
                        "use_bias": True,
                        "num_units": 768,
                        "num_heads": 12,
                        "output_dim": 768
                    },
                    "initializer": {
                        "type": "variance_scaling_initializer",
                        "kwargs": {
                            "factor": 1.0,
                            "mode": "FAN_AVG",
                            "uniform": True
                        }
                    },
                    "poswise_feedforward": {
                        "layers": [
                            {
                                "type": "Dense",
                                "kwargs": {
                                    "activation": "gelu",
                                    "name": "intermediate",
                                    "units": 3072,
                                    "use_bias": True
                                }
                            },
                            {
                                "type": "Dense",
                                "kwargs": {
                                    "activation": None,
                                    "name": "output",
                                    "units": 3072,
                                    "use_bias": True
                                }
                            }
                        ],
                        "name": "ffn"
                    }
                },
                "initializer": None,
                "name": "gpt2_encoder",
            }

        Here:

        The default parameters are values for 124M GPT2 model.

        `"pretrained_model_name"`: str or None
            The name of the pre-trained GPT2 model. If None, the model
            will be randomly initialized.

        `"embed"`: dict
            Hyperparameters for word embedding layer.

        `"vocab_size"`: int
            The vocabulary size of `inputs` in `GPT2Model`.

        `"position_embed"`: dict
            Hyperparameters for position embedding layer.

        `"position_size"`:  int
            The maximum sequence length that this model might ever be used with.

        `"decoder"`: dict
            Hyperparameters for the TransformerDecoder.
            See :func:`~texar.torch.modules.TransformerDecoder.default_hparams`
            for details.

        `"initializer"`: dict, optional
            Hyperparameters of the default initializer that initializes
            variables created in this module.
            See :func:`~texar.torch.core.get_initializer` for details.

        `"name"`: str
            Name of the module.
        """
        return {
            'encoder': {
                'name': 'encoder',
                'dim': 768,
                'num_blocks': 12,
                'use_bert_config': False,
                'embedding_dropout': 0,
                'residual_dropout': 0,
                'multihead_attention': {
                    'name': 'self',
                    'use_bias': True,
                    'num_units': 768,
                    'num_heads': 12,
                    'output_dim': 768
                },
                'initializer': {
                    'type': 'variance_scaling_initializer',
                    'kwargs': {
                        'factor': 1.0,
                        'mode': 'FAN_AVG',
                        'uniform': True
                    }
                },
                'poswise_feedforward': {
                    'layers': [
                        {
                            'type': 'Dense',
                            'kwargs': {
                                'activation': 'gelu',
                                'name': 'intermediate',
                                'units': 3072,
                                'use_bias': True
                            }
                        },
                        {
                            'type': 'Dense',
                            'kwargs': {
                                'activation': None,
                                'name': 'output',
                                'units': 768,
                                'use_bias': True
                            }
                        }
                    ],
                    'name': 'ffn',
                },
            },

            'pretrained_model_name': 'gpt2-small',
            'vocab_size': 50257,
            'context_size': 1024,
            'embedding_size': 768,
            'embed': {
                'dim': 768,
                'name': 'word_embeddings'
            },
            'position_size': 1024,
            'position_embed': {
                'dim': 768,
                'name': 'position_embeddings'
            },
            'initializer': None,
            'name': 'gpt2_encoder',
            '@no_typecheck': ['pretrained_model_name'],
        }

</source>
</class>

<class classid="22" nclones="2" nlines="28" similarity="73">
<source file="systems/texar-0.2.4/texar/tf/modules/decoders/gpt2_decoder_test.py" startline="60" endline="101" pcid="714">
    @pretrained_test
    def test_trainable_variables(self):
        r"""Tests the functionality of automatically collecting trainable
        variables.
        """

        inputs = tf.placeholder(dtype=tf.int32, shape=[2, 3])

        def get_variable_num(n_layers: int) -> int:
            return 1 + 1 + n_layers * 16 + 2

        # case 1: GPT2 small
        decoder = GPT2Decoder()
        _ = decoder(inputs=inputs)
        self.assertEqual(len(decoder.trainable_variables), get_variable_num(12))

        # case 2: GPT2 medium
        hparams = {
            "pretrained_model_name": "gpt2-medium",
        }
        decoder = GPT2Decoder(hparams=hparams)
        _ = decoder(inputs=inputs)
        self.assertEqual(len(decoder.trainable_variables), get_variable_num(24))

        # case 2: GPT2 large
        hparams = {
            "pretrained_model_name": "gpt2-large",
        }
        decoder = GPT2Decoder(hparams=hparams)
        _ = decoder(inputs=inputs)
        self.assertEqual(len(decoder.trainable_variables), get_variable_num(36))

        # case 3: self-designed GPT2
        hparams = {
            "pretrained_model_name": None,
            "decoder": {
                "num_blocks": 6,
            }
        }
        decoder = GPT2Decoder(hparams=hparams)
        _ = decoder(inputs=inputs)
        self.assertEqual(len(decoder.trainable_variables), get_variable_num(6))
</source>
<source file="systems/texar-0.2.4/texar/tf/modules/encoders/gpt2_encoder_test.py" startline="69" endline="110" pcid="835">
    def test_trainable_variables(self):
        r"""Tests the functionality of automatically collecting trainable
        variables.
        """

        inputs = tf.placeholder(dtype=tf.int32, shape=[None, None])

        def get_variable_num(n_layers: int) -> int:
            return 1 + 1 + n_layers * 16 + 2

        # case 1: GPT2 small
        encoder = GPT2Encoder()
        _ = encoder(inputs)
        self.assertEqual(len(encoder.trainable_variables), get_variable_num(12))

        # case 2: GPT2 medium
        hparams = {
            "pretrained_model_name": "gpt2-medium",
        }
        encoder = GPT2Encoder(hparams=hparams)
        _ = encoder(inputs)
        self.assertEqual(len(encoder.trainable_variables), get_variable_num(24))

        # case 3: GPT2 large
        hparams = {
            "pretrained_model_name": "gpt2-large",
        }
        encoder = GPT2Encoder(hparams=hparams)
        _ = encoder(inputs)
        self.assertEqual(len(encoder.trainable_variables), get_variable_num(36))

        # case 4: self-designed GPT2
        hparams = {
            "pretrained_model_name": None,
            "encoder": {
                "num_blocks": 6
            },
        }
        encoder = GPT2Encoder(hparams=hparams)
        _ = encoder(inputs)
        self.assertEqual(len(encoder.trainable_variables), get_variable_num(6))

</source>
</class>

<class classid="23" nclones="2" nlines="16" similarity="75">
<source file="systems/texar-0.2.4/texar/tf/modules/decoders/gpt2_decoder_test.py" startline="102" endline="122" pcid="716">

    def test_decode_train(self):
        r"""Tests train_greedy.
        """
        hparams = {
            "pretrained_model_name": None
        }
        decoder = GPT2Decoder(hparams=hparams)

        max_time = 8
        batch_size = 16
        inputs = tf.random_uniform([batch_size, max_time],
                                   maxval=50257, dtype=tf.int32)
        outputs = decoder(inputs=inputs)

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            outputs_ = sess.run(outputs)
            self.assertEqual(outputs_.logits.shape, (batch_size,
                                                     max_time, 50257))
            self.assertEqual(outputs_.sample_id.shape, (batch_size, max_time))
</source>
<source file="systems/texar-0.2.4/texar/tf/modules/encoders/gpt2_encoder_test.py" startline="111" endline="133" pcid="837">
    def test_encode(self):
        r"""Tests encoding.
        """
        # case 1: GPT2 small
        hparams = {
            "pretrained_model_name": None
        }
        encoder = GPT2Encoder(hparams=hparams)

        max_time = 8
        batch_size = 16
        inputs = tf.random_uniform([batch_size, max_time],
                                   maxval=30521, dtype=tf.int32)
        outputs = encoder(inputs)

        outputs_dim = encoder.hparams.encoder.dim
        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            outputs_ = sess.run(outputs)
            self.assertEqual(outputs_.shape, (batch_size,
                                              max_time, outputs_dim))


</source>
</class>

<class classid="24" nclones="2" nlines="20" similarity="70">
<source file="systems/texar-0.2.4/texar/tf/modules/decoders/rnn_decoder_base.py" startline="112" endline="138" pcid="721">
    def __init__(self,
                 cell=None,
                 vocab_size=None,
                 output_layer=None,
                 cell_dropout_mode=None,
                 hparams=None):
        ModuleBase.__init__(self, hparams)

        self._helper = None
        self._initial_state = None

        # Make rnn cell
        with tf.variable_scope(self.variable_scope):
            if cell is not None:
                self._cell = cell
            else:
                self._cell = layers.get_rnn_cell(
                    self._hparams.rnn_cell, cell_dropout_mode)
        self._beam_search_cell = None

        # Make the output layer
        self._output_layer, self._vocab_size = _make_output_layer(
            output_layer, vocab_size, self._hparams.output_layer_bias,
            self.variable_scope)

        self.max_decoding_length = None

</source>
<source file="systems/texar-0.2.4/texar/tf/modules/encoders/rnn_encoders.py" startline="286" endline="310" pcid="855">
    def __init__(self,
                 cell=None,
                 cell_dropout_mode=None,
                 output_layer=None,
                 hparams=None):
        RNNEncoderBase.__init__(self, hparams)

        # Make RNN cell
        with tf.variable_scope(self.variable_scope):
            if cell is not None:
                self._cell = cell
            else:
                self._cell = layers.get_rnn_cell(
                    self._hparams.rnn_cell, cell_dropout_mode)

        # Make output layer
        with tf.variable_scope(self.variable_scope):
            if output_layer is not None:
                self._output_layer = output_layer
                self._output_layer_hparams = None
            else:
                self._output_layer = _build_dense_output_layer(
                    self._hparams.output_layer)
                self._output_layer_hparams = self._hparams.output_layer

</source>
</class>

<class classid="25" nclones="2" nlines="20" similarity="90">
<source file="systems/texar-0.2.4/texar/tf/modules/decoders/transformer_decoders.py" startline="158" endline="258" pcid="738">
            self._cache = None
            self.max_decoding_length = None

    @staticmethod
    def default_hparams():
        """Returns a dictionary of hyperparameters with default values.

        .. code-block:: python

            {
                # Same as in TransformerEncoder
                "num_blocks": 6,
                "dim": 512,
                "embedding_dropout": 0.1,
                "residual_dropout": 0.1,
                "poswise_feedforward": default_transformer_poswise_net_hparams,
                "multihead_attention": {
                    'name': 'multihead_attention',
                    'num_units': 512,
                    'output_dim': 512,
                    'num_heads': 8,
                    'dropout_rate': 0.1,
                    'output_dim': 512,
                    'use_bias': False,
                },
                "initializer": None,
                "name": "transformer_decoder"
                # Additional for TransformerDecoder
                "embedding_tie": True,
                "output_layer_bias": False,
                "max_decoding_length": int(1e10),
            }

        Here:

        "num_blocks": int
            Number of stacked blocks.

        "dim": int
            Hidden dimension of the encoder.

        "embedding_dropout": float
            Dropout rate of the input word and position embeddings.

        "residual_dropout":  float
            Dropout rate of the residual connections.

        "poswise_feedforward": dict
            Hyperparameters for a feed-forward network used in residual
            connections.
            Make sure the dimension of the output tensor is equal to `dim`.

            See :func:`~texar.tf.modules.default_transformer_poswise_net_hparams`
            for details.

        "multihead_attention": dict
            Hyperparameters for the multihead attention strategy.
            Make sure the `output_dim` in this module is equal to `dim`.

            See :func:`~texar.tf.modules.MultiheadAttentionEncoder.default_hparams`
            for details.

        "initializer": dict, optional
            Hyperparameters of the default initializer that initializes
            variables created in this module.
            See :func:`~texar.tf.core.get_initializer` for details.

        "output_layer_bias": bool
            Whether to use bias to the output layer.
            Used only if :attr:`output_layer` is `None` when constructing
            the class instance.

        "max_decoding_length": int
            The maximum allowed number of decoding steps.
            Set to a very large number of avoid the length constraint.
            Ignored if provided in :meth:`_build` or
            "train_greedy" decoding is used.

            Length penalty coefficient. Refer to
            https://arxiv.org/abs/1609.08144 for more details.

        "name": str
            Name of the module.
        """
        return {
            "num_blocks": 6,
            "dim": 512,
            "embedding_tie": True,
            "output_layer_bias": False,
            "max_decoding_length": int(1e10),
            "embedding_dropout": 0.1,
            "residual_dropout": 0.1,
            "poswise_feedforward": default_transformer_poswise_net_hparams(),
            'multihead_attention': {
                'name': 'multihead_attention',
                'num_units': 512,
                'num_heads': 8,
                'dropout_rate': 0.1,
                'output_dim': 512,
                'use_bias': False,
            },
</source>
<source file="systems/texar-0.2.4/texar/tf/modules/encoders/transformer_encoders.py" startline="172" endline="271" pcid="815">
    def default_hparams():
        """Returns a dictionary of hyperparameters with default values.

        .. code-block:: python

            {
                "num_blocks": 6,
                "dim": 512,
                'use_bert_config': False,
                "embedding_dropout": 0.1,
                "residual_dropout": 0.1,
                "poswise_feedforward": default_transformer_poswise_net_hparams,
                'multihead_attention': {
                    'name': 'multihead_attention',
                    'num_units': 512,
                    'output_dim': 512,
                    'num_heads': 8,
                    'dropout_rate': 0.1,
                    'output_dim': 512,
                    'use_bias': False,
                },
                "initializer": None,
                "name": "transformer_encoder"
            }

        Here:

        "num_blocks": int
            Number of stacked blocks.

        "dim": int
            Hidden dimension of the encoders.

        "use_bert_config": bool
            If `False`, apply the standard Transformer Encoder architecture from
            the original paper `(Vaswani et al.) "Attention is All You Need"`.
            If `True`, apply the Transformer Encoder architecture used in BERT
            `(Devlin et al.)`.

            The differences lie in:

                1. The standard arch restricts the word embedding of PAD token \
                   to all zero. The BERT arch does not.

                2. The attention bias for padding tokens: \
                   The standard arch uses `-1e8` for nagative attention mask. \
                   BERT uses `-1e4` instead.

                3. The residual connections between internal tensors: \
                   In BERT, a residual layer connects the tensors *after* \
                   layer normalization. In the standard arch, the tensors are \
                   connected *before* layer normalization.

        "embedding_dropout": float
            Dropout rate of the input embedding.

        "residual_dropout":  float
            Dropout rate of the residual connections.

        "poswise_feedforward": dict
            Hyperparameters for a feed-forward network used in residual
            connections.
            Make sure the dimension of the output tensor is equal to `dim`.

            See :func:`~texar.tf.modules.default_transformer_poswise_net_hparams`
            for details.

        "multihead_attention": dict
            Hyperparameters for the multihead attention strategy.
            Make sure the "output_dim" in this module is equal to "dim".
            See :func:`~texar.tf.modules.MultiheadAttentionEncoder.default_harams`
            for details.

        "initializer": dict, optional
            Hyperparameters of the default initializer that initializes
            variables created in this module.
            See :func:`~texar.tf.core.get_initializer` for details.

        "name": str
            Name of the module.
        """
        return {
            'num_blocks': 6,
            'dim': 512,
            'use_bert_config': False,
            'embedding_dropout': 0.1,
            'residual_dropout': 0.1,
            'poswise_feedforward': default_transformer_poswise_net_hparams(),
            'multihead_attention': {
                'name': 'multihead_attention',
                'num_units': 512,
                'num_heads': 8,
                'dropout_rate': 0.1,
                'output_dim': 512,
                'use_bias': False,
            },
            'initializer': None,
            'name': 'transformer_encoder',
        }

</source>
</class>

<class classid="26" nclones="2" nlines="11" similarity="100">
<source file="systems/texar-0.2.4/texar/tf/modules/decoders/rnn_decoders_test.py" startline="67" endline="81" pcid="760">
    def test_output_layer(self):
        decoder = BasicRNNDecoder(vocab_size=self._vocab_size,
                                  output_layer=None)
        self.assertIsInstance(decoder, BasicRNNDecoder)

        decoder = BasicRNNDecoder(output_layer=tf.identity)
        self.assertIsInstance(decoder, BasicRNNDecoder)

        tensor = tf.random_uniform(
            [self._emb_dim, self._vocab_size], maxval=1, dtype=tf.float32
        )
        decoder = BasicRNNDecoder(output_layer=tensor)
        self.assertIsInstance(decoder, BasicRNNDecoder)
        self.assertEqual(decoder.vocab_size, self._vocab_size)

</source>
<source file="systems/texar-0.2.4/texar/tf/modules/decoders/transformer_decoders_test.py" startline="64" endline="78" pcid="793">
    def test_output_layer(self):
        decoder = TransformerDecoder(vocab_size=self._vocab_size,
                                     output_layer=None)
        self.assertIsInstance(decoder, TransformerDecoder)

        decoder = TransformerDecoder(output_layer=tf.identity)
        self.assertIsInstance(decoder, TransformerDecoder)

        tensor = tf.random_uniform(
            [self._emb_dim, self._vocab_size], maxval=1, dtype=tf.float32
        )
        decoder = TransformerDecoder(output_layer=tensor)
        self.assertIsInstance(decoder, TransformerDecoder)
        self.assertEqual(decoder.vocab_size, self._vocab_size)

</source>
</class>

<class classid="27" nclones="2" nlines="38" similarity="76">
<source file="systems/texar-0.2.4/texar/tf/modules/decoders/rnn_decoders_test.py" startline="238" endline="289" pcid="765">

    def test_decode_train(self):
        """Tests decoding in training mode.
        """
        seq_length = np.random.randint(
            self._max_time, size=[self._batch_size]) + 1
        encoder_values_length = tf.constant(seq_length)
        hparams = {
            "attention": {
                "kwargs": {
                    "num_units": self._attention_dim,
                    # Note: to use sparsemax in TF-CPU, it looks
                    # `memory_sequence_length` must equal max_time.
                    # "probability_fn": "sparsemax"
                }
            }
        }
        decoder = AttentionRNNDecoder(
            memory=self._encoder_output,
            memory_sequence_length=encoder_values_length,
            vocab_size=self._vocab_size,
            hparams=hparams)

        helper_train = get_helper(
            decoder.hparams.helper_train.type,
            inputs=self._inputs,
            sequence_length=[self._max_time] * self._batch_size,
            **decoder.hparams.helper_train.kwargs.todict())

        outputs, final_state, sequence_lengths = decoder(helper=helper_train)
        # 4+1 trainable variables: cell-kernel, cell-bias,
        # fc-weight, fc-bias, and
        # memory_layer: For LuongAttention, we only transform the memory layer;
        # thus num_units *must* match the expected query depth.
        self.assertEqual(len(decoder.trainable_variables), 5)

        cell_dim = decoder.hparams.rnn_cell.kwargs.num_units
        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            outputs_, final_state_, sequence_lengths_ = sess.run(
                [outputs, final_state, sequence_lengths],
                feed_dict={context.global_mode(): tf.estimator.ModeKeys.TRAIN})
            self.assertIsInstance(outputs_, AttentionRNNDecoderOutput)
            self.assertEqual(
                outputs_.logits.shape,
                (self._batch_size, self._max_time, self._vocab_size))
            self.assertEqual(
                outputs_.sample_id.shape, (self._batch_size, self._max_time))
            self.assertEqual(final_state_.cell_state[0].shape,
                             (self._batch_size, cell_dim))
            np.testing.assert_array_equal(
                sequence_lengths_, [self._max_time] * self._batch_size)
</source>
<source file="systems/texar-0.2.4/texar/tf/modules/decoders/rnn_decoders_test.py" startline="290" endline="339" pcid="766">

    def test_decode_infer(self):
        """Tests decoding in inference mode.
        """
        seq_length = np.random.randint(
            self._max_time, size=[self._batch_size]) + 1
        encoder_values_length = tf.constant(seq_length)
        hparams = {
            "attention": {
                "kwargs": {
                    "num_units": 256,
                }
            }
        }
        decoder = AttentionRNNDecoder(
            vocab_size=self._vocab_size,
            memory=self._encoder_output,
            memory_sequence_length=encoder_values_length,
            hparams=hparams)

        helper_infer = get_helper(
            decoder.hparams.helper_infer.type,
            embedding=self._embedding,
            start_tokens=[1] * self._batch_size,
            end_token=2,
            **decoder.hparams.helper_train.kwargs.todict())

        outputs, final_state, sequence_lengths = decoder(helper=helper_infer)

        # 4+1 trainable variables: cell-kernel, cell-bias,
        # fc-weight, fc-bias, and
        # memory_layer: For LuongAttention, we only transform the memory layer;
        # thus num_units *must* match the expected query depth.
        self.assertEqual(len(decoder.trainable_variables), 5)
        cell_dim = decoder.hparams.rnn_cell.kwargs.num_units
        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            outputs_, final_state_, sequence_lengths_ = sess.run(
                [outputs, final_state, sequence_lengths],
                feed_dict={context.global_mode():
                           tf.estimator.ModeKeys.PREDICT})
            self.assertIsInstance(outputs_, AttentionRNNDecoderOutput)
            max_length = max(sequence_lengths_)
            self.assertEqual(
                outputs_.logits.shape,
                (self._batch_size, max_length, self._vocab_size))
            self.assertEqual(
                outputs_.sample_id.shape, (self._batch_size, max_length))
            self.assertEqual(final_state_.cell_state[0].shape,
                             (self._batch_size, cell_dim))
</source>
</class>

<class classid="28" nclones="2" nlines="18" similarity="70">
<source file="systems/texar-0.2.4/texar/tf/modules/decoders/beam_search_decode_test.py" startline="182" endline="203" pcid="772">
    def test_attention_decoder_beam_search(self):
        """Tests beam search with RNNAttentionDecoder.
        """
        seq_length = np.random.randint(
            self._max_time, size=[self._batch_size]) + 1
        encoder_values_length = tf.constant(seq_length)
        hparams = {
            "attention": {
                "kwargs": {"num_units": self._attention_dim}
            },
            "rnn_cell": {
                "kwargs": {"num_units": self._cell_dim}
            }
        }
        decoder = tx.modules.AttentionRNNDecoder(
            vocab_size=self._vocab_size,
            memory=self._encoder_output,
            memory_sequence_length=encoder_values_length,
            hparams=hparams)

        self._test_beam_search(decoder)

</source>
<source file="systems/texar-0.2.4/texar/tf/modules/decoders/beam_search_decode_test.py" startline="204" endline="233" pcid="773">
    def test_attention_decoder_given_initial_state(self):
        """Tests beam search with RNNAttentionDecoder given initial state.
        """
        seq_length = np.random.randint(
            self._max_time, size=[self._batch_size]) + 1
        encoder_values_length = tf.constant(seq_length)
        hparams = {
            "attention": {
                "kwargs": {"num_units": self._attention_dim}
            },
            "rnn_cell": {
                "kwargs": {"num_units": self._cell_dim}
            }
        }
        decoder = tx.modules.AttentionRNNDecoder(
            vocab_size=self._vocab_size,
            memory=self._encoder_output,
            memory_sequence_length=encoder_values_length,
            hparams=hparams)

        state = decoder.cell.zero_state(self._batch_size, tf.float32)

        cell_state = state.cell_state
        self._test_beam_search(decoder, initial_state=cell_state)

        tiled_cell_state = tile_batch(cell_state, multiplier=self._beam_width)
        self._test_beam_search(
            decoder, tiled_initial_state=tiled_cell_state, initiated=True)


</source>
</class>

<class classid="29" nclones="5" nlines="17" similarity="70">
<source file="systems/texar-0.2.4/texar/tf/modules/decoders/transformer_decoders_test.py" startline="79" endline="103" pcid="794">
    def test_decode_train(self):
        """Tests train_greedy
        """
        decoder = TransformerDecoder(
            vocab_size=self._vocab_size,
            output_layer=self._output_layer
        )
        # 6 blocks
        # -self multihead_attention: 4 dense without bias + 2 layer norm vars
        # -encdec multihead_attention: 4 dense without bias + 2 layer norm vars
        # -poswise_network: Dense with bias, Dense with bias + 2 layer norm vars
        # 2 layer norm vars
        outputs = decoder(memory=self._memory,
                          memory_sequence_length=self._memory_sequence_length,
                          memory_attention_bias=None,
                          inputs=self._inputs,
                          decoding_strategy='train_greedy',
                          mode=tf.estimator.ModeKeys.TRAIN)
        self.assertEqual(len(decoder.trainable_variables), 110)
        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            outputs_ = sess.run(outputs)

            self.assertIsInstance(outputs_, TransformerDecoderOutput)

</source>
<source file="systems/texar-0.2.4/texar/tf/modules/decoders/transformer_decoders_test.py" startline="206" endline="227" pcid="799">
    def test_greedy_embedding_helper(self):
        """Tests with tf.contrib.seq2seq.GreedyEmbeddingHelper
        """
        decoder = TransformerDecoder(
            vocab_size=self._vocab_size,
            output_layer=self._output_layer
        )
        helper = tx_helper.GreedyEmbeddingHelper(
            self._embedding, self._start_tokens, self._end_token)
        outputs, length = decoder(
            memory=self._memory,
            memory_sequence_length=self._memory_sequence_length,
            memory_attention_bias=None,
            helper=helper,
            max_decoding_length=self._max_decode_len,
            mode=tf.estimator.ModeKeys.PREDICT)
        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            outputs_ = sess.run(outputs)
            self.assertIsInstance(outputs_, TransformerDecoderOutput)


</source>
<source file="systems/texar-0.2.4/texar/tf/modules/decoders/transformer_decoders_test.py" startline="104" endline="126" pcid="795">
    def test_decode_infer_greedy(self):
        """Tests train_greedy
        """
        decoder = TransformerDecoder(
            vocab_size=self._vocab_size,
            output_layer=self._output_layer
        )
        helper = tx_helper.GreedyEmbeddingHelper(
            self._embedding_fn, self._start_tokens, self._end_token)

        outputs, length = decoder(
            memory=self._memory,
            memory_sequence_length=self._memory_sequence_length,
            memory_attention_bias=None,
            inputs=None,
            helper=helper,
            max_decoding_length=self._max_decode_len,
            mode=tf.estimator.ModeKeys.PREDICT)
        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            outputs_ = sess.run(outputs)
            self.assertIsInstance(outputs_, TransformerDecoderOutput)

</source>
<source file="systems/texar-0.2.4/texar/tf/modules/decoders/transformer_decoders_test.py" startline="154" endline="176" pcid="797">
    def test_decode_infer_sample(self):
        """Tests infer_sample
        """
        decoder = TransformerDecoder(
            vocab_size=self._vocab_size,
            output_layer=self._output_layer
        )
        helper = tx_helper.SampleEmbeddingHelper(
            self._embedding_fn, self._start_tokens, self._end_token)

        outputs, length = decoder(
            memory=self._memory,
            memory_sequence_length=self._memory_sequence_length,
            memory_attention_bias=None,
            inputs=None,
            helper=helper,
            max_decoding_length=self._max_decode_len,
            mode=tf.estimator.ModeKeys.PREDICT)
        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            outputs_ = sess.run(outputs)
            self.assertIsInstance(outputs_, TransformerDecoderOutput)

</source>
<source file="systems/texar-0.2.4/texar/tf/modules/decoders/transformer_decoders_test.py" startline="127" endline="153" pcid="796">
    def test_infer_greedy_with_context_without_memory(self):
        """Tests train_greedy with context
        """
        decoder = TransformerDecoder(
            vocab_size=self._vocab_size,
            output_layer=self._output_layer
        )
        helper = tx_helper.GreedyEmbeddingHelper(
            self._embedding_fn, self._start_tokens, self._end_token)

        outputs, length = decoder(
            memory=None,
            memory_sequence_length=None,
            memory_attention_bias=None,
            inputs=None,
            decoding_strategy='infer_greedy',
            helper=helper,
            context=self._context,
            context_sequence_length=self._context_length,
            end_token=self._end_token,
            max_decoding_length=self._max_decode_len,
            mode=tf.estimator.ModeKeys.PREDICT)
        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            outputs_ = sess.run(outputs)
            self.assertIsInstance(outputs_, TransformerDecoderOutput)

</source>
</class>

<class classid="30" nclones="2" nlines="24" similarity="76">
<source file="systems/texar-0.2.4/texar/tf/modules/encoders/hierarchical_encoders_test.py" startline="81" endline="110" pcid="803">
    def test_depack(self):
        hparams = {
            "encoder_major_type": "BidirectionalRNNEncoder",
            "encoder_major_hparams": {
                "rnn_cell_fw": {
                    "type": "LSTMCell",
                    "kwargs": {
                        "num_units": 100
                    }
                }
            }
        }
        encoder = HierarchicalRNNEncoder(hparams=hparams)

        batch_size = 16
        max_major_time = 8
        max_minor_time = 6
        dim = 10
        inputs = tf.random_uniform(
            [batch_size, max_major_time, max_minor_time, dim],
            maxval=1,
            minval=-1,
            dtype=tf.float32)

        _, _ = encoder(inputs)

        self.assertEqual(
            encoder.states_minor_before_medium.h.shape[1],
            encoder.states_minor_after_medium.shape[1])

</source>
<source file="systems/texar-0.2.4/texar/tf/modules/encoders/hierarchical_encoders_test.py" startline="111" endline="148" pcid="804">
    def test_encoder_minor_as_birnn(self):
        """Tests encoder_minor as a BidirectionalRNNEncoder
        """
        hparams = {
            "encoder_minor_type": "BidirectionalRNNEncoder",
            "encoder_minor_hparams": {
                "rnn_cell_fw": {
                    "type": "LSTMCell",
                    "kwargs": {
                        "num_units": 100
                    }
                }
            },
            "encoder_major_hparams": {
                "rnn_cell": {
                    "type": "LSTMCell",
                    "kwargs": {
                        "num_units": 200
                    }
                }
            }
        }
        encoder = HierarchicalRNNEncoder(hparams=hparams)

        batch_size = 16
        max_major_time = 8
        max_minor_time = 6
        dim = 10
        inputs = tf.random_uniform(
            [batch_size, max_major_time, max_minor_time, dim],
            maxval=1,
            minval=-1,
            dtype=tf.float32)

        outputs, _ = encoder(inputs)
        self.assertEqual(list(outputs.shape), [16, 8, 200])


</source>
</class>

<class classid="31" nclones="2" nlines="33" similarity="100">
<source file="systems/texar-0.2.4/texar/tf/modules/encoders/conv_encoders_test.py" startline="21" endline="65" pcid="811">
    def test_encode(self):
        """Tests encode.
        """
        encoder_1 = Conv1DEncoder()
        self.assertEqual(len(encoder_1.layers), 4)
        self.assertTrue(isinstance(encoder_1.layer_by_name("conv_pool_1"),
                                   tx.core.MergeLayer))
        for layer in encoder_1.layers[0].layers:
            self.assertTrue(isinstance(layer, tx.core.SequentialLayer))

        inputs_1 = tf.ones([64, 16, 300], tf.float32)
        outputs_1 = encoder_1(inputs_1)
        self.assertEqual(outputs_1.shape, [64, 128])

        hparams = {
            # Conv layers
            "num_conv_layers": 2,
            "filters": 128,
            "kernel_size": [[3, 4, 5], 4],
            "other_conv_kwargs": {"padding": "same"},
            # Pooling layers
            "pooling": "AveragePooling",
            "pool_size": 2,
            "pool_strides": 1,
            # Dense layers
            "num_dense_layers": 3,
            "dense_size": [128, 128, 10],
            "dense_activation": "relu",
            "other_dense_kwargs": {"use_bias": False},
            # Dropout
            "dropout_conv": [0, 1, 2],
            "dropout_dense": 2
        }
        encoder_2 = Conv1DEncoder(hparams)
        # nlayers = nconv-pool + nconv + npool + ndense + ndropout + flatten
        self.assertEqual(len(encoder_2.layers), 1 + 1 + 1 + 3 + 4 + 1)
        self.assertTrue(isinstance(encoder_2.layer_by_name("conv_pool_1"),
                                   tx.core.MergeLayer))
        for layer in encoder_2.layers[1].layers:
            self.assertTrue(isinstance(layer, tx.core.SequentialLayer))

        inputs_2 = tf.ones([64, 16, 300], tf.float32)
        outputs_2 = encoder_2(inputs_2)
        self.assertEqual(outputs_2.shape, [64, 10])

</source>
<source file="systems/texar-0.2.4/texar/tf/modules/networks/conv_networks_test.py" startline="21" endline="65" pcid="1089">
    def test_feedforward(self):
        """Tests feed forward.
        """
        network_1 = Conv1DNetwork()
        self.assertEqual(len(network_1.layers), 4)
        self.assertTrue(isinstance(network_1.layer_by_name("conv_pool_1"),
                                   tx.core.MergeLayer))
        for layer in network_1.layers[0].layers:
            self.assertTrue(isinstance(layer, tx.core.SequentialLayer))

        inputs_1 = tf.ones([64, 16, 300], tf.float32)
        outputs_1 = network_1(inputs_1)
        self.assertEqual(outputs_1.shape, [64, 128])

        hparams = {
            # Conv layers
            "num_conv_layers": 2,
            "filters": 128,
            "kernel_size": [[3, 4, 5], 4],
            "other_conv_kwargs": {"padding": "same"},
            # Pooling layers
            "pooling": "AveragePooling",
            "pool_size": 2,
            "pool_strides": 1,
            # Dense layers
            "num_dense_layers": 3,
            "dense_size": [128, 128, 10],
            "dense_activation": "relu",
            "other_dense_kwargs": {"use_bias": False},
            # Dropout
            "dropout_conv": [0, 1, 2],
            "dropout_dense": 2
        }
        network_2 = Conv1DNetwork(hparams)
        # nlayers = nconv-pool + nconv + npool + ndense + ndropout + flatten
        self.assertEqual(len(network_2.layers), 1 + 1 + 1 + 3 + 4 + 1)
        self.assertTrue(isinstance(network_2.layer_by_name("conv_pool_1"),
                                   tx.core.MergeLayer))
        for layer in network_2.layers[1].layers:
            self.assertTrue(isinstance(layer, tx.core.SequentialLayer))

        inputs_2 = tf.ones([64, 16, 300], tf.float32)
        outputs_2 = network_2(inputs_2)
        self.assertEqual(outputs_2.shape, [64, 10])

</source>
</class>

<class classid="32" nclones="2" nlines="35" similarity="100">
<source file="systems/texar-0.2.4/texar/tf/modules/encoders/conv_encoders_test.py" startline="66" endline="115" pcid="812">
    def test_unknown_seq_length(self):
        """Tests use of pooling layer when the seq_length dimension of inputs
        is `None`.
        """
        encoder_1 = Conv1DEncoder()
        inputs_1 = tf.placeholder(tf.float32, [64, None, 300])
        outputs_1 = encoder_1(inputs_1)
        self.assertEqual(outputs_1.shape, [64, 128])

        hparams = {
            # Conv layers
            "num_conv_layers": 2,
            "filters": 128,
            "kernel_size": [[3, 4, 5], 4],
            # Pooling layers
            "pooling": "AveragePooling",
            "pool_size": [2, None],
            # Dense layers
            "num_dense_layers": 1,
            "dense_size": 10,
        }
        encoder = Conv1DEncoder(hparams)
        # nlayers = nconv-pool + nconv + npool + ndense + ndropout + flatten
        self.assertEqual(len(encoder.layers), 1 + 1 + 1 + 1 + 1 + 1)
        self.assertTrue(isinstance(encoder.layer_by_name('pool_2'),
                                   tx.core.AverageReducePooling1D))

        inputs = tf.placeholder(tf.float32, [64, None, 300])
        outputs = encoder(inputs)
        self.assertEqual(outputs.shape, [64, 10])

        hparams_2 = {
            # Conv layers
            "num_conv_layers": 1,
            "filters": 128,
            "kernel_size": 4,
            "other_conv_kwargs": {'data_format': 'channels_first'},
            # Pooling layers
            "pooling": "MaxPooling",
            "other_pool_kwargs": {'data_format': 'channels_first'},
            # Dense layers
            "num_dense_layers": 1,
            "dense_size": 10,
        }
        encoder_2 = Conv1DEncoder(hparams_2)
        inputs_2 = tf.placeholder(tf.float32, [64, 300, None])
        outputs_2 = encoder_2(inputs_2)
        self.assertEqual(outputs_2.shape, [64, 10])


</source>
<source file="systems/texar-0.2.4/texar/tf/modules/networks/conv_networks_test.py" startline="66" endline="114" pcid="1090">
    def test_unknown_seq_length(self):
        """Tests use of pooling layer when the seq_length dimension of inputs
        is `None`.
        """
        network_1 = Conv1DNetwork()
        inputs_1 = tf.placeholder(tf.float32, [64, None, 300])
        outputs_1 = network_1(inputs_1)
        self.assertEqual(outputs_1.shape, [64, 128])

        hparams = {
            # Conv layers
            "num_conv_layers": 2,
            "filters": 128,
            "kernel_size": [[3, 4, 5], 4],
            # Pooling layers
            "pooling": "AveragePooling",
            "pool_size": [2, None],
            # Dense layers
            "num_dense_layers": 1,
            "dense_size": 10,
        }
        network = Conv1DNetwork(hparams)
        # nlayers = nconv-pool + nconv + npool + ndense + ndropout + flatten
        self.assertEqual(len(network.layers), 1 + 1 + 1 + 1 + 1 + 1)
        self.assertTrue(isinstance(network.layer_by_name('pool_2'),
                                   tx.core.AverageReducePooling1D))

        inputs = tf.placeholder(tf.float32, [64, None, 300])
        outputs = network(inputs)
        self.assertEqual(outputs.shape, [64, 10])

        hparams_2 = {
            # Conv layers
            "num_conv_layers": 1,
            "filters": 128,
            "kernel_size": 4,
            "other_conv_kwargs": {'data_format': 'channels_first'},
            # Pooling layers
            "pooling": "MaxPooling",
            "other_pool_kwargs": {'data_format': 'channels_first'},
            # Dense layers
            "num_dense_layers": 1,
            "dense_size": 10,
        }
        network_2 = Conv1DNetwork(hparams_2)
        inputs_2 = tf.placeholder(tf.float32, [64, 300, None])
        outputs_2 = network_2(inputs_2)
        self.assertEqual(outputs_2.shape, [64, 10])

</source>
</class>

<class classid="33" nclones="2" nlines="27" similarity="84">
<source file="systems/texar-0.2.4/texar/tf/modules/encoders/bert_encoder_test.py" startline="31" endline="72" pcid="818">
    def test_hparams(self):
        """Tests the priority of the encoder arch parameter.
        """

        inputs = tf.placeholder(dtype=tf.int32, shape=[None, None])

        # case 1: set "pretrained_mode_name" by constructor argument
        hparams = {
            "pretrained_model_name": "bert-large-uncased",
        }
        encoder = BERTEncoder(pretrained_model_name="bert-base-uncased",
                              hparams=hparams)
        _, _ = encoder(inputs)
        self.assertEqual(encoder.hparams.encoder.num_blocks, 12)

        # case 2: set "pretrained_mode_name" by hparams
        hparams = {
            "pretrained_model_name": "bert-large-uncased",
            "encoder": {
                "num_blocks": 6
            }
        }
        encoder = BERTEncoder(hparams=hparams)
        _, _ = encoder(inputs)
        self.assertEqual(encoder.hparams.encoder.num_blocks, 24)

        # case 3: set to None in both hparams and constructor argument
        hparams = {
            "pretrained_model_name": None,
            "encoder": {
                "num_blocks": 6
            },
        }
        encoder = BERTEncoder(hparams=hparams)
        _, _ = encoder(inputs)
        self.assertEqual(encoder.hparams.encoder.num_blocks, 6)

        # case 4: using default hparams
        encoder = BERTEncoder()
        _, _ = encoder(inputs)
        self.assertEqual(encoder.hparams.encoder.num_blocks, 12)

</source>
<source file="systems/texar-0.2.4/texar/tf/modules/encoders/gpt2_encoder_test.py" startline="26" endline="67" pcid="834">
    def test_hparams(self):
        """Tests the priority of the encoder arch parameter.
        """

        inputs = tf.placeholder(dtype=tf.int32, shape=[None, None])

        # case 1: set "pretrained_mode_name" by constructor argument
        hparams = {
            "pretrained_model_name": "gpt2-medium",
        }
        encoder = GPT2Encoder(pretrained_model_name="gpt2-small",
                              hparams=hparams)
        _ = encoder(inputs)
        self.assertEqual(encoder.hparams.encoder.num_blocks, 12)

        # case 2: set "pretrained_mode_name" by hparams
        hparams = {
            "pretrained_model_name": "gpt2-small",
            "encoder": {
                "num_blocks": 6
            }
        }
        encoder = GPT2Encoder(hparams=hparams)
        _ = encoder(inputs)
        self.assertEqual(encoder.hparams.encoder.num_blocks, 12)

        # case 3: set to None in both hparams and constructor argument
        hparams = {
            "pretrained_model_name": None,
            "encoder": {
                "num_blocks": 6
            },
        }
        encoder = GPT2Encoder(hparams=hparams)
        _ = encoder(inputs)
        self.assertEqual(encoder.hparams.encoder.num_blocks, 6)

        # case 4: using default hparams
        encoder = GPT2Encoder()
        _ = encoder(inputs)
        self.assertEqual(encoder.hparams.encoder.num_blocks, 12)

</source>
</class>

<class classid="34" nclones="3" nlines="10" similarity="72">
<source file="systems/texar-0.2.4/texar/tf/modules/encoders/multihead_attention.py" startline="73" endline="123" pcid="881">
    def default_hparams():
        """Returns a dictionary of hyperparameters with default values.

        .. code-block:: python

            {
                "initializer": None,
                'num_heads': 8,
                'output_dim': 512,
                'num_units': 512,
                'dropout_rate': 0.1,
                'use_bias': False,
                "name": "multihead_attention"
            }

        Here:

        "initializer": dict, optional
            Hyperparameters of the default initializer that initializes
            variables created in this module.
            See :func:`~texar.tf.core.get_initializer` for details.

        "num_heads": int
            Number of heads for attention calculation.

        "output_dim": int
            Output dimension of the returned tensor.

        "num_units": int
            Hidden dimension of the unsplitted attention space.
            Should be devisible by `num_heads`.

        "dropout_rate: : float
            Dropout rate in the attention.

        "use_bias": bool
            Use bias when projecting the key, value and query.

        "name": str
            Name of the module.
        """
        return {
            'initializer': None,
            'num_heads': 8,
            'output_dim': 512,
            'num_units': 512,
            'dropout_rate': 0.1,
            'use_bias': False,
            "name": "multihead_attention",
        }

</source>
<source file="systems/texar-0.2.4/texar/tf/modules/pretrained/xlnet_utils.py" startline="79" endline="114" pcid="946">
    def default_hparams():
        r"""Returns a dictionary of hyperparameters with default values.

        .. code-block:: python

            {
                "hidden_dim": 768,
                "ffn_inner_dim": 3072,
                "dropout": 0.1,
                "activation": 'gelu'
            }

        Here

        `"hidden_dim"`: int
            Dimension of the layer fed as input to feed forward network

        `"ffn_inner_dim"`: int
            Inner dimension of the feed forward layer

        `"dropout"`: float
            Dropout rate for layers

        `"activation"`: str or callable
            Activation function applied to the output of the PositionWise FF.
            See :func:`~texar.tf.core.get_activation_fn` for more details.
        """
        return {
            "name": "ff",
            "initializer": None,
            "hidden_dim": 768,
            "ffn_inner_dim": 3072,
            "dropout": 0.1,
            "activation": 'gelu',
        }

</source>
<source file="systems/texar-0.2.4/texar/tf/modules/pretrained/xlnet_utils.py" startline="344" endline="402" pcid="955">
    def default_hparams():
        r"""Returns a dictionary of hyperparameters with default values.

        .. code-block:: python

            {
                "name": "rel_attn",
                "initializer": None,
                "num_heads": 12,
                "hidden_dim": 768,
                "head_dim": 64,
                "dropout": 0.1,
                "attention_dropout": 0.1,
                "use_segments": True
            }



        Here:

        The default parameters are values for cased XLNet-Base model.

        "initializer": dict, optional
            Hyperparameters of the default initializer that initializes
            variables created in this module.
            See :func:`~texar.tf.core.get_initializer` for details.

        "num_heads": int
            Number of heads in the attention

        "hidden_dim": int
            Hidden dimension of the embeddings

        "head_dim": int
            Size of the vectors after head projection.

        "dropout": float
            Dropout rate for layers

        "attention_dropout": float
            Dropout rate for attention layers

        "use_segments": bool
            Boolean to indicate if the input has segments

        "name": str
            Name of the module.
        """
        return {
            "name": "rel_attn",
            "initializer": None,
            "num_heads": 12,
            "hidden_dim": 768,
            "head_dim": 64,
            "dropout": 0.1,
            "attention_dropout": 0.1,
            "use_segments": True,
        }

</source>
</class>

<class classid="35" nclones="2" nlines="27" similarity="82">
<source file="systems/texar-0.2.4/texar/tf/modules/encoders/rnn_encoders_test.py" startline="24" endline="63" pcid="886">
    def test_trainable_variables(self):
        """Tests the functionality of automatically collecting trainable
        variables.
        """
        inputs = tf.placeholder(dtype=tf.float32, shape=[None, None, 100])

        # case 1
        encoder = UnidirectionalRNNEncoder()
        _, _ = encoder(inputs)
        self.assertEqual(len(encoder.trainable_variables), 2)

        # case 2
        hparams = {
            "rnn_cell": {
                "dropout": {
                    "input_keep_prob": 0.5
                }
            }
        }
        encoder = UnidirectionalRNNEncoder(hparams=hparams)
        _, _ = encoder(inputs)
        self.assertEqual(len(encoder.trainable_variables), 2)

        # case 3
        hparams = {
            "output_layer": {
                "num_layers": 2,
                "layer_size": [100, 6],
                "activation": "relu",
                "final_layer_activation": "identity",
                "dropout_layer_ids": [0, 1, 2],
                "variational_dropout": False
            }
        }
        encoder = UnidirectionalRNNEncoder(hparams=hparams)
        _, _ = encoder(inputs)
        self.assertEqual(len(encoder.trainable_variables), 2 + 2 + 2)
        _, _ = encoder(inputs)
        self.assertEqual(len(encoder.trainable_variables), 2 + 2 + 2)

</source>
<source file="systems/texar-0.2.4/texar/tf/modules/encoders/rnn_encoders_test.py" startline="133" endline="177" pcid="889">
    def test_trainable_variables(self):
        """Tests the functionality of automatically collecting trainable
        variables.
        """
        inputs = tf.placeholder(dtype=tf.float32, shape=[None, None, 100])

        # case 1
        encoder = BidirectionalRNNEncoder()
        _, _ = encoder(inputs)
        self.assertEqual(len(encoder.trainable_variables), 4)

        # case 2
        hparams = {
            "rnn_cell_fw": {
                "dropout": {
                    "input_keep_prob": 0.5
                }
            }
        }
        encoder = BidirectionalRNNEncoder(hparams=hparams)
        _, _ = encoder(inputs)
        self.assertEqual(len(encoder.trainable_variables), 4)

        # case 3
        hparams = {
            "output_layer_fw": {
                "num_layers": 2,
                "layer_size": [100, 6],
                "activation": "relu",
                "final_layer_activation": "identity",
                "dropout_layer_ids": [0, 1, 2],
                "variational_dropout": False
            },
            "output_layer_bw": {
                "num_layers": 3,
                "other_dense_kwargs": {"use_bias": False}
            },
            "output_layer_share_config": False
        }
        encoder = BidirectionalRNNEncoder(hparams=hparams)
        _, _ = encoder(inputs)
        self.assertEqual(len(encoder.trainable_variables), 4 + 4 + 3)
        _, _ = encoder(inputs)
        self.assertEqual(len(encoder.trainable_variables), 4 + 4 + 3)

</source>
</class>

<class classid="36" nclones="2" nlines="37" similarity="76">
<source file="systems/texar-0.2.4/texar/tf/modules/encoders/rnn_encoders_test.py" startline="64" endline="111" pcid="887">
    def test_encode(self):
        """Tests encoding.
        """
        # case 1
        encoder = UnidirectionalRNNEncoder()

        max_time = 8
        batch_size = 16
        emb_dim = 100
        inputs = tf.random_uniform([batch_size, max_time, emb_dim],
                                   maxval=1., dtype=tf.float32)
        outputs, state = encoder(inputs)

        cell_dim = encoder.hparams.rnn_cell.kwargs.num_units
        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            outputs_, state_ = sess.run([outputs, state])
            self.assertEqual(outputs_.shape, (batch_size, max_time, cell_dim))
            self.assertEqual(state_[0].shape, (batch_size, cell_dim))

        # case 2: with output layers
        hparams = {
            "output_layer": {
                "num_layers": 2,
                "layer_size": [100, 6],
                "dropout_layer_ids": [0, 1, 2],
                "variational_dropout": True
            }
        }
        encoder = UnidirectionalRNNEncoder(hparams=hparams)

        max_time = 8
        batch_size = 16
        emb_dim = 100
        inputs = tf.random_uniform([batch_size, max_time, emb_dim],
                                   maxval=1., dtype=tf.float32)
        outputs, state, cell_outputs, output_size = encoder(
            inputs, return_cell_output=True, return_output_size=True)

        self.assertEqual(output_size[0], 6)
        self.assertEqual(cell_outputs.shape[-1], encoder.cell.output_size)

        out_dim = encoder.hparams.output_layer.layer_size[-1]
        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            outputs_ = sess.run(outputs)
            self.assertEqual(outputs_.shape, (batch_size, max_time, out_dim))

</source>
<source file="systems/texar-0.2.4/texar/tf/modules/encoders/rnn_encoders_test.py" startline="178" endline="265" pcid="890">
    def test_encode(self):
        """Tests encoding.
        """
        # case 1
        encoder = BidirectionalRNNEncoder()

        max_time = 8
        batch_size = 16
        emb_dim = 100
        inputs = tf.random_uniform([batch_size, max_time, emb_dim],
                                   maxval=1., dtype=tf.float32)
        outputs, state = encoder(inputs)

        cell_dim = encoder.hparams.rnn_cell_fw.kwargs.num_units
        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            outputs_, state_ = sess.run([outputs, state])
            self.assertEqual(outputs_[0].shape,
                             (batch_size, max_time, cell_dim))
            self.assertEqual(state_[0][0].shape, (batch_size, cell_dim))

        # case 2: with output layers
        hparams = {
            "output_layer_fw": {
                "num_layers": 2,
                "layer_size": [100, 6],
                "dropout_layer_ids": [0, 1, 2],
                "variational_dropout": True
            }
        }
        encoder = BidirectionalRNNEncoder(hparams=hparams)

        max_time = 8
        batch_size = 16
        emb_dim = 100
        inputs = tf.random_uniform([batch_size, max_time, emb_dim],
                                   maxval=1., dtype=tf.float32)
        outputs, state, cell_outputs, output_size = encoder(
            inputs, return_cell_output=True, return_output_size=True)

        self.assertEqual(output_size[0][0], 6)
        self.assertEqual(output_size[1][0], 6)
        self.assertEqual(cell_outputs[0].shape[-1], encoder.cell_fw.output_size)
        self.assertEqual(cell_outputs[1].shape[-1], encoder.cell_bw.output_size)

        out_dim = encoder.hparams.output_layer_fw.layer_size[-1]
        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            outputs_ = sess.run(outputs)
            self.assertEqual(outputs_[0].shape, (batch_size, max_time, out_dim))
            self.assertEqual(outputs_[1].shape, (batch_size, max_time, out_dim))

# TODO(zhiting): not completed yet
# class HierarchicalForwardRNNEncoderTest(tf.test.TestCase):
#    """Tests HierarchicalForwardRNNEncoder class.
#    """
#
#    def test_trainable_variables(self):
#        """Tests the functionality of automatically collecting trainable
#        variables.
#        """
#        encoder = HierarchicalForwardRNNEncoder(vocab_size=2)
#        inputs = [[[1, 0], [0, 1], [0, 1]]]
#        _, _ = encoder(inputs)
#        self.assertEqual(len(encoder.trainable_variables), 5)
#
#    def test_encode(self):
#        """Tests encoding.
#        """
#        vocab_size = 4
#        encoder = HierarchicalForwardRNNEncoder(vocab_size=vocab_size)
#
#        max_major_time = 8
#        max_minor_time = 6
#        batch_size = 16
#        inputs = tf.random_uniform([batch_size, max_major_time, max_minor_time],
#                                   maxval=vocab_size,
#                                   dtype=tf.int32)
#        outputs, state = encoder(inputs)
#
#        cell_dim = encoder.hparams.rnn_cell.kwargs.num_units
#        with self.test_session() as sess:
#            sess.run(tf.global_variables_initializer())
#            outputs_, state_ = sess.run([outputs, state])
#            self.assertEqual(outputs_.shape, (batch_size, max_major_time, cell_dim))
#            self.assertEqual(state_[0].shape, (batch_size, cell_dim))


</source>
</class>

<class classid="37" nclones="2" nlines="28" similarity="82">
<source file="systems/texar-0.2.4/texar/tf/modules/connectors/connectors.py" startline="490" endline="566" pcid="915">
    def _build(self,
               distribution='MultivariateNormalDiag',
               distribution_kwargs=None,
               transform=True,
               num_samples=None):
        """Samples from a distribution and optionally performs transformation
        with an MLP layer.

        The distribution must be reparameterizable, i.e.,
        `distribution.reparameterization_type = FULLY_REPARAMETERIZED`.

        Args:
            distribution: A instance of subclass of
                :tf_main:`TF Distribution <distributions/Distribution>`,
                or :tf_hmpg:`tensorflow_probability Distribution <probability>`,
                Can be a class, its name or module path, or a class instance.
            distribution_kwargs (dict, optional): Keyword arguments for the
                distribution constructor. Ignored if `distribution` is a
                class instance.
            transform (bool): Whether to perform MLP transformation of the
                distribution samples. If `False`, the structure/shape of a
                sample must match :attr:`output_size`.
            num_samples (optional): An `int` or `int` Tensor. Number of samples
                to generate. If not given, generate a single sample. Note
                that if batch size has already been included in
                `distribution`'s dimensionality, `num_samples` should be
                left as `None`.

        Returns:
            A tuple (output, sample), where

            - output: A Tensor or a (nested) tuple of Tensors with the same \
            structure and size of :attr:`output_size`. The batch dimension \
            equals :attr:`num_samples` if specified, or is determined by the \
            distribution dimensionality. If :attr:`transform` is `False`, \
            :attr:`output` will be equal to :attr:`sample`.
            - sample: The sample from the distribution, prior to transformation.

        Raises:
            ValueError: If distribution cannot be reparametrized.
            ValueError: The output does not match :attr:`output_size`.
        """
        dstr = check_or_get_instance(
            distribution, distribution_kwargs,
            ["tensorflow.distributions", "tensorflow_probability.distributions",
             "texar.tf.custom"])

        if dstr.reparameterization_type == tf_dstr.NOT_REPARAMETERIZED:
            raise ValueError(
                "Distribution is not reparameterized: %s" % dstr.name)

        if num_samples:
            sample = dstr.sample(num_samples)
        else:
            sample = dstr.sample()

        # if dstr.event_shape == []:
        #    sample = tf.reshape(
        #        sample,
        #        sample.shape.concatenate(tf.TensorShape(1)))

        # sample = tf.cast(sample, tf.float32)
        if transform:
            fn_modules = ['tensorflow', 'tensorflow.nn', 'texar.tf.custom']
            activation_fn = get_function(self.hparams.activation_fn, fn_modules)
            output = _mlp_transform(sample, self._output_size, activation_fn)
        else:
            output = sample

        _assert_same_size(output, self._output_size)
        if not self._built:
            self._add_internal_trainable_variables()
            self._built = True

        return output, sample


</source>
<source file="systems/texar-0.2.4/texar/tf/modules/connectors/connectors.py" startline="621" endline="779" pcid="918">
    def _build(self,
               distribution='MultivariateNormalDiag',
               distribution_kwargs=None,
               transform=True,
               num_samples=None):
        """Samples from a distribution and optionally performs transformation
        with an MLP layer.

        The inputs and outputs are the same as
        :class:`~texar.tf.modules.ReparameterizedStochasticConnector` except that
        the distribution does not need to be reparameterizable, and gradient
        cannot be back-propagate through the samples.

        Args:
            distribution: A instance of subclass of
                :tf_main:`TF Distribution <distributions/Distribution>`,
                or :tf_hmpg:`tensorflow_probability Distribution <probability>`.
                Can be a class, its name or module path, or a class instance.
            distribution_kwargs (dict, optional): Keyword arguments for the
                distribution constructor. Ignored if `distribution` is a
                class instance.
            transform (bool): Whether to perform MLP transformation of the
                distribution samples. If `False`, the structure/shape of a
                sample must match :attr:`output_size`.
            num_samples (optional): An `int` or `int` Tensor. Number of samples
                to generate. If not given, generate a single sample. Note
                that if batch size has already been included in
                `distribution`'s dimensionality, `num_samples` should be
                left as `None`.

        Returns:
            A tuple (output, sample), where

            - output: A Tensor or a (nested) tuple of Tensors with the same \
            structure and size of :attr:`output_size`. The batch dimension \
            equals :attr:`num_samples` if specified, or is determined by the \
            distribution dimensionality. If :attr:`transform` is `False`, \
            :attr:`output` will be equal to :attr:`sample`.
            - sample: The sample from the distribution, prior to transformation.

        Raises:
            ValueError: The output does not match :attr:`output_size`.
        """
        dstr = check_or_get_instance(
            distribution, distribution_kwargs,
            ["tensorflow.distributions", "tensorflow_probability.distributions",
             "texar.tf.custom"])

        if num_samples:
            sample = dstr.sample(num_samples)
        else:
            sample = dstr.sample()

        if dstr.event_shape == []:
            sample = tf.reshape(sample,
                                sample.shape.concatenate(tf.TensorShape(1)))

        # Disable gradients through samples
        sample = tf.stop_gradient(sample)

        sample = tf.cast(sample, tf.float32)

        if transform:
            fn_modules = ['tensorflow', 'tensorflow.nn', 'texar.tf.custom']
            activation_fn = get_function(self.hparams.activation_fn, fn_modules)
            output = _mlp_transform(sample, self._output_size, activation_fn)
        else:
            output = sample

        _assert_same_size(output, self._output_size)
        if not self._built:
            self._add_internal_trainable_variables()
            self._built = True

        return output, sample


# class ConcatConnector(ConnectorBase):
#    """Concatenates multiple connectors into one connector. Used in, e.g.,
#    semi-supervised variational autoencoders, disentangled representation
#    learning, and other models.
#
#    Args:
#        output_size: Size of output excluding the batch dimension (eg.
#            :attr:`output_size = p` if :attr:`output.shape` is :attr:`[N, p]`).
#            Can be an int, a tuple of int, a Tensorshape, or a tuple of
#            TensorShapes.
#            For example, to transform to decoder state size, set
#            `output_size=decoder.cell.state_size`.
#        hparams (dict): Hyperparameters of the connector.
#    """
#
#    def __init__(self, output_size, hparams=None):
#        ConnectorBase.__init__(self, output_size, hparams)
#
#    @staticmethod
#    def default_hparams():
#        """Returns a dictionary of hyperparameters with default values.
#
#        Returns:
#            .. code-block:: python
#
#                {
#                    "activation_fn": "tensorflow.identity",
#                    "name": "concat_connector"
#                }
#
#            Here:
#
#            "activation_fn": (str or callable)
#                The name or full path to the activation function applied to
#                the outputs of the MLP layer. The activation functions can be:
#
#                - Built-in activation functions defined in :mod:`tf` or \
#                  :mod:`tf.nn`, e.g., :tf_main:`identity <identity>`.
#                - User-defined activation functions in `texar.tf.custom`.
#                - External activation functions. Must provide the full path, \
#                  e.g., "my_module.my_activation_fn".
#
#                The default value is :attr:`"identity"`, i.e., the MLP
#                transformation is linear.
#
#            "name": str
#                Name of the connector.
#
#                The default value is "concat_connector".
#        """
#        return {
#            "activation_fn": "tensorflow.identity",
#            "name": "concat_connector"
#        }
#
#    def _build(self, connector_inputs, transform=True):
#        """Concatenate multiple input connectors
#
#        Args:
#            connector_inputs: a list of connector states
#            transform (bool): If `True`, then the output are automatically
#                transformed to match :attr:`output_size`.
#
#        Returns:
#            A Tensor or a (nested) tuple of Tensors of the same structure of
#            the decoder state.
#        """
#        connector_inputs = [tf.cast(connector, tf.float32)
#                            for connector in connector_inputs]
#        output = tf.concat(connector_inputs, axis=1)
#
#        if transform:
#            fn_modules = ['texar.tf.custom', 'tensorflow', 'tensorflow.nn']
#            activation_fn = get_function(self.hparams.activation_fn,
#                                         fn_modules)
#            output = _mlp_transform(output, self._output_size, activation_fn)
#        _assert_same_size(output, self._output_size)
#
#        self._add_internal_trainable_variables()
#        self._built = True
#
#        return output
</source>
</class>

<class classid="38" nclones="5" nlines="36" similarity="72">
<source file="systems/texar-0.2.4/texar/tf/modules/regressors/xlnet_regressor.py" startline="70" endline="115" pcid="919">
    def __init__(self,
                 pretrained_model_name=None,
                 cache_dir=None,
                 hparams=None):
        RegressorBase.__init__(self, hparams)

        with tf.variable_scope(self.variable_scope):
            tf.get_variable_scope().set_initializer(
                get_initializer(self._hparams.initializer))
            # Creates the underlying encoder
            encoder_hparams = dict_fetch(
                hparams, XLNetEncoder.default_hparams())
            if encoder_hparams is not None:
                encoder_hparams['name'] = "encoder"
            self._encoder = XLNetEncoder(
                pretrained_model_name=pretrained_model_name,
                cache_dir=cache_dir,
                hparams=encoder_hparams)
            if self._hparams.use_projection:
                self.projection = get_layer(hparams={
                    "type": "Dense",
                    "kwargs": {
                        "units": self._encoder.output_size
                    }
                })

            # Creates an dropout layer
            drop_kwargs = {"rate": self._hparams.dropout}
            layer_hparams = {"type": "Dropout", "kwargs": drop_kwargs}
            self._dropout_layer = get_layer(hparams=layer_hparams)

            logit_kwargs = self._hparams.logit_layer_kwargs
            if logit_kwargs is None:
                logit_kwargs = {}
            elif not isinstance(logit_kwargs, HParams):
                raise ValueError(
                    "hparams['logit_layer_kwargs'] must be a dict.")
            else:
                logit_kwargs = logit_kwargs.todict()
            logit_kwargs.update({"units": 1})
            if 'name' not in logit_kwargs:
                logit_kwargs['name'] = "logit_layer"

            layer_hparams = {"type": "Dense", "kwargs": logit_kwargs}
            self._logit_layer = get_layer(hparams=layer_hparams)

</source>
<source file="systems/texar-0.2.4/texar/tf/modules/classifiers/gpt2_classifier.py" startline="65" endline="107" pcid="1009">
    def __init__(self,
                 pretrained_model_name=None,
                 cache_dir=None,
                 hparams=None):

        super().__init__(hparams=hparams)

        with tf.variable_scope(self.variable_scope):
            encoder_hparams = dict_fetch(
                hparams, GPT2Encoder.default_hparams())
            if encoder_hparams is not None:
                encoder_hparams['name'] = None

            self._encoder = GPT2Encoder(
                pretrained_model_name=pretrained_model_name,
                cache_dir=cache_dir,
                hparams=encoder_hparams)

            # Creates an dropout layer
            drop_kwargs = {"rate": self._hparams.dropout}
            layer_hparams = {"type": "Dropout", "kwargs": drop_kwargs}
            self._dropout_layer = get_layer(hparams=layer_hparams)

            # Creates an additional classification layer if needed
            self._num_classes = self._hparams.num_classes
            if self._num_classes <= 0:
                self._logit_layer = None
            else:
                logit_kwargs = self._hparams.logit_layer_kwargs
                if logit_kwargs is None:
                    logit_kwargs = {}
                elif not isinstance(logit_kwargs, HParams):
                    raise ValueError(
                        "hparams['logit_layer_kwargs'] must be a dict.")
                else:
                    logit_kwargs = logit_kwargs.todict()
                logit_kwargs.update({"units": self._num_classes})
                if 'name' not in logit_kwargs:
                    logit_kwargs['name'] = "logit_layer"

                layer_hparams = {"type": "Dense", "kwargs": logit_kwargs}
                self._logit_layer = get_layer(hparams=layer_hparams)

</source>
<source file="systems/texar-0.2.4/texar/tf/modules/classifiers/bert_classifier.py" startline="72" endline="114" pcid="1025">
    def __init__(self,
                 pretrained_model_name=None,
                 cache_dir=None,
                 hparams=None):

        super(BERTClassifier, self).__init__(hparams=hparams)

        with tf.variable_scope(self.variable_scope):
            # Creates the underlying encoder
            encoder_hparams = dict_fetch(
                hparams, BERTEncoder.default_hparams())
            if encoder_hparams is not None:
                encoder_hparams['name'] = None
            self._encoder = BERTEncoder(
                pretrained_model_name=pretrained_model_name,
                cache_dir=cache_dir,
                hparams=encoder_hparams)

            # Creates an dropout layer
            drop_kwargs = {"rate": self._hparams.dropout}
            layer_hparams = {"type": "Dropout", "kwargs": drop_kwargs}
            self._dropout_layer = get_layer(hparams=layer_hparams)

            # Creates an additional classification layer if needed
            self._num_classes = self._hparams.num_classes
            if self._num_classes <= 0:
                self._logit_layer = None
            else:
                logit_kwargs = self._hparams.logit_layer_kwargs
                if logit_kwargs is None:
                    logit_kwargs = {}
                elif not isinstance(logit_kwargs, HParams):
                    raise ValueError(
                        "hparams['logit_layer_kwargs'] must be a dict.")
                else:
                    logit_kwargs = logit_kwargs.todict()
                logit_kwargs.update({"units": self._num_classes})
                if 'name' not in logit_kwargs:
                    logit_kwargs['name'] = "logit_layer"

                layer_hparams = {"type": "Dense", "kwargs": logit_kwargs}
                self._logit_layer = get_layer(hparams=layer_hparams)

</source>
<source file="systems/texar-0.2.4/texar/tf/modules/classifiers/xlnet_classifier.py" startline="70" endline="120" pcid="1013">
    def __init__(self,
                 pretrained_model_name=None,
                 cache_dir=None,
                 hparams=None):
        super(XLNetClassifier, self).__init__(hparams=hparams)

        with tf.variable_scope(self.variable_scope):
            tf.get_variable_scope().set_initializer(
                get_initializer(self._hparams.initializer))
            # Creates the underlying encoder
            encoder_hparams = dict_fetch(
                hparams, XLNetEncoder.default_hparams())
            if encoder_hparams is not None:
                encoder_hparams['name'] = "encoder"
            self._encoder = XLNetEncoder(
                pretrained_model_name=pretrained_model_name,
                cache_dir=cache_dir,
                hparams=encoder_hparams)
            if self._hparams.use_projection:
                self.projection = get_layer(hparams={
                    "type": "Dense",
                    "kwargs": {
                        "units": self._encoder.output_size
                    }
                })

            # Creates an dropout layer
            drop_kwargs = {"rate": self._hparams.dropout}
            layer_hparams = {"type": "Dropout", "kwargs": drop_kwargs}
            self._dropout_layer = get_layer(hparams=layer_hparams)

            # Creates an additional classification layer if needed
            self._num_classes = self._hparams.num_classes
            if self._num_classes <= 0:
                self._logit_layer = None
            else:
                logit_kwargs = self._hparams.logit_layer_kwargs
                if logit_kwargs is None:
                    logit_kwargs = {}
                elif not isinstance(logit_kwargs, HParams):
                    raise ValueError(
                        "hparams['logit_layer_kwargs'] must be a dict.")
                else:
                    logit_kwargs = logit_kwargs.todict()
                logit_kwargs.update({"units": self._num_classes})
                if 'name' not in logit_kwargs:
                    logit_kwargs['name'] = "logit_layer"

                layer_hparams = {"type": "Dense", "kwargs": logit_kwargs}
                self._logit_layer = get_layer(hparams=layer_hparams)

</source>
<source file="systems/texar-0.2.4/texar/tf/modules/classifiers/rnn_classifiers.py" startline="79" endline="117" pcid="1017">

    def __init__(self,
                 cell=None,
                 cell_dropout_mode=None,
                 output_layer=None,
                 hparams=None):
        ClassifierBase.__init__(self, hparams)

        with tf.variable_scope(self.variable_scope):
            # Creates the underlying encoder
            encoder_hparams = utils.dict_fetch(
                hparams, UnidirectionalRNNEncoder.default_hparams())
            if encoder_hparams is not None:
                encoder_hparams['name'] = None
            self._encoder = UnidirectionalRNNEncoder(
                cell=cell,
                cell_dropout_mode=cell_dropout_mode,
                output_layer=output_layer,
                hparams=encoder_hparams)

            # Creates an additional classification layer if needed
            self._num_classes = self._hparams.num_classes
            if self._num_classes <= 0:
                self._logit_layer = None
            else:
                logit_kwargs = self._hparams.logit_layer_kwargs
                if logit_kwargs is None:
                    logit_kwargs = {}
                elif not isinstance(logit_kwargs, HParams):
                    raise ValueError(
                        "hparams['logit_layer_kwargs'] must be a dict.")
                else:
                    logit_kwargs = logit_kwargs.todict()
                logit_kwargs.update({"units": self._num_classes})
                if 'name' not in logit_kwargs:
                    logit_kwargs['name'] = "logit_layer"

                layer_hparams = {"type": "Dense", "kwargs": logit_kwargs}
                self._logit_layer = layers.get_layer(hparams=layer_hparams)
</source>
</class>

<class classid="39" nclones="4" nlines="11" similarity="81">
<source file="systems/texar-0.2.4/texar/tf/modules/regressors/xlnet_regressor.py" startline="117" endline="173" pcid="920">
    def default_hparams():
        r"""Returns a dictionary of hyperparameters with default values.

        .. code-block:: python

            {
                # (1) Same hyperparameters as in XLNetEncoder
                ...
                # (2) Additional hyperparameters
                "regr_strategy": "cls_time",
                "use_projection": True,
                "logit_layer_kwargs": None,
                "name": "xlnet_regressor",
            }

        Here:

        1. Same hyperparameters as in
           :class:`~texar.tf.modules.XLNetEncoder`.
           See the :meth:`~texar.tf.modules.XLNetEncoder.default_hparams`.
           An instance of XLNetEncoder is created for feature extraction.

        2. Additional hyperparameters:

            `"regr_strategy"`: str
                The regression strategy, one of:

                - **cls_time**: Sequence-level regression based on the
                  output of the first time step (which is the `CLS` token).
                  Each sequence has a prediction.
                - **all_time**: Sequence-level regression based on
                  the output of all time steps. Each sequence has a prediction.
                - **time_wise**: Step-wise regression, i.e., make
                  regression for each time step based on its output.

            `"logit_layer_kwargs"` : dict
                Keyword arguments for the logit Dense layer constructor,
                except for argument "units" which is set to "num_classes".
                Ignored if no extra logit layer is appended.

            `"use_projection"`: bool
                If `True`, an additional dense layer is added after
                the summary step.

            `"name"`: str
                Name of the regressor.
        """
        hparams = XLNetEncoder.default_hparams()
        hparams.update({
            "logit_layer_kwargs": None,
            "regr_strategy": "cls_time",
            "dropout": 0.1,
            "use_projection": True,
            "name": "xlnet_regressor"
        })
        return hparams

</source>
<source file="systems/texar-0.2.4/texar/tf/modules/classifiers/xlnet_classifier.py" startline="122" endline="189" pcid="1014">
    def default_hparams():
        r"""Returns a dictionary of hyperparameters with default values.

        .. code-block:: python

            {
                # (1) Same hyperparameters as in XLNetEncoder
                ...
                # (2) Additional hyperparameters
                "clas_strategy": "cls_time",
                "use_projection": True,
                "num_classes": 2,
                "logit_layer_kwargs": None,
                "name": "xlnet_classifier",
            }

        Here:

        1. Same hyperparameters as in
            :class:`~texar.tf.modules.XLNetEncoder`.
            See the :meth:`~texar.tf.modules.XLNetEncoder.default_hparams`.
            An instance of XLNetEncoder is created for feature extraction.

        2. Additional hyperparameters:

            `"clas_strategy"`: str
                The classification strategy, one of:

                - **cls_time**: Sequence-level classification based on the
                  output of the last time step (which is the `CLS` token).
                  Each sequence has a class.
                - **all_time**: Sequence-level classification based on
                  the output of all time steps. Each sequence has a class.
                - **time_wise**: Step-wise classification, i.e., make
                  classification for each time step based on its output.

            `"use_projection"`: bool
                If `True`, an additional `Dense` layer is added after the
                summary step.

            `"num_classes"`: int
                Number of classes:

                - If **> 0**, an additional dense layer is appended to the
                  encoder to compute the logits over classes.
                - If **<= 0**, no dense layer is appended. The number of
                  classes is assumed to be the final dense layer size of the
                  encoder.

            `"logit_layer_kwargs"` : dict
                Keyword arguments for the logit Dense layer constructor,
                except for argument "units" which is set to "num_classes".
                Ignored if no extra logit layer is appended.

            `"name"`: str
                Name of the classifier.
        """
        hparams = XLNetEncoder.default_hparams()
        hparams.update({
            "num_classes": 2,
            "logit_layer_kwargs": None,
            "clas_strategy": "cls_time",
            "dropout": 0.1,
            "use_projection": True,
            "name": "xlnet_classifier"
        })
        return hparams

</source>
<source file="systems/texar-0.2.4/texar/tf/modules/classifiers/gpt2_classifier.py" startline="109" endline="181" pcid="1010">
    def default_hparams():
        r"""Returns a dictionary of hyperparameters with default values.

        .. code-block:: python

            {
                # (1) Same hyperparameters as in GPT2Encoder
                ...
                # (2) Additional hyperparameters
                "num_classes": 2,
                "logit_layer_kwargs": None,
                "clas_strategy": `cls_time`,
                "max_seq_length": None,
                "dropout": 0.1,
                "name": `gpt2_classifier`
            }

        Here:

        1. Same hyperparameters as in
        :class:`~texar.tf.modules.GPT2Encoder`.
        See the :meth:`~texar.tf.modules.GPT2Encoder.default_hparams`.
        An instance of GPT2Encoder is created for feature extraction.

        2. Additional hyperparameters:

            `"num_classes"`: int
                Number of classes:

                - If **> 0**, an additional :tf_main:`Dense <layers/Dense>`
                  layer is appended to the encoder to compute the logits over
                  classes.
                - If **<= 0**, no dense layer is appended. The number of
                  classes is assumed to be the final dense layer size of the
                  encoder.

            `"logit_layer_kwargs"`: dict
                Keyword arguments for the logit Dense layer constructor,
                except for argument "units" which is set to `num_classes`.
                Ignored if no extra logit layer is appended.

            `"clas_strategy"`: str
                The classification strategy, one of:

                - **cls_time**: Sequence-level classification based on the
                  output of the first time step (which is the `CLS` token).
                  Each sequence has a class.
                - **all_time**: Sequence-level classification based on
                  the output of all time steps. Each sequence has a class.
                - **time_wise**: Step-wise classification, i.e., make
                  classification for each time step based on its output.

            `"max_seq_length"`: int, optional
                Maximum possible length of input sequences. Required if
                `clas_strategy` is `all_time`.

            `"dropout"`: float
                The dropout rate of the BERT encoder output.

            `"name"`: str
                Name of the classifier.
        """
        hparams = GPT2Encoder.default_hparams()
        hparams.update({
            "num_classes": 2,
            "logit_layer_kwargs": None,
            "clas_strategy": "cls_time",
            "max_seq_length": None,
            "dropout": 0.1,
            "name": "gpt2_classifier"
        })
        return hparams

</source>
<source file="systems/texar-0.2.4/texar/tf/modules/classifiers/bert_classifier.py" startline="116" endline="189" pcid="1026">
    def default_hparams():
        r"""Returns a dictionary of hyperparameters with default values.

        .. code-block:: python

            {
                # (1) Same hyperparameters as in BertEncoder
                ...
                # (2) Additional hyperparameters
                "num_classes": 2,
                "logit_layer_kwargs": None,
                "clas_strategy": "cls_time",
                "max_seq_length": None,
                "dropout": 0.1,
                "name": "bert_classifier"
            }

        Here:

        1. Same hyperparameters as in
        :class:`~texar.tf.modules.BertEncoder`.
        See the :meth:`~texar.tf.modules.BertEncoder.default_hparams`.
        An instance of BertEncoder is created for feature extraction.

        2. Additional hyperparameters:

            `"num_classes"`: int
                Number of classes:

                - If **> 0**, an additional :tf_main:`Dense <layers/Dense>`
                  layer is appended to the encoder to compute the logits over
                  classes.
                - If **<= 0**, no dense layer is appended. The number of
                  classes is assumed to be the final dense layer size of the
                  encoder.

            `"logit_layer_kwargs"`: dict
                Keyword arguments for the logit Dense layer constructor,
                except for argument "units" which is set to `num_classes`.
                Ignored if no extra logit layer is appended.

            `"clas_strategy"`: str
                The classification strategy, one of:

                - **cls_time**: Sequence-level classification based on the
                  output of the first time step (which is the `CLS` token).
                  Each sequence has a class.
                - **all_time**: Sequence-level classification based on
                  the output of all time steps. Each sequence has a class.
                - **time_wise**: Step-wise classification, i.e., make
                  classification for each time step based on its output.

            `"max_seq_length"`: int, optional
                Maximum possible length of input sequences. Required if
                `clas_strategy` is `all_time`.

            `"dropout"`: float
                The dropout rate of the BERT encoder output.

            `"name"`: str
                Name of the classifier.
        """

        hparams = BERTEncoder.default_hparams()
        hparams.update({
            "num_classes": 2,
            "logit_layer_kwargs": None,
            "clas_strategy": "cls_time",
            "max_seq_length": None,
            "dropout": 0.1,
            "name": "bert_classifier"
        })
        return hparams

</source>
</class>

<class classid="40" nclones="2" nlines="23" similarity="100">
<source file="systems/texar-0.2.4/texar/tf/modules/regressors/xlnet_regressor.py" startline="174" endline="221" pcid="921">
    def param_groups(self, lr=None, lr_layer_scale=1.0,
                     decay_base_params=False):
        r"""Create parameter groups for optimizers. When
        :attr:`lr_layer_decay_rate` is not 1.0, parameters from each layer form
        separate groups with different base learning rates.

        This method should be called before applying gradients to the variables
        through the optimizer. Particularly, after calling the optimizer's
        `compute_gradients` method, the user can call this method to get
        variable-specific learning rates for the network. The gradients for each
        variables can then be scaled accordingly. These scaled gradients are
        finally applied by calling optimizer's `apply_gradients` method.

        Args:
            lr (float): The learning rate. Can be omitted if
                :attr:`lr_layer_decay_rate` is 1.0.
            lr_layer_scale (float): Per-layer LR scaling rate. The `i`-th layer
                will be scaled by `lr_layer_scale ^ (num_layers - i - 1)`.
            decay_base_params (bool): If `True`, treat non-layer parameters
                (e.g. embeddings) as if they're in layer 0. If `False`, these
                parameters are not scaled.

        Returns: A dict mapping tensorflow variables to their learning rates.
        """
        vars_to_learning_rates = {}
        if lr_layer_scale != 1.0:
            if lr is None:
                raise ValueError(
                    "lr must be specified when lr_layer_decay_rate is not 1.0")

            scope = self.variable_scope.name
            projection_vars = tf.trainable_variables(scope=scope + "/dense")
            logits_vars = tf.trainable_variables(
                scope=self.variable_scope.name + "/logit_layer")
            finetune_vars = projection_vars + logits_vars
            for var in finetune_vars:
                vars_to_learning_rates[var] = lr

            vars_to_learning_rates.update(
                self._encoder.param_groups(lr=lr,
                                           lr_layer_scale=lr_layer_scale,
                                           decay_base_params=decay_base_params))
        else:
            for variable in self.trainable_variables:
                vars_to_learning_rates[variable] = lr

        return vars_to_learning_rates

</source>
<source file="systems/texar-0.2.4/texar/tf/modules/classifiers/xlnet_classifier.py" startline="190" endline="237" pcid="1015">
    def param_groups(self, lr=None, lr_layer_scale=1.0,
                     decay_base_params=False):
        r"""Create parameter groups for optimizers. When
        :attr:`lr_layer_decay_rate` is not 1.0, parameters from each layer form
        separate groups with different base learning rates.

        This method should be called before applying gradients to the variables
        through the optimizer. Particularly, after calling the optimizer's
        `compute_gradients` method, the user can call this method to get
        variable-specific learning rates for the network. The gradients for each
        variables can then be scaled accordingly. These scaled gradients are
        finally applied by calling optimizer's `apply_gradients` method.

        Args:
            lr (float): The learning rate. Can be omitted if
                :attr:`lr_layer_decay_rate` is 1.0.
            lr_layer_scale (float): Per-layer LR scaling rate. The `i`-th layer
                will be scaled by `lr_layer_scale ^ (num_layers - i - 1)`.
            decay_base_params (bool): If `True`, treat non-layer parameters
                (e.g. embeddings) as if they're in layer 0. If `False`, these
                parameters are not scaled.

        Returns: A dict mapping tensorflow variables to their learning rates.
        """
        vars_to_learning_rates = {}
        if lr_layer_scale != 1.0:
            if lr is None:
                raise ValueError(
                    "lr must be specified when lr_layer_decay_rate is not 1.0")

            scope = self.variable_scope.name
            projection_vars = tf.trainable_variables(scope=scope + "/dense")
            logits_vars = tf.trainable_variables(
                scope=self.variable_scope.name + "/logit_layer")
            finetune_vars = projection_vars + logits_vars
            for var in finetune_vars:
                vars_to_learning_rates[var] = lr

            vars_to_learning_rates.update(
                self._encoder.param_groups(lr=lr,
                                           lr_layer_scale=lr_layer_scale,
                                           decay_base_params=decay_base_params))
        else:
            for variable in self.trainable_variables:
                vars_to_learning_rates[variable] = lr

        return vars_to_learning_rates

</source>
</class>

<class classid="41" nclones="2" nlines="26" similarity="100">
<source file="systems/texar-0.2.4/texar/tf/modules/regressors/xlnet_regressor_test.py" startline="35" endline="72" pcid="924">
    def test_trainable_variables(self):
        """Tests the functionality of automatically collecting trainable
        variables.
        """
        inputs = tf.placeholder(dtype=tf.int32, shape=[None, None])

        # case 1
        hparams = {
            "pretrained_model_name": None,
        }
        regressor = XLNetRegressor(hparams=hparams)
        regressor(inputs)
        n_xlnet_vars = 162
        n_projection_vars = 2
        n_logits_vars = 2
        self.assertEqual(len(regressor.trainable_variables),
                         n_xlnet_vars + n_logits_vars + n_projection_vars)

        # case 2
        hparams = {
            "pretrained_model_name": None,
            "regr_strategy": "all_time"
        }
        regressor = XLNetRegressor(hparams=hparams)
        regressor(inputs)
        self.assertEqual(len(regressor.trainable_variables),
                         n_xlnet_vars + n_logits_vars + n_projection_vars)

        # case 3
        hparams = {
            "pretrained_model_name": None,
            "regr_strategy": "time_wise"
        }
        regressor = XLNetRegressor(hparams=hparams)
        regressor(inputs)
        self.assertEqual(len(regressor.trainable_variables),
                         n_xlnet_vars + n_logits_vars + n_projection_vars)

</source>
<source file="systems/texar-0.2.4/texar/tf/modules/classifiers/xlnet_classifier_test.py" startline="35" endline="72" pcid="1022">
    def test_trainable_variables(self):
        """Tests the functionality of automatically collecting trainable
        variables.
        """
        inputs = tf.placeholder(dtype=tf.int32, shape=[None, None])

        # case 1
        hparams = {
            "pretrained_model_name": None,
        }
        clas = XLNetClassifier(hparams=hparams)
        clas(inputs)
        n_xlnet_vars = 162
        n_projection_vars = 2
        n_logits_vars = 2
        self.assertEqual(len(clas.trainable_variables),
                         n_xlnet_vars + n_logits_vars + n_projection_vars)

        # case 2
        hparams = {
            "pretrained_model_name": None,
            "clas_strategy": "time_wise"
        }
        clas = XLNetClassifier(hparams=hparams)
        clas(inputs)
        self.assertEqual(len(clas.trainable_variables),
                         n_xlnet_vars + n_logits_vars + n_projection_vars)

        # case 3
        hparams = {
            "pretrained_model_name": None,
            "clas_strategy": "all_time"
        }
        clas = XLNetClassifier(hparams=hparams)
        clas(inputs)
        self.assertEqual(len(clas.trainable_variables),
                         n_xlnet_vars + n_logits_vars + n_projection_vars)

</source>
</class>

<class classid="42" nclones="2" nlines="15" similarity="93">
<source file="systems/texar-0.2.4/texar/tf/modules/qnets/qnets.py" startline="66" endline="126" pcid="931">
    def default_hparams():
        """Returns a dictionary of hyperparameters with default values.

        .. role:: python(code)
           :language: python

        .. code-block:: python

            {
                'network_type': 'FeedForwardNetwork',
                'network_hparams': {
                    'layers': [
                        {
                            'type': 'Dense',
                            'kwargs': {'units': 256, 'activation': 'relu'}
                        },
                        {
                            'type': 'Dense',
                            'kwargs': {'units': 256, 'activation': 'relu'}
                        },
                    ]
                },
                'name': 'q_net',
            }

        Here:

        "network_type": str or class or instance
            A network that takes in state and returns outputs for
            generating actions. This can be a class, its name or module path,
            or a class instance. Ignored if `network` is given to the
            constructor.

        "network_hparams": dict
            Hyperparameters for the network. With the :attr:`network_kwargs`
            argument to the constructor, a network is created with
            :python:`network_class(**network_kwargs, hparams=network_hparams)`.

            For example, the default values creates a two-layer dense network.

        "name": str
            Name of the Q net.
        """
        return {
            'network_type': 'FeedForwardNetwork',
            'network_hparams': {
                'layers': [
                    {
                        'type': 'Dense',
                        'kwargs': {'units': 256, 'activation': 'relu'}
                    },
                    {
                        'type': 'Dense',
                        'kwargs': {'units': 256, 'activation': 'relu'}
                    },
                ]
            },
            'name': 'q_net',
            '@no_typecheck': ['network_type', 'network_hparams']
        }

</source>
<source file="systems/texar-0.2.4/texar/tf/modules/policies/policy_nets.py" startline="68" endline="134" pcid="979">
    def default_hparams():
        """Returns a dictionary of hyperparameters with default values.

        .. role:: python(code)
           :language: python

        .. code-block:: python

            {
                'network_type': 'FeedForwardNetwork',
                'network_hparams': {
                    'layers': [
                        {
                            'type': 'Dense',
                            'kwargs': {'units': 256, 'activation': 'relu'}
                        },
                        {
                            'type': 'Dense',
                            'kwargs': {'units': 256, 'activation': 'relu'}
                        },
                    ]
                },
                'distribution_kwargs': None,
                'name': 'policy_net',
            }

        Here:

        "network_type": str or class or instance
            A network that takes in state and returns outputs for
            generating actions. This can be a class, its name or module path,
            or a class instance. Ignored if `network` is given to the
            constructor.

        "network_hparams": dict
            Hyperparameters for the network. With the :attr:`network_kwargs`
            argument to the constructor, a network is created with
            :python:`network_class(**network_kwargs, hparams=network_hparams)`.

            For example, the default values creates a two-layer dense network.

        "distribution_kwargs": dict, optional
            Keyword arguments for distribution constructor. A distribution
            would be created for action sampling.

        "name": str
            Name of the policy.
        """
        return {
            'network_type': 'FeedForwardNetwork',
            'network_hparams': {
                'layers': [
                    {
                        'type': 'Dense',
                        'kwargs': {'units': 256, 'activation': 'relu'}
                    },
                    {
                        'type': 'Dense',
                        'kwargs': {'units': 256, 'activation': 'relu'}
                    },
                ]
            },
            'distribution_kwargs': None,
            'name': 'policy_net',
            '@no_typecheck': ['network_type', 'network_hparams']
        }

</source>
</class>

<class classid="43" nclones="2" nlines="10" similarity="100">
<source file="systems/texar-0.2.4/texar/tf/modules/qnets/qnets.py" startline="127" endline="137" pcid="932">
    def _build_network(self, network, kwargs):
        if network is not None:
            self._network = network
        else:
            kwargs = utils.get_instance_kwargs(
                kwargs, self._hparams.network_hparams)
            self._network = utils.check_or_get_instance(
                self._hparams.network_type,
                kwargs,
                module_paths=['texar.tf.modules', 'texar.tf.custom'])

</source>
<source file="systems/texar-0.2.4/texar/tf/modules/policies/policy_nets.py" startline="135" endline="145" pcid="980">
    def _build_network(self, network, kwargs):
        if network is not None:
            self._network = network
        else:
            kwargs = utils.get_instance_kwargs(
                kwargs, self._hparams.network_hparams)
            self._network = utils.check_or_get_instance(
                self._hparams.network_type,
                kwargs,
                module_paths=['texar.tf.modules', 'texar.tf.custom'])

</source>
</class>

<class classid="44" nclones="2" nlines="12" similarity="100">
<source file="systems/texar-0.2.4/texar/tf/modules/qnets/qnets.py" startline="173" endline="186" pcid="935">
    def __init__(self,
                 action_space=None,
                 network=None,
                 network_kwargs=None,
                 hparams=None):
        QNetBase.__init__(self, hparams=hparams)

        with tf.variable_scope(self.variable_scope):
            if action_space is None:
                action_space = Space(
                    low=0, high=self._hparams.action_space, dtype=np.int32)
            self._action_space = action_space
            self._append_output_layer()

</source>
<source file="systems/texar-0.2.4/texar/tf/modules/policies/policy_nets.py" startline="186" endline="199" pcid="983">
    def __init__(self,
                 action_space=None,
                 network=None,
                 network_kwargs=None,
                 hparams=None):
        PolicyNetBase.__init__(self, hparams=hparams)

        with tf.variable_scope(self.variable_scope):
            if action_space is None:
                action_space = Space(
                    low=0, high=self._hparams.action_space, dtype=np.int32)
            self._action_space = action_space
            self._append_output_layer()

</source>
</class>

<class classid="45" nclones="2" nlines="11" similarity="100">
<source file="systems/texar-0.2.4/texar/tf/modules/qnets/qnets.py" startline="233" endline="246" pcid="937">
    def _append_output_layer(self):
        if not self._hparams.make_output_layer:
            return

        if self._action_space.shape != ():
            raise ValueError('Only scalar discrete action is supported.')
        else:
            output_size = self._action_space.high - self._action_space.low

        layer_hparams = {
            'type': 'Dense',
            'kwargs': {'units': output_size}}
        self._network.append_layer(layer_hparams)

</source>
<source file="systems/texar-0.2.4/texar/tf/modules/policies/policy_nets.py" startline="265" endline="279" pcid="985">
    def _append_output_layer(self):
        if not self._hparams.make_output_layer:
            return

        if self._action_space.shape != ():
            raise ValueError('Only scalar discrete action is supported.')
        else:
            output_size = self._action_space.high - self._action_space.low

        layer_hparams = {
            'type': 'Dense',
            'kwargs': {'units': output_size}
        }
        self._network.append_layer(layer_hparams)

</source>
</class>

<class classid="46" nclones="2" nlines="48" similarity="76">
<source file="systems/texar-0.2.4/texar/tf/modules/classifiers/rnn_classifiers_test.py" startline="46" endline="116" pcid="1003">

    def test_encode(self):
        """Tests encoding.
        """
        max_time = 8
        batch_size = 16
        emb_dim = 100
        inputs = tf.random_uniform([batch_size, max_time, emb_dim],
                                   maxval=1., dtype=tf.float32)

        # case 1
        clas = UnidirectionalRNNClassifier()
        logits, pred = clas(inputs)

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            logits_, pred_ = sess.run([logits, pred])
            self.assertEqual(logits_.shape, (batch_size, clas.num_classes))
            self.assertEqual(pred_.shape, (batch_size, ))

        # case 2
        hparams = {
            "num_classes": 10,
            "clas_strategy": "time_wise"
        }
        clas = UnidirectionalRNNClassifier(hparams=hparams)
        logits, pred = clas(inputs)

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            logits_, pred_ = sess.run([logits, pred])
            self.assertEqual(logits_.shape,
                             (batch_size, max_time, clas.num_classes))
            self.assertEqual(pred_.shape, (batch_size, max_time))

        # case 3
        hparams = {
            "output_layer": {
                "num_layers": 1,
                "layer_size": 10
            },
            "num_classes": 0,
            "clas_strategy": "time_wise"
        }
        clas = UnidirectionalRNNClassifier(hparams=hparams)
        logits, pred = clas(inputs)

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            logits_, pred_ = sess.run([logits, pred])
            self.assertEqual(logits_.shape,
                             (batch_size, max_time, 10))
            self.assertEqual(pred_.shape, (batch_size, max_time))

        # case 4
        hparams = {
            "num_classes": 10,
            "clas_strategy": "all_time",
            "max_seq_length": max_time
        }
        inputs = tf.placeholder(tf.float32, shape=[batch_size, 6, emb_dim])
        clas = UnidirectionalRNNClassifier(hparams=hparams)
        logits, pred = clas(inputs)

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            logits_, pred_ = sess.run(
                [logits, pred],
                feed_dict={inputs: np.random.randn(batch_size, 6, emb_dim)})
            self.assertEqual(logits_.shape, (batch_size, clas.num_classes))
            self.assertEqual(pred_.shape, (batch_size, ))
</source>
<source file="systems/texar-0.2.4/texar/tf/modules/classifiers/rnn_classifiers_test.py" startline="117" endline="178" pcid="1004">

    def test_binary(self):
        """Tests binary classification.
        """
        max_time = 8
        batch_size = 16
        emb_dim = 100
        inputs = tf.random_uniform([batch_size, max_time, emb_dim],
                                   maxval=1., dtype=tf.float32)

        # case 1 omittd

        # case 2
        hparams = {
            "num_classes": 1,
            "clas_strategy": "time_wise"
        }
        clas = UnidirectionalRNNClassifier(hparams=hparams)
        logits, pred = clas(inputs)

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            logits_, pred_ = sess.run([logits, pred])
            self.assertEqual(logits_.shape, (batch_size, max_time))
            self.assertEqual(pred_.shape, (batch_size, max_time))

        # case 3
        hparams = {
            "output_layer": {
                "num_layers": 1,
                "layer_size": 10
            },
            "num_classes": 1,
            "clas_strategy": "time_wise"
        }
        clas = UnidirectionalRNNClassifier(hparams=hparams)
        logits, pred = clas(inputs)

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            logits_, pred_ = sess.run([logits, pred])
            self.assertEqual(logits_.shape, (batch_size, max_time))
            self.assertEqual(pred_.shape, (batch_size, max_time))

        # case 4
        hparams = {
            "num_classes": 1,
            "clas_strategy": "all_time",
            "max_seq_length": max_time
        }
        inputs = tf.placeholder(tf.float32, shape=[batch_size, 6, emb_dim])
        clas = UnidirectionalRNNClassifier(hparams=hparams)
        logits, pred = clas(inputs)

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            logits_, pred_ = sess.run(
                [logits, pred],
                feed_dict={inputs: np.random.randn(batch_size, 6, emb_dim)})
            self.assertEqual(logits_.shape, (batch_size, ))
            self.assertEqual(pred_.shape, (batch_size, ))

</source>
</class>

<class classid="47" nclones="2" nlines="23" similarity="85">
<source file="systems/texar-0.2.4/texar/tf/modules/classifiers/bert_classifier_test.py" startline="34" endline="66" pcid="1006">
    def test_trainable_variables(self):
        """Tests the functionality of automatically collecting trainable
        variables.
        """
        inputs = tf.placeholder(dtype=tf.int32, shape=[None, None])

        # case 1
        hparams = {
            "pretrained_model_name": None,
        }
        clas = BERTClassifier(hparams=hparams)
        _, _ = clas(inputs)
        self.assertEqual(len(clas.trainable_variables), 199 + 2)

        # case 2
        hparams = {
            "pretrained_model_name": None,
            "clas_strategy": "all_time",
            "max_seq_length": 8,
        }
        clas = BERTClassifier(hparams=hparams)
        _, _ = clas(inputs)
        self.assertEqual(len(clas.trainable_variables), 199 + 2)

        # case 2
        hparams = {
            "pretrained_model_name": None,
            "clas_strategy": "time_wise",
        }
        clas = BERTClassifier(hparams=hparams)
        _, _ = clas(inputs)
        self.assertEqual(len(clas.trainable_variables), 199 + 2)

</source>
<source file="systems/texar-0.2.4/texar/tf/modules/classifiers/gpt2_classifier_test.py" startline="27" endline="59" pcid="1032">
    def test_trainable_variables(self):
        """Tests the functionality of automatically collecting trainable
        variables.
        """
        inputs = tf.placeholder(dtype=tf.int32, shape=[None, None])

        # case 1
        hparams = {
            "pretrained_model_name": None,
        }
        clas = GPT2Classifier(hparams=hparams)
        _, _ = clas(inputs)
        self.assertEqual(len(clas.trainable_variables), 198)

        # case 2
        hparams = {
            "pretrained_model_name": None,
            "clas_strategy": "all_time",
            "max_seq_length": 8,
        }
        clas = GPT2Classifier(hparams=hparams)
        _, _ = clas(inputs)
        self.assertEqual(len(clas.trainable_variables), 198)

        # case 3
        hparams = {
            "pretrained_model_name": None,
            "clas_strategy": "time_wise",
        }
        clas = GPT2Classifier(hparams=hparams)
        _, _ = clas(inputs)
        self.assertEqual(len(clas.trainable_variables), 198)

</source>
</class>

<class classid="48" nclones="6" nlines="51" similarity="71">
<source file="systems/texar-0.2.4/texar/tf/modules/classifiers/bert_classifier_test.py" startline="67" endline="140" pcid="1007">
    def test_encode(self):
        """Tests encoding.
        """
        max_time = 8
        batch_size = 16
        inputs = tf.random_uniform([batch_size, max_time],
                                   maxval=30521, dtype=tf.int32)
        # case 1
        hparams = {
            "pretrained_model_name": None,
        }
        clas = BERTClassifier(hparams=hparams)
        logits, pred = clas(inputs)

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            logits_, pred_ = sess.run([logits, pred])
            self.assertEqual(logits_.shape, (batch_size,
                                             clas.hparams.num_classes))
            self.assertEqual(pred_.shape, (batch_size, ))

        # case 2
        hparams = {
            "pretrained_model_name": None,
            "num_classes": 10,
            "clas_strategy": "time_wise"
        }
        clas = BERTClassifier(hparams=hparams)
        logits, pred = clas(inputs)

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            logits_, pred_ = sess.run([logits, pred])
            self.assertEqual(logits_.shape,
                             (batch_size, max_time, clas.hparams.num_classes))
            self.assertEqual(pred_.shape, (batch_size, max_time))

        # case 3
        hparams = {
            "pretrained_model_name": None,
            "num_classes": 0,
            "clas_strategy": "time_wise"
        }
        clas = BERTClassifier(hparams=hparams)
        logits, pred = clas(inputs)

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            logits_, pred_ = sess.run([logits, pred])
            self.assertEqual(logits_.shape,
                             (batch_size, max_time, clas.hparams.encoder.dim))
            self.assertEqual(pred_.shape, (batch_size, max_time))

        # case 4
        hparams = {
            "pretrained_model_name": None,
            "num_classes": 10,
            "clas_strategy": "all_time",
            "max_seq_length": max_time
        }
        inputs = tf.placeholder(tf.int32, shape=[batch_size, 6])
        clas = BERTClassifier(hparams=hparams)
        logits, pred = clas(inputs)

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            logits_, pred_ = sess.run(
                [logits, pred],
                feed_dict={inputs: np.random.randint(30521,
                                                     size=(batch_size, 6))})
            self.assertEqual(logits_.shape, (batch_size,
                                             clas.hparams.num_classes))
            self.assertEqual(pred_.shape, (batch_size, ))

</source>
<source file="systems/texar-0.2.4/texar/tf/modules/classifiers/xlnet_classifier_test.py" startline="73" endline="148" pcid="1023">
    def test_encode(self):
        """Tests encoding.
        """
        max_time = 8
        batch_size = 16
        inputs = tf.random_uniform([batch_size, max_time],
                                   maxval=30521, dtype=tf.int32)

        # case 1
        hparams = {
            "pretrained_model_name": None
        }
        clas = XLNetClassifier(hparams=hparams)
        logits, pred = clas(inputs)

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            logits_, pred_ = sess.run([logits, pred])
            self.assertEqual(logits_.shape, (batch_size,
                                             clas.hparams.num_classes))
            self.assertEqual(pred_.shape, (batch_size,))

        # case 2
        hparams = {
            "pretrained_model_name": None,
            "num_classes": 10,
            "clas_strategy": "time_wise"
        }
        clas = XLNetClassifier(hparams=hparams)
        logits, pred = clas(inputs)

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            logits_, pred_ = sess.run([logits, pred])
            self.assertEqual(logits_.shape,
                             (batch_size, max_time, clas.hparams.num_classes))
            self.assertEqual(pred_.shape, (batch_size, max_time))

        # case 3
        hparams = {
            "pretrained_model_name": None,
            "num_classes": 0,
            "clas_strategy": "time_wise"
        }
        clas = XLNetClassifier(hparams=hparams)
        logits, pred = clas(inputs)

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            logits_, pred_ = sess.run([logits, pred])
            self.assertEqual(logits_.shape,
                             (batch_size, max_time, clas.hparams.hidden_dim))
            self.assertEqual(pred_.shape, (batch_size, max_time))

        # case 4
        hparams = {
            "pretrained_model_name": None,
            "num_classes": 3,
            "clas_strategy": "all_time",
            "use_projection": False,
            "vocab_size": 40000
        }
        inputs = tf.placeholder(tf.int32, shape=[batch_size, 6])
        clas = XLNetClassifier(hparams=hparams)
        logits, pred = clas(inputs)

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            logits_, pred_ = sess.run(
                [logits, pred],
                feed_dict={inputs: np.random.randint(30521,
                                                     size=(batch_size, 6))})
            self.assertEqual(logits_.shape, (batch_size,
                                             clas.hparams.num_classes))
            self.assertEqual(pred_.shape, (batch_size,))

</source>
<source file="systems/texar-0.2.4/texar/tf/modules/classifiers/gpt2_classifier_test.py" startline="60" endline="126" pcid="1033">
    def test_classification(self):
        r"""Tests classificaiton.
        """
        max_time = 8
        batch_size = 16
        inputs = tf.random_uniform([batch_size, max_time],
                                   maxval=30521, dtype=tf.int32)

        # case 1
        hparams = {
            "pretrained_model_name": None,
        }
        classifier = GPT2Classifier(hparams=hparams)
        logits, preds = classifier(inputs)

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            logits_, pred_ = sess.run([logits, preds])
            self.assertEqual(logits_.shape, (batch_size, 2))
            self.assertEqual(pred_.shape, (batch_size,))

        # case 2
        hparams = {
            "pretrained_model_name": None,
            "num_classes": 10,
            "clas_strategy": "time_wise",
        }
        classifier = GPT2Classifier(hparams=hparams)
        logits, preds = classifier(inputs)

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            logits_, pred_ = sess.run([logits, preds])
            self.assertEqual(logits_.shape, (batch_size, max_time, 10))
            self.assertEqual(pred_.shape, (batch_size, max_time))

        # case 3
        hparams = {
            "pretrained_model_name": None,
            "num_classes": 0,
            "clas_strategy": "time_wise",
        }
        classifier = GPT2Classifier(hparams=hparams)
        logits, preds = classifier(inputs)

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            logits_, pred_ = sess.run([logits, preds])
            self.assertEqual(logits_.shape, (batch_size, max_time, 768))
            self.assertEqual(pred_.shape, (batch_size, max_time))

        # case 4
        hparams = {
            "pretrained_model_name": None,
            "num_classes": 10,
            "clas_strategy": "all_time",
            "max_seq_length": max_time,
        }
        classifier = GPT2Classifier(hparams=hparams)
        logits, preds = classifier(inputs)

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            logits_, pred_ = sess.run([logits, preds])
            self.assertEqual(logits_.shape, (batch_size, 10))
            self.assertEqual(pred_.shape, (batch_size,))

</source>
<source file="systems/texar-0.2.4/texar/tf/modules/classifiers/gpt2_classifier_test.py" startline="127" endline="182" pcid="1034">
    def test_binary(self):
        r"""Tests binary classification.
        """
        max_time = 8
        batch_size = 16
        inputs = tf.random_uniform([batch_size, max_time],
                                   maxval=30521, dtype=tf.int32)

        # case 1
        hparams = {
            "pretrained_model_name": None,
            "num_classes": 1,
            "clas_strategy": "time_wise",
        }
        classifier = GPT2Classifier(hparams=hparams)
        logits, preds = classifier(inputs)

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            logits_, pred_ = sess.run([logits, preds])
            self.assertEqual(logits_.shape, (batch_size, max_time))
            self.assertEqual(pred_.shape, (batch_size, max_time))

        # case 2
        hparams = {
            "pretrained_model_name": None,
            "num_classes": 1,
            "clas_strategy": "cls_time",
            "max_seq_length": max_time,
        }
        classifier = GPT2Classifier(hparams=hparams)
        logits, preds = classifier(inputs)

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            logits_, pred_ = sess.run([logits, preds])
            self.assertEqual(logits_.shape, (batch_size,))
            self.assertEqual(pred_.shape, (batch_size,))

        # case 3
        hparams = {
            "pretrained_model_name": None,
            "num_classes": 1,
            "clas_strategy": "all_time",
            "max_seq_length": max_time,
        }
        classifier = GPT2Classifier(hparams=hparams)
        logits, preds = classifier(inputs)

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            logits_, pred_ = sess.run([logits, preds])
            self.assertEqual(logits_.shape, (batch_size,))
            self.assertEqual(pred_.shape, (batch_size,))


</source>
<source file="systems/texar-0.2.4/texar/tf/modules/classifiers/xlnet_classifier_test.py" startline="149" endline="212" pcid="1024">
    def test_binary(self):
        """Tests binary classification.
        """
        max_time = 8
        batch_size = 16
        inputs = tf.random_uniform([batch_size, max_time],
                                   maxval=30521, dtype=tf.int32)

        # case 1
        hparams = {
            "pretrained_model_name": None,
            "num_classes": 1,
            "clas_strategy": "time_wise"
        }
        clas = XLNetClassifier(hparams=hparams)
        logits, pred = clas(inputs)

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            logits_, pred_ = sess.run([logits, pred])
            self.assertEqual(logits_.shape, (batch_size, max_time))
            self.assertEqual(pred_.shape, (batch_size, max_time))

        # case 2
        hparams = {
            "pretrained_model_name": None,
            "num_classes": 1,
            "clas_strategy": "cls_time",
            "max_seq_len": max_time
        }
        inputs = tf.placeholder(tf.int32, shape=[batch_size, 6])
        clas = XLNetClassifier(hparams=hparams)
        logits, pred = clas(inputs)

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            logits_, pred_ = sess.run(
                [logits, pred],
                feed_dict={inputs: np.random.randint(30521,
                                                     size=(batch_size, 6))})
            self.assertEqual(logits_.shape, (batch_size,))
            self.assertEqual(pred_.shape, (batch_size,))

        # case 3
        hparams = {
            "pretrained_model_name": None,
            "num_classes": 1,
            "clas_strategy": "all_time",
            "max_seq_len": max_time
        }
        inputs = tf.placeholder(tf.int32, shape=[batch_size, 6])
        clas = XLNetClassifier(hparams=hparams)
        logits, pred = clas(inputs)

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            logits_, pred_ = sess.run(
                [logits, pred],
                feed_dict={inputs: np.random.randint(30521,
                                                     size=(batch_size, 6))})
            self.assertEqual(logits_.shape, (batch_size,))
            self.assertEqual(pred_.shape, (batch_size,))


</source>
<source file="systems/texar-0.2.4/texar/tf/modules/classifiers/bert_classifier_test.py" startline="141" endline="204" pcid="1008">
    def test_binary(self):
        """Tests binary classification.
        """
        max_time = 8
        batch_size = 16
        inputs = tf.random_uniform([batch_size, max_time],
                                   maxval=30521, dtype=tf.int32)

        # case 2
        hparams = {
            "pretrained_model_name": None,
            "num_classes": 1,
            "clas_strategy": "time_wise"
        }
        clas = BERTClassifier(hparams=hparams)
        logits, pred = clas(inputs)

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            logits_, pred_ = sess.run([logits, pred])
            self.assertEqual(logits_.shape, (batch_size, max_time))
            self.assertEqual(pred_.shape, (batch_size, max_time))

        # case 3
        hparams = {
            "pretrained_model_name": None,
            "num_classes": 1,
            "clas_strategy": "cls_time",
            "max_seq_length": max_time
        }
        inputs = tf.placeholder(tf.int32, shape=[batch_size, 6])
        clas = BERTClassifier(hparams=hparams)
        logits, pred = clas(inputs)

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            logits_, pred_ = sess.run(
                [logits, pred],
                feed_dict={inputs: np.random.randint(30521,
                                                     size=(batch_size, 6))})
            self.assertEqual(logits_.shape, (batch_size, ))
            self.assertEqual(pred_.shape, (batch_size, ))

        # case 4
        hparams = {
            "pretrained_model_name": None,
            "num_classes": 1,
            "clas_strategy": "all_time",
            "max_seq_length": max_time
        }
        inputs = tf.placeholder(tf.int32, shape=[batch_size, 6])
        clas = BERTClassifier(hparams=hparams)
        logits, pred = clas(inputs)

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            logits_, pred_ = sess.run(
                [logits, pred],
                feed_dict={inputs: np.random.randint(30521,
                                                     size=(batch_size, 6))})
            self.assertEqual(logits_.shape, (batch_size, ))
            self.assertEqual(pred_.shape, (batch_size, ))


</source>
</class>

<class classid="49" nclones="2" nlines="52" similarity="81">
<source file="systems/texar-0.2.4/texar/tf/modules/classifiers/gpt2_classifier.py" startline="182" endline="283" pcid="1011">
    def _build(self,
               inputs,
               sequence_length=None,
               mode=None,
               **kwargs):
        r"""Feeds the inputs through the network and makes classification.

        The arguments are the same as in
        :class:`~texar.tf.modules.GPT2Encoder`.

        Args:
            inputs: A 2D Tensor of shape `[batch_size, max_time]`,
                containing the token ids of tokens in input sequences.
            sequence_length (optional): A 1D Tensor of shape `[batch_size]`.
                Input tokens beyond respective sequence lengths are masked
                out automatically.
            mode (optional): A tensor taking value in
                :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`,
                including `TRAIN`, `EVAL`, and `PREDICT`. Used to toggle
                dropout.
                If `None` (default), :func:`texar.tf.global_mode` is used.
            **kwargs: Keyword arguments.

        Returns:
            A tuple `(logits, pred)`, containing the logits over classes and
            the predictions, respectively.

            - If "clas_strategy"=="cls_time" or "all_time"

                - If "num_classes"==1, `logits` and `pred` are of both \
                  shape `[batch_size]`
                - If "num_classes">1, `logits` is of shape \
                  `[batch_size, num_classes]` and `pred` is of shape \
                  `[batch_size]`.

            - If "clas_strategy"=="time_wise",

                - If "num_classes"==1, `logits` and `pred` are of both \
                  shape `[batch_size, max_time]`
                - If "num_classes">1, `logits` is of shape \
                  `[batch_size, max_time, num_classes]` and `pred` is of shape \
                  `[batch_size, max_time]`.
        """
        enc_outputs = self._encoder(inputs, sequence_length, mode)

        # Compute logits
        strategy = self._hparams.clas_strategy
        if strategy == 'time_wise':
            logits = enc_outputs
        elif strategy == "cls_time":
            if sequence_length is None:
                logits = enc_outputs[:, -1, :]
            else:
                logits = tf.stack([enc_outputs[batch_idx, time_idx - 1, :]
                                   for batch_idx, time_idx in
                                   enumerate(sequence_length)], axis=0)
        elif strategy == "all_time":
            # Pad `enc_outputs` to have max_seq_length before flatten
            length_diff = self._hparams.max_seq_length - tf.shape(inputs)[1]
            length_diff = tf.reshape(length_diff, [1, 1])
            # Set `paddings = [[0, 0], [0, length_dif], [0, 0]]`
            paddings = tf.pad(length_diff, paddings=[[1, 1], [1, 0]])
            logit_input = tf.pad(enc_outputs, paddings=paddings)
            logit_input_dim = (self._hparams.encoder.dim *
                               self._hparams.max_seq_length)
            logits = tf.reshape(logit_input, [-1, logit_input_dim])
        else:
            raise ValueError('Unknown classification strategy: {}'.format(
                strategy))

        if self._logit_layer is not None:
            logits = self._dropout_layer(logits, training=mode)
            logits = self._logit_layer(logits)

        # Compute predications
        num_classes = self._hparams.num_classes
        is_binary = num_classes == 1
        is_binary = is_binary or (num_classes <= 0 and logits.shape[-1] == 1)

        if strategy == 'time_wise':
            if is_binary:
                pred = tf.squeeze(tf.greater(logits, 0), -1)
                logits = tf.squeeze(logits, -1)
            else:
                pred = tf.argmax(logits, axis=-1)
        else:
            if is_binary:
                pred = tf.greater(logits, 0)
                logits = tf.reshape(logits, [-1])
            else:
                pred = tf.argmax(logits, axis=-1)
            pred = tf.reshape(pred, [-1])
        pred = tf.cast(pred, tf.int64)

        if not self._built:
            self._add_internal_trainable_variables()
            if self._logit_layer:
                self._add_trainable_variable(
                    self._logit_layer.trainable_variables)
            self._built = True

        return logits, pred
</source>
<source file="systems/texar-0.2.4/texar/tf/modules/classifiers/bert_classifier.py" startline="190" endline="291" pcid="1027">
    def _build(self,
               inputs,
               sequence_length=None,
               segment_ids=None,
               mode=None,
               **kwargs):
        r"""Feeds the inputs through the network and makes classification.

        The arguments are the same as in
        :class:`~texar.tf.modules.BertEncoder`.

        Args:
            inputs: A 2D Tensor of shape `[batch_size, max_time]`,
                containing the token ids of tokens in input sequences.
            sequence_length (optional): A 1D Tensor of shape `[batch_size]`.
                Input tokens beyond respective sequence lengths are masked
                out automatically.
            segment_ids (optional): A 2D Tensor of shape
                `[batch_size, max_time]`, containing the segment ids
                of tokens in input sequences. If `None` (default), a tensor
                with all elements set to zero is used.
            mode (optional): A tensor taking value in
                :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`,
                including `TRAIN`, `EVAL`, and `PREDICT`. Used to toggle
                dropout.
                If `None` (default), :func:`texar.tf.global_mode` is used.
            **kwargs: Keyword arguments.

        Returns:
            A tuple `(logits, pred)`, containing the logits over classes and
            the predictions, respectively.

            - If "clas_strategy"=="cls_time" or "all_time"

                - If "num_classes"==1, `logits` and `pred` are of both \
                  shape `[batch_size]`
                - If "num_classes">1, `logits` is of shape \
                  `[batch_size, num_classes]` and `pred` is of shape \
                  `[batch_size]`.

            - If "clas_strategy"=="time_wise",

                - If "num_classes"==1, `logits` and `pred` are of both \
                  shape `[batch_size, max_time]`
                - If "num_classes">1, `logits` is of shape \
                  `[batch_size, max_time, num_classes]` and `pred` is of shape \
                  `[batch_size, max_time]`.
        """

        enc_outputs, pooled_output = self._encoder(inputs, sequence_length,
                                                   segment_ids, mode)

        # Compute logits
        stra = self._hparams.clas_strategy
        if stra == 'time_wise':
            logits = enc_outputs
        elif stra == 'cls_time':
            logits = pooled_output
        elif stra == 'all_time':
            # Pad `enc_outputs` to have max_seq_length before flatten
            length_diff = self._hparams.max_seq_length - tf.shape(inputs)[1]
            length_diff = tf.reshape(length_diff, [1, 1])
            # Set `paddings = [[0, 0], [0, length_dif], [0, 0]]`
            paddings = tf.pad(length_diff, paddings=[[1, 1], [1, 0]])
            logit_input = tf.pad(enc_outputs, paddings=paddings)
            logit_input_dim = self._hparams.hidden_size * \
                              self._hparams.max_seq_length
            logits = tf.reshape(logit_input, [-1, logit_input_dim])
        else:
            raise ValueError('Unknown classification strategy: {}'.format(stra))

        if self._logit_layer is not None:
            logits = self._dropout_layer(logits, training=mode)
            logits = self._logit_layer(logits)

        # Compute predications
        num_classes = self._hparams.num_classes
        is_binary = num_classes == 1
        is_binary = is_binary or (num_classes <= 0 and logits.shape[-1] == 1)

        if stra == 'time_wise':
            if is_binary:
                pred = tf.squeeze(tf.greater(logits, 0), -1)
                logits = tf.squeeze(logits, -1)
            else:
                pred = tf.argmax(logits, axis=-1)
        else:
            if is_binary:
                pred = tf.greater(logits, 0)
                logits = tf.reshape(logits, [-1])
            else:
                pred = tf.argmax(logits, axis=-1)
            pred = tf.reshape(pred, [-1])
        pred = tf.cast(pred, tf.int64)

        if not self._built:
            self._add_internal_trainable_variables()
            if self._logit_layer:
                self._add_trainable_variable(
                    self._logit_layer.trainable_variables)
            self._built = True

</source>
</class>

<class classid="50" nclones="2" nlines="14" similarity="85">
<source file="systems/texar-0.2.4/texar/tf/modules/embedders/embedders.py" startline="93" endline="112" pcid="1035">
    def __init__(self, init_value=None, vocab_size=None, hparams=None):
        EmbedderBase.__init__(self, hparams=hparams)

        if init_value is None and vocab_size is None:
            raise ValueError(
                "Either `init_value` or `vocab_size` is required.")

        self._init_parameterized_embedding(init_value, vocab_size,
                                           self._hparams)

        self._vocab_size = vocab_size
        if vocab_size is None:
            self._vocab_size = self._num_embeds
        if self._vocab_size != self._num_embeds:
            raise ValueError(
                'vocab_size must equal to init_value.shape[0].'
                'Got %d and %d' % (self._vocab_size, self._num_embeds))

        self._built = True

</source>
<source file="systems/texar-0.2.4/texar/tf/modules/embedders/position_embedders.py" startline="64" endline="86" pcid="1051">
    def __init__(self, init_value=None, position_size=None, hparams=None):
        EmbedderBase.__init__(self, hparams=hparams)

        if init_value is None and position_size is None:
            raise ValueError(
                "Either `init_value` or `position_size` is required."
            )

        self._init_parameterized_embedding(
            init_value, position_size, self._hparams
        )

        self._position_size = position_size
        if position_size is None:
            self._position_size = self._num_embeds
        if self._position_size != self._num_embeds:
            raise ValueError(
                "position_size must equal to init_value.shape[0]."
                "Got %d and %d" % (self._position_size, self._num_embeds)
            )

        self._built = True

</source>
</class>

<class classid="51" nclones="3" nlines="31" similarity="100">
<source file="systems/texar-0.2.4/examples/distributed_gpu/ptb_reader.py" startline="63" endline="102" pcid="1094">
def prepare_data(data_path):
    """Preprocess PTB data.
    """
    train_path = os.path.join(data_path, "ptb.train.txt")
    if not tf.gfile.Exists(train_path):
        url = 'http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz'
        tx.data.maybe_download(url, data_path, extract=True)
        data_path = os.path.join(data_path, 'simple-examples', 'data')

    train_path = os.path.join(data_path, "ptb.train.txt")
    valid_path = os.path.join(data_path, "ptb.valid.txt")
    test_path = os.path.join(data_path, "ptb.test.txt")

    word_to_id = tx.data.make_vocab(
        train_path, newline_token="<EOS>", return_type="dict")
    assert len(word_to_id) == 10000

    train_text = tx.data.read_words(
        train_path, newline_token="<EOS>")
    train_text_id = [word_to_id[w] for w in train_text if w in word_to_id]

    valid_text = tx.data.read_words(
        valid_path, newline_token="<EOS>")
    valid_text_id = [word_to_id[w] for w in valid_text if w in word_to_id]

    test_text = tx.data.read_words(
        test_path, newline_token="<EOS>")
    test_text_id = [word_to_id[w] for w in test_text if w in word_to_id]

    data = {
        "train_text": train_text,
        "valid_text": valid_text,
        "test_text": test_text,
        "train_text_id": train_text_id,
        "valid_text_id": valid_text_id,
        "test_text_id": test_text_id,
        "vocab": word_to_id,
        "vocab_size": len(word_to_id)
    }
    return data
</source>
<source file="systems/texar-0.2.4/examples/language_model_ptb/ptb_reader.py" startline="49" endline="88" pcid="1301">
def prepare_data(data_path):
    """Preprocess PTB data.
    """
    train_path = os.path.join(data_path, "ptb.train.txt")
    if not tf.gfile.Exists(train_path):
        url = 'http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz'
        tx.data.maybe_download(url, data_path, extract=True)
        data_path = os.path.join(data_path, 'simple-examples', 'data')

    train_path = os.path.join(data_path, "ptb.train.txt")
    valid_path = os.path.join(data_path, "ptb.valid.txt")
    test_path = os.path.join(data_path, "ptb.test.txt")

    word_to_id = tx.data.make_vocab(
        train_path, newline_token="<EOS>", return_type="dict")
    assert len(word_to_id) == 10000

    train_text = tx.data.read_words(
        train_path, newline_token="<EOS>")
    train_text_id = [word_to_id[w] for w in train_text if w in word_to_id]

    valid_text = tx.data.read_words(
        valid_path, newline_token="<EOS>")
    valid_text_id = [word_to_id[w] for w in valid_text if w in word_to_id]

    test_text = tx.data.read_words(
        test_path, newline_token="<EOS>")
    test_text_id = [word_to_id[w] for w in test_text if w in word_to_id]

    data = {
        "train_text": train_text,
        "valid_text": valid_text,
        "test_text": test_text,
        "train_text_id": train_text_id,
        "valid_text_id": valid_text_id,
        "test_text_id": test_text_id,
        "vocab": word_to_id,
        "vocab_size": len(word_to_id)
    }
    return data
</source>
<source file="systems/texar-0.2.4/examples/memory_network_lm/ptb_reader.py" startline="69" endline="108" pcid="1281">
def prepare_data(data_path):
    """Preprocess PTB data.
    """
    train_path = os.path.join(data_path, "ptb.train.txt")
    if not tf.gfile.Exists(train_path):
        url = 'http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz'
        tx.data.maybe_download(url, data_path, extract=True)
        data_path = os.path.join(data_path, 'simple-examples', 'data')

    train_path = os.path.join(data_path, "ptb.train.txt")
    valid_path = os.path.join(data_path, "ptb.valid.txt")
    test_path = os.path.join(data_path, "ptb.test.txt")

    word_to_id = tx.data.make_vocab(
        train_path, newline_token="<EOS>", return_type="dict")
    assert len(word_to_id) == 10000

    train_text = tx.data.read_words(
        train_path, newline_token="<EOS>")
    train_text_id = [word_to_id[w] for w in train_text if w in word_to_id]

    valid_text = tx.data.read_words(
        valid_path, newline_token="<EOS>")
    valid_text_id = [word_to_id[w] for w in valid_text if w in word_to_id]

    test_text = tx.data.read_words(
        test_path, newline_token="<EOS>")
    test_text_id = [word_to_id[w] for w in test_text if w in word_to_id]

    data = {
        "train_text": train_text,
        "valid_text": valid_text,
        "test_text": test_text,
        "train_text_id": train_text_id,
        "valid_text_id": valid_text_id,
        "test_text_id": test_text_id,
        "vocab": word_to_id,
        "vocab_size": len(word_to_id)
    }
    return data
</source>
</class>

<class classid="52" nclones="2" nlines="36" similarity="72">
<source file="systems/texar-0.2.4/examples/distributed_gpu/lm_ptb_distributed.py" startline="144" endline="190" pcid="1096">
    def _run_epoch(sess, data_iter, epoch, is_train=False, verbose=False):
        start_time = time.time()
        loss = 0.
        iters = 0

        fetches = {
            "mle_loss": mle_loss,
            "final_state": final_state,
        }
        if is_train:
            fetches["train_op"] = train_op
            epoch_size = (len(data["train_text_id"]) // batch_size - 1)\
                // num_steps

        mode = (tf.estimator.ModeKeys.TRAIN
                if is_train
                else tf.estimator.ModeKeys.EVAL)

        for step, (x, y) in enumerate(data_iter):
            if step == 0:
                state = sess.run(initial_state,
                                 feed_dict={inputs: x})

            feed_dict = {
                inputs: x, targets: y, global_step: epoch,
                tx.global_mode(): mode,
            }
            for i, (c, h) in enumerate(initial_state):
                feed_dict[c] = state[i].c
                feed_dict[h] = state[i].h

            rets = sess.run(fetches, feed_dict)
            loss += rets["mle_loss"]
            state = rets["final_state"]
            iters += num_steps

            ppl = np.exp(loss / iters)
            if verbose and is_train and hvd.rank() == 0 \
                and (step + 1) % (epoch_size // 10) == 0:
                tf.logging.info("%.3f perplexity: %.3f speed: %.0f wps" %
                                ((step + 1) * 1.0 / epoch_size, ppl,
                                 iters * batch_size / (
                                         time.time() - start_time)))
        _elapsed_time = time.time() - start_time
        tf.logging.info("epoch time elapsed: %f" % (_elapsed_time))
        ppl = np.exp(loss / iters)
        return ppl, _elapsed_time
</source>
<source file="systems/texar-0.2.4/examples/language_model_ptb/lm_ptb.py" startline="115" endline="155" pcid="1299">
    def _run_epoch(sess, data_iter, epoch, is_train=False, verbose=False):
        start_time = time.time()
        loss = 0.
        iters = 0
        state = sess.run(initial_state)

        fetches = {
            "mle_loss": mle_loss,
            "final_state": final_state,
        }
        if is_train:
            fetches["train_op"] = train_op
            epoch_size = (len(data["train_text_id"]) // batch_size - 1)\
                // num_steps

        mode = (tf.estimator.ModeKeys.TRAIN
                if is_train
                else tf.estimator.ModeKeys.EVAL)

        for step, (x, y) in enumerate(data_iter):
            feed_dict = {
                inputs: x, targets: y, global_step: epoch,
                tx.global_mode(): mode,
            }
            for i, (c, h) in enumerate(initial_state):
                feed_dict[c] = state[i].c
                feed_dict[h] = state[i].h

            rets = sess.run(fetches, feed_dict)
            loss += rets["mle_loss"]
            state = rets["final_state"]
            iters += num_steps

            ppl = np.exp(loss / iters)
            if verbose and is_train and step % (epoch_size // 10) == 10:
                print("%.3f perplexity: %.3f speed: %.0f wps" %
                      ((step + 1) * 1.0 / epoch_size, ppl,
                       iters * batch_size / (time.time() - start_time)))

        ppl = np.exp(loss / iters)
        return ppl
</source>
</class>

<class classid="53" nclones="2" nlines="17" similarity="70">
<source file="systems/texar-0.2.4/examples/bert/utils/data_utils.py" startline="146" endline="164" pcid="1111">
    def get_train_examples(self, data_dir):
        """See base class."""
        lines = self._read_tsv(
            os.path.join(data_dir, "multinli",
                         "multinli.train.%s.tsv" % self.language))
        examples = []
        for (i, line) in enumerate(lines):
            if i == 0:
                continue
            guid = "train-%d" % (i)
            text_a = tx.utils.compat_as_text(line[0])
            text_b = tx.utils.compat_as_text(line[1])
            label = tx.utils.compat_as_text(line[2])
            if label == tx.utils.compat_as_text("contradictory"):
                label = tx.utils.compat_as_text("contradiction")
            examples.append(InputExample(guid=guid, text_a=text_a,
                                         text_b=text_b, label=label))
        return examples

</source>
<source file="systems/texar-0.2.4/examples/bert/utils/data_utils.py" startline="165" endline="182" pcid="1112">
    def get_dev_examples(self, data_dir):
        """See base class."""
        lines = self._read_tsv(os.path.join(data_dir, "xnli.dev.tsv"))
        examples = []
        for (i, line) in enumerate(lines):
            if i == 0:
                continue
            guid = "dev-%d" % (i)
            language = tx.utils.compat_as_text(line[0])
            if language != tx.utils.compat_as_text(self.language):
                continue
            text_a = tx.utils.compat_as_text(line[6])
            text_b = tx.utils.compat_as_text(line[7])
            label = tx.utils.compat_as_text(line[1])
            examples.append(InputExample(guid=guid, text_a=text_a,
                                         text_b=text_b, label=label))
        return examples

</source>
</class>

<class classid="54" nclones="3" nlines="15" similarity="80">
<source file="systems/texar-0.2.4/examples/bert/utils/data_utils.py" startline="213" endline="231" pcid="1118">
    def _create_examples(lines, set_type):
        """Creates examples for the training and dev sets."""
        examples = []
        for (i, line) in enumerate(lines):
            if i == 0:
                continue
            guid = "%s-%s" % (set_type,
                              tx.utils.compat_as_text(line[0]))
            text_a = tx.utils.compat_as_text(line[8])
            text_b = tx.utils.compat_as_text(line[9])
            if set_type == "test":
                label = "contradiction"
            else:
                label = tx.utils.compat_as_text(line[-1])
            examples.append(InputExample(guid=guid, text_a=text_a,
                                         text_b=text_b, label=label))
        return examples


</source>
<source file="systems/texar-0.2.4/examples/bert/utils/data_utils.py" startline="258" endline="275" pcid="1123">
    def _create_examples(lines, set_type):
        """Creates examples for the training and dev sets."""
        examples = []
        for (i, line) in enumerate(lines):
            if i == 0:
                continue
            guid = "%s-%s" % (set_type, i)
            text_a = tx.utils.compat_as_text(line[3])
            text_b = tx.utils.compat_as_text(line[4])
            if set_type == "test":
                label = "0"
            else:
                label = tx.utils.compat_as_text(line[0])
            examples.append(InputExample(guid=guid, text_a=text_a,
                                         text_b=text_b, label=label))
        return examples


</source>
<source file="systems/texar-0.2.4/examples/bert/utils/data_utils.py" startline="302" endline="320" pcid="1128">
    def _create_examples(lines, set_type):
        """Creates examples for the training and dev sets."""
        examples = []
        for (i, line) in enumerate(lines):
            # Only the test set has a header
            if set_type == "test" and i == 0:
                continue
            guid = "%s-%s" % (set_type, i)
            if set_type == "test":
                text_a = tx.utils.compat_as_text(line[1])
                label = "0"
            else:
                text_a = tx.utils.compat_as_text(line[3])
                label = tx.utils.compat_as_text(line[1])
            examples.append(InputExample(guid=guid, text_a=text_a,
                                         text_b=None, label=label))
        return examples


</source>
</class>

<class classid="55" nclones="2" nlines="25" similarity="80">
<source file="systems/texar-0.2.4/examples/bert/bert_classifier_main.py" startline="138" endline="170" pcid="1137">
    def _train_epoch(sess):
        """Trains on the training set, and evaluates on the dev set
        periodically.
        """
        iterator.restart_dataset(sess, 'train')

        fetches = {
            'train_op': train_op,
            'loss': loss,
            'batch_size': batch_size,
            'step': global_step
        }

        while True:
            try:
                feed_dict = {
                    iterator.handle: iterator.get_handle(sess, 'train'),
                    tx.global_mode(): tf.estimator.ModeKeys.TRAIN,
                }
                rets = sess.run(fetches, feed_dict)
                step = rets['step']

                dis_steps = config_data.display_steps
                if _is_head() and dis_steps > 0 and step % dis_steps == 0:
                    tf.logging.info('step:%d; loss:%f;' % (step, rets['loss']))

                eval_steps = config_data.eval_steps
                if _is_head() and eval_steps > 0 and step % eval_steps == 0:
                    _eval_epoch(sess)

            except tf.errors.OutOfRangeError:
                break

</source>
<source file="systems/texar-0.2.4/examples/gpt-2/gpt2_train_main.py" startline="208" endline="244" pcid="1158">
    def _train_epoch(sess):
        """Trains on the training set, and evaluates on the dev set
        periodically.
        """
        iterator.restart_dataset(sess, 'train')

        fetches = {
            'loss': train_op,
            'step': global_step
        }

        while True:
            try:
                feed_dict = {
                    iterator.handle: iterator.get_handle(sess, 'train'),
                    tx.global_mode(): tf.estimator.ModeKeys.TRAIN,
                }
                rets = sess.run(fetches, feed_dict)
                step = rets['step']

                dis_steps = config_train.display_steps
                if _is_head() and dis_steps > 0 and step % dis_steps == 0:
                    tf.logging.info('step:%d; loss:%f' % (step, rets['loss']))

                eval_steps = config_train.eval_steps
                if _is_head() and eval_steps > 0 and step % eval_steps == 0:
                    _dev_epoch(sess)

                ckpt_steps = config_train.checkpoint_steps
                if _is_head() and ckpt_steps > 0 and step % ckpt_steps == 0:
                    ckpt_fn = os.path.join(FLAGS.output_dir, 'model.ckpt')
                    ckpt_fn = saver.save(sess, ckpt_fn, global_step=step)
                    tf.logging.info('Checkpoint to {}'.format(ckpt_fn))

            except tf.errors.OutOfRangeError:
                break

</source>
</class>

<class classid="56" nclones="2" nlines="28" similarity="70">
<source file="systems/texar-0.2.4/examples/bert/bert_classifier_main.py" startline="171" endline="200" pcid="1138">
    def _eval_epoch(sess):
        """Evaluates on the dev set.
        """
        iterator.restart_dataset(sess, 'eval')

        cum_acc = 0.0
        cum_loss = 0.0
        nsamples = 0
        fetches = {
            'accu': accu,
            'loss': loss,
            'batch_size': batch_size,
        }
        while True:
            try:
                feed_dict = {
                    iterator.handle: iterator.get_handle(sess, 'eval'),
                    tx.context.global_mode(): tf.estimator.ModeKeys.EVAL,
                }
                rets = sess.run(fetches, feed_dict)

                cum_acc += rets['accu'] * rets['batch_size']
                cum_loss += rets['loss'] * rets['batch_size']
                nsamples += rets['batch_size']
            except tf.errors.OutOfRangeError:
                break

        tf.logging.info('eval accu: {}; loss: {}; nsamples: {}'.format(
            cum_acc / nsamples, cum_loss / nsamples, nsamples))

</source>
<source file="systems/texar-0.2.4/examples/gpt-2/gpt2_train_main.py" startline="245" endline="283" pcid="1159">
    def _dev_epoch(sess):
        """Evaluates on the dev set.
        """
        iterator.restart_dataset(sess, 'dev')

        cum_loss = 0.
        cum_ppl = 0.
        nsamples = 0
        fetches = {
            'loss': loss,
            'ppl': ppl,
            'batch_size': batch_size,
        }
        while True:
            try:
                feed_dict = {
                    iterator.handle: iterator.get_handle(sess, 'dev'),
                    tx.context.global_mode(): tf.estimator.ModeKeys.EVAL,
                }
                rets = sess.run(fetches, feed_dict)

                cum_loss += rets['loss'] * rets['batch_size']
                cum_ppl += rets['ppl'] * rets['batch_size']
                nsamples += rets['batch_size']
            except tf.errors.OutOfRangeError:
                break

        avg_loss = cum_loss / nsamples
        avg_ppl = cum_ppl / nsamples
        tf.logging.info('dev loss: {}; ppl: {}; nsamples: {}'.format(
            avg_loss, avg_ppl, nsamples))

        if FLAGS.do_train and avg_loss < dev_best['loss']:
            dev_best['loss'] = avg_loss
            dev_best['ppl'] = avg_ppl
            ckpt_fn = os.path.join(FLAGS.output_dir, 'model_best.ckpt')
            ckpt_fn = saver_best.save(sess, ckpt_fn)
            tf.logging.info('Checkpoint best to {}'.format(ckpt_fn))

</source>
</class>

<class classid="57" nclones="5" nlines="33" similarity="70">
<source file="systems/texar-0.2.4/examples/seq2seq_exposure_bias/scheduled_sampling_main.py" startline="64" endline="116" pcid="1181">

def build_model(batch, train_data, self_sampling_proba):
    """
    Assembles the seq2seq model.
    It is the same as build_model() in baseline_seq2seq_attn.py except
    using ScheduledEmbeddingTrainingHelper.
    """
    source_embedder = tx.modules.WordEmbedder(
        vocab_size=train_data.source_vocab.size, hparams=config_model.embedder)

    encoder = tx.modules.BidirectionalRNNEncoder(
        hparams=config_model.encoder)

    enc_outputs, _ = encoder(source_embedder(batch['source_text_ids']))

    target_embedder = tx.modules.WordEmbedder(
        vocab_size=train_data.target_vocab.size, hparams=config_model.embedder)

    decoder = tx.modules.AttentionRNNDecoder(
        memory=tf.concat(enc_outputs, axis=2),
        memory_sequence_length=batch['source_length'],
        vocab_size=train_data.target_vocab.size,
        hparams=config_model.decoder)

    helper = tx.modules.get_helper(
        helper_type='ScheduledEmbeddingTrainingHelper',
        inputs=target_embedder(batch['target_text_ids'][:, :-1]),
        sequence_length=batch['target_length'] - 1,
        embedding=target_embedder,
        sampling_probability=self_sampling_proba)

    training_outputs, _, _ = decoder(
        helper=helper, initial_state=decoder.zero_state(
            batch_size=tf.shape(batch['target_length'])[0], dtype=tf.float32))

    train_op = tx.core.get_train_op(
        tx.losses.sequence_sparse_softmax_cross_entropy(
            labels=batch['target_text_ids'][:, 1:],
            logits=training_outputs.logits,
            sequence_length=batch['target_length'] - 1),
        hparams=config_model.opt)

    start_tokens = tf.ones_like(batch['target_length']) *\
                   train_data.target_vocab.bos_token_id
    beam_search_outputs, _, _ = \
        tx.modules.beam_search_decode(
            decoder_or_cell=decoder,
            embedding=target_embedder,
            start_tokens=start_tokens,
            end_token=train_data.target_vocab.eos_token_id,
            beam_width=config_model.beam_width,
            max_decoding_length=60)

</source>
<source file="systems/texar-0.2.4/examples/seq2seq_exposure_bias/baseline_seq2seq_attn_main.py" startline="50" endline="93" pcid="1193">
def build_model(batch, train_data):
    """Assembles the seq2seq model.
    """
    source_embedder = tx.modules.WordEmbedder(
        vocab_size=train_data.source_vocab.size, hparams=config_model.embedder)

    encoder = tx.modules.BidirectionalRNNEncoder(
        hparams=config_model.encoder)

    enc_outputs, _ = encoder(source_embedder(batch['source_text_ids']))

    target_embedder = tx.modules.WordEmbedder(
        vocab_size=train_data.target_vocab.size, hparams=config_model.embedder)

    decoder = tx.modules.AttentionRNNDecoder(
        memory=tf.concat(enc_outputs, axis=2),
        memory_sequence_length=batch['source_length'],
        vocab_size=train_data.target_vocab.size,
        hparams=config_model.decoder)

    training_outputs, _, _ = decoder(
        decoding_strategy='train_greedy',
        inputs=target_embedder(batch['target_text_ids'][:, :-1]),
        sequence_length=batch['target_length'] - 1)

    train_op = tx.core.get_train_op(
        tx.losses.sequence_sparse_softmax_cross_entropy(
            labels=batch['target_text_ids'][:, 1:],
            logits=training_outputs.logits,
            sequence_length=batch['target_length'] - 1),
        hparams=config_model.opt)

    start_tokens = tf.ones_like(batch['target_length']) *\
                   train_data.target_vocab.bos_token_id
    beam_search_outputs, _, _ = \
        tx.modules.beam_search_decode(
            decoder_or_cell=decoder,
            embedding=target_embedder,
            start_tokens=start_tokens,
            end_token=train_data.target_vocab.eos_token_id,
            beam_width=config_model.beam_width,
            max_decoding_length=60)

    return train_op, beam_search_outputs
</source>
<source file="systems/texar-0.2.4/examples/seq2seq_exposure_bias/raml_main.py" startline="93" endline="136" pcid="1226">
           tf.reduce_sum(training_rewards)


def build_model(batch, train_data, rewards):
    """
    Assembles the seq2seq model.
    Code in this function is basically the same of build_model() in
    baseline_seq2seq_attn_main.py except the normalization in loss_fn.
    """
    source_embedder = tx.modules.WordEmbedder(
        vocab_size=train_data.source_vocab.size, hparams=config_model.embedder)

    encoder = tx.modules.BidirectionalRNNEncoder(
        hparams=config_model.encoder)

    enc_outputs, _ = encoder(source_embedder(batch['source_text_ids']))

    target_embedder = tx.modules.WordEmbedder(
        vocab_size=train_data.target_vocab.size, hparams=config_model.embedder)

    decoder = tx.modules.AttentionRNNDecoder(
        memory=tf.concat(enc_outputs, axis=2),
        memory_sequence_length=batch['source_length'],
        vocab_size=train_data.target_vocab.size,
        hparams=config_model.decoder)

    training_outputs, _, _ = decoder(
        decoding_strategy='train_greedy',
        inputs=target_embedder(batch['target_text_ids'][:, :-1]),
        sequence_length=batch['target_length'] - 1)

    train_op = tx.core.get_train_op(
        raml_loss(batch, training_outputs, rewards),
        hparams=config_model.opt)

    start_tokens = tf.ones_like(batch['target_length']) *\
                   train_data.target_vocab.bos_token_id
    beam_search_outputs, _, _ = \
        tx.modules.beam_search_decode(
            decoder_or_cell=decoder,
            embedding=target_embedder,
            start_tokens=start_tokens,
            end_token=train_data.target_vocab.eos_token_id,
            beam_width=config_model.beam_width,
</source>
<source file="systems/texar-0.2.4/examples/seq2seq_attn/seq2seq_attn.py" startline="37" endline="80" pcid="1284">
def build_model(batch, train_data):
    """Assembles the seq2seq model.
    """
    source_embedder = tx.modules.WordEmbedder(
        vocab_size=train_data.source_vocab.size, hparams=config_model.embedder)

    encoder = tx.modules.BidirectionalRNNEncoder(
        hparams=config_model.encoder)

    enc_outputs, _ = encoder(source_embedder(batch['source_text_ids']))

    target_embedder = tx.modules.WordEmbedder(
        vocab_size=train_data.target_vocab.size, hparams=config_model.embedder)

    decoder = tx.modules.AttentionRNNDecoder(
        memory=tf.concat(enc_outputs, axis=2),
        memory_sequence_length=batch['source_length'],
        vocab_size=train_data.target_vocab.size,
        hparams=config_model.decoder)

    training_outputs, _, _ = decoder(
        decoding_strategy='train_greedy',
        inputs=target_embedder(batch['target_text_ids'][:, :-1]),
        sequence_length=batch['target_length'] - 1)

    mle_loss = tx.losses.sequence_sparse_softmax_cross_entropy(
        labels=batch['target_text_ids'][:, 1:],
        logits=training_outputs.logits,
        sequence_length=batch['target_length'] - 1)

    train_op = tx.core.get_train_op(mle_loss, hparams=config_model.opt)

    start_tokens = tf.ones_like(batch['target_length']) * \
            train_data.target_vocab.bos_token_id
    beam_search_outputs, _, _ = \
        tx.modules.beam_search_decode(
            decoder_or_cell=decoder,
            embedding=target_embedder,
            start_tokens=start_tokens,
            end_token=train_data.target_vocab.eos_token_id,
            beam_width=config_model.beam_width,
            max_decoding_length=60)

    return train_op, beam_search_outputs
</source>
<source file="systems/texar-0.2.4/examples/seq2seq_rl/seq2seq_attn_pg.py" startline="50" endline="89" pcid="1254">
def build_model(batch, train_data):
    """Assembles the seq2seq model.
    """
    source_embedder = tx.modules.WordEmbedder(
        vocab_size=train_data.source_vocab.size, hparams=config_model.embedder)

    encoder = tx.modules.BidirectionalRNNEncoder(
        hparams=config_model.encoder)

    enc_outputs, _ = encoder(source_embedder(batch['source_text_ids']))

    target_embedder = tx.modules.WordEmbedder(
        vocab_size=train_data.target_vocab.size, hparams=config_model.embedder)

    decoder = tx.modules.AttentionRNNDecoder(
        memory=tf.concat(enc_outputs, axis=2),
        memory_sequence_length=batch['source_length'],
        vocab_size=train_data.target_vocab.size,
        hparams=config_model.decoder)

    start_tokens = tf.ones_like(batch['target_length']) * \
            train_data.target_vocab.bos_token_id

    outputs, _, sequence_length = decoder(
        decoding_strategy='infer_sample',
        start_tokens=start_tokens,
        end_token=train_data.target_vocab.eos_token_id,
        embedding=target_embedder,
        max_decoding_length=30)

    beam_search_outputs, _, _ = \
        tx.modules.beam_search_decode(
            decoder_or_cell=decoder,
            embedding=target_embedder,
            start_tokens=start_tokens,
            end_token=train_data.target_vocab.eos_token_id,
            beam_width=config_model.beam_width,
            max_decoding_length=60)

    return outputs, sequence_length, beam_search_outputs
</source>
</class>

<class classid="58" nclones="3" nlines="109" similarity="77">
<source file="systems/texar-0.2.4/examples/seq2seq_exposure_bias/scheduled_sampling_main.py" startline="122" endline="259" pcid="1183">
    print(content, file=file)


def main():
    """Entrypoint.
    """
    train_data = tx.data.PairedTextData(hparams=config_data.train)
    val_data = tx.data.PairedTextData(hparams=config_data.val)
    test_data = tx.data.PairedTextData(hparams=config_data.test)
    data_iterator = tx.data.TrainTestDataIterator(
        train=train_data, val=val_data, test=test_data)

    batch = data_iterator.get_next()

    self_sampling_proba = tf.placeholder(shape=[], dtype=tf.float32)
    train_op, infer_outputs = \
        build_model(batch, train_data, self_sampling_proba)

    def _train_epoch(sess, epoch_no, total_step_counter):
        data_iterator.switch_to_train_data(sess)
        training_log_file = \
            open(log_dir + 'training_log' + str(epoch_no) + '.txt', 'w',
                 encoding='utf-8')

        step = 0
        while True:
            try:
                sampling_proba_ = 1. - inverse_sigmoid(total_step_counter)
                loss = sess.run(train_op, feed_dict={
                    self_sampling_proba: sampling_proba_})
                print("step={}, loss={:.4f}, self_proba={}".format(
                    step, loss, sampling_proba_), file=training_log_file)
                if step % config_data.observe_steps == 0:
                    print("step={}, loss={:.4f}, self_proba={}".format(
                        step, loss, sampling_proba_))
                training_log_file.flush()
                step += 1
                total_step_counter += 1
            except tf.errors.OutOfRangeError:
                break

    # code below this line is exactly the same as baseline_seq2seq_attn_main.py

    def _eval_epoch(sess, mode, epoch_no):
        if mode == 'val':
            data_iterator.switch_to_val_data(sess)
        else:
            data_iterator.switch_to_test_data(sess)

        refs, hypos = [], []
        while True:
            try:
                fetches = [
                    batch['target_text'][:, 1:],
                    infer_outputs.predicted_ids[:, :, 0]
                ]
                feed_dict = {
                    tx.global_mode(): tf.estimator.ModeKeys.EVAL
                }
                target_texts_ori, output_ids = \
                    sess.run(fetches, feed_dict=feed_dict)

                target_texts = tx.utils.strip_special_tokens(
                    target_texts_ori.tolist(), is_token_list=True)
                target_texts = tx.utils.str_join(target_texts)
                output_texts = tx.utils.map_ids_to_strs(
                    ids=output_ids, vocab=val_data.target_vocab)

                tx.utils.write_paired_text(
                    target_texts, output_texts,
                    log_dir + mode + '_results' + str(epoch_no) + '.txt',
                    append=True, mode='h', sep=' ||| ')

                for hypo, ref in zip(output_texts, target_texts):
                    if config_data.eval_metric == 'bleu':
                        hypos.append(hypo)
                        refs.append([ref])
                    elif config_data.eval_metric == 'rouge':
                        hypos.append(tx.utils.compat_as_text(hypo))
                        refs.append(tx.utils.compat_as_text(ref))
            except tf.errors.OutOfRangeError:
                break

        if config_data.eval_metric == 'bleu':
            return tx.evals.corpus_bleu_moses(
                list_of_references=refs, hypotheses=hypos)
        elif config_data.eval_metric == 'rouge':
            rouge = Rouge()
            return rouge.get_scores(hyps=hypos, refs=refs, avg=True)

    def _calc_reward(score):
        """
        Return the bleu score or the sum of (Rouge-1, Rouge-2, Rouge-L).
        """
        if config_data.eval_metric == 'bleu':
            return score
        elif config_data.eval_metric == 'rouge':
            return sum([value['f'] for key, value in score.items()])

    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        sess.run(tf.local_variables_initializer())
        sess.run(tf.tables_initializer())

        best_val_score = -1.
        total_step_counter = 1
        scores_file = open(log_dir + 'scores.txt', 'w', encoding='utf-8')
        for i in range(config_data.num_epochs):
            _train_epoch(sess, i, total_step_counter)

            val_score = _eval_epoch(sess, 'val', i)
            test_score = _eval_epoch(sess, 'test', i)

            best_val_score = max(best_val_score, _calc_reward(val_score))

            if config_data.eval_metric == 'bleu':
                print_stdout_and_file(
                    'val epoch={}, BLEU={:.4f}; best-ever={:.4f}'.format(
                        i, val_score, best_val_score), file=scores_file)

                print_stdout_and_file(
                    'test epoch={}, BLEU={:.4f}'.format(i, test_score),
                    file=scores_file)
                print_stdout_and_file('=' * 50, file=scores_file)

            elif config_data.eval_metric == 'rouge':
                print_stdout_and_file(
                    'valid epoch {}:'.format(i), file=scores_file)
                for key, value in val_score.items():
                    print_stdout_and_file(
                        '{}: {}'.format(key, value), file=scores_file)
                print_stdout_and_file('fsum: {}; best_val_fsum: {}'.format(
                    _calc_reward(val_score), best_val_score), file=scores_file)

                print_stdout_and_file(
                    'test epoch {}:'.format(i), file=scores_file)
                for key, value in test_score.items():
                    print_stdout_and_file(
</source>
<source file="systems/texar-0.2.4/examples/seq2seq_exposure_bias/baseline_seq2seq_attn_main.py" startline="99" endline="228" pcid="1195">


def main():
    """Entrypoint.
    """
    train_data = tx.data.PairedTextData(hparams=config_data.train)
    val_data = tx.data.PairedTextData(hparams=config_data.val)
    test_data = tx.data.PairedTextData(hparams=config_data.test)
    data_iterator = tx.data.TrainTestDataIterator(
        train=train_data, val=val_data, test=test_data)

    batch = data_iterator.get_next()

    train_op, infer_outputs = build_model(batch, train_data)

    def _train_epoch(sess, epoch_no):
        data_iterator.switch_to_train_data(sess)
        training_log_file = \
            open(log_dir + 'training_log' + str(epoch_no) + '.txt', 'w',
                 encoding='utf-8')

        step = 0
        while True:
            try:
                loss = sess.run(train_op)
                print("step={}, loss={:.4f}".format(step, loss),
                      file=training_log_file)
                if step % config_data.observe_steps == 0:
                    print("step={}, loss={:.4f}".format(step, loss))
                training_log_file.flush()
                step += 1
            except tf.errors.OutOfRangeError:
                break

    def _eval_epoch(sess, mode, epoch_no):
        if mode == 'val':
            data_iterator.switch_to_val_data(sess)
        else:
            data_iterator.switch_to_test_data(sess)

        refs, hypos = [], []
        while True:
            try:
                fetches = [
                    batch['target_text'][:, 1:],
                    infer_outputs.predicted_ids[:, :, 0]
                ]
                feed_dict = {
                    tx.global_mode(): tf.estimator.ModeKeys.EVAL
                }
                target_texts_ori, output_ids = \
                    sess.run(fetches, feed_dict=feed_dict)

                target_texts = tx.utils.strip_special_tokens(
                    target_texts_ori.tolist(), is_token_list=True)
                target_texts = tx.utils.str_join(target_texts)
                output_texts = tx.utils.map_ids_to_strs(
                    ids=output_ids, vocab=val_data.target_vocab)

                tx.utils.write_paired_text(
                    target_texts, output_texts,
                    log_dir + mode + '_results' + str(epoch_no) + '.txt',
                    append=True, mode='h', sep=' ||| ')

                for hypo, ref in zip(output_texts, target_texts):
                    if config_data.eval_metric == 'bleu':
                        hypos.append(hypo)
                        refs.append([ref])
                    elif config_data.eval_metric == 'rouge':
                        hypos.append(tx.utils.compat_as_text(hypo))
                        refs.append(tx.utils.compat_as_text(ref))
            except tf.errors.OutOfRangeError:
                break

        if config_data.eval_metric == 'bleu':
            return tx.evals.corpus_bleu_moses(
                list_of_references=refs, hypotheses=hypos)
        elif config_data.eval_metric == 'rouge':
            rouge = Rouge()
            return rouge.get_scores(hyps=hypos, refs=refs, avg=True)

    def _calc_reward(score):
        """
        Return the bleu score or the sum of (Rouge-1, Rouge-2, Rouge-L).
        """
        if config_data.eval_metric == 'bleu':
            return score
        elif config_data.eval_metric == 'rouge':
            return sum([value['f'] for key, value in score.items()])

    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        sess.run(tf.local_variables_initializer())
        sess.run(tf.tables_initializer())

        best_val_score = -1.
        scores_file = open(log_dir + 'scores.txt', 'w', encoding='utf-8')
        for i in range(config_data.num_epochs):
            _train_epoch(sess, i)

            val_score = _eval_epoch(sess, 'val', i)
            test_score = _eval_epoch(sess, 'test', i)

            best_val_score = max(best_val_score, _calc_reward(val_score))

            if config_data.eval_metric == 'bleu':
                print_stdout_and_file(
                    'val epoch={}, BLEU={:.4f}; best-ever={:.4f}'.format(
                        i, val_score, best_val_score), file=scores_file)

                print_stdout_and_file(
                    'test epoch={}, BLEU={:.4f}'.format(i, test_score),
                    file=scores_file)
                print_stdout_and_file('=' * 50, file=scores_file)

            elif config_data.eval_metric == 'rouge':
                print_stdout_and_file(
                    'valid epoch {}:'.format(i), file=scores_file)
                for key, value in val_score.items():
                    print_stdout_and_file(
                        '{}: {}'.format(key, value), file=scores_file)
                print_stdout_and_file('fsum: {}; best_val_fsum: {}'.format(
                    _calc_reward(val_score), best_val_score), file=scores_file)

                print_stdout_and_file(
                    'test epoch {}:'.format(i), file=scores_file)
                for key, value in test_score.items():
                    print_stdout_and_file(
                        '{}: {}'.format(key, value), file=scores_file)
                print_stdout_and_file('=' * 110, file=scores_file)
</source>
<source file="systems/texar-0.2.4/examples/seq2seq_exposure_bias/interpolation_main.py" startline="131" endline="310" pcid="1217">


def print_stdout_and_file(content, file):
    print(content)
    print(content, file=file)


def main():
    """Entrypoint.
    """
    training_data = tx.data.PairedTextData(hparams=config_data.train)
    val_data = tx.data.PairedTextData(hparams=config_data.val)
    test_data = tx.data.PairedTextData(hparams=config_data.test)
    data_iterator = tx.data.TrainTestDataIterator(
        train=training_data, val=val_data, test=test_data)

    batch = data_iterator.get_next()
    lambdas_ts = tf.placeholder(shape=[3], dtype=tf.float32)

    train_op, infer_outputs = build_model(batch, training_data, lambdas_ts)

    def _train_epoch(sess, epoch, lambdas):
        data_iterator.switch_to_train_data(sess)
        log_file = open(log_dir + 'training_log' + str(epoch) + '.txt', 'w',
                        encoding='utf-8')

        step = 0
        while True:
            try:
                loss = sess.run(train_op, feed_dict={
                    lambdas_ts: np.array(lambdas)})
                print("step={}, loss={:.4f}, lambdas={}".format(
                    step, loss, lambdas), file=log_file)
                if step % config_data.observe_steps == 0:
                    print("step={}, loss={:.4f}, lambdas={}".format(
                        step, loss, lambdas))
                log_file.flush()
                step += 1

            except tf.errors.OutOfRangeError:
                break

    def _eval_epoch(sess, mode, epoch_no):
        """
        This function is the same as _eval_epoch() in
        baseline_seq2seq_attn_main.py.
        """
        if mode == 'val':
            data_iterator.switch_to_val_data(sess)
        else:
            data_iterator.switch_to_test_data(sess)

        refs, hypos = [], []
        while True:
            try:
                fetches = [
                    batch['target_text'][:, 1:],
                    infer_outputs.predicted_ids[:, :, 0]
                ]
                feed_dict = {
                    tx.global_mode(): tf.estimator.ModeKeys.EVAL
                }
                target_texts_ori, output_ids = \
                    sess.run(fetches, feed_dict=feed_dict)

                target_texts = tx.utils.strip_special_tokens(
                    target_texts_ori.tolist(), is_token_list=True)
                target_texts = tx.utils.str_join(target_texts)
                output_texts = tx.utils.map_ids_to_strs(
                    ids=output_ids, vocab=val_data.target_vocab)

                tx.utils.write_paired_text(
                    target_texts, output_texts,
                    log_dir + mode + '_results' + str(epoch_no) + '.txt',
                    append=True, mode='h', sep=' ||| ')

                for hypo, ref in zip(output_texts, target_texts):
                    if config_data.eval_metric == 'bleu':
                        hypos.append(hypo)
                        refs.append([ref])
                    elif config_data.eval_metric == 'rouge':
                        hypos.append(tx.utils.compat_as_text(hypo))
                        refs.append(tx.utils.compat_as_text(ref))
            except tf.errors.OutOfRangeError:
                break

        if config_data.eval_metric == 'bleu':
            return tx.evals.corpus_bleu_moses(
                list_of_references=refs, hypotheses=hypos)
        elif config_data.eval_metric == 'rouge':
            rouge = Rouge()
            return rouge.get_scores(hyps=hypos, refs=refs, avg=True)

    def _calc_reward(score):
        """
        Return the bleu score or the sum of (Rouge-1, Rouge-2, Rouge-L).
        """
        if config_data.eval_metric == 'bleu':
            return score
        elif config_data.eval_metric == 'rouge':
            return sum([value['f'] for key, value in score.items()])

    def _anneal():
        """
        Operate lambdas when the reward of val set decrease.
        """
        def _update_self():
            """
            Decrease lambda_truth and increase lambda_self.
            """
            lambdas[1] -= FLAGS.delta_lambda_self
            lambdas[0] += FLAGS.delta_lambda_self
            updates.append('self')

        def _update_rew():
            """
            Decrease lambda_truth and increase lambda_reward.
            """
            lambdas[1] -= FLAGS.delta_lambda_reward
            lambdas[2] += FLAGS.delta_lambda_reward
            updates.append('rew')

        if updates[-FLAGS.lambda_reward_steps:] == \
                ['rew'] * FLAGS.lambda_reward_steps:
            _update_self()
        else:
            _update_rew()

    saver = tf.train.Saver(max_to_keep=2)
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        sess.run(tf.local_variables_initializer())
        sess.run(tf.tables_initializer())

        lambdas = FLAGS.lambdas_init
        updates = ['rew'] * FLAGS.lambda_reward_steps

        best_val_score, best_val_score_current_lambdas = -1., -1.
        scores_file = open(log_dir + 'scores.txt', 'w', encoding='utf-8')

        for i in range(config_data.num_epochs):
            print_stdout_and_file(
                'training epoch={}, lambdas={}'.format(i, lambdas),
                file=scores_file)
            _train_epoch(sess, i, lambdas)
            saver.save(sess, log_dir + 'models/model{}.ckpt'.format(i))

            val_score = _eval_epoch(sess, 'val', i)
            test_score = _eval_epoch(sess, 'test', i)

            if _calc_reward(val_score) < best_val_score_current_lambdas:
                _anneal()
                best_val_score_current_lambdas = -1.
                saver.restore(
                    sess, log_dir + 'models/model{}.ckpt'.format(i - 1))
            else:
                best_val_score_current_lambdas = _calc_reward(val_score)

            best_val_score = max(best_val_score, _calc_reward(val_score))

            if config_data.eval_metric == 'bleu':
                print_stdout_and_file(
                    'val epoch={}, BLEU={:.4f}; best-ever={:.4f}'.format(
                        i, val_score, best_val_score), file=scores_file)

                print_stdout_and_file(
                    'test epoch={}, BLEU={:.4f}'.format(i, test_score),
                    file=scores_file)
                print_stdout_and_file('=' * 50, file=scores_file)

            elif config_data.eval_metric == 'rouge':
                print_stdout_and_file(
                    'valid epoch {}:'.format(i), file=scores_file)
                for key, value in val_score.items():
                    print_stdout_and_file(
                        '{}: {}'.format(key, value), file=scores_file)
                print_stdout_and_file('fsum: {}; best_val_fsum: {}'.format(
                    _calc_reward(val_score), best_val_score), file=scores_file)

                print_stdout_and_file(
</source>
</class>

<class classid="59" nclones="4" nlines="38" similarity="100">
<source file="systems/texar-0.2.4/examples/seq2seq_exposure_bias/scheduled_sampling_main.py" startline="160" endline="205" pcid="1185">
            except tf.errors.OutOfRangeError:
                break

    # code below this line is exactly the same as baseline_seq2seq_attn_main.py

    def _eval_epoch(sess, mode, epoch_no):
        if mode == 'val':
            data_iterator.switch_to_val_data(sess)
        else:
            data_iterator.switch_to_test_data(sess)

        refs, hypos = [], []
        while True:
            try:
                fetches = [
                    batch['target_text'][:, 1:],
                    infer_outputs.predicted_ids[:, :, 0]
                ]
                feed_dict = {
                    tx.global_mode(): tf.estimator.ModeKeys.EVAL
                }
                target_texts_ori, output_ids = \
                    sess.run(fetches, feed_dict=feed_dict)

                target_texts = tx.utils.strip_special_tokens(
                    target_texts_ori.tolist(), is_token_list=True)
                target_texts = tx.utils.str_join(target_texts)
                output_texts = tx.utils.map_ids_to_strs(
                    ids=output_ids, vocab=val_data.target_vocab)

                tx.utils.write_paired_text(
                    target_texts, output_texts,
                    log_dir + mode + '_results' + str(epoch_no) + '.txt',
                    append=True, mode='h', sep=' ||| ')

                for hypo, ref in zip(output_texts, target_texts):
                    if config_data.eval_metric == 'bleu':
                        hypos.append(hypo)
                        refs.append([ref])
                    elif config_data.eval_metric == 'rouge':
                        hypos.append(tx.utils.compat_as_text(hypo))
                        refs.append(tx.utils.compat_as_text(ref))
            except tf.errors.OutOfRangeError:
                break

        if config_data.eval_metric == 'bleu':
</source>
<source file="systems/texar-0.2.4/examples/seq2seq_exposure_bias/raml_main.py" startline="240" endline="285" pcid="1230">
                print("step={}, loss={:.4f}".format(step, loss))
            training_log_file.flush()
            step += 1

    # code below this line is exactly the same as baseline_seq2seq_attn_main.py

    def _eval_epoch(sess, mode, epoch_no):
        if mode == 'val':
            data_iterator.switch_to_val_data(sess)
        else:
            data_iterator.switch_to_test_data(sess)

        refs, hypos = [], []
        while True:
            try:
                fetches = [
                    batch['target_text'][:, 1:],
                    infer_outputs.predicted_ids[:, :, 0]
                ]
                feed_dict = {
                    tx.global_mode(): tf.estimator.ModeKeys.EVAL
                }
                target_texts_ori, output_ids = \
                    sess.run(fetches, feed_dict=feed_dict)

                target_texts = tx.utils.strip_special_tokens(
                    target_texts_ori.tolist(), is_token_list=True)
                target_texts = tx.utils.str_join(target_texts)
                output_texts = tx.utils.map_ids_to_strs(
                    ids=output_ids, vocab=val_data.target_vocab)

                tx.utils.write_paired_text(
                    target_texts, output_texts,
                    log_dir + mode + '_results' + str(epoch_no) + '.txt',
                    append=True, mode='h', sep=' ||| ')

                for hypo, ref in zip(output_texts, target_texts):
                    if config_data.eval_metric == 'bleu':
                        hypos.append(hypo)
                        refs.append([ref])
                    elif config_data.eval_metric == 'rouge':
                        hypos.append(tx.utils.compat_as_text(hypo))
                        refs.append(tx.utils.compat_as_text(ref))
            except tf.errors.OutOfRangeError:
                break

</source>
<source file="systems/texar-0.2.4/examples/seq2seq_exposure_bias/baseline_seq2seq_attn_main.py" startline="130" endline="175" pcid="1197">
            except tf.errors.OutOfRangeError:
                break

    def _eval_epoch(sess, mode, epoch_no):
        if mode == 'val':
            data_iterator.switch_to_val_data(sess)
        else:
            data_iterator.switch_to_test_data(sess)

        refs, hypos = [], []
        while True:
            try:
                fetches = [
                    batch['target_text'][:, 1:],
                    infer_outputs.predicted_ids[:, :, 0]
                ]
                feed_dict = {
                    tx.global_mode(): tf.estimator.ModeKeys.EVAL
                }
                target_texts_ori, output_ids = \
                    sess.run(fetches, feed_dict=feed_dict)

                target_texts = tx.utils.strip_special_tokens(
                    target_texts_ori.tolist(), is_token_list=True)
                target_texts = tx.utils.str_join(target_texts)
                output_texts = tx.utils.map_ids_to_strs(
                    ids=output_ids, vocab=val_data.target_vocab)

                tx.utils.write_paired_text(
                    target_texts, output_texts,
                    log_dir + mode + '_results' + str(epoch_no) + '.txt',
                    append=True, mode='h', sep=' ||| ')

                for hypo, ref in zip(output_texts, target_texts):
                    if config_data.eval_metric == 'bleu':
                        hypos.append(hypo)
                        refs.append([ref])
                    elif config_data.eval_metric == 'rouge':
                        hypos.append(tx.utils.compat_as_text(hypo))
                        refs.append(tx.utils.compat_as_text(ref))
            except tf.errors.OutOfRangeError:
                break

        if config_data.eval_metric == 'bleu':
            return tx.evals.corpus_bleu_moses(
                list_of_references=refs, hypotheses=hypos)
</source>
<source file="systems/texar-0.2.4/examples/seq2seq_exposure_bias/interpolation_main.py" startline="166" endline="215" pcid="1219">
                        step, loss, lambdas))
                log_file.flush()
                step += 1

            except tf.errors.OutOfRangeError:
                break

    def _eval_epoch(sess, mode, epoch_no):
        """
        This function is the same as _eval_epoch() in
        baseline_seq2seq_attn_main.py.
        """
        if mode == 'val':
            data_iterator.switch_to_val_data(sess)
        else:
            data_iterator.switch_to_test_data(sess)

        refs, hypos = [], []
        while True:
            try:
                fetches = [
                    batch['target_text'][:, 1:],
                    infer_outputs.predicted_ids[:, :, 0]
                ]
                feed_dict = {
                    tx.global_mode(): tf.estimator.ModeKeys.EVAL
                }
                target_texts_ori, output_ids = \
                    sess.run(fetches, feed_dict=feed_dict)

                target_texts = tx.utils.strip_special_tokens(
                    target_texts_ori.tolist(), is_token_list=True)
                target_texts = tx.utils.str_join(target_texts)
                output_texts = tx.utils.map_ids_to_strs(
                    ids=output_ids, vocab=val_data.target_vocab)

                tx.utils.write_paired_text(
                    target_texts, output_texts,
                    log_dir + mode + '_results' + str(epoch_no) + '.txt',
                    append=True, mode='h', sep=' ||| ')

                for hypo, ref in zip(output_texts, target_texts):
                    if config_data.eval_metric == 'bleu':
                        hypos.append(hypo)
                        refs.append([ref])
                    elif config_data.eval_metric == 'rouge':
                        hypos.append(tx.utils.compat_as_text(hypo))
                        refs.append(tx.utils.compat_as_text(ref))
            except tf.errors.OutOfRangeError:
                break
</source>
</class>

<class classid="60" nclones="3" nlines="15" similarity="100">
<source file="systems/texar-0.2.4/examples/seq2seq_exposure_bias/utils/prepare_data.py" startline="28" endline="48" pcid="1213">
def prepare_data():
    """Downloads data.
    """
    if FLAGS.data == 'giga':
        tx.data.maybe_download(
            urls='https://drive.google.com/file/d/'
                 '12RZs7QFwjj6dfuYNQ_0Ah-ccH1xFDMD5/view?usp=sharing',
            path='./',
            filenames='giga.zip',
            extract=True)
    elif FLAGS.data == 'iwslt14':
        tx.data.maybe_download(
            urls='https://drive.google.com/file/d/'
                 '1y4mUWXRS2KstgHopCS9koZ42ENOh6Yb9/view?usp=sharing',
            path='./',
            filenames='iwslt14.zip',
            extract=True)
    else:
        raise ValueError('Unknown data: {}'.format(FLAGS.data))


</source>
<source file="systems/texar-0.2.4/examples/seq2seq_rl/prepare_data.py" startline="28" endline="48" pcid="1252">
def prepare_data():
    """Downloads data.
    """
    if FLAGS.data == 'iwslt14':
        tx.data.maybe_download(
            urls='https://drive.google.com/file/d/'
                 '1Vuv3bed10qUxrpldHdYoiWLzPKa4pNXd/view?usp=sharing',
            path='./',
            filenames='iwslt14.zip',
            extract=True)
    elif FLAGS.data == 'toy_copy':
        tx.data.maybe_download(
            urls='https://drive.google.com/file/d/'
                 '1fENE2rakm8vJ8d3voWBgW4hGlS6-KORW/view?usp=sharing',
            path='./',
            filenames='toy_copy.zip',
            extract=True)
    else:
        raise ValueError('Unknown data: {}'.format(FLAGS.data))


</source>
<source file="systems/texar-0.2.4/examples/seq2seq_attn/prepare_data.py" startline="28" endline="48" pcid="1288">
def prepare_data():
    """Downloads data.
    """
    if FLAGS.data == 'iwslt14':
        tx.data.maybe_download(
            urls='https://drive.google.com/file/d/'
                 '1y4mUWXRS2KstgHopCS9koZ42ENOh6Yb9/view?usp=sharing',
            path='./',
            filenames='iwslt14.zip',
            extract=True)
    elif FLAGS.data == 'toy_copy':
        tx.data.maybe_download(
            urls='https://drive.google.com/file/d/'
                 '1fENE2rakm8vJ8d3voWBgW4hGlS6-KORW/view?usp=sharing',
            path='./',
            filenames='toy_copy.zip',
            extract=True)
    else:
        raise ValueError('Unknown data: {}'.format(FLAGS.data))


</source>
</class>

<class classid="61" nclones="2" nlines="24" similarity="72">
<source file="systems/texar-0.2.4/examples/seq2seq_rl/seq2seq_attn_pg.py" startline="161" endline="191" pcid="1257">
                break

    def _eval_epoch(sess, mode):
        """`mode` is one of {'val', 'test'}
        """
        iterator.restart_dataset(sess, mode)

        refs, hypos = [], []
        while True:
            try:
                fetches = [
                    batch['target_text'][:, 1:],
                    infer_outputs.predicted_ids[:, :, 0]
                ]
                feed_dict = {
                    tx.global_mode(): tf.estimator.ModeKeys.PREDICT,
                    iterator.handle: iterator.get_handle(sess, mode)
                }
                target_texts, output_ids = \
                    sess.run(fetches, feed_dict=feed_dict)

                target_texts = tx.utils.strip_special_tokens(target_texts)
                output_texts = tx.utils.map_ids_to_strs(
                    ids=output_ids, vocab=val_data.target_vocab)

                for hypo, ref in zip(output_texts, target_texts):
                    hypos.append(hypo)
                    refs.append([ref])
            except tf.errors.OutOfRangeError:
                break

</source>
<source file="systems/texar-0.2.4/examples/seq2seq_attn/seq2seq_attn.py" startline="107" endline="138" pcid="1287">
                break

    def _eval_epoch(sess, mode):
        if mode == 'val':
            data_iterator.switch_to_val_data(sess)
        else:
            data_iterator.switch_to_test_data(sess)

        refs, hypos = [], []
        while True:
            try:
                fetches = [
                    batch['target_text'][:, 1:],
                    infer_outputs.predicted_ids[:, :, 0]
                ]
                feed_dict = {
                    tx.global_mode(): tf.estimator.ModeKeys.EVAL
                }
                target_texts_ori, output_ids = \
                    sess.run(fetches, feed_dict=feed_dict)

                target_texts = tx.utils.strip_special_tokens(
                    target_texts_ori, is_token_list=True)
                output_texts = tx.utils.map_ids_to_strs(
                    ids=output_ids, vocab=val_data.target_vocab)

                for hypo, ref in zip(output_texts, target_texts):
                    hypos.append(hypo)
                    refs.append([ref])
            except tf.errors.OutOfRangeError:
                break

</source>
</class>

<class classid="62" nclones="2" nlines="12" similarity="100">
<source file="systems/texar-0.2.4/examples/memory_network_lm/ptb_reader.py" startline="30" endline="48" pcid="1279">
def ptb_iterator(data, batch_size, num_steps):
    """Iterates through the ptb data.
    """
    data_length = len(data)
    batch_length = data_length // batch_size

    data = np.asarray(data[:batch_size * batch_length])
    data = data.reshape([batch_size, batch_length])

    epoch_size = (batch_length - 1) // num_steps
    if epoch_size == 0:
        raise ValueError("epoch_size == 0, decrease batch_size or num_steps")

    for i in range(epoch_size):
        x = data[:, i * num_steps: (i + 1) * num_steps]
        y = data[:, i * num_steps + 1: (i + 1) * num_steps + 1]
        yield (x, y)


</source>
<source file="systems/texar-0.2.4/examples/language_model_ptb/ptb_reader.py" startline="30" endline="48" pcid="1300">
def ptb_iterator(data, batch_size, num_steps):
    """Iterates through the ptb data.
    """
    data_length = len(data)
    batch_length = data_length // batch_size

    data = np.asarray(data[:batch_size * batch_length])
    data = data.reshape([batch_size, batch_length])

    epoch_size = (batch_length - 1) // num_steps
    if epoch_size == 0:
        raise ValueError("epoch_size == 0, decrease batch_size or num_steps")

    for i in range(epoch_size):
        x = data[:, i * num_steps: (i + 1) * num_steps]
        y = data[:, i * num_steps + 1: (i + 1) * num_steps + 1]
        yield (x, y)


</source>
</class>

<class classid="63" nclones="2" nlines="85" similarity="84">
<source file="systems/texar-0.2.4/examples/language_model_ptb/lm_ptb.py" startline="72" endline="179" pcid="1298">
def _main(_):
    # Data
    batch_size = config.batch_size
    num_steps = config.num_steps
    data = prepare_data(FLAGS.data_path)
    vocab_size = data["vocab_size"]

    inputs = tf.placeholder(tf.int32, [batch_size, num_steps])
    targets = tf.placeholder(tf.int32, [batch_size, num_steps])

    # Model architecture
    initializer = tf.random_uniform_initializer(
        -config.init_scale, config.init_scale)
    with tf.variable_scope("model", initializer=initializer):
        embedder = tx.modules.WordEmbedder(
            vocab_size=vocab_size, hparams=config.emb)
        emb_inputs = embedder(inputs)
        if config.keep_prob < 1:
            emb_inputs = tf.nn.dropout(
                emb_inputs, tx.utils.switch_dropout(config.keep_prob))

        decoder = tx.modules.BasicRNNDecoder(
            vocab_size=vocab_size, hparams={"rnn_cell": config.cell})
        initial_state = decoder.zero_state(batch_size, tf.float32)
        outputs, final_state, seq_lengths = decoder(
            decoding_strategy="train_greedy",
            impute_finished=True,
            inputs=emb_inputs,
            sequence_length=[num_steps] * batch_size,
            initial_state=initial_state)

    # Losses & train ops
    mle_loss = tx.losses.sequence_sparse_softmax_cross_entropy(
        labels=targets,
        logits=outputs.logits,
        sequence_length=seq_lengths)

    # Use global_step to pass epoch, for lr decay
    global_step = tf.placeholder(tf.int32)
    train_op = tx.core.get_train_op(
        mle_loss, global_step=global_step, increment_global_step=False,
        hparams=config.opt)

    def _run_epoch(sess, data_iter, epoch, is_train=False, verbose=False):
        start_time = time.time()
        loss = 0.
        iters = 0
        state = sess.run(initial_state)

        fetches = {
            "mle_loss": mle_loss,
            "final_state": final_state,
        }
        if is_train:
            fetches["train_op"] = train_op
            epoch_size = (len(data["train_text_id"]) // batch_size - 1)\
                // num_steps

        mode = (tf.estimator.ModeKeys.TRAIN
                if is_train
                else tf.estimator.ModeKeys.EVAL)

        for step, (x, y) in enumerate(data_iter):
            feed_dict = {
                inputs: x, targets: y, global_step: epoch,
                tx.global_mode(): mode,
            }
            for i, (c, h) in enumerate(initial_state):
                feed_dict[c] = state[i].c
                feed_dict[h] = state[i].h

            rets = sess.run(fetches, feed_dict)
            loss += rets["mle_loss"]
            state = rets["final_state"]
            iters += num_steps

            ppl = np.exp(loss / iters)
            if verbose and is_train and step % (epoch_size // 10) == 10:
                print("%.3f perplexity: %.3f speed: %.0f wps" %
                      ((step + 1) * 1.0 / epoch_size, ppl,
                       iters * batch_size / (time.time() - start_time)))

        ppl = np.exp(loss / iters)
        return ppl

    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        sess.run(tf.local_variables_initializer())
        sess.run(tf.tables_initializer())

        for epoch in range(config.num_epochs):
            # Train
            train_data_iter = ptb_iterator(
                data["train_text_id"], config.batch_size, num_steps)
            train_ppl = _run_epoch(
                sess, train_data_iter, epoch, is_train=True, verbose=True)
            print("Epoch: %d Train Perplexity: %.3f" % (epoch, train_ppl))
            # Valid
            valid_data_iter = ptb_iterator(
                data["valid_text_id"], config.batch_size, num_steps)
            valid_ppl = _run_epoch(sess, valid_data_iter, epoch)
            print("Epoch: %d Valid Perplexity: %.3f" % (epoch, valid_ppl))
        # Test
        test_data_iter = ptb_iterator(
            data["test_text_id"], batch_size, num_steps)
        test_ppl = _run_epoch(sess, test_data_iter, 0)
        print("Test Perplexity: %.3f" % (test_ppl))

</source>
<source file="systems/texar-0.2.4/examples/torchtext/lm_torchtext.py" startline="48" endline="162" pcid="1304">
def _main(_):
    # Data
    batch_size = config.batch_size
    num_steps = config.num_steps

    # setup vocabulary and data iterators with torchtext
    TEXT = data.Field()
    # make splits for data
    train, valid, test = datasets.PennTreebank.splits(TEXT)

    # build the vocabulary
    TEXT.build_vocab(train, vectors=None)
    vocab_size = len(TEXT.vocab)

    # make iterator for splits
    train_iter, valid_iter, test_iter = BatchFirstBPTTIterator.splits(
        (train, valid, test), batch_size=batch_size, bptt_len=num_steps,
        repeat=False)

    inputs = tf.placeholder(tf.int32, [batch_size, num_steps])
    targets = tf.placeholder(tf.int32, [batch_size, num_steps])

    # Model architecture
    initializer = tf.random_uniform_initializer(
        -config.init_scale, config.init_scale)
    with tf.variable_scope("model", initializer=initializer):
        embedder = tx.modules.WordEmbedder(
            vocab_size=vocab_size, hparams=config.emb)
        emb_inputs = embedder(inputs)
        if config.keep_prob < 1:
            emb_inputs = tf.nn.dropout(
                emb_inputs, tx.utils.switch_dropout(config.keep_prob))

        decoder = tx.modules.BasicRNNDecoder(
            vocab_size=vocab_size, hparams={"rnn_cell": config.cell})
        initial_state = decoder.zero_state(batch_size, tf.float32)
        outputs, final_state, seq_lengths = decoder(
            decoding_strategy="train_greedy",
            impute_finished=True,
            inputs=emb_inputs,
            sequence_length=[num_steps] * batch_size,
            initial_state=initial_state)

    # Losses & train ops
    mle_loss = tx.losses.sequence_sparse_softmax_cross_entropy(
        labels=targets,
        logits=outputs.logits,
        sequence_length=seq_lengths)

    # Use global_step to pass epoch, for lr decay
    global_step = tf.placeholder(tf.int32)
    train_op = tx.core.get_train_op(
        mle_loss, global_step=global_step, increment_global_step=False,
        hparams=config.opt)

    def _run_epoch(sess, data_iter, epoch, is_train=False, verbose=False):
        start_time = time.time()
        loss = 0.
        iters = 0
        state = sess.run(initial_state)

        fetches = {
            "mle_loss": mle_loss,
            "final_state": final_state,
        }
        if is_train:
            fetches["train_op"] = train_op

        mode = (tf.estimator.ModeKeys.TRAIN
                if is_train
                else tf.estimator.ModeKeys.EVAL)
        epoch_size = (len(train) // batch_size - 1) // num_steps
        for step, data_batch in enumerate(data_iter):
            feed_dict = {
                inputs: data_batch.text,
                targets: data_batch.target,
                global_step: epoch,
                tx.global_mode(): mode,
            }
            for i, (c, h) in enumerate(initial_state):
                feed_dict[c] = state[i].c
                feed_dict[h] = state[i].h

            rets = sess.run(fetches, feed_dict)
            loss += rets["mle_loss"]
            state = rets["final_state"]
            iters += num_steps

            ppl = np.exp(loss / iters)
            if verbose and step % (epoch_size // 10) == 10:
                print("%.3f perplexity: %.3f speed: %.0f wps" %
                      (step * 1.0 / epoch_size, ppl,
                       iters * batch_size / (time.time() - start_time)))

        ppl = np.exp(loss / iters)
        return ppl

    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        sess.run(tf.local_variables_initializer())
        sess.run(tf.tables_initializer())

        for epoch in range(config.num_epochs):
            # Train
            train_ppl = _run_epoch(
                sess, train_iter, epoch, is_train=True, verbose=True)
            print("Epoch: %d Train Perplexity: %.3f" % (epoch, train_ppl))
            # Valid
            valid_ppl = _run_epoch(sess, valid_iter, epoch)
            print("Epoch: %d Valid Perplexity: %.3f" % (epoch, valid_ppl))
        # Test
        test_ppl = _run_epoch(sess, test_iter, 0)
        print("Test Perplexity: %.3f" % (test_ppl))


</source>
</class>

</clones>

<clones>
<systeminfo processor="nicad6" system="ray-ray-1.11.0" granularity="functions" threshold="0%" minlines="10" maxlines="2500"/>
<cloneinfo npcs="7990" npairs="20"/>
<runinfo ncompares="160186" cputime="108341"/>
<classinfo nclasses="12"/>

<class classid="1" nclones="2" nlines="16" similarity="100">
<source file="systems/ray-ray-1.11.0/rllib/examples/models/shared_weights_model.py" startline="67" endline="84" pcid="1668">
    def __init__(self, observation_space, action_space, num_outputs,
                 model_config, name):
        super().__init__(observation_space, action_space, num_outputs,
                         model_config, name)

        inputs = tf.keras.layers.Input(observation_space.shape)
        with tf1.variable_scope(
                tf1.VariableScope(tf1.AUTO_REUSE, "shared"),
                reuse=tf1.AUTO_REUSE,
                auxiliary_name_scope=False):
            last_layer = tf.keras.layers.Dense(
                units=64, activation=tf.nn.relu, name="fc1")(inputs)
        output = tf.keras.layers.Dense(
            units=num_outputs, activation=None, name="fc_out")(last_layer)
        vf = tf.keras.layers.Dense(
            units=1, activation=None, name="value_out")(last_layer)
        self.base_model = tf.keras.models.Model(inputs, [output, vf])

</source>
<source file="systems/ray-ray-1.11.0/rllib/examples/models/shared_weights_model.py" startline="98" endline="117" pcid="1671">
    def __init__(self, observation_space, action_space, num_outputs,
                 model_config, name):
        super().__init__(observation_space, action_space, num_outputs,
                         model_config, name)

        inputs = tf.keras.layers.Input(observation_space.shape)

        # Weights shared with SharedWeightsModel1.
        with tf1.variable_scope(
                tf1.VariableScope(tf1.AUTO_REUSE, "shared"),
                reuse=tf1.AUTO_REUSE,
                auxiliary_name_scope=False):
            last_layer = tf.keras.layers.Dense(
                units=64, activation=tf.nn.relu, name="fc1")(inputs)
        output = tf.keras.layers.Dense(
            units=num_outputs, activation=None, name="fc_out")(last_layer)
        vf = tf.keras.layers.Dense(
            units=1, activation=None, name="value_out")(last_layer)
        self.base_model = tf.keras.models.Model(inputs, [output, vf])

</source>
</class>

<class classid="2" nclones="3" nlines="10" similarity="100">
<source file="systems/ray-ray-1.11.0/release/long_running_tests/workloads/actor_deaths.py" startline="73" endline="84" pcid="2191">
    def ping(self, num_pings):
        children_outputs = []
        for _ in range(num_pings):
            children_outputs += [
                child.ping.remote() for child in self.children
            ]
        try:
            ray.get(children_outputs)
        except Exception:
            # Replace the children if one of them died.
            self.__init__(len(self.children), self.death_probability)

</source>
<source file="systems/ray-ray-1.11.0/release/nightly_tests/stress_tests/test_dead_actors.py" startline="39" endline="50" pcid="2333">
    def ping(self, num_pings):
        children_outputs = []
        for _ in range(num_pings):
            children_outputs += [
                child.ping.remote() for child in self.children
            ]
        try:
            ray.get(children_outputs)
        except Exception:
            # Replace the children if one of them died.
            self.__init__(len(self.children), self.death_probability)

</source>
<source file="systems/ray-ray-1.11.0/python/ray/tests/test_threaded_actor.py" startline="219" endline="230" pcid="5853">
        def ping(self, num_pings):
            children_outputs = []
            for _ in range(num_pings):
                children_outputs += [
                    child.ping.remote() for child in self.children
                ]
            try:
                ray.get(children_outputs)
            except Exception:
                # Replace the children if one of them died.
                self.__init__(len(self.children), self.death_probability)

</source>
</class>

<class classid="3" nclones="2" nlines="10" similarity="100">
<source file="systems/ray-ray-1.11.0/release/serve_tests/workloads/serve_cluster_fault_tolerance_gcs.py" startline="28" endline="39" pcid="2225">
def request_with_retries(endpoint, timeout=3):
    start = time.time()
    while True:
        try:
            return requests.get(
                "http://127.0.0.1:8000" + endpoint, timeout=timeout)
        except requests.RequestException:
            if time.time() - start > timeout:
                raise TimeoutError
            time.sleep(0.1)


</source>
<source file="systems/ray-ray-1.11.0/release/serve_tests/workloads/serve_cluster_fault_tolerance.py" startline="30" endline="41" pcid="2229">
def request_with_retries(endpoint, timeout=3):
    start = time.time()
    while True:
        try:
            return requests.get(
                "http://127.0.0.1:8000" + endpoint, timeout=timeout)
        except requests.RequestException:
            if time.time() - start > timeout:
                raise TimeoutError
            time.sleep(0.1)


</source>
</class>

<class classid="4" nclones="3" nlines="12" similarity="100">
<source file="systems/ray-ray-1.11.0/doc/kubernetes/example_scripts/job_example.py" startline="18" endline="32" pcid="2378">
def wait_for_nodes(expected):
    # Wait for all nodes to join the cluster.
    while True:
        resources = ray.cluster_resources()
        node_keys = [key for key in resources if "node" in key]
        num_nodes = sum(resources[node_key] for node_key in node_keys)
        if num_nodes < expected:
            print("{} nodes have joined so far, waiting for {} more.".format(
                num_nodes, expected - num_nodes))
            sys.stdout.flush()
            time.sleep(1)
        else:
            break


</source>
<source file="systems/ray-ray-1.11.0/doc/kubernetes/example_scripts/run_on_head.py" startline="17" endline="31" pcid="2381">
def wait_for_nodes(expected):
    # Wait for all nodes to join the cluster.
    while True:
        resources = ray.cluster_resources()
        node_keys = [key for key in resources if "node" in key]
        num_nodes = sum(resources[node_key] for node_key in node_keys)
        if num_nodes < expected:
            print("{} nodes have joined so far, waiting for {} more.".format(
                num_nodes, expected - num_nodes))
            sys.stdout.flush()
            time.sleep(1)
        else:
            break


</source>
<source file="systems/ray-ray-1.11.0/doc/kubernetes/example_scripts/run_local_example.py" startline="25" endline="39" pcid="2384">
def wait_for_nodes(expected):
    # Wait for all nodes to join the cluster.
    while True:
        resources = ray.cluster_resources()
        node_keys = [key for key in resources if "node" in key]
        num_nodes = sum(resources[node_key] for node_key in node_keys)
        if num_nodes < expected:
            print("{} nodes have joined so far, waiting for {} more.".format(
                num_nodes, expected - num_nodes))
            sys.stdout.flush()
            time.sleep(1)
        else:
            break


</source>
</class>

<class classid="5" nclones="3" nlines="11" similarity="100">
<source file="systems/ray-ray-1.11.0/doc/kubernetes/example_scripts/job_example.py" startline="33" endline="48" pcid="2379">
def main():
    wait_for_nodes(3)

    # Check that objects can be transferred from each node to each other node.
    for i in range(10):
        print("Iteration {}".format(i))
        results = [
            gethostname.remote(gethostname.remote(())) for _ in range(100)
        ]
        print(Counter(ray.get(results)))
        sys.stdout.flush()

    print("Success!")
    sys.stdout.flush()


</source>
<source file="systems/ray-ray-1.11.0/doc/kubernetes/example_scripts/run_on_head.py" startline="32" endline="47" pcid="2382">
def main():
    wait_for_nodes(3)

    # Check that objects can be transferred from each node to each other node.
    for i in range(10):
        print("Iteration {}".format(i))
        results = [
            gethostname.remote(gethostname.remote(())) for _ in range(100)
        ]
        print(Counter(ray.get(results)))
        sys.stdout.flush()

    print("Success!")
    sys.stdout.flush()


</source>
<source file="systems/ray-ray-1.11.0/doc/kubernetes/example_scripts/run_local_example.py" startline="40" endline="55" pcid="2385">
def main():
    wait_for_nodes(3)

    # Check that objects can be transferred from each node to each other node.
    for i in range(10):
        print("Iteration {}".format(i))
        results = [
            gethostname.remote(gethostname.remote(())) for _ in range(100)
        ]
        print(Counter(ray.get(results)))
        sys.stdout.flush()

    print("Success!")
    sys.stdout.flush()


</source>
</class>

<class classid="6" nclones="2" nlines="11" similarity="100">
<source file="systems/ray-ray-1.11.0/python/ray/tune/tests/test_torch_trainable.py" startline="154" endline="166" pcid="3002">
def test_colocated_gpu(ray_4_node_gpu):  # noqa: F811
    assert ray.available_resources()["GPU"] == 8
    trainable_cls = DistributedTrainableCreator(
        _train_check_global,
        num_workers=4,
        num_gpus_per_worker=2,
        num_workers_per_host=1)
    trainable = trainable_cls()
    assert ray.available_resources().get("GPU", 0) == 0
    trainable.train()
    trainable.stop()


</source>
<source file="systems/ray-ray-1.11.0/python/ray/tune/tests/test_tensorflow_trainable.py" startline="115" endline="127" pcid="3194">
def test_colocated_gpu(ray_4_node_gpu):  # noqa: F811
    assert ray.available_resources()["GPU"] == 8
    trainable_cls = DistributedTrainableCreator(
        _train_check_global,
        num_workers=4,
        num_gpus_per_worker=2,
        num_workers_per_host=1)
    trainable = trainable_cls()
    assert ray.available_resources().get("GPU", 0) == 0
    trainable.train()
    trainable.stop()


</source>
</class>

<class classid="7" nclones="3" nlines="10" similarity="100">
<source file="systems/ray-ray-1.11.0/python/ray/tune/examples/mnist_ptl_mini.py" startline="29" endline="39" pcid="3773">
    def forward(self, x):
        batch_size, channels, width, height = x.size()
        x = x.view(batch_size, -1)
        x = self.layer_1(x)
        x = torch.relu(x)
        x = self.layer_2(x)
        x = torch.relu(x)
        x = self.layer_3(x)
        x = torch.log_softmax(x, dim=1)
        return x

</source>
<source file="systems/ray-ray-1.11.0/python/ray/tune/examples/mnist_pytorch_lightning.py" startline="48" endline="62" pcid="3852">

    def forward(self, x):
        batch_size, channels, width, height = x.size()
        x = x.view(batch_size, -1)

        x = self.layer_1(x)
        x = torch.relu(x)

        x = self.layer_2(x)
        x = torch.relu(x)

        x = self.layer_3(x)
        x = torch.log_softmax(x, dim=1)

        return x
</source>
<source file="systems/ray-ray-1.11.0/python/ray/util/sgd/torch/examples/pytorch-lightning/mnist-ptl.py" startline="37" endline="50" pcid="6840">
    def forward(self, x):
        batch_size, channels, width, height = x.size()

        # (b, 1, 28, 28) -> (b, 1*28*28)
        x = x.view(batch_size, -1)
        x = self.layer_1(x)
        x = torch.relu(x)
        x = self.layer_2(x)
        x = torch.relu(x)
        x = self.layer_3(x)

        x = torch.log_softmax(x, dim=1)
        return x

</source>
</class>

<class classid="8" nclones="2" nlines="10" similarity="100">
<source file="systems/ray-ray-1.11.0/python/ray/data/tests/mock_server.py" startline="50" endline="67" pcid="4431">
def stop_process(process):
    try:
        process.send_signal(signal.SIGTERM)
        process.communicate(timeout=20)
    except sp.TimeoutExpired:
        process.kill()
        outs, errors = process.communicate(timeout=20)
        exit_code = process.returncode
        msg = "Child process finished {} not in clean way: {} {}" \
            .format(exit_code, outs, errors)
        raise RuntimeError(msg)


# TODO(Clark): We should be able to use "session" scope here, but we've found
# that the s3_fs fixture ends up hanging with S3 ops timing out (or the server
# being unreachable). This appears to only be an issue when using the tmp_dir
# fixture as the S3 dir path. We should fix this since "session" scope should
# reduce a lot of the per-test overhead (2x faster execution for IO methods in
</source>
<source file="systems/ray-ray-1.11.0/python/ray/workflow/tests/mock_server.py" startline="48" endline="59" pcid="4527">
def stop_process(process):
    try:
        process.send_signal(signal.SIGTERM)
        process.communicate(timeout=20)
    except sp.TimeoutExpired:
        process.kill()
        outs, errors = process.communicate(timeout=20)
        exit_code = process.returncode
        msg = "Child process finished {} not in clean way: {} {}" \
            .format(exit_code, outs, errors)
        raise RuntimeError(msg)

</source>
</class>

<class classid="9" nclones="2" nlines="12" similarity="100">
<source file="systems/ray-ray-1.11.0/python/ray/tests/test_reference_counting.py" startline="36" endline="52" pcid="5081">
def _fill_object_store_and_get(obj, succeed=True, object_MiB=20,
                               num_objects=5):
    for _ in range(num_objects):
        ray.put(np.zeros(object_MiB * 1024 * 1024, dtype=np.uint8))

    if type(obj) is bytes:
        obj = ray.ObjectRef(obj)

    if succeed:
        wait_for_condition(
            lambda: ray.worker.global_worker.core_worker.object_exists(obj))
    else:
        wait_for_condition(
            lambda: not ray.worker.global_worker.core_worker.object_exists(obj)
        )


</source>
<source file="systems/ray-ray-1.11.0/python/ray/tests/test_reference_counting_2.py" startline="38" endline="56" pcid="5283">
def _fill_object_store_and_get(obj, succeed=True, object_MiB=20,
                               num_objects=5):
    for _ in range(num_objects):
        ray.put(np.zeros(object_MiB * 1024 * 1024, dtype=np.uint8))

    if type(obj) is bytes:
        obj = ray.ObjectRef(obj)

    if succeed:
        wait_for_condition(
            lambda: ray.worker.global_worker.core_worker.object_exists(obj))
    else:
        wait_for_condition(
            lambda: not ray.worker.global_worker.core_worker.object_exists(obj)
        )


# Test that an object containing object refs within it pins the inner IDs
# recursively and for submitted tasks.
</source>
</class>

<class classid="10" nclones="2" nlines="14" similarity="100">
<source file="systems/ray-ray-1.11.0/python/ray/tests/test_component_failures_2.py" startline="77" endline="93" pcid="5481">
def check_components_alive(cluster, component_type, check_component_alive):
    """Check that a given component type is alive on all worker nodes."""
    worker_nodes = get_other_nodes(cluster)
    assert len(worker_nodes) > 0
    for node in worker_nodes:
        process = node.all_processes[component_type][0].process
        if check_component_alive:
            assert process.poll() is None
        else:
            print("waiting for " + component_type + " with PID " +
                  str(process.pid) + "to terminate")
            process.wait()
            print("done waiting for " + component_type + " with PID " +
                  str(process.pid) + "to terminate")
            assert not process.poll() is None


</source>
<source file="systems/ray-ray-1.11.0/python/ray/tests/test_multinode_failures.py" startline="135" endline="151" pcid="5828">
def check_components_alive(cluster, component_type, check_component_alive):
    """Check that a given component type is alive on all worker nodes."""
    worker_nodes = get_other_nodes(cluster)
    assert len(worker_nodes) > 0
    for node in worker_nodes:
        process = node.all_processes[component_type][0].process
        if check_component_alive:
            assert process.poll() is None
        else:
            print("waiting for " + component_type + " with PID " +
                  str(process.pid) + "to terminate")
            process.wait()
            print("done waiting for " + component_type + " with PID " +
                  str(process.pid) + "to terminate")
            assert not process.poll() is None


</source>
</class>

<class classid="11" nclones="2" nlines="11" similarity="100">
<source file="systems/ray-ray-1.11.0/python/ray/util/sgd/tests/test_torch_basic.py" startline="40" endline="53" pcid="6999">
def test_single_step(ray_start_2_cpus, use_local):  # noqa: F811
    trainer = TorchTrainer(
        training_operator_cls=Operator,
        num_workers=1,
        use_local=use_local,
        use_gpu=False)
    metrics = trainer.train(num_steps=1)
    assert metrics[BATCH_COUNT] == 1

    val_metrics = trainer.validate(num_steps=1)
    assert val_metrics[BATCH_COUNT] == 1
    trainer.shutdown()


</source>
<source file="systems/ray-ray-1.11.0/python/ray/util/sgd/tests/test_ptl.py" startline="87" endline="100" pcid="7049">
@pytest.mark.parametrize("use_local", [True, False])
def test_single_step(ray_start_2_cpus, use_local):  # noqa: F811
    trainer = TorchTrainer(
        training_operator_cls=Operator,
        num_workers=1,
        use_local=use_local,
        use_gpu=False)
    metrics = trainer.train(num_steps=1)
    assert metrics[BATCH_COUNT] == 1

    val_metrics = trainer.validate(num_steps=1)
    assert val_metrics[BATCH_COUNT] == 1
    trainer.shutdown()

</source>
</class>

<class classid="12" nclones="2" nlines="13" similarity="100">
<source file="systems/ray-ray-1.11.0/python/ray/util/collective/tests/util.py" startline="100" endline="115" pcid="7351">
def create_collective_workers(num_workers=2,
                              group_name="default",
                              backend="nccl"):
    actors = [None] * num_workers
    for i in range(num_workers):
        actor = Worker.remote()
        ray.get([actor.init_tensors.remote()])
        actors[i] = actor
    world_size = num_workers
    init_results = ray.get([
        actor.init_group.remote(world_size, i, backend, group_name)
        for i, actor in enumerate(actors)
    ])
    return actors, init_results


</source>
<source file="systems/ray-ray-1.11.0/python/ray/util/collective/tests/cpu_util.py" startline="107" endline="122" pcid="7426">
def create_collective_workers(num_workers=2,
                              group_name="default",
                              backend="nccl"):
    actors = [None] * num_workers
    for i in range(num_workers):
        actor = Worker.remote()
        ray.get([actor.init_tensors.remote()])
        actors[i] = actor
    world_size = num_workers
    init_results = ray.get([
        actor.init_group.remote(world_size, i, backend, group_name)
        for i, actor in enumerate(actors)
    ])
    return actors, init_results


</source>
</class>

</clones>

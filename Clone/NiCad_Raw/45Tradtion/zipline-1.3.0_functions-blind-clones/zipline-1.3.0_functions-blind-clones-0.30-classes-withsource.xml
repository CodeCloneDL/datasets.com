<clones>
<systeminfo processor="nicad6" system="zipline-1.3.0" granularity="functions-blind" threshold="30%" minlines="10" maxlines="2500"/>
<cloneinfo npcs="1673" npairs="114"/>
<runinfo ncompares="73882" cputime="81508"/>
<classinfo nclasses="63"/>

<class classid="1" nclones="2" nlines="12" similarity="76">
<source file="systems/zipline-1.3.0/tests/test_registration_manager.py" startline="12" endline="33" pcid="7">
    def test_load_not_registered(self):
        rm = Registry(FakeInterface)

        msg = (
            "no FakeInterface factory registered under name 'ayy-lmao', "
            "options are: []"
        )
        with assert_raises_str(ValueError, msg):
            rm.load('ayy-lmao')

        # register in reverse order to test the sorting of the options
        rm.register('c', FakeInterface)
        rm.register('b', FakeInterface)
        rm.register('a', FakeInterface)

        msg = (
            "no FakeInterface factory registered under name 'ayy-lmao', "
            "options are: ['a', 'b', 'c']"
        )
        with assert_raises_str(ValueError, msg):
            rm.load('ayy-lmao')

</source>
<source file="systems/zipline-1.3.0/tests/metrics/test_core.py" startline="22" endline="38" pcid="459">
    def test_load_not_registered(self):
        msg = "no metrics set registered as 'ayy-lmao', options are: []"
        with assert_raises_str(ValueError, msg):
            self.load('ayy-lmao')

        # register in reverse order to test the sorting of the options
        self.register('c', set)
        self.register('b', set)
        self.register('a', set)

        msg = (
            "no metrics set registered as 'ayy-lmao', options are: "
            "['a', 'b', 'c']"
        )
        with assert_raises_str(ValueError, msg):
            self.load('ayy-lmao')

</source>
</class>

<class classid="2" nclones="2" nlines="25" similarity="84">
<source file="systems/zipline-1.3.0/tests/test_registration_manager.py" startline="34" endline="76" pcid="8">
    def test_register_decorator(self):
        rm = Registry(FakeInterface)

        @rm.register('ayy-lmao')
        class ProperDummyInterface(FakeInterface):
            pass

        def check_registered():
            assert_true(
                rm.is_registered('ayy-lmao'),
                "Class ProperDummyInterface wasn't properly registered under"
                "name 'ayy-lmao'"
            )
            self.assertIsInstance(rm.load('ayy-lmao'), ProperDummyInterface)

        # Check that we successfully registered.
        check_registered()

        # Try and fail to register with the same key again.
        m = "FakeInterface factory with name 'ayy-lmao' is already registered"
        with assert_raises_str(ValueError, m):
            @rm.register('ayy-lmao')
            class Fake(object):
                pass

        # check that the failed registration didn't break the previous
        # registration
        check_registered()

        # Unregister the key and assert that the key is now gone.
        rm.unregister('ayy-lmao')

        msg = (
            "no FakeInterface factory registered under name 'ayy-lmao', "
            "options are: []"
        )
        with assert_raises_str(ValueError, msg):
            rm.load('ayy-lmao')

        msg = "FakeInterface factory 'ayy-lmao' was not already registered"
        with assert_raises_str(ValueError, msg):
            rm.unregister('ayy-lmao')

</source>
<source file="systems/zipline-1.3.0/tests/test_registration_manager.py" startline="77" endline="119" pcid="10">
    def test_register_non_decorator(self):
        rm = Registry(FakeInterface)

        class ProperDummyInterface(FakeInterface):
            pass

        rm.register('ayy-lmao', ProperDummyInterface)

        def check_registered():
            assert_true(
                rm.is_registered('ayy-lmao'),
                "Class ProperDummyInterface wasn't properly registered under"
                "name 'ayy-lmao'"
            )
            self.assertIsInstance(rm.load('ayy-lmao'), ProperDummyInterface)

        # Check that we successfully registered.
        check_registered()

        class Fake(object):
            pass

        # Try and fail to register with the same key again.
        m = "FakeInterface factory with name 'ayy-lmao' is already registered"
        with assert_raises_str(ValueError, m):
            rm.register('ayy-lmao', Fake)

        # check that the failed registration didn't break the previous
        # registration
        check_registered()

        rm.unregister('ayy-lmao')

        msg = (
            "no FakeInterface factory registered under name 'ayy-lmao', "
            "options are: []"
        )
        with assert_raises_str(ValueError, msg):
            rm.load('ayy-lmao')

        msg = "FakeInterface factory 'ayy-lmao' was not already registered"
        with assert_raises_str(ValueError, msg):
            rm.unregister('ayy-lmao')
</source>
</class>

<class classid="3" nclones="2" nlines="15" similarity="70">
<source file="systems/zipline-1.3.0/tests/test_continuous_futures.py" startline="395" endline="411" pcid="17">
    def test_current_contract(self):
        cf_primary = self.asset_finder.create_continuous_future(
            'FO', 0, 'calendar', None)
        bar_data = self.create_bardata(
            lambda: pd.Timestamp('2016-01-26', tz='UTC'))
        contract = bar_data.current(cf_primary, 'contract')

        self.assertEqual(contract.symbol, 'FOF16')

        bar_data = self.create_bardata(
            lambda: pd.Timestamp('2016-01-27', tz='UTC'))
        contract = bar_data.current(cf_primary, 'contract')

        self.assertEqual(contract.symbol, 'FOG16',
                         'Auto close at beginning of session so FOG16 is now '
                         'the current contract.')

</source>
<source file="systems/zipline-1.3.0/tests/test_continuous_futures.py" startline="484" endline="506" pcid="20">
    def test_current_contract_volume_roll(self):
        cf_primary = self.asset_finder.create_continuous_future(
            'FO', 0, 'volume', None)
        bar_data = self.create_bardata(
            lambda: pd.Timestamp('2016-01-26', tz='UTC'))
        contract = bar_data.current(cf_primary, 'contract')

        self.assertEqual(contract.symbol, 'FOF16')

        bar_data = self.create_bardata(
            lambda: pd.Timestamp('2016-01-27', tz='UTC'))
        contract = bar_data.current(cf_primary, 'contract')

        self.assertEqual(contract.symbol, 'FOG16',
                         'Auto close at beginning of session. FOG16 is now '
                         'the current contract.')

        bar_data = self.create_bardata(
            lambda: pd.Timestamp('2016-02-29', tz='UTC'))
        contract = bar_data.current(cf_primary, 'contract')
        self.assertEqual(contract.symbol, 'FOH16',
                         'Volume switch to FOH16, should have triggered roll.')

</source>
</class>

<class classid="4" nclones="2" nlines="26" similarity="80">
<source file="systems/zipline-1.3.0/tests/test_continuous_futures.py" startline="412" endline="445" pcid="18">
    def test_get_value_contract_daily(self):
        cf_primary = self.asset_finder.create_continuous_future(
            'FO', 0, 'calendar', None)

        contract = self.data_portal.get_spot_value(
            cf_primary,
            'contract',
            pd.Timestamp('2016-01-26', tz='UTC'),
            'daily',
        )

        self.assertEqual(contract.symbol, 'FOF16')

        contract = self.data_portal.get_spot_value(
            cf_primary,
            'contract',
            pd.Timestamp('2016-01-27', tz='UTC'),
            'daily',
        )

        self.assertEqual(contract.symbol, 'FOG16',
                         'Auto close at beginning of session so FOG16 is now '
                         'the current contract.')

        # Test that the current contract outside of the continuous future's
        # start and end dates is None.
        contract = self.data_portal.get_spot_value(
            cf_primary,
            'contract',
            self.START_DATE - self.trading_calendar.day,
            'daily',
        )
        self.assertIsNone(contract)

</source>
<source file="systems/zipline-1.3.0/tests/test_continuous_futures.py" startline="446" endline="483" pcid="19">
    def test_get_value_close_daily(self):
        cf_primary = self.asset_finder.create_continuous_future(
            'FO', 0, 'calendar', None)

        value = self.data_portal.get_spot_value(
            cf_primary,
            'close',
            pd.Timestamp('2016-01-26', tz='UTC'),
            'daily',
        )

        self.assertEqual(value, 105011.44)

        value = self.data_portal.get_spot_value(
            cf_primary,
            'close',
            pd.Timestamp('2016-01-27', tz='UTC'),
            'daily',
        )

        self.assertEqual(value, 115021.44,
                         'Auto close at beginning of session so FOG16 is now '
                         'the current contract.')

        # Check a value which occurs after the end date of the last known
        # contract, to prevent a regression where the end date of the last
        # contract was used instead of the max date of all contracts.
        value = self.data_portal.get_spot_value(
            cf_primary,
            'close',
            pd.Timestamp('2016-03-26', tz='UTC'),
            'daily',
        )

        self.assertEqual(value, 135441.44,
                         'Value should be for FOJ16, even though last '
                         'contract ends before query date.')

</source>
</class>

<class classid="5" nclones="3" nlines="43" similarity="87">
<source file="systems/zipline-1.3.0/tests/test_continuous_futures.py" startline="653" endline="706" pcid="23">
    def test_history_sid_session(self):
        cf = self.data_portal.asset_finder.create_continuous_future(
            'FO', 0, 'calendar', None)
        window = self.data_portal.get_history_window(
            [cf],
            Timestamp('2016-03-04 18:01', tz='US/Eastern').tz_convert('UTC'),
            30, '1d', 'sid', 'minute')

        self.assertEqual(window.loc['2016-01-26', cf],
                         0,
                         "Should be FOF16 at beginning of window.")

        self.assertEqual(window.loc['2016-01-27', cf],
                         1,
                         "Should be FOG16 after first roll.")

        self.assertEqual(window.loc['2016-02-25', cf],
                         1,
                         "Should be FOG16 on session before roll.")

        self.assertEqual(window.loc['2016-02-26', cf],
                         2,
                         "Should be FOH16 on session with roll.")

        self.assertEqual(window.loc['2016-02-29', cf],
                         2,
                         "Should be FOH16 on session after roll.")

        # Advance the window a month.
        window = self.data_portal.get_history_window(
            [cf],
            Timestamp('2016-04-06 18:01', tz='US/Eastern').tz_convert('UTC'),
            30, '1d', 'sid', 'minute')

        self.assertEqual(window.loc['2016-02-25', cf],
                         1,
                         "Should be FOG16 at beginning of window.")

        self.assertEqual(window.loc['2016-02-26', cf],
                         2,
                         "Should be FOH16 on session with roll.")

        self.assertEqual(window.loc['2016-02-29', cf],
                         2,
                         "Should be FOH16 on session after roll.")

        self.assertEqual(window.loc['2016-03-24', cf],
                         3,
                         "Should be FOJ16 on session with roll.")

        self.assertEqual(window.loc['2016-03-28', cf],
                         3,
                         "Should be FOJ16 on session after roll.")

</source>
<source file="systems/zipline-1.3.0/tests/test_continuous_futures.py" startline="782" endline="845" pcid="26">
    def test_history_sid_session_volume_roll(self):
        cf = self.data_portal.asset_finder.create_continuous_future(
            'FO', 0, 'volume', None)
        window = self.data_portal.get_history_window(
            [cf],
            Timestamp('2016-03-04 18:01', tz='US/Eastern').tz_convert('UTC'),
            30, '1d', 'sid', 'minute')

        # Volume cuts out for FOF16 on 2016-01-25
        self.assertEqual(window.loc['2016-01-26', cf],
                         0,
                         "Should be FOF16 at beginning of window.")

        self.assertEqual(window.loc['2016-01-27', cf],
                         1,
                         "Should have rolled to FOG16.")

        self.assertEqual(window.loc['2016-02-26', cf],
                         1,
                         "Should be FOG16 on session before roll.")

        self.assertEqual(window.loc['2016-02-29', cf],
                         2,
                         "Should be FOH16 on session with roll.")

        self.assertEqual(window.loc['2016-03-01', cf],
                         2,
                         "Should be FOH16 on session after roll.")

        # Advance the window a month.
        window = self.data_portal.get_history_window(
            [cf],
            Timestamp('2016-04-06 18:01', tz='US/Eastern').tz_convert('UTC'),
            30, '1d', 'sid', 'minute')

        self.assertEqual(window.loc['2016-02-26', cf],
                         1,
                         "Should be FOG16 at beginning of window.")

        self.assertEqual(window.loc['2016-02-29', cf],
                         2,
                         "Should be FOH16 on roll session.")

        self.assertEqual(window.loc['2016-03-01', cf],
                         2,
                         "Should remain FOH16.")

        self.assertEqual(window.loc['2016-03-17', cf],
                         2,
                         "Should be FOH16 on session before volume cuts out.")

        self.assertEqual(window.loc['2016-03-18', cf],
                         2,
                         "Should be FOH16 on session where the volume of "
                         "FOH16 cuts out, the roll is upcoming.")

        self.assertEqual(window.loc['2016-03-24', cf],
                         3,
                         "Should have rolled to FOJ16.")

        self.assertEqual(window.loc['2016-03-28', cf],
                         3,
                         "Should have remained FOJ16.")

</source>
<source file="systems/zipline-1.3.0/tests/test_continuous_futures.py" startline="728" endline="781" pcid="25">
    def test_history_sid_session_secondary(self):
        cf = self.data_portal.asset_finder.create_continuous_future(
            'FO', 1, 'calendar', None)
        window = self.data_portal.get_history_window(
            [cf],
            Timestamp('2016-03-04 18:01', tz='US/Eastern').tz_convert('UTC'),
            30, '1d', 'sid', 'minute')

        self.assertEqual(window.loc['2016-01-26', cf],
                         1,
                         "Should be FOG16 at beginning of window.")

        self.assertEqual(window.loc['2016-01-27', cf],
                         2,
                         "Should be FOH16 after first roll.")

        self.assertEqual(window.loc['2016-02-25', cf],
                         2,
                         "Should be FOH16 on session before roll.")

        self.assertEqual(window.loc['2016-02-26', cf],
                         3,
                         "Should be FOJ16 on session with roll.")

        self.assertEqual(window.loc['2016-02-29', cf],
                         3,
                         "Should be FOJ16 on session after roll.")

        # Advance the window a month.
        window = self.data_portal.get_history_window(
            [cf],
            Timestamp('2016-04-06 18:01', tz='US/Eastern').tz_convert('UTC'),
            30, '1d', 'sid', 'minute')

        self.assertEqual(window.loc['2016-02-25', cf],
                         2,
                         "Should be FOH16 at beginning of window.")

        self.assertEqual(window.loc['2016-02-26', cf],
                         3,
                         "Should be FOJ16 on session with roll.")

        self.assertEqual(window.loc['2016-02-29', cf],
                         3,
                         "Should be FOJ16 on session after roll.")

        self.assertEqual(window.loc['2016-03-24', cf],
                         4,
                         "Should be FOK16 on session with roll.")

        self.assertEqual(window.loc['2016-03-28', cf],
                         4,
                         "Should be FOK16 on session after roll.")

</source>
</class>

<class classid="6" nclones="2" nlines="26" similarity="80">
<source file="systems/zipline-1.3.0/tests/test_continuous_futures.py" startline="846" endline="880" pcid="27">
    def test_history_sid_minute(self):
        cf = self.data_portal.asset_finder.create_continuous_future(
            'FO', 0, 'calendar', None)
        window = self.data_portal.get_history_window(
            [cf.sid],
            Timestamp('2016-01-26 18:01', tz='US/Eastern').tz_convert('UTC'),
            30, '1m', 'sid', 'minute')

        self.assertEqual(window.loc['2016-01-26 22:32', cf],
                         0,
                         "Should be FOF16 at beginning of window. A minute "
                         "which is in the 01-26 session, before the roll.")

        self.assertEqual(window.loc['2016-01-26 23:00', cf],
                         0,
                         "Should be FOF16 on on minute before roll minute.")

        self.assertEqual(window.loc['2016-01-26 23:01', cf],
                         1,
                         "Should be FOG16 on minute after roll.")

        # Advance the window a day.
        window = self.data_portal.get_history_window(
            [cf],
            Timestamp('2016-01-27 18:01', tz='US/Eastern').tz_convert('UTC'),
            30, '1m', 'sid', 'minute')

        self.assertEqual(window.loc['2016-01-27 22:32', cf],
                         1,
                         "Should be FOG16 at beginning of window.")

        self.assertEqual(window.loc['2016-01-27 23:01', cf],
                         1,
                         "Should remain FOG16 on next session.")

</source>
<source file="systems/zipline-1.3.0/tests/test_continuous_futures.py" startline="1109" endline="1143" pcid="31">
    def test_history_close_minute(self):
        cf = self.data_portal.asset_finder.create_continuous_future(
            'FO', 0, 'calendar', None)
        window = self.data_portal.get_history_window(
            [cf.sid],
            Timestamp('2016-02-25 18:01', tz='US/Eastern').tz_convert('UTC'),
            30, '1m', 'close', 'minute')

        self.assertEqual(window.loc['2016-02-25 22:32', cf],
                         115231.412,
                         "Should be FOG16 at beginning of window. A minute "
                         "which is in the 02-25 session, before the roll.")

        self.assertEqual(window.loc['2016-02-25 23:00', cf],
                         115231.440,
                         "Should be FOG16 on on minute before roll minute.")

        self.assertEqual(window.loc['2016-02-25 23:01', cf],
                         125240.001,
                         "Should be FOH16 on minute after roll.")

        # Advance the window a session.
        window = self.data_portal.get_history_window(
            [cf],
            Timestamp('2016-02-28 18:01', tz='US/Eastern').tz_convert('UTC'),
            30, '1m', 'close', 'minute')

        self.assertEqual(window.loc['2016-02-26 22:32', cf],
                         125241.412,
                         "Should be FOH16 at beginning of window.")

        self.assertEqual(window.loc['2016-02-28 23:01', cf],
                         125250.001,
                         "Should remain FOH16 on next session.")

</source>
</class>

<class classid="7" nclones="2" nlines="39" similarity="81">
<source file="systems/zipline-1.3.0/tests/test_continuous_futures.py" startline="881" endline="934" pcid="28">
    def test_history_close_session(self):
        cf = self.data_portal.asset_finder.create_continuous_future(
            'FO', 0, 'calendar', None)
        window = self.data_portal.get_history_window(
            [cf.sid],
            Timestamp('2016-03-06', tz='UTC'),
            30, '1d', 'close', 'daily')

        assert_almost_equal(
            window.loc['2016-01-26', cf],
            105011.440,
            err_msg="At beginning of window, should be FOG16's first value.")

        assert_almost_equal(
            window.loc['2016-02-26', cf],
            125241.440,
            err_msg="On session with roll, should be FOH16's 24th value.")

        assert_almost_equal(
            window.loc['2016-02-29', cf],
            125251.440,
            err_msg="After roll, Should be FOH16's 25th value.")

        # Advance the window a month.
        window = self.data_portal.get_history_window(
            [cf.sid],
            Timestamp('2016-04-06', tz='UTC'),
            30, '1d', 'close', 'daily')

        assert_almost_equal(
            window.loc['2016-02-24', cf],
            115221.440,
            err_msg="At beginning of window, should be FOG16's 22nd value.")

        assert_almost_equal(
            window.loc['2016-02-26', cf],
            125241.440,
            err_msg="On session with roll, should be FOH16's 24th value.")

        assert_almost_equal(
            window.loc['2016-02-29', cf],
            125251.440,
            err_msg="On session after roll, should be FOH16's 25th value.")

        assert_almost_equal(
            window.loc['2016-03-24', cf],
            135431.440,
            err_msg="On session with roll, should be FOJ16's 43rd value.")

        assert_almost_equal(
            window.loc['2016-03-28', cf],
            135441.440,
            err_msg="On session after roll, Should be FOJ16's 44th value.")

</source>
<source file="systems/zipline-1.3.0/tests/test_continuous_futures.py" startline="935" endline="978" pcid="29">
    def test_history_close_session_skip_volume(self):
        cf = self.data_portal.asset_finder.create_continuous_future(
            'MA', 0, 'volume', None)
        window = self.data_portal.get_history_window(
            [cf.sid],
            Timestamp('2016-03-06', tz='UTC'),
            30, '1d', 'close', 'daily')

        assert_almost_equal(
            window.loc['2016-01-26', cf],
            245011.440,
            err_msg="At beginning of window, should be MAG16's first value.")

        assert_almost_equal(
            window.loc['2016-02-26', cf],
            265241.440,
            err_msg="Should have skipped MAH16 to MAJ16.")

        assert_almost_equal(
            window.loc['2016-02-29', cf],
            265251.440,
            err_msg="Should have remained MAJ16.")

        # Advance the window a month.
        window = self.data_portal.get_history_window(
            [cf.sid],
            Timestamp('2016-04-06', tz='UTC'),
            30, '1d', 'close', 'daily')

        assert_almost_equal(
            window.loc['2016-02-24', cf],
            265221.440,
            err_msg="Should be MAJ16, having skipped MAH16.")

        assert_almost_equal(
            window.loc['2016-02-29', cf],
            265251.440,
            err_msg="Should be MAJ1 for rest of window.")

        assert_almost_equal(
            window.loc['2016-03-24', cf],
            265431.440,
            err_msg="Should be MAJ16 for rest of window.")

</source>
</class>

<class classid="8" nclones="2" nlines="39" similarity="100">
<source file="systems/zipline-1.3.0/tests/test_continuous_futures.py" startline="1144" endline="1207" pcid="32">
    def test_history_close_minute_adjusted(self):
        cf = self.data_portal.asset_finder.create_continuous_future(
            'FO', 0, 'calendar', None)
        cf_mul = self.data_portal.asset_finder.create_continuous_future(
            'FO', 0, 'calendar', 'mul')
        cf_add = self.data_portal.asset_finder.create_continuous_future(
            'FO', 0, 'calendar', 'add')
        window = self.data_portal.get_history_window(
            [cf, cf_mul, cf_add],
            Timestamp('2016-02-25 18:01', tz='US/Eastern').tz_convert('UTC'),
            30, '1m', 'close', 'minute')

        # Unadjusted: 115231.412
        # Adjustment based on roll:
        # 2016-02-25 23:00:00+00:00
        # front: 115231.440
        # back:  125231.440
        # Ratio: ~0.920
        # Difference: 10000.00
        self.assertEqual(window.loc['2016-02-25 22:32', cf_mul],
                         125231.41,
                         "Should be FOG16 at beginning of window. A minute "
                         "which is in the 02-25 session, before the roll.")

        self.assertEqual(window.loc['2016-02-25 22:32', cf_add],
                         125231.412,
                         "Should be FOG16 at beginning of window. A minute "
                         "which is in the 02-25 session, before the roll.")

        # Unadjusted: 115231.44
        # Should use same ratios as above.
        self.assertEqual(window.loc['2016-02-25 23:00', cf_mul],
                         125231.44,
                         "Should be FOG16 on on minute before roll minute, "
                         "adjusted.")

        self.assertEqual(window.loc['2016-02-25 23:00', cf_add],
                         125231.44,
                         "Should be FOG16 on on minute before roll minute, "
                         "adjusted.")

        self.assertEqual(window.loc['2016-02-25 23:01', cf_mul],
                         125240.001,
                         "Should be FOH16 on minute after roll, unadjusted.")

        self.assertEqual(window.loc['2016-02-25 23:01', cf_add],
                         125240.001,
                         "Should be FOH16 on minute after roll, unadjusted.")

        # Advance the window a session.
        window = self.data_portal.get_history_window(
            [cf, cf_mul, cf_add],
            Timestamp('2016-02-28 18:01', tz='US/Eastern').tz_convert('UTC'),
            30, '1m', 'close', 'minute')

        # No adjustments in this window.
        self.assertEqual(window.loc['2016-02-26 22:32', cf_mul],
                         125241.412,
                         "Should be FOH16 at beginning of window.")

        self.assertEqual(window.loc['2016-02-28 23:01', cf_mul],
                         125250.001,
                         "Should remain FOH16 on next session.")

</source>
<source file="systems/zipline-1.3.0/tests/test_continuous_futures.py" startline="1208" endline="1272" pcid="33">
    def test_history_close_minute_adjusted_volume_roll(self):
        cf = self.data_portal.asset_finder.create_continuous_future(
            'FO', 0, 'volume', None)
        cf_mul = self.data_portal.asset_finder.create_continuous_future(
            'FO', 0, 'volume', 'mul')
        cf_add = self.data_portal.asset_finder.create_continuous_future(
            'FO', 0, 'volume', 'add')
        window = self.data_portal.get_history_window(
            [cf, cf_mul, cf_add],
            Timestamp('2016-02-28 18:01', tz='US/Eastern').tz_convert('UTC'),
            30, '1m', 'close', 'minute')

        # Unadjusted: 115241.412
        # Adjustment based on roll:
        # 2016-02-25 23:00:00+00:00
        # front: 115241.440 (FOG16)
        # back:  125241.440 (FOH16)
        # Ratio: ~0.920
        # Difference: 10000.00
        self.assertEqual(window.loc['2016-02-26 22:32', cf_mul],
                         125242.973,
                         "Should be FOG16 at beginning of window. A minute "
                         "which is in the 02-25 session, before the roll.")

        self.assertEqual(window.loc['2016-02-26 22:32', cf_add],
                         125242.851,
                         "Should be FOG16 at beginning of window. A minute "
                         "which is in the 02-25 session, before the roll.")

        # Unadjusted: 115231.44
        # Should use same ratios as above.
        self.assertEqual(window.loc['2016-02-26 23:00', cf_mul],
                         125243.004,
                         "Should be FOG16 on minute before roll minute, "
                         "adjusted.")

        self.assertEqual(window.loc['2016-02-26 23:00', cf_add],
                         125242.879,
                         "Should be FOG16 on minute before roll minute, "
                         "adjusted.")

        self.assertEqual(window.loc['2016-02-28 23:01', cf_mul],
                         125250.001,
                         "Should be FOH16 on minute after roll, unadjusted.")

        self.assertEqual(window.loc['2016-02-28 23:01', cf_add],
                         125250.001,
                         "Should be FOH16 on minute after roll, unadjusted.")

        # Advance the window a session.
        window = self.data_portal.get_history_window(
            [cf, cf_mul, cf_add],
            Timestamp('2016-02-29 18:01', tz='US/Eastern').tz_convert('UTC'),
            30, '1m', 'close', 'minute')

        # No adjustments in this window.
        self.assertEqual(window.loc['2016-02-29 22:32', cf_mul],
                         125251.412,
                         "Should be FOH16 at beginning of window.")

        self.assertEqual(window.loc['2016-02-29 23:01', cf_mul],
                         125260.001,
                         "Should remain FOH16 on next session.")


</source>
</class>

<class classid="9" nclones="4" nlines="15" similarity="86">
<source file="systems/zipline-1.3.0/tests/test_continuous_futures.py" startline="1502" endline="1520" pcid="38">
    def test_volume_roll(self):
        """
        Test normally behaving rolls.
        """
        rolls = self.volume_roll_finder.get_rolls(
            root_symbol='CL',
            start=self.START_DATE + self.trading_calendar.day,
            end=self.second_end_date,
            offset=0,
        )
        self.assertEqual(
            rolls,
            [
                (1000, pd.Timestamp('2017-01-19', tz='UTC')),
                (1001, pd.Timestamp('2017-02-13', tz='UTC')),
                (1002, None),
            ],
        )

</source>
<source file="systems/zipline-1.3.0/tests/test_continuous_futures.py" startline="1555" endline="1572" pcid="41">
    def test_end_before_auto_close(self):
        # Test that we correctly roll from CLJ17 (1003) to CLK17 (1004) even
        # though CLJ17 has an auto close date after its end date.
        rolls = self.volume_roll_finder.get_rolls(
            root_symbol='CL',
            start=self.fourth_start_date,
            end=self.fourth_auto_close_date,
            offset=0,
        )
        self.assertEqual(
            rolls,
            [
                (1002, pd.Timestamp('2017-03-16', tz='UTC')),
                (1003, pd.Timestamp('2017-04-18', tz='UTC')),
                (1004, None),
            ],
        )

</source>
<source file="systems/zipline-1.3.0/tests/test_continuous_futures.py" startline="1573" endline="1593" pcid="42">
    def test_roll_window_ends_on_auto_close(self):
        """
        Test that when skipping over a low volume contract (CLM17), we use the
        correct roll date for the previous contract (CLK17) when that
        contract's auto close date falls on the end date of the roll window.
        """
        rolls = self.volume_roll_finder.get_rolls(
            root_symbol='CL',
            start=self.last_start_date,
            end=self.fifth_auto_close_date,
            offset=0,
        )
        self.assertEqual(
            rolls,
            [
                (1003, pd.Timestamp('2017-04-18', tz='UTC')),
                (1004, pd.Timestamp('2017-05-19', tz='UTC')),
                (1006, None),
            ],
        )

</source>
<source file="systems/zipline-1.3.0/tests/test_continuous_futures.py" startline="1534" endline="1554" pcid="40">
    def test_roll_in_grace_period(self):
        """
        The volume roll finder can look for data up to a week before the given
        date. This test asserts that we not only return the correct active
        contract during that previous week (grace period), but also that we do
        not go into exception if one of the contracts does not exist.
        """
        rolls = self.volume_roll_finder.get_rolls(
            root_symbol='CL',
            start=self.second_end_date,
            end=self.third_end_date,
            offset=0,
        )
        self.assertEqual(
            rolls,
            [
                (1002, pd.Timestamp('2017-03-16', tz='UTC')),
                (1003, None),
            ],
        )

</source>
</class>

<class classid="10" nclones="2" nlines="24" similarity="75">
<source file="systems/zipline-1.3.0/tests/data/test_dispatch_bar_reader.py" startline="54" endline="79" pcid="69">
    def make_future_minute_bar_data(cls):
        m_opens = [
            cls.trading_calendar.open_and_close_for_session(session)[0]
            for session in cls.trading_sessions['us_futures']]
        yield 10001, DataFrame({
            'open': [10000.5, 10001.5, nan],
            'high': [10000.9, 10001.9, nan],
            'low': [10000.1, 10001.1, nan],
            'close': [10000.3, 10001.3, nan],
            'volume': [1000, 1001, 0],
        }, index=m_opens)
        yield 10002, DataFrame({
            'open': [20000.5, nan, 20002.5],
            'high': [20000.9, nan, 20002.9],
            'low': [20000.1, nan, 20002.1],
            'close': [20000.3, nan, 20002.3],
            'volume': [2000, 0, 2002],
        }, index=m_opens)
        yield 10003, DataFrame({
            'open': [nan, 30001.5, 30002.5],
            'high': [nan, 30001.9, 30002.9],
            'low': [nan, 30001.1, 30002.1],
            'close': [nan, 30001.3, 30002.3],
            'volume': [0, 3001, 3002],
        }, index=m_opens)

</source>
<source file="systems/zipline-1.3.0/tests/data/test_dispatch_bar_reader.py" startline="81" endline="104" pcid="70">
    def make_equity_daily_bar_data(cls):
        sessions = cls.trading_sessions['NYSE']
        yield 1, DataFrame({
            'open': [100.5, 101.5, nan],
            'high': [100.9, 101.9, nan],
            'low': [100.1, 101.1, nan],
            'close': [100.3, 101.3, nan],
            'volume': [1000, 1001, 0],
        }, index=sessions)
        yield 2, DataFrame({
            'open': [200.5, nan, 202.5],
            'high': [200.9, nan, 202.9],
            'low': [200.1, nan, 202.1],
            'close': [200.3, nan, 202.3],
            'volume': [2000, 0, 2002],
        }, index=sessions)
        yield 3, DataFrame({
            'open': [301.5, 302.5, nan],
            'high': [301.9, 302.9, nan],
            'low': [301.1, 302.1, nan],
            'close': [301.3, 302.3, nan],
            'volume': [3001, 3002, 0],
        }, index=sessions)

</source>
</class>

<class classid="11" nclones="2" nlines="12" similarity="100">
<source file="systems/zipline-1.3.0/tests/data/test_dispatch_bar_reader.py" startline="106" endline="119" pcid="71">
    def make_futures_info(cls):
        return DataFrame({
            'sid': [10001, 10002, 10003],
            'root_symbol': ['FOO', 'BAR', 'BAZ'],
            'symbol': ['FOOA', 'BARA', 'BAZA'],
            'start_date': [cls.START_DATE] * 3,
            'end_date': [cls.END_DATE] * 3,
            # TODO: Make separate from 'end_date'
            'notice_date': [cls.END_DATE] * 3,
            'expiration_date': [cls.END_DATE] * 3,
            'multiplier': [500] * 3,
            'exchange': ['CME'] * 3,
        })

</source>
<source file="systems/zipline-1.3.0/tests/data/test_dispatch_bar_reader.py" startline="241" endline="254" pcid="76">
    def make_futures_info(cls):
        return DataFrame({
            'sid': [10001, 10002, 10003],
            'root_symbol': ['FOO', 'BAR', 'BAZ'],
            'symbol': ['FOOA', 'BARA', 'BAZA'],
            'start_date': [cls.START_DATE] * 3,
            'end_date': [cls.END_DATE] * 3,
            # TODO: Make separate from 'end_date'
            'notice_date': [cls.END_DATE] * 3,
            'expiration_date': [cls.END_DATE] * 3,
            'multiplier': [500] * 3,
            'exchange': ['CME'] * 3,
        })

</source>
</class>

<class classid="12" nclones="2" nlines="15" similarity="75">
<source file="systems/zipline-1.3.0/tests/data/test_dispatch_bar_reader.py" startline="121" endline="140" pcid="72">
    def init_class_fixtures(cls):
        super(AssetDispatchSessionBarTestCase, cls).init_class_fixtures()

        readers = {
            Equity: ReindexSessionBarReader(
                cls.trading_calendar,
                cls.bcolz_equity_daily_bar_reader,
                cls.START_DATE,
                cls.END_DATE),
            Future: MinuteResampleSessionBarReader(
                cls.trading_calendar,
                cls.bcolz_future_minute_bar_reader,
            )
        }
        cls.dispatch_reader = AssetDispatchSessionBarReader(
            cls.trading_calendar,
            cls.asset_finder,
            readers
        )

</source>
<source file="systems/zipline-1.3.0/tests/data/test_dispatch_bar_reader.py" startline="256" endline="272" pcid="77">
    def init_class_fixtures(cls):
        super(AssetDispatchMinuteBarTestCase, cls).init_class_fixtures()

        readers = {
            Equity: ReindexMinuteBarReader(
                cls.trading_calendar,
                cls.bcolz_equity_minute_bar_reader,
                cls.START_DATE,
                cls.END_DATE),
            Future: cls.bcolz_future_minute_bar_reader
        }
        cls.dispatch_reader = AssetDispatchMinuteBarReader(
            cls.trading_calendar,
            cls.asset_finder,
            readers
        )

</source>
</class>

<class classid="13" nclones="2" nlines="12" similarity="71">
<source file="systems/zipline-1.3.0/tests/utils/test_final.py" startline="15" endline="28" pcid="122">
    def setUpClass(cls):
        class ClassWithFinal(with_metaclass(FinalMeta, object)):
            a = final('ClassWithFinal: a')
            b = 'ClassWithFinal: b'

            @final
            def f(self):
                return 'ClassWithFinal: f'

            def g(self):
                return 'ClassWithFinal: g'

        cls.class_ = ClassWithFinal

</source>
<source file="systems/zipline-1.3.0/tests/utils/test_final.py" startline="161" endline="180" pcid="141">
    def setUpClass(cls):
        FinalABCMeta = compose_types(FinalMeta, ABCMeta)

        class ABCWithFinal(with_metaclass(FinalABCMeta, object)):
            a = final('ABCWithFinal: a')
            b = 'ABCWithFinal: b'

            @final
            def f(self):
                return 'ABCWithFinal: f'

            def g(self):
                return 'ABCWithFinal: g'

            @abstractmethod
            def h(self):
                raise NotImplementedError('h')

        cls.class_ = ABCWithFinal

</source>
</class>

<class classid="14" nclones="2" nlines="48" similarity="81">
<source file="systems/zipline-1.3.0/tests/history/generate_csvs.py" startline="24" endline="88" pcid="236">
def generate_daily_test_data(first_day,
                             last_day,
                             starting_open,
                             starting_volume,
                             multipliers_list,
                             path):

    days = TradingEnvironment.instance().days_in_range(first_day, last_day)

    days_count = len(days)
    o = np.zeros(days_count, dtype=np.uint32)
    h = np.zeros(days_count, dtype=np.uint32)
    l = np.zeros(days_count, dtype=np.uint32)
    c = np.zeros(days_count, dtype=np.uint32)
    v = np.zeros(days_count, dtype=np.uint32)

    last_open = starting_open * 1000
    last_volume = starting_volume

    for idx in range(days_count):
        new_open = last_open + round((random.random() * 5), 2)

        o[idx] = new_open
        h[idx] = new_open + round((random.random() * 10000), 2)
        l[idx] = new_open - round((random.random() * 10000),  2)
        c[idx] = (h[idx] + l[idx]) / 2
        v[idx] = int(last_volume + (random.randrange(-10, 10) * 1e4))

        last_open = o[idx]
        last_volume = v[idx]

    # now deal with multipliers
    if len(multipliers_list) > 0:
        range_start = 0

        for multiplier_info in multipliers_list:
            range_end = days.searchsorted(multiplier_info[0])

            # dividing by the multiplier because we're going backwards
            # and generating the original data that will then be adjusted.
            o[range_start:range_end] /= multiplier_info[1]
            h[range_start:range_end] /= multiplier_info[1]
            l[range_start:range_end] /= multiplier_info[1]
            c[range_start:range_end] /= multiplier_info[1]
            v[range_start:range_end] *= multiplier_info[1]

            range_start = range_end

    df = pd.DataFrame({
        "open": o,
        "high": h,
        "low": l,
        "close": c,
        "volume": v
    }, columns=[
        "open",
        "high",
        "low",
        "close",
        "volume"
    ], index=days)

    df.to_csv(path, index_label="day")


</source>
<source file="systems/zipline-1.3.0/tests/history/generate_csvs.py" startline="89" endline="166" pcid="237">
def generate_minute_test_data(first_day,
                              last_day,
                              starting_open,
                              starting_volume,
                              multipliers_list,
                              path):
    """
    Utility method to generate fake minute-level CSV data.
    :param first_day: first trading day
    :param last_day: last trading day
    :param starting_open: first open value, raw value.
    :param starting_volume: first volume value, raw value.
    :param multipliers_list: ordered list of pd.Timestamp -> float, one per day
            in the range
    :param path: path to save the CSV
    :return: None
    """

    full_minutes = BcolzMinuteBarWriter.full_minutes_for_days(
        first_day, last_day)
    minutes_count = len(full_minutes)

    minutes = TradingEnvironment.instance().minutes_for_days_in_range(
        first_day, last_day)

    o = np.zeros(minutes_count, dtype=np.uint32)
    h = np.zeros(minutes_count, dtype=np.uint32)
    l = np.zeros(minutes_count, dtype=np.uint32)
    c = np.zeros(minutes_count, dtype=np.uint32)
    v = np.zeros(minutes_count, dtype=np.uint32)

    last_open = starting_open * 1000
    last_volume = starting_volume

    for minute in minutes:
        # ugly, but works
        idx = full_minutes.searchsorted(minute)

        new_open = last_open + round((random.random() * 5), 2)

        o[idx] = new_open
        h[idx] = new_open + round((random.random() * 10000), 2)
        l[idx] = new_open - round((random.random() * 10000),  2)
        c[idx] = (h[idx] + l[idx]) / 2
        v[idx] = int(last_volume + (random.randrange(-10, 10) * 1e4))

        last_open = o[idx]
        last_volume = v[idx]

    # now deal with multipliers
    if len(multipliers_list) > 0:
        for idx, multiplier_info in enumerate(multipliers_list):
            start_idx = idx * 390
            end_idx = start_idx + 390

            # dividing by the multipler because we're going backwards
            # and generating the original data that will then be adjusted.
            o[start_idx:end_idx] /= multiplier_info[1]
            h[start_idx:end_idx] /= multiplier_info[1]
            l[start_idx:end_idx] /= multiplier_info[1]
            c[start_idx:end_idx] /= multiplier_info[1]
            v[start_idx:end_idx] *= multiplier_info[1]

    df = pd.DataFrame({
        "open": o,
        "high": h,
        "low": l,
        "close": c,
        "volume": v
    }, columns=[
        "open",
        "high",
        "low",
        "close",
        "volume"
    ], index=minutes)

    df.to_csv(path, index_label="minute")
</source>
</class>

<class classid="15" nclones="2" nlines="21" similarity="72">
<source file="systems/zipline-1.3.0/tests/test_api_shim.py" startline="333" endline="363" pcid="248">
    def test_data_items(self):
        """
        Test that we maintain backwards compat for data.[items | iteritems].

        We also want to assert that we warn that iterating over the assets
        in `data` is deprecated.
        """
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter("ignore", PerformanceWarning)
            warnings.simplefilter("default", ZiplineDeprecationWarning)
            algo = self.create_algo(data_items_algo)
            algo.run()

            self.assertEqual(4, len(w))

            for idx, warning in enumerate(w):
                self.assertEqual(
                    ZiplineDeprecationWarning,
                    warning.category
                )
                if idx % 2 == 0:
                    self.assertEqual(
                        "Iterating over the assets in `data` is deprecated.",
                        str(warning.message)
                    )
                else:
                    self.assertEqual(
                        "`data[sid(N)]` is deprecated. Use `data.current`.",
                        str(warning.message)
                    )

</source>
<source file="systems/zipline-1.3.0/tests/test_api_shim.py" startline="364" endline="394" pcid="249">
    def test_iterate_data(self):
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter("ignore", PerformanceWarning)
            warnings.simplefilter("default", ZiplineDeprecationWarning)

            algo = self.create_algo(simple_algo)
            algo.run()

            self.assertEqual(4, len(w))

            line_nos = [warning.lineno for warning in w]
            self.assertEqual(4, len(set(line_nos)))

            for idx, warning in enumerate(w):
                self.assertEqual(ZiplineDeprecationWarning,
                                 warning.category)

                self.assertEqual("<string>", warning.filename)
                self.assertEqual(line_nos[idx], warning.lineno)

                if idx < 2:
                    self.assertEqual(
                        "Checking whether an asset is in data is deprecated.",
                        str(warning.message)
                    )
                else:
                    self.assertEqual(
                        "Iterating over the assets in `data` is deprecated.",
                        str(warning.message)
                    )

</source>
</class>

<class classid="16" nclones="4" nlines="86" similarity="70">
<source file="systems/zipline-1.3.0/tests/pipeline/test_downsampling.py" startline="93" endline="214" pcid="318">
    def test_yearly(self, base_terms, calendar_name):
        downsampled_terms = tuple(
            t.downsample('year_start') for t in base_terms
        )
        all_terms = base_terms + downsampled_terms

        all_sessions = self.trading_sessions[calendar_name]
        end_session = all_sessions[-1]

        years = all_sessions.year
        sessions_in_2012 = all_sessions[years == 2012]
        sessions_in_2013 = all_sessions[years == 2013]
        sessions_in_2014 = all_sessions[years == 2014]

        # Simulate requesting computation where the unaltered lookback would
        # land exactly on the first date in 2014.  We shouldn't request any
        # additional rows for the regular terms or the downsampled terms.
        for i in range(0, 30, 5):
            start_session = sessions_in_2014[i]
            self.check_extra_row_calculations(
                all_terms,
                all_sessions,
                start_session,
                end_session,
                min_extra_rows=i,
                expected_extra_rows=i,
            )

        # Simulate requesting computation where the unaltered lookback would
        # land on the second date in 2014.  We should request one more extra
        # row in the downsampled terms to push us back to the first date in
        # 2014.
        for i in range(0, 30, 5):
            start_session = sessions_in_2014[i + 1]
            self.check_extra_row_calculations(
                downsampled_terms,
                all_sessions,
                start_session,
                end_session,
                min_extra_rows=i,
                expected_extra_rows=i + 1,
            )
            self.check_extra_row_calculations(
                base_terms,
                all_sessions,
                start_session,
                end_session,
                min_extra_rows=i,
                expected_extra_rows=i,
            )

        # Simulate requesting computation where the unaltered lookback would
        # land on the last date of 2013. The downsampled terms should request
        # enough extra rows to push us back to the start of 2013.
        for i in range(0, 30, 5):
            start_session = sessions_in_2014[i]
            self.check_extra_row_calculations(
                downsampled_terms,
                all_sessions,
                start_session,
                end_session,
                min_extra_rows=i + 1,
                expected_extra_rows=i + len(sessions_in_2013),
            )
            self.check_extra_row_calculations(
                base_terms,
                all_sessions,
                start_session,
                end_session,
                min_extra_rows=i + 1,
                expected_extra_rows=i + 1,
            )

        # Simulate requesting computation where the unaltered lookback would
        # land on the last date of 2012. The downsampled terms should request
        # enough extra rows to push us back to the first known date, which is
        # in the middle of 2012.
        for i in range(0, 30, 5):
            start_session = sessions_in_2013[i]
            self.check_extra_row_calculations(
                downsampled_terms,
                all_sessions,
                start_session,
                end_session,
                min_extra_rows=i + 1,
                expected_extra_rows=i + len(sessions_in_2012),
            )
            self.check_extra_row_calculations(
                base_terms,
                all_sessions,
                start_session,
                end_session,
                min_extra_rows=i + 1,
                expected_extra_rows=i + 1,
            )

        # Simulate requesting computation where the unaltered lookback would
        # land prior to the first date of 2012. The downsampled terms will fail
        # to request enough extra rows.
        for i in range(0, 30, 5):
            with self.assertRaisesRegexp(
                NoFurtherDataError,
                '\s*Insufficient data to compute Pipeline'
            ):
                self.check_extra_row_calculations(
                    downsampled_terms,
                    all_sessions,
                    all_sessions[i],
                    end_session,
                    min_extra_rows=i + 1,
                    expected_extra_rows=i + 1,
                )

            self.check_extra_row_calculations(
                base_terms,
                all_sessions,
                all_sessions[i],
                end_session,
                min_extra_rows=i + 1,
                expected_extra_rows=i + 1,
            )

</source>
<source file="systems/zipline-1.3.0/tests/pipeline/test_downsampling.py" startline="224" endline="321" pcid="319">
    def test_quarterly(self, calendar_name, base_terms):
        downsampled_terms = tuple(
            t.downsample('quarter_start') for t in base_terms
        )
        all_terms = base_terms + downsampled_terms

        # This region intersects with Q4 2013, Q1 2014, and Q2 2014.
        tmp = self.trading_sessions[calendar_name]
        all_sessions = tmp[tmp.slice_indexer('2013-12-15', '2014-04-30')]
        end_session = all_sessions[-1]

        months = all_sessions.month
        Q4_2013 = all_sessions[months == 12]
        Q1_2014 = all_sessions[(months == 1) | (months == 2) | (months == 3)]
        Q2_2014 = all_sessions[months == 4]

        # Simulate requesting computation where the unaltered lookback would
        # land exactly on the first date in Q2 2014.  We shouldn't request any
        # additional rows for the regular terms or the downsampled terms.
        for i in range(0, 15, 5):
            start_session = Q2_2014[i]
            self.check_extra_row_calculations(
                all_terms,
                all_sessions,
                start_session,
                end_session,
                min_extra_rows=i,
                expected_extra_rows=i,
            )

        # Simulate requesting computation where the unaltered lookback would
        # land exactly on the second date in Q2 2014.
        # The downsampled terms should request one more extra row.
        for i in range(0, 15, 5):
            start_session = Q2_2014[i + 1]
            self.check_extra_row_calculations(
                downsampled_terms,
                all_sessions,
                start_session,
                end_session,
                min_extra_rows=i,
                expected_extra_rows=i + 1,
            )
            self.check_extra_row_calculations(
                base_terms,
                all_sessions,
                start_session,
                end_session,
                min_extra_rows=i,
                expected_extra_rows=i,
            )

        # Simulate requesting computation where the unaltered lookback would
        # land exactly on the last date in Q1 2014.  The downsampled terms
        # should request enough extra rows to push us back to the first date of
        # Q1 2014.
        for i in range(0, 15, 5):
            start_session = Q2_2014[i]
            self.check_extra_row_calculations(
                downsampled_terms,
                all_sessions,
                start_session,
                end_session,
                min_extra_rows=i + 1,
                expected_extra_rows=i + len(Q1_2014),
            )
            self.check_extra_row_calculations(
                base_terms,
                all_sessions,
                start_session,
                end_session,
                min_extra_rows=i + 1,
                expected_extra_rows=i + 1,
            )

        # Simulate requesting computation where the unaltered lookback would
        # land exactly on the last date in Q4 2013.  The downsampled terms
        # should request enough extra rows to push us back to the first known
        # date, which is in the middle of december 2013.
        for i in range(0, 15, 5):
            start_session = Q1_2014[i]
            self.check_extra_row_calculations(
                downsampled_terms,
                all_sessions,
                start_session,
                end_session,
                min_extra_rows=i + 1,
                expected_extra_rows=i + len(Q4_2013),
            )
            self.check_extra_row_calculations(
                base_terms,
                all_sessions,
                start_session,
                end_session,
                min_extra_rows=i + 1,
                expected_extra_rows=i + 1,
            )

</source>
<source file="systems/zipline-1.3.0/tests/pipeline/test_downsampling.py" startline="331" endline="428" pcid="320">
    def test_monthly(self, calendar_name, base_terms):
        downsampled_terms = tuple(
            t.downsample('month_start') for t in base_terms
        )
        all_terms = base_terms + downsampled_terms

        # This region intersects with Dec 2013, Jan 2014, and Feb 2014.
        tmp = self.trading_sessions[calendar_name]
        all_sessions = tmp[tmp.slice_indexer('2013-12-15', '2014-02-28')]
        end_session = all_sessions[-1]

        months = all_sessions.month
        dec2013 = all_sessions[months == 12]
        jan2014 = all_sessions[months == 1]
        feb2014 = all_sessions[months == 2]

        # Simulate requesting computation where the unaltered lookback would
        # land exactly on the first date in feb 2014.  We shouldn't request any
        # additional rows for the regular terms or the downsampled terms.
        for i in range(0, 10, 2):
            start_session = feb2014[i]
            self.check_extra_row_calculations(
                all_terms,
                all_sessions,
                start_session,
                end_session,
                min_extra_rows=i,
                expected_extra_rows=i,
            )

        # Simulate requesting computation where the unaltered lookback would
        # land on the second date in feb 2014.  We should request one more
        # extra row in the downsampled terms to push us back to the first date
        # in 2014.
        for i in range(0, 10, 2):
            start_session = feb2014[i + 1]
            self.check_extra_row_calculations(
                downsampled_terms,
                all_sessions,
                start_session,
                end_session,
                min_extra_rows=i,
                expected_extra_rows=i + 1,
            )
            self.check_extra_row_calculations(
                base_terms,
                all_sessions,
                start_session,
                end_session,
                min_extra_rows=i,
                expected_extra_rows=i,
            )

        # Simulate requesting computation where the unaltered lookback would
        # land on the last date of jan 2014. The downsampled terms should
        # request enough extra rows to push us back to the start of jan 2014.
        for i in range(0, 10, 2):
            start_session = feb2014[i]
            self.check_extra_row_calculations(
                downsampled_terms,
                all_sessions,
                start_session,
                end_session,
                min_extra_rows=i + 1,
                expected_extra_rows=i + len(jan2014),
            )
            self.check_extra_row_calculations(
                base_terms,
                all_sessions,
                start_session,
                end_session,
                min_extra_rows=i + 1,
                expected_extra_rows=i + 1,
            )

        # Simulate requesting computation where the unaltered lookback would
        # land on the last date of dec 2013. The downsampled terms should
        # request enough extra rows to push us back to the first known date,
        # which is in the middle of december 2013.
        for i in range(0, 10, 2):
            start_session = jan2014[i]
            self.check_extra_row_calculations(
                downsampled_terms,
                all_sessions,
                start_session,
                end_session,
                min_extra_rows=i + 1,
                expected_extra_rows=i + len(dec2013),
            )
            self.check_extra_row_calculations(
                base_terms,
                all_sessions,
                start_session,
                end_session,
                min_extra_rows=i + 1,
                expected_extra_rows=i + 1,
            )

</source>
<source file="systems/zipline-1.3.0/tests/pipeline/test_downsampling.py" startline="438" endline="558" pcid="321">
    def test_weekly(self, calendar_name, base_terms):
        downsampled_terms = tuple(
            t.downsample('week_start') for t in base_terms
        )
        all_terms = base_terms + downsampled_terms

        #    December 2013
        # Mo Tu We Th Fr Sa Su
        #                    1
        #  2  3  4  5  6  7  8
        #  9 10 11 12 13 14 15
        # 16 17 18 19 20 21 22
        # 23 24 25 26 27 28 29
        # 30 31

        #     January 2014
        # Mo Tu We Th Fr Sa Su
        #        1  2  3  4  5
        #  6  7  8  9 10 11 12
        # 13 14 15 16 17 18 19
        # 20 21 22 23 24 25 26
        # 27 28 29 30 31

        # This region intersects with the last full week of 2013, the week
        # shared by 2013 and 2014, and the first full week of 2014.
        tmp = self.trading_sessions[calendar_name]
        all_sessions = tmp[tmp.slice_indexer('2013-12-27', '2014-01-12')]
        end_session = all_sessions[-1]

        week0 = all_sessions[
            all_sessions.slice_indexer('2013-12-27', '2013-12-29')
        ]
        week1 = all_sessions[
            all_sessions.slice_indexer('2013-12-30', '2014-01-05')
        ]
        week2 = all_sessions[
            all_sessions.slice_indexer('2014-01-06', '2014-01-12')
        ]

        # Simulate requesting computation where the unaltered lookback would
        # land exactly on the first date in week 2.  We shouldn't request any
        # additional rows for the regular terms or the downsampled terms.
        for i in range(3):
            start_session = week2[i]
            self.check_extra_row_calculations(
                all_terms,
                all_sessions,
                start_session,
                end_session,
                min_extra_rows=i,
                expected_extra_rows=i,
            )

        # Simulate requesting computation where the unaltered lookback would
        # land exactly on the second date in week 2.  The downsampled terms
        # should request one more extra row.
        for i in range(3):
            start_session = week2[i + 1]
            self.check_extra_row_calculations(
                downsampled_terms,
                all_sessions,
                start_session,
                end_session,
                min_extra_rows=i,
                expected_extra_rows=i + 1,
            )
            self.check_extra_row_calculations(
                base_terms,
                all_sessions,
                start_session,
                end_session,
                min_extra_rows=i,
                expected_extra_rows=i,
            )

        # Simulate requesting computation where the unaltered lookback would
        # land exactly on the last date in week 1.  The downsampled terms
        # should request enough extra rows to push us back to the first date of
        # week 1.
        for i in range(3):
            start_session = week2[i]
            self.check_extra_row_calculations(
                downsampled_terms,
                all_sessions,
                start_session,
                end_session,
                min_extra_rows=i + 1,
                expected_extra_rows=i + len(week1),
            )
            self.check_extra_row_calculations(
                base_terms,
                all_sessions,
                start_session,
                end_session,
                min_extra_rows=i + 1,
                expected_extra_rows=i + 1,
            )

        # Simulate requesting computation where the unaltered lookback would
        # land exactly on the last date in week0.  The downsampled terms
        # should request enough extra rows to push us back to the first known
        # date, which is in the middle of december 2013.
        for i in range(3):
            start_session = week1[i]
            self.check_extra_row_calculations(
                downsampled_terms,
                all_sessions,
                start_session,
                end_session,
                min_extra_rows=i + 1,
                expected_extra_rows=i + len(week0),
            )
            self.check_extra_row_calculations(
                base_terms,
                all_sessions,
                start_session,
                end_session,
                min_extra_rows=i + 1,
                expected_extra_rows=i + 1,
            )

</source>
</class>

<class classid="17" nclones="3" nlines="14" similarity="78">
<source file="systems/zipline-1.3.0/tests/pipeline/test_adjustment.py" startline="38" endline="52" pcid="363">
    def test_make_int_adjustment(self):
        result = adj.make_adjustment_from_indices(
            1, 2, 3, 4,
            adjustment_kind=adj.OVERWRITE,
            value=1,
        )
        expected = adj.Int64Overwrite(
            first_row=1,
            last_row=2,
            first_col=3,
            last_col=4,
            value=1,
        )
        self.assertEqual(result, expected)

</source>
<source file="systems/zipline-1.3.0/tests/pipeline/test_adjustment.py" startline="53" endline="68" pcid="364">
    def test_make_datetime_adjustment(self):
        overwrite_dt = make_datetime64ns(0)
        result = adj.make_adjustment_from_indices(
            1, 2, 3, 4,
            adjustment_kind=adj.OVERWRITE,
            value=overwrite_dt,
        )
        expected = adj.Datetime64Overwrite(
            first_row=1,
            last_row=2,
            first_col=3,
            last_col=4,
            value=overwrite_dt,
        )
        self.assertEqual(result, expected)

</source>
<source file="systems/zipline-1.3.0/tests/pipeline/test_adjustment.py" startline="70" endline="85" pcid="365">
    def test_make_object_adjustment(self, value):
        result = adj.make_adjustment_from_indices(
            1, 2, 3, 4,
            adjustment_kind=adj.OVERWRITE,
            value=value,
        )

        expected = adj.ObjectOverwrite(
            first_row=1,
            last_row=2,
            first_col=3,
            last_col=4,
            value=value,
        )
        self.assertEqual(result, expected)

</source>
</class>

<class classid="18" nclones="3" nlines="15" similarity="70">
<source file="systems/zipline-1.3.0/tests/test_execution_styles.py" startline="183" endline="203" pcid="380">
    def test_limit_order_prices(self,
                                price,
                                expected_limit_buy_or_stop_sell,
                                expected_limit_sell_or_stop_buy,
                                asset):
        """
        Test price getters for the LimitOrder class.
        """
        style = LimitOrder(
            price,
            asset=self.asset_finder.retrieve_asset(asset)
        )

        assert_equal(expected_limit_buy_or_stop_sell,
                     style.get_limit_price(is_buy=True))
        assert_equal(expected_limit_sell_or_stop_buy,
                     style.get_limit_price(is_buy=False))

        assert_equal(None, style.get_stop_price(_is_buy=True))
        assert_equal(None, style.get_stop_price(_is_buy=False))

</source>
<source file="systems/zipline-1.3.0/tests/test_execution_styles.py" startline="228" endline="253" pcid="382">
    def test_stop_limit_order_prices(self,
                                     price,
                                     expected_limit_buy_or_stop_sell,
                                     expected_limit_sell_or_stop_buy,
                                     asset):
        """
        Test price getters for StopLimitOrder class. Note that the expected
        rounding direction for stop prices is the reverse of that for limit
        prices.
        """

        style = StopLimitOrder(
            price,
            price + 1,
            asset=self.asset_finder.retrieve_asset(asset)
        )

        assert_equal(expected_limit_buy_or_stop_sell,
                     style.get_limit_price(is_buy=True))
        assert_equal(expected_limit_sell_or_stop_buy,
                     style.get_limit_price(is_buy=False))

        assert_equal(expected_limit_buy_or_stop_sell + 1,
                     style.get_stop_price(is_buy=False))
        assert_equal(expected_limit_sell_or_stop_buy + 1,
                     style.get_stop_price(is_buy=True))
</source>
<source file="systems/zipline-1.3.0/tests/test_execution_styles.py" startline="205" endline="226" pcid="381">
    def test_stop_order_prices(self,
                               price,
                               expected_limit_buy_or_stop_sell,
                               expected_limit_sell_or_stop_buy,
                               asset):
        """
        Test price getters for StopOrder class. Note that the expected rounding
        direction for stop prices is the reverse of that for limit prices.
        """
        style = StopOrder(
            price,
            asset=self.asset_finder.retrieve_asset(asset)
        )

        assert_equal(None, style.get_limit_price(_is_buy=False))
        assert_equal(None, style.get_limit_price(_is_buy=True))

        assert_equal(expected_limit_buy_or_stop_sell,
                     style.get_stop_price(is_buy=False))
        assert_equal(expected_limit_sell_or_stop_buy,
                     style.get_stop_price(is_buy=True))

</source>
</class>

<class classid="19" nclones="2" nlines="10" similarity="70">
<source file="systems/zipline-1.3.0/tests/events/test_events_nyse.py" startline="139" endline="156" pcid="385">
                self.assertEqual(m, dt + timedelta(days=trigger_day_offset) +
                                 timedelta(minutes=trigger_minute_offset))
                n_triggered += 1

        self.assertEqual(n_triggered, 1)

    def test_offset_too_far(self):
        minute_groups = minutes_for_days(self.cal, ordered_days=True)

        # Neither rule should ever fire, since they are configured to fire
        # 11+ hours after the open or before the close.  a NYSE session is
        # never longer than 6.5 hours.
        after_open_rule = AfterOpen(hours=11, minutes=11)
        after_open_rule.cal = self.cal

        before_close_rule = BeforeClose(hours=11, minutes=5)
        before_close_rule.cal = self.cal

</source>
<source file="systems/zipline-1.3.0/tests/events/test_events_cme.py" startline="28" endline="40" pcid="386">

    def test_far_after_open(self):
        minute_groups = minutes_for_days(self.cal, ordered_days=True)
        after_open = AfterOpen(hours=9, minutes=25)
        after_open.cal = self.cal

        for session_minutes in minute_groups:
            for i, minute in enumerate(session_minutes):
                if i != 564:
                    self.assertFalse(after_open.should_trigger(minute))
                else:
                    self.assertTrue(after_open.should_trigger(minute))

</source>
</class>

<class classid="20" nclones="2" nlines="27" similarity="100">
<source file="systems/zipline-1.3.0/tests/test_testing.py" startline="64" endline="93" pcid="392">
    def test_make_alternating_boolean_array(self):
        check_arrays(
            make_alternating_boolean_array((3, 3)),
            array(
                [[True,  False,  True],
                 [False,  True, False],
                 [True,  False,  True]]
            ),
        )
        check_arrays(
            make_alternating_boolean_array((3, 3), first_value=False),
            array(
                [[False,  True, False],
                 [True,  False,  True],
                 [False,  True, False]]
            ),
        )
        check_arrays(
            make_alternating_boolean_array((1, 3)),
            array([[True, False, True]]),
        )
        check_arrays(
            make_alternating_boolean_array((3, 1)),
            array([[True], [False], [True]]),
        )
        check_arrays(
            make_alternating_boolean_array((3, 0)),
            empty((3, 0), dtype=bool_dtype),
        )

</source>
<source file="systems/zipline-1.3.0/tests/test_testing.py" startline="94" endline="124" pcid="393">
    def test_make_cascading_boolean_array(self):
        check_arrays(
            make_cascading_boolean_array((3, 3)),
            array(
                [[True,   True, False],
                 [True,  False, False],
                 [False, False, False]]
            ),
        )
        check_arrays(
            make_cascading_boolean_array((3, 3), first_value=False),
            array(
                [[False, False, True],
                 [False,  True, True],
                 [True,   True, True]]
            ),
        )
        check_arrays(
            make_cascading_boolean_array((1, 3)),
            array([[True, True, False]]),
        )
        check_arrays(
            make_cascading_boolean_array((3, 1)),
            array([[False], [False], [False]]),
        )
        check_arrays(
            make_cascading_boolean_array((3, 0)),
            empty((3, 0), dtype=bool_dtype),
        )


</source>
</class>

<class classid="21" nclones="3" nlines="14" similarity="80">
<source file="systems/zipline-1.3.0/tests/test_security_list.py" startline="214" endline="236" pcid="421">
            algo.run()

        self.check_algo_exception(algo, ctx, 0)

    def test_algo_with_rl_violation_cumulative(self):
        """
        Add a new restriction, run a test long after both
        knowledge dates, make sure stock from original restriction
        set is still disallowed.
        """
        sim_params = factory.create_simulation_parameters(
            start=self.START_DATE + timedelta(days=7),
            num_days=4
        )

        with security_list_copy():
            add_security_data(['AAPL'], [])
            algo = self.make_algo(
                algo_class=RestrictedAlgoWithoutCheck,
                symbol='BZQ',
                sim_params=sim_params,
            )
            with self.assertRaises(TradingControlViolation) as ctx:
</source>
<source file="systems/zipline-1.3.0/tests/test_security_list.py" startline="255" endline="272" pcid="423">
                sim_params=sim_params,
            )
            algo.run()

    def test_algo_with_rl_violation_after_add(self):
        sim_params = factory.create_simulation_parameters(
            start=self.trading_day_before_first_kd,
            num_days=4,
        )
        with security_list_copy():
            add_security_data(['AAPL'], [])

            algo = self.make_algo(
                algo_class=RestrictedAlgoWithoutCheck,
                symbol='AAPL',
                sim_params=sim_params,
            )
            with self.assertRaises(TradingControlViolation) as ctx:
</source>
<source file="systems/zipline-1.3.0/tests/test_security_list.py" startline="237" endline="254" pcid="422">
                algo.run()

            self.check_algo_exception(algo, ctx, 0)

    def test_algo_without_rl_violation_after_delete(self):
        sim_params = factory.create_simulation_parameters(
            start=self.extra_knowledge_date,
            num_days=4,
        )

        with security_list_copy():
            # add a delete statement removing bzq
            # write a new delete statement file to disk
            add_security_data([], ['BZQ'])

            algo = self.make_algo(
                algo_class=RestrictedAlgoWithoutCheck,
                symbol='BZQ',
</source>
</class>

<class classid="22" nclones="2" nlines="23" similarity="82">
<source file="systems/zipline-1.3.0/tests/metrics/test_core.py" startline="39" endline="71" pcid="460">
    def test_register_decorator(self):
        ayy_lmao_set = set()

        @self.register('ayy-lmao')
        def ayy_lmao():
            return ayy_lmao_set

        expected_metrics_sets = mappingproxy({'ayy-lmao': ayy_lmao})
        assert_equal(self.metrics_sets, expected_metrics_sets)
        assert_is(self.load('ayy-lmao'), ayy_lmao_set)

        msg = "metrics set 'ayy-lmao' is already registered"
        with assert_raises_str(ValueError, msg):
            @self.register('ayy-lmao')
            def other():  # pragma: no cover
                raise AssertionError('dead')

        # ensure that the failed registration didn't break the previously
        # registered set
        assert_equal(self.metrics_sets, expected_metrics_sets)
        assert_is(self.load('ayy-lmao'), ayy_lmao_set)

        self.unregister('ayy-lmao')
        assert_equal(self.metrics_sets, mappingproxy({}))

        msg = "no metrics set registered as 'ayy-lmao', options are: []"
        with assert_raises_str(ValueError, msg):
            self.load('ayy-lmao')

        msg = "metrics set 'ayy-lmao' was not already registered"
        with assert_raises_str(ValueError, msg):
            self.unregister('ayy-lmao')

</source>
<source file="systems/zipline-1.3.0/tests/metrics/test_core.py" startline="72" endline="105" pcid="463">
    def test_register_non_decorator(self):
        ayy_lmao_set = set()

        def ayy_lmao():
            return ayy_lmao_set

        self.register('ayy-lmao', ayy_lmao)

        expected_metrics_sets = mappingproxy({'ayy-lmao': ayy_lmao})
        assert_equal(self.metrics_sets, expected_metrics_sets)
        assert_is(self.load('ayy-lmao'), ayy_lmao_set)

        def other():  # pragma: no cover
            raise AssertionError('dead')

        msg = "metrics set 'ayy-lmao' is already registered"
        with assert_raises_str(ValueError, msg):
            self.register('ayy-lmao', other)

        # ensure that the failed registration didn't break the previously
        # registered set
        assert_equal(self.metrics_sets, expected_metrics_sets)
        assert_is(self.load('ayy-lmao'), ayy_lmao_set)

        self.unregister('ayy-lmao')
        assert_equal(self.metrics_sets, mappingproxy({}))

        msg = "no metrics set registered as 'ayy-lmao', options are: []"
        with assert_raises_str(ValueError, msg):
            self.load('ayy-lmao')

        msg = "metrics set 'ayy-lmao' was not already registered"
        with assert_raises_str(ValueError, msg):
            self.unregister('ayy-lmao')
</source>
</class>

<class classid="23" nclones="2" nlines="11" similarity="100">
<source file="systems/zipline-1.3.0/tests/test_ordering.py" startline="96" endline="128" pcid="470">
    def test_order_equity_non_targeted(self, order_method, amount):
        # Every day, place an order for $10000 worth of sid(1)
        algotext = """
import zipline.api as api

def initialize(context):
    api.set_slippage(api.slippage.FixedSlippage(spread=0.0))
    api.set_commission(api.commission.PerShare(0))

    context.equity = api.sid(1)

    api.schedule_function(
        func=do_order,
        date_rule=api.date_rules.every_day(),
        time_rule=api.time_rules.market_open(),
    )

def do_order(context, data):
    context.ordered = True
    api.{order_func}(context.equity, {arg})
     """.format(order_func=order_method, arg=amount)
        result = self.run_algorithm(script=algotext)

        for orders in result.orders.values:
            assert_equal(len(orders), 1)
            assert_equal(orders[0]['amount'], 5000)
            assert_equal(orders[0]['sid'], self.EQUITY)

        for i, positions in enumerate(result.positions.values, start=1):
            assert_equal(len(positions), 1)
            assert_equal(positions[0]['amount'], 5000.0 * i)
            assert_equal(positions[0]['sid'], self.EQUITY)

</source>
<source file="systems/zipline-1.3.0/tests/test_ordering.py" startline="177" endline="209" pcid="472">
    def test_order_future_non_targeted(self, order_method, amount):
        # Every day, place an order for $10000 worth of sid(2)
        algotext = """
import zipline.api as api

def initialize(context):
    api.set_slippage(us_futures=api.slippage.FixedSlippage(spread=0.0))
    api.set_commission(us_futures=api.commission.PerTrade(0.0))

    context.future = api.sid(2)

    api.schedule_function(
        func=do_order,
        date_rule=api.date_rules.every_day(),
        time_rule=api.time_rules.market_open(),
    )

def do_order(context, data):
    context.ordered = True
    api.{order_func}(context.future, {arg})
     """.format(order_func=order_method, arg=amount)
        result = self.run_algorithm(script=algotext)

        for orders in result.orders.values:
            assert_equal(len(orders), 1)
            assert_equal(orders[0]['amount'], 500)
            assert_equal(orders[0]['sid'], self.FUTURE)

        for i, positions in enumerate(result.positions.values, start=1):
            assert_equal(len(positions), 1)
            assert_equal(positions[0]['amount'], 500.0 * i)
            assert_equal(positions[0]['sid'], self.FUTURE)

</source>
</class>

<class classid="24" nclones="2" nlines="11" similarity="100">
<source file="systems/zipline-1.3.0/tests/test_ordering.py" startline="135" endline="169" pcid="471">
    def test_order_equity_targeted(self, order_method, amount):
        # Every day, place an order for a target of $10000 worth of sid(1).
        # With no commissions or slippage, we should only place one order.
        algotext = """
import zipline.api as api

def initialize(context):
    api.set_slippage(api.slippage.FixedSlippage(spread=0.0))
    api.set_commission(api.commission.PerShare(0))

    context.equity = api.sid(1)

    api.schedule_function(
        func=do_order,
        date_rule=api.date_rules.every_day(),
        time_rule=api.time_rules.market_open(),
    )

def do_order(context, data):
    context.ordered = True
    api.{order_func}(context.equity, {arg})
     """.format(order_func=order_method, arg=amount)

        result = self.run_algorithm(script=algotext)

        assert_equal([len(ords) for ords in result.orders], [1, 0, 0, 0])
        order = result.orders.iloc[0][0]
        assert_equal(order['amount'], 5000)
        assert_equal(order['sid'], self.EQUITY)

        for positions in result.positions.values:
            assert_equal(len(positions), 1)
            assert_equal(positions[0]['amount'], 5000.0)
            assert_equal(positions[0]['sid'], self.EQUITY)

</source>
<source file="systems/zipline-1.3.0/tests/test_ordering.py" startline="217" endline="253" pcid="473">
    def test_order_future_targeted(self, order_method, amount):
        # Every day, place an order for a target of $10000 worth of sid(2).
        # With no commissions or slippage, we should only place one order.
        algotext = """
import zipline.api as api

def initialize(context):
    api.set_slippage(us_futures=api.slippage.FixedSlippage(spread=0.0))
    api.set_commission(us_futures=api.commission.PerTrade(0.0))

    context.future = api.sid(2)

    api.schedule_function(
        func=do_order,
        date_rule=api.date_rules.every_day(),
        time_rule=api.time_rules.market_open(),
    )

def do_order(context, data):
    context.ordered = True
    api.{order_func}(context.future, {arg})
     """.format(order_func=order_method, arg=amount)

        result = self.run_algorithm(script=algotext)

        # We should get one order on the first day.
        assert_equal([len(ords) for ords in result.orders], [1, 0, 0, 0])
        order = result.orders.iloc[0][0]
        assert_equal(order['amount'], 500)
        assert_equal(order['sid'], self.FUTURE)

        # Our position at the end of each day should be worth $10,000.
        for positions in result.positions.values:
            assert_equal(len(positions), 1)
            assert_equal(positions[0]['amount'], 500.0)
            assert_equal(positions[0]['sid'], self.FUTURE)

</source>
</class>

<class classid="25" nclones="4" nlines="11" similarity="75">
<source file="systems/zipline-1.3.0/tests/test_fetcher.py" startline="178" endline="201" pcid="487">
    def test_fetch_csv_with_multi_symbols(self):
        self.responses.add(
            self.responses.GET,
            'https://fake.urls.com/multi_signal_csv_data.csv',
            body=MULTI_SIGNAL_CSV_DATA,
            content_type='text/csv',
        )

        results = self.run_algo(
            """
from zipline.api import fetch_csv, record, sid

def initialize(context):
    fetch_csv('https://fake.urls.com/multi_signal_csv_data.csv')
    context.stocks = [sid(3766), sid(25317)]

def handle_data(context, data):
    record(ibm_signal=data.current(sid(3766), "signal"))
    record(dell_signal=data.current(sid(25317), "signal"))
    """)

        self.assertEqual(5, results["ibm_signal"].iloc[-1])
        self.assertEqual(5, results["dell_signal"].iloc[-1])

</source>
<source file="systems/zipline-1.3.0/tests/test_fetcher.py" startline="202" endline="233" pcid="488">
    def test_fetch_csv_with_pure_signal_file(self):
        self.responses.add(
            self.responses.GET,
            'https://fake.urls.com/cpiaucsl_data.csv',
            body=CPIAUCSL_DATA,
            content_type='text/csv',
        )

        results = self.run_algo(
            """
from zipline.api import fetch_csv, sid, record

def clean(df):
    return df.rename(columns={'Value':'cpi', 'Date':'date'})

def initialize(context):
    fetch_csv(
        'https://fake.urls.com/cpiaucsl_data.csv',
        symbol='urban',
        pre_func=clean,
        date_format='%Y-%m-%d'
        )
    context.stocks = [sid(3766), sid(25317)]

def handle_data(context, data):

    cur_cpi = data.current("urban", "cpi")
    record(cpi=cur_cpi)
            """)

        self.assertEqual(results["cpi"][-1], 203.1)

</source>
<source file="systems/zipline-1.3.0/tests/test_fetcher.py" startline="266" endline="298" pcid="490">
    def test_algo_fetch_csv_with_extra_symbols(self):
        self.responses.add(
            self.responses.GET,
            'https://fake.urls.com/aapl_ibm_csv_data.csv',
            body=AAPL_IBM_CSV_DATA,
            content_type='text/csv',
        )

        results = self.run_algo(
            """
from zipline.api import fetch_csv, record, sid

def normalize(df):
    df['scaled'] = df['signal'] * 10
    return df

def initialize(context):
    fetch_csv('https://fake.urls.com/aapl_ibm_csv_data.csv',
            post_func=normalize,
            mask=True)

def handle_data(context, data):
    record(
        signal=data.current(sid(24),"signal"),
        scaled=data.current(sid(24), "scaled"),
        price=data.current(sid(24), "price"))
            """
        )

        self.assertEqual(5, results["signal"][-1])
        self.assertEqual(50, results["scaled"][-1])
        self.assertEqual(24, results["price"][-1])  # fake value

</source>
<source file="systems/zipline-1.3.0/tests/test_fetcher.py" startline="234" endline="265" pcid="489">
    def test_algo_fetch_csv(self):
        self.responses.add(
            self.responses.GET,
            'https://fake.urls.com/aapl_csv_data.csv',
            body=AAPL_CSV_DATA,
            content_type='text/csv',
        )

        results = self.run_algo(
            """
from zipline.api import fetch_csv, record, sid

def normalize(df):
    df['scaled'] = df['signal'] * 10
    return df

def initialize(context):
    fetch_csv('https://fake.urls.com/aapl_csv_data.csv',
            post_func=normalize)
    context.checked_name = False

def handle_data(context, data):
    record(
        signal=data.current(sid(24), "signal"),
        scaled=data.current(sid(24), "scaled"),
        price=data.current(sid(24), "price"))
        """)

        self.assertEqual(5, results["signal"][-1])
        self.assertEqual(50, results["scaled"][-1])
        self.assertEqual(24, results["price"][-1])  # fake value

</source>
</class>

<class classid="26" nclones="2" nlines="15" similarity="70">
<source file="systems/zipline-1.3.0/tests/test_fetcher.py" startline="497" endline="545" pcid="497">
    def test_fetcher_universe_minute(self):
        self.responses.add(
            self.responses.GET,
            'https://fake.urls.com/fetcher_universe_data.csv',
            body=FETCHER_UNIVERSE_DATA,
            content_type='text/csv',
        )

        sim_params = factory.create_simulation_parameters(
            start=pd.Timestamp("2006-01-09", tz='UTC'),
            end=pd.Timestamp("2006-01-11", tz='UTC'),
            data_frequency="minute"
        )

        results = self.run_algo(
            """
from pandas import Timestamp
from zipline.api import fetch_csv, record, get_datetime

def initialize(context):
    fetch_csv(
        'https://fake.urls.com/fetcher_universe_data.csv',
        date_format='%m/%d/%Y'
    )
    context.expected_sids = {
        Timestamp('2006-01-09 00:00:00+0000', tz='UTC'):[24, 3766, 5061],
        Timestamp('2006-01-10 00:00:00+0000', tz='UTC'):[24, 3766, 5061],
        Timestamp('2006-01-11 00:00:00+0000', tz='UTC'):[24, 3766, 5061, 14848]
    }
    context.bar_count = 0

def handle_data(context, data):
    expected = context.expected_sids[get_datetime().replace(hour=0, minute=0)]
    actual = data.fetcher_assets
    for stk in expected:
        if stk not in actual:
            raise Exception("{stk} is missing".format(stk=stk))

    record(sid_count=len(actual))
    record(bar_count=context.bar_count)
    context.bar_count += 1
        """, sim_params=sim_params, data_frequency="minute"
        )

        self.assertEqual(3, len(results))
        self.assertEqual(3, results["sid_count"].iloc[0])
        self.assertEqual(3, results["sid_count"].iloc[1])
        self.assertEqual(4, results["sid_count"].iloc[2])

</source>
<source file="systems/zipline-1.3.0/tests/test_fetcher.py" startline="581" endline="612" pcid="499">
    def test_fetcher_bad_data(self):
        self.responses.add(
            self.responses.GET,
            'https://fake.urls.com/fetcher_nflx_data.csv',
            body=NFLX_DATA,
            content_type='text/csv',
        )

        sim_params = factory.create_simulation_parameters(
            start=pd.Timestamp("2013-06-12", tz='UTC'),
            end=pd.Timestamp("2013-06-14", tz='UTC'),
            data_frequency="minute"
        )

        results = self.run_algo("""
from zipline.api import fetch_csv, symbol
import numpy as np

def initialize(context):
    fetch_csv('https://fake.urls.com/fetcher_nflx_data.csv',
               date_column = 'Settlement Date',
               date_format = '%m/%d/%y')
    context.nflx = symbol('NFLX')
    context.aapl = symbol('AAPL')

def handle_data(context, data):
    assert np.isnan(data.current(context.nflx, 'invalid_column'))
    assert np.isnan(data.current(context.aapl, 'invalid_column'))
    assert np.isnan(data.current(context.aapl, 'dtc'))
""", sim_params=sim_params, data_frequency="minute")

        self.assertEqual(3, len(results))
</source>
</class>

<class classid="27" nclones="2" nlines="23" similarity="78">
<source file="systems/zipline-1.3.0/tests/finance/test_slippage.py" startline="101" endline="141" pcid="503">
    def test_allowed_asset_types(self):
        # Custom equities model.
        class MyEquitiesModel(EquitySlippageModel):
            def process_order(self, data, order):
                return 0, 0

        self.assertEqual(MyEquitiesModel.allowed_asset_types, (Equity,))

        # Custom futures model.
        class MyFuturesModel(FutureSlippageModel):
            def process_order(self, data, order):
                return 0, 0

        self.assertEqual(MyFuturesModel.allowed_asset_types, (Future,))

        # Custom model for both equities and futures.
        class MyMixedModel(EquitySlippageModel, FutureSlippageModel):
            def process_order(self, data, order):
                return 0, 0

        self.assertEqual(MyMixedModel.allowed_asset_types, (Equity, Future))

        # Equivalent custom model for both equities and futures.
        class MyMixedModel(SlippageModel):
            def process_order(self, data, order):
                return 0, 0

        self.assertEqual(MyMixedModel.allowed_asset_types, (Equity, Future))

        SomeType = type('SomeType', (object,), {})

        # A custom model that defines its own allowed types should take
        # precedence over the parent class definitions.
        class MyCustomModel(EquitySlippageModel, FutureSlippageModel):
            allowed_asset_types = (SomeType,)

            def process_order(self, data, order):
                return 0, 0

        self.assertEqual(MyCustomModel.allowed_asset_types, (SomeType,))

</source>
<source file="systems/zipline-1.3.0/tests/finance/test_commissions.py" startline="79" endline="119" pcid="542">
    def test_allowed_asset_types(self):
        # Custom equities model.
        class MyEquitiesModel(EquityCommissionModel):
            def calculate(self, order, transaction):
                return 0

        self.assertEqual(MyEquitiesModel.allowed_asset_types, (Equity,))

        # Custom futures model.
        class MyFuturesModel(FutureCommissionModel):
            def calculate(self, order, transaction):
                return 0

        self.assertEqual(MyFuturesModel.allowed_asset_types, (Future,))

        # Custom model for both equities and futures.
        class MyMixedModel(EquityCommissionModel, FutureCommissionModel):
            def calculate(self, order, transaction):
                return 0

        self.assertEqual(MyMixedModel.allowed_asset_types, (Equity, Future))

        # Equivalent custom model for both equities and futures.
        class MyMixedModel(CommissionModel):
            def calculate(self, order, transaction):
                return 0

        self.assertEqual(MyMixedModel.allowed_asset_types, (Equity, Future))

        SomeType = type('SomeType', (object,), {})

        # A custom model that defines its own allowed types should take
        # precedence over the parent class definitions.
        class MyCustomModel(EquityCommissionModel, FutureCommissionModel):
            allowed_asset_types = (SomeType,)

            def calculate(self, order, transaction):
                return 0

        self.assertEqual(MyCustomModel.allowed_asset_types, (SomeType,))

</source>
</class>

<class classid="28" nclones="3" nlines="11" similarity="90">
<source file="systems/zipline-1.3.0/tests/finance/test_slippage.py" startline="591" endline="602" pcid="513">
    def make_equity_minute_bar_data(cls):
        yield 133, pd.DataFrame(
            {
                'open': [3.00],
                'high': [3.15],
                'low': [2.85],
                'close': [3.00],
                'volume': [200],
            },
            index=[cls.minutes[0]],
        )

</source>
<source file="systems/zipline-1.3.0/tests/finance/test_slippage.py" startline="1160" endline="1171" pcid="532">
    @classmethod
    def make_equity_minute_bar_data(cls):
        yield 133, pd.DataFrame(
            {
                'open': [2.9],
                'high': [3.15],
                'low': [2.85],
                'close': [3.00],
                'volume': [200],
            },
            index=[cls.first_minute],
        )
</source>
<source file="systems/zipline-1.3.0/tests/finance/test_slippage.py" startline="616" endline="627" pcid="515">
    def make_future_minute_bar_data(cls):
        yield 1000, pd.DataFrame(
            {
                'open': [5.00],
                'high': [5.15],
                'low': [4.85],
                'close': [5.00],
                'volume': [100],
            },
            index=[cls.minutes[0]],
        )

</source>
</class>

<class classid="29" nclones="4" nlines="11" similarity="71">
<source file="systems/zipline-1.3.0/tests/finance/test_slippage.py" startline="756" endline="766" pcid="520">
    def make_futures_info(cls):
        return pd.DataFrame({
            'sid': [1000, 1001],
            'root_symbol': ['CL', 'FV'],
            'symbol': ['CLF07', 'FVF07'],
            'start_date': [cls.ASSET_START_DATE, cls.START_DATE],
            'end_date': [cls.END_DATE, cls.END_DATE],
            'multiplier': [500, 500],
            'exchange': ['CME', 'CME'],
        })

</source>
<source file="systems/zipline-1.3.0/tests/test_data_portal.py" startline="67" endline="82" pcid="570">
    def make_futures_info(cls):
        trading_sessions = cls.trading_sessions['us_futures']
        return pd.DataFrame({
            'sid': [10000, 10001],
            'root_symbol': ['BAR', 'BUZ'],
            'symbol': ['BARA', 'BUZZ'],
            'start_date': [trading_sessions[1], trading_sessions[0]],
            'end_date': [cls.END_DATE, cls.END_DATE],
            # TODO: Make separate from 'end_date'
            'notice_date': [cls.END_DATE, cls.END_DATE],
            'expiration_date': [cls.END_DATE, cls.END_DATE],
            'tick_size': [0.01, 0.0001],
            'multiplier': [500, 50000],
            'exchange': ['CME', 'CME'],
        })

</source>
<source file="systems/zipline-1.3.0/tests/finance/test_commissions.py" startline="28" endline="40" pcid="539">
    def make_futures_info(cls):
        return DataFrame({
            'sid': [1000, 1001],
            'root_symbol': ['CL', 'FV'],
            'symbol': ['CLF07', 'FVF07'],
            'start_date': [cls.START_DATE, cls.START_DATE],
            'end_date': [cls.END_DATE, cls.END_DATE],
            'notice_date': [cls.END_DATE, cls.END_DATE],
            'expiration_date': [cls.END_DATE, cls.END_DATE],
            'multiplier': [500, 500],
            'exchange': ['CME', 'CME'],
        })

</source>
<source file="systems/zipline-1.3.0/tests/finance/test_commissions.py" startline="329" endline="341" pcid="556">
    @classmethod
    def make_futures_info(cls):
        return DataFrame({
            'sid': [1000, 1001],
            'root_symbol': ['CL', 'FV'],
            'symbol': ['CLF07', 'FVF07'],
            'start_date': [cls.START_DATE, cls.START_DATE],
            'end_date': [cls.END_DATE, cls.END_DATE],
            'notice_date': [cls.END_DATE, cls.END_DATE],
            'expiration_date': [cls.END_DATE, cls.END_DATE],
            'multiplier': [500, 500],
            'exchange': ['CME', 'CME'],
        })
</source>
</class>

<class classid="30" nclones="2" nlines="12" similarity="75">
<source file="systems/zipline-1.3.0/tests/finance/test_slippage.py" startline="779" endline="792" pcid="522">
    def test_calculate_impact_buy(self):
        answer_key = [
            # We ordered 10 contracts, but are capped at 100 * 0.05 = 5
            (91485.500085168125, 5),
            (91486.500085169057, 5),
            (None, None),
        ]
        order = Order(
            dt=pd.Timestamp.now(tz='utc').round('min'),
            asset=self.ASSET,
            amount=10,
        )
        self._calculate_impact(order, answer_key)

</source>
<source file="systems/zipline-1.3.0/tests/finance/test_slippage.py" startline="793" endline="806" pcid="523">
    def test_calculate_impact_sell(self):
        answer_key = [
            # We ordered -10 contracts, but are capped at -(100 * 0.05) = -5
            (91485.499914831875, -5),
            (91486.499914830943, -5),
            (None, None),
        ]
        order = Order(
            dt=pd.Timestamp.now(tz='utc').round('min'),
            asset=self.ASSET,
            amount=-10,
        )
        self._calculate_impact(order, answer_key)

</source>
</class>

<class classid="31" nclones="2" nlines="15" similarity="73">
<source file="systems/zipline-1.3.0/tests/finance/test_commissions.py" startline="59" endline="78" pcid="541">
    def verify_per_trade_commissions(self,
                                     model,
                                     expected_commission,
                                     sid,
                                     order_amount=None,
                                     fill_amounts=None):
        fill_amounts = fill_amounts or [230, 170, 100]
        order_amount = order_amount or sum(fill_amounts)

        order, txns = self.generate_order_and_txns(
            sid, order_amount, fill_amounts,
        )

        self.assertEqual(expected_commission, model.calculate(order, txns[0]))

        order.commission = expected_commission

        self.assertEqual(0, model.calculate(order, txns[1]))
        self.assertEqual(0, model.calculate(order, txns[2]))

</source>
<source file="systems/zipline-1.3.0/tests/finance/test_commissions.py" startline="178" endline="195" pcid="551">

    def verify_per_unit_commissions(self,
                                    model,
                                    commission_totals,
                                    sid,
                                    order_amount=None,
                                    fill_amounts=None):
        fill_amounts = fill_amounts or [230, 170, 100]
        order_amount = order_amount or sum(fill_amounts)

        order, txns = self.generate_order_and_txns(
            sid, order_amount, fill_amounts,
        )

        for i, commission_total in enumerate(commission_totals):
            order.commission += model.calculate(order, txns[i])
            self.assertAlmostEqual(commission_total, order.commission)
            order.filled += txns[i].amount
</source>
</class>

<class classid="32" nclones="2" nlines="21" similarity="71">
<source file="systems/zipline-1.3.0/tests/finance/test_commissions.py" startline="223" endline="251" pcid="553">

    def test_per_share_with_minimum(self):
        # minimum is met by the first trade
        self.verify_per_unit_commissions(
            PerShare(cost=0.0075, min_trade_cost=1),
            commission_totals=[1.725, 3, 3.75],
            sid=1,
        )

        # minimum is met by the second trade
        self.verify_per_unit_commissions(
            PerShare(cost=0.0075, min_trade_cost=2.5),
            commission_totals=[2.5, 3, 3.75],
            sid=1,
        )

        # minimum is met by the third trade
        self.verify_per_unit_commissions(
            PerShare(cost=0.0075, min_trade_cost=3.5),
            commission_totals=[3.5, 3.5, 3.75],
            sid=1,
        )

        # minimum is not met by any of the trades
        self.verify_per_unit_commissions(
            PerShare(cost=0.0075, min_trade_cost=5.5),
            commission_totals=[5.5, 5.5, 5.5],
            sid=1,
        )
</source>
<source file="systems/zipline-1.3.0/tests/finance/test_commissions.py" startline="252" endline="280" pcid="554">

    def test_per_contract_with_minimum(self):
        # Minimum is met by the first trade.
        self.verify_per_unit_commissions(
            PerContract(cost=.01, exchange_fee=0.3, min_trade_cost=1),
            commission_totals=[2.6, 4.3, 5.3],
            sid=1000,
        )

        # Minimum is met by the second trade.
        self.verify_per_unit_commissions(
            PerContract(cost=.01, exchange_fee=0.3, min_trade_cost=3),
            commission_totals=[3.0, 4.3, 5.3],
            sid=1000,
        )

        # Minimum is met by the third trade.
        self.verify_per_unit_commissions(
            PerContract(cost=.01, exchange_fee=0.3, min_trade_cost=5),
            commission_totals=[5.0, 5.0, 5.3],
            sid=1000,
        )

        # Minimum is not met by any of the trades.
        self.verify_per_unit_commissions(
            PerContract(cost=.01, exchange_fee=0.3, min_trade_cost=7),
            commission_totals=[7.0, 7.0, 7.0],
            sid=1000,
        )
</source>
</class>

<class classid="33" nclones="3" nlines="10" similarity="80">
<source file="systems/zipline-1.3.0/tests/finance/test_commissions.py" startline="362" endline="378" pcid="559">

    def test_per_trade(self):
        results = self.get_results(
            self.code.format(
                commission="set_commission(commission.PerTrade(1))",
                sid=133,
                amount=300,
            )
        )

        # should be 3 fills at 100 shares apiece
        # one order split among 3 days, each copy of the order should have a
        # commission of one dollar
        for orders in results.orders[1:4]:
            self.assertEqual(1, orders[0]["commission"])

        self.verify_capital_used(results, [-1001, -1000, -1000])
</source>
<source file="systems/zipline-1.3.0/tests/finance/test_commissions.py" startline="396" endline="412" pcid="561">

    def test_per_share_no_minimum(self):
        results = self.get_results(
            self.code.format(
                commission="set_commission(commission.PerShare(0.05, None))",
                sid=133,
                amount=300,
            )
        )

        # should be 3 fills at 100 shares apiece
        # one order split among 3 days, each fill generates an additional
        # 100 * 0.05 = $5 in commission
        for i, orders in enumerate(results.orders[1:4]):
            self.assertEqual((i + 1) * 5, orders[0]["commission"])

        self.verify_capital_used(results, [-1005, -1005, -1005])
</source>
<source file="systems/zipline-1.3.0/tests/finance/test_commissions.py" startline="504" endline="521" pcid="564">

    def test_per_dollar(self):
        results = self.get_results(
            self.code.format(
                commission="set_commission(commission.PerDollar(0.01))",
                sid=133,
                amount=300,
            )
        )

        # should be 3 fills at 100 shares apiece, each fill is worth $1k, so
        # incremental commission of $1000 * 0.01 = $10

        # commissions should be $10, $20, $30
        for i, orders in enumerate(results.orders[1:4]):
            self.assertEqual((i + 1) * 10, orders[0]["commission"])

        self.verify_capital_used(results, [-1010, -1010, -1010])
</source>
</class>

<class classid="34" nclones="2" nlines="57" similarity="87">
<source file="systems/zipline-1.3.0/tests/test_data_portal.py" startline="84" endline="145" pcid="571">
    def make_equity_minute_bar_data(cls):
        trading_calendar = cls.trading_calendars[Equity]
        # No data on first day.
        dts = trading_calendar.minutes_for_session(cls.trading_days[0])
        dfs = []
        dfs.append(pd.DataFrame(
            {
                'open': full(len(dts), nan),
                'high': full(len(dts), nan),
                'low': full(len(dts), nan),
                'close': full(len(dts), nan),
                'volume': full(len(dts), 0),
            },
            index=dts))
        dts = trading_calendar.minutes_for_session(cls.trading_days[1])
        dfs.append(pd.DataFrame(
            {
                'open': append(100.5, full(len(dts) - 1, nan)),
                'high': append(100.9, full(len(dts) - 1, nan)),
                'low': append(100.1, full(len(dts) - 1, nan)),
                'close': append(100.3, full(len(dts) - 1, nan)),
                'volume': append(1000, full(len(dts) - 1, nan)),
            },
            index=dts))
        dts = trading_calendar.minutes_for_session(cls.trading_days[2])
        dfs.append(pd.DataFrame(
            {
                'open': [nan, 103.50, 102.50, 104.50, 101.50, nan],
                'high': [nan, 103.90, 102.90, 104.90, 101.90, nan],
                'low': [nan, 103.10, 102.10, 104.10, 101.10, nan],
                'close': [nan, 103.30, 102.30, 104.30, 101.30, nan],
                'volume': [0, 1003, 1002, 1004, 1001, 0]
            },
            index=dts[:6]
        ))
        dts = trading_calendar.minutes_for_session(cls.trading_days[3])
        dfs.append(pd.DataFrame(
            {
                'open': full(len(dts), nan),
                'high': full(len(dts), nan),
                'low': full(len(dts), nan),
                'close': full(len(dts), nan),
                'volume': full(len(dts), 0),
            },
            index=dts))
        asset1_df = pd.concat(dfs)
        yield 1, asset1_df

        asset2_df = pd.DataFrame(
            {
                'open': 1.0055,
                'high': 1.0059,
                'low': 1.0051,
                'close': 1.0055,
                'volume': 100,
            },
            index=asset1_df.index,
        )
        yield 2, asset2_df

        yield cls.DIVIDEND_ASSET_SID, asset2_df.copy()

</source>
<source file="systems/zipline-1.3.0/tests/test_data_portal.py" startline="147" endline="210" pcid="572">
    def make_future_minute_bar_data(cls):
        trading_calendar = cls.trading_calendars[Future]
        trading_sessions = cls.trading_sessions['us_futures']
        # No data on first day, future asset intentionally not on the same
        # dates as equities, so that cross-wiring of results do not create a
        # false positive.
        dts = trading_calendar.minutes_for_session(trading_sessions[1])
        dfs = []
        dfs.append(pd.DataFrame(
            {
                'open': full(len(dts), nan),
                'high': full(len(dts), nan),
                'low': full(len(dts), nan),
                'close': full(len(dts), nan),
                'volume': full(len(dts), 0),
            },
            index=dts))
        dts = trading_calendar.minutes_for_session(trading_sessions[2])
        dfs.append(pd.DataFrame(
            {
                'open': append(200.5, full(len(dts) - 1, nan)),
                'high': append(200.9, full(len(dts) - 1, nan)),
                'low': append(200.1, full(len(dts) - 1, nan)),
                'close': append(200.3, full(len(dts) - 1, nan)),
                'volume': append(2000, full(len(dts) - 1, nan)),
            },
            index=dts))
        dts = trading_calendar.minutes_for_session(trading_sessions[3])
        dfs.append(pd.DataFrame(
            {
                'open': [nan, 203.50, 202.50, 204.50, 201.50, nan],
                'high': [nan, 203.90, 202.90, 204.90, 201.90, nan],
                'low': [nan, 203.10, 202.10, 204.10, 201.10, nan],
                'close': [nan, 203.30, 202.30, 204.30, 201.30, nan],
                'volume': [0, 2003, 2002, 2004, 2001, 0]
            },
            index=dts[:6]
        ))
        dts = trading_calendar.minutes_for_session(trading_sessions[4])
        dfs.append(pd.DataFrame(
            {
                'open': full(len(dts), nan),
                'high': full(len(dts), nan),
                'low': full(len(dts), nan),
                'close': full(len(dts), nan),
                'volume': full(len(dts), 0),
            },
            index=dts))
        asset10000_df = pd.concat(dfs)
        yield 10000, asset10000_df

        missing_dts = trading_calendar.minutes_for_session(trading_sessions[0])
        asset10001_df = pd.DataFrame(
            {
                'open': 1.00549,
                'high': 1.00591,
                'low': 1.00507,
                'close': 1.0055,
                'volume': 100,
            },
            index=missing_dts.append(asset10000_df.index),
        )
        yield 10001, asset10001_df

</source>
</class>

<class classid="35" nclones="2" nlines="14" similarity="92">
<source file="systems/zipline-1.3.0/tests/test_data_portal.py" startline="236" endline="257" pcid="574">
    def test_get_last_traded_equity_minute(self):
        trading_calendar = self.trading_calendars[Equity]
        # Case: Missing data at front of data set, and request dt is before
        # first value.
        dts = trading_calendar.minutes_for_session(self.trading_days[0])
        asset = self.asset_finder.retrieve_asset(1)
        self.assertTrue(pd.isnull(
            self.data_portal.get_last_traded_dt(
                asset, dts[0], 'minute')))

        # Case: Data on requested dt.
        dts = trading_calendar.minutes_for_session(self.trading_days[2])

        self.assertEqual(dts[1],
                         self.data_portal.get_last_traded_dt(
                             asset, dts[1], 'minute'))

        # Case: No data on dt, but data occuring before dt.
        self.assertEqual(dts[4],
                         self.data_portal.get_last_traded_dt(
                             asset, dts[5], 'minute'))

</source>
<source file="systems/zipline-1.3.0/tests/test_data_portal.py" startline="258" endline="279" pcid="575">
    def test_get_last_traded_future_minute(self):
        asset = self.asset_finder.retrieve_asset(10000)
        trading_calendar = self.trading_calendars[Future]
        # Case: Missing data at front of data set, and request dt is before
        # first value.
        dts = trading_calendar.minutes_for_session(self.trading_days[0])
        self.assertTrue(pd.isnull(
            self.data_portal.get_last_traded_dt(
                asset, dts[0], 'minute')))

        # Case: Data on requested dt.
        dts = trading_calendar.minutes_for_session(self.trading_days[3])

        self.assertEqual(dts[1],
                         self.data_portal.get_last_traded_dt(
                             asset, dts[1], 'minute'))

        # Case: No data on dt, but data occuring before dt.
        self.assertEqual(dts[4],
                         self.data_portal.get_last_traded_dt(
                             asset, dts[5], 'minute'))

</source>
</class>

<class classid="36" nclones="2" nlines="30" similarity="100">
<source file="systems/zipline-1.3.0/tests/test_data_portal.py" startline="298" endline="336" pcid="577">
    def test_get_spot_value_equity_minute(self):
        trading_calendar = self.trading_calendars[Equity]
        asset = self.asset_finder.retrieve_asset(1)
        dts = trading_calendar.minutes_for_session(self.trading_days[2])

        # Case: Get data on exact dt.
        dt = dts[1]
        expected = OrderedDict({
            'open': 103.5,
            'high': 103.9,
            'low': 103.1,
            'close': 103.3,
            'volume': 1003,
            'price': 103.3
        })
        result = [self.data_portal.get_spot_value(asset,
                                                  field,
                                                  dt,
                                                  'minute')
                  for field in expected.keys()]
        assert_almost_equal(array(list(expected.values())), result)

        # Case: Get data on empty dt, return nan or most recent data for price.
        dt = dts[100]
        expected = OrderedDict({
            'open': nan,
            'high': nan,
            'low': nan,
            'close': nan,
            'volume': 0,
            'price': 101.3
        })
        result = [self.data_portal.get_spot_value(asset,
                                                  field,
                                                  dt,
                                                  'minute')
                  for field in expected.keys()]
        assert_almost_equal(array(list(expected.values())), result)

</source>
<source file="systems/zipline-1.3.0/tests/test_data_portal.py" startline="337" endline="375" pcid="578">
    def test_get_spot_value_future_minute(self):
        trading_calendar = self.trading_calendars[Future]
        asset = self.asset_finder.retrieve_asset(10000)
        dts = trading_calendar.minutes_for_session(self.trading_days[3])

        # Case: Get data on exact dt.
        dt = dts[1]
        expected = OrderedDict({
            'open': 203.5,
            'high': 203.9,
            'low': 203.1,
            'close': 203.3,
            'volume': 2003,
            'price': 203.3
        })
        result = [self.data_portal.get_spot_value(asset,
                                                  field,
                                                  dt,
                                                  'minute')
                  for field in expected.keys()]
        assert_almost_equal(array(list(expected.values())), result)

        # Case: Get data on empty dt, return nan or most recent data for price.
        dt = dts[100]
        expected = OrderedDict({
            'open': nan,
            'high': nan,
            'low': nan,
            'close': nan,
            'volume': 0,
            'price': 201.3
        })
        result = [self.data_portal.get_spot_value(asset,
                                                  field,
                                                  dt,
                                                  'minute')
                  for field in expected.keys()]
        assert_almost_equal(array(list(expected.values())), result)

</source>
</class>

<class classid="37" nclones="2" nlines="23" similarity="73">
<source file="systems/zipline-1.3.0/zipline/assets/synthetic.py" startline="11" endline="59" pcid="610">
def make_rotating_equity_info(num_assets,
                              first_start,
                              frequency,
                              periods_between_starts,
                              asset_lifetime):
    """
    Create a DataFrame representing lifetimes of assets that are constantly
    rotating in and out of existence.

    Parameters
    ----------
    num_assets : int
        How many assets to create.
    first_start : pd.Timestamp
        The start date for the first asset.
    frequency : str or pd.tseries.offsets.Offset (e.g. trading_day)
        Frequency used to interpret next two arguments.
    periods_between_starts : int
        Create a new asset every `frequency` * `periods_between_new`
    asset_lifetime : int
        Each asset exists for `frequency` * `asset_lifetime` days.

    Returns
    -------
    info : pd.DataFrame
        DataFrame representing newly-created assets.
    """
    return pd.DataFrame(
        {
            'symbol': [chr(ord('A') + i) for i in range(num_assets)],
            # Start a new asset every `periods_between_starts` days.
            'start_date': pd.date_range(
                first_start,
                freq=(periods_between_starts * frequency),
                periods=num_assets,
            ),
            # Each asset lasts for `asset_lifetime` days.
            'end_date': pd.date_range(
                first_start + (asset_lifetime * frequency),
                freq=(periods_between_starts * frequency),
                periods=num_assets,
            ),
            'exchange': 'TEST',
            'exchange_full': 'TEST FULL',
        },
        index=range(num_assets),
    )


</source>
<source file="systems/zipline-1.3.0/zipline/assets/synthetic.py" startline="117" endline="167" pcid="612">
def make_jagged_equity_info(num_assets,
                            start_date,
                            first_end,
                            frequency,
                            periods_between_ends,
                            auto_close_delta):
    """
    Create a DataFrame representing assets that all begin at the same start
    date, but have cascading end dates.

    Parameters
    ----------
    num_assets : int
        How many assets to create.
    start_date : pd.Timestamp
        The start date for all the assets.
    first_end : pd.Timestamp
        The date at which the first equity will end.
    frequency : str or pd.tseries.offsets.Offset (e.g. trading_day)
        Frequency used to interpret the next argument.
    periods_between_ends : int
        Starting after the first end date, end each asset every
        `frequency` * `periods_between_ends`.

    Returns
    -------
    info : pd.DataFrame
        DataFrame representing newly-created assets.
    """
    frame = pd.DataFrame(
        {
            'symbol': [chr(ord('A') + i) for i in range(num_assets)],
            'start_date': start_date,
            'end_date': pd.date_range(
                first_end,
                freq=(periods_between_ends * frequency),
                periods=num_assets,
            ),
            'exchange': 'TEST',
            'exchange_full': 'TEST FULL',
        },
        index=range(num_assets),
    )

    # Explicitly pass None to disable setting the auto_close_date column.
    if auto_close_delta is not None:
        frame['auto_close_date'] = frame['end_date'] + auto_close_delta

    return frame


</source>
</class>

<class classid="38" nclones="3" nlines="13" similarity="71">
<source file="systems/zipline-1.3.0/zipline/assets/asset_db_migrations.py" startline="111" endline="140" pcid="620">
def _downgrade_v1(op):
    """
    Downgrade assets db by removing the 'tick_size' column and renaming the
    'multiplier' column.
    """
    # Drop indices before batch
    # This is to prevent index collision when creating the temp table
    op.drop_index('ix_futures_contracts_root_symbol')
    op.drop_index('ix_futures_contracts_symbol')

    # Execute batch op to allow column modification in SQLite
    with op.batch_alter_table('futures_contracts') as batch_op:

        # Rename 'multiplier'
        batch_op.alter_column(column_name='multiplier',
                              new_column_name='contract_multiplier')

        # Delete 'tick_size'
        batch_op.drop_column('tick_size')

    # Recreate indices after batch
    op.create_index('ix_futures_contracts_root_symbol',
                    table_name='futures_contracts',
                    columns=['root_symbol'])
    op.create_index('ix_futures_contracts_symbol',
                    table_name='futures_contracts',
                    columns=['symbol'],
                    unique=True)


</source>
<source file="systems/zipline-1.3.0/zipline/assets/asset_db_migrations.py" startline="142" endline="163" pcid="621">
def _downgrade_v2(op):
    """
    Downgrade assets db by removing the 'auto_close_date' column.
    """
    # Drop indices before batch
    # This is to prevent index collision when creating the temp table
    op.drop_index('ix_equities_fuzzy_symbol')
    op.drop_index('ix_equities_company_symbol')

    # Execute batch op to allow column modification in SQLite
    with op.batch_alter_table('equities') as batch_op:
        batch_op.drop_column('auto_close_date')

    # Recreate indices after batch
    op.create_index('ix_equities_fuzzy_symbol',
                    table_name='equities',
                    columns=['fuzzy_symbol'])
    op.create_index('ix_equities_company_symbol',
                    table_name='equities',
                    columns=['company_symbol'])


</source>
<source file="systems/zipline-1.3.0/zipline/assets/asset_db_migrations.py" startline="213" endline="233" pcid="623">
def _downgrade_v4(op):
    """
    Downgrades assets db by copying the `exchange_full` column to `exchange`,
    then dropping the `exchange_full` column.
    """
    op.drop_index('ix_equities_fuzzy_symbol')
    op.drop_index('ix_equities_company_symbol')

    op.execute("UPDATE equities SET exchange = exchange_full")

    with op.batch_alter_table('equities') as batch_op:
        batch_op.drop_column('exchange_full')

    op.create_index('ix_equities_fuzzy_symbol',
                    table_name='equities',
                    columns=['fuzzy_symbol'])
    op.create_index('ix_equities_company_symbol',
                    table_name='equities',
                    columns=['company_symbol'])


</source>
</class>

<class classid="39" nclones="2" nlines="37" similarity="91">
<source file="systems/zipline-1.3.0/zipline/assets/asset_db_migrations.py" startline="165" endline="211" pcid="622">
def _downgrade_v3(op):
    """
    Downgrade assets db by adding a not null constraint on
    ``equities.first_traded``
    """
    op.create_table(
        '_new_equities',
        sa.Column(
            'sid',
            sa.Integer,
            unique=True,
            nullable=False,
            primary_key=True,
        ),
        sa.Column('symbol', sa.Text),
        sa.Column('company_symbol', sa.Text),
        sa.Column('share_class_symbol', sa.Text),
        sa.Column('fuzzy_symbol', sa.Text),
        sa.Column('asset_name', sa.Text),
        sa.Column('start_date', sa.Integer, default=0, nullable=False),
        sa.Column('end_date', sa.Integer, nullable=False),
        sa.Column('first_traded', sa.Integer, nullable=False),
        sa.Column('auto_close_date', sa.Integer),
        sa.Column('exchange', sa.Text),
    )
    op.execute(
        """
        insert into _new_equities
        select * from equities
        where equities.first_traded is not null
        """,
    )
    op.drop_table('equities')
    op.rename_table('_new_equities', 'equities')
    # we need to make sure the indices have the proper names after the rename
    op.create_index(
        'ix_equities_company_symbol',
        'equities',
        ['company_symbol'],
    )
    op.create_index(
        'ix_equities_fuzzy_symbol',
        'equities',
        ['fuzzy_symbol'],
    )


</source>
<source file="systems/zipline-1.3.0/zipline/assets/asset_db_migrations.py" startline="235" endline="312" pcid="624">
def _downgrade_v5(op):
    op.create_table(
        '_new_equities',
        sa.Column(
            'sid',
            sa.Integer,
            unique=True,
            nullable=False,
            primary_key=True,
        ),
        sa.Column('symbol', sa.Text),
        sa.Column('company_symbol', sa.Text),
        sa.Column('share_class_symbol', sa.Text),
        sa.Column('fuzzy_symbol', sa.Text),
        sa.Column('asset_name', sa.Text),
        sa.Column('start_date', sa.Integer, default=0, nullable=False),
        sa.Column('end_date', sa.Integer, nullable=False),
        sa.Column('first_traded', sa.Integer),
        sa.Column('auto_close_date', sa.Integer),
        sa.Column('exchange', sa.Text),
        sa.Column('exchange_full', sa.Text)
    )

    op.execute(
        """
        insert into _new_equities
        select
            equities.sid as sid,
            sym.symbol as symbol,
            sym.company_symbol as company_symbol,
            sym.share_class_symbol as share_class_symbol,
            sym.company_symbol || sym.share_class_symbol as fuzzy_symbol,
            equities.asset_name as asset_name,
            equities.start_date as start_date,
            equities.end_date as end_date,
            equities.first_traded as first_traded,
            equities.auto_close_date as auto_close_date,
            equities.exchange as exchange,
            equities.exchange_full as exchange_full
        from
            equities
        inner join
            -- Nested select here to take the most recently held ticker
            -- for each sid. The group by with no aggregation function will
            -- take the last element in the group, so we first order by
            -- the end date ascending to ensure that the groupby takes
            -- the last ticker.
            (select
                 *
             from
                 (select
                      *
                  from
                      equity_symbol_mappings
                  order by
                      equity_symbol_mappings.end_date asc)
             group by
                 sid) sym
        on
            equities.sid == sym.sid
        """,
    )
    op.drop_table('equity_symbol_mappings')
    op.drop_table('equities')
    op.rename_table('_new_equities', 'equities')
    # we need to make sure the indicies have the proper names after the rename
    op.create_index(
        'ix_equities_company_symbol',
        'equities',
        ['company_symbol'],
    )
    op.create_index(
        'ix_equities_fuzzy_symbol',
        'equities',
        ['fuzzy_symbol'],
    )


</source>
</class>

<class classid="40" nclones="2" nlines="13" similarity="92">
<source file="systems/zipline-1.3.0/zipline/data/bundles/quandl.py" startline="137" endline="152" pcid="672">
def parse_splits(data, show_progress):
    if show_progress:
        log.info('Parsing split data.')

    data['split_ratio'] = 1.0 / data.split_ratio
    data.rename(
        columns={
            'split_ratio': 'ratio',
            'date': 'effective_date',
        },
        inplace=True,
        copy=False,
    )
    return data


</source>
<source file="systems/zipline-1.3.0/zipline/data/bundles/quandl.py" startline="153" endline="168" pcid="673">
def parse_dividends(data, show_progress):
    if show_progress:
        log.info('Parsing dividend data.')

    data['record_date'] = data['declared_date'] = data['pay_date'] = pd.NaT
    data.rename(
        columns={
            'ex_dividend': 'amount',
            'date': 'ex_date',
        },
        inplace=True,
        copy=False,
    )
    return data


</source>
</class>

<class classid="41" nclones="2" nlines="51" similarity="75">
<source file="systems/zipline-1.3.0/zipline/data/continuous_future_reader.py" startline="12" endline="97" pcid="698">
    def load_raw_arrays(self, columns, start_date, end_date, assets):
        """
        Parameters
        ----------
        fields : list of str
            'sid'
        start_dt: Timestamp
           Beginning of the window range.
        end_dt: Timestamp
           End of the window range.
        sids : list of int
           The asset identifiers in the window.

        Returns
        -------
        list of np.ndarray
            A list with an entry per field of ndarrays with shape
            (minutes in range, sids) with a dtype of float64, containing the
            values for the respective field over start and end dt range.
        """
        rolls_by_asset = {}
        for asset in assets:
            rf = self._roll_finders[asset.roll_style]
            rolls_by_asset[asset] = rf.get_rolls(
                asset.root_symbol,
                start_date,
                end_date,
                asset.offset
            )

        num_sessions = len(
            self.trading_calendar.sessions_in_range(start_date, end_date)
        )
        shape = num_sessions, len(assets)

        results = []

        tc = self._bar_reader.trading_calendar
        sessions = tc.sessions_in_range(start_date, end_date)

        # Get partitions
        partitions_by_asset = {}
        for asset in assets:
            partitions = []
            partitions_by_asset[asset] = partitions

            rolls = rolls_by_asset[asset]
            start = start_date

            for roll in rolls:
                sid, roll_date = roll
                start_loc = sessions.get_loc(start)

                if roll_date is not None:
                    end = roll_date - sessions.freq
                    end_loc = sessions.get_loc(end)
                else:
                    end = end_date
                    end_loc = len(sessions) - 1

                partitions.append((sid, start, end, start_loc, end_loc))

                if roll_date is not None:
                    start = sessions[end_loc + 1]

        for column in columns:
            if column != 'volume' and column != 'sid':
                out = np.full(shape, np.nan)
            else:
                out = np.zeros(shape, dtype=np.int64)

            for i, asset in enumerate(assets):
                partitions = partitions_by_asset[asset]

                for sid, start, end, start_loc, end_loc in partitions:
                    if column != 'sid':
                        result = self._bar_reader.load_raw_arrays(
                            [column], start, end, [sid])[0][:, 0]
                    else:
                        result = int(sid)
                    out[start_loc:end_loc + 1, i] = result

            results.append(out)

        return results

</source>
<source file="systems/zipline-1.3.0/zipline/data/continuous_future_reader.py" startline="204" endline="283" pcid="706">
    def load_raw_arrays(self, columns, start_date, end_date, assets):
        """
        Parameters
        ----------
        fields : list of str
           'open', 'high', 'low', 'close', or 'volume'
        start_dt: Timestamp
           Beginning of the window range.
        end_dt: Timestamp
           End of the window range.
        sids : list of int
           The asset identifiers in the window.

        Returns
        -------
        list of np.ndarray
            A list with an entry per field of ndarrays with shape
            (minutes in range, sids) with a dtype of float64, containing the
            values for the respective field over start and end dt range.
        """
        rolls_by_asset = {}

        tc = self.trading_calendar
        start_session = tc.minute_to_session_label(start_date)
        end_session = tc.minute_to_session_label(end_date)

        for asset in assets:
            rf = self._roll_finders[asset.roll_style]
            rolls_by_asset[asset] = rf.get_rolls(
                asset.root_symbol,
                start_session,
                end_session, asset.offset)

        sessions = tc.sessions_in_range(start_date, end_date)

        minutes = tc.minutes_in_range(start_date, end_date)
        num_minutes = len(minutes)
        shape = num_minutes, len(assets)

        results = []

        # Get partitions
        partitions_by_asset = {}
        for asset in assets:
            partitions = []
            partitions_by_asset[asset] = partitions
            rolls = rolls_by_asset[asset]
            start = start_date
            for roll in rolls:
                sid, roll_date = roll
                start_loc = minutes.searchsorted(start)
                if roll_date is not None:
                    _, end = tc.open_and_close_for_session(
                        roll_date - sessions.freq)
                    end_loc = minutes.searchsorted(end)
                else:
                    end = end_date
                    end_loc = len(minutes) - 1
                partitions.append((sid, start, end, start_loc, end_loc))
                if roll[-1] is not None:
                    start, _ = tc.open_and_close_for_session(
                        tc.minute_to_session_label(minutes[end_loc + 1]))

        for column in columns:
            if column != 'volume':
                out = np.full(shape, np.nan)
            else:
                out = np.zeros(shape, dtype=np.uint32)
            for i, asset in enumerate(assets):
                partitions = partitions_by_asset[asset]
                for sid, start, end, start_loc, end_loc in partitions:
                    if column != 'sid':
                        result = self._bar_reader.load_raw_arrays(
                            [column], start, end, [sid])[0][:, 0]
                    else:
                        result = int(sid)
                    out[start_loc:end_loc + 1, i] = result
            results.append(out)
        return results

</source>
</class>

<class classid="42" nclones="4" nlines="55" similarity="75">
<source file="systems/zipline-1.3.0/zipline/data/resample.py" startline="219" endline="290" pcid="770">
    def opens(self, assets, dt):
        """
        The open field's aggregation returns the first value that occurs
        for the day, if there has been no data on or before the `dt` the open
        is `nan`.

        Once the first non-nan open is seen, that value remains constant per
        asset for the remainder of the day.

        Returns
        -------
        np.array with dtype=float64, in order of assets parameter.
        """
        market_open, prev_dt, dt_value, entries = self._prelude(dt, 'open')

        opens = []
        session_label = self._trading_calendar.minute_to_session_label(dt)

        for asset in assets:
            if not asset.is_alive_for_session(session_label):
                opens.append(np.NaN)
                continue

            if prev_dt is None:
                val = self._minute_reader.get_value(asset, dt, 'open')
                entries[asset] = (dt_value, val)
                opens.append(val)
                continue
            else:
                try:
                    last_visited_dt, first_open = entries[asset]
                    if last_visited_dt == dt_value:
                        opens.append(first_open)
                        continue
                    elif not pd.isnull(first_open):
                        opens.append(first_open)
                        entries[asset] = (dt_value, first_open)
                        continue
                    else:
                        after_last = pd.Timestamp(
                            last_visited_dt + self._one_min, tz='UTC')
                        window = self._minute_reader.load_raw_arrays(
                            ['open'],
                            after_last,
                            dt,
                            [asset],
                        )[0]
                        nonnan = window[~pd.isnull(window)]
                        if len(nonnan):
                            val = nonnan[0]
                        else:
                            val = np.nan
                        entries[asset] = (dt_value, val)
                        opens.append(val)
                        continue
                except KeyError:
                    window = self._minute_reader.load_raw_arrays(
                        ['open'],
                        market_open,
                        dt,
                        [asset],
                    )[0]
                    nonnan = window[~pd.isnull(window)]
                    if len(nonnan):
                        val = nonnan[0]
                    else:
                        val = np.nan
                    entries[asset] = (dt_value, val)
                    opens.append(val)
                    continue
        return np.array(opens)

</source>
<source file="systems/zipline-1.3.0/zipline/data/resample.py" startline="291" endline="359" pcid="771">
    def highs(self, assets, dt):
        """
        The high field's aggregation returns the largest high seen between
        the market open and the current dt.
        If there has been no data on or before the `dt` the high is `nan`.

        Returns
        -------
        np.array with dtype=float64, in order of assets parameter.
        """
        market_open, prev_dt, dt_value, entries = self._prelude(dt, 'high')

        highs = []
        session_label = self._trading_calendar.minute_to_session_label(dt)

        for asset in assets:
            if not asset.is_alive_for_session(session_label):
                highs.append(np.NaN)
                continue

            if prev_dt is None:
                val = self._minute_reader.get_value(asset, dt, 'high')
                entries[asset] = (dt_value, val)
                highs.append(val)
                continue
            else:
                try:
                    last_visited_dt, last_max = entries[asset]
                    if last_visited_dt == dt_value:
                        highs.append(last_max)
                        continue
                    elif last_visited_dt == prev_dt:
                        curr_val = self._minute_reader.get_value(
                            asset, dt, 'high')
                        if pd.isnull(curr_val):
                            val = last_max
                        elif pd.isnull(last_max):
                            val = curr_val
                        else:
                            val = max(last_max, curr_val)
                        entries[asset] = (dt_value, val)
                        highs.append(val)
                        continue
                    else:
                        after_last = pd.Timestamp(
                            last_visited_dt + self._one_min, tz='UTC')
                        window = self._minute_reader.load_raw_arrays(
                            ['high'],
                            after_last,
                            dt,
                            [asset],
                        )[0].T
                        val = np.nanmax(np.append(window, last_max))
                        entries[asset] = (dt_value, val)
                        highs.append(val)
                        continue
                except KeyError:
                    window = self._minute_reader.load_raw_arrays(
                        ['high'],
                        market_open,
                        dt,
                        [asset],
                    )[0].T
                    val = np.nanmax(window)
                    entries[asset] = (dt_value, val)
                    highs.append(val)
                    continue
        return np.array(highs)

</source>
<source file="systems/zipline-1.3.0/zipline/data/resample.py" startline="360" endline="423" pcid="772">
    def lows(self, assets, dt):
        """
        The low field's aggregation returns the smallest low seen between
        the market open and the current dt.
        If there has been no data on or before the `dt` the low is `nan`.

        Returns
        -------
        np.array with dtype=float64, in order of assets parameter.
        """
        market_open, prev_dt, dt_value, entries = self._prelude(dt, 'low')

        lows = []
        session_label = self._trading_calendar.minute_to_session_label(dt)

        for asset in assets:
            if not asset.is_alive_for_session(session_label):
                lows.append(np.NaN)
                continue

            if prev_dt is None:
                val = self._minute_reader.get_value(asset, dt, 'low')
                entries[asset] = (dt_value, val)
                lows.append(val)
                continue
            else:
                try:
                    last_visited_dt, last_min = entries[asset]
                    if last_visited_dt == dt_value:
                        lows.append(last_min)
                        continue
                    elif last_visited_dt == prev_dt:
                        curr_val = self._minute_reader.get_value(
                            asset, dt, 'low')
                        val = np.nanmin([last_min, curr_val])
                        entries[asset] = (dt_value, val)
                        lows.append(val)
                        continue
                    else:
                        after_last = pd.Timestamp(
                            last_visited_dt + self._one_min, tz='UTC')
                        window = self._minute_reader.load_raw_arrays(
                            ['low'],
                            after_last,
                            dt,
                            [asset],
                        )[0].T
                        val = np.nanmin(np.append(window, last_min))
                        entries[asset] = (dt_value, val)
                        lows.append(val)
                        continue
                except KeyError:
                    window = self._minute_reader.load_raw_arrays(
                        ['low'],
                        market_open,
                        dt,
                        [asset],
                    )[0].T
                    val = np.nanmin(window)
                    entries[asset] = (dt_value, val)
                    lows.append(val)
                    continue
        return np.array(lows)

</source>
<source file="systems/zipline-1.3.0/zipline/data/resample.py" startline="500" endline="564" pcid="775">
    def volumes(self, assets, dt):
        """
        The volume field's aggregation returns the sum of all volumes
        between the market open and the `dt`
        If there has been no data on or before the `dt` the volume is 0.

        Returns
        -------
        np.array with dtype=int64, in order of assets parameter.
        """
        market_open, prev_dt, dt_value, entries = self._prelude(dt, 'volume')

        volumes = []
        session_label = self._trading_calendar.minute_to_session_label(dt)

        for asset in assets:
            if not asset.is_alive_for_session(session_label):
                volumes.append(0)
                continue

            if prev_dt is None:
                val = self._minute_reader.get_value(asset, dt, 'volume')
                entries[asset] = (dt_value, val)
                volumes.append(val)
                continue
            else:
                try:
                    last_visited_dt, last_total = entries[asset]
                    if last_visited_dt == dt_value:
                        volumes.append(last_total)
                        continue
                    elif last_visited_dt == prev_dt:
                        val = self._minute_reader.get_value(
                            asset, dt, 'volume')
                        val += last_total
                        entries[asset] = (dt_value, val)
                        volumes.append(val)
                        continue
                    else:
                        after_last = pd.Timestamp(
                            last_visited_dt + self._one_min, tz='UTC')
                        window = self._minute_reader.load_raw_arrays(
                            ['volume'],
                            after_last,
                            dt,
                            [asset],
                        )[0]
                        val = np.nansum(window) + last_total
                        entries[asset] = (dt_value, val)
                        volumes.append(val)
                        continue
                except KeyError:
                    window = self._minute_reader.load_raw_arrays(
                        ['volume'],
                        market_open,
                        dt,
                        [asset],
                    )[0]
                    val = np.nansum(window)
                    entries[asset] = (dt_value, val)
                    volumes.append(val)
                    continue
        return np.array(volumes)


</source>
</class>

<class classid="43" nclones="2" nlines="17" similarity="73">
<source file="systems/zipline-1.3.0/zipline/utils/memoize.py" startline="63" endline="76" pcid="802">
            def wrapper(*args, **kwds):
                key = args
                if kwds:
                    key += kwd_mark + tuple(sorted(kwds.items()))
                try:
                    result = cache[key]
                    hits[0] += 1
                    return result
                except KeyError:
                    pass
                result = user_function(*args, **kwds)
                cache[key] = result
                misses[0] += 1
                return result
</source>
<source file="systems/zipline-1.3.0/zipline/utils/memoize.py" startline="84" endline="104" pcid="803">
            def wrapper(*args, **kwds):
                key = args
                if kwds:
                    key += kwd_mark + tuple(sorted(kwds.items()))
                with lock:
                    try:
                        result = cache[key]
                        cache_renew(key)    # record recent use of this key
                        hits[0] += 1
                        return result
                    except KeyError:
                        pass
                result = user_function(*args, **kwds)
                with lock:
                    cache[key] = result     # record recent use of this key
                    misses[0] += 1
                    if len(cache) > maxsize:
                        # purge least recently used cache entry
                        cache_popitem(False)
                return result

</source>
</class>

<class classid="44" nclones="2" nlines="12" similarity="75">
<source file="systems/zipline-1.3.0/zipline/utils/input_validation.py" startline="136" endline="163" pcid="972">
def ensure_dtype(func, argname, arg):
    """
    Argument preprocessor that converts the input into a numpy dtype.

    Examples
    --------
    >>> import numpy as np
    >>> from zipline.utils.preprocess import preprocess
    >>> @preprocess(dtype=ensure_dtype)
    ... def foo(dtype):
    ...     return dtype
    ...
    >>> foo(float)
    dtype('float64')
    """
    try:
        return dtype(arg)
    except TypeError:
        raise TypeError(
            "{func}() couldn't convert argument "
            "{argname}={arg!r} to a numpy dtype.".format(
                func=_qualified_name(func),
                argname=argname,
                arg=arg,
            ),
        )


</source>
<source file="systems/zipline-1.3.0/zipline/utils/input_validation.py" startline="164" endline="190" pcid="973">
def ensure_timezone(func, argname, arg):
    """Argument preprocessor that converts the input into a tzinfo object.

    Examples
    --------
    >>> from zipline.utils.preprocess import preprocess
    >>> @preprocess(tz=ensure_timezone)
    ... def foo(tz):
    ...     return tz
    >>> foo('utc')
    <UTC>
    """
    if isinstance(arg, tzinfo):
        return arg
    if isinstance(arg, string_types):
        return timezone(arg)

    raise TypeError(
        "{func}() couldn't convert argument "
        "{argname}={arg!r} to a timezone.".format(
            func=_qualified_name(func),
            argname=argname,
            arg=arg,
        ),
    )


</source>
</class>

<class classid="45" nclones="2" nlines="30" similarity="75">
<source file="systems/zipline-1.3.0/zipline/utils/input_validation.py" startline="220" endline="287" pcid="975">
def expect_dtypes(__funcname=_qualified_name, **named):
    """
    Preprocessing decorator that verifies inputs have expected numpy dtypes.

    Examples
    --------
    >>> from numpy import dtype, arange, int8, float64
    >>> @expect_dtypes(x=dtype(int8))
    ... def foo(x, y):
    ...    return x, y
    ...
    >>> foo(arange(3, dtype=int8), 'foo')
    (array([0, 1, 2], dtype=int8), 'foo')
    >>> foo(arange(3, dtype=float64), 'foo')  # doctest: +NORMALIZE_WHITESPACE
    ...                                       # doctest: +ELLIPSIS
    Traceback (most recent call last):
       ...
    TypeError: ...foo() expected a value with dtype 'int8' for argument 'x',
    but got 'float64' instead.
    """
    for name, type_ in iteritems(named):
        if not isinstance(type_, (dtype, tuple)):
            raise TypeError(
                "expect_dtypes() expected a numpy dtype or tuple of dtypes"
                " for argument {name!r}, but got {dtype} instead.".format(
                    name=name, dtype=dtype,
                )
            )

    if isinstance(__funcname, str):
        def get_funcname(_):
            return __funcname
    else:
        get_funcname = __funcname

    @preprocess(dtypes=call(lambda x: x if isinstance(x, tuple) else (x,)))
    def _expect_dtype(dtypes):
        """
        Factory for dtype-checking functions that work with the @preprocess
        decorator.
        """
        def error_message(func, argname, value):
            # If the bad value has a dtype, but it's wrong, show the dtype
            # name.  Otherwise just show the value.
            try:
                value_to_show = value.dtype.name
            except AttributeError:
                value_to_show = value
            return (
                "{funcname}() expected a value with dtype {dtype_str} "
                "for argument {argname!r}, but got {value!r} instead."
            ).format(
                funcname=get_funcname(func),
                dtype_str=' or '.join(repr(d.name) for d in dtypes),
                argname=argname,
                value=value_to_show,
            )

        def _actual_preprocessor(func, argname, argvalue):
            if getattr(argvalue, 'dtype', object()) not in dtypes:
                raise TypeError(error_message(func, argname, argvalue))
            return argvalue

        return _actual_preprocessor

    return preprocess(**valmap(_expect_dtype, named))


</source>
<source file="systems/zipline-1.3.0/zipline/utils/input_validation.py" startline="288" endline="350" pcid="980">
def expect_kinds(**named):
    """
    Preprocessing decorator that verifies inputs have expected dtype kinds.

    Examples
    --------
    >>> from numpy import int64, int32, float32
    >>> @expect_kinds(x='i')
    ... def foo(x):
    ...    return x
    ...
    >>> foo(int64(2))
    2
    >>> foo(int32(2))
    2
    >>> foo(float32(2))  # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
    Traceback (most recent call last):
       ...
    TypeError: ...foo() expected a numpy object of kind 'i' for argument 'x',
    but got 'f' instead.
    """
    for name, kind in iteritems(named):
        if not isinstance(kind, (str, tuple)):
            raise TypeError(
                "expect_dtype_kinds() expected a string or tuple of strings"
                " for argument {name!r}, but got {kind} instead.".format(
                    name=name, kind=dtype,
                )
            )

    @preprocess(kinds=call(lambda x: x if isinstance(x, tuple) else (x,)))
    def _expect_kind(kinds):
        """
        Factory for kind-checking functions that work the @preprocess
        decorator.
        """
        def error_message(func, argname, value):
            # If the bad value has a dtype, but it's wrong, show the dtype
            # kind.  Otherwise just show the value.
            try:
                value_to_show = value.dtype.kind
            except AttributeError:
                value_to_show = value
            return (
                "{funcname}() expected a numpy object of kind {kinds} "
                "for argument {argname!r}, but got {value!r} instead."
            ).format(
                funcname=_qualified_name(func),
                kinds=' or '.join(map(repr, kinds)),
                argname=argname,
                value=value_to_show,
            )

        def _actual_preprocessor(func, argname, argvalue):
            if getattrs(argvalue, ('dtype', 'kind'), object()) not in kinds:
                raise TypeError(error_message(func, argname, argvalue))
            return argvalue

        return _actual_preprocessor

    return preprocess(**valmap(_expect_kind, named))


</source>
</class>

<class classid="46" nclones="2" nlines="25" similarity="88">
<source file="systems/zipline-1.3.0/zipline/utils/input_validation.py" startline="531" endline="609" pcid="992">
def expect_bounded(__funcname=_qualified_name, **named):
    """
    Preprocessing decorator verifying that inputs fall INCLUSIVELY between
    bounds.

    Bounds should be passed as a pair of ``(min_value, max_value)``.

    ``None`` may be passed as ``min_value`` or ``max_value`` to signify that
    the input is only bounded above or below.

    Examples
    --------
    >>> @expect_bounded(x=(1, 5))
    ... def foo(x):
    ...    return x + 1
    ...
    >>> foo(1)
    2
    >>> foo(5)
    6
    >>> foo(6)  # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
    Traceback (most recent call last):
       ...
    ValueError: ...foo() expected a value inclusively between 1 and 5 for
    argument 'x', but got 6 instead.

    >>> @expect_bounded(x=(2, None))
    ... def foo(x):
    ...    return x
    ...
    >>> foo(100000)
    100000
    >>> foo(1)  # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
    Traceback (most recent call last):
       ...
    ValueError: ...foo() expected a value greater than or equal to 2 for
    argument 'x', but got 1 instead.

    >>> @expect_bounded(x=(None, 5))
    ... def foo(x):
    ...    return x
    ...
    >>> foo(6)  # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
    Traceback (most recent call last):
       ...
    ValueError: ...foo() expected a value less than or equal to 5 for
    argument 'x', but got 6 instead.
    """
    def _make_bounded_check(bounds):
        (lower, upper) = bounds
        if lower is None:
            def should_fail(value):
                return value > upper
            predicate_descr = "less than or equal to " + str(upper)
        elif upper is None:
            def should_fail(value):
                return value < lower
            predicate_descr = "greater than or equal to " + str(lower)
        else:
            def should_fail(value):
                return not (lower <= value <= upper)
            predicate_descr = "inclusively between %s and %s" % bounds

        template = (
            "%(funcname)s() expected a value {predicate}"
            " for argument '%(argname)s', but got %(actual)s instead."
        ).format(predicate=predicate_descr)

        return make_check(
            exc_type=ValueError,
            template=template,
            pred=should_fail,
            actual=repr,
            funcname=__funcname,
        )

    return _expect_bounded(_make_bounded_check, __funcname=__funcname, **named)


</source>
<source file="systems/zipline-1.3.0/zipline/utils/input_validation.py" startline="610" endline="688" pcid="997">
def expect_strictly_bounded(__funcname=_qualified_name, **named):
    """
    Preprocessing decorator verifying that inputs fall EXCLUSIVELY between
    bounds.

    Bounds should be passed as a pair of ``(min_value, max_value)``.

    ``None`` may be passed as ``min_value`` or ``max_value`` to signify that
    the input is only bounded above or below.

    Examples
    --------
    >>> @expect_strictly_bounded(x=(1, 5))
    ... def foo(x):
    ...    return x + 1
    ...
    >>> foo(2)
    3
    >>> foo(4)
    5
    >>> foo(5)  # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
    Traceback (most recent call last):
       ...
    ValueError: ...foo() expected a value exclusively between 1 and 5 for
    argument 'x', but got 5 instead.

    >>> @expect_strictly_bounded(x=(2, None))
    ... def foo(x):
    ...    return x
    ...
    >>> foo(100000)
    100000
    >>> foo(2)  # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
    Traceback (most recent call last):
       ...
    ValueError: ...foo() expected a value strictly greater than 2 for
    argument 'x', but got 2 instead.

    >>> @expect_strictly_bounded(x=(None, 5))
    ... def foo(x):
    ...    return x
    ...
    >>> foo(5)  # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
    Traceback (most recent call last):
       ...
    ValueError: ...foo() expected a value strictly less than 5 for
    argument 'x', but got 5 instead.
    """
    def _make_bounded_check(bounds):
        (lower, upper) = bounds
        if lower is None:
            def should_fail(value):
                return value >= upper
            predicate_descr = "strictly less than " + str(upper)
        elif upper is None:
            def should_fail(value):
                return value <= lower
            predicate_descr = "strictly greater than " + str(lower)
        else:
            def should_fail(value):
                return not (lower < value < upper)
            predicate_descr = "exclusively between %s and %s" % bounds

        template = (
            "%(funcname)s() expected a value {predicate}"
            " for argument '%(argname)s', but got %(actual)s instead."
        ).format(predicate=predicate_descr)

        return make_check(
            exc_type=ValueError,
            template=template,
            pred=should_fail,
            actual=repr,
            funcname=__funcname,
        )

    return _expect_bounded(_make_bounded_check, __funcname=__funcname, **named)


</source>
</class>

<class classid="47" nclones="2" nlines="10" similarity="100">
<source file="systems/zipline-1.3.0/zipline/utils/events.py" startline="360" endline="372" pcid="1119">
    def __init__(self, offset=None, **kwargs):
        self.offset = _build_offset(
            offset,
            kwargs,
            datetime.timedelta(minutes=1),  # Defaults to the first minute.
        )

        self._period_start = None
        self._period_end = None
        self._period_close = None

        self._one_minute = datetime.timedelta(minutes=1)

</source>
<source file="systems/zipline-1.3.0/zipline/utils/events.py" startline="416" endline="428" pcid="1122">
    def __init__(self, offset=None, **kwargs):
        self.offset = _build_offset(
            offset,
            kwargs,
            datetime.timedelta(minutes=1),  # Defaults to the last minute.
        )

        self._period_start = None
        self._period_close = None
        self._period_end = None

        self._one_minute = datetime.timedelta(minutes=1)

</source>
</class>

<class classid="48" nclones="2" nlines="16" similarity="76">
<source file="systems/zipline-1.3.0/zipline/pipeline/loaders/utils.py" startline="27" endline="80" pcid="1205">

def next_event_indexer(all_dates,
                       all_sids,
                       event_dates,
                       event_timestamps,
                       event_sids):
    """
    Construct an index array that, when applied to an array of values, produces
    a 2D array containing the values associated with the next event for each
    sid at each moment in time.

    Locations where no next event was known will be filled with -1.

    Parameters
    ----------
    all_dates : ndarray[datetime64[ns], ndim=1]
        Row labels for the target output.
    all_sids : ndarray[int, ndim=1]
        Column labels for the target output.
    event_dates : ndarray[datetime64[ns], ndim=1]
        Dates on which each input events occurred/will occur.  ``event_dates``
        must be in sorted order, and may not contain any NaT values.
    event_timestamps : ndarray[datetime64[ns], ndim=1]
        Dates on which we learned about each input event.
    event_sids : ndarray[int, ndim=1]
        Sids assocated with each input event.

    Returns
    -------
    indexer : ndarray[int, ndim=2]
        An array of shape (len(all_dates), len(all_sids)) of indices into
        ``event_{dates,timestamps,sids}``.
    """
    validate_event_metadata(event_dates, event_timestamps, event_sids)
    out = np.full((len(all_dates), len(all_sids)), -1, dtype=np.int64)

    sid_ixs = all_sids.searchsorted(event_sids)
    # side='right' here ensures that we include the event date itself
    # if it's in all_dates.
    dt_ixs = all_dates.searchsorted(event_dates, side='right')
    ts_ixs = all_dates.searchsorted(event_timestamps)

    # Walk backward through the events, writing the index of the event into
    # slots ranging from the event's timestamp to its asof.  This depends for
    # correctness on the fact that event_dates is sorted in ascending order,
    # because we need to overwrite later events with earlier ones if their
    # eligible windows overlap.
    for i in range(len(event_sids) - 1, -1, -1):
        start_ix = ts_ixs[i]
        end_ix = dt_ixs[i]
        out[start_ix:end_ix, sid_ixs[i]] = i

    return out

</source>
<source file="systems/zipline-1.3.0/zipline/pipeline/loaders/utils.py" startline="81" endline="133" pcid="1206">

def previous_event_indexer(all_dates,
                           all_sids,
                           event_dates,
                           event_timestamps,
                           event_sids):
    """
    Construct an index array that, when applied to an array of values, produces
    a 2D array containing the values associated with the previous event for
    each sid at each moment in time.

    Locations where no previous event was known will be filled with -1.

    Parameters
    ----------
    all_dates : ndarray[datetime64[ns], ndim=1]
        Row labels for the target output.
    all_sids : ndarray[int, ndim=1]
        Column labels for the target output.
    event_dates : ndarray[datetime64[ns], ndim=1]
        Dates on which each input events occurred/will occur.  ``event_dates``
        must be in sorted order, and may not contain any NaT values.
    event_timestamps : ndarray[datetime64[ns], ndim=1]
        Dates on which we learned about each input event.
    event_sids : ndarray[int, ndim=1]
        Sids assocated with each input event.

    Returns
    -------
    indexer : ndarray[int, ndim=2]
        An array of shape (len(all_dates), len(all_sids)) of indices into
        ``event_{dates,timestamps,sids}``.
    """
    validate_event_metadata(event_dates, event_timestamps, event_sids)
    out = np.full((len(all_dates), len(all_sids)), -1, dtype=np.int64)

    eff_dts = np.maximum(event_dates, event_timestamps)
    sid_ixs = all_sids.searchsorted(event_sids)
    dt_ixs = all_dates.searchsorted(eff_dts)

    # Walk backwards through the events, writing the index of the event into
    # slots ranging from max(event_date, event_timestamp) to the start of the
    # previously-written event.  This depends for correctness on the fact that
    # event_dates is sorted in ascending order, because we need to have written
    # later events so we know where to stop forward-filling earlier events.
    last_written = {}
    for i in range(len(event_dates) - 1, -1, -1):
        sid_ix = sid_ixs[i]
        dt_ix = dt_ixs[i]
        out[dt_ix:last_written.get(sid_ix, None), sid_ix] = i
        last_written[sid_ix] = dt_ix
    return out

</source>
</class>

<class classid="49" nclones="2" nlines="11" similarity="90">
<source file="systems/zipline-1.3.0/zipline/pipeline/factors/basic.py" startline="198" endline="241" pcid="1293">
    def from_span(cls, inputs, window_length, span, **kwargs):
        """
        Convenience constructor for passing `decay_rate` in terms of `span`.

        Forwards `decay_rate` as `1 - (2.0 / (1 + span))`.  This provides the
        behavior equivalent to passing `span` to pandas.ewma.

        Examples
        --------
        .. code-block:: python

            # Equivalent to:
            # my_ewma = EWMA(
            #    inputs=[USEquityPricing.close],
            #    window_length=30,
            #    decay_rate=(1 - (2.0 / (1 + 15.0))),
            # )
            my_ewma = EWMA.from_span(
                inputs=[USEquityPricing.close],
                window_length=30,
                span=15,
            )

        Notes
        -----
        This classmethod is provided by both
        :class:`ExponentialWeightedMovingAverage` and
        :class:`ExponentialWeightedMovingStdDev`.
        """
        if span <= 1:
            raise ValueError(
                "`span` must be a positive number. %s was passed." % span
            )

        decay_rate = (1.0 - (2.0 / (1.0 + span)))
        assert 0.0 < decay_rate <= 1.0

        return cls(
            inputs=inputs,
            window_length=window_length,
            decay_rate=decay_rate,
            **kwargs
        )

</source>
<source file="systems/zipline-1.3.0/zipline/pipeline/factors/basic.py" startline="244" endline="287" pcid="1294">
    def from_halflife(cls, inputs, window_length, halflife, **kwargs):
        """
        Convenience constructor for passing ``decay_rate`` in terms of half
        life.

        Forwards ``decay_rate`` as ``exp(log(.5) / halflife)``.  This provides
        the behavior equivalent to passing `halflife` to pandas.ewma.

        Examples
        --------
        .. code-block:: python

            # Equivalent to:
            # my_ewma = EWMA(
            #    inputs=[USEquityPricing.close],
            #    window_length=30,
            #    decay_rate=np.exp(np.log(0.5) / 15),
            # )
            my_ewma = EWMA.from_halflife(
                inputs=[USEquityPricing.close],
                window_length=30,
                halflife=15,
            )

        Notes
        -----
        This classmethod is provided by both
        :class:`ExponentialWeightedMovingAverage` and
        :class:`ExponentialWeightedMovingStdDev`.
        """
        if halflife <= 0:
            raise ValueError(
                "`span` must be a positive number. %s was passed." % halflife
            )
        decay_rate = exp(log(.5) / halflife)
        assert 0.0 < decay_rate <= 1.0

        return cls(
            inputs=inputs,
            window_length=window_length,
            decay_rate=decay_rate,
            **kwargs
        )

</source>
</class>

<class classid="50" nclones="2" nlines="13" similarity="100">
<source file="systems/zipline-1.3.0/zipline/pipeline/factors/statistical.py" startline="38" endline="53" pcid="1300">
    def __new__(cls,
                base_factor,
                target,
                correlation_length,
                mask=NotSpecified):
        if target.ndim == 2 and base_factor.mask is not target.mask:
            raise IncompatibleTerms(term_1=base_factor, term_2=target)

        return super(_RollingCorrelation, cls).__new__(
            cls,
            inputs=[base_factor, target],
            window_length=correlation_length,
            mask=mask,
        )


</source>
<source file="systems/zipline-1.3.0/zipline/pipeline/factors/statistical.py" startline="182" endline="196" pcid="1303">
    def __new__(cls,
                dependent,
                independent,
                regression_length,
                mask=NotSpecified):
        if independent.ndim == 2 and dependent.mask is not independent.mask:
            raise IncompatibleTerms(term_1=dependent, term_2=independent)

        return super(RollingLinearRegression, cls).__new__(
            cls,
            inputs=[dependent, independent],
            window_length=regression_length,
            mask=mask,
        )

</source>
</class>

<class classid="51" nclones="3" nlines="16" similarity="100">
<source file="systems/zipline-1.3.0/zipline/pipeline/factors/statistical.py" startline="296" endline="315" pcid="1306">
    def __new__(cls,
                target,
                returns_length,
                correlation_length,
                mask=NotSpecified):
        # Use the `SingleAsset` filter here because it protects against
        # inputting a non-existent target asset.
        returns = Returns(
            window_length=returns_length,
            mask=(AssetExists() | SingleAsset(asset=target)),
        )
        return super(RollingPearsonOfReturns, cls).__new__(
            cls,
            base_factor=returns,
            target=returns[target],
            correlation_length=correlation_length,
            mask=mask,
        )


</source>
<source file="systems/zipline-1.3.0/zipline/pipeline/factors/statistical.py" startline="346" endline="365" pcid="1307">
    def __new__(cls,
                target,
                returns_length,
                correlation_length,
                mask=NotSpecified):
        # Use the `SingleAsset` filter here because it protects against
        # inputting a non-existent target asset.
        returns = Returns(
            window_length=returns_length,
            mask=(AssetExists() | SingleAsset(asset=target)),
        )
        return super(RollingSpearmanOfReturns, cls).__new__(
            cls,
            base_factor=returns,
            target=returns[target],
            correlation_length=correlation_length,
            mask=mask,
        )


</source>
<source file="systems/zipline-1.3.0/zipline/pipeline/factors/statistical.py" startline="465" endline="484" pcid="1308">
    def __new__(cls,
                target,
                returns_length,
                regression_length,
                mask=NotSpecified):
        # Use the `SingleAsset` filter here because it protects against
        # inputting a non-existent target asset.
        returns = Returns(
            window_length=returns_length,
            mask=(AssetExists() | SingleAsset(asset=target)),
        )
        return super(RollingLinearRegressionOfReturns, cls).__new__(
            cls,
            dependent=returns,
            independent=returns[target],
            regression_length=regression_length,
            mask=mask,
        )


</source>
</class>

<class classid="52" nclones="2" nlines="22" similarity="86">
<source file="systems/zipline-1.3.0/zipline/testing/predicates.py" startline="436" endline="461" pcid="1353">
def assert_dict_equal(result, expected, path=(), msg='', **kwargs):
    _check_sets(
        viewkeys(result),
        viewkeys(expected),
        msg,
        path + ('.%s()' % ('viewkeys' if PY2 else 'keys'),),
        'key',
    )

    failures = []
    for k, (resultv, expectedv) in iteritems(dzip_exact(result, expected)):
        try:
            assert_equal(
                resultv,
                expectedv,
                path=path + ('[%r]' % (k,),),
                msg=msg,
                **kwargs
            )
        except AssertionError as e:
            failures.append(str(e))

    if failures:
        raise AssertionError('\n'.join(failures))


</source>
<source file="systems/zipline-1.3.0/zipline/testing/predicates.py" startline="463" endline="492" pcid="1354">
def asssert_mappingproxy_equal(result, expected, path=(), msg='', **kwargs):
    # mappingproxies compare like dict but shouldn't compare to dicts
    _check_sets(
        set(result),
        set(expected),
        msg,
        path + ('.keys()',),
        'key',
    )

    failures = []
    for k, resultv in iteritems(result):
        # we know this exists because of the _check_sets call above
        expectedv = expected[k]

        try:
            assert_equal(
                resultv,
                expectedv,
                path=path + ('[%r]' % (k,),),
                msg=msg,
                **kwargs
            )
        except AssertionError as e:
            failures.append(str(e))

    if failures:
        raise AssertionError('\n'.join(failures))


</source>
</class>

<class classid="53" nclones="2" nlines="29" similarity="100">
<source file="systems/zipline-1.3.0/zipline/_version.py" startline="63" endline="95" pcid="1374">
def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False):
    assert isinstance(commands, list)
    p = None
    for c in commands:
        try:
            dispcmd = str([c] + args)
            # remember shell=False, so use git.cmd on windows, not just git
            p = subprocess.Popen([c] + args, cwd=cwd, stdout=subprocess.PIPE,
                                 stderr=(subprocess.PIPE if hide_stderr
                                         else None))
            break
        except EnvironmentError:
            e = sys.exc_info()[1]
            if e.errno == errno.ENOENT:
                continue
            if verbose:
                print("unable to run %s" % dispcmd)
                print(e)
            return None
    else:
        if verbose:
            print("unable to find command, tried %s" % (commands,))
        return None
    stdout = p.communicate()[0].strip()
    if sys.version_info[0] >= 3:
        stdout = stdout.decode()
    if p.returncode != 0:
        if verbose:
            print("unable to run %s (error)" % dispcmd)
        return None
    return stdout


</source>
<source file="systems/zipline-1.3.0/versioneer.py" startline="436" endline="466" pcid="1646">
def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False):
    assert isinstance(commands, list)
    p = None
    for c in commands:
        try:
            dispcmd = str([c] + args)
            # remember shell=False, so use git.cmd on windows, not just git
            p = subprocess.Popen([c] + args, cwd=cwd, stdout=subprocess.PIPE,
                                 stderr=(subprocess.PIPE if hide_stderr
                                         else None))
            break
        except EnvironmentError:
            e = sys.exc_info()[1]
            if e.errno == errno.ENOENT:
                continue
            if verbose:
                print("unable to run %s" % dispcmd)
                print(e)
            return None
    else:
        if verbose:
            print("unable to find command, tried %s" % (commands,))
        return None
    stdout = p.communicate()[0].strip()
    if sys.version_info[0] >= 3:
        stdout = stdout.decode()
    if p.returncode != 0:
        if verbose:
            print("unable to run %s (error)" % dispcmd)
        return None
    return stdout
</source>
</class>

<class classid="54" nclones="2" nlines="17" similarity="100">
<source file="systems/zipline-1.3.0/zipline/_version.py" startline="111" endline="133" pcid="1376">
def git_get_keywords(versionfile_abs):
    # the code embedded in _version.py can just fetch the value of these
    # keywords. When used from setup.py, we don't want to import _version.py,
    # so we do it with a regexp instead. This function is not used from
    # _version.py.
    keywords = {}
    try:
        f = open(versionfile_abs, "r")
        for line in f.readlines():
            if line.strip().startswith("git_refnames ="):
                mo = re.search(r'=\s*"(.*)"', line)
                if mo:
                    keywords["refnames"] = mo.group(1)
            if line.strip().startswith("git_full ="):
                mo = re.search(r'=\s*"(.*)"', line)
                if mo:
                    keywords["full"] = mo.group(1)
        f.close()
    except EnvironmentError:
        pass
    return keywords


</source>
<source file="systems/zipline-1.3.0/versioneer.py" startline="931" endline="953" pcid="1647">
def git_get_keywords(versionfile_abs):
    # the code embedded in _version.py can just fetch the value of these
    # keywords. When used from setup.py, we don't want to import _version.py,
    # so we do it with a regexp instead. This function is not used from
    # _version.py.
    keywords = {}
    try:
        f = open(versionfile_abs, "r")
        for line in f.readlines():
            if line.strip().startswith("git_refnames ="):
                mo = re.search(r'=\s*"(.*)"', line)
                if mo:
                    keywords["refnames"] = mo.group(1)
            if line.strip().startswith("git_full ="):
                mo = re.search(r'=\s*"(.*)"', line)
                if mo:
                    keywords["full"] = mo.group(1)
        f.close()
    except EnvironmentError:
        pass
    return keywords


</source>
</class>

<class classid="55" nclones="2" nlines="30" similarity="100">
<source file="systems/zipline-1.3.0/zipline/_version.py" startline="135" endline="178" pcid="1377">
def git_versions_from_keywords(keywords, tag_prefix, verbose):
    if not keywords:
        raise NotThisMethod("no keywords at all, weird")
    refnames = keywords["refnames"].strip()
    if refnames.startswith("$Format"):
        if verbose:
            print("keywords are unexpanded, not using")
        raise NotThisMethod("unexpanded keywords, not a git-archive tarball")
    refs = set([r.strip() for r in refnames.strip("()").split(",")])
    # starting in git-1.8.3, tags are listed as "tag: foo-1.0" instead of
    # just "foo-1.0". If we see a "tag: " prefix, prefer those.
    TAG = "tag: "
    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])
    if not tags:
        # Either we're using git < 1.8.3, or there really are no tags. We use
        # a heuristic: assume all version tags have a digit. The old git %d
        # expansion behaves like git log --decorate=short and strips out the
        # refs/heads/ and refs/tags/ prefixes that would let us distinguish
        # between branches and tags. By ignoring refnames without digits, we
        # filter out many common branch names like "release" and
        # "stabilization", as well as "HEAD" and "master".
        tags = set([r for r in refs if re.search(r'\d', r)])
        if verbose:
            print("discarding '%s', no digits" % ",".join(refs-tags))
    if verbose:
        print("likely tags: %s" % ",".join(sorted(tags)))
    for ref in sorted(tags):
        # sorting will prefer e.g. "2.0" over "2.0rc1"
        if ref.startswith(tag_prefix):
            r = ref[len(tag_prefix):]
            if verbose:
                print("picking %s" % r)
            return {"version": r,
                    "full-revisionid": keywords["full"].strip(),
                    "dirty": False, "error": None
                    }
    # no suitable tags, so version is "0+unknown", but full hex is still there
    if verbose:
        print("no suitable tags, using unknown + full revision id")
    return {"version": "0+unknown",
            "full-revisionid": keywords["full"].strip(),
            "dirty": False, "error": "no suitable tags"}


</source>
<source file="systems/zipline-1.3.0/versioneer.py" startline="955" endline="998" pcid="1648">
def git_versions_from_keywords(keywords, tag_prefix, verbose):
    if not keywords:
        raise NotThisMethod("no keywords at all, weird")
    refnames = keywords["refnames"].strip()
    if refnames.startswith("$Format"):
        if verbose:
            print("keywords are unexpanded, not using")
        raise NotThisMethod("unexpanded keywords, not a git-archive tarball")
    refs = set([r.strip() for r in refnames.strip("()").split(",")])
    # starting in git-1.8.3, tags are listed as "tag: foo-1.0" instead of
    # just "foo-1.0". If we see a "tag: " prefix, prefer those.
    TAG = "tag: "
    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])
    if not tags:
        # Either we're using git < 1.8.3, or there really are no tags. We use
        # a heuristic: assume all version tags have a digit. The old git %d
        # expansion behaves like git log --decorate=short and strips out the
        # refs/heads/ and refs/tags/ prefixes that would let us distinguish
        # between branches and tags. By ignoring refnames without digits, we
        # filter out many common branch names like "release" and
        # "stabilization", as well as "HEAD" and "master".
        tags = set([r for r in refs if re.search(r'\d', r)])
        if verbose:
            print("discarding '%s', no digits" % ",".join(refs-tags))
    if verbose:
        print("likely tags: %s" % ",".join(sorted(tags)))
    for ref in sorted(tags):
        # sorting will prefer e.g. "2.0" over "2.0rc1"
        if ref.startswith(tag_prefix):
            r = ref[len(tag_prefix):]
            if verbose:
                print("picking %s" % r)
            return {"version": r,
                    "full-revisionid": keywords["full"].strip(),
                    "dirty": False, "error": None
                    }
    # no suitable tags, so version is "0+unknown", but full hex is still there
    if verbose:
        print("no suitable tags, using unknown + full revision id")
    return {"version": "0+unknown",
            "full-revisionid": keywords["full"].strip(),
            "dirty": False, "error": "no suitable tags"}


</source>
</class>

<class classid="56" nclones="2" nlines="50" similarity="100">
<source file="systems/zipline-1.3.0/zipline/_version.py" startline="180" endline="260" pcid="1378">
def git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):
    # this runs 'git' from the root of the source tree. This only gets called
    # if the git-archive 'subst' keywords were *not* expanded, and
    # _version.py hasn't already been rewritten with a short version string,
    # meaning we're inside a checked out source tree.

    if not os.path.exists(os.path.join(root, ".git")):
        if verbose:
            print("no .git in %s" % root)
        raise NotThisMethod("no .git directory")

    GITS = ["git"]
    if sys.platform == "win32":
        GITS = ["git.cmd", "git.exe"]
    # if there is a tag, this yields TAG-NUM-gHEX[-dirty]
    # if there are no tags, this yields HEX[-dirty] (no NUM)
    describe_out = run_command(GITS, ["describe", "--tags", "--dirty",
                                      "--always", "--long"],
                               cwd=root)
    # --long was added in git-1.5.5
    if describe_out is None:
        raise NotThisMethod("'git describe' failed")
    describe_out = describe_out.strip()
    full_out = run_command(GITS, ["rev-parse", "HEAD"], cwd=root)
    if full_out is None:
        raise NotThisMethod("'git rev-parse' failed")
    full_out = full_out.strip()

    pieces = {}
    pieces["long"] = full_out
    pieces["short"] = full_out[:7]  # maybe improved later
    pieces["error"] = None

    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]
    # TAG might have hyphens.
    git_describe = describe_out

    # look for -dirty suffix
    dirty = git_describe.endswith("-dirty")
    pieces["dirty"] = dirty
    if dirty:
        git_describe = git_describe[:git_describe.rindex("-dirty")]

    # now we have TAG-NUM-gHEX or HEX

    if "-" in git_describe:
        # TAG-NUM-gHEX
        mo = re.search(r'^(.+)-(\d+)-g([0-9a-f]+)$', git_describe)
        if not mo:
            # unparseable. Maybe git-describe is misbehaving?
            pieces["error"] = ("unable to parse git-describe output: '%s'"
                               % describe_out)
            return pieces

        # tag
        full_tag = mo.group(1)
        if not full_tag.startswith(tag_prefix):
            if verbose:
                fmt = "tag '%s' doesn't start with prefix '%s'"
                print(fmt % (full_tag, tag_prefix))
            pieces["error"] = ("tag '%s' doesn't start with prefix '%s'"
                               % (full_tag, tag_prefix))
            return pieces
        pieces["closest-tag"] = full_tag[len(tag_prefix):]

        # distance: number of commits since tag
        pieces["distance"] = int(mo.group(2))

        # commit: short hex revision ID
        pieces["short"] = mo.group(3)

    else:
        # HEX: no tags
        pieces["closest-tag"] = None
        count_out = run_command(GITS, ["rev-list", "HEAD", "--count"],
                                cwd=root)
        pieces["distance"] = int(count_out)  # total number of commits

    return pieces


</source>
<source file="systems/zipline-1.3.0/versioneer.py" startline="1000" endline="1080" pcid="1649">
def git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):
    # this runs 'git' from the root of the source tree. This only gets called
    # if the git-archive 'subst' keywords were *not* expanded, and
    # _version.py hasn't already been rewritten with a short version string,
    # meaning we're inside a checked out source tree.

    if not os.path.exists(os.path.join(root, ".git")):
        if verbose:
            print("no .git in %s" % root)
        raise NotThisMethod("no .git directory")

    GITS = ["git"]
    if sys.platform == "win32":
        GITS = ["git.cmd", "git.exe"]
    # if there is a tag, this yields TAG-NUM-gHEX[-dirty]
    # if there are no tags, this yields HEX[-dirty] (no NUM)
    describe_out = run_command(GITS, ["describe", "--tags", "--dirty",
                                      "--always", "--long"],
                               cwd=root)
    # --long was added in git-1.5.5
    if describe_out is None:
        raise NotThisMethod("'git describe' failed")
    describe_out = describe_out.strip()
    full_out = run_command(GITS, ["rev-parse", "HEAD"], cwd=root)
    if full_out is None:
        raise NotThisMethod("'git rev-parse' failed")
    full_out = full_out.strip()

    pieces = {}
    pieces["long"] = full_out
    pieces["short"] = full_out[:7]  # maybe improved later
    pieces["error"] = None

    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]
    # TAG might have hyphens.
    git_describe = describe_out

    # look for -dirty suffix
    dirty = git_describe.endswith("-dirty")
    pieces["dirty"] = dirty
    if dirty:
        git_describe = git_describe[:git_describe.rindex("-dirty")]

    # now we have TAG-NUM-gHEX or HEX

    if "-" in git_describe:
        # TAG-NUM-gHEX
        mo = re.search(r'^(.+)-(\d+)-g([0-9a-f]+)$', git_describe)
        if not mo:
            # unparseable. Maybe git-describe is misbehaving?
            pieces["error"] = ("unable to parse git-describe output: '%s'"
                               % describe_out)
            return pieces

        # tag
        full_tag = mo.group(1)
        if not full_tag.startswith(tag_prefix):
            if verbose:
                fmt = "tag '%s' doesn't start with prefix '%s'"
                print(fmt % (full_tag, tag_prefix))
            pieces["error"] = ("tag '%s' doesn't start with prefix '%s'"
                               % (full_tag, tag_prefix))
            return pieces
        pieces["closest-tag"] = full_tag[len(tag_prefix):]

        # distance: number of commits since tag
        pieces["distance"] = int(mo.group(2))

        # commit: short hex revision ID
        pieces["short"] = mo.group(3)

    else:
        # HEX: no tags
        pieces["closest-tag"] = None
        count_out = run_command(GITS, ["rev-list", "HEAD", "--count"],
                                cwd=root)
        pieces["distance"] = int(count_out)  # total number of commits

    return pieces


</source>
</class>

<class classid="57" nclones="6" nlines="13" similarity="71">
<source file="systems/zipline-1.3.0/zipline/_version.py" startline="267" endline="290" pcid="1380">
def render_pep440(pieces):
    # now build up version string, with post-release "local version
    # identifier". Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you
    # get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty

    # exceptions:
    # 1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]

    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += plus_or_dot(pieces)
            rendered += "%d.g%s" % (pieces["distance"], pieces["short"])
            if pieces["dirty"]:
                rendered += ".dirty"
    else:
        # exception #1
        rendered = "0+untagged.%d.g%s" % (pieces["distance"],
                                          pieces["short"])
        if pieces["dirty"]:
            rendered += ".dirty"
    return rendered


</source>
<source file="systems/zipline-1.3.0/versioneer.py" startline="1175" endline="1198" pcid="1655">
def render_pep440(pieces):
    # now build up version string, with post-release "local version
    # identifier". Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you
    # get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty

    # exceptions:
    # 1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]

    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += plus_or_dot(pieces)
            rendered += "%d.g%s" % (pieces["distance"], pieces["short"])
            if pieces["dirty"]:
                rendered += ".dirty"
    else:
        # exception #1
        rendered = "0+untagged.%d.g%s" % (pieces["distance"],
                                          pieces["short"])
        if pieces["dirty"]:
            rendered += ".dirty"
    return rendered


</source>
<source file="systems/zipline-1.3.0/versioneer.py" startline="1241" endline="1260" pcid="1658">
def render_pep440_old(pieces):
    # TAG[.postDISTANCE[.dev0]] . The ".dev0" means dirty.

    # exceptions:
    # 1: no tags. 0.postDISTANCE[.dev0]

    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += ".post%d" % pieces["distance"]
            if pieces["dirty"]:
                rendered += ".dev0"
    else:
        # exception #1
        rendered = "0.post%d" % pieces["distance"]
        if pieces["dirty"]:
            rendered += ".dev0"
    return rendered


</source>
<source file="systems/zipline-1.3.0/zipline/_version.py" startline="333" endline="352" pcid="1383">
def render_pep440_old(pieces):
    # TAG[.postDISTANCE[.dev0]] . The ".dev0" means dirty.

    # exceptions:
    # 1: no tags. 0.postDISTANCE[.dev0]

    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += ".post%d" % pieces["distance"]
            if pieces["dirty"]:
                rendered += ".dev0"
    else:
        # exception #1
        rendered = "0.post%d" % pieces["distance"]
        if pieces["dirty"]:
            rendered += ".dev0"
    return rendered


</source>
<source file="systems/zipline-1.3.0/versioneer.py" startline="1215" endline="1240" pcid="1657">
def render_pep440_post(pieces):
    # TAG[.postDISTANCE[.dev0]+gHEX] . The ".dev0" means dirty. Note that
    # .dev0 sorts backwards (a dirty tree will appear "older" than the
    # corresponding clean one), but you shouldn't be releasing software with
    # -dirty anyways.

    # exceptions:
    # 1: no tags. 0.postDISTANCE[.dev0]

    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += ".post%d" % pieces["distance"]
            if pieces["dirty"]:
                rendered += ".dev0"
            rendered += plus_or_dot(pieces)
            rendered += "g%s" % pieces["short"]
    else:
        # exception #1
        rendered = "0.post%d" % pieces["distance"]
        if pieces["dirty"]:
            rendered += ".dev0"
        rendered += "+g%s" % pieces["short"]
    return rendered


</source>
<source file="systems/zipline-1.3.0/zipline/_version.py" startline="307" endline="332" pcid="1382">
def render_pep440_post(pieces):
    # TAG[.postDISTANCE[.dev0]+gHEX] . The ".dev0" means dirty. Note that
    # .dev0 sorts backwards (a dirty tree will appear "older" than the
    # corresponding clean one), but you shouldn't be releasing software with
    # -dirty anyways.

    # exceptions:
    # 1: no tags. 0.postDISTANCE[.dev0]

    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += ".post%d" % pieces["distance"]
            if pieces["dirty"]:
                rendered += ".dev0"
            rendered += plus_or_dot(pieces)
            rendered += "g%s" % pieces["short"]
    else:
        # exception #1
        rendered = "0.post%d" % pieces["distance"]
        if pieces["dirty"]:
            rendered += ".dev0"
        rendered += "+g%s" % pieces["short"]
    return rendered


</source>
</class>

<class classid="58" nclones="2" nlines="10" similarity="100">
<source file="systems/zipline-1.3.0/zipline/_version.py" startline="353" endline="371" pcid="1384">
def render_git_describe(pieces):
    # TAG[-DISTANCE-gHEX][-dirty], like 'git describe --tags --dirty
    # --always'

    # exceptions:
    # 1: no tags. HEX[-dirty]  (note: no 'g' prefix)

    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"]:
            rendered += "-%d-g%s" % (pieces["distance"], pieces["short"])
    else:
        # exception #1
        rendered = pieces["short"]
    if pieces["dirty"]:
        rendered += "-dirty"
    return rendered


</source>
<source file="systems/zipline-1.3.0/versioneer.py" startline="1261" endline="1279" pcid="1659">
def render_git_describe(pieces):
    # TAG[-DISTANCE-gHEX][-dirty], like 'git describe --tags --dirty
    # --always'

    # exceptions:
    # 1: no tags. HEX[-dirty]  (note: no 'g' prefix)

    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"]:
            rendered += "-%d-g%s" % (pieces["distance"], pieces["short"])
    else:
        # exception #1
        rendered = pieces["short"]
    if pieces["dirty"]:
        rendered += "-dirty"
    return rendered


</source>
</class>

<class classid="59" nclones="2" nlines="24" similarity="100">
<source file="systems/zipline-1.3.0/zipline/_version.py" startline="390" endline="418" pcid="1386">
def render(pieces, style):
    if pieces["error"]:
        return {"version": "unknown",
                "full-revisionid": pieces.get("long"),
                "dirty": None,
                "error": pieces["error"]}

    if not style or style == "default":
        style = "pep440"  # the default

    if style == "pep440":
        rendered = render_pep440(pieces)
    elif style == "pep440-pre":
        rendered = render_pep440_pre(pieces)
    elif style == "pep440-post":
        rendered = render_pep440_post(pieces)
    elif style == "pep440-old":
        rendered = render_pep440_old(pieces)
    elif style == "git-describe":
        rendered = render_git_describe(pieces)
    elif style == "git-describe-long":
        rendered = render_git_describe_long(pieces)
    else:
        raise ValueError("unknown style '%s'" % style)

    return {"version": rendered, "full-revisionid": pieces["long"],
            "dirty": pieces["dirty"], "error": None}


</source>
<source file="systems/zipline-1.3.0/versioneer.py" startline="1298" endline="1326" pcid="1661">
def render(pieces, style):
    if pieces["error"]:
        return {"version": "unknown",
                "full-revisionid": pieces.get("long"),
                "dirty": None,
                "error": pieces["error"]}

    if not style or style == "default":
        style = "pep440"  # the default

    if style == "pep440":
        rendered = render_pep440(pieces)
    elif style == "pep440-pre":
        rendered = render_pep440_pre(pieces)
    elif style == "pep440-post":
        rendered = render_pep440_post(pieces)
    elif style == "pep440-old":
        rendered = render_pep440_old(pieces)
    elif style == "git-describe":
        rendered = render_git_describe(pieces)
    elif style == "git-describe-long":
        rendered = render_git_describe_long(pieces)
    else:
        raise ValueError("unknown style '%s'" % style)

    return {"version": rendered, "full-revisionid": pieces["long"],
            "dirty": pieces["dirty"], "error": None}


</source>
</class>

<class classid="60" nclones="2" nlines="18" similarity="100">
<source file="systems/zipline-1.3.0/zipline/finance/controls.py" startline="174" endline="198" pcid="1513">
    def __init__(self, on_error, asset=None, max_shares=None,
                 max_notional=None):
        super(MaxOrderSize, self).__init__(on_error,
                                           asset=asset,
                                           max_shares=max_shares,
                                           max_notional=max_notional)
        self.asset = asset
        self.max_shares = max_shares
        self.max_notional = max_notional

        if max_shares is None and max_notional is None:
            raise ValueError(
                "Must supply at least one of max_shares and max_notional"
            )

        if max_shares and max_shares < 0:
            raise ValueError(
                "max_shares cannot be negative."
            )

        if max_notional and max_notional < 0:
            raise ValueError(
                "max_notional must be positive."
            )

</source>
<source file="systems/zipline-1.3.0/zipline/finance/controls.py" startline="232" endline="256" pcid="1515">
    def __init__(self, on_error, asset=None, max_shares=None,
                 max_notional=None):
        super(MaxPositionSize, self).__init__(on_error,
                                              asset=asset,
                                              max_shares=max_shares,
                                              max_notional=max_notional)
        self.asset = asset
        self.max_shares = max_shares
        self.max_notional = max_notional

        if max_shares is None and max_notional is None:
            raise ValueError(
                "Must supply at least one of max_shares and max_notional"
            )

        if max_shares and max_shares < 0:
            raise ValueError(
                "max_shares cannot be negative."
            )

        if max_notional and max_notional < 0:
            raise ValueError(
                "max_notional must be positive."
            )

</source>
</class>

<class classid="61" nclones="2" nlines="18" similarity="75">
<source file="systems/zipline-1.3.0/zipline/finance/controls.py" startline="199" endline="225" pcid="1514">
    def validate(self,
                 asset,
                 amount,
                 portfolio,
                 algo_datetime,
                 algo_current_data):
        """
        Fail if the magnitude of the given order exceeds either self.max_shares
        or self.max_notional.
        """

        if self.asset is not None and self.asset != asset:
            return

        if self.max_shares is not None and abs(amount) > self.max_shares:
            self.handle_violation(asset, amount, algo_datetime)

        current_asset_price = algo_current_data.current(asset, "price")
        order_value = amount * current_asset_price

        too_much_value = (self.max_notional is not None and
                          abs(order_value) > self.max_notional)

        if too_much_value:
            self.handle_violation(asset, amount, algo_datetime)


</source>
<source file="systems/zipline-1.3.0/zipline/finance/controls.py" startline="257" endline="289" pcid="1516">
    def validate(self,
                 asset,
                 amount,
                 portfolio,
                 algo_datetime,
                 algo_current_data):
        """
        Fail if the given order would cause the magnitude of our position to be
        greater in shares than self.max_shares or greater in dollar value than
        self.max_notional.
        """

        if self.asset is not None and self.asset != asset:
            return

        current_share_count = portfolio.positions[asset].amount
        shares_post_order = current_share_count + amount

        too_many_shares = (self.max_shares is not None and
                           abs(shares_post_order) > self.max_shares)
        if too_many_shares:
            self.handle_violation(asset, amount, algo_datetime)

        current_price = algo_current_data.current(asset, "price")
        value_post_order = shares_post_order * current_price

        too_much_value = (self.max_notional is not None and
                          abs(value_post_order) > self.max_notional)

        if too_much_value:
            self.handle_violation(asset, amount, algo_datetime)


</source>
</class>

<class classid="62" nclones="3" nlines="12" similarity="71">
<source file="systems/zipline-1.3.0/zipline/finance/blotter/simulation_blotter.py" startline="156" endline="176" pcid="1532">
    def cancel(self, order_id, relay_status=True):
        if order_id not in self.orders:
            return

        cur_order = self.orders[order_id]

        if cur_order.open:
            order_list = self.open_orders[cur_order.asset]
            if cur_order in order_list:
                order_list.remove(cur_order)

            if cur_order in self.new_orders:
                self.new_orders.remove(cur_order)
            cur_order.cancel()
            cur_order.dt = self.current_dt

            if relay_status:
                # we want this order's new status to be relayed out
                # along with newly placed orders.
                self.new_orders.append(cur_order)

</source>
<source file="systems/zipline-1.3.0/zipline/finance/blotter/simulation_blotter.py" startline="242" endline="265" pcid="1535">
    def reject(self, order_id, reason=''):
        """
        Mark the given order as 'rejected', which is functionally similar to
        cancelled. The distinction is that rejections are involuntary (and
        usually include a message from a broker indicating why the order was
        rejected) while cancels are typically user-driven.
        """
        if order_id not in self.orders:
            return

        cur_order = self.orders[order_id]

        order_list = self.open_orders[cur_order.asset]
        if cur_order in order_list:
            order_list.remove(cur_order)

        if cur_order in self.new_orders:
            self.new_orders.remove(cur_order)
        cur_order.reject(reason=reason)
        cur_order.dt = self.current_dt
        # we want this order's new status to be relayed out
        # along with newly placed orders.
        self.new_orders.append(cur_order)

</source>
<source file="systems/zipline-1.3.0/zipline/finance/blotter/simulation_blotter.py" startline="266" endline="284" pcid="1536">
    def hold(self, order_id, reason=''):
        """
        Mark the order with order_id as 'held'. Held is functionally similar
        to 'open'. When a fill (full or partial) arrives, the status
        will automatically change back to open/filled as necessary.
        """
        if order_id not in self.orders:
            return

        cur_order = self.orders[order_id]
        if cur_order.open:
            if cur_order in self.new_orders:
                self.new_orders.remove(cur_order)
            cur_order.hold(reason=reason)
            cur_order.dt = self.current_dt
            # we want this order's new status to be relayed out
            # along with newly placed orders.
            self.new_orders.append(cur_order)

</source>
</class>

<class classid="63" nclones="2" nlines="29" similarity="80">
<source file="systems/zipline-1.3.0/zipline/finance/metrics/tracker.py" startline="204" endline="244" pcid="1617">
    def handle_minute_close(self, dt, data_portal):
        """
        Handles the close of the given minute in minute emission.

        Parameters
        ----------
        dt : Timestamp
            The minute that is ending

        Returns
        -------
        A minute perf packet.
        """
        self.sync_last_sale_prices(dt, data_portal)

        packet = {
            'period_start': self._first_session,
            'period_end': self._last_session,
            'capital_base': self._capital_base,
            'minute_perf': {
                'period_open': self._market_open,
                'period_close': dt,
            },
            'cumulative_perf': {
                'period_open': self._first_session,
                'period_close': self._last_session,
            },
            'progress': self._progress(self),
            'cumulative_risk_metrics': {},
        }
        ledger = self._ledger
        ledger.end_of_bar(self._session_count)
        self.end_of_bar(
            packet,
            ledger,
            dt,
            self._session_count,
            data_portal,
        )
        return packet

</source>
<source file="systems/zipline-1.3.0/zipline/finance/metrics/tracker.py" startline="277" endline="329" pcid="1619">
    def handle_market_close(self, dt, data_portal):
        """Handles the close of the given day.

        Parameters
        ----------
        dt : Timestamp
            The most recently completed simulation datetime.
        data_portal : DataPortal
            The current data portal.

        Returns
        -------
        A daily perf packet.
        """
        completed_session = self._current_session

        if self.emission_rate == 'daily':
            # this method is called for both minutely and daily emissions, but
            # this chunk of code here only applies for daily emissions. (since
            # it's done every minute, elsewhere, for minutely emission).
            self.sync_last_sale_prices(dt, data_portal)

        session_ix = self._session_count
        # increment the day counter before we move markers forward.
        self._session_count += 1

        packet = {
            'period_start': self._first_session,
            'period_end': self._last_session,
            'capital_base': self._capital_base,
            'daily_perf': {
                'period_open': self._market_open,
                'period_close': dt,
            },
            'cumulative_perf': {
                'period_open': self._first_session,
                'period_close': self._last_session,
            },
            'progress': self._progress(self),
            'cumulative_risk_metrics': {},
        }
        ledger = self._ledger
        ledger.end_of_session(session_ix)
        self.end_of_session(
            packet,
            ledger,
            completed_session,
            session_ix,
            data_portal,
        )

        return packet

</source>
</class>

</clones>

<html>
<head>
    <title>NiCad6 Clone Report</title>
    <style type="text/css">
        body {font-family:sans-serif;}
        table {background-color:white; border:0px; padding:0px; border-spacing:4px; width:auto; margin-left:30px; margin-right:auto;}
        td {background-color:rgba(192,212,238,0.8); border:0px; padding:8px; width:auto; vertical-align:top; border-radius:8px}
        pre {background-color:white; padding:4px;}
        a {color:darkblue;}
    </style>
</head>
<body>
<h2>NiCad6 Clone Report</h2>
<table>
<tr style="font-size:14pt">
<td><b>System:</b> &nbsp; texar-0.2.4</td>
<td><b>Clone pairs:</b> &nbsp; 27</td>
<td><b>Clone classes:</b> &nbsp; 18</td>
</tr>
<tr style="font-size:12pt">
<td style="background-color:white">Clone type: &nbsp; 2</td>
<td style="background-color:white">Granularity: &nbsp; functions-blind</td>
<td style="background-color:white">Max diff threshold: &nbsp; 0%</td>
<td style="background-color:white">Clone size: &nbsp; 10 - 2500 lines</td>
<td style="background-color:white">Total functions-blind: &nbsp; 1305</td>
</tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 1:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag74')" href="javascript:;">
texar-0.2.4/texar/tf/agents/seq_pg_agent_test.py: 34-45
</a>
<div class="mid" id="frag74" style="display:none"><pre>
    def setUp(self):
        tf.test.TestCase.setUp(self)
        self._vocab_size = 4
        self._max_time = 8
        self._batch_size = 16
        self._emb_dim = 20
        self._inputs = tf.random_uniform(
            [self._batch_size, self._max_time, self._emb_dim],
            maxval=1., dtype=tf.float32)
        self._embedding = tf.random_uniform(
            [self._vocab_size, self._emb_dim], maxval=1., dtype=tf.float32)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag758')" href="javascript:;">
texar-0.2.4/texar/tf/modules/decoders/rnn_decoders_test.py: 29-40
</a>
<div class="mid" id="frag758" style="display:none"><pre>
    def setUp(self):
        tf.test.TestCase.setUp(self)
        self._vocab_size = 4
        self._max_time = 8
        self._batch_size = 16
        self._emb_dim = 20
        self._inputs = tf.random_uniform(
            [self._batch_size, self._max_time, self._emb_dim],
            maxval=1., dtype=tf.float32)
        self._embedding = tf.random_uniform(
            [self._vocab_size, self._emb_dim], maxval=1., dtype=tf.float32)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 2:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag332')" href="javascript:;">
texar-0.2.4/texar/tf/data/data/scalar_data.py: 192-205
</a>
<div class="mid" id="frag332" style="display:none"><pre>
    def _process_dataset(self, dataset, hparams, data_spec):
        chained_tran, data_spec = self._make_processor(
            hparams["dataset"], data_spec,
            name_prefix=hparams["dataset"]["data_name"])
        num_parallel_calls = hparams["num_parallel_calls"]
        dataset = dataset.map(
            lambda *args: chained_tran(dsutils.maybe_tuple(args)),
            num_parallel_calls=num_parallel_calls)

        # Truncates data count
        dataset = dataset.take(hparams["max_dataset_size"])

        return dataset, data_spec

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag349')" href="javascript:;">
texar-0.2.4/texar/tf/data/data/tfrecord_data.py: 341-354
</a>
<div class="mid" id="frag349" style="display:none"><pre>
    def _process_dataset(self, dataset, hparams, data_spec):
        chained_tran, data_spec = self._make_processor(
            hparams["dataset"], data_spec,
            name_prefix=hparams["dataset"]["data_name"])
        num_parallel_calls = hparams["num_parallel_calls"]
        dataset = dataset.map(
            lambda *args: chained_tran(dsutils.maybe_tuple(args)),
            num_parallel_calls=num_parallel_calls)

        # Truncates data count
        dataset = dataset.take(hparams["max_dataset_size"])

        return dataset, data_spec

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 3:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag540')" href="javascript:;">
texar-0.2.4/texar/tf/core/layers_test.py: 207-224
</a>
<div class="mid" id="frag540" style="display:none"><pre>
    def test_max_reduce_pooling_layer(self):
        """Tests :class:`texar.tf.core.MaxReducePooling1D`.
        """
        pool_layer = layers.MaxReducePooling1D()

        inputs = tf.random_uniform(
            [self._batch_size, self._seq_length, self._emb_dim])
        output_shape = pool_layer.compute_output_shape(inputs.get_shape())
        output = pool_layer(inputs)
        output_reduce = tf.reduce_max(inputs, axis=1)
        self.assertEqual(output.get_shape(), output_shape)
        self.assertEqual(output.get_shape(), [self._batch_size, self._emb_dim])

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            output_, output_reduce_ = sess.run([output, output_reduce])
            np.testing.assert_array_equal(output_, output_reduce_)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag541')" href="javascript:;">
texar-0.2.4/texar/tf/core/layers_test.py: 225-243
</a>
<div class="mid" id="frag541" style="display:none"><pre>
    def test_average_reduce_pooling_layer(self):
        """Tests :class:`texar.tf.core.AverageReducePooling1D`.
        """
        pool_layer = layers.AverageReducePooling1D()

        inputs = tf.random_uniform(
            [self._batch_size, self._seq_length, self._emb_dim])
        output_shape = pool_layer.compute_output_shape(inputs.get_shape())
        output = pool_layer(inputs)
        output_reduce = tf.reduce_mean(inputs, axis=1)
        self.assertEqual(output.get_shape(), output_shape)
        self.assertEqual(output.get_shape(), [self._batch_size, self._emb_dim])

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            output_, output_reduce_ = sess.run([output, output_reduce])
            np.testing.assert_array_equal(output_, output_reduce_)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 4:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag760')" href="javascript:;">
texar-0.2.4/texar/tf/modules/decoders/rnn_decoders_test.py: 67-81
</a>
<div class="mid" id="frag760" style="display:none"><pre>
    def test_output_layer(self):
        decoder = BasicRNNDecoder(vocab_size=self._vocab_size,
                                  output_layer=None)
        self.assertIsInstance(decoder, BasicRNNDecoder)

        decoder = BasicRNNDecoder(output_layer=tf.identity)
        self.assertIsInstance(decoder, BasicRNNDecoder)

        tensor = tf.random_uniform(
            [self._emb_dim, self._vocab_size], maxval=1, dtype=tf.float32
        )
        decoder = BasicRNNDecoder(output_layer=tensor)
        self.assertIsInstance(decoder, BasicRNNDecoder)
        self.assertEqual(decoder.vocab_size, self._vocab_size)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag793')" href="javascript:;">
texar-0.2.4/texar/tf/modules/decoders/transformer_decoders_test.py: 64-78
</a>
<div class="mid" id="frag793" style="display:none"><pre>
    def test_output_layer(self):
        decoder = TransformerDecoder(vocab_size=self._vocab_size,
                                     output_layer=None)
        self.assertIsInstance(decoder, TransformerDecoder)

        decoder = TransformerDecoder(output_layer=tf.identity)
        self.assertIsInstance(decoder, TransformerDecoder)

        tensor = tf.random_uniform(
            [self._emb_dim, self._vocab_size], maxval=1, dtype=tf.float32
        )
        decoder = TransformerDecoder(output_layer=tensor)
        self.assertIsInstance(decoder, TransformerDecoder)
        self.assertEqual(decoder.vocab_size, self._vocab_size)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 5:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag795')" href="javascript:;">
texar-0.2.4/texar/tf/modules/decoders/transformer_decoders_test.py: 104-126
</a>
<div class="mid" id="frag795" style="display:none"><pre>
    def test_decode_infer_greedy(self):
        """Tests train_greedy
        """
        decoder = TransformerDecoder(
            vocab_size=self._vocab_size,
            output_layer=self._output_layer
        )
        helper = tx_helper.GreedyEmbeddingHelper(
            self._embedding_fn, self._start_tokens, self._end_token)

        outputs, length = decoder(
            memory=self._memory,
            memory_sequence_length=self._memory_sequence_length,
            memory_attention_bias=None,
            inputs=None,
            helper=helper,
            max_decoding_length=self._max_decode_len,
            mode=tf.estimator.ModeKeys.PREDICT)
        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            outputs_ = sess.run(outputs)
            self.assertIsInstance(outputs_, TransformerDecoderOutput)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag797')" href="javascript:;">
texar-0.2.4/texar/tf/modules/decoders/transformer_decoders_test.py: 154-176
</a>
<div class="mid" id="frag797" style="display:none"><pre>
    def test_decode_infer_sample(self):
        """Tests infer_sample
        """
        decoder = TransformerDecoder(
            vocab_size=self._vocab_size,
            output_layer=self._output_layer
        )
        helper = tx_helper.SampleEmbeddingHelper(
            self._embedding_fn, self._start_tokens, self._end_token)

        outputs, length = decoder(
            memory=self._memory,
            memory_sequence_length=self._memory_sequence_length,
            memory_attention_bias=None,
            inputs=None,
            helper=helper,
            max_decoding_length=self._max_decode_len,
            mode=tf.estimator.ModeKeys.PREDICT)
        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            outputs_ = sess.run(outputs)
            self.assertIsInstance(outputs_, TransformerDecoderOutput)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 6:</b> &nbsp; 2 fragments, nominal size 33 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag811')" href="javascript:;">
texar-0.2.4/texar/tf/modules/encoders/conv_encoders_test.py: 21-65
</a>
<div class="mid" id="frag811" style="display:none"><pre>
    def test_encode(self):
        """Tests encode.
        """
        encoder_1 = Conv1DEncoder()
        self.assertEqual(len(encoder_1.layers), 4)
        self.assertTrue(isinstance(encoder_1.layer_by_name("conv_pool_1"),
                                   tx.core.MergeLayer))
        for layer in encoder_1.layers[0].layers:
            self.assertTrue(isinstance(layer, tx.core.SequentialLayer))

        inputs_1 = tf.ones([64, 16, 300], tf.float32)
        outputs_1 = encoder_1(inputs_1)
        self.assertEqual(outputs_1.shape, [64, 128])

        hparams = {
            # Conv layers
            "num_conv_layers": 2,
            "filters": 128,
            "kernel_size": [[3, 4, 5], 4],
            "other_conv_kwargs": {"padding": "same"},
            # Pooling layers
            "pooling": "AveragePooling",
            "pool_size": 2,
            "pool_strides": 1,
            # Dense layers
            "num_dense_layers": 3,
            "dense_size": [128, 128, 10],
            "dense_activation": "relu",
            "other_dense_kwargs": {"use_bias": False},
            # Dropout
            "dropout_conv": [0, 1, 2],
            "dropout_dense": 2
        }
        encoder_2 = Conv1DEncoder(hparams)
        # nlayers = nconv-pool + nconv + npool + ndense + ndropout + flatten
        self.assertEqual(len(encoder_2.layers), 1 + 1 + 1 + 3 + 4 + 1)
        self.assertTrue(isinstance(encoder_2.layer_by_name("conv_pool_1"),
                                   tx.core.MergeLayer))
        for layer in encoder_2.layers[1].layers:
            self.assertTrue(isinstance(layer, tx.core.SequentialLayer))

        inputs_2 = tf.ones([64, 16, 300], tf.float32)
        outputs_2 = encoder_2(inputs_2)
        self.assertEqual(outputs_2.shape, [64, 10])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1089')" href="javascript:;">
texar-0.2.4/texar/tf/modules/networks/conv_networks_test.py: 21-65
</a>
<div class="mid" id="frag1089" style="display:none"><pre>
    def test_feedforward(self):
        """Tests feed forward.
        """
        network_1 = Conv1DNetwork()
        self.assertEqual(len(network_1.layers), 4)
        self.assertTrue(isinstance(network_1.layer_by_name("conv_pool_1"),
                                   tx.core.MergeLayer))
        for layer in network_1.layers[0].layers:
            self.assertTrue(isinstance(layer, tx.core.SequentialLayer))

        inputs_1 = tf.ones([64, 16, 300], tf.float32)
        outputs_1 = network_1(inputs_1)
        self.assertEqual(outputs_1.shape, [64, 128])

        hparams = {
            # Conv layers
            "num_conv_layers": 2,
            "filters": 128,
            "kernel_size": [[3, 4, 5], 4],
            "other_conv_kwargs": {"padding": "same"},
            # Pooling layers
            "pooling": "AveragePooling",
            "pool_size": 2,
            "pool_strides": 1,
            # Dense layers
            "num_dense_layers": 3,
            "dense_size": [128, 128, 10],
            "dense_activation": "relu",
            "other_dense_kwargs": {"use_bias": False},
            # Dropout
            "dropout_conv": [0, 1, 2],
            "dropout_dense": 2
        }
        network_2 = Conv1DNetwork(hparams)
        # nlayers = nconv-pool + nconv + npool + ndense + ndropout + flatten
        self.assertEqual(len(network_2.layers), 1 + 1 + 1 + 3 + 4 + 1)
        self.assertTrue(isinstance(network_2.layer_by_name("conv_pool_1"),
                                   tx.core.MergeLayer))
        for layer in network_2.layers[1].layers:
            self.assertTrue(isinstance(layer, tx.core.SequentialLayer))

        inputs_2 = tf.ones([64, 16, 300], tf.float32)
        outputs_2 = network_2(inputs_2)
        self.assertEqual(outputs_2.shape, [64, 10])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 7:</b> &nbsp; 2 fragments, nominal size 35 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag812')" href="javascript:;">
texar-0.2.4/texar/tf/modules/encoders/conv_encoders_test.py: 66-115
</a>
<div class="mid" id="frag812" style="display:none"><pre>
    def test_unknown_seq_length(self):
        """Tests use of pooling layer when the seq_length dimension of inputs
        is `None`.
        """
        encoder_1 = Conv1DEncoder()
        inputs_1 = tf.placeholder(tf.float32, [64, None, 300])
        outputs_1 = encoder_1(inputs_1)
        self.assertEqual(outputs_1.shape, [64, 128])

        hparams = {
            # Conv layers
            "num_conv_layers": 2,
            "filters": 128,
            "kernel_size": [[3, 4, 5], 4],
            # Pooling layers
            "pooling": "AveragePooling",
            "pool_size": [2, None],
            # Dense layers
            "num_dense_layers": 1,
            "dense_size": 10,
        }
        encoder = Conv1DEncoder(hparams)
        # nlayers = nconv-pool + nconv + npool + ndense + ndropout + flatten
        self.assertEqual(len(encoder.layers), 1 + 1 + 1 + 1 + 1 + 1)
        self.assertTrue(isinstance(encoder.layer_by_name('pool_2'),
                                   tx.core.AverageReducePooling1D))

        inputs = tf.placeholder(tf.float32, [64, None, 300])
        outputs = encoder(inputs)
        self.assertEqual(outputs.shape, [64, 10])

        hparams_2 = {
            # Conv layers
            "num_conv_layers": 1,
            "filters": 128,
            "kernel_size": 4,
            "other_conv_kwargs": {'data_format': 'channels_first'},
            # Pooling layers
            "pooling": "MaxPooling",
            "other_pool_kwargs": {'data_format': 'channels_first'},
            # Dense layers
            "num_dense_layers": 1,
            "dense_size": 10,
        }
        encoder_2 = Conv1DEncoder(hparams_2)
        inputs_2 = tf.placeholder(tf.float32, [64, 300, None])
        outputs_2 = encoder_2(inputs_2)
        self.assertEqual(outputs_2.shape, [64, 10])


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1090')" href="javascript:;">
texar-0.2.4/texar/tf/modules/networks/conv_networks_test.py: 66-114
</a>
<div class="mid" id="frag1090" style="display:none"><pre>
    def test_unknown_seq_length(self):
        """Tests use of pooling layer when the seq_length dimension of inputs
        is `None`.
        """
        network_1 = Conv1DNetwork()
        inputs_1 = tf.placeholder(tf.float32, [64, None, 300])
        outputs_1 = network_1(inputs_1)
        self.assertEqual(outputs_1.shape, [64, 128])

        hparams = {
            # Conv layers
            "num_conv_layers": 2,
            "filters": 128,
            "kernel_size": [[3, 4, 5], 4],
            # Pooling layers
            "pooling": "AveragePooling",
            "pool_size": [2, None],
            # Dense layers
            "num_dense_layers": 1,
            "dense_size": 10,
        }
        network = Conv1DNetwork(hparams)
        # nlayers = nconv-pool + nconv + npool + ndense + ndropout + flatten
        self.assertEqual(len(network.layers), 1 + 1 + 1 + 1 + 1 + 1)
        self.assertTrue(isinstance(network.layer_by_name('pool_2'),
                                   tx.core.AverageReducePooling1D))

        inputs = tf.placeholder(tf.float32, [64, None, 300])
        outputs = network(inputs)
        self.assertEqual(outputs.shape, [64, 10])

        hparams_2 = {
            # Conv layers
            "num_conv_layers": 1,
            "filters": 128,
            "kernel_size": 4,
            "other_conv_kwargs": {'data_format': 'channels_first'},
            # Pooling layers
            "pooling": "MaxPooling",
            "other_pool_kwargs": {'data_format': 'channels_first'},
            # Dense layers
            "num_dense_layers": 1,
            "dense_size": 10,
        }
        network_2 = Conv1DNetwork(hparams_2)
        inputs_2 = tf.placeholder(tf.float32, [64, 300, None])
        outputs_2 = network_2(inputs_2)
        self.assertEqual(outputs_2.shape, [64, 10])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 8:</b> &nbsp; 2 fragments, nominal size 23 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag921')" href="javascript:;">
texar-0.2.4/texar/tf/modules/regressors/xlnet_regressor.py: 174-221
</a>
<div class="mid" id="frag921" style="display:none"><pre>
    def param_groups(self, lr=None, lr_layer_scale=1.0,
                     decay_base_params=False):
        r"""Create parameter groups for optimizers. When
        :attr:`lr_layer_decay_rate` is not 1.0, parameters from each layer form
        separate groups with different base learning rates.

        This method should be called before applying gradients to the variables
        through the optimizer. Particularly, after calling the optimizer's
        `compute_gradients` method, the user can call this method to get
        variable-specific learning rates for the network. The gradients for each
        variables can then be scaled accordingly. These scaled gradients are
        finally applied by calling optimizer's `apply_gradients` method.

        Args:
            lr (float): The learning rate. Can be omitted if
                :attr:`lr_layer_decay_rate` is 1.0.
            lr_layer_scale (float): Per-layer LR scaling rate. The `i`-th layer
                will be scaled by `lr_layer_scale ^ (num_layers - i - 1)`.
            decay_base_params (bool): If `True`, treat non-layer parameters
                (e.g. embeddings) as if they're in layer 0. If `False`, these
                parameters are not scaled.

        Returns: A dict mapping tensorflow variables to their learning rates.
        """
        vars_to_learning_rates = {}
        if lr_layer_scale != 1.0:
            if lr is None:
                raise ValueError(
                    "lr must be specified when lr_layer_decay_rate is not 1.0")

            scope = self.variable_scope.name
            projection_vars = tf.trainable_variables(scope=scope + "/dense")
            logits_vars = tf.trainable_variables(
                scope=self.variable_scope.name + "/logit_layer")
            finetune_vars = projection_vars + logits_vars
            for var in finetune_vars:
                vars_to_learning_rates[var] = lr

            vars_to_learning_rates.update(
                self._encoder.param_groups(lr=lr,
                                           lr_layer_scale=lr_layer_scale,
                                           decay_base_params=decay_base_params))
        else:
            for variable in self.trainable_variables:
                vars_to_learning_rates[variable] = lr

        return vars_to_learning_rates

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1015')" href="javascript:;">
texar-0.2.4/texar/tf/modules/classifiers/xlnet_classifier.py: 190-237
</a>
<div class="mid" id="frag1015" style="display:none"><pre>
    def param_groups(self, lr=None, lr_layer_scale=1.0,
                     decay_base_params=False):
        r"""Create parameter groups for optimizers. When
        :attr:`lr_layer_decay_rate` is not 1.0, parameters from each layer form
        separate groups with different base learning rates.

        This method should be called before applying gradients to the variables
        through the optimizer. Particularly, after calling the optimizer's
        `compute_gradients` method, the user can call this method to get
        variable-specific learning rates for the network. The gradients for each
        variables can then be scaled accordingly. These scaled gradients are
        finally applied by calling optimizer's `apply_gradients` method.

        Args:
            lr (float): The learning rate. Can be omitted if
                :attr:`lr_layer_decay_rate` is 1.0.
            lr_layer_scale (float): Per-layer LR scaling rate. The `i`-th layer
                will be scaled by `lr_layer_scale ^ (num_layers - i - 1)`.
            decay_base_params (bool): If `True`, treat non-layer parameters
                (e.g. embeddings) as if they're in layer 0. If `False`, these
                parameters are not scaled.

        Returns: A dict mapping tensorflow variables to their learning rates.
        """
        vars_to_learning_rates = {}
        if lr_layer_scale != 1.0:
            if lr is None:
                raise ValueError(
                    "lr must be specified when lr_layer_decay_rate is not 1.0")

            scope = self.variable_scope.name
            projection_vars = tf.trainable_variables(scope=scope + "/dense")
            logits_vars = tf.trainable_variables(
                scope=self.variable_scope.name + "/logit_layer")
            finetune_vars = projection_vars + logits_vars
            for var in finetune_vars:
                vars_to_learning_rates[var] = lr

            vars_to_learning_rates.update(
                self._encoder.param_groups(lr=lr,
                                           lr_layer_scale=lr_layer_scale,
                                           decay_base_params=decay_base_params))
        else:
            for variable in self.trainable_variables:
                vars_to_learning_rates[variable] = lr

        return vars_to_learning_rates

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 9:</b> &nbsp; 2 fragments, nominal size 26 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag924')" href="javascript:;">
texar-0.2.4/texar/tf/modules/regressors/xlnet_regressor_test.py: 35-72
</a>
<div class="mid" id="frag924" style="display:none"><pre>
    def test_trainable_variables(self):
        """Tests the functionality of automatically collecting trainable
        variables.
        """
        inputs = tf.placeholder(dtype=tf.int32, shape=[None, None])

        # case 1
        hparams = {
            "pretrained_model_name": None,
        }
        regressor = XLNetRegressor(hparams=hparams)
        regressor(inputs)
        n_xlnet_vars = 162
        n_projection_vars = 2
        n_logits_vars = 2
        self.assertEqual(len(regressor.trainable_variables),
                         n_xlnet_vars + n_logits_vars + n_projection_vars)

        # case 2
        hparams = {
            "pretrained_model_name": None,
            "regr_strategy": "all_time"
        }
        regressor = XLNetRegressor(hparams=hparams)
        regressor(inputs)
        self.assertEqual(len(regressor.trainable_variables),
                         n_xlnet_vars + n_logits_vars + n_projection_vars)

        # case 3
        hparams = {
            "pretrained_model_name": None,
            "regr_strategy": "time_wise"
        }
        regressor = XLNetRegressor(hparams=hparams)
        regressor(inputs)
        self.assertEqual(len(regressor.trainable_variables),
                         n_xlnet_vars + n_logits_vars + n_projection_vars)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1022')" href="javascript:;">
texar-0.2.4/texar/tf/modules/classifiers/xlnet_classifier_test.py: 35-72
</a>
<div class="mid" id="frag1022" style="display:none"><pre>
    def test_trainable_variables(self):
        """Tests the functionality of automatically collecting trainable
        variables.
        """
        inputs = tf.placeholder(dtype=tf.int32, shape=[None, None])

        # case 1
        hparams = {
            "pretrained_model_name": None,
        }
        clas = XLNetClassifier(hparams=hparams)
        clas(inputs)
        n_xlnet_vars = 162
        n_projection_vars = 2
        n_logits_vars = 2
        self.assertEqual(len(clas.trainable_variables),
                         n_xlnet_vars + n_logits_vars + n_projection_vars)

        # case 2
        hparams = {
            "pretrained_model_name": None,
            "clas_strategy": "time_wise"
        }
        clas = XLNetClassifier(hparams=hparams)
        clas(inputs)
        self.assertEqual(len(clas.trainable_variables),
                         n_xlnet_vars + n_logits_vars + n_projection_vars)

        # case 3
        hparams = {
            "pretrained_model_name": None,
            "clas_strategy": "all_time"
        }
        clas = XLNetClassifier(hparams=hparams)
        clas(inputs)
        self.assertEqual(len(clas.trainable_variables),
                         n_xlnet_vars + n_logits_vars + n_projection_vars)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 10:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag932')" href="javascript:;">
texar-0.2.4/texar/tf/modules/qnets/qnets.py: 127-137
</a>
<div class="mid" id="frag932" style="display:none"><pre>
    def _build_network(self, network, kwargs):
        if network is not None:
            self._network = network
        else:
            kwargs = utils.get_instance_kwargs(
                kwargs, self._hparams.network_hparams)
            self._network = utils.check_or_get_instance(
                self._hparams.network_type,
                kwargs,
                module_paths=['texar.tf.modules', 'texar.tf.custom'])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag980')" href="javascript:;">
texar-0.2.4/texar/tf/modules/policies/policy_nets.py: 135-145
</a>
<div class="mid" id="frag980" style="display:none"><pre>
    def _build_network(self, network, kwargs):
        if network is not None:
            self._network = network
        else:
            kwargs = utils.get_instance_kwargs(
                kwargs, self._hparams.network_hparams)
            self._network = utils.check_or_get_instance(
                self._hparams.network_type,
                kwargs,
                module_paths=['texar.tf.modules', 'texar.tf.custom'])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 11:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag935')" href="javascript:;">
texar-0.2.4/texar/tf/modules/qnets/qnets.py: 173-186
</a>
<div class="mid" id="frag935" style="display:none"><pre>
    def __init__(self,
                 action_space=None,
                 network=None,
                 network_kwargs=None,
                 hparams=None):
        QNetBase.__init__(self, hparams=hparams)

        with tf.variable_scope(self.variable_scope):
            if action_space is None:
                action_space = Space(
                    low=0, high=self._hparams.action_space, dtype=np.int32)
            self._action_space = action_space
            self._append_output_layer()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag983')" href="javascript:;">
texar-0.2.4/texar/tf/modules/policies/policy_nets.py: 186-199
</a>
<div class="mid" id="frag983" style="display:none"><pre>
    def __init__(self,
                 action_space=None,
                 network=None,
                 network_kwargs=None,
                 hparams=None):
        PolicyNetBase.__init__(self, hparams=hparams)

        with tf.variable_scope(self.variable_scope):
            if action_space is None:
                action_space = Space(
                    low=0, high=self._hparams.action_space, dtype=np.int32)
            self._action_space = action_space
            self._append_output_layer()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 12:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag937')" href="javascript:;">
texar-0.2.4/texar/tf/modules/qnets/qnets.py: 233-246
</a>
<div class="mid" id="frag937" style="display:none"><pre>
    def _append_output_layer(self):
        if not self._hparams.make_output_layer:
            return

        if self._action_space.shape != ():
            raise ValueError('Only scalar discrete action is supported.')
        else:
            output_size = self._action_space.high - self._action_space.low

        layer_hparams = {
            'type': 'Dense',
            'kwargs': {'units': output_size}}
        self._network.append_layer(layer_hparams)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag985')" href="javascript:;">
texar-0.2.4/texar/tf/modules/policies/policy_nets.py: 265-279
</a>
<div class="mid" id="frag985" style="display:none"><pre>
    def _append_output_layer(self):
        if not self._hparams.make_output_layer:
            return

        if self._action_space.shape != ():
            raise ValueError('Only scalar discrete action is supported.')
        else:
            output_size = self._action_space.high - self._action_space.low

        layer_hparams = {
            'type': 'Dense',
            'kwargs': {'units': output_size}
        }
        self._network.append_layer(layer_hparams)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 13:</b> &nbsp; 2 fragments, nominal size 48 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1008')" href="javascript:;">
texar-0.2.4/texar/tf/modules/classifiers/bert_classifier_test.py: 141-204
</a>
<div class="mid" id="frag1008" style="display:none"><pre>
    def test_binary(self):
        """Tests binary classification.
        """
        max_time = 8
        batch_size = 16
        inputs = tf.random_uniform([batch_size, max_time],
                                   maxval=30521, dtype=tf.int32)

        # case 2
        hparams = {
            "pretrained_model_name": None,
            "num_classes": 1,
            "clas_strategy": "time_wise"
        }
        clas = BERTClassifier(hparams=hparams)
        logits, pred = clas(inputs)

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            logits_, pred_ = sess.run([logits, pred])
            self.assertEqual(logits_.shape, (batch_size, max_time))
            self.assertEqual(pred_.shape, (batch_size, max_time))

        # case 3
        hparams = {
            "pretrained_model_name": None,
            "num_classes": 1,
            "clas_strategy": "cls_time",
            "max_seq_length": max_time
        }
        inputs = tf.placeholder(tf.int32, shape=[batch_size, 6])
        clas = BERTClassifier(hparams=hparams)
        logits, pred = clas(inputs)

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            logits_, pred_ = sess.run(
                [logits, pred],
                feed_dict={inputs: np.random.randint(30521,
                                                     size=(batch_size, 6))})
            self.assertEqual(logits_.shape, (batch_size, ))
            self.assertEqual(pred_.shape, (batch_size, ))

        # case 4
        hparams = {
            "pretrained_model_name": None,
            "num_classes": 1,
            "clas_strategy": "all_time",
            "max_seq_length": max_time
        }
        inputs = tf.placeholder(tf.int32, shape=[batch_size, 6])
        clas = BERTClassifier(hparams=hparams)
        logits, pred = clas(inputs)

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            logits_, pred_ = sess.run(
                [logits, pred],
                feed_dict={inputs: np.random.randint(30521,
                                                     size=(batch_size, 6))})
            self.assertEqual(logits_.shape, (batch_size, ))
            self.assertEqual(pred_.shape, (batch_size, ))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1024')" href="javascript:;">
texar-0.2.4/texar/tf/modules/classifiers/xlnet_classifier_test.py: 149-212
</a>
<div class="mid" id="frag1024" style="display:none"><pre>
    def test_binary(self):
        """Tests binary classification.
        """
        max_time = 8
        batch_size = 16
        inputs = tf.random_uniform([batch_size, max_time],
                                   maxval=30521, dtype=tf.int32)

        # case 1
        hparams = {
            "pretrained_model_name": None,
            "num_classes": 1,
            "clas_strategy": "time_wise"
        }
        clas = XLNetClassifier(hparams=hparams)
        logits, pred = clas(inputs)

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            logits_, pred_ = sess.run([logits, pred])
            self.assertEqual(logits_.shape, (batch_size, max_time))
            self.assertEqual(pred_.shape, (batch_size, max_time))

        # case 2
        hparams = {
            "pretrained_model_name": None,
            "num_classes": 1,
            "clas_strategy": "cls_time",
            "max_seq_len": max_time
        }
        inputs = tf.placeholder(tf.int32, shape=[batch_size, 6])
        clas = XLNetClassifier(hparams=hparams)
        logits, pred = clas(inputs)

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            logits_, pred_ = sess.run(
                [logits, pred],
                feed_dict={inputs: np.random.randint(30521,
                                                     size=(batch_size, 6))})
            self.assertEqual(logits_.shape, (batch_size,))
            self.assertEqual(pred_.shape, (batch_size,))

        # case 3
        hparams = {
            "pretrained_model_name": None,
            "num_classes": 1,
            "clas_strategy": "all_time",
            "max_seq_len": max_time
        }
        inputs = tf.placeholder(tf.int32, shape=[batch_size, 6])
        clas = XLNetClassifier(hparams=hparams)
        logits, pred = clas(inputs)

        with self.test_session() as sess:
            sess.run(tf.global_variables_initializer())
            logits_, pred_ = sess.run(
                [logits, pred],
                feed_dict={inputs: np.random.randint(30521,
                                                     size=(batch_size, 6))})
            self.assertEqual(logits_.shape, (batch_size,))
            self.assertEqual(pred_.shape, (batch_size,))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 14:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1010')" href="javascript:;">
texar-0.2.4/texar/tf/modules/classifiers/gpt2_classifier.py: 109-181
</a>
<div class="mid" id="frag1010" style="display:none"><pre>
    def default_hparams():
        r"""Returns a dictionary of hyperparameters with default values.

        .. code-block:: python

            {
                # (1) Same hyperparameters as in GPT2Encoder
                ...
                # (2) Additional hyperparameters
                "num_classes": 2,
                "logit_layer_kwargs": None,
                "clas_strategy": `cls_time`,
                "max_seq_length": None,
                "dropout": 0.1,
                "name": `gpt2_classifier`
            }

        Here:

        1. Same hyperparameters as in
        :class:`~texar.tf.modules.GPT2Encoder`.
        See the :meth:`~texar.tf.modules.GPT2Encoder.default_hparams`.
        An instance of GPT2Encoder is created for feature extraction.

        2. Additional hyperparameters:

            `"num_classes"`: int
                Number of classes:

                - If **&gt; 0**, an additional :tf_main:`Dense &lt;layers/Dense&gt;`
                  layer is appended to the encoder to compute the logits over
                  classes.
                - If **&lt;= 0**, no dense layer is appended. The number of
                  classes is assumed to be the final dense layer size of the
                  encoder.

            `"logit_layer_kwargs"`: dict
                Keyword arguments for the logit Dense layer constructor,
                except for argument "units" which is set to `num_classes`.
                Ignored if no extra logit layer is appended.

            `"clas_strategy"`: str
                The classification strategy, one of:

                - **cls_time**: Sequence-level classification based on the
                  output of the first time step (which is the `CLS` token).
                  Each sequence has a class.
                - **all_time**: Sequence-level classification based on
                  the output of all time steps. Each sequence has a class.
                - **time_wise**: Step-wise classification, i.e., make
                  classification for each time step based on its output.

            `"max_seq_length"`: int, optional
                Maximum possible length of input sequences. Required if
                `clas_strategy` is `all_time`.

            `"dropout"`: float
                The dropout rate of the BERT encoder output.

            `"name"`: str
                Name of the classifier.
        """
        hparams = GPT2Encoder.default_hparams()
        hparams.update({
            "num_classes": 2,
            "logit_layer_kwargs": None,
            "clas_strategy": "cls_time",
            "max_seq_length": None,
            "dropout": 0.1,
            "name": "gpt2_classifier"
        })
        return hparams

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1026')" href="javascript:;">
texar-0.2.4/texar/tf/modules/classifiers/bert_classifier.py: 116-189
</a>
<div class="mid" id="frag1026" style="display:none"><pre>
    def default_hparams():
        r"""Returns a dictionary of hyperparameters with default values.

        .. code-block:: python

            {
                # (1) Same hyperparameters as in BertEncoder
                ...
                # (2) Additional hyperparameters
                "num_classes": 2,
                "logit_layer_kwargs": None,
                "clas_strategy": "cls_time",
                "max_seq_length": None,
                "dropout": 0.1,
                "name": "bert_classifier"
            }

        Here:

        1. Same hyperparameters as in
        :class:`~texar.tf.modules.BertEncoder`.
        See the :meth:`~texar.tf.modules.BertEncoder.default_hparams`.
        An instance of BertEncoder is created for feature extraction.

        2. Additional hyperparameters:

            `"num_classes"`: int
                Number of classes:

                - If **&gt; 0**, an additional :tf_main:`Dense &lt;layers/Dense&gt;`
                  layer is appended to the encoder to compute the logits over
                  classes.
                - If **&lt;= 0**, no dense layer is appended. The number of
                  classes is assumed to be the final dense layer size of the
                  encoder.

            `"logit_layer_kwargs"`: dict
                Keyword arguments for the logit Dense layer constructor,
                except for argument "units" which is set to `num_classes`.
                Ignored if no extra logit layer is appended.

            `"clas_strategy"`: str
                The classification strategy, one of:

                - **cls_time**: Sequence-level classification based on the
                  output of the first time step (which is the `CLS` token).
                  Each sequence has a class.
                - **all_time**: Sequence-level classification based on
                  the output of all time steps. Each sequence has a class.
                - **time_wise**: Step-wise classification, i.e., make
                  classification for each time step based on its output.

            `"max_seq_length"`: int, optional
                Maximum possible length of input sequences. Required if
                `clas_strategy` is `all_time`.

            `"dropout"`: float
                The dropout rate of the BERT encoder output.

            `"name"`: str
                Name of the classifier.
        """

        hparams = BERTEncoder.default_hparams()
        hparams.update({
            "num_classes": 2,
            "logit_layer_kwargs": None,
            "clas_strategy": "cls_time",
            "max_seq_length": None,
            "dropout": 0.1,
            "name": "bert_classifier"
        })
        return hparams

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 15:</b> &nbsp; 3 fragments, nominal size 31 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1094')" href="javascript:;">
texar-0.2.4/examples/distributed_gpu/ptb_reader.py: 63-102
</a>
<div class="mid" id="frag1094" style="display:none"><pre>
def prepare_data(data_path):
    """Preprocess PTB data.
    """
    train_path = os.path.join(data_path, "ptb.train.txt")
    if not tf.gfile.Exists(train_path):
        url = 'http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz'
        tx.data.maybe_download(url, data_path, extract=True)
        data_path = os.path.join(data_path, 'simple-examples', 'data')

    train_path = os.path.join(data_path, "ptb.train.txt")
    valid_path = os.path.join(data_path, "ptb.valid.txt")
    test_path = os.path.join(data_path, "ptb.test.txt")

    word_to_id = tx.data.make_vocab(
        train_path, newline_token="&lt;EOS&gt;", return_type="dict")
    assert len(word_to_id) == 10000

    train_text = tx.data.read_words(
        train_path, newline_token="&lt;EOS&gt;")
    train_text_id = [word_to_id[w] for w in train_text if w in word_to_id]

    valid_text = tx.data.read_words(
        valid_path, newline_token="&lt;EOS&gt;")
    valid_text_id = [word_to_id[w] for w in valid_text if w in word_to_id]

    test_text = tx.data.read_words(
        test_path, newline_token="&lt;EOS&gt;")
    test_text_id = [word_to_id[w] for w in test_text if w in word_to_id]

    data = {
        "train_text": train_text,
        "valid_text": valid_text,
        "test_text": test_text,
        "train_text_id": train_text_id,
        "valid_text_id": valid_text_id,
        "test_text_id": test_text_id,
        "vocab": word_to_id,
        "vocab_size": len(word_to_id)
    }
    return data
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1301')" href="javascript:;">
texar-0.2.4/examples/language_model_ptb/ptb_reader.py: 49-88
</a>
<div class="mid" id="frag1301" style="display:none"><pre>
def prepare_data(data_path):
    """Preprocess PTB data.
    """
    train_path = os.path.join(data_path, "ptb.train.txt")
    if not tf.gfile.Exists(train_path):
        url = 'http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz'
        tx.data.maybe_download(url, data_path, extract=True)
        data_path = os.path.join(data_path, 'simple-examples', 'data')

    train_path = os.path.join(data_path, "ptb.train.txt")
    valid_path = os.path.join(data_path, "ptb.valid.txt")
    test_path = os.path.join(data_path, "ptb.test.txt")

    word_to_id = tx.data.make_vocab(
        train_path, newline_token="&lt;EOS&gt;", return_type="dict")
    assert len(word_to_id) == 10000

    train_text = tx.data.read_words(
        train_path, newline_token="&lt;EOS&gt;")
    train_text_id = [word_to_id[w] for w in train_text if w in word_to_id]

    valid_text = tx.data.read_words(
        valid_path, newline_token="&lt;EOS&gt;")
    valid_text_id = [word_to_id[w] for w in valid_text if w in word_to_id]

    test_text = tx.data.read_words(
        test_path, newline_token="&lt;EOS&gt;")
    test_text_id = [word_to_id[w] for w in test_text if w in word_to_id]

    data = {
        "train_text": train_text,
        "valid_text": valid_text,
        "test_text": test_text,
        "train_text_id": train_text_id,
        "valid_text_id": valid_text_id,
        "test_text_id": test_text_id,
        "vocab": word_to_id,
        "vocab_size": len(word_to_id)
    }
    return data
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1281')" href="javascript:;">
texar-0.2.4/examples/memory_network_lm/ptb_reader.py: 69-108
</a>
<div class="mid" id="frag1281" style="display:none"><pre>
def prepare_data(data_path):
    """Preprocess PTB data.
    """
    train_path = os.path.join(data_path, "ptb.train.txt")
    if not tf.gfile.Exists(train_path):
        url = 'http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz'
        tx.data.maybe_download(url, data_path, extract=True)
        data_path = os.path.join(data_path, 'simple-examples', 'data')

    train_path = os.path.join(data_path, "ptb.train.txt")
    valid_path = os.path.join(data_path, "ptb.valid.txt")
    test_path = os.path.join(data_path, "ptb.test.txt")

    word_to_id = tx.data.make_vocab(
        train_path, newline_token="&lt;EOS&gt;", return_type="dict")
    assert len(word_to_id) == 10000

    train_text = tx.data.read_words(
        train_path, newline_token="&lt;EOS&gt;")
    train_text_id = [word_to_id[w] for w in train_text if w in word_to_id]

    valid_text = tx.data.read_words(
        valid_path, newline_token="&lt;EOS&gt;")
    valid_text_id = [word_to_id[w] for w in valid_text if w in word_to_id]

    test_text = tx.data.read_words(
        test_path, newline_token="&lt;EOS&gt;")
    test_text_id = [word_to_id[w] for w in test_text if w in word_to_id]

    data = {
        "train_text": train_text,
        "valid_text": valid_text,
        "test_text": test_text,
        "train_text_id": train_text_id,
        "valid_text_id": valid_text_id,
        "test_text_id": test_text_id,
        "vocab": word_to_id,
        "vocab_size": len(word_to_id)
    }
    return data
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 16:</b> &nbsp; 4 fragments, nominal size 38 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1185')" href="javascript:;">
texar-0.2.4/examples/seq2seq_exposure_bias/scheduled_sampling_main.py: 160-205
</a>
<div class="mid" id="frag1185" style="display:none"><pre>
            except tf.errors.OutOfRangeError:
                break

    # code below this line is exactly the same as baseline_seq2seq_attn_main.py

    def _eval_epoch(sess, mode, epoch_no):
        if mode == 'val':
            data_iterator.switch_to_val_data(sess)
        else:
            data_iterator.switch_to_test_data(sess)

        refs, hypos = [], []
        while True:
            try:
                fetches = [
                    batch['target_text'][:, 1:],
                    infer_outputs.predicted_ids[:, :, 0]
                ]
                feed_dict = {
                    tx.global_mode(): tf.estimator.ModeKeys.EVAL
                }
                target_texts_ori, output_ids = \
                    sess.run(fetches, feed_dict=feed_dict)

                target_texts = tx.utils.strip_special_tokens(
                    target_texts_ori.tolist(), is_token_list=True)
                target_texts = tx.utils.str_join(target_texts)
                output_texts = tx.utils.map_ids_to_strs(
                    ids=output_ids, vocab=val_data.target_vocab)

                tx.utils.write_paired_text(
                    target_texts, output_texts,
                    log_dir + mode + '_results' + str(epoch_no) + '.txt',
                    append=True, mode='h', sep=' ||| ')

                for hypo, ref in zip(output_texts, target_texts):
                    if config_data.eval_metric == 'bleu':
                        hypos.append(hypo)
                        refs.append([ref])
                    elif config_data.eval_metric == 'rouge':
                        hypos.append(tx.utils.compat_as_text(hypo))
                        refs.append(tx.utils.compat_as_text(ref))
            except tf.errors.OutOfRangeError:
                break

        if config_data.eval_metric == 'bleu':
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1230')" href="javascript:;">
texar-0.2.4/examples/seq2seq_exposure_bias/raml_main.py: 240-285
</a>
<div class="mid" id="frag1230" style="display:none"><pre>
                print("step={}, loss={:.4f}".format(step, loss))
            training_log_file.flush()
            step += 1

    # code below this line is exactly the same as baseline_seq2seq_attn_main.py

    def _eval_epoch(sess, mode, epoch_no):
        if mode == 'val':
            data_iterator.switch_to_val_data(sess)
        else:
            data_iterator.switch_to_test_data(sess)

        refs, hypos = [], []
        while True:
            try:
                fetches = [
                    batch['target_text'][:, 1:],
                    infer_outputs.predicted_ids[:, :, 0]
                ]
                feed_dict = {
                    tx.global_mode(): tf.estimator.ModeKeys.EVAL
                }
                target_texts_ori, output_ids = \
                    sess.run(fetches, feed_dict=feed_dict)

                target_texts = tx.utils.strip_special_tokens(
                    target_texts_ori.tolist(), is_token_list=True)
                target_texts = tx.utils.str_join(target_texts)
                output_texts = tx.utils.map_ids_to_strs(
                    ids=output_ids, vocab=val_data.target_vocab)

                tx.utils.write_paired_text(
                    target_texts, output_texts,
                    log_dir + mode + '_results' + str(epoch_no) + '.txt',
                    append=True, mode='h', sep=' ||| ')

                for hypo, ref in zip(output_texts, target_texts):
                    if config_data.eval_metric == 'bleu':
                        hypos.append(hypo)
                        refs.append([ref])
                    elif config_data.eval_metric == 'rouge':
                        hypos.append(tx.utils.compat_as_text(hypo))
                        refs.append(tx.utils.compat_as_text(ref))
            except tf.errors.OutOfRangeError:
                break

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1197')" href="javascript:;">
texar-0.2.4/examples/seq2seq_exposure_bias/baseline_seq2seq_attn_main.py: 130-175
</a>
<div class="mid" id="frag1197" style="display:none"><pre>
            except tf.errors.OutOfRangeError:
                break

    def _eval_epoch(sess, mode, epoch_no):
        if mode == 'val':
            data_iterator.switch_to_val_data(sess)
        else:
            data_iterator.switch_to_test_data(sess)

        refs, hypos = [], []
        while True:
            try:
                fetches = [
                    batch['target_text'][:, 1:],
                    infer_outputs.predicted_ids[:, :, 0]
                ]
                feed_dict = {
                    tx.global_mode(): tf.estimator.ModeKeys.EVAL
                }
                target_texts_ori, output_ids = \
                    sess.run(fetches, feed_dict=feed_dict)

                target_texts = tx.utils.strip_special_tokens(
                    target_texts_ori.tolist(), is_token_list=True)
                target_texts = tx.utils.str_join(target_texts)
                output_texts = tx.utils.map_ids_to_strs(
                    ids=output_ids, vocab=val_data.target_vocab)

                tx.utils.write_paired_text(
                    target_texts, output_texts,
                    log_dir + mode + '_results' + str(epoch_no) + '.txt',
                    append=True, mode='h', sep=' ||| ')

                for hypo, ref in zip(output_texts, target_texts):
                    if config_data.eval_metric == 'bleu':
                        hypos.append(hypo)
                        refs.append([ref])
                    elif config_data.eval_metric == 'rouge':
                        hypos.append(tx.utils.compat_as_text(hypo))
                        refs.append(tx.utils.compat_as_text(ref))
            except tf.errors.OutOfRangeError:
                break

        if config_data.eval_metric == 'bleu':
            return tx.evals.corpus_bleu_moses(
                list_of_references=refs, hypotheses=hypos)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1219')" href="javascript:;">
texar-0.2.4/examples/seq2seq_exposure_bias/interpolation_main.py: 166-215
</a>
<div class="mid" id="frag1219" style="display:none"><pre>
                        step, loss, lambdas))
                log_file.flush()
                step += 1

            except tf.errors.OutOfRangeError:
                break

    def _eval_epoch(sess, mode, epoch_no):
        """
        This function is the same as _eval_epoch() in
        baseline_seq2seq_attn_main.py.
        """
        if mode == 'val':
            data_iterator.switch_to_val_data(sess)
        else:
            data_iterator.switch_to_test_data(sess)

        refs, hypos = [], []
        while True:
            try:
                fetches = [
                    batch['target_text'][:, 1:],
                    infer_outputs.predicted_ids[:, :, 0]
                ]
                feed_dict = {
                    tx.global_mode(): tf.estimator.ModeKeys.EVAL
                }
                target_texts_ori, output_ids = \
                    sess.run(fetches, feed_dict=feed_dict)

                target_texts = tx.utils.strip_special_tokens(
                    target_texts_ori.tolist(), is_token_list=True)
                target_texts = tx.utils.str_join(target_texts)
                output_texts = tx.utils.map_ids_to_strs(
                    ids=output_ids, vocab=val_data.target_vocab)

                tx.utils.write_paired_text(
                    target_texts, output_texts,
                    log_dir + mode + '_results' + str(epoch_no) + '.txt',
                    append=True, mode='h', sep=' ||| ')

                for hypo, ref in zip(output_texts, target_texts):
                    if config_data.eval_metric == 'bleu':
                        hypos.append(hypo)
                        refs.append([ref])
                    elif config_data.eval_metric == 'rouge':
                        hypos.append(tx.utils.compat_as_text(hypo))
                        refs.append(tx.utils.compat_as_text(ref))
            except tf.errors.OutOfRangeError:
                break
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 17:</b> &nbsp; 3 fragments, nominal size 15 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1213')" href="javascript:;">
texar-0.2.4/examples/seq2seq_exposure_bias/utils/prepare_data.py: 28-48
</a>
<div class="mid" id="frag1213" style="display:none"><pre>
def prepare_data():
    """Downloads data.
    """
    if FLAGS.data == 'giga':
        tx.data.maybe_download(
            urls='https://drive.google.com/file/d/'
                 '12RZs7QFwjj6dfuYNQ_0Ah-ccH1xFDMD5/view?usp=sharing',
            path='./',
            filenames='giga.zip',
            extract=True)
    elif FLAGS.data == 'iwslt14':
        tx.data.maybe_download(
            urls='https://drive.google.com/file/d/'
                 '1y4mUWXRS2KstgHopCS9koZ42ENOh6Yb9/view?usp=sharing',
            path='./',
            filenames='iwslt14.zip',
            extract=True)
    else:
        raise ValueError('Unknown data: {}'.format(FLAGS.data))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1288')" href="javascript:;">
texar-0.2.4/examples/seq2seq_attn/prepare_data.py: 28-48
</a>
<div class="mid" id="frag1288" style="display:none"><pre>
def prepare_data():
    """Downloads data.
    """
    if FLAGS.data == 'iwslt14':
        tx.data.maybe_download(
            urls='https://drive.google.com/file/d/'
                 '1y4mUWXRS2KstgHopCS9koZ42ENOh6Yb9/view?usp=sharing',
            path='./',
            filenames='iwslt14.zip',
            extract=True)
    elif FLAGS.data == 'toy_copy':
        tx.data.maybe_download(
            urls='https://drive.google.com/file/d/'
                 '1fENE2rakm8vJ8d3voWBgW4hGlS6-KORW/view?usp=sharing',
            path='./',
            filenames='toy_copy.zip',
            extract=True)
    else:
        raise ValueError('Unknown data: {}'.format(FLAGS.data))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1252')" href="javascript:;">
texar-0.2.4/examples/seq2seq_rl/prepare_data.py: 28-48
</a>
<div class="mid" id="frag1252" style="display:none"><pre>
def prepare_data():
    """Downloads data.
    """
    if FLAGS.data == 'iwslt14':
        tx.data.maybe_download(
            urls='https://drive.google.com/file/d/'
                 '1Vuv3bed10qUxrpldHdYoiWLzPKa4pNXd/view?usp=sharing',
            path='./',
            filenames='iwslt14.zip',
            extract=True)
    elif FLAGS.data == 'toy_copy':
        tx.data.maybe_download(
            urls='https://drive.google.com/file/d/'
                 '1fENE2rakm8vJ8d3voWBgW4hGlS6-KORW/view?usp=sharing',
            path='./',
            filenames='toy_copy.zip',
            extract=True)
    else:
        raise ValueError('Unknown data: {}'.format(FLAGS.data))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 18:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1279')" href="javascript:;">
texar-0.2.4/examples/memory_network_lm/ptb_reader.py: 30-48
</a>
<div class="mid" id="frag1279" style="display:none"><pre>
def ptb_iterator(data, batch_size, num_steps):
    """Iterates through the ptb data.
    """
    data_length = len(data)
    batch_length = data_length // batch_size

    data = np.asarray(data[:batch_size * batch_length])
    data = data.reshape([batch_size, batch_length])

    epoch_size = (batch_length - 1) // num_steps
    if epoch_size == 0:
        raise ValueError("epoch_size == 0, decrease batch_size or num_steps")

    for i in range(epoch_size):
        x = data[:, i * num_steps: (i + 1) * num_steps]
        y = data[:, i * num_steps + 1: (i + 1) * num_steps + 1]
        yield (x, y)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1300')" href="javascript:;">
texar-0.2.4/examples/language_model_ptb/ptb_reader.py: 30-48
</a>
<div class="mid" id="frag1300" style="display:none"><pre>
def ptb_iterator(data, batch_size, num_steps):
    """Iterates through the ptb data.
    """
    data_length = len(data)
    batch_length = data_length // batch_size

    data = np.asarray(data[:batch_size * batch_length])
    data = data.reshape([batch_size, batch_length])

    epoch_size = (batch_length - 1) // num_steps
    if epoch_size == 0:
        raise ValueError("epoch_size == 0, decrease batch_size or num_steps")

    for i in range(epoch_size):
        x = data[:, i * num_steps: (i + 1) * num_steps]
        y = data[:, i * num_steps + 1: (i + 1) * num_steps + 1]
        yield (x, y)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<script language="JavaScript">
function ShowHide(divId) { 
    if(document.getElementById(divId).style.display == 'none') {
        document.getElementById(divId).style.display='block';
    } else { 
        document.getElementById(divId).style.display = 'none';
    } 
}
</script>
</body>
</html>

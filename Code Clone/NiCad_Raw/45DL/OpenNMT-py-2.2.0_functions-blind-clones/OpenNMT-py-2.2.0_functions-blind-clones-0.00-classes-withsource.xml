<clones>
<systeminfo processor="nicad6" system="OpenNMT-py-2.2.0" granularity="functions-blind" threshold="0%" minlines="10" maxlines="2500"/>
<cloneinfo npcs="606" npairs="3"/>
<runinfo ncompares="998" cputime="35947"/>
<classinfo nclasses="3"/>

<class classid="1" nclones="2" nlines="10" similarity="100">
<source file="systems/OpenNMT-py-2.2.0/onmt/encoders/ggnn_encoder.py" startline="302" endline="317" pcid="76">
        self.total_hidden_dim = hidden_size * num_layers

        # Build a linear layer for each
        self.bridge = nn.ModuleList([nn.Linear(self.total_hidden_dim,
                                               self.total_hidden_dim,
                                               bias=True)
                                     for _ in range(number_of_states)])

    def _bridge(self, hidden):
        """Forward hidden state through bridge."""
        def bottle_hidden(linear, states):
            """
            Transform from 3D to 2D, apply linear and return initial size
            """
            size = states.size()
            result = linear(states.view(-1, self.total_hidden_dim))
</source>
<source file="systems/OpenNMT-py-2.2.0/onmt/encoders/rnn_encoder.py" startline="99" endline="115" pcid="82">

    def _bridge(self, hidden):
        """Forward hidden state through bridge."""
        def bottle_hidden(linear, states):
            """
            Transform from 3D to 2D, apply linear and return initial size
            """
            size = states.size()
            result = linear(states.view(-1, self.total_hidden_dim))
            return F.relu(result).view(size)

        if isinstance(hidden, tuple):  # LSTM
            outs = tuple([bottle_hidden(layer, hidden[ix])
                          for ix, layer in enumerate(self.bridge)])
        else:
            outs = bottle_hidden(self.bridge[0], hidden)
        return outs
</source>
</class>

<class classid="2" nclones="2" nlines="10" similarity="100">
<source file="systems/OpenNMT-py-2.2.0/onmt/translate/penalties.py" startline="34" endline="44" pcid="435">
    def _coverage_penalty(self, cov_pen):
        if cov_pen == "wu":
            return self.coverage_wu
        elif cov_pen == "summary":
            return self.coverage_summary
        elif self._pen_is_none(cov_pen):
            return self.coverage_none
        else:
            raise NotImplementedError("No '{:s}' coverage penalty.".format(
                cov_pen))

</source>
<source file="systems/OpenNMT-py-2.2.0/onmt/translate/penalties.py" startline="45" endline="59" pcid="436">
    def _length_penalty(self, length_pen):
        if length_pen == "wu":
            return self.length_wu
        elif length_pen == "avg":
            return self.length_average
        elif self._pen_is_none(length_pen):
            return self.length_none
        else:
            raise NotImplementedError("No '{:s}' length penalty.".format(
                length_pen))

    # Below are all the different penalty terms implemented so far.
    # Subtract coverage penalty from topk log probs.
    # Divide topk log probs by length penalty.

</source>
</class>

<class classid="3" nclones="2" nlines="19" similarity="100">
<source file="systems/OpenNMT-py-2.2.0/onmt/modules/embeddings.py" startline="264" endline="289" pcid="563">

def read_embeddings(path, skip_lines=0, filter_set=None):
    """
    Read an embeddings file in the glove format.
    """
    embs = dict()
    total_vectors_in_file = 0
    with open(path, 'rb') as f:
        for i, line in enumerate(f):
            if i < skip_lines:
                continue
            if not line:
                break
            if len(line) == 0:
                # is this reachable?
                continue

            l_split = line.decode('utf8').strip().split(' ')
            if len(l_split) == 2:
                continue
            total_vectors_in_file += 1
            if filter_set is not None and l_split[0] not in filter_set:
                continue
            embs[l_split[0]] = [float(em) for em in l_split[1:]]
    return embs, total_vectors_in_file

</source>
<source file="systems/OpenNMT-py-2.2.0/tools/embeddings_to_torch.py" startline="29" endline="51" pcid="579">
def read_embeddings(file_enc, skip_lines=0, filter_set=None):
    embs = dict()
    total_vectors_in_file = 0
    with open(file_enc, 'rb') as f:
        for i, line in enumerate(f):
            if i < skip_lines:
                continue
            if not line:
                break
            if len(line) == 0:
                # is this reachable?
                continue

            l_split = line.decode('utf8').strip().split(' ')
            if len(l_split) == 2:
                continue
            total_vectors_in_file += 1
            if filter_set is not None and l_split[0] not in filter_set:
                continue
            embs[l_split[0]] = [float(em) for em in l_split[1:]]
    return embs, total_vectors_in_file


</source>
</class>

</clones>

<html>
<head>
    <title>NiCad6 Clone Report</title>
    <style type="text/css">
        body {font-family:sans-serif;}
        table {background-color:white; border:0px; padding:0px; border-spacing:4px; width:auto; margin-left:30px; margin-right:auto;}
        td {background-color:rgba(192,212,238,0.8); border:0px; padding:8px; width:auto; vertical-align:top; border-radius:8px}
        pre {background-color:white; padding:4px;}
        a {color:darkblue;}
    </style>
</head>
<body>
<h2>NiCad6 Clone Report</h2>
<table>
<tr style="font-size:14pt">
<td><b>System:</b> &nbsp; ludwig-0.5rc2</td>
<td><b>Clone pairs:</b> &nbsp; 18</td>
<td><b>Clone classes:</b> &nbsp; 4</td>
</tr>
<tr style="font-size:12pt">
<td style="background-color:white">Clone type: &nbsp; 1</td>
<td style="background-color:white">Granularity: &nbsp; functions</td>
<td style="background-color:white">Max diff threshold: &nbsp; 0%</td>
<td style="background-color:white">Clone size: &nbsp; 10 - 2500 lines</td>
<td style="background-color:white">Total functions: &nbsp; 1475</td>
</tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 1:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag80')" href="javascript:;">
ludwig-0.5rc2/tests/integration_tests/test_graph_execution.py: 57-68
</a>
<div class="mid" id="frag80" style="display:none"><pre>
def test_experiment_multiple_seq_seq(csv_filename, output_features):
    input_features = [
        text_feature(vocab_size=100, min_len=1, encoder="stacked_cnn"),
        number_feature(normalization="zscore"),
        category_feature(vocab_size=10, embedding_size=5),
        set_feature(),
        sequence_feature(vocab_size=10, max_len=10, encoder="embed"),
    ]
    output_features = output_features

    rel_path = generate_data(input_features, output_features, csv_filename)
    run_experiment(input_features, output_features, dataset=rel_path)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag161')" href="javascript:;">
ludwig-0.5rc2/tests/integration_tests/test_experiment.py: 245-258
</a>
<div class="mid" id="frag161" style="display:none"><pre>
def test_experiment_multiple_seq_seq(csv_filename, output_features):
    input_features = [
        text_feature(vocab_size=100, min_len=1, encoder="stacked_cnn"),
        number_feature(normalization="zscore"),
        category_feature(vocab_size=10, embedding_size=5),
        set_feature(),
        sequence_feature(vocab_size=10, max_len=10, encoder="embed"),
    ]
    output_features = output_features

    rel_path = generate_data(input_features, output_features, csv_filename)
    run_experiment(input_features, output_features, dataset=rel_path)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 2:</b> &nbsp; 5 fragments, nominal size 14 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag475')" href="javascript:;">
ludwig-0.5rc2/ludwig/encoders/text_encoders.py: 118-133
</a>
<div class="mid" id="frag475" style="display:none"><pre>
    def forward(self, inputs: torch.Tensor, mask: Optional[torch.Tensor] = None) -&gt; Dict[str, torch.Tensor]:
        if mask is not None:
            mask = mask.to(torch.int32)
        transformer_outputs = self.transformer(
            input_ids=inputs,
            attention_mask=mask,
            token_type_ids=torch.zeros_like(inputs),
        )
        if self.reduce_output == "cls_pooled":
            hidden = transformer_outputs[1]
        else:
            hidden = transformer_outputs[0][:, 1:-1, :]
            hidden = self.reduce_sequence(hidden, self.reduce_output)

        return {"encoder_output": hidden}

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag535')" href="javascript:;">
ludwig-0.5rc2/ludwig/encoders/text_encoders.py: 1439-1454
</a>
<div class="mid" id="frag535" style="display:none"><pre>
    def forward(self, inputs: torch.Tensor, mask: Optional[torch.Tensor] = None) -&gt; Dict[str, torch.Tensor]:
        if mask is not None:
            mask = mask.to(torch.int32)
        transformer_outputs = self.transformer(
            input_ids=inputs,
            attention_mask=mask,
            token_type_ids=torch.zeros_like(inputs),
        )
        if self.reduce_output == "cls_pooled":
            hidden = transformer_outputs[1]
        else:
            hidden = transformer_outputs[0][:, 1:-1, :]
            hidden = self.reduce_sequence(hidden, self.reduce_output)

        return {"encoder_output": hidden}

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag485')" href="javascript:;">
ludwig-0.5rc2/ludwig/encoders/text_encoders.py: 334-349
</a>
<div class="mid" id="frag485" style="display:none"><pre>
    def forward(self, inputs: torch.Tensor, mask: Optional[torch.Tensor] = None) -&gt; Dict[str, torch.Tensor]:
        if mask is not None:
            mask = mask.to(torch.int32)
        transformer_outputs = self.transformer(
            input_ids=inputs,
            attention_mask=mask,
            token_type_ids=torch.zeros_like(inputs),
        )
        if self.reduce_output == "cls_pooled":
            hidden = transformer_outputs[1]
        else:
            hidden = transformer_outputs[0][:, 1:-1, :]
            hidden = self.reduce_sequence(hidden, self.reduce_output)

        return {"encoder_output": hidden}

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag510')" href="javascript:;">
ludwig-0.5rc2/ludwig/encoders/text_encoders.py: 882-896
</a>
<div class="mid" id="frag510" style="display:none"><pre>
    def forward(self, inputs: torch.Tensor, mask: Optional[torch.Tensor] = None) -&gt; Dict[str, torch.Tensor]:
        if mask is not None:
            mask = mask.to(torch.int32)
        transformer_outputs = self.transformer(
            input_ids=inputs,
            attention_mask=mask,
            token_type_ids=torch.zeros_like(inputs),
        )
        if self.reduce_output == "cls_pooled":
            hidden = transformer_outputs[1]
        else:
            hidden = transformer_outputs[0][:, 1:-1, :]  # bos + [sent] + sep
            hidden = self.reduce_sequence(hidden, self.reduce_output)
        return {"encoder_output": hidden}

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag490')" href="javascript:;">
ludwig-0.5rc2/ludwig/encoders/text_encoders.py: 452-467
</a>
<div class="mid" id="frag490" style="display:none"><pre>
    def forward(self, inputs: torch.Tensor, mask: Optional[torch.Tensor] = None) -&gt; Dict[str, torch.Tensor]:
        if mask is not None:
            mask = mask.to(torch.int32)
        transformer_outputs = self.transformer(
            input_ids=inputs,
            attention_mask=mask,
            token_type_ids=torch.zeros_like(inputs),
        )
        if self.reduce_output == "cls_pooled":
            hidden = transformer_outputs[1]
        else:
            hidden = transformer_outputs[0][:, 1:-1, :]
            hidden = self.reduce_sequence(hidden, self.reduce_output)

        return {"encoder_output": hidden}

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 3:</b> &nbsp; 4 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag495')" href="javascript:;">
ludwig-0.5rc2/ludwig/encoders/text_encoders.py: 593-605
</a>
<div class="mid" id="frag495" style="display:none"><pre>
    def forward(self, inputs: torch.Tensor, mask: Optional[torch.Tensor] = None) -&gt; Dict[str, torch.Tensor]:

        if mask is not None:
            mask = mask.to(torch.int32)
        transformer_outputs = self.transformer(
            input_ids=inputs,
            attention_mask=mask,
            token_type_ids=torch.zeros_like(inputs),
        )
        hidden = transformer_outputs[0]
        hidden = self.reduce_sequence(hidden, self.reduce_output)
        return {"encoder_output": hidden}

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag500')" href="javascript:;">
ludwig-0.5rc2/ludwig/encoders/text_encoders.py: 699-710
</a>
<div class="mid" id="frag500" style="display:none"><pre>
    def forward(self, inputs: torch.Tensor, mask: Optional[torch.Tensor] = None) -&gt; Dict[str, torch.Tensor]:
        if mask is not None:
            mask = mask.to(torch.int32)
        transformer_outputs = self.transformer(
            input_ids=inputs,
            attention_mask=mask,
            token_type_ids=torch.zeros_like(inputs),
        )
        hidden = transformer_outputs[0]
        hidden = self.reduce_sequence(hidden, self.reduce_output)
        return {"encoder_output": hidden}

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag530')" href="javascript:;">
ludwig-0.5rc2/ludwig/encoders/text_encoders.py: 1332-1343
</a>
<div class="mid" id="frag530" style="display:none"><pre>
    def forward(self, inputs: torch.Tensor, mask: Optional[torch.Tensor] = None) -&gt; Dict[str, torch.Tensor]:
        if mask is not None:
            mask = mask.to(torch.int32)
        transformer_outputs = self.transformer(
            input_ids=inputs,
            attention_mask=mask,
            token_type_ids=torch.zeros_like(inputs),
        )
        hidden = transformer_outputs[0]
        hidden = self.reduce_sequence(hidden, self.reduce_output)
        return {"encoder_output": hidden}

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag505')" href="javascript:;">
ludwig-0.5rc2/ludwig/encoders/text_encoders.py: 801-812
</a>
<div class="mid" id="frag505" style="display:none"><pre>
    def forward(self, inputs: torch.Tensor, mask: Optional[torch.Tensor] = None) -&gt; Dict[str, torch.Tensor]:
        if mask is not None:
            mask = mask.to(torch.int32)
        transformer_outputs = self.transformer(
            input_ids=inputs,
            attention_mask=mask,
            token_type_ids=torch.zeros_like(inputs),
        )
        hidden = transformer_outputs[0]
        hidden = self.reduce_sequence(hidden, self.reduce_output)
        return {"encoder_output": hidden}

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 4:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag545')" href="javascript:;">
ludwig-0.5rc2/ludwig/encoders/text_encoders.py: 1679-1690
</a>
<div class="mid" id="frag545" style="display:none"><pre>
    def forward(self, inputs: torch.Tensor, mask: Optional[torch.Tensor] = None) -&gt; Dict[str, torch.Tensor]:
        if mask is not None:
            mask = mask.to(torch.int32)
        transformer_outputs = self.transformer(
            input_ids=inputs,
            attention_mask=mask,
            token_type_ids=torch.zeros_like(inputs),
        )
        hidden = transformer_outputs[0][:, 1:-1, :]
        hidden = self.reduce_sequence(hidden, self.reduce_output)
        return {"encoder_output": hidden}

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag550')" href="javascript:;">
ludwig-0.5rc2/ludwig/encoders/text_encoders.py: 1789-1800
</a>
<div class="mid" id="frag550" style="display:none"><pre>
    def forward(self, inputs: torch.Tensor, mask: Optional[torch.Tensor] = None) -&gt; Dict[str, torch.Tensor]:
        if mask is not None:
            mask = mask.to(torch.int32)
        transformer_outputs = self.transformer(
            input_ids=inputs,
            attention_mask=mask,
            token_type_ids=torch.zeros_like(inputs),
        )
        hidden = transformer_outputs[0][:, 1:-1, :]
        hidden = self.reduce_sequence(hidden, self.reduce_output)
        return {"encoder_output": hidden}

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<script language="JavaScript">
function ShowHide(divId) { 
    if(document.getElementById(divId).style.display == 'none') {
        document.getElementById(divId).style.display='block';
    } else { 
        document.getElementById(divId).style.display = 'none';
    } 
}
</script>
</body>
</html>

<clones>
<systeminfo processor="nicad6" system="torch-points3d-1.3.0" granularity="functions-blind" threshold="0%" minlines="10" maxlines="2500"/>
<cloneinfo npcs="1434" npairs="20"/>
<runinfo ncompares="5311" cputime="57213"/>
<classinfo nclasses="16"/>

<class classid="1" nclones="2" nlines="17" similarity="100">
<source file="systems/torch-points3d-1.3.0/scripts/test_registration_scripts/misc.py" startline="4" endline="24" pcid="49">
def read_gt_log(path):
    """
    read the gt.log of evaluation set of 3DMatch or ETH Dataset and parse it.
    """
    list_pair = []
    list_mat = []
    with open(path, "r") as f:
        all_mat = f.readlines()
    mat = np.zeros((4, 4))
    for i in range(len(all_mat)):
        if i % 5 == 0:
            if i != 0:
                list_mat.append(mat)
            mat = np.zeros((4, 4))
            list_pair.append(list(map(int, all_mat[i].split("\t")[:-1])))
        else:
            line = all_mat[i].split("\t")

            mat[i % 5 - 1] = np.asarray(line[:4], dtype=np.float)
    list_mat.append(mat)
    return list_pair, list_mat
</source>
<source file="systems/torch-points3d-1.3.0/scripts/test_registration_scripts/descriptor_matcher.py" startline="17" endline="39" pcid="50">
def read_gt_log(path):
    """
    read the gt.log of evaluation set of 3DMatch or ETH Dataset and parse it.
    """
    list_pair = []
    list_mat = []
    with open(path, "r") as f:
        all_mat = f.readlines()
    mat = np.zeros((4, 4))
    for i in range(len(all_mat)):
        if i % 5 == 0:
            if i != 0:
                list_mat.append(mat)
            mat = np.zeros((4, 4))
            list_pair.append(list(map(int, all_mat[i].split("\t")[:-1])))
        else:
            line = all_mat[i].split("\t")

            mat[i % 5 - 1] = np.asarray(line[:4], dtype=np.float)
    list_mat.append(mat)
    return list_pair, list_mat


</source>
</class>

<class classid="2" nclones="3" nlines="14" similarity="100">
<source file="systems/torch-points3d-1.3.0/torch_points3d/applications/rsconv.py" startline="122" endline="148" pcid="88">
    def forward(self, data, *args, **kwargs):
        """ This method does a forward on the Unet

        Parameters:
        -----------
        data
            A dictionary that contains the data itself and its metadata information. Should contain
                x -- Features [B, N, C]
                pos -- Points [B, N, 3]
        """
        self._set_input(data)
        data = self.input
        stack_down = [data]
        for i in range(len(self.down_modules) - 1):
            data = self.down_modules[i](data)
            stack_down.append(data)
        data = self.down_modules[-1](data)

        if not isinstance(self.inner_modules[0], Identity):
            stack_down.append(data)
            data = self.inner_modules[0](data)

        if self.has_mlp_head:
            data.x = self.mlp(data.x)
        return data


</source>
<source file="systems/torch-points3d-1.3.0/torch_points3d/applications/kpconv.py" startline="121" endline="154" pcid="120">
    def forward(self, data, *args, **kwargs):
        """
        Parameters
        -----------
        data:
            A dictionary that contains the data itself and its metadata information. Should contain
            - pos [N, 3]
            - x [N, C]
            - multiscale (optional) precomputed data for the down convolutions
            - upsample (optional) precomputed data for the up convolutions

        Returns
        --------
        data:
            - pos [1, 3] - Dummy pos
            - x [1, output_nc]
        """
        self._set_input(data)
        data = self.input
        stack_down = [data]
        for i in range(len(self.down_modules) - 1):
            data = self.down_modules[i](data)
            stack_down.append(data)
        data = self.down_modules[-1](data)

        if not isinstance(self.inner_modules[0], Identity):
            stack_down.append(data)
            data = self.inner_modules[0](data)

        if self.has_mlp_head:
            data.x = self.mlp(data.x)
        return data


</source>
<source file="systems/torch-points3d-1.3.0/torch_points3d/applications/pointnet2.py" startline="127" endline="152" pcid="139">
    def forward(self, data, *args, **kwargs):
        """
        Parameters:
        -----------
        data
            A dictionary that contains the data itself and its metadata information. Should contain
                x -- Features [B, N, C]
                pos -- Points [B, N, 3]
        """
        self._set_input(data)
        data = self.input
        stack_down = [data]
        for i in range(len(self.down_modules) - 1):
            data = self.down_modules[i](data)
            stack_down.append(data)
        data = self.down_modules[-1](data)

        if not isinstance(self.inner_modules[0], Identity):
            stack_down.append(data)
            data = self.inner_modules[0](data)

        if self.has_mlp_head:
            data.x = self.mlp(data.x)
        return data


</source>
</class>

<class classid="3" nclones="2" nlines="11" similarity="100">
<source file="systems/torch-points3d-1.3.0/torch_points3d/applications/minkowski.py" startline="130" endline="158" pcid="99">
    def forward(self, data, *args, **kwargs):
        """
        Parameters:
        -----------
        data
            A SparseTensor that contains the data itself and its metadata information. Should contain
                F -- Features [N, C]
                coords -- Coords [N, 4]

        Returns
        --------
        data:
            - x [1, output_nc]

        """
        self._set_input(data)
        data = self.input
        for i in range(len(self.down_modules)):
            data = self.down_modules[i](data)

        out = Batch(x=data.F, batch=data.C[:, 0].long().to(data.F.device))
        if not isinstance(self.inner_modules[0], Identity):
            out = self.inner_modules[0](out)

        if self.has_mlp_head:
            out.x = self.mlp(out.x)
        return out


</source>
<source file="systems/torch-points3d-1.3.0/torch_points3d/applications/sparseconv3d.py" startline="140" endline="168" pcid="130">
    def forward(self, data, *args, **kwargs):
        """
        Parameters:
        -----------
        data
            A SparseTensor that contains the data itself and its metadata information. Should contain
                F -- Features [N, C]
                coords -- Coords [N, 4]

        Returns
        --------
        data:
            - x [1, output_nc]

        """
        self._set_input(data)
        data = self.input
        for i in range(len(self.down_modules)):
            data = self.down_modules[i](data)

        out = Batch(x=data.F, batch=data.C[:, 0].long().to(data.F.device))
        if not isinstance(self.inner_modules[0], Identity):
            out = self.inner_modules[0](out)

        if self.has_mlp_head:
            out.x = self.mlp(out.x)
        return out


</source>
</class>

<class classid="4" nclones="2" nlines="23" similarity="100">
<source file="systems/torch-points3d-1.3.0/torch_points3d/models/segmentation/rsconv.py" startline="19" endline="49" pcid="223">
    def __init__(self, option, model_type, dataset, modules):
        # call the initialization method of UnwrappedUnetBasedModel
        UnwrappedUnetBasedModel.__init__(self, option, model_type, dataset, modules)
        self._num_classes = dataset.num_classes
        self._weight_classes = dataset.weight_classes
        self._use_category = getattr(option, "use_category", False)
        if self._use_category:
            if not dataset.class_to_segments:
                raise ValueError(
                    "The dataset needs to specify a class_to_segments property when using category information for segmentation"
                )
            self._num_categories = len(dataset.class_to_segments.keys())
            log.info("Using category information for the predictions with %i categories", self._num_categories)
        else:
            self._num_categories = 0

        # Last MLP
        last_mlp_opt = copy.deepcopy(option.mlp_cls)

        self.FC_layer = Seq()
        last_mlp_opt.nn[0] += self._num_categories
        for i in range(1, len(last_mlp_opt.nn)):
            self.FC_layer.append(Conv1D(last_mlp_opt.nn[i - 1], last_mlp_opt.nn[i], bn=True, bias=False))
        if last_mlp_opt.dropout:
            self.FC_layer.append(torch.nn.Dropout(p=last_mlp_opt.dropout))

        self.FC_layer.append(Conv1D(last_mlp_opt.nn[-1], self._num_classes, activation=None, bias=True, bn=False))
        self.loss_names = ["loss_seg"]

        self.visual_names = ["data_visual"]

</source>
<source file="systems/torch-points3d-1.3.0/torch_points3d/models/segmentation/pointnet2.py" startline="35" endline="65" pcid="264">
    def __init__(self, option, model_type, dataset, modules):
        # call the initialization method of UnetBasedModel
        UnetBasedModel.__init__(self, option, model_type, dataset, modules)
        self._num_classes = dataset.num_classes
        self._weight_classes = dataset.weight_classes
        self._use_category = getattr(option, "use_category", False)
        if self._use_category:
            if not dataset.class_to_segments:
                raise ValueError(
                    "The dataset needs to specify a class_to_segments property when using category information for segmentation"
                )
            self._num_categories = len(dataset.class_to_segments.keys())
            log.info("Using category information for the predictions with %i categories", self._num_categories)
        else:
            self._num_categories = 0

        # Last MLP
        last_mlp_opt = copy.deepcopy(option.mlp_cls)

        self.FC_layer = Seq()
        last_mlp_opt.nn[0] += self._num_categories
        for i in range(1, len(last_mlp_opt.nn)):
            self.FC_layer.append(Conv1D(last_mlp_opt.nn[i - 1], last_mlp_opt.nn[i], bn=True, bias=False))
        if last_mlp_opt.dropout:
            self.FC_layer.append(torch.nn.Dropout(p=last_mlp_opt.dropout))

        self.FC_layer.append(Conv1D(last_mlp_opt.nn[-1], self._num_classes, activation=None, bias=True, bn=False))
        self.loss_names = ["loss_seg"]

        self.visual_names = ["data_visual"]

</source>
</class>

<class classid="5" nclones="2" nlines="27" similarity="100">
<source file="systems/torch-points3d-1.3.0/torch_points3d/models/segmentation/kpconv.py" startline="106" endline="141" pcid="237">
    def forward(self, *args, **kwargs) -> Any:
        """Run forward pass. This will be called by both functions <optimize_parameters> and <test>."""
        stack_down = []

        data = self.input
        for i in range(len(self.down_modules) - 1):
            data = self.down_modules[i](data, precomputed=self.pre_computed)
            stack_down.append(data)

        data = self.down_modules[-1](data, precomputed=self.pre_computed)
        innermost = False

        if not isinstance(self.inner_modules[0], Identity):
            stack_down.append(data)
            data = self.inner_modules[0](data)
            innermost = True

        for i in range(len(self.up_modules)):
            if i == 0 and innermost:
                data = self.up_modules[i]((data, stack_down.pop()))
            else:
                data = self.up_modules[i]((data, stack_down.pop()), precomputed=self.upsample)

        last_feature = data.x
        if self._use_category:
            self.output = self.FC_layer(last_feature, self.category)
        else:
            self.output = self.FC_layer(last_feature)

        if self.labels is not None:
            self.compute_loss()

        self.data_visual = self.input
        self.data_visual.pred = torch.max(self.output, -1)[1]
        return self.output

</source>
<source file="systems/torch-points3d-1.3.0/torch_points3d/models/segmentation/ppnet.py" startline="100" endline="135" pcid="256">
    def forward(self, *args, **kwargs) -> Any:
        """Run forward pass. This will be called by both functions <optimize_parameters> and <test>."""
        stack_down = []

        data = self.input
        for i in range(len(self.down_modules) - 1):
            data = self.down_modules[i](data, precomputed=self.pre_computed)
            stack_down.append(data)

        data = self.down_modules[-1](data, precomputed=self.pre_computed)
        innermost = False

        if not isinstance(self.inner_modules[0], Identity):
            stack_down.append(data)
            data = self.inner_modules[0](data)
            innermost = True

        for i in range(len(self.up_modules)):
            if i == 0 and innermost:
                data = self.up_modules[i]((data, stack_down.pop()))
            else:
                data = self.up_modules[i]((data, stack_down.pop()), precomputed=self.upsample)

        last_feature = data.x
        if self._use_category:
            self.output = self.FC_layer(last_feature, self.category)
        else:
            self.output = self.FC_layer(last_feature)

        if self.labels is not None:
            self.compute_loss()

        self.data_visual = self.input
        self.data_visual.pred = torch.max(self.output, -1)[1]
        return self.output

</source>
</class>

<class classid="6" nclones="3" nlines="16" similarity="100">
<source file="systems/torch-points3d-1.3.0/torch_points3d/models/base_architectures/unet.py" startline="198" endline="217" pcid="275">
    def _fetch_arguments_from_list(self, opt, index):
        """Fetch the arguments for a single convolution from multiple lists
        of arguments - for models specified in the compact format.
        """
        args = {}
        for o, v in opt.items():
            name = str(o)
            if is_list(v) and len(getattr(opt, o)) > 0:
                if name[-1] == "s" and name not in SPECIAL_NAMES:
                    name = name[:-1]
                v_index = v[index]
                if is_list(v_index):
                    v_index = list(v_index)
                args[name] = v_index
            else:
                if is_list(v):
                    v = list(v)
                args[name] = v
        return args

</source>
<source file="systems/torch-points3d-1.3.0/torch_points3d/models/base_architectures/backbone.py" startline="110" endline="129" pcid="299">
    def _fetch_arguments_from_list(self, opt, index):
        """Fetch the arguments for a single convolution from multiple lists
        of arguments - for models specified in the compact format.
        """
        args = {}
        for o, v in opt.items():
            name = str(o)
            if is_list(v) and len(getattr(opt, o)) > 0:
                if name[-1] == "s" and name not in SPECIAL_NAMES:
                    name = name[:-1]
                v_index = v[index]
                if is_list(v_index):
                    v_index = list(v_index)
                args[name] = v_index
            else:
                if is_list(v):
                    v = list(v)
                args[name] = v
        return args

</source>
<source file="systems/torch-points3d-1.3.0/torch_points3d/models/base_architectures/unet.py" startline="456" endline="475" pcid="290">
    def _fetch_arguments_from_list(self, opt, index):
        """Fetch the arguments for a single convolution from multiple lists
        of arguments - for models specified in the compact format.
        """
        args = {}
        for o, v in opt.items():
            name = str(o)
            if is_list(v) and len(getattr(opt, o)) > 0:
                if name[-1] == "s" and name not in SPECIAL_NAMES:
                    name = name[:-1]
                v_index = v[index]
                if is_list(v_index):
                    v_index = list(v_index)
                args[name] = v_index
            else:
                if is_list(v):
                    v = list(v)
                args[name] = v
        return args

</source>
</class>

<class classid="7" nclones="2" nlines="25" similarity="100">
<source file="systems/torch-points3d-1.3.0/torch_points3d/datasets/registration/test3dmatch.py" startline="119" endline="148" pcid="383">
    def __init__(self, dataset_opt):
        super().__init__(dataset_opt)
        pre_transform = self.pre_transform
        ss_transform = getattr(self, "ss_transform", None)
        train_transform = self.train_transform
        test_transform = self.test_transform

        self.train_dataset = TestPair3DMatch(
            root=self._data_path,
            pre_transform=pre_transform,
            transform=train_transform,
            num_pos_pairs=dataset_opt.num_pos_pairs,
            max_dist_overlap=dataset_opt.max_dist_overlap,
            self_supervised=True,
            min_size_block=dataset_opt.min_size_block,
            max_size_block=dataset_opt.max_size_block,
            ss_transform=ss_transform,
            min_points=dataset_opt.min_points,
            use_fps=dataset_opt.use_fps
        )

        self.test_dataset = TestPair3DMatch(
                root=self._data_path,
                pre_transform=pre_transform,
                transform=test_transform,
                num_pos_pairs=50,
                max_dist_overlap=dataset_opt.max_dist_overlap,
                self_supervised=False
            )

</source>
<source file="systems/torch-points3d-1.3.0/torch_points3d/datasets/registration/testeth.py" startline="234" endline="262" pcid="436">
    def __init__(self, dataset_opt):
        super().__init__(dataset_opt)
        pre_transform = self.pre_transform
        ss_transform = getattr(self, "ss_transform", None)
        train_transform = self.train_transform
        test_transform = self.test_transform

        self.train_dataset = TestPairETH2(
            root=self._data_path,
            pre_transform=pre_transform,
            transform=train_transform,
            num_pos_pairs=dataset_opt.num_pos_pairs,
            max_dist_overlap=dataset_opt.max_dist_overlap,
            self_supervised=True,
            min_size_block=dataset_opt.min_size_block,
            max_size_block=dataset_opt.max_size_block,
            ss_transform=ss_transform,
            min_points=dataset_opt.min_points,
            use_fps=dataset_opt.use_fps
        )

        self.test_dataset = TestPairETH2(
                root=self._data_path,
                pre_transform=pre_transform,
                transform=test_transform,
                num_pos_pairs=50,
                max_dist_overlap=dataset_opt.max_dist_overlap,
                self_supervised=False
            )
</source>
</class>

<class classid="8" nclones="2" nlines="29" similarity="100">
<source file="systems/torch-points3d-1.3.0/torch_points3d/datasets/registration/testkaist.py" startline="24" endline="53" pcid="411">
    def __init__(self, root,
                 transform=None,
                 pre_transform=None,
                 pre_filter=None,
                 verbose=False,
                 debug=False,
                 num_pos_pairs=200,
                 max_dist_overlap=0.01,
                 self_supervised=False,
                 min_size_block=2,
                 max_size_block=3,
                 min_points=500,
                 ss_transform=None,
                 use_fps=False):
        self.link_pairs = "https://cloud.mines-paristech.fr/index.php/s/4cTpY4CKPAXFGk4/download"
        BasePCRBTest.__init__(self,
                              root=root,
                              transform=transform,
                              pre_transform=pre_transform,
                              pre_filter=pre_filter,
                              verbose=verbose, debug=debug,
                              max_dist_overlap=max_dist_overlap,
                              num_pos_pairs=num_pos_pairs,
                              self_supervised=self_supervised,
                              min_size_block=min_size_block,
                              max_size_block=max_size_block,
                              min_points=min_points,
                              ss_transform=ss_transform,
                              use_fps=use_fps)

</source>
<source file="systems/torch-points3d-1.3.0/torch_points3d/datasets/registration/testplanetary.py" startline="36" endline="65" pcid="459">
    def __init__(self, root,
                 transform=None,
                 pre_transform=None,
                 pre_filter=None,
                 verbose=False,
                 debug=False,
                 num_pos_pairs=200,
                 max_dist_overlap=0.01,
                 self_supervised=False,
                 min_size_block=2,
                 max_size_block=3,
                 min_points=500,
                 ss_transform=None,
                 use_fps=False):
        self.link_pairs = "https://cloud.mines-paristech.fr/index.php/s/7cqiTMIIqwvMOtA/download"
        BasePCRBTest.__init__(self,
                              root=root,
                              transform=transform,
                              pre_transform=pre_transform,
                              pre_filter=pre_filter,
                              verbose=verbose, debug=debug,
                              max_dist_overlap=max_dist_overlap,
                              num_pos_pairs=num_pos_pairs,
                              self_supervised=self_supervised,
                              min_size_block=min_size_block,
                              max_size_block=max_size_block,
                              min_points=min_points,
                              ss_transform=ss_transform,
                              use_fps=use_fps)

</source>
</class>

<class classid="9" nclones="2" nlines="25" similarity="100">
<source file="systems/torch-points3d-1.3.0/torch_points3d/datasets/registration/testkaist.py" startline="84" endline="111" pcid="414">
    def __init__(self, dataset_opt):

        super().__init__(dataset_opt)
        pre_transform = self.pre_transform
        train_transform = self.train_transform
        ss_transform = getattr(self, "ss_transform", None)
        test_transform = self.test_transform

        # training is similar to test but only unsupervised training is allowed XD
        self.train_dataset = TestPairKaist(
            root=self._data_path,
            pre_transform=pre_transform,
            transform=train_transform,
            max_dist_overlap=dataset_opt.max_dist_overlap,
            self_supervised=True,
            min_size_block=dataset_opt.min_size_block,
            max_size_block=dataset_opt.max_size_block,
            num_pos_pairs=dataset_opt.num_pos_pairs,
            min_points=dataset_opt.min_points,
            ss_transform=ss_transform,
            use_fps=dataset_opt.use_fps)
        self.test_dataset = TestPairKaist(
            root=self._data_path,
            pre_transform=pre_transform,
            transform=test_transform,
            max_dist_overlap=dataset_opt.max_dist_overlap,
            num_pos_pairs=dataset_opt.num_pos_pairs,
            self_supervised=False)
</source>
<source file="systems/torch-points3d-1.3.0/torch_points3d/datasets/registration/testplanetary.py" startline="114" endline="142" pcid="462">
    def __init__(self, dataset_opt):

        super().__init__(dataset_opt)
        pre_transform = self.pre_transform
        train_transform = self.train_transform
        ss_transform = getattr(self, "ss_transform", None)
        test_transform = self.test_transform

        # training is similar to test but only unsupervised training is allowed XD
        self.train_dataset = TestPairPlanetary(
            root=self._data_path,
            pre_transform=pre_transform,
            transform=train_transform,
            max_dist_overlap=dataset_opt.max_dist_overlap,
            self_supervised=True,
            min_size_block=dataset_opt.min_size_block,
            max_size_block=dataset_opt.max_size_block,
            num_pos_pairs=dataset_opt.num_pos_pairs,
            min_points=dataset_opt.min_points,
            ss_transform=ss_transform,
            use_fps=dataset_opt.use_fps
        )
        self.test_dataset = TestPairPlanetary(
            root=self._data_path,
            pre_transform=pre_transform,
            transform=test_transform,
            max_dist_overlap=dataset_opt.max_dist_overlap,
            num_pos_pairs=dataset_opt.num_pos_pairs,
            self_supervised=False)
</source>
</class>

<class classid="10" nclones="2" nlines="23" similarity="100">
<source file="systems/torch-points3d-1.3.0/torch_points3d/datasets/registration/testtum.py" startline="91" endline="116" pcid="425">
    def __init__(self, dataset_opt):

        super().__init__(dataset_opt)
        pre_transform = self.pre_transform
        train_transform = self.train_transform
        ss_transform = getattr(self, "ss_transform", None)
        test_transform = self.test_transform

        # training is similar to test but only unsupervised training is allowed XD
        self.train_dataset = TestPairTUM(root=self._data_path,
                                         pre_transform=pre_transform,
                                         transform=train_transform,
                                         max_dist_overlap=dataset_opt.max_dist_overlap,
                                         self_supervised=True,
                                         min_size_block=dataset_opt.min_size_block,
                                         max_size_block=dataset_opt.max_size_block,
                                         num_pos_pairs=dataset_opt.num_pos_pairs,
                                         min_points=dataset_opt.min_points,
                                         ss_transform=ss_transform,
                                         use_fps=dataset_opt.use_fps)
        self.test_dataset = TestPairTUM(root=self._data_path,
                                        pre_transform=pre_transform,
                                        transform=test_transform,
                                        max_dist_overlap=dataset_opt.max_dist_overlap,
                                        num_pos_pairs=dataset_opt.num_pos_pairs,
                                        self_supervised=False)
</source>
<source file="systems/torch-points3d-1.3.0/torch_points3d/datasets/registration/testeth.py" startline="139" endline="166" pcid="430">
    def __init__(self, dataset_opt):

        super().__init__(dataset_opt)
        pre_transform = self.pre_transform
        train_transform = self.train_transform
        ss_transform = getattr(self, "ss_transform", None)
        test_transform = self.test_transform

        # training is similar to test but only unsupervised training is allowed XD
        self.train_dataset = TestPairETH(root=self._data_path,
                                         pre_transform=pre_transform,
                                         transform=train_transform,
                                         max_dist_overlap=dataset_opt.max_dist_overlap,
                                         self_supervised=True,
                                         min_size_block=dataset_opt.min_size_block,
                                         max_size_block=dataset_opt.max_size_block,
                                         num_pos_pairs=dataset_opt.num_pos_pairs,
                                         min_points=dataset_opt.min_points,
                                         ss_transform=ss_transform,
                                         use_fps=dataset_opt.use_fps)
        self.test_dataset = TestPairETH(root=self._data_path,
                                        pre_transform=pre_transform,
                                        transform=test_transform,
                                        max_dist_overlap=dataset_opt.max_dist_overlap,
                                        num_pos_pairs=dataset_opt.num_pos_pairs,
                                        self_supervised=False)


</source>
</class>

<class classid="11" nclones="2" nlines="17" similarity="100">
<source file="systems/torch-points3d-1.3.0/torch_points3d/modules/RSConv/dense.py" startline="145" endline="166" pcid="996">
    def _prepare_features(self, x, pos, new_pos, idx):
        new_pos_trans = pos.transpose(1, 2).contiguous()
        grouped_pos_absolute = tp.grouping_operation(new_pos_trans, idx)  # (B, 3, npoint, nsample)
        centroids = new_pos.transpose(1, 2).unsqueeze(-1)
        grouped_pos_normalized = grouped_pos_absolute - centroids

        if x is not None:
            grouped_features = tp.grouping_operation(x, idx)
            if self.use_xyz:
                new_features = torch.cat(
                    [grouped_pos_absolute, grouped_pos_normalized, grouped_features], dim=1
                )  # (B, 3 + 3 + C, npoint, nsample)
            else:
                new_features = grouped_features
        else:
            assert self.use_xyz, "Cannot have not features and not use xyz as a feature!"
            new_features = torch.cat(
                [grouped_pos_absolute, grouped_pos_normalized], dim=1
            )  # (B, 3 + 3 npoint, nsample)

        return new_features, centroids

</source>
<source file="systems/torch-points3d-1.3.0/torch_points3d/modules/RSConv/dense.py" startline="441" endline="462" pcid="1007">
    def _prepare_features(self, x, pos, new_pos, idx):
        new_pos_trans = pos.transpose(1, 2).contiguous()
        grouped_pos_absolute = tp.grouping_operation(new_pos_trans, idx)  # (B, 3, npoint, nsample)
        centroids = new_pos.transpose(1, 2).unsqueeze(-1)
        grouped_pos_normalized = grouped_pos_absolute - centroids

        if x is not None:
            grouped_features = tp.grouping_operation(x, idx)
            if self.use_xyz:
                new_features = torch.cat(
                    [grouped_pos_absolute, grouped_pos_normalized, grouped_features], dim=1
                )  # (B, 3 + 3 + C, npoint, nsample)
            else:
                new_features = grouped_features
        else:
            assert self.use_xyz, "Cannot have not features and not use xyz as a feature!"
            new_features = torch.cat(
                [grouped_pos_absolute, grouped_pos_normalized], dim=1
            )  # (B, 3 + 3 npoint, nsample)

        return new_features, centroids

</source>
</class>

<class classid="12" nclones="2" nlines="41" similarity="100">
<source file="systems/torch-points3d-1.3.0/torch_points3d/modules/MinkowskiEngine/networks.py" startline="187" endline="247" pcid="1063">
    def forward(self, x):
        out = self.conv0p1s1(x)
        out = self.bn0(out)
        out_p1 = self.relu(out)

        out = self.conv1p1s2(out_p1)
        out = self.bn1(out)
        out = self.relu(out)
        out_b1p2 = self.block1(out)

        out = self.conv2p2s2(out_b1p2)
        out = self.bn2(out)
        out = self.relu(out)
        out_b2p4 = self.block2(out)

        out = self.conv3p4s2(out_b2p4)
        out = self.bn3(out)
        out = self.relu(out)
        out_b3p8 = self.block3(out)

        # tensor_stride=16
        out = self.conv4p8s2(out_b3p8)
        out = self.bn4(out)
        out = self.relu(out)
        out = self.block4(out)

        # tensor_stride=8
        out = self.convtr4p16s2(out)
        out = self.bntr4(out)
        out = self.relu(out)

        out = ME.cat(out, out_b3p8)
        out = self.block5(out)

        # tensor_stride=4
        out = self.convtr5p8s2(out)
        out = self.bntr5(out)
        out = self.relu(out)

        out = ME.cat(out, out_b2p4)
        out = self.block6(out)

        # tensor_stride=2
        out = self.convtr6p4s2(out)
        out = self.bntr6(out)
        out = self.relu(out)

        out = ME.cat(out, out_b1p2)
        out = self.block7(out)

        # tensor_stride=1
        out = self.convtr7p2s2(out)
        out = self.bntr7(out)
        out = self.relu(out)

        out = ME.cat(out, out_p1)
        out = self.block8(out)

        return self.final(out)


</source>
<source file="systems/torch-points3d-1.3.0/torch_points3d/modules/MinkowskiEngine/res16unet.py" startline="450" endline="510" pcid="1108">
    def forward(self, x):
        out = self.conv0p1s1(x)
        out = self.bn0(out)
        out_p1 = self.relu(out)

        out = self.conv1p1s2(out_p1)
        out = self.bn1(out)
        out = self.relu(out)
        out_b1p2 = self.block1(out)

        out = self.conv2p2s2(out_b1p2)
        out = self.bn2(out)
        out = self.relu(out)
        out_b2p4 = self.block2(out)

        out = self.conv3p4s2(out_b2p4)
        out = self.bn3(out)
        out = self.relu(out)
        out_b3p8 = self.block3(out)

        # pixel_dist=16
        out = self.conv4p8s2(out_b3p8)
        out = self.bn4(out)
        out = self.relu(out)
        out = self.block4(out)

        # pixel_dist=8
        out = self.convtr4p16s2(out)
        out = self.bntr4(out)
        out = self.relu(out)

        out = me.cat(out, out_b3p8)
        out = self.block5(out)

        # pixel_dist=4
        out = self.convtr5p8s2(out)
        out = self.bntr5(out)
        out = self.relu(out)

        out = me.cat(out, out_b2p4)
        out = self.block6(out)

        # pixel_dist=2
        out = self.convtr6p4s2(out)
        out = self.bntr6(out)
        out = self.relu(out)

        out = me.cat(out, out_b1p2)
        out = self.block7(out)

        # pixel_dist=1
        out = self.convtr7p2s2(out)
        out = self.bntr7(out)
        out = self.relu(out)

        out = me.cat(out, out_p1)
        out = self.block8(out)

        return self.final(out)


</source>
</class>

<class classid="13" nclones="2" nlines="12" similarity="100">
<source file="systems/torch-points3d-1.3.0/torch_points3d/modules/MinkowskiEngine/modules.py" startline="42" endline="60" pcid="1065">
    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.norm1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.norm2(out)

        if self.downsample is not None:
            residual = self.downsample(x)

        out += residual
        out = self.relu(out)

        return out


</source>
<source file="systems/torch-points3d-1.3.0/torch_points3d/modules/MinkowskiEngine/res16unet.py" startline="36" endline="54" pcid="1096">
    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.norm1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.norm2(out)

        if self.downsample is not None:
            residual = self.downsample(x)

        out += residual
        out = self.relu(out)

        return out


</source>
</class>

<class classid="14" nclones="2" nlines="15" similarity="100">
<source file="systems/torch-points3d-1.3.0/torch_points3d/modules/MinkowskiEngine/modules.py" startline="82" endline="104" pcid="1067">
    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.norm1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.norm2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.norm3(out)

        if self.downsample is not None:
            residual = self.downsample(x)

        out += residual
        out = self.relu(out)

        return out


</source>
<source file="systems/torch-points3d-1.3.0/torch_points3d/modules/MinkowskiEngine/res16unet.py" startline="95" endline="117" pcid="1098">
    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.norm1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.norm2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.norm3(out)

        if self.downsample is not None:
            residual = self.downsample(x)

        out += residual
        out = self.relu(out)

        return out


</source>
</class>

<class classid="15" nclones="2" nlines="18" similarity="100">
<source file="systems/torch-points3d-1.3.0/torch_points3d/modules/SparseConv3d/nn/minkowski.py" startline="6" endline="25" pcid="1122">
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int = 3,
        stride: int = 1,
        dilation: int = 1,
        bias: bool = False,
    ) -> None:
        super().__init__(
            in_channels,
            out_channels,
            kernel_size=kernel_size,
            stride=stride,
            dilation=dilation,
            bias=bias,
            dimension=3,
        )


</source>
<source file="systems/torch-points3d-1.3.0/torch_points3d/modules/SparseConv3d/nn/minkowski.py" startline="27" endline="46" pcid="1123">
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int = 3,
        stride: int = 1,
        dilation: int = 1,
        bias: bool = False,
    ) -> None:
        super().__init__(
            in_channels,
            out_channels,
            kernel_size=kernel_size,
            stride=stride,
            dilation=dilation,
            bias=bias,
            dimension=3,
        )


</source>
</class>

<class classid="16" nclones="2" nlines="12" similarity="100">
<source file="systems/torch-points3d-1.3.0/test/test_shapenetforward.py" startline="60" endline="72" pcid="1373">
    def test_predictupsampledense(self):
        dataset = ForwardShapenetDataset(self.config)
        dataset.create_dataloaders(MockModel(DictConfig({"conv_type": "DENSE"})), 2, False, 1, False)
        forward_set = dataset.test_dataloaders[0]
        for b in forward_set:
            output = torch.tensor([[1, 0], [1, 0], [0, 1], [0, 1]])
            predicted = dataset.predict_original_samples(b, "DENSE", output)
            self.assertEqual(len(predicted), 2)
            self.assertEqual(predicted["example1.txt"].shape, (3, 4))
            self.assertEqual(predicted["example2.txt"].shape, (4, 4))
            npt.assert_allclose(predicted["example1.txt"][:, -1], np.asarray([0, 0, 0]))
            npt.assert_allclose(predicted["example2.txt"][:, -1], np.asarray([1, 1, 1, 1]))

</source>
<source file="systems/torch-points3d-1.3.0/test/test_shapenetforward.py" startline="73" endline="85" pcid="1374">
    def test_predictupsamplepartialdense(self):
        dataset = ForwardShapenetDataset(self.config)
        dataset.create_dataloaders(MockModel(DictConfig({"conv_type": "PARTIAL_DENSE"})), 2, False, 1, False)
        forward_set = dataset.test_dataloaders[0]
        for b in forward_set:
            output = torch.tensor([[1, 0], [1, 0], [0, 1], [0, 1]])
            predicted = dataset.predict_original_samples(b, "PARTIAL_DENSE", output)
            self.assertEqual(len(predicted), 2)
            self.assertEqual(predicted["example1.txt"].shape, (3, 4))
            self.assertEqual(predicted["example2.txt"].shape, (4, 4))
            npt.assert_allclose(predicted["example1.txt"][:, -1], np.asarray([0, 0, 0]))
            npt.assert_allclose(predicted["example2.txt"][:, -1], np.asarray([1, 1, 1, 1]))

</source>
</class>

</clones>

<html>
<head>
    <title>NiCad6 Clone Report</title>
    <style type="text/css">
        body {font-family:sans-serif;}
        table {background-color:white; border:0px; padding:0px; border-spacing:4px; width:auto; margin-left:30px; margin-right:auto;}
        td {background-color:rgba(192,212,238,0.8); border:0px; padding:8px; width:auto; vertical-align:top; border-radius:8px}
        pre {background-color:white; padding:4px;}
        a {color:darkblue;}
    </style>
</head>
<body>
<h2>NiCad6 Clone Report</h2>
<table>
<tr style="font-size:14pt">
<td><b>System:</b> &nbsp; pytorchvideo-0.1.3</td>
<td><b>Clone pairs:</b> &nbsp; 14</td>
<td><b>Clone classes:</b> &nbsp; 10</td>
</tr>
<tr style="font-size:12pt">
<td style="background-color:white">Clone type: &nbsp; 2</td>
<td style="background-color:white">Granularity: &nbsp; functions-blind</td>
<td style="background-color:white">Max diff threshold: &nbsp; 0%</td>
<td style="background-color:white">Clone size: &nbsp; 10 - 2500 lines</td>
<td style="background-color:white">Total functions-blind: &nbsp; 475</td>
</tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 1:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag14')" href="javascript:;">
pytorchvideo-0.1.3/tutorials/video_classification_example/train.py: 296-318
</a>
<div class="mid" id="frag14" style="display:none"><pre>
    def train_dataloader(self):
        """
        Defines the train DataLoader that the PyTorch Lightning Trainer trains/tests with.
        """
        sampler = DistributedSampler if self.trainer.use_ddp else RandomSampler
        train_transform = self._make_transforms(mode="train")
        self.train_dataset = LimitDataset(
            pytorchvideo.data.Kinetics(
                data_path=os.path.join(self.args.data_path, "train.csv"),
                clip_sampler=pytorchvideo.data.make_clip_sampler(
                    "random", self.args.clip_duration
                ),
                video_path_prefix=self.args.video_path_prefix,
                transform=train_transform,
                video_sampler=sampler,
            )
        )
        return torch.utils.data.DataLoader(
            self.train_dataset,
            batch_size=self.args.batch_size,
            num_workers=self.args.workers,
        )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag15')" href="javascript:;">
pytorchvideo-0.1.3/tutorials/video_classification_example/train.py: 319-342
</a>
<div class="mid" id="frag15" style="display:none"><pre>
    def val_dataloader(self):
        """
        Defines the train DataLoader that the PyTorch Lightning Trainer trains/tests with.
        """
        sampler = DistributedSampler if self.trainer.use_ddp else RandomSampler
        val_transform = self._make_transforms(mode="val")
        self.val_dataset = LimitDataset(
            pytorchvideo.data.Kinetics(
                data_path=os.path.join(self.args.data_path, "val.csv"),
                clip_sampler=pytorchvideo.data.make_clip_sampler(
                    "uniform", self.args.clip_duration
                ),
                video_path_prefix=self.args.video_path_prefix,
                transform=val_transform,
                video_sampler=sampler,
            )
        )
        return torch.utils.data.DataLoader(
            self.val_dataset,
            batch_size=self.args.batch_size,
            num_workers=self.args.workers,
        )


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 2:</b> &nbsp; 2 fragments, nominal size 19 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag52')" href="javascript:;">
pytorchvideo-0.1.3/pytorchvideo/data/kinetics.py: 17-70
</a>
<div class="mid" id="frag52" style="display:none"><pre>
def Kinetics(
    data_path: str,
    clip_sampler: ClipSampler,
    video_sampler: Type[torch.utils.data.Sampler] = torch.utils.data.RandomSampler,
    transform: Optional[Callable[[Dict[str, Any]], Dict[str, Any]]] = None,
    video_path_prefix: str = "",
    decode_audio: bool = True,
    decoder: str = "pyav",
) -&gt; LabeledVideoDataset:
    """
    A helper function to create ``LabeledVideoDataset`` object for the Kinetics dataset.

    Args:
        data_path (str): Path to the data. The path type defines how the data
            should be read:

            * For a file path, the file is read and each line is parsed into a
              video path and label.
            * For a directory, the directory structure defines the classes
              (i.e. each subdirectory is a class).

        clip_sampler (ClipSampler): Defines how clips should be sampled from each
                video. See the clip sampling documentation for more information.

        video_sampler (Type[torch.utils.data.Sampler]): Sampler for the internal
                video container. This defines the order videos are decoded and,
                if necessary, the distributed split.

        transform (Callable): This callable is evaluated on the clip output before
                the clip is returned. It can be used for user defined preprocessing and
                augmentations to the clips. See the ``LabeledVideoDataset`` class for clip
                output format.

        video_path_prefix (str): Path to root directory with the videos that are
                loaded in ``LabeledVideoDataset``. All the video paths before loading
                are prefixed with this path.

        decode_audio (bool): If True, also decode audio from video.

        decoder (str): Defines what type of decoder used to decode a video.

    """

    torch._C._log_api_usage_once("PYTORCHVIDEO.dataset.Kinetics")

    return labeled_video_dataset(
        data_path,
        clip_sampler,
        video_sampler,
        transform,
        video_path_prefix,
        decode_audio,
        decoder,
    )
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag79')" href="javascript:;">
pytorchvideo-0.1.3/pytorchvideo/data/ucf101.py: 17-70
</a>
<div class="mid" id="frag79" style="display:none"><pre>
def Ucf101(
    data_path: str,
    clip_sampler: ClipSampler,
    video_sampler: Type[torch.utils.data.Sampler] = torch.utils.data.RandomSampler,
    transform: Optional[Callable[[Dict[str, Any]], Dict[str, Any]]] = None,
    video_path_prefix: str = "",
    decode_audio: bool = True,
    decoder: str = "pyav",
) -&gt; LabeledVideoDataset:
    """
    A helper function to create ``LabeledVideoDataset`` object for the Ucf101 dataset.

    Args:
        data_path (str): Path to the data. The path type defines how the data
            should be read:

            * For a file path, the file is read and each line is parsed into a
              video path and label.
            * For a directory, the directory structure defines the classes
              (i.e. each subdirectory is a class).

        clip_sampler (ClipSampler): Defines how clips should be sampled from each
                video. See the clip sampling documentation for more information.

        video_sampler (Type[torch.utils.data.Sampler]): Sampler for the internal
                video container. This defines the order videos are decoded and,
                if necessary, the distributed split.

        transform (Callable): This callable is evaluated on the clip output before
                the clip is returned. It can be used for user defined preprocessing and
                augmentations to the clips. See the ``LabeledVideoDataset`` class for clip
                output format.

        video_path_prefix (str): Path to root directory with the videos that are
                loaded in ``LabeledVideoDataset``. All the video paths before loading
                are prefixed with this path.

        decode_audio (bool): If True, also decode audio from video.

        decoder (str): Defines what type of decoder used to decode a video.

    """

    torch._C._log_api_usage_once("PYTORCHVIDEO.dataset.Ucf101")

    return labeled_video_dataset(
        data_path,
        clip_sampler,
        video_sampler,
        transform,
        video_path_prefix,
        decode_audio,
        decoder,
    )
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 3:</b> &nbsp; 2 fragments, nominal size 21 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag114')" href="javascript:;">
pytorchvideo-0.1.3/pytorchvideo/accelerator/deployment/mobile_cpu/transmuter/transmuter_mobile_cpu.py: 130-161
</a>
<div class="mid" id="frag114" style="display:none"><pre>
def transmute_Conv3d3x1x1BnAct(input_module: nn.Module):
    """
    Given an input_module, transmutes it into a equivalent Conv3d3x1x1BnAct.
    Returns None if no equivalent Conv3d3x1x1BnAct is found, else returns
    an instance of equivalent Conv3d3x1x1BnAct.
    Args:
        input_module (nn.Module): input module to find an equivalent Conv3d3x1x1BnAct
    """
    if not isinstance(input_module, nn.Conv3d):
        return None

    if (
        input_module.kernel_size == (3, 1, 1)
        and input_module.stride == (1, 1, 1)
        and input_module.padding == (1, 0, 0)
        and input_module.dilation == (1, 1, 1)
        and input_module.padding_mode == "zeros"
    ):
        module = Conv3d3x1x1BnAct(
            in_channels=input_module.in_channels,
            out_channels=input_module.out_channels,
            bias=False if input_module.bias is None else True,
            groups=input_module.groups,
            activation="identity",
            use_bn=False,
        )
        module.kernel.conv.load_state_dict(input_module.state_dict())
        return module
    else:
        return None


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag115')" href="javascript:;">
pytorchvideo-0.1.3/pytorchvideo/accelerator/deployment/mobile_cpu/transmuter/transmuter_mobile_cpu.py: 162-193
</a>
<div class="mid" id="frag115" style="display:none"><pre>
def transmute_Conv3d5x1x1BnAct(input_module: nn.Module):
    """
    Given an input_module, transmutes it into a equivalent Conv3d5x1x1BnAct.
    Returns None if no equivalent Conv3d5x1x1BnAct is found, else returns
    an instance of equivalent Conv3d5x1x1BnAct.
    Args:
        input_module (nn.Module): input module to find an equivalent Conv3d5x1x1BnAct
    """
    if not isinstance(input_module, nn.Conv3d):
        return None

    if (
        input_module.kernel_size == (5, 1, 1)
        and input_module.stride == (1, 1, 1)
        and input_module.padding == (2, 0, 0)
        and input_module.dilation == (1, 1, 1)
        and input_module.padding_mode == "zeros"
    ):
        module = Conv3d5x1x1BnAct(
            in_channels=input_module.in_channels,
            out_channels=input_module.out_channels,
            bias=False if input_module.bias is None else True,
            groups=input_module.groups,
            activation="identity",
            use_bn=False,
        )
        module.kernel.conv.load_state_dict(input_module.state_dict())
        return module
    else:
        return None


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 4:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag174')" href="javascript:;">
pytorchvideo-0.1.3/pytorchvideo/transforms/mix.py: 35-53
</a>
<div class="mid" id="frag174" style="display:none"><pre>
    def __init__(
        self,
        alpha: float = 1.0,
        label_smoothing: float = 0.0,
        num_classes: int = 400,
    ) -&gt; None:
        """
        This implements MixUp for videos.

        Args:
            alpha (float): Mixup alpha value.
            label_smoothing (float): Label smoothing value.
            num_classes (int): Number of total classes.
        """
        super().__init__()
        self.mixup_beta_sampler = torch.distributions.beta.Beta(alpha, alpha)
        self.label_smoothing = label_smoothing
        self.num_classes = num_classes

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag176')" href="javascript:;">
pytorchvideo-0.1.3/pytorchvideo/transforms/mix.py: 85-103
</a>
<div class="mid" id="frag176" style="display:none"><pre>
    def __init__(
        self,
        alpha: float = 1.0,
        label_smoothing: float = 0.0,
        num_classes: int = 400,
    ) -&gt; None:
        """
        This implements CutMix for videos.

        Args:
            alpha (float): CutMix alpha value.
            label_smoothing (float): Label smoothing value.
            num_classes (int): Number of total classes.
        """
        super().__init__()
        self.cutmix_beta_sampler = torch.distributions.beta.Beta(alpha, alpha)
        self.label_smoothing = label_smoothing
        self.num_classes = num_classes

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 5:</b> &nbsp; 3 fragments, nominal size 16 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag268')" href="javascript:;">
pytorchvideo-0.1.3/tests/test_models_stem.py: 279-303
</a>
<div class="mid" id="frag268" style="display:none"><pre>
    def _get_inputs(input_dim: int = 3) -&gt; torch.tensor:
        """
        Provide different tensors as test cases.

        Yield:
            (torch.tensor): tensor as test case input.
        """
        # Prepare random tensor as test cases.
        shapes = (
            # Forward succeeded.
            (1, input_dim, 3, 7, 7),
            (1, input_dim, 5, 7, 7),
            (1, input_dim, 7, 7, 7),
            (2, input_dim, 3, 7, 7),
            (4, input_dim, 3, 7, 7),
            (8, input_dim, 3, 7, 7),
            (2, input_dim, 3, 7, 14),
            (2, input_dim, 3, 14, 7),
            (2, input_dim, 3, 14, 14),
            # Forward failed.
            (8, input_dim * 2, 3, 7, 7),
            (8, input_dim * 4, 5, 7, 7),
        )
        for shape in shapes:
            yield torch.rand(shape)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag466')" href="javascript:;">
pytorchvideo-0.1.3/tests/test_layers_convolutions.py: 98-124
</a>
<div class="mid" id="frag466" style="display:none"><pre>
    def _get_inputs(input_dim: int = 3) -&gt; torch.tensor:
        """
        Provide different tensors as test cases.

        Yield:
            (torch.tensor): tensor as test case input.
        """
        # Prepare random tensor as test cases.
        shapes = (
            # Forward succeeded.
            (1, input_dim, 3, 7, 7),
            (1, input_dim, 5, 7, 7),
            (1, input_dim, 7, 7, 7),
            (2, input_dim, 3, 7, 7),
            (4, input_dim, 3, 7, 7),
            (8, input_dim, 3, 7, 7),
            (2, input_dim, 3, 7, 14),
            (2, input_dim, 3, 14, 7),
            (2, input_dim, 3, 14, 14),
            # Forward failed.
            (8, input_dim * 2, 3, 7, 7),
            (8, input_dim * 4, 5, 7, 7),
        )
        for shape in shapes:
            yield torch.rand(shape)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag469')" href="javascript:;">
pytorchvideo-0.1.3/tests/test_layers_convolutions.py: 195-219
</a>
<div class="mid" id="frag469" style="display:none"><pre>
    def _get_inputs(input_dim: int = 3) -&gt; torch.tensor:
        """
        Provide different tensors as test cases.

        Yield:
            (torch.tensor): tensor as test case input.
        """
        # Prepare random tensor as test cases.
        shapes = (
            # Forward succeeded.
            (1, input_dim, 3, 7, 7),
            (1, input_dim, 5, 7, 7),
            (1, input_dim, 7, 7, 7),
            (2, input_dim, 3, 7, 7),
            (4, input_dim, 3, 7, 7),
            (8, input_dim, 3, 7, 7),
            (2, input_dim, 3, 7, 14),
            (2, input_dim, 3, 14, 7),
            (2, input_dim, 3, 14, 14),
            # Forward failed.
            (8, input_dim * 2, 3, 7, 7),
            (8, input_dim * 4, 5, 7, 7),
        )
        for shape in shapes:
            yield torch.rand(shape)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 6:</b> &nbsp; 3 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag272')" href="javascript:;">
pytorchvideo-0.1.3/tests/test_data_json_dataset.py: 18-30
</a>
<div class="mid" id="frag272" style="display:none"><pre>
    def test_recognition_random_clip_sampler(self):
        total_duration = 0.05
        with mock_json_annotations() as (annotation_json, labels, duration):
            clip_sampler = make_clip_sampler("random", total_duration)
            dataset = json_dataset.clip_recognition_dataset(
                data_path=annotation_json,
                clip_sampler=clip_sampler,
                decode_audio=False,
            )

            self.assertEqual(dataset.num_videos, 4)
            self.assertEqual(len(list(iter(dataset))), 4)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag273')" href="javascript:;">
pytorchvideo-0.1.3/tests/test_data_json_dataset.py: 31-43
</a>
<div class="mid" id="frag273" style="display:none"><pre>
    def test_recognition_uniform_clip_sampler(self):
        total_duration = 0.05
        with mock_json_annotations() as (annotation_json, labels, duration):
            clip_sampler = make_clip_sampler("uniform", total_duration)
            dataset = json_dataset.clip_recognition_dataset(
                data_path=annotation_json,
                clip_sampler=clip_sampler,
                decode_audio=False,
            )

            self.assertEqual(dataset.num_videos, 4)
            self.assertEqual(len(list(iter(dataset))), 4)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag274')" href="javascript:;">
pytorchvideo-0.1.3/tests/test_data_json_dataset.py: 44-57
</a>
<div class="mid" id="frag274" style="display:none"><pre>
    def test_video_only_frame_video_dataset(self):
        total_duration = 2.0
        with mock_json_annotations() as (annotation_json, labels, duration):
            clip_sampler = make_clip_sampler("random", total_duration)
            dataset = json_dataset.video_only_dataset(
                data_path=annotation_json,
                clip_sampler=clip_sampler,
                decode_audio=False,
            )

            self.assertEqual(dataset.num_videos, 2)
            self.assertEqual(len(list(iter(dataset))), 2)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 7:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag365')" href="javascript:;">
pytorchvideo-0.1.3/tests/test_models_head.py: 156-181
</a>
<div class="mid" id="frag365" style="display:none"><pre>
    def _get_inputs(input_dim: int = 8) -&gt; torch.tensor:
        """
        Provide different tensors as test cases.

        Yield:
            (torch.tensor): tensor as test case input.
        """
        # Prepare random tensor as test cases.
        shapes = (
            # Forward succeeded.
            (1, input_dim, 5, 7, 7),
            (2, input_dim, 5, 7, 7),
            (4, input_dim, 5, 7, 7),
            (4, input_dim, 5, 7, 7),
            (4, input_dim, 7, 7, 7),
            (4, input_dim, 7, 7, 14),
            (4, input_dim, 7, 14, 7),
            (4, input_dim, 7, 14, 14),
            # Forward failed.
            (8, input_dim * 2, 3, 7, 7),
            (8, input_dim * 4, 5, 7, 7),
        )
        for shape in shapes:
            yield torch.rand(shape)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag441')" href="javascript:;">
pytorchvideo-0.1.3/tests/test_layers_nonlocal_net.py: 136-159
</a>
<div class="mid" id="frag441" style="display:none"><pre>
    def _get_inputs(input_dim: int = 8) -&gt; torch.tensor:
        """
        Provide different tensors as test cases.

        Yield:
            (torch.tensor): tensor as test case input.
        """
        # Prepare random tensor as test cases.
        shapes = (
            # Forward succeeded.
            (1, input_dim, 5, 7, 7),
            (2, input_dim, 5, 7, 7),
            (4, input_dim, 5, 7, 7),
            (4, input_dim, 5, 7, 7),
            (4, input_dim, 7, 7, 7),
            (4, input_dim, 7, 7, 14),
            (4, input_dim, 7, 14, 7),
            (4, input_dim, 7, 14, 14),
            # Forward failed.
            (8, input_dim * 2, 3, 7, 7),
            (8, input_dim * 4, 5, 7, 7),
        )
        for shape in shapes:
            yield torch.rand(shape)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 8:</b> &nbsp; 2 fragments, nominal size 39 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag409')" href="javascript:;">
pytorchvideo-0.1.3/tests/test_data_epic_kitchen_utils.py: 40-80
</a>
<div class="mid" id="frag409" style="display:none"><pre>
def get_flat_video_frames(directory, file_extension):
    return {
        "P02_001": VideoFrameInfo(
            video_id="P02_001",
            location=f"{directory}/P02_001",
            frame_file_stem="frame_",
            frame_string_length=16,
            min_frame_number=1,
            max_frame_number=3000,
            file_extension=file_extension,
        ),
        "P02_002": VideoFrameInfo(
            video_id="P02_002",
            location=f"{directory}/P02_002",
            frame_file_stem="frame_",
            frame_string_length=16,
            min_frame_number=2,
            max_frame_number=3001,
            file_extension=file_extension,
        ),
        "P02_005": VideoFrameInfo(
            video_id="P02_005",
            location=f"{directory}/P02_005",
            frame_file_stem="frame_",
            frame_string_length=16,
            min_frame_number=1,
            max_frame_number=30003,
            file_extension=file_extension,
        ),
        "P07_002": VideoFrameInfo(
            video_id="P07_002",
            location=f"{directory}/P07_002",
            frame_file_stem="frame_",
            frame_string_length=16,
            min_frame_number=2,
            max_frame_number=1530,
            file_extension=file_extension,
        ),
    }


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag410')" href="javascript:;">
pytorchvideo-0.1.3/tests/test_data_epic_kitchen_utils.py: 81-121
</a>
<div class="mid" id="frag410" style="display:none"><pre>
def get_nested_video_frames(directory, file_extension):
    return {
        "P02_001": VideoFrameInfo(
            video_id="P02_001",
            location=f"{directory}/P02",
            frame_file_stem="P02_001_",
            frame_string_length=16,
            min_frame_number=1,
            max_frame_number=3000,
            file_extension=file_extension,
        ),
        "P02_002": VideoFrameInfo(
            video_id="P02_002",
            location=f"{directory}/P02",
            frame_file_stem="P02_002_",
            frame_string_length=16,
            min_frame_number=2,
            max_frame_number=3001,
            file_extension=file_extension,
        ),
        "P02_005": VideoFrameInfo(
            video_id="P02_005",
            location=f"{directory}/P02",
            frame_file_stem="P02_005_",
            frame_string_length=16,
            min_frame_number=1,
            max_frame_number=30003,
            file_extension=file_extension,
        ),
        "P07_002": VideoFrameInfo(
            video_id="P07_002",
            location=f"{directory}/P07",
            frame_file_stem="P07_002_",
            frame_string_length=16,
            min_frame_number=2,
            max_frame_number=1530,
            file_extension=file_extension,
        ),
    }


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 9:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag412')" href="javascript:;">
pytorchvideo-0.1.3/tests/test_data_epic_kitchen_utils.py: 126-140
</a>
<div class="mid" id="frag412" style="display:none"><pre>
    def test_build_frame_manifest_from_flat_directory(self, multithreading=True):
        with tempfile.TemporaryDirectory(prefix="TestEpicKitchenUtils") as tempdir:
            video_frames_expected = get_flat_video_frames(tempdir, "jpg")
            write_mock_frame_files(video_frames_expected, tempdir, "jpg")

            video_frames = build_frame_manifest_from_flat_directory(
                tempdir, multithreading
            )

            self.assertEqual(len(video_frames_expected), len(video_frames))
            for video_id in video_frames_expected:
                self.assertEqual(
                    video_frames[video_id], video_frames_expected[video_id]
                )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag414')" href="javascript:;">
pytorchvideo-0.1.3/tests/test_data_epic_kitchen_utils.py: 144-157
</a>
<div class="mid" id="frag414" style="display:none"><pre>
    def test_build_frame_manifest_from_nested_directory(self, multithreading=True):
        with tempfile.TemporaryDirectory(prefix="TestEpicKitchenUtils") as tempdir:
            video_frames_expected = get_nested_video_frames(tempdir, "png")
            write_mock_frame_files(video_frames_expected, tempdir, "png")

            video_frames = build_frame_manifest_from_nested_directory(
                tempdir, multithreading
            )
            self.assertEqual(len(video_frames_expected), len(video_frames))
            for video_id in video_frames_expected:
                self.assertEqual(
                    video_frames[video_id], video_frames_expected[video_id]
                )

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 10:</b> &nbsp; 2 fragments, nominal size 19 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag449')" href="javascript:;">
pytorchvideo-0.1.3/tests/test_accelerator_efficient_blocks_mobile_cpu_conv3d.py: 84-114
</a>
<div class="mid" id="frag449" style="display:none"><pre>
    def test_Conv3d3x1x1BnAct_equivalency(self):
        for input_temporal in range(3):
            input_size = (1, 3, input_temporal + 1, 6, 6)
            # Input tensor
            input_tensor = torch.randn(input_size)
            # A conv block
            l0 = Conv3d3x1x1BnAct(3, 6)
            l0.eval()
            out0 = l0(input_tensor)
            # Replicate the conv block
            l0_1 = deepcopy(l0)
            # Convert into deployment mode
            l0_1.convert(input_size)  # Input tensor size is (1,3,4,6,6)
            out1 = l0_1(input_tensor)
            # Check output size
            assert (
                out0.size() == out1.size()
            ), f"Sizes of out0 {out0.size()} and out1 {out1.size()} are different."
            # Check arithmetic equivalency
            max_err = float(torch.max(torch.abs(out0 - out1)))
            rel_err = torch.abs((out0 - out1) / out0)
            max_rel_err = float(torch.max(rel_err))
            logging.info(
                (
                    "test_Conv3d3x1x1BnAct_equivalency: "
                    f"input tensor size: {input_size}"
                    f"max_err {max_err}, max_rel_err {max_rel_err}"
                )
            )
            self.assertTrue(max_err &lt; 1e-3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag450')" href="javascript:;">
pytorchvideo-0.1.3/tests/test_accelerator_efficient_blocks_mobile_cpu_conv3d.py: 115-144
</a>
<div class="mid" id="frag450" style="display:none"><pre>
    def test_Conv3d5x1x1BnAct_equivalency(self):
        for input_temporal in range(5):
            input_size = (1, 3, input_temporal + 1, 6, 6)
            # Input tensor
            input_tensor = torch.randn(input_size)
            # A conv block
            l0 = Conv3d5x1x1BnAct(3, 6)
            l0.eval()
            out0 = l0(input_tensor)
            # Replicate the conv block
            l0_1 = deepcopy(l0)
            # Convert into deployment mode
            l0_1.convert(input_size)  # Input tensor size is (1,3,4,6,6)
            out1 = l0_1(input_tensor)
            # Check output size
            assert (
                out0.size() == out1.size()
            ), f"Sizes of out0 {out0.size()} and out1 {out1.size()} are different."
            # Check arithmetic equivalency
            max_err = float(torch.max(torch.abs(out0 - out1)))
            rel_err = torch.abs((out0 - out1) / out0)
            max_rel_err = float(torch.max(rel_err))
            logging.info(
                (
                    "test_Conv3d5x1x1BnAct_equivalency: "
                    f"input tensor size: {input_size}"
                    f"max_err {max_err}, max_rel_err {max_rel_err}"
                )
            )
            self.assertTrue(max_err &lt; 1e-3)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<script language="JavaScript">
function ShowHide(divId) { 
    if(document.getElementById(divId).style.display == 'none') {
        document.getElementById(divId).style.display='block';
    } else { 
        document.getElementById(divId).style.display = 'none';
    } 
}
</script>
</body>
</html>

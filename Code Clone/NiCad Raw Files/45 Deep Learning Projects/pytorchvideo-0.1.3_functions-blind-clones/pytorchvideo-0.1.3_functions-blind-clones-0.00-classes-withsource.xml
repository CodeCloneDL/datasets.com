<clones>
<systeminfo processor="nicad6" system="pytorchvideo-0.1.3" granularity="functions-blind" threshold="0%" minlines="10" maxlines="2500"/>
<cloneinfo npcs="475" npairs="14"/>
<runinfo ncompares="847" cputime="43534"/>
<classinfo nclasses="10"/>

<class classid="1" nclones="2" nlines="17" similarity="100">
<source file="systems/pytorchvideo-0.1.3/tutorials/video_classification_example/train.py" startline="296" endline="318" pcid="14">
    def train_dataloader(self):
        """
        Defines the train DataLoader that the PyTorch Lightning Trainer trains/tests with.
        """
        sampler = DistributedSampler if self.trainer.use_ddp else RandomSampler
        train_transform = self._make_transforms(mode="train")
        self.train_dataset = LimitDataset(
            pytorchvideo.data.Kinetics(
                data_path=os.path.join(self.args.data_path, "train.csv"),
                clip_sampler=pytorchvideo.data.make_clip_sampler(
                    "random", self.args.clip_duration
                ),
                video_path_prefix=self.args.video_path_prefix,
                transform=train_transform,
                video_sampler=sampler,
            )
        )
        return torch.utils.data.DataLoader(
            self.train_dataset,
            batch_size=self.args.batch_size,
            num_workers=self.args.workers,
        )

</source>
<source file="systems/pytorchvideo-0.1.3/tutorials/video_classification_example/train.py" startline="319" endline="342" pcid="15">
    def val_dataloader(self):
        """
        Defines the train DataLoader that the PyTorch Lightning Trainer trains/tests with.
        """
        sampler = DistributedSampler if self.trainer.use_ddp else RandomSampler
        val_transform = self._make_transforms(mode="val")
        self.val_dataset = LimitDataset(
            pytorchvideo.data.Kinetics(
                data_path=os.path.join(self.args.data_path, "val.csv"),
                clip_sampler=pytorchvideo.data.make_clip_sampler(
                    "uniform", self.args.clip_duration
                ),
                video_path_prefix=self.args.video_path_prefix,
                transform=val_transform,
                video_sampler=sampler,
            )
        )
        return torch.utils.data.DataLoader(
            self.val_dataset,
            batch_size=self.args.batch_size,
            num_workers=self.args.workers,
        )


</source>
</class>

<class classid="2" nclones="2" nlines="19" similarity="100">
<source file="systems/pytorchvideo-0.1.3/pytorchvideo/data/kinetics.py" startline="17" endline="70" pcid="52">
def Kinetics(
    data_path: str,
    clip_sampler: ClipSampler,
    video_sampler: Type[torch.utils.data.Sampler] = torch.utils.data.RandomSampler,
    transform: Optional[Callable[[Dict[str, Any]], Dict[str, Any]]] = None,
    video_path_prefix: str = "",
    decode_audio: bool = True,
    decoder: str = "pyav",
) -> LabeledVideoDataset:
    """
    A helper function to create ``LabeledVideoDataset`` object for the Kinetics dataset.

    Args:
        data_path (str): Path to the data. The path type defines how the data
            should be read:

            * For a file path, the file is read and each line is parsed into a
              video path and label.
            * For a directory, the directory structure defines the classes
              (i.e. each subdirectory is a class).

        clip_sampler (ClipSampler): Defines how clips should be sampled from each
                video. See the clip sampling documentation for more information.

        video_sampler (Type[torch.utils.data.Sampler]): Sampler for the internal
                video container. This defines the order videos are decoded and,
                if necessary, the distributed split.

        transform (Callable): This callable is evaluated on the clip output before
                the clip is returned. It can be used for user defined preprocessing and
                augmentations to the clips. See the ``LabeledVideoDataset`` class for clip
                output format.

        video_path_prefix (str): Path to root directory with the videos that are
                loaded in ``LabeledVideoDataset``. All the video paths before loading
                are prefixed with this path.

        decode_audio (bool): If True, also decode audio from video.

        decoder (str): Defines what type of decoder used to decode a video.

    """

    torch._C._log_api_usage_once("PYTORCHVIDEO.dataset.Kinetics")

    return labeled_video_dataset(
        data_path,
        clip_sampler,
        video_sampler,
        transform,
        video_path_prefix,
        decode_audio,
        decoder,
    )
</source>
<source file="systems/pytorchvideo-0.1.3/pytorchvideo/data/ucf101.py" startline="17" endline="70" pcid="79">
def Ucf101(
    data_path: str,
    clip_sampler: ClipSampler,
    video_sampler: Type[torch.utils.data.Sampler] = torch.utils.data.RandomSampler,
    transform: Optional[Callable[[Dict[str, Any]], Dict[str, Any]]] = None,
    video_path_prefix: str = "",
    decode_audio: bool = True,
    decoder: str = "pyav",
) -> LabeledVideoDataset:
    """
    A helper function to create ``LabeledVideoDataset`` object for the Ucf101 dataset.

    Args:
        data_path (str): Path to the data. The path type defines how the data
            should be read:

            * For a file path, the file is read and each line is parsed into a
              video path and label.
            * For a directory, the directory structure defines the classes
              (i.e. each subdirectory is a class).

        clip_sampler (ClipSampler): Defines how clips should be sampled from each
                video. See the clip sampling documentation for more information.

        video_sampler (Type[torch.utils.data.Sampler]): Sampler for the internal
                video container. This defines the order videos are decoded and,
                if necessary, the distributed split.

        transform (Callable): This callable is evaluated on the clip output before
                the clip is returned. It can be used for user defined preprocessing and
                augmentations to the clips. See the ``LabeledVideoDataset`` class for clip
                output format.

        video_path_prefix (str): Path to root directory with the videos that are
                loaded in ``LabeledVideoDataset``. All the video paths before loading
                are prefixed with this path.

        decode_audio (bool): If True, also decode audio from video.

        decoder (str): Defines what type of decoder used to decode a video.

    """

    torch._C._log_api_usage_once("PYTORCHVIDEO.dataset.Ucf101")

    return labeled_video_dataset(
        data_path,
        clip_sampler,
        video_sampler,
        transform,
        video_path_prefix,
        decode_audio,
        decoder,
    )
</source>
</class>

<class classid="3" nclones="2" nlines="21" similarity="100">
<source file="systems/pytorchvideo-0.1.3/pytorchvideo/accelerator/deployment/mobile_cpu/transmuter/transmuter_mobile_cpu.py" startline="130" endline="161" pcid="114">
def transmute_Conv3d3x1x1BnAct(input_module: nn.Module):
    """
    Given an input_module, transmutes it into a equivalent Conv3d3x1x1BnAct.
    Returns None if no equivalent Conv3d3x1x1BnAct is found, else returns
    an instance of equivalent Conv3d3x1x1BnAct.
    Args:
        input_module (nn.Module): input module to find an equivalent Conv3d3x1x1BnAct
    """
    if not isinstance(input_module, nn.Conv3d):
        return None

    if (
        input_module.kernel_size == (3, 1, 1)
        and input_module.stride == (1, 1, 1)
        and input_module.padding == (1, 0, 0)
        and input_module.dilation == (1, 1, 1)
        and input_module.padding_mode == "zeros"
    ):
        module = Conv3d3x1x1BnAct(
            in_channels=input_module.in_channels,
            out_channels=input_module.out_channels,
            bias=False if input_module.bias is None else True,
            groups=input_module.groups,
            activation="identity",
            use_bn=False,
        )
        module.kernel.conv.load_state_dict(input_module.state_dict())
        return module
    else:
        return None


</source>
<source file="systems/pytorchvideo-0.1.3/pytorchvideo/accelerator/deployment/mobile_cpu/transmuter/transmuter_mobile_cpu.py" startline="162" endline="193" pcid="115">
def transmute_Conv3d5x1x1BnAct(input_module: nn.Module):
    """
    Given an input_module, transmutes it into a equivalent Conv3d5x1x1BnAct.
    Returns None if no equivalent Conv3d5x1x1BnAct is found, else returns
    an instance of equivalent Conv3d5x1x1BnAct.
    Args:
        input_module (nn.Module): input module to find an equivalent Conv3d5x1x1BnAct
    """
    if not isinstance(input_module, nn.Conv3d):
        return None

    if (
        input_module.kernel_size == (5, 1, 1)
        and input_module.stride == (1, 1, 1)
        and input_module.padding == (2, 0, 0)
        and input_module.dilation == (1, 1, 1)
        and input_module.padding_mode == "zeros"
    ):
        module = Conv3d5x1x1BnAct(
            in_channels=input_module.in_channels,
            out_channels=input_module.out_channels,
            bias=False if input_module.bias is None else True,
            groups=input_module.groups,
            activation="identity",
            use_bn=False,
        )
        module.kernel.conv.load_state_dict(input_module.state_dict())
        return module
    else:
        return None


</source>
</class>

<class classid="4" nclones="2" nlines="10" similarity="100">
<source file="systems/pytorchvideo-0.1.3/pytorchvideo/transforms/mix.py" startline="35" endline="53" pcid="174">
    def __init__(
        self,
        alpha: float = 1.0,
        label_smoothing: float = 0.0,
        num_classes: int = 400,
    ) -> None:
        """
        This implements MixUp for videos.

        Args:
            alpha (float): Mixup alpha value.
            label_smoothing (float): Label smoothing value.
            num_classes (int): Number of total classes.
        """
        super().__init__()
        self.mixup_beta_sampler = torch.distributions.beta.Beta(alpha, alpha)
        self.label_smoothing = label_smoothing
        self.num_classes = num_classes

</source>
<source file="systems/pytorchvideo-0.1.3/pytorchvideo/transforms/mix.py" startline="85" endline="103" pcid="176">
    def __init__(
        self,
        alpha: float = 1.0,
        label_smoothing: float = 0.0,
        num_classes: int = 400,
    ) -> None:
        """
        This implements CutMix for videos.

        Args:
            alpha (float): CutMix alpha value.
            label_smoothing (float): Label smoothing value.
            num_classes (int): Number of total classes.
        """
        super().__init__()
        self.cutmix_beta_sampler = torch.distributions.beta.Beta(alpha, alpha)
        self.label_smoothing = label_smoothing
        self.num_classes = num_classes

</source>
</class>

<class classid="5" nclones="3" nlines="16" similarity="100">
<source file="systems/pytorchvideo-0.1.3/tests/test_models_stem.py" startline="279" endline="303" pcid="268">
    def _get_inputs(input_dim: int = 3) -> torch.tensor:
        """
        Provide different tensors as test cases.

        Yield:
            (torch.tensor): tensor as test case input.
        """
        # Prepare random tensor as test cases.
        shapes = (
            # Forward succeeded.
            (1, input_dim, 3, 7, 7),
            (1, input_dim, 5, 7, 7),
            (1, input_dim, 7, 7, 7),
            (2, input_dim, 3, 7, 7),
            (4, input_dim, 3, 7, 7),
            (8, input_dim, 3, 7, 7),
            (2, input_dim, 3, 7, 14),
            (2, input_dim, 3, 14, 7),
            (2, input_dim, 3, 14, 14),
            # Forward failed.
            (8, input_dim * 2, 3, 7, 7),
            (8, input_dim * 4, 5, 7, 7),
        )
        for shape in shapes:
            yield torch.rand(shape)
</source>
<source file="systems/pytorchvideo-0.1.3/tests/test_layers_convolutions.py" startline="98" endline="124" pcid="466">
    def _get_inputs(input_dim: int = 3) -> torch.tensor:
        """
        Provide different tensors as test cases.

        Yield:
            (torch.tensor): tensor as test case input.
        """
        # Prepare random tensor as test cases.
        shapes = (
            # Forward succeeded.
            (1, input_dim, 3, 7, 7),
            (1, input_dim, 5, 7, 7),
            (1, input_dim, 7, 7, 7),
            (2, input_dim, 3, 7, 7),
            (4, input_dim, 3, 7, 7),
            (8, input_dim, 3, 7, 7),
            (2, input_dim, 3, 7, 14),
            (2, input_dim, 3, 14, 7),
            (2, input_dim, 3, 14, 14),
            # Forward failed.
            (8, input_dim * 2, 3, 7, 7),
            (8, input_dim * 4, 5, 7, 7),
        )
        for shape in shapes:
            yield torch.rand(shape)


</source>
<source file="systems/pytorchvideo-0.1.3/tests/test_layers_convolutions.py" startline="195" endline="219" pcid="469">
    def _get_inputs(input_dim: int = 3) -> torch.tensor:
        """
        Provide different tensors as test cases.

        Yield:
            (torch.tensor): tensor as test case input.
        """
        # Prepare random tensor as test cases.
        shapes = (
            # Forward succeeded.
            (1, input_dim, 3, 7, 7),
            (1, input_dim, 5, 7, 7),
            (1, input_dim, 7, 7, 7),
            (2, input_dim, 3, 7, 7),
            (4, input_dim, 3, 7, 7),
            (8, input_dim, 3, 7, 7),
            (2, input_dim, 3, 7, 14),
            (2, input_dim, 3, 14, 7),
            (2, input_dim, 3, 14, 14),
            # Forward failed.
            (8, input_dim * 2, 3, 7, 7),
            (8, input_dim * 4, 5, 7, 7),
        )
        for shape in shapes:
            yield torch.rand(shape)
</source>
</class>

<class classid="6" nclones="3" nlines="11" similarity="100">
<source file="systems/pytorchvideo-0.1.3/tests/test_data_json_dataset.py" startline="18" endline="30" pcid="272">
    def test_recognition_random_clip_sampler(self):
        total_duration = 0.05
        with mock_json_annotations() as (annotation_json, labels, duration):
            clip_sampler = make_clip_sampler("random", total_duration)
            dataset = json_dataset.clip_recognition_dataset(
                data_path=annotation_json,
                clip_sampler=clip_sampler,
                decode_audio=False,
            )

            self.assertEqual(dataset.num_videos, 4)
            self.assertEqual(len(list(iter(dataset))), 4)

</source>
<source file="systems/pytorchvideo-0.1.3/tests/test_data_json_dataset.py" startline="31" endline="43" pcid="273">
    def test_recognition_uniform_clip_sampler(self):
        total_duration = 0.05
        with mock_json_annotations() as (annotation_json, labels, duration):
            clip_sampler = make_clip_sampler("uniform", total_duration)
            dataset = json_dataset.clip_recognition_dataset(
                data_path=annotation_json,
                clip_sampler=clip_sampler,
                decode_audio=False,
            )

            self.assertEqual(dataset.num_videos, 4)
            self.assertEqual(len(list(iter(dataset))), 4)

</source>
<source file="systems/pytorchvideo-0.1.3/tests/test_data_json_dataset.py" startline="44" endline="57" pcid="274">
    def test_video_only_frame_video_dataset(self):
        total_duration = 2.0
        with mock_json_annotations() as (annotation_json, labels, duration):
            clip_sampler = make_clip_sampler("random", total_duration)
            dataset = json_dataset.video_only_dataset(
                data_path=annotation_json,
                clip_sampler=clip_sampler,
                decode_audio=False,
            )

            self.assertEqual(dataset.num_videos, 2)
            self.assertEqual(len(list(iter(dataset))), 2)


</source>
</class>

<class classid="7" nclones="2" nlines="15" similarity="100">
<source file="systems/pytorchvideo-0.1.3/tests/test_models_head.py" startline="156" endline="181" pcid="365">
    def _get_inputs(input_dim: int = 8) -> torch.tensor:
        """
        Provide different tensors as test cases.

        Yield:
            (torch.tensor): tensor as test case input.
        """
        # Prepare random tensor as test cases.
        shapes = (
            # Forward succeeded.
            (1, input_dim, 5, 7, 7),
            (2, input_dim, 5, 7, 7),
            (4, input_dim, 5, 7, 7),
            (4, input_dim, 5, 7, 7),
            (4, input_dim, 7, 7, 7),
            (4, input_dim, 7, 7, 14),
            (4, input_dim, 7, 14, 7),
            (4, input_dim, 7, 14, 14),
            # Forward failed.
            (8, input_dim * 2, 3, 7, 7),
            (8, input_dim * 4, 5, 7, 7),
        )
        for shape in shapes:
            yield torch.rand(shape)


</source>
<source file="systems/pytorchvideo-0.1.3/tests/test_layers_nonlocal_net.py" startline="136" endline="159" pcid="441">
    def _get_inputs(input_dim: int = 8) -> torch.tensor:
        """
        Provide different tensors as test cases.

        Yield:
            (torch.tensor): tensor as test case input.
        """
        # Prepare random tensor as test cases.
        shapes = (
            # Forward succeeded.
            (1, input_dim, 5, 7, 7),
            (2, input_dim, 5, 7, 7),
            (4, input_dim, 5, 7, 7),
            (4, input_dim, 5, 7, 7),
            (4, input_dim, 7, 7, 7),
            (4, input_dim, 7, 7, 14),
            (4, input_dim, 7, 14, 7),
            (4, input_dim, 7, 14, 14),
            # Forward failed.
            (8, input_dim * 2, 3, 7, 7),
            (8, input_dim * 4, 5, 7, 7),
        )
        for shape in shapes:
            yield torch.rand(shape)
</source>
</class>

<class classid="8" nclones="2" nlines="39" similarity="100">
<source file="systems/pytorchvideo-0.1.3/tests/test_data_epic_kitchen_utils.py" startline="40" endline="80" pcid="409">
def get_flat_video_frames(directory, file_extension):
    return {
        "P02_001": VideoFrameInfo(
            video_id="P02_001",
            location=f"{directory}/P02_001",
            frame_file_stem="frame_",
            frame_string_length=16,
            min_frame_number=1,
            max_frame_number=3000,
            file_extension=file_extension,
        ),
        "P02_002": VideoFrameInfo(
            video_id="P02_002",
            location=f"{directory}/P02_002",
            frame_file_stem="frame_",
            frame_string_length=16,
            min_frame_number=2,
            max_frame_number=3001,
            file_extension=file_extension,
        ),
        "P02_005": VideoFrameInfo(
            video_id="P02_005",
            location=f"{directory}/P02_005",
            frame_file_stem="frame_",
            frame_string_length=16,
            min_frame_number=1,
            max_frame_number=30003,
            file_extension=file_extension,
        ),
        "P07_002": VideoFrameInfo(
            video_id="P07_002",
            location=f"{directory}/P07_002",
            frame_file_stem="frame_",
            frame_string_length=16,
            min_frame_number=2,
            max_frame_number=1530,
            file_extension=file_extension,
        ),
    }


</source>
<source file="systems/pytorchvideo-0.1.3/tests/test_data_epic_kitchen_utils.py" startline="81" endline="121" pcid="410">
def get_nested_video_frames(directory, file_extension):
    return {
        "P02_001": VideoFrameInfo(
            video_id="P02_001",
            location=f"{directory}/P02",
            frame_file_stem="P02_001_",
            frame_string_length=16,
            min_frame_number=1,
            max_frame_number=3000,
            file_extension=file_extension,
        ),
        "P02_002": VideoFrameInfo(
            video_id="P02_002",
            location=f"{directory}/P02",
            frame_file_stem="P02_002_",
            frame_string_length=16,
            min_frame_number=2,
            max_frame_number=3001,
            file_extension=file_extension,
        ),
        "P02_005": VideoFrameInfo(
            video_id="P02_005",
            location=f"{directory}/P02",
            frame_file_stem="P02_005_",
            frame_string_length=16,
            min_frame_number=1,
            max_frame_number=30003,
            file_extension=file_extension,
        ),
        "P07_002": VideoFrameInfo(
            video_id="P07_002",
            location=f"{directory}/P07",
            frame_file_stem="P07_002_",
            frame_string_length=16,
            min_frame_number=2,
            max_frame_number=1530,
            file_extension=file_extension,
        ),
    }


</source>
</class>

<class classid="9" nclones="2" nlines="10" similarity="100">
<source file="systems/pytorchvideo-0.1.3/tests/test_data_epic_kitchen_utils.py" startline="126" endline="140" pcid="412">
    def test_build_frame_manifest_from_flat_directory(self, multithreading=True):
        with tempfile.TemporaryDirectory(prefix="TestEpicKitchenUtils") as tempdir:
            video_frames_expected = get_flat_video_frames(tempdir, "jpg")
            write_mock_frame_files(video_frames_expected, tempdir, "jpg")

            video_frames = build_frame_manifest_from_flat_directory(
                tempdir, multithreading
            )

            self.assertEqual(len(video_frames_expected), len(video_frames))
            for video_id in video_frames_expected:
                self.assertEqual(
                    video_frames[video_id], video_frames_expected[video_id]
                )

</source>
<source file="systems/pytorchvideo-0.1.3/tests/test_data_epic_kitchen_utils.py" startline="144" endline="157" pcid="414">
    def test_build_frame_manifest_from_nested_directory(self, multithreading=True):
        with tempfile.TemporaryDirectory(prefix="TestEpicKitchenUtils") as tempdir:
            video_frames_expected = get_nested_video_frames(tempdir, "png")
            write_mock_frame_files(video_frames_expected, tempdir, "png")

            video_frames = build_frame_manifest_from_nested_directory(
                tempdir, multithreading
            )
            self.assertEqual(len(video_frames_expected), len(video_frames))
            for video_id in video_frames_expected:
                self.assertEqual(
                    video_frames[video_id], video_frames_expected[video_id]
                )

</source>
</class>

<class classid="10" nclones="2" nlines="19" similarity="100">
<source file="systems/pytorchvideo-0.1.3/tests/test_accelerator_efficient_blocks_mobile_cpu_conv3d.py" startline="84" endline="114" pcid="449">
    def test_Conv3d3x1x1BnAct_equivalency(self):
        for input_temporal in range(3):
            input_size = (1, 3, input_temporal + 1, 6, 6)
            # Input tensor
            input_tensor = torch.randn(input_size)
            # A conv block
            l0 = Conv3d3x1x1BnAct(3, 6)
            l0.eval()
            out0 = l0(input_tensor)
            # Replicate the conv block
            l0_1 = deepcopy(l0)
            # Convert into deployment mode
            l0_1.convert(input_size)  # Input tensor size is (1,3,4,6,6)
            out1 = l0_1(input_tensor)
            # Check output size
            assert (
                out0.size() == out1.size()
            ), f"Sizes of out0 {out0.size()} and out1 {out1.size()} are different."
            # Check arithmetic equivalency
            max_err = float(torch.max(torch.abs(out0 - out1)))
            rel_err = torch.abs((out0 - out1) / out0)
            max_rel_err = float(torch.max(rel_err))
            logging.info(
                (
                    "test_Conv3d3x1x1BnAct_equivalency: "
                    f"input tensor size: {input_size}"
                    f"max_err {max_err}, max_rel_err {max_rel_err}"
                )
            )
            self.assertTrue(max_err < 1e-3)

</source>
<source file="systems/pytorchvideo-0.1.3/tests/test_accelerator_efficient_blocks_mobile_cpu_conv3d.py" startline="115" endline="144" pcid="450">
    def test_Conv3d5x1x1BnAct_equivalency(self):
        for input_temporal in range(5):
            input_size = (1, 3, input_temporal + 1, 6, 6)
            # Input tensor
            input_tensor = torch.randn(input_size)
            # A conv block
            l0 = Conv3d5x1x1BnAct(3, 6)
            l0.eval()
            out0 = l0(input_tensor)
            # Replicate the conv block
            l0_1 = deepcopy(l0)
            # Convert into deployment mode
            l0_1.convert(input_size)  # Input tensor size is (1,3,4,6,6)
            out1 = l0_1(input_tensor)
            # Check output size
            assert (
                out0.size() == out1.size()
            ), f"Sizes of out0 {out0.size()} and out1 {out1.size()} are different."
            # Check arithmetic equivalency
            max_err = float(torch.max(torch.abs(out0 - out1)))
            rel_err = torch.abs((out0 - out1) / out0)
            max_rel_err = float(torch.max(rel_err))
            logging.info(
                (
                    "test_Conv3d5x1x1BnAct_equivalency: "
                    f"input tensor size: {input_size}"
                    f"max_err {max_err}, max_rel_err {max_rel_err}"
                )
            )
            self.assertTrue(max_err < 1e-3)
</source>
</class>

</clones>

<clones>
<systeminfo processor="nicad6" system="addons-0.16.1" granularity="functions-blind" threshold="0%" minlines="10" maxlines="2500"/>
<cloneinfo npcs="1095" npairs="27"/>
<runinfo ncompares="5042" cputime="45693"/>
<classinfo nclasses="21"/>

<class classid="1" nclones="3" nlines="20" similarity="100">
<source file="systems/addons-0.16.1/build_deps/toolchains/gpu/find_cuda_config.py" startline="335" endline="367" pcid="43">
def _find_cublas_config(base_paths, required_version, cuda_version):

    if _at_least_version(cuda_version, "10.1"):

        def get_header_version(path):
            version = (
                _get_header_version(path, name)
                for name in ("CUBLAS_VER_MAJOR", "CUBLAS_VER_MINOR", "CUBLAS_VER_PATCH")
            )
            return ".".join(version)

        header_path, header_version = _find_header(
            base_paths, "cublas_api.h", required_version, get_header_version
        )
        # cuBLAS uses the major version only.
        cublas_version = header_version.split(".")[0]

    else:
        # There is no version info available before CUDA 10.1, just find the file.
        header_version = cuda_version
        header_path = _find_file(base_paths, _header_paths(), "cublas_api.h")
        # cuBLAS version is the same as CUDA version (x.y).
        cublas_version = required_version

    library_path = _find_library(base_paths, "cublas", cublas_version)

    return {
        "cublas_version": header_version,
        "cublas_include_dir": os.path.dirname(header_path),
        "cublas_library_dir": os.path.dirname(library_path),
    }


</source>
<source file="systems/addons-0.16.1/build_deps/toolchains/gpu/find_cuda_config.py" startline="432" endline="461" pcid="49">
def _find_cufft_config(base_paths, required_version, cuda_version):

    if _at_least_version(cuda_version, "11.0"):

        def get_header_version(path):
            version = (
                _get_header_version(path, name)
                for name in ("CUFFT_VER_MAJOR", "CUFFT_VER_MINOR", "CUFFT_VER_PATCH")
            )
            return ".".join(version)

        header_path, header_version = _find_header(
            base_paths, "cufft.h", required_version, get_header_version
        )
        cufft_version = header_version.split(".")[0]

    else:
        header_version = cuda_version
        header_path = _find_file(base_paths, _header_paths(), "cufft.h")
        cufft_version = required_version

    library_path = _find_library(base_paths, "cufft", cufft_version)

    return {
        "cufft_version": header_version,
        "cufft_include_dir": os.path.dirname(header_path),
        "cufft_library_dir": os.path.dirname(library_path),
    }


</source>
<source file="systems/addons-0.16.1/build_deps/toolchains/gpu/find_cuda_config.py" startline="402" endline="431" pcid="47">
def _find_curand_config(base_paths, required_version, cuda_version):

    if _at_least_version(cuda_version, "11.0"):

        def get_header_version(path):
            version = (
                _get_header_version(path, name)
                for name in ("CURAND_VER_MAJOR", "CURAND_VER_MINOR", "CURAND_VER_PATCH")
            )
            return ".".join(version)

        header_path, header_version = _find_header(
            base_paths, "curand.h", required_version, get_header_version
        )
        curand_version = header_version.split(".")[0]

    else:
        header_version = cuda_version
        header_path = _find_file(base_paths, _header_paths(), "curand.h")
        curand_version = required_version

    library_path = _find_library(base_paths, "curand", curand_version)

    return {
        "curand_version": header_version,
        "curand_include_dir": os.path.dirname(header_path),
        "curand_library_dir": os.path.dirname(library_path),
    }


</source>
</class>

<class classid="2" nclones="2" nlines="24" similarity="100">
<source file="systems/addons-0.16.1/build_deps/toolchains/gpu/find_cuda_config.py" startline="368" endline="401" pcid="45">
def _find_cusolver_config(base_paths, required_version, cuda_version):

    if _at_least_version(cuda_version, "11.0"):

        def get_header_version(path):
            version = (
                _get_header_version(path, name)
                for name in (
                    "CUSOLVER_VER_MAJOR",
                    "CUSOLVER_VER_MINOR",
                    "CUSOLVER_VER_PATCH",
                )
            )
            return ".".join(version)

        header_path, header_version = _find_header(
            base_paths, "cusolver_common.h", required_version, get_header_version
        )
        cusolver_version = header_version.split(".")[0]

    else:
        header_version = cuda_version
        header_path = _find_file(base_paths, _header_paths(), "cusolver_common.h")
        cusolver_version = required_version

    library_path = _find_library(base_paths, "cusolver", cusolver_version)

    return {
        "cusolver_version": header_version,
        "cusolver_include_dir": os.path.dirname(header_path),
        "cusolver_library_dir": os.path.dirname(library_path),
    }


</source>
<source file="systems/addons-0.16.1/build_deps/toolchains/gpu/find_cuda_config.py" startline="484" endline="517" pcid="53">
def _find_cusparse_config(base_paths, required_version, cuda_version):

    if _at_least_version(cuda_version, "11.0"):

        def get_header_version(path):
            version = (
                _get_header_version(path, name)
                for name in (
                    "CUSPARSE_VER_MAJOR",
                    "CUSPARSE_VER_MINOR",
                    "CUSPARSE_VER_PATCH",
                )
            )
            return ".".join(version)

        header_path, header_version = _find_header(
            base_paths, "cusparse.h", required_version, get_header_version
        )
        cusparse_version = header_version.split(".")[0]

    else:
        header_version = cuda_version
        header_path = _find_file(base_paths, _header_paths(), "cusparse.h")
        cusparse_version = required_version

    library_path = _find_library(base_paths, "cusparse", cusparse_version)

    return {
        "cusparse_version": header_version,
        "cusparse_include_dir": os.path.dirname(header_path),
        "cusparse_library_dir": os.path.dirname(library_path),
    }


</source>
</class>

<class classid="3" nclones="3" nlines="10" similarity="100">
<source file="systems/addons-0.16.1/tensorflow_addons/optimizers/tests/cocob_test.py" startline="22" endline="37" pcid="110">
def run_dense_sample(iterations, expected, optimizer):
    var_0 = tf.Variable([1.0, 2.0], dtype=tf.dtypes.float32)
    var_1 = tf.Variable([3.0, 4.0], dtype=tf.dtypes.float32)

    grad_0 = tf.constant([0.1, 0.2], dtype=tf.dtypes.float32)
    grad_1 = tf.constant([0.03, 0.04], dtype=tf.dtypes.float32)

    grads_and_vars = list(zip([grad_0, grad_1], [var_0, var_1]))

    for _ in range(iterations):
        optimizer.apply_gradients(grads_and_vars)

    np.testing.assert_allclose(var_0.read_value(), expected[0], atol=2e-4)
    np.testing.assert_allclose(var_1.read_value(), expected[1], atol=2e-4)


</source>
<source file="systems/addons-0.16.1/tensorflow_addons/optimizers/tests/adabelief_test.py" startline="24" endline="39" pcid="164">
def run_dense_sample(iterations, expected, optimizer):
    var_0 = tf.Variable([1.0, 2.0], dtype=tf.dtypes.float32)
    var_1 = tf.Variable([3.0, 4.0], dtype=tf.dtypes.float32)

    grad_0 = tf.constant([0.1, 0.2], dtype=tf.dtypes.float32)
    grad_1 = tf.constant([0.03, 0.04], dtype=tf.dtypes.float32)

    grads_and_vars = list(zip([grad_0, grad_1], [var_0, var_1]))

    for _ in range(iterations):
        optimizer.apply_gradients(grads_and_vars)

    np.testing.assert_allclose(var_0.read_value(), expected[0], atol=2e-4)
    np.testing.assert_allclose(var_1.read_value(), expected[1], atol=2e-4)


</source>
<source file="systems/addons-0.16.1/tensorflow_addons/optimizers/tests/rectified_adam_test.py" startline="24" endline="39" pcid="257">
def run_dense_sample(iterations, expected, optimizer):
    var_0 = tf.Variable([1.0, 2.0], dtype=tf.dtypes.float32)
    var_1 = tf.Variable([3.0, 4.0], dtype=tf.dtypes.float32)

    grad_0 = tf.constant([0.1, 0.2], dtype=tf.dtypes.float32)
    grad_1 = tf.constant([0.03, 0.04], dtype=tf.dtypes.float32)

    grads_and_vars = list(zip([grad_0, grad_1], [var_0, var_1]))

    for _ in range(iterations):
        optimizer.apply_gradients(grads_and_vars)

    np.testing.assert_allclose(var_0.read_value(), expected[0], atol=2e-4)
    np.testing.assert_allclose(var_1.read_value(), expected[1], atol=2e-4)


</source>
</class>

<class classid="4" nclones="2" nlines="10" similarity="100">
<source file="systems/addons-0.16.1/tensorflow_addons/optimizers/tests/adabelief_test.py" startline="40" endline="55" pcid="165">
def run_sparse_sample(iterations, expected, optimizer):
    var_0 = tf.Variable([1.0, 2.0])
    var_1 = tf.Variable([3.0, 4.0])

    grad_0 = tf.IndexedSlices(tf.constant([0.1]), tf.constant([0]), tf.constant([2]))
    grad_1 = tf.IndexedSlices(tf.constant([0.04]), tf.constant([1]), tf.constant([2]))

    grads_and_vars = list(zip([grad_0, grad_1], [var_0, var_1]))

    for _ in range(iterations):
        optimizer.apply_gradients(grads_and_vars)

    np.testing.assert_allclose(var_0.read_value(), expected[0], atol=2e-4)
    np.testing.assert_allclose(var_1.read_value(), expected[1], atol=2e-4)


</source>
<source file="systems/addons-0.16.1/tensorflow_addons/optimizers/tests/rectified_adam_test.py" startline="40" endline="55" pcid="258">
def run_sparse_sample(iterations, expected, optimizer):
    var_0 = tf.Variable([1.0, 2.0])
    var_1 = tf.Variable([3.0, 4.0])

    grad_0 = tf.IndexedSlices(tf.constant([0.1]), tf.constant([0]), tf.constant([2]))
    grad_1 = tf.IndexedSlices(tf.constant([0.04]), tf.constant([1]), tf.constant([2]))

    grads_and_vars = list(zip([grad_0, grad_1], [var_0, var_1]))

    for _ in range(iterations):
        optimizer.apply_gradients(grads_and_vars)

    np.testing.assert_allclose(var_0.read_value(), expected[0], atol=2e-4)
    np.testing.assert_allclose(var_1.read_value(), expected[1], atol=2e-4)


</source>
</class>

<class classid="5" nclones="2" nlines="10" similarity="100">
<source file="systems/addons-0.16.1/tensorflow_addons/optimizers/tests/adabelief_test.py" startline="160" endline="173" pcid="176">
def test_dense_sample_with_lookahead():
    # Expected values are obtained from the original implementation
    # of Ranger
    run_dense_sample(
        iterations=100,
        expected=[[0.88910455, 1.889104], [2.8891046, 3.8891046]],
        optimizer=Lookahead(
            AdaBelief(lr=1e-3, beta_1=0.95, rectify=False),
            sync_period=6,
            slow_step_size=0.45,
        ),
    )


</source>
<source file="systems/addons-0.16.1/tensorflow_addons/optimizers/tests/adabelief_test.py" startline="175" endline="188" pcid="177">
def test_sparse_sample_with_lookahead():
    # Expected values are obtained from the previous implementation
    # of Ranger.
    run_sparse_sample(
        iterations=150,
        expected=[[0.8114481, 2.0], [3.0, 3.8114486]],
        optimizer=Lookahead(
            AdaBelief(lr=1e-3, beta_1=0.95, rectify=False),
            sync_period=6,
            slow_step_size=0.45,
        ),
    )


</source>
</class>

<class classid="6" nclones="2" nlines="12" similarity="100">
<source file="systems/addons-0.16.1/tensorflow_addons/optimizers/tests/conditional_gradient_test.py" startline="56" endline="71" pcid="193">
def test_like_dist_belief_nuclear_cg01():
    db_grad, db_out = _db_params_nuclear_cg01()
    num_samples = len(db_grad)
    var0 = tf.Variable([0.0] * num_samples)
    grads0 = tf.constant([0.0] * num_samples)
    ord = "nuclear"
    cg_opt = cg_lib.ConditionalGradient(learning_rate=0.1, lambda_=0.1, ord=ord)

    for i in range(num_samples):
        grads0 = tf.constant(db_grad[i])
        cg_opt.apply_gradients(zip([grads0], [var0]))
        np.testing.assert_allclose(
            np.array(db_out[i]), var0.numpy(), rtol=1e-6, atol=1e-6
        )


</source>
<source file="systems/addons-0.16.1/tensorflow_addons/optimizers/tests/conditional_gradient_test.py" startline="830" endline="845" pcid="215">
def test_like_dist_belief_frobenius_cg01():
    db_grad, db_out = _db_params_frobenius_cg01()
    num_samples = len(db_grad)
    var0 = tf.Variable([0.0] * num_samples)
    grads0 = tf.constant([0.0] * num_samples)
    ord = "fro"
    cg_opt = cg_lib.ConditionalGradient(learning_rate=0.1, lambda_=0.1, ord=ord)

    for i in range(num_samples):
        grads0 = tf.constant(db_grad[i])
        cg_opt.apply_gradients(zip([grads0], [var0]))
        np.testing.assert_allclose(
            np.array(db_out[i]), var0.numpy(), rtol=1e-06, atol=1e-06
        )


</source>
</class>

<class classid="7" nclones="2" nlines="11" similarity="100">
<source file="systems/addons-0.16.1/tensorflow_addons/optimizers/tests/conditional_gradient_test.py" startline="397" endline="413" pcid="205">
def test_variables_across_graphs_frobenius():
    optimizer = cg_lib.ConditionalGradient(0.01, 0.5, ord="fro")
    var0 = tf.Variable([1.0, 2.0], dtype=tf.float32, name="var0")
    var1 = tf.Variable([3.0, 4.0], dtype=tf.float32, name="var1")

    def loss():
        return tf.math.reduce_sum(var0 + var1)

    optimizer.minimize(loss, var_list=[var0, var1])
    optimizer_variables = optimizer.variables()
    # There should be three items. The first item is iteration,
    # and one item for each variable.
    assert optimizer_variables[1].name.startswith("ConditionalGradient/var0")
    assert optimizer_variables[2].name.startswith("ConditionalGradient/var1")
    assert 3 == len(optimizer_variables)


</source>
<source file="systems/addons-0.16.1/tensorflow_addons/optimizers/tests/conditional_gradient_test.py" startline="415" endline="431" pcid="207">
def test_variables_across_graphs_nuclear():
    optimizer = cg_lib.ConditionalGradient(0.01, 0.5, ord="nuclear")
    var0 = tf.Variable([1.0, 2.0], dtype=tf.float32, name="var0")
    var1 = tf.Variable([3.0, 4.0], dtype=tf.float32, name="var1")

    def loss():
        return tf.math.reduce_sum(var0 + var1)

    optimizer.minimize(loss, var_list=[var0, var1])
    optimizer_variables = optimizer.variables()
    # There should be three items. The first item is iteration,
    # and one item for each variable.
    assert optimizer_variables[1].name.startswith("ConditionalGradient/var0")
    assert optimizer_variables[2].name.startswith("ConditionalGradient/var1")
    assert 3 == len(optimizer_variables)


</source>
</class>

<class classid="8" nclones="2" nlines="244" similarity="100">
<source file="systems/addons-0.16.1/tensorflow_addons/optimizers/tests/conditional_gradient_test.py" startline="568" endline="828" pcid="214">
def _db_params_frobenius_cg01():
    """Return dist-belief conditional_gradient values.

    Return values been generated from the dist-belief
    conditional_gradient unittest, running with a learning rate of 0.1
    and a lambda_ of 0.1.

    These values record how a parameter vector of size 10, initialized
    with 0.0, gets updated with 10 consecutive conditional_gradient
    steps.
    It uses random gradients.

    Returns:
        db_grad: The gradients to apply
        db_out: The parameters after the conditional_gradient update.
    """
    db_grad = [[]] * 10
    db_out = [[]] * 10
    db_grad[0] = [
        0.00096264342,
        0.17914793,
        0.93945462,
        0.41396621,
        0.53037018,
        0.93197989,
        0.78648776,
        0.50036013,
        0.55345792,
        0.96722615,
    ]
    db_out[0] = [
        -4.1555551e-05,
        -7.7334875e-03,
        -4.0554531e-02,
        -1.7870162e-02,
        -2.2895107e-02,
        -4.0231861e-02,
        -3.3951234e-02,
        -2.1599628e-02,
        -2.3891762e-02,
        -4.1753378e-02,
    ]
    db_grad[1] = [
        0.17075552,
        0.88821375,
        0.20873757,
        0.25236958,
        0.57578111,
        0.15312378,
        0.5513742,
        0.94687688,
        0.16012503,
        0.22159521,
    ]
    db_out[1] = [
        -0.00961733,
        -0.0507779,
        -0.01580694,
        -0.01599489,
        -0.03470477,
        -0.01264373,
        -0.03443632,
        -0.05546713,
        -0.01140388,
        -0.01665068,
    ]
    db_grad[2] = [
        0.35077485,
        0.47304362,
        0.44412705,
        0.44368884,
        0.078527533,
        0.81223965,
        0.31168157,
        0.43203235,
        0.16792089,
        0.24644311,
    ]
    db_out[2] = [
        -0.02462724,
        -0.03699233,
        -0.03154434,
        -0.03153357,
        -0.00876844,
        -0.05606323,
        -0.02447166,
        -0.03469437,
        -0.0124694,
        -0.01829169,
    ]
    db_grad[3] = [
        0.9694621,
        0.75035888,
        0.28171822,
        0.83813518,
        0.53807181,
        0.3728098,
        0.81454384,
        0.03848977,
        0.89759839,
        0.93665648,
    ]
    db_out[3] = [
        -0.04124615,
        -0.03371741,
        -0.0144246,
        -0.03668303,
        -0.02240246,
        -0.02052062,
        -0.03503307,
        -0.00500922,
        -0.03715545,
        -0.0393002,
    ]
    db_grad[4] = [
        0.38578293,
        0.8536852,
        0.88722926,
        0.66276771,
        0.13678469,
        0.94036359,
        0.69107032,
        0.81897682,
        0.5433259,
        0.67860287,
    ]
    db_out[4] = [
        -0.01979208,
        -0.0380417,
        -0.03747472,
        -0.0305847,
        -0.00779536,
        -0.04024222,
        -0.03156913,
        -0.0337613,
        -0.02578116,
        -0.03148952,
    ]
    db_grad[5] = [
        0.27885768,
        0.76100707,
        0.24625534,
        0.81354135,
        0.18959245,
        0.48038563,
        0.84163809,
        0.41172323,
        0.83259648,
        0.44941229,
    ]
    db_out[5] = [
        -0.01555188,
        -0.04084422,
        -0.01573331,
        -0.04265549,
        -0.01000746,
        -0.02740575,
        -0.04412147,
        -0.02341569,
        -0.0431026,
        -0.02502293,
    ]
    db_grad[6] = [
        0.27233034,
        0.056316052,
        0.5039115,
        0.24105175,
        0.35697976,
        0.75913221,
        0.73577434,
        0.16014607,
        0.57500273,
        0.071136251,
    ]
    db_out[6] = [
        -0.01890448,
        -0.00767214,
        -0.03367592,
        -0.01962219,
        -0.02374279,
        -0.05110247,
        -0.05128598,
        -0.01254396,
        -0.04094185,
        -0.00703416,
    ]
    db_grad[7] = [
        0.58697265,
        0.2494842,
        0.08106143,
        0.39954534,
        0.15892942,
        0.12683646,
        0.74053431,
        0.16033,
        0.66625422,
        0.73515922,
    ]
    db_out[7] = [
        -0.03772914,
        -0.01599993,
        -0.00831695,
        -0.02635719,
        -0.01207801,
        -0.01285448,
        -0.05034328,
        -0.01104364,
        -0.04477356,
        -0.04558991,
    ]
    db_grad[8] = [
        0.8215279,
        0.41994119,
        0.95172721,
        0.68000203,
        0.79439718,
        0.43384039,
        0.55561525,
        0.22567581,
        0.93331909,
        0.29438227,
    ]
    db_out[8] = [
        -0.03919835,
        -0.01970845,
        -0.04187151,
        -0.03195836,
        -0.03546333,
        -0.01999326,
        -0.02899324,
        -0.01083582,
        -0.04472339,
        -0.01725317,
    ]
    db_grad[9] = [
        0.68297005,
        0.67758518,
        0.1748755,
        0.13266537,
        0.70697063,
        0.055731893,
        0.68593478,
        0.50580865,
        0.12602448,
        0.093537711,
    ]
    db_out[9] = [
        -0.04510314,
        -0.04282944,
        -0.0147322,
        -0.0111956,
        -0.04617687,
        -0.00535998,
        -0.0442614,
        -0.03158399,
        -0.01207165,
        -0.00736567,
    ]
    return db_grad, db_out


</source>
<source file="systems/addons-0.16.1/tensorflow_addons/optimizers/tests/conditional_gradient_test.py" startline="1079" endline="1339" pcid="219">
def _db_params_nuclear_cg01():
    """Return dist-belief conditional_gradient values.

    Return values been generated from the dist-belief
    conditional_gradient unittest, running with a learning rate of 0.1
    and a lambda_ of 0.1.

    These values record how a parameter vector of size 10, initialized
    with 0.0, gets updated with 10 consecutive conditional_gradient
    steps.
    It uses random gradients.

    Returns:
        db_grad: The gradients to apply
        db_out: The parameters after the conditional_gradient update.
    """
    db_grad = [[]] * 10
    db_out = [[]] * 10
    db_grad[0] = [
        0.00096264342,
        0.17914793,
        0.93945462,
        0.41396621,
        0.53037018,
        0.93197989,
        0.78648776,
        0.50036013,
        0.55345792,
        0.96722615,
    ]
    db_out[0] = [
        -4.1552783e-05,
        -7.7334875e-03,
        -4.0554535e-02,
        -1.7870164e-02,
        -2.2895109e-02,
        -4.0231861e-02,
        -3.3951234e-02,
        -2.1599628e-02,
        -2.3891764e-02,
        -4.1753381e-02,
    ]
    db_grad[1] = [
        0.17075552,
        0.88821375,
        0.20873757,
        0.25236958,
        0.57578111,
        0.15312378,
        0.5513742,
        0.94687688,
        0.16012503,
        0.22159521,
    ]
    db_out[1] = [
        -0.00961733,
        -0.0507779,
        -0.01580694,
        -0.01599489,
        -0.03470477,
        -0.01264373,
        -0.03443632,
        -0.05546713,
        -0.01140388,
        -0.01665068,
    ]
    db_grad[2] = [
        0.35077485,
        0.47304362,
        0.44412705,
        0.44368884,
        0.078527533,
        0.81223965,
        0.31168157,
        0.43203235,
        0.16792089,
        0.24644311,
    ]
    db_out[2] = [
        -0.02462724,
        -0.03699233,
        -0.03154433,
        -0.03153357,
        -0.00876844,
        -0.05606324,
        -0.02447166,
        -0.03469437,
        -0.0124694,
        -0.01829169,
    ]
    db_grad[3] = [
        0.9694621,
        0.75035888,
        0.28171822,
        0.83813518,
        0.53807181,
        0.3728098,
        0.81454384,
        0.03848977,
        0.89759839,
        0.93665648,
    ]
    db_out[3] = [
        -0.04124615,
        -0.03371741,
        -0.0144246,
        -0.03668303,
        -0.02240246,
        -0.02052062,
        -0.03503307,
        -0.00500922,
        -0.03715545,
        -0.0393002,
    ]
    db_grad[4] = [
        0.38578293,
        0.8536852,
        0.88722926,
        0.66276771,
        0.13678469,
        0.94036359,
        0.69107032,
        0.81897682,
        0.5433259,
        0.67860287,
    ]
    db_out[4] = [
        -0.01979207,
        -0.0380417,
        -0.03747472,
        -0.0305847,
        -0.00779536,
        -0.04024221,
        -0.03156913,
        -0.0337613,
        -0.02578116,
        -0.03148951,
    ]
    db_grad[5] = [
        0.27885768,
        0.76100707,
        0.24625534,
        0.81354135,
        0.18959245,
        0.48038563,
        0.84163809,
        0.41172323,
        0.83259648,
        0.44941229,
    ]
    db_out[5] = [
        -0.01555188,
        -0.04084422,
        -0.01573331,
        -0.04265549,
        -0.01000746,
        -0.02740575,
        -0.04412147,
        -0.02341569,
        -0.0431026,
        -0.02502293,
    ]
    db_grad[6] = [
        0.27233034,
        0.056316052,
        0.5039115,
        0.24105175,
        0.35697976,
        0.75913221,
        0.73577434,
        0.16014607,
        0.57500273,
        0.071136251,
    ]
    db_out[6] = [
        -0.01890448,
        -0.00767214,
        -0.03367592,
        -0.01962219,
        -0.02374278,
        -0.05110246,
        -0.05128598,
        -0.01254396,
        -0.04094184,
        -0.00703416,
    ]
    db_grad[7] = [
        0.58697265,
        0.2494842,
        0.08106143,
        0.39954534,
        0.15892942,
        0.12683646,
        0.74053431,
        0.16033,
        0.66625422,
        0.73515922,
    ]
    db_out[7] = [
        -0.03772915,
        -0.01599993,
        -0.00831695,
        -0.0263572,
        -0.01207801,
        -0.01285448,
        -0.05034329,
        -0.01104364,
        -0.04477356,
        -0.04558992,
    ]
    db_grad[8] = [
        0.8215279,
        0.41994119,
        0.95172721,
        0.68000203,
        0.79439718,
        0.43384039,
        0.55561525,
        0.22567581,
        0.93331909,
        0.29438227,
    ]
    db_out[8] = [
        -0.03919835,
        -0.01970845,
        -0.04187151,
        -0.03195836,
        -0.03546333,
        -0.01999326,
        -0.02899324,
        -0.01083582,
        -0.04472339,
        -0.01725317,
    ]
    db_grad[9] = [
        0.68297005,
        0.67758518,
        0.1748755,
        0.13266537,
        0.70697063,
        0.055731893,
        0.68593478,
        0.50580865,
        0.12602448,
        0.093537711,
    ]
    db_out[9] = [
        -0.04510314,
        -0.04282944,
        -0.0147322,
        -0.0111956,
        -0.04617687,
        -0.00535998,
        -0.0442614,
        -0.031584,
        -0.01207165,
        -0.00736567,
    ]
    return db_grad, db_out


</source>
</class>

<class classid="9" nclones="2" nlines="11" similarity="100">
<source file="systems/addons-0.16.1/tensorflow_addons/optimizers/tests/lookahead_test.py" startline="71" endline="84" pcid="276">
def test_dense_exact_ratio():
    for k in [5, 10, 100]:
        for alpha in [0.3, 0.7]:
            optimizer = tf.keras.optimizers.get("adam")
            vals, quick_vars = run_dense_sample(k, optimizer)
            optimizer = Lookahead("adam", sync_period=k, slow_step_size=alpha)
            _, slow_vars = run_dense_sample(k, optimizer)
            for val, quick, slow in zip(vals, quick_vars, slow_vars):
                expected = val + (quick - val) * alpha
                np.testing.assert_allclose(
                    expected.numpy(), slow.numpy(), rtol=1e-06, atol=1e-06
                )


</source>
<source file="systems/addons-0.16.1/tensorflow_addons/optimizers/tests/lookahead_test.py" startline="86" endline="99" pcid="277">
def test_sparse_exact_ratio():
    for k in [5, 10, 100]:
        for alpha in [0.3, 0.7]:
            optimizer = tf.keras.optimizers.get("adam")
            vals, quick_vars = run_sparse_sample(k, optimizer)
            optimizer = Lookahead("adam", sync_period=k, slow_step_size=alpha)
            _, slow_vars = run_sparse_sample(k, optimizer)
            for val, quick, slow in zip(vals, quick_vars, slow_vars):
                expected = val + (quick - val) * alpha
                np.testing.assert_allclose(
                    expected.numpy(), slow.numpy(), rtol=1e-06, atol=1e-06
                )


</source>
</class>

<class classid="10" nclones="2" nlines="15" similarity="100">
<source file="systems/addons-0.16.1/tensorflow_addons/optimizers/tests/lookahead_test.py" startline="101" endline="122" pcid="278">
def test_fit_simple_linear_model():
    np.random.seed(0x2019)
    tf.random.set_seed(0x2019)

    x = np.random.standard_normal((10000, 3))
    w = np.random.standard_normal((3, 1))
    y = np.dot(x, w) + np.random.standard_normal((10000, 1)) * 1e-4

    model = tf.keras.models.Sequential()
    model.add(tf.keras.layers.Dense(input_shape=(3,), units=1))
    model.compile(Lookahead("sgd"), loss="mse")

    model.fit(x, y, epochs=3)

    x = np.random.standard_normal((100, 3))
    y = np.dot(x, w)
    predicted = model.predict(x)

    max_abs_diff = np.max(np.abs(predicted - y))
    assert max_abs_diff < 1e-3


</source>
<source file="systems/addons-0.16.1/tensorflow_addons/optimizers/tests/lookahead_test.py" startline="124" endline="144" pcid="279">
def test_fit_simple_linear_model_mixed_precision():
    np.random.seed(0x2019)
    tf.random.set_seed(0x2019)

    x = np.random.standard_normal((10000, 3))
    w = np.random.standard_normal((3, 1))
    y = np.dot(x, w) + np.random.standard_normal((10000, 1)) * 1e-4

    model = tf.keras.models.Sequential()
    model.add(tf.keras.layers.Dense(input_shape=(3,), units=1))
    model.compile(Lookahead("sgd"), loss="mse")
    model.fit(x, y, epochs=3)

    x = np.random.standard_normal((100, 3))
    y = np.dot(x, w)
    predicted = model.predict(x)

    max_abs_diff = np.max(np.abs(predicted - y))
    assert max_abs_diff < 2.3e-3


</source>
</class>

<class classid="11" nclones="2" nlines="22" similarity="100">
<source file="systems/addons-0.16.1/tensorflow_addons/callbacks/tests/avg_model_checkpoint_test.py" startline="58" endline="81" pcid="316">
def test_mode_auto(tmp_path):
    test_model_filepath = str(tmp_path / "test_model.h5")
    x, y, model = get_data_and_model()
    monitor = "val_loss"
    save_best_only = False
    mode = "auto"
    avg_model_ckpt = AverageModelCheckpoint(
        update_weights=True,
        filepath=test_model_filepath,
        monitor=monitor,
        save_best_only=save_best_only,
        mode=mode,
    )
    model.fit(
        x,
        y,
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        validation_data=(x, y),
        callbacks=[avg_model_ckpt],
    )
    assert os.path.exists(test_model_filepath)


</source>
<source file="systems/addons-0.16.1/tensorflow_addons/callbacks/tests/avg_model_checkpoint_test.py" startline="82" endline="105" pcid="317">
def test_mode_min(tmp_path):
    test_model_filepath = str(tmp_path / "test_model.h5")
    x, y, model = get_data_and_model()
    monitor = "val_loss"
    save_best_only = False
    mode = "min"
    avg_model_ckpt = AverageModelCheckpoint(
        update_weights=True,
        filepath=test_model_filepath,
        monitor=monitor,
        save_best_only=save_best_only,
        mode=mode,
    )
    model.fit(
        x,
        y,
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        validation_data=(x, y),
        callbacks=[avg_model_ckpt],
    )
    assert os.path.exists(test_model_filepath)


</source>
</class>

<class classid="12" nclones="2" nlines="14" similarity="100">
<source file="systems/addons-0.16.1/tensorflow_addons/losses/tests/npairs_test.py" startline="61" endline="95" pcid="391">
def test_single_label():
    """Test single label, which is the same with `NpairsLoss`."""
    nml_obj = npairs.NpairsMultilabelLoss()
    # batch size = 4, hidden size = 2
    y_true = tf.constant(
        [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]], dtype=tf.int64
    )
    # features of anchors
    f = tf.constant(
        [[1.0, 1.0], [1.0, -1.0], [-1.0, 1.0], [-1.0, -1.0]], dtype=tf.float32
    )
    # features of positive samples
    fp = tf.constant(
        [[1.0, 1.0], [1.0, -1.0], [-1.0, 1.0], [-1.0, -1.0]], dtype=tf.float32
    )
    # similarity matrix
    y_pred = tf.matmul(f, fp, transpose_a=False, transpose_b=True)
    loss = nml_obj(y_true, y_pred)

    # Loss = 1/4 * \sum_i log(1 + \sum_{j != i} exp(f_i*fp_j^T-f_i*f_i^T))
    # Compute loss for i = 0, 1, 2, 3 without multiplier 1/4
    # i = 0 => log(1 + sum([exp(-2), exp(-2), exp(-4)])) = 0.253846
    # i = 1 => log(1 + sum([exp(-2), exp(-4), exp(-2)])) = 0.253846
    # i = 2 => log(1 + sum([exp(-2), exp(-4), exp(-2)])) = 0.253846
    # i = 3 => log(1 + sum([exp(-4), exp(-2), exp(-2)])) = 0.253846
    # Loss = (0.253856 + 0.253856 + 0.253856 + 0.253856) / 4 = 0.253856

    np.testing.assert_allclose(loss, 0.253856, rtol=1e-06, atol=1e-06)

    # Test sparse tensor
    y_true = tf.sparse.from_dense(y_true)
    loss = nml_obj(y_true, y_pred)
    np.testing.assert_allclose(loss, 0.253856, rtol=1e-06, atol=1e-06)


</source>
<source file="systems/addons-0.16.1/tensorflow_addons/losses/tests/npairs_test.py" startline="96" endline="135" pcid="392">
def test_multilabel():
    nml_obj = npairs.NpairsMultilabelLoss()
    # batch size = 4, hidden size = 2
    y_true = tf.constant(
        [[1, 1, 0, 0], [0, 1, 1, 0], [0, 0, 1, 1], [0, 0, 0, 1]], dtype=tf.int64
    )
    # features of anchors
    f = tf.constant(
        [[1.0, 1.0], [1.0, -1.0], [-1.0, 1.0], [-1.0, -1.0]], dtype=tf.float32
    )
    # features of positive samples
    fp = tf.constant(
        [[1.0, 1.0], [1.0, -1.0], [-1.0, 1.0], [-1.0, -1.0]], dtype=tf.float32
    )
    # similarity matrix
    y_pred = tf.matmul(f, fp, transpose_a=False, transpose_b=True)
    loss = nml_obj(y_true, y_pred)

    # Loss = \sum_i log(1 + \sum_{j != i} exp(f_i*fp_j^T-f_i*f_i^T))
    # Because of multilabel, the label matrix is normalized so that each
    # row sums to one. That's why the multiplier before log exists.
    # Compute loss for i = 0, 1, 2, 3 without multiplier 1/4
    # i = 0 => 2/3 * log(1 + sum([exp(-2), exp(-2), exp(-4)])) +
    #          1/3 * log(1 + sum([exp(2) , exp(0) , exp(-2)])) = 0.920522
    # i = 1 => 1/4 * log(1 + sum([exp(2) , exp(-2), exp(0) ])) +
    #          1/2 * log(1 + sum([exp(-2), exp(-4), exp(-2)])) +
    #          1/4 * log(1 + sum([exp(2) , exp(4) , exp(2) ])) = 1.753856
    # i = 2 => 1/4 * log(1 + sum([exp(2) , exp(4) , exp(2) ])) +
    #          1/2 * log(1 + sum([exp(-2), exp(-4), exp(-2)])) +
    #          1/4 * log(1 + sum([exp(0) , exp(-2), exp(2) ])) = 1.753856
    # i = 4 => 1/2 * log(1 + sum([exp(-2), exp(0) , exp(2) ])) +
    #          1/2 * log(1 + sum([exp(-4), exp(-2), exp(-2)])) = 1.253856
    # Loss = (0.920522 + 1.753856 + 1.753856 + 1.253856) / 4 = 1.420522

    np.testing.assert_allclose(loss, 1.420522, rtol=1e-06, atol=1e-06)

    # Test sparse tensor
    y_true = tf.sparse.from_dense(y_true)
    loss = nml_obj(y_true, y_pred)
    np.testing.assert_allclose(loss, 1.420522, rtol=1e-06, atol=1e-06)
</source>
</class>

<class classid="13" nclones="3" nlines="10" similarity="100">
<source file="systems/addons-0.16.1/tensorflow_addons/losses/tests/sparsemax_loss_test.py" startline="28" endline="51" pcid="423">
def _np_sparsemax(z):
    z = z - np.mean(z, axis=1)[:, np.newaxis]

    # sort z
    z_sorted = np.sort(z, axis=1)[:, ::-1]

    # calculate k(z)
    z_cumsum = np.cumsum(z_sorted, axis=1)
    k = np.arange(1, z.shape[1] + 1)
    z_check = 1 + k * z_sorted > z_cumsum
    # use argmax to get the index by row as .nonzero() doesn't
    # take an axis argument. np.argmax return the first index, but the last
    # index is required here, use np.flip to get the last index and
    # `z.shape[axis]` to compensate for np.flip afterwards.
    k_z = z.shape[1] - np.argmax(z_check[:, ::-1], axis=1)

    # calculate tau(z)
    tau_sum = z_cumsum[np.arange(0, z.shape[0]), k_z - 1]
    tau_z = ((tau_sum - 1) / k_z).reshape(-1, 1)

    # calculate p
    return np.maximum(0, z - tau_z)


</source>
<source file="systems/addons-0.16.1/tensorflow_addons/activations/tests/sparsemax_test.py" startline="26" endline="49" pcid="478">
def _np_sparsemax(z):
    z = z - np.mean(z, axis=1)[:, np.newaxis]

    # sort z
    z_sorted = np.sort(z, axis=1)[:, ::-1]

    # calculate k(z)
    z_cumsum = np.cumsum(z_sorted, axis=1)
    k = np.arange(1, z.shape[1] + 1)
    z_check = 1 + k * z_sorted > z_cumsum
    # use argmax to get the index by row as .nonzero() doesn't
    # take an axis argument. np.argmax return the first index, but the last
    # index is required here, use np.flip to get the last index and
    # `z.shape[axis]` to compensate for np.flip afterwards.
    k_z = z.shape[1] - np.argmax(z_check[:, ::-1], axis=1)

    # calculate tau(z)
    tau_sum = z_cumsum[np.arange(0, z.shape[0]), k_z - 1]
    tau_z = ((tau_sum - 1) / k_z).reshape(-1, 1)

    # calculate p
    return np.maximum(0, z - tau_z)


</source>
<source file="systems/addons-0.16.1/tensorflow_addons/layers/tests/sparsemax_test.py" startline="26" endline="49" pcid="994">
def _np_sparsemax(z):
    z = z - np.mean(z, axis=1)[:, np.newaxis]

    # sort z
    z_sorted = np.sort(z, axis=1)[:, ::-1]

    # calculate k(z)
    z_cumsum = np.cumsum(z_sorted, axis=1)
    k = np.arange(1, z.shape[1] + 1)
    z_check = 1 + k * z_sorted > z_cumsum
    # use argmax to get the index by row as .nonzero() doesn't
    # take an axis argument. np.argmax return the first index, but the last
    # index is required here, use np.flip to get the last index and
    # `z.shape[axis]` to compensate for np.flip afterwards.
    k_z = z.shape[1] - np.argmax(z_check[:, ::-1], axis=1)

    # calculate tau(z)
    tau_sum = z_cumsum[np.arange(0, z.shape[0]), k_z - 1]
    tau_z = ((tau_sum - 1) / k_z).reshape(-1, 1)

    # calculate p
    return np.maximum(0, z - tau_z)


</source>
</class>

<class classid="14" nclones="2" nlines="10" similarity="100">
<source file="systems/addons-0.16.1/tensorflow_addons/image/tests/interpolate_spline_test.py" startline="158" endline="175" pcid="547">
def test_1d_interpolation():
    """Regression test for interpolation with 1-D points."""

    tp = _QuadraticPlusSinProblem1D()
    (query_points, _, train_points, train_values) = tp.get_problem(dtype="float64")

    for order in (1, 2, 3):
        for reg_weight in (0, 0.01):
            interp = interpolate_spline(
                train_points, train_values, query_points, order, reg_weight
            )

            target_interpolation = tp.HARDCODED_QUERY_VALUES[(order, reg_weight)]
            target_interpolation = np.array(target_interpolation)

            np.testing.assert_allclose(interp[0, :, 0], target_interpolation)


</source>
<source file="systems/addons-0.16.1/tensorflow_addons/image/tests/interpolate_spline_test.py" startline="176" endline="193" pcid="548">
def test_nd_linear_interpolation():
    """Regression test for interpolation with N-D points."""

    tp = _QuadraticPlusSinProblemND()
    (query_points, _, train_points, train_values) = tp.get_problem(dtype="float64")

    for order in (1, 2, 3):
        for reg_weight in (0, 0.01):
            interp = interpolate_spline(
                train_points, train_values, query_points, order, reg_weight
            )

            target_interpolation = tp.HARDCODED_QUERY_VALUES[(order, reg_weight)]
            target_interpolation = np.array(target_interpolation)

            np.testing.assert_allclose(interp[0, :, 0], target_interpolation)


</source>
</class>

<class classid="15" nclones="2" nlines="37" similarity="100">
<source file="systems/addons-0.16.1/tensorflow_addons/metrics/tests/multilabel_confusion_matrix_test.py" startline="49" endline="90" pcid="721">
def test_mcm_4_classes(dtype):
    actuals = tf.constant(
        [
            [1, 0, 0, 1],
            [0, 0, 1, 1],
            [1, 0, 0, 1],
            [1, 1, 0, 0],
            [0, 1, 0, 1],
            [1, 0, 0, 1],
            [0, 0, 1, 1],
            [1, 0, 0, 1],
            [0, 1, 1, 0],
            [0, 1, 0, 1],
        ],
        dtype=dtype,
    )
    preds = tf.constant(
        [
            [1, 0, 1, 0],
            [0, 0, 1, 1],
            [0, 0, 0, 1],
            [1, 1, 0, 0],
            [1, 0, 0, 0],
            [1, 0, 0, 1],
            [0, 0, 1, 1],
            [1, 0, 0, 1],
            [0, 1, 0, 0],
            [0, 0, 0, 1],
        ],
        dtype=dtype,
    )

    # Initialize
    mcm_obj = MultiLabelConfusionMatrix(num_classes=4, dtype=dtype)
    mcm_obj.update_state(actuals, preds)
    # Check results
    check_results(
        mcm_obj,
        [[[4, 1], [1, 4]], [[6, 0], [2, 2]], [[6, 1], [1, 2]], [[2, 0], [2, 6]]],
    )


</source>
<source file="systems/addons-0.16.1/tensorflow_addons/metrics/tests/multilabel_confusion_matrix_test.py" startline="92" endline="131" pcid="722">
def test_multiclass(dtype):
    actuals = tf.constant(
        [
            [1, 0, 0, 0],
            [0, 0, 1, 0],
            [0, 0, 0, 1],
            [0, 1, 0, 0],
            [0, 1, 0, 0],
            [1, 0, 0, 0],
            [0, 0, 1, 0],
            [1, 0, 0, 0],
            [0, 0, 1, 0],
            [0, 0, 0, 1],
        ],
        dtype=dtype,
    )
    preds = tf.constant(
        [
            [1, 0, 0, 0],
            [0, 0, 1, 0],
            [0, 0, 0, 1],
            [1, 0, 0, 0],
            [1, 0, 0, 0],
            [1, 0, 0, 0],
            [0, 0, 1, 0],
            [1, 0, 0, 0],
            [0, 1, 0, 0],
            [0, 0, 0, 1],
        ],
        dtype=dtype,
    )

    # Initialize
    mcm_obj = MultiLabelConfusionMatrix(num_classes=4, dtype=dtype)
    mcm_obj.update_state(actuals, preds)
    # Check results
    check_results(
        mcm_obj,
        [[[5, 2], [0, 3]], [[7, 1], [2, 0]], [[7, 0], [1, 2]], [[8, 0], [0, 2]]],
    )
</source>
</class>

<class classid="16" nclones="2" nlines="24" similarity="100">
<source file="systems/addons-0.16.1/tensorflow_addons/seq2seq/tests/attention_wrapper_test.py" startline="471" endline="499" pcid="813">
def test_bahdanau_normalized_dtype(dtype):
    dummy_data = DummyData2()
    encoder_outputs = dummy_data.encoder_outputs.astype(dtype)
    decoder_inputs = dummy_data.decoder_inputs.astype(dtype)
    attention_mechanism = wrapper.BahdanauAttention(
        units=dummy_data.units,
        memory=encoder_outputs,
        memory_sequence_length=dummy_data.encoder_sequence_length,
        normalize=True,
        dtype=dtype,
    )
    cell = tf.keras.layers.LSTMCell(
        dummy_data.units, recurrent_activation="sigmoid", dtype=dtype
    )
    cell = wrapper.AttentionWrapper(cell, attention_mechanism, dtype=dtype)

    sampler = sampler_py.TrainingSampler()
    my_decoder = basic_decoder.BasicDecoder(cell=cell, sampler=sampler, dtype=dtype)

    final_outputs, final_state, _ = my_decoder(
        decoder_inputs,
        initial_state=cell.get_initial_state(batch_size=dummy_data.batch, dtype=dtype),
        sequence_length=dummy_data.decoder_sequence_length,
    )
    assert isinstance(final_outputs, basic_decoder.BasicDecoderOutput)
    assert final_outputs.rnn_output.dtype == dtype
    assert isinstance(final_state, wrapper.AttentionWrapperState)


</source>
<source file="systems/addons-0.16.1/tensorflow_addons/seq2seq/tests/attention_wrapper_test.py" startline="501" endline="530" pcid="814">
def test_luong_scaled_dtype(dtype):
    dummy_data = DummyData2()
    # Test case for GitHub issue 18099
    encoder_outputs = dummy_data.encoder_outputs.astype(dtype)
    decoder_inputs = dummy_data.decoder_inputs.astype(dtype)
    attention_mechanism = wrapper.LuongAttention(
        units=dummy_data.units,
        memory=encoder_outputs,
        memory_sequence_length=dummy_data.encoder_sequence_length,
        scale=True,
        dtype=dtype,
    )
    cell = tf.keras.layers.LSTMCell(
        dummy_data.units, recurrent_activation="sigmoid", dtype=dtype
    )
    cell = wrapper.AttentionWrapper(cell, attention_mechanism, dtype=dtype)

    sampler = sampler_py.TrainingSampler()
    my_decoder = basic_decoder.BasicDecoder(cell=cell, sampler=sampler, dtype=dtype)

    final_outputs, final_state, _ = my_decoder(
        decoder_inputs,
        initial_state=cell.get_initial_state(batch_size=dummy_data.batch, dtype=dtype),
        sequence_length=dummy_data.decoder_sequence_length,
    )
    assert isinstance(final_outputs, basic_decoder.BasicDecoderOutput)
    assert final_outputs.rnn_output.dtype == dtype
    assert isinstance(final_state, wrapper.AttentionWrapperState)


</source>
</class>

<class classid="17" nclones="2" nlines="16" similarity="100">
<source file="systems/addons-0.16.1/tensorflow_addons/text/tests/crf_wrapper_test.py" startline="26" endline="46" pcid="833">
def get_test_data():
    x = np.array(
        [
            [
                # O   B-X  I-X  B-Y  I-Y
                [0.0, 1.0, 0.0, 0.0, 0.0],
                [0.0, 0.0, 1.0, 0.0, 0.0],
                [0.0, 0.0, 1.0, 0.0, 0.0],
            ],
            [
                # O   B-X  I-X  B-Y  I-Y
                [0.0, 1.0, 0.0, 0.0, 0.0],
                [0.0, 1.0, 0.0, 0.0, 0.0],
                [0.0, 1.0, 0.0, 0.0, 0.0],
            ],
        ]
    )
    y = np.array([[1, 2, 2], [1, 1, 1]])  # B-X  I-X  I-X  # B-X  B-X  B-X
    return x, y


</source>
<source file="systems/addons-0.16.1/tensorflow_addons/layers/tests/crf_test.py" startline="31" endline="51" pcid="956">
def get_test_data():
    x = np.array(
        [
            [
                # O   B-X  I-X  B-Y  I-Y
                [0.0, 1.0, 0.0, 0.0, 0.0],
                [0.0, 0.0, 1.0, 0.0, 0.0],
                [0.0, 0.0, 1.0, 0.0, 0.0],
            ],
            [
                # O   B-X  I-X  B-Y  I-Y
                [0.0, 1.0, 0.0, 0.0, 0.0],
                [0.0, 1.0, 0.0, 0.0, 0.0],
                [0.0, 1.0, 0.0, 0.0, 0.0],
            ],
        ]
    )
    y = np.array([[1, 2, 2], [1, 1, 1]])  # B-X  I-X  I-X  # B-X  B-X  B-X
    return x, y


</source>
</class>

<class classid="18" nclones="2" nlines="21" similarity="100">
<source file="systems/addons-0.16.1/tensorflow_addons/layers/tests/adaptive_pooling_test.py" startline="32" endline="55" pcid="941">
def test_avg_1d():
    valid_input = np.arange(start=0.0, stop=12.0, step=1.0).astype(np.float32)
    valid_input = np.reshape(valid_input, (1, 12, 1))
    output = np.array([1.0, 4.0, 7.0, 10.0]).astype(np.float32)
    output = np.reshape(output, (1, 4, 1))
    test_utils.layer_test(
        AdaptiveAveragePooling1D,
        kwargs={"output_size": 4, "data_format": "channels_last"},
        input_data=valid_input,
        expected_output=output,
    )

    valid_input = np.arange(start=0.0, stop=12.0, step=1.0).astype(np.float32)
    valid_input = np.reshape(valid_input, (1, 1, 12))
    output = np.array([1.0, 4.0, 7.0, 10.0]).astype(np.float32)
    output = np.reshape(output, (1, 1, 4))
    test_utils.layer_test(
        AdaptiveAveragePooling1D,
        kwargs={"output_size": 4, "data_format": "channels_first"},
        input_data=valid_input,
        expected_output=output,
    )


</source>
<source file="systems/addons-0.16.1/tensorflow_addons/layers/tests/adaptive_pooling_test.py" startline="111" endline="134" pcid="944">
def test_max_1d():
    valid_input = np.arange(start=0.0, stop=12.0, step=1.0).astype(np.float32)
    valid_input = np.reshape(valid_input, (1, 12, 1))
    output = np.array([2.0, 5.0, 8.0, 11.0]).astype(np.float32)
    output = np.reshape(output, (1, 4, 1))
    test_utils.layer_test(
        AdaptiveMaxPooling1D,
        kwargs={"output_size": 4, "data_format": "channels_last"},
        input_data=valid_input,
        expected_output=output,
    )

    valid_input = np.arange(start=0.0, stop=12.0, step=1.0).astype(np.float32)
    valid_input = np.reshape(valid_input, (1, 1, 12))
    output = np.array([2.0, 5.0, 8.0, 11.0]).astype(np.float32)
    output = np.reshape(output, (1, 1, 4))
    test_utils.layer_test(
        AdaptiveMaxPooling1D,
        kwargs={"output_size": 4, "data_format": "channels_first"},
        input_data=valid_input,
        expected_output=output,
    )


</source>
</class>

<class classid="19" nclones="2" nlines="21" similarity="100">
<source file="systems/addons-0.16.1/tensorflow_addons/layers/tests/adaptive_pooling_test.py" startline="57" endline="80" pcid="942">
def test_avg_2d():
    valid_input = np.arange(start=0.0, stop=40.0, step=1.0).astype(np.float32)
    valid_input = np.reshape(valid_input, (1, 4, 10, 1))
    output = np.array([[7.0, 12.0], [27.0, 32.0]]).astype(np.float32)
    output = np.reshape(output, (1, 2, 2, 1))
    test_utils.layer_test(
        AdaptiveAveragePooling2D,
        kwargs={"output_size": (2, 2), "data_format": "channels_last"},
        input_data=valid_input,
        expected_output=output,
    )

    valid_input = np.arange(start=0.0, stop=40.0, step=1.0).astype(np.float32)
    valid_input = np.reshape(valid_input, (1, 1, 4, 10))
    output = np.array([[7.0, 12.0], [27.0, 32.0]]).astype(np.float32)
    output = np.reshape(output, (1, 1, 2, 2))
    test_utils.layer_test(
        AdaptiveAveragePooling2D,
        kwargs={"output_size": (2, 2), "data_format": "channels_first"},
        input_data=valid_input,
        expected_output=output,
    )


</source>
<source file="systems/addons-0.16.1/tensorflow_addons/layers/tests/adaptive_pooling_test.py" startline="136" endline="159" pcid="945">
def test_max_2d():
    valid_input = np.arange(start=0.0, stop=40.0, step=1.0).astype(np.float32)
    valid_input = np.reshape(valid_input, (1, 4, 10, 1))
    output = np.array([[14.0, 19.0], [34.0, 39.0]]).astype(np.float32)
    output = np.reshape(output, (1, 2, 2, 1))
    test_utils.layer_test(
        AdaptiveMaxPooling2D,
        kwargs={"output_size": (2, 2), "data_format": "channels_last"},
        input_data=valid_input,
        expected_output=output,
    )

    valid_input = np.arange(start=0.0, stop=40.0, step=1.0).astype(np.float32)
    valid_input = np.reshape(valid_input, (1, 1, 4, 10))
    output = np.array([[14.0, 19.0], [34.0, 39.0]]).astype(np.float32)
    output = np.reshape(output, (1, 1, 2, 2))
    test_utils.layer_test(
        AdaptiveMaxPooling2D,
        kwargs={"output_size": (2, 2), "data_format": "channels_first"},
        input_data=valid_input,
        expected_output=output,
    )


</source>
</class>

<class classid="20" nclones="2" nlines="23" similarity="100">
<source file="systems/addons-0.16.1/tensorflow_addons/layers/tests/adaptive_pooling_test.py" startline="82" endline="109" pcid="943">
def test_avg_3d():
    valid_input = np.arange(start=0.0, stop=80.0, step=1.0).astype(np.float32)
    valid_input = np.reshape(valid_input, (1, 4, 10, 2, 1))
    output = np.array(
        [[[14.0, 15.0], [24.0, 25.0]], [[54.0, 55.0], [64.0, 65.0]]]
    ).astype(np.float32)
    output = np.reshape(output, (1, 2, 2, 2, 1))
    test_utils.layer_test(
        AdaptiveAveragePooling3D,
        kwargs={"output_size": (2, 2, 2), "data_format": "channels_last"},
        input_data=valid_input,
        expected_output=output,
    )

    valid_input = np.arange(start=0.0, stop=80.0, step=1.0).astype(np.float32)
    valid_input = np.reshape(valid_input, (1, 1, 4, 10, 2))
    output = np.array(
        [[[14.0, 15.0], [24.0, 25.0]], [[54.0, 55.0], [64.0, 65.0]]]
    ).astype(np.float32)
    output = np.reshape(output, (1, 1, 2, 2, 2))
    test_utils.layer_test(
        AdaptiveAveragePooling3D,
        kwargs={"output_size": (2, 2, 2), "data_format": "channels_first"},
        input_data=valid_input,
        expected_output=output,
    )


</source>
<source file="systems/addons-0.16.1/tensorflow_addons/layers/tests/adaptive_pooling_test.py" startline="161" endline="186" pcid="946">
def test_max_3d():
    valid_input = np.arange(start=0.0, stop=80.0, step=1.0).astype(np.float32)
    valid_input = np.reshape(valid_input, (1, 4, 10, 2, 1))
    output = np.array(
        [[[28.0, 29.0], [38.0, 39.0]], [[68.0, 69.0], [78.0, 79.0]]]
    ).astype(np.float32)
    output = np.reshape(output, (1, 2, 2, 2, 1))
    test_utils.layer_test(
        AdaptiveMaxPooling3D,
        kwargs={"output_size": (2, 2, 2), "data_format": "channels_last"},
        input_data=valid_input,
        expected_output=output,
    )

    valid_input = np.arange(start=0.0, stop=80.0, step=1.0).astype(np.float32)
    valid_input = np.reshape(valid_input, (1, 1, 4, 10, 2))
    output = np.array(
        [[[28.0, 29.0], [38.0, 39.0]], [[68.0, 69.0], [78.0, 79.0]]]
    ).astype(np.float32)
    output = np.reshape(output, (1, 1, 2, 2, 2))
    test_utils.layer_test(
        AdaptiveMaxPooling3D,
        kwargs={"output_size": (2, 2, 2), "data_format": "channels_first"},
        input_data=valid_input,
        expected_output=output,
    )
</source>
</class>

<class classid="21" nclones="2" nlines="10" similarity="100">
<source file="systems/addons-0.16.1/tensorflow_addons/layers/tests/normalizations_test.py" startline="368" endline="380" pcid="1019">
def test_with_beta(dtype):
    set_random_seed()
    inputs = np.random.rand(28, 28, 1).astype(dtype)
    inputs = np.expand_dims(inputs, axis=0)
    frn = FilterResponseNormalization(
        beta_initializer="ones", gamma_initializer="ones", dtype=dtype
    )
    frn.build((None, 28, 28, 1))
    observed = frn(inputs)
    expected = calculate_frn(inputs, beta=1, gamma=1, dtype=dtype)
    np.testing.assert_allclose(expected[0], observed[0])


</source>
<source file="systems/addons-0.16.1/tensorflow_addons/layers/tests/normalizations_test.py" startline="383" endline="395" pcid="1020">
def test_with_gamma(dtype):
    set_random_seed()
    inputs = np.random.rand(28, 28, 1).astype(dtype)
    inputs = np.expand_dims(inputs, axis=0)
    frn = FilterResponseNormalization(
        beta_initializer="zeros", gamma_initializer="ones", dtype=dtype
    )
    frn.build((None, 28, 28, 1))
    observed = frn(inputs)
    expected = calculate_frn(inputs, beta=0, gamma=1, dtype=dtype)
    np.testing.assert_allclose(expected[0], observed[0])


</source>
</class>

</clones>

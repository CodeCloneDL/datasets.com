<html>
<head>
    <title>NiCad6 Clone Report</title>
    <style type="text/css">
        body {font-family:sans-serif;}
        table {background-color:white; border:0px; padding:0px; border-spacing:4px; width:auto; margin-left:30px; margin-right:auto;}
        td {background-color:rgba(192,212,238,0.8); border:0px; padding:8px; width:auto; vertical-align:top; border-radius:8px}
        pre {background-color:white; padding:4px;}
        a {color:darkblue;}
    </style>
</head>
<body>
<h2>NiCad6 Clone Report</h2>
<table>
<tr style="font-size:14pt">
<td><b>System:</b> &nbsp; scrapy-2.6.1</td>
<td><b>Clone pairs:</b> &nbsp; 106</td>
<td><b>Clone classes:</b> &nbsp; 34</td>
</tr>
<tr style="font-size:12pt">
<td style="background-color:white">Clone type: &nbsp; 3-2</td>
<td style="background-color:white">Granularity: &nbsp; functions-blind</td>
<td style="background-color:white">Max diff threshold: &nbsp; 30%</td>
<td style="background-color:white">Clone size: &nbsp; 10 - 2500 lines</td>
<td style="background-color:white">Total functions-blind: &nbsp; 2008</td>
</tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 1:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 91%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag39')" href="javascript:;">
scrapy-2.6.1/tests/test_downloadermiddleware_redirect.py: 88-102
</a>
<div class="mid" id="frag39" style="display:none"><pre>
    def test_redirect_302_head(self):
        url = 'http://www.example.com/302'
        url2 = 'http://www.example.com/redirected2'
        req = Request(url, method='HEAD')
        rsp = Response(url, headers={'Location': url2}, status=302)

        req2 = self.mw.process_response(req, rsp, self.spider)
        assert isinstance(req2, Request)
        self.assertEqual(req2.url, url2)
        self.assertEqual(req2.method, 'HEAD')

        # response without Location header but with status code is 3XX should be ignored
        del rsp.headers['Location']
        assert self.mw.process_response(req, rsp, self.spider) is rsp

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag40')" href="javascript:;">
scrapy-2.6.1/tests/test_downloadermiddleware_redirect.py: 103-118
</a>
<div class="mid" id="frag40" style="display:none"><pre>
    def test_redirect_302_relative(self):
        url = 'http://www.example.com/302'
        url2 = '///i8n.example2.com/302'
        url3 = 'http://i8n.example2.com/302'
        req = Request(url, method='HEAD')
        rsp = Response(url, headers={'Location': url2}, status=302)

        req2 = self.mw.process_response(req, rsp, self.spider)
        assert isinstance(req2, Request)
        self.assertEqual(req2.url, url3)
        self.assertEqual(req2.method, 'HEAD')

        # response without Location header but with status code is 3XX should be ignored
        del rsp.headers['Location']
        assert self.mw.process_response(req, rsp, self.spider) is rsp

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 2:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag43')" href="javascript:;">
scrapy-2.6.1/tests/test_downloadermiddleware_redirect.py: 139-153
</a>
<div class="mid" id="frag43" style="display:none"><pre>
    def test_redirect_urls(self):
        req1 = Request('http://scrapytest.org/first')
        rsp1 = Response('http://scrapytest.org/first', headers={'Location': '/redirected'}, status=302)
        req2 = self.mw.process_response(req1, rsp1, self.spider)
        rsp2 = Response('http://scrapytest.org/redirected', headers={'Location': '/redirected2'}, status=302)
        req3 = self.mw.process_response(req2, rsp2, self.spider)

        self.assertEqual(req2.url, 'http://scrapytest.org/redirected')
        self.assertEqual(req2.meta['redirect_urls'], ['http://scrapytest.org/first'])
        self.assertEqual(req3.url, 'http://scrapytest.org/redirected2')
        self.assertEqual(
            req3.meta['redirect_urls'],
            ['http://scrapytest.org/first', 'http://scrapytest.org/redirected']
        )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag58')" href="javascript:;">
scrapy-2.6.1/tests/test_downloadermiddleware_redirect.py: 268-283
</a>
<div class="mid" id="frag58" style="display:none"><pre>
    def test_redirect_urls(self):
        req1 = Request('http://scrapytest.org/first')
        rsp1 = HtmlResponse(req1.url, body=self._body(url='/redirected'))
        req2 = self.mw.process_response(req1, rsp1, self.spider)
        assert isinstance(req2, Request), req2
        rsp2 = HtmlResponse(req2.url, body=self._body(url='/redirected2'))
        req3 = self.mw.process_response(req2, rsp2, self.spider)
        assert isinstance(req3, Request), req3
        self.assertEqual(req2.url, 'http://scrapytest.org/redirected')
        self.assertEqual(req2.meta['redirect_urls'], ['http://scrapytest.org/first'])
        self.assertEqual(req3.url, 'http://scrapytest.org/redirected2')
        self.assertEqual(
            req3.meta['redirect_urls'],
            ['http://scrapytest.org/first', 'http://scrapytest.org/redirected']
        )

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 3:</b> &nbsp; 3 fragments, nominal size 11 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag212')" href="javascript:;">
scrapy-2.6.1/tests/test_responsetypes.py: 9-21
</a>
<div class="mid" id="frag212" style="display:none"><pre>
    def test_from_filename(self):
        mappings = [
            ('data.bin', Response),
            ('file.txt', TextResponse),
            ('file.xml.gz', Response),
            ('file.xml', XmlResponse),
            ('file.html', HtmlResponse),
            ('file.unknownext', Response),
        ]
        for source, cls in mappings:
            retcls = responsetypes.from_filename(source)
            assert retcls is cls, f"{source} ==&gt; {retcls} != {cls}"

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag215')" href="javascript:;">
scrapy-2.6.1/tests/test_responsetypes.py: 52-62
</a>
<div class="mid" id="frag215" style="display:none"><pre>
    def test_from_body(self):
        mappings = [
            (b'\x03\x02\xdf\xdd\x23', Response),
            (b'Some plain text\ndata with tabs\t and null bytes\0', TextResponse),
            (b'&lt;html&gt;&lt;head&gt;&lt;title&gt;Hello&lt;/title&gt;&lt;/head&gt;', HtmlResponse),
            (b'&lt;?xml version="1.0" encoding="utf-8"', XmlResponse),
        ]
        for source, cls in mappings:
            retcls = responsetypes.from_body(source)
            assert retcls is cls, f"{source} ==&gt; {retcls} != {cls}"

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag214')" href="javascript:;">
scrapy-2.6.1/tests/test_responsetypes.py: 37-51
</a>
<div class="mid" id="frag214" style="display:none"><pre>
    def test_from_content_type(self):
        mappings = [
            ('text/html; charset=UTF-8', HtmlResponse),
            ('text/xml; charset=UTF-8', XmlResponse),
            ('application/xhtml+xml; charset=UTF-8', HtmlResponse),
            ('application/vnd.wap.xhtml+xml; charset=utf-8', HtmlResponse),
            ('application/xml; charset=UTF-8', XmlResponse),
            ('application/octet-stream', Response),
            ('application/x-json; encoding=UTF8;charset=UTF-8', TextResponse),
            ('application/json-amazonui-streaming;charset=UTF-8', TextResponse),
        ]
        for source, cls in mappings:
            retcls = responsetypes.from_content_type(source)
            assert retcls is cls, f"{source} ==&gt; {retcls} != {cls}"

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 4:</b> &nbsp; 3 fragments, nominal size 13 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag235')" href="javascript:;">
scrapy-2.6.1/tests/test_request_attribute_binding.py: 91-103
</a>
<div class="mid" id="frag235" style="display:none"><pre>
    def test_downloader_middleware_raise_exception(self):
        url = self.mockserver.url("/status?n=200")
        runner = CrawlerRunner(settings={
            "DOWNLOADER_MIDDLEWARES": {
                RaiseExceptionRequestMiddleware: 590,
            },
        })
        crawler = runner.create_crawler(SingleRequestSpider)
        yield crawler.crawl(seed=url, mockserver=self.mockserver)
        failure = crawler.spider.meta["failure"]
        self.assertEqual(failure.request.url, url)
        self.assertIsInstance(failure.value, ZeroDivisionError)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag239')" href="javascript:;">
scrapy-2.6.1/tests/test_request_attribute_binding.py: 163-182
</a>
<div class="mid" id="frag239" style="display:none"><pre>
    def test_downloader_middleware_do_not_override_in_process_exception(self):
        """
        An exception is raised but caught by the next middleware, which
        returns a Response without a specific 'request' attribute.

        The spider callback should receive the original response.request
        """
        url = self.mockserver.url("/status?n=200")
        runner = CrawlerRunner(settings={
            "DOWNLOADER_MIDDLEWARES": {
                RaiseExceptionRequestMiddleware: 590,
                CatchExceptionDoNotOverrideRequestMiddleware: 595,
            },
        })
        crawler = runner.create_crawler(SingleRequestSpider)
        yield crawler.crawl(seed=url, mockserver=self.mockserver)
        response = crawler.spider.meta["responses"][0]
        self.assertEqual(response.body, b"Caught ZeroDivisionError")
        self.assertEqual(response.request.url, url)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag238')" href="javascript:;">
scrapy-2.6.1/tests/test_request_attribute_binding.py: 142-161
</a>
<div class="mid" id="frag238" style="display:none"><pre>
    def test_downloader_middleware_override_in_process_exception(self):
        """
        An exception is raised but caught by the next middleware, which
        returns a Response with a specific 'request' attribute.

        The spider callback should receive the overridden response.request
        """
        url = self.mockserver.url("/status?n=200")
        runner = CrawlerRunner(settings={
            "DOWNLOADER_MIDDLEWARES": {
                RaiseExceptionRequestMiddleware: 590,
                CatchExceptionOverrideRequestMiddleware: 595,
            },
        })
        crawler = runner.create_crawler(SingleRequestSpider)
        yield crawler.crawl(seed=url, mockserver=self.mockserver)
        response = crawler.spider.meta["responses"][0]
        self.assertEqual(response.body, b"Caught ZeroDivisionError")
        self.assertEqual(response.request.url, OVERRIDEN_URL)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 5:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 92%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag260')" href="javascript:;">
scrapy-2.6.1/tests/test_loader.py: 247-266
</a>
<div class="mid" id="frag260" style="display:none"><pre>
    def test_output_processor(self):

        class TempItem(Item):
            temp = Field()

            def __init__(self, *args, **kwargs):
                super().__init__(self, *args, **kwargs)
                self.setdefault('temp', 0.3)

        class TempLoader(ItemLoader):
            default_item_class = TempItem
            default_input_processor = Identity()
            default_output_processor = Compose(TakeFirst())

        loader = TempLoader()
        item = loader.load_item()
        self.assertIsInstance(item, TempItem)
        self.assertEqual(dict(item), {'temp': 0.3})


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1254')" href="javascript:;">
scrapy-2.6.1/tests/test_loader_deprecated.py: 578-595
</a>
<div class="mid" id="frag1254" style="display:none"><pre>
    def test_output_processor(self):

        class TempDict(dict):
            def __init__(self, *args, **kwargs):
                super().__init__(self, *args, **kwargs)
                self.setdefault('temp', 0.3)

        class TempLoader(ItemLoader):
            default_item_class = TempDict
            default_input_processor = Identity()
            default_output_processor = Compose(TakeFirst())

        loader = TempLoader()
        item = loader.load_item()
        self.assertIsInstance(item, TempDict)
        self.assertEqual(dict(item), {'temp': 0.3})


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 6:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag279')" href="javascript:;">
scrapy-2.6.1/tests/test_loader.py: 448-463
</a>
<div class="mid" id="frag279" style="display:none"><pre>
    def test_nested_xpath(self):
        l = NestedItemLoader(response=self.response)

        nl = l.nested_xpath("//header")
        nl.add_xpath('name', 'div/text()')
        nl.add_css('name_div', '#id')
        nl.add_value('name_value', nl.selector.xpath('div[@id = "id"]/text()').getall())

        self.assertEqual(l.get_output_value('name'), ['marta'])
        self.assertEqual(l.get_output_value('name_div'), ['&lt;div id="id"&gt;marta&lt;/div&gt;'])
        self.assertEqual(l.get_output_value('name_value'), ['marta'])

        self.assertEqual(l.get_output_value('name'), nl.get_output_value('name'))
        self.assertEqual(l.get_output_value('name_div'), nl.get_output_value('name_div'))
        self.assertEqual(l.get_output_value('name_value'), nl.get_output_value('name_value'))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag280')" href="javascript:;">
scrapy-2.6.1/tests/test_loader.py: 464-478
</a>
<div class="mid" id="frag280" style="display:none"><pre>
    def test_nested_css(self):
        l = NestedItemLoader(response=self.response)
        nl = l.nested_css("header")
        nl.add_xpath('name', 'div/text()')
        nl.add_css('name_div', '#id')
        nl.add_value('name_value', nl.selector.xpath('div[@id = "id"]/text()').getall())

        self.assertEqual(l.get_output_value('name'), ['marta'])
        self.assertEqual(l.get_output_value('name_div'), ['&lt;div id="id"&gt;marta&lt;/div&gt;'])
        self.assertEqual(l.get_output_value('name_value'), ['marta'])

        self.assertEqual(l.get_output_value('name'), nl.get_output_value('name'))
        self.assertEqual(l.get_output_value('name_div'), nl.get_output_value('name_div'))
        self.assertEqual(l.get_output_value('name_value'), nl.get_output_value('name_value'))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 7:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag459')" href="javascript:;">
scrapy-2.6.1/tests/test_downloadermiddleware_httpproxy.py: 63-75
</a>
<div class="mid" id="frag459" style="display:none"><pre>
    def test_proxy_auth(self):
        os.environ['http_proxy'] = 'https://user:pass@proxy:3128'
        mw = HttpProxyMiddleware()
        req = Request('http://scrapytest.org')
        assert mw.process_request(req, spider) is None
        self.assertEqual(req.meta, {'proxy': 'https://proxy:3128'})
        self.assertEqual(req.headers.get('Proxy-Authorization'), b'Basic dXNlcjpwYXNz')
        # proxy from request.meta
        req = Request('http://scrapytest.org', meta={'proxy': 'https://username:password@proxy:3128'})
        assert mw.process_request(req, spider) is None
        self.assertEqual(req.meta, {'proxy': 'https://proxy:3128'})
        self.assertEqual(req.headers.get('Proxy-Authorization'), b'Basic dXNlcm5hbWU6cGFzc3dvcmQ=')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag460')" href="javascript:;">
scrapy-2.6.1/tests/test_downloadermiddleware_httpproxy.py: 76-88
</a>
<div class="mid" id="frag460" style="display:none"><pre>
    def test_proxy_auth_empty_passwd(self):
        os.environ['http_proxy'] = 'https://user:@proxy:3128'
        mw = HttpProxyMiddleware()
        req = Request('http://scrapytest.org')
        assert mw.process_request(req, spider) is None
        self.assertEqual(req.meta, {'proxy': 'https://proxy:3128'})
        self.assertEqual(req.headers.get('Proxy-Authorization'), b'Basic dXNlcjo=')
        # proxy from request.meta
        req = Request('http://scrapytest.org', meta={'proxy': 'https://username:@proxy:3128'})
        assert mw.process_request(req, spider) is None
        self.assertEqual(req.meta, {'proxy': 'https://proxy:3128'})
        self.assertEqual(req.headers.get('Proxy-Authorization'), b'Basic dXNlcm5hbWU6')

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 8:</b> &nbsp; 2 fragments, nominal size 19 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag492')" href="javascript:;">
scrapy-2.6.1/tests/test_contracts.py: 292-320
</a>
<div class="mid" id="frag492" style="display:none"><pre>
    def test_returns(self):
        spider = TestSpider()
        response = ResponseMock()

        # returns_item
        request = self.conman.from_method(spider.returns_item, self.results)
        request.callback(response)
        self.should_succeed()

        # returns_dict_item
        request = self.conman.from_method(spider.returns_dict_item, self.results)
        request.callback(response)
        self.should_succeed()

        # returns_request
        request = self.conman.from_method(spider.returns_request, self.results)
        request.callback(response)
        self.should_succeed()

        # returns_fail
        request = self.conman.from_method(spider.returns_fail, self.results)
        request.callback(response)
        self.should_fail()

        # returns_dict_fail
        request = self.conman.from_method(spider.returns_dict_fail, self.results)
        request.callback(response)
        self.should_fail()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag493')" href="javascript:;">
scrapy-2.6.1/tests/test_contracts.py: 321-351
</a>
<div class="mid" id="frag493" style="display:none"><pre>
    def test_scrapes(self):
        spider = TestSpider()
        response = ResponseMock()

        # scrapes_item_ok
        request = self.conman.from_method(spider.scrapes_item_ok, self.results)
        request.callback(response)
        self.should_succeed()

        # scrapes_dict_item_ok
        request = self.conman.from_method(spider.scrapes_dict_item_ok, self.results)
        request.callback(response)
        self.should_succeed()

        # scrapes_item_fail
        request = self.conman.from_method(spider.scrapes_item_fail, self.results)
        request.callback(response)
        self.should_fail()

        # scrapes_dict_item_fail
        request = self.conman.from_method(spider.scrapes_dict_item_fail, self.results)
        request.callback(response)
        self.should_fail()

        # scrapes_multiple_missing_fields
        request = self.conman.from_method(spider.scrapes_multiple_missing_fields, self.results)
        request.callback(response)
        self.should_fail()
        message = 'ContractFail: Missing fields: name, url'
        assert message in self.results.failures[-1][-1]

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 9:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag512')" href="javascript:;">
scrapy-2.6.1/tests/test_squeues_request.py: 47-63
</a>
<div class="mid" id="frag512" style="display:none"><pre>
    def test_one_element_with_peek(self):
        if not hasattr(queuelib.queue.FifoMemoryQueue, "peek"):
            raise unittest.SkipTest("The queuelib queues do not define peek")
        q = self.queue()
        self.assertEqual(len(q), 0)
        self.assertIsNone(q.peek())
        self.assertIsNone(q.pop())
        req = Request("http://www.example.com")
        q.push(req)
        self.assertEqual(len(q), 1)
        self.assertEqual(q.peek().url, req.url)
        self.assertEqual(q.pop().url, req.url)
        self.assertEqual(len(q), 0)
        self.assertIsNone(q.peek())
        self.assertIsNone(q.pop())
        q.close()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag513')" href="javascript:;">
scrapy-2.6.1/tests/test_squeues_request.py: 64-80
</a>
<div class="mid" id="frag513" style="display:none"><pre>
    def test_one_element_without_peek(self):
        if hasattr(queuelib.queue.FifoMemoryQueue, "peek"):
            raise unittest.SkipTest("The queuelib queues define peek")
        q = self.queue()
        self.assertEqual(len(q), 0)
        self.assertIsNone(q.pop())
        req = Request("http://www.example.com")
        q.push(req)
        self.assertEqual(len(q), 1)
        with self.assertRaises(NotImplementedError, msg="The underlying queue class does not implement 'peek'"):
            q.peek()
        self.assertEqual(q.pop().url, req.url)
        self.assertEqual(len(q), 0)
        self.assertIsNone(q.pop())
        q.close()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 10:</b> &nbsp; 5 fragments, nominal size 25 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag514')" href="javascript:;">
scrapy-2.6.1/tests/test_squeues_request.py: 82-108
</a>
<div class="mid" id="frag514" style="display:none"><pre>
    def test_fifo_with_peek(self):
        if not hasattr(queuelib.queue.FifoMemoryQueue, "peek"):
            raise unittest.SkipTest("The queuelib queues do not define peek")
        q = self.queue()
        self.assertEqual(len(q), 0)
        self.assertIsNone(q.peek())
        self.assertIsNone(q.pop())
        req1 = Request("http://www.example.com/1")
        req2 = Request("http://www.example.com/2")
        req3 = Request("http://www.example.com/3")
        q.push(req1)
        q.push(req2)
        q.push(req3)
        self.assertEqual(len(q), 3)
        self.assertEqual(q.peek().url, req1.url)
        self.assertEqual(q.pop().url, req1.url)
        self.assertEqual(len(q), 2)
        self.assertEqual(q.peek().url, req2.url)
        self.assertEqual(q.pop().url, req2.url)
        self.assertEqual(len(q), 1)
        self.assertEqual(q.peek().url, req3.url)
        self.assertEqual(q.pop().url, req3.url)
        self.assertEqual(len(q), 0)
        self.assertIsNone(q.peek())
        self.assertIsNone(q.pop())
        q.close()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag517')" href="javascript:;">
scrapy-2.6.1/tests/test_squeues_request.py: 162-186
</a>
<div class="mid" id="frag517" style="display:none"><pre>
    def test_lifo_without_peek(self):
        if hasattr(queuelib.queue.FifoMemoryQueue, "peek"):
            raise unittest.SkipTest("The queuelib queues do not define peek")
        q = self.queue()
        self.assertEqual(len(q), 0)
        self.assertIsNone(q.pop())
        req1 = Request("http://www.example.com/1")
        req2 = Request("http://www.example.com/2")
        req3 = Request("http://www.example.com/3")
        q.push(req1)
        q.push(req2)
        q.push(req3)
        with self.assertRaises(NotImplementedError, msg="The underlying queue class does not implement 'peek'"):
            q.peek()
        self.assertEqual(len(q), 3)
        self.assertEqual(q.pop().url, req3.url)
        self.assertEqual(len(q), 2)
        self.assertEqual(q.pop().url, req2.url)
        self.assertEqual(len(q), 1)
        self.assertEqual(q.pop().url, req1.url)
        self.assertEqual(len(q), 0)
        self.assertIsNone(q.pop())
        q.close()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag515')" href="javascript:;">
scrapy-2.6.1/tests/test_squeues_request.py: 109-133
</a>
<div class="mid" id="frag515" style="display:none"><pre>
    def test_fifo_without_peek(self):
        if hasattr(queuelib.queue.FifoMemoryQueue, "peek"):
            raise unittest.SkipTest("The queuelib queues do not define peek")
        q = self.queue()
        self.assertEqual(len(q), 0)
        self.assertIsNone(q.pop())
        req1 = Request("http://www.example.com/1")
        req2 = Request("http://www.example.com/2")
        req3 = Request("http://www.example.com/3")
        q.push(req1)
        q.push(req2)
        q.push(req3)
        with self.assertRaises(NotImplementedError, msg="The underlying queue class does not implement 'peek'"):
            q.peek()
        self.assertEqual(len(q), 3)
        self.assertEqual(q.pop().url, req1.url)
        self.assertEqual(len(q), 2)
        self.assertEqual(q.pop().url, req2.url)
        self.assertEqual(len(q), 1)
        self.assertEqual(q.pop().url, req3.url)
        self.assertEqual(len(q), 0)
        self.assertIsNone(q.pop())
        q.close()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1170')" href="javascript:;">
scrapy-2.6.1/tests/test_pqueues.py: 44-67
</a>
<div class="mid" id="frag1170" style="display:none"><pre>
    def test_peek(self):
        if not hasattr(queuelib.queue.FifoMemoryQueue, "peek"):
            raise unittest.SkipTest("queuelib.queue.FifoMemoryQueue.peek is undefined")
        temp_dir = tempfile.mkdtemp()
        queue = ScrapyPriorityQueue.from_crawler(self.crawler, FifoMemoryQueue, temp_dir)
        self.assertEqual(len(queue), 0)
        self.assertIsNone(queue.peek())
        req1 = Request("https://example.org/1")
        req2 = Request("https://example.org/2")
        req3 = Request("https://example.org/3")
        queue.push(req1)
        queue.push(req2)
        queue.push(req3)
        self.assertEqual(len(queue), 3)
        self.assertEqual(queue.peek().url, req1.url)
        self.assertEqual(queue.pop().url, req1.url)
        self.assertEqual(len(queue), 2)
        self.assertEqual(queue.peek().url, req2.url)
        self.assertEqual(queue.pop().url, req2.url)
        self.assertEqual(len(queue), 1)
        self.assertEqual(queue.peek().url, req3.url)
        self.assertEqual(queue.pop().url, req3.url)
        self.assertEqual(queue.close(), [])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag516')" href="javascript:;">
scrapy-2.6.1/tests/test_squeues_request.py: 135-161
</a>
<div class="mid" id="frag516" style="display:none"><pre>
    def test_lifo_with_peek(self):
        if not hasattr(queuelib.queue.FifoMemoryQueue, "peek"):
            raise unittest.SkipTest("The queuelib queues do not define peek")
        q = self.queue()
        self.assertEqual(len(q), 0)
        self.assertIsNone(q.peek())
        self.assertIsNone(q.pop())
        req1 = Request("http://www.example.com/1")
        req2 = Request("http://www.example.com/2")
        req3 = Request("http://www.example.com/3")
        q.push(req1)
        q.push(req2)
        q.push(req3)
        self.assertEqual(len(q), 3)
        self.assertEqual(q.peek().url, req3.url)
        self.assertEqual(q.pop().url, req3.url)
        self.assertEqual(len(q), 2)
        self.assertEqual(q.peek().url, req2.url)
        self.assertEqual(q.pop().url, req2.url)
        self.assertEqual(len(q), 1)
        self.assertEqual(q.peek().url, req1.url)
        self.assertEqual(q.pop().url, req1.url)
        self.assertEqual(len(q), 0)
        self.assertIsNone(q.peek())
        self.assertIsNone(q.pop())
        q.close()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 11:</b> &nbsp; 2 fragments, nominal size 21 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag542')" href="javascript:;">
scrapy-2.6.1/tests/test_utils_conf.py: 161-181
</a>
<div class="mid" id="frag542" style="display:none"><pre>
    def test_feed_complete_default_values_from_settings_empty(self):
        feed = {}
        settings = Settings({
            "FEED_EXPORT_ENCODING": "custom encoding",
            "FEED_EXPORT_FIELDS": ["f1", "f2", "f3"],
            "FEED_EXPORT_INDENT": 42,
            "FEED_STORE_EMPTY": True,
            "FEED_URI_PARAMS": (1, 2, 3, 4),
            "FEED_EXPORT_BATCH_ITEM_COUNT": 2,
        })
        new_feed = feed_complete_default_values_from_settings(feed, settings)
        self.assertEqual(new_feed, {
            "encoding": "custom encoding",
            "fields": ["f1", "f2", "f3"],
            "indent": 42,
            "store_empty": True,
            "uri_params": (1, 2, 3, 4),
            "batch_item_count": 2,
            "item_export_kwargs": {},
        })

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag543')" href="javascript:;">
scrapy-2.6.1/tests/test_utils_conf.py: 182-205
</a>
<div class="mid" id="frag543" style="display:none"><pre>
    def test_feed_complete_default_values_from_settings_non_empty(self):
        feed = {
            "encoding": "other encoding",
            "fields": None,
        }
        settings = Settings({
            "FEED_EXPORT_ENCODING": "custom encoding",
            "FEED_EXPORT_FIELDS": ["f1", "f2", "f3"],
            "FEED_EXPORT_INDENT": 42,
            "FEED_STORE_EMPTY": True,
            "FEED_EXPORT_BATCH_ITEM_COUNT": 2,
        })
        new_feed = feed_complete_default_values_from_settings(feed, settings)
        self.assertEqual(new_feed, {
            "encoding": "other encoding",
            "fields": None,
            "indent": 42,
            "store_empty": True,
            "uri_params": None,
            "batch_item_count": 2,
            "item_export_kwargs": {},
        })


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 12:</b> &nbsp; 8 fragments, nominal size 17 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag582')" href="javascript:;">
scrapy-2.6.1/tests/test_spider.py: 203-222
</a>
<div class="mid" id="frag582" style="display:none"><pre>
    def test_rule_without_link_extractor(self):

        response = HtmlResponse("http://example.org/somepage/index.html", body=self.test_body)

        class _CrawlSpider(self.spider_class):
            name = "test"
            allowed_domains = ['example.org']
            rules = (
                Rule(),
            )

        spider = _CrawlSpider()
        output = list(spider._requests_to_follow(response))
        self.assertEqual(len(output), 3)
        self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))
        self.assertEqual([r.url for r in output],
                         ['http://example.org/somepage/item/12.html',
                          'http://example.org/about.html',
                          'http://example.org/nofollow.html'])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag583')" href="javascript:;">
scrapy-2.6.1/tests/test_spider.py: 223-245
</a>
<div class="mid" id="frag583" style="display:none"><pre>
    def test_process_links(self):

        response = HtmlResponse("http://example.org/somepage/index.html", body=self.test_body)

        class _CrawlSpider(self.spider_class):
            name = "test"
            allowed_domains = ['example.org']
            rules = (
                Rule(LinkExtractor(), process_links="dummy_process_links"),
            )

            def dummy_process_links(self, links):
                return links

        spider = _CrawlSpider()
        output = list(spider._requests_to_follow(response))
        self.assertEqual(len(output), 3)
        self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))
        self.assertEqual([r.url for r in output],
                         ['http://example.org/somepage/item/12.html',
                          'http://example.org/about.html',
                          'http://example.org/nofollow.html'])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag585')" href="javascript:;">
scrapy-2.6.1/tests/test_spider.py: 246-271
</a>
<div class="mid" id="frag585" style="display:none"><pre>
    def test_process_links_filter(self):

        response = HtmlResponse("http://example.org/somepage/index.html", body=self.test_body)

        class _CrawlSpider(self.spider_class):
            import re

            name = "test"
            allowed_domains = ['example.org']
            rules = (
                Rule(LinkExtractor(), process_links="filter_process_links"),
            )
            _test_regex = re.compile('nofollow')

            def filter_process_links(self, links):
                return [link for link in links
                        if not self._test_regex.search(link.url)]

        spider = _CrawlSpider()
        output = list(spider._requests_to_follow(response))
        self.assertEqual(len(output), 2)
        self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))
        self.assertEqual([r.url for r in output],
                         ['http://example.org/somepage/item/12.html',
                          'http://example.org/about.html'])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag589')" href="javascript:;">
scrapy-2.6.1/tests/test_spider.py: 296-318
</a>
<div class="mid" id="frag589" style="display:none"><pre>
    def test_process_request(self):

        response = HtmlResponse("http://example.org/somepage/index.html", body=self.test_body)

        def process_request_change_domain(request, response):
            return request.replace(url=request.url.replace('.org', '.com'))

        class _CrawlSpider(self.spider_class):
            name = "test"
            allowed_domains = ['example.org']
            rules = (
                Rule(LinkExtractor(), process_request=process_request_change_domain),
            )

        spider = _CrawlSpider()
        output = list(spider._requests_to_follow(response))
        self.assertEqual(len(output), 3)
        self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))
        self.assertEqual([r.url for r in output],
                         ['http://example.com/somepage/item/12.html',
                          'http://example.com/about.html',
                          'http://example.com/nofollow.html'])

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag587')" href="javascript:;">
scrapy-2.6.1/tests/test_spider.py: 272-295
</a>
<div class="mid" id="frag587" style="display:none"><pre>
    def test_process_links_generator(self):

        response = HtmlResponse("http://example.org/somepage/index.html", body=self.test_body)

        class _CrawlSpider(self.spider_class):
            name = "test"
            allowed_domains = ['example.org']
            rules = (
                Rule(LinkExtractor(), process_links="dummy_process_links"),
            )

            def dummy_process_links(self, links):
                for link in links:
                    yield link

        spider = _CrawlSpider()
        output = list(spider._requests_to_follow(response))
        self.assertEqual(len(output), 3)
        self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))
        self.assertEqual([r.url for r in output],
                         ['http://example.org/somepage/item/12.html',
                          'http://example.org/about.html',
                          'http://example.org/nofollow.html'])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag593')" href="javascript:;">
scrapy-2.6.1/tests/test_spider.py: 345-367
</a>
<div class="mid" id="frag593" style="display:none"><pre>
    def test_process_request_instance_method(self):

        response = HtmlResponse("http://example.org/somepage/index.html", body=self.test_body)

        class _CrawlSpider(self.spider_class):
            name = "test"
            allowed_domains = ['example.org']
            rules = (
                Rule(LinkExtractor(), process_request='process_request_upper'),
            )

            def process_request_upper(self, request, response):
                return request.replace(url=request.url.upper())

        spider = _CrawlSpider()
        output = list(spider._requests_to_follow(response))
        self.assertEqual(len(output), 3)
        self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))
        self.assertEqual([r.url for r in output],
                         ['http://EXAMPLE.ORG/SOMEPAGE/ITEM/12.HTML',
                          'http://EXAMPLE.ORG/ABOUT.HTML',
                          'http://EXAMPLE.ORG/NOFOLLOW.HTML'])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag591')" href="javascript:;">
scrapy-2.6.1/tests/test_spider.py: 319-344
</a>
<div class="mid" id="frag591" style="display:none"><pre>
    def test_process_request_with_response(self):

        response = HtmlResponse("http://example.org/somepage/index.html", body=self.test_body)

        def process_request_meta_response_class(request, response):
            request.meta['response_class'] = response.__class__.__name__
            return request

        class _CrawlSpider(self.spider_class):
            name = "test"
            allowed_domains = ['example.org']
            rules = (
                Rule(LinkExtractor(), process_request=process_request_meta_response_class),
            )

        spider = _CrawlSpider()
        output = list(spider._requests_to_follow(response))
        self.assertEqual(len(output), 3)
        self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))
        self.assertEqual([r.url for r in output],
                         ['http://example.org/somepage/item/12.html',
                          'http://example.org/about.html',
                          'http://example.org/nofollow.html'])
        self.assertEqual([r.meta['response_class'] for r in output],
                         ['HtmlResponse', 'HtmlResponse', 'HtmlResponse'])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag595')" href="javascript:;">
scrapy-2.6.1/tests/test_spider.py: 368-393
</a>
<div class="mid" id="frag595" style="display:none"><pre>
    def test_process_request_instance_method_with_response(self):

        response = HtmlResponse("http://example.org/somepage/index.html", body=self.test_body)

        class _CrawlSpider(self.spider_class):
            name = "test"
            allowed_domains = ['example.org']
            rules = (
                Rule(LinkExtractor(), process_request='process_request_meta_response_class'),
            )

            def process_request_meta_response_class(self, request, response):
                request.meta['response_class'] = response.__class__.__name__
                return request

        spider = _CrawlSpider()
        output = list(spider._requests_to_follow(response))
        self.assertEqual(len(output), 3)
        self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))
        self.assertEqual([r.url for r in output],
                         ['http://example.org/somepage/item/12.html',
                          'http://example.org/about.html',
                          'http://example.org/nofollow.html'])
        self.assertEqual([r.meta['response_class'] for r in output],
                         ['HtmlResponse', 'HtmlResponse', 'HtmlResponse'])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 13:</b> &nbsp; 3 fragments, nominal size 17 lines, similarity 77%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag606')" href="javascript:;">
scrapy-2.6.1/tests/test_spider.py: 500-531
</a>
<div class="mid" id="frag606" style="display:none"><pre>
    def test_sitemap_filter(self):
        sitemap = b"""&lt;?xml version="1.0" encoding="UTF-8"?&gt;
    &lt;urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9"
        xmlns:xhtml="http://www.w3.org/1999/xhtml"&gt;
        &lt;url&gt;
            &lt;loc&gt;http://www.example.com/english/&lt;/loc&gt;
            &lt;lastmod&gt;2010-01-01&lt;/lastmod&gt;
        &lt;/url&gt;
        &lt;url&gt;
            &lt;loc&gt;http://www.example.com/portuguese/&lt;/loc&gt;
            &lt;lastmod&gt;2005-01-01&lt;/lastmod&gt;
        &lt;/url&gt;
    &lt;/urlset&gt;"""

        class FilteredSitemapSpider(self.spider_class):
            def sitemap_filter(self, entries):
                from datetime import datetime
                for entry in entries:
                    date_time = datetime.strptime(entry['lastmod'], '%Y-%m-%d')
                    if date_time.year &gt; 2008:
                        yield entry

        r = TextResponse(url="http://www.example.com/sitemap.xml", body=sitemap)
        spider = self.spider_class("example.com")
        self.assertEqual([req.url for req in spider._parse_sitemap(r)],
                         ['http://www.example.com/english/',
                          'http://www.example.com/portuguese/'])

        spider = FilteredSitemapSpider("example.com")
        self.assertEqual([req.url for req in spider._parse_sitemap(r)],
                         ['http://www.example.com/english/'])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag610')" href="javascript:;">
scrapy-2.6.1/tests/test_spider.py: 567-598
</a>
<div class="mid" id="frag610" style="display:none"><pre>
    def test_sitemapindex_filter(self):
        sitemap = b"""&lt;?xml version="1.0" encoding="UTF-8"?&gt;
    &lt;sitemapindex xmlns="http://www.sitemaps.org/schemas/sitemap/0.9"&gt;
        &lt;sitemap&gt;
            &lt;loc&gt;http://www.example.com/sitemap1.xml&lt;/loc&gt;
            &lt;lastmod&gt;2004-01-01T20:00:00+00:00&lt;/lastmod&gt;
        &lt;/sitemap&gt;
        &lt;sitemap&gt;
            &lt;loc&gt;http://www.example.com/sitemap2.xml&lt;/loc&gt;
            &lt;lastmod&gt;2005-01-01&lt;/lastmod&gt;
        &lt;/sitemap&gt;
    &lt;/sitemapindex&gt;"""

        class FilteredSitemapSpider(self.spider_class):
            def sitemap_filter(self, entries):
                from datetime import datetime
                for entry in entries:
                    date_time = datetime.strptime(entry['lastmod'].split('T')[0], '%Y-%m-%d')
                    if date_time.year &gt; 2004:
                        yield entry

        r = TextResponse(url="http://www.example.com/sitemap.xml", body=sitemap)
        spider = self.spider_class("example.com")
        self.assertEqual([req.url for req in spider._parse_sitemap(r)],
                         ['http://www.example.com/sitemap1.xml',
                          'http://www.example.com/sitemap2.xml'])

        spider = FilteredSitemapSpider("example.com")
        self.assertEqual([req.url for req in spider._parse_sitemap(r)],
                         ['http://www.example.com/sitemap2.xml'])


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag608')" href="javascript:;">
scrapy-2.6.1/tests/test_spider.py: 532-566
</a>
<div class="mid" id="frag608" style="display:none"><pre>
    def test_sitemap_filter_with_alternate_links(self):
        sitemap = b"""&lt;?xml version="1.0" encoding="UTF-8"?&gt;
    &lt;urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9"
        xmlns:xhtml="http://www.w3.org/1999/xhtml"&gt;
        &lt;url&gt;
            &lt;loc&gt;http://www.example.com/english/article_1/&lt;/loc&gt;
            &lt;lastmod&gt;2010-01-01&lt;/lastmod&gt;
            &lt;xhtml:link rel="alternate" hreflang="de"
                href="http://www.example.com/deutsch/article_1/"/&gt;
        &lt;/url&gt;
        &lt;url&gt;
            &lt;loc&gt;http://www.example.com/english/article_2/&lt;/loc&gt;
            &lt;lastmod&gt;2015-01-01&lt;/lastmod&gt;
        &lt;/url&gt;
    &lt;/urlset&gt;"""

        class FilteredSitemapSpider(self.spider_class):
            def sitemap_filter(self, entries):
                for entry in entries:
                    alternate_links = entry.get('alternate', tuple())
                    for link in alternate_links:
                        if '/deutsch/' in link:
                            entry['loc'] = link
                            yield entry

        r = TextResponse(url="http://www.example.com/sitemap.xml", body=sitemap)
        spider = self.spider_class("example.com")
        self.assertEqual([req.url for req in spider._parse_sitemap(r)],
                         ['http://www.example.com/english/article_1/',
                          'http://www.example.com/english/article_2/'])

        spider = FilteredSitemapSpider("example.com")
        self.assertEqual([req.url for req in spider._parse_sitemap(r)],
                         ['http://www.example.com/deutsch/article_1/'])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 14:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag651')" href="javascript:;">
scrapy-2.6.1/tests/test_exporters.py: 150-162
</a>
<div class="mid" id="frag651" style="display:none"><pre>
    def test_export_list(self):
        i1 = self.item_class(name='Joseph', age='22')
        i2 = self.item_class(name='Maria', age=[i1])
        i3 = self.item_class(name='Jesus', age=[i2])
        ie = self._get_exporter()
        exported = ie.export_item(i3)
        self.assertEqual(
            exported,
            {'age': [{'age': [{'age': '22', 'name': 'Joseph'}], 'name': 'Maria'}], 'name': 'Jesus'}
        )
        self.assertEqual(type(exported['age'][0]), dict)
        self.assertEqual(type(exported['age'][0]['age'][0]), dict)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag652')" href="javascript:;">
scrapy-2.6.1/tests/test_exporters.py: 163-175
</a>
<div class="mid" id="frag652" style="display:none"><pre>
    def test_export_item_dict_list(self):
        i1 = self.item_class(name='Joseph', age='22')
        i2 = dict(name='Maria', age=[i1])
        i3 = self.item_class(name='Jesus', age=[i2])
        ie = self._get_exporter()
        exported = ie.export_item(i3)
        self.assertEqual(
            exported,
            {'age': [{'age': [{'age': '22', 'name': 'Joseph'}], 'name': 'Maria'}], 'name': 'Jesus'}
        )
        self.assertEqual(type(exported['age'][0]), dict)
        self.assertEqual(type(exported['age'][0]['age'][0]), dict)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 15:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag699')" href="javascript:;">
scrapy-2.6.1/tests/test_exporters.py: 565-575
</a>
<div class="mid" id="frag699" style="display:none"><pre>
    def test_nested_item(self):
        i1 = self.item_class(name='Joseph\xa3', age='22')
        i2 = self.item_class(name='Maria', age=i1)
        i3 = self.item_class(name='Jesus', age=i2)
        self.ie.start_exporting()
        self.ie.export_item(i3)
        self.ie.finish_exporting()
        exported = json.loads(to_unicode(self.output.getvalue()))
        expected = {'name': 'Jesus', 'age': {'name': 'Maria', 'age': ItemAdapter(i1).asdict()}}
        self.assertEqual(exported, [expected])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag700')" href="javascript:;">
scrapy-2.6.1/tests/test_exporters.py: 576-586
</a>
<div class="mid" id="frag700" style="display:none"><pre>
    def test_nested_dict_item(self):
        i1 = dict(name='Joseph\xa3', age='22')
        i2 = self.item_class(name='Maria', age=i1)
        i3 = dict(name='Jesus', age=i2)
        self.ie.start_exporting()
        self.ie.export_item(i3)
        self.ie.finish_exporting()
        exported = json.loads(to_unicode(self.output.getvalue()))
        expected = {'name': 'Jesus', 'age': {'name': 'Maria', 'age': i1}}
        self.assertEqual(exported, [expected])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 16:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag711')" href="javascript:;">
scrapy-2.6.1/tests/test_spidermiddleware.py: 69-85
</a>
<div class="mid" id="frag711" style="display:none"><pre>
    def test_invalid_process_spider_exception(self):

        class InvalidProcessSpiderOutputExceptionMiddleware:
            def process_spider_exception(self, response, exception, spider):
                return 1

        class RaiseExceptionProcessSpiderOutputMiddleware:
            def process_spider_output(self, response, result, spider):
                raise Exception()

        self.mwman._add_middleware(InvalidProcessSpiderOutputExceptionMiddleware())
        self.mwman._add_middleware(RaiseExceptionProcessSpiderOutputMiddleware())
        result = self._scrape_response()
        self.assertIsInstance(result, Failure)
        self.assertIsInstance(result.value, _InvalidOutput)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag714')" href="javascript:;">
scrapy-2.6.1/tests/test_spidermiddleware.py: 89-103
</a>
<div class="mid" id="frag714" style="display:none"><pre>
    def test_process_spider_exception_return_none(self):

        class ProcessSpiderExceptionReturnNoneMiddleware:
            def process_spider_exception(self, response, exception, spider):
                return None

        class RaiseExceptionProcessSpiderOutputMiddleware:
            def process_spider_output(self, response, result, spider):
                1 / 0

        self.mwman._add_middleware(ProcessSpiderExceptionReturnNoneMiddleware())
        self.mwman._add_middleware(RaiseExceptionProcessSpiderOutputMiddleware())
        result = self._scrape_response()
        self.assertIsInstance(result, Failure)
        self.assertIsInstance(result.value, ZeroDivisionError)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 17:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag818')" href="javascript:;">
scrapy-2.6.1/tests/test_utils_iterators.py: 383-395
</a>
<div class="mid" id="frag818" style="display:none"><pre>
    def test_csviter_headers(self):
        sample = get_testdata('feeds', 'feed-sample3.csv').splitlines()
        headers, body = sample[0].split(b','), b'\n'.join(sample[1:])

        response = TextResponse(url="http://example.com/", body=body)
        csv = csviter(response, headers=[h.decode('utf-8') for h in headers])

        self.assertEqual([row for row in csv],
                         [{'id': '1', 'name': 'alpha', 'value': 'foobar'},
                          {'id': '2', 'name': 'unicode', 'value': '\xfan\xedc\xf3d\xe9\u203d'},
                          {'id': '3', 'name': 'multi', 'value': 'foo\nbar'},
                          {'id': '4', 'name': 'empty', 'value': ''}])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag819')" href="javascript:;">
scrapy-2.6.1/tests/test_utils_iterators.py: 396-408
</a>
<div class="mid" id="frag819" style="display:none"><pre>
    def test_csviter_falserow(self):
        body = get_testdata('feeds', 'feed-sample3.csv')
        body = b'\n'.join((body, b'a,b', b'a,b,c,d'))

        response = TextResponse(url="http://example.com/", body=body)
        csv = csviter(response)

        self.assertEqual([row for row in csv],
                         [{'id': '1', 'name': 'alpha', 'value': 'foobar'},
                          {'id': '2', 'name': 'unicode', 'value': '\xfan\xedc\xf3d\xe9\u203d'},
                          {'id': '3', 'name': 'multi', 'value': "foo\nbar"},
                          {'id': '4', 'name': 'empty', 'value': ''}])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 18:</b> &nbsp; 3 fragments, nominal size 11 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag868')" href="javascript:;">
scrapy-2.6.1/tests/test_downloadermiddleware_robotstxt.py: 30-51
</a>
<div class="mid" id="frag868" style="display:none"><pre>
    def _get_successful_crawler(self):
        crawler = self.crawler
        crawler.settings.set('ROBOTSTXT_OBEY', True)
        ROBOTS = """
User-Agent: *
Disallow: /admin/
Disallow: /static/
# taken from https://en.wikipedia.org/robots.txt
Disallow: /wiki/K%C3%A4ytt%C3%A4j%C3%A4:
Disallow: /wiki/Kyttj:
User-Agent: UnicdeBt
Disallow: /some/randome/page.html
""".encode('utf-8')
        response = TextResponse('http://site.local/robots.txt', body=ROBOTS)

        def return_response(request):
            deferred = Deferred()
            reactor.callFromThread(deferred.callback, response)
            return deferred
        crawler.engine.download.side_effect = return_response
        return crawler

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag876')" href="javascript:;">
scrapy-2.6.1/tests/test_downloadermiddleware_robotstxt.py: 100-111
</a>
<div class="mid" id="frag876" style="display:none"><pre>
    def _get_emptybody_crawler(self):
        crawler = self.crawler
        crawler.settings.set('ROBOTSTXT_OBEY', True)
        response = Response('http://site.local/robots.txt')

        def return_response(request):
            deferred = Deferred()
            reactor.callFromThread(deferred.callback, response)
            return deferred
        crawler.engine.download.side_effect = return_response
        return crawler

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag873')" href="javascript:;">
scrapy-2.6.1/tests/test_downloadermiddleware_robotstxt.py: 77-88
</a>
<div class="mid" id="frag873" style="display:none"><pre>
    def _get_garbage_crawler(self):
        crawler = self.crawler
        crawler.settings.set('ROBOTSTXT_OBEY', True)
        response = Response('http://site.local/robots.txt', body=b'GIF89a\xd3\x00\xfe\x00\xa2')

        def return_response(request):
            deferred = Deferred()
            reactor.callFromThread(deferred.callback, response)
            return deferred
        crawler.engine.download.side_effect = return_response
        return crawler

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 19:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag879')" href="javascript:;">
scrapy-2.6.1/tests/test_downloadermiddleware_robotstxt.py: 121-136
</a>
<div class="mid" id="frag879" style="display:none"><pre>
    def test_robotstxt_error(self):
        self.crawler.settings.set('ROBOTSTXT_OBEY', True)
        err = error.DNSLookupError('Robotstxt address not found')

        def return_failure(request):
            deferred = Deferred()
            reactor.callFromThread(deferred.errback, failure.Failure(err))
            return deferred
        self.crawler.engine.download.side_effect = return_failure

        middleware = RobotsTxtMiddleware(self.crawler)
        middleware._logerror = mock.MagicMock(side_effect=middleware._logerror)
        deferred = middleware.process_request(Request('http://site.local'), None)
        deferred.addCallback(lambda _: self.assertTrue(middleware._logerror.called))
        return deferred

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag883')" href="javascript:;">
scrapy-2.6.1/tests/test_downloadermiddleware_robotstxt.py: 150-165
</a>
<div class="mid" id="frag883" style="display:none"><pre>
    def test_ignore_robotstxt_request(self):
        self.crawler.settings.set('ROBOTSTXT_OBEY', True)

        def ignore_request(request):
            deferred = Deferred()
            reactor.callFromThread(deferred.errback, failure.Failure(IgnoreRequest()))
            return deferred
        self.crawler.engine.download.side_effect = ignore_request

        middleware = RobotsTxtMiddleware(self.crawler)
        mw_module_logger.error = mock.MagicMock()

        d = self.assertNotIgnored(Request('http://site.local/allowed'), middleware)
        d.addCallback(lambda _: self.assertFalse(mw_module_logger.error.called))
        return d

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 20:</b> &nbsp; 2 fragments, nominal size 23 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag900')" href="javascript:;">
scrapy-2.6.1/tests/test_dupefilters.py: 162-190
</a>
<div class="mid" id="frag900" style="display:none"><pre>
    def test_log(self):
        with LogCapture() as log:
            settings = {'DUPEFILTER_DEBUG': False,
                        'DUPEFILTER_CLASS': FromCrawlerRFPDupeFilter}
            crawler = get_crawler(SimpleSpider, settings_dict=settings)
            scheduler = Scheduler.from_crawler(crawler)
            spider = SimpleSpider.from_crawler(crawler)

            dupefilter = scheduler.df
            dupefilter.open()

            r1 = Request('http://scrapytest.org/index.html')
            r2 = Request('http://scrapytest.org/index.html')

            dupefilter.log(r1, spider)
            dupefilter.log(r2, spider)

            assert crawler.stats.get_value('dupefilter/filtered') == 2
            log.check_present(
                (
                    'scrapy.dupefilters',
                    'DEBUG',
                    'Filtered duplicate request: &lt;GET http://scrapytest.org/index.html&gt; - no more'
                    ' duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)'
                )
            )

            dupefilter.close('finished')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag901')" href="javascript:;">
scrapy-2.6.1/tests/test_dupefilters.py: 191-226
</a>
<div class="mid" id="frag901" style="display:none"><pre>
    def test_log_debug(self):
        with LogCapture() as log:
            settings = {'DUPEFILTER_DEBUG': True,
                        'DUPEFILTER_CLASS': FromCrawlerRFPDupeFilter}
            crawler = get_crawler(SimpleSpider, settings_dict=settings)
            scheduler = Scheduler.from_crawler(crawler)
            spider = SimpleSpider.from_crawler(crawler)

            dupefilter = scheduler.df
            dupefilter.open()

            r1 = Request('http://scrapytest.org/index.html')
            r2 = Request('http://scrapytest.org/index.html',
                         headers={'Referer': 'http://scrapytest.org/INDEX.html'})

            dupefilter.log(r1, spider)
            dupefilter.log(r2, spider)

            assert crawler.stats.get_value('dupefilter/filtered') == 2
            log.check_present(
                (
                    'scrapy.dupefilters',
                    'DEBUG',
                    'Filtered duplicate request: &lt;GET http://scrapytest.org/index.html&gt; (referer: None)'
                )
            )
            log.check_present(
                (
                    'scrapy.dupefilters',
                    'DEBUG',
                    'Filtered duplicate request: &lt;GET http://scrapytest.org/index.html&gt;'
                    ' (referer: http://scrapytest.org/INDEX.html)'
                )
            )

            dupefilter.close('finished')
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 21:</b> &nbsp; 5 fragments, nominal size 10 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag964')" href="javascript:;">
scrapy-2.6.1/tests/test_downloadermiddleware_httpcompression.py: 99-110
</a>
<div class="mid" id="frag964" style="display:none"><pre>
    def test_process_response_gzip(self):
        response = self._getresponse('gzip')
        request = response.request

        self.assertEqual(response.headers['Content-Encoding'], b'gzip')
        newresponse = self.mw.process_response(request, response, self.spider)
        assert newresponse is not response
        assert newresponse.body.startswith(b'&lt;!DOCTYPE')
        assert 'Content-Encoding' not in newresponse.headers
        self.assertStatsEqual('httpcompression/response_count', 1)
        self.assertStatsEqual('httpcompression/response_bytes', 74837)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag966')" href="javascript:;">
scrapy-2.6.1/tests/test_downloadermiddleware_httpcompression.py: 123-137
</a>
<div class="mid" id="frag966" style="display:none"><pre>
    def test_process_response_br(self):
        try:
            import brotli  # noqa: F401
        except ImportError:
            raise SkipTest("no brotli")
        response = self._getresponse('br')
        request = response.request
        self.assertEqual(response.headers['Content-Encoding'], b'br')
        newresponse = self.mw.process_response(request, response, self.spider)
        assert newresponse is not response
        assert newresponse.body.startswith(b"&lt;!DOCTYPE")
        assert 'Content-Encoding' not in newresponse.headers
        self.assertStatsEqual('httpcompression/response_count', 1)
        self.assertStatsEqual('httpcompression/response_bytes', 74837)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag968')" href="javascript:;">
scrapy-2.6.1/tests/test_downloadermiddleware_httpcompression.py: 159-170
</a>
<div class="mid" id="frag968" style="display:none"><pre>
    def test_process_response_rawdeflate(self):
        response = self._getresponse('rawdeflate')
        request = response.request

        self.assertEqual(response.headers['Content-Encoding'], b'deflate')
        newresponse = self.mw.process_response(request, response, self.spider)
        assert newresponse is not response
        assert newresponse.body.startswith(b'&lt;!DOCTYPE')
        assert 'Content-Encoding' not in newresponse.headers
        self.assertStatsEqual('httpcompression/response_count', 1)
        self.assertStatsEqual('httpcompression/response_bytes', 74840)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag965')" href="javascript:;">
scrapy-2.6.1/tests/test_downloadermiddleware_httpcompression.py: 111-122
</a>
<div class="mid" id="frag965" style="display:none"><pre>
    def test_process_response_gzip_no_stats(self):
        mw = HttpCompressionMiddleware()
        response = self._getresponse('gzip')
        request = response.request

        self.assertEqual(response.headers['Content-Encoding'], b'gzip')
        newresponse = mw.process_response(request, response, self.spider)
        self.assertEqual(mw.stats, None)
        assert newresponse is not response
        assert newresponse.body.startswith(b'&lt;!DOCTYPE')
        assert 'Content-Encoding' not in newresponse.headers

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag969')" href="javascript:;">
scrapy-2.6.1/tests/test_downloadermiddleware_httpcompression.py: 171-182
</a>
<div class="mid" id="frag969" style="display:none"><pre>
    def test_process_response_zlibdelate(self):
        response = self._getresponse('zlibdeflate')
        request = response.request

        self.assertEqual(response.headers['Content-Encoding'], b'deflate')
        newresponse = self.mw.process_response(request, response, self.spider)
        assert newresponse is not response
        assert newresponse.body.startswith(b'&lt;!DOCTYPE')
        assert 'Content-Encoding' not in newresponse.headers
        self.assertStatsEqual('httpcompression/response_count', 1)
        self.assertStatsEqual('httpcompression/response_bytes', 74840)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 22:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag972')" href="javascript:;">
scrapy-2.6.1/tests/test_downloadermiddleware_httpcompression.py: 202-222
</a>
<div class="mid" id="frag972" style="display:none"><pre>
    def test_process_response_encoding_inside_body(self):
        headers = {
            'Content-Type': 'text/html',
            'Content-Encoding': 'gzip',
        }
        f = BytesIO()
        plainbody = (b'&lt;html&gt;&lt;head&gt;&lt;title&gt;Some page&lt;/title&gt;'
                     b'&lt;meta http-equiv="Content-Type" content="text/html; charset=gb2312"&gt;')
        zf = GzipFile(fileobj=f, mode='wb')
        zf.write(plainbody)
        zf.close()
        response = Response("http;//www.example.com/", headers=headers, body=f.getvalue())
        request = Request("http://www.example.com/")

        newresponse = self.mw.process_response(request, response, self.spider)
        assert isinstance(newresponse, HtmlResponse)
        self.assertEqual(newresponse.body, plainbody)
        self.assertEqual(newresponse.encoding, resolve_encoding('gb2312'))
        self.assertStatsEqual('httpcompression/response_count', 1)
        self.assertStatsEqual('httpcompression/response_bytes', 104)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag973')" href="javascript:;">
scrapy-2.6.1/tests/test_downloadermiddleware_httpcompression.py: 223-243
</a>
<div class="mid" id="frag973" style="display:none"><pre>
    def test_process_response_force_recalculate_encoding(self):
        headers = {
            'Content-Type': 'text/html',
            'Content-Encoding': 'gzip',
        }
        f = BytesIO()
        plainbody = (b'&lt;html&gt;&lt;head&gt;&lt;title&gt;Some page&lt;/title&gt;'
                     b'&lt;meta http-equiv="Content-Type" content="text/html; charset=gb2312"&gt;')
        zf = GzipFile(fileobj=f, mode='wb')
        zf.write(plainbody)
        zf.close()
        response = HtmlResponse("http;//www.example.com/page.html", headers=headers, body=f.getvalue())
        request = Request("http://www.example.com/")

        newresponse = self.mw.process_response(request, response, self.spider)
        assert isinstance(newresponse, HtmlResponse)
        self.assertEqual(newresponse.body, plainbody)
        self.assertEqual(newresponse.encoding, resolve_encoding('gb2312'))
        self.assertStatsEqual('httpcompression/response_count', 1)
        self.assertStatsEqual('httpcompression/response_bytes', 104)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 23:</b> &nbsp; 3 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag975')" href="javascript:;">
scrapy-2.6.1/tests/test_downloadermiddleware_httpcompression.py: 261-272
</a>
<div class="mid" id="frag975" style="display:none"><pre>
    def test_process_response_gzipped_contenttype(self):
        response = self._getresponse('gzip')
        response.headers['Content-Type'] = 'application/gzip'
        request = response.request

        newresponse = self.mw.process_response(request, response, self.spider)
        self.assertIsNot(newresponse, response)
        self.assertTrue(newresponse.body.startswith(b'&lt;!DOCTYPE'))
        self.assertNotIn('Content-Encoding', newresponse.headers)
        self.assertStatsEqual('httpcompression/response_count', 1)
        self.assertStatsEqual('httpcompression/response_bytes', 74837)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag976')" href="javascript:;">
scrapy-2.6.1/tests/test_downloadermiddleware_httpcompression.py: 273-284
</a>
<div class="mid" id="frag976" style="display:none"><pre>
    def test_process_response_gzip_app_octetstream_contenttype(self):
        response = self._getresponse('gzip')
        response.headers['Content-Type'] = 'application/octet-stream'
        request = response.request

        newresponse = self.mw.process_response(request, response, self.spider)
        self.assertIsNot(newresponse, response)
        self.assertTrue(newresponse.body.startswith(b'&lt;!DOCTYPE'))
        self.assertNotIn('Content-Encoding', newresponse.headers)
        self.assertStatsEqual('httpcompression/response_count', 1)
        self.assertStatsEqual('httpcompression/response_bytes', 74837)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag977')" href="javascript:;">
scrapy-2.6.1/tests/test_downloadermiddleware_httpcompression.py: 285-296
</a>
<div class="mid" id="frag977" style="display:none"><pre>
    def test_process_response_gzip_binary_octetstream_contenttype(self):
        response = self._getresponse('x-gzip')
        response.headers['Content-Type'] = 'binary/octet-stream'
        request = response.request

        newresponse = self.mw.process_response(request, response, self.spider)
        self.assertIsNot(newresponse, response)
        self.assertTrue(newresponse.body.startswith(b'&lt;!DOCTYPE'))
        self.assertNotIn('Content-Encoding', newresponse.headers)
        self.assertStatsEqual('httpcompression/response_count', 1)
        self.assertStatsEqual('httpcompression/response_bytes', 74837)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 24:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 84%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1040')" href="javascript:;">
scrapy-2.6.1/tests/test_utils_deprecate.py: 71-86
</a>
<div class="mid" id="frag1040" style="display:none"><pre>
    def test_subclassing_warns_only_on_direct_childs(self):
        Deprecated = create_deprecated_class('Deprecated', NewName,
                                             warn_once=False,
                                             warn_category=MyWarning)

        with warnings.catch_warnings(record=True) as w:
            class UserClass(Deprecated):
                pass

            class NoWarnOnMe(UserClass):
                pass

        w = self._mywarnings(w)
        self.assertEqual(len(w), 1)
        self.assertIn('UserClass', str(w[0].message))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1041')" href="javascript:;">
scrapy-2.6.1/tests/test_utils_deprecate.py: 87-104
</a>
<div class="mid" id="frag1041" style="display:none"><pre>
    def test_subclassing_warns_once_by_default(self):
        Deprecated = create_deprecated_class('Deprecated', NewName,
                                             warn_category=MyWarning)

        with warnings.catch_warnings(record=True) as w:
            class UserClass(Deprecated):
                pass

            class FooClass(Deprecated):
                pass

            class BarClass(Deprecated):
                pass

        w = self._mywarnings(w)
        self.assertEqual(len(w), 1)
        self.assertIn('UserClass', str(w[0].message))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 25:</b> &nbsp; 4 fragments, nominal size 12 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1065')" href="javascript:;">
scrapy-2.6.1/tests/test_downloadermiddleware_retry.py: 133-145
</a>
<div class="mid" id="frag1065" style="display:none"><pre>
    def test_with_settings_zero(self):
        max_retry_times = 0
        settings = {'RETRY_TIMES': max_retry_times}
        spider, middleware = self.get_spider_and_middleware(settings)
        req = Request(self.invalid_url)
        self._test_retry(
            req,
            DNSLookupError('foo'),
            max_retry_times,
            spider=spider,
            middleware=middleware,
        )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1066')" href="javascript:;">
scrapy-2.6.1/tests/test_downloadermiddleware_retry.py: 146-158
</a>
<div class="mid" id="frag1066" style="display:none"><pre>
    def test_with_metakey_zero(self):
        max_retry_times = 0
        spider, middleware = self.get_spider_and_middleware()
        meta = {'max_retry_times': max_retry_times}
        req = Request(self.invalid_url, meta=meta)
        self._test_retry(
            req,
            DNSLookupError('foo'),
            max_retry_times,
            spider=spider,
            middleware=middleware,
        )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1067')" href="javascript:;">
scrapy-2.6.1/tests/test_downloadermiddleware_retry.py: 159-171
</a>
<div class="mid" id="frag1067" style="display:none"><pre>
    def test_without_metakey(self):
        max_retry_times = 5
        settings = {'RETRY_TIMES': max_retry_times}
        spider, middleware = self.get_spider_and_middleware(settings)
        req = Request(self.invalid_url)
        self._test_retry(
            req,
            DNSLookupError('foo'),
            max_retry_times,
            spider=spider,
            middleware=middleware,
        )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1070')" href="javascript:;">
scrapy-2.6.1/tests/test_downloadermiddleware_retry.py: 222-237
</a>
<div class="mid" id="frag1070" style="display:none"><pre>
    def test_with_dont_retry(self):
        max_retry_times = 4
        spider, middleware = self.get_spider_and_middleware()
        meta = {
            'max_retry_times': max_retry_times,
            'dont_retry': True,
        }
        req = Request(self.invalid_url, meta=meta)
        self._test_retry(
            req,
            DNSLookupError('foo'),
            0,
            spider=spider,
            middleware=middleware,
        )

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 26:</b> &nbsp; 2 fragments, nominal size 21 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1068')" href="javascript:;">
scrapy-2.6.1/tests/test_downloadermiddleware_retry.py: 172-196
</a>
<div class="mid" id="frag1068" style="display:none"><pre>
    def test_with_metakey_greater(self):
        meta_max_retry_times = 3
        middleware_max_retry_times = 2

        req1 = Request(self.invalid_url, meta={'max_retry_times': meta_max_retry_times})
        req2 = Request(self.invalid_url)

        settings = {'RETRY_TIMES': middleware_max_retry_times}
        spider, middleware = self.get_spider_and_middleware(settings)

        self._test_retry(
            req1,
            DNSLookupError('foo'),
            meta_max_retry_times,
            spider=spider,
            middleware=middleware,
        )
        self._test_retry(
            req2,
            DNSLookupError('foo'),
            middleware_max_retry_times,
            spider=spider,
            middleware=middleware,
        )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1069')" href="javascript:;">
scrapy-2.6.1/tests/test_downloadermiddleware_retry.py: 197-221
</a>
<div class="mid" id="frag1069" style="display:none"><pre>
    def test_with_metakey_lesser(self):
        meta_max_retry_times = 4
        middleware_max_retry_times = 5

        req1 = Request(self.invalid_url, meta={'max_retry_times': meta_max_retry_times})
        req2 = Request(self.invalid_url)

        settings = {'RETRY_TIMES': middleware_max_retry_times}
        spider, middleware = self.get_spider_and_middleware(settings)

        self._test_retry(
            req1,
            DNSLookupError('foo'),
            meta_max_retry_times,
            spider=spider,
            middleware=middleware,
        )
        self._test_retry(
            req2,
            DNSLookupError('foo'),
            middleware_max_retry_times,
            spider=spider,
            middleware=middleware,
        )

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 27:</b> &nbsp; 2 fragments, nominal size 24 lines, similarity 95%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1073')" href="javascript:;">
scrapy-2.6.1/tests/test_downloadermiddleware_retry.py: 264-289
</a>
<div class="mid" id="frag1073" style="display:none"><pre>
    def test_basic_usage(self):
        request = Request('https://example.com')
        spider = self.get_spider()
        with LogCapture() as log:
            new_request = get_retry_request(
                request,
                spider=spider,
            )
        self.assertIsInstance(new_request, Request)
        self.assertNotEqual(new_request, request)
        self.assertEqual(new_request.dont_filter, True)
        expected_retry_times = 1
        self.assertEqual(new_request.meta['retry_times'], expected_retry_times)
        self.assertEqual(new_request.priority, -1)
        expected_reason = "unspecified"
        for stat in ('retry/count', f'retry/reason_count/{expected_reason}'):
            self.assertEqual(spider.crawler.stats.get_value(stat), 1)
        log.check_present(
            (
                "scrapy.downloadermiddlewares.retry",
                "DEBUG",
                f"Retrying {request} (failed {expected_retry_times} times): "
                f"{expected_reason}",
            )
        )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1075')" href="javascript:;">
scrapy-2.6.1/tests/test_downloadermiddleware_retry.py: 316-342
</a>
<div class="mid" id="frag1075" style="display:none"><pre>
    def test_one_retry(self):
        request = Request('https://example.com')
        spider = self.get_spider()
        with LogCapture() as log:
            new_request = get_retry_request(
                request,
                spider=spider,
                max_retry_times=1,
            )
        self.assertIsInstance(new_request, Request)
        self.assertNotEqual(new_request, request)
        self.assertEqual(new_request.dont_filter, True)
        expected_retry_times = 1
        self.assertEqual(new_request.meta['retry_times'], expected_retry_times)
        self.assertEqual(new_request.priority, -1)
        expected_reason = "unspecified"
        for stat in ('retry/count', f'retry/reason_count/{expected_reason}'):
            self.assertEqual(spider.crawler.stats.get_value(stat), 1)
        log.check_present(
            (
                "scrapy.downloadermiddlewares.retry",
                "DEBUG",
                f"Retrying {request} (failed {expected_retry_times} times): "
                f"{expected_reason}",
            )
        )

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 28:</b> &nbsp; 3 fragments, nominal size 10 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1079')" href="javascript:;">
scrapy-2.6.1/tests/test_downloadermiddleware_retry.py: 411-421
</a>
<div class="mid" id="frag1079" style="display:none"><pre>
    def test_max_retry_times_meta(self):
        max_retry_times = 0
        spider = self.get_spider({'RETRY_TIMES': max_retry_times + 1})
        meta = {'max_retry_times': max_retry_times}
        request = Request('https://example.com', meta=meta)
        new_request = get_retry_request(
            request,
            spider=spider,
        )
        self.assertEqual(new_request, None)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1082')" href="javascript:;">
scrapy-2.6.1/tests/test_downloadermiddleware_retry.py: 444-454
</a>
<div class="mid" id="frag1082" style="display:none"><pre>
    def test_priority_adjust_argument(self):
        priority_adjust = 1
        spider = self.get_spider({'RETRY_PRIORITY_ADJUST': priority_adjust + 1})
        request = Request('https://example.com')
        new_request = get_retry_request(
            request,
            spider=spider,
            priority_adjust=priority_adjust,
        )
        self.assertEqual(new_request.priority, priority_adjust)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1080')" href="javascript:;">
scrapy-2.6.1/tests/test_downloadermiddleware_retry.py: 422-433
</a>
<div class="mid" id="frag1080" style="display:none"><pre>
    def test_max_retry_times_argument(self):
        max_retry_times = 0
        spider = self.get_spider({'RETRY_TIMES': max_retry_times + 1})
        meta = {'max_retry_times': max_retry_times + 1}
        request = Request('https://example.com', meta=meta)
        new_request = get_retry_request(
            request,
            spider=spider,
            max_retry_times=max_retry_times,
        )
        self.assertEqual(new_request, None)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 29:</b> &nbsp; 6 fragments, nominal size 19 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1085')" href="javascript:;">
scrapy-2.6.1/tests/test_downloadermiddleware_retry.py: 476-497
</a>
<div class="mid" id="frag1085" style="display:none"><pre>
    def test_reason_string(self):
        request = Request('https://example.com')
        spider = self.get_spider()
        expected_reason = 'because'
        with LogCapture() as log:
            get_retry_request(
                request,
                spider=spider,
                reason=expected_reason,
            )
        expected_retry_times = 1
        for stat in ('retry/count', f'retry/reason_count/{expected_reason}'):
            self.assertEqual(spider.crawler.stats.get_value(stat), 1)
        log.check_present(
            (
                "scrapy.downloadermiddlewares.retry",
                "DEBUG",
                f"Retrying {request} (failed {expected_retry_times} times): "
                f"{expected_reason}",
            )
        )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1089')" href="javascript:;">
scrapy-2.6.1/tests/test_downloadermiddleware_retry.py: 573-597
</a>
<div class="mid" id="frag1089" style="display:none"><pre>
    def test_reason_custom_exception_class(self):
        request = Request('https://example.com')
        spider = self.get_spider()
        expected_reason = IgnoreRequest
        expected_reason_string = 'scrapy.exceptions.IgnoreRequest'
        with LogCapture() as log:
            get_retry_request(
                request,
                spider=spider,
                reason=expected_reason,
            )
        expected_retry_times = 1
        stat = spider.crawler.stats.get_value(
            f'retry/reason_count/{expected_reason_string}'
        )
        self.assertEqual(stat, 1)
        log.check_present(
            (
                "scrapy.downloadermiddlewares.retry",
                "DEBUG",
                f"Retrying {request} (failed {expected_retry_times} times): "
                f"{expected_reason}",
            )
        )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1090')" href="javascript:;">
scrapy-2.6.1/tests/test_downloadermiddleware_retry.py: 598-617
</a>
<div class="mid" id="frag1090" style="display:none"><pre>
    def test_custom_logger(self):
        logger = logging.getLogger("custom-logger")
        request = Request("https://example.com")
        spider = self.get_spider()
        expected_reason = "because"
        with LogCapture() as log:
            get_retry_request(
                request,
                spider=spider,
                reason=expected_reason,
                logger=logger,
            )
        log.check_present(
            (
                "custom-logger",
                "DEBUG",
                f"Retrying {request} (failed 1 times): {expected_reason}",
            )
        )

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1088')" href="javascript:;">
scrapy-2.6.1/tests/test_downloadermiddleware_retry.py: 548-572
</a>
<div class="mid" id="frag1088" style="display:none"><pre>
    def test_reason_custom_exception(self):
        request = Request('https://example.com')
        spider = self.get_spider()
        expected_reason = IgnoreRequest()
        expected_reason_string = 'scrapy.exceptions.IgnoreRequest'
        with LogCapture() as log:
            get_retry_request(
                request,
                spider=spider,
                reason=expected_reason,
            )
        expected_retry_times = 1
        stat = spider.crawler.stats.get_value(
            f'retry/reason_count/{expected_reason_string}'
        )
        self.assertEqual(stat, 1)
        log.check_present(
            (
                "scrapy.downloadermiddlewares.retry",
                "DEBUG",
                f"Retrying {request} (failed {expected_retry_times} times): "
                f"{expected_reason}",
            )
        )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1087')" href="javascript:;">
scrapy-2.6.1/tests/test_downloadermiddleware_retry.py: 523-547
</a>
<div class="mid" id="frag1087" style="display:none"><pre>
    def test_reason_builtin_exception_class(self):
        request = Request('https://example.com')
        spider = self.get_spider()
        expected_reason = NotImplementedError
        expected_reason_string = 'builtins.NotImplementedError'
        with LogCapture() as log:
            get_retry_request(
                request,
                spider=spider,
                reason=expected_reason,
            )
        expected_retry_times = 1
        stat = spider.crawler.stats.get_value(
            f'retry/reason_count/{expected_reason_string}'
        )
        self.assertEqual(stat, 1)
        log.check_present(
            (
                "scrapy.downloadermiddlewares.retry",
                "DEBUG",
                f"Retrying {request} (failed {expected_retry_times} times): "
                f"{expected_reason}",
            )
        )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1086')" href="javascript:;">
scrapy-2.6.1/tests/test_downloadermiddleware_retry.py: 498-522
</a>
<div class="mid" id="frag1086" style="display:none"><pre>
    def test_reason_builtin_exception(self):
        request = Request('https://example.com')
        spider = self.get_spider()
        expected_reason = NotImplementedError()
        expected_reason_string = 'builtins.NotImplementedError'
        with LogCapture() as log:
            get_retry_request(
                request,
                spider=spider,
                reason=expected_reason,
            )
        expected_retry_times = 1
        stat = spider.crawler.stats.get_value(
            f'retry/reason_count/{expected_reason_string}'
        )
        self.assertEqual(stat, 1)
        log.check_present(
            (
                "scrapy.downloadermiddlewares.retry",
                "DEBUG",
                f"Retrying {request} (failed {expected_retry_times} times): "
                f"{expected_reason}",
            )
        )

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 30:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1140')" href="javascript:;">
scrapy-2.6.1/tests/test_robotstxt_interface.py: 45-63
</a>
<div class="mid" id="frag1140" style="display:none"><pre>
    def test_allowed_wildcards(self):
        robotstxt_robotstxt_body = """User-agent: first
                                Disallow: /disallowed/*/end$

                                User-agent: second
                                Allow: /*allowed
                                Disallow: /
                                """.encode('utf-8')
        rp = self.parser_cls.from_crawler(crawler=None, robotstxt_body=robotstxt_robotstxt_body)

        self.assertTrue(rp.allowed("https://www.site.local/disallowed", "first"))
        self.assertFalse(rp.allowed("https://www.site.local/disallowed/xyz/end", "first"))
        self.assertFalse(rp.allowed("https://www.site.local/disallowed/abc/end", "first"))
        self.assertTrue(rp.allowed("https://www.site.local/disallowed/xyz/endinglater", "first"))

        self.assertTrue(rp.allowed("https://www.site.local/allowed", "second"))
        self.assertTrue(rp.allowed("https://www.site.local/is_still_allowed", "second"))
        self.assertTrue(rp.allowed("https://www.site.local/is_allowed_too", "second"))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1145')" href="javascript:;">
scrapy-2.6.1/tests/test_robotstxt_interface.py: 95-116
</a>
<div class="mid" id="frag1145" style="display:none"><pre>
    def test_unicode_url_and_useragent(self):
        robotstxt_robotstxt_body = """
        User-Agent: *
        Disallow: /admin/
        Disallow: /static/
        # taken from https://en.wikipedia.org/robots.txt
        Disallow: /wiki/K%C3%A4ytt%C3%A4j%C3%A4:
        Disallow: /wiki/Kyttj:

        User-Agent: UnicdeBt
        Disallow: /some/randome/page.html""".encode('utf-8')
        rp = self.parser_cls.from_crawler(crawler=None, robotstxt_body=robotstxt_robotstxt_body)
        self.assertTrue(rp.allowed("https://site.local/", "*"))
        self.assertFalse(rp.allowed("https://site.local/admin/", "*"))
        self.assertFalse(rp.allowed("https://site.local/static/", "*"))
        self.assertTrue(rp.allowed("https://site.local/admin/", "UnicdeBt"))
        self.assertFalse(rp.allowed("https://site.local/wiki/K%C3%A4ytt%C3%A4j%C3%A4:", "*"))
        self.assertFalse(rp.allowed("https://site.local/wiki/Kyttj:", "*"))
        self.assertTrue(rp.allowed("https://site.local/some/randome/page.html", "*"))
        self.assertFalse(rp.allowed("https://site.local/some/randome/page.html", "UnicdeBt"))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 31:</b> &nbsp; 2 fragments, nominal size 19 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1174')" href="javascript:;">
scrapy-2.6.1/tests/test_pqueues.py: 100-117
</a>
<div class="mid" id="frag1174" style="display:none"><pre>
    def test_push_pop(self):
        self.assertEqual(len(self.queue), 0)
        self.assertIsNone(self.queue.pop())
        req1 = Request("http://www.example.com/1")
        req2 = Request("http://www.example.com/2")
        req3 = Request("http://www.example.com/3")
        self.queue.push(req1)
        self.queue.push(req2)
        self.queue.push(req3)
        self.assertEqual(len(self.queue), 3)
        self.assertEqual(self.queue.pop().url, req1.url)
        self.assertEqual(len(self.queue), 2)
        self.assertEqual(self.queue.pop().url, req2.url)
        self.assertEqual(len(self.queue), 1)
        self.assertEqual(self.queue.pop().url, req3.url)
        self.assertEqual(len(self.queue), 0)
        self.assertIsNone(self.queue.pop())

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1176')" href="javascript:;">
scrapy-2.6.1/tests/test_pqueues.py: 125-144
</a>
<div class="mid" id="frag1176" style="display:none"><pre>
    def test_peek(self):
        if not hasattr(queuelib.queue.FifoMemoryQueue, "peek"):
            raise unittest.SkipTest("queuelib.queue.FifoMemoryQueue.peek is undefined")
        self.assertEqual(len(self.queue), 0)
        req1 = Request("https://example.org/1")
        req2 = Request("https://example.org/2")
        req3 = Request("https://example.org/3")
        self.queue.push(req1)
        self.queue.push(req2)
        self.queue.push(req3)
        self.assertEqual(len(self.queue), 3)
        self.assertEqual(self.queue.peek().url, req1.url)
        self.assertEqual(self.queue.pop().url, req1.url)
        self.assertEqual(len(self.queue), 2)
        self.assertEqual(self.queue.peek().url, req2.url)
        self.assertEqual(self.queue.pop().url, req2.url)
        self.assertEqual(len(self.queue), 1)
        self.assertEqual(self.queue.peek().url, req3.url)
        self.assertEqual(self.queue.pop().url, req3.url)
        self.assertIsNone(self.queue.peek())
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 32:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 93%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1183')" href="javascript:;">
scrapy-2.6.1/tests/test_spidermiddleware_referer.py: 526-541
</a>
<div class="mid" id="frag1183" style="display:none"><pre>
    def test_valid_name(self):
        for s, p in [
            (POLICY_SCRAPY_DEFAULT, DefaultReferrerPolicy),
            (POLICY_NO_REFERRER, NoReferrerPolicy),
            (POLICY_NO_REFERRER_WHEN_DOWNGRADE, NoReferrerWhenDowngradePolicy),
            (POLICY_SAME_ORIGIN, SameOriginPolicy),
            (POLICY_ORIGIN, OriginPolicy),
            (POLICY_STRICT_ORIGIN, StrictOriginPolicy),
            (POLICY_ORIGIN_WHEN_CROSS_ORIGIN, OriginWhenCrossOriginPolicy),
            (POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN, StrictOriginWhenCrossOriginPolicy),
            (POLICY_UNSAFE_URL, UnsafeUrlPolicy),
        ]:
            settings = Settings({'REFERRER_POLICY': s})
            mw = RefererMiddleware(settings)
            self.assertEqual(mw.default_policy, p)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1184')" href="javascript:;">
scrapy-2.6.1/tests/test_spidermiddleware_referer.py: 542-557
</a>
<div class="mid" id="frag1184" style="display:none"><pre>
    def test_valid_name_casevariants(self):
        for s, p in [
            (POLICY_SCRAPY_DEFAULT, DefaultReferrerPolicy),
            (POLICY_NO_REFERRER, NoReferrerPolicy),
            (POLICY_NO_REFERRER_WHEN_DOWNGRADE, NoReferrerWhenDowngradePolicy),
            (POLICY_SAME_ORIGIN, SameOriginPolicy),
            (POLICY_ORIGIN, OriginPolicy),
            (POLICY_STRICT_ORIGIN, StrictOriginPolicy),
            (POLICY_ORIGIN_WHEN_CROSS_ORIGIN, OriginWhenCrossOriginPolicy),
            (POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN, StrictOriginWhenCrossOriginPolicy),
            (POLICY_UNSAFE_URL, UnsafeUrlPolicy),
        ]:
            settings = Settings({'REFERRER_POLICY': s.upper()})
            mw = RefererMiddleware(settings)
            self.assertEqual(mw.default_policy, p)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 33:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1365')" href="javascript:;">
scrapy-2.6.1/scrapy/extensions/memusage.py: 77-95
</a>
<div class="mid" id="frag1365" style="display:none"><pre>
    def _check_limit(self):
        if self.get_virtual_size() &gt; self.limit:
            self.crawler.stats.set_value('memusage/limit_reached', 1)
            mem = self.limit/1024/1024
            logger.error("Memory usage exceeded %(memusage)dM. Shutting down Scrapy...",
                         {'memusage': mem}, extra={'crawler': self.crawler})
            if self.notify_mails:
                subj = (
                    f"{self.crawler.settings['BOT_NAME']} terminated: "
                    f"memory usage exceeded {mem}M at {socket.gethostname()}"
                )
                self._send_report(self.notify_mails, subj)
                self.crawler.stats.set_value('memusage/limit_notified', 1)

            if self.crawler.engine.spider is not None:
                self.crawler.engine.close_spider(self.crawler.engine.spider, 'memusage_exceeded')
            else:
                self.crawler.stop()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1366')" href="javascript:;">
scrapy-2.6.1/scrapy/extensions/memusage.py: 96-112
</a>
<div class="mid" id="frag1366" style="display:none"><pre>
    def _check_warning(self):
        if self.warned: # warn only once
            return
        if self.get_virtual_size() &gt; self.warning:
            self.crawler.stats.set_value('memusage/warning_reached', 1)
            mem = self.warning/1024/1024
            logger.warning("Memory usage reached %(memusage)dM",
                           {'memusage': mem}, extra={'crawler': self.crawler})
            if self.notify_mails:
                subj = (
                    f"{self.crawler.settings['BOT_NAME']} warning: "
                    f"memory usage reached {mem}M at {socket.gethostname()}"
                )
                self._send_report(self.notify_mails, subj)
                self.crawler.stats.set_value('memusage/warning_notified', 1)
            self.warned = True

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 34:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1845')" href="javascript:;">
scrapy-2.6.1/scrapy/core/downloader/middleware.py: 38-50
</a>
<div class="mid" id="frag1845" style="display:none"><pre>
        def process_request(request: Request):
            for method in self.methods['process_request']:
                method = cast(Callable, method)
                response = yield deferred_from_coro(method(request=request, spider=spider))
                if response is not None and not isinstance(response, (Response, Request)):
                    raise _InvalidOutput(
                        f"Middleware {method.__qualname__} must return None, Response or "
                        f"Request, got {response.__class__.__name__}"
                    )
                if response:
                    return response
            return (yield download_func(request=request, spider=spider))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1847')" href="javascript:;">
scrapy-2.6.1/scrapy/core/downloader/middleware.py: 71-84
</a>
<div class="mid" id="frag1847" style="display:none"><pre>
        def process_exception(failure: Failure):
            exception = failure.value
            for method in self.methods['process_exception']:
                method = cast(Callable, method)
                response = yield deferred_from_coro(method(request=request, exception=exception, spider=spider))
                if response is not None and not isinstance(response, (Response, Request)):
                    raise _InvalidOutput(
                        f"Middleware {method.__qualname__} must return None, Response or "
                        f"Request, got {type(response)}"
                    )
                if response:
                    return response
            return failure

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<script language="JavaScript">
function ShowHide(divId) { 
    if(document.getElementById(divId).style.display == 'none') {
        document.getElementById(divId).style.display='block';
    } else { 
        document.getElementById(divId).style.display = 'none';
    } 
}
</script>
</body>
</html>

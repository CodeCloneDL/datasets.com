<html>
<head>
    <title>NiCad6 Clone Report</title>
    <style type="text/css">
        body {font-family:sans-serif;}
        table {background-color:white; border:0px; padding:0px; border-spacing:4px; width:auto; margin-left:30px; margin-right:auto;}
        td {background-color:rgba(192,212,238,0.8); border:0px; padding:8px; width:auto; vertical-align:top; border-radius:8px}
        pre {background-color:white; padding:4px;}
        a {color:darkblue;}
    </style>
</head>
<body>
<h2>NiCad6 Clone Report</h2>
<table>
<tr style="font-size:14pt">
<td><b>System:</b> &nbsp; sonnet-2.0.0</td>
<td><b>Clone pairs:</b> &nbsp; 121</td>
<td><b>Clone classes:</b> &nbsp; 33</td>
</tr>
<tr style="font-size:12pt">
<td style="background-color:white">Clone type: &nbsp; 3-2</td>
<td style="background-color:white">Granularity: &nbsp; functions-blind</td>
<td style="background-color:white">Max diff threshold: &nbsp; 30%</td>
<td style="background-color:white">Clone size: &nbsp; 10 - 2500 lines</td>
<td style="background-color:white">Total functions-blind: &nbsp; 1040</td>
</tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 1:</b> &nbsp; 3 fragments, nominal size 21 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag32')" href="javascript:;">
sonnet-2.0.0/sonnet/src/distribute/batch_norm.py: 47-93
</a>
<div class="mid" id="frag32" style="display:none"><pre>
  def __init__(self,
               create_scale: bool,
               create_offset: bool,
               moving_mean: metrics.Metric,
               moving_variance: metrics.Metric,
               eps: types.FloatLike = 1e-5,
               scale_init: Optional[initializers.Initializer] = None,
               offset_init: Optional[initializers.Initializer] = None,
               data_format: Text = "channels_last",
               name: Optional[Text] = None):
    """Constructs a ``CrossReplicaBatchNorm`` module.

    Args:
      create_scale: whether to create a trainable scale per channel applied
        after the normalization.
      create_offset: whether to create a trainable offset per channel applied
        after normalization and scaling.
      moving_mean: An object which keeps track of the moving average of the mean
        which can be used to normalize at test time. This object must have an
        update method which takes a value and updates the internal state and a
        value property which returns the current mean.
      moving_variance: An object which keeps track of the moving average of the
        variance which can be used to normalize at test time. This object must
        have an update method which takes a value and updates the internal state
        and a value property which returns the current variance.
      eps: Small epsilon to avoid division by zero variance. Defaults to
        ``1e-5``.
      scale_init: Optional initializer for the scale variable. Can only be set
        if ``create_scale=True``. By default scale is initialized to ``1``.
      offset_init: Optional initializer for the offset variable. Can only be set
        if ``create_offset=True``. By default offset is initialized to ``0``.
      data_format: The data format of the input. Can be either
        ``channels_first``, ``channels_last``, ``N...C`` or ``NC...``. By
        default it is ``channels_last``.
      name: Name of the module.
    """
    super(CrossReplicaBatchNorm, self).__init__(
        create_scale=create_scale,
        create_offset=create_offset,
        moving_mean=moving_mean,
        moving_variance=moving_variance,
        eps=eps,
        scale_init=scale_init,
        offset_init=offset_init,
        data_format=data_format,
        name=name)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag153')" href="javascript:;">
sonnet-2.0.0/sonnet/src/axis_norm.py: 230-270
</a>
<div class="mid" id="frag153" style="display:none"><pre>
  def __init__(self,
               create_scale: bool,
               create_offset: bool,
               eps: types.FloatLike = 1e-5,
               scale_init: Optional[initializers.Initializer] = None,
               offset_init: Optional[initializers.Initializer] = None,
               data_format: Text = "channels_last",
               name: Optional[Text] = None):
    """Constructs an ``InstanceNorm`` module.

    This method creates a module which normalizes over the spatial dimensions.

    Args:
      create_scale: ``bool`` representing whether to create a trainable scale
        per channel applied after the normalization.
      create_offset: ``bool`` representing whether to create a trainable offset
        per channel applied after normalization and scaling.
      eps: Small epsilon to avoid division by zero variance. Defaults to
        ``1e-5``.
      scale_init: Optional initializer for the scale variable. Can only be set
        if ``create_scale=True``. By default scale is initialized to ``1``.
      offset_init: Optional initializer for the offset variable. Can only be set
        if ``create_offset=True``. By default offset is initialized to ``0``.
      data_format: The data format of the input. Can be either
        ``channels_first``, ``channels_last``, ``N...C`` or ``NC...``. By
        default it is ``channels_last``.
      name: Name of the module.
    """
    if utils.get_channel_index(data_format) == 1:
      axis = slice(2, None)
    else:  # channel_index = -1
      axis = slice(1, -1)
    super(InstanceNorm, self).__init__(
        axis=axis,
        create_scale=create_scale,
        create_offset=create_offset,
        eps=eps,
        scale_init=scale_init,
        offset_init=offset_init,
        data_format=data_format,
        name=name)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag72')" href="javascript:;">
sonnet-2.0.0/sonnet/src/batch_norm.py: 282-326
</a>
<div class="mid" id="frag72" style="display:none"><pre>
  def __init__(self,
               create_scale: bool,
               create_offset: bool,
               decay_rate: float = 0.999,
               eps: types.FloatLike = 1e-5,
               scale_init: Optional[initializers.Initializer] = None,
               offset_init: Optional[initializers.Initializer] = None,
               data_format: Text = "channels_last",
               name: Optional[Text] = None):
    """Constructs a ``BatchNorm`` module.

    Args:
      create_scale: whether to create a trainable scale per channel applied
        after the normalization.
      create_offset: whether to create a trainable offset per channel applied
        after normalization and scaling.
      decay_rate: Decay rate of the exponential moving averages of the mean and
        variance.
      eps: Small epsilon to avoid division by zero variance. Defaults to
        ``1e-5``.
      scale_init: Optional initializer for the scale variable. Can only be set
        if ``create_scale=True``. By default scale is initialized to ``1``.
      offset_init: Optional initializer for the offset variable. Can only be set
        if ``create_offset=True``. By default offset is initialized to ``0``.
      data_format: The data format of the input. Can be either
        ``channels_first``, ``channels_last``, ``N...C`` or ``NC...``. By
        default it is ``channels_last``.
      name: Name of the module.
    """
    with tf.name_scope(name or "batch_norm"):
      moving_mean = moving_averages.ExponentialMovingAverage(
          decay_rate, name="moving_mean")
      moving_variance = moving_averages.ExponentialMovingAverage(
          decay_rate, name="moving_variance")

    super(BatchNorm, self).__init__(
        create_scale=create_scale,
        create_offset=create_offset,
        moving_mean=moving_mean,
        moving_variance=moving_variance,
        eps=eps,
        scale_init=scale_init,
        offset_init=offset_init,
        data_format=data_format,
        name=name)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 2:</b> &nbsp; 2 fragments, nominal size 36 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag143')" href="javascript:;">
sonnet-2.0.0/sonnet/src/conv.py: 37-105
</a>
<div class="mid" id="frag143" style="display:none"><pre>
  def __init__(self,
               num_spatial_dims: int,
               output_channels: int,
               kernel_shape: Union[int, Sequence[int]],
               stride: Union[int, Sequence[int]] = 1,
               rate: Union[int, Sequence[int]] = 1,
               padding: Union[Text, pad.Paddings] = "SAME",
               with_bias: bool = True,
               w_init: Optional[initializers.Initializer] = None,
               b_init: Optional[initializers.Initializer] = None,
               data_format: Optional[Text] = None,
               name: Optional[Text] = None):
    """Constructs a `ConvND` module.

    Args:
      num_spatial_dims: The number of spatial dimensions of the input.
      output_channels: The number of output channels.
      kernel_shape: Sequence of kernel sizes (of length num_spatial_dims), or an
        integer. `kernel_shape` will be expanded to define a kernel size in all
        dimensions.
      stride: Sequence of strides (of length num_spatial_dims), or an integer.
        `stride` will be expanded to define stride in all dimensions.
      rate: Sequence of dilation rates (of length num_spatial_dims), or integer
        that is used to define dilation rate in all dimensions. 1 corresponds to
        standard ND convolution, `rate &gt; 1` corresponds to dilated convolution.
      padding: Padding to apply to the input. This can either "SAME", "VALID" or
        a callable or sequence of callables up to size N. Any callables must
        take a single integer argument equal to the effective kernel size and
        return a list of two integers representing the padding before and after.
        See snt.pad.* for more details and example functions.
      with_bias: Whether to include bias parameters. Default `True`.
      w_init: Optional initializer for the weights. By default the weights are
        initialized truncated random normal values with a standard deviation of
        `1 / sqrt(input_feature_size)`, which is commonly used when the inputs
        are zero centered (see https://arxiv.org/abs/1502.03167v3).
      b_init: Optional initializer for the bias. By default the bias is
        initialized to zero.
      data_format: The data format of the input.
      name: Name of the module.
    """
    super(ConvND, self).__init__(name=name)

    if not 1 &lt;= num_spatial_dims &lt;= 3:
      raise ValueError(
          "We only support convoltion operations for num_spatial_dims=1, 2 or "
          "3, received num_spatial_dims={}.".format(num_spatial_dims))
    self._num_spatial_dims = num_spatial_dims
    self.output_channels = output_channels
    self.kernel_shape = kernel_shape
    self.stride = stride
    self.rate = rate

    if isinstance(padding, six.string_types):
      self.conv_padding = padding.upper()
      self.padding_func = None
    else:
      self.conv_padding = "VALID"
      self.padding_func = padding

    self.data_format = data_format
    self._channel_index = utils.get_channel_index(data_format)
    self.with_bias = with_bias

    self.w_init = w_init
    if with_bias:
      self.b_init = b_init if b_init is not None else initializers.Zeros()
    elif b_init is not None:
      raise ValueError("When not using a bias the b_init must be None.")

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag884')" href="javascript:;">
sonnet-2.0.0/sonnet/src/conv_transpose.py: 44-113
</a>
<div class="mid" id="frag884" style="display:none"><pre>
  def __init__(self,
               num_spatial_dims: int,
               output_channels: int,
               kernel_shape: Union[int, Sequence[int]],
               output_shape: Optional[types.ShapeLike] = None,
               stride: Union[int, Sequence[int]] = 1,
               rate: Union[int, Sequence[int]] = 1,
               padding: Text = "SAME",
               with_bias: bool = True,
               w_init: Optional[initializers.Initializer] = None,
               b_init: Optional[initializers.Initializer] = None,
               data_format: Optional[Text] = None,
               name: Optional[Text] = None):
    """Constructs a `ConvNDTranspose` module.

    Args:
      num_spatial_dims: Number of spatial dimensions of the input.
      output_channels: Number of output channels.
      kernel_shape: Sequence of integers (of length num_spatial_dims), or an
        integer representing kernel shape. `kernel_shape` will be expanded to
        define a kernel size in all dimensions.
      output_shape: Output shape of the spatial dimensions of a transpose
        convolution. Can be either an integer or an iterable of integers or a
        `TensorShape` of length `num_spatial_dims`. If a `None` value is given,
        a default shape is automatically calculated.
      stride: Sequence of integers (of length num_spatial_dims), or an integer.
        `stride` will be expanded to define stride in all dimensions.
      rate: Sequence of integers (of length num_spatial_dims), or integer that
        is used to define dilation rate in all dimensions. 1 corresponds to
        standard ND convolution, `rate &gt; 1` corresponds to dilated convolution.
      padding: Padding algorithm, either "SAME" or "VALID".
      with_bias: Boolean, whether to include bias parameters. Default `True`.
      w_init: Optional initializer for the weights. By default the weights are
        initialized truncated random normal values with a standard deviation of
        `1 / sqrt(input_feature_size)`, which is commonly used when the
        inputs are zero centered (see https://arxiv.org/abs/1502.03167v3).
      b_init: Optional initializer for the bias. By default the bias is
        initialized to zero.
      data_format: The data format of the input.
      name: Name of the module.
    """
    super(ConvNDTranspose, self).__init__(name=name)

    if not 1 &lt;= num_spatial_dims &lt;= 3:
      raise ValueError(
          "We only support transpose convolution operations for "
          "num_spatial_dims=1, 2 or 3, received num_spatial_dims={}.".format(
              num_spatial_dims))
    self._num_spatial_dims = num_spatial_dims
    self._output_channels = output_channels
    self._kernel_shape = kernel_shape
    self._output_shape = output_shape
    self._stride = stride
    self._rate = rate

    if padding == "SAME" or padding == "VALID":
      self._padding = padding
    else:
      raise TypeError("ConvNDTranspose only takes string padding, please "
                      "provide either `SAME` or `VALID`.")
    self._data_format = data_format
    self._channel_index = utils.get_channel_index(data_format)
    self._with_bias = with_bias

    self._w_init = w_init
    if with_bias:
      self._b_init = b_init if b_init is not None else initializers.Zeros()
    elif b_init is not None:
      raise ValueError("When not using a bias the b_init must be None.")

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 3:</b> &nbsp; 6 fragments, nominal size 23 lines, similarity 88%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag147')" href="javascript:;">
sonnet-2.0.0/sonnet/src/conv.py: 173-223
</a>
<div class="mid" id="frag147" style="display:none"><pre>
  def __init__(self,
               output_channels: int,
               kernel_shape: Union[int, Sequence[int]],
               stride: Union[int, Sequence[int]] = 1,
               rate: Union[int, Sequence[int]] = 1,
               padding: Union[Text, pad.Paddings] = "SAME",
               with_bias: bool = True,
               w_init: Optional[initializers.Initializer] = None,
               b_init: Optional[initializers.Initializer] = None,
               data_format: Text = "NWC",
               name: Optional[Text] = None):
    """Constructs a ``Conv1D`` module.

    Args:
      output_channels: The number of output channels.
      kernel_shape: Sequence of length 1, or an integer. ``kernel_shape`` will
        be expanded to define a kernel size in all dimensions.
      stride: Sequence of strides of length 1, or an integer. ``stride`` will be
        expanded to define stride in all dimensions.
      rate: Sequence of dilation rates of length 1, or integer that is used to
        define dilation rate in all dimensions. 1 corresponds to standard
        convolution, ``rate &gt; 1`` corresponds to dilated convolution.
      padding: Padding to apply to the input. This can be either ``SAME``,
        ``VALID`` or a callable or sequence of callables of size 1. Any
        callables must take a single integer argument equal to the effective
        kernel size and return a list of two integers representing the padding
        before and after. See snt.pad.* for more details and example functions.
      with_bias: Whether to include bias parameters. Default ``True``.
      w_init: Optional initializer for the weights. By default the weights are
        initialized truncated random normal values with a standard deviation of
        ``1``/``sqrt(input_feature_size)``, which is commonly used when the
        inputs are zero centered (see https://arxiv.org/abs/1502.03167v3).
      b_init: Optional initializer for the bias. By default the bias is
        initialized to zero.
      data_format: The data format of the input.
      name: Name of the module.
    """
    super(Conv1D, self).__init__(
        num_spatial_dims=1,
        output_channels=output_channels,
        kernel_shape=kernel_shape,
        stride=stride,
        rate=rate,
        padding=padding,
        with_bias=with_bias,
        w_init=w_init,
        b_init=b_init,
        data_format=data_format,
        name=name)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag890')" href="javascript:;">
sonnet-2.0.0/sonnet/src/conv_transpose.py: 254-307
</a>
<div class="mid" id="frag890" style="display:none"><pre>
  def __init__(self,
               output_channels: int,
               kernel_shape: Union[int, Sequence[int]],
               output_shape: Optional[types.ShapeLike] = None,
               stride: Union[int, Sequence[int]] = 1,
               rate: Union[int, Sequence[int]] = 1,
               padding: Text = "SAME",
               with_bias: bool = True,
               w_init: Optional[initializers.Initializer] = None,
               b_init: Optional[initializers.Initializer] = None,
               data_format: Text = "NHWC",
               name: Optional[Text] = None):
    """Constructs a `Conv2DTranspose` module.

    Args:
      output_channels: An integer, The number of output channels.
      kernel_shape: Sequence of integers (of length 2), or an integer
        representing kernel shape. `kernel_shape` will be expanded to define a
        kernel size in all dimensions.
      output_shape: Output shape of the spatial dimensions of a transpose
        convolution. Can be either an integer or an iterable of integers or
        `Dimension`s, or a `TensorShape` (of length 2). If a `None` value is
        given, a default shape is automatically calculated.
      stride: Sequence of integers (of length 2), or an integer. `stride` will
        be expanded to define stride in all dimensions.
      rate: Sequence of integers (of length 2), or integer that is used to
        define dilation rate in all dimensions. 1 corresponds to standard 2D
        convolution, `rate &gt; 1` corresponds to dilated convolution.
      padding: Padding algorithm, either "SAME" or "VALID".
      with_bias: Boolean, whether to include bias parameters. Default `True`.
      w_init: Optional initializer for the weights. By default the weights are
        initialized truncated random normal values with a standard deviation of
        `1 / sqrt(input_feature_size)`, which is commonly used when the
        inputs are zero centered (see https://arxiv.org/abs/1502.03167v3).
      b_init: Optional initializer for the bias. By default the bias is
        initialized to zero.
      data_format: The data format of the input.
      name: Name of the module.
    """
    super(Conv2DTranspose, self).__init__(
        num_spatial_dims=2,
        output_channels=output_channels,
        kernel_shape=kernel_shape,
        output_shape=output_shape,
        stride=stride,
        rate=rate,
        padding=padding,
        with_bias=with_bias,
        w_init=w_init,
        b_init=b_init,
        data_format=data_format,
        name=name)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag889')" href="javascript:;">
sonnet-2.0.0/sonnet/src/conv_transpose.py: 197-250
</a>
<div class="mid" id="frag889" style="display:none"><pre>
  def __init__(self,
               output_channels: int,
               kernel_shape: Union[int, Sequence[int]],
               output_shape: Optional[types.ShapeLike] = None,
               stride: Union[int, Sequence[int]] = 1,
               rate: Union[int, Sequence[int]] = 1,
               padding: Text = "SAME",
               with_bias: bool = True,
               w_init: Optional[initializers.Initializer] = None,
               b_init: Optional[initializers.Initializer] = None,
               data_format: Text = "NWC",
               name: Optional[Text] = None):
    """Constructs a `Conv1DTranspose` module.

    Args:
      output_channels: Number of output channels.
      kernel_shape: Sequence of integers (of length 1), or an integer
        representing kernel shape. `kernel_shape` will be expanded to define a
        kernel size in all dimensions.
      output_shape: Output shape of the spatial dimensions of a transpose
        convolution. Can be either an integer or an iterable of integers or
        `Dimension`s, or a `TensorShape` (of length 1). If a `None` value is
        given, a default shape is automatically calculated.
      stride: Sequence of integers (of length 1), or an integer. `stride` will
        be expanded to define stride in all dimensions.
      rate: Sequence of integers (of length 1), or integer that is used to
        define dilation rate in all dimensions. 1 corresponds to standard 1D
        convolution, `rate &gt; 1` corresponds to dilated convolution.
      padding: Padding algorithm, either "SAME" or "VALID".
      with_bias: Boolean, whether to include bias parameters. Default `True`.
      w_init: Optional initializer for the weights. By default the weights are
        initialized truncated random normal values with a standard deviation of
        `1 / sqrt(input_feature_size)`, which is commonly used when the
        inputs are zero centered (see https://arxiv.org/abs/1502.03167v3).
      b_init: Optional initializer for the bias. By default the bias is
        initialized to zero.
      data_format: The data format of the input.
      name: Name of the module.
    """
    super(Conv1DTranspose, self).__init__(
        num_spatial_dims=1,
        output_channels=output_channels,
        kernel_shape=kernel_shape,
        output_shape=output_shape,
        stride=stride,
        rate=rate,
        padding=padding,
        with_bias=with_bias,
        w_init=w_init,
        b_init=b_init,
        data_format=data_format,
        name=name)


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag149')" href="javascript:;">
sonnet-2.0.0/sonnet/src/conv.py: 282-331
</a>
<div class="mid" id="frag149" style="display:none"><pre>
  def __init__(self,
               output_channels: int,
               kernel_shape: Union[int, Sequence[int]],
               stride: Union[int, Sequence[int]] = 1,
               rate: Union[int, Sequence[int]] = 1,
               padding: Union[Text, pad.Paddings] = "SAME",
               with_bias: bool = True,
               w_init: Optional[initializers.Initializer] = None,
               b_init: Optional[initializers.Initializer] = None,
               data_format: Text = "NDHWC",
               name: Optional[Text] = None):
    """Constructs a ``Conv3D`` module.

    Args:
      output_channels: The number of output channels.
      kernel_shape: Sequence of kernel sizes (of length 3), or an integer.
        ``kernel_shape`` will be expanded to define a kernel size in all
        dimensions.
      stride: Sequence of strides (of length 3), or an integer. `stride` will be
        expanded to define stride in all dimensions.
      rate: Sequence of dilation rates (of length 3), or integer that is used to
        define dilation rate in all dimensions. 1 corresponds to standard
        convolution, ``rate &gt; 1`` corresponds to dilated convolution.
      padding: Padding to apply to the input. This can either ``SAME``,
        ``VALID`` or a callable or sequence of callables up to size N. Any
        callables must take a single integer argument equal to the effective
        kernel size and return a list of two integers representing the padding
        before and after. See snt.pad.* for more details and example functions.
      with_bias: Whether to include bias parameters. Default ``True``.
      w_init: Optional initializer for the weights. By default the weights are
        initialized truncated random normal values with a standard deviation of
        ``1 / sqrt(input_feature_size)``, which is commonly used when the inputs
        are zero centered (see https://arxiv.org/abs/1502.03167v3).
      b_init: Optional initializer for the bias. By default the bias is
        initialized to zero.
      data_format: The data format of the input.
      name: Name of the module.
    """
    super(Conv3D, self).__init__(
        num_spatial_dims=3,
        output_channels=output_channels,
        kernel_shape=kernel_shape,
        stride=stride,
        rate=rate,
        padding=padding,
        with_bias=with_bias,
        w_init=w_init,
        b_init=b_init,
        data_format=data_format,
        name=name)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag891')" href="javascript:;">
sonnet-2.0.0/sonnet/src/conv_transpose.py: 311-362
</a>
<div class="mid" id="frag891" style="display:none"><pre>
  def __init__(self,
               output_channels: int,
               kernel_shape: Union[int, Sequence[int]],
               output_shape: Optional[types.ShapeLike] = None,
               stride: Union[int, Sequence[int]] = 1,
               rate: Union[int, Sequence[int]] = 1,
               padding: Text = "SAME",
               with_bias: bool = True,
               w_init: Optional[initializers.Initializer] = None,
               b_init: Optional[initializers.Initializer] = None,
               data_format: Text = "NDHWC",
               name: Optional[Text] = None):
    """Constructs a `Conv3DTranspose` module.

    Args:
      output_channels: An integer, The number of output channels.
      kernel_shape: Sequence of integers (of length 3), or an integer
        representing kernel shape. `kernel_shape` will be expanded to define a
        kernel size in all dimensions.
      output_shape: Output shape of the spatial dimensions of a transpose
        convolution. Can be either an integer or an iterable of integers or
        `Dimension`s, or a `TensorShape` (of length 3). If a None value is
        given, a default shape is automatically calculated.
      stride: Sequence of integers (of length 3), or an integer. `stride` will
        be expanded to define stride in all dimensions.
      rate: Sequence of integers (of length 3), or integer that is used to
        define dilation rate in all dimensions. 1 corresponds to standard 3D
        convolution, `rate &gt; 1` corresponds to dilated convolution.
      padding: Padding algorithm, either "SAME" or "VALID".
      with_bias: Boolean, whether to include bias parameters. Default `True`.
      w_init: Optional initializer for the weights. By default the weights are
        initialized truncated random normal values with a standard deviation of
        `1 / sqrt(input_feature_size)`, which is commonly used when the
        inputs are zero centered (see https://arxiv.org/abs/1502.03167v3).
      b_init: Optional initializer for the bias. By default the bias is
        initialized to zero.
      data_format: The data format of the input.
      name: Name of the module.
    """
    super(Conv3DTranspose, self).__init__(
        num_spatial_dims=3,
        output_channels=output_channels,
        kernel_shape=kernel_shape,
        output_shape=output_shape,
        stride=stride,
        rate=rate,
        padding=padding,
        with_bias=with_bias,
        w_init=w_init,
        b_init=b_init,
        data_format=data_format,
        name=name)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag148')" href="javascript:;">
sonnet-2.0.0/sonnet/src/conv.py: 227-278
</a>
<div class="mid" id="frag148" style="display:none"><pre>
  def __init__(self,
               output_channels: int,
               kernel_shape: Union[int, Sequence[int]],
               stride: Union[int, Sequence[int]] = 1,
               rate: Union[int, Sequence[int]] = 1,
               padding: Union[Text, pad.Paddings] = "SAME",
               with_bias: bool = True,
               w_init: Optional[initializers.Initializer] = None,
               b_init: Optional[initializers.Initializer] = None,
               data_format: Text = "NHWC",
               name: Optional[Text] = None):
    """Constructs a ``Conv2D`` module.

    Args:
      output_channels: The number of output channels.
      kernel_shape: Sequence of kernel sizes (of length 2), or an integer.
        ``kernel_shape`` will be expanded to define a kernel size in all
        dimensions.
      stride: Sequence of strides (of length 2), or an integer. ``stride`` will
        be expanded to define stride in all dimensions.
      rate: Sequence of dilation rates (of length 2), or integer that is used to
        define dilation rate in all dimensions. 1 corresponds to standard
        convolution, ``rate &gt; 1`` corresponds to dilated convolution.
      padding: Padding to apply to the input. This can either ``SAME``,
        ``VALID`` or a callable or sequence of callables of size 2. Any
        callables must take a single integer argument equal to the effective
        kernel size and return a list of two integers representing the padding
        before and after. See snt.pad.* for more details and example functions.
      with_bias: Whether to include bias parameters. Default ``True``.
      w_init: Optional initializer for the weights. By default the weights are
        initialized truncated random normal values with a standard deviation of
        ``1 / sqrt(input_feature_size)``, which is commonly used when the inputs
        are zero centered (see https://arxiv.org/abs/1502.03167v3).
      b_init: Optional initializer for the bias. By default the bias is
        initialized to zero.
      data_format: The data format of the input.
      name: Name of the module.
    """
    super(Conv2D, self).__init__(
        num_spatial_dims=2,
        output_channels=output_channels,
        kernel_shape=kernel_shape,
        stride=stride,
        rate=rate,
        padding=padding,
        with_bias=with_bias,
        w_init=w_init,
        b_init=b_init,
        data_format=data_format,
        name=name)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 4:</b> &nbsp; 2 fragments, nominal size 36 lines, similarity 86%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag150')" href="javascript:;">
sonnet-2.0.0/sonnet/src/axis_norm.py: 72-136
</a>
<div class="mid" id="frag150" style="display:none"><pre>
  def __init__(self,
               axis: types.Axis,
               create_scale: bool,
               create_offset: bool,
               eps: types.FloatLike = 1e-5,
               scale_init: Optional[initializers.Initializer] = None,
               offset_init: Optional[initializers.Initializer] = None,
               data_format: Text = "channels_last",
               name: Optional[Text] = None):
    r"""Constructs an ``LayerNorm`` module.

    Args:
      axis: An ``int``, ``slice`` or sequence of ``int``\s representing the axes
        which should be normalized across. Typical usages are: ``1`` or ``-1``
        for normalization over just the channels and ``slice(1, None)``,
        ``slice(2, None)`` for normalization over the spatial and channel
        dimensions whilst avoiding the batch and/or time dimensions.
      create_scale: ``bool`` representing whether to create a trainable scale
        per channel applied after the normalization.
      create_offset: ``bool`` representing whether to create a trainable offset
        per channel applied after normalization and scaling.
      eps: Small epsilon to avoid division by zero variance. Defaults to
        ``1e-5``.
      scale_init: Optional initializer for the scale variable. Can only be set
        if ``create_scale=True``. By default scale is initialized to ``1``.
      offset_init: Optional initializer for the offset variable. Can only be set
        if ``create_offset=True``. By default offset is initialized to ``0``.
      data_format: The data format of the input. Can be either
        ``channels_first``, ``channels_last``, ``N...C`` or ``NC...``. By
        default it is ``channels_last``.
      name: Name of the module.
    """
    super(LayerNorm, self).__init__(name=name)

    if isinstance(axis, slice):
      self._axis = axis
    elif isinstance(axis, six.integer_types):
      self._axis = (axis,)
    elif (isinstance(axis, collections.Iterable) and
          all(isinstance(ax, six.integer_types) for ax in axis)):
      self._axis = axis
    else:
      raise ValueError("`axis` should be an int, slice or iterable of ints.")

    self._eps = eps

    self._data_format = data_format
    self._channel_index = utils.get_channel_index(data_format)

    self._rank = None

    self._create_scale = create_scale
    self._create_offset = create_offset

    if self._create_scale:
      self._scale_init = (
          scale_init if scale_init is not None else initializers.Ones())
    elif scale_init is not None:
      raise ValueError("Cannot set `scale_init` if `create_scale=False`.")
    if self._create_offset:
      self._offset_init = (
          offset_init if offset_init is not None else initializers.Zeros())
    elif offset_init is not None:
      raise ValueError("Cannot set `offset_init` if `create_offset=False`.")

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag462')" href="javascript:;">
sonnet-2.0.0/sonnet/src/group_norm.py: 74-139
</a>
<div class="mid" id="frag462" style="display:none"><pre>
  def __init__(self,
               groups: int,
               axis: types.Axis = slice(1, None),
               create_scale: bool = True,
               create_offset: bool = True,
               eps: types.FloatLike = 1e-5,
               scale_init: Optional[initializers.Initializer] = None,
               offset_init: Optional[initializers.Initializer] = None,
               data_format: Text = "channels_last",
               name: Optional[Text] = None):
    """Constructs a ``GroupNorm`` module.

    Args:
      groups: number of groups to divide the channels by. The number of channels
        must be divisible by this.
      axis: ``int``, ``slice`` or sequence of ints representing the axes which
        should be normalized across. By default this is all but the first
        dimension. For time series data use `slice(2, None)` to average over the
        none Batch and Time data.
      create_scale: whether to create a trainable scale per channel applied
        after the normalization.
      create_offset: whether to create a trainable offset per channel applied
        after normalization and scaling.
      eps: Small epsilon to add to the variance to avoid division by zero.
        Defaults to ``1e-5``.
      scale_init: Optional initializer for the scale variable. Can only be set
        if ``create_scale=True``. By default scale is initialized to ``1``.
      offset_init: Optional initializer for the offset variable. Can only be set
        if ``create_offset=True``. By default offset is initialized to ``0``.
      data_format: The data format of the input. Can be either
        ``channels_first``, ``channels_last``, ``N...C`` or ``NC...``. By
        default it is ``channels_last``.
      name: Name of the module.
    """
    super(GroupNorm, self).__init__(name=name)

    if isinstance(axis, slice):
      self._axis = axis
    elif isinstance(axis, six.integer_types):
      self._axis = [axis]
    elif (isinstance(axis, collections.Iterable) and
          all(isinstance(ax, six.integer_types) for ax in axis)):
      self._axis = axis
    else:
      raise ValueError("`axis` should be an int, slice or iterable of ints.")

    self._groups = groups
    self._eps = eps

    self._data_format = data_format
    self._channel_index = utils.get_channel_index(data_format)

    self._create_scale = create_scale
    self._create_offset = create_offset

    if self._create_scale:
      self._scale_init = (
          scale_init if scale_init is not None else initializers.Ones())
    elif scale_init is not None:
      raise ValueError("Cannot set `scale_init` if `create_scale=False`.")
    if self._create_offset:
      self._offset_init = (
          offset_init if offset_init is not None else initializers.Zeros())
    elif offset_init is not None:
      raise ValueError("Cannot set `offset_init` if `create_offset=False`.")

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 5:</b> &nbsp; 2 fragments, nominal size 30 lines, similarity 84%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag151')" href="javascript:;">
sonnet-2.0.0/sonnet/src/axis_norm.py: 137-187
</a>
<div class="mid" id="frag151" style="display:none"><pre>
  def __call__(self,
               inputs: tf.Tensor,
               scale: Optional[tf.Tensor] = None,
               offset: Optional[tf.Tensor] = None) -&gt; tf.Tensor:
    """Returns normalized inputs.

    Args:
      inputs: An n-D tensor of the ``data_format`` specified in the constructor
        on which the transformation is performed.
      scale: A tensor up to n-D. The shape of this tensor must be broadcastable
        to the shape of ``inputs``. This is the scale applied to the normalized
        inputs. This cannot be passed in if the module was constructed with
        ``create_scale=True``.
      offset: A tensor up to n-D. The shape of this tensor must be broadcastable
        to the shape of ``inputs``. This is the offset applied to the normalized
        ``inputs``. This cannot be passed in if the module was constructed with
        ``create_offset=True``.

    Returns:
      An n-d tensor of the same shape as inputs that has been normalized.
    """
    self._initialize(inputs)
    if self._create_scale:
      if scale is not None:
        raise ValueError(
            "Cannot pass `scale` at call time if `create_scale=True`.")
      scale = self.scale

    if self._create_offset:
      if offset is not None:
        raise ValueError(
            "Cannot pass `offset` at call time if `create_offset=True`.")
      offset = self.offset

    if len(inputs.shape) != self._rank:
      raise ValueError(
          "The rank of the inputs cannot change between calls, the"
          " original call was rank={} but this call was rank={}.".format(
              self._rank, len(inputs.shape)))

    mean, var = tf.nn.moments(inputs, self._axis, keepdims=True)

    normalized = tf.nn.batch_normalization(
        inputs,
        mean=mean,
        variance=var,
        scale=scale,
        offset=offset,
        variance_epsilon=self._eps)
    return normalized

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag463')" href="javascript:;">
sonnet-2.0.0/sonnet/src/group_norm.py: 140-194
</a>
<div class="mid" id="frag463" style="display:none"><pre>
  def __call__(self,
               inputs: tf.Tensor,
               scale: Optional[tf.Tensor] = None,
               offset: Optional[tf.Tensor] = None):
    """Returns normalized inputs.

    Args:
      inputs: An n-D tensor of the ``data_format`` specified in the constructor
        on which the transformation is performed.
      scale: A tensor up to n-D. The shape of this tensor must be broadcastable
        to the shape of ``inputs``. This is the scale applied to the normalized
        inputs. This cannot be passed in if the module was constructed with
        ``create_scale=True``.
      offset: A tensor up to n-D. The shape of this tensor must be broadcastable
        to the shape of ``inputs``. This is the offset applied to the normalized
        ``inputs``. This cannot be passed in if the module was constructed with
        ``create_offset=True``.

    Returns:
      An n-d tensor of the same shape as inputs that has been normalized.
    """
    self._initialize(inputs)
    if self._create_scale:
      if scale is not None:
        raise ValueError(
            "Cannot pass `scale` at call time if `create_scale=True`.")
      scale = self.scale

    if self._create_offset:
      if offset is not None:
        raise ValueError(
            "Cannot pass `offset` at call time if `create_offset=True`.")
      offset = self.offset

    if len(inputs.shape) != self._rank:
      raise ValueError(
          "The rank of the inputs cannot change between calls, the"
          " original call was rank={} but this call was rank={}.".format(
              self._rank, len(inputs.shape)))

    inputs = tf.reshape(inputs, self._inputs_reshape)
    mean, var = tf.nn.moments(inputs, self._axis, keepdims=True)

    normalized = tf.nn.batch_normalization(
        inputs,
        mean=mean,
        variance=var,
        scale=None,
        offset=None,
        variance_epsilon=self._eps)
    outputs = tf.reshape(normalized, self._outputs_reshape)
    outputs = outputs * scale if scale is not None else outputs
    outputs = outputs + offset if offset is not None else outputs
    return outputs

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 6:</b> &nbsp; 3 fragments, nominal size 12 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag154')" href="javascript:;">
sonnet-2.0.0/sonnet/src/batch_norm_test.py: 33-47
</a>
<div class="mid" id="frag154" style="display:none"><pre>
  def testSimpleTraining(self):
    layer = batch_norm.BaseBatchNorm(
        moving_mean=TestMetric(),
        moving_variance=TestMetric(),
        create_scale=False,
        create_offset=False)

    inputs = tf.ones([2, 3, 3, 5])
    scale = tf.constant(0.5, shape=(5,))
    offset = tf.constant(2.0, shape=(5,))

    outputs = layer(inputs, True, scale=scale, offset=offset).numpy()
    self.assertAllEqual(outputs, tf.fill(inputs.shape, 2.0))
    self.assertEqual((0, 1, 2), layer._axis)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag162')" href="javascript:;">
sonnet-2.0.0/sonnet/src/batch_norm_test.py: 170-186
</a>
<div class="mid" id="frag162" style="display:none"><pre>
  def testUsingTestStats(self):
    layer = batch_norm.BaseBatchNorm(
        moving_mean=TestMetric(),
        moving_variance=TestMetric(),
        create_scale=False,
        create_offset=False)

    inputs = tf.ones([2, 3, 3, 5])
    scale = tf.constant(0.5, shape=(5,))
    offset = tf.constant(2.0, shape=(5,))

    outputs = layer(inputs, True, scale=scale, offset=offset).numpy()
    self.assertAllEqual(outputs, tf.fill(inputs.shape, 2.0))
    outputs = layer(inputs, False, scale=scale, offset=offset).numpy()
    for x in np.nditer(outputs):
      self.assertAllClose(x, 2.0, rtol=1e-5, atol=1e-3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag156')" href="javascript:;">
sonnet-2.0.0/sonnet/src/batch_norm_test.py: 64-78
</a>
<div class="mid" id="frag156" style="display:none"><pre>
  def testSimpleTraining3D(self):
    layer = batch_norm.BaseBatchNorm(
        moving_mean=TestMetric(),
        moving_variance=TestMetric(),
        create_scale=False,
        create_offset=False)

    inputs = tf.ones([2, 3, 3, 3, 5])
    scale = tf.constant(0.5, shape=(5,))
    offset = tf.constant(2.0, shape=(5,))

    outputs = layer(inputs, True, scale=scale, offset=offset).numpy()
    self.assertAllEqual(outputs, tf.fill(inputs.shape, 2.0))
    self.assertEqual((0, 1, 2, 3), layer._axis)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 7:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag230')" href="javascript:;">
sonnet-2.0.0/sonnet/src/base_test.py: 180-193
</a>
<div class="mid" id="frag230" style="display:none"><pre>
  def test_get_attr_doesnt_enter_name_scope(self):
    scope_names = []

    class GetAttrModule(base.Module):

      def __getattr__(self, name):
        scope_names.append((name, get_name_scope()))
        return super(GetAttrModule, self).__getattr__(name)

    mod = GetAttrModule()
    with self.assertRaises(AttributeError):
      mod.does_not_exist  # pylint: disable=pointless-statement
    self.assertIn(("does_not_exist", ""), scope_names)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag232')" href="javascript:;">
sonnet-2.0.0/sonnet/src/base_test.py: 194-208
</a>
<div class="mid" id="frag232" style="display:none"><pre>
  def test_get_attribute_doesnt_enter_name_scope(self):
    scope_names = []

    class GetAttributeModule(base.Module):

      def __getattribute__(self, name):
        scope_names.append((name, get_name_scope()))
        return super(GetAttributeModule, self).__getattribute__(name)

    mod = GetAttributeModule()
    with self.assertRaises(AttributeError):
      mod.does_not_exist  # pylint: disable=pointless-statement
    self.assertIn(("does_not_exist", ""), scope_names)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 8:</b> &nbsp; 3 fragments, nominal size 13 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag319')" href="javascript:;">
sonnet-2.0.0/sonnet/src/optimizers/sgd_test.py: 59-72
</a>
<div class="mid" id="frag319" style="display:none"><pre>
  def testVariableLearningRate(self):
    parameters = [tf.Variable([1., 2.]), tf.Variable([3., 4.])]
    updates = [tf.constant([5., 5.]), tf.constant([3., 3.])]
    learning_rate = tf.Variable(3.)
    optimizer = self.make_optimizer(learning_rate=learning_rate)
    optimizer.apply(updates, parameters)
    self.assertAllClose([[-14., -13.], [-6., -5.]],
                        [x.numpy() for x in parameters])
    learning_rate.assign_sub(1.)
    self.assertEqual(2., optimizer.learning_rate.numpy())
    optimizer.apply(updates, parameters)
    self.assertAllClose([[-24., -23.], [-12., -11.]],
                        [x.numpy() for x in parameters])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag362')" href="javascript:;">
sonnet-2.0.0/sonnet/src/optimizers/adam_test.py: 121-135
</a>
<div class="mid" id="frag362" style="display:none"><pre>
  def testVariableHyperParams(self):
    parameters = [tf.Variable([1., 2.]), tf.Variable([3., 4.])]
    updates = [tf.constant([5., 5.]), tf.constant([3., 3.])]
    learning_rate = tf.Variable(0.001)
    optimizer = self.make_optimizer(learning_rate=learning_rate)
    optimizer.apply(updates, parameters)
    self.assertAllClose([[0.999, 1.999], [2.999, 3.999]],
                        [x.numpy() for x in parameters])
    learning_rate.assign(0.1)
    self.assertAlmostEqual(0.1, optimizer.learning_rate.numpy())
    optimizer.apply(updates, parameters)
    self.assertAllClose([[0.899, 1.899], [2.899, 3.899]],
                        [x.numpy() for x in parameters],
                        rtol=1e-4)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag330')" href="javascript:;">
sonnet-2.0.0/sonnet/src/optimizers/rmsprop_test.py: 156-169
</a>
<div class="mid" id="frag330" style="display:none"><pre>
  def testVariableHyperParams(self):
    parameters = [tf.Variable([1., 2.]), tf.Variable([3., 4.])]
    updates = [tf.constant([5., 5.]), tf.constant([3., 3.])]
    learning_rate = tf.Variable(0.1)
    optimizer = self.make_optimizer(learning_rate=learning_rate)
    optimizer.apply(updates, parameters)
    self.assertAllClose([[0.683772, 1.683772], [2.683772, 3.683772]],
                        [x.numpy() for x in parameters])
    learning_rate.assign(0.01)
    self.assertAlmostEqual(0.01, optimizer.learning_rate.numpy())
    optimizer.apply(updates, parameters)
    self.assertAllClose([[0.660831, 1.660831], [2.660831, 3.660831]],
                        [x.numpy() for x in parameters])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 9:</b> &nbsp; 5 fragments, nominal size 13 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag326')" href="javascript:;">
sonnet-2.0.0/sonnet/src/optimizers/rmsprop_test.py: 66-82
</a>
<div class="mid" id="frag326" style="display:none"><pre>
  def testDense(self):
    parameters = [tf.Variable([1., 2.]), tf.Variable([3., 4.])]
    updates = [tf.constant([5., 5.]), tf.constant([3., 3.])]
    optimizer = self.make_optimizer(learning_rate=0.1)
    # Step 1 of RMSProp
    optimizer.apply(updates, parameters)
    self.assertAllClose([[0.683772, 1.683772], [2.683772, 3.683772]],
                        [x.numpy() for x in parameters])
    # Step 2 of RMSProp
    optimizer.apply(updates, parameters)
    self.assertAllClose([[0.454357, 1.454357], [2.454357, 3.454357]],
                        [x.numpy() for x in parameters])
    # Step 3 of RMSProp
    optimizer.apply(updates, parameters)
    self.assertAllClose([[0.262262, 1.262262], [2.262262, 3.262262]],
                        [x.numpy() for x in parameters])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag342')" href="javascript:;">
sonnet-2.0.0/sonnet/src/optimizers/momentum_test.py: 62-78
</a>
<div class="mid" id="frag342" style="display:none"><pre>
  def testDense(self):
    parameters = [tf.Variable([1., 2.]), tf.Variable([3., 4.])]
    updates = [tf.constant([5., 5.]), tf.constant([3., 3.])]
    optimizer = self.make_optimizer(learning_rate=0.1, momentum=0.9)
    # Step 1 of Momentum
    optimizer.apply(updates, parameters)
    self.assertAllClose([[0.5, 1.5], [2.7, 3.7]],
                        [x.numpy() for x in parameters])
    # Step 2 of Momentum
    optimizer.apply(updates, parameters)
    self.assertAllClose([[-0.45, 0.55], [2.13, 3.13]],
                        [x.numpy() for x in parameters])
    # Step 3 of Momentum
    optimizer.apply(updates, parameters)
    self.assertAllClose([[-1.805, -0.805], [1.317, 2.317]],
                        [x.numpy() for x in parameters])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag327')" href="javascript:;">
sonnet-2.0.0/sonnet/src/optimizers/rmsprop_test.py: 83-99
</a>
<div class="mid" id="frag327" style="display:none"><pre>
  def testDenseCentered(self):
    parameters = [tf.Variable([1., 2.]), tf.Variable([3., 4.])]
    updates = [tf.constant([5., 5.]), tf.constant([3., 3.])]
    optimizer = self.make_optimizer(learning_rate=0.1, centered=True)
    # Step 1 of RMSProp
    optimizer.apply(updates, parameters)
    self.assertAllClose([[0.666667, 1.666667], [2.666667, 3.666667]],
                        [x.numpy() for x in parameters])
    # Step 2 of RMSProp
    optimizer.apply(updates, parameters)
    self.assertAllClose([[0.41176, 1.41176], [2.41176, 3.41176]],
                        [x.numpy() for x in parameters])
    # Step 3 of RMSProp
    optimizer.apply(updates, parameters)
    self.assertAllClose([[0.186776, 1.186776], [2.186776, 3.186776]],
                        [x.numpy() for x in parameters])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag343')" href="javascript:;">
sonnet-2.0.0/sonnet/src/optimizers/momentum_test.py: 79-96
</a>
<div class="mid" id="frag343" style="display:none"><pre>
  def testDenseNesterov(self):
    parameters = [tf.Variable([1., 2.]), tf.Variable([3., 4.])]
    updates = [tf.constant([5., 5.]), tf.constant([3., 3.])]
    optimizer = self.make_optimizer(
        learning_rate=0.1, momentum=0.9, use_nesterov=True)
    # Step 1 of Momentum
    optimizer.apply(updates, parameters)
    self.assertAllClose([[0.05, 1.05], [2.43, 3.43]],
                        [x.numpy() for x in parameters])
    # Step 2 of Momentum
    optimizer.apply(updates, parameters)
    self.assertAllClose([[-1.305, -0.305], [1.617, 2.617]],
                        [x.numpy() for x in parameters])
    # Step 3 of Momentum
    optimizer.apply(updates, parameters)
    self.assertAllClose([[-3.0245, -2.0245], [0.5853, 1.5853]],
                        [x.numpy() for x in parameters])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag360')" href="javascript:;">
sonnet-2.0.0/sonnet/src/optimizers/adam_test.py: 63-79
</a>
<div class="mid" id="frag360" style="display:none"><pre>
  def testDense(self):
    parameters = [tf.Variable([1., 2.]), tf.Variable([3., 4.])]
    updates = [tf.constant([5., 5.]), tf.constant([3., 3.])]
    optimizer = self.make_optimizer(learning_rate=0.001)
    # Step 1 of Adam
    optimizer.apply(updates, parameters)
    self.assertAllClose([[0.999, 1.999], [2.999, 3.999]],
                        [x.numpy() for x in parameters])
    # Step 2 of Adam
    optimizer.apply(updates, parameters)
    self.assertAllClose([[0.998, 1.998], [2.998, 3.998]],
                        [x.numpy() for x in parameters])
    # Step 3 of Adam
    optimizer.apply(updates, parameters)
    self.assertAllClose([[0.997, 1.997], [2.997, 3.997]],
                        [x.numpy() for x in parameters])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 10:</b> &nbsp; 2 fragments, nominal size 22 lines, similarity 95%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag328')" href="javascript:;">
sonnet-2.0.0/sonnet/src/optimizers/rmsprop_test.py: 100-127
</a>
<div class="mid" id="frag328" style="display:none"><pre>
  def testSparse(self):
    if self.primary_device in ("GPU", "TPU"):
      self.skipTest("IndexedSlices not supported on {}.".format(
          self.primary_device))

    parameters = [tf.Variable([[1.], [2.]]), tf.Variable([[3.], [4.]])]
    updates = [
        tf.IndexedSlices(
            tf.constant([0.1], shape=[1, 1]), tf.constant([0]),
            tf.constant([2, 1])),
        tf.IndexedSlices(
            tf.constant([0.01], shape=[1, 1]), tf.constant([1]),
            tf.constant([2, 1]))
    ]
    optimizer = self.make_optimizer(learning_rate=3.)
    # Step 1 of RMSProp
    optimizer.apply(updates, parameters)
    self.assertAllClose([[-8.486831], [2.0]], parameters[0].numpy(), rtol=1e-4)
    self.assertAllClose([[3.0], [-5.486784]], parameters[1].numpy(), rtol=1e-4)
    # Step 2 of RMSProp
    optimizer.apply(updates, parameters)
    self.assertAllClose([[-15.369301], [2.0]], parameters[0].numpy(), rtol=1e-4)
    self.assertAllClose([[3.0], [-12.369237]], parameters[1].numpy(), rtol=1e-4)
    # Step 3 of RMSProp
    optimizer.apply(updates, parameters)
    self.assertAllClose([[-21.132141], [2.0]], parameters[0].numpy(), rtol=1e-4)
    self.assertAllClose([[3.0], [-18.132067]], parameters[1].numpy(), rtol=1e-4)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag329')" href="javascript:;">
sonnet-2.0.0/sonnet/src/optimizers/rmsprop_test.py: 128-155
</a>
<div class="mid" id="frag329" style="display:none"><pre>
  def testSparseCentered(self):
    if self.primary_device in ("GPU", "TPU"):
      self.skipTest("IndexedSlices not supported on {}.".format(
          self.primary_device))

    parameters = [tf.Variable([[1.], [2.]]), tf.Variable([[3.], [4.]])]
    updates = [
        tf.IndexedSlices(
            tf.constant([0.1], shape=[1, 1]), tf.constant([0]),
            tf.constant([2, 1])),
        tf.IndexedSlices(
            tf.constant([0.01], shape=[1, 1]), tf.constant([1]),
            tf.constant([2, 1]))
    ]
    optimizer = self.make_optimizer(learning_rate=3., centered=True)
    # Step 1 of RMSProp
    optimizer.apply(updates, parameters)
    self.assertAllClose([[-8.999999], [2.0]], parameters[0].numpy(), rtol=1e-4)
    self.assertAllClose([[3.0], [-5.999944]], parameters[1].numpy(), rtol=1e-4)
    # Step 2 of RMSProp
    optimizer.apply(updates, parameters)
    self.assertAllClose([[-16.64719], [2.0]], parameters[0].numpy(), rtol=1e-4)
    self.assertAllClose([[3.0], [-13.647109]], parameters[1].numpy(), rtol=1e-4)
    # Step 3 of RMSProp
    optimizer.apply(updates, parameters)
    self.assertAllClose([[-23.396709], [2.0]], parameters[0].numpy(), rtol=1e-4)
    self.assertAllClose([[3.0], [-20.39661]], parameters[1].numpy(), rtol=1e-4)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 11:</b> &nbsp; 3 fragments, nominal size 26 lines, similarity 92%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag406')" href="javascript:;">
sonnet-2.0.0/sonnet/src/conv_test.py: 128-157
</a>
<div class="mid" id="frag406" style="display:none"><pre>
  def testFunction(self, with_bias, padding):
    conv1 = conv.ConvND(
        num_spatial_dims=2,
        output_channels=1,
        kernel_shape=3,
        stride=1,
        padding=padding,
        with_bias=with_bias,
        data_format="NHWC",
        **create_constant_initializers(1.0, 1.0, with_bias))
    conv2 = conv.ConvND(
        num_spatial_dims=2,
        output_channels=1,
        kernel_shape=3,
        stride=1,
        padding=padding,
        with_bias=with_bias,
        data_format="NHWC",
        **create_constant_initializers(1.0, 1.0, with_bias))
    defun_conv = tf.function(conv2)

    iterations = 5

    for _ in range(iterations):
      x = tf.random.uniform([1, 5, 5, 1])
      y1 = conv1(x)
      y2 = defun_conv(x)

      self.assertAllClose(self.evaluate(y1), self.evaluate(y2), atol=1e-4)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag574')" href="javascript:;">
sonnet-2.0.0/sonnet/src/conv_transpose_test.py: 65-96
</a>
<div class="mid" id="frag574" style="display:none"><pre>
  def testGraphConv(self, with_bias, padding):
    conv1 = conv_transpose.ConvNDTranspose(
        num_spatial_dims=2,
        output_channels=1,
        output_shape=None,
        kernel_shape=3,
        stride=1,
        padding=padding,
        with_bias=with_bias,
        data_format="NHWC",
        **create_constant_initializers(1.0, 1.0, with_bias))
    conv2 = conv_transpose.ConvNDTranspose(
        num_spatial_dims=2,
        output_channels=1,
        output_shape=None,
        kernel_shape=3,
        stride=1,
        padding=padding,
        with_bias=with_bias,
        data_format="NHWC",
        **create_constant_initializers(1.0, 1.0, with_bias))
    defun_conv = tf.function(conv2)

    iterations = 5

    for _ in range(iterations):
      x = tf.random.uniform([1, 3, 3, 1])
      y1 = conv1(x)
      y2 = defun_conv(x)

      self.assertAllClose(self.evaluate(y1), self.evaluate(y2), atol=1e-4)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag755')" href="javascript:;">
sonnet-2.0.0/sonnet/src/depthwise_conv_test.py: 73-100
</a>
<div class="mid" id="frag755" style="display:none"><pre>
  def testFunction(self, with_bias, padding):
    conv1 = depthwise_conv.DepthwiseConv2D(
        channel_multiplier=1,
        kernel_shape=3,
        stride=1,
        padding=padding,
        with_bias=with_bias,
        data_format="NHWC",
        **create_constant_initializers(1.0, 1.0, with_bias))
    conv2 = depthwise_conv.DepthwiseConv2D(
        channel_multiplier=1,
        kernel_shape=3,
        stride=1,
        padding=padding,
        with_bias=with_bias,
        data_format="NHWC",
        **create_constant_initializers(1.0, 1.0, with_bias))
    defun_conv = tf.function(conv2)

    iterations = 5

    for _ in range(iterations):
      x = tf.random.uniform([1, 5, 5, 1])
      y1 = conv1(x)
      y2 = defun_conv(x)

      self.assertAllClose(self.evaluate(y1), self.evaluate(y2), atol=1e-4)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 12:</b> &nbsp; 5 fragments, nominal size 12 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag407')" href="javascript:;">
sonnet-2.0.0/sonnet/src/conv_test.py: 158-173
</a>
<div class="mid" id="frag407" style="display:none"><pre>
  def testUnknownBatchSizeNHWC(self):
    x = tf.TensorSpec([None, 5, 5, 3], dtype=tf.float32)

    c = conv.ConvND(
        num_spatial_dims=2,
        output_channels=2,
        kernel_shape=3,
        data_format="NHWC")
    defun_conv = tf.function(c).get_concrete_function(x)

    out1 = defun_conv(tf.ones([3, 5, 5, 3]))
    self.assertEqual(out1.shape, [3, 5, 5, 2])

    out2 = defun_conv(tf.ones([5, 5, 5, 3]))
    self.assertEqual(out2.shape, [5, 5, 5, 2])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag408')" href="javascript:;">
sonnet-2.0.0/sonnet/src/conv_test.py: 174-191
</a>
<div class="mid" id="frag408" style="display:none"><pre>
  def testUnknownBatchSizeNCHW(self):
    if self.primary_device == "CPU":
      self.skipTest("NCHW not supported on CPU")

    x = tf.TensorSpec([None, 3, 5, 5], dtype=tf.float32)
    c = conv.ConvND(
        num_spatial_dims=2,
        output_channels=2,
        kernel_shape=3,
        data_format="NCHW")
    defun_conv = tf.function(c).get_concrete_function(x)

    out1 = defun_conv(tf.ones([3, 3, 5, 5]))
    self.assertEqual(out1.shape, [3, 2, 5, 5])

    out2 = defun_conv(tf.ones([5, 3, 5, 5]))
    self.assertEqual(out2.shape, [5, 2, 5, 5])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag576')" href="javascript:;">
sonnet-2.0.0/sonnet/src/conv_transpose_test.py: 113-131
</a>
<div class="mid" id="frag576" style="display:none"><pre>
  def testUnknownBatchSizeNCHW(self):
    if self.primary_device == "CPU":
      self.skipTest("NCHW not supported on CPU")

    x = tf.TensorSpec([None, 3, 5, 5], dtype=tf.float32)

    c = conv_transpose.ConvNDTranspose(
        num_spatial_dims=2,
        output_channels=2,
        kernel_shape=3,
        data_format="NCHW")
    defun_conv = tf.function(c).get_concrete_function(x)

    out1 = defun_conv(tf.ones([3, 3, 5, 5]))
    self.assertEqual(out1.shape, [3, 2, 5, 5])

    out2 = defun_conv(tf.ones([5, 3, 5, 5]))
    self.assertEqual(out2.shape, [5, 2, 5, 5])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag575')" href="javascript:;">
sonnet-2.0.0/sonnet/src/conv_transpose_test.py: 97-112
</a>
<div class="mid" id="frag575" style="display:none"><pre>
  def testUnknownBatchSizeNHWC(self):
    x = tf.TensorSpec([None, 5, 5, 3], dtype=tf.float32)

    c = conv_transpose.ConvNDTranspose(
        num_spatial_dims=2,
        output_channels=2,
        kernel_shape=3,
        data_format="NHWC")
    defun_conv = tf.function(c).get_concrete_function(x)

    out1 = defun_conv(tf.ones([3, 5, 5, 3]))
    self.assertEqual(out1.shape, [3, 5, 5, 2])

    out2 = defun_conv(tf.ones([5, 5, 5, 3]))
    self.assertEqual(out2.shape, [5, 5, 5, 2])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag757')" href="javascript:;">
sonnet-2.0.0/sonnet/src/depthwise_conv_test.py: 114-128
</a>
<div class="mid" id="frag757" style="display:none"><pre>
  def testUnknownBatchSizeNCHW(self):
    if self.primary_device == "CPU":
      self.skipTest("NCHW not supported on CPU")

    x = tf.TensorSpec([None, 3, 5, 5], dtype=tf.float32)
    c = depthwise_conv.DepthwiseConv2D(
        channel_multiplier=1, kernel_shape=3, data_format="NCHW")
    defun_conv = tf.function(c).get_concrete_function(x)

    out1 = defun_conv(tf.ones([3, 3, 5, 5]))
    self.assertEqual(out1.shape, [3, 3, 5, 5])

    out2 = defun_conv(tf.ones([5, 3, 5, 5]))
    self.assertEqual(out2.shape, [5, 3, 5, 5])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 13:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag409')" href="javascript:;">
sonnet-2.0.0/sonnet/src/conv_test.py: 193-206
</a>
<div class="mid" id="frag409" style="display:none"><pre>
  def testUnknownChannels(self, autograph):
    x = tf.TensorSpec([3, 3, 3, None], dtype=tf.float32)

    c = conv.ConvND(
        num_spatial_dims=2,
        output_channels=1,
        kernel_shape=3,
        data_format="NHWC")
    defun_conv = tf.function(c, autograph=autograph)

    with self.assertRaisesRegex(ValueError,
                                "The number of input channels must be known"):
      defun_conv.get_concrete_function(x)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag577')" href="javascript:;">
sonnet-2.0.0/sonnet/src/conv_transpose_test.py: 133-146
</a>
<div class="mid" id="frag577" style="display:none"><pre>
  def testUnknownChannels(self, autograph):
    x = tf.TensorSpec([3, 3, 3, None], dtype=tf.float32)

    c = conv_transpose.ConvNDTranspose(
        num_spatial_dims=2,
        output_channels=1,
        kernel_shape=3,
        data_format="NHWC")
    defun_conv = tf.function(c, autograph=autograph)

    with self.assertRaisesRegex(ValueError,
                                "The number of input channels must be known"):
      defun_conv.get_concrete_function(x)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 14:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag410')" href="javascript:;">
sonnet-2.0.0/sonnet/src/conv_test.py: 207-227
</a>
<div class="mid" id="frag410" style="display:none"><pre>
  def testUnknownSpatialDims(self):
    x = tf.TensorSpec([3, None, None, 3], dtype=tf.float32)

    c = conv.ConvND(
        num_spatial_dims=2,
        output_channels=1,
        kernel_shape=3,
        data_format="NHWC")
    defun_conv = tf.function(c).get_concrete_function(x)

    out = defun_conv(tf.ones([3, 5, 5, 3]))
    expected_out = c(tf.ones([3, 5, 5, 3]))
    self.assertEqual(out.shape, [3, 5, 5, 1])
    self.assertAllEqual(self.evaluate(out), self.evaluate(expected_out))

    out = defun_conv(tf.ones([3, 4, 4, 3]))
    expected_out = c(tf.ones([3, 4, 4, 3]))
    self.assertEqual(out.shape, [3, 4, 4, 1])
    self.assertAllEqual(self.evaluate(out), self.evaluate(expected_out))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag758')" href="javascript:;">
sonnet-2.0.0/sonnet/src/depthwise_conv_test.py: 129-145
</a>
<div class="mid" id="frag758" style="display:none"><pre>
  def testUnknownSpatialDims(self):
    x = tf.TensorSpec([3, None, None, 3], dtype=tf.float32)

    c = depthwise_conv.DepthwiseConv2D(
        channel_multiplier=1, kernel_shape=3, data_format="NHWC")
    defun_conv = tf.function(c).get_concrete_function(x)

    out = defun_conv(tf.ones([3, 5, 5, 3]))
    expected_out = c(tf.ones([3, 5, 5, 3]))
    self.assertEqual(out.shape, [3, 5, 5, 3])
    self.assertAllEqual(self.evaluate(out), self.evaluate(expected_out))

    out = defun_conv(tf.ones([3, 4, 4, 3]))
    expected_out = c(tf.ones([3, 4, 4, 3]))
    self.assertEqual(out.shape, [3, 4, 4, 3])
    self.assertAllEqual(self.evaluate(out), self.evaluate(expected_out))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 15:</b> &nbsp; 12 fragments, nominal size 17 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag411')" href="javascript:;">
sonnet-2.0.0/sonnet/src/conv_test.py: 231-251
</a>
<div class="mid" id="frag411" style="display:none"><pre>
  def testComputationPaddingSame(self, with_bias):
    expected_out = [[4, 6, 6, 6, 4], [6, 9, 9, 9, 6], [6, 9, 9, 9, 6],
                    [6, 9, 9, 9, 6], [4, 6, 6, 6, 4]]
    conv1 = conv.Conv2D(
        output_channels=1,
        kernel_shape=3,
        stride=1,
        padding="SAME",
        with_bias=with_bias,
        **create_constant_initializers(1.0, 1.0, with_bias))

    out = conv1(tf.ones([1, 5, 5, 1], dtype=tf.float32))
    self.assertEqual(out.shape, [1, 5, 5, 1])
    out = tf.squeeze(out, axis=(0, 3))

    expected_out = np.asarray(expected_out, dtype=np.float32)
    if with_bias:
      expected_out += 1

    self.assertAllClose(self.evaluate(out), expected_out)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag579')" href="javascript:;">
sonnet-2.0.0/sonnet/src/conv_transpose_test.py: 175-195
</a>
<div class="mid" id="frag579" style="display:none"><pre>
  def testComputationPaddingSame(self, with_bias):
    expected_out = [[4, 6, 4], [6, 9, 6], [4, 6, 4]]
    expected_out = np.asarray(expected_out, dtype=np.float32)
    if with_bias:
      expected_out += 1

    conv_transpose1 = conv_transpose.Conv2DTranspose(
        output_channels=1,
        output_shape=None,
        kernel_shape=3,
        stride=1,
        padding="SAME",
        with_bias=with_bias,
        **create_constant_initializers(1.0, 1.0, with_bias))

    out = conv_transpose1(tf.ones([1, 3, 3, 1], dtype=tf.float32))
    self.assertEqual(out.shape, [1, 3, 3, 1])
    out = tf.squeeze(out, axis=(0, 3))

    self.assertAllClose(self.evaluate(out), expected_out)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag580')" href="javascript:;">
sonnet-2.0.0/sonnet/src/conv_transpose_test.py: 197-218
</a>
<div class="mid" id="frag580" style="display:none"><pre>
  def testComputationPaddingValid(self, with_bias):
    expected_out = [[1, 2, 3, 2, 1], [2, 4, 6, 4, 2], [3, 6, 9, 6, 3],
                    [2, 4, 6, 4, 2], [1, 2, 3, 2, 1]]
    expected_out = np.asarray(expected_out, dtype=np.float32)
    if with_bias:
      expected_out += 1

    conv1 = conv_transpose.Conv2DTranspose(
        output_channels=1,
        output_shape=None,
        kernel_shape=3,
        stride=1,
        padding="VALID",
        with_bias=with_bias,
        **create_constant_initializers(1.0, 1.0, with_bias))

    out = conv1(tf.ones([1, 3, 3, 1], dtype=tf.float32))
    self.assertEqual(out.shape, [1, 5, 5, 1])
    out = tf.squeeze(out, axis=(0, 3))

    self.assertAllClose(self.evaluate(out), expected_out)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag413')" href="javascript:;">
sonnet-2.0.0/sonnet/src/conv_test.py: 277-296
</a>
<div class="mid" id="frag413" style="display:none"><pre>
  def testComputationPaddingSame(self, with_bias):
    expected_out = [2, 3, 3, 3, 2]
    conv1 = conv.Conv1D(
        output_channels=1,
        kernel_shape=3,
        stride=1,
        padding="SAME",
        with_bias=with_bias,
        **create_constant_initializers(1.0, 1.0, with_bias))

    out = conv1(tf.ones([1, 5, 1], dtype=tf.float32))
    self.assertEqual(out.shape, [1, 5, 1])
    out = tf.squeeze(out, axis=(0, 2))

    expected_out = np.asarray(expected_out, dtype=np.float32)
    if with_bias:
      expected_out += 1

    self.assertAllClose(self.evaluate(out), expected_out)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag412')" href="javascript:;">
sonnet-2.0.0/sonnet/src/conv_test.py: 253-273
</a>
<div class="mid" id="frag412" style="display:none"><pre>
  def testComputationPaddingValid(self, with_bias):
    expected_out = [[9, 9, 9], [9, 9, 9], [9, 9, 9]]
    conv1 = conv.Conv2D(
        output_channels=1,
        kernel_shape=3,
        stride=1,
        padding="VALID",
        with_bias=with_bias,
        **create_constant_initializers(1.0, 1.0, with_bias))

    out = conv1(tf.ones([1, 5, 5, 1], dtype=tf.float32))
    self.assertEqual(out.shape, [1, 3, 3, 1])
    out = tf.squeeze(out, axis=(0, 3))

    expected_out = np.asarray(expected_out, dtype=np.float32)
    if with_bias:
      expected_out += 1

    self.assertAllClose(self.evaluate(out), expected_out)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag582')" href="javascript:;">
sonnet-2.0.0/sonnet/src/conv_transpose_test.py: 236-256
</a>
<div class="mid" id="frag582" style="display:none"><pre>
  def testComputationPaddingSame(self, with_bias):
    expected_out = [2, 3, 2]
    expected_out = np.asarray(expected_out, dtype=np.float32)
    if with_bias:
      expected_out += 1

    conv1 = conv_transpose.Conv1DTranspose(
        output_channels=1,
        output_shape=None,
        kernel_shape=3,
        stride=1,
        padding="SAME",
        with_bias=with_bias,
        **create_constant_initializers(1.0, 1.0, with_bias))

    out = conv1(tf.ones([1, 3, 1], dtype=tf.float32))
    self.assertEqual(out.shape, [1, 3, 1])
    out = tf.squeeze(out, axis=(0, 2))

    self.assertAllClose(self.evaluate(out), expected_out)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag584')" href="javascript:;">
sonnet-2.0.0/sonnet/src/conv_transpose_test.py: 283-305
</a>
<div class="mid" id="frag584" style="display:none"><pre>
  def testComputationPaddingSame(self, with_bias):
    expected_out = np.asarray([
        8, 12, 8, 12, 18, 12, 8, 12, 8, 12, 18, 12, 18, 27, 18, 12, 18, 12, 8,
        12, 8, 12, 18, 12, 8, 12, 8
    ]).reshape((3, 3, 3))
    if with_bias:
      expected_out += 1

    conv_transpose1 = conv_transpose.Conv3DTranspose(
        output_channels=1,
        output_shape=None,
        kernel_shape=3,
        stride=1,
        padding="SAME",
        with_bias=with_bias,
        **create_constant_initializers(1.0, 1.0, with_bias))

    out = conv_transpose1(tf.ones([1, 3, 3, 3, 1], dtype=tf.float32))
    self.assertEqual(out.shape, [1, 3, 3, 3, 1])
    out = tf.squeeze(out, axis=(0, 4))

    self.assertAllClose(self.evaluate(out), expected_out)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag583')" href="javascript:;">
sonnet-2.0.0/sonnet/src/conv_transpose_test.py: 258-279
</a>
<div class="mid" id="frag583" style="display:none"><pre>
  def testComputationPaddingValid(self, with_bias):
    expected_out = [1, 2, 3, 2, 1]
    expected_out = np.asarray(expected_out, dtype=np.float32)
    if with_bias:
      expected_out += 1

    conv1 = conv_transpose.Conv1DTranspose(
        output_channels=1,
        output_shape=None,
        kernel_shape=3,
        stride=1,
        padding="VALID",
        with_bias=with_bias,
        **create_constant_initializers(1.0, 1.0, with_bias))

    out = conv1(tf.ones([1, 3, 1], dtype=tf.float32))
    self.assertEqual(out.shape, [1, 5, 1])
    out = tf.squeeze(out, axis=(0, 2))

    self.assertAllClose(self.evaluate(out), expected_out)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag414')" href="javascript:;">
sonnet-2.0.0/sonnet/src/conv_test.py: 298-318
</a>
<div class="mid" id="frag414" style="display:none"><pre>
  def testComputationPaddingValid(self, with_bias):
    expected_out = [3, 3, 3]
    expected_out = np.asarray(expected_out, dtype=np.float32)
    if with_bias:
      expected_out += 1

    conv1 = conv.Conv1D(
        output_channels=1,
        kernel_shape=3,
        stride=1,
        padding="VALID",
        with_bias=with_bias,
        **create_constant_initializers(1.0, 1.0, with_bias))

    out = conv1(tf.ones([1, 5, 1], dtype=tf.float32))
    self.assertEqual(out.shape, [1, 3, 1])
    out = tf.squeeze(out, axis=(0, 2))

    self.assertAllClose(self.evaluate(out), expected_out)


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag585')" href="javascript:;">
sonnet-2.0.0/sonnet/src/conv_transpose_test.py: 307-334
</a>
<div class="mid" id="frag585" style="display:none"><pre>
  def testComputationPaddingValid(self, with_bias):
    expected_out = np.asarray([
        1, 2, 3, 2, 1, 2, 4, 6, 4, 2, 3, 6, 9, 6, 3, 2, 4, 6, 4, 2, 1, 2, 3, 2,
        1, 2, 4, 6, 4, 2, 4, 8, 12, 8, 4, 6, 12, 18, 12, 6, 4, 8, 12, 8, 4, 2,
        4, 6, 4, 2, 3, 6, 9, 6, 3, 6, 12, 18, 12, 6, 9, 18, 27, 18, 9, 6, 12,
        18, 12, 6, 3, 6, 9, 6, 3, 2, 4, 6, 4, 2, 4, 8, 12, 8, 4, 6, 12, 18, 12,
        6, 4, 8, 12, 8, 4, 2, 4, 6, 4, 2, 1, 2, 3, 2, 1, 2, 4, 6, 4, 2, 3, 6, 9,
        6, 3, 2, 4, 6, 4, 2, 1, 2, 3, 2, 1.
    ]).reshape((5, 5, 5))
    if with_bias:
      expected_out += 1

    conv1 = conv_transpose.Conv3DTranspose(
        output_channels=1,
        output_shape=None,
        kernel_shape=3,
        stride=1,
        padding="VALID",
        with_bias=with_bias,
        **create_constant_initializers(1.0, 1.0, with_bias))

    out = conv1(tf.ones([1, 3, 3, 3, 1], dtype=tf.float32))
    self.assertEqual(out.shape, [1, 5, 5, 5, 1])
    out = tf.squeeze(out, axis=(0, 4))

    self.assertAllClose(self.evaluate(out), expected_out)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag416')" href="javascript:;">
sonnet-2.0.0/sonnet/src/conv_test.py: 350-372
</a>
<div class="mid" id="frag416" style="display:none"><pre>
  def testComputationPaddingValid(self, with_bias):
    expected_out = np.asarray([
        28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,
        28, 28, 28, 28, 28, 28, 28, 28, 28
    ]).reshape((3, 3, 3))
    if not with_bias:
      expected_out -= 1

    conv1 = conv.Conv3D(
        output_channels=1,
        kernel_shape=3,
        stride=1,
        padding="VALID",
        with_bias=with_bias,
        **create_constant_initializers(1.0, 1.0, with_bias))

    out = conv1(tf.ones([1, 5, 5, 5, 1], dtype=tf.float32))
    self.assertEqual(out.shape, [1, 3, 3, 3, 1])
    out = tf.squeeze(out, axis=(0, 4))

    self.assertAllClose(self.evaluate(out), expected_out)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag415')" href="javascript:;">
sonnet-2.0.0/sonnet/src/conv_test.py: 322-348
</a>
<div class="mid" id="frag415" style="display:none"><pre>
  def testComputationPaddingSame(self, with_bias):
    expected_out = np.asarray([
        9, 13, 13, 13, 9, 13, 19, 19, 19, 13, 13, 19, 19, 19, 13, 13, 19, 19,
        19, 13, 9, 13, 13, 13, 9, 13, 19, 19, 19, 13, 19, 28, 28, 28, 19, 19,
        28, 28, 28, 19, 19, 28, 28, 28, 19, 13, 19, 19, 19, 13, 13, 19, 19, 19,
        13, 19, 28, 28, 28, 19, 19, 28, 28, 28, 19, 19, 28, 28, 28, 19, 13, 19,
        19, 19, 13, 13, 19, 19, 19, 13, 19, 28, 28, 28, 19, 19, 28, 28, 28, 19,
        19, 28, 28, 28, 19, 13, 19, 19, 19, 13, 9, 13, 13, 13, 9, 13, 19, 19,
        19, 13, 13, 19, 19, 19, 13, 13, 19, 19, 19, 13, 9, 13, 13, 13, 9
    ]).reshape((5, 5, 5))
    if not with_bias:
      expected_out -= 1

    conv1 = conv.Conv3D(
        output_channels=1,
        kernel_shape=3,
        stride=1,
        padding="SAME",
        with_bias=with_bias,
        **create_constant_initializers(1.0, 1.0, with_bias))

    out = conv1(tf.ones([1, 5, 5, 5, 1], dtype=tf.float32))
    self.assertEqual(out.shape, [1, 5, 5, 5, 1])
    out = tf.squeeze(out, axis=(0, 4))

    self.assertAllClose(self.evaluate(out), expected_out)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 16:</b> &nbsp; 4 fragments, nominal size 11 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag441')" href="javascript:;">
sonnet-2.0.0/sonnet/src/group_norm_test.py: 40-53
</a>
<div class="mid" id="frag441" style="display:none"><pre>
  def testSimpleCaseVar(self):
    layer = group_norm.GroupNorm(
        groups=5,
        create_scale=True,
        create_offset=True,
        scale_init=initializers.Constant(0.5),
        offset_init=initializers.Constant(2.0))

    inputs = tf.ones([2, 3, 3, 10])

    outputs = layer(inputs).numpy()
    for x in np.nditer(outputs):
      self.assertEqual(x, 2.0)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag782')" href="javascript:;">
sonnet-2.0.0/sonnet/src/axis_norm_test.py: 52-65
</a>
<div class="mid" id="frag782" style="display:none"><pre>
  def testSimpleCaseNCHWVar(self):
    layer = axis_norm.LayerNorm([1, 2],
                                create_scale=True,
                                create_offset=True,
                                scale_init=initializers.Constant(0.5),
                                offset_init=initializers.Constant(2.0),
                                data_format="NCHW")

    inputs = tf.ones([2, 5, 3, 3])

    outputs = layer(inputs).numpy()
    for x in np.nditer(outputs):
      self.assertEqual(x, 2.0)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag781')" href="javascript:;">
sonnet-2.0.0/sonnet/src/axis_norm_test.py: 39-51
</a>
<div class="mid" id="frag781" style="display:none"><pre>
  def testSimpleCaseVar(self):
    layer = axis_norm.LayerNorm([1, 2],
                                create_scale=True,
                                create_offset=True,
                                scale_init=initializers.Constant(0.5),
                                offset_init=initializers.Constant(2.0))

    inputs = tf.ones([2, 3, 3, 5])

    outputs = layer(inputs).numpy()
    for x in np.nditer(outputs):
      self.assertEqual(x, 2.0)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag442')" href="javascript:;">
sonnet-2.0.0/sonnet/src/group_norm_test.py: 54-68
</a>
<div class="mid" id="frag442" style="display:none"><pre>
  def testSimpleCaseNCHWVar(self):
    layer = group_norm.GroupNorm(
        groups=5,
        create_scale=True,
        create_offset=True,
        scale_init=initializers.Constant(0.5),
        offset_init=initializers.Constant(2.0),
        data_format="NCHW")

    inputs = tf.ones([2, 10, 3, 3])

    outputs = layer(inputs).numpy()
    for x in np.nditer(outputs):
      self.assertEqual(x, 2.0)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 17:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag456')" href="javascript:;">
sonnet-2.0.0/sonnet/src/group_norm_test.py: 215-229
</a>
<div class="mid" id="frag456" style="display:none"><pre>
  def testRankChanges(self):
    layer = group_norm.GroupNorm(
        groups=5, create_scale=False, create_offset=False)

    inputs = tf.ones([2, 3, 3, 5])
    scale = tf.constant(0.5, shape=(5,))
    offset = tf.constant(2.0, shape=(5,))

    layer(inputs, scale, offset)

    with self.assertRaisesRegexp(
        ValueError,
        "The rank of the inputs cannot change between calls, the original"):
      layer(tf.ones([2, 3, 3, 4, 5]), scale, offset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag796')" href="javascript:;">
sonnet-2.0.0/sonnet/src/axis_norm_test.py: 208-221
</a>
<div class="mid" id="frag796" style="display:none"><pre>
  def testRankChanges(self):
    layer = axis_norm.LayerNorm((1, 2), create_scale=False, create_offset=False)

    inputs = tf.ones([2, 3, 3, 5])
    scale = tf.constant(0.5, shape=(5,))
    offset = tf.constant(2.0, shape=(5,))

    layer(inputs, scale, offset)

    with self.assertRaisesRegexp(
        ValueError,
        "The rank of the inputs cannot change between calls, the original"):
      layer(tf.ones([2, 3, 3, 4, 5]), scale, offset)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 18:</b> &nbsp; 2 fragments, nominal size 19 lines, similarity 84%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag459')" href="javascript:;">
sonnet-2.0.0/sonnet/src/group_norm_test.py: 256-279
</a>
<div class="mid" id="frag459" style="display:none"><pre>
  def testBatchSizeAgnostic(self):
    layer = group_norm.GroupNorm(
        groups=5, create_scale=False, create_offset=False)
    inputs_spec = tf.TensorSpec([None, 3, 3, 10], dtype=tf.float32)
    params_spec = tf.TensorSpec([None], dtype=tf.float32)
    function_layer = tf.function(layer).get_concrete_function(
        inputs_spec, params_spec, params_spec)

    scale = tf.constant(0.5, shape=(10,))
    offset = tf.constant(2.0, shape=(10,))

    outputs = function_layer(tf.ones([2, 3, 3, 10]), scale, offset)
    self.assertEqual(outputs.shape, [2, 3, 3, 10])
    for x in np.nditer(outputs):
      self.assertEqual(x, 2.0)

    scale = tf.constant(0.5, shape=(10,))
    offset = tf.constant(2.0, shape=(10,))

    outputs = function_layer(tf.ones([3, 3, 3, 10]), scale, offset)
    self.assertEqual(outputs.shape, [3, 3, 3, 10])
    for x in np.nditer(outputs):
      self.assertEqual(x, 2.0)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag798')" href="javascript:;">
sonnet-2.0.0/sonnet/src/axis_norm_test.py: 235-257
</a>
<div class="mid" id="frag798" style="display:none"><pre>
  def testShapeAgnostic(self):
    layer = axis_norm.LayerNorm((1, 2), create_scale=False, create_offset=False)
    inputs_spec = tf.TensorSpec([None, None, None, None], dtype=tf.float32)
    params_spec = tf.TensorSpec([None], dtype=tf.float32)
    function_layer = tf.function(layer).get_concrete_function(
        inputs_spec, params_spec, params_spec)

    scale = tf.constant(0.5, shape=(5,))
    offset = tf.constant(2.0, shape=(5,))

    outputs = function_layer(tf.ones([2, 3, 3, 5]), scale, offset)
    self.assertEqual(outputs.shape, [2, 3, 3, 5])
    for x in np.nditer(outputs):
      self.assertEqual(x, 2.0)

    scale = tf.constant(0.5, shape=(3,))
    offset = tf.constant(2.0, shape=(3,))

    outputs = function_layer(tf.ones([3, 4, 6, 3]), scale, offset)
    self.assertEqual(outputs.shape, [3, 4, 6, 3])
    for x in np.nditer(outputs):
      self.assertEqual(x, 2.0)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 19:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag465')" href="javascript:;">
sonnet-2.0.0/sonnet/src/linear.py: 36-63
</a>
<div class="mid" id="frag465" style="display:none"><pre>
  def __init__(self,
               output_size: int,
               with_bias: bool = True,
               w_init: Optional[initializers.Initializer] = None,
               b_init: Optional[initializers.Initializer] = None,
               name: Optional[Text] = None):
    """Constructs a `Linear` module.

    Args:
      output_size: Output dimensionality.
      with_bias: Whether to include bias parameters. Default `True`.
      w_init: Optional initializer for the weights. By default the weights are
        initialized truncated random normal values with a standard deviation of
        `1 / sqrt(input_feature_size)`, which is commonly used when the inputs
        are zero centered (see https://arxiv.org/abs/1502.03167v3).
      b_init: Optional initializer for the bias. By default the bias is
        initialized to zero.
      name: Name of the module.
    """
    super(Linear, self).__init__(name=name)
    self.output_size = output_size
    self.with_bias = with_bias
    self.w_init = w_init
    if with_bias:
      self.b_init = b_init if b_init is not None else initializers.Zeros()
    elif b_init is not None:
      raise ValueError("When not using a bias the b_init must be None.")

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag805')" href="javascript:;">
sonnet-2.0.0/sonnet/src/parallel_linear.py: 44-71
</a>
<div class="mid" id="frag805" style="display:none"><pre>
  def __init__(self,
               output_size: int,
               with_bias: bool = True,
               w_init: Optional[initializers.Initializer] = None,
               b_init: Optional[initializers.Initializer] = None,
               name: Optional[Text] = None):
    """Constructs a `ParallelLinear` module.

    Args:
      output_size: Output dimensionality.
      with_bias: Whether to include bias parameters. Default `True`.
      w_init: Optional initializer for the weights. By default the weights are
        initialized truncated random normal values with a standard deviation of
        `1 / sqrt(input_feature_size)`, which is commonly used when the inputs
        are zero centered (see https://arxiv.org/abs/1502.03167v3).
      b_init: Optional initializer for the bias. By default the bias is
        initialized to zero.
      name: Name of the module.
    """
    super(ParallelLinears, self).__init__(name=name)
    self.output_size = output_size
    self.with_bias = with_bias
    self.w_init = w_init
    if with_bias:
      self.b_init = b_init if b_init is not None else initializers.Zeros()
    elif b_init is not None:
      raise ValueError("When not using a bias the b_init must be None.")

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 20:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 93%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag488')" href="javascript:;">
sonnet-2.0.0/sonnet/src/nets/dnc/util_test.py: 33-50
</a>
<div class="mid" id="frag488" style="display:none"><pre>
  def testShape(self, initial_shape, final_shape):
    first_shape = tf.TensorShape([3, 3])
    second_shape = tf.TensorShape([5])
    segment_shapes = [first_shape, second_shape]

    inputs_shape = (
        initial_shape +
        [first_shape.num_elements() + second_shape.num_elements()] +
        final_shape)

    inputs = tf.random.uniform(inputs_shape)
    first, second = util.segment_dim(
        inputs, dim=len(initial_shape), shapes=segment_shapes)
    self.assertAllEqual(first.shape.as_list(),
                        initial_shape + first_shape.as_list() + final_shape)
    self.assertAllEqual(second.shape.as_list(),
                        initial_shape + second_shape.as_list() + final_shape)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag489')" href="javascript:;">
sonnet-2.0.0/sonnet/src/nets/dnc/util_test.py: 53-70
</a>
<div class="mid" id="frag489" style="display:none"><pre>
  def testShapeNegative(self, initial_shape, final_shape):
    first_shape = tf.TensorShape([3, 3])
    second_shape = tf.TensorShape([5])
    segment_shapes = [first_shape, second_shape]

    inputs_shape = (
        initial_shape +
        [first_shape.num_elements() + second_shape.num_elements()] +
        final_shape)

    inputs = tf.random.uniform(inputs_shape)
    first, second = util.segment_dim(
        inputs, dim=-len(final_shape) - 1, shapes=segment_shapes)
    self.assertAllEqual(first.shape.as_list(),
                        initial_shape + first_shape.as_list() + final_shape)
    self.assertAllEqual(second.shape.as_list(),
                        initial_shape + second_shape.as_list() + final_shape)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 21:</b> &nbsp; 5 fragments, nominal size 11 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag499')" href="javascript:;">
sonnet-2.0.0/sonnet/src/nets/dnc/write_test.py: 28-40
</a>
<div class="mid" id="frag499" style="display:none"><pre>
  def testShape(self):
    batch_size = 16
    num_writes = 2
    memory_size = 5
    word_size = 3

    mem = tf.random.uniform([batch_size, memory_size, word_size])
    write_address = tf.random.uniform([batch_size, num_writes, memory_size])
    reset_row_weights = tf.random.uniform([batch_size, num_writes])
    eraser = write.erase_rows(mem, write_address, reset_row_weights)
    self.assertAllEqual(eraser.shape.as_list(),
                        [batch_size, memory_size, word_size])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag505')" href="javascript:;">
sonnet-2.0.0/sonnet/src/nets/dnc/write_test.py: 178-190
</a>
<div class="mid" id="frag505" style="display:none"><pre>
  def testShape(self):
    batch_size = 4
    num_writes = 2
    memory_size = 5
    word_size = 3

    mem = tf.random.uniform([batch_size, memory_size, word_size])
    write_address = tf.random.uniform([batch_size, num_writes, memory_size])
    values = tf.random.uniform([batch_size, num_writes, word_size])
    writer = write.additive_write(mem, write_address, values)
    self.assertAllEqual(writer.shape.as_list(),
                        [batch_size, memory_size, word_size])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag526')" href="javascript:;">
sonnet-2.0.0/sonnet/src/nets/dnc/read_test.py: 28-39
</a>
<div class="mid" id="frag526" style="display:none"><pre>
  def testShape(self):
    batch_size = 4
    num_reads = 2
    memory_size = 5
    word_size = 3

    mem = tf.random.uniform([batch_size, memory_size, word_size])
    weights = tf.random.uniform([batch_size, num_reads, memory_size])
    values_read = read.read(mem, weights)
    self.assertAllEqual(values_read.shape.as_list(),
                        [batch_size, num_reads, word_size])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag501')" href="javascript:;">
sonnet-2.0.0/sonnet/src/nets/dnc/write_test.py: 84-96
</a>
<div class="mid" id="frag501" style="display:none"><pre>
  def testShape(self):
    batch_size = 1
    num_writes = 2
    memory_size = 5
    word_size = 3

    mem = tf.random.uniform([batch_size, memory_size, word_size])
    write_address = tf.random.uniform([batch_size, num_writes, memory_size])
    reset_weights = tf.random.uniform([batch_size, num_writes, word_size])
    writer = write.erase(mem, write_address, reset_weights)
    self.assertTrue(writer.shape.as_list(),
                    [batch_size, memory_size, word_size])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag503')" href="javascript:;">
sonnet-2.0.0/sonnet/src/nets/dnc/write_test.py: 137-150
</a>
<div class="mid" id="frag503" style="display:none"><pre>
  def testShape(self):
    batch_size = 4
    num_writes = 2
    memory_size = 5
    word_size = 3

    mem = tf.random.uniform([batch_size, memory_size, word_size])
    write_address = tf.random.uniform([batch_size, num_writes, memory_size])
    reset_weights = tf.random.uniform([batch_size, num_writes, word_size])
    values = tf.random.uniform([batch_size, num_writes, word_size])
    writer = write.erase_and_write(mem, write_address, reset_weights, values)
    self.assertTrue(writer.shape.as_list(),
                    [batch_size, memory_size, word_size])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 22:</b> &nbsp; 2 fragments, nominal size 25 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag500')" href="javascript:;">
sonnet-2.0.0/sonnet/src/nets/dnc/write_test.py: 41-81
</a>
<div class="mid" id="frag500" style="display:none"><pre>
  def testValues(self):
    num_writes = 2
    memory_size = 5
    word_size = 3

    # Random memory, weights and values (batch_size=1)
    mem = tf.random.uniform((1, memory_size, word_size))
    mem_np = mem.numpy()
    # Non-repeated indices in [0, memory_size)
    perm = np.random.permutation(memory_size)
    indices_np = perm[:num_writes]
    excluded_indices_np = perm[num_writes:]

    # One-hot representation
    write_address = tf.constant(
        np.expand_dims(np.eye(memory_size)[indices_np], axis=0),
        dtype=tf.float32)
    reset_row_weights = tf.ones((1, num_writes))

    erased_mem = write.erase_rows(mem, write_address, reset_row_weights)

    not_erased_mem = write.erase_rows(mem, write_address, reset_row_weights * 0)

    erased_mem_np = erased_mem.numpy()

    # Rows specified in indices should have been erased.
    self.assertAllClose(
        erased_mem_np[0, indices_np, :],
        np.zeros((num_writes, word_size)),
        atol=2e-3)

    # Other rows should not have been erased.
    self.assertAllClose(
        erased_mem_np[0, excluded_indices_np, :],
        mem_np[0, excluded_indices_np, :],
        atol=2e-3)

    # Write with reset weights zero'd out and nothing should change.
    self.assertAllEqual(not_erased_mem.numpy(), mem_np)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag502')" href="javascript:;">
sonnet-2.0.0/sonnet/src/nets/dnc/write_test.py: 97-134
</a>
<div class="mid" id="frag502" style="display:none"><pre>
  def testValues(self):
    num_writes = 2
    memory_size = 5
    word_size = 3

    # Random memory, weights and values (batch_size=1)
    mem = tf.random.uniform([1, memory_size, word_size])
    mem_np = mem.numpy()
    # Non-repeated indices in [0, memory_size)
    perm = np.random.permutation(memory_size)
    indices = perm[:num_writes]
    excluded_indices = perm[num_writes:]
    # One-hot representation
    write_address = tf.constant(
        np.expand_dims(np.eye(memory_size)[indices], axis=0), dtype=tf.float32)
    reset_weights = tf.ones([1, num_writes, word_size])
    erased_mem = write.erase(mem, write_address, reset_weights)
    not_erased_mem = write.erase(mem, write_address, reset_weights * 0.)

    erased_mem_np = erased_mem.numpy()
    not_erased_mem_np = not_erased_mem.numpy()

    # Rows specified in indices should have been erased.
    self.assertAllClose(
        erased_mem_np[0, indices, :],
        np.zeros((num_writes, word_size)),
        atol=2e-3)

    # Other rows should not have been erased.
    self.assertAllClose(
        erased_mem_np[0, excluded_indices, :],
        mem_np[0, excluded_indices, :],
        atol=2e-3)

    # Write with reset weights zero'd out and nothing should change.
    self.assertAllEqual(not_erased_mem_np, mem_np)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 23:</b> &nbsp; 2 fragments, nominal size 50 lines, similarity 74%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag531')" href="javascript:;">
sonnet-2.0.0/sonnet/src/nets/resnet.py: 36-95
</a>
<div class="mid" id="frag531" style="display:none"><pre>
  def __init__(self,
               channels: int,
               stride: Union[int, Sequence[int]],
               use_projection: bool,
               bn_config: Mapping[Text, float],
               name: Optional[Text] = None):
    super(BottleNeckBlockV1, self).__init__(name=name)
    self._channels = channels
    self._stride = stride
    self._use_projection = use_projection
    self._bn_config = bn_config

    batchnorm_args = {"create_scale": True, "create_offset": True}
    batchnorm_args.update(bn_config)

    if self._use_projection:
      self._proj_conv = conv.Conv2D(
          output_channels=channels,
          kernel_shape=1,
          stride=stride,
          with_bias=False,
          padding=pad.same,
          name="shortcut_conv")
      self._proj_batchnorm = batch_norm.BatchNorm(
          name="shortcut_batchnorm", **batchnorm_args)

    self._layers = []
    conv_0 = conv.Conv2D(
        output_channels=channels // 4,
        kernel_shape=1,
        stride=1,
        with_bias=False,
        padding=pad.same,
        name="conv_0")
    self._layers.append(
        [conv_0,
         batch_norm.BatchNorm(name="batchnorm_0", **batchnorm_args)])

    conv_1 = conv.Conv2D(
        output_channels=channels // 4,
        kernel_shape=3,
        stride=stride,
        with_bias=False,
        padding=pad.same,
        name="conv_1")
    self._layers.append(
        [conv_1,
         batch_norm.BatchNorm(name="batchnorm_1", **batchnorm_args)])

    conv_2 = conv.Conv2D(
        output_channels=channels,
        kernel_shape=1,
        stride=1,
        with_bias=False,
        padding=pad.same,
        name="conv_2")
    batchnorm_2 = batch_norm.BatchNorm(
        name="batchnorm_2", scale_init=initializers.Zeros(), **batchnorm_args)
    self._layers.append([conv_2, batchnorm_2])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag533')" href="javascript:;">
sonnet-2.0.0/sonnet/src/nets/resnet.py: 115-170
</a>
<div class="mid" id="frag533" style="display:none"><pre>
  def __init__(self,
               channels: int,
               stride: Union[int, Sequence[int]],
               use_projection: bool,
               bn_config: Mapping[Text, float],
               name: Optional[Text] = None):
    super(BottleNeckBlockV2, self).__init__(name=name)
    self._channels = channels
    self._stride = stride
    self._use_projection = use_projection
    self._bn_config = bn_config

    batchnorm_args = {"create_scale": True, "create_offset": True}
    batchnorm_args.update(bn_config)

    if self._use_projection:
      self._proj_conv = conv.Conv2D(
          output_channels=channels,
          kernel_shape=1,
          stride=stride,
          with_bias=False,
          padding=pad.same,
          name="shortcut_conv")

    self._conv_0 = conv.Conv2D(
        output_channels=channels // 4,
        kernel_shape=1,
        stride=1,
        with_bias=False,
        padding=pad.same,
        name="conv_0")

    self._bn_0 = batch_norm.BatchNorm(name="batchnorm_0", **batchnorm_args)

    self._conv_1 = conv.Conv2D(
        output_channels=channels // 4,
        kernel_shape=3,
        stride=stride,
        with_bias=False,
        padding=pad.same,
        name="conv_1")

    self._bn_1 = batch_norm.BatchNorm(name="batchnorm_1", **batchnorm_args)

    self._conv_2 = conv.Conv2D(
        output_channels=channels,
        kernel_shape=1,
        stride=1,
        with_bias=False,
        padding=pad.same,
        name="conv_2")

    # NOTE: Some implementations of ResNet50 v2 suggest initializing gamma/scale
    # here to zeros.
    self._bn_2 = batch_norm.BatchNorm(name="batchnorm_2", **batchnorm_args)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 24:</b> &nbsp; 4 fragments, nominal size 11 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag626')" href="javascript:;">
sonnet-2.0.0/sonnet/src/recurrent_test.py: 70-83
</a>
<div class="mid" id="frag626" style="display:none"><pre>
  def testInitialization(self):
    core = recurrent.VanillaRNN(
        hidden_size=self.hidden_size,
        w_i_init=initializers.Ones(),
        w_h_init=initializers.Ones(),
        b_init=initializers.Ones())
    inputs = tf.random.uniform([self.batch_size, self.input_size])
    prev_state = core.initial_state(self.batch_size)
    core(inputs, prev_state)

    for v in core.variables:
      self.assertAllClose(self.evaluate(v), self.evaluate(tf.ones_like(v)))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag654')" href="javascript:;">
sonnet-2.0.0/sonnet/src/recurrent_test.py: 486-499
</a>
<div class="mid" id="frag654" style="display:none"><pre>
  def testInitialization(self):
    core = recurrent.GRU(
        hidden_size=self.hidden_size,
        w_i_init=initializers.Ones(),
        w_h_init=initializers.Ones(),
        b_init=initializers.Ones())
    inputs = tf.random.uniform([self.batch_size, self.input_size])
    prev_state = core.initial_state(self.batch_size)
    core(inputs, prev_state)

    for v in core.variables:
      self.assertAllClose(self.evaluate(v), self.evaluate(tf.ones_like(v)))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag645')" href="javascript:;">
sonnet-2.0.0/sonnet/src/recurrent_test.py: 337-351
</a>
<div class="mid" id="frag645" style="display:none"><pre>
  def testInitialization(self):
    unrolled_lstm = recurrent.UnrolledLSTM(
        hidden_size=self.hidden_size,
        forget_bias=0.0,
        w_i_init=initializers.Ones(),
        w_h_init=initializers.Ones(),
        b_init=initializers.Ones())
    input_sequence = tf.random.uniform([1, self.batch_size, self.input_size])
    initial_state = unrolled_lstm.initial_state(self.batch_size)
    unrolled_lstm(input_sequence, initial_state)

    for v in unrolled_lstm.variables:
      self.assertAllClose(self.evaluate(v), self.evaluate(tf.ones_like(v)))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag660')" href="javascript:;">
sonnet-2.0.0/sonnet/src/recurrent_test.py: 563-576
</a>
<div class="mid" id="frag660" style="display:none"><pre>
  def testInitialization(self):
    core = recurrent.CuDNNGRU(
        hidden_size=self.hidden_size,
        w_i_init=initializers.Ones(),
        w_h_init=initializers.Ones(),
        b_init=initializers.Ones())
    inputs = tf.random.uniform([1, self.batch_size, self.input_size])
    prev_state = core.initial_state(self.batch_size)
    core(inputs, prev_state)

    for v in core.variables:
      self.assertAllClose(self.evaluate(v), self.evaluate(tf.ones_like(v)))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 25:</b> &nbsp; 3 fragments, nominal size 13 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag678')" href="javascript:;">
sonnet-2.0.0/sonnet/src/recurrent_test.py: 782-797
</a>
<div class="mid" id="frag678" style="display:none"><pre>
  def testVariableLengthOneZeroLength(self, use_tf_function, unroll_fn):
    if use_tf_function:
      unroll_fn = tf.function(unroll_fn)

    sequence_length = tf.constant([0] + [self.num_steps] *
                                  (self.batch_size - 1))
    initial_state = self.core.initial_state(self.batch_size)
    input_sequence = tf.random.uniform([self.num_steps, self.batch_size, 1])
    output_sequence, _ = unroll_fn(
        self.core,
        input_sequence,
        initial_state,
        sequence_length=sequence_length)

    self.assertConsistentWithLength(output_sequence, sequence_length)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag679')" href="javascript:;">
sonnet-2.0.0/sonnet/src/recurrent_test.py: 798-812
</a>
<div class="mid" id="frag679" style="display:none"><pre>
  def testVariableLengthRange(self, use_tf_function, unroll_fn):
    if use_tf_function:
      unroll_fn = tf.function(unroll_fn)

    sequence_length = tf.range(self.batch_size)
    initial_state = self.core.initial_state(self.batch_size)
    input_sequence = tf.random.uniform([self.num_steps, self.batch_size, 1])
    output_sequence, _ = unroll_fn(
        self.core,
        input_sequence,
        initial_state,
        sequence_length=sequence_length)

    self.assertConsistentWithLength(output_sequence, sequence_length)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag681')" href="javascript:;">
sonnet-2.0.0/sonnet/src/recurrent_test.py: 823-838
</a>
<div class="mid" id="frag681" style="display:none"><pre>
  def testVariableLengthAllFull(self, use_tf_function, unroll_fn):
    if use_tf_function:
      unroll_fn = tf.function(unroll_fn)

    initial_state = self.core.initial_state(self.batch_size)
    input_sequence = tf.random.uniform([self.num_steps, self.batch_size, 1])
    output_sequence, final_state = unroll_fn(
        self.core,
        input_sequence,
        initial_state,
        sequence_length=tf.constant([self.num_steps] * self.batch_size))
    expected_output_sequence, expected_final_state = unroll_fn(
        self.core, input_sequence, initial_state)
    self.assertAllClose(output_sequence, expected_output_sequence)
    self.assertAllClose(final_state, expected_final_state)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 26:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag721')" href="javascript:;">
sonnet-2.0.0/sonnet/src/mixed_precision_test.py: 84-96
</a>
<div class="mid" id="frag721" style="display:none"><pre>
  def test_float16_mode_disable_class(self, test_class):
    mixed_precision.enable(tf.float32)

    x = tf.Variable([[1., 9.], [5., 0.]])
    d = test_class(x)
    d.check_type = mixed_precision.modes([tf.float32, tf.float16])(d.check_type)

    mixed_precision.enable(tf.float16)
    self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)
    self.assertEqual(d.check_type(x, tf.float16).dtype, tf.float32)
    mixed_precision.disable()
    self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag735')" href="javascript:;">
sonnet-2.0.0/sonnet/src/mixed_precision_test.py: 253-266
</a>
<div class="mid" id="frag735" style="display:none"><pre>
  def test_scoping_disable(self, test_class):
    mixed_precision.enable(tf.float32)

    x = tf.Variable([[1., 9.], [8., 9.]])
    d = test_class(x)
    d.check_type = mixed_precision.modes([tf.float32, tf.float16])(d.check_type)

    with mixed_precision.scope(tf.float16):
      self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)
      self.assertEqual(d.check_type(x, tf.float16).dtype, tf.float32)

      mixed_precision.disable()
      self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 27:</b> &nbsp; 3 fragments, nominal size 14 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag723')" href="javascript:;">
sonnet-2.0.0/sonnet/src/mixed_precision_test.py: 120-136
</a>
<div class="mid" id="frag723" style="display:none"><pre>
  def test_float16_mode_eligible_multiple_instances_class(self, test_class):
    mixed_precision.enable(tf.float32)

    x = tf.Variable([[1., 9.], [5., 0.]])
    d = test_class(x)
    d.check_type = mixed_precision.modes([tf.float32, tf.float16])(d.check_type)

    d2 = test_class(x)
    d2.check_type = mixed_precision.modes([tf.float32, tf.float16])(
        d2.check_type)

    mixed_precision.enable(tf.float16)
    self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)
    self.assertEqual(d.check_type(x, tf.float16).dtype, tf.float32)
    self.assertEqual(d2.check_type(x, tf.float32).dtype, tf.float32)
    self.assertEqual(d2.check_type(x, tf.float16).dtype, tf.float32)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag724')" href="javascript:;">
sonnet-2.0.0/sonnet/src/mixed_precision_test.py: 137-154
</a>
<div class="mid" id="frag724" style="display:none"><pre>
  def test_float16_mode_ineligible_multiple_instances_class(self, test_class):
    mixed_precision.enable(tf.float32)

    x = tf.Variable([[1., 9.], [5., 0.]])
    d = test_class(x)
    d.check_type = mixed_precision.modes([tf.float32, tf.bfloat16])(
        d.check_type)

    d2 = test_class(x)
    d2.check_type = mixed_precision.modes([tf.float32, tf.bfloat16])(
        d2.check_type)

    mixed_precision.enable(tf.float16)
    self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)
    self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)
    self.assertEqual(d2.check_type(x, tf.float32).dtype, tf.float32)
    self.assertEqual(d2.check_type(x, tf.float32).dtype, tf.float32)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag725')" href="javascript:;">
sonnet-2.0.0/sonnet/src/mixed_precision_test.py: 155-173
</a>
<div class="mid" id="frag725" style="display:none"><pre>
  def test_float16_mode_multiple_instances_different_eligibility_class(
      self, test_class):
    mixed_precision.enable(tf.float32)

    x = tf.Variable([[1., 9.], [5., 0.]])
    d = test_class(x)
    d.check_type = mixed_precision.modes([tf.float32, tf.bfloat16])(
        d.check_type)

    d2 = test_class(x)
    d2.check_type = mixed_precision.modes([tf.float32, tf.float16])(
        d2.check_type)

    mixed_precision.enable(tf.float16)
    self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)
    self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)
    self.assertEqual(d2.check_type(x, tf.float32).dtype, tf.float32)
    self.assertEqual(d2.check_type(x, tf.float16).dtype, tf.float32)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 28:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag728')" href="javascript:;">
sonnet-2.0.0/sonnet/src/mixed_precision_test.py: 198-212
</a>
<div class="mid" id="frag728" style="display:none"><pre>
  def test_function_create_module_eligible(self, test_class):
    mixed_precision.enable(tf.float16)

    @mixed_precision.modes([tf.float32, tf.float16])
    def model():
      x = tf.Variable([[1., 9.], [8., 9.]])
      d = test_class(x)
      d.check_type = mixed_precision.modes([tf.float32, tf.float16])(
          d.check_type)

      self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)
      self.assertEqual(d.check_type(x, tf.float16).dtype, tf.float32)

    model()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag730')" href="javascript:;">
sonnet-2.0.0/sonnet/src/mixed_precision_test.py: 213-227
</a>
<div class="mid" id="frag730" style="display:none"><pre>
  def test_function_create_module_ineligible(self, test_class):
    mixed_precision.enable(tf.float16)

    @mixed_precision.modes([tf.float32, tf.float16])
    def model():
      x = tf.Variable([[1., 9.], [8., 9.]])
      d = test_class(x)
      d.check_type = mixed_precision.modes([tf.float32, tf.bfloat16])(
          d.check_type)

      self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)
      self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)

    model()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 29:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag739')" href="javascript:;">
sonnet-2.0.0/sonnet/src/mixed_precision_test.py: 303-316
</a>
<div class="mid" id="frag739" style="display:none"><pre>
  def test_float32_mode_eligible_func(self):
    mixed_precision.enable(tf.float32)
    self.assertEqual(mixed_precision._get_mixed_precision_mode(), tf.float32)

    @mixed_precision.modes([tf.float32, tf.float16])
    def fwd_func(x):
      self.assertEqual(x.dtype, tf.float32)
      return x

    x = tf.Variable([[1., 3], [5., 7.]])
    self.assertEqual(x.dtype, tf.float32)
    self.assertEqual(fwd_func(x).dtype, tf.float32)
    self.assertEqual(fwd_func(x).dtype, tf.float32)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag741')" href="javascript:;">
sonnet-2.0.0/sonnet/src/mixed_precision_test.py: 317-331
</a>
<div class="mid" id="frag741" style="display:none"><pre>
  def test_float16_mode_ineligible_func(self):
    mixed_precision.enable(tf.float32)

    @mixed_precision.modes([tf.float32, tf.bfloat16])
    def fwd_func(x):
      self.assertEqual(x.dtype, tf.float32)
      return x

    x = tf.Variable([[1., 3], [5., 7.]])
    self.assertEqual(x.dtype, tf.float32)

    mixed_precision.enable(tf.float16)
    self.assertEqual(fwd_func(x).dtype, tf.float32)
    self.assertEqual(fwd_func(x).dtype, tf.float32)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 30:</b> &nbsp; 3 fragments, nominal size 14 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag760')" href="javascript:;">
sonnet-2.0.0/sonnet/src/depthwise_conv_test.py: 159-177
</a>
<div class="mid" id="frag760" style="display:none"><pre>
  def testComputationSame(self, with_bias):
    conv1 = depthwise_conv.DepthwiseConv2D(
        channel_multiplier=1,
        kernel_shape=[3, 3],
        stride=1,
        padding="SAME",
        with_bias=with_bias,
        **create_constant_initializers(1.0, 1.0, with_bias))

    out = conv1(tf.ones([1, 5, 5, 1]))
    expected_out = np.array([[5, 7, 7, 7, 5], [7, 10, 10, 10, 7],
                             [7, 10, 10, 10, 7], [7, 10, 10, 10, 7],
                             [5, 7, 7, 7, 5]])
    if not with_bias:
      expected_out -= 1

    self.assertEqual(out.shape, [1, 5, 5, 1])
    self.assertAllClose(np.reshape(out.numpy(), [5, 5]), expected_out)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag761')" href="javascript:;">
sonnet-2.0.0/sonnet/src/depthwise_conv_test.py: 179-195
</a>
<div class="mid" id="frag761" style="display:none"><pre>
  def testComputationValid(self, with_bias):
    conv1 = depthwise_conv.DepthwiseConv2D(
        channel_multiplier=1,
        kernel_shape=[3, 3],
        stride=1,
        padding="VALID",
        with_bias=with_bias,
        **create_constant_initializers(1.0, 1.0, with_bias))

    out = conv1(tf.ones([1, 5, 5, 1]))
    expected_out = np.array([[10, 10, 10], [10, 10, 10], [10, 10, 10]])
    if not with_bias:
      expected_out -= 1

    self.assertEqual(out.shape, [1, 3, 3, 1])
    self.assertAllClose(np.reshape(out.numpy(), [3, 3]), expected_out)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag762')" href="javascript:;">
sonnet-2.0.0/sonnet/src/depthwise_conv_test.py: 197-212
</a>
<div class="mid" id="frag762" style="display:none"><pre>
  def testComputationValidMultiChannel(self, with_bias):
    conv1 = depthwise_conv.DepthwiseConv2D(
        channel_multiplier=1,
        kernel_shape=[3, 3],
        stride=1,
        padding="VALID",
        with_bias=with_bias,
        **create_constant_initializers(1.0, 1.0, with_bias))

    out = conv1(tf.ones([1, 5, 5, 3]))
    expected_out = np.array([[[10] * 3] * 3] * 3)
    if not with_bias:
      expected_out -= 1

    self.assertAllClose(np.reshape(out.numpy(), [3, 3, 3]), expected_out)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 31:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag844')" href="javascript:;">
sonnet-2.0.0/sonnet/src/utils_test.py: 105-120
</a>
<div class="mid" id="frag844" style="display:none"><pre>
  def test_unbound_method(self):

    @utils.decorator
    def double(wrapped, instance, args, kwargs):
      self.assertIs(instance, o)
      return 2 * wrapped(*args, **kwargs)

    class MyObject(object):

      @double
      def f(self, x, y):
        return x**y

    o = MyObject()
    self.assertEqual(o.f(3, 4), 2 * (3**4))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag847')" href="javascript:;">
sonnet-2.0.0/sonnet/src/utils_test.py: 121-136
</a>
<div class="mid" id="frag847" style="display:none"><pre>
  def test_bound_method(self):

    @utils.decorator
    def double(wrapped, instance, args, kwargs):
      self.assertIs(instance, o)
      return 2 * wrapped(*args, **kwargs)

    class MyObject(object):

      def f(self, x, y):
        return x**y

    o = MyObject()
    self.assertEqual(double(o.f)(3, 4), 2 * (3**4))  # pylint: disable=no-value-for-parameter


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 32:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 86%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag999')" href="javascript:;">
sonnet-2.0.0/sonnet/src/conformance/pickle_test.py: 35-53
</a>
<div class="mid" id="frag999" style="display:none"><pre>
  def test_pickle(self, golden):
    m1 = golden.create_module()
    golden.create_all_variables(m1)
    m2 = pickle.loads(pickle.dumps(m1))
    self.assertIsNot(m1, m2)

    # Check that module variables are recreated with equivalent properties.
    for v1, v2 in zip(m1.variables, m2.variables):
      self.assertIsNot(v1, v2)
      self.assertEqual(v1.name, v2.name)
      self.assertEqual(v1.device, v2.device)
      self.assertAllEqual(v1.read_value(), v2.read_value())

    if golden.deterministic:
      y1 = golden.forward(m1)
      y2 = golden.forward(m2)
      tree.map_structure(self.assertAllEqual, y1, y2)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1005')" href="javascript:;">
sonnet-2.0.0/sonnet/src/conformance/copy_test.py: 34-53
</a>
<div class="mid" id="frag1005" style="display:none"><pre>
  def test_copy(self, golden):
    m1 = golden.create_module()
    golden.create_all_variables(m1)
    m2 = copy.deepcopy(m1)
    self.assertIsNot(m1, m2)

    # Check that module variables are recreated with equivalent properties.
    for v1, v2 in zip(m1.variables, m2.variables):
      self.assertIsNot(v1, v2)
      if tf.version.GIT_VERSION == "unknown":
        # TODO(tomhennigan) Remove version check when TF 2.1 is released.
        self.assertEqual(v1.name, v2.name)
      self.assertEqual(v1.device, v2.device)
      self.assertAllEqual(v1.read_value(), v2.read_value())

    if golden.deterministic:
      y1 = golden.forward(m1)
      y2 = golden.forward(m2)
      tree.map_structure(self.assertAllEqual, y1, y2)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 33:</b> &nbsp; 3 fragments, nominal size 11 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1012')" href="javascript:;">
sonnet-2.0.0/sonnet/src/conformance/function_test.py: 40-50
</a>
<div class="mid" id="frag1012" style="display:none"><pre>
  def test_trace(
      self,
      module_fn: ModuleFn,
      input_shape: Tuple[int],
      dtype: tf.DType,
      autograph: bool,
  ):
    module = module_fn()
    forward = tf.function(module, autograph=autograph)
    forward(tf.ones(input_shape, dtype=dtype))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1013')" href="javascript:;">
sonnet-2.0.0/sonnet/src/conformance/function_test.py: 53-64
</a>
<div class="mid" id="frag1013" style="display:none"><pre>
  def test_create_variables_eagerly(
      self,
      module_fn: ModuleFn,
      input_shape: Tuple[int],
      dtype: tf.DType,
      autograph: bool,
  ):
    module = module_fn()
    f = snt.distribute.create_variables_eagerly(module)
    forward = tf.function(f, autograph=autograph)
    forward(tf.ones(input_shape, dtype=dtype))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1014')" href="javascript:;">
sonnet-2.0.0/sonnet/src/conformance/function_test.py: 67-79
</a>
<div class="mid" id="frag1014" style="display:none"><pre>
  def test_trace_batch_agnostic(
      self,
      module_fn: ModuleFn,
      input_shape: Tuple[int],
      dtype: tf.DType,
      autograph: bool,
  ):
    module = module_fn()
    forward = tf.function(module, autograph=autograph)
    input_spec = tf.TensorSpec((None,) + input_shape[1:], dtype=dtype)
    cf = forward.get_concrete_function(input_spec)
    cf(tf.ones(input_shape, dtype=dtype))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<script language="JavaScript">
function ShowHide(divId) { 
    if(document.getElementById(divId).style.display == 'none') {
        document.getElementById(divId).style.display='block';
    } else { 
        document.getElementById(divId).style.display = 'none';
    } 
}
</script>
</body>
</html>

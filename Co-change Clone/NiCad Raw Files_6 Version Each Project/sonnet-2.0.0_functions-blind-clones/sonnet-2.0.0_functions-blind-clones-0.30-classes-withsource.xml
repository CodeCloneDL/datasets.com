<clones>
<systeminfo processor="nicad6" system="sonnet-2.0.0" granularity="functions-blind" threshold="30%" minlines="10" maxlines="2500"/>
<cloneinfo npcs="1040" npairs="121"/>
<runinfo ncompares="27807" cputime="54924"/>
<classinfo nclasses="33"/>

<class classid="1" nclones="3" nlines="21" similarity="70">
<source file="systems/sonnet-2.0.0/sonnet/src/distribute/batch_norm.py" startline="47" endline="93" pcid="32">
  def __init__(self,
               create_scale: bool,
               create_offset: bool,
               moving_mean: metrics.Metric,
               moving_variance: metrics.Metric,
               eps: types.FloatLike = 1e-5,
               scale_init: Optional[initializers.Initializer] = None,
               offset_init: Optional[initializers.Initializer] = None,
               data_format: Text = "channels_last",
               name: Optional[Text] = None):
    """Constructs a ``CrossReplicaBatchNorm`` module.

    Args:
      create_scale: whether to create a trainable scale per channel applied
        after the normalization.
      create_offset: whether to create a trainable offset per channel applied
        after normalization and scaling.
      moving_mean: An object which keeps track of the moving average of the mean
        which can be used to normalize at test time. This object must have an
        update method which takes a value and updates the internal state and a
        value property which returns the current mean.
      moving_variance: An object which keeps track of the moving average of the
        variance which can be used to normalize at test time. This object must
        have an update method which takes a value and updates the internal state
        and a value property which returns the current variance.
      eps: Small epsilon to avoid division by zero variance. Defaults to
        ``1e-5``.
      scale_init: Optional initializer for the scale variable. Can only be set
        if ``create_scale=True``. By default scale is initialized to ``1``.
      offset_init: Optional initializer for the offset variable. Can only be set
        if ``create_offset=True``. By default offset is initialized to ``0``.
      data_format: The data format of the input. Can be either
        ``channels_first``, ``channels_last``, ``N...C`` or ``NC...``. By
        default it is ``channels_last``.
      name: Name of the module.
    """
    super(CrossReplicaBatchNorm, self).__init__(
        create_scale=create_scale,
        create_offset=create_offset,
        moving_mean=moving_mean,
        moving_variance=moving_variance,
        eps=eps,
        scale_init=scale_init,
        offset_init=offset_init,
        data_format=data_format,
        name=name)

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/axis_norm.py" startline="230" endline="270" pcid="153">
  def __init__(self,
               create_scale: bool,
               create_offset: bool,
               eps: types.FloatLike = 1e-5,
               scale_init: Optional[initializers.Initializer] = None,
               offset_init: Optional[initializers.Initializer] = None,
               data_format: Text = "channels_last",
               name: Optional[Text] = None):
    """Constructs an ``InstanceNorm`` module.

    This method creates a module which normalizes over the spatial dimensions.

    Args:
      create_scale: ``bool`` representing whether to create a trainable scale
        per channel applied after the normalization.
      create_offset: ``bool`` representing whether to create a trainable offset
        per channel applied after normalization and scaling.
      eps: Small epsilon to avoid division by zero variance. Defaults to
        ``1e-5``.
      scale_init: Optional initializer for the scale variable. Can only be set
        if ``create_scale=True``. By default scale is initialized to ``1``.
      offset_init: Optional initializer for the offset variable. Can only be set
        if ``create_offset=True``. By default offset is initialized to ``0``.
      data_format: The data format of the input. Can be either
        ``channels_first``, ``channels_last``, ``N...C`` or ``NC...``. By
        default it is ``channels_last``.
      name: Name of the module.
    """
    if utils.get_channel_index(data_format) == 1:
      axis = slice(2, None)
    else:  # channel_index = -1
      axis = slice(1, -1)
    super(InstanceNorm, self).__init__(
        axis=axis,
        create_scale=create_scale,
        create_offset=create_offset,
        eps=eps,
        scale_init=scale_init,
        offset_init=offset_init,
        data_format=data_format,
        name=name)
</source>
<source file="systems/sonnet-2.0.0/sonnet/src/batch_norm.py" startline="282" endline="326" pcid="72">
  def __init__(self,
               create_scale: bool,
               create_offset: bool,
               decay_rate: float = 0.999,
               eps: types.FloatLike = 1e-5,
               scale_init: Optional[initializers.Initializer] = None,
               offset_init: Optional[initializers.Initializer] = None,
               data_format: Text = "channels_last",
               name: Optional[Text] = None):
    """Constructs a ``BatchNorm`` module.

    Args:
      create_scale: whether to create a trainable scale per channel applied
        after the normalization.
      create_offset: whether to create a trainable offset per channel applied
        after normalization and scaling.
      decay_rate: Decay rate of the exponential moving averages of the mean and
        variance.
      eps: Small epsilon to avoid division by zero variance. Defaults to
        ``1e-5``.
      scale_init: Optional initializer for the scale variable. Can only be set
        if ``create_scale=True``. By default scale is initialized to ``1``.
      offset_init: Optional initializer for the offset variable. Can only be set
        if ``create_offset=True``. By default offset is initialized to ``0``.
      data_format: The data format of the input. Can be either
        ``channels_first``, ``channels_last``, ``N...C`` or ``NC...``. By
        default it is ``channels_last``.
      name: Name of the module.
    """
    with tf.name_scope(name or "batch_norm"):
      moving_mean = moving_averages.ExponentialMovingAverage(
          decay_rate, name="moving_mean")
      moving_variance = moving_averages.ExponentialMovingAverage(
          decay_rate, name="moving_variance")

    super(BatchNorm, self).__init__(
        create_scale=create_scale,
        create_offset=create_offset,
        moving_mean=moving_mean,
        moving_variance=moving_variance,
        eps=eps,
        scale_init=scale_init,
        offset_init=offset_init,
        data_format=data_format,
        name=name)
</source>
</class>

<class classid="2" nclones="2" nlines="36" similarity="80">
<source file="systems/sonnet-2.0.0/sonnet/src/conv.py" startline="37" endline="105" pcid="143">
  def __init__(self,
               num_spatial_dims: int,
               output_channels: int,
               kernel_shape: Union[int, Sequence[int]],
               stride: Union[int, Sequence[int]] = 1,
               rate: Union[int, Sequence[int]] = 1,
               padding: Union[Text, pad.Paddings] = "SAME",
               with_bias: bool = True,
               w_init: Optional[initializers.Initializer] = None,
               b_init: Optional[initializers.Initializer] = None,
               data_format: Optional[Text] = None,
               name: Optional[Text] = None):
    """Constructs a `ConvND` module.

    Args:
      num_spatial_dims: The number of spatial dimensions of the input.
      output_channels: The number of output channels.
      kernel_shape: Sequence of kernel sizes (of length num_spatial_dims), or an
        integer. `kernel_shape` will be expanded to define a kernel size in all
        dimensions.
      stride: Sequence of strides (of length num_spatial_dims), or an integer.
        `stride` will be expanded to define stride in all dimensions.
      rate: Sequence of dilation rates (of length num_spatial_dims), or integer
        that is used to define dilation rate in all dimensions. 1 corresponds to
        standard ND convolution, `rate > 1` corresponds to dilated convolution.
      padding: Padding to apply to the input. This can either "SAME", "VALID" or
        a callable or sequence of callables up to size N. Any callables must
        take a single integer argument equal to the effective kernel size and
        return a list of two integers representing the padding before and after.
        See snt.pad.* for more details and example functions.
      with_bias: Whether to include bias parameters. Default `True`.
      w_init: Optional initializer for the weights. By default the weights are
        initialized truncated random normal values with a standard deviation of
        `1 / sqrt(input_feature_size)`, which is commonly used when the inputs
        are zero centered (see https://arxiv.org/abs/1502.03167v3).
      b_init: Optional initializer for the bias. By default the bias is
        initialized to zero.
      data_format: The data format of the input.
      name: Name of the module.
    """
    super(ConvND, self).__init__(name=name)

    if not 1 <= num_spatial_dims <= 3:
      raise ValueError(
          "We only support convoltion operations for num_spatial_dims=1, 2 or "
          "3, received num_spatial_dims={}.".format(num_spatial_dims))
    self._num_spatial_dims = num_spatial_dims
    self.output_channels = output_channels
    self.kernel_shape = kernel_shape
    self.stride = stride
    self.rate = rate

    if isinstance(padding, six.string_types):
      self.conv_padding = padding.upper()
      self.padding_func = None
    else:
      self.conv_padding = "VALID"
      self.padding_func = padding

    self.data_format = data_format
    self._channel_index = utils.get_channel_index(data_format)
    self.with_bias = with_bias

    self.w_init = w_init
    if with_bias:
      self.b_init = b_init if b_init is not None else initializers.Zeros()
    elif b_init is not None:
      raise ValueError("When not using a bias the b_init must be None.")

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/conv_transpose.py" startline="44" endline="113" pcid="884">
  def __init__(self,
               num_spatial_dims: int,
               output_channels: int,
               kernel_shape: Union[int, Sequence[int]],
               output_shape: Optional[types.ShapeLike] = None,
               stride: Union[int, Sequence[int]] = 1,
               rate: Union[int, Sequence[int]] = 1,
               padding: Text = "SAME",
               with_bias: bool = True,
               w_init: Optional[initializers.Initializer] = None,
               b_init: Optional[initializers.Initializer] = None,
               data_format: Optional[Text] = None,
               name: Optional[Text] = None):
    """Constructs a `ConvNDTranspose` module.

    Args:
      num_spatial_dims: Number of spatial dimensions of the input.
      output_channels: Number of output channels.
      kernel_shape: Sequence of integers (of length num_spatial_dims), or an
        integer representing kernel shape. `kernel_shape` will be expanded to
        define a kernel size in all dimensions.
      output_shape: Output shape of the spatial dimensions of a transpose
        convolution. Can be either an integer or an iterable of integers or a
        `TensorShape` of length `num_spatial_dims`. If a `None` value is given,
        a default shape is automatically calculated.
      stride: Sequence of integers (of length num_spatial_dims), or an integer.
        `stride` will be expanded to define stride in all dimensions.
      rate: Sequence of integers (of length num_spatial_dims), or integer that
        is used to define dilation rate in all dimensions. 1 corresponds to
        standard ND convolution, `rate > 1` corresponds to dilated convolution.
      padding: Padding algorithm, either "SAME" or "VALID".
      with_bias: Boolean, whether to include bias parameters. Default `True`.
      w_init: Optional initializer for the weights. By default the weights are
        initialized truncated random normal values with a standard deviation of
        `1 / sqrt(input_feature_size)`, which is commonly used when the
        inputs are zero centered (see https://arxiv.org/abs/1502.03167v3).
      b_init: Optional initializer for the bias. By default the bias is
        initialized to zero.
      data_format: The data format of the input.
      name: Name of the module.
    """
    super(ConvNDTranspose, self).__init__(name=name)

    if not 1 <= num_spatial_dims <= 3:
      raise ValueError(
          "We only support transpose convolution operations for "
          "num_spatial_dims=1, 2 or 3, received num_spatial_dims={}.".format(
              num_spatial_dims))
    self._num_spatial_dims = num_spatial_dims
    self._output_channels = output_channels
    self._kernel_shape = kernel_shape
    self._output_shape = output_shape
    self._stride = stride
    self._rate = rate

    if padding == "SAME" or padding == "VALID":
      self._padding = padding
    else:
      raise TypeError("ConvNDTranspose only takes string padding, please "
                      "provide either `SAME` or `VALID`.")
    self._data_format = data_format
    self._channel_index = utils.get_channel_index(data_format)
    self._with_bias = with_bias

    self._w_init = w_init
    if with_bias:
      self._b_init = b_init if b_init is not None else initializers.Zeros()
    elif b_init is not None:
      raise ValueError("When not using a bias the b_init must be None.")

</source>
</class>

<class classid="3" nclones="6" nlines="23" similarity="88">
<source file="systems/sonnet-2.0.0/sonnet/src/conv.py" startline="173" endline="223" pcid="147">
  def __init__(self,
               output_channels: int,
               kernel_shape: Union[int, Sequence[int]],
               stride: Union[int, Sequence[int]] = 1,
               rate: Union[int, Sequence[int]] = 1,
               padding: Union[Text, pad.Paddings] = "SAME",
               with_bias: bool = True,
               w_init: Optional[initializers.Initializer] = None,
               b_init: Optional[initializers.Initializer] = None,
               data_format: Text = "NWC",
               name: Optional[Text] = None):
    """Constructs a ``Conv1D`` module.

    Args:
      output_channels: The number of output channels.
      kernel_shape: Sequence of length 1, or an integer. ``kernel_shape`` will
        be expanded to define a kernel size in all dimensions.
      stride: Sequence of strides of length 1, or an integer. ``stride`` will be
        expanded to define stride in all dimensions.
      rate: Sequence of dilation rates of length 1, or integer that is used to
        define dilation rate in all dimensions. 1 corresponds to standard
        convolution, ``rate > 1`` corresponds to dilated convolution.
      padding: Padding to apply to the input. This can be either ``SAME``,
        ``VALID`` or a callable or sequence of callables of size 1. Any
        callables must take a single integer argument equal to the effective
        kernel size and return a list of two integers representing the padding
        before and after. See snt.pad.* for more details and example functions.
      with_bias: Whether to include bias parameters. Default ``True``.
      w_init: Optional initializer for the weights. By default the weights are
        initialized truncated random normal values with a standard deviation of
        ``1``/``sqrt(input_feature_size)``, which is commonly used when the
        inputs are zero centered (see https://arxiv.org/abs/1502.03167v3).
      b_init: Optional initializer for the bias. By default the bias is
        initialized to zero.
      data_format: The data format of the input.
      name: Name of the module.
    """
    super(Conv1D, self).__init__(
        num_spatial_dims=1,
        output_channels=output_channels,
        kernel_shape=kernel_shape,
        stride=stride,
        rate=rate,
        padding=padding,
        with_bias=with_bias,
        w_init=w_init,
        b_init=b_init,
        data_format=data_format,
        name=name)


</source>
<source file="systems/sonnet-2.0.0/sonnet/src/conv_transpose.py" startline="254" endline="307" pcid="890">
  def __init__(self,
               output_channels: int,
               kernel_shape: Union[int, Sequence[int]],
               output_shape: Optional[types.ShapeLike] = None,
               stride: Union[int, Sequence[int]] = 1,
               rate: Union[int, Sequence[int]] = 1,
               padding: Text = "SAME",
               with_bias: bool = True,
               w_init: Optional[initializers.Initializer] = None,
               b_init: Optional[initializers.Initializer] = None,
               data_format: Text = "NHWC",
               name: Optional[Text] = None):
    """Constructs a `Conv2DTranspose` module.

    Args:
      output_channels: An integer, The number of output channels.
      kernel_shape: Sequence of integers (of length 2), or an integer
        representing kernel shape. `kernel_shape` will be expanded to define a
        kernel size in all dimensions.
      output_shape: Output shape of the spatial dimensions of a transpose
        convolution. Can be either an integer or an iterable of integers or
        `Dimension`s, or a `TensorShape` (of length 2). If a `None` value is
        given, a default shape is automatically calculated.
      stride: Sequence of integers (of length 2), or an integer. `stride` will
        be expanded to define stride in all dimensions.
      rate: Sequence of integers (of length 2), or integer that is used to
        define dilation rate in all dimensions. 1 corresponds to standard 2D
        convolution, `rate > 1` corresponds to dilated convolution.
      padding: Padding algorithm, either "SAME" or "VALID".
      with_bias: Boolean, whether to include bias parameters. Default `True`.
      w_init: Optional initializer for the weights. By default the weights are
        initialized truncated random normal values with a standard deviation of
        `1 / sqrt(input_feature_size)`, which is commonly used when the
        inputs are zero centered (see https://arxiv.org/abs/1502.03167v3).
      b_init: Optional initializer for the bias. By default the bias is
        initialized to zero.
      data_format: The data format of the input.
      name: Name of the module.
    """
    super(Conv2DTranspose, self).__init__(
        num_spatial_dims=2,
        output_channels=output_channels,
        kernel_shape=kernel_shape,
        output_shape=output_shape,
        stride=stride,
        rate=rate,
        padding=padding,
        with_bias=with_bias,
        w_init=w_init,
        b_init=b_init,
        data_format=data_format,
        name=name)


</source>
<source file="systems/sonnet-2.0.0/sonnet/src/conv_transpose.py" startline="197" endline="250" pcid="889">
  def __init__(self,
               output_channels: int,
               kernel_shape: Union[int, Sequence[int]],
               output_shape: Optional[types.ShapeLike] = None,
               stride: Union[int, Sequence[int]] = 1,
               rate: Union[int, Sequence[int]] = 1,
               padding: Text = "SAME",
               with_bias: bool = True,
               w_init: Optional[initializers.Initializer] = None,
               b_init: Optional[initializers.Initializer] = None,
               data_format: Text = "NWC",
               name: Optional[Text] = None):
    """Constructs a `Conv1DTranspose` module.

    Args:
      output_channels: Number of output channels.
      kernel_shape: Sequence of integers (of length 1), or an integer
        representing kernel shape. `kernel_shape` will be expanded to define a
        kernel size in all dimensions.
      output_shape: Output shape of the spatial dimensions of a transpose
        convolution. Can be either an integer or an iterable of integers or
        `Dimension`s, or a `TensorShape` (of length 1). If a `None` value is
        given, a default shape is automatically calculated.
      stride: Sequence of integers (of length 1), or an integer. `stride` will
        be expanded to define stride in all dimensions.
      rate: Sequence of integers (of length 1), or integer that is used to
        define dilation rate in all dimensions. 1 corresponds to standard 1D
        convolution, `rate > 1` corresponds to dilated convolution.
      padding: Padding algorithm, either "SAME" or "VALID".
      with_bias: Boolean, whether to include bias parameters. Default `True`.
      w_init: Optional initializer for the weights. By default the weights are
        initialized truncated random normal values with a standard deviation of
        `1 / sqrt(input_feature_size)`, which is commonly used when the
        inputs are zero centered (see https://arxiv.org/abs/1502.03167v3).
      b_init: Optional initializer for the bias. By default the bias is
        initialized to zero.
      data_format: The data format of the input.
      name: Name of the module.
    """
    super(Conv1DTranspose, self).__init__(
        num_spatial_dims=1,
        output_channels=output_channels,
        kernel_shape=kernel_shape,
        output_shape=output_shape,
        stride=stride,
        rate=rate,
        padding=padding,
        with_bias=with_bias,
        w_init=w_init,
        b_init=b_init,
        data_format=data_format,
        name=name)


</source>
<source file="systems/sonnet-2.0.0/sonnet/src/conv.py" startline="282" endline="331" pcid="149">
  def __init__(self,
               output_channels: int,
               kernel_shape: Union[int, Sequence[int]],
               stride: Union[int, Sequence[int]] = 1,
               rate: Union[int, Sequence[int]] = 1,
               padding: Union[Text, pad.Paddings] = "SAME",
               with_bias: bool = True,
               w_init: Optional[initializers.Initializer] = None,
               b_init: Optional[initializers.Initializer] = None,
               data_format: Text = "NDHWC",
               name: Optional[Text] = None):
    """Constructs a ``Conv3D`` module.

    Args:
      output_channels: The number of output channels.
      kernel_shape: Sequence of kernel sizes (of length 3), or an integer.
        ``kernel_shape`` will be expanded to define a kernel size in all
        dimensions.
      stride: Sequence of strides (of length 3), or an integer. `stride` will be
        expanded to define stride in all dimensions.
      rate: Sequence of dilation rates (of length 3), or integer that is used to
        define dilation rate in all dimensions. 1 corresponds to standard
        convolution, ``rate > 1`` corresponds to dilated convolution.
      padding: Padding to apply to the input. This can either ``SAME``,
        ``VALID`` or a callable or sequence of callables up to size N. Any
        callables must take a single integer argument equal to the effective
        kernel size and return a list of two integers representing the padding
        before and after. See snt.pad.* for more details and example functions.
      with_bias: Whether to include bias parameters. Default ``True``.
      w_init: Optional initializer for the weights. By default the weights are
        initialized truncated random normal values with a standard deviation of
        ``1 / sqrt(input_feature_size)``, which is commonly used when the inputs
        are zero centered (see https://arxiv.org/abs/1502.03167v3).
      b_init: Optional initializer for the bias. By default the bias is
        initialized to zero.
      data_format: The data format of the input.
      name: Name of the module.
    """
    super(Conv3D, self).__init__(
        num_spatial_dims=3,
        output_channels=output_channels,
        kernel_shape=kernel_shape,
        stride=stride,
        rate=rate,
        padding=padding,
        with_bias=with_bias,
        w_init=w_init,
        b_init=b_init,
        data_format=data_format,
        name=name)
</source>
<source file="systems/sonnet-2.0.0/sonnet/src/conv_transpose.py" startline="311" endline="362" pcid="891">
  def __init__(self,
               output_channels: int,
               kernel_shape: Union[int, Sequence[int]],
               output_shape: Optional[types.ShapeLike] = None,
               stride: Union[int, Sequence[int]] = 1,
               rate: Union[int, Sequence[int]] = 1,
               padding: Text = "SAME",
               with_bias: bool = True,
               w_init: Optional[initializers.Initializer] = None,
               b_init: Optional[initializers.Initializer] = None,
               data_format: Text = "NDHWC",
               name: Optional[Text] = None):
    """Constructs a `Conv3DTranspose` module.

    Args:
      output_channels: An integer, The number of output channels.
      kernel_shape: Sequence of integers (of length 3), or an integer
        representing kernel shape. `kernel_shape` will be expanded to define a
        kernel size in all dimensions.
      output_shape: Output shape of the spatial dimensions of a transpose
        convolution. Can be either an integer or an iterable of integers or
        `Dimension`s, or a `TensorShape` (of length 3). If a None value is
        given, a default shape is automatically calculated.
      stride: Sequence of integers (of length 3), or an integer. `stride` will
        be expanded to define stride in all dimensions.
      rate: Sequence of integers (of length 3), or integer that is used to
        define dilation rate in all dimensions. 1 corresponds to standard 3D
        convolution, `rate > 1` corresponds to dilated convolution.
      padding: Padding algorithm, either "SAME" or "VALID".
      with_bias: Boolean, whether to include bias parameters. Default `True`.
      w_init: Optional initializer for the weights. By default the weights are
        initialized truncated random normal values with a standard deviation of
        `1 / sqrt(input_feature_size)`, which is commonly used when the
        inputs are zero centered (see https://arxiv.org/abs/1502.03167v3).
      b_init: Optional initializer for the bias. By default the bias is
        initialized to zero.
      data_format: The data format of the input.
      name: Name of the module.
    """
    super(Conv3DTranspose, self).__init__(
        num_spatial_dims=3,
        output_channels=output_channels,
        kernel_shape=kernel_shape,
        output_shape=output_shape,
        stride=stride,
        rate=rate,
        padding=padding,
        with_bias=with_bias,
        w_init=w_init,
        b_init=b_init,
        data_format=data_format,
        name=name)
</source>
<source file="systems/sonnet-2.0.0/sonnet/src/conv.py" startline="227" endline="278" pcid="148">
  def __init__(self,
               output_channels: int,
               kernel_shape: Union[int, Sequence[int]],
               stride: Union[int, Sequence[int]] = 1,
               rate: Union[int, Sequence[int]] = 1,
               padding: Union[Text, pad.Paddings] = "SAME",
               with_bias: bool = True,
               w_init: Optional[initializers.Initializer] = None,
               b_init: Optional[initializers.Initializer] = None,
               data_format: Text = "NHWC",
               name: Optional[Text] = None):
    """Constructs a ``Conv2D`` module.

    Args:
      output_channels: The number of output channels.
      kernel_shape: Sequence of kernel sizes (of length 2), or an integer.
        ``kernel_shape`` will be expanded to define a kernel size in all
        dimensions.
      stride: Sequence of strides (of length 2), or an integer. ``stride`` will
        be expanded to define stride in all dimensions.
      rate: Sequence of dilation rates (of length 2), or integer that is used to
        define dilation rate in all dimensions. 1 corresponds to standard
        convolution, ``rate > 1`` corresponds to dilated convolution.
      padding: Padding to apply to the input. This can either ``SAME``,
        ``VALID`` or a callable or sequence of callables of size 2. Any
        callables must take a single integer argument equal to the effective
        kernel size and return a list of two integers representing the padding
        before and after. See snt.pad.* for more details and example functions.
      with_bias: Whether to include bias parameters. Default ``True``.
      w_init: Optional initializer for the weights. By default the weights are
        initialized truncated random normal values with a standard deviation of
        ``1 / sqrt(input_feature_size)``, which is commonly used when the inputs
        are zero centered (see https://arxiv.org/abs/1502.03167v3).
      b_init: Optional initializer for the bias. By default the bias is
        initialized to zero.
      data_format: The data format of the input.
      name: Name of the module.
    """
    super(Conv2D, self).__init__(
        num_spatial_dims=2,
        output_channels=output_channels,
        kernel_shape=kernel_shape,
        stride=stride,
        rate=rate,
        padding=padding,
        with_bias=with_bias,
        w_init=w_init,
        b_init=b_init,
        data_format=data_format,
        name=name)


</source>
</class>

<class classid="4" nclones="2" nlines="36" similarity="86">
<source file="systems/sonnet-2.0.0/sonnet/src/axis_norm.py" startline="72" endline="136" pcid="150">
  def __init__(self,
               axis: types.Axis,
               create_scale: bool,
               create_offset: bool,
               eps: types.FloatLike = 1e-5,
               scale_init: Optional[initializers.Initializer] = None,
               offset_init: Optional[initializers.Initializer] = None,
               data_format: Text = "channels_last",
               name: Optional[Text] = None):
    r"""Constructs an ``LayerNorm`` module.

    Args:
      axis: An ``int``, ``slice`` or sequence of ``int``\s representing the axes
        which should be normalized across. Typical usages are: ``1`` or ``-1``
        for normalization over just the channels and ``slice(1, None)``,
        ``slice(2, None)`` for normalization over the spatial and channel
        dimensions whilst avoiding the batch and/or time dimensions.
      create_scale: ``bool`` representing whether to create a trainable scale
        per channel applied after the normalization.
      create_offset: ``bool`` representing whether to create a trainable offset
        per channel applied after normalization and scaling.
      eps: Small epsilon to avoid division by zero variance. Defaults to
        ``1e-5``.
      scale_init: Optional initializer for the scale variable. Can only be set
        if ``create_scale=True``. By default scale is initialized to ``1``.
      offset_init: Optional initializer for the offset variable. Can only be set
        if ``create_offset=True``. By default offset is initialized to ``0``.
      data_format: The data format of the input. Can be either
        ``channels_first``, ``channels_last``, ``N...C`` or ``NC...``. By
        default it is ``channels_last``.
      name: Name of the module.
    """
    super(LayerNorm, self).__init__(name=name)

    if isinstance(axis, slice):
      self._axis = axis
    elif isinstance(axis, six.integer_types):
      self._axis = (axis,)
    elif (isinstance(axis, collections.Iterable) and
          all(isinstance(ax, six.integer_types) for ax in axis)):
      self._axis = axis
    else:
      raise ValueError("`axis` should be an int, slice or iterable of ints.")

    self._eps = eps

    self._data_format = data_format
    self._channel_index = utils.get_channel_index(data_format)

    self._rank = None

    self._create_scale = create_scale
    self._create_offset = create_offset

    if self._create_scale:
      self._scale_init = (
          scale_init if scale_init is not None else initializers.Ones())
    elif scale_init is not None:
      raise ValueError("Cannot set `scale_init` if `create_scale=False`.")
    if self._create_offset:
      self._offset_init = (
          offset_init if offset_init is not None else initializers.Zeros())
    elif offset_init is not None:
      raise ValueError("Cannot set `offset_init` if `create_offset=False`.")

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/group_norm.py" startline="74" endline="139" pcid="462">
  def __init__(self,
               groups: int,
               axis: types.Axis = slice(1, None),
               create_scale: bool = True,
               create_offset: bool = True,
               eps: types.FloatLike = 1e-5,
               scale_init: Optional[initializers.Initializer] = None,
               offset_init: Optional[initializers.Initializer] = None,
               data_format: Text = "channels_last",
               name: Optional[Text] = None):
    """Constructs a ``GroupNorm`` module.

    Args:
      groups: number of groups to divide the channels by. The number of channels
        must be divisible by this.
      axis: ``int``, ``slice`` or sequence of ints representing the axes which
        should be normalized across. By default this is all but the first
        dimension. For time series data use `slice(2, None)` to average over the
        none Batch and Time data.
      create_scale: whether to create a trainable scale per channel applied
        after the normalization.
      create_offset: whether to create a trainable offset per channel applied
        after normalization and scaling.
      eps: Small epsilon to add to the variance to avoid division by zero.
        Defaults to ``1e-5``.
      scale_init: Optional initializer for the scale variable. Can only be set
        if ``create_scale=True``. By default scale is initialized to ``1``.
      offset_init: Optional initializer for the offset variable. Can only be set
        if ``create_offset=True``. By default offset is initialized to ``0``.
      data_format: The data format of the input. Can be either
        ``channels_first``, ``channels_last``, ``N...C`` or ``NC...``. By
        default it is ``channels_last``.
      name: Name of the module.
    """
    super(GroupNorm, self).__init__(name=name)

    if isinstance(axis, slice):
      self._axis = axis
    elif isinstance(axis, six.integer_types):
      self._axis = [axis]
    elif (isinstance(axis, collections.Iterable) and
          all(isinstance(ax, six.integer_types) for ax in axis)):
      self._axis = axis
    else:
      raise ValueError("`axis` should be an int, slice or iterable of ints.")

    self._groups = groups
    self._eps = eps

    self._data_format = data_format
    self._channel_index = utils.get_channel_index(data_format)

    self._create_scale = create_scale
    self._create_offset = create_offset

    if self._create_scale:
      self._scale_init = (
          scale_init if scale_init is not None else initializers.Ones())
    elif scale_init is not None:
      raise ValueError("Cannot set `scale_init` if `create_scale=False`.")
    if self._create_offset:
      self._offset_init = (
          offset_init if offset_init is not None else initializers.Zeros())
    elif offset_init is not None:
      raise ValueError("Cannot set `offset_init` if `create_offset=False`.")

</source>
</class>

<class classid="5" nclones="2" nlines="30" similarity="84">
<source file="systems/sonnet-2.0.0/sonnet/src/axis_norm.py" startline="137" endline="187" pcid="151">
  def __call__(self,
               inputs: tf.Tensor,
               scale: Optional[tf.Tensor] = None,
               offset: Optional[tf.Tensor] = None) -> tf.Tensor:
    """Returns normalized inputs.

    Args:
      inputs: An n-D tensor of the ``data_format`` specified in the constructor
        on which the transformation is performed.
      scale: A tensor up to n-D. The shape of this tensor must be broadcastable
        to the shape of ``inputs``. This is the scale applied to the normalized
        inputs. This cannot be passed in if the module was constructed with
        ``create_scale=True``.
      offset: A tensor up to n-D. The shape of this tensor must be broadcastable
        to the shape of ``inputs``. This is the offset applied to the normalized
        ``inputs``. This cannot be passed in if the module was constructed with
        ``create_offset=True``.

    Returns:
      An n-d tensor of the same shape as inputs that has been normalized.
    """
    self._initialize(inputs)
    if self._create_scale:
      if scale is not None:
        raise ValueError(
            "Cannot pass `scale` at call time if `create_scale=True`.")
      scale = self.scale

    if self._create_offset:
      if offset is not None:
        raise ValueError(
            "Cannot pass `offset` at call time if `create_offset=True`.")
      offset = self.offset

    if len(inputs.shape) != self._rank:
      raise ValueError(
          "The rank of the inputs cannot change between calls, the"
          " original call was rank={} but this call was rank={}.".format(
              self._rank, len(inputs.shape)))

    mean, var = tf.nn.moments(inputs, self._axis, keepdims=True)

    normalized = tf.nn.batch_normalization(
        inputs,
        mean=mean,
        variance=var,
        scale=scale,
        offset=offset,
        variance_epsilon=self._eps)
    return normalized

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/group_norm.py" startline="140" endline="194" pcid="463">
  def __call__(self,
               inputs: tf.Tensor,
               scale: Optional[tf.Tensor] = None,
               offset: Optional[tf.Tensor] = None):
    """Returns normalized inputs.

    Args:
      inputs: An n-D tensor of the ``data_format`` specified in the constructor
        on which the transformation is performed.
      scale: A tensor up to n-D. The shape of this tensor must be broadcastable
        to the shape of ``inputs``. This is the scale applied to the normalized
        inputs. This cannot be passed in if the module was constructed with
        ``create_scale=True``.
      offset: A tensor up to n-D. The shape of this tensor must be broadcastable
        to the shape of ``inputs``. This is the offset applied to the normalized
        ``inputs``. This cannot be passed in if the module was constructed with
        ``create_offset=True``.

    Returns:
      An n-d tensor of the same shape as inputs that has been normalized.
    """
    self._initialize(inputs)
    if self._create_scale:
      if scale is not None:
        raise ValueError(
            "Cannot pass `scale` at call time if `create_scale=True`.")
      scale = self.scale

    if self._create_offset:
      if offset is not None:
        raise ValueError(
            "Cannot pass `offset` at call time if `create_offset=True`.")
      offset = self.offset

    if len(inputs.shape) != self._rank:
      raise ValueError(
          "The rank of the inputs cannot change between calls, the"
          " original call was rank={} but this call was rank={}.".format(
              self._rank, len(inputs.shape)))

    inputs = tf.reshape(inputs, self._inputs_reshape)
    mean, var = tf.nn.moments(inputs, self._axis, keepdims=True)

    normalized = tf.nn.batch_normalization(
        inputs,
        mean=mean,
        variance=var,
        scale=None,
        offset=None,
        variance_epsilon=self._eps)
    outputs = tf.reshape(normalized, self._outputs_reshape)
    outputs = outputs * scale if scale is not None else outputs
    outputs = outputs + offset if offset is not None else outputs
    return outputs

</source>
</class>

<class classid="6" nclones="3" nlines="12" similarity="78">
<source file="systems/sonnet-2.0.0/sonnet/src/batch_norm_test.py" startline="33" endline="47" pcid="154">
  def testSimpleTraining(self):
    layer = batch_norm.BaseBatchNorm(
        moving_mean=TestMetric(),
        moving_variance=TestMetric(),
        create_scale=False,
        create_offset=False)

    inputs = tf.ones([2, 3, 3, 5])
    scale = tf.constant(0.5, shape=(5,))
    offset = tf.constant(2.0, shape=(5,))

    outputs = layer(inputs, True, scale=scale, offset=offset).numpy()
    self.assertAllEqual(outputs, tf.fill(inputs.shape, 2.0))
    self.assertEqual((0, 1, 2), layer._axis)

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/batch_norm_test.py" startline="170" endline="186" pcid="162">
  def testUsingTestStats(self):
    layer = batch_norm.BaseBatchNorm(
        moving_mean=TestMetric(),
        moving_variance=TestMetric(),
        create_scale=False,
        create_offset=False)

    inputs = tf.ones([2, 3, 3, 5])
    scale = tf.constant(0.5, shape=(5,))
    offset = tf.constant(2.0, shape=(5,))

    outputs = layer(inputs, True, scale=scale, offset=offset).numpy()
    self.assertAllEqual(outputs, tf.fill(inputs.shape, 2.0))
    outputs = layer(inputs, False, scale=scale, offset=offset).numpy()
    for x in np.nditer(outputs):
      self.assertAllClose(x, 2.0, rtol=1e-5, atol=1e-3)

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/batch_norm_test.py" startline="64" endline="78" pcid="156">
  def testSimpleTraining3D(self):
    layer = batch_norm.BaseBatchNorm(
        moving_mean=TestMetric(),
        moving_variance=TestMetric(),
        create_scale=False,
        create_offset=False)

    inputs = tf.ones([2, 3, 3, 3, 5])
    scale = tf.constant(0.5, shape=(5,))
    offset = tf.constant(2.0, shape=(5,))

    outputs = layer(inputs, True, scale=scale, offset=offset).numpy()
    self.assertAllEqual(outputs, tf.fill(inputs.shape, 2.0))
    self.assertEqual((0, 1, 2, 3), layer._axis)

</source>
</class>

<class classid="7" nclones="2" nlines="10" similarity="100">
<source file="systems/sonnet-2.0.0/sonnet/src/base_test.py" startline="180" endline="193" pcid="230">
  def test_get_attr_doesnt_enter_name_scope(self):
    scope_names = []

    class GetAttrModule(base.Module):

      def __getattr__(self, name):
        scope_names.append((name, get_name_scope()))
        return super(GetAttrModule, self).__getattr__(name)

    mod = GetAttrModule()
    with self.assertRaises(AttributeError):
      mod.does_not_exist  # pylint: disable=pointless-statement
    self.assertIn(("does_not_exist", ""), scope_names)

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/base_test.py" startline="194" endline="208" pcid="232">
  def test_get_attribute_doesnt_enter_name_scope(self):
    scope_names = []

    class GetAttributeModule(base.Module):

      def __getattribute__(self, name):
        scope_names.append((name, get_name_scope()))
        return super(GetAttributeModule, self).__getattribute__(name)

    mod = GetAttributeModule()
    with self.assertRaises(AttributeError):
      mod.does_not_exist  # pylint: disable=pointless-statement
    self.assertIn(("does_not_exist", ""), scope_names)


</source>
</class>

<class classid="8" nclones="3" nlines="13" similarity="71">
<source file="systems/sonnet-2.0.0/sonnet/src/optimizers/sgd_test.py" startline="59" endline="72" pcid="319">
  def testVariableLearningRate(self):
    parameters = [tf.Variable([1., 2.]), tf.Variable([3., 4.])]
    updates = [tf.constant([5., 5.]), tf.constant([3., 3.])]
    learning_rate = tf.Variable(3.)
    optimizer = self.make_optimizer(learning_rate=learning_rate)
    optimizer.apply(updates, parameters)
    self.assertAllClose([[-14., -13.], [-6., -5.]],
                        [x.numpy() for x in parameters])
    learning_rate.assign_sub(1.)
    self.assertEqual(2., optimizer.learning_rate.numpy())
    optimizer.apply(updates, parameters)
    self.assertAllClose([[-24., -23.], [-12., -11.]],
                        [x.numpy() for x in parameters])

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/optimizers/adam_test.py" startline="121" endline="135" pcid="362">
  def testVariableHyperParams(self):
    parameters = [tf.Variable([1., 2.]), tf.Variable([3., 4.])]
    updates = [tf.constant([5., 5.]), tf.constant([3., 3.])]
    learning_rate = tf.Variable(0.001)
    optimizer = self.make_optimizer(learning_rate=learning_rate)
    optimizer.apply(updates, parameters)
    self.assertAllClose([[0.999, 1.999], [2.999, 3.999]],
                        [x.numpy() for x in parameters])
    learning_rate.assign(0.1)
    self.assertAlmostEqual(0.1, optimizer.learning_rate.numpy())
    optimizer.apply(updates, parameters)
    self.assertAllClose([[0.899, 1.899], [2.899, 3.899]],
                        [x.numpy() for x in parameters],
                        rtol=1e-4)

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/optimizers/rmsprop_test.py" startline="156" endline="169" pcid="330">
  def testVariableHyperParams(self):
    parameters = [tf.Variable([1., 2.]), tf.Variable([3., 4.])]
    updates = [tf.constant([5., 5.]), tf.constant([3., 3.])]
    learning_rate = tf.Variable(0.1)
    optimizer = self.make_optimizer(learning_rate=learning_rate)
    optimizer.apply(updates, parameters)
    self.assertAllClose([[0.683772, 1.683772], [2.683772, 3.683772]],
                        [x.numpy() for x in parameters])
    learning_rate.assign(0.01)
    self.assertAlmostEqual(0.01, optimizer.learning_rate.numpy())
    optimizer.apply(updates, parameters)
    self.assertAllClose([[0.660831, 1.660831], [2.660831, 3.660831]],
                        [x.numpy() for x in parameters])

</source>
</class>

<class classid="9" nclones="5" nlines="13" similarity="71">
<source file="systems/sonnet-2.0.0/sonnet/src/optimizers/rmsprop_test.py" startline="66" endline="82" pcid="326">
  def testDense(self):
    parameters = [tf.Variable([1., 2.]), tf.Variable([3., 4.])]
    updates = [tf.constant([5., 5.]), tf.constant([3., 3.])]
    optimizer = self.make_optimizer(learning_rate=0.1)
    # Step 1 of RMSProp
    optimizer.apply(updates, parameters)
    self.assertAllClose([[0.683772, 1.683772], [2.683772, 3.683772]],
                        [x.numpy() for x in parameters])
    # Step 2 of RMSProp
    optimizer.apply(updates, parameters)
    self.assertAllClose([[0.454357, 1.454357], [2.454357, 3.454357]],
                        [x.numpy() for x in parameters])
    # Step 3 of RMSProp
    optimizer.apply(updates, parameters)
    self.assertAllClose([[0.262262, 1.262262], [2.262262, 3.262262]],
                        [x.numpy() for x in parameters])

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/optimizers/momentum_test.py" startline="62" endline="78" pcid="342">
  def testDense(self):
    parameters = [tf.Variable([1., 2.]), tf.Variable([3., 4.])]
    updates = [tf.constant([5., 5.]), tf.constant([3., 3.])]
    optimizer = self.make_optimizer(learning_rate=0.1, momentum=0.9)
    # Step 1 of Momentum
    optimizer.apply(updates, parameters)
    self.assertAllClose([[0.5, 1.5], [2.7, 3.7]],
                        [x.numpy() for x in parameters])
    # Step 2 of Momentum
    optimizer.apply(updates, parameters)
    self.assertAllClose([[-0.45, 0.55], [2.13, 3.13]],
                        [x.numpy() for x in parameters])
    # Step 3 of Momentum
    optimizer.apply(updates, parameters)
    self.assertAllClose([[-1.805, -0.805], [1.317, 2.317]],
                        [x.numpy() for x in parameters])

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/optimizers/rmsprop_test.py" startline="83" endline="99" pcid="327">
  def testDenseCentered(self):
    parameters = [tf.Variable([1., 2.]), tf.Variable([3., 4.])]
    updates = [tf.constant([5., 5.]), tf.constant([3., 3.])]
    optimizer = self.make_optimizer(learning_rate=0.1, centered=True)
    # Step 1 of RMSProp
    optimizer.apply(updates, parameters)
    self.assertAllClose([[0.666667, 1.666667], [2.666667, 3.666667]],
                        [x.numpy() for x in parameters])
    # Step 2 of RMSProp
    optimizer.apply(updates, parameters)
    self.assertAllClose([[0.41176, 1.41176], [2.41176, 3.41176]],
                        [x.numpy() for x in parameters])
    # Step 3 of RMSProp
    optimizer.apply(updates, parameters)
    self.assertAllClose([[0.186776, 1.186776], [2.186776, 3.186776]],
                        [x.numpy() for x in parameters])

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/optimizers/momentum_test.py" startline="79" endline="96" pcid="343">
  def testDenseNesterov(self):
    parameters = [tf.Variable([1., 2.]), tf.Variable([3., 4.])]
    updates = [tf.constant([5., 5.]), tf.constant([3., 3.])]
    optimizer = self.make_optimizer(
        learning_rate=0.1, momentum=0.9, use_nesterov=True)
    # Step 1 of Momentum
    optimizer.apply(updates, parameters)
    self.assertAllClose([[0.05, 1.05], [2.43, 3.43]],
                        [x.numpy() for x in parameters])
    # Step 2 of Momentum
    optimizer.apply(updates, parameters)
    self.assertAllClose([[-1.305, -0.305], [1.617, 2.617]],
                        [x.numpy() for x in parameters])
    # Step 3 of Momentum
    optimizer.apply(updates, parameters)
    self.assertAllClose([[-3.0245, -2.0245], [0.5853, 1.5853]],
                        [x.numpy() for x in parameters])

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/optimizers/adam_test.py" startline="63" endline="79" pcid="360">
  def testDense(self):
    parameters = [tf.Variable([1., 2.]), tf.Variable([3., 4.])]
    updates = [tf.constant([5., 5.]), tf.constant([3., 3.])]
    optimizer = self.make_optimizer(learning_rate=0.001)
    # Step 1 of Adam
    optimizer.apply(updates, parameters)
    self.assertAllClose([[0.999, 1.999], [2.999, 3.999]],
                        [x.numpy() for x in parameters])
    # Step 2 of Adam
    optimizer.apply(updates, parameters)
    self.assertAllClose([[0.998, 1.998], [2.998, 3.998]],
                        [x.numpy() for x in parameters])
    # Step 3 of Adam
    optimizer.apply(updates, parameters)
    self.assertAllClose([[0.997, 1.997], [2.997, 3.997]],
                        [x.numpy() for x in parameters])

</source>
</class>

<class classid="10" nclones="2" nlines="22" similarity="95">
<source file="systems/sonnet-2.0.0/sonnet/src/optimizers/rmsprop_test.py" startline="100" endline="127" pcid="328">
  def testSparse(self):
    if self.primary_device in ("GPU", "TPU"):
      self.skipTest("IndexedSlices not supported on {}.".format(
          self.primary_device))

    parameters = [tf.Variable([[1.], [2.]]), tf.Variable([[3.], [4.]])]
    updates = [
        tf.IndexedSlices(
            tf.constant([0.1], shape=[1, 1]), tf.constant([0]),
            tf.constant([2, 1])),
        tf.IndexedSlices(
            tf.constant([0.01], shape=[1, 1]), tf.constant([1]),
            tf.constant([2, 1]))
    ]
    optimizer = self.make_optimizer(learning_rate=3.)
    # Step 1 of RMSProp
    optimizer.apply(updates, parameters)
    self.assertAllClose([[-8.486831], [2.0]], parameters[0].numpy(), rtol=1e-4)
    self.assertAllClose([[3.0], [-5.486784]], parameters[1].numpy(), rtol=1e-4)
    # Step 2 of RMSProp
    optimizer.apply(updates, parameters)
    self.assertAllClose([[-15.369301], [2.0]], parameters[0].numpy(), rtol=1e-4)
    self.assertAllClose([[3.0], [-12.369237]], parameters[1].numpy(), rtol=1e-4)
    # Step 3 of RMSProp
    optimizer.apply(updates, parameters)
    self.assertAllClose([[-21.132141], [2.0]], parameters[0].numpy(), rtol=1e-4)
    self.assertAllClose([[3.0], [-18.132067]], parameters[1].numpy(), rtol=1e-4)

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/optimizers/rmsprop_test.py" startline="128" endline="155" pcid="329">
  def testSparseCentered(self):
    if self.primary_device in ("GPU", "TPU"):
      self.skipTest("IndexedSlices not supported on {}.".format(
          self.primary_device))

    parameters = [tf.Variable([[1.], [2.]]), tf.Variable([[3.], [4.]])]
    updates = [
        tf.IndexedSlices(
            tf.constant([0.1], shape=[1, 1]), tf.constant([0]),
            tf.constant([2, 1])),
        tf.IndexedSlices(
            tf.constant([0.01], shape=[1, 1]), tf.constant([1]),
            tf.constant([2, 1]))
    ]
    optimizer = self.make_optimizer(learning_rate=3., centered=True)
    # Step 1 of RMSProp
    optimizer.apply(updates, parameters)
    self.assertAllClose([[-8.999999], [2.0]], parameters[0].numpy(), rtol=1e-4)
    self.assertAllClose([[3.0], [-5.999944]], parameters[1].numpy(), rtol=1e-4)
    # Step 2 of RMSProp
    optimizer.apply(updates, parameters)
    self.assertAllClose([[-16.64719], [2.0]], parameters[0].numpy(), rtol=1e-4)
    self.assertAllClose([[3.0], [-13.647109]], parameters[1].numpy(), rtol=1e-4)
    # Step 3 of RMSProp
    optimizer.apply(updates, parameters)
    self.assertAllClose([[-23.396709], [2.0]], parameters[0].numpy(), rtol=1e-4)
    self.assertAllClose([[3.0], [-20.39661]], parameters[1].numpy(), rtol=1e-4)

</source>
</class>

<class classid="11" nclones="3" nlines="26" similarity="92">
<source file="systems/sonnet-2.0.0/sonnet/src/conv_test.py" startline="128" endline="157" pcid="406">
  def testFunction(self, with_bias, padding):
    conv1 = conv.ConvND(
        num_spatial_dims=2,
        output_channels=1,
        kernel_shape=3,
        stride=1,
        padding=padding,
        with_bias=with_bias,
        data_format="NHWC",
        **create_constant_initializers(1.0, 1.0, with_bias))
    conv2 = conv.ConvND(
        num_spatial_dims=2,
        output_channels=1,
        kernel_shape=3,
        stride=1,
        padding=padding,
        with_bias=with_bias,
        data_format="NHWC",
        **create_constant_initializers(1.0, 1.0, with_bias))
    defun_conv = tf.function(conv2)

    iterations = 5

    for _ in range(iterations):
      x = tf.random.uniform([1, 5, 5, 1])
      y1 = conv1(x)
      y2 = defun_conv(x)

      self.assertAllClose(self.evaluate(y1), self.evaluate(y2), atol=1e-4)

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/conv_transpose_test.py" startline="65" endline="96" pcid="574">
  def testGraphConv(self, with_bias, padding):
    conv1 = conv_transpose.ConvNDTranspose(
        num_spatial_dims=2,
        output_channels=1,
        output_shape=None,
        kernel_shape=3,
        stride=1,
        padding=padding,
        with_bias=with_bias,
        data_format="NHWC",
        **create_constant_initializers(1.0, 1.0, with_bias))
    conv2 = conv_transpose.ConvNDTranspose(
        num_spatial_dims=2,
        output_channels=1,
        output_shape=None,
        kernel_shape=3,
        stride=1,
        padding=padding,
        with_bias=with_bias,
        data_format="NHWC",
        **create_constant_initializers(1.0, 1.0, with_bias))
    defun_conv = tf.function(conv2)

    iterations = 5

    for _ in range(iterations):
      x = tf.random.uniform([1, 3, 3, 1])
      y1 = conv1(x)
      y2 = defun_conv(x)

      self.assertAllClose(self.evaluate(y1), self.evaluate(y2), atol=1e-4)

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/depthwise_conv_test.py" startline="73" endline="100" pcid="755">
  def testFunction(self, with_bias, padding):
    conv1 = depthwise_conv.DepthwiseConv2D(
        channel_multiplier=1,
        kernel_shape=3,
        stride=1,
        padding=padding,
        with_bias=with_bias,
        data_format="NHWC",
        **create_constant_initializers(1.0, 1.0, with_bias))
    conv2 = depthwise_conv.DepthwiseConv2D(
        channel_multiplier=1,
        kernel_shape=3,
        stride=1,
        padding=padding,
        with_bias=with_bias,
        data_format="NHWC",
        **create_constant_initializers(1.0, 1.0, with_bias))
    defun_conv = tf.function(conv2)

    iterations = 5

    for _ in range(iterations):
      x = tf.random.uniform([1, 5, 5, 1])
      y1 = conv1(x)
      y2 = defun_conv(x)

      self.assertAllClose(self.evaluate(y1), self.evaluate(y2), atol=1e-4)

</source>
</class>

<class classid="12" nclones="5" nlines="12" similarity="71">
<source file="systems/sonnet-2.0.0/sonnet/src/conv_test.py" startline="158" endline="173" pcid="407">
  def testUnknownBatchSizeNHWC(self):
    x = tf.TensorSpec([None, 5, 5, 3], dtype=tf.float32)

    c = conv.ConvND(
        num_spatial_dims=2,
        output_channels=2,
        kernel_shape=3,
        data_format="NHWC")
    defun_conv = tf.function(c).get_concrete_function(x)

    out1 = defun_conv(tf.ones([3, 5, 5, 3]))
    self.assertEqual(out1.shape, [3, 5, 5, 2])

    out2 = defun_conv(tf.ones([5, 5, 5, 3]))
    self.assertEqual(out2.shape, [5, 5, 5, 2])

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/conv_test.py" startline="174" endline="191" pcid="408">
  def testUnknownBatchSizeNCHW(self):
    if self.primary_device == "CPU":
      self.skipTest("NCHW not supported on CPU")

    x = tf.TensorSpec([None, 3, 5, 5], dtype=tf.float32)
    c = conv.ConvND(
        num_spatial_dims=2,
        output_channels=2,
        kernel_shape=3,
        data_format="NCHW")
    defun_conv = tf.function(c).get_concrete_function(x)

    out1 = defun_conv(tf.ones([3, 3, 5, 5]))
    self.assertEqual(out1.shape, [3, 2, 5, 5])

    out2 = defun_conv(tf.ones([5, 3, 5, 5]))
    self.assertEqual(out2.shape, [5, 2, 5, 5])

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/conv_transpose_test.py" startline="113" endline="131" pcid="576">
  def testUnknownBatchSizeNCHW(self):
    if self.primary_device == "CPU":
      self.skipTest("NCHW not supported on CPU")

    x = tf.TensorSpec([None, 3, 5, 5], dtype=tf.float32)

    c = conv_transpose.ConvNDTranspose(
        num_spatial_dims=2,
        output_channels=2,
        kernel_shape=3,
        data_format="NCHW")
    defun_conv = tf.function(c).get_concrete_function(x)

    out1 = defun_conv(tf.ones([3, 3, 5, 5]))
    self.assertEqual(out1.shape, [3, 2, 5, 5])

    out2 = defun_conv(tf.ones([5, 3, 5, 5]))
    self.assertEqual(out2.shape, [5, 2, 5, 5])

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/conv_transpose_test.py" startline="97" endline="112" pcid="575">
  def testUnknownBatchSizeNHWC(self):
    x = tf.TensorSpec([None, 5, 5, 3], dtype=tf.float32)

    c = conv_transpose.ConvNDTranspose(
        num_spatial_dims=2,
        output_channels=2,
        kernel_shape=3,
        data_format="NHWC")
    defun_conv = tf.function(c).get_concrete_function(x)

    out1 = defun_conv(tf.ones([3, 5, 5, 3]))
    self.assertEqual(out1.shape, [3, 5, 5, 2])

    out2 = defun_conv(tf.ones([5, 5, 5, 3]))
    self.assertEqual(out2.shape, [5, 5, 5, 2])

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/depthwise_conv_test.py" startline="114" endline="128" pcid="757">
  def testUnknownBatchSizeNCHW(self):
    if self.primary_device == "CPU":
      self.skipTest("NCHW not supported on CPU")

    x = tf.TensorSpec([None, 3, 5, 5], dtype=tf.float32)
    c = depthwise_conv.DepthwiseConv2D(
        channel_multiplier=1, kernel_shape=3, data_format="NCHW")
    defun_conv = tf.function(c).get_concrete_function(x)

    out1 = defun_conv(tf.ones([3, 3, 5, 5]))
    self.assertEqual(out1.shape, [3, 3, 5, 5])

    out2 = defun_conv(tf.ones([5, 3, 5, 5]))
    self.assertEqual(out2.shape, [5, 3, 5, 5])

</source>
</class>

<class classid="13" nclones="2" nlines="11" similarity="100">
<source file="systems/sonnet-2.0.0/sonnet/src/conv_test.py" startline="193" endline="206" pcid="409">
  def testUnknownChannels(self, autograph):
    x = tf.TensorSpec([3, 3, 3, None], dtype=tf.float32)

    c = conv.ConvND(
        num_spatial_dims=2,
        output_channels=1,
        kernel_shape=3,
        data_format="NHWC")
    defun_conv = tf.function(c, autograph=autograph)

    with self.assertRaisesRegex(ValueError,
                                "The number of input channels must be known"):
      defun_conv.get_concrete_function(x)

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/conv_transpose_test.py" startline="133" endline="146" pcid="577">
  def testUnknownChannels(self, autograph):
    x = tf.TensorSpec([3, 3, 3, None], dtype=tf.float32)

    c = conv_transpose.ConvNDTranspose(
        num_spatial_dims=2,
        output_channels=1,
        kernel_shape=3,
        data_format="NHWC")
    defun_conv = tf.function(c, autograph=autograph)

    with self.assertRaisesRegex(ValueError,
                                "The number of input channels must be known"):
      defun_conv.get_concrete_function(x)

</source>
</class>

<class classid="14" nclones="2" nlines="15" similarity="75">
<source file="systems/sonnet-2.0.0/sonnet/src/conv_test.py" startline="207" endline="227" pcid="410">
  def testUnknownSpatialDims(self):
    x = tf.TensorSpec([3, None, None, 3], dtype=tf.float32)

    c = conv.ConvND(
        num_spatial_dims=2,
        output_channels=1,
        kernel_shape=3,
        data_format="NHWC")
    defun_conv = tf.function(c).get_concrete_function(x)

    out = defun_conv(tf.ones([3, 5, 5, 3]))
    expected_out = c(tf.ones([3, 5, 5, 3]))
    self.assertEqual(out.shape, [3, 5, 5, 1])
    self.assertAllEqual(self.evaluate(out), self.evaluate(expected_out))

    out = defun_conv(tf.ones([3, 4, 4, 3]))
    expected_out = c(tf.ones([3, 4, 4, 3]))
    self.assertEqual(out.shape, [3, 4, 4, 1])
    self.assertAllEqual(self.evaluate(out), self.evaluate(expected_out))


</source>
<source file="systems/sonnet-2.0.0/sonnet/src/depthwise_conv_test.py" startline="129" endline="145" pcid="758">
  def testUnknownSpatialDims(self):
    x = tf.TensorSpec([3, None, None, 3], dtype=tf.float32)

    c = depthwise_conv.DepthwiseConv2D(
        channel_multiplier=1, kernel_shape=3, data_format="NHWC")
    defun_conv = tf.function(c).get_concrete_function(x)

    out = defun_conv(tf.ones([3, 5, 5, 3]))
    expected_out = c(tf.ones([3, 5, 5, 3]))
    self.assertEqual(out.shape, [3, 5, 5, 3])
    self.assertAllEqual(self.evaluate(out), self.evaluate(expected_out))

    out = defun_conv(tf.ones([3, 4, 4, 3]))
    expected_out = c(tf.ones([3, 4, 4, 3]))
    self.assertEqual(out.shape, [3, 4, 4, 3])
    self.assertAllEqual(self.evaluate(out), self.evaluate(expected_out))

</source>
</class>

<class classid="15" nclones="12" nlines="17" similarity="70">
<source file="systems/sonnet-2.0.0/sonnet/src/conv_test.py" startline="231" endline="251" pcid="411">
  def testComputationPaddingSame(self, with_bias):
    expected_out = [[4, 6, 6, 6, 4], [6, 9, 9, 9, 6], [6, 9, 9, 9, 6],
                    [6, 9, 9, 9, 6], [4, 6, 6, 6, 4]]
    conv1 = conv.Conv2D(
        output_channels=1,
        kernel_shape=3,
        stride=1,
        padding="SAME",
        with_bias=with_bias,
        **create_constant_initializers(1.0, 1.0, with_bias))

    out = conv1(tf.ones([1, 5, 5, 1], dtype=tf.float32))
    self.assertEqual(out.shape, [1, 5, 5, 1])
    out = tf.squeeze(out, axis=(0, 3))

    expected_out = np.asarray(expected_out, dtype=np.float32)
    if with_bias:
      expected_out += 1

    self.assertAllClose(self.evaluate(out), expected_out)

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/conv_transpose_test.py" startline="175" endline="195" pcid="579">
  def testComputationPaddingSame(self, with_bias):
    expected_out = [[4, 6, 4], [6, 9, 6], [4, 6, 4]]
    expected_out = np.asarray(expected_out, dtype=np.float32)
    if with_bias:
      expected_out += 1

    conv_transpose1 = conv_transpose.Conv2DTranspose(
        output_channels=1,
        output_shape=None,
        kernel_shape=3,
        stride=1,
        padding="SAME",
        with_bias=with_bias,
        **create_constant_initializers(1.0, 1.0, with_bias))

    out = conv_transpose1(tf.ones([1, 3, 3, 1], dtype=tf.float32))
    self.assertEqual(out.shape, [1, 3, 3, 1])
    out = tf.squeeze(out, axis=(0, 3))

    self.assertAllClose(self.evaluate(out), expected_out)

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/conv_transpose_test.py" startline="197" endline="218" pcid="580">
  def testComputationPaddingValid(self, with_bias):
    expected_out = [[1, 2, 3, 2, 1], [2, 4, 6, 4, 2], [3, 6, 9, 6, 3],
                    [2, 4, 6, 4, 2], [1, 2, 3, 2, 1]]
    expected_out = np.asarray(expected_out, dtype=np.float32)
    if with_bias:
      expected_out += 1

    conv1 = conv_transpose.Conv2DTranspose(
        output_channels=1,
        output_shape=None,
        kernel_shape=3,
        stride=1,
        padding="VALID",
        with_bias=with_bias,
        **create_constant_initializers(1.0, 1.0, with_bias))

    out = conv1(tf.ones([1, 3, 3, 1], dtype=tf.float32))
    self.assertEqual(out.shape, [1, 5, 5, 1])
    out = tf.squeeze(out, axis=(0, 3))

    self.assertAllClose(self.evaluate(out), expected_out)

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/conv_test.py" startline="277" endline="296" pcid="413">
  def testComputationPaddingSame(self, with_bias):
    expected_out = [2, 3, 3, 3, 2]
    conv1 = conv.Conv1D(
        output_channels=1,
        kernel_shape=3,
        stride=1,
        padding="SAME",
        with_bias=with_bias,
        **create_constant_initializers(1.0, 1.0, with_bias))

    out = conv1(tf.ones([1, 5, 1], dtype=tf.float32))
    self.assertEqual(out.shape, [1, 5, 1])
    out = tf.squeeze(out, axis=(0, 2))

    expected_out = np.asarray(expected_out, dtype=np.float32)
    if with_bias:
      expected_out += 1

    self.assertAllClose(self.evaluate(out), expected_out)

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/conv_test.py" startline="253" endline="273" pcid="412">
  def testComputationPaddingValid(self, with_bias):
    expected_out = [[9, 9, 9], [9, 9, 9], [9, 9, 9]]
    conv1 = conv.Conv2D(
        output_channels=1,
        kernel_shape=3,
        stride=1,
        padding="VALID",
        with_bias=with_bias,
        **create_constant_initializers(1.0, 1.0, with_bias))

    out = conv1(tf.ones([1, 5, 5, 1], dtype=tf.float32))
    self.assertEqual(out.shape, [1, 3, 3, 1])
    out = tf.squeeze(out, axis=(0, 3))

    expected_out = np.asarray(expected_out, dtype=np.float32)
    if with_bias:
      expected_out += 1

    self.assertAllClose(self.evaluate(out), expected_out)


</source>
<source file="systems/sonnet-2.0.0/sonnet/src/conv_transpose_test.py" startline="236" endline="256" pcid="582">
  def testComputationPaddingSame(self, with_bias):
    expected_out = [2, 3, 2]
    expected_out = np.asarray(expected_out, dtype=np.float32)
    if with_bias:
      expected_out += 1

    conv1 = conv_transpose.Conv1DTranspose(
        output_channels=1,
        output_shape=None,
        kernel_shape=3,
        stride=1,
        padding="SAME",
        with_bias=with_bias,
        **create_constant_initializers(1.0, 1.0, with_bias))

    out = conv1(tf.ones([1, 3, 1], dtype=tf.float32))
    self.assertEqual(out.shape, [1, 3, 1])
    out = tf.squeeze(out, axis=(0, 2))

    self.assertAllClose(self.evaluate(out), expected_out)

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/conv_transpose_test.py" startline="283" endline="305" pcid="584">
  def testComputationPaddingSame(self, with_bias):
    expected_out = np.asarray([
        8, 12, 8, 12, 18, 12, 8, 12, 8, 12, 18, 12, 18, 27, 18, 12, 18, 12, 8,
        12, 8, 12, 18, 12, 8, 12, 8
    ]).reshape((3, 3, 3))
    if with_bias:
      expected_out += 1

    conv_transpose1 = conv_transpose.Conv3DTranspose(
        output_channels=1,
        output_shape=None,
        kernel_shape=3,
        stride=1,
        padding="SAME",
        with_bias=with_bias,
        **create_constant_initializers(1.0, 1.0, with_bias))

    out = conv_transpose1(tf.ones([1, 3, 3, 3, 1], dtype=tf.float32))
    self.assertEqual(out.shape, [1, 3, 3, 3, 1])
    out = tf.squeeze(out, axis=(0, 4))

    self.assertAllClose(self.evaluate(out), expected_out)

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/conv_transpose_test.py" startline="258" endline="279" pcid="583">
  def testComputationPaddingValid(self, with_bias):
    expected_out = [1, 2, 3, 2, 1]
    expected_out = np.asarray(expected_out, dtype=np.float32)
    if with_bias:
      expected_out += 1

    conv1 = conv_transpose.Conv1DTranspose(
        output_channels=1,
        output_shape=None,
        kernel_shape=3,
        stride=1,
        padding="VALID",
        with_bias=with_bias,
        **create_constant_initializers(1.0, 1.0, with_bias))

    out = conv1(tf.ones([1, 3, 1], dtype=tf.float32))
    self.assertEqual(out.shape, [1, 5, 1])
    out = tf.squeeze(out, axis=(0, 2))

    self.assertAllClose(self.evaluate(out), expected_out)


</source>
<source file="systems/sonnet-2.0.0/sonnet/src/conv_test.py" startline="298" endline="318" pcid="414">
  def testComputationPaddingValid(self, with_bias):
    expected_out = [3, 3, 3]
    expected_out = np.asarray(expected_out, dtype=np.float32)
    if with_bias:
      expected_out += 1

    conv1 = conv.Conv1D(
        output_channels=1,
        kernel_shape=3,
        stride=1,
        padding="VALID",
        with_bias=with_bias,
        **create_constant_initializers(1.0, 1.0, with_bias))

    out = conv1(tf.ones([1, 5, 1], dtype=tf.float32))
    self.assertEqual(out.shape, [1, 3, 1])
    out = tf.squeeze(out, axis=(0, 2))

    self.assertAllClose(self.evaluate(out), expected_out)


</source>
<source file="systems/sonnet-2.0.0/sonnet/src/conv_transpose_test.py" startline="307" endline="334" pcid="585">
  def testComputationPaddingValid(self, with_bias):
    expected_out = np.asarray([
        1, 2, 3, 2, 1, 2, 4, 6, 4, 2, 3, 6, 9, 6, 3, 2, 4, 6, 4, 2, 1, 2, 3, 2,
        1, 2, 4, 6, 4, 2, 4, 8, 12, 8, 4, 6, 12, 18, 12, 6, 4, 8, 12, 8, 4, 2,
        4, 6, 4, 2, 3, 6, 9, 6, 3, 6, 12, 18, 12, 6, 9, 18, 27, 18, 9, 6, 12,
        18, 12, 6, 3, 6, 9, 6, 3, 2, 4, 6, 4, 2, 4, 8, 12, 8, 4, 6, 12, 18, 12,
        6, 4, 8, 12, 8, 4, 2, 4, 6, 4, 2, 1, 2, 3, 2, 1, 2, 4, 6, 4, 2, 3, 6, 9,
        6, 3, 2, 4, 6, 4, 2, 1, 2, 3, 2, 1.
    ]).reshape((5, 5, 5))
    if with_bias:
      expected_out += 1

    conv1 = conv_transpose.Conv3DTranspose(
        output_channels=1,
        output_shape=None,
        kernel_shape=3,
        stride=1,
        padding="VALID",
        with_bias=with_bias,
        **create_constant_initializers(1.0, 1.0, with_bias))

    out = conv1(tf.ones([1, 3, 3, 3, 1], dtype=tf.float32))
    self.assertEqual(out.shape, [1, 5, 5, 5, 1])
    out = tf.squeeze(out, axis=(0, 4))

    self.assertAllClose(self.evaluate(out), expected_out)


</source>
<source file="systems/sonnet-2.0.0/sonnet/src/conv_test.py" startline="350" endline="372" pcid="416">
  def testComputationPaddingValid(self, with_bias):
    expected_out = np.asarray([
        28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,
        28, 28, 28, 28, 28, 28, 28, 28, 28
    ]).reshape((3, 3, 3))
    if not with_bias:
      expected_out -= 1

    conv1 = conv.Conv3D(
        output_channels=1,
        kernel_shape=3,
        stride=1,
        padding="VALID",
        with_bias=with_bias,
        **create_constant_initializers(1.0, 1.0, with_bias))

    out = conv1(tf.ones([1, 5, 5, 5, 1], dtype=tf.float32))
    self.assertEqual(out.shape, [1, 3, 3, 3, 1])
    out = tf.squeeze(out, axis=(0, 4))

    self.assertAllClose(self.evaluate(out), expected_out)


</source>
<source file="systems/sonnet-2.0.0/sonnet/src/conv_test.py" startline="322" endline="348" pcid="415">
  def testComputationPaddingSame(self, with_bias):
    expected_out = np.asarray([
        9, 13, 13, 13, 9, 13, 19, 19, 19, 13, 13, 19, 19, 19, 13, 13, 19, 19,
        19, 13, 9, 13, 13, 13, 9, 13, 19, 19, 19, 13, 19, 28, 28, 28, 19, 19,
        28, 28, 28, 19, 19, 28, 28, 28, 19, 13, 19, 19, 19, 13, 13, 19, 19, 19,
        13, 19, 28, 28, 28, 19, 19, 28, 28, 28, 19, 19, 28, 28, 28, 19, 13, 19,
        19, 19, 13, 13, 19, 19, 19, 13, 19, 28, 28, 28, 19, 19, 28, 28, 28, 19,
        19, 28, 28, 28, 19, 13, 19, 19, 19, 13, 9, 13, 13, 13, 9, 13, 19, 19,
        19, 13, 13, 19, 19, 19, 13, 13, 19, 19, 19, 13, 9, 13, 13, 13, 9
    ]).reshape((5, 5, 5))
    if not with_bias:
      expected_out -= 1

    conv1 = conv.Conv3D(
        output_channels=1,
        kernel_shape=3,
        stride=1,
        padding="SAME",
        with_bias=with_bias,
        **create_constant_initializers(1.0, 1.0, with_bias))

    out = conv1(tf.ones([1, 5, 5, 5, 1], dtype=tf.float32))
    self.assertEqual(out.shape, [1, 5, 5, 5, 1])
    out = tf.squeeze(out, axis=(0, 4))

    self.assertAllClose(self.evaluate(out), expected_out)

</source>
</class>

<class classid="16" nclones="4" nlines="11" similarity="72">
<source file="systems/sonnet-2.0.0/sonnet/src/group_norm_test.py" startline="40" endline="53" pcid="441">
  def testSimpleCaseVar(self):
    layer = group_norm.GroupNorm(
        groups=5,
        create_scale=True,
        create_offset=True,
        scale_init=initializers.Constant(0.5),
        offset_init=initializers.Constant(2.0))

    inputs = tf.ones([2, 3, 3, 10])

    outputs = layer(inputs).numpy()
    for x in np.nditer(outputs):
      self.assertEqual(x, 2.0)

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/axis_norm_test.py" startline="52" endline="65" pcid="782">
  def testSimpleCaseNCHWVar(self):
    layer = axis_norm.LayerNorm([1, 2],
                                create_scale=True,
                                create_offset=True,
                                scale_init=initializers.Constant(0.5),
                                offset_init=initializers.Constant(2.0),
                                data_format="NCHW")

    inputs = tf.ones([2, 5, 3, 3])

    outputs = layer(inputs).numpy()
    for x in np.nditer(outputs):
      self.assertEqual(x, 2.0)

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/axis_norm_test.py" startline="39" endline="51" pcid="781">
  def testSimpleCaseVar(self):
    layer = axis_norm.LayerNorm([1, 2],
                                create_scale=True,
                                create_offset=True,
                                scale_init=initializers.Constant(0.5),
                                offset_init=initializers.Constant(2.0))

    inputs = tf.ones([2, 3, 3, 5])

    outputs = layer(inputs).numpy()
    for x in np.nditer(outputs):
      self.assertEqual(x, 2.0)

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/group_norm_test.py" startline="54" endline="68" pcid="442">
  def testSimpleCaseNCHWVar(self):
    layer = group_norm.GroupNorm(
        groups=5,
        create_scale=True,
        create_offset=True,
        scale_init=initializers.Constant(0.5),
        offset_init=initializers.Constant(2.0),
        data_format="NCHW")

    inputs = tf.ones([2, 10, 3, 3])

    outputs = layer(inputs).numpy()
    for x in np.nditer(outputs):
      self.assertEqual(x, 2.0)

</source>
</class>

<class classid="17" nclones="2" nlines="11" similarity="81">
<source file="systems/sonnet-2.0.0/sonnet/src/group_norm_test.py" startline="215" endline="229" pcid="456">
  def testRankChanges(self):
    layer = group_norm.GroupNorm(
        groups=5, create_scale=False, create_offset=False)

    inputs = tf.ones([2, 3, 3, 5])
    scale = tf.constant(0.5, shape=(5,))
    offset = tf.constant(2.0, shape=(5,))

    layer(inputs, scale, offset)

    with self.assertRaisesRegexp(
        ValueError,
        "The rank of the inputs cannot change between calls, the original"):
      layer(tf.ones([2, 3, 3, 4, 5]), scale, offset)

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/axis_norm_test.py" startline="208" endline="221" pcid="796">
  def testRankChanges(self):
    layer = axis_norm.LayerNorm((1, 2), create_scale=False, create_offset=False)

    inputs = tf.ones([2, 3, 3, 5])
    scale = tf.constant(0.5, shape=(5,))
    offset = tf.constant(2.0, shape=(5,))

    layer(inputs, scale, offset)

    with self.assertRaisesRegexp(
        ValueError,
        "The rank of the inputs cannot change between calls, the original"):
      layer(tf.ones([2, 3, 3, 4, 5]), scale, offset)

</source>
</class>

<class classid="18" nclones="2" nlines="19" similarity="84">
<source file="systems/sonnet-2.0.0/sonnet/src/group_norm_test.py" startline="256" endline="279" pcid="459">
  def testBatchSizeAgnostic(self):
    layer = group_norm.GroupNorm(
        groups=5, create_scale=False, create_offset=False)
    inputs_spec = tf.TensorSpec([None, 3, 3, 10], dtype=tf.float32)
    params_spec = tf.TensorSpec([None], dtype=tf.float32)
    function_layer = tf.function(layer).get_concrete_function(
        inputs_spec, params_spec, params_spec)

    scale = tf.constant(0.5, shape=(10,))
    offset = tf.constant(2.0, shape=(10,))

    outputs = function_layer(tf.ones([2, 3, 3, 10]), scale, offset)
    self.assertEqual(outputs.shape, [2, 3, 3, 10])
    for x in np.nditer(outputs):
      self.assertEqual(x, 2.0)

    scale = tf.constant(0.5, shape=(10,))
    offset = tf.constant(2.0, shape=(10,))

    outputs = function_layer(tf.ones([3, 3, 3, 10]), scale, offset)
    self.assertEqual(outputs.shape, [3, 3, 3, 10])
    for x in np.nditer(outputs):
      self.assertEqual(x, 2.0)

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/axis_norm_test.py" startline="235" endline="257" pcid="798">
  def testShapeAgnostic(self):
    layer = axis_norm.LayerNorm((1, 2), create_scale=False, create_offset=False)
    inputs_spec = tf.TensorSpec([None, None, None, None], dtype=tf.float32)
    params_spec = tf.TensorSpec([None], dtype=tf.float32)
    function_layer = tf.function(layer).get_concrete_function(
        inputs_spec, params_spec, params_spec)

    scale = tf.constant(0.5, shape=(5,))
    offset = tf.constant(2.0, shape=(5,))

    outputs = function_layer(tf.ones([2, 3, 3, 5]), scale, offset)
    self.assertEqual(outputs.shape, [2, 3, 3, 5])
    for x in np.nditer(outputs):
      self.assertEqual(x, 2.0)

    scale = tf.constant(0.5, shape=(3,))
    offset = tf.constant(2.0, shape=(3,))

    outputs = function_layer(tf.ones([3, 4, 6, 3]), scale, offset)
    self.assertEqual(outputs.shape, [3, 4, 6, 3])
    for x in np.nditer(outputs):
      self.assertEqual(x, 2.0)

</source>
</class>

<class classid="19" nclones="2" nlines="14" similarity="100">
<source file="systems/sonnet-2.0.0/sonnet/src/linear.py" startline="36" endline="63" pcid="465">
  def __init__(self,
               output_size: int,
               with_bias: bool = True,
               w_init: Optional[initializers.Initializer] = None,
               b_init: Optional[initializers.Initializer] = None,
               name: Optional[Text] = None):
    """Constructs a `Linear` module.

    Args:
      output_size: Output dimensionality.
      with_bias: Whether to include bias parameters. Default `True`.
      w_init: Optional initializer for the weights. By default the weights are
        initialized truncated random normal values with a standard deviation of
        `1 / sqrt(input_feature_size)`, which is commonly used when the inputs
        are zero centered (see https://arxiv.org/abs/1502.03167v3).
      b_init: Optional initializer for the bias. By default the bias is
        initialized to zero.
      name: Name of the module.
    """
    super(Linear, self).__init__(name=name)
    self.output_size = output_size
    self.with_bias = with_bias
    self.w_init = w_init
    if with_bias:
      self.b_init = b_init if b_init is not None else initializers.Zeros()
    elif b_init is not None:
      raise ValueError("When not using a bias the b_init must be None.")

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/parallel_linear.py" startline="44" endline="71" pcid="805">
  def __init__(self,
               output_size: int,
               with_bias: bool = True,
               w_init: Optional[initializers.Initializer] = None,
               b_init: Optional[initializers.Initializer] = None,
               name: Optional[Text] = None):
    """Constructs a `ParallelLinear` module.

    Args:
      output_size: Output dimensionality.
      with_bias: Whether to include bias parameters. Default `True`.
      w_init: Optional initializer for the weights. By default the weights are
        initialized truncated random normal values with a standard deviation of
        `1 / sqrt(input_feature_size)`, which is commonly used when the inputs
        are zero centered (see https://arxiv.org/abs/1502.03167v3).
      b_init: Optional initializer for the bias. By default the bias is
        initialized to zero.
      name: Name of the module.
    """
    super(ParallelLinears, self).__init__(name=name)
    self.output_size = output_size
    self.with_bias = with_bias
    self.w_init = w_init
    if with_bias:
      self.b_init = b_init if b_init is not None else initializers.Zeros()
    elif b_init is not None:
      raise ValueError("When not using a bias the b_init must be None.")

</source>
</class>

<class classid="20" nclones="2" nlines="15" similarity="93">
<source file="systems/sonnet-2.0.0/sonnet/src/nets/dnc/util_test.py" startline="33" endline="50" pcid="488">
  def testShape(self, initial_shape, final_shape):
    first_shape = tf.TensorShape([3, 3])
    second_shape = tf.TensorShape([5])
    segment_shapes = [first_shape, second_shape]

    inputs_shape = (
        initial_shape +
        [first_shape.num_elements() + second_shape.num_elements()] +
        final_shape)

    inputs = tf.random.uniform(inputs_shape)
    first, second = util.segment_dim(
        inputs, dim=len(initial_shape), shapes=segment_shapes)
    self.assertAllEqual(first.shape.as_list(),
                        initial_shape + first_shape.as_list() + final_shape)
    self.assertAllEqual(second.shape.as_list(),
                        initial_shape + second_shape.as_list() + final_shape)

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/nets/dnc/util_test.py" startline="53" endline="70" pcid="489">
  def testShapeNegative(self, initial_shape, final_shape):
    first_shape = tf.TensorShape([3, 3])
    second_shape = tf.TensorShape([5])
    segment_shapes = [first_shape, second_shape]

    inputs_shape = (
        initial_shape +
        [first_shape.num_elements() + second_shape.num_elements()] +
        final_shape)

    inputs = tf.random.uniform(inputs_shape)
    first, second = util.segment_dim(
        inputs, dim=-len(final_shape) - 1, shapes=segment_shapes)
    self.assertAllEqual(first.shape.as_list(),
                        initial_shape + first_shape.as_list() + final_shape)
    self.assertAllEqual(second.shape.as_list(),
                        initial_shape + second_shape.as_list() + final_shape)

</source>
</class>

<class classid="21" nclones="5" nlines="11" similarity="75">
<source file="systems/sonnet-2.0.0/sonnet/src/nets/dnc/write_test.py" startline="28" endline="40" pcid="499">
  def testShape(self):
    batch_size = 16
    num_writes = 2
    memory_size = 5
    word_size = 3

    mem = tf.random.uniform([batch_size, memory_size, word_size])
    write_address = tf.random.uniform([batch_size, num_writes, memory_size])
    reset_row_weights = tf.random.uniform([batch_size, num_writes])
    eraser = write.erase_rows(mem, write_address, reset_row_weights)
    self.assertAllEqual(eraser.shape.as_list(),
                        [batch_size, memory_size, word_size])

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/nets/dnc/write_test.py" startline="178" endline="190" pcid="505">
  def testShape(self):
    batch_size = 4
    num_writes = 2
    memory_size = 5
    word_size = 3

    mem = tf.random.uniform([batch_size, memory_size, word_size])
    write_address = tf.random.uniform([batch_size, num_writes, memory_size])
    values = tf.random.uniform([batch_size, num_writes, word_size])
    writer = write.additive_write(mem, write_address, values)
    self.assertAllEqual(writer.shape.as_list(),
                        [batch_size, memory_size, word_size])

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/nets/dnc/read_test.py" startline="28" endline="39" pcid="526">
  def testShape(self):
    batch_size = 4
    num_reads = 2
    memory_size = 5
    word_size = 3

    mem = tf.random.uniform([batch_size, memory_size, word_size])
    weights = tf.random.uniform([batch_size, num_reads, memory_size])
    values_read = read.read(mem, weights)
    self.assertAllEqual(values_read.shape.as_list(),
                        [batch_size, num_reads, word_size])

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/nets/dnc/write_test.py" startline="84" endline="96" pcid="501">
  def testShape(self):
    batch_size = 1
    num_writes = 2
    memory_size = 5
    word_size = 3

    mem = tf.random.uniform([batch_size, memory_size, word_size])
    write_address = tf.random.uniform([batch_size, num_writes, memory_size])
    reset_weights = tf.random.uniform([batch_size, num_writes, word_size])
    writer = write.erase(mem, write_address, reset_weights)
    self.assertTrue(writer.shape.as_list(),
                    [batch_size, memory_size, word_size])

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/nets/dnc/write_test.py" startline="137" endline="150" pcid="503">
  def testShape(self):
    batch_size = 4
    num_writes = 2
    memory_size = 5
    word_size = 3

    mem = tf.random.uniform([batch_size, memory_size, word_size])
    write_address = tf.random.uniform([batch_size, num_writes, memory_size])
    reset_weights = tf.random.uniform([batch_size, num_writes, word_size])
    values = tf.random.uniform([batch_size, num_writes, word_size])
    writer = write.erase_and_write(mem, write_address, reset_weights, values)
    self.assertTrue(writer.shape.as_list(),
                    [batch_size, memory_size, word_size])

</source>
</class>

<class classid="22" nclones="2" nlines="25" similarity="76">
<source file="systems/sonnet-2.0.0/sonnet/src/nets/dnc/write_test.py" startline="41" endline="81" pcid="500">
  def testValues(self):
    num_writes = 2
    memory_size = 5
    word_size = 3

    # Random memory, weights and values (batch_size=1)
    mem = tf.random.uniform((1, memory_size, word_size))
    mem_np = mem.numpy()
    # Non-repeated indices in [0, memory_size)
    perm = np.random.permutation(memory_size)
    indices_np = perm[:num_writes]
    excluded_indices_np = perm[num_writes:]

    # One-hot representation
    write_address = tf.constant(
        np.expand_dims(np.eye(memory_size)[indices_np], axis=0),
        dtype=tf.float32)
    reset_row_weights = tf.ones((1, num_writes))

    erased_mem = write.erase_rows(mem, write_address, reset_row_weights)

    not_erased_mem = write.erase_rows(mem, write_address, reset_row_weights * 0)

    erased_mem_np = erased_mem.numpy()

    # Rows specified in indices should have been erased.
    self.assertAllClose(
        erased_mem_np[0, indices_np, :],
        np.zeros((num_writes, word_size)),
        atol=2e-3)

    # Other rows should not have been erased.
    self.assertAllClose(
        erased_mem_np[0, excluded_indices_np, :],
        mem_np[0, excluded_indices_np, :],
        atol=2e-3)

    # Write with reset weights zero'd out and nothing should change.
    self.assertAllEqual(not_erased_mem.numpy(), mem_np)


</source>
<source file="systems/sonnet-2.0.0/sonnet/src/nets/dnc/write_test.py" startline="97" endline="134" pcid="502">
  def testValues(self):
    num_writes = 2
    memory_size = 5
    word_size = 3

    # Random memory, weights and values (batch_size=1)
    mem = tf.random.uniform([1, memory_size, word_size])
    mem_np = mem.numpy()
    # Non-repeated indices in [0, memory_size)
    perm = np.random.permutation(memory_size)
    indices = perm[:num_writes]
    excluded_indices = perm[num_writes:]
    # One-hot representation
    write_address = tf.constant(
        np.expand_dims(np.eye(memory_size)[indices], axis=0), dtype=tf.float32)
    reset_weights = tf.ones([1, num_writes, word_size])
    erased_mem = write.erase(mem, write_address, reset_weights)
    not_erased_mem = write.erase(mem, write_address, reset_weights * 0.)

    erased_mem_np = erased_mem.numpy()
    not_erased_mem_np = not_erased_mem.numpy()

    # Rows specified in indices should have been erased.
    self.assertAllClose(
        erased_mem_np[0, indices, :],
        np.zeros((num_writes, word_size)),
        atol=2e-3)

    # Other rows should not have been erased.
    self.assertAllClose(
        erased_mem_np[0, excluded_indices, :],
        mem_np[0, excluded_indices, :],
        atol=2e-3)

    # Write with reset weights zero'd out and nothing should change.
    self.assertAllEqual(not_erased_mem_np, mem_np)


</source>
</class>

<class classid="23" nclones="2" nlines="50" similarity="74">
<source file="systems/sonnet-2.0.0/sonnet/src/nets/resnet.py" startline="36" endline="95" pcid="531">
  def __init__(self,
               channels: int,
               stride: Union[int, Sequence[int]],
               use_projection: bool,
               bn_config: Mapping[Text, float],
               name: Optional[Text] = None):
    super(BottleNeckBlockV1, self).__init__(name=name)
    self._channels = channels
    self._stride = stride
    self._use_projection = use_projection
    self._bn_config = bn_config

    batchnorm_args = {"create_scale": True, "create_offset": True}
    batchnorm_args.update(bn_config)

    if self._use_projection:
      self._proj_conv = conv.Conv2D(
          output_channels=channels,
          kernel_shape=1,
          stride=stride,
          with_bias=False,
          padding=pad.same,
          name="shortcut_conv")
      self._proj_batchnorm = batch_norm.BatchNorm(
          name="shortcut_batchnorm", **batchnorm_args)

    self._layers = []
    conv_0 = conv.Conv2D(
        output_channels=channels // 4,
        kernel_shape=1,
        stride=1,
        with_bias=False,
        padding=pad.same,
        name="conv_0")
    self._layers.append(
        [conv_0,
         batch_norm.BatchNorm(name="batchnorm_0", **batchnorm_args)])

    conv_1 = conv.Conv2D(
        output_channels=channels // 4,
        kernel_shape=3,
        stride=stride,
        with_bias=False,
        padding=pad.same,
        name="conv_1")
    self._layers.append(
        [conv_1,
         batch_norm.BatchNorm(name="batchnorm_1", **batchnorm_args)])

    conv_2 = conv.Conv2D(
        output_channels=channels,
        kernel_shape=1,
        stride=1,
        with_bias=False,
        padding=pad.same,
        name="conv_2")
    batchnorm_2 = batch_norm.BatchNorm(
        name="batchnorm_2", scale_init=initializers.Zeros(), **batchnorm_args)
    self._layers.append([conv_2, batchnorm_2])

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/nets/resnet.py" startline="115" endline="170" pcid="533">
  def __init__(self,
               channels: int,
               stride: Union[int, Sequence[int]],
               use_projection: bool,
               bn_config: Mapping[Text, float],
               name: Optional[Text] = None):
    super(BottleNeckBlockV2, self).__init__(name=name)
    self._channels = channels
    self._stride = stride
    self._use_projection = use_projection
    self._bn_config = bn_config

    batchnorm_args = {"create_scale": True, "create_offset": True}
    batchnorm_args.update(bn_config)

    if self._use_projection:
      self._proj_conv = conv.Conv2D(
          output_channels=channels,
          kernel_shape=1,
          stride=stride,
          with_bias=False,
          padding=pad.same,
          name="shortcut_conv")

    self._conv_0 = conv.Conv2D(
        output_channels=channels // 4,
        kernel_shape=1,
        stride=1,
        with_bias=False,
        padding=pad.same,
        name="conv_0")

    self._bn_0 = batch_norm.BatchNorm(name="batchnorm_0", **batchnorm_args)

    self._conv_1 = conv.Conv2D(
        output_channels=channels // 4,
        kernel_shape=3,
        stride=stride,
        with_bias=False,
        padding=pad.same,
        name="conv_1")

    self._bn_1 = batch_norm.BatchNorm(name="batchnorm_1", **batchnorm_args)

    self._conv_2 = conv.Conv2D(
        output_channels=channels,
        kernel_shape=1,
        stride=1,
        with_bias=False,
        padding=pad.same,
        name="conv_2")

    # NOTE: Some implementations of ResNet50 v2 suggest initializing gamma/scale
    # here to zeros.
    self._bn_2 = batch_norm.BatchNorm(name="batchnorm_2", **batchnorm_args)

</source>
</class>

<class classid="24" nclones="4" nlines="11" similarity="83">
<source file="systems/sonnet-2.0.0/sonnet/src/recurrent_test.py" startline="70" endline="83" pcid="626">
  def testInitialization(self):
    core = recurrent.VanillaRNN(
        hidden_size=self.hidden_size,
        w_i_init=initializers.Ones(),
        w_h_init=initializers.Ones(),
        b_init=initializers.Ones())
    inputs = tf.random.uniform([self.batch_size, self.input_size])
    prev_state = core.initial_state(self.batch_size)
    core(inputs, prev_state)

    for v in core.variables:
      self.assertAllClose(self.evaluate(v), self.evaluate(tf.ones_like(v)))


</source>
<source file="systems/sonnet-2.0.0/sonnet/src/recurrent_test.py" startline="486" endline="499" pcid="654">
  def testInitialization(self):
    core = recurrent.GRU(
        hidden_size=self.hidden_size,
        w_i_init=initializers.Ones(),
        w_h_init=initializers.Ones(),
        b_init=initializers.Ones())
    inputs = tf.random.uniform([self.batch_size, self.input_size])
    prev_state = core.initial_state(self.batch_size)
    core(inputs, prev_state)

    for v in core.variables:
      self.assertAllClose(self.evaluate(v), self.evaluate(tf.ones_like(v)))


</source>
<source file="systems/sonnet-2.0.0/sonnet/src/recurrent_test.py" startline="337" endline="351" pcid="645">
  def testInitialization(self):
    unrolled_lstm = recurrent.UnrolledLSTM(
        hidden_size=self.hidden_size,
        forget_bias=0.0,
        w_i_init=initializers.Ones(),
        w_h_init=initializers.Ones(),
        b_init=initializers.Ones())
    input_sequence = tf.random.uniform([1, self.batch_size, self.input_size])
    initial_state = unrolled_lstm.initial_state(self.batch_size)
    unrolled_lstm(input_sequence, initial_state)

    for v in unrolled_lstm.variables:
      self.assertAllClose(self.evaluate(v), self.evaluate(tf.ones_like(v)))


</source>
<source file="systems/sonnet-2.0.0/sonnet/src/recurrent_test.py" startline="563" endline="576" pcid="660">
  def testInitialization(self):
    core = recurrent.CuDNNGRU(
        hidden_size=self.hidden_size,
        w_i_init=initializers.Ones(),
        w_h_init=initializers.Ones(),
        b_init=initializers.Ones())
    inputs = tf.random.uniform([1, self.batch_size, self.input_size])
    prev_state = core.initial_state(self.batch_size)
    core(inputs, prev_state)

    for v in core.variables:
      self.assertAllClose(self.evaluate(v), self.evaluate(tf.ones_like(v)))


</source>
</class>

<class classid="25" nclones="3" nlines="13" similarity="71">
<source file="systems/sonnet-2.0.0/sonnet/src/recurrent_test.py" startline="782" endline="797" pcid="678">
  def testVariableLengthOneZeroLength(self, use_tf_function, unroll_fn):
    if use_tf_function:
      unroll_fn = tf.function(unroll_fn)

    sequence_length = tf.constant([0] + [self.num_steps] *
                                  (self.batch_size - 1))
    initial_state = self.core.initial_state(self.batch_size)
    input_sequence = tf.random.uniform([self.num_steps, self.batch_size, 1])
    output_sequence, _ = unroll_fn(
        self.core,
        input_sequence,
        initial_state,
        sequence_length=sequence_length)

    self.assertConsistentWithLength(output_sequence, sequence_length)

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/recurrent_test.py" startline="798" endline="812" pcid="679">
  def testVariableLengthRange(self, use_tf_function, unroll_fn):
    if use_tf_function:
      unroll_fn = tf.function(unroll_fn)

    sequence_length = tf.range(self.batch_size)
    initial_state = self.core.initial_state(self.batch_size)
    input_sequence = tf.random.uniform([self.num_steps, self.batch_size, 1])
    output_sequence, _ = unroll_fn(
        self.core,
        input_sequence,
        initial_state,
        sequence_length=sequence_length)

    self.assertConsistentWithLength(output_sequence, sequence_length)

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/recurrent_test.py" startline="823" endline="838" pcid="681">
  def testVariableLengthAllFull(self, use_tf_function, unroll_fn):
    if use_tf_function:
      unroll_fn = tf.function(unroll_fn)

    initial_state = self.core.initial_state(self.batch_size)
    input_sequence = tf.random.uniform([self.num_steps, self.batch_size, 1])
    output_sequence, final_state = unroll_fn(
        self.core,
        input_sequence,
        initial_state,
        sequence_length=tf.constant([self.num_steps] * self.batch_size))
    expected_output_sequence, expected_final_state = unroll_fn(
        self.core, input_sequence, initial_state)
    self.assertAllClose(output_sequence, expected_output_sequence)
    self.assertAllClose(final_state, expected_final_state)

</source>
</class>

<class classid="26" nclones="2" nlines="10" similarity="90">
<source file="systems/sonnet-2.0.0/sonnet/src/mixed_precision_test.py" startline="84" endline="96" pcid="721">
  def test_float16_mode_disable_class(self, test_class):
    mixed_precision.enable(tf.float32)

    x = tf.Variable([[1., 9.], [5., 0.]])
    d = test_class(x)
    d.check_type = mixed_precision.modes([tf.float32, tf.float16])(d.check_type)

    mixed_precision.enable(tf.float16)
    self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)
    self.assertEqual(d.check_type(x, tf.float16).dtype, tf.float32)
    mixed_precision.disable()
    self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/mixed_precision_test.py" startline="253" endline="266" pcid="735">
  def test_scoping_disable(self, test_class):
    mixed_precision.enable(tf.float32)

    x = tf.Variable([[1., 9.], [8., 9.]])
    d = test_class(x)
    d.check_type = mixed_precision.modes([tf.float32, tf.float16])(d.check_type)

    with mixed_precision.scope(tf.float16):
      self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)
      self.assertEqual(d.check_type(x, tf.float16).dtype, tf.float32)

      mixed_precision.disable()
      self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)

</source>
</class>

<class classid="27" nclones="3" nlines="14" similarity="73">
<source file="systems/sonnet-2.0.0/sonnet/src/mixed_precision_test.py" startline="120" endline="136" pcid="723">
  def test_float16_mode_eligible_multiple_instances_class(self, test_class):
    mixed_precision.enable(tf.float32)

    x = tf.Variable([[1., 9.], [5., 0.]])
    d = test_class(x)
    d.check_type = mixed_precision.modes([tf.float32, tf.float16])(d.check_type)

    d2 = test_class(x)
    d2.check_type = mixed_precision.modes([tf.float32, tf.float16])(
        d2.check_type)

    mixed_precision.enable(tf.float16)
    self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)
    self.assertEqual(d.check_type(x, tf.float16).dtype, tf.float32)
    self.assertEqual(d2.check_type(x, tf.float32).dtype, tf.float32)
    self.assertEqual(d2.check_type(x, tf.float16).dtype, tf.float32)

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/mixed_precision_test.py" startline="137" endline="154" pcid="724">
  def test_float16_mode_ineligible_multiple_instances_class(self, test_class):
    mixed_precision.enable(tf.float32)

    x = tf.Variable([[1., 9.], [5., 0.]])
    d = test_class(x)
    d.check_type = mixed_precision.modes([tf.float32, tf.bfloat16])(
        d.check_type)

    d2 = test_class(x)
    d2.check_type = mixed_precision.modes([tf.float32, tf.bfloat16])(
        d2.check_type)

    mixed_precision.enable(tf.float16)
    self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)
    self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)
    self.assertEqual(d2.check_type(x, tf.float32).dtype, tf.float32)
    self.assertEqual(d2.check_type(x, tf.float32).dtype, tf.float32)

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/mixed_precision_test.py" startline="155" endline="173" pcid="725">
  def test_float16_mode_multiple_instances_different_eligibility_class(
      self, test_class):
    mixed_precision.enable(tf.float32)

    x = tf.Variable([[1., 9.], [5., 0.]])
    d = test_class(x)
    d.check_type = mixed_precision.modes([tf.float32, tf.bfloat16])(
        d.check_type)

    d2 = test_class(x)
    d2.check_type = mixed_precision.modes([tf.float32, tf.float16])(
        d2.check_type)

    mixed_precision.enable(tf.float16)
    self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)
    self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)
    self.assertEqual(d2.check_type(x, tf.float32).dtype, tf.float32)
    self.assertEqual(d2.check_type(x, tf.float16).dtype, tf.float32)

</source>
</class>

<class classid="28" nclones="2" nlines="11" similarity="100">
<source file="systems/sonnet-2.0.0/sonnet/src/mixed_precision_test.py" startline="198" endline="212" pcid="728">
  def test_function_create_module_eligible(self, test_class):
    mixed_precision.enable(tf.float16)

    @mixed_precision.modes([tf.float32, tf.float16])
    def model():
      x = tf.Variable([[1., 9.], [8., 9.]])
      d = test_class(x)
      d.check_type = mixed_precision.modes([tf.float32, tf.float16])(
          d.check_type)

      self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)
      self.assertEqual(d.check_type(x, tf.float16).dtype, tf.float32)

    model()

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/mixed_precision_test.py" startline="213" endline="227" pcid="730">
  def test_function_create_module_ineligible(self, test_class):
    mixed_precision.enable(tf.float16)

    @mixed_precision.modes([tf.float32, tf.float16])
    def model():
      x = tf.Variable([[1., 9.], [8., 9.]])
      d = test_class(x)
      d.check_type = mixed_precision.modes([tf.float32, tf.bfloat16])(
          d.check_type)

      self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)
      self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)

    model()

</source>
</class>

<class classid="29" nclones="2" nlines="11" similarity="90">
<source file="systems/sonnet-2.0.0/sonnet/src/mixed_precision_test.py" startline="303" endline="316" pcid="739">
  def test_float32_mode_eligible_func(self):
    mixed_precision.enable(tf.float32)
    self.assertEqual(mixed_precision._get_mixed_precision_mode(), tf.float32)

    @mixed_precision.modes([tf.float32, tf.float16])
    def fwd_func(x):
      self.assertEqual(x.dtype, tf.float32)
      return x

    x = tf.Variable([[1., 3], [5., 7.]])
    self.assertEqual(x.dtype, tf.float32)
    self.assertEqual(fwd_func(x).dtype, tf.float32)
    self.assertEqual(fwd_func(x).dtype, tf.float32)

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/mixed_precision_test.py" startline="317" endline="331" pcid="741">
  def test_float16_mode_ineligible_func(self):
    mixed_precision.enable(tf.float32)

    @mixed_precision.modes([tf.float32, tf.bfloat16])
    def fwd_func(x):
      self.assertEqual(x.dtype, tf.float32)
      return x

    x = tf.Variable([[1., 3], [5., 7.]])
    self.assertEqual(x.dtype, tf.float32)

    mixed_precision.enable(tf.float16)
    self.assertEqual(fwd_func(x).dtype, tf.float32)
    self.assertEqual(fwd_func(x).dtype, tf.float32)

</source>
</class>

<class classid="30" nclones="3" nlines="14" similarity="78">
<source file="systems/sonnet-2.0.0/sonnet/src/depthwise_conv_test.py" startline="159" endline="177" pcid="760">
  def testComputationSame(self, with_bias):
    conv1 = depthwise_conv.DepthwiseConv2D(
        channel_multiplier=1,
        kernel_shape=[3, 3],
        stride=1,
        padding="SAME",
        with_bias=with_bias,
        **create_constant_initializers(1.0, 1.0, with_bias))

    out = conv1(tf.ones([1, 5, 5, 1]))
    expected_out = np.array([[5, 7, 7, 7, 5], [7, 10, 10, 10, 7],
                             [7, 10, 10, 10, 7], [7, 10, 10, 10, 7],
                             [5, 7, 7, 7, 5]])
    if not with_bias:
      expected_out -= 1

    self.assertEqual(out.shape, [1, 5, 5, 1])
    self.assertAllClose(np.reshape(out.numpy(), [5, 5]), expected_out)

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/depthwise_conv_test.py" startline="179" endline="195" pcid="761">
  def testComputationValid(self, with_bias):
    conv1 = depthwise_conv.DepthwiseConv2D(
        channel_multiplier=1,
        kernel_shape=[3, 3],
        stride=1,
        padding="VALID",
        with_bias=with_bias,
        **create_constant_initializers(1.0, 1.0, with_bias))

    out = conv1(tf.ones([1, 5, 5, 1]))
    expected_out = np.array([[10, 10, 10], [10, 10, 10], [10, 10, 10]])
    if not with_bias:
      expected_out -= 1

    self.assertEqual(out.shape, [1, 3, 3, 1])
    self.assertAllClose(np.reshape(out.numpy(), [3, 3]), expected_out)

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/depthwise_conv_test.py" startline="197" endline="212" pcid="762">
  def testComputationValidMultiChannel(self, with_bias):
    conv1 = depthwise_conv.DepthwiseConv2D(
        channel_multiplier=1,
        kernel_shape=[3, 3],
        stride=1,
        padding="VALID",
        with_bias=with_bias,
        **create_constant_initializers(1.0, 1.0, with_bias))

    out = conv1(tf.ones([1, 5, 5, 3]))
    expected_out = np.array([[[10] * 3] * 3] * 3)
    if not with_bias:
      expected_out -= 1

    self.assertAllClose(np.reshape(out.numpy(), [3, 3, 3]), expected_out)

</source>
</class>

<class classid="31" nclones="2" nlines="11" similarity="81">
<source file="systems/sonnet-2.0.0/sonnet/src/utils_test.py" startline="105" endline="120" pcid="844">
  def test_unbound_method(self):

    @utils.decorator
    def double(wrapped, instance, args, kwargs):
      self.assertIs(instance, o)
      return 2 * wrapped(*args, **kwargs)

    class MyObject(object):

      @double
      def f(self, x, y):
        return x**y

    o = MyObject()
    self.assertEqual(o.f(3, 4), 2 * (3**4))

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/utils_test.py" startline="121" endline="136" pcid="847">
  def test_bound_method(self):

    @utils.decorator
    def double(wrapped, instance, args, kwargs):
      self.assertIs(instance, o)
      return 2 * wrapped(*args, **kwargs)

    class MyObject(object):

      def f(self, x, y):
        return x**y

    o = MyObject()
    self.assertEqual(double(o.f)(3, 4), 2 * (3**4))  # pylint: disable=no-value-for-parameter


</source>
</class>

<class classid="32" nclones="2" nlines="15" similarity="86">
<source file="systems/sonnet-2.0.0/sonnet/src/conformance/pickle_test.py" startline="35" endline="53" pcid="999">
  def test_pickle(self, golden):
    m1 = golden.create_module()
    golden.create_all_variables(m1)
    m2 = pickle.loads(pickle.dumps(m1))
    self.assertIsNot(m1, m2)

    # Check that module variables are recreated with equivalent properties.
    for v1, v2 in zip(m1.variables, m2.variables):
      self.assertIsNot(v1, v2)
      self.assertEqual(v1.name, v2.name)
      self.assertEqual(v1.device, v2.device)
      self.assertAllEqual(v1.read_value(), v2.read_value())

    if golden.deterministic:
      y1 = golden.forward(m1)
      y2 = golden.forward(m2)
      tree.map_structure(self.assertAllEqual, y1, y2)


</source>
<source file="systems/sonnet-2.0.0/sonnet/src/conformance/copy_test.py" startline="34" endline="53" pcid="1005">
  def test_copy(self, golden):
    m1 = golden.create_module()
    golden.create_all_variables(m1)
    m2 = copy.deepcopy(m1)
    self.assertIsNot(m1, m2)

    # Check that module variables are recreated with equivalent properties.
    for v1, v2 in zip(m1.variables, m2.variables):
      self.assertIsNot(v1, v2)
      if tf.version.GIT_VERSION == "unknown":
        # TODO(tomhennigan) Remove version check when TF 2.1 is released.
        self.assertEqual(v1.name, v2.name)
      self.assertEqual(v1.device, v2.device)
      self.assertAllEqual(v1.read_value(), v2.read_value())

    if golden.deterministic:
      y1 = golden.forward(m1)
      y2 = golden.forward(m2)
      tree.map_structure(self.assertAllEqual, y1, y2)

</source>
</class>

<class classid="33" nclones="3" nlines="11" similarity="83">
<source file="systems/sonnet-2.0.0/sonnet/src/conformance/function_test.py" startline="40" endline="50" pcid="1012">
  def test_trace(
      self,
      module_fn: ModuleFn,
      input_shape: Tuple[int],
      dtype: tf.DType,
      autograph: bool,
  ):
    module = module_fn()
    forward = tf.function(module, autograph=autograph)
    forward(tf.ones(input_shape, dtype=dtype))

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/conformance/function_test.py" startline="53" endline="64" pcid="1013">
  def test_create_variables_eagerly(
      self,
      module_fn: ModuleFn,
      input_shape: Tuple[int],
      dtype: tf.DType,
      autograph: bool,
  ):
    module = module_fn()
    f = snt.distribute.create_variables_eagerly(module)
    forward = tf.function(f, autograph=autograph)
    forward(tf.ones(input_shape, dtype=dtype))

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/conformance/function_test.py" startline="67" endline="79" pcid="1014">
  def test_trace_batch_agnostic(
      self,
      module_fn: ModuleFn,
      input_shape: Tuple[int],
      dtype: tf.DType,
      autograph: bool,
  ):
    module = module_fn()
    forward = tf.function(module, autograph=autograph)
    input_spec = tf.TensorSpec((None,) + input_shape[1:], dtype=dtype)
    cf = forward.get_concrete_function(input_spec)
    cf(tf.ones(input_shape, dtype=dtype))

</source>
</class>

</clones>

<clones>
<systeminfo processor="nicad6" system="MONAI-0.8.0" granularity="functions-blind" threshold="30%" minlines="10" maxlines="2500"/>
<cloneinfo npcs="1782" npairs="159"/>
<runinfo ncompares="72423" cputime="83168"/>
<classinfo nclasses="83"/>

<class classid="1" nclones="2" nlines="11" similarity="80">
<source file="systems/MONAI-0.8.0/tests/test_concat_itemsd.py" startline="21" endline="32" pcid="6">
    def test_tensor_values(self):
        device = torch.device("cuda:0") if torch.cuda.is_available() else torch.device("cpu:0")
        input_data = {
            "img1": torch.tensor([[0, 1], [1, 2]], device=device),
            "img2": torch.tensor([[0, 1], [1, 2]], device=device),
        }
        result = ConcatItemsd(keys=["img1", "img2"], name="cat_img")(input_data)
        self.assertTrue("cat_img" in result)
        result["cat_img"] += 1
        torch.testing.assert_allclose(result["img1"], torch.tensor([[0, 1], [1, 2]], device=device))
        torch.testing.assert_allclose(result["cat_img"], torch.tensor([[1, 2], [2, 3], [1, 2], [2, 3]], device=device))

</source>
<source file="systems/MONAI-0.8.0/tests/test_copy_itemsd.py" startline="42" endline="53" pcid="1123">
    def test_tensor_values(self):
        device = torch.device("cuda:0") if torch.cuda.is_available() else torch.device("cpu:0")
        input_data = {
            "img": torch.tensor([[0, 1], [1, 2]], device=device),
            "seg": torch.tensor([[0, 1], [1, 2]], device=device),
        }
        result = CopyItemsd(keys="img", times=1, names="img_1")(input_data)
        self.assertTrue("img_1" in result)
        result["img_1"] += 1
        torch.testing.assert_allclose(result["img"], torch.tensor([[0, 1], [1, 2]], device=device))
        torch.testing.assert_allclose(result["img_1"], torch.tensor([[1, 2], [2, 3]], device=device))

</source>
</class>

<class classid="2" nclones="2" nlines="26" similarity="75">
<source file="systems/MONAI-0.8.0/tests/test_prepare_batch_default.py" startline="26" endline="52" pcid="13">
    def test_content(self):
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        dataloader = [
            {
                "image": torch.tensor([1, 2]),
                "label": torch.tensor([3, 4]),
                "extra1": torch.tensor([5, 6]),
                "extra2": 16,
                "extra3": "test",
            }
        ]
        # set up engine
        evaluator = SupervisedEvaluator(
            device=device,
            val_data_loader=dataloader,
            epoch_length=1,
            network=TestNet(),
            non_blocking=False,
            prepare_batch=PrepareBatchDefault(),
            decollate=False,
        )
        evaluator.run()
        output = evaluator.state.output
        assert_allclose(output["image"], torch.tensor([1, 2], device=device))
        assert_allclose(output["label"], torch.tensor([3, 4], device=device))


</source>
<source file="systems/MONAI-0.8.0/tests/test_prepare_batch_extra_input.py" startline="43" endline="74" pcid="99">
    def test_content(self, input_args, expected_value):
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        dataloader = [
            {
                "image": torch.tensor([1, 2]),
                "label": torch.tensor([3, 4]),
                "extra1": torch.tensor([5, 6]),
                "extra2": 16,
                "extra3": "test",
            }
        ]
        # set up engine
        evaluator = SupervisedEvaluator(
            device=device,
            val_data_loader=dataloader,
            epoch_length=1,
            network=TestNet(),
            non_blocking=True,
            prepare_batch=PrepareBatchExtraInput(**input_args),
            decollate=False,
        )
        evaluator.run()
        output = evaluator.state.output
        assert_allclose(output["image"], torch.tensor([1, 2], device=device))
        assert_allclose(output["label"], torch.tensor([3, 4], device=device))
        for k, v in output["pred"].items():
            if isinstance(v, torch.Tensor):
                assert_allclose(v, expected_value[k].to(device))
            else:
                self.assertEqual(v, expected_value[k])


</source>
</class>

<class classid="3" nclones="2" nlines="17" similarity="94">
<source file="systems/MONAI-0.8.0/tests/hvd_evenly_divisible_all_gather.py" startline="26" endline="46" pcid="23">
    def _run(self):
        if hvd.rank() == 0:
            data1 = torch.tensor([[1, 2], [3, 4]])
            data2 = torch.tensor([[1.0, 2.0]])
            data3 = torch.tensor(7)

        if hvd.rank() == 1:
            data1 = torch.tensor([[5, 6]])
            data2 = torch.tensor([[3.0, 4.0], [5.0, 6.0]])
            data3 = torch.tensor(8)

        result1 = evenly_divisible_all_gather(data=data1, concat=True)
        torch.testing.assert_allclose(result1, torch.tensor([[1, 2], [3, 4], [5, 6]]))
        result2 = evenly_divisible_all_gather(data=data2, concat=False)
        for r, e in zip(result2, [torch.tensor([[1.0, 2.0]]), torch.tensor([[3.0, 4.0], [5.0, 6.0]])]):
            torch.testing.assert_allclose(r, e)
        result3 = evenly_divisible_all_gather(data=data3, concat=False)
        for r in result3:
            torch.testing.assert_allclose(r.ndimension(), 0)


</source>
<source file="systems/MONAI-0.8.0/tests/test_evenly_divisible_all_gather_dist.py" startline="26" endline="46" pcid="942">
    def _run(self):
        if dist.get_rank() == 0:
            data1 = torch.tensor([[1, 2], [3, 4]])
            data2 = torch.tensor([[1.0, 2.0]])
            data3 = torch.tensor(7)

        if dist.get_rank() == 1:
            data1 = torch.tensor([[5, 6]])
            data2 = torch.tensor([[3.0, 4.0], [5.0, 6.0]])
            data3 = torch.tensor(8)

        result1 = evenly_divisible_all_gather(data=data1, concat=True)
        torch.testing.assert_allclose(result1, torch.tensor([[1, 2], [3, 4], [5, 6]]))
        result2 = evenly_divisible_all_gather(data=data2, concat=False)
        for r, e in zip(result2, [torch.tensor([[1.0, 2.0]]), torch.tensor([[3.0, 4.0], [5.0, 6.0]])]):
            torch.testing.assert_allclose(r, e)
        result3 = evenly_divisible_all_gather(data=data3, concat=False)
        for r in result3:
            self.assertEqual(r.ndimension(), 0)


</source>
</class>

<class classid="4" nclones="2" nlines="29" similarity="72">
<source file="systems/MONAI-0.8.0/tests/test_nvtx_transform.py" startline="69" endline="102" pcid="42">
    def test_nvtx_transfroms_array(self, input):
        # with prob == 0.0
        transforms = Compose(
            [
                RandMark("Mark: Transforms Start!"),
                RandRangePush("Range: RandFlip"),
                RandFlip(prob=0.0),
                RandRangePop(),
                RangePush("Range: ToTensor"),
                ToTensor(),
                RangePop(),
                Mark("Mark: Transforms End!"),
            ]
        )
        output = transforms(input)
        self.assertIsInstance(output, torch.Tensor)
        np.testing.assert_array_equal(input, output)
        # with prob == 1.0
        transforms = Compose(
            [
                RandMark("Mark: Transforms Start!"),
                RandRangePush("Range: RandFlip"),
                RandFlip(prob=1.0),
                RandRangePop(),
                RangePush("Range: ToTensor"),
                ToTensor(),
                RangePop(),
                Mark("Mark: Transforms End!"),
            ]
        )
        output = transforms(input)
        self.assertIsInstance(output, torch.Tensor)
        np.testing.assert_array_equal(input, Flip()(output.numpy()))

</source>
<source file="systems/MONAI-0.8.0/tests/test_nvtx_transform.py" startline="105" endline="139" pcid="43">
    def test_nvtx_transfroms_dict(self, input):
        # with prob == 0.0
        transforms = Compose(
            [
                RandMarkD("Mark: Transforms (p=0) Start!"),
                RandRangePushD("Range: RandFlipD"),
                RandFlipD(keys="image", prob=0.0),
                RandRangePopD(),
                RangePushD("Range: ToTensorD"),
                ToTensorD(keys=("image")),
                RangePopD(),
                MarkD("Mark: Transforms (p=0) End!"),
            ]
        )
        output = transforms(input)
        self.assertIsInstance(output["image"], torch.Tensor)
        np.testing.assert_array_equal(input["image"], output["image"])
        # with prob == 1.0
        transforms = Compose(
            [
                RandMarkD("Mark: Transforms (p=1) Start!"),
                RandRangePushD("Range: RandFlipD"),
                RandFlipD(keys="image", prob=1.0),
                RandRangePopD(),
                RangePushD("Range: ToTensorD"),
                ToTensorD(keys=("image")),
                RangePopD(),
                MarkD("Mark: Transforms (p=1) End!"),
            ]
        )
        output = transforms(input)
        self.assertIsInstance(output["image"], torch.Tensor)
        np.testing.assert_array_equal(input["image"], Flip()(output["image"].numpy()))


</source>
</class>

<class classid="5" nclones="2" nlines="18" similarity="78">
<source file="systems/MONAI-0.8.0/tests/test_handler_segmentation_saver.py" startline="31" endline="55" pcid="44">
    def test_saved_content(self, output_ext):
        with tempfile.TemporaryDirectory() as tempdir:

            # set up engine
            def _train_func(engine, batch):
                engine.state.batch = decollate_batch(batch)
                return [torch.randint(0, 255, (1, 2, 2)).float() for _ in range(8)]

            engine = Engine(_train_func)

            # set up testing handler
            saver = SegmentationSaver(output_dir=tempdir, output_postfix="seg", output_ext=output_ext, scale=255)
            saver.attach(engine)

            data = [
                {
                    "filename_or_obj": ["testfile" + str(i) + ".nii.gz" for i in range(8)],
                    "patch_index": torch.tensor(list(range(8))),
                }
            ]
            engine.run(data, max_epochs=1)
            for i in range(8):
                filepath = os.path.join("testfile" + str(i), "testfile" + str(i) + "_seg" + f"_{i}" + output_ext)
                self.assertTrue(os.path.exists(os.path.join(tempdir, filepath)))

</source>
<source file="systems/MONAI-0.8.0/tests/test_handler_segmentation_saver.py" startline="57" endline="84" pcid="46">
    def test_save_resized_content(self, output_ext):
        with tempfile.TemporaryDirectory() as tempdir:

            # set up engine
            def _train_func(engine, batch):
                engine.state.batch = decollate_batch(batch)
                return [torch.randint(0, 255, (1, 2, 2)).float() for _ in range(8)]

            engine = Engine(_train_func)

            # set up testing handler
            saver = SegmentationSaver(output_dir=tempdir, output_postfix="seg", output_ext=output_ext, scale=255)
            saver.attach(engine)

            data = [
                {
                    "filename_or_obj": ["testfile" + str(i) + ".nii.gz" for i in range(8)],
                    "spatial_shape": torch.tensor([[28, 28] for _ in range(8)]),
                    "affine": torch.tensor([np.diag(np.ones(4)) * 5 for _ in range(8)]),
                    "original_affine": torch.tensor([np.diag(np.ones(4)) * 1.0 for _ in range(8)]),
                }
            ]
            engine.run(data, max_epochs=1)
            for i in range(8):
                filepath = os.path.join("testfile" + str(i), "testfile" + str(i) + "_seg" + output_ext)
                self.assertTrue(os.path.exists(os.path.join(tempdir, filepath)))


</source>
</class>

<class classid="6" nclones="2" nlines="25" similarity="87">
<source file="systems/MONAI-0.8.0/tests/test_data_stats.py" startline="143" endline="168" pcid="97">
    def test_file(self, input_data, expected_print):
        with tempfile.TemporaryDirectory() as tempdir:
            filename = os.path.join(tempdir, "test_data_stats.log")
            handler = logging.FileHandler(filename, mode="w")
            handler.setLevel(logging.INFO)
            input_param = {
                "prefix": "test data",
                "data_type": True,
                "data_shape": True,
                "value_range": True,
                "data_value": True,
                "additional_info": np.mean,
                "logger_handler": handler,
            }
            transform = DataStats(**input_param)
            _ = transform(input_data)
            _logger = logging.getLogger(transform._logger_name)
            for h in _logger.handlers[:]:
                h.close()
                _logger.removeHandler(h)
            with open(filename) as f:
                content = f.read()
            if sys.platform != "win32":
                self.assertEqual(content, expected_print)


</source>
<source file="systems/MONAI-0.8.0/tests/test_data_statsd.py" startline="166" endline="192" pcid="547">
    def test_file(self, input_data, expected_print):
        with tempfile.TemporaryDirectory() as tempdir:
            filename = os.path.join(tempdir, "test_stats.log")
            handler = logging.FileHandler(filename, mode="w")
            handler.setLevel(logging.INFO)
            input_param = {
                "keys": "img",
                "prefix": "test data",
                "data_shape": True,
                "value_range": True,
                "data_value": True,
                "additional_info": np.mean,
                "logger_handler": handler,
            }
            transform = DataStatsd(**input_param)
            _ = transform(input_data)
            _logger = logging.getLogger(transform.printer._logger_name)
            for h in _logger.handlers[:]:
                h.close()
                _logger.removeHandler(h)
            del handler
            with open(filename) as f:
                content = f.read()
            if sys.platform != "win32":
                self.assertEqual(content, expected_print)


</source>
</class>

<class classid="7" nclones="2" nlines="15" similarity="81">
<source file="systems/MONAI-0.8.0/tests/test_handler_tb_stats.py" startline="23" endline="45" pcid="120">
    def test_metrics_print(self):
        with tempfile.TemporaryDirectory() as tempdir:

            # set up engine
            def _train_func(engine, batch):
                return [batch + 1.0]

            engine = Engine(_train_func)

            # set up dummy metric
            @engine.on(Events.EPOCH_COMPLETED)
            def _update_metric(engine):
                current_metric = engine.state.metrics.get("acc", 0.1)
                engine.state.metrics["acc"] = current_metric + 0.1

            # set up testing handler
            stats_handler = TensorBoardStatsHandler(log_dir=tempdir)
            stats_handler.attach(engine)
            engine.run(range(3), max_epochs=2)
            stats_handler.close()
            # check logging output
            self.assertTrue(len(glob.glob(tempdir)) > 0)

</source>
<source file="systems/MONAI-0.8.0/tests/test_handler_mlflow.py" startline="24" endline="49" pcid="168">
    def test_metrics_track(self):
        with tempfile.TemporaryDirectory() as tempdir:

            # set up engine
            def _train_func(engine, batch):
                return [batch + 1.0]

            engine = Engine(_train_func)

            # set up dummy metric
            @engine.on(Events.EPOCH_COMPLETED)
            def _update_metric(engine):
                current_metric = engine.state.metrics.get("acc", 0.1)
                engine.state.metrics["acc"] = current_metric + 0.1
                engine.state.test = current_metric

            # set up testing handler
            test_path = os.path.join(tempdir, "mlflow_test")
            handler = MLFlowHandler(tracking_uri=Path(test_path).as_uri(), state_attributes=["test"])
            handler.attach(engine)
            engine.run(range(3), max_epochs=2)
            handler.close()
            # check logging output
            self.assertTrue(len(glob.glob(test_path)) > 0)


</source>
</class>

<class classid="8" nclones="2" nlines="20" similarity="80">
<source file="systems/MONAI-0.8.0/tests/test_affine_transform.py" startline="319" endline="342" pcid="139">
    def test_forward_2d(self):
        x = torch.rand(2, 1, 4, 4)
        theta = torch.Tensor([[[0, -1, 0], [1, 0, 0]]]).repeat(2, 1, 1)
        grid = torch.nn.functional.affine_grid(theta, x.size(), align_corners=False)
        expected = torch.nn.functional.grid_sample(x, grid, align_corners=False)
        expected = expected.detach().cpu().numpy()

        actual = AffineTransform(normalized=True, reverse_indexing=False)(x, theta)
        actual = actual.detach().cpu().numpy()
        np.testing.assert_allclose(actual, expected)
        np.testing.assert_allclose(list(theta.shape), [2, 2, 3])

        theta = torch.Tensor([[0, -1, 0], [1, 0, 0]])
        actual = AffineTransform(normalized=True, reverse_indexing=False)(x, theta)
        actual = actual.detach().cpu().numpy()
        np.testing.assert_allclose(actual, expected)
        np.testing.assert_allclose(list(theta.shape), [2, 3])

        theta = torch.Tensor([[[0, -1, 0], [1, 0, 0]]])
        actual = AffineTransform(normalized=True, reverse_indexing=False)(x, theta)
        actual = actual.detach().cpu().numpy()
        np.testing.assert_allclose(actual, expected)
        np.testing.assert_allclose(list(theta.shape), [1, 2, 3])

</source>
<source file="systems/MONAI-0.8.0/tests/test_affine_transform.py" startline="343" endline="367" pcid="140">
    def test_forward_3d(self):
        x = torch.rand(2, 1, 4, 4, 4)
        theta = torch.Tensor([[[0, 0, -1, 0], [1, 0, 0, 0], [0, 0, 1, 0]]]).repeat(2, 1, 1)
        grid = torch.nn.functional.affine_grid(theta, x.size(), align_corners=False)
        expected = torch.nn.functional.grid_sample(x, grid, align_corners=False)
        expected = expected.detach().cpu().numpy()

        actual = AffineTransform(normalized=True, reverse_indexing=False)(x, theta)
        actual = actual.detach().cpu().numpy()
        np.testing.assert_allclose(actual, expected)
        np.testing.assert_allclose(list(theta.shape), [2, 3, 4])

        theta = torch.Tensor([[0, 0, -1, 0], [1, 0, 0, 0], [0, 0, 1, 0]])
        actual = AffineTransform(normalized=True, reverse_indexing=False)(x, theta)
        actual = actual.detach().cpu().numpy()
        np.testing.assert_allclose(actual, expected)
        np.testing.assert_allclose(list(theta.shape), [3, 4])

        theta = torch.Tensor([[[0, 0, -1, 0], [1, 0, 0, 0], [0, 0, 1, 0]]])
        actual = AffineTransform(normalized=True, reverse_indexing=False)(x, theta)
        actual = actual.detach().cpu().numpy()
        np.testing.assert_allclose(actual, expected)
        np.testing.assert_allclose(list(theta.shape), [1, 3, 4])


</source>
</class>

<class classid="9" nclones="2" nlines="15" similarity="86">
<source file="systems/MONAI-0.8.0/tests/test_resized.py" startline="44" endline="62" pcid="163">
    def test_correct_results(self, spatial_size, mode):
        resize = Resized("img", spatial_size, mode=mode)
        _order = 0
        if mode.endswith("linear"):
            _order = 1
        if spatial_size == (32, -1):
            spatial_size = (32, 64)
        expected = [
            skimage.transform.resize(
                channel, spatial_size, order=_order, clip=False, preserve_range=False, anti_aliasing=False
            )
            for channel in self.imt[0]
        ]

        expected = np.stack(expected).astype(np.float32)
        for p in TEST_NDARRAYS:
            out = resize({"img": p(self.imt[0])})["img"]
            assert_allclose(out, expected, type_test=False, atol=0.9)

</source>
<source file="systems/MONAI-0.8.0/tests/test_resize.py" startline="41" endline="59" pcid="1084">
    def test_correct_results(self, spatial_size, mode):
        resize = Resize(spatial_size, mode=mode)
        _order = 0
        if mode.endswith("linear"):
            _order = 1
        if spatial_size == (32, -1):
            spatial_size = (32, 64)
        expected = [
            skimage.transform.resize(
                channel, spatial_size, order=_order, clip=False, preserve_range=False, anti_aliasing=False
            )
            for channel in self.imt[0]
        ]

        expected = np.stack(expected).astype(np.float32)
        for p in TEST_NDARRAYS:
            out = resize(p(self.imt[0]))
            assert_allclose(out, expected, type_test=False, atol=0.9)

</source>
</class>

<class classid="10" nclones="4" nlines="12" similarity="78">
<source file="systems/MONAI-0.8.0/tests/test_handler_parameter_scheduler.py" startline="77" endline="89" pcid="176">
    def test_exponential_scheduler(self):
        net = ToyNet(value=-1)
        engine = Engine(lambda e, b: None)
        ParamSchedulerHandler(
            parameter_setter=net.set_value,
            value_calculator="exponential",
            vc_kwargs={"initial_value": 10, "gamma": 0.99},
            epoch_level=True,
            event=Events.EPOCH_COMPLETED,
        ).attach(engine)
        engine.run([0] * 8, max_epochs=2)
        torch.testing.assert_allclose(net.get_value(), 10 * 0.99 * 0.99)

</source>
<source file="systems/MONAI-0.8.0/tests/test_handler_parameter_scheduler.py" startline="90" endline="102" pcid="177">
    def test_step_scheduler(self):
        net = ToyNet(value=-1)
        engine = Engine(lambda e, b: None)
        ParamSchedulerHandler(
            parameter_setter=net.set_value,
            value_calculator="step",
            vc_kwargs={"initial_value": 10, "gamma": 0.99, "step_size": 5},
            epoch_level=True,
            event=Events.EPOCH_COMPLETED,
        ).attach(engine)
        engine.run([0] * 8, max_epochs=10)
        torch.testing.assert_allclose(net.get_value(), 10 * 0.99 * 0.99)

</source>
<source file="systems/MONAI-0.8.0/tests/test_handler_parameter_scheduler.py" startline="103" endline="115" pcid="178">
    def test_multistep_scheduler(self):
        net = ToyNet(value=-1)
        engine = Engine(lambda e, b: None)
        ParamSchedulerHandler(
            parameter_setter=net.set_value,
            value_calculator="multistep",
            vc_kwargs={"initial_value": 10, "gamma": 0.99, "milestones": [3, 6]},
            epoch_level=True,
            event=Events.EPOCH_COMPLETED,
        ).attach(engine)
        engine.run([0] * 8, max_epochs=10)
        torch.testing.assert_allclose(net.get_value(), 10 * 0.99 * 0.99)

</source>
<source file="systems/MONAI-0.8.0/tests/test_handler_parameter_scheduler.py" startline="116" endline="132" pcid="179">
    def test_custom_scheduler(self):
        def custom_logic(initial_value, gamma, current_step):
            return initial_value * gamma ** (current_step % 9)

        net = ToyNet(value=-1)
        engine = Engine(lambda e, b: None)
        ParamSchedulerHandler(
            parameter_setter=net.set_value,
            value_calculator=custom_logic,
            vc_kwargs={"initial_value": 10, "gamma": 0.99},
            epoch_level=True,
            event=Events.EPOCH_COMPLETED,
        ).attach(engine)
        engine.run([0] * 8, max_epochs=2)
        torch.testing.assert_allclose(net.get_value(), 10 * 0.99 * 0.99)


</source>
</class>

<class classid="11" nclones="2" nlines="10" similarity="100">
<source file="systems/MONAI-0.8.0/tests/test_as_channel_lastd.py" startline="29" endline="40" pcid="197">
    def test_shape(self, in_type, input_param, expected_shape):
        test_data = {
            "image": in_type(np.random.randint(0, 2, size=[1, 2, 3, 4])),
            "label": in_type(np.random.randint(0, 2, size=[1, 2, 3, 4])),
            "extra": in_type(np.random.randint(0, 2, size=[1, 2, 3, 4])),
        }
        result = AsChannelLastd(**input_param)(test_data)
        self.assertTupleEqual(result["image"].shape, expected_shape)
        self.assertTupleEqual(result["label"].shape, expected_shape)
        self.assertTupleEqual(result["extra"].shape, expected_shape)


</source>
<source file="systems/MONAI-0.8.0/tests/test_as_channel_firstd.py" startline="29" endline="40" pcid="980">
    def test_shape(self, in_type, input_param, expected_shape):
        test_data = {
            "image": in_type(np.random.randint(0, 2, size=[1, 2, 3, 4])),
            "label": in_type(np.random.randint(0, 2, size=[1, 2, 3, 4])),
            "extra": in_type(np.random.randint(0, 2, size=[1, 2, 3, 4])),
        }
        result = AsChannelFirstd(**input_param)(test_data)
        self.assertTupleEqual(result["image"].shape, expected_shape)
        self.assertTupleEqual(result["label"].shape, expected_shape)
        self.assertTupleEqual(result["extra"].shape, expected_shape)


</source>
</class>

<class classid="12" nclones="4" nlines="11" similarity="75">
<source file="systems/MONAI-0.8.0/tests/test_integration_workflows_gan.py" startline="130" endline="142" pcid="201">
    def setUp(self):
        set_determinism(seed=0)

        self.data_dir = tempfile.mkdtemp()
        for i in range(40):
            im, _ = create_test_image_2d(64, 64, num_objs=3, rad_max=14, num_seg_classes=1, channel_dim=-1)
            n = nib.Nifti1Image(im, np.eye(4))
            nib.save(n, os.path.join(self.data_dir, f"img{i:d}.nii.gz"))

        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu:0")
        monai.config.print_config()
        logging.basicConfig(stream=sys.stdout, level=logging.INFO)

</source>
<source file="systems/MONAI-0.8.0/tests/test_integration_workflows.py" startline="282" endline="296" pcid="688">
    def setUp(self):
        set_determinism(seed=0)

        self.data_dir = tempfile.mkdtemp()
        for i in range(40):
            im, seg = create_test_image_3d(128, 128, 128, num_seg_classes=1, channel_dim=-1)
            n = nib.Nifti1Image(im, np.eye(4))
            nib.save(n, os.path.join(self.data_dir, f"img{i:d}.nii.gz"))
            n = nib.Nifti1Image(seg, np.eye(4))
            nib.save(n, os.path.join(self.data_dir, f"seg{i:d}.nii.gz"))

        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu:0")
        monai.config.print_config()
        logging.basicConfig(stream=sys.stdout, level=logging.INFO)

</source>
<source file="systems/MONAI-0.8.0/tests/test_integration_segmentation_3d.py" startline="232" endline="244" pcid="1089">
    def setUp(self):
        set_determinism(seed=0)

        self.data_dir = tempfile.mkdtemp()
        for i in range(40):
            im, seg = create_test_image_3d(128, 128, 128, num_seg_classes=1, channel_dim=-1)
            n = nib.Nifti1Image(im, np.eye(4))
            nib.save(n, os.path.join(self.data_dir, f"img{i:d}.nii.gz"))
            n = nib.Nifti1Image(seg, np.eye(4))
            nib.save(n, os.path.join(self.data_dir, f"seg{i:d}.nii.gz"))

        self.device = "cuda:0" if torch.cuda.is_available() else "cpu:0"

</source>
<source file="systems/MONAI-0.8.0/tests/test_integration_fast_train.py" startline="62" endline="73" pcid="895">
    def setUp(self):
        set_determinism(seed=0)
        monai.config.print_config()

        self.data_dir = tempfile.mkdtemp()
        for i in range(41):
            im, seg = create_test_image_3d(128, 128, 128, num_seg_classes=1, channel_dim=-1)
            n = nib.Nifti1Image(im, np.eye(4))
            nib.save(n, os.path.join(self.data_dir, f"img{i:d}.nii.gz"))
            n = nib.Nifti1Image(seg, np.eye(4))
            nib.save(n, os.path.join(self.data_dir, f"seg{i:d}.nii.gz"))

</source>
</class>

<class classid="13" nclones="3" nlines="18" similarity="73">
<source file="systems/MONAI-0.8.0/tests/test_compute_regression_metrics.py" startline="88" endline="118" pcid="221">
    def test_compare_numpy(self):
        set_determinism(seed=123)
        device = "cuda" if torch.cuda.is_available() else "cpu"

        # regression metrics to check + truth metric function in numpy
        metrics = [MSEMetric, MAEMetric, RMSEMetric, partial(PSNRMetric, max_val=1.0)]
        metrics_np = [msemetric_np, maemetric_np, rmsemetric_np, partial(psnrmetric_np, max_val=1.0)]

        # define variations in batch/base_dims/spatial_dims
        batch_dims = [1, 2, 4, 16]
        base_dims = [16, 32, 64]
        spatial_dims = [2, 3, 4]

        # iterate over all variations and check shapes for different reduction functions
        for batch in batch_dims:
            for spatial in spatial_dims:
                for base in base_dims:

                    # create random tensors
                    in_tensor_a = torch.rand((batch,) + (base,) * (spatial - 1)).to(device)
                    in_tensor_b = torch.rand((batch,) + (base,) * (spatial - 1)).to(device)

                    # check metrics
                    for mt_fn, mt_fn_np in zip(metrics, metrics_np):
                        mt = mt_fn(reduction="mean")
                        mt(y_pred=in_tensor_a, y=in_tensor_b)
                        out_tensor = mt.aggregate()
                        out_np = mt_fn_np(y_pred=in_tensor_a.cpu().numpy(), y=in_tensor_b.cpu().numpy())

                        np.testing.assert_allclose(out_tensor.cpu().numpy(), out_np, atol=1e-4)

</source>
<source file="systems/MONAI-0.8.0/tests/test_compute_regression_metrics.py" startline="166" endline="193" pcid="224">
    def test_diff_input(self):
        set_determinism(seed=123)
        device = "cuda" if torch.cuda.is_available() else "cpu"
        metrics = [MSEMetric, MAEMetric, RMSEMetric, partial(PSNRMetric, max_val=1.0)]
        results = [1.0, 1.0, 1.0, 0.0]

        # define variations in batch/base_dims/spatial_dims
        batch_dims = [1, 2, 4, 16]
        base_dims = [16, 32, 64]
        spatial_dims = [2, 3, 4]

        # iterate over all variations and check shapes for different reduction functions
        for batch in batch_dims:
            for spatial in spatial_dims:
                for base in base_dims:

                    # create random tensors
                    in_tensor_a = torch.zeros((batch,) + (base,) * (spatial - 1)).to(device)
                    in_tensor_b = torch.ones((batch,) + (base,) * (spatial - 1)).to(device)

                    # check metrics
                    for mt_fn, rs in zip(metrics, results):
                        mt = mt_fn(reduction="mean")
                        mt(in_tensor_a, in_tensor_b)
                        out_tensor = mt.aggregate()
                        np.testing.assert_allclose(out_tensor.cpu(), rs, atol=1e-4)


</source>
<source file="systems/MONAI-0.8.0/tests/test_compute_regression_metrics.py" startline="140" endline="165" pcid="223">
    def test_same_input(self):
        set_determinism(seed=123)
        device = "cuda" if torch.cuda.is_available() else "cpu"
        metrics = [MSEMetric, MAEMetric, RMSEMetric, partial(PSNRMetric, max_val=1.0)]
        results = [0.0, 0.0, 0.0, float("inf")]

        # define variations in batch/base_dims/spatial_dims
        batch_dims = [1, 2, 4, 16]
        base_dims = [16, 32, 64]
        spatial_dims = [2, 3, 4]

        # iterate over all variations and check shapes for different reduction functions
        for batch in batch_dims:
            for spatial in spatial_dims:
                for base in base_dims:

                    # create random tensors
                    in_tensor = torch.rand((batch,) + (base,) * (spatial - 1)).to(device)

                    # check metrics
                    for mt_fn, rs in zip(metrics, results):
                        mt = mt_fn(reduction="mean")
                        mt(in_tensor, in_tensor)
                        out_tensor = mt.aggregate()
                        np.testing.assert_allclose(out_tensor.cpu(), rs, atol=1e-4)

</source>
</class>

<class classid="14" nclones="2" nlines="68" similarity="85">
<source file="systems/MONAI-0.8.0/tests/test_persistentdataset.py" startline="72" endline="146" pcid="230">
    def test_shape(self, transform, expected_shape):
        test_image = nib.Nifti1Image(np.random.randint(0, 2, size=[128, 128, 128]), np.eye(4))
        with tempfile.TemporaryDirectory() as tempdir:
            nib.save(test_image, os.path.join(tempdir, "test_image1.nii.gz"))
            nib.save(test_image, os.path.join(tempdir, "test_label1.nii.gz"))
            nib.save(test_image, os.path.join(tempdir, "test_extra1.nii.gz"))
            nib.save(test_image, os.path.join(tempdir, "test_image2.nii.gz"))
            nib.save(test_image, os.path.join(tempdir, "test_label2.nii.gz"))
            nib.save(test_image, os.path.join(tempdir, "test_extra2.nii.gz"))
            test_data = [
                {
                    "image": os.path.join(tempdir, "test_image1.nii.gz"),
                    "label": os.path.join(tempdir, "test_label1.nii.gz"),
                    "extra": os.path.join(tempdir, "test_extra1.nii.gz"),
                },
                {
                    "image": os.path.join(tempdir, "test_image2.nii.gz"),
                    "label": os.path.join(tempdir, "test_label2.nii.gz"),
                    "extra": os.path.join(tempdir, "test_extra2.nii.gz"),
                },
            ]

            cache_dir = os.path.join(os.path.join(tempdir, "cache"), "data")
            dataset_precached = PersistentDataset(data=test_data, transform=transform, cache_dir=cache_dir)
            data1_precached = dataset_precached[0]
            data2_precached = dataset_precached[1]

            dataset_postcached = PersistentDataset(data=test_data, transform=transform, cache_dir=cache_dir)
            data1_postcached = dataset_postcached[0]
            data2_postcached = dataset_postcached[1]
            data3_postcached = dataset_postcached[0:2]

            if transform is None:
                self.assertEqual(data1_precached["image"], os.path.join(tempdir, "test_image1.nii.gz"))
                self.assertEqual(data2_precached["label"], os.path.join(tempdir, "test_label2.nii.gz"))
                self.assertEqual(data1_postcached["image"], os.path.join(tempdir, "test_image1.nii.gz"))
                self.assertEqual(data2_postcached["extra"], os.path.join(tempdir, "test_extra2.nii.gz"))
            else:
                self.assertTupleEqual(data1_precached["image"].shape, expected_shape)
                self.assertTupleEqual(data1_precached["label"].shape, expected_shape)
                self.assertTupleEqual(data1_precached["extra"].shape, expected_shape)
                self.assertTupleEqual(data2_precached["image"].shape, expected_shape)
                self.assertTupleEqual(data2_precached["label"].shape, expected_shape)
                self.assertTupleEqual(data2_precached["extra"].shape, expected_shape)

                self.assertTupleEqual(data1_postcached["image"].shape, expected_shape)
                self.assertTupleEqual(data1_postcached["label"].shape, expected_shape)
                self.assertTupleEqual(data1_postcached["extra"].shape, expected_shape)
                self.assertTupleEqual(data2_postcached["image"].shape, expected_shape)
                self.assertTupleEqual(data2_postcached["label"].shape, expected_shape)
                self.assertTupleEqual(data2_postcached["extra"].shape, expected_shape)
                for d in data3_postcached:
                    self.assertTupleEqual(d["image"].shape, expected_shape)

            # update the data to cache
            test_data_new = [
                {
                    "image": os.path.join(tempdir, "test_image1_new.nii.gz"),
                    "label": os.path.join(tempdir, "test_label1_new.nii.gz"),
                    "extra": os.path.join(tempdir, "test_extra1_new.nii.gz"),
                },
                {
                    "image": os.path.join(tempdir, "test_image2_new.nii.gz"),
                    "label": os.path.join(tempdir, "test_label2_new.nii.gz"),
                    "extra": os.path.join(tempdir, "test_extra2_new.nii.gz"),
                },
            ]
            dataset_postcached.set_data(data=test_data_new)
            # test new exchanged cache content
            if transform is None:
                self.assertEqual(dataset_postcached[0]["image"], os.path.join(tempdir, "test_image1_new.nii.gz"))
                self.assertEqual(dataset_postcached[0]["label"], os.path.join(tempdir, "test_label1_new.nii.gz"))
                self.assertEqual(dataset_postcached[1]["extra"], os.path.join(tempdir, "test_extra2_new.nii.gz"))


</source>
<source file="systems/MONAI-0.8.0/tests/test_lmdbdataset.py" startline="125" endline="204" pcid="593">
    def test_shape(self, transform, expected_shape, kwargs=None):
        kwargs = kwargs or {}
        test_image = nib.Nifti1Image(np.random.randint(0, 2, size=[128, 128, 128]), np.eye(4))
        with tempfile.TemporaryDirectory() as tempdir:
            nib.save(test_image, os.path.join(tempdir, "test_image1.nii.gz"))
            nib.save(test_image, os.path.join(tempdir, "test_label1.nii.gz"))
            nib.save(test_image, os.path.join(tempdir, "test_extra1.nii.gz"))
            nib.save(test_image, os.path.join(tempdir, "test_image2.nii.gz"))
            nib.save(test_image, os.path.join(tempdir, "test_label2.nii.gz"))
            nib.save(test_image, os.path.join(tempdir, "test_extra2.nii.gz"))
            test_data = [
                {
                    "image": os.path.join(tempdir, "test_image1.nii.gz"),
                    "label": os.path.join(tempdir, "test_label1.nii.gz"),
                    "extra": os.path.join(tempdir, "test_extra1.nii.gz"),
                },
                {
                    "image": os.path.join(tempdir, "test_image2.nii.gz"),
                    "label": os.path.join(tempdir, "test_label2.nii.gz"),
                    "extra": os.path.join(tempdir, "test_extra2.nii.gz"),
                },
            ]

            cache_dir = os.path.join(os.path.join(tempdir, "cache"), "data")
            dataset_precached = LMDBDataset(
                data=test_data, transform=transform, progress=False, cache_dir=cache_dir, **kwargs
            )
            data1_precached = dataset_precached[0]
            data2_precached = dataset_precached[1]

            dataset_postcached = LMDBDataset(
                data=test_data, transform=transform, progress=False, cache_dir=cache_dir, **kwargs
            )
            data1_postcached = dataset_postcached[0]
            data2_postcached = dataset_postcached[1]

            if transform is None:
                self.assertEqual(data1_precached["image"], os.path.join(tempdir, "test_image1.nii.gz"))
                self.assertEqual(data2_precached["label"], os.path.join(tempdir, "test_label2.nii.gz"))
                self.assertEqual(data1_postcached["image"], os.path.join(tempdir, "test_image1.nii.gz"))
                self.assertEqual(data2_postcached["extra"], os.path.join(tempdir, "test_extra2.nii.gz"))
            else:
                self.assertTupleEqual(data1_precached["image"].shape, expected_shape)
                self.assertTupleEqual(data1_precached["label"].shape, expected_shape)
                self.assertTupleEqual(data1_precached["extra"].shape, expected_shape)
                self.assertTupleEqual(data2_precached["image"].shape, expected_shape)
                self.assertTupleEqual(data2_precached["label"].shape, expected_shape)
                self.assertTupleEqual(data2_precached["extra"].shape, expected_shape)

                self.assertTupleEqual(data1_postcached["image"].shape, expected_shape)
                self.assertTupleEqual(data1_postcached["label"].shape, expected_shape)
                self.assertTupleEqual(data1_postcached["extra"].shape, expected_shape)
                self.assertTupleEqual(data2_postcached["image"].shape, expected_shape)
                self.assertTupleEqual(data2_postcached["label"].shape, expected_shape)
                self.assertTupleEqual(data2_postcached["extra"].shape, expected_shape)

            # update the data to cache
            test_data_new = [
                {
                    "image": os.path.join(tempdir, "test_image1_new.nii.gz"),
                    "label": os.path.join(tempdir, "test_label1_new.nii.gz"),
                    "extra": os.path.join(tempdir, "test_extra1_new.nii.gz"),
                },
                {
                    "image": os.path.join(tempdir, "test_image2_new.nii.gz"),
                    "label": os.path.join(tempdir, "test_label2_new.nii.gz"),
                    "extra": os.path.join(tempdir, "test_extra2_new.nii.gz"),
                },
            ]
            # test new exchanged cache content
            if transform is None:
                dataset_postcached.set_data(data=test_data_new)
                self.assertEqual(dataset_postcached[0]["image"], os.path.join(tempdir, "test_image1_new.nii.gz"))
                self.assertEqual(dataset_postcached[0]["label"], os.path.join(tempdir, "test_label1_new.nii.gz"))
                self.assertEqual(dataset_postcached[1]["extra"], os.path.join(tempdir, "test_extra2_new.nii.gz"))
            else:
                with self.assertRaises(RuntimeError):
                    dataset_postcached.set_data(data=test_data_new)  # filename list updated, files do not exist


</source>
</class>

<class classid="15" nclones="2" nlines="10" similarity="70">
<source file="systems/MONAI-0.8.0/tests/test_grid_dataset.py" startline="35" endline="45" pcid="234">
    def test_shape(self):
        test_dataset = ["vwxyz", "helloworld", "worldfoobar"]
        result = GridPatchDataset(dataset=test_dataset, patch_iter=identity_generator, with_coordinates=False)
        output = []
        n_workers = 0 if sys.platform == "win32" else 2
        for item in DataLoader(result, batch_size=3, num_workers=n_workers):
            output.append("".join(item))
        expected = ["vwx", "wor", "yzh", "ldf", "ell", "oob", "owo", "ar", "rld"]
        self.assertEqual(sorted(output), sorted(expected))
        self.assertEqual(len("".join(expected)), len("".join(test_dataset)))

</source>
<source file="systems/MONAI-0.8.0/tests/test_patch_dataset.py" startline="28" endline="40" pcid="1336">
    def test_shape(self):
        test_dataset = ["vwxyz", "hello", "world"]
        n_per_image = len(test_dataset[0])

        result = PatchDataset(dataset=test_dataset, patch_func=identity, samples_per_image=n_per_image)

        output = []
        n_workers = 0 if sys.platform == "win32" else 2
        for item in DataLoader(result, batch_size=3, num_workers=n_workers):
            output.append("".join(item))
        expected = ["vwx", "yzh", "ell", "owo", "rld"]
        self.assertEqual(output, expected)

</source>
</class>

<class classid="16" nclones="4" nlines="10" similarity="100">
<source file="systems/MONAI-0.8.0/tests/test_handler_hausdorff_distance.py" startline="22" endline="48" pcid="256">
def create_spherical_seg_3d(
    radius: float = 20.0, centre: Tuple[int, int, int] = (49, 49, 49), im_shape: Tuple[int, int, int] = (99, 99, 99)
) -> np.ndarray:
    """
    Return a 3D image with a sphere inside. Voxel values will be
    1 inside the sphere, and 0 elsewhere.

    Args:
        radius: radius of sphere (in terms of number of voxels, can be partial)
        centre: location of sphere centre.
        im_shape: shape of image to create

    See also:
        :py:meth:`~create_test_image_3d`
    """
    # Create image
    image = np.zeros(im_shape, dtype=np.int32)
    spy, spx, spz = np.ogrid[
        -centre[0] : im_shape[0] - centre[0], -centre[1] : im_shape[1] - centre[1], -centre[2] : im_shape[2] - centre[2]
    ]
    circle = (spx * spx + spy * spy + spz * spz) <= radius * radius

    image[circle] = 1
    image[~circle] = 0
    return image


</source>
<source file="systems/MONAI-0.8.0/tests/test_surface_distance.py" startline="22" endline="48" pcid="262">
def create_spherical_seg_3d(
    radius: float = 20.0, centre: Tuple[int, int, int] = (49, 49, 49), im_shape: Tuple[int, int, int] = (99, 99, 99)
) -> np.ndarray:
    """
    Return a 3D image with a sphere inside. Voxel values will be
    1 inside the sphere, and 0 elsewhere.

    Args:
        radius: radius of sphere (in terms of number of voxels, can be partial)
        centre: location of sphere centre.
        im_shape: shape of image to create

    See also:
        :py:meth:`~create_test_image_3d`
    """
    # Create image
    image = np.zeros(im_shape, dtype=np.int32)
    spy, spx, spz = np.ogrid[
        -centre[0] : im_shape[0] - centre[0], -centre[1] : im_shape[1] - centre[1], -centre[2] : im_shape[2] - centre[2]
    ]
    circle = (spx * spx + spy * spy + spz * spz) <= radius * radius

    image[circle] = 1
    image[~circle] = 0
    return image


</source>
<source file="systems/MONAI-0.8.0/tests/test_handler_surface_distance.py" startline="22" endline="48" pcid="1358">
def create_spherical_seg_3d(
    radius: float = 20.0, centre: Tuple[int, int, int] = (49, 49, 49), im_shape: Tuple[int, int, int] = (99, 99, 99)
) -> np.ndarray:
    """
    Return a 3D image with a sphere inside. Voxel values will be
    1 inside the sphere, and 0 elsewhere.

    Args:
        radius: radius of sphere (in terms of number of voxels, can be partial)
        centre: location of sphere centre.
        im_shape: shape of image to create

    See also:
        :py:meth:`~create_test_image_3d`
    """
    # Create image
    image = np.zeros(im_shape, dtype=np.int32)
    spy, spx, spz = np.ogrid[
        -centre[0] : im_shape[0] - centre[0], -centre[1] : im_shape[1] - centre[1], -centre[2] : im_shape[2] - centre[2]
    ]
    circle = (spx * spx + spy * spy + spz * spz) <= radius * radius

    image[circle] = 1
    image[~circle] = 0
    return image


</source>
<source file="systems/MONAI-0.8.0/tests/test_hausdorff_distance.py" startline="22" endline="48" pcid="1206">
def create_spherical_seg_3d(
    radius: float = 20.0, centre: Tuple[int, int, int] = (49, 49, 49), im_shape: Tuple[int, int, int] = (99, 99, 99)
) -> np.ndarray:
    """
    Return a 3D image with a sphere inside. Voxel values will be
    1 inside the sphere, and 0 elsewhere.

    Args:
        radius: radius of sphere (in terms of number of voxels, can be partial)
        centre: location of sphere centre.
        im_shape: shape of image to create

    See also:
        :py:meth:`~create_test_image_3d`
    """
    # Create image
    image = np.zeros(im_shape, dtype=np.int32)
    spy, spx, spz = np.ogrid[
        -centre[0] : im_shape[0] - centre[0], -centre[1] : im_shape[1] - centre[1], -centre[2] : im_shape[2] - centre[2]
    ]
    circle = (spx * spx + spy * spy + spz * spz) <= radius * radius

    image[circle] = 1
    image[~circle] = 0
    return image


</source>
</class>

<class classid="17" nclones="2" nlines="18" similarity="88">
<source file="systems/MONAI-0.8.0/tests/test_handler_hausdorff_distance.py" startline="63" endline="84" pcid="257">
    def test_compute(self):
        hd_metric = HausdorffDistance(include_background=True)

        def _val_func(engine, batch):
            pass

        engine = Engine(_val_func)
        hd_metric.attach(engine, "hausdorff_distance")

        y_pred, y = TEST_SAMPLE_1
        hd_metric.update([y_pred, y])
        self.assertEqual(hd_metric.compute(), 10)
        y_pred, y = TEST_SAMPLE_2
        hd_metric.update([y_pred, y])
        self.assertEqual(hd_metric.compute(), 5)
        y_pred, y = TEST_SAMPLE_3
        hd_metric.update([y_pred, y])
        self.assertEqual(hd_metric.compute(), float("inf"))
        y_pred, y = TEST_SAMPLE_4
        hd_metric.update([y_pred, y])
        self.assertEqual(hd_metric.compute(), float("inf"))

</source>
<source file="systems/MONAI-0.8.0/tests/test_handler_surface_distance.py" startline="63" endline="84" pcid="1359">
    def test_compute(self):
        sur_metric = SurfaceDistance(include_background=True)

        def _val_func(engine, batch):
            pass

        engine = Engine(_val_func)
        sur_metric.attach(engine, "surface_distance")

        y_pred, y = TEST_SAMPLE_1
        sur_metric.update([y_pred, y])
        self.assertAlmostEqual(sur_metric.compute(), 4.17133, places=4)
        y_pred, y = TEST_SAMPLE_2
        sur_metric.update([y_pred, y])
        self.assertAlmostEqual(sur_metric.compute(), 2.08566, places=4)
        y_pred, y = TEST_SAMPLE_3
        sur_metric.update([y_pred, y])
        self.assertAlmostEqual(sur_metric.compute(), float("inf"))
        y_pred, y = TEST_SAMPLE_4
        sur_metric.update([y_pred, y])
        self.assertAlmostEqual(sur_metric.compute(), float("inf"))

</source>
</class>

<class classid="18" nclones="2" nlines="11" similarity="100">
<source file="systems/MONAI-0.8.0/tests/test_handler_hausdorff_distance.py" startline="92" endline="107" pcid="260">
    def test_reduction(self):
        hd_metric = HausdorffDistance(include_background=True, reduction="mean_channel")

        def _val_func(engine, batch):
            pass

        engine = Engine(_val_func)
        hd_metric.attach(engine, "hausdorff_distance")

        y_pred, y = TEST_SAMPLE_1
        hd_metric.update([y_pred, y])
        y_pred, y = TEST_SAMPLE_2
        hd_metric.update([y_pred, y])
        torch.testing.assert_allclose(hd_metric.compute().float(), torch.tensor([10.0, 0.0]))


</source>
<source file="systems/MONAI-0.8.0/tests/test_handler_surface_distance.py" startline="92" endline="107" pcid="1362">
    def test_reduction(self):
        sur_metric = SurfaceDistance(include_background=True, reduction="mean_channel")

        def _val_func(engine, batch):
            pass

        engine = Engine(_val_func)
        sur_metric.attach(engine, "surface_distance")

        y_pred, y = TEST_SAMPLE_1
        sur_metric.update([y_pred, y])
        y_pred, y = TEST_SAMPLE_2
        sur_metric.update([y_pred, y])
        torch.testing.assert_allclose(sur_metric.compute().float(), torch.tensor([4.1713, 0.0000]))


</source>
</class>

<class classid="19" nclones="2" nlines="20" similarity="80">
<source file="systems/MONAI-0.8.0/tests/test_surface_distance.py" startline="106" endline="126" pcid="263">
    def test_value(self, input_data, expected_value):
        if len(input_data) == 3:
            [seg_1, seg_2, metric] = input_data
        else:
            [seg_1, seg_2] = input_data
            metric = "euclidean"
        ct = 0
        seg_1 = torch.tensor(seg_1)
        seg_2 = torch.tensor(seg_2)
        for symmetric in [True, False]:
            sur_metric = SurfaceDistanceMetric(include_background=False, symmetric=symmetric, distance_metric=metric)
            # shape of seg_1, seg_2 are: HWD, converts to BNHWD
            batch, n_class = 2, 3
            batch_seg_1 = seg_1.unsqueeze(0).unsqueeze(0).repeat([batch, n_class, 1, 1, 1])
            batch_seg_2 = seg_2.unsqueeze(0).unsqueeze(0).repeat([batch, n_class, 1, 1, 1])
            sur_metric(batch_seg_1, batch_seg_2)
            result = sur_metric.aggregate()
            expected_value_curr = expected_value[ct]
            np.testing.assert_allclose(expected_value_curr, result, rtol=1e-7)
            ct += 1

</source>
<source file="systems/MONAI-0.8.0/tests/test_hausdorff_distance.py" startline="111" endline="134" pcid="1207">
    def test_value(self, input_data, expected_value):
        percentile = None
        if len(input_data) == 3:
            [seg_1, seg_2, percentile] = input_data
        else:
            [seg_1, seg_2] = input_data
        ct = 0
        seg_1 = torch.tensor(seg_1)
        seg_2 = torch.tensor(seg_2)
        for metric in ["euclidean", "chessboard", "taxicab"]:
            for directed in [True, False]:
                hd_metric = HausdorffDistanceMetric(
                    include_background=False, distance_metric=metric, percentile=percentile, directed=directed
                )
                # shape of seg_1, seg_2 are: HWD, converts to BNHWD
                batch, n_class = 2, 3
                batch_seg_1 = seg_1.unsqueeze(0).unsqueeze(0).repeat([batch, n_class, 1, 1, 1])
                batch_seg_2 = seg_2.unsqueeze(0).unsqueeze(0).repeat([batch, n_class, 1, 1, 1])
                hd_metric(batch_seg_1, batch_seg_2)
                result = hd_metric.aggregate()
                expected_value_curr = expected_value[ct]
                np.testing.assert_allclose(expected_value_curr, result, rtol=1e-7)
                ct += 1

</source>
</class>

<class classid="20" nclones="2" nlines="11" similarity="81">
<source file="systems/MONAI-0.8.0/tests/test_surface_distance.py" startline="128" endline="141" pcid="264">
    def test_nans(self, input_data):
        [seg_1, seg_2] = input_data
        seg_1 = torch.tensor(seg_1)
        seg_2 = torch.tensor(seg_2)
        sur_metric = SurfaceDistanceMetric(include_background=False, get_not_nans=True)
        # test list of channel-first Tensor
        batch_seg_1 = [seg_1.unsqueeze(0)]
        batch_seg_2 = [seg_2.unsqueeze(0)]
        sur_metric(batch_seg_1, batch_seg_2)
        result, not_nans = sur_metric.aggregate()
        np.testing.assert_allclose(0, result, rtol=1e-7)
        np.testing.assert_allclose(0, not_nans, rtol=1e-7)


</source>
<source file="systems/MONAI-0.8.0/tests/test_hausdorff_distance.py" startline="136" endline="148" pcid="1208">
    def test_nans(self, input_data):
        [seg_1, seg_2] = input_data
        seg_1 = torch.tensor(seg_1)
        seg_2 = torch.tensor(seg_2)
        hd_metric = HausdorffDistanceMetric(include_background=False, get_not_nans=True)
        batch_seg_1 = seg_1.unsqueeze(0).unsqueeze(0)
        batch_seg_2 = seg_2.unsqueeze(0).unsqueeze(0)
        hd_metric(batch_seg_1, batch_seg_2)
        result, not_nans = hd_metric.aggregate()
        np.testing.assert_allclose(0, result, rtol=1e-7)
        np.testing.assert_allclose(0, not_nans, rtol=1e-7)


</source>
</class>

<class classid="21" nclones="3" nlines="12" similarity="72">
<source file="systems/MONAI-0.8.0/tests/test_png_saver.py" startline="45" endline="58" pcid="269">
    def test_saved_content_spatial_size(self):
        with tempfile.TemporaryDirectory() as tempdir:

            saver = PNGSaver(output_dir=tempdir, output_postfix="seg", output_ext=".png", scale=255)

            meta_data = {
                "filename_or_obj": ["testfile" + str(i) + ".jpg" for i in range(8)],
                "spatial_shape": [(4, 4) for i in range(8)],
            }
            saver.save_batch(torch.randint(1, 200, (8, 1, 2, 2)), meta_data)
            for i in range(8):
                filepath = os.path.join("testfile" + str(i), "testfile" + str(i) + "_seg.png")
                self.assertTrue(os.path.exists(os.path.join(tempdir, filepath)))

</source>
<source file="systems/MONAI-0.8.0/tests/test_nifti_saver.py" startline="36" endline="50" pcid="887">
    def test_saved_resize_content(self):
        with tempfile.TemporaryDirectory() as tempdir:

            saver = NiftiSaver(output_dir=tempdir, output_postfix="seg", output_ext=".nii.gz", dtype=np.float32)

            meta_data = {
                "filename_or_obj": ["testfile" + str(i) + ".nii" for i in range(8)],
                "affine": [np.diag(np.ones(4)) * 5] * 8,
                "original_affine": [np.diag(np.ones(4)) * 1.0] * 8,
            }
            saver.save_batch(torch.randint(0, 255, (8, 8, 2, 2)), meta_data)
            for i in range(8):
                filepath = os.path.join("testfile" + str(i), "testfile" + str(i) + "_seg.nii.gz")
                self.assertTrue(os.path.exists(os.path.join(tempdir, filepath)))

</source>
<source file="systems/MONAI-0.8.0/tests/test_nifti_saver.py" startline="51" endline="66" pcid="888">
    def test_saved_3d_resize_content(self):
        with tempfile.TemporaryDirectory() as tempdir:

            saver = NiftiSaver(output_dir=tempdir, output_postfix="seg", output_ext=".nii.gz", dtype=np.float32)

            meta_data = {
                "filename_or_obj": ["testfile" + str(i) + ".nii.gz" for i in range(8)],
                "spatial_shape": [(10, 10, 2)] * 8,
                "affine": [np.diag(np.ones(4)) * 5] * 8,
                "original_affine": [np.diag(np.ones(4)) * 1.0] * 8,
            }
            saver.save_batch(torch.randint(0, 255, (8, 8, 1, 2, 2)), meta_data)
            for i in range(8):
                filepath = os.path.join("testfile" + str(i), "testfile" + str(i) + "_seg.nii.gz")
                self.assertTrue(os.path.exists(os.path.join(tempdir, filepath)))

</source>
</class>

<class classid="22" nclones="3" nlines="18" similarity="77">
<source file="systems/MONAI-0.8.0/tests/test_rand_weighted_cropd.py" startline="21" endline="38" pcid="307">
    def test_rand_weighted_crop_small_roi(self):
        for p in TEST_NDARRAYS:
            for q in TEST_NDARRAYS:
                img = self.seg1[0]
                n_samples = 3
                crop = RandWeightedCropd("img", "w", (10, 12), n_samples)
                weight = np.zeros_like(img)
                weight[0, 30, 17] = 1.1
                weight[0, 40, 31] = 1
                weight[0, 80, 21] = 1
                crop.set_random_state(10)
                d = {"img": p(img), "w": q(weight)}
                result = crop(d)
                self.assertTrue(len(result) == n_samples)
                np.testing.assert_allclose(result[0]["img"].shape, (1, 10, 12))
                for c, e in zip(crop.centers, [[80, 21], [30, 17], [40, 31]]):
                    assert_allclose(c, e, type_test=False)

</source>
<source file="systems/MONAI-0.8.0/tests/test_rand_weighted_cropd.py" startline="39" endline="57" pcid="308">
    def test_rand_weighted_crop_default_roi(self):
        for p in TEST_NDARRAYS:
            for q in TEST_NDARRAYS:
                img = self.imt[0]
                n_samples = 3
                crop = RandWeightedCropd("im", "weight", (10, -1), n_samples, "coords")
                weight = np.zeros_like(img)
                weight[0, 30, 17] = 1.1
                weight[0, 40, 31] = 1
                weight[0, 80, 21] = 1
                crop.set_random_state(10)
                data = {"im": p(img), "weight": q(weight), "others": np.nan}
                result = crop(data)
                self.assertTrue(len(result) == n_samples)
                np.testing.assert_allclose(result[0]["im"].shape, (1, 10, 64))
                for c, e in zip(crop.centers, [[14, 32], [105, 32], [20, 32]]):
                    assert_allclose(c, e, type_test=False)
                assert_allclose(result[1]["coords"], [105, 32], type_test=False)

</source>
<source file="systems/MONAI-0.8.0/tests/test_rand_weighted_cropd.py" startline="58" endline="76" pcid="309">
    def test_rand_weighted_crop_large_roi(self):
        for p in TEST_NDARRAYS:
            for q in TEST_NDARRAYS:
                img = self.segn[0]
                n_samples = 3
                crop = RandWeightedCropd(("img", "seg"), "weight", (10000, 400), n_samples, "location")
                weight = np.zeros_like(img)
                weight[0, 30, 17] = 1.1
                weight[0, 10, 1] = 1
                crop.set_random_state(10)
                data = {"img": p(img), "seg": p(self.imt[0]), "weight": q(weight)}
                result = crop(data)
                self.assertTrue(len(result) == n_samples)
                np.testing.assert_allclose(result[0]["img"].shape, (1, 128, 64))
                np.testing.assert_allclose(result[0]["seg"].shape, (1, 128, 64))
                for c, e in zip(crop.centers, [[64, 32], [64, 32], [64, 32]]):
                    assert_allclose(c, e, type_test=False)
                assert_allclose(result[1]["location"], [64, 32], type_test=False)

</source>
</class>

<class classid="23" nclones="3" nlines="17" similarity="76">
<source file="systems/MONAI-0.8.0/tests/test_rand_weighted_cropd.py" startline="77" endline="95" pcid="310">
    def test_rand_weighted_crop_bad_w(self):
        for p in TEST_NDARRAYS:
            for q in TEST_NDARRAYS:
                img = self.imt[0]
                n_samples = 3
                crop = RandWeightedCropd(("img", "seg"), "w", (20, 40), n_samples)
                weight = np.zeros_like(img)
                weight[0, 30, 17] = np.inf
                weight[0, 10, 1] = -np.inf
                weight[0, 10, 20] = -np.nan
                crop.set_random_state(10)
                result = crop({"img": p(img), "seg": p(self.segn[0]), "w": q(weight)})
                self.assertTrue(len(result) == n_samples)
                np.testing.assert_allclose(result[0]["img"].shape, (1, 20, 40))
                np.testing.assert_allclose(result[0]["seg"].shape, (1, 20, 40))
                for c, e in zip(crop.centers, [[63, 37], [31, 43], [66, 20]]):
                    assert_allclose(c, e, type_test=False)


</source>
<source file="systems/MONAI-0.8.0/tests/test_rand_weighted_cropd.py" startline="148" endline="165" pcid="314">
    def test_rand_weighted_crop_bad_w(self):
        for p in TEST_NDARRAYS:
            for q in TEST_NDARRAYS:
                img = self.imt[0]
                n_samples = 3
                crop = RandWeightedCropd(("img", "seg"), "w", (48, 64, 80), n_samples)
                weight = np.zeros_like(img)
                weight[0, 30, 17] = np.inf
                weight[0, 10, 1] = -np.inf
                weight[0, 10, 20] = -np.nan
                crop.set_random_state(10)
                result = crop({"img": p(img), "seg": p(self.segn[0]), "w": q(weight)})
                self.assertTrue(len(result) == n_samples)
                np.testing.assert_allclose(result[0]["img"].shape, (1, 48, 64, 80))
                np.testing.assert_allclose(result[0]["seg"].shape, (1, 48, 64, 80))
                for c, e in zip(crop.centers, [[24, 32, 40], [24, 32, 40], [24, 32, 40]]):
                    assert_allclose(c, e, type_test=False)

</source>
<source file="systems/MONAI-0.8.0/tests/test_rand_weighted_cropd.py" startline="114" endline="131" pcid="312">
    def test_rand_weighted_crop_default_roi(self):
        for p in TEST_NDARRAYS:
            for q in TEST_NDARRAYS:
                img = self.imt[0]
                n_samples = 3
                crop = RandWeightedCropd(("img", "seg"), "w", (10, -1, -1), n_samples)
                weight = np.zeros_like(img)
                weight[0, 7, 17] = 1.1
                weight[0, 13, 31] = 1.1
                weight[0, 24, 21] = 1
                crop.set_random_state(10)
                result = crop({"img": p(img), "seg": p(self.segn[0]), "w": q(weight)})
                self.assertTrue(len(result) == n_samples)
                np.testing.assert_allclose(result[0]["img"].shape, (1, 10, 64, 80))
                np.testing.assert_allclose(result[0]["seg"].shape, (1, 10, 64, 80))
                for c, e in zip(crop.centers, [[14, 32, 40], [41, 32, 40], [20, 32, 40]]):
                    assert_allclose(c, e, type_test=False)

</source>
</class>

<class classid="24" nclones="2" nlines="16" similarity="93">
<source file="systems/MONAI-0.8.0/tests/test_rand_weighted_cropd.py" startline="97" endline="113" pcid="311">
    def test_rand_weighted_crop_small_roi(self):
        for p in TEST_NDARRAYS:
            for q in TEST_NDARRAYS:
                img = self.seg1[0]
                n_samples = 3
                crop = RandWeightedCropd("img", "w", (8, 10, 12), n_samples)
                weight = np.zeros_like(img)
                weight[0, 5, 30, 17] = 1.1
                weight[0, 8, 40, 31] = 1
                weight[0, 11, 23, 21] = 1
                crop.set_random_state(10)
                result = crop({"img": p(img), "w": q(weight)})
                self.assertTrue(len(result) == n_samples)
                np.testing.assert_allclose(result[0]["img"].shape, (1, 8, 10, 12))
                for c, e in zip(crop.centers, [[11, 23, 21], [5, 30, 17], [8, 40, 31]]):
                    assert_allclose(c, e, type_test=False)

</source>
<source file="systems/MONAI-0.8.0/tests/test_rand_weighted_cropd.py" startline="132" endline="147" pcid="313">
    def test_rand_weighted_crop_large_roi(self):
        for p in TEST_NDARRAYS:
            for q in TEST_NDARRAYS:
                img = self.segn[0]
                n_samples = 3
                crop = RandWeightedCropd("img", "w", (10000, 400, 80), n_samples)
                weight = np.zeros_like(img)
                weight[0, 30, 17, 20] = 1.1
                weight[0, 10, 1, 17] = 1
                crop.set_random_state(10)
                result = crop({"img": p(img), "w": q(weight)})
                self.assertTrue(len(result) == n_samples)
                np.testing.assert_allclose(result[0]["img"].shape, (1, 48, 64, 80))
                for c, e in zip(crop.centers, [[24, 32, 40], [24, 32, 40], [24, 32, 40]]):
                    assert_allclose(c, e, type_test=False)

</source>
</class>

<class classid="25" nclones="3" nlines="21" similarity="81">
<source file="systems/MONAI-0.8.0/tests/test_vis_gradcam.py" startline="56" endline="80" pcid="331">
    def test_shape(self, input_data, expected_shape):
        if input_data["model"] == "densenet2d":
            model = DenseNet121(spatial_dims=2, in_channels=1, out_channels=3)
        if input_data["model"] == "densenet3d":
            model = DenseNet(
                spatial_dims=3, in_channels=1, out_channels=3, init_features=2, growth_rate=2, block_config=(6,)
            )
        if input_data["model"] == "senet2d":
            model = SEResNet50(spatial_dims=2, in_channels=3, num_classes=4)
        if input_data["model"] == "senet3d":
            model = SEResNet50(spatial_dims=3, in_channels=3, num_classes=4)
        device = "cuda:0" if torch.cuda.is_available() else "cpu"
        model.to(device)
        model.eval()
        cam = GradCAM(nn_module=model, target_layers=input_data["target_layers"])
        image = torch.rand(input_data["shape"], device=device)
        result = cam(x=image, layer_idx=-1)
        np.testing.assert_array_equal(cam.nn_module.class_idx.cpu(), model(image).max(1)[-1].cpu())
        fea_shape = cam.feature_map_size(input_data["shape"], device=device)
        self.assertTupleEqual(fea_shape, input_data["feature_shape"])
        self.assertTupleEqual(result.shape, expected_shape)
        # check result is same whether class_idx=None is used or not
        result2 = cam(x=image, layer_idx=-1, class_idx=model(image).max(1)[-1].cpu())
        torch.testing.assert_allclose(result, result2)

</source>
<source file="systems/MONAI-0.8.0/tests/test_vis_cam.py" startline="69" endline="90" pcid="534">
    def test_shape(self, input_data, expected_shape):
        if input_data["model"] == "densenet2d":
            model = DenseNet121(spatial_dims=2, in_channels=1, out_channels=3)
        if input_data["model"] == "densenet3d":
            model = DenseNet(
                spatial_dims=3, in_channels=1, out_channels=3, init_features=2, growth_rate=2, block_config=(6,)
            )
        if input_data["model"] == "senet2d":
            model = SEResNet50(spatial_dims=2, in_channels=3, num_classes=4)
        if input_data["model"] == "senet3d":
            model = SEResNet50(spatial_dims=3, in_channels=3, num_classes=4)
        device = "cuda:0" if torch.cuda.is_available() else "cpu"
        model.to(device)
        model.eval()
        cam = CAM(nn_module=model, target_layers=input_data["target_layers"], fc_layers=input_data["fc_layers"])
        image = torch.rand(input_data["shape"], device=device)
        result = cam(x=image, layer_idx=-1)
        fea_shape = cam.feature_map_size(input_data["shape"], device=device)
        self.assertTupleEqual(fea_shape, input_data["feature_shape"])
        self.assertTupleEqual(result.shape, expected_shape)


</source>
<source file="systems/MONAI-0.8.0/tests/test_vis_gradcampp.py" startline="55" endline="76" pcid="981">
    def test_shape(self, input_data, expected_shape):
        if input_data["model"] == "densenet2d":
            model = DenseNet121(spatial_dims=2, in_channels=1, out_channels=3)
        if input_data["model"] == "densenet3d":
            model = DenseNet(
                spatial_dims=3, in_channels=1, out_channels=3, init_features=2, growth_rate=2, block_config=(6,)
            )
        if input_data["model"] == "senet2d":
            model = SEResNet50(spatial_dims=2, in_channels=3, num_classes=4)
        if input_data["model"] == "senet3d":
            model = SEResNet50(spatial_dims=3, in_channels=3, num_classes=4)
        device = "cuda:0" if torch.cuda.is_available() else "cpu"
        model.to(device)
        model.eval()
        cam = GradCAMpp(nn_module=model, target_layers=input_data["target_layers"])
        image = torch.rand(input_data["shape"], device=device)
        result = cam(x=image, layer_idx=-1)
        fea_shape = cam.feature_map_size(input_data["shape"], device=device)
        self.assertTupleEqual(fea_shape, input_data["feature_shape"])
        self.assertTupleEqual(result.shape, expected_shape)


</source>
</class>

<class classid="26" nclones="5" nlines="19" similarity="70">
<source file="systems/MONAI-0.8.0/tests/test_focal_loss.py" startline="24" endline="46" pcid="355">
    def test_consistency_with_cross_entropy_2d(self):
        """For gamma=0 the focal loss reduces to the cross entropy loss"""
        focal_loss = FocalLoss(to_onehot_y=False, gamma=0.0, reduction="mean", weight=1.0)
        ce = nn.BCEWithLogitsLoss(reduction="mean")
        max_error = 0
        class_num = 10
        batch_size = 128
        for _ in range(100):
            # Create a random tensor of shape (batch_size, class_num, 8, 4)
            x = torch.rand(batch_size, class_num, 8, 4, requires_grad=True)
            # Create a random batch of classes
            l = torch.randint(low=0, high=2, size=(batch_size, class_num, 8, 4)).float()
            if torch.cuda.is_available():
                x = x.cuda()
                l = l.cuda()
            output0 = focal_loss(x, l)
            output1 = ce(x, l)
            a = float(output0.cpu().detach())
            b = float(output1.cpu().detach())
            if abs(a - b) > max_error:
                max_error = abs(a - b)
        self.assertAlmostEqual(max_error, 0.0, places=3)

</source>
<source file="systems/MONAI-0.8.0/tests/test_focal_loss.py" startline="99" endline="122" pcid="358">
    def test_consistency_with_cross_entropy_classification(self):
        """for gamma=0 the focal loss reduces to the cross entropy loss"""
        focal_loss = FocalLoss(to_onehot_y=True, gamma=0.0, reduction="mean")
        ce = nn.BCEWithLogitsLoss(reduction="mean")
        max_error = 0
        class_num = 10
        batch_size = 128
        for _ in range(100):
            # Create a random scores tensor of shape (batch_size, class_num)
            x = torch.rand(batch_size, class_num, requires_grad=True)
            # Create a random batch of classes
            l = torch.randint(low=0, high=class_num, size=(batch_size, 1))
            l = l.long()
            if torch.cuda.is_available():
                x = x.cuda()
                l = l.cuda()
            output0 = focal_loss(x, l)
            output1 = ce(x, one_hot(l, num_classes=class_num))
            a = float(output0.cpu().detach())
            b = float(output1.cpu().detach())
            if abs(a - b) > max_error:
                max_error = abs(a - b)
        self.assertAlmostEqual(max_error, 0.0, places=3)

</source>
<source file="systems/MONAI-0.8.0/tests/test_focal_loss.py" startline="47" endline="75" pcid="356">
    def test_consistency_with_cross_entropy_2d_no_reduction(self):
        """For gamma=0 the focal loss reduces to the cross entropy loss"""
        import numpy as np

        focal_loss = FocalLoss(to_onehot_y=False, gamma=0.0, reduction="none", weight=1.0)
        ce = nn.BCEWithLogitsLoss(reduction="none")
        max_error = 0
        class_num = 10
        batch_size = 128
        for _ in range(100):
            # Create a random tensor of shape (batch_size, class_num, 8, 4)
            x = torch.rand(batch_size, class_num, 8, 4, requires_grad=True)
            # Create a random batch of classes
            l = torch.randint(low=0, high=2, size=(batch_size, class_num, 8, 4)).float()
            if torch.cuda.is_available():
                x = x.cuda()
                l = l.cuda()
            output0 = focal_loss(x, l)
            output1 = ce(x, l)
            a = output0.cpu().detach().numpy()
            b = output1.cpu().detach().numpy()
            error = np.abs(a - b)
            max_error = np.maximum(error, max_error)
            # if np.all(np.abs(a - b) > max_error):
            #     max_error = np.abs(a - b)

        assert np.allclose(max_error, 0)
        # self.assertAlmostEqual(max_error, 0.0, places=3)

</source>
<source file="systems/MONAI-0.8.0/tests/test_focal_loss.py" startline="123" endline="146" pcid="359">
    def test_consistency_with_cross_entropy_classification_01(self):
        # for gamma=0.1 the focal loss differs from the cross entropy loss
        focal_loss = FocalLoss(to_onehot_y=True, gamma=0.1, reduction="mean")
        ce = nn.BCEWithLogitsLoss(reduction="mean")
        max_error = 0
        class_num = 10
        batch_size = 128
        for _ in range(100):
            # Create a random scores tensor of shape (batch_size, class_num)
            x = torch.rand(batch_size, class_num, requires_grad=True)
            # Create a random batch of classes
            l = torch.randint(low=0, high=class_num, size=(batch_size, 1))
            l = l.long()
            if torch.cuda.is_available():
                x = x.cuda()
                l = l.cuda()
            output0 = focal_loss(x, l)
            output1 = ce(x, one_hot(l, num_classes=class_num))
            a = float(output0.cpu().detach())
            b = float(output1.cpu().detach())
            if abs(a - b) > max_error:
                max_error = abs(a - b)
        self.assertNotAlmostEqual(max_error, 0.0, places=3)

</source>
<source file="systems/MONAI-0.8.0/tests/test_focal_loss.py" startline="76" endline="98" pcid="357">
    def test_consistency_with_cross_entropy_2d_onehot_label(self):
        """For gamma=0 the focal loss reduces to the cross entropy loss"""
        focal_loss = FocalLoss(to_onehot_y=True, gamma=0.0, reduction="mean")
        ce = nn.BCEWithLogitsLoss(reduction="mean")
        max_error = 0
        class_num = 10
        batch_size = 128
        for _ in range(100):
            # Create a random tensor of shape (batch_size, class_num, 8, 4)
            x = torch.rand(batch_size, class_num, 8, 4, requires_grad=True)
            # Create a random batch of classes
            l = torch.randint(low=0, high=class_num, size=(batch_size, 1, 8, 4))
            if torch.cuda.is_available():
                x = x.cuda()
                l = l.cuda()
            output0 = focal_loss(x, l)
            output1 = ce(x, one_hot(l, num_classes=class_num))
            a = float(output0.cpu().detach())
            b = float(output1.cpu().detach())
            if abs(a - b) > max_error:
                max_error = abs(a - b)
        self.assertAlmostEqual(max_error, 0.0, places=3)

</source>
</class>

<class classid="27" nclones="2" nlines="16" similarity="87">
<source file="systems/MONAI-0.8.0/tests/test_generate_label_classes_crop_centers.py" startline="50" endline="67" pcid="394">
    def test_type_shape(self, input_data, expected_type, expected_count, expected_shape):
        results = []
        for p in TEST_NDARRAYS + (None,):
            input_data = deepcopy(input_data)
            if p is not None:
                input_data["indices"] = p(input_data["indices"])
            set_determinism(0)
            result = generate_label_classes_crop_centers(**input_data)
            self.assertIsInstance(result, expected_type)
            self.assertEqual(len(result), expected_count)
            self.assertEqual(len(result[0]), expected_shape)
            # check for consistency between numpy, torch and torch.cuda
            results.append(result)
            if len(results) > 1:
                for x, y in zip(result[0], result[-1]):
                    assert_allclose(x, y, type_test=False)


</source>
<source file="systems/MONAI-0.8.0/tests/test_generate_pos_neg_label_crop_centers.py" startline="41" endline="60" pcid="1049">
    def test_type_shape(self, input_data, expected_type, expected_count, expected_shape):
        results = []
        for p in TEST_NDARRAYS + (None,):
            input_data = deepcopy(input_data)
            if p is not None:
                for k in ["fg_indices", "bg_indices"]:
                    input_data[k] = p(input_data[k])
            set_determinism(0)
            result = generate_pos_neg_label_crop_centers(**input_data)
            self.assertIsInstance(result, expected_type)
            self.assertEqual(len(result), expected_count)
            self.assertEqual(len(result[0]), expected_shape)
            # check for consistency between numpy, torch and torch.cuda
            results.append(result)
            if len(results) > 1:
                # compare every crop center
                for x, y in zip(results[0], results[-1]):
                    assert_allclose(x, y, type_test=False)


</source>
</class>

<class classid="28" nclones="2" nlines="29" similarity="86">
<source file="systems/MONAI-0.8.0/tests/test_separable_filter.py" startline="46" endline="78" pcid="397">
    def test_3d(self):
        a = torch.tensor(
            [[list(range(7)), list(range(7)), list(range(7))], [list(range(7)), list(range(7)), list(range(7))]],
            dtype=torch.float,
        )
        a = a[None][None]
        a = a.expand(2, 3, -1, -1, -1)
        expected = np.array(
            [
                [
                    [4.0, 12.0, 24.0, 36.0, 48.0, 60.0, 44.0],
                    [6.0, 18.0, 36.0, 54.0, 72.0, 90.0, 66.0],
                    [4.0, 12.0, 24.0, 36.0, 48.0, 60.0, 44.0],
                ],
                [
                    [4.0, 12.0, 24.0, 36.0, 48.0, 60.0, 44.0],
                    [6.0, 18.0, 36.0, 54.0, 72.0, 90.0, 66.0],
                    [4.0, 12.0, 24.0, 36.0, 48.0, 60.0, 44.0],
                ],
            ]
        )
        expected = expected
        # testing shapes
        k = torch.tensor([1, 1, 1])
        for kernel in (k, [k] * 3):
            out = separable_filtering(a, kernel)
            np.testing.assert_allclose(out.cpu().numpy()[1][2], expected, rtol=1e-4)
            if torch.cuda.is_available():
                out = separable_filtering(
                    a.cuda(), kernel.cuda() if isinstance(kernel, torch.Tensor) else [k.cuda() for k in kernel]
                )
                np.testing.assert_allclose(out.cpu().numpy()[0][1], expected, rtol=1e-4)

</source>
<source file="systems/MONAI-0.8.0/tests/test_apply_filter.py" startline="46" endline="76" pcid="992">
    def test_3d(self):
        a = torch.tensor(
            [[list(range(7)), list(range(7)), list(range(7))], [list(range(7)), list(range(7)), list(range(7))]],
            dtype=torch.float,
        )
        a = a[None][None]
        a = a.expand(2, 3, -1, -1, -1)
        expected = np.array(
            [
                [
                    [2.0, 6.0, 12.0, 18.0, 24.0, 30.0, 22.0],
                    [3.0, 9.0, 18.0, 27.0, 36.0, 45.0, 33.0],
                    [2.0, 6.0, 12.0, 18.0, 24.0, 30.0, 22.0],
                ],
                [
                    [2.0, 6.0, 12.0, 18.0, 24.0, 30.0, 22.0],
                    [3.0, 9.0, 18.0, 27.0, 36.0, 45.0, 33.0],
                    [2.0, 6.0, 12.0, 18.0, 24.0, 30.0, 22.0],
                ],
            ]
        )
        expected = expected
        # testing shapes
        k = torch.tensor([[[1, 1, 1], [1, 1, 1], [1, 1, 1]]])
        for kernel in (k, k[None], k[None][None]):
            out = apply_filter(a, kernel)
            np.testing.assert_allclose(out.cpu().numpy()[1][2], expected, rtol=1e-4)
            if torch.cuda.is_available():
                out = apply_filter(a.cuda(), kernel.cuda())
                np.testing.assert_allclose(out.cpu().numpy()[0][1], expected, rtol=1e-4)

</source>
</class>

<class classid="29" nclones="2" nlines="10" similarity="100">
<source file="systems/MONAI-0.8.0/tests/test_numpy_reader.py" startline="25" endline="36" pcid="430">
    def test_npy(self):
        test_data = np.random.randint(0, 256, size=[3, 4, 4])
        with tempfile.TemporaryDirectory() as tempdir:
            filepath = os.path.join(tempdir, "test_data.npy")
            np.save(filepath, test_data)

            reader = NumpyReader()
            result = reader.get_data(reader.read(filepath))
        np.testing.assert_allclose(result[1]["spatial_shape"], test_data.shape)
        np.testing.assert_allclose(result[0].shape, test_data.shape)
        np.testing.assert_allclose(result[0], test_data)

</source>
<source file="systems/MONAI-0.8.0/tests/test_numpy_reader.py" startline="37" endline="48" pcid="431">
    def test_npz1(self):
        test_data1 = np.random.randint(0, 256, size=[3, 4, 4])
        with tempfile.TemporaryDirectory() as tempdir:
            filepath = os.path.join(tempdir, "test_data.npy")
            np.save(filepath, test_data1)

            reader = NumpyReader()
            result = reader.get_data(reader.read(filepath))
        np.testing.assert_allclose(result[1]["spatial_shape"], test_data1.shape)
        np.testing.assert_allclose(result[0].shape, test_data1.shape)
        np.testing.assert_allclose(result[0], test_data1)

</source>
</class>

<class classid="30" nclones="2" nlines="11" similarity="81">
<source file="systems/MONAI-0.8.0/tests/test_numpy_reader.py" startline="49" endline="61" pcid="432">
    def test_npz2(self):
        test_data1 = np.random.randint(0, 256, size=[3, 4, 4])
        test_data2 = np.random.randint(0, 256, size=[3, 4, 4])
        with tempfile.TemporaryDirectory() as tempdir:
            filepath = os.path.join(tempdir, "test_data.npz")
            np.savez(filepath, test_data1, test_data2)

            reader = NumpyReader()
            result = reader.get_data(reader.read(filepath))
        np.testing.assert_allclose(result[1]["spatial_shape"], test_data1.shape)
        np.testing.assert_allclose(result[0].shape, (2, 3, 4, 4))
        np.testing.assert_allclose(result[0], np.stack([test_data1, test_data2]))

</source>
<source file="systems/MONAI-0.8.0/tests/test_numpy_reader.py" startline="62" endline="74" pcid="433">
    def test_npz3(self):
        test_data1 = np.random.randint(0, 256, size=[3, 4, 4])
        test_data2 = np.random.randint(0, 256, size=[3, 4, 4])
        with tempfile.TemporaryDirectory() as tempdir:
            filepath = os.path.join(tempdir, "test_data.npz")
            np.savez(filepath, test1=test_data1, test2=test_data2)

            reader = NumpyReader(npz_keys=["test1", "test2"])
            result = reader.get_data(reader.read(filepath))
        np.testing.assert_allclose(result[1]["spatial_shape"], test_data1.shape)
        np.testing.assert_allclose(result[0].shape, (2, 3, 4, 4))
        np.testing.assert_allclose(result[0], np.stack([test_data1, test_data2]))

</source>
</class>

<class classid="31" nclones="4" nlines="27" similarity="85">
<source file="systems/MONAI-0.8.0/tests/test_handler_regression_metrics_dist.py" startline="63" endline="102" pcid="447">
    def _compute(self):
        device = f"cuda:{dist.get_rank()}" if torch.cuda.is_available() else "cpu"
        metric = MeanSquaredError()

        def _val_func(engine, batch):
            pass

        engine = Engine(_val_func)
        metric.attach(engine, "MSE")

        # get testing data
        batch = BATCH_SIZE
        base = BASE_DIM_SIZE
        spatial = SPATIAL_DIM
        in_tensor_a1 = torch.rand((batch,) + (base,) * (spatial - 1))
        in_tensor_b1 = torch.rand((batch,) + (base,) * (spatial - 1))

        in_tensor_a2 = torch.rand((batch,) + (base,) * (spatial - 1))
        in_tensor_b2 = torch.rand((batch,) + (base,) * (spatial - 1))

        if dist.get_rank() == 0:
            y_pred = in_tensor_a1.to(device)
            y = in_tensor_b1.to(device)
            metric.update([y_pred, y])

        if dist.get_rank() == 1:
            y_pred = in_tensor_a2.to(device)
            y = in_tensor_b2.to(device)
            metric.update([y_pred, y])

        out_tensor = metric.compute()

        # do numpy functions to get ground truth referece
        out_tensor_np1 = msemetric_np(y_pred=in_tensor_a1.cpu().numpy(), y=in_tensor_b1.cpu().numpy())
        out_tensor_np2 = msemetric_np(y_pred=in_tensor_a2.cpu().numpy(), y=in_tensor_b2.cpu().numpy())
        out_tensor_np = (out_tensor_np1 + out_tensor_np2) / 2.0

        np.testing.assert_allclose(out_tensor, out_tensor_np, rtol=1e-04, atol=1e-04)


</source>
<source file="systems/MONAI-0.8.0/tests/test_handler_regression_metrics_dist.py" startline="109" endline="148" pcid="450">
    def _compute(self):
        device = f"cuda:{dist.get_rank()}" if torch.cuda.is_available() else "cpu"
        metric = MeanAbsoluteError()

        def _val_func(engine, batch):
            pass

        engine = Engine(_val_func)
        metric.attach(engine, "MAE")

        # get testing data
        batch = BATCH_SIZE
        base = BASE_DIM_SIZE
        spatial = SPATIAL_DIM
        in_tensor_a1 = torch.rand((batch,) + (base,) * (spatial - 1))
        in_tensor_b1 = torch.rand((batch,) + (base,) * (spatial - 1))

        in_tensor_a2 = torch.rand((batch,) + (base,) * (spatial - 1))
        in_tensor_b2 = torch.rand((batch,) + (base,) * (spatial - 1))

        if dist.get_rank() == 0:
            y_pred = in_tensor_a1.to(device)
            y = in_tensor_b1.to(device)
            metric.update([y_pred, y])

        if dist.get_rank() == 1:
            y_pred = in_tensor_a2.to(device)
            y = in_tensor_b2.to(device)
            metric.update([y_pred, y])

        out_tensor = metric.compute()

        # do numpy functions to get ground truth referece
        out_tensor_np1 = maemetric_np(y_pred=in_tensor_a1.cpu().numpy(), y=in_tensor_b1.cpu().numpy())
        out_tensor_np2 = maemetric_np(y_pred=in_tensor_a2.cpu().numpy(), y=in_tensor_b2.cpu().numpy())
        out_tensor_np = (out_tensor_np1 + out_tensor_np2) / 2.0

        np.testing.assert_allclose(out_tensor, out_tensor_np, rtol=1e-04, atol=1e-04)


</source>
<source file="systems/MONAI-0.8.0/tests/test_handler_regression_metrics_dist.py" startline="155" endline="194" pcid="453">
    def _compute(self):
        device = f"cuda:{dist.get_rank()}" if torch.cuda.is_available() else "cpu"
        metric = RootMeanSquaredError()

        def _val_func(engine, batch):
            pass

        engine = Engine(_val_func)
        metric.attach(engine, "RMSE")

        # get testing data
        batch = BATCH_SIZE
        base = BASE_DIM_SIZE
        spatial = SPATIAL_DIM
        in_tensor_a1 = torch.rand((batch,) + (base,) * (spatial - 1))
        in_tensor_b1 = torch.rand((batch,) + (base,) * (spatial - 1))

        in_tensor_a2 = torch.rand((batch,) + (base,) * (spatial - 1))
        in_tensor_b2 = torch.rand((batch,) + (base,) * (spatial - 1))

        if dist.get_rank() == 0:
            y_pred = in_tensor_a1.to(device)
            y = in_tensor_b1.to(device)
            metric.update([y_pred, y])

        if dist.get_rank() == 1:
            y_pred = in_tensor_a2.to(device)
            y = in_tensor_b2.to(device)
            metric.update([y_pred, y])

        out_tensor = metric.compute()

        # do numpy functions to get ground truth referece
        out_tensor_np1 = rmsemetric_np(y_pred=in_tensor_a1.cpu().numpy(), y=in_tensor_b1.cpu().numpy())
        out_tensor_np2 = rmsemetric_np(y_pred=in_tensor_a2.cpu().numpy(), y=in_tensor_b2.cpu().numpy())
        out_tensor_np = (out_tensor_np1 + out_tensor_np2) / 2.0

        np.testing.assert_allclose(out_tensor, out_tensor_np, rtol=1e-04, atol=1e-04)


</source>
<source file="systems/MONAI-0.8.0/tests/test_handler_regression_metrics_dist.py" startline="201" endline="241" pcid="456">
    def _compute(self):
        device = f"cuda:{dist.get_rank()}" if torch.cuda.is_available() else "cpu"
        max_val = 1.0
        metric = PeakSignalToNoiseRatio(max_val=max_val)

        def _val_func(engine, batch):
            pass

        engine = Engine(_val_func)
        metric.attach(engine, "PSNR")

        # get testing data
        batch = BATCH_SIZE
        base = BASE_DIM_SIZE
        spatial = SPATIAL_DIM
        in_tensor_a1 = torch.rand((batch,) + (base,) * (spatial - 1))
        in_tensor_b1 = torch.rand((batch,) + (base,) * (spatial - 1))

        in_tensor_a2 = torch.rand((batch,) + (base,) * (spatial - 1))
        in_tensor_b2 = torch.rand((batch,) + (base,) * (spatial - 1))

        if dist.get_rank() == 0:
            y_pred = in_tensor_a1.to(device)
            y = in_tensor_b1.to(device)
            metric.update([y_pred, y])

        if dist.get_rank() == 1:
            y_pred = in_tensor_a2.to(device)
            y = in_tensor_b2.to(device)
            metric.update([y_pred, y])

        out_tensor = metric.compute()

        # do numpy functions to get ground truth referece
        out_tensor_np1 = psnrmetric_np(max_val=max_val, y_pred=in_tensor_a1.cpu().numpy(), y=in_tensor_b1.cpu().numpy())
        out_tensor_np2 = psnrmetric_np(max_val=max_val, y_pred=in_tensor_a2.cpu().numpy(), y=in_tensor_b2.cpu().numpy())
        out_tensor_np = (out_tensor_np1 + out_tensor_np2) / 2.0

        np.testing.assert_allclose(out_tensor, out_tensor_np, rtol=1e-04, atol=1e-04)


</source>
</class>

<class classid="32" nclones="2" nlines="14" similarity="71">
<source file="systems/MONAI-0.8.0/tests/test_ensure_channel_first.py" startline="47" endline="61" pcid="459">
    def test_load_nifti(self, input_param, filenames, original_channel_dim):
        if original_channel_dim is None:
            test_image = np.random.rand(128, 128, 128)
        elif original_channel_dim == -1:
            test_image = np.random.rand(128, 128, 128, 1)

        with tempfile.TemporaryDirectory() as tempdir:
            for i, name in enumerate(filenames):
                filenames[i] = os.path.join(tempdir, name)
                nib.save(nib.Nifti1Image(test_image, np.eye(4)), filenames[i])
            for p in TEST_NDARRAYS:
                result, header = LoadImage(**input_param)(filenames)
                result = EnsureChannelFirst()(p(result), header)
                self.assertEqual(result.shape[0], len(filenames))

</source>
<source file="systems/MONAI-0.8.0/tests/test_ensure_channel_firstd.py" startline="33" endline="48" pcid="1154">
    def test_load_nifti(self, input_param, filenames, original_channel_dim):
        if original_channel_dim is None:
            test_image = np.random.rand(128, 128, 128)
        elif original_channel_dim == -1:
            test_image = np.random.rand(128, 128, 128, 1)

        with tempfile.TemporaryDirectory() as tempdir:
            for i, name in enumerate(filenames):
                filenames[i] = os.path.join(tempdir, name)
                nib.save(nib.Nifti1Image(test_image, np.eye(4)), filenames[i])
            for p in TEST_NDARRAYS:
                result = LoadImaged(**input_param)({"img": filenames})
                result["img"] = p(result["img"])
                result = EnsureChannelFirstd(**input_param)(result)
                self.assertEqual(result["img"].shape[0], len(filenames))

</source>
</class>

<class classid="33" nclones="2" nlines="32" similarity="79">
<source file="systems/MONAI-0.8.0/tests/test_handler_regression_metrics.py" startline="47" endline="88" pcid="490">
    def test_compute(self):
        set_determinism(seed=123)
        device = "cuda" if torch.cuda.is_available() else "cpu"

        # regression metrics to check + truth metric function in numpy
        metrics = [
            MeanSquaredError,
            MeanAbsoluteError,
            RootMeanSquaredError,
            partial(PeakSignalToNoiseRatio, max_val=1.0),
        ]
        metrics_np = [msemetric_np, maemetric_np, rmsemetric_np, partial(psnrmetric_np, max_val=1.0)]

        # define variations in batch/base_dims/spatial_dims
        batch_dims = [1, 2, 4, 16]
        base_dims = [16, 32, 64]
        spatial_dims = [2, 3, 4]

        # iterate over all variations and check shapes for different reduction functions
        for mt_fn, mt_fn_np in zip(metrics, metrics_np):

            for batch in batch_dims:
                for spatial in spatial_dims:
                    for base in base_dims:
                        mt_fn_obj = mt_fn(**{"save_details": False})

                        # create random tensor
                        in_tensor_a1 = torch.rand((batch,) + (base,) * (spatial - 1)).to(device)
                        in_tensor_b1 = torch.rand((batch,) + (base,) * (spatial - 1)).to(device)
                        mt_fn_obj.update([in_tensor_a1, in_tensor_b1])
                        out_tensor_np1 = mt_fn_np(y_pred=in_tensor_a1.cpu().numpy(), y=in_tensor_b1.cpu().numpy())

                        in_tensor_a2 = torch.rand((batch,) + (base,) * (spatial - 1)).to(device)
                        in_tensor_b2 = torch.rand((batch,) + (base,) * (spatial - 1)).to(device)
                        mt_fn_obj.update([in_tensor_a2, in_tensor_b2])
                        out_tensor_np2 = mt_fn_np(y_pred=in_tensor_a2.cpu().numpy(), y=in_tensor_b2.cpu().numpy())

                        out_tensor = mt_fn_obj.compute()
                        out_tensor_np = (out_tensor_np1 + out_tensor_np2) / 2.0

                        np.testing.assert_allclose(out_tensor, out_tensor_np, atol=1e-4)

</source>
<source file="systems/MONAI-0.8.0/tests/test_handler_regression_metrics.py" startline="89" endline="135" pcid="491">
    def test_compute_engine(self):
        set_determinism(seed=123)
        device = "cuda" if torch.cuda.is_available() else "cpu"

        # regression metrics to check + truth metric function in numpy
        metrics_names = ["MSE", "MAE", "RMSE", "PSNR"]
        metrics = [
            MeanSquaredError,
            MeanAbsoluteError,
            RootMeanSquaredError,
            partial(PeakSignalToNoiseRatio, max_val=1.0),
        ]
        metrics_np = [msemetric_np, maemetric_np, rmsemetric_np, partial(psnrmetric_np, max_val=1.0)]

        def _val_func(engine, batch):
            pass

        # define variations in batch/base_dims/spatial_dims
        batch_dims = [1, 2, 4, 16]
        base_dims = [16, 32, 64]
        spatial_dims = [2, 3, 4]

        # iterate over all variations and check shapes for different reduction functions
        for mt_fn_name, mt_fn, mt_fn_np in zip(metrics_names, metrics, metrics_np):
            for batch in batch_dims:
                for spatial in spatial_dims:
                    for base in base_dims:
                        mt_fn_obj = mt_fn()  # 'save_details' == True
                        engine = Engine(_val_func)
                        mt_fn_obj.attach(engine, mt_fn_name)

                        # create random tensor
                        in_tensor_a1 = torch.rand((batch,) + (base,) * (spatial - 1)).to(device)
                        in_tensor_b1 = torch.rand((batch,) + (base,) * (spatial - 1)).to(device)
                        mt_fn_obj.update([in_tensor_a1, in_tensor_b1])
                        out_tensor_np1 = mt_fn_np(y_pred=in_tensor_a1.cpu().numpy(), y=in_tensor_b1.cpu().numpy())

                        in_tensor_a2 = torch.rand((batch,) + (base,) * (spatial - 1)).to(device)
                        in_tensor_b2 = torch.rand((batch,) + (base,) * (spatial - 1)).to(device)
                        mt_fn_obj.update([in_tensor_a2, in_tensor_b2])
                        out_tensor_np2 = mt_fn_np(y_pred=in_tensor_a2.cpu().numpy(), y=in_tensor_b2.cpu().numpy())

                        out_tensor = mt_fn_obj.compute()
                        out_tensor_np = (out_tensor_np1 + out_tensor_np2) / 2.0

                        np.testing.assert_allclose(out_tensor, out_tensor_np, atol=1e-4)

</source>
</class>

<class classid="34" nclones="2" nlines="12" similarity="75">
<source file="systems/MONAI-0.8.0/tests/test_adjust_contrast.py" startline="29" endline="42" pcid="504">
    def test_correct_results(self, gamma):
        adjuster = AdjustContrast(gamma=gamma)
        for p in TEST_NDARRAYS:
            result = adjuster(p(self.imt))
            if gamma == 1.0:
                expected = self.imt
            else:
                epsilon = 1e-7
                img_min = self.imt.min()
                img_range = self.imt.max() - img_min
                expected = np.power(((self.imt - img_min) / float(img_range + epsilon)), gamma) * img_range + img_min
            assert_allclose(expected, result, rtol=1e-05, type_test=False)


</source>
<source file="systems/MONAI-0.8.0/tests/test_adjust_contrastd.py" startline="29" endline="42" pcid="565">
    def test_correct_results(self, gamma):
        adjuster = AdjustContrastd("img", gamma=gamma)
        for p in TEST_NDARRAYS:
            result = adjuster({"img": p(self.imt)})
            if gamma == 1.0:
                expected = self.imt
            else:
                epsilon = 1e-7
                img_min = self.imt.min()
                img_range = self.imt.max() - img_min
                expected = np.power(((self.imt - img_min) / float(img_range + epsilon)), gamma) * img_range + img_min
            assert_allclose(expected, result["img"], rtol=1e-05, type_test=False)


</source>
</class>

<class classid="35" nclones="4" nlines="11" similarity="72">
<source file="systems/MONAI-0.8.0/tests/test_rand_cucim_dict_transform.py" startline="92" endline="104" pcid="555">
    def test_tramsforms_numpy_single(self, params, input, expected):
        input = {"image": input}
        # apply_prob=1.0
        output = RandCuCIMd(keys="image", apply_prob=1.0, **params)(input)["image"]
        self.assertTrue(output.dtype == expected.dtype)
        self.assertTrue(isinstance(output, np.ndarray))
        cp.testing.assert_allclose(output, expected)
        # apply_prob=0.0
        output = RandCuCIMd(keys="image", apply_prob=0.0, **params)(input)["image"]
        self.assertTrue(output.dtype == input["image"].dtype)
        self.assertTrue(isinstance(output, np.ndarray))
        cp.testing.assert_allclose(output, input["image"])

</source>
<source file="systems/MONAI-0.8.0/tests/test_rand_cucim_dict_transform.py" startline="143" endline="156" pcid="557">
    def test_tramsforms_cupy_single(self, params, input, expected):
        input = {"image": cp.asarray(input)}
        expected = cp.asarray(expected)
        # apply_prob=1.0
        output = RandCuCIMd(keys="image", apply_prob=1.0, **params)(input)["image"]
        self.assertTrue(output.dtype == expected.dtype)
        self.assertTrue(isinstance(output, cp.ndarray))
        cp.testing.assert_allclose(output, expected)
        # apply_prob=0.0
        output = RandCuCIMd(keys="image", apply_prob=0.0, **params)(input)["image"]
        self.assertTrue(output.dtype == input["image"].dtype)
        self.assertTrue(isinstance(output, cp.ndarray))
        cp.testing.assert_allclose(output, input["image"])

</source>
<source file="systems/MONAI-0.8.0/tests/test_rand_cucim_dict_transform.py" startline="169" endline="183" pcid="558">
    def test_tramsforms_cupy_batch(self, params, input, expected):
        input = {"image": cp.asarray(input)[cp.newaxis, ...]}
        expected = cp.asarray(expected)[cp.newaxis, ...]
        # apply_prob=1.0
        output = RandCuCIMd(keys="image", **params)(input)["image"]
        self.assertTrue(output.dtype == expected.dtype)
        self.assertTrue(isinstance(output, cp.ndarray))
        cp.testing.assert_allclose(output, expected)
        # apply_prob=0.0
        output = RandCuCIMd(keys="image", apply_prob=0.0, **params)(input)["image"]
        self.assertTrue(output.dtype == input["image"].dtype)
        self.assertTrue(isinstance(output, cp.ndarray))
        cp.testing.assert_allclose(output, input["image"])


</source>
<source file="systems/MONAI-0.8.0/tests/test_rand_cucim_dict_transform.py" startline="117" endline="130" pcid="556">
    def test_tramsforms_numpy_batch(self, params, input, expected):
        input = {"image": input[cp.newaxis, ...]}
        expected = expected[cp.newaxis, ...]
        # apply_prob=1.0
        output = RandCuCIMd(keys="image", apply_prob=1.0, **params)(input)["image"]
        self.assertTrue(output.dtype == expected.dtype)
        self.assertTrue(isinstance(output, np.ndarray))
        cp.testing.assert_allclose(output, expected)
        # apply_prob=0.0
        output = RandCuCIMd(keys="image", apply_prob=0.0, **params)(input)["image"]
        self.assertTrue(output.dtype == input["image"].dtype)
        self.assertTrue(isinstance(output, np.ndarray))
        cp.testing.assert_allclose(output, input["image"])

</source>
</class>

<class classid="36" nclones="2" nlines="10" similarity="80">
<source file="systems/MONAI-0.8.0/tests/test_npzdictitemdataset.py" startline="22" endline="36" pcid="559">
    def test_load_stream(self):
        dat0 = np.random.rand(10, 1, 4, 4)
        dat1 = np.random.rand(10, 1, 4, 4)

        npzfile = BytesIO()
        np.savez_compressed(npzfile, dat0=dat0, dat1=dat1)
        npzfile.seek(0)

        npzds = NPZDictItemDataset(npzfile, {"dat0": "images", "dat1": "seg"})

        item = npzds[0]

        np.testing.assert_allclose(item["images"].shape, (1, 4, 4))
        np.testing.assert_allclose(item["seg"].shape, (1, 4, 4))

</source>
<source file="systems/MONAI-0.8.0/tests/test_npzdictitemdataset.py" startline="37" endline="53" pcid="560">
    def test_load_file(self):
        dat0 = np.random.rand(10, 1, 4, 4)
        dat1 = np.random.rand(10, 1, 4, 4)

        with tempfile.TemporaryDirectory() as tempdir:
            npzfile = f"{tempdir}/test.npz"

            np.savez_compressed(npzfile, dat0=dat0, dat1=dat1)

            npzds = NPZDictItemDataset(npzfile, {"dat0": "images", "dat1": "seg"})

            item = npzds[0]

            np.testing.assert_allclose(item["images"].shape, (1, 4, 4))
            np.testing.assert_allclose(item["seg"].shape, (1, 4, 4))


</source>
</class>

<class classid="37" nclones="2" nlines="13" similarity="84">
<source file="systems/MONAI-0.8.0/tests/test_ensure_type.py" startline="22" endline="34" pcid="582">
    def test_array_input(self):
        test_datas = [np.array([[1, 2], [3, 4]]), torch.as_tensor([[1, 2], [3, 4]])]
        if torch.cuda.is_available():
            test_datas.append(test_datas[-1].cuda())
        for test_data in test_datas:
            for dtype in ("tensor", "NUMPY"):
                result = EnsureType(dtype, dtype=np.float32 if dtype == "NUMPY" else None, device="cpu")(test_data)
                if dtype == "NUMPY":
                    self.assertTrue(result.dtype == np.float32)
                self.assertTrue(isinstance(result, torch.Tensor if dtype == "tensor" else np.ndarray))
                assert_allclose(result, test_data, type_test=False)
                self.assertTupleEqual(result.shape, (2, 2))

</source>
<source file="systems/MONAI-0.8.0/tests/test_ensure_typed.py" startline="22" endline="36" pcid="1213">
    def test_array_input(self):
        test_datas = [np.array([[1, 2], [3, 4]]), torch.as_tensor([[1, 2], [3, 4]])]
        if torch.cuda.is_available():
            test_datas.append(test_datas[-1].cuda())
        for test_data in test_datas:
            for dtype in ("tensor", "NUMPY"):
                result = EnsureTyped(
                    keys="data", data_type=dtype, dtype=np.float32 if dtype == "NUMPY" else None, device="cpu"
                )({"data": test_data})["data"]
                if dtype == "NUMPY":
                    self.assertTrue(result.dtype == np.float32)
                self.assertTrue(isinstance(result, torch.Tensor if dtype == "tensor" else np.ndarray))
                assert_allclose(result, test_data, type_test=False)
                self.assertTupleEqual(result.shape, (2, 2))

</source>
</class>

<class classid="38" nclones="2" nlines="13" similarity="92">
<source file="systems/MONAI-0.8.0/tests/test_ensure_type.py" startline="35" endline="48" pcid="583">
    def test_single_input(self):
        test_datas = [5, 5.0, False, np.asarray(5), torch.tensor(5)]
        if torch.cuda.is_available():
            test_datas.append(test_datas[-1].cuda())
        for test_data in test_datas:
            for dtype in ("tensor", "numpy"):
                result = EnsureType(data_type=dtype, device="cpu")(test_data)
                self.assertTrue(isinstance(result, torch.Tensor if dtype == "tensor" else np.ndarray))
                if isinstance(test_data, bool):
                    self.assertFalse(result)
                else:
                    assert_allclose(result, test_data, type_test=False)
                self.assertEqual(result.ndim, 0)

</source>
<source file="systems/MONAI-0.8.0/tests/test_ensure_typed.py" startline="37" endline="50" pcid="1214">
    def test_single_input(self):
        test_datas = [5, 5.0, False, np.asarray(5), torch.tensor(5)]
        if torch.cuda.is_available():
            test_datas.append(test_datas[-1].cuda())
        for test_data in test_datas:
            for dtype in ("tensor", "numpy"):
                result = EnsureTyped(keys="data", data_type=dtype)({"data": test_data})["data"]
                self.assertTrue(isinstance(result, torch.Tensor if dtype == "tensor" else np.ndarray))
                if isinstance(test_data, bool):
                    self.assertFalse(result)
                else:
                    assert_allclose(result, test_data, type_test=False)
                self.assertEqual(result.ndim, 0)

</source>
</class>

<class classid="39" nclones="2" nlines="11" similarity="72">
<source file="systems/MONAI-0.8.0/tests/test_ensure_type.py" startline="60" endline="71" pcid="585">
    def test_list_tuple(self):
        for dtype in ("tensor", "numpy"):
            result = EnsureType(data_type=dtype, wrap_sequence=False)([[1, 2], [3, 4]])
            self.assertTrue(isinstance(result, list))
            self.assertTrue(isinstance(result[0][1], torch.Tensor if dtype == "tensor" else np.ndarray))
            torch.testing.assert_allclose(result[1][0], torch.as_tensor(3))
            # tuple of numpy arrays
            result = EnsureType(data_type=dtype, wrap_sequence=False)((np.array([1, 2]), np.array([3, 4])))
            self.assertTrue(isinstance(result, tuple))
            self.assertTrue(isinstance(result[0], torch.Tensor if dtype == "tensor" else np.ndarray))
            torch.testing.assert_allclose(result[1], torch.as_tensor([3, 4]))

</source>
<source file="systems/MONAI-0.8.0/tests/test_ensure_typed.py" startline="62" endline="75" pcid="1216">
    def test_list_tuple(self):
        for dtype in ("tensor", "numpy"):
            result = EnsureTyped(keys="data", data_type=dtype, wrap_sequence=False)({"data": [[1, 2], [3, 4]]})["data"]
            self.assertTrue(isinstance(result, list))
            self.assertTrue(isinstance(result[0][1], torch.Tensor if dtype == "tensor" else np.ndarray))
            torch.testing.assert_allclose(result[1][0], torch.as_tensor(3))
            # tuple of numpy arrays
            result = EnsureTyped(keys="data", data_type=dtype, wrap_sequence=False)(
                {"data": (np.array([1, 2]), np.array([3, 4]))}
            )["data"]
            self.assertTrue(isinstance(result, tuple))
            self.assertTrue(isinstance(result[0], torch.Tensor if dtype == "tensor" else np.ndarray))
            torch.testing.assert_allclose(result[1], torch.as_tensor([3, 4]))

</source>
</class>

<class classid="40" nclones="2" nlines="15" similarity="92">
<source file="systems/MONAI-0.8.0/tests/test_ensure_type.py" startline="72" endline="89" pcid="586">
    def test_dict(self):
        # simulate complicated input data
        test_data = {
            "img": np.array([1.0, 2.0], dtype=np.float32),
            "meta": {"dims": 3, "size": np.array([1, 2, 3]), "path": "temp/test"},
            "extra": None,
        }
        for dtype in ("tensor", "numpy"):
            result = EnsureType(data_type=dtype)(test_data)
            self.assertTrue(isinstance(result, dict))
            self.assertTrue(isinstance(result["img"], torch.Tensor if dtype == "tensor" else np.ndarray))
            torch.testing.assert_allclose(result["img"], torch.as_tensor([1.0, 2.0]))
            self.assertTrue(isinstance(result["meta"]["size"], torch.Tensor if dtype == "tensor" else np.ndarray))
            torch.testing.assert_allclose(result["meta"]["size"], torch.as_tensor([1, 2, 3]))
            self.assertEqual(result["meta"]["path"], "temp/test")
            self.assertEqual(result["extra"], None)


</source>
<source file="systems/MONAI-0.8.0/tests/test_ensure_typed.py" startline="76" endline="93" pcid="1217">
    def test_dict(self):
        # simulate complicated input data
        test_data = {
            "img": np.array([1.0, 2.0], dtype=np.float32),
            "meta": {"dims": 3, "size": np.array([1, 2, 3]), "path": "temp/test"},
            "extra": None,
        }
        for dtype in ("tensor", "numpy"):
            result = EnsureTyped(keys="data", data_type=dtype, device="cpu")({"data": test_data})["data"]
            self.assertTrue(isinstance(result, dict))
            self.assertTrue(isinstance(result["img"], torch.Tensor if dtype == "tensor" else np.ndarray))
            torch.testing.assert_allclose(result["img"], torch.as_tensor([1.0, 2.0]))
            self.assertTrue(isinstance(result["meta"]["size"], torch.Tensor if dtype == "tensor" else np.ndarray))
            torch.testing.assert_allclose(result["meta"]["size"], torch.as_tensor([1, 2, 3]))
            self.assertEqual(result["meta"]["path"], "temp/test")
            self.assertEqual(result["extra"], None)


</source>
</class>

<class classid="41" nclones="2" nlines="26" similarity="80">
<source file="systems/MONAI-0.8.0/tests/test_lmdbdataset.py" startline="93" endline="123" pcid="592">
    def test_cache(self):
        """testing no inplace change to the hashed item"""
        items = [[list(range(i))] for i in range(5)]

        with tempfile.TemporaryDirectory() as tempdir:
            ds = LMDBDataset(items, transform=_InplaceXform(), cache_dir=tempdir, lmdb_kwargs={"map_size": 10 * 1024})
            self.assertEqual(items, [[[]], [[0]], [[0, 1]], [[0, 1, 2]], [[0, 1, 2, 3]]])
            ds1 = LMDBDataset(items, transform=_InplaceXform(), cache_dir=tempdir, lmdb_kwargs={"map_size": 10 * 1024})
            self.assertEqual(list(ds1), list(ds))
            self.assertEqual(items, [[[]], [[0]], [[0, 1]], [[0, 1, 2]], [[0, 1, 2, 3]]])

            ds = LMDBDataset(
                items,
                transform=_InplaceXform(),
                cache_dir=tempdir,
                lmdb_kwargs={"map_size": 10 * 1024},
                hash_func=json_hashing,
            )
            self.assertEqual(items, [[[]], [[0]], [[0, 1]], [[0, 1, 2]], [[0, 1, 2, 3]]])
            ds1 = LMDBDataset(
                items,
                transform=_InplaceXform(),
                cache_dir=tempdir,
                lmdb_kwargs={"map_size": 10 * 1024},
                hash_func=json_hashing,
            )
            self.assertEqual(list(ds1), list(ds))
            self.assertEqual(items, [[[]], [[0]], [[0, 1]], [[0, 1, 2]], [[0, 1, 2, 3]]])

        self.assertTrue(isinstance(ds1.info(), dict))

</source>
<source file="systems/MONAI-0.8.0/tests/test_lmdbdataset.py" startline="214" endline="243" pcid="596">
    def test_mp_cache(self):
        items = [[list(range(i))] for i in range(5)]

        ds = LMDBDataset(items, transform=_InplaceXform(), cache_dir=self.tempdir, lmdb_kwargs={"map_size": 10 * 1024})
        self.assertEqual(items, [[[]], [[0]], [[0, 1]], [[0, 1, 2]], [[0, 1, 2, 3]]])
        ds1 = LMDBDataset(items, transform=_InplaceXform(), cache_dir=self.tempdir, lmdb_kwargs={"map_size": 10 * 1024})
        self.assertEqual(list(ds1), list(ds))
        self.assertEqual(items, [[[]], [[0]], [[0, 1]], [[0, 1, 2]], [[0, 1, 2, 3]]])

        ds = LMDBDataset(
            items,
            transform=_InplaceXform(),
            cache_dir=self.tempdir,
            lmdb_kwargs={"map_size": 10 * 1024},
            hash_func=json_hashing,
        )
        self.assertEqual(items, [[[]], [[0]], [[0, 1]], [[0, 1, 2]], [[0, 1, 2, 3]]])
        ds1 = LMDBDataset(
            items,
            transform=_InplaceXform(),
            cache_dir=self.tempdir,
            lmdb_kwargs={"map_size": 10 * 1024},
            hash_func=json_hashing,
        )
        self.assertEqual(list(ds1), list(ds))
        self.assertEqual(items, [[[]], [[0]], [[0, 1]], [[0, 1, 2]], [[0, 1, 2, 3]]])

        self.assertTrue(isinstance(ds1.info(), dict))


</source>
</class>

<class classid="42" nclones="4" nlines="21" similarity="71">
<source file="systems/MONAI-0.8.0/tests/test_handler_checkpoint_loader.py" startline="60" endline="81" pcid="600">
    def test_two_save_one_load(self):
        logging.basicConfig(stream=sys.stdout, level=logging.INFO)
        net1 = torch.nn.PReLU()
        optimizer = optim.SGD(net1.parameters(), lr=0.02)
        data1 = net1.state_dict()
        data1["weight"] = torch.tensor([0.1])
        net1.load_state_dict(data1)
        net2 = torch.nn.PReLU()
        data2 = net2.state_dict()
        data2["weight"] = torch.tensor([0.2])
        net2.load_state_dict(data2)
        with tempfile.TemporaryDirectory() as tempdir:
            engine = Engine(lambda e, b: None)
            save_dict = {"net": net1, "opt": optimizer}
            CheckpointSaver(save_dir=tempdir, save_dict=save_dict, save_final=True).attach(engine)
            engine.run([0] * 8, max_epochs=5)
            path = tempdir + "/checkpoint_final_iteration=40.pt"
            engine = Engine(lambda e, b: None)
            CheckpointLoader(load_path=path, load_dict={"net": net2}, strict=True).attach(engine)
            engine.run([0] * 8, max_epochs=1)
            torch.testing.assert_allclose(net2.state_dict()["weight"], torch.tensor([0.1]))

</source>
<source file="systems/MONAI-0.8.0/tests/test_handler_checkpoint_loader.py" startline="126" endline="148" pcid="603">
    def test_partial_over_load(self):
        logging.basicConfig(stream=sys.stdout, level=logging.INFO)
        net1 = torch.nn.Sequential(*[torch.nn.PReLU()])
        data1 = net1.state_dict()
        data1["0.weight"] = torch.tensor([0.1])
        net1.load_state_dict(data1)

        net2 = torch.nn.Sequential(*[torch.nn.PReLU(), torch.nn.PReLU()])
        data2 = net2.state_dict()
        data2["0.weight"] = torch.tensor([0.2])
        data2["1.weight"] = torch.tensor([0.3])
        net2.load_state_dict(data2)

        with tempfile.TemporaryDirectory() as tempdir:
            engine = Engine(lambda e, b: None)
            CheckpointSaver(save_dir=tempdir, save_dict={"net": net1}, save_final=True).attach(engine)
            engine.run([0] * 8, max_epochs=5)
            path = tempdir + "/net_final_iteration=40.pt"
            engine = Engine(lambda e, b: None)
            CheckpointLoader(load_path=path, load_dict={"net": net2}, strict=False).attach(engine)
            engine.run([0] * 8, max_epochs=1)
            torch.testing.assert_allclose(net2.state_dict()["0.weight"].cpu(), torch.tensor([0.1]))

</source>
<source file="systems/MONAI-0.8.0/tests/test_handler_checkpoint_loader.py" startline="103" endline="125" pcid="602">
    def test_partial_under_load(self):
        logging.basicConfig(stream=sys.stdout, level=logging.INFO)
        net1 = torch.nn.Sequential(*[torch.nn.PReLU(), torch.nn.PReLU()])
        data1 = net1.state_dict()
        data1["0.weight"] = torch.tensor([0.1])
        data1["1.weight"] = torch.tensor([0.2])
        net1.load_state_dict(data1)

        net2 = torch.nn.Sequential(*[torch.nn.PReLU()])
        data2 = net2.state_dict()
        data2["0.weight"] = torch.tensor([0.3])
        net2.load_state_dict(data2)

        with tempfile.TemporaryDirectory() as tempdir:
            engine = Engine(lambda e, b: None)
            CheckpointSaver(save_dir=tempdir, save_dict={"net": net1}, save_final=True).attach(engine)
            engine.run([0] * 8, max_epochs=5)
            path = tempdir + "/net_final_iteration=40.pt"
            engine = Engine(lambda e, b: None)
            CheckpointLoader(load_path=path, load_dict={"net": net2}, strict=False).attach(engine)
            engine.run([0] * 8, max_epochs=1)
            torch.testing.assert_allclose(net2.state_dict()["0.weight"].cpu(), torch.tensor([0.1]))

</source>
<source file="systems/MONAI-0.8.0/tests/test_handler_checkpoint_loader.py" startline="82" endline="102" pcid="601">
    def test_save_single_device_load_multi_devices(self):
        logging.basicConfig(stream=sys.stdout, level=logging.INFO)
        net1 = torch.nn.PReLU()
        data1 = net1.state_dict()
        data1["weight"] = torch.tensor([0.1])
        net1.load_state_dict(data1)
        net2 = torch.nn.PReLU()
        data2 = net2.state_dict()
        data2["weight"] = torch.tensor([0.2])
        net2.load_state_dict(data2)
        net2 = torch.nn.DataParallel(net2)
        with tempfile.TemporaryDirectory() as tempdir:
            engine = Engine(lambda e, b: None)
            CheckpointSaver(save_dir=tempdir, save_dict={"net": net1}, save_final=True).attach(engine)
            engine.run([0] * 8, max_epochs=5)
            path = tempdir + "/net_final_iteration=40.pt"
            engine = Engine(lambda e, b: None)
            CheckpointLoader(load_path=path, load_dict={"net": net2}, strict=True).attach(engine)
            engine.run([0] * 8, max_epochs=1)
            torch.testing.assert_allclose(net2.state_dict()["module.weight"].cpu(), torch.tensor([0.1]))

</source>
</class>

<class classid="43" nclones="3" nlines="11" similarity="71">
<source file="systems/MONAI-0.8.0/tests/test_efficientnet.py" startline="250" endline="266" pcid="617">
    def test_shape(self, input_param, input_shape, expected_shape):
        device = "cuda" if torch.cuda.is_available() else "cpu"

        try:
            # initialize model
            net = EfficientNetBN(**input_param).to(device)
        except (ContentTooShortError, HTTPError, RuntimeError) as e:
            print(str(e))
            return  # skipping the tests because of http errors

        # run inference with random tensor
        with eval_mode(net):
            result = net(torch.randn(input_shape).to(device))

        # check output shape
        self.assertEqual(result.shape, expected_shape)

</source>
<source file="systems/MONAI-0.8.0/tests/test_efficientnet.py" startline="386" endline="405" pcid="625">
    def test_shape(self, input_param, input_shape, expected_shapes):
        device = "cuda" if torch.cuda.is_available() else "cpu"

        try:
            # initialize model
            net = EfficientNetBNFeatures(**input_param).to(device)
        except (ContentTooShortError, HTTPError, RuntimeError) as e:
            print(str(e))
            return  # skipping the tests because of http errors

        # run inference with random tensor
        with eval_mode(net):
            features = net(torch.randn(input_shape).to(device))

        # check output shape
        self.assertEqual(len(features), len(expected_shapes))
        for feature, expected_shape in zip(features, expected_shapes):
            self.assertEqual(feature.shape, torch.Size(expected_shape))


</source>
<source file="systems/MONAI-0.8.0/tests/test_efficientnet.py" startline="268" endline="289" pcid="618">
    def test_non_default_shapes(self, input_param, input_shape, expected_shape):
        device = "cuda" if torch.cuda.is_available() else "cpu"

        try:
            # initialize model
            net = EfficientNetBN(**input_param).to(device)
        except (ContentTooShortError, HTTPError, RuntimeError) as e:
            print(str(e))
            return  # skipping the tests because of http errors

        # override input shape with different variations
        num_dims = len(input_shape) - 2
        non_default_sizes = [128, 256, 512]
        for candidate_size in non_default_sizes:
            input_shape = input_shape[0:2] + (candidate_size,) * num_dims
            # run inference with random tensor
            with eval_mode(net):
                result = net(torch.randn(input_shape).to(device))

            # check output shape
            self.assertEqual(result.shape, expected_shape)

</source>
</class>

<class classid="44" nclones="2" nlines="11" similarity="75">
<source file="systems/MONAI-0.8.0/tests/test_rand_k_space_spike_noise.py" startline="46" endline="56" pcid="668">
    def test_0_prob(self, im_shape, im_type, channel_wise):
        im = self.get_data(im_shape, im_type)
        intensity_range = [14, 15]
        t = RandKSpaceSpikeNoise(0.0, intensity_range, channel_wise)
        out = t(im)
        self.assertEqual(type(im), type(out))
        if isinstance(out, torch.Tensor):
            self.assertEqual(out.device, im.device)
            im, out = im.cpu(), out.cpu()
        np.testing.assert_allclose(im, out)

</source>
<source file="systems/MONAI-0.8.0/tests/test_rand_k_space_spike_noise.py" startline="58" endline="70" pcid="669">
    def test_1_prob(self, im_shape, im_type, channel_wise):
        im = self.get_data(im_shape, im_type)
        intensity_range = [14, 14]
        t = RandKSpaceSpikeNoise(1.0, intensity_range, channel_wise)
        out = t(im)
        base_t = KSpaceSpikeNoise(t.sampled_locs, [14])
        out = out - base_t(im)
        self.assertEqual(type(im), type(out))
        if isinstance(out, torch.Tensor):
            self.assertEqual(out.device, im.device)
            im, out = im.cpu(), out.cpu()
        np.testing.assert_allclose(out, im * 0)

</source>
</class>

<class classid="45" nclones="4" nlines="15" similarity="71">
<source file="systems/MONAI-0.8.0/tests/test_orientationd.py" startline="29" endline="44" pcid="711">
    def test_orntd_3d(self):
        data = {
            "seg": np.ones((2, 1, 2, 3)),
            "img": np.ones((2, 1, 2, 3)),
            "seg_meta_dict": {"affine": np.eye(4)},
            "img_meta_dict": {"affine": np.eye(4)},
        }
        ornt = Orientationd(keys=("img", "seg"), axcodes="PLI")
        res = ornt(data)
        np.testing.assert_allclose(res["img"].shape, (2, 2, 1, 3))
        np.testing.assert_allclose(res["seg"].shape, (2, 2, 1, 3))
        code = nib.aff2axcodes(res["seg_meta_dict"]["affine"], ornt.ornt_transform.labels)
        self.assertEqual(code, ("P", "L", "I"))
        code = nib.aff2axcodes(res["img_meta_dict"]["affine"], ornt.ornt_transform.labels)
        self.assertEqual(code, ("P", "L", "I"))

</source>
<source file="systems/MONAI-0.8.0/tests/test_orientationd.py" startline="75" endline="90" pcid="714">
    def test_orntd_canonical(self):
        data = {
            "seg": np.ones((2, 1, 2, 3)),
            "img": np.ones((2, 1, 2, 3)),
            "seg_meta_dict": {"affine": np.eye(4)},
            "img_meta_dict": {"affine": np.eye(4)},
        }
        ornt = Orientationd(keys=("img", "seg"), as_closest_canonical=True)
        res = ornt(data)
        np.testing.assert_allclose(res["img"].shape, (2, 1, 2, 3))
        np.testing.assert_allclose(res["seg"].shape, (2, 1, 2, 3))
        code = nib.aff2axcodes(res["seg_meta_dict"]["affine"], ornt.ornt_transform.labels)
        self.assertEqual(code, ("R", "A", "S"))
        code = nib.aff2axcodes(res["img_meta_dict"]["affine"], ornt.ornt_transform.labels)
        self.assertEqual(code, ("R", "A", "S"))

</source>
<source file="systems/MONAI-0.8.0/tests/test_orientationd.py" startline="60" endline="74" pcid="713">
    def test_orntd_1d(self):
        data = {
            "seg": np.ones((2, 3)),
            "img": np.ones((2, 3)),
            "seg_meta_dict": {"affine": np.eye(4)},
            "img_meta_dict": {"affine": np.eye(4)},
        }
        ornt = Orientationd(keys=("img", "seg"), axcodes="L")
        res = ornt(data)
        np.testing.assert_allclose(res["img"].shape, (2, 3))
        code = nib.aff2axcodes(res["seg_meta_dict"]["affine"], ornt.ornt_transform.labels)
        self.assertEqual(code, ("L", "A", "S"))
        code = nib.aff2axcodes(res["img_meta_dict"]["affine"], ornt.ornt_transform.labels)
        self.assertEqual(code, ("L", "A", "S"))

</source>
<source file="systems/MONAI-0.8.0/tests/test_orientationd.py" startline="45" endline="59" pcid="712">
    def test_orntd_2d(self):
        data = {
            "seg": np.ones((2, 1, 3)),
            "img": np.ones((2, 1, 3)),
            "seg_meta_dict": {"affine": np.eye(4)},
            "img_meta_dict": {"affine": np.eye(4)},
        }
        ornt = Orientationd(keys=("img", "seg"), axcodes="PLI")
        res = ornt(data)
        np.testing.assert_allclose(res["img"].shape, (2, 3, 1))
        code = nib.aff2axcodes(res["seg_meta_dict"]["affine"], ornt.ornt_transform.labels)
        self.assertEqual(code, ("P", "L", "S"))
        code = nib.aff2axcodes(res["img_meta_dict"]["affine"], ornt.ornt_transform.labels)
        self.assertEqual(code, ("P", "L", "S"))

</source>
</class>

<class classid="46" nclones="3" nlines="11" similarity="72">
<source file="systems/MONAI-0.8.0/tests/test_rand_cucim_transform.py" startline="116" endline="129" pcid="717">
    def test_tramsforms_numpy_batch(self, params, input, expected):
        input = input[cp.newaxis, ...]
        expected = expected[cp.newaxis, ...]
        # apply_prob=1.0
        output = RandCuCIM(apply_prob=1.0, **params)(input)
        self.assertTrue(output.dtype == expected.dtype)
        self.assertTrue(isinstance(output, np.ndarray))
        cp.testing.assert_allclose(output, expected)
        # apply_prob=0.0
        output = RandCuCIM(apply_prob=0.0, **params)(input)
        self.assertTrue(output.dtype == input.dtype)
        self.assertTrue(isinstance(output, np.ndarray))
        cp.testing.assert_allclose(output, input)

</source>
<source file="systems/MONAI-0.8.0/tests/test_rand_cucim_transform.py" startline="142" endline="155" pcid="718">
    def test_tramsforms_cupy_single(self, params, input, expected):
        input = cp.asarray(input)
        expected = cp.asarray(expected)
        # apply_prob=1.0
        output = RandCuCIM(apply_prob=1.0, **params)(input)
        self.assertTrue(output.dtype == expected.dtype)
        self.assertTrue(isinstance(output, cp.ndarray))
        cp.testing.assert_allclose(output, expected)
        # apply_prob=0.0
        output = RandCuCIM(apply_prob=0.0, **params)(input)
        self.assertTrue(output.dtype == input.dtype)
        self.assertTrue(isinstance(output, cp.ndarray))
        cp.testing.assert_allclose(output, input)

</source>
<source file="systems/MONAI-0.8.0/tests/test_rand_cucim_transform.py" startline="168" endline="182" pcid="719">
    def test_tramsforms_cupy_batch(self, params, input, expected):
        input = cp.asarray(input)[cp.newaxis, ...]
        expected = cp.asarray(expected)[cp.newaxis, ...]
        # apply_prob=1.0
        output = RandCuCIM(**params)(input)
        self.assertTrue(output.dtype == expected.dtype)
        self.assertTrue(isinstance(output, cp.ndarray))
        cp.testing.assert_allclose(output, expected)
        # apply_prob=0.0
        output = RandCuCIM(apply_prob=0.0, **params)(input)
        self.assertTrue(output.dtype == input.dtype)
        self.assertTrue(isinstance(output, cp.ndarray))
        cp.testing.assert_allclose(output, input)


</source>
</class>

<class classid="47" nclones="2" nlines="11" similarity="81">
<source file="systems/MONAI-0.8.0/tests/test_handler_classification_saver.py" startline="45" endline="56" pcid="805">
            def _test_file(filename):
                filepath = os.path.join(tempdir, filename)
                self.assertTrue(os.path.exists(filepath))
                with open(filepath) as f:
                    reader = csv.reader(f)
                    i = 0
                    for row in reader:
                        self.assertEqual(row[0], "testfile" + str(i))
                        self.assertEqual(np.array(row[1:]).astype(np.float32), 0.0)
                        i += 1
                    self.assertEqual(i, 8)

</source>
<source file="systems/MONAI-0.8.0/tests/test_save_classificationd.py" startline="84" endline="95" pcid="839">
            def _test_file(filename, count):
                filepath = os.path.join(tempdir, filename)
                self.assertTrue(os.path.exists(filepath))
                with open(filepath) as f:
                    reader = csv.reader(f)
                    i = 0
                    for row in reader:
                        self.assertEqual(row[0], "testfile" + str(i))
                        self.assertEqual(np.array(row[1:]).astype(np.float32), 0.0)
                        i += 1
                    self.assertEqual(i, count)

</source>
</class>

<class classid="48" nclones="2" nlines="23" similarity="91">
<source file="systems/MONAI-0.8.0/tests/test_label_to_contour.py" startline="117" endline="141" pcid="818">
def gen_fixed_img(array_type):
    img = np.array(
        [
            [0, 0, 0, 1, 1, 1, 1],
            [0, 0, 0, 1, 1, 1, 1],
            [0, 0, 1, 1, 1, 1, 1],
            [0, 1, 1, 1, 1, 1, 1],
            [1, 1, 1, 1, 1, 1, 1],
        ],
        dtype=np.float32,
    )
    batch_size, channels = 10, 6
    img = array_type(np.tile(img, (batch_size, channels, 1, 1)))
    expected_output_for_img = array_type(
        [
            [0, 0, 0, 1, 1, 1, 1],
            [0, 0, 0, 1, 0, 0, 1],
            [0, 0, 1, 1, 0, 0, 1],
            [0, 1, 1, 0, 0, 0, 1],
            [1, 1, 1, 1, 1, 1, 1],
        ]
    )
    return img, expected_output_for_img


</source>
<source file="systems/MONAI-0.8.0/tests/test_label_to_contourd.py" startline="117" endline="142" pcid="1143">
def gen_fixed_img(array_type):
    img = np.array(
        [
            [0, 0, 0, 1, 1, 1, 1],
            [0, 0, 0, 1, 1, 1, 1],
            [0, 0, 1, 1, 1, 1, 1],
            [0, 1, 1, 1, 1, 1, 1],
            [1, 1, 1, 1, 1, 1, 1],
        ],
        dtype=np.float32,
    )
    batch_size, channels = 10, 6
    img = np.tile(img, (batch_size, channels, 1, 1))
    img = array_type(img)
    expected_output_for_img = array_type(
        [
            [0, 0, 0, 1, 1, 1, 1],
            [0, 0, 0, 1, 0, 0, 1],
            [0, 0, 1, 1, 0, 0, 1],
            [0, 1, 1, 0, 0, 0, 1],
            [1, 1, 1, 1, 1, 1, 1],
        ]
    )
    return img, expected_output_for_img


</source>
</class>

<class classid="49" nclones="4" nlines="12" similarity="100">
<source file="systems/MONAI-0.8.0/tests/test_masked_dice_loss.py" startline="134" endline="147" pcid="834">
    def test_input_warnings(self):
        chn_input = torch.ones((1, 1, 3))
        chn_target = torch.ones((1, 1, 3))
        with self.assertWarns(Warning):
            loss = MaskedDiceLoss(include_background=False)
            loss.forward(chn_input, chn_target)
        with self.assertWarns(Warning):
            loss = MaskedDiceLoss(softmax=True)
            loss.forward(chn_input, chn_target)
        with self.assertWarns(Warning):
            loss = MaskedDiceLoss(to_onehot_y=True)
            loss.forward(chn_input, chn_target)


</source>
<source file="systems/MONAI-0.8.0/tests/test_generalized_dice_loss.py" startline="163" endline="175" pcid="1151">
    def test_input_warnings(self):
        chn_input = torch.ones((1, 1, 3))
        chn_target = torch.ones((1, 1, 3))
        with self.assertWarns(Warning):
            loss = GeneralizedDiceLoss(include_background=False)
            loss.forward(chn_input, chn_target)
        with self.assertWarns(Warning):
            loss = GeneralizedDiceLoss(softmax=True)
            loss.forward(chn_input, chn_target)
        with self.assertWarns(Warning):
            loss = GeneralizedDiceLoss(to_onehot_y=True)
            loss.forward(chn_input, chn_target)

</source>
<source file="systems/MONAI-0.8.0/tests/test_tversky_loss.py" startline="165" endline="177" pcid="929">
    def test_input_warnings(self):
        chn_input = torch.ones((1, 1, 3))
        chn_target = torch.ones((1, 1, 3))
        with self.assertWarns(Warning):
            loss = TverskyLoss(include_background=False)
            loss.forward(chn_input, chn_target)
        with self.assertWarns(Warning):
            loss = TverskyLoss(softmax=True)
            loss.forward(chn_input, chn_target)
        with self.assertWarns(Warning):
            loss = TverskyLoss(to_onehot_y=True)
            loss.forward(chn_input, chn_target)

</source>
<source file="systems/MONAI-0.8.0/tests/test_dice_loss.py" startline="174" endline="186" pcid="988">
    def test_input_warnings(self):
        chn_input = torch.ones((1, 1, 3))
        chn_target = torch.ones((1, 1, 3))
        with self.assertWarns(Warning):
            loss = DiceLoss(include_background=False)
            loss.forward(chn_input, chn_target)
        with self.assertWarns(Warning):
            loss = DiceLoss(softmax=True)
            loss.forward(chn_input, chn_target)
        with self.assertWarns(Warning):
            loss = DiceLoss(to_onehot_y=True)
            loss.forward(chn_input, chn_target)

</source>
</class>

<class classid="50" nclones="2" nlines="10" similarity="90">
<source file="systems/MONAI-0.8.0/tests/test_rand_scale_intensityd.py" startline="21" endline="33" pcid="879">
    def test_value(self):
        key = "img"
        for p in TEST_NDARRAYS:
            scaler = RandScaleIntensityd(keys=[key], factors=0.5, prob=1.0)
            scaler.set_random_state(seed=0)
            result = scaler({key: p(self.imt)})
            np.random.seed(0)
            # simulate the randomize function of transform
            np.random.random()
            expected = (self.imt * (1 + np.random.uniform(low=-0.5, high=0.5))).astype(np.float32)
            assert_allclose(result[key], p(expected))


</source>
<source file="systems/MONAI-0.8.0/tests/test_rand_shift_intensityd.py" startline="21" endline="32" pcid="1039">
    def test_value(self):
        key = "img"
        for p in TEST_NDARRAYS:
            shifter = RandShiftIntensityd(keys=[key], offsets=1.0, prob=1.0)
            shifter.set_random_state(seed=0)
            result = shifter({key: p(self.imt)})
            np.random.seed(0)
            # simulate the randomize() of transform
            np.random.random()
            expected = self.imt + np.random.uniform(low=-1.0, high=1.0)
            assert_allclose(result[key], p(expected))

</source>
</class>

<class classid="51" nclones="3" nlines="17" similarity="72">
<source file="systems/MONAI-0.8.0/tests/test_load_decathlon_datalist.py" startline="22" endline="41" pcid="902">
    def test_seg_values(self):
        with tempfile.TemporaryDirectory() as tempdir:
            test_data = {
                "name": "Spleen",
                "description": "Spleen Segmentation",
                "labels": {"0": "background", "1": "spleen"},
                "training": [
                    {"image": "spleen_19.nii.gz", "label": "spleen_19.nii.gz"},
                    {"image": "spleen_31.nii.gz", "label": "spleen_31.nii.gz"},
                ],
                "test": ["spleen_15.nii.gz", "spleen_23.nii.gz"],
            }
            json_str = json.dumps(test_data)
            file_path = os.path.join(tempdir, "test_data.json")
            with open(file_path, "w") as json_file:
                json_file.write(json_str)
            result = load_decathlon_datalist(file_path, True, "training", tempdir)
            self.assertEqual(result[0]["image"], os.path.join(tempdir, "spleen_19.nii.gz"))
            self.assertEqual(result[0]["label"], os.path.join(tempdir, "spleen_19.nii.gz"))

</source>
<source file="systems/MONAI-0.8.0/tests/test_load_decathlon_datalist.py" startline="42" endline="58" pcid="903">
    def test_cls_values(self):
        with tempfile.TemporaryDirectory() as tempdir:
            test_data = {
                "name": "ChestXRay",
                "description": "Chest X-ray classification",
                "labels": {"0": "background", "1": "chest"},
                "training": [{"image": "chest_19.nii.gz", "label": 0}, {"image": "chest_31.nii.gz", "label": 1}],
                "test": ["chest_15.nii.gz", "chest_23.nii.gz"],
            }
            json_str = json.dumps(test_data)
            file_path = os.path.join(tempdir, "test_data.json")
            with open(file_path, "w") as json_file:
                json_file.write(json_str)
            result = load_decathlon_datalist(file_path, False, "training", tempdir)
            self.assertEqual(result[0]["image"], os.path.join(tempdir, "chest_19.nii.gz"))
            self.assertEqual(result[0]["label"], 0)

</source>
<source file="systems/MONAI-0.8.0/tests/test_load_decathlon_datalist.py" startline="85" endline="99" pcid="905">
    def test_seg_no_labels(self):
        with tempfile.TemporaryDirectory() as tempdir:
            test_data = {
                "name": "Spleen",
                "description": "Spleen Segmentation",
                "labels": {"0": "background", "1": "spleen"},
                "test": ["spleen_15.nii.gz", "spleen_23.nii.gz"],
            }
            json_str = json.dumps(test_data)
            file_path = os.path.join(tempdir, "test_data.json")
            with open(file_path, "w") as json_file:
                json_file.write(json_str)
            result = load_decathlon_datalist(file_path, True, "test", tempdir)
            self.assertEqual(result[0]["image"], os.path.join(tempdir, "spleen_15.nii.gz"))

</source>
</class>

<class classid="52" nclones="2" nlines="15" similarity="73">
<source file="systems/MONAI-0.8.0/tests/test_dice_focal_loss.py" startline="22" endline="38" pcid="1002">
    def test_result_onehot_target_include_bg(self):
        size = [3, 3, 5, 5]
        label = torch.randint(low=0, high=2, size=size)
        pred = torch.randn(size)
        for reduction in ["sum", "mean", "none"]:
            common_params = {"include_background": True, "to_onehot_y": False, "reduction": reduction}
            for focal_weight in [None, torch.tensor([1.0, 1.0, 2.0]), (3, 2.0, 1)]:
                for lambda_focal in [0.5, 1.0, 1.5]:
                    dice_focal = DiceFocalLoss(
                        focal_weight=focal_weight, gamma=1.0, lambda_focal=lambda_focal, **common_params
                    )
                    dice = DiceLoss(**common_params)
                    focal = FocalLoss(weight=focal_weight, gamma=1.0, **common_params)
                    result = dice_focal(pred, label)
                    expected_val = dice(pred, label) + lambda_focal * focal(pred, label)
                    np.testing.assert_allclose(result, expected_val)

</source>
<source file="systems/MONAI-0.8.0/tests/test_dice_focal_loss.py" startline="39" endline="54" pcid="1003">
    def test_result_no_onehot_no_bg(self):
        size = [3, 3, 5, 5]
        label = torch.randint(low=0, high=2, size=size)
        label = torch.argmax(label, dim=1, keepdim=True)
        pred = torch.randn(size)
        for reduction in ["sum", "mean", "none"]:
            common_params = {"include_background": False, "to_onehot_y": True, "reduction": reduction}
            for focal_weight in [2.0, torch.tensor([1.0, 2.0]), (2.0, 1)]:
                for lambda_focal in [0.5, 1.0, 1.5]:
                    dice_focal = DiceFocalLoss(focal_weight=focal_weight, lambda_focal=lambda_focal, **common_params)
                    dice = DiceLoss(**common_params)
                    focal = FocalLoss(weight=focal_weight, **common_params)
                    result = dice_focal(pred, label)
                    expected_val = dice(pred, label) + lambda_focal * focal(pred, label)
                    np.testing.assert_allclose(result, expected_val)

</source>
</class>

<class classid="53" nclones="2" nlines="16" similarity="93">
<source file="systems/MONAI-0.8.0/tests/test_nvtx_decorator.py" startline="47" endline="70" pcid="1116">
    def test_tranform_array(self, input):
        transforms = Compose([Range("random flip")(Flip()), Range()(ToTensor())])
        # Apply transforms
        output = transforms(input)

        # Decorate with NVTX Range
        transforms1 = Range()(transforms)
        transforms2 = Range("Transforms2")(transforms)
        transforms3 = Range(name="Transforms3", methods="__call__")(transforms)

        # Apply transforms with Range
        output1 = transforms1(input)
        output2 = transforms2(input)
        output3 = transforms3(input)

        # Check the outputs
        self.assertIsInstance(output, torch.Tensor)
        self.assertIsInstance(output1, torch.Tensor)
        self.assertIsInstance(output2, torch.Tensor)
        self.assertIsInstance(output3, torch.Tensor)
        np.testing.assert_equal(output.numpy(), output1.numpy())
        np.testing.assert_equal(output.numpy(), output1.numpy())
        np.testing.assert_equal(output.numpy(), output3.numpy())

</source>
<source file="systems/MONAI-0.8.0/tests/test_nvtx_decorator.py" startline="140" endline="165" pcid="1119">
    def test_network(self, input):
        # Create a network
        model = torch.nn.Sequential(torch.nn.ReLU(), torch.nn.Sigmoid())

        # Forward
        output = model(input)

        # Decorate with NVTX Range
        model1 = Range()(model)
        model2 = Range("Model2")(model)
        model3 = Range(name="Model3", methods="forward")(model)

        # Forward with Range
        output1 = model1(input)
        output2 = model2(input)
        output3 = model3(input)

        # Check the outputs
        self.assertIsInstance(output, torch.Tensor)
        self.assertIsInstance(output1, torch.Tensor)
        self.assertIsInstance(output2, torch.Tensor)
        self.assertIsInstance(output3, torch.Tensor)
        np.testing.assert_equal(output.numpy(), output1.numpy())
        np.testing.assert_equal(output.numpy(), output2.numpy())
        np.testing.assert_equal(output.numpy(), output3.numpy())

</source>
</class>

<class classid="54" nclones="4" nlines="20" similarity="72">
<source file="systems/MONAI-0.8.0/tests/test_copy_model_state.py" startline="60" endline="81" pcid="1137">
    def test_set_state(self, device_0, device_1):
        set_determinism(0)
        model_one = _TestModelOne(10, 20, 3)
        model_two = _TestModelTwo(10, 20, 10, 4)
        model_one.to(device_0)
        model_two.to(device_1)
        model_dict, ch, unch = copy_model_state(model_one, model_two)
        x = np.random.randn(4, 10)
        x = torch.tensor(x, device=device_0, dtype=torch.float32)
        output = model_one(x).detach().cpu().numpy()
        expected = np.array(
            [
                [-0.36076584, -0.03177825, -0.7702266],
                [-0.0526831, -0.15855855, -0.01149344],
                [-0.3760508, -0.22485238, -0.0634037],
                [0.5977675, -0.67991066, 0.1919502],
            ]
        )
        np.testing.assert_allclose(output, expected, atol=1e-3)
        self.assertEqual(len(ch), 2)
        self.assertEqual(len(unch), 2)

</source>
<source file="systems/MONAI-0.8.0/tests/test_copy_model_state.py" startline="127" endline="152" pcid="1140">
    def test_set_map_across(self, device_0, device_1):
        set_determinism(0)
        model_one = _TestModelOne(10, 10, 3)
        model_two = _TestModelTwo(10, 10, 10, 4)
        model_one.to(device_0)
        model_two.to(device_1)
        # test weight map
        model_dict, ch, unch = copy_model_state(
            model_one, model_two, mapping={"layer_1.weight": "layer.weight", "layer_1.bias": "layer_1.weight"}
        )
        model_one.load_state_dict(model_dict)
        x = np.random.randn(4, 10)
        x = torch.tensor(x, device=device_0, dtype=torch.float32)
        output = model_one(x).detach().cpu().numpy()
        expected = np.array(
            [
                [0.8244487, -0.19650555, 0.65723234],
                [0.71239626, 0.25617486, 0.5247122],
                [0.24168758, 1.0301148, 0.39089814],
                [0.25791705, 0.8653245, 0.14833644],
            ]
        )
        np.testing.assert_allclose(output, expected, atol=1e-3)
        self.assertEqual(len(ch), 2)
        self.assertEqual(len(unch), 2)

</source>
<source file="systems/MONAI-0.8.0/tests/test_copy_model_state.py" startline="103" endline="125" pcid="1139">
    def test_set_exclude_vars(self, device_0, device_1):
        set_determinism(0)
        model_one = _TestModelOne(10, 20, 3)
        model_two = _TestModelTwo(10, 20, 10, 4)
        model_one.to(device_0)
        model_two.to(device_1)
        # test skip layer.bias
        model_dict, ch, unch = copy_model_state(model_one, model_two, exclude_vars="layer.bias")
        x = np.random.randn(4, 10)
        x = torch.tensor(x, device=device_0, dtype=torch.float32)
        output = model_one(x).detach().cpu().numpy()
        expected = np.array(
            [
                [-0.34172416, 0.0375042, -0.98340976],
                [-0.03364138, -0.08927619, -0.2246768],
                [-0.35700908, -0.15556987, -0.27658707],
                [0.61680925, -0.6106281, -0.02123314],
            ]
        )
        np.testing.assert_allclose(output, expected, atol=1e-3)
        self.assertEqual(len(ch), 1)
        self.assertEqual(len(unch), 3)

</source>
<source file="systems/MONAI-0.8.0/tests/test_copy_model_state.py" startline="154" endline="180" pcid="1141">
    def test_set_prefix(self, device_0, device_1):
        set_determinism(0)
        model_one = torch.nn.Sequential(_TestModelOne(10, 20, 3))
        model_two = _TestModelTwo(10, 20, 10, 4)
        model_one.to(device_0)
        model_two.to(device_1)
        # test skip layer.bias
        model_dict, ch, unch = copy_model_state(
            model_one, model_two, dst_prefix="0.", exclude_vars="layer.bias", inplace=False
        )
        model_one.load_state_dict(model_dict)
        x = np.random.randn(4, 10)
        x = torch.tensor(x, device=device_0, dtype=torch.float32)
        output = model_one(x).detach().cpu().numpy()
        expected = np.array(
            [
                [-0.360766, -0.031778, -0.770227],
                [-0.052683, -0.158559, -0.011493],
                [-0.376051, -0.224852, -0.063404],
                [0.597767, -0.679911, 0.19195],
            ]
        )
        np.testing.assert_allclose(output, expected, atol=1e-3)
        self.assertEqual(len(ch), 2)
        self.assertEqual(len(unch), 2)


</source>
</class>

<class classid="55" nclones="2" nlines="18" similarity="100">
<source file="systems/MONAI-0.8.0/tests/test_create_grid_and_affine.py" startline="232" endline="250" pcid="1164">
    def test_create_scale(self):
        test_assert(create_scale, (2, 2), np.array([[2.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]))
        test_assert(create_scale, (2, [2, 2, 2]), np.array([[2.0, 0.0, 0.0], [0.0, 2.0, 0.0], [0.0, 0.0, 1.0]]))
        test_assert(
            create_scale,
            (3, [1.5, 2.4]),
            np.array([[1.5, 0.0, 0.0, 0.0], [0.0, 2.4, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]]),
        )
        test_assert(
            create_scale,
            (3, 1.5),
            np.array([[1.5, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]]),
        )
        test_assert(
            create_scale,
            (3, [1, 2, 3, 4, 5]),
            np.array([[1.0, 0.0, 0.0, 0.0], [0.0, 2.0, 0.0, 0.0], [0.0, 0.0, 3.0, 0.0], [0.0, 0.0, 0.0, 1.0]]),
        )

</source>
<source file="systems/MONAI-0.8.0/tests/test_create_grid_and_affine.py" startline="251" endline="270" pcid="1165">
    def test_create_translate(self):
        test_assert(create_translate, (2, 2), np.array([[1.0, 0.0, 2.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]))
        test_assert(create_translate, (2, [2, 2, 2]), np.array([[1.0, 0.0, 2.0], [0.0, 1.0, 2.0], [0.0, 0.0, 1.0]]))
        test_assert(
            create_translate,
            (3, [1.5, 2.4]),
            np.array([[1.0, 0.0, 0.0, 1.5], [0.0, 1.0, 0.0, 2.4], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]]),
        )
        test_assert(
            create_translate,
            (3, 1.5),
            np.array([[1.0, 0.0, 0.0, 1.5], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]]),
        )
        test_assert(
            create_translate,
            (3, [1, 2, 3, 4, 5]),
            np.array([[1.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 2.0], [0.0, 0.0, 1.0, 3.0], [0.0, 0.0, 0.0, 1.0]]),
        )


</source>
</class>

<class classid="56" nclones="2" nlines="19" similarity="84">
<source file="systems/MONAI-0.8.0/tests/test_compose.py" startline="60" endline="81" pcid="1310">
    def test_list_dict_compose(self):
        def a(d):  # transform to handle dict data
            d = dict(d)
            d["a"] += 1
            return d

        def b(d):  # transform to generate a batch list of data
            d = dict(d)
            d["b"] += 1
            d = [d] * 5
            return d

        def c(d):  # transform to handle dict data
            d = dict(d)
            d["c"] += 1
            return d

        transforms = Compose([a, a, b, c, c])
        value = transforms({"a": 0, "b": 0, "c": 0})
        for item in value:
            self.assertDictEqual(item, {"a": 2, "b": 1, "c": 2})

</source>
<source file="systems/MONAI-0.8.0/tests/test_compose.py" startline="102" endline="124" pcid="1320">
    def test_list_dict_compose_no_map(self):
        def a(d):  # transform to handle dict data
            d = dict(d)
            d["a"] += 1
            return d

        def b(d):  # transform to generate a batch list of data
            d = dict(d)
            d["b"] += 1
            d = [d] * 5
            return d

        def c(d):  # transform to handle dict data
            d = [dict(di) for di in d]
            for di in d:
                di["c"] += 1
            return d

        transforms = Compose([a, a, b, c, c], map_items=False)
        value = transforms({"a": 0, "b": 0, "c": 0})
        for item in value:
            self.assertDictEqual(item, {"a": 2, "b": 1, "c": 2})

</source>
</class>

<class classid="57" nclones="2" nlines="18" similarity="83">
<source file="systems/MONAI-0.8.0/tests/test_compose.py" startline="161" endline="183" pcid="1331">
    def test_data_loader(self):
        xform_1 = Compose([_RandXform()])
        train_ds = Dataset([1], transform=xform_1)

        xform_1.set_random_state(123)
        out_1 = train_ds[0]
        self.assertAlmostEqual(out_1, 0.2045649)

        set_determinism(seed=123)
        train_loader = DataLoader(train_ds, num_workers=0)
        out_1 = next(iter(train_loader))
        self.assertAlmostEqual(out_1.cpu().item(), 0.84291356)

        if sys.platform != "win32":  # skip multi-worker tests on win32
            train_loader = DataLoader(train_ds, num_workers=1)
            out_1 = next(iter(train_loader))
            self.assertAlmostEqual(out_1.cpu().item(), 0.180814653)

            train_loader = DataLoader(train_ds, num_workers=2)
            out_1 = next(iter(train_loader))
            self.assertAlmostEqual(out_1.cpu().item(), 0.04293707)
        set_determinism(None)

</source>
<source file="systems/MONAI-0.8.0/tests/test_compose.py" startline="184" endline="205" pcid="1332">
    def test_data_loader_2(self):
        set_determinism(seed=123)
        xform_2 = Compose([_RandXform(), _RandXform()])
        train_ds = Dataset([1], transform=xform_2)

        out_2 = train_ds[0]
        self.assertAlmostEqual(out_2, 0.4092510)

        train_loader = DataLoader(train_ds, num_workers=0)
        out_2 = next(iter(train_loader))
        self.assertAlmostEqual(out_2.cpu().item(), 0.7858843729)

        if sys.platform != "win32":  # skip multi-worker tests on win32
            train_loader = DataLoader(train_ds, num_workers=1)
            out_2 = next(iter(train_loader))
            self.assertAlmostEqual(out_2.cpu().item(), 0.305763411)

            train_loader = DataLoader(train_ds, num_workers=2)
            out_1 = next(iter(train_loader))
            self.assertAlmostEqual(out_1.cpu().item(), 0.131966779)
        set_determinism(None)

</source>
</class>

<class classid="58" nclones="3" nlines="22" similarity="75">
<source file="systems/MONAI-0.8.0/tests/test_handler_stats.py" startline="26" endline="60" pcid="1365">
    def test_metrics_print(self):
        log_stream = StringIO()
        log_handler = logging.StreamHandler(log_stream)
        log_handler.setLevel(logging.INFO)
        key_to_handler = "test_logging"
        key_to_print = "testing_metric"

        # set up engine
        def _train_func(engine, batch):
            return [torch.tensor(0.0)]

        engine = Engine(_train_func)

        # set up dummy metric
        @engine.on(Events.EPOCH_COMPLETED)
        def _update_metric(engine):
            current_metric = engine.state.metrics.get(key_to_print, 0.1)
            engine.state.metrics[key_to_print] = current_metric + 0.1

        # set up testing handler
        stats_handler = StatsHandler(name=key_to_handler, logger_handler=log_handler)
        stats_handler.attach(engine)

        engine.run(range(3), max_epochs=2)

        # check logging output
        output_str = log_stream.getvalue()
        log_handler.close()
        grep = re.compile(f".*{key_to_handler}.*")
        has_key_word = re.compile(f".*{key_to_print}.*")
        for idx, line in enumerate(output_str.split("\n")):
            if grep.match(line):
                if idx in [5, 10]:
                    self.assertTrue(has_key_word.match(line))

</source>
<source file="systems/MONAI-0.8.0/tests/test_handler_stats.py" startline="61" endline="89" pcid="1368">
    def test_loss_print(self):
        log_stream = StringIO()
        log_handler = logging.StreamHandler(log_stream)
        log_handler.setLevel(logging.INFO)
        key_to_handler = "test_logging"
        key_to_print = "myLoss"

        # set up engine
        def _train_func(engine, batch):
            return [torch.tensor(0.0)]

        engine = Engine(_train_func)

        # set up testing handler
        stats_handler = StatsHandler(name=key_to_handler, tag_name=key_to_print, logger_handler=log_handler)
        stats_handler.attach(engine)

        engine.run(range(3), max_epochs=2)

        # check logging output
        output_str = log_stream.getvalue()
        log_handler.close()
        grep = re.compile(f".*{key_to_handler}.*")
        has_key_word = re.compile(f".*{key_to_print}.*")
        for idx, line in enumerate(output_str.split("\n")):
            if grep.match(line):
                if idx in [1, 2, 3, 6, 7, 8]:
                    self.assertTrue(has_key_word.match(line))

</source>
<source file="systems/MONAI-0.8.0/tests/test_handler_stats.py" startline="90" endline="120" pcid="1370">
    def test_loss_dict(self):
        log_stream = StringIO()
        log_handler = logging.StreamHandler(log_stream)
        log_handler.setLevel(logging.INFO)
        key_to_handler = "test_logging"
        key_to_print = "myLoss1"

        # set up engine
        def _train_func(engine, batch):
            return [torch.tensor(0.0)]

        engine = Engine(_train_func)

        # set up testing handler
        stats_handler = StatsHandler(
            name=key_to_handler, output_transform=lambda x: {key_to_print: x}, logger_handler=log_handler
        )
        stats_handler.attach(engine)

        engine.run(range(3), max_epochs=2)

        # check logging output
        output_str = log_stream.getvalue()
        log_handler.close()
        grep = re.compile(f".*{key_to_handler}.*")
        has_key_word = re.compile(f".*{key_to_print}.*")
        for idx, line in enumerate(output_str.split("\n")):
            if grep.match(line):
                if idx in [1, 2, 3, 6, 7, 8]:
                    self.assertTrue(has_key_word.match(line))

</source>
</class>

<class classid="59" nclones="2" nlines="52" similarity="83">
<source file="systems/MONAI-0.8.0/monai/apps/deepgrow/dataset.py" startline="135" endline="210" pcid="1401">
def _save_data_2d(vol_idx, vol_image, vol_label, dataset_dir, relative_path):
    data_list = []

    if len(vol_image.shape) == 4:
        logging.info(
            "4D-Image, pick only first series; Image: {}; Label: {}".format(
                vol_image.shape, vol_label.shape if vol_label is not None else None
            )
        )
        vol_image = vol_image[0]
        vol_image = np.moveaxis(vol_image, -1, 0)

    image_count = 0
    label_count = 0
    unique_labels_count = 0
    for sid in range(vol_image.shape[0]):
        image = vol_image[sid, ...]
        label = vol_label[sid, ...] if vol_label is not None else None

        if vol_label is not None and np.sum(label) == 0:
            continue

        image_file_prefix = f"vol_idx_{vol_idx:0>4d}_slice_{sid:0>3d}"
        image_file = os.path.join(dataset_dir, "images", image_file_prefix)
        image_file += ".npy"

        os.makedirs(os.path.join(dataset_dir, "images"), exist_ok=True)
        np.save(image_file, image)
        image_count += 1

        # Test Data
        if vol_label is None:
            data_list.append(
                {"image": image_file.replace(dataset_dir + os.pathsep, "") if relative_path else image_file}
            )
            continue

        # For all Labels
        unique_labels = np.unique(label.flatten())
        unique_labels = unique_labels[unique_labels != 0]
        unique_labels_count = max(unique_labels_count, len(unique_labels))

        for idx in unique_labels:
            label_file_prefix = f"{image_file_prefix}_region_{int(idx):0>2d}"
            label_file = os.path.join(dataset_dir, "labels", label_file_prefix)
            label_file += ".npy"

            os.makedirs(os.path.join(dataset_dir, "labels"), exist_ok=True)
            curr_label = (label == idx).astype(np.float32)
            np.save(label_file, curr_label)

            label_count += 1
            data_list.append(
                {
                    "image": image_file.replace(dataset_dir + os.pathsep, "") if relative_path else image_file,
                    "label": label_file.replace(dataset_dir + os.pathsep, "") if relative_path else label_file,
                    "region": int(idx),
                }
            )

    if unique_labels_count >= 20:
        logging.warning(f"Unique labels {unique_labels_count} exceeds 20. Please check if this is correct.")

    logging.info(
        "{} => Image Shape: {} => {}; Label Shape: {} => {}; Unique Labels: {}".format(
            vol_idx,
            vol_image.shape,
            image_count,
            vol_label.shape if vol_label is not None else None,
            label_count,
            unique_labels_count,
        )
    )
    return data_list


</source>
<source file="systems/MONAI-0.8.0/monai/apps/deepgrow/dataset.py" startline="211" endline="275" pcid="1402">
def _save_data_3d(vol_idx, vol_image, vol_label, dataset_dir, relative_path):
    data_list = []

    if len(vol_image.shape) == 4:
        logging.info(
            "4D-Image, pick only first series; Image: {}; Label: {}".format(
                vol_image.shape, vol_label.shape if vol_label is not None else None
            )
        )
        vol_image = vol_image[0]
        vol_image = np.moveaxis(vol_image, -1, 0)

    image_count = 0
    label_count = 0
    unique_labels_count = 0

    image_file_prefix = f"vol_idx_{vol_idx:0>4d}"
    image_file = os.path.join(dataset_dir, "images", image_file_prefix)
    image_file += ".npy"

    os.makedirs(os.path.join(dataset_dir, "images"), exist_ok=True)
    np.save(image_file, vol_image)
    image_count += 1

    # Test Data
    if vol_label is None:
        data_list.append({"image": image_file.replace(dataset_dir + os.pathsep, "") if relative_path else image_file})
    else:
        # For all Labels
        unique_labels = np.unique(vol_label.flatten())
        unique_labels = unique_labels[unique_labels != 0]
        unique_labels_count = max(unique_labels_count, len(unique_labels))

        for idx in unique_labels:
            label_file_prefix = f"{image_file_prefix}_region_{int(idx):0>2d}"
            label_file = os.path.join(dataset_dir, "labels", label_file_prefix)
            label_file += ".npy"

            curr_label = (vol_label == idx).astype(np.float32)
            os.makedirs(os.path.join(dataset_dir, "labels"), exist_ok=True)
            np.save(label_file, curr_label)

            label_count += 1
            data_list.append(
                {
                    "image": image_file.replace(dataset_dir + os.pathsep, "") if relative_path else image_file,
                    "label": label_file.replace(dataset_dir + os.pathsep, "") if relative_path else label_file,
                    "region": int(idx),
                }
            )

    if unique_labels_count >= 20:
        logging.warning(f"Unique labels {unique_labels_count} exceeds 20. Please check if this is correct.")

    logging.info(
        "{} => Image Shape: {} => {}; Label Shape: {} => {}; Unique Labels: {}".format(
            vol_idx,
            vol_image.shape,
            image_count,
            vol_label.shape if vol_label is not None else None,
            label_count,
            unique_labels_count,
        )
    )
    return data_list
</source>
</class>

<class classid="60" nclones="2" nlines="12" similarity="83">
<source file="systems/MONAI-0.8.0/monai/apps/pathology/transforms/stain/dictionary.py" startline="45" endline="56" pcid="1406">
    def __init__(
        self,
        keys: KeysCollection,
        tli: float = 240,
        alpha: float = 1,
        beta: float = 0.15,
        max_cref: Union[tuple, np.ndarray] = (1.9705, 1.0308),
        allow_missing_keys: bool = False,
    ) -> None:
        super().__init__(keys, allow_missing_keys)
        self.extractor = ExtractHEStains(tli=tli, alpha=alpha, beta=beta, max_cref=max_cref)

</source>
<source file="systems/MONAI-0.8.0/monai/apps/pathology/transforms/stain/dictionary.py" startline="90" endline="102" pcid="1408">
    def __init__(
        self,
        keys: KeysCollection,
        tli: float = 240,
        alpha: float = 1,
        beta: float = 0.15,
        target_he: Union[tuple, np.ndarray] = ((0.5626, 0.2159), (0.7201, 0.8012), (0.4062, 0.5581)),
        max_cref: Union[tuple, np.ndarray] = (1.9705, 1.0308),
        allow_missing_keys: bool = False,
    ) -> None:
        super().__init__(keys, allow_missing_keys)
        self.normalizer = NormalizeHEStains(tli=tli, alpha=alpha, beta=beta, target_he=target_he, max_cref=max_cref)

</source>
</class>

<class classid="61" nclones="2" nlines="16" similarity="76">
<source file="systems/MONAI-0.8.0/monai/engines/multi_gpu_supervised_trainer.py" startline="50" endline="98" pcid="1428">
def create_multigpu_supervised_trainer(
    net: torch.nn.Module,
    optimizer: Optimizer,
    loss_fn: Callable,
    devices: Optional[Sequence[torch.device]] = None,
    non_blocking: bool = False,
    prepare_batch: Callable = _prepare_batch,
    output_transform: Callable = _default_transform,
    distributed: bool = False,
):
    """
    Derived from `create_supervised_trainer` in Ignite.

    Factory function for creating a trainer for supervised models.

    Args:
        net: the network to train.
        optimizer: the optimizer to use.
        loss_fn: the loss function to use.
        devices: device(s) type specification (default: None).
            Applies to both model and batches. None is all devices used, empty list is CPU only.
        non_blocking: if True and this copy is between CPU and GPU, the copy may occur asynchronously
            with respect to the host. For other cases, this argument has no effect.
        prepare_batch: function that receives `batch`, `device`, `non_blocking` and outputs
            tuple of tensors `(batch_x, batch_y)`.
        output_transform: function that receives 'x', 'y', 'y_pred', 'loss' and returns value
            to be assigned to engine's state.output after each iteration. Default is returning `loss.item()`.
        distributed: whether convert model to `DistributedDataParallel`, if have multiple devices, use
            the first device as output device.

    Returns:
        Engine: a trainer engine with supervised update function.

    Note:
        `engine.state.output` for this engine is defined by `output_transform` parameter and is the loss
        of the processed batch by default.
    """

    devices_ = get_devices_spec(devices)
    if distributed:
        net = DistributedDataParallel(net, device_ids=devices_)
    elif len(devices_) > 1:
        net = DataParallel(net)

    return create_supervised_trainer(
        net, optimizer, loss_fn, devices_[0], non_blocking, prepare_batch, output_transform
    )


</source>
<source file="systems/MONAI-0.8.0/monai/engines/multi_gpu_supervised_trainer.py" startline="99" endline="143" pcid="1429">
def create_multigpu_supervised_evaluator(
    net: torch.nn.Module,
    metrics: Optional[Dict[str, Metric]] = None,
    devices: Optional[Sequence[torch.device]] = None,
    non_blocking: bool = False,
    prepare_batch: Callable = _prepare_batch,
    output_transform: Callable = _default_eval_transform,
    distributed: bool = False,
):
    """
    Derived from `create_supervised_evaluator` in Ignite.

    Factory function for creating an evaluator for supervised models.

    Args:
        net: the model to train.
        metrics: a map of metric names to Metrics.
        devices: device(s) type specification (default: None).
            Applies to both model and batches. None is all devices used, empty list is CPU only.
        non_blocking: if True and this copy is between CPU and GPU, the copy may occur asynchronously
            with respect to the host. For other cases, this argument has no effect.
        prepare_batch: function that receives `batch`, `device`, `non_blocking` and outputs
            tuple of tensors `(batch_x, batch_y)`.
        output_transform: function that receives 'x', 'y', 'y_pred' and returns value
            to be assigned to engine's state.output after each iteration. Default is returning `(y_pred, y,)`
            which fits output expected by metrics. If you change it you should use `output_transform` in metrics.
        distributed: whether convert model to `DistributedDataParallel`, if have multiple devices, use
            the first device as output device.

    Note:
        `engine.state.output` for this engine is defined by `output_transform` parameter and is
        a tuple of `(batch_pred, batch_y)` by default.

    Returns:
        Engine: an evaluator engine with supervised inference function.
    """

    devices_ = get_devices_spec(devices)

    if distributed:
        net = DistributedDataParallel(net, device_ids=devices_)
    elif len(devices_) > 1:
        net = DataParallel(net)

    return create_supervised_evaluator(net, metrics, devices_[0], non_blocking, prepare_batch, output_transform)
</source>
</class>

<class classid="62" nclones="4" nlines="15" similarity="70">
<source file="systems/MONAI-0.8.0/monai/handlers/earlystop_handler.py" startline="52" endline="70" pcid="1459">
    def __init__(
        self,
        patience: int,
        score_function: Callable,
        trainer: Optional[Engine] = None,
        min_delta: float = 0.0,
        cumulative_delta: bool = False,
        epoch_level: bool = True,
    ) -> None:
        self.patience = patience
        self.score_function = score_function
        self.min_delta = min_delta
        self.cumulative_delta = cumulative_delta
        self.epoch_level = epoch_level
        self._handler = None

        if trainer is not None:
            self.set_trainer(trainer=trainer)

</source>
<source file="systems/MONAI-0.8.0/monai/metrics/hausdorff_distance.py" startline="53" endline="69" pcid="1611">
    def __init__(
        self,
        include_background: bool = False,
        distance_metric: str = "euclidean",
        percentile: Optional[float] = None,
        directed: bool = False,
        reduction: Union[MetricReduction, str] = MetricReduction.MEAN,
        get_not_nans: bool = False,
    ) -> None:
        super().__init__()
        self.include_background = include_background
        self.distance_metric = distance_metric
        self.percentile = percentile
        self.directed = directed
        self.reduction = reduction
        self.get_not_nans = get_not_nans

</source>
<source file="systems/MONAI-0.8.0/monai/metrics/surface_distance.py" startline="48" endline="62" pcid="1627">
    def __init__(
        self,
        include_background: bool = False,
        symmetric: bool = False,
        distance_metric: str = "euclidean",
        reduction: Union[MetricReduction, str] = MetricReduction.MEAN,
        get_not_nans: bool = False,
    ) -> None:
        super().__init__()
        self.include_background = include_background
        self.distance_metric = distance_metric
        self.symmetric = symmetric
        self.reduction = reduction
        self.get_not_nans = get_not_nans

</source>
<source file="systems/MONAI-0.8.0/monai/metrics/meandice.py" startline="46" endline="56" pcid="1631">
    def __init__(
        self,
        include_background: bool = True,
        reduction: Union[MetricReduction, str] = MetricReduction.MEAN,
        get_not_nans: bool = False,
    ) -> None:
        super().__init__()
        self.include_background = include_background
        self.reduction = reduction
        self.get_not_nans = get_not_nans

</source>
</class>

<class classid="63" nclones="3" nlines="17" similarity="83">
<source file="systems/MONAI-0.8.0/monai/handlers/hausdorff_distance.py" startline="24" endline="65" pcid="1479">
    def __init__(
        self,
        include_background: bool = False,
        distance_metric: str = "euclidean",
        percentile: Optional[float] = None,
        directed: bool = False,
        reduction: Union[MetricReduction, str] = MetricReduction.MEAN,
        output_transform: Callable = lambda x: x,
        save_details: bool = True,
    ) -> None:
        """

        Args:
            include_background: whether to include distance computation on the first channel of the predicted output.
                Defaults to ``False``.
            distance_metric: : [``"euclidean"``, ``"chessboard"``, ``"taxicab"``]
                the metric used to compute surface distance. Defaults to ``"euclidean"``.
            percentile: an optional float number between 0 and 100. If specified, the corresponding
                percentile of the Hausdorff Distance rather than the maximum result will be achieved.
                Defaults to ``None``.
            directed: whether to calculate directed Hausdorff distance. Defaults to ``False``.
            reduction: {``"none"``, ``"mean"``, ``"sum"``, ``"mean_batch"``, ``"sum_batch"``,
                ``"mean_channel"``, ``"sum_channel"``}
                Define the mode to reduce computation result. Defaults to ``"mean"``.
            output_transform: callable to extract `y_pred` and `y` from `ignite.engine.state.output` then
                construct `(y_pred, y)` pair, where `y_pred` and `y` can be `batch-first` Tensors or
                lists of `channel-first` Tensors. the form of `(y_pred, y)` is required by the `update()`.
                `engine.state` and `output_transform` inherit from the ignite concept:
                https://pytorch.org/ignite/concepts.html#state, explanation and usage example are in the tutorial:
                https://github.com/Project-MONAI/tutorials/blob/master/modules/batch_output_transform.ipynb.
            save_details: whether to save metric computation details per image, for example: hausdorff distance
                of every image. default to True, will save to `engine.state.metric_details` dict with the metric name as key.

        """
        metric_fn = HausdorffDistanceMetric(
            include_background=include_background,
            distance_metric=distance_metric,
            percentile=percentile,
            directed=directed,
            reduction=reduction,
        )
        super().__init__(metric_fn=metric_fn, output_transform=output_transform, save_details=save_details)
</source>
<source file="systems/MONAI-0.8.0/monai/handlers/surface_distance.py" startline="24" endline="61" pcid="1516">
    def __init__(
        self,
        include_background: bool = False,
        symmetric: bool = False,
        distance_metric: str = "euclidean",
        reduction: Union[MetricReduction, str] = MetricReduction.MEAN,
        output_transform: Callable = lambda x: x,
        save_details: bool = True,
    ) -> None:
        """

        Args:
            include_background: whether to include distance computation on the first channel of the predicted output.
                Defaults to ``False``.
            symmetric: whether to calculate the symmetric average surface distance between
                `seg_pred` and `seg_gt`. Defaults to ``False``.
            distance_metric: : [``"euclidean"``, ``"chessboard"``, ``"taxicab"``]
                the metric used to compute surface distance. Defaults to ``"euclidean"``.
            reduction: {``"none"``, ``"mean"``, ``"sum"``, ``"mean_batch"``, ``"sum_batch"``,
                ``"mean_channel"``, ``"sum_channel"``}
                Define the mode to reduce computation result. Defaults to ``"mean"``.
            output_transform: callable to extract `y_pred` and `y` from `ignite.engine.state.output` then
                construct `(y_pred, y)` pair, where `y_pred` and `y` can be `batch-first` Tensors or
                lists of `channel-first` Tensors. the form of `(y_pred, y)` is required by the `update()`.
                `engine.state` and `output_transform` inherit from the ignite concept:
                https://pytorch.org/ignite/concepts.html#state, explanation and usage example are in the tutorial:
                https://github.com/Project-MONAI/tutorials/blob/master/modules/batch_output_transform.ipynb.
            save_details: whether to save metric computation details per image, for example: surface dice
                of every image. default to True, will save to `engine.state.metric_details` dict with the metric name as key.

        """
        metric_fn = SurfaceDistanceMetric(
            include_background=include_background,
            symmetric=symmetric,
            distance_metric=distance_metric,
            reduction=reduction,
        )
        super().__init__(metric_fn=metric_fn, output_transform=output_transform, save_details=save_details)
</source>
<source file="systems/MONAI-0.8.0/monai/handlers/confusion_matrix.py" startline="24" endline="68" pcid="1529">
    def __init__(
        self,
        include_background: bool = True,
        metric_name: str = "hit_rate",
        compute_sample: bool = False,
        reduction: Union[MetricReduction, str] = MetricReduction.MEAN,
        output_transform: Callable = lambda x: x,
        save_details: bool = True,
    ) -> None:
        """

        Args:
            include_background: whether to skip metric computation on the first channel of
                the predicted output. Defaults to True.
            metric_name: [``"sensitivity"``, ``"specificity"``, ``"precision"``, ``"negative predictive value"``,
                ``"miss rate"``, ``"fall out"``, ``"false discovery rate"``, ``"false omission rate"``,
                ``"prevalence threshold"``, ``"threat score"``, ``"accuracy"``, ``"balanced accuracy"``,
                ``"f1 score"``, ``"matthews correlation coefficient"``, ``"fowlkes mallows index"``,
                ``"informedness"``, ``"markedness"``]
                Some of the metrics have multiple aliases (as shown in the wikipedia page aforementioned),
                and you can also input those names instead.
            compute_sample: when reducing, if ``True``, each sample's metric will be computed based on each confusion matrix first.
                if ``False``, compute reduction on the confusion matrices first, defaults to ``False``.
            reduction: {``"none"``, ``"mean"``, ``"sum"``, ``"mean_batch"``, ``"sum_batch"``,
                ``"mean_channel"``, ``"sum_channel"``}
            output_transform: callable to extract `y_pred` and `y` from `ignite.engine.state.output` then
                construct `(y_pred, y)` pair, where `y_pred` and `y` can be `batch-first` Tensors or
                lists of `channel-first` Tensors. the form of `(y_pred, y)` is required by the `update()`.
                `engine.state` and `output_transform` inherit from the ignite concept:
                https://pytorch.org/ignite/concepts.html#state, explanation and usage example are in the tutorial:
                https://github.com/Project-MONAI/tutorials/blob/master/modules/batch_output_transform.ipynb.
            save_details: whether to save metric computation details per image, for example: TP/TN/FP/FN of every image.
                default to True, will save to `engine.state.metric_details` dict with the metric name as key.

        See also:
            :py:meth:`monai.metrics.confusion_matrix`
        """
        metric_fn = ConfusionMatrixMetric(
            include_background=include_background,
            metric_name=metric_name,
            compute_sample=compute_sample,
            reduction=reduction,
        )
        self.metric_name = metric_name
        super().__init__(metric_fn=metric_fn, output_transform=output_transform, save_details=save_details)
</source>
</class>

<class classid="64" nclones="3" nlines="11" similarity="75">
<source file="systems/MONAI-0.8.0/monai/handlers/checkpoint_saver.py" startline="251" endline="268" pcid="1497">
    def completed(self, engine: Engine) -> None:
        """Callback for train or validation/evaluation completed Event.
        Save final checkpoint if configure save_final is True.

        Args:
            engine: Ignite Engine, it can be a trainer, validator or evaluator.
        """
        if not callable(self._final_checkpoint):
            raise AssertionError("Error: _final_checkpoint function not specified.")
        # delete previous saved final checkpoint if existing
        self._delete_previous_final_ckpt()
        self._final_checkpoint(engine)
        if self.logger is None:
            raise AssertionError
        if not hasattr(self.logger, "info"):
            raise AssertionError("Error, provided logger has not info attribute.")
        self.logger.info(f"Train completed, saved final checkpoint: {self._final_checkpoint.last_checkpoint}")

</source>
<source file="systems/MONAI-0.8.0/monai/handlers/checkpoint_saver.py" startline="269" endline="289" pcid="1498">
    def exception_raised(self, engine: Engine, e: Exception) -> None:
        """Callback for train or validation/evaluation exception raised Event.
        Save current data as final checkpoint if configure save_final is True. This callback may be skipped
        because the logic with Ignite can only trigger the first attached handler for `EXCEPTION_RAISED` event.

        Args:
            engine: Ignite Engine, it can be a trainer, validator or evaluator.
            e: the exception caught in Ignite during engine.run().
        """
        if not callable(self._final_checkpoint):
            raise AssertionError("Error: _final_checkpoint function not specified.")
        # delete previous saved final checkpoint if existing
        self._delete_previous_final_ckpt()
        self._final_checkpoint(engine)
        if self.logger is None:
            raise AssertionError
        if not hasattr(self.logger, "info"):
            raise AssertionError("Error, provided logger has not info attribute.")
        self.logger.info(f"Exception raised, saved the last checkpoint: {self._final_checkpoint.last_checkpoint}")
        raise e

</source>
<source file="systems/MONAI-0.8.0/monai/handlers/checkpoint_saver.py" startline="300" endline="317" pcid="1500">
    def interval_completed(self, engine: Engine) -> None:
        """Callback for train epoch/iteration completed Event.
        Save checkpoint if configure save_interval = N

        Args:
            engine: Ignite Engine, it can be a trainer, validator or evaluator.
        """
        if not callable(self._interval_checkpoint):
            raise AssertionError("Error: _interval_checkpoint function not specified.")
        self._interval_checkpoint(engine)
        if self.logger is None:
            raise AssertionError
        if not hasattr(self.logger, "info"):
            raise AssertionError("Error, provided logger has not info attribute.")
        if self.epoch_level:
            self.logger.info(f"Saved checkpoint at epoch: {engine.state.epoch}")
        else:
            self.logger.info(f"Saved checkpoint at iteration: {engine.state.iteration}")
</source>
</class>

<class classid="65" nclones="2" nlines="29" similarity="83">
<source file="systems/MONAI-0.8.0/monai/_version.py" startline="70" endline="104" pcid="1570">
def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False,
                env=None):
    """Call the given command(s)."""
    assert isinstance(commands, list)
    p = None
    for c in commands:
        try:
            dispcmd = str([c] + args)
            # remember shell=False, so use git.cmd on windows, not just git
            p = subprocess.Popen([c] + args, cwd=cwd, env=env,
                                 stdout=subprocess.PIPE,
                                 stderr=(subprocess.PIPE if hide_stderr
                                         else None))
            break
        except EnvironmentError:
            e = sys.exc_info()[1]
            if e.errno == errno.ENOENT:
                continue
            if verbose:
                print("unable to run %s" % dispcmd)
                print(e)
            return None, None
    else:
        if verbose:
            print("unable to find command, tried %s" % (commands,))
        return None, None
    stdout = p.communicate()[0].strip().decode()
    if p.returncode != 0:
        if verbose:
            print("unable to run %s (error)" % dispcmd)
            print("stdout was %s" % stdout)
        return None, p.returncode
    return stdout, p.returncode


</source>
<source file="systems/MONAI-0.8.0/versioneer.py" startline="380" endline="412" pcid="1753">
def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):
    """Call the given command(s)."""
    assert isinstance(commands, list)
    p = None
    for c in commands:
        try:
            dispcmd = str([c] + args)
            # remember shell=False, so use git.cmd on windows, not just git
            p = subprocess.Popen(
                [c] + args, cwd=cwd, env=env, stdout=subprocess.PIPE, stderr=(subprocess.PIPE if hide_stderr else None)
            )
            break
        except EnvironmentError:
            e = sys.exc_info()[1]
            if e.errno == errno.ENOENT:
                continue
            if verbose:
                print("unable to run %s" % dispcmd)
                print(e)
            return None, None
    else:
        if verbose:
            print("unable to find command, tried %s" % (commands,))
        return None, None
    stdout = p.communicate()[0].strip().decode()
    if p.returncode != 0:
        if verbose:
            print("unable to run %s (error)" % dispcmd)
            print("stdout was %s" % stdout)
        return None, p.returncode
    return stdout, p.returncode


</source>
</class>

<class classid="66" nclones="2" nlines="21" similarity="100">
<source file="systems/MONAI-0.8.0/monai/_version.py" startline="131" endline="158" pcid="1572">
def git_get_keywords(versionfile_abs):
    """Extract version information from the given file."""
    # the code embedded in _version.py can just fetch the value of these
    # keywords. When used from setup.py, we don't want to import _version.py,
    # so we do it with a regexp instead. This function is not used from
    # _version.py.
    keywords = {}
    try:
        f = open(versionfile_abs, "r")
        for line in f.readlines():
            if line.strip().startswith("git_refnames ="):
                mo = re.search(r'=\s*"(.*)"', line)
                if mo:
                    keywords["refnames"] = mo.group(1)
            if line.strip().startswith("git_full ="):
                mo = re.search(r'=\s*"(.*)"', line)
                if mo:
                    keywords["full"] = mo.group(1)
            if line.strip().startswith("git_date ="):
                mo = re.search(r'=\s*"(.*)"', line)
                if mo:
                    keywords["date"] = mo.group(1)
        f.close()
    except EnvironmentError:
        pass
    return keywords


</source>
<source file="systems/MONAI-0.8.0/versioneer.py" startline="944" endline="971" pcid="1754">
def git_get_keywords(versionfile_abs):
    """Extract version information from the given file."""
    # the code embedded in _version.py can just fetch the value of these
    # keywords. When used from setup.py, we don't want to import _version.py,
    # so we do it with a regexp instead. This function is not used from
    # _version.py.
    keywords = {}
    try:
        f = open(versionfile_abs, "r")
        for line in f.readlines():
            if line.strip().startswith("git_refnames ="):
                mo = re.search(r'=\s*"(.*)"', line)
                if mo:
                    keywords["refnames"] = mo.group(1)
            if line.strip().startswith("git_full ="):
                mo = re.search(r'=\s*"(.*)"', line)
                if mo:
                    keywords["full"] = mo.group(1)
            if line.strip().startswith("git_date ="):
                mo = re.search(r'=\s*"(.*)"', line)
                if mo:
                    keywords["date"] = mo.group(1)
        f.close()
    except EnvironmentError:
        pass
    return keywords


</source>
</class>

<class classid="67" nclones="2" nlines="39" similarity="75">
<source file="systems/MONAI-0.8.0/monai/_version.py" startline="160" endline="217" pcid="1573">
def git_versions_from_keywords(keywords, tag_prefix, verbose):
    """Get version information from git keywords."""
    if not keywords:
        raise NotThisMethod("no keywords at all, weird")
    date = keywords.get("date")
    if date is not None:
        # Use only the last line.  Previous lines may contain GPG signature
        # information.
        date = date.splitlines()[-1]

        # git-2.2.0 added "%cI", which expands to an ISO-8601 -compliant
        # datestamp. However we prefer "%ci" (which expands to an "ISO-8601
        # -like" string, which we must then edit to make compliant), because
        # it's been around since git-1.5.3, and it's too difficult to
        # discover which version we're using, or to work around using an
        # older one.
        date = date.strip().replace(" ", "T", 1).replace(" ", "", 1)
    refnames = keywords["refnames"].strip()
    if refnames.startswith("$Format"):
        if verbose:
            print("keywords are unexpanded, not using")
        raise NotThisMethod("unexpanded keywords, not a git-archive tarball")
    refs = set([r.strip() for r in refnames.strip("()").split(",")])
    # starting in git-1.8.3, tags are listed as "tag: foo-1.0" instead of
    # just "foo-1.0". If we see a "tag: " prefix, prefer those.
    TAG = "tag: "
    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])
    if not tags:
        # Either we're using git < 1.8.3, or there really are no tags. We use
        # a heuristic: assume all version tags have a digit. The old git %d
        # expansion behaves like git log --decorate=short and strips out the
        # refs/heads/ and refs/tags/ prefixes that would let us distinguish
        # between branches and tags. By ignoring refnames without digits, we
        # filter out many common branch names like "release" and
        # "stabilization", as well as "HEAD" and "master".
        tags = set([r for r in refs if re.search(r'\d', r)])
        if verbose:
            print("discarding '%s', no digits" % ",".join(refs - tags))
    if verbose:
        print("likely tags: %s" % ",".join(sorted(tags)))
    for ref in sorted(tags):
        # sorting will prefer e.g. "2.0" over "2.0rc1"
        if ref.startswith(tag_prefix):
            r = ref[len(tag_prefix):]
            if verbose:
                print("picking %s" % r)
            return {"version": r,
                    "full-revisionid": keywords["full"].strip(),
                    "dirty": False, "error": None,
                    "date": date}
    # no suitable tags, so version is "0+unknown", but full hex is still there
    if verbose:
        print("no suitable tags, using unknown + full revision id")
    return {"version": "0+unknown",
            "full-revisionid": keywords["full"].strip(),
            "dirty": False, "error": "no suitable tags", "date": None}


</source>
<source file="systems/MONAI-0.8.0/versioneer.py" startline="973" endline="1037" pcid="1755">
def git_versions_from_keywords(keywords, tag_prefix, verbose):
    """Get version information from git keywords."""
    if not keywords:
        raise NotThisMethod("no keywords at all, weird")
    date = keywords.get("date")
    if date is not None:
        # Use only the last line.  Previous lines may contain GPG signature
        # information.
        date = date.splitlines()[-1]

        # git-2.2.0 added "%cI", which expands to an ISO-8601 -compliant
        # datestamp. However we prefer "%ci" (which expands to an "ISO-8601
        # -like" string, which we must then edit to make compliant), because
        # it's been around since git-1.5.3, and it's too difficult to
        # discover which version we're using, or to work around using an
        # older one.
        date = date.strip().replace(" ", "T", 1).replace(" ", "", 1)
    refnames = keywords["refnames"].strip()
    if refnames.startswith("$Format"):
        if verbose:
            print("keywords are unexpanded, not using")
        raise NotThisMethod("unexpanded keywords, not a git-archive tarball")
    refs = set([r.strip() for r in refnames.strip("()").split(",")])
    # starting in git-1.8.3, tags are listed as "tag: foo-1.0" instead of
    # just "foo-1.0". If we see a "tag: " prefix, prefer those.
    TAG = "tag: "
    tags = set([r[len(TAG) :] for r in refs if r.startswith(TAG)])
    if not tags:
        # Either we're using git < 1.8.3, or there really are no tags. We use
        # a heuristic: assume all version tags have a digit. The old git %d
        # expansion behaves like git log --decorate=short and strips out the
        # refs/heads/ and refs/tags/ prefixes that would let us distinguish
        # between branches and tags. By ignoring refnames without digits, we
        # filter out many common branch names like "release" and
        # "stabilization", as well as "HEAD" and "master".
        tags = set([r for r in refs if re.search(r"\d", r)])
        if verbose:
            print("discarding '%s', no digits" % ",".join(refs - tags))
    if verbose:
        print("likely tags: %s" % ",".join(sorted(tags)))
    for ref in sorted(tags):
        # sorting will prefer e.g. "2.0" over "2.0rc1"
        if ref.startswith(tag_prefix):
            r = ref[len(tag_prefix) :]
            if verbose:
                print("picking %s" % r)
            return {
                "version": r,
                "full-revisionid": keywords["full"].strip(),
                "dirty": False,
                "error": None,
                "date": date,
            }
    # no suitable tags, so version is "0+unknown", but full hex is still there
    if verbose:
        print("no suitable tags, using unknown + full revision id")
    return {
        "version": "0+unknown",
        "full-revisionid": keywords["full"].strip(),
        "dirty": False,
        "error": "no suitable tags",
        "date": None,
    }


</source>
</class>

<class classid="68" nclones="2" nlines="54" similarity="75">
<source file="systems/MONAI-0.8.0/monai/_version.py" startline="219" endline="312" pcid="1574">
def git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):
    """Get version from 'git describe' in the root of the source tree.

    This only gets called if the git-archive 'subst' keywords were *not*
    expanded, and _version.py hasn't already been rewritten with a short
    version string, meaning we're inside a checked out source tree.
    """
    GITS = ["git"]
    if sys.platform == "win32":
        GITS = ["git.cmd", "git.exe"]

    out, rc = run_command(GITS, ["rev-parse", "--git-dir"], cwd=root,
                          hide_stderr=True)
    if rc != 0:
        if verbose:
            print("Directory %s not under git control" % root)
        raise NotThisMethod("'git rev-parse --git-dir' returned error")

    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]
    # if there isn't one, this yields HEX[-dirty] (no NUM)
    describe_out, rc = run_command(GITS, ["describe", "--tags", "--dirty",
                                          "--always", "--long",
                                          "--match", "%s*" % tag_prefix],
                                   cwd=root)
    # --long was added in git-1.5.5
    if describe_out is None:
        raise NotThisMethod("'git describe' failed")
    describe_out = describe_out.strip()
    full_out, rc = run_command(GITS, ["rev-parse", "HEAD"], cwd=root)
    if full_out is None:
        raise NotThisMethod("'git rev-parse' failed")
    full_out = full_out.strip()

    pieces = {}
    pieces["long"] = full_out
    pieces["short"] = full_out[:7]  # maybe improved later
    pieces["error"] = None

    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]
    # TAG might have hyphens.
    git_describe = describe_out

    # look for -dirty suffix
    dirty = git_describe.endswith("-dirty")
    pieces["dirty"] = dirty
    if dirty:
        git_describe = git_describe[:git_describe.rindex("-dirty")]

    # now we have TAG-NUM-gHEX or HEX

    if "-" in git_describe:
        # TAG-NUM-gHEX
        mo = re.search(r'^(.+)-(\d+)-g([0-9a-f]+)$', git_describe)
        if not mo:
            # unparseable. Maybe git-describe is misbehaving?
            pieces["error"] = ("unable to parse git-describe output: '%s'"
                               % describe_out)
            return pieces

        # tag
        full_tag = mo.group(1)
        if not full_tag.startswith(tag_prefix):
            if verbose:
                fmt = "tag '%s' doesn't start with prefix '%s'"
                print(fmt % (full_tag, tag_prefix))
            pieces["error"] = ("tag '%s' doesn't start with prefix '%s'"
                               % (full_tag, tag_prefix))
            return pieces
        pieces["closest-tag"] = full_tag[len(tag_prefix):]

        # distance: number of commits since tag
        pieces["distance"] = int(mo.group(2))

        # commit: short hex revision ID
        pieces["short"] = mo.group(3)

    else:
        # HEX: no tags
        pieces["closest-tag"] = None
        count_out, rc = run_command(GITS, ["rev-list", "HEAD", "--count"],
                                    cwd=root)
        pieces["distance"] = int(count_out)  # total number of commits

    # commit date: see ISO-8601 comment in git_versions_from_keywords()
    date = run_command(GITS, ["show", "-s", "--format=%ci", "HEAD"],
                       cwd=root)[0].strip()
    # Use only the last line.  Previous lines may contain GPG signature
    # information.
    date = date.splitlines()[-1]
    pieces["date"] = date.strip().replace(" ", "T", 1).replace(" ", "", 1)

    return pieces


</source>
<source file="systems/MONAI-0.8.0/versioneer.py" startline="1039" endline="1126" pcid="1756">
def git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):
    """Get version from 'git describe' in the root of the source tree.

    This only gets called if the git-archive 'subst' keywords were *not*
    expanded, and _version.py hasn't already been rewritten with a short
    version string, meaning we're inside a checked out source tree.
    """
    GITS = ["git"]
    if sys.platform == "win32":
        GITS = ["git.cmd", "git.exe"]

    out, rc = run_command(GITS, ["rev-parse", "--git-dir"], cwd=root, hide_stderr=True)
    if rc != 0:
        if verbose:
            print("Directory %s not under git control" % root)
        raise NotThisMethod("'git rev-parse --git-dir' returned error")

    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]
    # if there isn't one, this yields HEX[-dirty] (no NUM)
    describe_out, rc = run_command(
        GITS, ["describe", "--tags", "--dirty", "--always", "--long", "--match", "%s*" % tag_prefix], cwd=root
    )
    # --long was added in git-1.5.5
    if describe_out is None:
        raise NotThisMethod("'git describe' failed")
    describe_out = describe_out.strip()
    full_out, rc = run_command(GITS, ["rev-parse", "HEAD"], cwd=root)
    if full_out is None:
        raise NotThisMethod("'git rev-parse' failed")
    full_out = full_out.strip()

    pieces = {}
    pieces["long"] = full_out
    pieces["short"] = full_out[:7]  # maybe improved later
    pieces["error"] = None

    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]
    # TAG might have hyphens.
    git_describe = describe_out

    # look for -dirty suffix
    dirty = git_describe.endswith("-dirty")
    pieces["dirty"] = dirty
    if dirty:
        git_describe = git_describe[: git_describe.rindex("-dirty")]

    # now we have TAG-NUM-gHEX or HEX

    if "-" in git_describe:
        # TAG-NUM-gHEX
        mo = re.search(r"^(.+)-(\d+)-g([0-9a-f]+)$", git_describe)
        if not mo:
            # unparseable. Maybe git-describe is misbehaving?
            pieces["error"] = "unable to parse git-describe output: '%s'" % describe_out
            return pieces

        # tag
        full_tag = mo.group(1)
        if not full_tag.startswith(tag_prefix):
            if verbose:
                fmt = "tag '%s' doesn't start with prefix '%s'"
                print(fmt % (full_tag, tag_prefix))
            pieces["error"] = "tag '%s' doesn't start with prefix '%s'" % (full_tag, tag_prefix)
            return pieces
        pieces["closest-tag"] = full_tag[len(tag_prefix) :]

        # distance: number of commits since tag
        pieces["distance"] = int(mo.group(2))

        # commit: short hex revision ID
        pieces["short"] = mo.group(3)

    else:
        # HEX: no tags
        pieces["closest-tag"] = None
        count_out, rc = run_command(GITS, ["rev-list", "HEAD", "--count"], cwd=root)
        pieces["distance"] = int(count_out)  # total number of commits

    # commit date: see ISO-8601 comment in git_versions_from_keywords()
    date = run_command(GITS, ["show", "-s", "--format=%ci", "HEAD"], cwd=root)[0].strip()
    # Use only the last line.  Previous lines may contain GPG signature
    # information.
    date = date.splitlines()[-1]
    pieces["date"] = date.strip().replace(" ", "T", 1).replace(" ", "", 1)

    return pieces


</source>
</class>

<class classid="69" nclones="6" nlines="13" similarity="71">
<source file="systems/MONAI-0.8.0/monai/_version.py" startline="320" endline="344" pcid="1576">
def render_pep440(pieces):
    """Build up version string, with post-release "local version identifier".

    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you
    get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty

    Exceptions:
    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += plus_or_dot(pieces)
            rendered += "%d.g%s" % (pieces["distance"], pieces["short"])
            if pieces["dirty"]:
                rendered += ".dirty"
    else:
        # exception #1
        rendered = "0+untagged.%d.g%s" % (pieces["distance"],
                                          pieces["short"])
        if pieces["dirty"]:
            rendered += ".dirty"
    return rendered


</source>
<source file="systems/MONAI-0.8.0/monai/_version.py" startline="388" endline="409" pcid="1579">
def render_pep440_old(pieces):
    """TAG[.postDISTANCE[.dev0]] .

    The ".dev0" means dirty.

    Exceptions:
    1: no tags. 0.postDISTANCE[.dev0]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += ".post%d" % pieces["distance"]
            if pieces["dirty"]:
                rendered += ".dev0"
    else:
        # exception #1
        rendered = "0.post%d" % pieces["distance"]
        if pieces["dirty"]:
            rendered += ".dev0"
    return rendered


</source>
<source file="systems/MONAI-0.8.0/versioneer.py" startline="1243" endline="1266" pcid="1762">
def render_pep440(pieces):
    """Build up version string, with post-release "local version identifier".

    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you
    get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty

    Exceptions:
    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += plus_or_dot(pieces)
            rendered += "%d.g%s" % (pieces["distance"], pieces["short"])
            if pieces["dirty"]:
                rendered += ".dirty"
    else:
        # exception #1
        rendered = "0+untagged.%d.g%s" % (pieces["distance"], pieces["short"])
        if pieces["dirty"]:
            rendered += ".dirty"
    return rendered


</source>
<source file="systems/MONAI-0.8.0/versioneer.py" startline="1310" endline="1331" pcid="1765">
def render_pep440_old(pieces):
    """TAG[.postDISTANCE[.dev0]] .

    The ".dev0" means dirty.

    Exceptions:
    1: no tags. 0.postDISTANCE[.dev0]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += ".post%d" % pieces["distance"]
            if pieces["dirty"]:
                rendered += ".dev0"
    else:
        # exception #1
        rendered = "0.post%d" % pieces["distance"]
        if pieces["dirty"]:
            rendered += ".dev0"
    return rendered


</source>
<source file="systems/MONAI-0.8.0/versioneer.py" startline="1283" endline="1309" pcid="1764">
def render_pep440_post(pieces):
    """TAG[.postDISTANCE[.dev0]+gHEX] .

    The ".dev0" means dirty. Note that .dev0 sorts backwards
    (a dirty tree will appear "older" than the corresponding clean one),
    but you shouldn't be releasing software with -dirty anyways.

    Exceptions:
    1: no tags. 0.postDISTANCE[.dev0]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += ".post%d" % pieces["distance"]
            if pieces["dirty"]:
                rendered += ".dev0"
            rendered += plus_or_dot(pieces)
            rendered += "g%s" % pieces["short"]
    else:
        # exception #1
        rendered = "0.post%d" % pieces["distance"]
        if pieces["dirty"]:
            rendered += ".dev0"
        rendered += "+g%s" % pieces["short"]
    return rendered


</source>
<source file="systems/MONAI-0.8.0/monai/_version.py" startline="361" endline="387" pcid="1578">
def render_pep440_post(pieces):
    """TAG[.postDISTANCE[.dev0]+gHEX] .

    The ".dev0" means dirty. Note that .dev0 sorts backwards
    (a dirty tree will appear "older" than the corresponding clean one),
    but you shouldn't be releasing software with -dirty anyways.

    Exceptions:
    1: no tags. 0.postDISTANCE[.dev0]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += ".post%d" % pieces["distance"]
            if pieces["dirty"]:
                rendered += ".dev0"
            rendered += plus_or_dot(pieces)
            rendered += "g%s" % pieces["short"]
    else:
        # exception #1
        rendered = "0.post%d" % pieces["distance"]
        if pieces["dirty"]:
            rendered += ".dev0"
        rendered += "+g%s" % pieces["short"]
    return rendered


</source>
</class>

<class classid="70" nclones="2" nlines="10" similarity="100">
<source file="systems/MONAI-0.8.0/monai/_version.py" startline="410" endline="429" pcid="1580">
def render_git_describe(pieces):
    """TAG[-DISTANCE-gHEX][-dirty].

    Like 'git describe --tags --dirty --always'.

    Exceptions:
    1: no tags. HEX[-dirty]  (note: no 'g' prefix)
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"]:
            rendered += "-%d-g%s" % (pieces["distance"], pieces["short"])
    else:
        # exception #1
        rendered = pieces["short"]
    if pieces["dirty"]:
        rendered += "-dirty"
    return rendered


</source>
<source file="systems/MONAI-0.8.0/versioneer.py" startline="1332" endline="1351" pcid="1766">
def render_git_describe(pieces):
    """TAG[-DISTANCE-gHEX][-dirty].

    Like 'git describe --tags --dirty --always'.

    Exceptions:
    1: no tags. HEX[-dirty]  (note: no 'g' prefix)
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"]:
            rendered += "-%d-g%s" % (pieces["distance"], pieces["short"])
    else:
        # exception #1
        rendered = pieces["short"]
    if pieces["dirty"]:
        rendered += "-dirty"
    return rendered


</source>
</class>

<class classid="71" nclones="2" nlines="29" similarity="70">
<source file="systems/MONAI-0.8.0/monai/_version.py" startline="450" endline="481" pcid="1582">
def render(pieces, style):
    """Render the given version pieces into the requested style."""
    if pieces["error"]:
        return {"version": "unknown",
                "full-revisionid": pieces.get("long"),
                "dirty": None,
                "error": pieces["error"],
                "date": None}

    if not style or style == "default":
        style = "pep440"  # the default

    if style == "pep440":
        rendered = render_pep440(pieces)
    elif style == "pep440-pre":
        rendered = render_pep440_pre(pieces)
    elif style == "pep440-post":
        rendered = render_pep440_post(pieces)
    elif style == "pep440-old":
        rendered = render_pep440_old(pieces)
    elif style == "git-describe":
        rendered = render_git_describe(pieces)
    elif style == "git-describe-long":
        rendered = render_git_describe_long(pieces)
    else:
        raise ValueError("unknown style '%s'" % style)

    return {"version": rendered, "full-revisionid": pieces["long"],
            "dirty": pieces["dirty"], "error": None,
            "date": pieces.get("date")}


</source>
<source file="systems/MONAI-0.8.0/versioneer.py" startline="1372" endline="1409" pcid="1768">
def render(pieces, style):
    """Render the given version pieces into the requested style."""
    if pieces["error"]:
        return {
            "version": "unknown",
            "full-revisionid": pieces.get("long"),
            "dirty": None,
            "error": pieces["error"],
            "date": None,
        }

    if not style or style == "default":
        style = "pep440"  # the default

    if style == "pep440":
        rendered = render_pep440(pieces)
    elif style == "pep440-pre":
        rendered = render_pep440_pre(pieces)
    elif style == "pep440-post":
        rendered = render_pep440_post(pieces)
    elif style == "pep440-old":
        rendered = render_pep440_old(pieces)
    elif style == "git-describe":
        rendered = render_git_describe(pieces)
    elif style == "git-describe-long":
        rendered = render_git_describe_long(pieces)
    else:
        raise ValueError("unknown style '%s'" % style)

    return {
        "version": rendered,
        "full-revisionid": pieces["long"],
        "dirty": pieces["dirty"],
        "error": None,
        "date": pieces.get("date"),
    }


</source>
</class>

<class classid="72" nclones="2" nlines="24" similarity="100">
<source file="systems/MONAI-0.8.0/monai/transforms/smooth_field/dictionary.py" startline="45" endline="70" pcid="1591">
    def __init__(
        self,
        keys: KeysCollection,
        spatial_size: Union[Sequence[int], int],
        rand_size: Union[Sequence[int], int],
        padder: Optional[Transform] = None,
        mode: Union[InterpolateMode, str] = InterpolateMode.AREA,
        align_corners: Optional[bool] = None,
        prob: float = 0.1,
        gamma: Union[Sequence[float], float] = (0.5, 4.5),
        apply_same_field: bool = True,
    ):
        RandomizableTransform.__init__(self, prob)
        MapTransform.__init__(self, keys)

        self.trans = RandSmoothFieldAdjustContrast(
            spatial_size=spatial_size,
            rand_size=rand_size,
            padder=padder,
            mode=mode,
            align_corners=align_corners,
            prob=1.0,
            gamma=gamma,
        )
        self.apply_same_field = apply_same_field

</source>
<source file="systems/MONAI-0.8.0/monai/transforms/smooth_field/dictionary.py" startline="120" endline="145" pcid="1595">
    def __init__(
        self,
        keys: KeysCollection,
        spatial_size: Union[Sequence[int], int],
        rand_size: Union[Sequence[int], int],
        padder: Optional[Transform] = None,
        mode: Union[InterpolateMode, str] = InterpolateMode.AREA,
        align_corners: Optional[bool] = None,
        prob: float = 0.1,
        gamma: Union[Sequence[float], float] = (0.1, 1.0),
        apply_same_field: bool = True,
    ):
        RandomizableTransform.__init__(self, prob)
        MapTransform.__init__(self, keys)

        self.trans = RandSmoothFieldAdjustIntensity(
            spatial_size=spatial_size,
            rand_size=rand_size,
            padder=padder,
            mode=mode,
            align_corners=align_corners,
            prob=1.0,
            gamma=gamma,
        )
        self.apply_same_field = apply_same_field

</source>
</class>

<class classid="73" nclones="2" nlines="10" similarity="100">
<source file="systems/MONAI-0.8.0/monai/transforms/smooth_field/dictionary.py" startline="84" endline="100" pcid="1594">
    def __call__(self, data: Mapping[Hashable, np.ndarray]) -> Mapping[Hashable, np.ndarray]:
        self.randomize()

        if not self._do_transform:
            return data

        d = dict(data)

        for key in self.key_iterator(d):
            if not self.apply_same_field:
                self.randomize()  # new field for every key

            d[key] = self.trans(d[key], False)

        return d


</source>
<source file="systems/MONAI-0.8.0/monai/transforms/smooth_field/dictionary.py" startline="157" endline="171" pcid="1598">
    def __call__(self, data: Mapping[Hashable, np.ndarray]) -> Mapping[Hashable, np.ndarray]:
        self.randomize()

        if not self._do_transform:
            return data

        d = dict(data)

        for key in self.key_iterator(d):
            if not self.apply_same_field:
                self.randomize()  # new field for every key

            d[key] = self.trans(d[key], False)

        return d
</source>
</class>

<class classid="74" nclones="2" nlines="18" similarity="94">
<source file="systems/MONAI-0.8.0/monai/metrics/hausdorff_distance.py" startline="70" endline="101" pcid="1612">
    def _compute_tensor(self, y_pred: torch.Tensor, y: torch.Tensor):  # type: ignore
        """
        Args:
            y_pred: input data to compute, typical segmentation model output.
                It must be one-hot format and first dim is batch, example shape: [16, 3, 32, 32]. The values
                should be binarized.
            y: ground truth to compute the distance. It must be one-hot format and first dim is batch.
                The values should be binarized.

        Raises:
            ValueError: when `y` is not a binarized tensor.
            ValueError: when `y_pred` has less than three dimensions.
        """
        if not isinstance(y_pred, torch.Tensor) or not isinstance(y, torch.Tensor):
            raise ValueError("y_pred and y must be PyTorch Tensor.")
        if not torch.all(y_pred.byte() == y_pred):
            warnings.warn("y_pred should be a binarized tensor.")
        if not torch.all(y.byte() == y):
            raise ValueError("y should be a binarized tensor.")
        dims = y_pred.ndimension()
        if dims < 3:
            raise ValueError("y_pred should have at least three dimensions.")
        # compute (BxC) for each channel for each batch
        return compute_hausdorff_distance(
            y_pred=y_pred,
            y=y,
            include_background=self.include_background,
            distance_metric=self.distance_metric,
            percentile=self.percentile,
            directed=self.directed,
        )

</source>
<source file="systems/MONAI-0.8.0/monai/metrics/surface_distance.py" startline="63" endline="93" pcid="1628">
    def _compute_tensor(self, y_pred: torch.Tensor, y: torch.Tensor):  # type: ignore
        """
        Args:
            y_pred: input data to compute, typical segmentation model output.
                It must be one-hot format and first dim is batch, example shape: [16, 3, 32, 32]. The values
                should be binarized.
            y: ground truth to compute the distance. It must be one-hot format and first dim is batch.
                The values should be binarized.

        Raises:
            ValueError: when `y` is not a binarized tensor.
            ValueError: when `y_pred` has less than three dimensions.
        """
        if not isinstance(y_pred, torch.Tensor) or not isinstance(y, torch.Tensor):
            raise ValueError("y_pred and y must be PyTorch Tensor.")
        if not torch.all(y_pred.byte() == y_pred):
            warnings.warn("y_pred should be a binarized tensor.")
        if not torch.all(y.byte() == y):
            raise ValueError("y should be a binarized tensor.")
        dims = y_pred.ndimension()
        if dims < 3:
            raise ValueError("y_pred should have at least three dimensions.")
        # compute (BxC) for each channel for each batch
        return compute_average_surface_distance(
            y_pred=y_pred,
            y=y,
            include_background=self.include_background,
            symmetric=self.symmetric,
            distance_metric=self.distance_metric,
        )

</source>
</class>

<class classid="75" nclones="2" nlines="19" similarity="84">
<source file="systems/MONAI-0.8.0/monai/networks/nets/basic_unet.py" startline="28" endline="63" pcid="1651">
    def __init__(
        self,
        spatial_dims: int,
        in_chns: int,
        out_chns: int,
        act: Union[str, tuple],
        norm: Union[str, tuple],
        bias: bool,
        dropout: Union[float, tuple] = 0.0,
        dim: Optional[int] = None,
    ):
        """
        Args:
            spatial_dims: number of spatial dimensions.
            in_chns: number of input channels.
            out_chns: number of output channels.
            act: activation type and arguments.
            norm: feature normalization type and arguments.
            bias: whether to have a bias term in convolution blocks.
            dropout: dropout ratio. Defaults to no dropout.

        .. deprecated:: 0.6.0
            ``dim`` is deprecated, use ``spatial_dims`` instead.
        """
        super().__init__()

        if dim is not None:
            spatial_dims = dim
        conv_0 = Convolution(spatial_dims, in_chns, out_chns, act=act, norm=norm, dropout=dropout, bias=bias, padding=1)
        conv_1 = Convolution(
            spatial_dims, out_chns, out_chns, act=act, norm=norm, dropout=dropout, bias=bias, padding=1
        )
        self.add_module("conv_0", conv_0)
        self.add_module("conv_1", conv_1)


</source>
<source file="systems/MONAI-0.8.0/monai/networks/nets/basic_unet.py" startline="68" endline="100" pcid="1652">
    def __init__(
        self,
        spatial_dims: int,
        in_chns: int,
        out_chns: int,
        act: Union[str, tuple],
        norm: Union[str, tuple],
        bias: bool,
        dropout: Union[float, tuple] = 0.0,
        dim: Optional[int] = None,
    ):
        """
        Args:
            spatial_dims: number of spatial dimensions.
            in_chns: number of input channels.
            out_chns: number of output channels.
            act: activation type and arguments.
            norm: feature normalization type and arguments.
            bias: whether to have a bias term in convolution blocks.
            dropout: dropout ratio. Defaults to no dropout.

        .. deprecated:: 0.6.0
            ``dim`` is deprecated, use ``spatial_dims`` instead.
        """
        super().__init__()
        if dim is not None:
            spatial_dims = dim
        max_pooling = Pool["MAX", spatial_dims](kernel_size=2)
        convs = TwoConv(spatial_dims, in_chns, out_chns, act, norm, bias, dropout)
        self.add_module("max_pooling", max_pooling)
        self.add_module("convs", convs)


</source>
</class>

<class classid="76" nclones="2" nlines="14" similarity="92">
<source file="systems/MONAI-0.8.0/monai/networks/nets/classifier.py" startline="84" endline="99" pcid="1658">
    def __init__(
        self,
        in_shape: Sequence[int],
        channels: Sequence[int],
        strides: Sequence[int],
        kernel_size: Union[Sequence[int], int] = 3,
        num_res_units: int = 2,
        act=Act.PRELU,
        norm=Norm.INSTANCE,
        dropout: Optional[float] = 0.25,
        bias: bool = True,
        last_act=Act.SIGMOID,
    ) -> None:
        super().__init__(in_shape, 1, channels, strides, kernel_size, num_res_units, act, norm, dropout, bias, last_act)


</source>
<source file="systems/MONAI-0.8.0/monai/networks/nets/classifier.py" startline="118" endline="131" pcid="1659">
    def __init__(
        self,
        in_shape: Sequence[int],
        channels: Sequence[int],
        strides: Sequence[int],
        kernel_size: Union[Sequence[int], int] = 3,
        num_res_units: int = 2,
        act=Act.PRELU,
        norm=Norm.INSTANCE,
        dropout: Optional[float] = 0.25,
        bias: bool = True,
    ) -> None:
        super().__init__(in_shape, 1, channels, strides, kernel_size, num_res_units, act, norm, dropout, bias, None)

</source>
</class>

<class classid="77" nclones="2" nlines="37" similarity="83">
<source file="systems/MONAI-0.8.0/monai/networks/nets/vitautoenc.py" startline="33" endline="101" pcid="1662">
    def __init__(
        self,
        in_channels: int,
        img_size: Union[Sequence[int], int],
        patch_size: Union[Sequence[int], int],
        hidden_size: int = 768,
        mlp_dim: int = 3072,
        num_layers: int = 12,
        num_heads: int = 12,
        pos_embed: str = "conv",
        dropout_rate: float = 0.0,
        spatial_dims: int = 3,
    ) -> None:
        """
        Args:
            in_channels: dimension of input channels or the number of channels for input
            img_size: dimension of input image.
            patch_size: dimension of patch size.
            hidden_size: dimension of hidden layer.
            mlp_dim: dimension of feedforward layer.
            num_layers: number of transformer blocks.
            num_heads: number of attention heads.
            pos_embed: position embedding layer type.
            dropout_rate: faction of the input units to drop.
            spatial_dims: number of spatial dimensions.

        Examples::

            # for single channel input with image size of (96,96,96), conv position embedding and segmentation backbone
            # It will provide an output of same size as that of the input
            >>> net = ViTAutoEnc(in_channels=1, patch_size=(16,16,16), img_size=(96,96,96), pos_embed='conv')

            # for 3-channel with image size of (128,128,128), output will be same size as of input
            >>> net = ViTAutoEnc(in_channels=3, patch_size=(16,16,16), img_size=(128,128,128), pos_embed='conv')

        """

        super().__init__()

        if not (0 <= dropout_rate <= 1):
            raise ValueError("dropout_rate should be between 0 and 1.")

        if hidden_size % num_heads != 0:
            raise ValueError("hidden_size should be divisible by num_heads.")

        if spatial_dims == 2:
            raise ValueError("Not implemented for 2 dimensions, please try 3")

        self.patch_embedding = PatchEmbeddingBlock(
            in_channels=in_channels,
            img_size=img_size,
            patch_size=patch_size,
            hidden_size=hidden_size,
            num_heads=num_heads,
            pos_embed=pos_embed,
            dropout_rate=dropout_rate,
            spatial_dims=spatial_dims,
        )
        self.blocks = nn.ModuleList(
            [TransformerBlock(hidden_size, mlp_dim, num_heads, dropout_rate) for i in range(num_layers)]
        )
        self.norm = nn.LayerNorm(hidden_size)

        new_patch_size = (4, 4, 4)
        self.conv3d_transpose = nn.ConvTranspose3d(hidden_size, 16, kernel_size=new_patch_size, stride=new_patch_size)
        self.conv3d_transpose_1 = nn.ConvTranspose3d(
            in_channels=16, out_channels=1, kernel_size=new_patch_size, stride=new_patch_size
        )

</source>
<source file="systems/MONAI-0.8.0/monai/networks/nets/vit.py" startline="30" endline="99" pcid="1664">
    def __init__(
        self,
        in_channels: int,
        img_size: Union[Sequence[int], int],
        patch_size: Union[Sequence[int], int],
        hidden_size: int = 768,
        mlp_dim: int = 3072,
        num_layers: int = 12,
        num_heads: int = 12,
        pos_embed: str = "conv",
        classification: bool = False,
        num_classes: int = 2,
        dropout_rate: float = 0.0,
        spatial_dims: int = 3,
    ) -> None:
        """
        Args:
            in_channels: dimension of input channels.
            img_size: dimension of input image.
            patch_size: dimension of patch size.
            hidden_size: dimension of hidden layer.
            mlp_dim: dimension of feedforward layer.
            num_layers: number of transformer blocks.
            num_heads: number of attention heads.
            pos_embed: position embedding layer type.
            classification: bool argument to determine if classification is used.
            num_classes: number of classes if classification is used.
            dropout_rate: faction of the input units to drop.
            spatial_dims: number of spatial dimensions.

        Examples::

            # for single channel input with image size of (96,96,96), conv position embedding and segmentation backbone
            >>> net = ViT(in_channels=1, img_size=(96,96,96), pos_embed='conv')

            # for 3-channel with image size of (128,128,128), 24 layers and classification backbone
            >>> net = ViT(in_channels=3, img_size=(128,128,128), pos_embed='conv', classification=True)

            # for 3-channel with image size of (224,224), 12 layers and classification backbone
            >>> net = ViT(in_channels=3, img_size=(224,224), pos_embed='conv', classification=True, spatial_dims=2)

        """

        super().__init__()

        if not (0 <= dropout_rate <= 1):
            raise ValueError("dropout_rate should be between 0 and 1.")

        if hidden_size % num_heads != 0:
            raise ValueError("hidden_size should be divisible by num_heads.")

        self.classification = classification
        self.patch_embedding = PatchEmbeddingBlock(
            in_channels=in_channels,
            img_size=img_size,
            patch_size=patch_size,
            hidden_size=hidden_size,
            num_heads=num_heads,
            pos_embed=pos_embed,
            dropout_rate=dropout_rate,
            spatial_dims=spatial_dims,
        )
        self.blocks = nn.ModuleList(
            [TransformerBlock(hidden_size, mlp_dim, num_heads, dropout_rate) for i in range(num_layers)]
        )
        self.norm = nn.LayerNorm(hidden_size)
        if self.classification:
            self.cls_token = nn.Parameter(torch.zeros(1, 1, hidden_size))
            self.classification_head = nn.Sequential(nn.Linear(hidden_size, num_classes), nn.Tanh())

</source>
</class>

<class classid="78" nclones="2" nlines="12" similarity="76">
<source file="systems/MONAI-0.8.0/monai/networks/blocks/segresnet_block.py" startline="73" endline="87" pcid="1687">
    def forward(self, x):

        identity = x

        x = self.norm1(x)
        x = self.relu(x)
        x = self.conv1(x)

        x = self.norm2(x)
        x = self.relu(x)
        x = self.conv2(x)

        x += identity

        return x
</source>
<source file="systems/MONAI-0.8.0/monai/networks/blocks/dynunet_block.py" startline="77" endline="91" pcid="1689">
    def forward(self, inp):
        residual = inp
        out = self.conv1(inp)
        out = self.norm1(out)
        out = self.lrelu(out)
        out = self.conv2(out)
        out = self.norm2(out)
        if self.downsample:
            residual = self.conv3(residual)
            residual = self.norm3(residual)
        out += residual
        out = self.lrelu(out)
        return out


</source>
</class>

<class classid="79" nclones="2" nlines="30" similarity="78">
<source file="systems/MONAI-0.8.0/monai/networks/blocks/dynunet_block.py" startline="41" endline="76" pcid="1688">
    def __init__(
        self,
        spatial_dims: int,
        in_channels: int,
        out_channels: int,
        kernel_size: Union[Sequence[int], int],
        stride: Union[Sequence[int], int],
        norm_name: Union[Tuple, str],
        act_name: Union[Tuple, str] = ("leakyrelu", {"inplace": True, "negative_slope": 0.01}),
        dropout: Optional[Union[Tuple, str, float]] = None,
    ):
        super().__init__()
        self.conv1 = get_conv_layer(
            spatial_dims,
            in_channels,
            out_channels,
            kernel_size=kernel_size,
            stride=stride,
            dropout=dropout,
            conv_only=True,
        )
        self.conv2 = get_conv_layer(
            spatial_dims, out_channels, out_channels, kernel_size=kernel_size, stride=1, dropout=dropout, conv_only=True
        )
        self.conv3 = get_conv_layer(
            spatial_dims, in_channels, out_channels, kernel_size=1, stride=stride, dropout=dropout, conv_only=True
        )
        self.lrelu = get_act_layer(name=act_name)
        self.norm1 = get_norm_layer(name=norm_name, spatial_dims=spatial_dims, channels=out_channels)
        self.norm2 = get_norm_layer(name=norm_name, spatial_dims=spatial_dims, channels=out_channels)
        self.norm3 = get_norm_layer(name=norm_name, spatial_dims=spatial_dims, channels=out_channels)
        self.downsample = in_channels != out_channels
        stride_np = np.atleast_1d(stride)
        if not np.all(stride_np == 1):
            self.downsample = True

</source>
<source file="systems/MONAI-0.8.0/monai/networks/blocks/dynunet_block.py" startline="110" endline="137" pcid="1690">
    def __init__(
        self,
        spatial_dims: int,
        in_channels: int,
        out_channels: int,
        kernel_size: Union[Sequence[int], int],
        stride: Union[Sequence[int], int],
        norm_name: Union[Tuple, str],
        act_name: Union[Tuple, str] = ("leakyrelu", {"inplace": True, "negative_slope": 0.01}),
        dropout: Optional[Union[Tuple, str, float]] = None,
    ):
        super().__init__()
        self.conv1 = get_conv_layer(
            spatial_dims,
            in_channels,
            out_channels,
            kernel_size=kernel_size,
            stride=stride,
            dropout=dropout,
            conv_only=True,
        )
        self.conv2 = get_conv_layer(
            spatial_dims, out_channels, out_channels, kernel_size=kernel_size, stride=1, dropout=dropout, conv_only=True
        )
        self.lrelu = get_act_layer(name=act_name)
        self.norm1 = get_norm_layer(name=norm_name, spatial_dims=spatial_dims, channels=out_channels)
        self.norm2 = get_norm_layer(name=norm_name, spatial_dims=spatial_dims, channels=out_channels)

</source>
</class>

<class classid="80" nclones="2" nlines="38" similarity="76">
<source file="systems/MONAI-0.8.0/monai/networks/blocks/dynunet_block.py" startline="168" endline="204" pcid="1692">
    def __init__(
        self,
        spatial_dims: int,
        in_channels: int,
        out_channels: int,
        kernel_size: Union[Sequence[int], int],
        stride: Union[Sequence[int], int],
        upsample_kernel_size: Union[Sequence[int], int],
        norm_name: Union[Tuple, str],
        act_name: Union[Tuple, str] = ("leakyrelu", {"inplace": True, "negative_slope": 0.01}),
        dropout: Optional[Union[Tuple, str, float]] = None,
        trans_bias: bool = False,
    ):
        super().__init__()
        upsample_stride = upsample_kernel_size
        self.transp_conv = get_conv_layer(
            spatial_dims,
            in_channels,
            out_channels,
            kernel_size=upsample_kernel_size,
            stride=upsample_stride,
            dropout=dropout,
            bias=trans_bias,
            conv_only=True,
            is_transposed=True,
        )
        self.conv_block = UnetBasicBlock(
            spatial_dims,
            out_channels + out_channels,
            out_channels,
            kernel_size=kernel_size,
            stride=1,
            dropout=dropout,
            norm_name=norm_name,
            act_name=act_name,
        )

</source>
<source file="systems/MONAI-0.8.0/monai/networks/blocks/unetr_block.py" startline="27" endline="79" pcid="1726">
    def __init__(
        self,
        spatial_dims: int,
        in_channels: int,
        out_channels: int,
        kernel_size: Union[Sequence[int], int],
        upsample_kernel_size: Union[Sequence[int], int],
        norm_name: Union[Tuple, str],
        res_block: bool = False,
    ) -> None:
        """
        Args:
            spatial_dims: number of spatial dimensions.
            in_channels: number of input channels.
            out_channels: number of output channels.
            kernel_size: convolution kernel size.
            upsample_kernel_size: convolution kernel size for transposed convolution layers.
            norm_name: feature normalization type and arguments.
            res_block: bool argument to determine if residual block is used.

        """

        super().__init__()
        upsample_stride = upsample_kernel_size
        self.transp_conv = get_conv_layer(
            spatial_dims,
            in_channels,
            out_channels,
            kernel_size=upsample_kernel_size,
            stride=upsample_stride,
            conv_only=True,
            is_transposed=True,
        )

        if res_block:
            self.conv_block = UnetResBlock(
                spatial_dims,
                out_channels + out_channels,
                out_channels,
                kernel_size=kernel_size,
                stride=1,
                norm_name=norm_name,
            )
        else:
            self.conv_block = UnetBasicBlock(  # type: ignore
                spatial_dims,
                out_channels + out_channels,
                out_channels,
                kernel_size=kernel_size,
                stride=1,
                norm_name=norm_name,
            )

</source>
</class>

<class classid="81" nclones="2" nlines="11" similarity="72">
<source file="systems/MONAI-0.8.0/monai/networks/blocks/dynunet_block.py" startline="260" endline="273" pcid="1697">
def get_padding(
    kernel_size: Union[Sequence[int], int], stride: Union[Sequence[int], int]
) -> Union[Tuple[int, ...], int]:

    kernel_size_np = np.atleast_1d(kernel_size)
    stride_np = np.atleast_1d(stride)
    padding_np = (kernel_size_np - stride_np + 1) / 2
    if np.min(padding_np) < 0:
        raise AssertionError("padding value should not be negative, please change the kernel size and/or stride.")
    padding = tuple(int(p) for p in padding_np)

    return padding if len(padding) > 1 else padding[0]


</source>
<source file="systems/MONAI-0.8.0/monai/networks/blocks/dynunet_block.py" startline="274" endline="286" pcid="1698">
def get_output_padding(
    kernel_size: Union[Sequence[int], int], stride: Union[Sequence[int], int], padding: Union[Sequence[int], int]
) -> Union[Tuple[int, ...], int]:
    kernel_size_np = np.atleast_1d(kernel_size)
    stride_np = np.atleast_1d(stride)
    padding_np = np.atleast_1d(padding)

    out_padding_np = 2 * padding_np + stride_np - kernel_size_np
    if np.min(out_padding_np) < 0:
        raise AssertionError("out_padding value should not be negative, please change the kernel size and/or stride.")
    out_padding = tuple(int(p) for p in out_padding_np)

    return out_padding if len(out_padding) > 1 else out_padding[0]
</source>
</class>

<class classid="82" nclones="2" nlines="32" similarity="81">
<source file="systems/MONAI-0.8.0/monai/networks/blocks/dints_block.py" startline="28" endline="72" pcid="1699">
    def __init__(
        self,
        in_channel: int,
        out_channel: int,
        spatial_dims: int = 3,
        act_name: Union[Tuple, str] = "RELU",
        norm_name: Union[Tuple, str] = "INSTANCE",
    ):
        """
        Args:
            in_channel: number of input channels
            out_channel: number of output channels
            spatial_dims: number of spatial dimensions
            act_name: activation layer type and arguments.
            norm_name: feature normalization type and arguments.
        """
        super().__init__()
        self._in_channel = in_channel
        self._out_channel = out_channel
        self._spatial_dims = spatial_dims
        if self._spatial_dims not in (2, 3):
            raise ValueError("spatial_dims must be 2 or 3.")

        conv_type = Conv[Conv.CONV, self._spatial_dims]
        mode = "trilinear" if self._spatial_dims == 3 else "bilinear"
        self.add_module("up", torch.nn.Upsample(scale_factor=2, mode=mode, align_corners=True))
        self.add_module("acti", get_act_layer(name=act_name))
        self.add_module(
            "conv",
            conv_type(
                in_channels=self._in_channel,
                out_channels=self._out_channel,
                kernel_size=1,
                stride=1,
                padding=0,
                groups=1,
                bias=False,
                dilation=1,
            ),
        )
        self.add_module(
            "norm", get_norm_layer(name=norm_name, spatial_dims=self._spatial_dims, channels=self._out_channel)
        )


</source>
<source file="systems/MONAI-0.8.0/monai/networks/blocks/dints_block.py" startline="230" endline="272" pcid="1703">
    def __init__(
        self,
        in_channel: int,
        out_channel: int,
        kernel_size: int = 3,
        padding: int = 1,
        spatial_dims: int = 3,
        act_name: Union[Tuple, str] = "RELU",
        norm_name: Union[Tuple, str] = "INSTANCE",
    ):
        """
        Args:
            in_channel: number of input channels.
            out_channel: number of output channels.
            kernel_size: kernel size of the convolution.
            padding: padding size of the convolution.
            spatial_dims: number of spatial dimensions.
            act_name: activation layer type and arguments.
            norm_name: feature normalization type and arguments.
        """
        super().__init__()
        self._in_channel = in_channel
        self._out_channel = out_channel
        self._spatial_dims = spatial_dims

        conv_type = Conv[Conv.CONV, self._spatial_dims]
        self.add_module("acti", get_act_layer(name=act_name))
        self.add_module(
            "conv",
            conv_type(
                in_channels=self._in_channel,
                out_channels=self._out_channel,
                kernel_size=kernel_size,
                stride=1,
                padding=padding,
                groups=1,
                bias=False,
                dilation=1,
            ),
        )
        self.add_module(
            "norm", get_norm_layer(name=norm_name, spatial_dims=self._spatial_dims, channels=self._out_channel)
        )
</source>
</class>

<class classid="83" nclones="2" nlines="20" similarity="100">
<source file="systems/MONAI-0.8.0/versioneer.py" startline="1620" endline="1642" pcid="1777">
            def run(self):
                root = get_root()
                cfg = get_config_from_root(root)
                versions = get_versions()
                target_versionfile = cfg.versionfile_source
                print("UPDATING %s" % target_versionfile)
                write_to_version_file(target_versionfile, versions)

                _build_exe.run(self)
                os.unlink(target_versionfile)
                with open(cfg.versionfile_source, "w") as f:
                    LONG = LONG_VERSION_PY[cfg.VCS]
                    f.write(
                        LONG
                        % {
                            "DOLLAR": "$",
                            "STYLE": cfg.style,
                            "TAG_PREFIX": cfg.tag_prefix,
                            "PARENTDIR_PREFIX": cfg.parentdir_prefix,
                            "VERSIONFILE_SOURCE": cfg.versionfile_source,
                        }
                    )

</source>
<source file="systems/MONAI-0.8.0/versioneer.py" startline="1650" endline="1672" pcid="1778">
            def run(self):
                root = get_root()
                cfg = get_config_from_root(root)
                versions = get_versions()
                target_versionfile = cfg.versionfile_source
                print("UPDATING %s" % target_versionfile)
                write_to_version_file(target_versionfile, versions)

                _py2exe.run(self)
                os.unlink(target_versionfile)
                with open(cfg.versionfile_source, "w") as f:
                    LONG = LONG_VERSION_PY[cfg.VCS]
                    f.write(
                        LONG
                        % {
                            "DOLLAR": "$",
                            "STYLE": cfg.style,
                            "TAG_PREFIX": cfg.tag_prefix,
                            "PARENTDIR_PREFIX": cfg.parentdir_prefix,
                            "VERSIONFILE_SOURCE": cfg.versionfile_source,
                        }
                    )

</source>
</class>

</clones>

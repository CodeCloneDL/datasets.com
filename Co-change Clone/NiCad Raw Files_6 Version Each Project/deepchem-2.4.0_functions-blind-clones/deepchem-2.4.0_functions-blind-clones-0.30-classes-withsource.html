<html>
<head>
    <title>NiCad6 Clone Report</title>
    <style type="text/css">
        body {font-family:sans-serif;}
        table {background-color:white; border:0px; padding:0px; border-spacing:4px; width:auto; margin-left:30px; margin-right:auto;}
        td {background-color:rgba(192,212,238,0.8); border:0px; padding:8px; width:auto; vertical-align:top; border-radius:8px}
        pre {background-color:white; padding:4px;}
        a {color:darkblue;}
    </style>
</head>
<body>
<h2>NiCad6 Clone Report</h2>
<table>
<tr style="font-size:14pt">
<td><b>System:</b> &nbsp; deepchem-2.4.0</td>
<td><b>Clone pairs:</b> &nbsp; 835</td>
<td><b>Clone classes:</b> &nbsp; 149</td>
</tr>
<tr style="font-size:12pt">
<td style="background-color:white">Clone type: &nbsp; 3-2</td>
<td style="background-color:white">Granularity: &nbsp; functions-blind</td>
<td style="background-color:white">Max diff threshold: &nbsp; 30%</td>
<td style="background-color:white">Clone size: &nbsp; 10 - 2500 lines</td>
<td style="background-color:white">Total functions-blind: &nbsp; 2021</td>
</tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 1:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 87%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag47')" href="javascript:;">
deepchem-2.4.0/contrib/torch/pytorch_graphconv.py: 152-176
</a>
<div class="mid" id="frag47" style="display:none"><pre>
    def __init__(self, net, lr, weight_decay):
        """
        net: an instance of class GraphConvolution
        lr: float, learning rate 
        weight_decay: float
        """
        self.net = net
        self.criterion = nn.CrossEntropyLoss()
        self.input_x = torch.FloatTensor(-1, self.net.max_n_atoms, self.net.n_atom_types)
        self.input_g = torch.FloatTensor(-1, self.net.max_n_atoms, self.net.max_n_atoms)
        self.label = torch.FloatTensor(-1)
        
        self.net.cuda()
        self.criterion = nn.CrossEntropyLoss()
        self.criterion.cuda()
        
        self.input_x, self.input_g, self.label = self.input_x.cuda(), self.input_g.cuda(), self.label.cuda()

        self.lr = lr
        self.weight_decay = weight_decay
        # setup optimizer
        self.optimizer = optim.Adam(self.net.parameters(),
                               lr=self.lr,
                               weight_decay=self.weight_decay)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag50')" href="javascript:;">
deepchem-2.4.0/contrib/torch/pytorch_graphconv.py: 307-335
</a>
<div class="mid" id="frag50" style="display:none"><pre>
    def __init__(self, net, lr, weight_decay, n_tasks):
        """
        net: an instance of class GraphConvolution
        lr: float, learning rate 
        weight_decay: float
        n_tasks: int, number of tasks
        """
        self.net = net
        self.criterion = nn.CrossEntropyLoss()
        self.input_x = torch.FloatTensor(-1, self.net.max_n_atoms, self.net.n_atom_types)
        self.input_g = torch.FloatTensor(-1, self.net.max_n_atoms, self.net.max_n_atoms)
        self.label = torch.FloatTensor(-1)
        
        self.net.cuda()

        self.criterion = nn.CrossEntropyLoss()
        self.criterion.cuda()
        
        self.input_x, self.input_g, self.label = self.input_x.cuda(), self.input_g.cuda(), self.label.cuda()

        self.lr = lr
        self.weight_decay = weight_decay
        # setup optimizer
        self.optimizer = optim.Adam(self.net.parameters(),
                               lr=self.lr,
                               weight_decay=self.weight_decay)

        self.n_tasks = n_tasks

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 2:</b> &nbsp; 2 fragments, nominal size 24 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag48')" href="javascript:;">
deepchem-2.4.0/contrib/torch/pytorch_graphconv.py: 177-216
</a>
<div class="mid" id="frag48" style="display:none"><pre>
    def train_epoch(self, train_features, y_train, batch_size=32,
                    shuffle_train_inds=True):
        """
        train_features: list of dictionaries. each dictionary represents one sample feature. 
            key "x" maps to max_n_atoms x p feature matrix. key "g" maps to square adjacency matrix 
        y_train: numpy array of labels 
        """

        train_inds = range(0, len(train_features))
        if shuffle_train_inds:
            random.shuffle(train_inds)

        for b in range(0, len(train_inds)/batch_size):
            batch_inds = [train_inds[idx] for idx in range(b*batch_size, (b+1)*batch_size)]
            
            train_x_batch = np.concatenate([np.expand_dims(train_features[idx]["x"], 0) for idx in batch_inds], axis=0)
            train_g_batch = np.concatenate([np.expand_dims(train_features[idx]["g"], 0) for idx in batch_inds], axis=0)

            xb = torch.from_numpy(train_x_batch.astype(np.float32)).cuda()
            gb = torch.from_numpy(train_g_batch.astype(np.float32)).cuda()
            yb = torch.from_numpy(y_train[batch_inds].astype(np.float32)).cuda()

            self.net.train()
            self.net.zero_grad()
            
            self.input_x.resize_as_(xb).copy_(xb)
            self.input_g.resize_as_(gb).copy_(gb)
            self.label.resize_as_(yb).copy_(yb)
            
            input_xv = Variable(self.input_x)
            input_gv = Variable(self.input_g)
            label_v = Variable(self.label)

            output = self.net(input_gv, input_xv)
            
            err = self.criterion(output, label_v)
            err.backward()
            
            self.optimizer.step()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag52')" href="javascript:;">
deepchem-2.4.0/contrib/torch/pytorch_graphconv.py: 359-392
</a>
<div class="mid" id="frag52" style="display:none"><pre>
    def train_epoch(self, train_features, y_train, batch_size=32,
                    shuffle_train_inds=True):
        train_inds = range(0, len(train_features))
        if shuffle_train_inds:
            random.shuffle(train_inds)

        for b in range(0, len(train_inds)/batch_size):
            batch_inds = [train_inds[idx] for idx in range(b*batch_size, (b+1)*batch_size)]
            
            train_x_batch = np.concatenate([np.expand_dims(train_features[idx]["x"], 0) for idx in batch_inds], axis=0)
            train_g_batch = np.concatenate([np.expand_dims(train_features[idx]["g"], 0) for idx in batch_inds], axis=0)

            xb = torch.from_numpy(train_x_batch.astype(np.float32)).cuda()
            gb = torch.from_numpy(train_g_batch.astype(np.float32)).cuda()
            yb = torch.from_numpy(y_train[batch_inds].astype(np.float32)).cuda()

            self.net.train()
            self.net.zero_grad()
            
            self.input_x.resize_as_(xb).copy_(xb)
            self.input_g.resize_as_(gb).copy_(gb)
            self.label.resize_as_(yb).copy_(yb)
            
            input_xv = Variable(self.input_x)
            input_gv = Variable(self.input_g)
            label_v = Variable(self.label)

            output = self.net(input_gv, input_xv)
            
            err = self.multitask_loss(output, label_v)
            err.backward()
            
            self.optimizer.step()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 3:</b> &nbsp; 2 fragments, nominal size 61 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag49')" href="javascript:;">
deepchem-2.4.0/contrib/torch/pytorch_graphconv.py: 217-300
</a>
<div class="mid" id="frag49" style="display:none"><pre>
    def evaluate(self, train_features,
                       test_features,
                       y_train,
                       y_test, 
                       transformer,
                       batch_size=32):
        
        self.net.eval()
        print("TRAIN:")
        
        o = []
        l = []

        train_inds = range(0, len(train_features))

        for b in range(0, len(train_features)/batch_size):
            batch_inds = [train_inds[idx] for idx in range(b*batch_size, (b+1)*batch_size)]
            
            train_x_batch = np.concatenate([np.expand_dims(train_features[idx]["x"], 0) for idx in batch_inds], axis=0)
            train_g_batch = np.concatenate([np.expand_dims(train_features[idx]["g"], 0) for idx in batch_inds], axis=0)

            xb = torch.from_numpy(train_x_batch.astype(np.float32)).cuda()
            gb = torch.from_numpy(train_g_batch.astype(np.float32)).cuda()
            
            self.input_x.resize_as_(xb).copy_(xb)
            self.input_g.resize_as_(gb).copy_(gb)
            
            input_xv = Variable(self.input_x)
            input_gv = Variable(self.input_g)

            output = self.net(input_gv, input_xv)
            
            if transformer is not None:
                o.append(transformer.inverse_transform(output.data.cpu().numpy().reshape((-1,1))).flatten())
                l.append(transformer.inverse_transform(y_train[batch_inds].reshape((-1,1))).flatten())
            else:
                o.append(output.data.cpu().numpy().reshape((-1,1)).flatten())
                l.append(y_train[batch_inds].reshape((-1,1)).flatten())

        o = np.concatenate(o)
        l = np.concatenate(l)
        print("RMSE:")
        print(np.sqrt(np.mean(np.square(l-o))))
        print("ROC AUC:")
        print(roc_auc_score(l, o))
        
        o = []
        l = []

        print("TEST:")
        test_inds = range(0, len(test_features))

        for b in range(0, len(test_features)/batch_size):
            batch_inds = [test_inds[idx] for idx in range(b*batch_size, (b+1)*batch_size)]
            
            test_x_batch = np.concatenate([np.expand_dims(test_features[idx]["x"], 0) for idx in batch_inds], axis=0)
            test_g_batch = np.concatenate([np.expand_dims(test_features[idx]["g"], 0) for idx in batch_inds], axis=0)

            xb = torch.from_numpy(test_x_batch.astype(np.float32)).cuda()
            gb = torch.from_numpy(test_g_batch.astype(np.float32)).cuda()
            
            self.input_x.resize_as_(xb).copy_(xb)
            self.input_g.resize_as_(gb).copy_(gb)
            
            input_xv = Variable(self.input_x)
            input_gv = Variable(self.input_g)

            output = self.net(input_gv, input_xv)
            
            if transformer is not None:
                o.append(transformer.inverse_transform(output.data.cpu().numpy().reshape((-1,1))).flatten())
                l.append(transformer.inverse_transform(y_test[batch_inds].reshape((-1,1))).flatten())
            else:
                o.append(output.data.cpu().numpy().reshape((-1,1)).flatten())
                l.append(y_test[batch_inds].reshape((-1,1)).flatten())

        o = np.concatenate(o)
        l = np.concatenate(l)
        print("RMSE:")
        print(np.sqrt(np.mean(np.square(l-o))))
        print("ROC AUC:")
        print(roc_auc_score(l, o))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag53')" href="javascript:;">
deepchem-2.4.0/contrib/torch/pytorch_graphconv.py: 393-474
</a>
<div class="mid" id="frag53" style="display:none"><pre>
    def evaluate(self, train_features,
                       test_features,
                       y_train,
                       y_test, 
                       transformer,
                       batch_size=32):
        
        self.net.eval()
        print("TRAIN:")
        
        o = []
        l = []

        train_inds = range(0, len(train_features))

        for b in range(0, len(train_features)/batch_size):
            batch_inds = [train_inds[idx] for idx in range(b*batch_size, (b+1)*batch_size)]
            
            train_x_batch = np.concatenate([np.expand_dims(train_features[idx]["x"], 0) for idx in batch_inds], axis=0)
            train_g_batch = np.concatenate([np.expand_dims(train_features[idx]["g"], 0) for idx in batch_inds], axis=0)

            xb = torch.from_numpy(train_x_batch.astype(np.float32)).cuda()
            gb = torch.from_numpy(train_g_batch.astype(np.float32)).cuda()
            
            self.input_x.resize_as_(xb).copy_(xb)
            self.input_g.resize_as_(gb).copy_(gb)
            
            input_xv = Variable(self.input_x)
            input_gv = Variable(self.input_g)

            output = self.net(input_gv, input_xv)
            
            if transformer is not None:
                o.append(transformer.inverse_transform(output.data.cpu().numpy().reshape((-1,1))).flatten())
                l.append(transformer.inverse_transform(y_train[batch_inds].reshape((-1,1))).flatten())
            else:
                o.append(output.data.cpu().numpy().reshape((-1,1)).flatten())
                l.append(y_train[batch_inds].reshape((-1,1)).flatten())

        o = np.concatenate(o)
        l = np.concatenate(l)
        print("RMSE:")
        print(np.sqrt(np.mean(np.square(l-o))))
        print("ROC AUC:")
        print(roc_auc_score(l, o))
        
        o = []
        l = []

        print("TEST:")
        test_inds = range(0, len(test_features))

        for b in range(0, len(test_features)/batch_size):
            batch_inds = [test_inds[idx] for idx in range(b*batch_size, (b+1)*batch_size)]
            
            test_x_batch = np.concatenate([np.expand_dims(test_features[idx]["x"], 0) for idx in batch_inds], axis=0)
            test_g_batch = np.concatenate([np.expand_dims(test_features[idx]["g"], 0) for idx in batch_inds], axis=0)

            xb = torch.from_numpy(test_x_batch.astype(np.float32)).cuda()
            gb = torch.from_numpy(test_g_batch.astype(np.float32)).cuda()
            
            self.input_x.resize_as_(xb).copy_(xb)
            self.input_g.resize_as_(gb).copy_(gb)
            
            input_xv = Variable(self.input_x)
            input_gv = Variable(self.input_g)

            output = self.net(input_gv, input_xv)
            
            if transformer is not None:
                o.append(transformer.inverse_transform(output.data.cpu().numpy().reshape((-1,1))).flatten())
                l.append(transformer.inverse_transform(y_test[batch_inds].reshape((-1,1))).flatten())
            else:
                o.append(output.data.cpu().numpy().reshape((-1,1)).flatten())
                l.append(y_test[batch_inds].reshape((-1,1)).flatten())

        o = np.concatenate(o)
        l = np.concatenate(l)
        print("RMSE:")
        print(np.sqrt(np.mean(np.square(l-o))))
        print("ROC AUC:")
        print(roc_auc_score(l, o))
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 4:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 77%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag63')" href="javascript:;">
deepchem-2.4.0/contrib/torch/torch_model.py: 177-203
</a>
<div class="mid" id="frag63" style="display:none"><pre>
  def predict(self, dataset, transformers=[]):
    """
    Uses self to make predictions on provided Dataset object.

    Returns:
      y_pred: numpy ndarray of shape (n_samples,)
    """
    y_preds = []
    n_tasks = self.n_tasks
    for (X_batch, _, _, ids_batch) in dataset.iterbatches(
        self.batch_size, deterministic=True):
      n_samples = len(X_batch)
      y_pred_batch = self.predict_on_batch(X_batch)
      assert y_pred_batch.shape == (n_samples, n_tasks)
      y_pred_batch = undo_transforms(y_pred_batch, transformers)
      y_preds.append(y_pred_batch)
    y_pred = np.vstack(y_preds)

    # The iterbatches does padding with zero-weight examples on the last batch.
    # Remove padded examples.
    n_samples = len(dataset)
    y_pred = np.reshape(y_pred, (n_samples, n_tasks))
    # Special case to handle singletasks.
    if n_tasks == 1:
      y_pred = np.reshape(y_pred, (n_samples,))
    return y_pred

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag285')" href="javascript:;">
deepchem-2.4.0/contrib/tensorflow_models/__init__.py: 520-550
</a>
<div class="mid" id="frag285" style="display:none"><pre>
  def predict(self, dataset, transformers=[]):
    """
    Uses self to make predictions on provided Dataset object.

    Returns:
      y_pred: numpy ndarray of shape (n_samples,)
    """
    y_preds = []
    n_tasks = self.get_num_tasks()
    ind = 0

    for (X_batch, _, _, ids_batch) in dataset.iterbatches(
        self.batch_size, deterministic=True):
      n_samples = len(X_batch)
      y_pred_batch = self.predict_on_batch(X_batch)
      # Discard any padded predictions
      y_pred_batch = y_pred_batch[:n_samples]
      y_pred_batch = np.reshape(y_pred_batch, (n_samples, n_tasks))
      y_pred_batch = undo_transforms(y_pred_batch, transformers)
      y_preds.append(y_pred_batch)
    y_pred = np.vstack(y_preds)

    # The iterbatches does padding with zero-weight examples on the last batch.
    # Remove padded examples.
    n_samples = len(dataset)
    y_pred = np.reshape(y_pred, (n_samples, n_tasks))
    # Special case to handle singletasks.
    if n_tasks == 1:
      y_pred = np.reshape(y_pred, (n_samples,))
    return y_pred

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 5:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag64')" href="javascript:;">
deepchem-2.4.0/contrib/torch/torch_model.py: 204-221
</a>
<div class="mid" id="frag64" style="display:none"><pre>
  def predict_proba(self, dataset, transformers=[], n_classes=2):
    y_preds = []
    n_tasks = self.n_tasks
    for (X_batch, y_batch, w_batch, ids_batch) in dataset.iterbatches(
        self.batch_size, deterministic=True):
      n_samples = len(X_batch)
      y_pred_batch = self.predict_proba_on_batch(X_batch)
      assert y_pred_batch.shape == (n_samples, n_tasks, n_classes)
      y_pred_batch = undo_transforms(y_pred_batch, transformers)
      y_preds.append(y_pred_batch)
    y_pred = np.vstack(y_preds)
    # The iterbatches does padding with zero-weight examples on the last batch.
    # Remove padded examples.
    n_samples = len(dataset)
    y_pred = y_pred[:n_samples]
    y_pred = np.reshape(y_pred, (n_samples, n_tasks, n_classes))
    return y_pred

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag286')" href="javascript:;">
deepchem-2.4.0/contrib/tensorflow_models/__init__.py: 551-599
</a>
<div class="mid" id="frag286" style="display:none"><pre>
  def predict_proba(self, dataset, transformers=[], n_classes=2):
    """
    TODO: Do transformers even make sense here?

    Returns:
      y_pred: numpy ndarray of shape (n_samples, n_classes*n_tasks)
    """
    y_preds = []
    n_tasks = self.get_num_tasks()

    for (X_batch, y_batch, w_batch, ids_batch) in dataset.iterbatches(
        self.batch_size, deterministic=True):
      n_samples = len(X_batch)
      y_pred_batch = self.predict_proba_on_batch(X_batch)
      y_pred_batch = y_pred_batch[:n_samples]
      y_pred_batch = np.reshape(y_pred_batch, (n_samples, n_tasks, n_classes))
      y_pred_batch = undo_transforms(y_pred_batch, transformers)
      y_preds.append(y_pred_batch)
    y_pred = np.vstack(y_preds)
    # The iterbatches does padding with zero-weight examples on the last batch.
    # Remove padded examples.
    n_samples = len(dataset)
    y_pred = y_pred[:n_samples]
    y_pred = np.reshape(y_pred, (n_samples, n_tasks, n_classes))
    return y_pred

  # TODO(rbharath): Verify this can be safely removed.
  #def evaluate(self, dataset, metrics, transformers=[]):
  #  """
  #  Evaluates the performance of this model on specified dataset.
  #
  #  Parameters
  #  ----------
  #  dataset: dc.data.Dataset
  #    Dataset object.
  #  metric: deepchem.metrics.Metric
  #    Evaluation metric
  #  transformers: list
  #    List of deepchem.transformers.Transformer

  #  Returns
  #  -------
  #  dict
  #    Maps tasks to scores under metric.
  #  """
  #  evaluator = Evaluator(self, dataset, transformers)
  #  scores = evaluator.compute_model_performance(metrics)
  #  return scores

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 6:</b> &nbsp; 4 fragments, nominal size 15 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag74')" href="javascript:;">
deepchem-2.4.0/contrib/vina_model/test_vina_model.py: 82-102
</a>
<div class="mid" id="frag74" style="display:none"><pre>
  def test_put_atoms_in_cells(self):
    """Test that atoms can be partitioned into spatial cells."""
    N = 10
    start = 0
    stop = 4
    nbr_cutoff = 1
    ndim = 3
    k = 5
    # The number of cells which we should theoretically have
    n_cells = ((stop - start) / nbr_cutoff)**ndim

    with self.session() as sess:
      cells = get_cells(start, stop, nbr_cutoff, ndim=ndim)
      coords = np.random.rand(N, ndim)
      _, atoms_in_cells = put_atoms_in_cells(coords, cells, N, n_cells, ndim, k)
      atoms_in_cells = atoms_in_cells.eval()
      assert len(atoms_in_cells) == n_cells
      # Each atom neighbors tensor should be (k, ndim) shaped.
      for atoms in atoms_in_cells:
        assert atoms.shape == (k, ndim)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag75')" href="javascript:;">
deepchem-2.4.0/contrib/vina_model/test_vina_model.py: 103-124
</a>
<div class="mid" id="frag75" style="display:none"><pre>
  def test_compute_neighbor_cells(self):
    """Test that indices of neighboring cells can be computed."""
    N = 10
    start = 0
    stop = 4
    nbr_cutoff = 1
    ndim = 3
    # The number of cells which we should theoretically have
    n_cells = ((stop - start) / nbr_cutoff)**ndim

    # TODO(rbharath): The test below only checks that shapes work out.
    # Need to do a correctness implementation vs. a simple CPU impl.

    with self.session() as sess:
      cells = get_cells(start, stop, nbr_cutoff, ndim=ndim)
      nbr_cells = compute_neighbor_cells(cells, ndim, n_cells)
      nbr_cells = nbr_cells.eval()
      assert len(nbr_cells) == n_cells
      nbr_cells = [nbr_cell for nbr_cell in nbr_cells]
      for nbr_cell in nbr_cells:
        assert nbr_cell.shape == (26,)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag76')" href="javascript:;">
deepchem-2.4.0/contrib/vina_model/test_vina_model.py: 125-146
</a>
<div class="mid" id="frag76" style="display:none"><pre>
  def test_compute_closest_neighbors(self):
    """Test that closest neighbors can be computed properly"""
    N = 10
    start = 0
    stop = 4
    nbr_cutoff = 1
    ndim = 3
    k = 5
    # The number of cells which we should theoretically have
    n_cells = ((stop - start) / nbr_cutoff)**ndim

    # TODO(rbharath): The test below only checks that shapes work out.
    # Need to do a correctness implementation vs. a simple CPU impl.

    with self.session() as sess:
      cells = get_cells(start, stop, nbr_cutoff, ndim=ndim)
      nbr_cells = compute_neighbor_cells(cells, ndim, n_cells)
      coords = np.random.rand(N, ndim)
      _, atoms_in_cells = put_atoms_in_cells(coords, cells, N, n_cells, ndim, k)
      nbrs = compute_closest_neighbors(coords, cells, atoms_in_cells, nbr_cells,
                                       N, n_cells)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag77')" href="javascript:;">
deepchem-2.4.0/contrib/vina_model/test_vina_model.py: 147-167
</a>
<div class="mid" id="frag77" style="display:none"><pre>
  def test_get_cells_for_atoms(self):
    """Test that atoms are placed in the correct cells."""
    N = 10
    start = 0
    stop = 4
    nbr_cutoff = 1
    ndim = 3
    k = 5
    # The number of cells which we should theoretically have
    n_cells = ((stop - start) / nbr_cutoff)**ndim

    # TODO(rbharath): The test below only checks that shapes work out.
    # Need to do a correctness implementation vs. a simple CPU impl.

    with self.session() as sess:
      cells = get_cells(start, stop, nbr_cutoff, ndim=ndim)
      coords = np.random.rand(N, ndim)
      cells_for_atoms = get_cells_for_atoms(coords, cells, N, n_cells, ndim)
      cells_for_atoms = cells_for_atoms.eval()
      assert cells_for_atoms.shape == (N, 1)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 7:</b> &nbsp; 3 fragments, nominal size 49 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag117')" href="javascript:;">
deepchem-2.4.0/contrib/rl/test_mcts.py: 15-89
</a>
<div class="mid" id="frag117" style="display:none"><pre>
  def test_roulette(self):
    """Test training a policy for the roulette environment."""

    # This is modeled after the Roulette-v0 environment from OpenAI Gym.
    # The player can bet on any number from 0 to 36, or walk away (which ends the
    # game).  The average reward for any bet is slightly negative, so the best
    # strategy is to walk away.

    class RouletteEnvironment(dc.rl.Environment):

      def __init__(self):
        super(RouletteEnvironment, self).__init__([(1,)], 38)
        self._state = [np.array([0])]

      def step(self, action):
        if action == 37:
          self._terminated = True  # Walk away.
          return 0.0
        wheel = np.random.randint(37)
        if wheel == 0:
          if action == 0:
            return 35.0
          return -1.0
        if action != 0 and wheel % 2 == action % 2:
          return 1.0
        return -1.0

      def reset(self):
        self._terminated = False

    env = RouletteEnvironment()

    # This policy just learns a constant probability for each action, and a constant for the value.

    class TestPolicy(dc.rl.Policy):

      def create_layers(self, state, **kwargs):
        action = Variable(np.ones(env.n_actions))
        output = SoftMax(
            in_layers=[Reshape(in_layers=[action], shape=(-1, env.n_actions))])
        value = Variable([0.0])
        return {'action_prob': output, 'value': value}

    # Optimize it.

    mcts = dc.rl.MCTS(
        env,
        TestPolicy(),
        max_search_depth=5,
        n_search_episodes=200,
        optimizer=Adam(learning_rate=0.005))
    mcts.fit(10, steps_per_iteration=50, epochs_per_iteration=50)

    # It should have learned that the expected value is very close to zero, and that the best
    # action is to walk away.

    action_prob, value = mcts.predict([[0]])
    assert -0.5 &lt; value[0] &lt; 0.5
    assert action_prob.argmax() == 37
    assert mcts.select_action([[0]], deterministic=True) == 37

    # Verify that we can create a new MCTS object, reload the parameters from the first one, and
    # get the same result.

    new_mcts = dc.rl.MCTS(env, TestPolicy(), model_dir=mcts._graph.model_dir)
    new_mcts.restore()
    action_prob2, value2 = new_mcts.predict([[0]])
    assert value2 == value

    # Do the same thing, only using the "restore" argument to fit().

    new_mcts = dc.rl.MCTS(env, TestPolicy(), model_dir=mcts._graph.model_dir)
    new_mcts.fit(0, restore=True)
    action_prob2, value2 = new_mcts.predict([[0]])
    assert value2 == value
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1277')" href="javascript:;">
deepchem-2.4.0/deepchem/rl/tests/test_a2c.py: 16-101
</a>
<div class="mid" id="frag1277" style="display:none"><pre>
  def test_roulette(self):
    """Test training a policy for the roulette environment."""

    # This is modeled after the Roulette-v0 environment from OpenAI Gym.
    # The player can bet on any number from 0 to 36, or walk away (which ends the
    # game).  The average reward for any bet is slightly negative, so the best
    # strategy is to walk away.

    class RouletteEnvironment(dc.rl.Environment):

      def __init__(self):
        super(RouletteEnvironment, self).__init__([(1,)], 38)
        self._state = [np.array([0])]

      def step(self, action):
        if action == 37:
          self._terminated = True  # Walk away.
          return 0.0
        wheel = np.random.randint(37)
        if wheel == 0:
          if action == 0:
            return 35.0
          return -1.0
        if action != 0 and wheel % 2 == action % 2:
          return 1.0
        return -1.0

      def reset(self):
        self._terminated = False

    env = RouletteEnvironment()

    # This policy just learns a constant probability for each action, and a constant for the value.

    class TestPolicy(dc.rl.Policy):

      def __init__(self):
        super(TestPolicy, self).__init__(['action_prob', 'value'])

      def create_model(self, **kwargs):

        class TestModel(tf.keras.Model):

          def __init__(self):
            super(TestModel, self).__init__(**kwargs)
            self.action = tf.Variable(np.ones(env.n_actions, np.float32))
            self.value = tf.Variable([0.0], tf.float32)

          def call(self, inputs, **kwargs):
            prob = tf.nn.softmax(tf.reshape(self.action, (-1, env.n_actions)))
            return (prob, self.value)

        return TestModel()

    # Optimize it.

    a2c = dc.rl.A2C(
        env,
        TestPolicy(),
        max_rollout_length=20,
        optimizer=Adam(learning_rate=0.001))
    a2c.fit(100000)

    # It should have learned that the expected value is very close to zero, and that the best
    # action is to walk away.

    action_prob, value = a2c.predict([[0]])
    assert -0.5 &lt; value[0] &lt; 0.5
    assert action_prob.argmax() == 37
    assert a2c.select_action([[0]], deterministic=True) == 37

    # Verify that we can create a new A2C object, reload the parameters from the first one, and
    # get the same result.

    new_a2c = dc.rl.A2C(env, TestPolicy(), model_dir=a2c._model.model_dir)
    new_a2c.restore()
    action_prob2, value2 = new_a2c.predict([[0]])
    assert value2 == value

    # Do the same thing, only using the "restore" argument to fit().

    new_a2c = dc.rl.A2C(env, TestPolicy(), model_dir=a2c._model.model_dir)
    new_a2c.fit(0, restore=True)
    action_prob2, value2 = new_a2c.predict([[0]])
    assert value2 == value

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1256')" href="javascript:;">
deepchem-2.4.0/deepchem/rl/tests/test_ppo.py: 16-102
</a>
<div class="mid" id="frag1256" style="display:none"><pre>
  def test_roulette(self):
    """Test training a policy for the roulette environment."""

    # This is modeled after the Roulette-v0 environment from OpenAI Gym.
    # The player can bet on any number from 0 to 36, or walk away (which ends the
    # game).  The average reward for any bet is slightly negative, so the best
    # strategy is to walk away.

    class RouletteEnvironment(dc.rl.Environment):

      def __init__(self):
        super(RouletteEnvironment, self).__init__([(1,)], 38)
        self._state = [np.array([0])]

      def step(self, action):
        if action == 37:
          self._terminated = True  # Walk away.
          return 0.0
        wheel = np.random.randint(37)
        if wheel == 0:
          if action == 0:
            return 35.0
          return -1.0
        if action != 0 and wheel % 2 == action % 2:
          return 1.0
        return -1.0

      def reset(self):
        self._terminated = False

    env = RouletteEnvironment()

    # This policy just learns a constant probability for each action, and a constant for the value.

    class TestPolicy(dc.rl.Policy):

      def __init__(self):
        super(TestPolicy, self).__init__(['action_prob', 'value'])

      def create_model(self, **kwargs):

        class TestModel(tf.keras.Model):

          def __init__(self):
            super(TestModel, self).__init__(**kwargs)
            self.action = tf.Variable(np.ones(env.n_actions, np.float32))
            self.value = tf.Variable([0.0], tf.float32)

          def call(self, inputs, **kwargs):
            prob = tf.nn.softmax(tf.reshape(self.action, (-1, env.n_actions)))
            return (prob, self.value)

        return TestModel()

    # Optimize it.

    ppo = dc.rl.PPO(
        env,
        TestPolicy(),
        max_rollout_length=20,
        optimization_epochs=8,
        optimizer=Adam(learning_rate=0.003))
    ppo.fit(80000)

    # It should have learned that the expected value is very close to zero, and that the best
    # action is to walk away.

    action_prob, value = ppo.predict([[0]])
    assert -0.8 &lt; value[0] &lt; 0.5
    assert action_prob.argmax() == 37
    assert ppo.select_action([[0]], deterministic=True) == 37

    # Verify that we can create a new PPO object, reload the parameters from the first one, and
    # get the same result.

    new_ppo = dc.rl.PPO(env, TestPolicy(), model_dir=ppo._model.model_dir)
    new_ppo.restore()
    action_prob2, value2 = new_ppo.predict([[0]])
    assert value2 == value

    # Do the same thing, only using the "restore" argument to fit().

    new_ppo = dc.rl.PPO(env, TestPolicy(), model_dir=ppo._model.model_dir)
    new_ppo.fit(0, restore=True)
    action_prob2, value2 = new_ppo.predict([[0]])
    assert value2 == value

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 8:</b> &nbsp; 4 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag119')" href="javascript:;">
deepchem-2.4.0/contrib/rl/test_mcts.py: 29-41
</a>
<div class="mid" id="frag119" style="display:none"><pre>
      def step(self, action):
        if action == 37:
          self._terminated = True  # Walk away.
          return 0.0
        wheel = np.random.randint(37)
        if wheel == 0:
          if action == 0:
            return 35.0
          return -1.0
        if action != 0 and wheel % 2 == action % 2:
          return 1.0
        return -1.0

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1248')" href="javascript:;">
deepchem-2.4.0/deepchem/rl/tests/test_rl_reload.py: 13-25
</a>
<div class="mid" id="frag1248" style="display:none"><pre>
  def step(self, action):
    if action == 37:
      self._terminated = True  # Walk away.
      return 0.0
    wheel = np.random.randint(37)
    if wheel == 0:
      if action == 0:
        return 35.0
      return -1.0
    if action != 0 and wheel % 2 == action % 2:
      return 1.0
    return -1.0

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1279')" href="javascript:;">
deepchem-2.4.0/deepchem/rl/tests/test_a2c.py: 30-42
</a>
<div class="mid" id="frag1279" style="display:none"><pre>
      def step(self, action):
        if action == 37:
          self._terminated = True  # Walk away.
          return 0.0
        wheel = np.random.randint(37)
        if wheel == 0:
          if action == 0:
            return 35.0
          return -1.0
        if action != 0 and wheel % 2 == action % 2:
          return 1.0
        return -1.0

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1258')" href="javascript:;">
deepchem-2.4.0/deepchem/rl/tests/test_ppo.py: 30-42
</a>
<div class="mid" id="frag1258" style="display:none"><pre>
      def step(self, action):
        if action == 37:
          self._terminated = True  # Walk away.
          return 0.0
        wheel = np.random.randint(37)
        if wheel == 0:
          if action == 0:
            return 35.0
          return -1.0
        if action != 0 and wheel % 2 == action % 2:
          return 1.0
        return -1.0

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 9:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag134')" href="javascript:;">
deepchem-2.4.0/contrib/dragonn/simulations.py: 95-134
</a>
<div class="mid" id="frag134" style="display:none"><pre>
def simulate_single_motif_detection(motif_name, seq_length,
                                    num_pos, num_neg, GC_fraction):
    """
    Simulates two classes of seqeuences:
        - Positive class sequence with a motif
          embedded anywhere in the sequence
        - Negative class sequence without the motif

    Parameters
    ----------
    motif_name : str
        encode motif name
    seq_length : int
        length of sequence
    num_pos : int
        number of positive class sequences
    num_neg : int
        number of negative class sequences
    GC_fraction : float
        GC fraction in background sequence

    Returns
    -------
    sequence_arr : 1darray
        Array with sequence strings.
    y : 1darray
        Array with positive/negative class labels.
    embedding_arr: 1darray
        Array of embedding objects.
    """
    motif_sequence_arr, positive_embedding_arr = simple_motif_embedding(
        motif_name, seq_length, num_pos, GC_fraction)
    random_sequence_arr, negative_embedding_arr = simple_motif_embedding(
        None, seq_length, num_neg, GC_fraction)
    sequence_arr = np.concatenate((motif_sequence_arr, random_sequence_arr))
    y = np.array([[True]] * num_pos + [[False]] * num_neg)
    embedding_arr = positive_embedding_arr + negative_embedding_arr
    return sequence_arr, y, embedding_arr


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1816')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/dnasim.py: 107-146
</a>
<div class="mid" id="frag1816" style="display:none"><pre>
def simulate_single_motif_detection(motif_name, seq_length, num_pos, num_neg,
                                    GC_fraction):
  """
    Simulates two classes of seqeuences:
        - Positive class sequence with a motif
          embedded anywhere in the sequence
        - Negative class sequence without the motif

    Parameters
    ----------
    motif_name : str
        encode motif name
    seq_length : int
        length of sequence
    num_pos : int
        number of positive class sequences
    num_neg : int
        number of negative class sequences
    GC_fraction : float
        GC fraction in background sequence

    Returns
    -------
    sequence_arr : 1darray
        Array with sequence strings.
    y : 1darray
        Array with positive/negative class labels.
    embedding_arr: 1darray
        Array of embedding objects.
    """
  motif_sequence_arr, positive_embedding_arr = simple_motif_embedding(
      motif_name, seq_length, num_pos, GC_fraction)
  random_sequence_arr, negative_embedding_arr = simple_motif_embedding(
      None, seq_length, num_neg, GC_fraction)
  sequence_arr = np.concatenate((motif_sequence_arr, random_sequence_arr))
  y = np.array([[True]] * num_pos + [[False]] * num_neg)
  embedding_arr = positive_embedding_arr + negative_embedding_arr
  return sequence_arr, y, embedding_arr


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 10:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 87%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag142')" href="javascript:;">
deepchem-2.4.0/contrib/dragonn/utils.py: 59-96
</a>
<div class="mid" id="frag142" style="display:none"><pre>
def get_pssm_scores(encoded_sequences, pssm):
  """
  Convolves pssm and its reverse complement with encoded sequences
  and returns the maximum score at each position of each sequence.

  Parameters
  ----------
  encoded_sequences: 3darray
        (num_examples, 1, 4, seq_length) array
  pssm: 2darray
      (4, pssm_length) array

  Returns
  -------
  scores: 2darray
      (num_examples, seq_length) array
  """
  encoded_sequences = encoded_sequences.squeeze(axis=1)
  # initialize fwd and reverse scores to -infinity
  fwd_scores = np.full_like(encoded_sequences, -np.inf, float)
  rc_scores = np.full_like(encoded_sequences, -np.inf, float)
  # cross-correlate separately for each base,
  # for both the PSSM and its reverse complement
  for base_indx in range(encoded_sequences.shape[1]):
    base_pssm = pssm[base_indx][None]
    base_pssm_rc = base_pssm[:, ::-1]
    fwd_scores[:, base_indx, :] = correlate2d(
        encoded_sequences[:, base_indx, :], base_pssm, mode='same')
    rc_scores[:, base_indx, :] = correlate2d(
        encoded_sequences[:, -(base_indx + 1), :], base_pssm_rc, mode='same')
  # sum over the bases
  fwd_scores = fwd_scores.sum(axis=1)
  rc_scores = rc_scores.sum(axis=1)
  # take max of fwd and reverse scores at each position
  scores = np.maximum(fwd_scores, rc_scores)
  return scores


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1870')" href="javascript:;">
deepchem-2.4.0/deepchem/metrics/genomic_metrics.py: 78-116
</a>
<div class="mid" id="frag1870" style="display:none"><pre>
def get_pssm_scores(encoded_sequences: np.ndarray,
                    pssm: np.ndarray) -&gt; np.ndarray:
  """
  Convolves pssm and its reverse complement with encoded sequences
  and returns the maximum score at each position of each sequence.

  Parameters
  ----------
  encoded_sequences: np.ndarray
    A numpy array of shape `(N_sequences, N_letters, sequence_length, 1)`.
  pssm: np.ndarray
    A numpy array of shape `(4, pssm_length)`.

  Returns
  -------
  scores: np.ndarray
    A numpy array of shape `(N_sequences, sequence_length)`.
  """
  encoded_sequences = encoded_sequences.squeeze(axis=3)
  # initialize fwd and reverse scores to -infinity
  fwd_scores = np.full_like(encoded_sequences, -np.inf, float)
  rc_scores = np.full_like(encoded_sequences, -np.inf, float)
  # cross-correlate separately for each base,
  # for both the PSSM and its reverse complement
  for base_indx in range(encoded_sequences.shape[1]):
    base_pssm = pssm[base_indx][None]
    base_pssm_rc = base_pssm[:, ::-1]
    fwd_scores[:, base_indx, :] = correlate2d(
        encoded_sequences[:, base_indx, :], base_pssm, mode='same')
    rc_scores[:, base_indx, :] = correlate2d(
        encoded_sequences[:, -(base_indx + 1), :], base_pssm_rc, mode='same')
  # sum over the bases
  fwd_scores = fwd_scores.sum(axis=1)
  rc_scores = rc_scores.sum(axis=1)
  # take max of fwd and reverse scores at each position
  scores = np.maximum(fwd_scores, rc_scores)
  return scores


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 11:</b> &nbsp; 2 fragments, nominal size 40 lines, similarity 97%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag162')" href="javascript:;">
deepchem-2.4.0/contrib/one_shot_models/multitask_classifier.py: 73-129
</a>
<div class="mid" id="frag162" style="display:none"><pre>
  def __init__(self,
               model,
               n_tasks,
               n_feat,
               logdir=None,
               batch_size=50,
               final_loss='cross_entropy',
               learning_rate=.001,
               optimizer_type="adam",
               learning_rate_decay_time=1000,
               beta1=.9,
               beta2=.999,
               pad_batches=True,
               verbose=True):

    warnings.warn(
        "MultitaskGraphClassifier is deprecated. "
        "Will be removed in DeepChem 1.4.", DeprecationWarning)
    super(MultitaskGraphClassifier, self).__init__(
        model_dir=logdir, verbose=verbose)
    self.n_tasks = n_tasks
    self.final_loss = final_loss
    self.model = model
    self.sess = tf.Session(graph=self.model.graph)

    with self.model.graph.as_default():
      # Extract model info
      self.batch_size = batch_size
      self.pad_batches = pad_batches
      # Get graph topology for x
      self.graph_topology = self.model.get_graph_topology()
      self.feat_dim = n_feat

      # Raw logit outputs
      self.logits = self.build()
      self.loss_op = self.add_training_loss(self.final_loss, self.logits)
      self.outputs = self.add_softmax(self.logits)

      self.learning_rate = learning_rate
      self.T = learning_rate_decay_time
      self.optimizer_type = optimizer_type

      self.optimizer_beta1 = beta1
      self.optimizer_beta2 = beta2

      # Set epsilon
      self.epsilon = 1e-7
      self.add_optimizer()

      # Initialize
      self.init_fn = tf.global_variables_initializer()
      self.sess.run(self.init_fn)

      # Path to save checkpoint files, which matches the
      # replicated supervisor's default path.
      self._save_path = os.path.join(self.model_dir, 'model.ckpt')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag205')" href="javascript:;">
deepchem-2.4.0/contrib/one_shot_models/multitask_regressor.py: 32-88
</a>
<div class="mid" id="frag205" style="display:none"><pre>
  def __init__(self,
               model,
               n_tasks,
               n_feat,
               logdir=None,
               batch_size=50,
               final_loss='weighted_L2',
               learning_rate=.001,
               optimizer_type="adam",
               learning_rate_decay_time=1000,
               beta1=.9,
               beta2=.999,
               pad_batches=True,
               verbose=True):

    warnings.warn(
        "MultitaskGraphRegressor is deprecated. "
        "Will be removed in DeepChem 1.4.", DeprecationWarning)

    super(MultitaskGraphRegressor, self).__init__(
        model_dir=logdir, verbose=verbose)
    self.n_tasks = n_tasks
    self.final_loss = final_loss
    self.model = model
    self.sess = tf.Session(graph=self.model.graph)

    with self.model.graph.as_default():
      # Extract model info
      self.batch_size = batch_size
      self.pad_batches = pad_batches
      # Get graph topology for x
      self.graph_topology = self.model.get_graph_topology()
      self.feat_dim = n_feat

      # Building outputs
      self.outputs = self.build()
      self.loss_op = self.add_training_loss(self.final_loss, self.outputs)

      self.learning_rate = learning_rate
      self.T = learning_rate_decay_time
      self.optimizer_type = optimizer_type

      self.optimizer_beta1 = beta1
      self.optimizer_beta2 = beta2

      # Set epsilon
      self.epsilon = 1e-7
      self.add_optimizer()

      # Initialize
      self.init_fn = tf.global_variables_initializer()
      self.sess.run(self.init_fn)

      # Path to save checkpoint files, which matches the
      # replicated supervisor's default path.
      self._save_path = os.path.join(self.model_dir, 'model.ckpt')

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 12:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag164')" href="javascript:;">
deepchem-2.4.0/contrib/one_shot_models/multitask_classifier.py: 146-158
</a>
<div class="mid" id="frag164" style="display:none"><pre>
  def add_optimizer(self):
    if self.optimizer_type == "adam":
      self.optimizer = tf.train.AdamOptimizer(
          self.learning_rate,
          beta1=self.optimizer_beta1,
          beta2=self.optimizer_beta2,
          epsilon=self.epsilon)
    else:
      raise ValueError("Optimizer type not recognized.")

    # Get train function
    self.train_op = self.optimizer.minimize(self.loss_op)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag207')" href="javascript:;">
deepchem-2.4.0/contrib/one_shot_models/multitask_regressor.py: 110-122
</a>
<div class="mid" id="frag207" style="display:none"><pre>
  def add_optimizer(self):
    if self.optimizer_type == "adam":
      self.optimizer = tf.train.AdamOptimizer(
          self.learning_rate,
          beta1=self.optimizer_beta1,
          beta2=self.optimizer_beta2,
          epsilon=self.epsilon)
    else:
      raise ValueError("Optimizer type not recognized.")

    # Get train function
    self.train_op = self.optimizer.minimize(self.loss_op)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 13:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag165')" href="javascript:;">
deepchem-2.4.0/contrib/one_shot_models/multitask_classifier.py: 159-180
</a>
<div class="mid" id="frag165" style="display:none"><pre>
  def construct_feed_dict(self, X_b, y_b=None, w_b=None, training=True):
    """Get initial information about task normalization"""
    # TODO(rbharath): I believe this is total amount of data
    n_samples = len(X_b)
    if y_b is None:
      y_b = np.zeros((n_samples, self.n_tasks))
    if w_b is None:
      w_b = np.zeros((n_samples, self.n_tasks))
    targets_dict = {self.label_placeholder: y_b, self.weight_placeholder: w_b}

    # Get graph information
    atoms_dict = self.graph_topology.batch_to_feed_dict(X_b)

    # TODO (hraut-&gt;rhbarath): num_datapoints should be a vector, with ith element being
    # the number of labeled data points in target_i. This is to normalize each task
    # num_dat_dict = {self.num_datapoints_placeholder : self.}

    # Get other optimizer information
    # TODO(rbharath): Figure out how to handle phase appropriately
    feed_dict = merge_dicts([targets_dict, atoms_dict])
    return feed_dict

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag208')" href="javascript:;">
deepchem-2.4.0/contrib/one_shot_models/multitask_regressor.py: 123-144
</a>
<div class="mid" id="frag208" style="display:none"><pre>
  def construct_feed_dict(self, X_b, y_b=None, w_b=None, training=True):
    """Get initial information about task normalization"""
    # TODO(rbharath): I believe this is total amount of data
    n_samples = len(X_b)
    if y_b is None:
      y_b = np.zeros((n_samples, self.n_tasks))
    if w_b is None:
      w_b = np.zeros((n_samples, self.n_tasks))
    targets_dict = {self.label_placeholder: y_b, self.weight_placeholder: w_b}

    # Get graph information
    atoms_dict = self.graph_topology.batch_to_feed_dict(X_b)

    # TODO (hraut-&gt;rhbarath): num_datapoints should be a vector, with ith element being
    # the number of labeled data points in target_i. This is to normalize each task
    # num_dat_dict = {self.num_datapoints_placeholder : self.}

    # Get other optimizer information
    # TODO(rbharath): Figure out how to handle phase appropriately
    feed_dict = merge_dicts([targets_dict, atoms_dict])
    return feed_dict

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 14:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 77%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag166')" href="javascript:;">
deepchem-2.4.0/contrib/one_shot_models/multitask_classifier.py: 181-207
</a>
<div class="mid" id="frag166" style="display:none"><pre>
  def add_training_loss(self, final_loss, logits):
    """Computes loss using logits."""
    loss_fn = get_loss_fn(final_loss)  # Get loss function
    task_losses = []
    # label_placeholder of shape (batch_size, n_tasks). Split into n_tasks
    # tensors of shape (batch_size,)
    task_labels = tf.split(
        axis=1, num_or_size_splits=self.n_tasks, value=self.label_placeholder)
    task_weights = tf.split(
        axis=1, num_or_size_splits=self.n_tasks, value=self.weight_placeholder)
    for task in range(self.n_tasks):
      task_label_vector = task_labels[task]
      task_weight_vector = task_weights[task]
      # Convert the labels into one-hot vector encodings.
      one_hot_labels = tf.cast(
          tf.one_hot(tf.cast(tf.squeeze(task_label_vector), tf.int32), 2),
          tf.float32)
      # Since we use tf.nn.softmax_cross_entropy_with_logits note that we pass in
      # un-softmaxed logits rather than softmax outputs.
      task_loss = loss_fn(logits[task], one_hot_labels, task_weight_vector)
      task_losses.append(task_loss)
    # It's ok to divide by just the batch_size rather than the number of nonzero
    # examples (effect averages out)
    total_loss = tf.add_n(task_losses)
    total_loss = tf.math.divide(total_loss, self.batch_size)
    return total_loss

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag209')" href="javascript:;">
deepchem-2.4.0/contrib/one_shot_models/multitask_regressor.py: 145-166
</a>
<div class="mid" id="frag209" style="display:none"><pre>
  def add_training_loss(self, final_loss, outputs):
    """Computes loss using logits."""
    loss_fn = get_loss_fn(final_loss)  # Get loss function
    task_losses = []
    # label_placeholder of shape (batch_size, n_tasks). Split into n_tasks
    # tensors of shape (batch_size,)
    task_labels = tf.split(
        axis=1, num_or_size_splits=self.n_tasks, value=self.label_placeholder)
    task_weights = tf.split(
        axis=1, num_or_size_splits=self.n_tasks, value=self.weight_placeholder)
    for task in range(self.n_tasks):
      task_label_vector = task_labels[task]
      task_weight_vector = task_weights[task]
      task_loss = loss_fn(outputs[task], tf.squeeze(task_label_vector),
                          tf.squeeze(task_weight_vector))
      task_losses.append(task_loss)
    # It's ok to divide by just the batch_size rather than the number of nonzero
    # examples (effect averages out)
    total_loss = tf.add_n(task_losses)
    total_loss = tf.math.divide(total_loss, self.batch_size)
    return total_loss

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 15:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag168')" href="javascript:;">
deepchem-2.4.0/contrib/one_shot_models/multitask_classifier.py: 216-235
</a>
<div class="mid" id="frag168" style="display:none"><pre>
  def fit(self,
          dataset,
          nb_epoch=10,
          max_checkpoints_to_keep=5,
          log_every_N_batches=50,
          checkpoint_interval=10,
          **kwargs):
    # Perform the optimization
    log("Training for %d epochs" % nb_epoch, self.verbose)

    # TODO(rbharath): Disabling saving for now to try to debug.
    for epoch in range(nb_epoch):
      log("Starting epoch %d" % epoch, self.verbose)
      for batch_num, (X_b, y_b, w_b, ids_b) in enumerate(
          dataset.iterbatches(self.batch_size, pad_batches=self.pad_batches)):
        if batch_num % log_every_N_batches == 0:
          log("On batch %d" % batch_num, self.verbose)
        self.sess.run(
            self.train_op, feed_dict=self.construct_feed_dict(X_b, y_b, w_b))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag210')" href="javascript:;">
deepchem-2.4.0/contrib/one_shot_models/multitask_regressor.py: 167-186
</a>
<div class="mid" id="frag210" style="display:none"><pre>
  def fit(self,
          dataset,
          nb_epoch=10,
          max_checkpoints_to_keep=5,
          log_every_N_batches=50,
          checkpoint_interval=10,
          **kwargs):
    # Perform the optimization
    log("Training for %d epochs" % nb_epoch, self.verbose)

    # TODO(rbharath): Disabling saving for now to try to debug.
    for epoch in range(nb_epoch):
      log("Starting epoch %d" % epoch, self.verbose)
      for batch_num, (X_b, y_b, w_b, ids_b) in enumerate(
          dataset.iterbatches(self.batch_size, pad_batches=self.pad_batches)):
        if batch_num % log_every_N_batches == 0:
          log("On batch %d" % batch_num, self.verbose)
        self.sess.run(
            self.train_op, feed_dict=self.construct_feed_dict(X_b, y_b, w_b))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 16:</b> &nbsp; 3 fragments, nominal size 12 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag172')" href="javascript:;">
deepchem-2.4.0/contrib/one_shot_models/multitask_classifier.py: 252-269
</a>
<div class="mid" id="frag172" style="display:none"><pre>
  def predict_on_batch(self, X):
    """Return model output for the provided input.
    """
    if self.pad_batches:
      X = pad_features(self.batch_size, X)
    # run eval data through the model
    n_tasks = self.n_tasks
    with self.sess.as_default():
      feed_dict = self.construct_feed_dict(X)
      # Shape (n_samples, n_tasks)
      batch_outputs = self.sess.run(self.outputs, feed_dict=feed_dict)

    n_samples = len(X)
    outputs = np.zeros((n_samples, self.n_tasks))
    for task, output in enumerate(batch_outputs):
      outputs[:, task] = np.argmax(output, axis=1)
    return outputs

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag213')" href="javascript:;">
deepchem-2.4.0/contrib/one_shot_models/multitask_regressor.py: 198-215
</a>
<div class="mid" id="frag213" style="display:none"><pre>
  def predict_on_batch(self, X):
    """Return model output for the provided input.
    """
    if self.pad_batches:
      X = pad_features(self.batch_size, X)
    # run eval data through the model
    n_tasks = self.n_tasks
    with self.sess.as_default():
      feed_dict = self.construct_feed_dict(X)
      # Shape (n_samples, n_tasks)
      batch_outputs = self.sess.run(self.outputs, feed_dict=feed_dict)

    n_samples = len(X)
    outputs = np.zeros((n_samples, self.n_tasks))
    for task, output in enumerate(batch_outputs):
      outputs[:, task] = output
    return outputs

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag173')" href="javascript:;">
deepchem-2.4.0/contrib/one_shot_models/multitask_classifier.py: 270-285
</a>
<div class="mid" id="frag173" style="display:none"><pre>
  def predict_proba_on_batch(self, X, n_classes=2):
    """Returns class probabilities on batch"""
    # run eval data through the model
    if self.pad_batches:
      X = pad_features(self.batch_size, X)
    n_tasks = self.n_tasks
    with self.sess.as_default():
      feed_dict = self.construct_feed_dict(X)
      batch_outputs = self.sess.run(self.outputs, feed_dict=feed_dict)

    n_samples = len(X)
    outputs = np.zeros((n_samples, self.n_tasks, n_classes))
    for task, output in enumerate(batch_outputs):
      outputs[:, task, :] = output
    return outputs

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 17:</b> &nbsp; 2 fragments, nominal size 31 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag193')" href="javascript:;">
deepchem-2.4.0/contrib/one_shot_models/graph_topology.py: 389-435
</a>
<div class="mid" id="frag193" style="display:none"><pre>
  def __init__(self,
               max_atoms=50,
               n_atom_feat=75,
               n_pair_feat=14,
               name='Weave_topology'):
    """
    Parameters
    ----------
    max_atoms: int, optional
      maximum number of atoms in a molecule
    n_atom_feat: int, optional
      number of basic features of each atom
    n_pair_feat: int, optional
      number of basic features of each pair
    """
    warnings.warn("WeaveGraphTopology is deprecated. "
                  "Will be removed in DeepChem 1.4.", DeprecationWarning)

    #self.n_atoms = n_atoms
    self.name = name
    self.max_atoms = max_atoms
    self.n_atom_feat = n_atom_feat
    self.n_pair_feat = n_pair_feat

    self.atom_features_placeholder = tf.placeholder(
        dtype='float32',
        shape=(None, self.max_atoms, self.n_atom_feat),
        name=self.name + '_atom_features')
    self.atom_mask_placeholder = tf.placeholder(
        dtype='float32',
        shape=(None, self.max_atoms),
        name=self.name + '_atom_mask')
    self.pair_features_placeholder = tf.placeholder(
        dtype='float32',
        shape=(None, self.max_atoms, self.max_atoms, self.n_pair_feat),
        name=self.name + '_pair_features')
    self.pair_mask_placeholder = tf.placeholder(
        dtype='float32',
        shape=(None, self.max_atoms, self.max_atoms),
        name=self.name + '_pair_mask')
    self.membership_placeholder = tf.placeholder(
        dtype='int32', shape=(None,), name=self.name + '_membership')
    # Define the list of tensors to be used as topology
    self.topology = [self.atom_mask_placeholder, self.pair_mask_placeholder]
    self.inputs = [self.atom_features_placeholder]
    self.inputs += self.topology

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag196')" href="javascript:;">
deepchem-2.4.0/contrib/one_shot_models/graph_topology.py: 493-543
</a>
<div class="mid" id="frag196" style="display:none"><pre>

  def __init__(self,
               batch_size,
               max_atoms=50,
               n_atom_feat=75,
               n_pair_feat=14,
               name='Weave_topology'):
    """
    Parameters
    ----------
    batch_size: int
      number of molecules in a batch
    max_atoms: int, optional
      maximum number of atoms in a molecule
    n_atom_feat: int, optional
      number of basic features of each atom
    n_pair_feat: int, optional
      number of basic features of each pair
    """
    warnings.warn("AlternateWeaveGraphTopology is deprecated. "
                  "Will be removed in DeepChem 1.4.", DeprecationWarning)

    #self.n_atoms = n_atoms
    self.name = name
    self.batch_size = batch_size
    self.max_atoms = max_atoms * batch_size
    self.n_atom_feat = n_atom_feat
    self.n_pair_feat = n_pair_feat

    self.atom_features_placeholder = tf.placeholder(
        dtype='float32',
        shape=(None, self.n_atom_feat),
        name=self.name + '_atom_features')
    self.pair_features_placeholder = tf.placeholder(
        dtype='float32',
        shape=(None, self.n_pair_feat),
        name=self.name + '_pair_features')
    self.pair_split_placeholder = tf.placeholder(
        dtype='int32', shape=(None,), name=self.name + '_pair_split')
    self.atom_split_placeholder = tf.placeholder(
        dtype='int32', shape=(None,), name=self.name + '_atom_split')
    self.atom_to_pair_placeholder = tf.placeholder(
        dtype='int32', shape=(None, 2), name=self.name + '_atom_to_pair')

    # Define the list of tensors to be used as topology
    self.topology = [
        self.pair_split_placeholder, self.atom_split_placeholder,
        self.atom_to_pair_placeholder
    ]
    self.inputs = [self.atom_features_placeholder]
    self.inputs += self.topology
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 18:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag203')" href="javascript:;">
deepchem-2.4.0/contrib/one_shot_models/tests/test_graph_models.py: 57-84
</a>
<div class="mid" id="frag203" style="display:none"><pre>
  def test_sample_attn_lstm_architecture(self):
    """Tests that an attention architecture can be created without crash."""
    max_depth = 5
    n_test = 5
    n_support = 11
    n_feat = 10
    batch_size = 3

    support_model = SequentialSupportGraph(n_feat)

    # Add layers
    support_model.add(dc.nn.GraphConv(64, n_feat, activation='relu'))
    # Need to add batch-norm separately to test/support due to differing
    # shapes.
    support_model.add_test(dc.nn.BatchNormalization(epsilon=1e-5, mode=1))
    support_model.add_support(dc.nn.BatchNormalization(epsilon=1e-5, mode=1))
    support_model.add(dc.nn.GraphPool())

    # Apply an attention lstm layer
    support_model.join(
        dc.nn.AttnLSTMEmbedding(n_test, n_support, 64, max_depth))

    # Gather Projection
    support_model.add(dc.nn.Dense(128, 64))
    support_model.add_test(dc.nn.BatchNormalization(epsilon=1e-5, mode=1))
    support_model.add_support(dc.nn.BatchNormalization(epsilon=1e-5, mode=1))
    support_model.add(dc.nn.GraphGather(batch_size, activation="tanh"))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag204')" href="javascript:;">
deepchem-2.4.0/contrib/one_shot_models/tests/test_graph_models.py: 85-111
</a>
<div class="mid" id="frag204" style="display:none"><pre>
  def test_sample_resi_lstm_architecture(self):
    """Tests that an attention architecture can be created without crash."""
    max_depth = 5
    n_test = 5
    n_support = 11
    n_feat = 10
    batch_size = 3

    support_model = SequentialSupportGraph(n_feat)

    # Add layers
    support_model.add(dc.nn.GraphConv(64, n_feat, activation='relu'))
    # Need to add batch-norm separately to test/support due to differing
    # shapes.
    support_model.add_test(dc.nn.BatchNormalization(epsilon=1e-5, mode=1))
    support_model.add_support(dc.nn.BatchNormalization(epsilon=1e-5, mode=1))
    support_model.add(dc.nn.GraphPool())

    # Apply an attention lstm layer
    support_model.join(
        dc.nn.ResiLSTMEmbedding(n_test, n_support, 64, max_depth))

    # Gather Projection
    support_model.add(dc.nn.Dense(128, 64))
    support_model.add_test(dc.nn.BatchNormalization(epsilon=1e-5, mode=1))
    support_model.add_support(dc.nn.BatchNormalization(epsilon=1e-5, mode=1))
    support_model.add(dc.nn.GraphGather(batch_size, activation="tanh"))
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 19:</b> &nbsp; 4 fragments, nominal size 11 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag224')" href="javascript:;">
deepchem-2.4.0/contrib/one_shot_models/graph_models.py: 103-116
</a>
<div class="mid" id="frag224" style="display:none"><pre>

  def add(self, layer):
    """Adds a new layer to model."""
    with self.graph.as_default():
      if type(layer).__name__ in ['DTNNStep']:
        self.output = layer([self.output] +
                            self.graph_topology.get_topology_placeholders())
      elif type(layer).__name__ in ['DTNNGather']:
        self.output = layer(
            [self.output, self.graph_topology.atom_membership_placeholder])
      else:
        self.output = layer(self.output)
      self.layers.append(layer)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag226')" href="javascript:;">
deepchem-2.4.0/contrib/one_shot_models/graph_models.py: 139-152
</a>
<div class="mid" id="frag226" style="display:none"><pre>

  def add(self, layer):
    """Adds a new layer to model."""
    with self.graph.as_default():
      if type(layer).__name__ in ['DAGLayer']:
        self.output = layer([self.output] +
                            self.graph_topology.get_topology_placeholders())
      elif type(layer).__name__ in ['DAGGather']:
        self.output = layer(
            [self.output, self.graph_topology.membership_placeholder])
      else:
        self.output = layer(self.output)
      self.layers.append(layer)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag230')" href="javascript:;">
deepchem-2.4.0/contrib/one_shot_models/graph_models.py: 230-244
</a>
<div class="mid" id="frag230" style="display:none"><pre>

  def add(self, layer):
    """Adds a new layer to model."""
    with self.graph.as_default():
      if type(layer).__name__ in ['AlternateWeaveLayer']:
        self.output, self.output_P = layer([
            self.output, self.output_P
        ] + self.graph_topology.get_topology_placeholders())
      elif type(layer).__name__ in ['AlternateWeaveGather']:
        self.output = layer(
            [self.output, self.graph_topology.atom_split_placeholder])
      else:
        self.output = layer(self.output)
      self.layers.append(layer)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag228')" href="javascript:;">
deepchem-2.4.0/contrib/one_shot_models/graph_models.py: 181-198
</a>
<div class="mid" id="frag228" style="display:none"><pre>

  def add(self, layer):
    """Adds a new layer to model."""
    with self.graph.as_default():
      if type(layer).__name__ in ['WeaveLayer']:
        self.output, self.output_P = layer([
            self.output, self.output_P
        ] + self.graph_topology.get_topology_placeholders())
      elif type(layer).__name__ in ['WeaveConcat']:
        self.output = layer(
            [self.output, self.graph_topology.atom_mask_placeholder])
      elif type(layer).__name__ in ['WeaveGather']:
        self.output = layer(
            [self.output, self.graph_topology.membership_placeholder])
      else:
        self.output = layer(self.output)
      self.layers.append(layer)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 20:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag257')" href="javascript:;">
deepchem-2.4.0/contrib/tensorflow_models/test_utils.py: 87-102
</a>
<div class="mid" id="frag257" style="display:none"><pre>
  def testMeanWithMask(self):
    self.Check(
        utils.Mean, features=[[9999], [1], [2]], expected=1.5, mask=[0, 1, 1])
    self.Check(
        utils.Mean,
        features=[[0, 1], [9999, 9999]],
        expected=[0, 1],
        axis=0,
        mask=[1, 0])
    self.Check(
        utils.Mean,
        features=[[[0, 1], [9999, 9999]], [[9999, 9999], [6, 7]]],
        expected=[0.5, 6.5],
        axis=[0, 2],
        mask=[[1, 0], [0, 1]])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag259')" href="javascript:;">
deepchem-2.4.0/contrib/tensorflow_models/test_utils.py: 113-128
</a>
<div class="mid" id="frag259" style="display:none"><pre>
  def testVarianceWithMask(self):
    self.Check(
        utils.Variance, features=[[0], [1], [2]], expected=0.25, mask=[0, 1, 1])
    self.Check(
        utils.Variance,
        features=[[0, 2], [9999, 9999], [4, 4]],
        expected=[4, 1],
        axis=0,
        mask=[1, 0, 1])
    self.Check(
        utils.Variance,
        features=[[[0, 1], [9999, 9999]], [[9999, 9999], [6, 8]]],
        expected=[0.25, 1],
        axis=[0, 2],
        mask=[[1, 0], [0, 1]])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 21:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag261')" href="javascript:;">
deepchem-2.4.0/contrib/tensorflow_models/test_utils.py: 170-184
</a>
<div class="mid" id="frag261" style="display:none"><pre>
  def testSkewness(self):
    with self.session() as sess:
      features = np.random.random((3, 4, 5))
      features_t = tf.constant(features, dtype=tf.float32)
      self.assertAllClose(
          sess.run(utils.Skewness(features_t)),
          scipy.stats.skew(features, axis=None),
          rtol=1e-5,
          atol=1e-5)
      self.assertAllClose(
          sess.run(utils.Skewness(features_t, 1)),
          scipy.stats.skew(features, axis=1),
          rtol=1e-5,
          atol=1e-5)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag262')" href="javascript:;">
deepchem-2.4.0/contrib/tensorflow_models/test_utils.py: 185-200
</a>
<div class="mid" id="frag262" style="display:none"><pre>
  def testKurtosis(self):
    with self.session() as sess:
      features = np.random.random((3, 4, 5))
      features_t = tf.constant(features, dtype=tf.float32)
      self.assertAllClose(
          sess.run(utils.Kurtosis(features_t)),
          scipy.stats.kurtosis(features, axis=None),
          rtol=1e-5,
          atol=1e-5)
      self.assertAllClose(
          sess.run(utils.Kurtosis(features_t, 1)),
          scipy.stats.kurtosis(features, axis=1),
          rtol=1e-5,
          atol=1e-5)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 22:</b> &nbsp; 3 fragments, nominal size 13 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag280')" href="javascript:;">
deepchem-2.4.0/contrib/tensorflow_models/__init__.py: 439-458
</a>
<div class="mid" id="frag280" style="display:none"><pre>
  def add_example_weight_placeholders(self, graph, name_scopes):
    """Add Placeholders for example weights for each task.

    This method creates the following Placeholders for each task:
      weights_%d: Label tensor with shape batch_size.

    Placeholders are wrapped in identity ops to avoid the error caused by
    feeding and fetching the same tensor.
    """
    weights = []
    placeholder_scope = TensorflowGraph.get_placeholder_scope(
        graph, name_scopes)
    with placeholder_scope:
      for task in range(self.n_tasks):
        weights.append(
            tf.identity(
                tf.placeholder(
                    tf.float32, shape=[None], name='weights_%d' % task)))
    return weights

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag296')" href="javascript:;">
deepchem-2.4.0/contrib/tensorflow_models/__init__.py: 798-819
</a>
<div class="mid" id="frag296" style="display:none"><pre>
  def add_label_placeholders(self, graph, name_scopes):
    """Add Placeholders for labels for each task.

    This method creates the following Placeholders for each task:
      labels_%d: Label tensor with shape batch_size.

    Placeholders are wrapped in identity ops to avoid the error caused by
    feeding and fetching the same tensor.
    """
    placeholder_scope = TensorflowGraph.get_placeholder_scope(
        graph, name_scopes)
    with graph.as_default():
      batch_size = self.batch_size
      labels = []
      with placeholder_scope:
        for task in range(self.n_tasks):
          labels.append(
              tf.identity(
                  tf.placeholder(
                      tf.float32, shape=[None], name='labels_%d' % task)))
    return labels

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag290')" href="javascript:;">
deepchem-2.4.0/contrib/tensorflow_models/__init__.py: 645-669
</a>
<div class="mid" id="frag290" style="display:none"><pre>
  def add_label_placeholders(self, graph, name_scopes):
    """Add Placeholders for labels for each task.

    This method creates the following Placeholders for each task:
      labels_%d: Label tensor with shape batch_size x n_classes.

    Placeholders are wrapped in identity ops to avoid the error caused by
    feeding and fetching the same tensor.
    """
    placeholder_scope = TensorflowGraph.get_placeholder_scope(
        graph, name_scopes)
    with graph.as_default():
      batch_size = self.batch_size
      n_classes = self.n_classes
      labels = []
      with placeholder_scope:
        for task in range(self.n_tasks):
          labels.append(
              tf.identity(
                  tf.placeholder(
                      tf.float32,
                      shape=[None, n_classes],
                      name='labels_%d' % task)))
      return labels

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 23:</b> &nbsp; 3 fragments, nominal size 15 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag298')" href="javascript:;">
deepchem-2.4.0/contrib/hagcn/hagcn_layers.py: 22-53
</a>
<div class="mid" id="frag298" style="display:none"><pre>
  def __init__(self,
               num_nodes,
               num_node_features,
               batch_size=64,
               init='glorot_uniform',
               combine_method='linear',
               **kwargs):
    """
      Parameters
      ----------
      num_nodes: int
        Number of nodes in the graph
      num_node_features: int
        Number of features per node in the graph
      batch_size: int, optional
        Batch size used for training
      init: str, optional
        Initialization method for the weights
      combine_method: str, optional
        How to combine adjacency matrix and node features

    """

    if combine_method not in ['linear', 'prod']:
      raise ValueError('Combine method needs to be one of linear or product')
    self.num_nodes = num_nodes
    self.num_node_features = num_node_features
    self.batch_size = batch_size
    self.init = initializations.get(init)
    self.combine_method = combine_method
    super(AdaptiveFilter, self).__init__(**kwargs)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag308')" href="javascript:;">
deepchem-2.4.0/contrib/hagcn/hagcn_model.py: 24-63
</a>
<div class="mid" id="frag308" style="display:none"><pre>
  def __init__(self,
               max_nodes,
               num_node_features,
               n_tasks=1,
               k_max=1,
               task_mode='graph',
               combine_method='linear',
               **kwargs):
    """
      Parameters
      ----------
      max_nodes: int
        Maximum number of nodes (atoms) graphs in dataset can have
      num_node_features: int
        Number of features per node
      atoms: list
        List of atoms available across train, valid, test
      k_max: int, optional
        Largest k-hop neighborhood per atom
      batch_size: int, optional
        Batch size used
      task_mode: str, optional
        Whether the model is used for node based tasks or edge based tasks or graph tasks
      combine_method: str, optional
        Combining the inputs for the AdaptiveFilterLayer
    """

    if task_mode not in ['graph', 'node', 'edge']:
      raise ValueError('task_mode must be one of graph, node, edge')

    self.k_max = k_max
    self.n_tasks = n_tasks
    self.max_nodes = max_nodes
    self.num_node_features = num_node_features
    self.task_mode = task_mode
    self.combine_method = combine_method
    super(HAGCN, self).__init__(**kwargs)

    self._build()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag303')" href="javascript:;">
deepchem-2.4.0/contrib/hagcn/hagcn_layers.py: 101-129
</a>
<div class="mid" id="frag303" style="display:none"><pre>
  def __init__(self,
               num_nodes,
               num_node_features,
               batch_size=64,
               init='glorot_uniform',
               **kwargs):
    """
      Parameters
      ----------
      num_nodes: int
        Number of nodes in the graph
      num_node_features: int
        Number of features per node in the graph
      batch_size: int, optional
        Batch size used for training
      init: str, optional
        Initialization method for the weights
      combine_method: str, optional
        How to combine adjacency matrix and node features

    """

    self.num_nodes = num_nodes
    self.num_node_features = num_node_features
    self.batch_size = batch_size
    self.init = initializations.get(init)

    super(KOrderGraphConv, self).__init__(**kwargs)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 24:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag322')" href="javascript:;">
deepchem-2.4.0/contrib/autoencoder_models/autoencoder.py: 21-54
</a>
<div class="mid" id="frag322" style="display:none"><pre>
  def __init__(self,
               model_dir=None,
               weights_file="model.h5",
               verbose=True,
               charset_length=len(zinc_charset),
               latent_rep_size=292):
    """

    Parameters
    ----------
    model_dir: str
      Folder to store cached weights
    weights_file: str
      File to store cached weights in model_dir
    verbose: bool
      True for more logging
    charset_length: int
      Length of one_hot_encoded vectors
    latent_rep_size: int
      How large a 1D Vector for latent representation
    """
    warnings.warn("TensorflowMoleculeEncoder Deprecated. "
                  "Will be removed in DeepChem 1.4.", DeprecationWarning)
    super(TensorflowMoleculeEncoder, self).__init__(
        model_dir=model_dir, verbose=verbose)
    weights_file = os.path.join(model_dir, weights_file)
    if os.path.isfile(weights_file):
      m = MoleculeVAE()
      m.load(charset_length, weights_file, latent_rep_size=latent_rep_size)
      self.model = m
    else:
      # TODO (LESWING) Lazy Load
      raise ValueError("Model file %s doesn't exist" % weights_file)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag326')" href="javascript:;">
deepchem-2.4.0/contrib/autoencoder_models/autoencoder.py: 90-123
</a>
<div class="mid" id="frag326" style="display:none"><pre>
  def __init__(self,
               model_dir=None,
               weights_file="model.h5",
               verbose=True,
               charset_length=len(zinc_charset),
               latent_rep_size=292):
    """

    Parameters
    ----------
    model_dir: str
      Folder to store cached weights
    weights_file: str
      File to store cached weights in model_dir
    verbose: bool
      True for more logging
    charset_length: int
      Length of one_hot_encoded vectors
    latent_rep_size: int
      How large a 1D Vector for latent representation
    """
    warnings.warn("TensorflowMoleculeDecoder Deprecated. "
                  "Will be removed in DeepChem 1.4.", DeprecationWarning)
    super(TensorflowMoleculeDecoder, self).__init__(
        model_dir=model_dir, verbose=verbose)
    weights_file = os.path.join(model_dir, weights_file)
    if os.path.isfile(weights_file):
      m = MoleculeVAE()
      m.load(charset_length, weights_file, latent_rep_size=latent_rep_size)
      self.model = m
    else:
      # TODO (LESWING) Lazy Load
      raise ValueError("Model file %s doesn't exist" % weights_file)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 25:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag323')" href="javascript:;">
deepchem-2.4.0/contrib/autoencoder_models/autoencoder.py: 56-74
</a>
<div class="mid" id="frag323" style="display:none"><pre>
  def zinc_encoder():
    """
    Returns
    -------
    obj
      An Encoder with weights that were trained on the zinc dataset
    """
    current_dir = os.path.dirname(os.path.realpath(__file__))
    weights_filename = "zinc_model.h5"
    weights_file = os.path.join(current_dir, weights_filename)

    if not os.path.exists(weights_file):
      download_url("http://karlleswing.com/misc/keras-molecule/model.h5",
                   current_dir)
      mv_cmd = "mv model.h5 %s" % weights_file
      call(mv_cmd.split())
    return TensorflowMoleculeEncoder(
        model_dir=current_dir, weights_file=weights_filename)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag328')" href="javascript:;">
deepchem-2.4.0/contrib/autoencoder_models/autoencoder.py: 128-146
</a>
<div class="mid" id="frag328" style="display:none"><pre>
  def zinc_decoder():
    """
    Returns
    -------
    obj
      A Decoder with weights that were trained on the zinc dataset
    """
    current_dir = os.path.dirname(os.path.realpath(__file__))
    weights_filename = "zinc_model.h5"
    weights_file = os.path.join(current_dir, weights_filename)

    if not os.path.exists(weights_file):
      download_url("http://karlleswing.com/misc/keras-molecule/model.h5",
                   current_dir)
      mv_cmd = "mv model.h5 %s" % weights_file
      call(mv_cmd.split())
    return TensorflowMoleculeDecoder(
        model_dir=current_dir, weights_file=weights_filename)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 26:</b> &nbsp; 3 fragments, nominal size 18 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag331')" href="javascript:;">
deepchem-2.4.0/contrib/atomicconv/splits/pdbbind_temporal_split.py: 8-42
</a>
<div class="mid" id="frag331" style="display:none"><pre>
def load_pdbbind_labels(labels_file):
  """Loads pdbbind labels as dataframe

  Parameters
  ----------
  labels_file: str
    Location of PDBbind datafile.

  Returns
  -------
  contents_df: pd.DataFrame
    Dataframe containing contents of PDBbind datafile.

  """

  contents = []
  with open(labels_file) as f:
    for line in f:
      if line.startswith("#"):
        continue
      else:
        splitline = line.split()
        if len(splitline) == 8:
          contents.append(splitline)
        else:
          print("Incorrect data format")
          print(splitline)

  contents_df = pd.DataFrame(
      contents,
      columns=("PDB code", "resolution", "release year", "-logKd/Ki", "Kd/Ki",
               "ignore-this-field", "reference", "ligand name"))
  return contents_df


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1981')" href="javascript:;">
deepchem-2.4.0/examples/binding_pockets/binding_pocket_datasets.py: 63-87
</a>
<div class="mid" id="frag1981" style="display:none"><pre>
def load_pdbbind_labels(labels_file):
  """Loads pdbbind labels as dataframe"""
  # Some complexes have labels but no PDB files. Filter these manually
  missing_pdbs = ["1d2v", "1jou", "1s8j", "1cam", "4mlt", "4o7d"]
  contents = []
  with open(labels_file) as f:
    for line in f:
      if line.startswith("#"):
        continue
      else:
        # Some of the ligand-names are of form (FMN ox). Use regex
        # to merge into form (FMN-ox)
        p = re.compile('\(([^\)\s]*) ([^\)\s]*)\)')
        line = p.sub('(\\1-\\2)', line)
        elts = line.split()
        # Filter if missing PDB files
        if elts[0] in missing_pdbs:
          continue
        contents.append(elts)
  contents_df = pd.DataFrame(
      contents,
      columns=("PDB code", "resolution", "release year", "-logKd/Ki", "Kd/Ki",
               "ignore-this-field", "reference", "ligand name"))
  return contents_df

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag343')" href="javascript:;">
deepchem-2.4.0/contrib/atomicconv/feat/atomicnet_pdbbind_datasets.py: 15-49
</a>
<div class="mid" id="frag343" style="display:none"><pre>
def load_pdbbind_labels(labels_file):
  """Loads pdbbind labels as dataframe

  Parameters
  ----------
  labels_file: str
    Location of PDBbind datafile.

  Returns
  -------
  contents_df: pd.DataFrame
    Dataframe containing contents of PDBbind datafile.

  """

  contents = []
  with open(labels_file) as f:
    for line in f:
      if line.startswith("#"):
        continue
      else:
        splitline = line.split()
        if len(splitline) == 8:
          contents.append(splitline)
        else:
          print("Incorrect data format")
          print(splitline)

  contents_df = pd.DataFrame(
      contents,
      columns=("PDB code", "resolution", "release year", "-logKd/Ki", "Kd/Ki",
               "ignore-this-field", "reference", "ligand name"))
  return contents_df


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 27:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 84%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag333')" href="javascript:;">
deepchem-2.4.0/contrib/atomicconv/models/atomicnet_ops.py: 15-54
</a>
<div class="mid" id="frag333" style="display:none"><pre>
def InitializeWeightsBiases(prev_layer_size,
                            size,
                            weights=None,
                            biases=None,
                            name=None):
  """Initializes weights and biases to be used in a fully-connected layer.

  Parameters
  ----------
  prev_layer_size: int
    Number of features in previous layer.
  size: int 
    Number of nodes in this layer.
  weights: tf.Tensor, optional (Default None)
    Weight tensor.
  biases: tf.Tensor, optional (Default None)
    Bias tensor.
  name: str 
    Name for this op, optional (Defaults to 'fully_connected' if None)

  Returns
  -------
  weights: tf.Variable
    Initialized weights.
  biases: tf.Variable
    Initialized biases.

  """

  if weights is None:
    weights = tf.truncated_normal([prev_layer_size, size], stddev=0.01)
  if biases is None:
    biases = tf.zeros([size])

  with tf.name_scope(name, 'fully_connected', [weights, biases]):
    w = tf.Variable(weights, name='w')
    b = tf.Variable(biases, name='b')
  return w, b


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag554')" href="javascript:;">
deepchem-2.4.0/deepchem/models/atomic_conv.py: 17-55
</a>
<div class="mid" id="frag554" style="display:none"><pre>
def initializeWeightsBiases(prev_layer_size,
                            size,
                            weights=None,
                            biases=None,
                            name=None):
  """Initializes weights and biases to be used in a fully-connected layer.

  Parameters
  ----------
  prev_layer_size: int
    Number of features in previous layer.
  size: int
    Number of nodes in this layer.
  weights: tf.Tensor, optional (Default None)
    Weight tensor.
  biases: tf.Tensor, optional (Default None)
    Bias tensor.
  name: str
    Name for this op, optional (Defaults to 'fully_connected' if None)

  Returns
  -------
  weights: tf.Variable
    Initialized weights.
  biases: tf.Variable
    Initialized biases.

  """

  if weights is None:
    weights = tf.random.truncated_normal([prev_layer_size, size], stddev=0.01)
  if biases is None:
    biases = tf.zeros([size])

  w = tf.Variable(weights, name='w')
  b = tf.Variable(biases, name='b')
  return w, b


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 28:</b> &nbsp; 4 fragments, nominal size 14 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag374')" href="javascript:;">
deepchem-2.4.0/deepchem/splits/tests/test_task_splitter.py: 14-33
</a>
<div class="mid" id="frag374" style="display:none"><pre>
  def test_multitask_train_valid_test_split(self):
    """
    Test TaskSplitter train/valid/test split on multitask dataset.
    """
    n_samples = 100
    n_features = 10
    n_tasks = 10
    X = np.random.rand(n_samples, n_features)
    p = .05  # proportion actives
    y = np.random.binomial(1, p, size=(n_samples, n_tasks))
    dataset = dc.data.NumpyDataset(X, y)

    task_splitter = dc.splits.TaskSplitter()
    train, valid, test = task_splitter.train_valid_test_split(
        dataset, frac_train=.4, frac_valid=.3, frac_test=.3)

    assert len(train.get_task_names()) == 4
    assert len(valid.get_task_names()) == 3
    assert len(test.get_task_names()) == 3

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag377')" href="javascript:;">
deepchem-2.4.0/deepchem/splits/tests/test_task_splitter.py: 73-93
</a>
<div class="mid" id="frag377" style="display:none"><pre>
  def test_uneven_train_valid_test_split(self):
    """
    Test train/valid/test split works when proportions don't divide n_tasks.
    """
    n_samples = 100
    n_features = 10
    n_tasks = 11
    X = np.random.rand(n_samples, n_features)
    p = .05  # proportion actives
    y = np.random.binomial(1, p, size=(n_samples, n_tasks))
    dataset = dc.data.NumpyDataset(X, y)

    task_splitter = dc.splits.TaskSplitter()
    train, valid, test = task_splitter.train_valid_test_split(
        dataset, frac_train=.4, frac_valid=.3, frac_test=.3)

    assert len(train.get_task_names()) == 4
    assert len(valid.get_task_names()) == 3
    # Note that the extra task goes to test
    assert len(test.get_task_names()) == 4

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag375')" href="javascript:;">
deepchem-2.4.0/deepchem/splits/tests/test_task_splitter.py: 34-52
</a>
<div class="mid" id="frag375" style="display:none"><pre>
  def test_multitask_K_fold_split(self):
    """
    Test TaskSplitter K-fold split on multitask dataset.
    """
    n_samples = 100
    n_features = 10
    n_tasks = 10
    X = np.random.rand(n_samples, n_features)
    p = .05  # proportion actives
    y = np.random.binomial(1, p, size=(n_samples, n_tasks))
    dataset = dc.data.NumpyDataset(X, y)
    K = 5

    task_splitter = dc.splits.TaskSplitter()
    fold_datasets = task_splitter.k_fold_split(dataset, K)

    for fold_dataset in fold_datasets:
      assert len(fold_dataset.get_task_names()) == 2

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag376')" href="javascript:;">
deepchem-2.4.0/deepchem/splits/tests/test_task_splitter.py: 53-72
</a>
<div class="mid" id="frag376" style="display:none"><pre>
  def test_uneven_k_fold_split(self):
    """
    Test k-fold-split works when K does not divide n_tasks.
    """
    n_samples = 100
    n_features = 10
    n_tasks = 17
    X = np.random.rand(n_samples, n_features)
    p = .05  # proportion actives
    y = np.random.binomial(1, p, size=(n_samples, n_tasks))
    dataset = dc.data.NumpyDataset(X, y)
    K = 4
    task_splitter = dc.splits.TaskSplitter()
    fold_datasets = task_splitter.k_fold_split(dataset, K)

    for fold in range(K - 1):
      fold_dataset = fold_datasets[fold]
      assert len(fold_dataset.get_task_names()) == 4
    assert len(fold_datasets[-1].get_task_names()) == 5

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 29:</b> &nbsp; 3 fragments, nominal size 12 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag379')" href="javascript:;">
deepchem-2.4.0/deepchem/splits/tests/test_splitter.py: 12-26
</a>
<div class="mid" id="frag379" style="display:none"><pre>
def load_sparse_multitask_dataset():
  """Load sparse tox multitask data, sample dataset."""
  current_dir = os.path.dirname(os.path.abspath(__file__))
  featurizer = dc.feat.CircularFingerprint(size=1024)
  tasks = [
      "task1", "task2", "task3", "task4", "task5", "task6", "task7", "task8",
      "task9"
  ]
  input_file = os.path.join(current_dir,
                            "../../models/tests/sparse_multitask_example.csv")
  loader = dc.data.CSVLoader(
      tasks=tasks, feature_field="smiles", featurizer=featurizer)
  return loader.create_dataset(input_file)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag380')" href="javascript:;">
deepchem-2.4.0/deepchem/splits/tests/test_splitter.py: 27-42
</a>
<div class="mid" id="frag380" style="display:none"><pre>
def load_multitask_data():
  """Load example multitask data."""
  current_dir = os.path.dirname(os.path.abspath(__file__))
  featurizer = dc.feat.CircularFingerprint(size=1024)
  tasks = [
      "task0", "task1", "task2", "task3", "task4", "task5", "task6", "task7",
      "task8", "task9", "task10", "task11", "task12", "task13", "task14",
      "task15", "task16"
  ]
  input_file = os.path.join(current_dir,
                            "../../models/tests/multitask_example.csv")
  loader = dc.data.CSVLoader(
      tasks=tasks, feature_field="smiles", featurizer=featurizer)
  return loader.create_dataset(input_file)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1397')" href="javascript:;">
deepchem-2.4.0/deepchem/data/tests/test_datasets.py: 30-45
</a>
<div class="mid" id="frag1397" style="display:none"><pre>
def load_multitask_data():
  """Load example multitask data."""
  current_dir = os.path.dirname(os.path.abspath(__file__))
  featurizer = dc.feat.CircularFingerprint(size=1024)
  tasks = [
      "task0", "task1", "task2", "task3", "task4", "task5", "task6", "task7",
      "task8", "task9", "task10", "task11", "task12", "task13", "task14",
      "task15", "task16"
  ]
  input_file = os.path.join(current_dir,
                            "../../models/tests/multitask_example.csv")
  loader = dc.data.CSVLoader(
      tasks=tasks, feature_field="smiles", featurizer=featurizer)
  return loader.create_dataset(input_file)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 30:</b> &nbsp; 3 fragments, nominal size 11 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag384')" href="javascript:;">
deepchem-2.4.0/deepchem/splits/tests/test_splitter.py: 100-115
</a>
<div class="mid" id="frag384" style="display:none"><pre>
  def test_singletask_random_split(self):
    """
    Test singletask RandomSplitter class.
    """
    solubility_dataset = load_solubility_data()
    random_splitter = dc.splits.RandomSplitter()
    train_data, valid_data, test_data = \
      random_splitter.train_valid_test_split(
        solubility_dataset, frac_train=0.8, frac_valid=0.1, frac_test=0.1)
    assert len(train_data) == 8
    assert len(valid_data) == 1
    assert len(test_data) == 1

    merged_dataset = dc.data.DiskDataset.merge(
        [train_data, valid_data, test_data])
    assert sorted(merged_dataset.ids) == (sorted(solubility_dataset.ids))
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag385')" href="javascript:;">
deepchem-2.4.0/deepchem/splits/tests/test_splitter.py: 116-132
</a>
<div class="mid" id="frag385" style="display:none"><pre>

  def test_singletask_index_split(self):
    """
    Test singletask IndexSplitter class.
    """
    solubility_dataset = load_solubility_data()
    random_splitter = dc.splits.IndexSplitter()
    train_data, valid_data, test_data = \
      random_splitter.train_valid_test_split(
        solubility_dataset)
    assert len(train_data) == 8
    assert len(valid_data) == 1
    assert len(test_data) == 1

    merged_dataset = dc.data.DiskDataset.merge(
        [train_data, valid_data, test_data])
    assert sorted(merged_dataset.ids) == (sorted(solubility_dataset.ids))
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag388')" href="javascript:;">
deepchem-2.4.0/deepchem/splits/tests/test_splitter.py: 164-179
</a>
<div class="mid" id="frag388" style="display:none"><pre>
    s1 = set(train_data.ids)
    assert valid_data.ids[0] not in s1
    assert test_data.ids[0] not in s1

  def test_singletask_stratified_split(self):
    """
    Test singletask SingletaskStratifiedSplitter class.
    """
    solubility_dataset = load_solubility_data()
    stratified_splitter = dc.splits.ScaffoldSplitter()
    train_data, valid_data, test_data = \
      stratified_splitter.train_valid_test_split(
        solubility_dataset, frac_train=0.8, frac_valid=0.1, frac_test=0.1)
    assert len(train_data) == 8
    assert len(valid_data) == 1
    assert len(test_data) == 1
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 31:</b> &nbsp; 3 fragments, nominal size 19 lines, similarity 85%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag392')" href="javascript:;">
deepchem-2.4.0/deepchem/splits/tests/test_splitter.py: 222-246
</a>
<div class="mid" id="frag392" style="display:none"><pre>
      train, cv = fold_datasets[fold][0], fold_datasets[fold][1]
      self.assertTrue(cv.X[0] == fold)
      train_data = set(list(train.X))
      self.assertFalse(fold in train_data)
      self.assertEqual(K - 1, len(train))
      self.assertEqual(1, len(cv))

  def test_singletask_random_k_fold_split(self):
    """
    Test singletask RandomSplitter class.
    """
    solubility_dataset = load_solubility_data()
    random_splitter = dc.splits.RandomSplitter()
    ids_set = set(solubility_dataset.ids)

    K = 5
    fold_datasets = random_splitter.k_fold_split(solubility_dataset, K)
    for fold in range(K):
      fold_dataset = fold_datasets[fold][1]
      # Verify lengths is 10/k == 2
      assert len(fold_dataset) == 2
      # Verify that compounds in this fold are subset of original compounds
      fold_ids_set = set(fold_dataset.ids)
      assert fold_ids_set.issubset(ids_set)
      # Verify that no two folds have overlapping compounds.
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag394')" href="javascript:;">
deepchem-2.4.0/deepchem/splits/tests/test_splitter.py: 277-306
</a>
<div class="mid" id="frag394" style="display:none"><pre>
        other_fold_ids_set = set(other_fold_dataset.ids)
        assert fold_ids_set.isdisjoint(other_fold_ids_set)

    merged_dataset = dc.data.DiskDataset.merge([x[1] for x in fold_datasets])
    assert len(merged_dataset) == len(solubility_dataset)
    assert sorted(merged_dataset.ids) == (sorted(solubility_dataset.ids))

  def test_singletask_scaffold_k_fold_split(self):
    """
    Test singletask ScaffoldSplitter class.
    """
    solubility_dataset = load_solubility_data()
    scaffold_splitter = dc.splits.ScaffoldSplitter()
    ids_set = set(solubility_dataset.ids)

    K = 5
    fold_datasets = scaffold_splitter.k_fold_split(solubility_dataset, K)

    for fold in range(K):
      fold_dataset = fold_datasets[fold][1]
      # Verify lengths is 10/k == 2
      assert len(fold_dataset) == 2
      # Verify that compounds in this fold are subset of original compounds
      fold_ids_set = set(fold_dataset.ids)
      assert fold_ids_set.issubset(ids_set)
      # Verify that no two folds have overlapping compounds.
      for other_fold in range(K):
        if fold == other_fold:
          continue
        other_fold_dataset = fold_datasets[other_fold][1]
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag393')" href="javascript:;">
deepchem-2.4.0/deepchem/splits/tests/test_splitter.py: 247-276
</a>
<div class="mid" id="frag393" style="display:none"><pre>
      for other_fold in range(K):
        if fold == other_fold:
          continue
        other_fold_dataset = fold_datasets[other_fold][1]
        other_fold_ids_set = set(other_fold_dataset.ids)
        assert fold_ids_set.isdisjoint(other_fold_ids_set)

  def test_singletask_index_k_fold_split(self):
    """
    Test singletask IndexSplitter class.
    """
    solubility_dataset = load_solubility_data()
    index_splitter = dc.splits.IndexSplitter()
    ids_set = set(solubility_dataset.ids)

    K = 5
    fold_datasets = index_splitter.k_fold_split(solubility_dataset, K)

    for fold in range(K):
      fold_dataset = fold_datasets[fold][1]
      # Verify lengths is 10/k == 2
      assert len(fold_dataset) == 2
      # Verify that compounds in this fold are subset of original compounds
      fold_ids_set = set(fold_dataset.ids)
      assert fold_ids_set.issubset(ids_set)
      # Verify that no two folds have overlapping compounds.
      for other_fold in range(K):
        if fold == other_fold:
          continue
        other_fold_dataset = fold_datasets[other_fold][1]
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 32:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag395')" href="javascript:;">
deepchem-2.4.0/deepchem/splits/tests/test_splitter.py: 307-330
</a>
<div class="mid" id="frag395" style="display:none"><pre>
        other_fold_ids_set = set(other_fold_dataset.ids)
        assert fold_ids_set.isdisjoint(other_fold_ids_set)

    merged_dataset = dc.data.DiskDataset.merge([x[1] for x in fold_datasets])
    assert len(merged_dataset) == len(solubility_dataset)
    assert sorted(merged_dataset.ids) == (sorted(solubility_dataset.ids))

  def test_singletask_stratified_column_indices(self):
    """
    Test RandomStratifiedSplitter's split method on simple singletas.
    """
    # Test singletask case.
    n_samples = 100
    n_positives = 20
    n_tasks = 1

    X = np.ones(n_samples)
    y = np.zeros((n_samples, n_tasks))
    y[:n_positives] = 1
    w = np.ones((n_samples, n_tasks))
    dataset = dc.data.NumpyDataset(X, y, w)
    stratified_splitter = dc.splits.RandomStratifiedSplitter()
    train, valid, test = stratified_splitter.split(dataset, 0.5, 0, 0.5)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag396')" href="javascript:;">
deepchem-2.4.0/deepchem/splits/tests/test_splitter.py: 331-357
</a>
<div class="mid" id="frag396" style="display:none"><pre>
    # The split index should partition dataset in half.
    assert len(train) == 50
    assert len(valid) == 0
    assert len(test) == 50
    assert np.count_nonzero(y[train]) == 10
    assert np.count_nonzero(y[test]) == 10

  def test_singletask_stratified_column_indices_mask(self):
    """
    Test RandomStratifiedSplitter's split method on dataset with mask.
    """
    # Test singletask case.
    n_samples = 100
    n_positives = 20
    n_tasks = 1

    # Test case where some weights are zero (i.e. masked)
    X = np.ones(n_samples)
    y = np.zeros((n_samples, n_tasks))
    y[:n_positives] = 1
    w = np.ones((n_samples, n_tasks))
    # Set half the positives to have zero weight
    w[:n_positives // 2] = 0
    dataset = dc.data.NumpyDataset(X, y, w)

    stratified_splitter = dc.splits.RandomStratifiedSplitter()
    train, valid, test = stratified_splitter.split(dataset, 0.5, 0, 0.5)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 33:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag397')" href="javascript:;">
deepchem-2.4.0/deepchem/splits/tests/test_splitter.py: 358-378
</a>
<div class="mid" id="frag397" style="display:none"><pre>

    # There are 10 nonzero actives.
    # The split index should partition this into half, so expect 5
    w_present = (w != 0)
    y_present = y * w_present
    assert np.count_nonzero(y_present[train]) == 5

  def test_multitask_stratified_column_indices(self):
    """
    Test RandomStratifiedSplitter split on multitask dataset.
    """
    n_samples = 100
    n_tasks = 10
    p = .05  # proportion actives
    X = np.ones(n_samples)
    y = np.random.binomial(1, p, size=(n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))
    dataset = dc.data.NumpyDataset(X, y, w)

    stratified_splitter = dc.splits.RandomStratifiedSplitter()
    train, valid, test = stratified_splitter.split(dataset, 0.5, 0, 0.5)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag398')" href="javascript:;">
deepchem-2.4.0/deepchem/splits/tests/test_splitter.py: 379-404
</a>
<div class="mid" id="frag398" style="display:none"><pre>

    for task in range(n_tasks):
      task_actives = np.count_nonzero(y[:, task])
      # The split index should partition the positives for each task roughly in half.
      target = task_actives / 2
      assert target - 2 &lt;= np.count_nonzero(y[train, task]) &lt;= target + 2

  def test_multitask_stratified_column_indices_masked(self):
    """
    Test RandomStratifiedSplitter split on multitask dataset.
    """
    n_samples = 200
    n_tasks = 10
    p = .05  # proportion actives
    X = np.ones(n_samples)
    y = np.random.binomial(1, p, size=(n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))
    # Mask half the examples
    w[:n_samples // 2] = 0
    dataset = dc.data.NumpyDataset(X, y, w)

    stratified_splitter = dc.splits.RandomStratifiedSplitter()
    train, valid, test = stratified_splitter.split(dataset, 0.5, 0, 0.5)

    w_present = (w != 0)
    y_present = y * w_present
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 34:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag427')" href="javascript:;">
deepchem-2.4.0/deepchem/dock/tests/test_docking.py: 65-81
</a>
<div class="mid" id="frag427" style="display:none"><pre>
  def test_docker_specified_pocket(self):
    """Test that Docker can dock into spec. pocket."""
    # Let's turn on logging since this test will run for a while
    logging.basicConfig(level=logging.INFO)
    vpg = dc.dock.VinaPoseGenerator()
    docker = dc.dock.Docker(vpg)
    docked_outputs = docker.dock(
        (self.protein_file, self.ligand_file),
        centroid=(10, 10, 10),
        box_dims=(10, 10, 10),
        exhaustiveness=1,
        num_modes=1,
        out_dir="/tmp")

    # Check returned files exist
    assert len(list(docked_outputs)) == 1

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag428')" href="javascript:;">
deepchem-2.4.0/deepchem/dock/tests/test_docking.py: 83-99
</a>
<div class="mid" id="frag428" style="display:none"><pre>
  def test_pocket_docker_dock(self):
    """Test that Docker can find pockets and dock dock."""
    # Let's turn on logging since this test will run for a while
    logging.basicConfig(level=logging.INFO)
    pocket_finder = dc.dock.ConvexHullPocketFinder()
    vpg = dc.dock.VinaPoseGenerator(pocket_finder=pocket_finder)
    docker = dc.dock.Docker(vpg)
    docked_outputs = docker.dock(
        (self.protein_file, self.ligand_file),
        exhaustiveness=1,
        num_modes=1,
        num_pockets=1,
        out_dir="/tmp")

    # Check returned files exist
    assert len(list(docked_outputs)) == 1

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 35:</b> &nbsp; 4 fragments, nominal size 20 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag435')" href="javascript:;">
deepchem-2.4.0/deepchem/dock/tests/test_pose_generation.py: 33-60
</a>
<div class="mid" id="frag435" style="display:none"><pre>
  def test_vina_poses_and_scores(self):
    """Test that VinaPoseGenerator generates poses and scores

    This test takes some time to run, about a minute and a half on
    development laptop.
    """
    # Let's turn on logging since this test will run for a while
    logging.basicConfig(level=logging.INFO)
    current_dir = os.path.dirname(os.path.realpath(__file__))
    protein_file = os.path.join(current_dir, "1jld_protein.pdb")
    ligand_file = os.path.join(current_dir, "1jld_ligand.sdf")

    vpg = dc.dock.VinaPoseGenerator(pocket_finder=None)
    with tempfile.TemporaryDirectory() as tmp:
      poses, scores = vpg.generate_poses(
          (protein_file, ligand_file),
          exhaustiveness=1,
          num_modes=1,
          out_dir=tmp,
          generate_scores=True)

    assert len(poses) == 1
    assert len(scores) == 1
    protein, ligand = poses[0]
    from rdkit import Chem
    assert isinstance(protein, Chem.Mol)
    assert isinstance(ligand, Chem.Mol)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag438')" href="javascript:;">
deepchem-2.4.0/deepchem/dock/tests/test_pose_generation.py: 123-153
</a>
<div class="mid" id="frag438" style="display:none"><pre>
  def test_pocket_vina_poses(self):
    """Test that VinaPoseGenerator creates pose files.

    This test is quite slow and takes about 5 minutes to run on a
    development laptop.
    """
    # Let's turn on logging since this test will run for a while
    logging.basicConfig(level=logging.INFO)
    current_dir = os.path.dirname(os.path.realpath(__file__))
    protein_file = os.path.join(current_dir, "1jld_protein.pdb")
    ligand_file = os.path.join(current_dir, "1jld_ligand.sdf")

    # Note this may download autodock Vina...
    convex_finder = dc.dock.ConvexHullPocketFinder()
    vpg = dc.dock.VinaPoseGenerator(pocket_finder=convex_finder)
    with tempfile.TemporaryDirectory() as tmp:
      poses, scores = vpg.generate_poses(
          (protein_file, ligand_file),
          exhaustiveness=1,
          num_modes=1,
          num_pockets=2,
          out_dir=tmp,
          generate_scores=True)

    assert len(poses) == 2
    assert len(scores) == 2
    from rdkit import Chem
    for pose in poses:
      protein, ligand = pose
      assert isinstance(protein, Chem.Mol)
      assert isinstance(ligand, Chem.Mol)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag437')" href="javascript:;">
deepchem-2.4.0/deepchem/dock/tests/test_pose_generation.py: 90-121
</a>
<div class="mid" id="frag437" style="display:none"><pre>
  def test_vina_pose_specified_centroid(self):
    """Test that VinaPoseGenerator creates pose files with specified centroid/box dims.

    This test takes some time to run, about a minute and a half on
    development laptop.
    """
    # Let's turn on logging since this test will run for a while
    logging.basicConfig(level=logging.INFO)
    current_dir = os.path.dirname(os.path.realpath(__file__))
    protein_file = os.path.join(current_dir, "1jld_protein.pdb")
    ligand_file = os.path.join(current_dir, "1jld_ligand.sdf")

    centroid = np.array([56.21891368, 25.95862964, 3.58950065])
    box_dims = np.array([51.354, 51.243, 55.608])
    vpg = dc.dock.VinaPoseGenerator(pocket_finder=None)
    with tempfile.TemporaryDirectory() as tmp:
      poses, scores = vpg.generate_poses(
          (protein_file, ligand_file),
          centroid=centroid,
          box_dims=box_dims,
          exhaustiveness=1,
          num_modes=1,
          out_dir=tmp,
          generate_scores=True)

    assert len(poses) == 1
    assert len(scores) == 1
    protein, ligand = poses[0]
    from rdkit import Chem
    assert isinstance(protein, Chem.Mol)
    assert isinstance(ligand, Chem.Mol)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag436')" href="javascript:;">
deepchem-2.4.0/deepchem/dock/tests/test_pose_generation.py: 62-88
</a>
<div class="mid" id="frag436" style="display:none"><pre>
  def test_vina_poses_no_scores(self):
    """Test that VinaPoseGenerator generates poses.

    This test takes some time to run, about a minute and a half on
    development laptop.
    """
    # Let's turn on logging since this test will run for a while
    logging.basicConfig(level=logging.INFO)
    current_dir = os.path.dirname(os.path.realpath(__file__))
    protein_file = os.path.join(current_dir, "1jld_protein.pdb")
    ligand_file = os.path.join(current_dir, "1jld_ligand.sdf")

    vpg = dc.dock.VinaPoseGenerator(pocket_finder=None)
    with tempfile.TemporaryDirectory() as tmp:
      poses = vpg.generate_poses(
          (protein_file, ligand_file),
          exhaustiveness=1,
          num_modes=1,
          out_dir=tmp,
          generate_scores=False)

    assert len(poses) == 1
    protein, ligand = poses[0]
    from rdkit import Chem
    assert isinstance(protein, Chem.Mol)
    assert isinstance(ligand, Chem.Mol)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 36:</b> &nbsp; 3 fragments, nominal size 11 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag475')" href="javascript:;">
deepchem-2.4.0/deepchem/models/optimizers.py: 122-132
</a>
<div class="mid" id="frag475" style="display:none"><pre>
  def _create_tf_optimizer(self, global_step):
    import tensorflow as tf
    if isinstance(self.learning_rate, LearningRateSchedule):
      learning_rate = self.learning_rate._create_tf_tensor(global_step)
    else:
      learning_rate = self.learning_rate
    return tf.keras.optimizers.Adagrad(
        learning_rate=learning_rate,
        initial_accumulator_value=self.initial_accumulator_value,
        epsilon=self.epsilon)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag481')" href="javascript:;">
deepchem-2.4.0/deepchem/models/optimizers.py: 219-230
</a>
<div class="mid" id="frag481" style="display:none"><pre>
  def _create_tf_optimizer(self, global_step):
    import tensorflow as tf
    if isinstance(self.learning_rate, LearningRateSchedule):
      learning_rate = self.learning_rate._create_tf_tensor(global_step)
    else:
      learning_rate = self.learning_rate
    return tf.keras.optimizers.RMSprop(
        learning_rate=learning_rate,
        momentum=self.momentum,
        rho=self.decay,
        epsilon=self.epsilon)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag478')" href="javascript:;">
deepchem-2.4.0/deepchem/models/optimizers.py: 172-183
</a>
<div class="mid" id="frag478" style="display:none"><pre>
  def _create_tf_optimizer(self, global_step):
    import tensorflow as tf
    if isinstance(self.learning_rate, LearningRateSchedule):
      learning_rate = self.learning_rate._create_tf_tensor(global_step)
    else:
      learning_rate = self.learning_rate
    return tf.keras.optimizers.Adam(
        learning_rate=learning_rate,
        beta_1=self.beta1,
        beta_2=self.beta2,
        epsilon=self.epsilon)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 37:</b> &nbsp; 7 fragments, nominal size 16 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag501')" href="javascript:;">
deepchem-2.4.0/deepchem/models/text_cnn.py: 228-246
</a>
<div class="mid" id="frag501" style="display:none"><pre>
  def default_generator(self,
                        dataset,
                        epochs=1,
                        mode='fit',
                        deterministic=True,
                        pad_batches=True):
    """Transfer smiles strings to fixed length integer vectors"""
    for epoch in range(epochs):
      for (X_b, y_b, w_b, ids_b) in dataset.iterbatches(
          batch_size=self.batch_size,
          deterministic=deterministic,
          pad_batches=pad_batches):
        if y_b is not None:
          if self.mode == 'classification':
            y_b = to_one_hot(y_b.flatten(), 2).reshape(-1, self.n_tasks, 2)
        # Transform SMILES sequence to integers
        X_b = self.smiles_to_seq_batch(ids_b)
        yield ([X_b], [y_b], [w_b])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag974')" href="javascript:;">
deepchem-2.4.0/deepchem/models/robust_multitask.py: 180-195
</a>
<div class="mid" id="frag974" style="display:none"><pre>
  def default_generator(self,
                        dataset,
                        epochs=1,
                        mode='fit',
                        deterministic=True,
                        pad_batches=True):
    for epoch in range(epochs):
      for (X_b, y_b, w_b, ids_b) in dataset.iterbatches(
          batch_size=self.batch_size,
          deterministic=deterministic,
          pad_batches=pad_batches):
        if y_b is not None:
          y_b = to_one_hot(y_b.flatten(), self.n_classes).reshape(
              -1, self.n_tasks, self.n_classes)
        yield ([X_b], [y_b], [w_b])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag801')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_torch_model.py: 227-243
</a>
<div class="mid" id="frag801" style="display:none"><pre>
    def default_generator(self,
                          dataset,
                          epochs=1,
                          mode='fit',
                          deterministic=True,
                          pad_batches=True):
      for epoch in range(epochs):
        for (X_b, y_b, w_b, ids_b) in dataset.iterbatches(
            batch_size=self.batch_size,
            deterministic=deterministic,
            pad_batches=pad_batches):
          if mode == 'predict':
            dropout = np.array(False)
          else:
            dropout = np.array(True)
          yield ([X_b, dropout], [y_b], [w_b])

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag870')" href="javascript:;">
deepchem-2.4.0/deepchem/models/chemnet_models.py: 174-190
</a>
<div class="mid" id="frag870" style="display:none"><pre>
  def default_generator(self,
                        dataset,
                        epochs=1,
                        mode='fit',
                        deterministic=True,
                        pad_batches=True):
    for epoch in range(epochs):
      for (X_b, y_b, w_b, ids_b) in dataset.iterbatches(
          batch_size=self.batch_size,
          deterministic=deterministic,
          pad_batches=pad_batches):
        if self.mode == 'classification':
          y_b = to_one_hot(y_b.flatten(), self.n_classes).reshape(
              -1, self.n_tasks, self.n_classes)
        yield ([X_b], [y_b], [w_b])


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag515')" href="javascript:;">
deepchem-2.4.0/deepchem/models/scscore.py: 81-93
</a>
<div class="mid" id="frag515" style="display:none"><pre>
  def default_generator(self,
                        dataset,
                        epochs=1,
                        mode='fit',
                        deterministic=True,
                        pad_batches=True):
    for epoch in range(epochs):
      for (X_b, y_b, w_b, ids_b) in dataset.iterbatches(
          batch_size=self.batch_size,
          deterministic=deterministic,
          pad_batches=pad_batches):
        yield ([X_b[:, 0], X_b[:, 1]], [y_b], [w_b])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag972')" href="javascript:;">
deepchem-2.4.0/deepchem/models/cnn.py: 239-258
</a>
<div class="mid" id="frag972" style="display:none"><pre>
  def default_generator(self,
                        dataset,
                        epochs=1,
                        mode='fit',
                        deterministic=True,
                        pad_batches=True):
    for epoch in range(epochs):
      for (X_b, y_b, w_b, ids_b) in dataset.iterbatches(
          batch_size=self.batch_size,
          deterministic=deterministic,
          pad_batches=pad_batches):
        if self.mode == 'classification':
          if y_b is not None:
            y_b = to_one_hot(y_b.flatten(), self.n_classes).reshape(
                -1, self.n_tasks, self.n_classes)
        if mode == 'predict':
          dropout = np.array(0.0)
        else:
          dropout = np.array(1.0)
        yield ([X_b, dropout], [y_b], [w_b])
</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag645')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_kerasmodel.py: 202-218
</a>
<div class="mid" id="frag645" style="display:none"><pre>
    def default_generator(self,
                          dataset,
                          epochs=1,
                          mode='fit',
                          deterministic=True,
                          pad_batches=True):
      for epoch in range(epochs):
        for (X_b, y_b, w_b, ids_b) in dataset.iterbatches(
            batch_size=self.batch_size,
            deterministic=deterministic,
            pad_batches=pad_batches):
          if mode == 'predict':
            dropout = np.array(0.0)
          else:
            dropout = np.array(1.0)
          yield ([X_b, dropout], [y_b], [w_b])

</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 38:</b> &nbsp; 5 fragments, nominal size 47 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag540')" href="javascript:;">
deepchem-2.4.0/deepchem/models/chemnet_layers.py: 76-143
</a>
<div class="mid" id="frag540" style="display:none"><pre>
  def _build_layer_components(self):
    """Builds the layers components and set _layers attribute."""
    self.conv_block1 = [
        Conv2D(
            self.num_filters,
            kernel_size=(1, 1),
            strides=1,
            padding="same",
            activation=tf.nn.relu)
    ]

    self.conv_block2 = [
        Conv2D(
            filters=self.num_filters,
            kernel_size=(1, 1),
            strides=1,
            activation=tf.nn.relu,
            padding="same")
    ]

    self.conv_block2.append(
        Conv2D(
            filters=self.num_filters,
            kernel_size=(3, 3),
            strides=1,
            activation=tf.nn.relu,
            padding="same"))

    self.conv_block3 = [
        Conv2D(
            filters=self.num_filters,
            kernel_size=1,
            strides=1,
            activation=tf.nn.relu,
            padding="same")
    ]

    self.conv_block3.append(
        Conv2D(
            filters=int(self.num_filters * 1.5),
            kernel_size=(3, 3),
            strides=1,
            activation=tf.nn.relu,
            padding="same"))
    self.conv_block3.append(
        Conv2D(
            filters=self.num_filters * 2,
            kernel_size=(3, 3),
            strides=1,
            activation=tf.nn.relu,
            padding="same"))

    self.conv_block4 = [
        Conv2D(
            filters=self.input_dim,
            kernel_size=(1, 1),
            strides=1,
            padding="same")
    ]

    self.concat_layer = Concatenate()
    self.add_layer = Add()
    self.activation_layer = ReLU()

    self._layers = self.conv_block1 + self.conv_block2 + self.conv_block3 + self.conv_block4
    self._layers.extend(
        [self.concat_layer, self.add_layer, self.activation_layer])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag552')" href="javascript:;">
deepchem-2.4.0/deepchem/models/chemnet_layers.py: 455-521
</a>
<div class="mid" id="frag552" style="display:none"><pre>
  def _build_layer_components(self):
    """Builds the layers components and set _layers attribute."""
    self.max_pool1 = MaxPool2D(pool_size=(3, 3), strides=2, padding="valid")

    self.conv_block1 = [
        Conv2D(
            self.num_filters,
            kernel_size=1,
            strides=1,
            padding="same",
            activation=tf.nn.relu)
    ]
    self.conv_block1.append(
        Conv2D(
            int(self.num_filters * 1.5),
            kernel_size=3,
            strides=2,
            padding="valid",
            activation=tf.nn.relu))

    self.conv_block2 = [
        Conv2D(
            filters=self.num_filters,
            kernel_size=1,
            strides=1,
            activation=tf.nn.relu,
            padding="same")
    ]
    self.conv_block2.append(
        Conv2D(
            filters=int(self.num_filters * 1.125),
            kernel_size=3,
            strides=2,
            activation=tf.nn.relu,
            padding="valid"))

    self.conv_block3 = [
        Conv2D(
            filters=self.num_filters,
            kernel_size=1,
            strides=1,
            activation=tf.nn.relu,
            padding="same")
    ]
    self.conv_block3.append(
        Conv2D(
            filters=int(self.num_filters * 1.125),
            kernel_size=(3, 1),
            strides=1,
            activation=tf.nn.relu,
            padding="same"))

    self.conv_block3.append(
        Conv2D(
            filters=int(self.num_filters * 1.25),
            kernel_size=(3, 3),
            strides=2,
            activation=tf.nn.relu,
            padding="valid"))

    self.concat_layer = Concatenate()
    self.activation_layer = ReLU()

    self._layers = self.conv_block1 + self.conv_block2 + self.conv_block3
    self._layers.extend(
        [self.max_pool1, self.concat_layer, self.activation_layer])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag546')" href="javascript:;">
deepchem-2.4.0/deepchem/models/chemnet_layers.py: 287-336
</a>
<div class="mid" id="frag546" style="display:none"><pre>
  def _build_layer_components(self):
    """Builds the layers components and set _layers attribute."""
    self.conv_block1 = [
        Conv2D(
            self.num_filters,
            kernel_size=(1, 1),
            strides=1,
            padding="same",
            activation=tf.nn.relu)
    ]

    self.conv_block2 = [
        Conv2D(
            filters=self.num_filters,
            kernel_size=1,
            strides=1,
            activation=tf.nn.relu,
            padding="same")
    ]
    self.conv_block2.append(
        Conv2D(
            filters=int(self.num_filters * 1.16),
            kernel_size=(1, 3),
            strides=1,
            activation=tf.nn.relu,
            padding="same"))
    self.conv_block2.append(
        Conv2D(
            filters=int(self.num_filters * 1.33),
            kernel_size=(3, 1),
            strides=1,
            activation=tf.nn.relu,
            padding="same"))

    self.conv_block3 = [
        Conv2D(
            filters=self.input_dim,
            kernel_size=(1, 1),
            strides=1,
            padding="same")
    ]

    self.concat_layer = Concatenate()
    self.add_layer = Add()
    self.activation_layer = ReLU()

    self._layers = self.conv_block1 + self.conv_block2 + self.conv_block3
    self._layers.extend(
        [self.concat_layer, self.add_layer, self.activation_layer])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag543')" href="javascript:;">
deepchem-2.4.0/deepchem/models/chemnet_layers.py: 193-239
</a>
<div class="mid" id="frag543" style="display:none"><pre>
  def _build_layer_components(self):
    """Builds the layers components and set _layers attribute."""
    self.conv_block1 = [
        Conv2D(
            self.num_filters,
            kernel_size=1,
            strides=1,
            padding="same",
            activation=tf.nn.relu)
    ]

    self.conv_block2 = [
        Conv2D(
            filters=self.num_filters,
            kernel_size=(1, 1),
            strides=1,
            activation=tf.nn.relu,
            padding="same")
    ]
    self.conv_block2.append(
        Conv2D(
            filters=int(self.num_filters * 1.25),
            kernel_size=(1, 7),
            strides=1,
            activation=tf.nn.relu,
            padding="same"))
    self.conv_block2.append(
        Conv2D(
            filters=int(self.num_filters * 1.5),
            kernel_size=(7, 1),
            strides=1,
            activation=tf.nn.relu,
            padding="same"))

    self.conv_block3 = [
        Conv2D(
            filters=self.input_dim, kernel_size=1, strides=1, padding="same")
    ]

    self.concat_layer = Concatenate()
    self.add_layer = Add()
    self.activation_layer = ReLU()

    self._layers = self.conv_block1 + self.conv_block2 + self.conv_block3
    self._layers.extend(
        [self.concat_layer, self.add_layer, self.activation_layer])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag549')" href="javascript:;">
deepchem-2.4.0/deepchem/models/chemnet_layers.py: 377-419
</a>
<div class="mid" id="frag549" style="display:none"><pre>
  def _build_layer_components(self):
    """Builds the layers components and set _layers attribute."""
    self.max_pool1 = MaxPool2D(pool_size=(3, 3), strides=2, padding="valid")

    self.conv_block1 = [
        Conv2D(
            int(self.num_filters * 1.5),
            kernel_size=(3, 3),
            strides=2,
            padding="valid",
            activation=tf.nn.relu)
    ]

    self.conv_block2 = [
        Conv2D(
            filters=self.num_filters,
            kernel_size=1,
            strides=1,
            activation=tf.nn.relu,
            padding="same")
    ]
    self.conv_block2.append(
        Conv2D(
            filters=self.num_filters,
            kernel_size=3,
            strides=1,
            activation=tf.nn.relu,
            padding="same"))
    self.conv_block2.append(
        Conv2D(
            filters=int(self.num_filters * 1.5),
            kernel_size=3,
            strides=2,
            activation=tf.nn.relu,
            padding="valid"))

    self.concat_layer = Concatenate()
    self.activation_layer = ReLU()

    self._layers = self.conv_block1 + self.conv_block2
    self._layers.extend(
        [self.max_pool1, self.concat_layer, self.activation_layer])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 39:</b> &nbsp; 4 fragments, nominal size 13 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag544')" href="javascript:;">
deepchem-2.4.0/deepchem/models/chemnet_layers.py: 240-260
</a>
<div class="mid" id="frag544" style="display:none"><pre>
  def call(self, inputs):
    """Invoked when __call__ method of the layer is used."""
    conv1 = inputs
    for layer in self.conv_block1:
      conv1 = layer(conv1)

    conv2 = inputs
    for layer in self.conv_block2:
      conv2 = layer(conv2)

    concat_conv = self.concat_layer([conv1, conv2])

    conv3 = concat_conv
    for layer in self.conv_block3:
      conv3 = layer(conv3)

    output = self.add_layer([conv3, inputs])
    output = self.activation_layer(output)
    return output


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag547')" href="javascript:;">
deepchem-2.4.0/deepchem/models/chemnet_layers.py: 337-357
</a>
<div class="mid" id="frag547" style="display:none"><pre>
  def call(self, inputs):
    """Invoked when __call__ method of the layer is used."""
    conv1 = inputs
    for layer in self.conv_block1:
      conv1 = layer(conv1)

    conv2 = inputs
    for layer in self.conv_block2:
      conv2 = layer(conv2)

    concat_conv = self.concat_layer([conv1, conv2])

    conv3 = concat_conv
    for layer in self.conv_block3:
      conv3 = layer(conv3)

    output = self.add_layer([conv3, inputs])
    output = self.activation_layer(output)
    return output


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag553')" href="javascript:;">
deepchem-2.4.0/deepchem/models/chemnet_layers.py: 522-539
</a>
<div class="mid" id="frag553" style="display:none"><pre>
  def call(self, inputs):
    """Invoked when __call__ method of the layer is used."""
    maxpool1 = self.max_pool1(inputs)
    conv1 = inputs
    for layer in self.conv_block1:
      conv1 = layer(conv1)

    conv2 = inputs
    for layer in self.conv_block2:
      conv2 = layer(conv2)

    conv3 = inputs
    for layer in self.conv_block3:
      conv3 = layer(conv3)

    concat = self.concat_layer([maxpool1, conv1, conv2, conv3])
    output = self.activation_layer(concat)
    return output
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag550')" href="javascript:;">
deepchem-2.4.0/deepchem/models/chemnet_layers.py: 420-435
</a>
<div class="mid" id="frag550" style="display:none"><pre>
  def call(self, inputs):
    """Invoked when __call__ method of the layer is used."""
    maxpool1 = self.max_pool1(inputs)
    conv1 = inputs
    for layer in self.conv_block1:
      conv1 = layer(conv1)

    conv2 = inputs
    for layer in self.conv_block2:
      conv2 = layer(conv2)

    output = self.concat_layer([maxpool1, conv1, conv2])
    output = self.activation_layer(output)
    return output


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 40:</b> &nbsp; 6 fragments, nominal size 25 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag566')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_gat.py: 22-56
</a>
<div class="mid" id="frag566" style="display:none"><pre>
def test_gat_regression():
  # load datasets
  featurizer = MolGraphConvFeaturizer()
  tasks, dataset, transformers, metric = get_dataset(
      'regression', featurizer=featurizer)

  # initialize models
  n_tasks = len(tasks)
  model = GATModel(
      mode='regression',
      n_tasks=n_tasks,
      number_atom_features=30,
      batch_size=10,
      learning_rate=0.001)

  # overfit test
  model.fit(dataset, nb_epoch=500)
  scores = model.evaluate(dataset, [metric], transformers)
  assert scores['mean_absolute_error'] &lt; 0.5

  # test on a small MoleculeNet dataset
  from deepchem.molnet import load_delaney

  tasks, all_dataset, transformers = load_delaney(featurizer=featurizer)
  train_set, _, _ = all_dataset
  model = dc.models.GATModel(
      mode='regression',
      n_tasks=len(tasks),
      graph_attention_layers=[2],
      n_attention_heads=1,
      residual=False,
      predictor_hidden_feats=2)
  model.fit(train_set, nb_epoch=1)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag635')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_gcn.py: 57-91
</a>
<div class="mid" id="frag635" style="display:none"><pre>
def test_gcn_classification():
  # load datasets
  featurizer = MolGraphConvFeaturizer()
  tasks, dataset, transformers, metric = get_dataset(
      'classification', featurizer=featurizer)

  # initialize models
  n_tasks = len(tasks)
  model = GCNModel(
      mode='classification',
      n_tasks=n_tasks,
      number_atom_features=30,
      batch_size=10,
      learning_rate=0.0003)

  # overfit test
  model.fit(dataset, nb_epoch=70)
  scores = model.evaluate(dataset, [metric], transformers)
  assert scores['mean-roc_auc_score'] &gt;= 0.85

  # test on a small MoleculeNet dataset
  from deepchem.molnet import load_bace_classification

  tasks, all_dataset, transformers = load_bace_classification(
      featurizer=featurizer)
  train_set, _, _ = all_dataset
  model = dc.models.GCNModel(
      mode='classification',
      n_tasks=len(tasks),
      graph_conv_layers=[2],
      residual=False,
      predictor_hidden_feats=2)
  model.fit(train_set, nb_epoch=1)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag634')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_gcn.py: 22-54
</a>
<div class="mid" id="frag634" style="display:none"><pre>
def test_gcn_regression():
  # load datasets
  featurizer = MolGraphConvFeaturizer()
  tasks, dataset, transformers, metric = get_dataset(
      'regression', featurizer=featurizer)

  # initialize models
  n_tasks = len(tasks)
  model = GCNModel(
      mode='regression',
      n_tasks=n_tasks,
      number_atom_features=30,
      batch_size=10,
      learning_rate=0.003)

  # overfit test
  model.fit(dataset, nb_epoch=300)
  scores = model.evaluate(dataset, [metric], transformers)
  assert scores['mean_absolute_error'] &lt; 0.5

  # test on a small MoleculeNet dataset
  from deepchem.molnet import load_delaney

  tasks, all_dataset, transformers = load_delaney(featurizer=featurizer)
  train_set, _, _ = all_dataset
  model = dc.models.GCNModel(
      n_tasks=len(tasks),
      graph_conv_layers=[2],
      residual=False,
      predictor_hidden_feats=2)
  model.fit(train_set, nb_epoch=1)


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag567')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_gat.py: 59-94
</a>
<div class="mid" id="frag567" style="display:none"><pre>
def test_gat_classification():
  # load datasets
  featurizer = MolGraphConvFeaturizer()
  tasks, dataset, transformers, metric = get_dataset(
      'classification', featurizer=featurizer)

  # initialize models
  n_tasks = len(tasks)
  model = GATModel(
      mode='classification',
      n_tasks=n_tasks,
      number_atom_features=30,
      batch_size=10,
      learning_rate=0.001)

  # overfit test
  model.fit(dataset, nb_epoch=100)
  scores = model.evaluate(dataset, [metric], transformers)
  assert scores['mean-roc_auc_score'] &gt;= 0.85

  # test on a small MoleculeNet dataset
  from deepchem.molnet import load_bace_classification

  tasks, all_dataset, transformers = load_bace_classification(
      featurizer=featurizer)
  train_set, _, _ = all_dataset
  model = dc.models.GATModel(
      mode='classification',
      n_tasks=len(tasks),
      graph_attention_layers=[2],
      n_attention_heads=1,
      residual=False,
      predictor_hidden_feats=2)
  model.fit(train_set, nb_epoch=1)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag864')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_attentivefp.py: 53-86
</a>
<div class="mid" id="frag864" style="display:none"><pre>
def test_attentivefp_classification():
  # load datasets
  featurizer = MolGraphConvFeaturizer(use_edges=True)
  tasks, dataset, transformers, metric = get_dataset(
      'classification', featurizer=featurizer)

  # initialize models
  n_tasks = len(tasks)
  model = AttentiveFPModel(
      mode='classification',
      n_tasks=n_tasks,
      batch_size=10,
      learning_rate=0.001)

  # overfit test
  model.fit(dataset, nb_epoch=100)
  scores = model.evaluate(dataset, [metric], transformers)
  assert scores['mean-roc_auc_score'] &gt;= 0.85

  # test on a small MoleculeNet dataset
  from deepchem.molnet import load_bace_classification

  tasks, all_dataset, transformers = load_bace_classification(
      featurizer=featurizer)
  train_set, _, _ = all_dataset
  model = AttentiveFPModel(
      mode='classification',
      n_tasks=len(tasks),
      num_layers=1,
      num_timesteps=1,
      graph_feat_size=2)
  model.fit(train_set, nb_epoch=1)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag787')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_mpnn.py: 55-90
</a>
<div class="mid" id="frag787" style="display:none"><pre>
def test_mpnn_classification():
  # load datasets
  featurizer = MolGraphConvFeaturizer(use_edges=True)
  tasks, dataset, transformers, metric = get_dataset(
      'classification', featurizer=featurizer)

  # initialize models
  n_tasks = len(tasks)
  model = MPNNModel(
      mode='classification',
      n_tasks=n_tasks,
      batch_size=10,
      learning_rate=0.001)

  # overfit test
  model.fit(dataset, nb_epoch=200)
  scores = model.evaluate(dataset, [metric], transformers)
  assert scores['mean-roc_auc_score'] &gt;= 0.85

  # test on a small MoleculeNet dataset
  from deepchem.molnet import load_bace_classification

  tasks, all_dataset, transformers = load_bace_classification(
      featurizer=featurizer)
  train_set, _, _ = all_dataset
  model = MPNNModel(
      mode='classification',
      n_tasks=len(tasks),
      node_out_feats=2,
      edge_hidden_feats=2,
      num_step_message_passing=1,
      num_step_set2set=1,
      num_layer_set2set=1)
  model.fit(train_set, nb_epoch=1)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 41:</b> &nbsp; 4 fragments, nominal size 29 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag568')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_gat.py: 97-132
</a>
<div class="mid" id="frag568" style="display:none"><pre>
def test_gat_reload():
  # load datasets
  featurizer = MolGraphConvFeaturizer()
  tasks, dataset, transformers, metric = get_dataset(
      'classification', featurizer=featurizer)

  # initialize models
  n_tasks = len(tasks)
  model_dir = tempfile.mkdtemp()
  model = GATModel(
      mode='classification',
      n_tasks=n_tasks,
      number_atom_features=30,
      model_dir=model_dir,
      batch_size=10,
      learning_rate=0.001)

  model.fit(dataset, nb_epoch=100)
  scores = model.evaluate(dataset, [metric], transformers)
  assert scores['mean-roc_auc_score'] &gt;= 0.85

  reloaded_model = GATModel(
      mode='classification',
      n_tasks=n_tasks,
      number_atom_features=30,
      model_dir=model_dir,
      batch_size=10,
      learning_rate=0.001)
  reloaded_model.restore()

  pred_mols = ["CCCC", "CCCCCO", "CCCCC"]
  X_pred = featurizer(pred_mols)
  random_dataset = dc.data.NumpyDataset(X_pred)
  original_pred = model.predict(random_dataset)
  reload_pred = reloaded_model.predict(random_dataset)
  assert np.all(original_pred == reload_pred)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag636')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_gcn.py: 94-129
</a>
<div class="mid" id="frag636" style="display:none"><pre>
def test_gcn_reload():
  # load datasets
  featurizer = MolGraphConvFeaturizer()
  tasks, dataset, transformers, metric = get_dataset(
      'classification', featurizer=featurizer)

  # initialize models
  n_tasks = len(tasks)
  model_dir = tempfile.mkdtemp()
  model = GCNModel(
      mode='classification',
      n_tasks=n_tasks,
      number_atom_features=30,
      model_dir=model_dir,
      batch_size=10,
      learning_rate=0.0003)

  model.fit(dataset, nb_epoch=70)
  scores = model.evaluate(dataset, [metric], transformers)
  assert scores['mean-roc_auc_score'] &gt;= 0.85

  reloaded_model = GCNModel(
      mode='classification',
      n_tasks=n_tasks,
      number_atom_features=30,
      model_dir=model_dir,
      batch_size=10,
      learning_rate=0.0003)
  reloaded_model.restore()

  pred_mols = ["CCCC", "CCCCCO", "CCCCC"]
  X_pred = featurizer(pred_mols)
  random_dataset = dc.data.NumpyDataset(X_pred)
  original_pred = model.predict(random_dataset)
  reload_pred = reloaded_model.predict(random_dataset)
  assert np.all(original_pred == reload_pred)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag788')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_mpnn.py: 93-126
</a>
<div class="mid" id="frag788" style="display:none"><pre>
def test_mpnn_reload():
  # load datasets
  featurizer = MolGraphConvFeaturizer(use_edges=True)
  tasks, dataset, transformers, metric = get_dataset(
      'classification', featurizer=featurizer)

  # initialize models
  n_tasks = len(tasks)
  model_dir = tempfile.mkdtemp()
  model = MPNNModel(
      mode='classification',
      n_tasks=n_tasks,
      model_dir=model_dir,
      batch_size=10,
      learning_rate=0.001)

  model.fit(dataset, nb_epoch=200)
  scores = model.evaluate(dataset, [metric], transformers)
  assert scores['mean-roc_auc_score'] &gt;= 0.85

  reloaded_model = MPNNModel(
      mode='classification',
      n_tasks=n_tasks,
      model_dir=model_dir,
      batch_size=10,
      learning_rate=0.001)
  reloaded_model.restore()

  pred_mols = ["CCCC", "CCCCCO", "CCCCC"]
  X_pred = featurizer(pred_mols)
  random_dataset = dc.data.NumpyDataset(X_pred)
  original_pred = model.predict(random_dataset)
  reload_pred = reloaded_model.predict(random_dataset)
  assert np.all(original_pred == reload_pred)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag865')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_attentivefp.py: 89-122
</a>
<div class="mid" id="frag865" style="display:none"><pre>
def test_attentivefp_reload():
  # load datasets
  featurizer = MolGraphConvFeaturizer(use_edges=True)
  tasks, dataset, transformers, metric = get_dataset(
      'classification', featurizer=featurizer)

  # initialize models
  n_tasks = len(tasks)
  model_dir = tempfile.mkdtemp()
  model = AttentiveFPModel(
      mode='classification',
      n_tasks=n_tasks,
      model_dir=model_dir,
      batch_size=10,
      learning_rate=0.001)

  model.fit(dataset, nb_epoch=100)
  scores = model.evaluate(dataset, [metric], transformers)
  assert scores['mean-roc_auc_score'] &gt;= 0.85

  reloaded_model = AttentiveFPModel(
      mode='classification',
      n_tasks=n_tasks,
      model_dir=model_dir,
      batch_size=10,
      learning_rate=0.001)
  reloaded_model.restore()

  pred_mols = ["CCCC", "CCCCCO", "CCCCC"]
  X_pred = featurizer(pred_mols)
  random_dataset = dc.data.NumpyDataset(X_pred)
  original_pred = model.predict(random_dataset)
  reload_pred = reloaded_model.predict(random_dataset)
  assert np.all(original_pred == reload_pred)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 42:</b> &nbsp; 2 fragments, nominal size 27 lines, similarity 85%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag570')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_weave_models.py: 45-81
</a>
<div class="mid" id="frag570" style="display:none"><pre>
def test_compute_features_on_infinity_distance():
  """Test that WeaveModel correctly transforms WeaveMol objects into tensors with infinite max_pair_distance."""
  featurizer = dc.feat.WeaveFeaturizer(max_pair_distance=None)
  X = featurizer(["C", "CCC"])
  batch_size = 20
  model = WeaveModel(
      1,
      batch_size=batch_size,
      mode='classification',
      fully_connected_layer_sizes=[2000, 1000],
      batch_normalize=True,
      batch_normalize_kwargs={
          "fused": False,
          "trainable": True,
          "renorm": True
      },
      learning_rage=0.0005)
  atom_feat, pair_feat, pair_split, atom_split, atom_to_pair = model.compute_features_on_batch(
      X)

  # There are 4 atoms each of which have 75 atom features
  assert atom_feat.shape == (4, 75)
  # There are 10 pairs with infinity distance and 14 pair features
  assert pair_feat.shape == (10, 14)
  # 4 atoms in total
  assert atom_split.shape == (4,)
  assert np.all(atom_split == np.array([0, 1, 1, 1]))
  # 10 pairs in total
  assert pair_split.shape == (10,)
  assert np.all(pair_split == np.array([0, 1, 1, 1, 2, 2, 2, 3, 3, 3]))
  # 10 pairs in total each with start/finish
  assert atom_to_pair.shape == (10, 2)
  assert np.all(
      atom_to_pair == np.array([[0, 0], [1, 1], [1, 2], [1, 3], [2, 1], [2, 2],
                                [2, 3], [3, 1], [3, 2], [3, 3]]))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag571')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_weave_models.py: 82-122
</a>
<div class="mid" id="frag571" style="display:none"><pre>
def test_compute_features_on_distance_1():
  """Test that WeaveModel correctly transforms WeaveMol objects into tensors with finite max_pair_distance."""
  featurizer = dc.feat.WeaveFeaturizer(max_pair_distance=1)
  X = featurizer(["C", "CCC"])
  batch_size = 20
  model = WeaveModel(
      1,
      batch_size=batch_size,
      mode='classification',
      fully_connected_layer_sizes=[2000, 1000],
      batch_normalize=True,
      batch_normalize_kwargs={
          "fused": False,
          "trainable": True,
          "renorm": True
      },
      learning_rage=0.0005)
  atom_feat, pair_feat, pair_split, atom_split, atom_to_pair = model.compute_features_on_batch(
      X)

  # There are 4 atoms each of which have 75 atom features
  assert atom_feat.shape == (4, 75)
  # There are 8 pairs with distance 1 and 14 pair features. (To see why 8,
  # there's the self pair for "C". For "CCC" there are 7 pairs including self
  # connections and accounting for symmetry.)
  assert pair_feat.shape == (8, 14)
  # 4 atoms in total
  assert atom_split.shape == (4,)
  assert np.all(atom_split == np.array([0, 1, 1, 1]))
  # 10 pairs in total
  assert pair_split.shape == (8,)
  # The center atom is self connected and to both neighbors so it appears
  # thrice. The canonical ranking used in MolecularFeaturizer means this
  # central atom is ranked last in ordering.
  assert np.all(pair_split == np.array([0, 1, 1, 2, 2, 3, 3, 3]))
  # 10 pairs in total each with start/finish
  assert atom_to_pair.shape == (8, 2)
  assert np.all(atom_to_pair == np.array([[0, 0], [1, 1], [1, 3], [2, 2],
                                          [2, 3], [3, 1], [3, 2], [3, 3]]))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 43:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag572')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_weave_models.py: 125-141
</a>
<div class="mid" id="frag572" style="display:none"><pre>
def test_weave_model():
  tasks, dataset, transformers, metric = get_dataset(
      'classification', 'Weave', data_points=10)

  batch_size = 10
  model = WeaveModel(
      len(tasks),
      batch_size=batch_size,
      mode='classification',
      final_conv_activation_fn=None,
      dropouts=0,
      learning_rage=0.0003)
  model.fit(dataset, nb_epoch=100)
  scores = model.evaluate(dataset, [metric], transformers)
  assert scores['mean-roc_auc_score'] &gt;= 0.9


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag573')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_weave_models.py: 143-162
</a>
<div class="mid" id="frag573" style="display:none"><pre>
def test_weave_regression_model():
  import numpy as np
  import tensorflow as tf
  tf.random.set_seed(123)
  np.random.seed(123)
  tasks, dataset, transformers, metric = get_dataset(
      'regression', 'Weave', data_points=10)

  batch_size = 10
  model = WeaveModel(
      len(tasks),
      batch_size=batch_size,
      mode='regression',
      dropouts=0,
      learning_rate=0.00003)
  model.fit(dataset, nb_epoch=400)
  scores = model.evaluate(dataset, [metric], transformers)
  assert scores['mean_absolute_error'] &lt; 0.1


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 44:</b> &nbsp; 17 fragments, nominal size 14 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag575')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_layers_from_config.py: 9-23
</a>
<div class="mid" id="frag575" style="display:none"><pre>
def test_interatomic_l2_distance():
  N_atoms = 10
  M_nbrs = 15
  ndim = 20

  layer = dc.models.layers.InteratomicL2Distances(
      N_atoms=N_atoms, M_nbrs=M_nbrs, ndim=ndim)
  config = layer.get_config()
  layer_copied = dc.models.layers.InteratomicL2Distances.from_config(config)

  assert layer_copied.N_atoms == layer.N_atoms
  assert layer_copied.M_nbrs == layer.M_nbrs
  assert layer_copied.ndim == layer.ndim


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag604')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_layers_from_config.py: 496-509
</a>
<div class="mid" id="frag604" style="display:none"><pre>
def test_set_gather():
  M = 10
  batch_size = 16
  n_hidden = 100
  init = 'orthogonal'

  layer = dc.models.layers.SetGather(M, batch_size, n_hidden, init)
  config = layer.get_config()
  layer_copied = dc.models.layers.SetGather.from_config(config)

  assert layer_copied.M == layer.M
  assert layer_copied.batch_size == layer.batch_size
  assert layer_copied.n_hidden == layer.n_hidden
  assert layer_copied.init == layer.init
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag596')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_layers_from_config.py: 353-367
</a>
<div class="mid" id="frag596" style="display:none"><pre>
def test_dtnn_embedding():
  n_embedding = 30
  periodic_table_length = 30
  init = 'glorot_uniform'

  layer = dc.models.layers.DTNNEmbedding(n_embedding, periodic_table_length,
                                         init)
  config = layer.get_config()
  layer_copied = dc.models.layers.DTNNEmbedding.from_config(config)

  assert layer_copied.n_embedding == layer.n_embedding
  assert layer_copied.periodic_table_length == layer.periodic_table_length
  assert layer_copied.init == layer.init


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag602')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_layers_from_config.py: 472-484
</a>
<div class="mid" id="frag602" style="display:none"><pre>
def test_edge_network():
  n_pair_features = 8
  n_hidden = 100
  init = 'glorot_uniform'
  layer = dc.models.layers.EdgeNetwork(n_pair_features, n_hidden, init)
  config = layer.get_config()
  layer_copied = dc.models.layers.EdgeNetwork.from_config(config)

  assert layer_copied.n_pair_features == layer.n_pair_features
  assert layer_copied.n_hidden == layer.n_hidden
  assert layer_copied.init == layer.init


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag597')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_layers_from_config.py: 368-386
</a>
<div class="mid" id="frag597" style="display:none"><pre>
def test_dtnn_step():
  n_embedding = 30
  n_distance = 100
  n_hidden = 60
  init = 'glorot_uniform'
  activation = 'tanh'

  layer = dc.models.layers.DTNNStep(n_embedding, n_distance, n_hidden, init,
                                    activation)
  config = layer.get_config()
  layer_copied = dc.models.layers.DTNNStep.from_config(config)

  assert layer_copied.n_embedding == layer.n_embedding
  assert layer_copied.n_distance == layer.n_distance
  assert layer_copied.n_hidden == layer.n_hidden
  assert layer_copied.init == layer.init
  assert layer_copied.activation == layer.activation


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag601')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_layers_from_config.py: 457-471
</a>
<div class="mid" id="frag601" style="display:none"><pre>
def test_message_passing():
  T = 20
  message_fn = 'enn'
  update_fn = 'gru'
  n_hidden = 100
  layer = dc.models.layers.MessagePassing(T, message_fn, update_fn, n_hidden)
  config = layer.get_config()
  layer_copied = dc.models.layers.MessagePassing.from_config(config)

  assert layer_copied.T == layer.T
  assert layer_copied.message_fn == layer.message_fn
  assert layer_copied.update_fn == layer.update_fn
  assert layer_copied.n_hidden == layer.n_hidden


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag580')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_layers_from_config.py: 92-108
</a>
<div class="mid" id="frag580" style="display:none"><pre>
def test_attn_lstm_embedding():
  n_test = 10
  n_support = 100
  n_feat = 20
  max_depth = 3

  layer = dc.models.layers.AttnLSTMEmbedding(n_test, n_support, n_feat,
                                             max_depth)
  config = layer.get_config()
  layer_copied = dc.models.layers.AttnLSTMEmbedding.from_config(config)

  assert layer_copied.n_test == layer.n_test
  assert layer_copied.n_support == layer.n_support
  assert layer_copied.n_feat == layer.n_feat
  assert layer_copied.max_depth == layer.max_depth


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag581')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_layers_from_config.py: 109-125
</a>
<div class="mid" id="frag581" style="display:none"><pre>
def test_iterref_lstm_embedding():
  n_test = 10
  n_support = 100
  n_feat = 20
  max_depth = 3

  layer = dc.models.layers.IterRefLSTMEmbedding(n_test, n_support, n_feat,
                                                max_depth)
  config = layer.get_config()
  layer_copied = dc.models.layers.IterRefLSTMEmbedding.from_config(config)

  assert layer_copied.n_test == layer.n_test
  assert layer_copied.n_support == layer.n_support
  assert layer_copied.n_feat == layer.n_feat
  assert layer_copied.max_depth == layer.max_depth


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag593')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_layers_from_config.py: 279-293
</a>
<div class="mid" id="frag593" style="display:none"><pre>
def test_highway():
  activation_fn = 'relu'
  biases_initializer = 'zeros'
  weights_initializer = None

  layer = dc.models.layers.Highway(activation_fn, biases_initializer,
                                   weights_initializer)
  config = layer.get_config()
  layer_copied = dc.models.layers.Highway.from_config(config)

  assert layer_copied.activation_fn == layer.activation_fn
  assert layer_copied.biases_initializer == layer.biases_initializer
  assert layer_copied.weights_initializer == layer.weights_initializer


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag589')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_layers_from_config.py: 221-234
</a>
<div class="mid" id="frag589" style="display:none"><pre>
def test_atomic_convolution():
  atom_types = None
  radial_params = list()
  boxsize = None

  layer = dc.models.layers.AtomicConvolution(atom_types, radial_params, boxsize)
  config = layer.get_config()
  layer_copied = dc.models.layers.AtomicConvolution.from_config(config)

  assert layer_copied.atom_types == layer.atom_types
  assert layer_copied.radial_params == layer.radial_params
  assert layer_copied.boxsize == layer.boxsize


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag595')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_layers_from_config.py: 331-352
</a>
<div class="mid" id="frag595" style="display:none"><pre>
def test_weave_gather():
  batch_size = 32
  n_input = 128
  gaussian_expand = True
  compress_post_gaussian_expansion = False
  init = 'glorot_uniform'
  activation = 'tanh'

  layer = dc.models.layers.WeaveGather(batch_size, n_input, gaussian_expand,
                                       compress_post_gaussian_expansion, init,
                                       activation)
  config = layer.get_config()
  layer_copied = dc.models.layers.WeaveGather.from_config(config)

  assert layer_copied.batch_size == layer.batch_size
  assert layer_copied.n_input == layer.n_input
  assert layer_copied.gaussian_expand == layer.gaussian_expand
  assert layer_copied.compress_post_gaussian_expansion == layer.compress_post_gaussian_expansion
  assert layer_copied.init == layer.init
  assert layer_copied.activation == layer.activation


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag588')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_layers_from_config.py: 200-220
</a>
<div class="mid" id="frag588" style="display:none"><pre>
def test_neighbor_list():
  N_atoms = 10
  M_nbrs = 15
  ndim = 20
  nbr_cutoff = 5
  start = 1
  stop = 7

  layer = dc.models.layers.NeighborList(N_atoms, M_nbrs, ndim, nbr_cutoff,
                                        start, stop)
  config = layer.get_config()
  layer_copied = dc.models.layers.VinaFreeEnergy.from_config(config)

  assert layer_copied.N_atoms == layer.N_atoms
  assert layer_copied.M_nbrs == layer.M_nbrs
  assert layer_copied.ndim == layer.ndim
  assert layer_copied.nbr_cutoff == layer.nbr_cutoff
  assert layer_copied.start == layer.start
  assert layer_copied.stop == layer.stop


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag579')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_layers_from_config.py: 70-91
</a>
<div class="mid" id="frag579" style="display:none"><pre>
def test_lstmstep():
  output_dim = 100
  input_dim = 50
  init_fn = 'glorot_uniform'
  inner_init_fn = 'orthogonal'
  activation_fn = 'tanh'
  inner_activation_fn = 'hard_sigmoid'

  layer = dc.models.layers.LSTMStep(output_dim, input_dim, init_fn,
                                    inner_init_fn, activation_fn,
                                    inner_activation_fn)
  config = layer.get_config()
  layer_copied = dc.models.layers.LSTMStep.from_config(config)

  assert layer_copied.output_dim == layer.output_dim
  assert layer_copied.input_dim == layer.input_dim
  assert layer_copied.init == layer.init
  assert layer_copied.inner_init == layer.inner_init
  assert layer_copied.activation == layer.activation
  assert layer_copied.inner_activation == layer.inner_activation


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag598')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_layers_from_config.py: 387-407
</a>
<div class="mid" id="frag598" style="display:none"><pre>
def test_dtnn_gather():
  n_embedding = 30
  n_outputs = 100
  layer_sizes = [100]
  output_activation = True
  init = 'glorot_uniform'
  activation = 'tanh'

  layer = dc.models.layers.DTNNGather(n_embedding, n_outputs, layer_sizes,
                                      output_activation, init, activation)
  config = layer.get_config()
  layer_copied = dc.models.layers.DTNNGather.from_config(config)

  assert layer_copied.n_embedding == layer.n_embedding
  assert layer_copied.n_outputs == layer.n_outputs
  assert layer_copied.layer_sizes == layer.layer_sizes
  assert layer_copied.output_activation == layer.output_activation
  assert layer_copied.init == layer.init
  assert layer_copied.activation == layer.activation


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag600')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_layers_from_config.py: 434-456
</a>
<div class="mid" id="frag600" style="display:none"><pre>
def test_dag_gather():
  n_graph_feat = 30
  n_outputs = 30
  max_atoms = 50
  layer_sizes = [100]
  init = 'glorot_uniform'
  activation = 'relu'
  dropout = None

  layer = dc.models.layers.DAGGather(n_graph_feat, n_outputs, max_atoms,
                                     layer_sizes, init, activation, dropout)
  config = layer.get_config()
  layer_copied = dc.models.layers.DAGGather.from_config(config)

  assert layer_copied.n_graph_feat == layer.n_graph_feat
  assert layer_copied.n_outputs == layer.n_outputs
  assert layer_copied.max_atoms == layer.max_atoms
  assert layer_copied.layer_sizes == layer.layer_sizes
  assert layer_copied.init == layer.init
  assert layer_copied.activation == layer.activation
  assert layer_copied.dropout == layer.dropout


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag587')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_layers_from_config.py: 175-199
</a>
<div class="mid" id="frag587" style="display:none"><pre>
def test_vina_free_energy():
  N_atoms = 10
  M_nbrs = 15
  ndim = 20
  nbr_cutoff = 5
  start = 1
  stop = 7
  stddev = 0.3
  Nrot = 1

  layer = dc.models.layers.VinaFreeEnergy(N_atoms, M_nbrs, ndim, nbr_cutoff,
                                          start, stop, stddev, Nrot)
  config = layer.get_config()
  layer_copied = dc.models.layers.VinaFreeEnergy.from_config(config)

  assert layer_copied.N_atoms == layer.N_atoms
  assert layer_copied.M_nbrs == layer.M_nbrs
  assert layer_copied.ndim == layer.ndim
  assert layer_copied.nbr_cutoff == layer.nbr_cutoff
  assert layer_copied.start == layer.start
  assert layer_copied.stop == layer.stop
  assert layer_copied.stddev == layer.stddev
  assert layer_copied.Nrot == layer_copied.Nrot


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag599')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_layers_from_config.py: 408-433
</a>
<div class="mid" id="frag599" style="display:none"><pre>
def test_dag():
  n_graph_feat = 30
  n_atom_feat = 75
  max_atoms = 50
  layer_sizes = [100]
  init = 'glorot_uniform'
  activation = 'relu'
  dropout = None
  batch_size = 64

  layer = dc.models.layers.DAGLayer(n_graph_feat, n_atom_feat, max_atoms,
                                    layer_sizes, init, activation, dropout,
                                    batch_size)
  config = layer.get_config()
  layer_copied = dc.models.layers.DAGLayer.from_config(config)

  assert layer_copied.n_graph_feat == layer.n_graph_feat
  assert layer_copied.n_atom_feat == layer.n_atom_feat
  assert layer_copied.max_atoms == layer.max_atoms
  assert layer_copied.layer_sizes == layer.layer_sizes
  assert layer_copied.init == layer.init
  assert layer_copied.activation == layer.activation
  assert layer_copied.dropout == layer.dropout
  assert layer_copied.batch_size == layer.batch_size


</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 45:</b> &nbsp; 15 fragments, nominal size 23 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag607')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_robust.py: 6-36
</a>
<div class="mid" id="frag607" style="display:none"><pre>
def test_singletask_robust_multitask_classification():
  """Test robust multitask singletask classification."""
  n_tasks = 1
  n_samples = 10
  n_features = 3
  n_classes = 2

  # Generate dummy dataset
  np.random.seed(123)
  ids = np.arange(n_samples)
  X = np.random.rand(n_samples, n_features)
  y = np.zeros((n_samples, n_tasks))
  w = np.ones((n_samples, n_tasks))
  dataset = dc.data.NumpyDataset(X, y, w, ids)

  classification_metric = dc.metrics.Metric(
      dc.metrics.accuracy_score, task_averager=np.mean)
  model = dc.models.RobustMultitaskClassifier(
      n_tasks,
      n_features,
      layer_sizes=[50],
      bypass_layer_sizes=[10],
      dropouts=[0.],
      learning_rate=0.003,
      weight_init_stddevs=[.1],
      batch_size=n_samples)

  # Fit trained model
  model.fit(dataset, nb_epoch=1)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag697')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_overfit.py: 380-414
</a>
<div class="mid" id="frag697" style="display:none"><pre>
def test_robust_multitask_classification_overfit():
  """Test robust multitask overfits tiny data."""
  n_tasks = 10
  n_samples = 10
  n_features = 3
  n_classes = 2

  # Generate dummy dataset
  np.random.seed(123)
  ids = np.arange(n_samples)
  X = np.random.rand(n_samples, n_features)
  y = np.zeros((n_samples, n_tasks))
  w = np.ones((n_samples, n_tasks))
  dataset = dc.data.NumpyDataset(X, y, w, ids)

  classification_metric = dc.metrics.Metric(
      dc.metrics.accuracy_score, task_averager=np.mean)
  model = dc.models.RobustMultitaskClassifier(
      n_tasks,
      n_features,
      layer_sizes=[50],
      bypass_layer_sizes=[10],
      dropouts=[0.],
      learning_rate=0.003,
      weight_init_stddevs=[.1],
      batch_size=n_samples)

  # Fit trained model
  model.fit(dataset, nb_epoch=25)

  # Eval model on train
  scores = model.evaluate(dataset, [classification_metric])
  assert scores[classification_metric.name] &gt; .9


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag704')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_overfit.py: 579-614
</a>
<div class="mid" id="frag704" style="display:none"><pre>
def test_progressive_classification_overfit():
  """Test progressive multitask overfits tiny data."""
  np.random.seed(123)
  n_tasks = 5
  n_samples = 10
  n_features = 6

  # Generate dummy dataset
  np.random.seed(123)
  ids = np.arange(n_samples)
  X = np.random.rand(n_samples, n_features)
  y = np.random.randint(2, size=(n_samples, n_tasks))
  w = np.ones((n_samples, n_tasks))

  dataset = dc.data.NumpyDataset(X, y, w, ids)

  metric = dc.metrics.Metric(dc.metrics.accuracy_score, task_averager=np.mean)
  model = dc.models.ProgressiveMultitaskClassifier(
      n_tasks,
      n_features,
      layer_sizes=[50],
      bypass_layer_sizes=[10],
      dropouts=[0.],
      learning_rate=0.001,
      weight_init_stddevs=[.1],
      alpha_init_stddevs=[.02],
      batch_size=n_samples)

  # Fit trained model
  model.fit(dataset, nb_epoch=300)

  # Eval model on train
  scores = model.evaluate(dataset, [metric])
  assert scores[metric.name] &gt; .9


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag703')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_overfit.py: 541-578
</a>
<div class="mid" id="frag703" style="display:none"><pre>
def test_robust_multitask_regression_overfit():
  """Test robust multitask overfits tiny data."""
  np.random.seed(123)
  tf.random.set_seed(123)
  n_tasks = 10
  n_samples = 10
  n_features = 3
  n_classes = 2

  # Generate dummy dataset
  np.random.seed(123)
  ids = np.arange(n_samples)
  X = np.random.rand(n_samples, n_features)
  y = np.zeros((n_samples, n_tasks))
  w = np.ones((n_samples, n_tasks))

  dataset = dc.data.NumpyDataset(X, y, w, ids)

  regression_metric = dc.metrics.Metric(
      dc.metrics.mean_squared_error, task_averager=np.mean, mode="regression")
  model = dc.models.RobustMultitaskRegressor(
      n_tasks,
      n_features,
      layer_sizes=[50],
      bypass_layer_sizes=[10],
      dropouts=[0.],
      learning_rate=0.003,
      weight_init_stddevs=[.1],
      batch_size=n_samples)

  # Fit trained model
  model.fit(dataset, nb_epoch=25)

  # Eval model on train
  scores = model.evaluate(dataset, [regression_metric])
  assert scores[regression_metric.name] &lt; .2


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag608')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_robust.py: 37-68
</a>
<div class="mid" id="frag608" style="display:none"><pre>
def test_singletask_robust_multitask_regression():
  """Test singletask robust multitask regression."""
  np.random.seed(123)
  tf.random.set_seed(123)
  n_tasks = 1
  n_samples = 10
  n_features = 3
  n_classes = 2

  # Generate dummy dataset
  np.random.seed(123)
  ids = np.arange(n_samples)
  X = np.random.rand(n_samples, n_features)
  y = np.zeros((n_samples, n_tasks))
  w = np.ones((n_samples, n_tasks))

  dataset = dc.data.NumpyDataset(X, y, w, ids)

  regression_metric = dc.metrics.Metric(
      dc.metrics.mean_squared_error, task_averager=np.mean, mode="regression")
  model = dc.models.RobustMultitaskRegressor(
      n_tasks,
      n_features,
      layer_sizes=[50],
      bypass_layer_sizes=[10],
      dropouts=[0.],
      learning_rate=0.003,
      weight_init_stddevs=[.1],
      batch_size=n_samples)

  # Fit trained model
  model.fit(dataset, nb_epoch=1)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag705')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_overfit.py: 615-650
</a>
<div class="mid" id="frag705" style="display:none"><pre>
def test_progressive_regression_overfit():
  """Test progressive multitask overfits tiny data."""
  np.random.seed(123)
  n_tasks = 5
  n_samples = 10
  n_features = 6

  # Generate dummy dataset
  np.random.seed(123)
  ids = np.arange(n_samples)
  X = np.random.rand(n_samples, n_features)
  y = np.random.rand(n_samples, n_tasks)
  w = np.ones((n_samples, n_tasks))

  dataset = dc.data.NumpyDataset(X, y, w, ids)

  metric = dc.metrics.Metric(dc.metrics.rms_score, task_averager=np.mean)
  model = dc.models.ProgressiveMultitaskRegressor(
      n_tasks,
      n_features,
      layer_sizes=[50],
      bypass_layer_sizes=[10],
      dropouts=[0.],
      learning_rate=0.002,
      weight_init_stddevs=[.1],
      alpha_init_stddevs=[.02],
      batch_size=n_samples)

  # Fit trained model
  model.fit(dataset, nb_epoch=200)

  # Eval model on train
  scores = model.evaluate(dataset, [metric])
  assert scores[metric.name] &lt; .2


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag689')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_overfit.py: 134-165
</a>
<div class="mid" id="frag689" style="display:none"><pre>
def test_classification_overfit():
  """Test that MultitaskClassifier can overfit simple classification datasets."""
  n_samples = 10
  n_features = 3
  n_tasks = 1
  n_classes = 2

  # Generate dummy dataset
  np.random.seed(123)
  ids = np.arange(n_samples)
  X = np.random.rand(n_samples, n_features)
  y = np.zeros((n_samples, n_tasks))
  w = np.ones((n_samples, n_tasks))
  dataset = dc.data.NumpyDataset(X, y, w, ids)

  classification_metric = dc.metrics.Metric(dc.metrics.accuracy_score)
  model = dc.models.MultitaskClassifier(
      n_tasks,
      n_features,
      dropouts=[0.],
      weight_init_stddevs=[.1],
      batch_size=n_samples,
      optimizer=Adam(learning_rate=0.0003, beta1=0.9, beta2=0.999))

  # Fit trained model
  model.fit(dataset, nb_epoch=100)

  # Eval model on train
  scores = model.evaluate(dataset, [classification_metric])
  assert scores[classification_metric.name] &gt; .9


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag696')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_overfit.py: 347-379
</a>
<div class="mid" id="frag696" style="display:none"><pre>
def test_multitask_classification_overfit():
  """Test MultitaskClassifier overfits tiny data."""
  n_tasks = 10
  n_samples = 10
  n_features = 3
  n_classes = 2

  # Generate dummy dataset
  np.random.seed(123)
  ids = np.arange(n_samples)
  X = np.random.rand(n_samples, n_features)
  y = np.zeros((n_samples, n_tasks))
  w = np.ones((n_samples, n_tasks))
  dataset = dc.data.NumpyDataset(X, y, w, ids)

  classification_metric = dc.metrics.Metric(
      dc.metrics.accuracy_score, task_averager=np.mean, n_tasks=n_tasks)
  model = dc.models.MultitaskClassifier(
      n_tasks,
      n_features,
      dropouts=[0.],
      weight_init_stddevs=[.1],
      batch_size=n_samples,
      optimizer=Adam(learning_rate=0.0003, beta1=0.9, beta2=0.999))

  # Fit trained model
  model.fit(dataset)

  # Eval model on train
  scores = model.evaluate(dataset, [classification_metric])
  assert scores[classification_metric.name] &gt; .9


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag702')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_overfit.py: 507-540
</a>
<div class="mid" id="frag702" style="display:none"><pre>
def test_residual_regression_overfit():
  """Test that a residual multitask network can overfit tiny data."""
  n_tasks = 10
  n_samples = 10
  n_features = 10
  n_classes = 2

  # Generate dummy dataset
  np.random.seed(123)
  ids = np.arange(n_samples)
  X = np.random.rand(n_samples, n_features)
  y = np.random.rand(n_samples, n_tasks)
  w = np.ones((n_samples, n_tasks))

  dataset = dc.data.NumpyDataset(X, y, w, ids)

  regression_metric = dc.metrics.Metric(
      dc.metrics.mean_squared_error, task_averager=np.mean, mode="regression")
  model = dc.models.MultitaskRegressor(
      n_tasks,
      n_features,
      layer_sizes=[20] * 10,
      dropouts=0.0,
      batch_size=n_samples,
      residual=True)

  # Fit trained model
  model.fit(dataset, nb_epoch=1000)

  # Eval model on train
  scores = model.evaluate(dataset, [regression_metric])
  assert scores[regression_metric.name] &lt; .02


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag692')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_overfit.py: 233-267
</a>
<div class="mid" id="frag692" style="display:none"><pre>
def test_skewed_classification_overfit():
  """Test MultitaskClassifier can overfit 0/1 datasets with few actives."""
  #n_samples = 100
  n_samples = 100
  n_features = 3
  n_tasks = 1
  n_classes = 2

  # Generate dummy dataset
  np.random.seed(123)
  p = .05
  ids = np.arange(n_samples)
  X = np.random.rand(n_samples, n_features)
  y = np.random.binomial(1, p, size=(n_samples, n_tasks))
  w = np.ones((n_samples, n_tasks))

  dataset = dc.data.NumpyDataset(X, y, w, ids)

  classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
  model = dc.models.MultitaskClassifier(
      n_tasks,
      n_features,
      dropouts=[0.],
      weight_init_stddevs=[.1],
      batch_size=n_samples,
      learning_rate=0.003)

  # Fit trained model
  model.fit(dataset, nb_epoch=100)

  # Eval model on train
  scores = model.evaluate(dataset, [classification_metric])
  assert scores[classification_metric.name] &gt; .75


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag690')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_overfit.py: 166-197
</a>
<div class="mid" id="frag690" style="display:none"><pre>
def test_residual_classification_overfit():
  """Test that a residual network can overfit simple classification datasets."""
  n_samples = 10
  n_features = 5
  n_tasks = 1
  n_classes = 2

  # Generate dummy dataset
  np.random.seed(123)
  ids = np.arange(n_samples)
  X = np.random.rand(n_samples, n_features)
  y = np.random.randint(2, size=(n_samples, n_tasks))
  w = np.ones((n_samples, n_tasks))
  dataset = dc.data.NumpyDataset(X, y, w, ids)

  classification_metric = dc.metrics.Metric(dc.metrics.accuracy_score)
  model = dc.models.MultitaskClassifier(
      n_tasks,
      n_features,
      layer_sizes=[20] * 10,
      dropouts=0.0,
      batch_size=n_samples,
      residual=True)

  # Fit trained model
  model.fit(dataset, nb_epoch=500)

  # Eval model on train
  scores = model.evaluate(dataset, [classification_metric])
  assert scores[classification_metric.name] &gt; .9


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag688')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_overfit.py: 102-133
</a>
<div class="mid" id="frag688" style="display:none"><pre>
def test_regression_overfit():
  """Test that MultitaskRegressor can overfit simple regression datasets."""
  n_samples = 10
  n_features = 3
  n_tasks = 1

  # Generate dummy dataset
  np.random.seed(123)
  ids = np.arange(n_samples)
  X = np.random.rand(n_samples, n_features)
  y = np.zeros((n_samples, n_tasks))
  w = np.ones((n_samples, n_tasks))
  dataset = dc.data.NumpyDataset(X, y, w, ids)

  regression_metric = dc.metrics.Metric(dc.metrics.mean_squared_error)
  # TODO(rbharath): This breaks with optimizer="momentum". Why?
  model = dc.models.MultitaskRegressor(
      n_tasks,
      n_features,
      dropouts=[0.],
      weight_init_stddevs=[np.sqrt(6) / np.sqrt(1000)],
      batch_size=n_samples,
      learning_rate=0.003)

  # Fit trained model
  model.fit(dataset, nb_epoch=100)

  # Eval model on train
  scores = model.evaluate(dataset, [regression_metric])
  assert scores[regression_metric.name] &lt; .1


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag701')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_overfit.py: 478-506
</a>
<div class="mid" id="frag701" style="display:none"><pre>
def test_multitask_regression_overfit():
  """Test MultitaskRegressor overfits tiny data."""
  n_tasks = 10
  n_samples = 10
  n_features = 10
  n_classes = 2

  # Generate dummy dataset
  np.random.seed(123)
  ids = np.arange(n_samples)
  X = np.random.rand(n_samples, n_features)
  y = np.random.rand(n_samples, n_tasks)
  w = np.ones((n_samples, n_tasks))

  dataset = dc.data.NumpyDataset(X, y, w, ids)

  regression_metric = dc.metrics.Metric(
      dc.metrics.mean_squared_error, task_averager=np.mean, mode="regression")
  model = dc.models.MultitaskRegressor(
      n_tasks, n_features, dropouts=0.0, batch_size=n_samples)

  # Fit trained model
  model.fit(dataset, nb_epoch=1000)

  # Eval model on train
  scores = model.evaluate(dataset, [regression_metric])
  assert scores[regression_metric.name] &lt; .02


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag693')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_overfit.py: 268-312
</a>
<div class="mid" id="frag693" style="display:none"><pre>
def test_skewed_missing_classification_overfit():
  """MultitaskClassifier, skewed data, few actives

  Test MultitaskClassifier overfit 0/1 datasets with missing data and few
  actives. This is intended to be as close to singletask MUV datasets as
  possible.
  """
  n_samples = 5120
  n_features = 6
  n_tasks = 1
  n_classes = 2

  # Generate dummy dataset
  np.random.seed(123)
  p = .002
  ids = np.arange(n_samples)
  X = np.random.rand(n_samples, n_features)
  y = np.random.binomial(1, p, size=(n_samples, n_tasks))
  w = np.ones((n_samples, n_tasks))
  y_flat, w_flat = np.squeeze(y), np.squeeze(w)
  y_nonzero = y_flat[w_flat != 0]
  num_nonzero = np.count_nonzero(y_nonzero)
  weight_nonzero = len(y_nonzero) / num_nonzero
  w_flat[y_flat != 0] = weight_nonzero
  w = np.reshape(w_flat, (n_samples, n_tasks))

  dataset = dc.data.DiskDataset.from_numpy(X, y, w, ids)

  classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
  model = dc.models.MultitaskClassifier(
      n_tasks,
      n_features,
      dropouts=[0.],
      weight_init_stddevs=[1.],
      batch_size=n_samples,
      learning_rate=0.003)

  # Fit trained model
  model.fit(dataset, nb_epoch=100)

  # Eval model on train
  scores = model.evaluate(dataset, [classification_metric])
  assert scores[classification_metric.name] &gt; .7


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag691')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_overfit.py: 199-232
</a>
<div class="mid" id="frag691" style="display:none"><pre>
def test_fittransform_regression_overfit():
  """Test that MultitaskFitTransformRegressor can overfit simple regression datasets."""
  n_samples = 10
  n_features = 3
  n_tasks = 1

  # Generate dummy dataset
  np.random.seed(123)
  tf.random.set_seed(123)
  ids = np.arange(n_samples)
  X = np.random.rand(n_samples, n_features, n_features)
  y = np.zeros((n_samples, n_tasks))
  w = np.ones((n_samples, n_tasks))
  dataset = dc.data.NumpyDataset(X, y, w, ids)

  fit_transformers = [dc.trans.CoulombFitTransformer(dataset)]
  regression_metric = dc.metrics.Metric(dc.metrics.mean_squared_error)
  model = dc.models.MultitaskFitTransformRegressor(
      n_tasks, [n_features, n_features],
      dropouts=[0.01],
      weight_init_stddevs=[np.sqrt(6) / np.sqrt(1000)],
      batch_size=n_samples,
      fit_transformers=fit_transformers,
      n_evals=1,
      optimizer=Adam(learning_rate=0.003, beta1=0.9, beta2=0.999))

  # Fit trained model
  model.fit(dataset, nb_epoch=100)

  # Eval model on train
  scores = model.evaluate(dataset, [regression_metric])
  assert scores[regression_metric.name] &lt; .1


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 46:</b> &nbsp; 4 fragments, nominal size 19 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag638')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_kerasmodel.py: 39-60
</a>
<div class="mid" id="frag638" style="display:none"><pre>
def test_overfit_sequential_model():
  """Test fitting a KerasModel defined as a sequential model."""
  n_data_points = 10
  n_features = 2
  X = np.random.rand(n_data_points, n_features)
  y = (X[:, 0] &gt; X[:, 1]).astype(np.float32)
  dataset = dc.data.NumpyDataset(X, y)
  keras_model = tf.keras.Sequential([
      tf.keras.layers.Dense(10, activation='relu'),
      tf.keras.layers.Dense(1, activation='sigmoid')
  ])
  model = dc.models.KerasModel(
      keras_model, dc.models.losses.BinaryCrossEntropy(), learning_rate=0.005)
  model.fit(dataset, nb_epoch=1000)
  prediction = np.squeeze(model.predict_on_batch(X))
  assert np.array_equal(y, np.round(prediction))
  metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
  generator = model.default_generator(dataset, pad_batches=False)
  scores = model.evaluate_generator(generator, [metric])
  assert scores[metric.name] &gt; 0.9


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag640')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_kerasmodel.py: 84-108
</a>
<div class="mid" id="frag640" style="display:none"><pre>
def test_fit_on_batch():
  """Test fitting a KerasModel to individual batches."""
  n_data_points = 10
  n_features = 2
  X = np.random.rand(n_data_points, n_features)
  y = (X[:, 0] &gt; X[:, 1]).astype(np.float32)
  dataset = dc.data.NumpyDataset(X, y)
  keras_model = tf.keras.Sequential([
      tf.keras.layers.Dense(10, activation='relu'),
      tf.keras.layers.Dense(1, activation='sigmoid')
  ])
  model = dc.models.KerasModel(
      keras_model, dc.models.losses.BinaryCrossEntropy(), learning_rate=0.005)
  i = 0
  for X, y, w, ids in dataset.iterbatches(model.batch_size, 500):
    i += 1
    model.fit_on_batch(X, y, w, checkpoint=False)
  prediction = np.squeeze(model.predict_on_batch(X))
  assert np.array_equal(y, np.round(prediction))
  metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
  generator = model.default_generator(dataset, pad_batches=False)
  scores = model.evaluate_generator(generator, [metric])
  assert scores[metric.name] &gt; 0.9


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag792')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_torch_model.py: 56-76
</a>
<div class="mid" id="frag792" style="display:none"><pre>
def test_overfit_sequential_model():
  """Test fitting a TorchModel defined as a sequential model."""
  n_data_points = 10
  n_features = 2
  X = np.random.rand(n_data_points, n_features)
  y = (X[:, 0] &gt; X[:, 1]).astype(np.float32)
  dataset = dc.data.NumpyDataset(X, y)
  pytorch_model = torch.nn.Sequential(
      torch.nn.Linear(2, 10), torch.nn.ReLU(), torch.nn.Linear(10, 1),
      torch.nn.Sigmoid())
  model = dc.models.TorchModel(
      pytorch_model, dc.models.losses.BinaryCrossEntropy(), learning_rate=0.005)
  model.fit(dataset, nb_epoch=1000)
  prediction = np.squeeze(model.predict_on_batch(X))
  assert np.array_equal(y, np.round(prediction))
  metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
  generator = model.default_generator(dataset, pad_batches=False)
  scores = model.evaluate_generator(generator, [metric])
  assert scores[metric.name] &gt; 0.9


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag794')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_torch_model.py: 101-124
</a>
<div class="mid" id="frag794" style="display:none"><pre>
def test_fit_on_batch():
  """Test fitting a TorchModel to individual batches."""
  n_data_points = 10
  n_features = 2
  X = np.random.rand(n_data_points, n_features)
  y = (X[:, 0] &gt; X[:, 1]).astype(np.float32)
  dataset = dc.data.NumpyDataset(X, y)
  pytorch_model = torch.nn.Sequential(
      torch.nn.Linear(2, 10), torch.nn.ReLU(), torch.nn.Linear(10, 1),
      torch.nn.Sigmoid())
  model = dc.models.TorchModel(
      pytorch_model, dc.models.losses.BinaryCrossEntropy(), learning_rate=0.005)
  i = 0
  for X, y, w, ids in dataset.iterbatches(model.batch_size, 500):
    i += 1
    model.fit_on_batch(X, y, w, checkpoint=False)
  prediction = np.squeeze(model.predict_on_batch(X))
  assert np.array_equal(y, np.round(prediction))
  metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
  generator = model.default_generator(dataset, pad_batches=False)
  scores = model.evaluate_generator(generator, [metric])
  assert scores[metric.name] &gt; 0.9


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 47:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag639')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_kerasmodel.py: 61-83
</a>
<div class="mid" id="frag639" style="display:none"><pre>
def test_fit_use_all_losses():
  """Test fitting a KerasModel and getting a loss curve back."""
  n_data_points = 10
  n_features = 2
  X = np.random.rand(n_data_points, n_features)
  y = (X[:, 0] &gt; X[:, 1]).astype(np.float32)
  dataset = dc.data.NumpyDataset(X, y)
  keras_model = tf.keras.Sequential([
      tf.keras.layers.Dense(10, activation='relu'),
      tf.keras.layers.Dense(1, activation='sigmoid')
  ])
  model = dc.models.KerasModel(
      keras_model,
      dc.models.losses.BinaryCrossEntropy(),
      learning_rate=0.005,
      log_frequency=10)
  losses = []
  model.fit(dataset, nb_epoch=1000, all_losses=losses)
  # Each epoch is a single step for this model
  assert len(losses) == 100
  assert np.count_nonzero(np.array(losses)) == 100


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag793')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_torch_model.py: 78-99
</a>
<div class="mid" id="frag793" style="display:none"><pre>
def test_fit_use_all_losses():
  """Test fitting a TorchModel and getting a loss curve back."""
  n_data_points = 10
  n_features = 2
  X = np.random.rand(n_data_points, n_features)
  y = (X[:, 0] &gt; X[:, 1]).astype(np.float32)
  dataset = dc.data.NumpyDataset(X, y)
  pytorch_model = torch.nn.Sequential(
      torch.nn.Linear(2, 10), torch.nn.ReLU(), torch.nn.Linear(10, 1),
      torch.nn.Sigmoid())
  model = dc.models.TorchModel(
      pytorch_model,
      dc.models.losses.BinaryCrossEntropy(),
      learning_rate=0.005,
      log_frequency=10)
  losses = []
  model.fit(dataset, nb_epoch=1000, all_losses=losses)
  # Each epoch is a single step for this model
  assert len(losses) == 100
  assert np.count_nonzero(np.array(losses)) == 100


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 48:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 87%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag641')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_kerasmodel.py: 109-136
</a>
<div class="mid" id="frag641" style="display:none"><pre>
def test_checkpointing():
  """Test loading and saving checkpoints with KerasModel."""
  # Create two models using the same model directory.

  keras_model1 = tf.keras.Sequential([tf.keras.layers.Dense(10)])
  keras_model2 = tf.keras.Sequential([tf.keras.layers.Dense(10)])
  model1 = dc.models.KerasModel(keras_model1, dc.models.losses.L2Loss())
  model2 = dc.models.KerasModel(
      keras_model2, dc.models.losses.L2Loss(), model_dir=model1.model_dir)

  # Check that they produce different results.

  X = np.random.rand(5, 5)
  y1 = model1.predict_on_batch(X)
  y2 = model2.predict_on_batch(X)
  assert not np.array_equal(y1, y2)

  # Save a checkpoint from the first model and load it into the second one,
  # and make sure they now match.

  model1.save_checkpoint()
  model2.restore()
  y3 = model1.predict_on_batch(X)
  y4 = model2.predict_on_batch(X)
  assert np.array_equal(y1, y3)
  assert np.array_equal(y1, y4)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag795')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_torch_model.py: 126-153
</a>
<div class="mid" id="frag795" style="display:none"><pre>
def test_checkpointing():
  """Test loading and saving checkpoints with TorchModel."""
  # Create two models using the same model directory.

  pytorch_model1 = torch.nn.Sequential(torch.nn.Linear(5, 10))
  pytorch_model2 = torch.nn.Sequential(torch.nn.Linear(5, 10))
  model1 = dc.models.TorchModel(pytorch_model1, dc.models.losses.L2Loss())
  model2 = dc.models.TorchModel(
      pytorch_model2, dc.models.losses.L2Loss(), model_dir=model1.model_dir)

  # Check that they produce different results.

  X = np.random.rand(5, 5)
  y1 = model1.predict_on_batch(X)
  y2 = model2.predict_on_batch(X)
  assert not np.array_equal(y1, y2)

  # Save a checkpoint from the first model and load it into the second one,
  # and make sure they now match.

  model1.save_checkpoint()
  model2.restore()
  y3 = model1.predict_on_batch(X)
  y4 = model2.predict_on_batch(X)
  assert np.array_equal(y1, y3)
  assert np.array_equal(y1, y4)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 49:</b> &nbsp; 2 fragments, nominal size 24 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag642')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_kerasmodel.py: 137-172
</a>
<div class="mid" id="frag642" style="display:none"><pre>
def test_fit_restore():
  """Test specifying restore=True when calling fit()."""
  n_data_points = 10
  n_features = 2
  X = np.random.rand(n_data_points, n_features)
  y = (X[:, 0] &gt; X[:, 1]).astype(np.float32)
  dataset = dc.data.NumpyDataset(X, y)

  # Train a model to overfit the dataset.

  keras_model = tf.keras.Sequential([
      tf.keras.layers.Dense(10, activation='relu'),
      tf.keras.layers.Dense(1, activation='sigmoid')
  ])
  model = dc.models.KerasModel(
      keras_model, dc.models.losses.BinaryCrossEntropy(), learning_rate=0.005)
  model.fit(dataset, nb_epoch=1000)
  prediction = np.squeeze(model.predict_on_batch(X))
  assert np.array_equal(y, np.round(prediction))

  # Create an identical model, do a single step of fitting with restore=True,
  # and make sure it got restored correctly.

  keras_model2 = tf.keras.Sequential([
      tf.keras.layers.Dense(10, activation='relu'),
      tf.keras.layers.Dense(1, activation='sigmoid')
  ])
  model2 = dc.models.KerasModel(
      keras_model2,
      dc.models.losses.BinaryCrossEntropy(),
      model_dir=model.model_dir)
  model2.fit(dataset, nb_epoch=1, restore=True)
  prediction = np.squeeze(model2.predict_on_batch(X))
  assert np.array_equal(y, np.round(prediction))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag796')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_torch_model.py: 155-188
</a>
<div class="mid" id="frag796" style="display:none"><pre>
def test_fit_restore():
  """Test specifying restore=True when calling fit()."""
  n_data_points = 10
  n_features = 2
  X = np.random.rand(n_data_points, n_features)
  y = (X[:, 0] &gt; X[:, 1]).astype(np.float32)
  dataset = dc.data.NumpyDataset(X, y)

  # Train a model to overfit the dataset.

  pytorch_model = torch.nn.Sequential(
      torch.nn.Linear(2, 10), torch.nn.ReLU(), torch.nn.Linear(10, 1),
      torch.nn.Sigmoid())
  model = dc.models.TorchModel(
      pytorch_model, dc.models.losses.BinaryCrossEntropy(), learning_rate=0.005)
  model.fit(dataset, nb_epoch=1000)
  prediction = np.squeeze(model.predict_on_batch(X))
  assert np.array_equal(y, np.round(prediction))

  # Create an identical model, do a single step of fitting with restore=True,
  # and make sure it got restored correctly.

  pytorch_model2 = torch.nn.Sequential(
      torch.nn.Linear(2, 10), torch.nn.ReLU(), torch.nn.Linear(10, 1),
      torch.nn.Sigmoid())
  model2 = dc.models.TorchModel(
      pytorch_model2,
      dc.models.losses.BinaryCrossEntropy(),
      model_dir=model.model_dir)
  model2.fit(dataset, nb_epoch=1, restore=True)
  prediction = np.squeeze(model2.predict_on_batch(X))
  assert np.array_equal(y, np.round(prediction))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 50:</b> &nbsp; 2 fragments, nominal size 20 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag646')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_kerasmodel.py: 233-260
</a>
<div class="mid" id="frag646" style="display:none"><pre>
def test_saliency_mapping():
  """Test computing a saliency map."""
  n_tasks = 3
  n_features = 5
  keras_model = tf.keras.Sequential([
      tf.keras.layers.Dense(20, activation='tanh'),
      tf.keras.layers.Dense(n_tasks)
  ])
  model = dc.models.KerasModel(keras_model, dc.models.losses.L2Loss())
  x = np.random.random(n_features)
  s = model.compute_saliency(x)
  assert s.shape[0] == n_tasks
  assert s.shape[1] == n_features

  # Take a tiny step in the direction of s and see if the output changes by
  # the expected amount.

  delta = 0.01
  for task in range(n_tasks):
    norm = np.sqrt(np.sum(s[task]**2))
    step = 0.5 * delta / norm
    pred1 = model.predict_on_batch((x + s[task] * step).reshape(
        (1, n_features))).flatten()
    pred2 = model.predict_on_batch((x - s[task] * step).reshape(
        (1, n_features))).flatten()
    assert np.allclose(pred1[task], (pred2 + norm * delta)[task])


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag802')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_torch_model.py: 260-286
</a>
<div class="mid" id="frag802" style="display:none"><pre>
def test_saliency_mapping():
  """Test computing a saliency map."""
  n_tasks = 3
  n_features = 5
  pytorch_model = torch.nn.Sequential(
      torch.nn.Linear(n_features, 20), torch.nn.Tanh(),
      torch.nn.Linear(20, n_tasks))
  model = dc.models.TorchModel(pytorch_model, dc.models.losses.L2Loss())
  x = np.random.random(n_features)
  s = model.compute_saliency(x)
  assert s.shape[0] == n_tasks
  assert s.shape[1] == n_features

  # Take a tiny step in the direction of s and see if the output changes by
  # the expected amount.

  delta = 0.01
  for task in range(n_tasks):
    norm = np.sqrt(np.sum(s[task]**2))
    step = 0.5 * delta / norm
    pred1 = model.predict_on_batch((x + s[task] * step).reshape(
        (1, n_features))).flatten()
    pred2 = model.predict_on_batch((x - s[task] * step).reshape(
        (1, n_features))).flatten()
    assert np.allclose(pred1[task], (pred2 + norm * delta)[task], atol=1e-6)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 51:</b> &nbsp; 2 fragments, nominal size 21 lines, similarity 85%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag648')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_kerasmodel.py: 276-299
</a>
<div class="mid" id="frag648" style="display:none"><pre>
def test_tensorboard():
  """Test logging to Tensorboard."""
  n_data_points = 20
  n_features = 2
  X = np.random.rand(n_data_points, n_features)
  y = [[0.0, 1.0] for x in range(n_data_points)]
  dataset = dc.data.NumpyDataset(X, y)
  keras_model = tf.keras.Sequential([
      tf.keras.layers.Dense(2, activation='softmax'),
  ])
  model = dc.models.KerasModel(
      keras_model,
      dc.models.losses.CategoricalCrossEntropy(),
      tensorboard=True,
      log_frequency=1)
  model.fit(dataset, nb_epoch=10)
  files_in_dir = os.listdir(model.model_dir)
  event_file = list(filter(lambda x: x.startswith("events"), files_in_dir))
  assert len(event_file) &gt; 0
  event_file = os.path.join(model.model_dir, event_file[0])
  file_size = os.stat(event_file).st_size
  assert file_size &gt; 0


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag806')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_torch_model.py: 314-336
</a>
<div class="mid" id="frag806" style="display:none"><pre>
def test_tensorboard():
  """Test logging to Tensorboard."""
  n_data_points = 20
  n_features = 2
  X = np.random.rand(n_data_points, n_features)
  y = [[0.0, 1.0] for x in range(n_data_points)]
  dataset = dc.data.NumpyDataset(X, y)
  pytorch_model = torch.nn.Sequential(
      torch.nn.Linear(n_features, 2), torch.nn.Softmax(dim=1))
  model = dc.models.TorchModel(
      pytorch_model,
      dc.models.losses.CategoricalCrossEntropy(),
      tensorboard=True,
      log_frequency=1)
  model.fit(dataset, nb_epoch=10)
  files_in_dir = os.listdir(model.model_dir)
  event_file = list(filter(lambda x: x.startswith("events"), files_in_dir))
  assert len(event_file) &gt; 0
  event_file = os.path.join(model.model_dir, event_file[0])
  file_size = os.stat(event_file).st_size
  assert file_size &gt; 0


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 52:</b> &nbsp; 4 fragments, nominal size 26 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag649')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_kerasmodel.py: 300-335
</a>
<div class="mid" id="frag649" style="display:none"><pre>
def test_fit_variables():
  """Test training a subset of the variables in a model."""

  class VarModel(tf.keras.Model):

    def __init__(self, **kwargs):
      super(VarModel, self).__init__(**kwargs)
      self.var1 = tf.Variable([0.5])
      self.var2 = tf.Variable([0.5])

    def call(self, inputs, training=False):
      return [self.var1, self.var2]

  def loss(outputs, labels, weights):
    return (outputs[0] * outputs[1] - labels[0])**2

  keras_model = VarModel()
  model = dc.models.KerasModel(keras_model, loss, learning_rate=0.01)
  x = np.ones((1, 1))
  vars = model.predict_on_batch(x)
  assert np.allclose(vars[0], 0.5)
  assert np.allclose(vars[1], 0.5)
  model.fit_generator([(x, x, x)] * 300)
  vars = model.predict_on_batch(x)
  assert np.allclose(vars[0], 1.0)
  assert np.allclose(vars[1], 1.0)
  model.fit_generator([(x, 2 * x, x)] * 300, variables=[keras_model.var1])
  vars = model.predict_on_batch(x)
  assert np.allclose(vars[0], 2.0)
  assert np.allclose(vars[1], 1.0)
  model.fit_generator([(x, x, x)] * 300, variables=[keras_model.var2])
  vars = model.predict_on_batch(x)
  assert np.allclose(vars[0], 2.0)
  assert np.allclose(vars[1], 0.5)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag653')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_kerasmodel.py: 336-367
</a>
<div class="mid" id="frag653" style="display:none"><pre>
def test_fit_loss():
  """Test specifying a different loss function when calling fit()."""

  class VarModel(tf.keras.Model):

    def __init__(self, **kwargs):
      super(VarModel, self).__init__(**kwargs)
      self.var1 = tf.Variable([0.5])
      self.var2 = tf.Variable([0.5])

    def call(self, inputs, training=False):
      return [self.var1, self.var2]

  def loss1(outputs, labels, weights):
    return (outputs[0] * outputs[1] - labels[0])**2

  def loss2(outputs, labels, weights):
    return (outputs[0] + outputs[1] - labels[0])**2

  keras_model = VarModel()
  model = dc.models.KerasModel(keras_model, loss1, learning_rate=0.01)
  x = np.ones((1, 1))
  vars = model.predict_on_batch(x)
  assert np.allclose(vars[0], 0.5)
  assert np.allclose(vars[1], 0.5)
  model.fit_generator([(x, x, x)] * 300)
  vars = model.predict_on_batch(x)
  assert np.allclose(vars[0], 1.0)
  assert np.allclose(vars[1], 1.0)
  model.fit_generator([(x, 3 * x, x)] * 300, loss=loss2)
  vars = model.predict_on_batch(x)
  assert np.allclose(vars[0] + vars[1], 3.0)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag807')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_torch_model.py: 338-373
</a>
<div class="mid" id="frag807" style="display:none"><pre>
def test_fit_variables():
  """Test training a subset of the variables in a model."""

  class VarModel(torch.nn.Module):

    def __init__(self, **kwargs):
      super(VarModel, self).__init__(**kwargs)
      self.var1 = torch.nn.Parameter(torch.Tensor([0.5]))
      self.var2 = torch.nn.Parameter(torch.Tensor([0.5]))

    def forward(self, inputs):
      return [self.var1, self.var2]

  def loss(outputs, labels, weights):
    return (outputs[0] * outputs[1] - labels[0])**2

  pytorch_model = VarModel()
  model = dc.models.TorchModel(pytorch_model, loss, learning_rate=0.02)
  x = np.ones((1, 1))
  vars = model.predict_on_batch(x)
  assert np.allclose(vars[0], 0.5)
  assert np.allclose(vars[1], 0.5)
  model.fit_generator([(x, x, x)] * 300)
  vars = model.predict_on_batch(x)
  assert np.allclose(vars[0], 1.0)
  assert np.allclose(vars[1], 1.0)
  model.fit_generator([(x, 2 * x, x)] * 300, variables=[pytorch_model.var1])
  vars = model.predict_on_batch(x)
  assert np.allclose(vars[0], 2.0)
  assert np.allclose(vars[1], 1.0)
  model.fit_generator([(x, x, x)] * 300, variables=[pytorch_model.var2])
  vars = model.predict_on_batch(x)
  assert np.allclose(vars[0], 2.0)
  assert np.allclose(vars[1], 0.5)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag811')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_torch_model.py: 375-406
</a>
<div class="mid" id="frag811" style="display:none"><pre>
def test_fit_loss():
  """Test specifying a different loss function when calling fit()."""

  class VarModel(torch.nn.Module):

    def __init__(self):
      super(VarModel, self).__init__()
      self.var1 = torch.nn.Parameter(torch.Tensor([0.5]))
      self.var2 = torch.nn.Parameter(torch.Tensor([0.5]))

    def forward(self, inputs):
      return [self.var1, self.var2]

  def loss1(outputs, labels, weights):
    return (outputs[0] * outputs[1] - labels[0])**2

  def loss2(outputs, labels, weights):
    return (outputs[0] + outputs[1] - labels[0])**2

  pytorch_model = VarModel()
  model = dc.models.TorchModel(pytorch_model, loss1, learning_rate=0.01)
  x = np.ones((1, 1))
  vars = model.predict_on_batch(x)
  assert np.allclose(vars[0], 0.5)
  assert np.allclose(vars[1], 0.5)
  model.fit_generator([(x, x, x)] * 300)
  vars = model.predict_on_batch(x)
  assert np.allclose(vars[0], 1.0)
  assert np.allclose(vars[1], 1.0)
  model.fit_generator([(x, 3 * x, x)] * 300, loss=loss2)
  vars = model.predict_on_batch(x)
  assert np.allclose(vars[0] + vars[1], 3.0)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 53:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag669')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_losses.py: 128-139
</a>
<div class="mid" id="frag669" style="display:none"><pre>
  def test_sigmoid_cross_entropy_tf(self):
    """Test SigmoidCrossEntropy."""
    loss = losses.SigmoidCrossEntropy()
    y = [[0.1, 0.8], [0.4, 0.6]]
    outputs = tf.constant(y)
    labels = tf.constant([[0.0, 1.0], [1.0, 0.0]])
    result = loss._compute_tf_loss(outputs, labels).numpy()
    sigmoid = 1.0 / (1.0 + np.exp(-np.array(y)))
    expected = [[-np.log(1 - sigmoid[0, 0]), -np.log(sigmoid[0, 1])],
                [-np.log(sigmoid[1, 0]), -np.log(1 - sigmoid[1, 1])]]
    assert np.allclose(expected, result)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag670')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_losses.py: 141-152
</a>
<div class="mid" id="frag670" style="display:none"><pre>
  def test_sigmoid_cross_entropy_pytorch(self):
    """Test SigmoidCrossEntropy."""
    loss = losses.SigmoidCrossEntropy()
    y = [[0.1, 0.8], [0.4, 0.6]]
    outputs = torch.tensor(y)
    labels = torch.tensor([[0.0, 1.0], [1.0, 0.0]])
    result = loss._create_pytorch_loss()(outputs, labels).numpy()
    sigmoid = 1.0 / (1.0 + np.exp(-np.array(y)))
    expected = [[-np.log(1 - sigmoid[0, 0]), -np.log(sigmoid[0, 1])],
                [-np.log(sigmoid[1, 0]), -np.log(1 - sigmoid[1, 1])]]
    assert np.allclose(expected, result)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 54:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 85%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag673')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_losses.py: 178-194
</a>
<div class="mid" id="frag673" style="display:none"><pre>
  def test_sparse_softmax_cross_entropy_tf(self):
    """Test SparseSoftmaxCrossEntropy."""
    loss = losses.SparseSoftmaxCrossEntropy()
    y = np.array([[0.1, 0.8], [0.4, 0.6]])
    outputs = tf.constant(y)
    labels = tf.constant([1, 0])
    result = loss._compute_tf_loss(outputs, labels).numpy()
    softmax = np.exp(y) / np.expand_dims(np.sum(np.exp(y), axis=1), 1)
    expected = [-np.log(softmax[0, 1]), -np.log(softmax[1, 0])]
    assert np.allclose(expected, result)

    labels = tf.constant([[1], [0]])
    result = loss._compute_tf_loss(outputs, labels).numpy()
    softmax = np.exp(y) / np.expand_dims(np.sum(np.exp(y), axis=1), 1)
    expected = [-np.log(softmax[0, 1]), -np.log(softmax[1, 0])]
    assert np.allclose(expected, result)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag674')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_losses.py: 196-212
</a>
<div class="mid" id="frag674" style="display:none"><pre>
  def test_sparse_softmax_cross_entropy_pytorch(self):
    """Test SparseSoftmaxCrossEntropy."""
    loss = losses.SparseSoftmaxCrossEntropy()
    y = np.array([[0.1, 0.8], [0.4, 0.6]])
    outputs = torch.tensor(y)
    labels = torch.tensor([1, 0])
    result = loss._create_pytorch_loss()(outputs, labels).numpy()
    softmax = np.exp(y) / np.expand_dims(np.sum(np.exp(y), axis=1), 1)
    expected = [-np.log(softmax[0, 1]), -np.log(softmax[1, 0])]
    assert np.allclose(expected, result)

    labels = torch.tensor([[1], [0]])
    result = loss._create_pytorch_loss()(outputs, labels).numpy()
    softmax = np.exp(y) / np.expand_dims(np.sum(np.exp(y), axis=1), 1)
    expected = [-np.log(softmax[0, 1]), -np.log(softmax[1, 0])]
    assert np.allclose(expected, result)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 55:</b> &nbsp; 2 fragments, nominal size 20 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag675')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_losses.py: 214-237
</a>
<div class="mid" id="frag675" style="display:none"><pre>
  def test_VAE_ELBO_tf(self):
    """."""
    loss = losses.VAE_ELBO()
    logvar = tf.constant([[1.0, 1.3], [0.6, 1.2]])
    mu = tf.constant([[0.2, 0.7], [1.2, 0.4]])
    x = tf.constant([[0.9, 0.4, 0.8], [0.3, 0, 1]])
    reconstruction_x = tf.constant([[0.8, 0.3, 0.7], [0.2, 0, 0.9]])
    result = loss._compute_tf_loss(logvar, mu, x, reconstruction_x).numpy()
    expected = [
        0.5 * np.mean([
            0.04 + 1.0 - np.log(1e-20 + 1.0) - 1,
            0.49 + 1.69 - np.log(1e-20 + 1.69) - 1
        ]) - np.mean(
            np.array([0.9, 0.4, 0.8]) * np.log([0.8, 0.3, 0.7]) +
            np.array([0.1, 0.6, 0.2]) * np.log([0.2, 0.7, 0.3])),
        0.5 * np.mean([
            1.44 + 0.36 - np.log(1e-20 + 0.36) - 1,
            0.16 + 1.44 - np.log(1e-20 + 1.44) - 1
        ]) - np.mean(
            np.array([0.3, 0, 1]) * np.log([0.2, 1e-20, 0.9]) +
            np.array([0.7, 1, 0]) * np.log([0.8, 1, 0.1]))
    ]
    assert np.allclose(expected, result)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag676')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_losses.py: 239-263
</a>
<div class="mid" id="frag676" style="display:none"><pre>
  def test_VAE_ELBO_pytorch(self):
    """."""
    loss = losses.VAE_ELBO()
    logvar = torch.tensor([[1.0, 1.3], [0.6, 1.2]])
    mu = torch.tensor([[0.2, 0.7], [1.2, 0.4]])
    x = torch.tensor([[0.9, 0.4, 0.8], [0.3, 0, 1]])
    reconstruction_x = torch.tensor([[0.8, 0.3, 0.7], [0.2, 0, 0.9]])
    result = loss._create_pytorch_loss()(logvar, mu, x,
                                         reconstruction_x).numpy()
    expected = [
        0.5 * np.mean([
            0.04 + 1.0 - np.log(1e-20 + 1.0) - 1,
            0.49 + 1.69 - np.log(1e-20 + 1.69) - 1
        ]) - np.mean(
            np.array([0.9, 0.4, 0.8]) * np.log([0.8, 0.3, 0.7]) +
            np.array([0.1, 0.6, 0.2]) * np.log([0.2, 0.7, 0.3])),
        0.5 * np.mean([
            1.44 + 0.36 - np.log(1e-20 + 0.36) - 1,
            0.16 + 1.44 - np.log(1e-20 + 1.44) - 1
        ]) - np.mean(
            np.array([0.3, 0, 1]) * np.log([0.2, 1e-20, 0.9]) +
            np.array([0.7, 1, 0]) * np.log([0.8, 1, 0.1]))
    ]
    assert np.allclose(expected, result)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 56:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 91%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag677')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_losses.py: 265-281
</a>
<div class="mid" id="frag677" style="display:none"><pre>
  def test_VAE_KLDivergence_tf(self):
    """."""
    loss = losses.VAE_KLDivergence()
    logvar = tf.constant([[1.0, 1.3], [0.6, 1.2]])
    mu = tf.constant([[0.2, 0.7], [1.2, 0.4]])
    result = loss._compute_tf_loss(logvar, mu).numpy()
    expected = [
        0.5 * np.mean([
            0.04 + 1.0 - np.log(1e-20 + 1.0) - 1,
            0.49 + 1.69 - np.log(1e-20 + 1.69) - 1
        ]), 0.5 * np.mean([
            1.44 + 0.36 - np.log(1e-20 + 0.36) - 1,
            0.16 + 1.44 - np.log(1e-20 + 1.44) - 1
        ])
    ]
    assert np.allclose(expected, result)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag678')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_losses.py: 283-299
</a>
<div class="mid" id="frag678" style="display:none"><pre>
  def test_VAE_KLDivergence_pytorch(self):
    """."""
    loss = losses.VAE_KLDivergence()
    logvar = torch.tensor([[1.0, 1.3], [0.6, 1.2]])
    mu = torch.tensor([[0.2, 0.7], [1.2, 0.4]])
    result = loss._create_pytorch_loss()(logvar, mu).numpy()
    expected = [
        0.5 * np.mean([
            0.04 + 1.0 - np.log(1e-20 + 1.0) - 1,
            0.49 + 1.69 - np.log(1e-20 + 1.69) - 1
        ]), 0.5 * np.mean([
            1.44 + 0.36 - np.log(1e-20 + 0.36) - 1,
            0.16 + 1.44 - np.log(1e-20 + 1.44) - 1
        ])
    ]
    assert np.allclose(expected, result)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 57:</b> &nbsp; 2 fragments, nominal size 26 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag683')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_pretrained_torch.py: 39-72
</a>
<div class="mid" id="frag683" style="display:none"><pre>
  def test_load_from_pretrained(self):
    """Tests loading pretrained model."""
    source_model = MLP(
        hidden_layer_size=self.hidden_layer_size,
        feature_dim=self.feature_dim,
        batch_size=10)

    source_model.fit(self.dataset, nb_epoch=1000, checkpoint_interval=0)

    dest_model = MLP(
        feature_dim=self.feature_dim,
        hidden_layer_size=self.hidden_layer_size,
        n_tasks=10)

    assignment_map = dict()
    value_map = dict()
    source_vars = list(source_model.model.parameters())
    dest_vars = list(dest_model.model.parameters())[:-2]

    for idx, dest_var in enumerate(dest_vars):
      source_var = source_vars[idx]
      assignment_map[source_var] = dest_var
      value_map[source_var] = source_var.detach().numpy()

    dest_model.load_from_pretrained(
        source_model=source_model,
        assignment_map=assignment_map,
        value_map=value_map)

    for source_var, dest_var in assignment_map.items():
      source_val = source_var.detach().numpy()
      dest_val = dest_var.detach().numpy()
      np.testing.assert_array_almost_equal(source_val, dest_val)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag819')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_pretrained_keras.py: 48-80
</a>
<div class="mid" id="frag819" style="display:none"><pre>
  def test_load_from_pretrained(self):
    """Tests loading pretrained model."""
    source_model = MLP(
        hidden_layer_size=self.hidden_layer_size,
        feature_dim=self.feature_dim,
        batch_size=10)

    source_model.fit(self.dataset, nb_epoch=1000, checkpoint_interval=0)

    dest_model = MLP(
        feature_dim=self.feature_dim,
        hidden_layer_size=self.hidden_layer_size,
        n_tasks=10)

    assignment_map = dict()
    value_map = dict()
    dest_vars = dest_model.model.trainable_variables[:-2]

    for idx, dest_var in enumerate(dest_vars):
      source_var = source_model.model.trainable_variables[idx]
      assignment_map[source_var.experimental_ref()] = dest_var
      value_map[source_var.experimental_ref()] = source_var.numpy()

    dest_model.load_from_pretrained(
        source_model=source_model,
        assignment_map=assignment_map,
        value_map=value_map)

    for source_var, dest_var in assignment_map.items():
      source_val = source_var.deref().numpy()
      dest_val = dest_var.numpy()
      np.testing.assert_array_almost_equal(source_val, dest_val)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 58:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag684')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_pretrained_torch.py: 73-93
</a>
<div class="mid" id="frag684" style="display:none"><pre>
  def test_restore_equivalency(self):
    """Test for restore based pretrained model loading."""
    source_model = MLP(
        feature_dim=self.feature_dim,
        hidden_layer_size=self.hidden_layer_size,
        learning_rate=0.003)

    source_model.fit(self.dataset, nb_epoch=1000)

    dest_model = MLP(
        feature_dim=self.feature_dim, hidden_layer_size=self.hidden_layer_size)

    dest_model.load_from_pretrained(
        source_model=source_model,
        assignment_map=None,
        value_map=None,
        model_dir=None,
        include_top=True)

    predictions = np.squeeze(dest_model.predict_on_batch(self.dataset.X))
    np.testing.assert_array_almost_equal(self.dataset.y, np.round(predictions))
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag821')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_pretrained_keras.py: 138-156
</a>
<div class="mid" id="frag821" style="display:none"><pre>
  def test_restore_equivalency(self):
    """Test for restore based pretrained model loading."""
    source_model = MLP(
        feature_dim=self.feature_dim, hidden_layer_size=self.hidden_layer_size)

    source_model.fit(self.dataset, nb_epoch=1000)

    dest_model = MLP(
        feature_dim=self.feature_dim, hidden_layer_size=self.hidden_layer_size)

    dest_model.load_from_pretrained(
        source_model=source_model,
        assignment_map=None,
        value_map=None,
        model_dir=None,
        include_top=True)

    predictions = np.squeeze(dest_model.predict_on_batch(self.dataset.X))
    np.testing.assert_array_almost_equal(self.dataset.y, np.round(predictions))
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 59:</b> &nbsp; 3 fragments, nominal size 17 lines, similarity 88%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag685')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_overfit.py: 19-45
</a>
<div class="mid" id="frag685" style="display:none"><pre>
def test_sklearn_regression_overfit():
  """Test that sklearn models can overfit simple regression datasets."""
  n_samples = 10
  n_features = 3
  n_tasks = 1

  # Generate dummy dataset
  np.random.seed(123)
  ids = np.arange(n_samples)
  X = np.random.rand(n_samples, n_features)
  y = np.random.rand(n_samples, n_tasks)
  w = np.ones((n_samples, n_tasks))
  dataset = dc.data.NumpyDataset(X, y, w, ids)

  regression_metric = dc.metrics.Metric(dc.metrics.r2_score)
  sklearn_model = RandomForestRegressor()
  model = dc.models.SklearnModel(sklearn_model)

  # Fit trained model
  model.fit(dataset)
  model.save()

  # Eval model on train
  scores = model.evaluate(dataset, [regression_metric])
  assert scores[regression_metric.name] &gt; .7


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag687')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_overfit.py: 73-101
</a>
<div class="mid" id="frag687" style="display:none"><pre>
def test_sklearn_skewed_classification_overfit():
  """Test sklearn models can overfit 0/1 datasets with few actives."""
  n_samples = 100
  n_features = 3
  n_tasks = 1

  # Generate dummy dataset
  np.random.seed(123)
  p = .05
  ids = np.arange(n_samples)
  X = np.random.rand(n_samples, n_features)
  y = np.random.binomial(1, p, size=(n_samples, n_tasks))
  w = np.ones((n_samples, n_tasks))

  dataset = dc.data.NumpyDataset(X, y, w, ids)

  classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
  sklearn_model = RandomForestClassifier()
  model = dc.models.SklearnModel(sklearn_model)

  # Fit trained model
  model.fit(dataset)
  model.save()

  # Eval model on train
  scores = model.evaluate(dataset, [classification_metric])
  assert scores[classification_metric.name] &gt; .9


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag686')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_overfit.py: 46-72
</a>
<div class="mid" id="frag686" style="display:none"><pre>
def test_sklearn_classification_overfit():
  """Test that sklearn models can overfit simple classification datasets."""
  n_samples = 10
  n_features = 3
  n_tasks = 1

  # Generate dummy dataset
  np.random.seed(123)
  ids = np.arange(n_samples)
  X = np.random.rand(n_samples, n_features)
  y = np.random.randint(2, size=(n_samples, n_tasks))
  w = np.ones((n_samples, n_tasks))
  dataset = dc.data.NumpyDataset(X, y, w, ids)

  classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
  sklearn_model = RandomForestClassifier()
  model = dc.models.SklearnModel(sklearn_model)

  # Fit trained model
  model.fit(dataset)
  model.save()

  # Eval model on train
  scores = model.evaluate(dataset, [classification_metric])
  assert scores[classification_metric.name] &gt; .9


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 60:</b> &nbsp; 2 fragments, nominal size 21 lines, similarity 95%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag694')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_overfit.py: 313-345
</a>
<div class="mid" id="frag694" style="display:none"><pre>
def test_sklearn_multitask_classification_overfit():
  """Test SKLearn singletask-to-multitask overfits tiny data."""
  n_tasks = 10
  tasks = ["task%d" % task for task in range(n_tasks)]
  n_samples = 10
  n_features = 3

  # Generate dummy dataset
  np.random.seed(123)
  ids = np.arange(n_samples)
  X = np.random.rand(n_samples, n_features)
  y = np.random.randint(2, size=(n_samples, n_tasks))
  w = np.ones((n_samples, n_tasks))
  dataset = dc.data.DiskDataset.from_numpy(X, y, w, ids)

  classification_metric = dc.metrics.Metric(
      dc.metrics.roc_auc_score, task_averager=np.mean)

  def model_builder(model_dir):
    sklearn_model = RandomForestClassifier()
    return dc.models.SklearnModel(sklearn_model, model_dir)

  model = dc.models.SingletaskToMultitask(tasks, model_builder)

  # Fit trained model
  model.fit(dataset)
  model.save()

  # Eval model on train
  scores = model.evaluate(dataset, [classification_metric])
  assert scores[classification_metric.name] &gt; .9


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag699')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_overfit.py: 444-477
</a>
<div class="mid" id="frag699" style="display:none"><pre>
def test_sklearn_multitask_regression_overfit():
  """Test SKLearn singletask-to-multitask overfits tiny regression data."""
  n_tasks = 2
  tasks = ["task%d" % task for task in range(n_tasks)]
  n_samples = 10
  n_features = 3

  # Generate dummy dataset
  np.random.seed(123)
  ids = np.arange(n_samples)
  X = np.random.rand(n_samples, n_features)
  y = np.random.rand(n_samples, n_tasks)
  w = np.ones((n_samples, n_tasks))

  dataset = dc.data.DiskDataset.from_numpy(X, y, w, ids)

  regression_metric = dc.metrics.Metric(
      dc.metrics.r2_score, task_averager=np.mean)

  def model_builder(model_dir):
    sklearn_model = RandomForestRegressor()
    return dc.models.SklearnModel(sklearn_model, model_dir)

  model = dc.models.SingletaskToMultitask(tasks, model_builder)

  # Fit trained model
  model.fit(dataset)
  model.save()

  # Eval model on train
  scores = model.evaluate(dataset, [regression_metric])
  assert scores[regression_metric.name] &gt; .7


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 61:</b> &nbsp; 6 fragments, nominal size 27 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag707')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_overfit.py: 683-722
</a>
<div class="mid" id="frag707" style="display:none"><pre>
def test_DAG_singletask_regression_overfit():
  """Test DAG regressor multitask overfits tiny data."""
  np.random.seed(123)
  tf.random.set_seed(123)
  n_tasks = 1
  current_dir = os.path.dirname(os.path.abspath(__file__))

  # Load mini log-solubility dataset.
  featurizer = dc.feat.ConvMolFeaturizer()
  tasks = ["outcome"]
  input_file = os.path.join(current_dir, "example_regression.csv")
  loader = dc.data.CSVLoader(
      tasks=tasks, feature_field="smiles", featurizer=featurizer)
  dataset = loader.create_dataset(input_file)

  regression_metric = dc.metrics.Metric(
      dc.metrics.pearson_r2_score, task_averager=np.mean)

  n_feat = 75
  batch_size = 10
  transformer = dc.trans.DAGTransformer(max_atoms=50)
  dataset = transformer.transform(dataset)

  model = dc.models.DAGModel(
      n_tasks,
      max_atoms=50,
      n_atom_feat=n_feat,
      batch_size=batch_size,
      learning_rate=0.001,
      use_queue=False,
      mode="regression")

  # Fit trained model
  model.fit(dataset, nb_epoch=1200)
  # Eval model on train
  scores = model.evaluate(dataset, [regression_metric])

  assert scores[regression_metric.name] &gt; .8


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag710')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_overfit.py: 795-835
</a>
<div class="mid" id="frag710" style="display:none"><pre>
def test_MPNN_singletask_regression_overfit():
  """Test MPNN overfits tiny data."""
  np.random.seed(123)
  tf.random.set_seed(123)
  n_tasks = 1
  current_dir = os.path.dirname(os.path.abspath(__file__))

  # Load mini log-solubility dataset.
  featurizer = dc.feat.WeaveFeaturizer()
  tasks = ["outcome"]
  input_file = os.path.join(current_dir, "example_regression.csv")
  loader = dc.data.CSVLoader(
      tasks=tasks, feature_field="smiles", featurizer=featurizer)
  dataset = loader.create_dataset(input_file)

  regression_metric = dc.metrics.Metric(
      dc.metrics.pearson_r2_score, task_averager=np.mean)

  n_atom_feat = 75
  n_pair_feat = 14
  batch_size = 10
  model = dc.models.MPNNModel(
      n_tasks,
      n_atom_feat=n_atom_feat,
      n_pair_feat=n_pair_feat,
      T=2,
      M=3,
      batch_size=batch_size,
      learning_rate=0.001,
      use_queue=False,
      mode="regression")

  # Fit trained model
  model.fit(dataset, nb_epoch=50)

  # Eval model on train
  scores = model.evaluate(dataset, [regression_metric])

  assert scores[regression_metric.name] &gt; .8


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag709')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_overfit.py: 758-793
</a>
<div class="mid" id="frag709" style="display:none"><pre>
def test_weave_singletask_regression_overfit():
  """Test weave model overfits tiny data."""
  np.random.seed(123)
  tf.random.set_seed(123)
  n_tasks = 1
  current_dir = os.path.dirname(os.path.abspath(__file__))

  # Load mini log-solubility dataset.
  featurizer = dc.feat.WeaveFeaturizer()
  tasks = ["outcome"]
  input_file = os.path.join(current_dir, "example_regression.csv")
  loader = dc.data.CSVLoader(
      tasks=tasks, feature_field="smiles", featurizer=featurizer)
  dataset = loader.create_dataset(input_file)

  regression_metric = dc.metrics.Metric(
      dc.metrics.pearson_r2_score, task_averager=np.mean)

  batch_size = 10

  model = dc.models.WeaveModel(
      n_tasks,
      batch_size=batch_size,
      learning_rate=0.0003,
      dropout=0.0,
      mode="regression")

  # Fit trained model
  model.fit(dataset, nb_epoch=120)

  # Eval model on train
  scores = model.evaluate(dataset, [regression_metric])

  assert scores[regression_metric.name] &gt; .8


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag708')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_overfit.py: 723-756
</a>
<div class="mid" id="frag708" style="display:none"><pre>
def test_weave_singletask_classification_overfit():
  """Test weave model overfits tiny data."""
  np.random.seed(123)
  tf.random.set_seed(123)
  n_tasks = 1
  current_dir = os.path.dirname(os.path.abspath(__file__))

  # Load mini log-solubility dataset.
  featurizer = dc.feat.WeaveFeaturizer()
  tasks = ["outcome"]
  input_file = os.path.join(current_dir, "example_classification.csv")
  loader = dc.data.CSVLoader(
      tasks=tasks, feature_field="smiles", featurizer=featurizer)
  dataset = loader.create_dataset(input_file)

  classification_metric = dc.metrics.Metric(dc.metrics.accuracy_score)

  batch_size = 10
  model = dc.models.WeaveModel(
      n_tasks,
      batch_size=batch_size,
      learning_rate=0.0003,
      dropout=0.0,
      mode="classification")

  # Fit trained model
  model.fit(dataset, nb_epoch=100)

  # Eval model on train
  scores = model.evaluate(dataset, [classification_metric])

  assert scores[classification_metric.name] &gt; .65


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag711')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_overfit.py: 836-872
</a>
<div class="mid" id="frag711" style="display:none"><pre>
def test_textCNN_singletask_classification_overfit():
  """Test textCNN model overfits tiny data."""
  np.random.seed(123)
  tf.random.set_seed(123)
  n_tasks = 1
  current_dir = os.path.dirname(os.path.abspath(__file__))

  featurizer = dc.feat.RawFeaturizer()
  tasks = ["outcome"]
  input_file = os.path.join(current_dir, "example_classification.csv")
  loader = dc.data.CSVLoader(
      tasks=tasks, feature_field="smiles", featurizer=featurizer)
  dataset = loader.create_dataset(input_file)

  classification_metric = dc.metrics.Metric(dc.metrics.accuracy_score)

  char_dict, length = dc.models.TextCNNModel.build_char_dict(dataset)
  batch_size = 10

  model = dc.models.TextCNNModel(
      n_tasks,
      char_dict,
      seq_length=length,
      batch_size=batch_size,
      learning_rate=0.001,
      use_queue=False,
      mode="classification")

  # Fit trained model
  model.fit(dataset, nb_epoch=200)

  # Eval model on train
  scores = model.evaluate(dataset, [classification_metric])

  assert scores[classification_metric.name] &gt; .8


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag712')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_overfit.py: 874-910
</a>
<div class="mid" id="frag712" style="display:none"><pre>
def test_textCNN_singletask_regression_overfit():
  """Test textCNN model overfits tiny data."""
  np.random.seed(123)
  tf.random.set_seed(123)
  n_tasks = 1
  current_dir = os.path.dirname(os.path.abspath(__file__))

  # Load mini log-solubility dataset.
  featurizer = dc.feat.RawFeaturizer()
  tasks = ["outcome"]
  input_file = os.path.join(current_dir, "example_regression.csv")
  loader = dc.data.CSVLoader(
      tasks=tasks, feature_field="smiles", featurizer=featurizer)
  dataset = loader.create_dataset(input_file)

  regression_metric = dc.metrics.Metric(
      dc.metrics.pearson_r2_score, task_averager=np.mean)

  char_dict, length = dc.models.TextCNNModel.build_char_dict(dataset)
  batch_size = 10

  model = dc.models.TextCNNModel(
      n_tasks,
      char_dict,
      seq_length=length,
      batch_size=batch_size,
      learning_rate=0.001,
      use_queue=False,
      mode="regression")

  # Fit trained model
  model.fit(dataset, nb_epoch=200)

  # Eval model on train
  scores = model.evaluate(dataset, [regression_metric])

  assert scores[regression_metric.name] &gt; .9
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 62:</b> &nbsp; 4 fragments, nominal size 17 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag732')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_gbdt_model.py: 16-42
</a>
<div class="mid" id="frag732" style="display:none"><pre>
def test_singletask_regression_with_xgboost():
  np.random.seed(123)

  # prepare dataset
  dataset = load_diabetes()
  X, y = dataset.data, dataset.target
  frac_train = .7
  X_train, X_test, y_train, y_test = \
    train_test_split(X, y, train_size=frac_train)
  train_dataset = dc.data.NumpyDataset(X_train, y_train)
  test_dataset = dc.data.NumpyDataset(X_test, y_test)

  # global setting
  regression_metric = dc.metrics.Metric(dc.metrics.mae_score)
  params = {'early_stopping_rounds': 25}

  # xgboost test
  xgb_model = xgboost.XGBRegressor(
      n_estimators=50, random_state=123, verbose=False)
  model = dc.models.GBDTModel(xgb_model, **params)
  # fit trained model
  model.fit(train_dataset)
  model.save()
  # eval model on test
  scores = model.evaluate(test_dataset, [regression_metric])
  assert scores[regression_metric.name] &lt; 55

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag738')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_gbdt_model.py: 138-164
</a>
<div class="mid" id="frag738" style="display:none"><pre>
  score = scores[regression_metric.name]
  assert score &lt; 55


def test_classification_with_xgboost():
  """Test that sklearn models can learn on simple classification datasets."""
  np.random.seed(123)

  # prepare dataset
  dataset = load_digits(n_class=2)
  X, y = dataset.data, dataset.target
  frac_train = .7
  X_train, X_test, y_train, y_test = \
    train_test_split(X, y, train_size=frac_train)
  train_dataset = dc.data.NumpyDataset(X_train, y_train)
  test_dataset = dc.data.NumpyDataset(X_test, y_test)

  # global setting
  classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
  params = {'early_stopping_rounds': 25}

  # xgboost test
  xgb_model = xgboost.XGBClassifier(n_estimators=50, seed=123, verbose=False)
  model = dc.models.GBDTModel(xgb_model, **params)
  # fit trained model
  model.fit(train_dataset)
  model.save()
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag739')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_gbdt_model.py: 165-191
</a>
<div class="mid" id="frag739" style="display:none"><pre>
  # eval model on test
  scores = model.evaluate(test_dataset, [classification_metric])
  assert scores[classification_metric.name] &gt; .9


def test_classification_with_lightgbm():
  """Test that sklearn models can learn on simple classification datasets."""
  np.random.seed(123)

  # prepare dataset
  dataset = load_digits(n_class=2)
  X, y = dataset.data, dataset.target
  frac_train = .7
  X_train, X_test, y_train, y_test = \
    train_test_split(X, y, train_size=frac_train)
  train_dataset = dc.data.NumpyDataset(X_train, y_train)
  test_dataset = dc.data.NumpyDataset(X_test, y_test)

  # global setting
  classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
  params = {'early_stopping_rounds': 25}

  # lightgbm test
  lgbm_model = lightgbm.LGBMClassifier(n_estimators=50, seed=123, silent=True)
  model = dc.models.GBDTModel(lgbm_model, **params)
  # fit trained model
  model.fit(train_dataset)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag733')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_gbdt_model.py: 43-69
</a>
<div class="mid" id="frag733" style="display:none"><pre>

def test_singletask_regression_with_lightgbm():
  np.random.seed(123)

  # prepare dataset
  dataset = load_diabetes()
  X, y = dataset.data, dataset.target
  frac_train = .7
  X_train, X_test, y_train, y_test = \
    train_test_split(X, y, train_size=frac_train)
  train_dataset = dc.data.NumpyDataset(X_train, y_train)
  test_dataset = dc.data.NumpyDataset(X_test, y_test)

  # global setting
  regression_metric = dc.metrics.Metric(dc.metrics.mae_score)
  params = {'early_stopping_rounds': 25}

  # lightgbm test
  lgbm_model = lightgbm.LGBMRegressor(
      n_estimators=50, random_state=123, silent=True)
  model = dc.models.GBDTModel(lgbm_model, **params)
  # fit trained model
  model.fit(train_dataset)
  model.save()
  # eval model on test
  scores = model.evaluate(test_dataset, [regression_metric])
  assert scores[regression_metric.name] &lt; 55
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 63:</b> &nbsp; 2 fragments, nominal size 23 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag734')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_gbdt_model.py: 70-103
</a>
<div class="mid" id="frag734" style="display:none"><pre>


def test_multitask_regression_with_xgboost():
  np.random.seed(123)

  # prepare dataset
  n_tasks = 4
  tasks = range(n_tasks)
  dataset = load_diabetes()
  X, y = dataset.data, dataset.target
  y = np.reshape(y, (len(y), 1))
  y = np.hstack([y] * n_tasks)
  frac_train = .7
  X_train, X_test, y_train, y_test = \
    train_test_split(X, y, train_size=frac_train)
  train_dataset = dc.data.DiskDataset.from_numpy(X_train, y_train)
  test_dataset = dc.data.DiskDataset.from_numpy(X_test, y_test)

  # global setting
  regression_metric = dc.metrics.Metric(dc.metrics.mae_score)
  params = {'early_stopping_rounds': 25}

  # xgboost test
  def xgboost_builder(model_dir):
    xgb_model = xgboost.XGBRegressor(n_estimators=50, seed=123, verbose=False)
    return dc.models.GBDTModel(xgb_model, model_dir, **params)

  model = dc.models.SingletaskToMultitask(tasks, xgboost_builder)
  # fit trained model
  model.fit(train_dataset)
  model.save()
  # eval model on test
  scores = model.evaluate(test_dataset, [regression_metric])
  score = scores[regression_metric.name]
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag736')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_gbdt_model.py: 104-137
</a>
<div class="mid" id="frag736" style="display:none"><pre>
  assert score &lt; 55


def test_multitask_regression_with_lightgbm():
  np.random.seed(123)

  # prepare dataset
  n_tasks = 4
  tasks = range(n_tasks)
  dataset = load_diabetes()
  X, y = dataset.data, dataset.target
  y = np.reshape(y, (len(y), 1))
  y = np.hstack([y] * n_tasks)
  frac_train = .7
  X_train, X_test, y_train, y_test = \
    train_test_split(X, y, train_size=frac_train)
  train_dataset = dc.data.DiskDataset.from_numpy(X_train, y_train)
  test_dataset = dc.data.DiskDataset.from_numpy(X_test, y_test)

  # global setting
  regression_metric = dc.metrics.Metric(dc.metrics.mae_score)
  params = {'early_stopping_rounds': 25}

  # lightgbm test
  def lightgbm_builder(model_dir):
    lgbm_model = lightgbm.LGBMRegressor(n_estimators=50, seed=123, silent=False)
    return dc.models.GBDTModel(lgbm_model, model_dir, **params)

  model = dc.models.SingletaskToMultitask(tasks, lightgbm_builder)
  # fit trained model
  model.fit(train_dataset)
  model.save()
  # eval model on test
  scores = model.evaluate(test_dataset, [regression_metric])
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 64:</b> &nbsp; 2 fragments, nominal size 23 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag740')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_gbdt_model.py: 192-226
</a>
<div class="mid" id="frag740" style="display:none"><pre>
  model.save()
  # eval model on test
  scores = model.evaluate(test_dataset, [classification_metric])
  assert scores[classification_metric.name] &gt; .9


def test_reload_with_xgboost():
  np.random.seed(123)

  # prepare dataset
  dataset = load_diabetes()
  X, y = dataset.data, dataset.target
  frac_train = .7
  X_train, X_test, y_train, y_test = \
    train_test_split(X, y, train_size=frac_train)
  train_dataset = dc.data.NumpyDataset(X_train, y_train)
  test_dataset = dc.data.NumpyDataset(X_test, y_test)

  # global setting
  regression_metric = dc.metrics.Metric(dc.metrics.mae_score)
  model_dir = tempfile.mkdtemp()
  params = {'early_stopping_rounds': 25, 'model_dir': model_dir}

  # xgboost test
  xgb_model = xgboost.XGBRegressor(
      n_estimators=50, random_state=123, verbose=False)
  model = dc.models.GBDTModel(xgb_model, **params)
  # fit trained model
  model.fit(train_dataset)
  model.save()
  # reload
  reloaded_model = dc.models.GBDTModel(None, model_dir)
  reloaded_model.reload()
  # check predictions match on test dataset
  original_pred = model.predict(test_dataset)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag741')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_gbdt_model.py: 227-259
</a>
<div class="mid" id="frag741" style="display:none"><pre>
  reload_pred = reloaded_model.predict(test_dataset)
  assert np.all(original_pred == reload_pred)
  # eval model on test
  scores = reloaded_model.evaluate(test_dataset, [regression_metric])
  assert scores[regression_metric.name] &lt; 55


def test_reload_with_lightgbm():
  np.random.seed(123)

  # prepare dataset
  dataset = load_diabetes()
  X, y = dataset.data, dataset.target
  frac_train = .7
  X_train, X_test, y_train, y_test = \
    train_test_split(X, y, train_size=frac_train)
  train_dataset = dc.data.NumpyDataset(X_train, y_train)
  test_dataset = dc.data.NumpyDataset(X_test, y_test)

  # global setting
  regression_metric = dc.metrics.Metric(dc.metrics.mae_score)
  model_dir = tempfile.mkdtemp()
  params = {'early_stopping_rounds': 25, 'model_dir': model_dir}

  # lightgbm test
  lgbm_model = lightgbm.LGBMRegressor(
      n_estimators=50, random_state=123, silent=True)
  model = dc.models.GBDTModel(lgbm_model, **params)
  # fit trained model
  model.fit(train_dataset)
  model.save()
  # reload
  reloaded_model = dc.models.GBDTModel(None, model_dir)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 65:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 88%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag745')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_chemnet_models.py: 88-107
</a>
<div class="mid" id="frag745" style="display:none"><pre>
def test_smiles_to_vec_regression():
  n_tasks = 5
  max_seq_len = 20
  dataset, metric, char_to_idx = get_dataset(
      mode="regression",
      featurizer="smiles2seq",
      n_tasks=n_tasks,
      max_seq_len=max_seq_len)
  model = Smiles2Vec(
      char_to_idx=char_to_idx,
      max_seq_len=max_seq_len,
      use_conv=True,
      n_tasks=n_tasks,
      model_dir=None,
      mode="regression")
  model.fit(dataset, nb_epoch=500)
  scores = model.evaluate(dataset, [metric], [])
  assert scores['mean_absolute_error'] &lt; 0.1


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag746')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_chemnet_models.py: 109-128
</a>
<div class="mid" id="frag746" style="display:none"><pre>
def test_smiles_to_vec_classification():
  n_tasks = 5
  max_seq_len = 20
  dataset, metric, char_to_idx, = get_dataset(
      mode="classification",
      featurizer="smiles2seq",
      n_tasks=n_tasks,
      max_seq_len=max_seq_len)
  model = Smiles2Vec(
      char_to_idx=char_to_idx,
      max_seq_len=max_seq_len,
      use_conv=True,
      n_tasks=n_tasks,
      model_dir=None,
      mode="classification")
  model.fit(dataset, nb_epoch=500)
  scores = model.evaluate(dataset, [metric], [])
  assert scores['mean-roc_auc_score'] &gt;= 0.9


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 66:</b> &nbsp; 3 fragments, nominal size 20 lines, similarity 82%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag748')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_cnn.py: 9-36
</a>
<div class="mid" id="frag748" style="display:none"><pre>
  def test_1d_cnn_regression(self):
    """Test that a 1D CNN can overfit simple regression datasets."""
    n_samples = 10
    n_features = 3
    n_tasks = 1

    np.random.seed(123)
    X = np.random.rand(n_samples, 10, n_features)
    y = np.random.randint(2, size=(n_samples, n_tasks)).astype(np.float32)
    dataset = dc.data.NumpyDataset(X, y)

    regression_metric = dc.metrics.Metric(dc.metrics.mean_squared_error)
    model = dc.models.CNN(
        n_tasks,
        n_features,
        dims=1,
        dropouts=0,
        kernel_size=3,
        mode='regression',
        learning_rate=0.003)

    # Fit trained model
    model.fit(dataset, nb_epoch=200)

    # Eval model on train
    scores = model.evaluate(dataset, [regression_metric])
    assert scores[regression_metric.name] &lt; 0.1

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag749')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_cnn.py: 37-64
</a>
<div class="mid" id="frag749" style="display:none"><pre>
  def test_2d_cnn_classification(self):
    """Test that a 2D CNN can overfit simple classification datasets."""
    n_samples = 10
    n_features = 3
    n_tasks = 1

    np.random.seed(123)
    X = np.random.rand(n_samples, 10, 10, n_features)
    y = np.random.randint(2, size=(n_samples, n_tasks)).astype(np.float32)
    dataset = dc.data.NumpyDataset(X, y)

    classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
    model = dc.models.CNN(
        n_tasks,
        n_features,
        dims=2,
        dropouts=0,
        kernel_size=3,
        mode='classification',
        learning_rate=0.003)

    # Fit trained model
    model.fit(dataset, nb_epoch=100)

    # Eval model on train
    scores = model.evaluate(dataset, [classification_metric])
    assert scores[classification_metric.name] &gt; 0.9

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag750')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_cnn.py: 65-95
</a>
<div class="mid" id="frag750" style="display:none"><pre>
  def test_residual_cnn_classification(self):
    """Test that a residual CNN can overfit simple classification datasets."""
    n_samples = 10
    n_features = 3
    n_tasks = 1

    np.random.seed(123)
    X = np.random.rand(n_samples, 10, n_features)
    y = np.random.randint(2, size=(n_samples, n_tasks)).astype(np.float32)
    dataset = dc.data.NumpyDataset(X, y)

    classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
    model = dc.models.CNN(
        n_tasks,
        n_features,
        dims=1,
        dropouts=0,
        layer_filters=[30] * 10,
        kernel_size=3,
        mode='classification',
        padding='same',
        residual=True,
        learning_rate=0.003)

    # Fit trained model
    model.fit(dataset, nb_epoch=100)

    # Eval model on train
    scores = model.evaluate(dataset, [classification_metric])
    assert scores[classification_metric.name] &gt; 0.9

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 67:</b> &nbsp; 3 fragments, nominal size 17 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag761')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_layers.py: 221-242
</a>
<div class="mid" id="frag761" style="display:none"><pre>
def test_graph_conv():
  """Test invoking GraphConv."""
  out_channels = 2
  n_atoms = 4  # In CCC and C, there are 4 atoms
  raw_smiles = ['CCC', 'C']
  from rdkit import Chem
  mols = [Chem.MolFromSmiles(s) for s in raw_smiles]
  featurizer = dc.feat.graph_features.ConvMolFeaturizer()
  mols = featurizer.featurize(mols)
  multi_mol = dc.feat.mol_graphs.ConvMol.agglomerate_mols(mols)
  atom_features = multi_mol.get_atom_features().astype(np.float32)
  degree_slice = multi_mol.deg_slice
  membership = multi_mol.membership
  deg_adjs = multi_mol.get_deg_adjacency_lists()[1:]
  args = [atom_features, degree_slice, membership] + deg_adjs
  layer = layers.GraphConv(out_channels)
  result = layer(args)
  assert result.shape == (n_atoms, out_channels)
  num_deg = 2 * layer.max_degree + (1 - layer.min_degree)
  assert len(layer.trainable_variables) == 2 * num_deg


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag763')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_layers.py: 262-282
</a>
<div class="mid" id="frag763" style="display:none"><pre>
def test_graph_gather():
  """Test invoking GraphGather."""
  batch_size = 2
  n_features = 75
  n_atoms = 4  # In CCC and C, there are 4 atoms
  raw_smiles = ['CCC', 'C']
  from rdkit import Chem
  mols = [Chem.MolFromSmiles(s) for s in raw_smiles]
  featurizer = dc.feat.graph_features.ConvMolFeaturizer()
  mols = featurizer.featurize(mols)
  multi_mol = dc.feat.mol_graphs.ConvMol.agglomerate_mols(mols)
  atom_features = multi_mol.get_atom_features().astype(np.float32)
  degree_slice = multi_mol.deg_slice
  membership = multi_mol.membership
  deg_adjs = multi_mol.get_deg_adjacency_lists()[1:]
  args = [atom_features, degree_slice, membership] + deg_adjs
  result = layers.GraphGather(batch_size)(args)
  # TODO(rbharath): Why is it 2*n_features instead of n_features?
  assert result.shape == (batch_size, 2 * n_features)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag762')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_layers.py: 243-260
</a>
<div class="mid" id="frag762" style="display:none"><pre>
def test_graph_pool():
  """Test invoking GraphPool."""
  n_atoms = 4  # In CCC and C, there are 4 atoms
  raw_smiles = ['CCC', 'C']
  from rdkit import Chem
  mols = [Chem.MolFromSmiles(s) for s in raw_smiles]
  featurizer = dc.feat.graph_features.ConvMolFeaturizer()
  mols = featurizer.featurize(mols)
  multi_mol = dc.feat.mol_graphs.ConvMol.agglomerate_mols(mols)
  atom_features = multi_mol.get_atom_features().astype(np.float32)
  degree_slice = multi_mol.deg_slice
  membership = multi_mol.membership
  deg_adjs = multi_mol.get_deg_adjacency_lists()[1:]
  args = [atom_features, degree_slice, membership] + deg_adjs
  result = layers.GraphPool()(args)
  assert result.shape[0] == n_atoms
  # TODO What should shape[1] be?  It's not documented.

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 68:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag765')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_layers.py: 300-314
</a>
<div class="mid" id="frag765" style="display:none"><pre>
def test_attn_lstm_embedding():
  """Test invoking AttnLSTMEmbedding."""
  max_depth = 5
  n_test = 5
  n_support = 11
  n_feat = 10
  test = np.random.rand(n_test, n_feat).astype(np.float32)
  support = np.random.rand(n_support, n_feat).astype(np.float32)
  layer = layers.AttnLSTMEmbedding(n_test, n_support, n_feat, max_depth)
  test_out, support_out = layer([test, support])
  assert test_out.shape == (n_test, n_feat)
  assert support_out.shape == (n_support, n_feat)
  assert len(layer.trainable_variables) == 4


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag766')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_layers.py: 315-329
</a>
<div class="mid" id="frag766" style="display:none"><pre>
def test_iter_ref_lstm_embedding():
  """Test invoking IterRefLSTMEmbedding."""
  max_depth = 5
  n_test = 5
  n_support = 11
  n_feat = 10
  test = np.random.rand(n_test, n_feat).astype(np.float32)
  support = np.random.rand(n_support, n_feat).astype(np.float32)
  layer = layers.IterRefLSTMEmbedding(n_test, n_support, n_feat, max_depth)
  test_out, support_out = layer([test, support])
  assert test_out.shape == (n_test, n_feat)
  assert support_out.shape == (n_support, n_feat)
  assert len(layer.trainable_variables) == 8


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 69:</b> &nbsp; 2 fragments, nominal size 19 lines, similarity 89%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag779')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_sklearn_model.py: 13-41
</a>
<div class="mid" id="frag779" style="display:none"><pre>
def test_sklearn_regression():
  """Test that sklearn models can learn on simple regression datasets."""
  np.random.seed(123)

  dataset = sklearn.datasets.load_diabetes()
  X, y = dataset.data, dataset.target
  y = np.expand_dims(y, 1)
  frac_train = .7
  n_samples = len(X)
  n_train = int(frac_train * n_samples)
  X_train, y_train = X[:n_train], y[:n_train]
  X_test, y_test = X[n_train:], y[n_train:]
  train_dataset = dc.data.NumpyDataset(X_train, y_train)
  test_dataset = dc.data.NumpyDataset(X_test, y_test)

  regression_metric = dc.metrics.Metric(dc.metrics.r2_score)

  sklearn_model = LinearRegression()
  model = dc.models.SklearnModel(sklearn_model)

  # Fit trained model
  model.fit(train_dataset)
  model.save()

  # Eval model on test
  scores = model.evaluate(test_dataset, [regression_metric])
  assert scores[regression_metric.name] &gt; .5


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag783')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_sklearn_model.py: 122-148
</a>
<div class="mid" id="frag783" style="display:none"><pre>
def test_sklearn_classification():
  """Test that sklearn models can learn on simple classification datasets."""
  np.random.seed(123)
  dataset = sklearn.datasets.load_digits(n_class=2)
  X, y = dataset.data, dataset.target

  frac_train = .7
  n_samples = len(X)
  n_train = int(frac_train * n_samples)
  X_train, y_train = X[:n_train], y[:n_train]
  X_test, y_test = X[n_train:], y[n_train:]
  train_dataset = dc.data.NumpyDataset(X_train, y_train)
  test_dataset = dc.data.NumpyDataset(X_test, y_test)

  classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
  sklearn_model = LogisticRegression()
  model = dc.models.SklearnModel(sklearn_model)

  # Fit trained model
  model.fit(train_dataset)
  model.save()

  # Eval model on test
  scores = model.evaluate(test_dataset, [classification_metric])
  assert scores[classification_metric.name] &gt; .5


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 70:</b> &nbsp; 2 fragments, nominal size 25 lines, similarity 88%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag781')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_sklearn_model.py: 86-121
</a>
<div class="mid" id="frag781" style="display:none"><pre>
def test_sklearn_multitask_regression():
  """Test that sklearn models can learn on simple multitask regression."""
  np.random.seed(123)
  n_tasks = 4
  tasks = range(n_tasks)
  dataset = sklearn.datasets.load_diabetes()
  X, y = dataset.data, dataset.target
  y = np.reshape(y, (len(y), 1))
  y = np.hstack([y] * n_tasks)

  frac_train = .7
  n_samples = len(X)
  n_train = int(frac_train * n_samples)
  X_train, y_train = X[:n_train], y[:n_train]
  X_test, y_test = X[n_train:], y[n_train:]
  train_dataset = dc.data.DiskDataset.from_numpy(X_train, y_train)
  test_dataset = dc.data.DiskDataset.from_numpy(X_test, y_test)

  regression_metric = dc.metrics.Metric(dc.metrics.r2_score)

  def model_builder(model_dir):
    sklearn_model = LinearRegression()
    return dc.models.SklearnModel(sklearn_model, model_dir)

  model = dc.models.SingletaskToMultitask(tasks, model_builder)

  # Fit trained model
  model.fit(train_dataset)
  model.save()

  # Eval model on test
  scores = model.evaluate(test_dataset, [regression_metric])
  score = scores[regression_metric.name]
  assert score &gt; .5


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag784')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_sklearn_model.py: 149-180
</a>
<div class="mid" id="frag784" style="display:none"><pre>
def test_sklearn_multitask_classification():
  """Test that sklearn models can learn on simple multitask classification."""
  np.random.seed(123)
  n_tasks = 4
  tasks = range(n_tasks)
  dataset = sklearn.datasets.load_digits(n_class=2)
  X, y = dataset.data, dataset.target
  y = np.reshape(y, (len(y), 1))
  y = np.hstack([y] * n_tasks)

  frac_train = .7
  n_samples = len(X)
  n_train = int(frac_train * n_samples)
  X_train, y_train = X[:n_train], y[:n_train]
  X_test, y_test = X[n_train:], y[n_train:]
  train_dataset = dc.data.DiskDataset.from_numpy(X_train, y_train)
  test_dataset = dc.data.DiskDataset.from_numpy(X_test, y_test)

  classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)

  def model_builder(model_dir):
    sklearn_model = LogisticRegression()
    return dc.models.SklearnModel(sklearn_model, model_dir)

  model = dc.models.SingletaskToMultitask(tasks, model_builder)

  # Fit trained model
  model.fit(train_dataset)
  model.save()
  # Eval model on test
  scores = model.evaluate(test_dataset, [classification_metric])
  assert scores[classification_metric.name] &gt; .5
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 71:</b> &nbsp; 2 fragments, nominal size 20 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag786')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_mpnn.py: 22-52
</a>
<div class="mid" id="frag786" style="display:none"><pre>
def test_mpnn_regression():
  # load datasets
  featurizer = MolGraphConvFeaturizer(use_edges=True)
  tasks, dataset, transformers, metric = get_dataset(
      'regression', featurizer=featurizer)

  # initialize models
  n_tasks = len(tasks)
  model = MPNNModel(mode='regression', n_tasks=n_tasks, batch_size=10)

  # overfit test
  model.fit(dataset, nb_epoch=400)
  scores = model.evaluate(dataset, [metric], transformers)
  assert scores['mean_absolute_error'] &lt; 0.5

  # test on a small MoleculeNet dataset
  from deepchem.molnet import load_delaney

  tasks, all_dataset, transformers = load_delaney(featurizer=featurizer)
  train_set, _, _ = all_dataset
  model = MPNNModel(
      mode='regression',
      n_tasks=len(tasks),
      node_out_feats=2,
      edge_hidden_feats=2,
      num_step_message_passing=1,
      num_step_set2set=1,
      num_layer_set2set=1)
  model.fit(train_set, nb_epoch=1)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag863')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_attentivefp.py: 22-50
</a>
<div class="mid" id="frag863" style="display:none"><pre>
def test_attentivefp_regression():
  # load datasets
  featurizer = MolGraphConvFeaturizer(use_edges=True)
  tasks, dataset, transformers, metric = get_dataset(
      'regression', featurizer=featurizer)

  # initialize models
  n_tasks = len(tasks)
  model = AttentiveFPModel(mode='regression', n_tasks=n_tasks, batch_size=10)

  # overfit test
  model.fit(dataset, nb_epoch=100)
  scores = model.evaluate(dataset, [metric], transformers)
  assert scores['mean_absolute_error'] &lt; 0.5

  # test on a small MoleculeNet dataset
  from deepchem.molnet import load_delaney

  tasks, all_dataset, transformers = load_delaney(featurizer=featurizer)
  train_set, _, _ = all_dataset
  model = AttentiveFPModel(
      mode='regression',
      n_tasks=len(tasks),
      num_layers=1,
      num_timesteps=1,
      graph_feat_size=2)
  model.fit(train_set, nb_epoch=1)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 72:</b> &nbsp; 9 fragments, nominal size 41 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag823')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_reload.py: 58-113
</a>
<div class="mid" id="frag823" style="display:none"><pre>
def test_multitaskregressor_reload():
  """Test that MultitaskRegressor can be reloaded correctly."""
  n_samples = 10
  n_features = 3
  n_tasks = 1

  # Generate dummy dataset
  np.random.seed(123)
  ids = np.arange(n_samples)
  X = np.random.rand(n_samples, n_features)
  y = np.random.rand(n_samples, n_tasks)
  w = np.ones((n_samples, n_tasks))

  dataset = dc.data.NumpyDataset(X, y, w, ids)
  regression_metric = dc.metrics.Metric(dc.metrics.mean_squared_error)

  model_dir = tempfile.mkdtemp()
  model = dc.models.MultitaskRegressor(
      n_tasks,
      n_features,
      dropouts=[0.],
      weight_init_stddevs=[np.sqrt(6) / np.sqrt(1000)],
      batch_size=n_samples,
      learning_rate=0.003,
      model_dir=model_dir)

  # Fit trained model
  model.fit(dataset, nb_epoch=100)

  # Eval model on train
  scores = model.evaluate(dataset, [regression_metric])
  assert scores[regression_metric.name] &lt; .1

  # Reload trained model
  reloaded_model = dc.models.MultitaskRegressor(
      n_tasks,
      n_features,
      dropouts=[0.],
      weight_init_stddevs=[np.sqrt(6) / np.sqrt(1000)],
      batch_size=n_samples,
      learning_rate=0.003,
      model_dir=model_dir)
  reloaded_model.restore()

  # Check predictions match on random sample
  Xpred = np.random.rand(n_samples, n_features)
  predset = dc.data.NumpyDataset(Xpred)
  origpred = model.predict(predset)
  reloadpred = reloaded_model.predict(predset)
  assert np.all(origpred == reloadpred)

  # Eval model on train
  scores = reloaded_model.evaluate(dataset, [regression_metric])
  assert scores[regression_metric.name] &lt; 0.1


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag831')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_reload.py: 508-569
</a>
<div class="mid" id="frag831" style="display:none"><pre>
def test_progressivemultitaskregressor_reload():
  """Test that ProgressiveMultitaskRegressor can be reloaded correctly."""
  n_samples = 10
  n_features = 3
  n_tasks = 1

  # Generate dummy dataset
  np.random.seed(123)
  ids = np.arange(n_samples)
  X = np.random.rand(n_samples, n_features)
  y = np.random.rand(n_samples, n_tasks)
  w = np.ones((n_samples, n_tasks))

  dataset = dc.data.NumpyDataset(X, y, w, ids)
  regression_metric = dc.metrics.Metric(dc.metrics.mean_squared_error)

  model_dir = tempfile.mkdtemp()
  model = dc.models.ProgressiveMultitaskRegressor(
      n_tasks,
      n_features,
      layer_sizes=[50],
      bypass_layer_sizes=[10],
      dropouts=[0.],
      learning_rate=0.001,
      weight_init_stddevs=[.1],
      alpha_init_stddevs=[.02],
      batch_size=n_samples,
      model_dir=model_dir)

  # Fit trained model
  model.fit(dataset, nb_epoch=100)

  # Eval model on train
  scores = model.evaluate(dataset, [regression_metric])
  assert scores[regression_metric.name] &lt; .1

  # Reload trained model
  reloaded_model = dc.models.ProgressiveMultitaskRegressor(
      n_tasks,
      n_features,
      layer_sizes=[50],
      bypass_layer_sizes=[10],
      dropouts=[0.],
      learning_rate=0.001,
      weight_init_stddevs=[.1],
      alpha_init_stddevs=[.02],
      batch_size=n_samples,
      model_dir=model_dir)
  reloaded_model.restore()

  # Check predictions match on random sample
  Xpred = np.random.rand(n_samples, n_features)
  predset = dc.data.NumpyDataset(Xpred)
  origpred = model.predict(predset)
  reloadpred = reloaded_model.predict(predset)
  assert np.all(origpred == reloadpred)

  # Eval model on train
  scores = reloaded_model.evaluate(dataset, [regression_metric])
  assert scores[regression_metric.name] &lt; 0.1


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag825')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_reload.py: 168-223
</a>
<div class="mid" id="frag825" style="display:none"><pre>
def test_residual_classification_reload():
  """Test that a residual network can reload correctly."""
  n_samples = 10
  n_features = 5
  n_tasks = 1
  n_classes = 2

  # Generate dummy dataset
  np.random.seed(123)
  ids = np.arange(n_samples)
  X = np.random.rand(n_samples, n_features)
  y = np.random.randint(2, size=(n_samples, n_tasks))
  w = np.ones((n_samples, n_tasks))
  dataset = dc.data.NumpyDataset(X, y, w, ids)

  classification_metric = dc.metrics.Metric(dc.metrics.accuracy_score)
  model_dir = tempfile.mkdtemp()
  model = dc.models.MultitaskClassifier(
      n_tasks,
      n_features,
      layer_sizes=[20] * 10,
      dropouts=0.0,
      batch_size=n_samples,
      residual=True,
      model_dir=model_dir)

  # Fit trained model
  model.fit(dataset, nb_epoch=500)

  # Eval model on train
  scores = model.evaluate(dataset, [classification_metric])
  assert scores[classification_metric.name] &gt; .9

  # Reload trained model
  reloaded_model = dc.models.MultitaskClassifier(
      n_tasks,
      n_features,
      layer_sizes=[20] * 10,
      dropouts=0.0,
      batch_size=n_samples,
      residual=True,
      model_dir=model_dir)
  reloaded_model.restore()

  # Check predictions match on random sample
  Xpred = np.random.rand(n_samples, n_features)
  predset = dc.data.NumpyDataset(Xpred)
  origpred = model.predict(predset)
  reloadpred = reloaded_model.predict(predset)
  assert np.all(origpred == reloadpred)

  # Eval model on train
  scores = reloaded_model.evaluate(dataset, [classification_metric])
  assert scores[classification_metric.name] &gt; .9


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag824')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_reload.py: 114-167
</a>
<div class="mid" id="frag824" style="display:none"><pre>
def test_multitaskclassification_reload():
  """Test that MultitaskClassifier can be reloaded correctly."""
  n_samples = 10
  n_features = 3
  n_tasks = 1
  n_classes = 2

  # Generate dummy dataset
  np.random.seed(123)
  ids = np.arange(n_samples)
  X = np.random.rand(n_samples, n_features)
  y = np.zeros((n_samples, n_tasks))
  w = np.ones((n_samples, n_tasks))
  dataset = dc.data.NumpyDataset(X, y, w, ids)

  classification_metric = dc.metrics.Metric(dc.metrics.accuracy_score)
  model_dir = tempfile.mkdtemp()
  model = dc.models.MultitaskClassifier(
      n_tasks,
      n_features,
      dropouts=[0.],
      weight_init_stddevs=[.1],
      batch_size=n_samples,
      optimizer=dc.models.optimizers.Adam(
          learning_rate=0.0003, beta1=0.9, beta2=0.999),
      model_dir=model_dir)

  # Fit trained model
  model.fit(dataset, nb_epoch=100)

  # Reload trained model
  reloaded_model = dc.models.MultitaskClassifier(
      n_tasks,
      n_features,
      dropouts=[0.],
      weight_init_stddevs=[.1],
      batch_size=n_samples,
      optimizer=dc.models.optimizers.Adam(
          learning_rate=0.0003, beta1=0.9, beta2=0.999),
      model_dir=model_dir)
  reloaded_model.restore()

  # Check predictions match on random sample
  Xpred = np.random.rand(n_samples, n_features)
  predset = dc.data.NumpyDataset(Xpred)
  origpred = model.predict(predset)
  reloadpred = reloaded_model.predict(predset)
  assert np.all(origpred == reloadpred)

  # Eval model on train
  scores = reloaded_model.evaluate(dataset, [classification_metric])
  assert scores[classification_metric.name] &gt; .9


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag828')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_reload.py: 328-387
</a>
<div class="mid" id="frag828" style="display:none"><pre>
def test_robust_multitask_regressor_reload():
  """Test that RobustMultitaskRegressor can be reloaded correctly."""
  n_tasks = 10
  n_samples = 10
  n_features = 3

  # Generate dummy dataset
  np.random.seed(123)
  ids = np.arange(n_samples)
  X = np.random.rand(n_samples, n_features)
  y = np.random.rand(n_samples, n_tasks)
  w = np.ones((n_samples, n_tasks))

  dataset = dc.data.NumpyDataset(X, y, w, ids)
  regression_metric = dc.metrics.Metric(dc.metrics.mean_squared_error)

  model_dir = tempfile.mkdtemp()
  model = dc.models.RobustMultitaskRegressor(
      n_tasks,
      n_features,
      layer_sizes=[50],
      bypass_layer_sizes=[10],
      dropouts=[0.],
      learning_rate=0.003,
      weight_init_stddevs=[.1],
      batch_size=n_samples,
      model_dir=model_dir)

  # Fit trained model
  model.fit(dataset, nb_epoch=100)

  # Eval model on train
  scores = model.evaluate(dataset, [regression_metric])
  assert scores[regression_metric.name] &lt; .1

  # Reload trained model
  reloaded_model = dc.models.RobustMultitaskRegressor(
      n_tasks,
      n_features,
      layer_sizes=[50],
      bypass_layer_sizes=[10],
      dropouts=[0.],
      learning_rate=0.003,
      weight_init_stddevs=[.1],
      batch_size=n_samples,
      model_dir=model_dir)
  reloaded_model.restore()

  # Check predictions match on random sample
  Xpred = np.random.rand(n_samples, n_features)
  predset = dc.data.NumpyDataset(Xpred)
  origpred = model.predict(predset)
  reloadpred = reloaded_model.predict(predset)
  assert np.all(origpred == reloadpred)

  # Eval model on train
  scores = reloaded_model.evaluate(dataset, [regression_metric])
  assert scores[regression_metric.name] &lt; 0.1


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag836')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_reload.py: 840-894
</a>
<div class="mid" id="frag836" style="display:none"><pre>
def test_1d_cnn_regression_reload():
  """Test that a 1D CNN can reload."""
  n_samples = 10
  n_features = 3
  n_tasks = 1

  np.random.seed(123)
  X = np.random.rand(n_samples, 10, n_features)
  y = np.random.randint(2, size=(n_samples, n_tasks)).astype(np.float32)
  dataset = dc.data.NumpyDataset(X, y)

  regression_metric = dc.metrics.Metric(dc.metrics.mean_squared_error)
  model_dir = tempfile.mkdtemp()

  model = dc.models.CNN(
      n_tasks,
      n_features,
      dims=1,
      dropouts=0,
      kernel_size=3,
      mode='regression',
      learning_rate=0.003,
      model_dir=model_dir)

  # Fit trained model
  model.fit(dataset, nb_epoch=200)

  # Eval model on train
  scores = model.evaluate(dataset, [regression_metric])
  assert scores[regression_metric.name] &lt; 0.1

  # Reload trained model
  reloaded_model = dc.models.CNN(
      n_tasks,
      n_features,
      dims=1,
      dropouts=0,
      kernel_size=3,
      mode='regression',
      learning_rate=0.003,
      model_dir=model_dir)
  reloaded_model.restore()

  # Check predictions match on random sample
  Xpred = np.random.rand(n_samples, 10, n_features)
  predset = dc.data.NumpyDataset(Xpred)
  origpred = model.predict(predset)
  reloadpred = reloaded_model.predict(predset)
  assert np.all(origpred == reloadpred)

  # Eval model on train
  scores = reloaded_model.evaluate(dataset, [regression_metric])
  assert scores[regression_metric.name] &lt; 0.1


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag826')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_reload.py: 224-284
</a>
<div class="mid" id="frag826" style="display:none"><pre>
def test_robust_multitask_classification_reload():
  """Test robust multitask overfits tiny data."""
  n_tasks = 10
  n_samples = 10
  n_features = 3
  n_classes = 2

  # Generate dummy dataset
  np.random.seed(123)
  ids = np.arange(n_samples)
  X = np.random.rand(n_samples, n_features)
  y = np.zeros((n_samples, n_tasks))
  w = np.ones((n_samples, n_tasks))
  dataset = dc.data.NumpyDataset(X, y, w, ids)

  classification_metric = dc.metrics.Metric(
      dc.metrics.accuracy_score, task_averager=np.mean)
  model_dir = tempfile.mkdtemp()
  model = dc.models.RobustMultitaskClassifier(
      n_tasks,
      n_features,
      layer_sizes=[50],
      bypass_layer_sizes=[10],
      dropouts=[0.],
      learning_rate=0.003,
      weight_init_stddevs=[.1],
      batch_size=n_samples,
      model_dir=model_dir)

  # Fit trained model
  model.fit(dataset, nb_epoch=25)

  # Eval model on train
  scores = model.evaluate(dataset, [classification_metric])
  assert scores[classification_metric.name] &gt; .9

  # Reloaded Trained Model
  reloaded_model = dc.models.RobustMultitaskClassifier(
      n_tasks,
      n_features,
      layer_sizes=[50],
      bypass_layer_sizes=[10],
      dropouts=[0.],
      learning_rate=0.003,
      weight_init_stddevs=[.1],
      batch_size=n_samples,
      model_dir=model_dir)
  reloaded_model.restore()

  # Check predictions match on random sample
  Xpred = np.random.rand(n_samples, n_features)
  predset = dc.data.NumpyDataset(Xpred)
  origpred = model.predict(predset)
  reloadpred = reloaded_model.predict(predset)
  assert np.all(origpred == reloadpred)

  # Eval model on train
  scores = reloaded_model.evaluate(dataset, [classification_metric])
  assert scores[classification_metric.name] &gt; .9


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag830')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_reload.py: 444-507
</a>
<div class="mid" id="frag830" style="display:none"><pre>
def test_progressive_classification_reload():
  """Test progressive multitask can reload."""
  np.random.seed(123)
  n_tasks = 5
  n_samples = 10
  n_features = 6

  # Generate dummy dataset
  np.random.seed(123)
  ids = np.arange(n_samples)
  X = np.random.rand(n_samples, n_features)
  y = np.random.randint(2, size=(n_samples, n_tasks))
  w = np.ones((n_samples, n_tasks))

  dataset = dc.data.NumpyDataset(X, y, w, ids)

  classification_metric = dc.metrics.Metric(
      dc.metrics.accuracy_score, task_averager=np.mean)
  model_dir = tempfile.mkdtemp()
  model = dc.models.ProgressiveMultitaskClassifier(
      n_tasks,
      n_features,
      layer_sizes=[50],
      bypass_layer_sizes=[10],
      dropouts=[0.],
      learning_rate=0.001,
      weight_init_stddevs=[.1],
      alpha_init_stddevs=[.02],
      batch_size=n_samples,
      model_dir=model_dir)

  # Fit trained model
  model.fit(dataset, nb_epoch=400)

  # Eval model on train
  scores = model.evaluate(dataset, [classification_metric])
  assert scores[classification_metric.name] &gt; .9

  # Reload Trained Model
  reloaded_model = dc.models.ProgressiveMultitaskClassifier(
      n_tasks,
      n_features,
      layer_sizes=[50],
      bypass_layer_sizes=[10],
      dropouts=[0.],
      learning_rate=0.001,
      weight_init_stddevs=[.1],
      alpha_init_stddevs=[.02],
      batch_size=n_samples,
      model_dir=model_dir)
  reloaded_model.restore()

  # Check predictions match on random sample
  Xpred = np.random.rand(n_samples, n_features)
  predset = dc.data.NumpyDataset(Xpred)
  origpred = model.predict(predset)
  reloadpred = reloaded_model.predict(predset)
  assert np.all(origpred == reloadpred)

  # Eval model on train
  scores = reloaded_model.evaluate(dataset, [classification_metric])
  assert scores[classification_metric.name] &gt; .9


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag829')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_reload.py: 388-442
</a>
<div class="mid" id="frag829" style="display:none"><pre>
def test_IRV_multitask_classification_reload():
  """Test IRV classifier can be reloaded."""
  n_tasks = 5
  n_samples = 10
  n_features = 128
  n_classes = 2

  # Generate dummy dataset
  np.random.seed(123)
  ids = np.arange(n_samples)
  X = np.random.randint(2, size=(n_samples, n_features))
  y = np.ones((n_samples, n_tasks))
  w = np.ones((n_samples, n_tasks))
  dataset = dc.data.NumpyDataset(X, y, w, ids)
  IRV_transformer = dc.trans.IRVTransformer(5, n_tasks, dataset)
  dataset_trans = IRV_transformer.transform(dataset)

  classification_metric = dc.metrics.Metric(
      dc.metrics.accuracy_score, task_averager=np.mean)
  model_dir = tempfile.mkdtemp()
  model = dc.models.MultitaskIRVClassifier(
      n_tasks,
      K=5,
      learning_rate=0.01,
      batch_size=n_samples,
      model_dir=model_dir)

  # Fit trained model
  model.fit(dataset_trans)

  # Eval model on train
  scores = model.evaluate(dataset_trans, [classification_metric])
  assert scores[classification_metric.name] &gt; .9

  # Reload Trained Model
  reloaded_model = dc.models.MultitaskIRVClassifier(
      n_tasks,
      K=5,
      learning_rate=0.01,
      batch_size=n_samples,
      model_dir=model_dir)
  reloaded_model.restore()

  # Check predictions match on random sample
  Xpred = np.random.rand(n_samples, n_features)
  predset = dc.data.NumpyDataset(Xpred)
  origpred = model.predict(predset)
  reloadpred = reloaded_model.predict(predset)
  assert np.all(origpred == reloadpred)

  # Eval model on train
  scores = reloaded_model.evaluate(dataset, [classification_metric])
  assert scores[classification_metric.name] &gt; .9


</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 73:</b> &nbsp; 4 fragments, nominal size 48 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag832')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_reload.py: 570-637
</a>
<div class="mid" id="frag832" style="display:none"><pre>
def test_DAG_regression_reload():
  """Test DAG regressor reloads."""
  np.random.seed(123)
  tf.random.set_seed(123)
  n_tasks = 1

  # Load mini log-solubility dataset.
  featurizer = dc.feat.ConvMolFeaturizer()
  tasks = ["outcome"]
  mols = ["CC", "CCO", "CC", "CCC", "CCCCO", "CO", "CC", "CCCCC", "CCC", "CCCO"]
  n_samples = len(mols)
  X = featurizer(mols)
  y = np.random.rand(n_samples, n_tasks)
  dataset = dc.data.NumpyDataset(X, y)

  regression_metric = dc.metrics.Metric(
      dc.metrics.pearson_r2_score, task_averager=np.mean)

  n_feat = 75
  batch_size = 10
  transformer = dc.trans.DAGTransformer(max_atoms=50)
  dataset = transformer.transform(dataset)

  model_dir = tempfile.mkdtemp()
  model = dc.models.DAGModel(
      n_tasks,
      max_atoms=50,
      n_atom_feat=n_feat,
      batch_size=batch_size,
      learning_rate=0.001,
      use_queue=False,
      mode="regression",
      model_dir=model_dir)

  # Fit trained model
  model.fit(dataset, nb_epoch=100)

  # Eval model on train
  scores = model.evaluate(dataset, [regression_metric])
  assert scores[regression_metric.name] &gt; .1

  reloaded_model = dc.models.DAGModel(
      n_tasks,
      max_atoms=50,
      n_atom_feat=n_feat,
      batch_size=batch_size,
      learning_rate=0.001,
      use_queue=False,
      mode="regression",
      model_dir=model_dir)

  reloaded_model.restore()

  # Check predictions match on random sample
  predmols = ["CCCC", "CCCCCO", "CCCCC"]
  Xpred = featurizer(predmols)
  predset = dc.data.NumpyDataset(Xpred)
  predset = transformer.transform(predset)
  origpred = model.predict(predset)
  reloadpred = reloaded_model.predict(predset)

  assert np.all(origpred == reloadpred)

  # Eval model on train
  scores = reloaded_model.evaluate(dataset, [regression_metric])
  assert scores[regression_metric.name] &gt; .1


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag833')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_reload.py: 638-701
</a>
<div class="mid" id="frag833" style="display:none"><pre>
def test_weave_classification_reload():
  """Test weave model can be reloaded."""
  np.random.seed(123)
  tf.random.set_seed(123)
  n_tasks = 1

  # Load mini log-solubility dataset.
  featurizer = dc.feat.WeaveFeaturizer()
  tasks = ["outcome"]
  mols = ["CC", "CCCCC", "CCCCC", "CCC", "COOO", "COO", "OO"]
  n_samples = len(mols)
  X = featurizer(mols)
  y = [1, 1, 1, 1, 0, 0, 0]
  dataset = dc.data.NumpyDataset(X, y)

  classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)

  batch_size = 5

  model_dir = tempfile.mkdtemp()
  model = dc.models.WeaveModel(
      n_tasks,
      batch_size=batch_size,
      learning_rate=0.01,
      mode="classification",
      dropouts=0.0,
      model_dir=model_dir)

  # Fit trained model
  model.fit(dataset, nb_epoch=100)

  # Eval model on train
  scores = model.evaluate(dataset, [classification_metric])
  assert scores[classification_metric.name] &gt; .6

  # Check predictions match on random sample
  predmols = ["CCCC", "CCCCCO", "CCCCC"]
  Xpred = featurizer(predmols)

  predset = dc.data.NumpyDataset(Xpred)
  origpred = model.predict(predset)

  reloaded_model = dc.models.WeaveModel(
      n_tasks,
      batch_size=batch_size,
      learning_rate=0.003,
      mode="classification",
      dropouts=0.0,
      model_dir=model_dir)
  reloaded_model.restore()

  # Check predictions match on random sample
  predmols = ["CCCC", "CCCCCO", "CCCCC"]
  Xpred = featurizer(predmols)
  predset = dc.data.NumpyDataset(Xpred)
  origpred = model.predict(predset)
  reloadpred = reloaded_model.predict(predset)
  assert np.all(origpred == reloadpred)

  #Eval model on train
  scores = reloaded_model.evaluate(dataset, [classification_metric])
  assert scores[classification_metric.name] &gt; .6


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag834')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_reload.py: 702-769
</a>
<div class="mid" id="frag834" style="display:none"><pre>
def test_MPNN_regression_reload():
  """Test MPNN can reload datasets."""
  np.random.seed(123)
  tf.random.set_seed(123)
  n_tasks = 1

  # Load mini log-solubility dataset.
  featurizer = dc.feat.WeaveFeaturizer()
  tasks = ["outcome"]
  mols = ["C", "CO", "CC"]
  n_samples = len(mols)
  X = featurizer(mols)
  y = np.random.rand(n_samples, n_tasks)
  dataset = dc.data.NumpyDataset(X, y)

  regression_metric = dc.metrics.Metric(
      dc.metrics.pearson_r2_score, task_averager=np.mean)

  n_atom_feat = 75
  n_pair_feat = 14
  batch_size = 10
  model_dir = tempfile.mkdtemp()
  model = dc.models.MPNNModel(
      n_tasks,
      n_atom_feat=n_atom_feat,
      n_pair_feat=n_pair_feat,
      T=2,
      M=3,
      batch_size=batch_size,
      learning_rate=0.001,
      use_queue=False,
      mode="regression",
      model_dir=model_dir)

  # Fit trained model
  model.fit(dataset, nb_epoch=50)

  # Eval model on train
  scores = model.evaluate(dataset, [regression_metric])
  assert scores[regression_metric.name] &gt; .8

  # Reload trained model
  reloaded_model = dc.models.MPNNModel(
      n_tasks,
      n_atom_feat=n_atom_feat,
      n_pair_feat=n_pair_feat,
      T=2,
      M=3,
      batch_size=batch_size,
      learning_rate=0.001,
      use_queue=False,
      mode="regression",
      model_dir=model_dir)
  reloaded_model.restore()

  # Eval model on train
  scores = reloaded_model.evaluate(dataset, [regression_metric])
  assert scores[regression_metric.name] &gt; .8

  # Check predictions match on random sample
  predmols = ["CCCC", "CCCCCO", "CCCCC"]
  Xpred = featurizer(predmols)
  predset = dc.data.NumpyDataset(Xpred)
  origpred = model.predict(predset)
  reloadpred = reloaded_model.predict(predset)
  assert np.all(origpred == reloadpred)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag835')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_reload.py: 770-839
</a>
<div class="mid" id="frag835" style="display:none"><pre>
def test_textCNN_classification_reload():
  """Test textCNN model reloadinng."""
  np.random.seed(123)
  tf.random.set_seed(123)
  n_tasks = 1

  featurizer = dc.feat.RawFeaturizer()
  tasks = ["outcome"]
  mols = ["C", "CO", "CC"]
  n_samples = len(mols)
  X = featurizer(mols)
  y = np.random.randint(2, size=(n_samples, n_tasks))
  dataset = dc.data.NumpyDataset(X, y, ids=mols)

  classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)

  char_dict, length = dc.models.TextCNNModel.build_char_dict(dataset)
  batch_size = 3

  model_dir = tempfile.mkdtemp()
  model = dc.models.TextCNNModel(
      n_tasks,
      char_dict,
      seq_length=length,
      batch_size=batch_size,
      learning_rate=0.001,
      use_queue=False,
      mode="classification",
      model_dir=model_dir)

  # Fit trained model
  model.fit(dataset, nb_epoch=200)

  # Eval model on train
  scores = model.evaluate(dataset, [classification_metric])
  assert scores[classification_metric.name] &gt; .8

  # Reload trained model
  reloaded_model = dc.models.TextCNNModel(
      n_tasks,
      char_dict,
      seq_length=length,
      batch_size=batch_size,
      learning_rate=0.001,
      use_queue=False,
      mode="classification",
      model_dir=model_dir)
  reloaded_model.restore()

  # Eval model on train
  scores = reloaded_model.evaluate(dataset, [classification_metric])
  assert scores[classification_metric.name] &gt; .8

  assert len(reloaded_model.model.get_weights()) == len(
      model.model.get_weights())
  for (reloaded, orig) in zip(reloaded_model.model.get_weights(),
                              model.model.get_weights()):
    assert np.all(reloaded == orig)

  # Check predictions match on random sample
  predmols = ["CCCC", "CCCCCO", "CCCCC"]
  Xpred = featurizer(predmols)
  predset = dc.data.NumpyDataset(Xpred, ids=predmols)
  origpred = model.predict(predset)
  reloadpred = reloaded_model.predict(predset)
  assert np.all(origpred == reloadpred)

  assert len(model.model.layers) == len(reloaded_model.model.layers)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 74:</b> &nbsp; 2 fragments, nominal size 29 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag844')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_api.py: 42-81
</a>
<div class="mid" id="frag844" style="display:none"><pre>
def test_singletask_sklearn_rf_user_specified_regression_API():
  """Test of singletask RF USF regression API."""
  featurizer = dc.feat.UserDefinedFeaturizer(
      ["user-specified1", "user-specified2"])
  tasks = ["log-solubility"]
  current_dir = os.path.dirname(os.path.abspath(__file__))
  input_file = os.path.join(current_dir, "user_specified_example.csv")
  loader = dc.data.UserCSVLoader(
      tasks=tasks, feature_field="smiles", featurizer=featurizer)
  dataset = loader.create_dataset(input_file)

  splitter = dc.splits.RandomSplitter()
  train_dataset, test_dataset = splitter.train_test_split(dataset)

  transformers = [
      dc.trans.NormalizationTransformer(
          transform_y=True, dataset=train_dataset)
  ]
  for dataset in [train_dataset, test_dataset]:
    for transformer in transformers:
      dataset = transformer.transform(dataset)

  regression_metrics = [
      dc.metrics.Metric(dc.metrics.r2_score),
      dc.metrics.Metric(dc.metrics.mean_squared_error),
      dc.metrics.Metric(dc.metrics.mean_absolute_error)
  ]

  sklearn_model = RandomForestRegressor()
  model = dc.models.SklearnModel(sklearn_model)

  # Fit trained model
  model.fit(train_dataset)
  model.save()

  # Eval model on train/test
  _ = model.evaluate(train_dataset, regression_metrics, transformers)
  _ = model.evaluate(test_dataset, regression_metrics, transformers)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag845')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_api.py: 82-125
</a>
<div class="mid" id="frag845" style="display:none"><pre>
def test_singletask_sklearn_rf_RDKIT_descriptor_regression_API():
  """Test of singletask RF RDKIT-descriptor regression API."""
  splittype = "scaffold"
  featurizer = dc.feat.RDKitDescriptors()
  tasks = ["log-solubility"]

  current_dir = os.path.dirname(os.path.abspath(__file__))
  input_file = os.path.join(current_dir, "example.csv")
  loader = dc.data.CSVLoader(
      tasks=tasks, feature_field="smiles", featurizer=featurizer)
  dataset = loader.create_dataset(input_file)

  splitter = dc.splits.ScaffoldSplitter()
  train_dataset, test_dataset = splitter.train_test_split(dataset)

  transformers = [
      dc.trans.NormalizationTransformer(
          transform_X=True, dataset=train_dataset),
      dc.trans.ClippingTransformer(transform_X=True, dataset=train_dataset),
      dc.trans.NormalizationTransformer(
          transform_y=True, dataset=train_dataset)
  ]
  for dataset in [train_dataset, test_dataset]:
    for transformer in transformers:
      dataset = transformer.transform(dataset)

  regression_metrics = [
      dc.metrics.Metric(dc.metrics.r2_score),
      dc.metrics.Metric(dc.metrics.mean_squared_error),
      dc.metrics.Metric(dc.metrics.mean_absolute_error)
  ]

  sklearn_model = RandomForestRegressor()
  model = dc.models.SklearnModel(sklearn_model)

  # Fit trained model
  model.fit(train_dataset)
  model.save()

  # Eval model on train/test
  _ = model.evaluate(train_dataset, regression_metrics, transformers)
  _ = model.evaluate(test_dataset, regression_metrics, transformers)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 75:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag848')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_graph_models.py: 42-57
</a>
<div class="mid" id="frag848" style="display:none"><pre>
def test_graph_conv_model():
  tasks, dataset, transformers, metric = get_dataset('classification',
                                                     'GraphConv')

  batch_size = 10
  model = GraphConvModel(
      len(tasks),
      batch_size=batch_size,
      batch_normalize=False,
      mode='classification')

  model.fit(dataset, nb_epoch=20)
  scores = model.evaluate(dataset, [metric], transformers)
  assert scores['mean-roc_auc_score'] &gt;= 0.9


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag850')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_graph_models.py: 77-91
</a>
<div class="mid" id="frag850" style="display:none"><pre>
def test_graph_conv_regression_model():
  tasks, dataset, transformers, metric = get_dataset('regression', 'GraphConv')

  batch_size = 10
  model = GraphConvModel(
      len(tasks),
      batch_size=batch_size,
      batch_normalize=False,
      mode='regression')

  model.fit(dataset, nb_epoch=100)
  scores = model.evaluate(dataset, [metric], transformers)
  assert scores['mean_absolute_error'] &lt; 0.1


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 76:</b> &nbsp; 2 fragments, nominal size 20 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag851')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_graph_models.py: 92-115
</a>
<div class="mid" id="frag851" style="display:none"><pre>
def test_graph_conv_regression_uncertainty():
  tasks, dataset, transformers, metric = get_dataset('regression', 'GraphConv')

  batch_size = 10
  model = GraphConvModel(
      len(tasks),
      batch_size=batch_size,
      batch_normalize=False,
      mode='regression',
      dropout=0.1,
      uncertainty=True)

  model.fit(dataset, nb_epoch=100)

  # Predict the output and uncertainty.
  pred, std = model.predict_uncertainty(dataset)
  mean_error = np.mean(np.abs(dataset.y - pred))
  mean_value = np.mean(np.abs(dataset.y))
  mean_std = np.mean(std)
  assert mean_error &lt; 0.5 * mean_value
  assert mean_std &gt; 0.5 * mean_error
  assert mean_std &lt; mean_value


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag858')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_graph_models.py: 271-298
</a>
<div class="mid" id="frag858" style="display:none"><pre>
def test_mpnn_regression_uncertainty():
  tasks, dataset, transformers, metric = get_dataset('regression', 'Weave')

  batch_size = 10
  model = MPNNModel(
      len(tasks),
      mode='regression',
      n_hidden=75,
      n_atom_feat=75,
      n_pair_feat=14,
      T=1,
      M=1,
      dropout=0.1,
      batch_size=batch_size,
      uncertainty=True)

  model.fit(dataset, nb_epoch=40)

  # Predict the output and uncertainty.
  pred, std = model.predict_uncertainty(dataset)
  mean_error = np.mean(np.abs(dataset.y - pred))
  mean_value = np.mean(np.abs(dataset.y))
  mean_std = np.mean(std)
  assert mean_error &lt; 0.5 * mean_value
  assert mean_std &gt; 0.5 * mean_error
  assert mean_std &lt; mean_value


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 77:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag853')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_graph_models.py: 145-166
</a>
<div class="mid" id="frag853" style="display:none"><pre>
def test_dag_model():
  tasks, dataset, transformers, metric = get_dataset('classification',
                                                     'GraphConv')

  batch_size = 10
  max_atoms = max([mol.get_num_atoms() for mol in dataset.X])
  transformer = dc.trans.DAGTransformer(max_atoms=max_atoms)
  dataset = transformer.transform(dataset)

  model = DAGModel(
      len(tasks),
      max_atoms=max_atoms,
      mode='classification',
      learning_rate=0.03,
      batch_size=batch_size,
      use_queue=False)

  model.fit(dataset, nb_epoch=40)
  scores = model.evaluate(dataset, [metric], transformers)
  assert scores['mean-roc_auc_score'] &gt;= 0.9


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag854')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_graph_models.py: 168-191
</a>
<div class="mid" id="frag854" style="display:none"><pre>
def test_dag_regression_model():
  import tensorflow as tf
  np.random.seed(1234)
  tf.random.set_seed(1234)
  tasks, dataset, transformers, metric = get_dataset('regression', 'GraphConv')

  batch_size = 10
  max_atoms = max([mol.get_num_atoms() for mol in dataset.X])
  transformer = dc.trans.DAGTransformer(max_atoms=max_atoms)
  dataset = transformer.transform(dataset)

  model = DAGModel(
      len(tasks),
      max_atoms=max_atoms,
      mode='regression',
      learning_rate=0.03,
      batch_size=batch_size,
      use_queue=False)

  model.fit(dataset, nb_epoch=1200)
  scores = model.evaluate(dataset, [metric], transformers)
  assert scores['mean_absolute_error'] &lt; 0.15


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 78:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 93%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag856')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_graph_models.py: 231-249
</a>
<div class="mid" id="frag856" style="display:none"><pre>
def test_mpnn_model():
  tasks, dataset, transformers, metric = get_dataset('classification', 'Weave')

  batch_size = 10
  model = MPNNModel(
      len(tasks),
      mode='classification',
      n_hidden=75,
      n_atom_feat=75,
      n_pair_feat=14,
      T=1,
      M=1,
      batch_size=batch_size)

  model.fit(dataset, nb_epoch=40)
  scores = model.evaluate(dataset, [metric], transformers)
  assert scores['mean-roc_auc_score'] &gt;= 0.9


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag857')" href="javascript:;">
deepchem-2.4.0/deepchem/models/tests/test_graph_models.py: 251-269
</a>
<div class="mid" id="frag857" style="display:none"><pre>
def test_mpnn_regression_model():
  tasks, dataset, transformers, metric = get_dataset('regression', 'Weave')

  batch_size = 10
  model = MPNNModel(
      len(tasks),
      mode='regression',
      n_hidden=75,
      n_atom_feat=75,
      n_pair_feat=14,
      T=1,
      M=1,
      batch_size=batch_size)

  model.fit(dataset, nb_epoch=60)
  scores = model.evaluate(dataset, [metric], transformers)
  assert scores['mean_absolute_error'] &lt; 0.1


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 79:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag899')" href="javascript:;">
deepchem-2.4.0/deepchem/models/seqtoseq.py: 197-209
</a>
<div class="mid" id="frag899" style="display:none"><pre>
  def _create_decoder(self, n_layers, dropout):
    """Create the decoder as a tf.keras.Model."""
    input = Input(shape=(self._embedding_dimension,))
    prev_layer = layers.Stack()(self._max_output_length * [input])
    for i in range(n_layers):
      if dropout &gt; 0.0:
        prev_layer = Dropout(dropout)(prev_layer)
      prev_layer = GRU(
          self._embedding_dimension, return_sequences=True)(prev_layer)
    output = Dense(
        len(self._output_tokens), activation=tf.nn.softmax)(prev_layer)
    return tf.keras.Model(inputs=input, outputs=output)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag914')" href="javascript:;">
deepchem-2.4.0/deepchem/models/seqtoseq.py: 523-536
</a>
<div class="mid" id="frag914" style="display:none"><pre>
  def _create_decoder(self, n_layers, dropout):
    """Create the decoder as a tf.keras.Model."""
    input = Input(shape=(self._embedding_dimension,))
    prev_layer = Dense(self._embedding_dimension, activation=tf.nn.relu)(input)
    prev_layer = layers.Stack()(self._max_output_length * [prev_layer])
    for i in range(3):
      if dropout &gt; 0.0:
        prev_layer = Dropout(dropout)(prev_layer)
      prev_layer = GRU(
          self._decoder_dimension, return_sequences=True)(prev_layer)
    output = Dense(
        len(self._output_tokens), activation=tf.nn.softmax)(prev_layer)
    return tf.keras.Model(inputs=input, outputs=output)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 80:</b> &nbsp; 2 fragments, nominal size 21 lines, similarity 77%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag967')" href="javascript:;">
deepchem-2.4.0/deepchem/models/losses.py: 387-411
</a>
<div class="mid" id="frag967" style="display:none"><pre>
def _make_tf_shapes_consistent(output, labels):
  """Try to make inputs have the same shape by adding dimensions of size 1."""
  import tensorflow as tf
  shape1 = output.shape
  shape2 = labels.shape
  len1 = len(shape1)
  len2 = len(shape2)
  if len1 == len2:
    return (output, labels)
  if isinstance(shape1, tf.TensorShape):
    shape1 = tuple(shape1.as_list())
  if isinstance(shape2, tf.TensorShape):
    shape2 = tuple(shape2.as_list())
  if len1 &gt; len2 and all(i == 1 for i in shape1[len2:]):
    for i in range(len1 - len2):
      labels = tf.expand_dims(labels, -1)
    return (output, labels)
  if len2 &gt; len1 and all(i == 1 for i in shape2[len1:]):
    for i in range(len2 - len1):
      output = tf.expand_dims(output, -1)
    return (output, labels)
  raise ValueError("Incompatible shapes for outputs and labels: %s versus %s" %
                   (str(shape1), str(shape2)))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag968')" href="javascript:;">
deepchem-2.4.0/deepchem/models/losses.py: 412-434
</a>
<div class="mid" id="frag968" style="display:none"><pre>
def _make_pytorch_shapes_consistent(output, labels):
  """Try to make inputs have the same shape by adding dimensions of size 1."""
  import torch
  shape1 = output.shape
  shape2 = labels.shape
  len1 = len(shape1)
  len2 = len(shape2)
  if len1 == len2:
    return (output, labels)
  shape1 = tuple(shape1)
  shape2 = tuple(shape2)
  if len1 &gt; len2 and all(i == 1 for i in shape1[len2:]):
    for i in range(len1 - len2):
      labels = torch.unsqueeze(labels, -1)
    return (output, labels)
  if len2 &gt; len1 and all(i == 1 for i in shape2[len1:]):
    for i in range(len2 - len1):
      output = torch.unsqueeze(output, -1)
    return (output, labels)
  raise ValueError("Incompatible shapes for outputs and labels: %s versus %s" %
                   (str(shape1), str(shape2)))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 81:</b> &nbsp; 2 fragments, nominal size 88 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag973')" href="javascript:;">
deepchem-2.4.0/deepchem/models/robust_multitask.py: 35-179
</a>
<div class="mid" id="frag973" style="display:none"><pre>
  def __init__(self,
               n_tasks,
               n_features,
               layer_sizes=[1000],
               weight_init_stddevs=0.02,
               bias_init_consts=1.0,
               weight_decay_penalty=0.0,
               weight_decay_penalty_type="l2",
               dropouts=0.5,
               activation_fns=tf.nn.relu,
               n_classes=2,
               bypass_layer_sizes=[100],
               bypass_weight_init_stddevs=[.02],
               bypass_bias_init_consts=[1.],
               bypass_dropouts=[.5],
               **kwargs):
    """  Create a RobustMultitaskClassifier.

    Parameters
    ----------
    n_tasks: int
      number of tasks
    n_features: int
      number of features
    layer_sizes: list
      the size of each dense layer in the network.  The length of this list determines the number of layers.
    weight_init_stddevs: list or float
      the standard deviation of the distribution to use for weight initialization of each layer.  The length
      of this list should equal len(layer_sizes).  Alternatively this may be a single value instead of a list,
      in which case the same value is used for every layer.
    bias_init_consts: list or loat
      the value to initialize the biases in each layer to.  The length of this list should equal len(layer_sizes).
      Alternatively this may be a single value instead of a list, in which case the same value is used for every layer.
    weight_decay_penalty: float
      the magnitude of the weight decay penalty to use
    weight_decay_penalty_type: str
      the type of penalty to use for weight decay, either 'l1' or 'l2'
    dropouts: list or float
      the dropout probablity to use for each layer.  The length of this list should equal len(layer_sizes).
      Alternatively this may be a single value instead of a list, in which case the same value is used for every layer.
    activation_fns: list or object
      the Tensorflow activation function to apply to each layer.  The length of this list should equal
      len(layer_sizes).  Alternatively this may be a single value instead of a list, in which case the
      same value is used for every layer.
    n_classes: int
      the number of classes
    bypass_layer_sizes: list
      the size of each dense layer in the bypass network. The length of this list determines the number of bypass layers.
    bypass_weight_init_stddevs: list or float
      the standard deviation of the distribution to use for weight initialization of bypass layers.
      same requirements as weight_init_stddevs
    bypass_bias_init_consts: list or float
      the value to initialize the biases in bypass layers
      same requirements as bias_init_consts
    bypass_dropouts: list or float
      the dropout probablity to use for bypass layers.
      same requirements as dropouts
    """
    self.n_tasks = n_tasks
    self.n_features = n_features
    self.n_classes = n_classes
    n_layers = len(layer_sizes)
    if not isinstance(weight_init_stddevs, SequenceCollection):
      weight_init_stddevs = [weight_init_stddevs] * n_layers
    if not isinstance(bias_init_consts, SequenceCollection):
      bias_init_consts = [bias_init_consts] * n_layers
    if not isinstance(dropouts, SequenceCollection):
      dropouts = [dropouts] * n_layers
    if not isinstance(activation_fns, SequenceCollection):
      activation_fns = [activation_fns] * n_layers
    if weight_decay_penalty != 0.0:
      if weight_decay_penalty_type == 'l1':
        regularizer = tf.keras.regularizers.l1(weight_decay_penalty)
      else:
        regularizer = tf.keras.regularizers.l2(weight_decay_penalty)
    else:
      regularizer = None

    n_bypass_layers = len(bypass_layer_sizes)
    if not isinstance(bypass_weight_init_stddevs, SequenceCollection):
      bypass_weight_init_stddevs = [bypass_weight_init_stddevs
                                   ] * n_bypass_layers
    if not isinstance(bypass_bias_init_consts, SequenceCollection):
      bypass_bias_init_consts = [bypass_bias_init_consts] * n_bypass_layers
    if not isinstance(bypass_dropouts, SequenceCollection):
      bypass_dropouts = [bypass_dropouts] * n_bypass_layers
    bypass_activation_fns = [activation_fns[0]] * n_bypass_layers

    # Add the input features.
    mol_features = tf.keras.Input(shape=(n_features,))
    prev_layer = mol_features

    # Add the shared dense layers
    for size, weight_stddev, bias_const, dropout, activation_fn in zip(
        layer_sizes, weight_init_stddevs, bias_init_consts, dropouts,
        activation_fns):
      layer = tf.keras.layers.Dense(
          size,
          activation=activation_fn,
          kernel_initializer=tf.keras.initializers.TruncatedNormal(
              stddev=weight_stddev),
          bias_initializer=tf.constant_initializer(value=bias_const),
          kernel_regularizer=regularizer)(prev_layer)
      if dropout &gt; 0.0:
        layer = tf.keras.layers.Dropout(rate=dropout)(layer)
      prev_layer = layer
    top_multitask_layer = prev_layer

    task_outputs = []
    for i in range(self.n_tasks):
      prev_layer = mol_features
      # Add task-specific bypass layers
      for size, weight_stddev, bias_const, dropout, activation_fn in zip(
          bypass_layer_sizes, bypass_weight_init_stddevs,
          bypass_bias_init_consts, bypass_dropouts, bypass_activation_fns):
        layer = tf.keras.layers.Dense(
            size,
            activation=activation_fn,
            kernel_initializer=tf.keras.initializers.TruncatedNormal(
                stddev=weight_stddev),
            bias_initializer=tf.constant_initializer(value=bias_const),
            kernel_regularizer=regularizer)(prev_layer)
        if dropout &gt; 0.0:
          layer = tf.keras.layers.Dropout(rate=dropout)(layer)
        prev_layer = layer
      top_bypass_layer = prev_layer

      if n_bypass_layers &gt; 0:
        task_layer = tf.keras.layers.Concatenate(axis=1)(
            [top_multitask_layer, top_bypass_layer])
      else:
        task_layer = top_multitask_layer

      task_out = tf.keras.layers.Dense(n_classes)(task_layer)
      task_outputs.append(task_out)

    logits = Stack(axis=1)(task_outputs)
    output = tf.keras.layers.Softmax()(logits)
    model = tf.keras.Model(inputs=mol_features, outputs=[output, logits])
    super(RobustMultitaskClassifier, self).__init__(
        model,
        SoftmaxCrossEntropy(),
        output_types=['prediction', 'loss'],
        **kwargs)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag976')" href="javascript:;">
deepchem-2.4.0/deepchem/models/robust_multitask.py: 224-360
</a>
<div class="mid" id="frag976" style="display:none"><pre>
  def __init__(self,
               n_tasks,
               n_features,
               layer_sizes=[1000],
               weight_init_stddevs=0.02,
               bias_init_consts=1.0,
               weight_decay_penalty=0.0,
               weight_decay_penalty_type="l2",
               dropouts=0.5,
               activation_fns=tf.nn.relu,
               bypass_layer_sizes=[100],
               bypass_weight_init_stddevs=[.02],
               bypass_bias_init_consts=[1.],
               bypass_dropouts=[.5],
               **kwargs):
    """ Create a RobustMultitaskRegressor.

    Parameters
    ----------
    n_tasks: int
      number of tasks
    n_features: int
      number of features
    layer_sizes: list
      the size of each dense layer in the network.  The length of this list determines the number of layers.
    weight_init_stddevs: list or float
      the standard deviation of the distribution to use for weight initialization of each layer.  The length
      of this list should equal len(layer_sizes).  Alternatively this may be a single value instead of a list,
      in which case the same value is used for every layer.
    bias_init_consts: list or loat
      the value to initialize the biases in each layer to.  The length of this list should equal len(layer_sizes).
      Alternatively this may be a single value instead of a list, in which case the same value is used for every layer.
    weight_decay_penalty: float
      the magnitude of the weight decay penalty to use
    weight_decay_penalty_type: str
      the type of penalty to use for weight decay, either 'l1' or 'l2'
    dropouts: list or float
      the dropout probablity to use for each layer.  The length of this list should equal len(layer_sizes).
      Alternatively this may be a single value instead of a list, in which case the same value is used for every layer.
    activation_fns: list or object
      the Tensorflow activation function to apply to each layer.  The length of this list should equal
      len(layer_sizes).  Alternatively this may be a single value instead of a list, in which case the
      same value is used for every layer.
    bypass_layer_sizes: list
      the size of each dense layer in the bypass network. The length of this list determines the number of bypass layers.
    bypass_weight_init_stddevs: list or float
      the standard deviation of the distribution to use for weight initialization of bypass layers.
      same requirements as weight_init_stddevs
    bypass_bias_init_consts: list or float
      the value to initialize the biases in bypass layers
      same requirements as bias_init_consts
    bypass_dropouts: list or float
      the dropout probablity to use for bypass layers.
      same requirements as dropouts
    """
    self.n_tasks = n_tasks
    self.n_features = n_features
    n_layers = len(layer_sizes)
    if not isinstance(weight_init_stddevs, SequenceCollection):
      weight_init_stddevs = [weight_init_stddevs] * n_layers
    if not isinstance(bias_init_consts, SequenceCollection):
      bias_init_consts = [bias_init_consts] * n_layers
    if not isinstance(dropouts, SequenceCollection):
      dropouts = [dropouts] * n_layers
    if not isinstance(activation_fns, SequenceCollection):
      activation_fns = [activation_fns] * n_layers
    if weight_decay_penalty != 0.0:
      if weight_decay_penalty_type == 'l1':
        regularizer = tf.keras.regularizers.l1(weight_decay_penalty)
      else:
        regularizer = tf.keras.regularizers.l2(weight_decay_penalty)
    else:
      regularizer = None

    n_bypass_layers = len(bypass_layer_sizes)
    if not isinstance(bypass_weight_init_stddevs, SequenceCollection):
      bypass_weight_init_stddevs = [bypass_weight_init_stddevs
                                   ] * n_bypass_layers
    if not isinstance(bypass_bias_init_consts, SequenceCollection):
      bypass_bias_init_consts = [bypass_bias_init_consts] * n_bypass_layers
    if not isinstance(bypass_dropouts, SequenceCollection):
      bypass_dropouts = [bypass_dropouts] * n_bypass_layers
    bypass_activation_fns = [activation_fns[0]] * n_bypass_layers

    # Add the input features.
    mol_features = tf.keras.Input(shape=(n_features,))
    prev_layer = mol_features

    # Add the shared dense layers
    for size, weight_stddev, bias_const, dropout, activation_fn in zip(
        layer_sizes, weight_init_stddevs, bias_init_consts, dropouts,
        activation_fns):
      layer = tf.keras.layers.Dense(
          size,
          activation=activation_fn,
          kernel_initializer=tf.keras.initializers.TruncatedNormal(
              stddev=weight_stddev),
          bias_initializer=tf.constant_initializer(value=bias_const),
          kernel_regularizer=regularizer)(prev_layer)
      if dropout &gt; 0.0:
        layer = tf.keras.layers.Dropout(rate=dropout)(layer)
      prev_layer = layer
    top_multitask_layer = prev_layer

    task_outputs = []
    for i in range(self.n_tasks):
      prev_layer = mol_features
      # Add task-specific bypass layers
      for size, weight_stddev, bias_const, dropout, activation_fn in zip(
          bypass_layer_sizes, bypass_weight_init_stddevs,
          bypass_bias_init_consts, bypass_dropouts, bypass_activation_fns):
        layer = tf.keras.layers.Dense(
            size,
            activation=activation_fn,
            kernel_initializer=tf.keras.initializers.TruncatedNormal(
                stddev=weight_stddev),
            bias_initializer=tf.constant_initializer(value=bias_const),
            kernel_regularizer=regularizer)(prev_layer)
        if dropout &gt; 0.0:
          layer = tf.keras.layers.Dropout(rate=dropout)(layer)
        prev_layer = layer
      top_bypass_layer = prev_layer

      if n_bypass_layers &gt; 0:
        task_layer = tf.keras.layers.Concatenate(axis=1)(
            [top_multitask_layer, top_bypass_layer])
      else:
        task_layer = top_multitask_layer

      task_out = tf.keras.layers.Dense(1)(task_layer)
      task_outputs.append(task_out)

    outputs = Stack(axis=1)(task_outputs)
    model = tf.keras.Model(inputs=mol_features, outputs=outputs)
    super(RobustMultitaskRegressor, self).__init__(
        model, L2Loss(), output_types=['prediction'], **kwargs)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 82:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 86%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1023')" href="javascript:;">
deepchem-2.4.0/deepchem/feat/molecule_featurizers/coulomb_matrices.py: 41-74
</a>
<div class="mid" id="frag1023" style="display:none"><pre>
  def __init__(self,
               max_atoms: int,
               remove_hydrogens: bool = False,
               randomize: bool = False,
               upper_tri: bool = False,
               n_samples: int = 1,
               seed: Optional[int] = None):
    """Initialize this featurizer.

    Parameters
    ----------
    max_atoms: int
      The maximum number of atoms expected for molecules this featurizer will
      process.
    remove_hydrogens: bool, optional (default False)
      If True, remove hydrogens before processing them.
    randomize: bool, optional (default False)
      If True, use method `randomize_coulomb_matrices` to randomize Coulomb matrices.
    upper_tri: bool, optional (default False)
      Generate only upper triangle part of Coulomb matrices.
    n_samples: int, optional (default 1)
      If `randomize` is set to True, the number of random samples to draw.
    seed: int, optional (default None)
      Random seed to use.
    """
    self.max_atoms = int(max_atoms)
    self.remove_hydrogens = remove_hydrogens
    self.randomize = randomize
    self.upper_tri = upper_tri
    self.n_samples = n_samples
    if seed is not None:
      seed = int(seed)
    self.seed = seed

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1028')" href="javascript:;">
deepchem-2.4.0/deepchem/feat/molecule_featurizers/coulomb_matrices.py: 233-262
</a>
<div class="mid" id="frag1028" style="display:none"><pre>
  def __init__(self,
               max_atoms: int,
               remove_hydrogens: bool = False,
               randomize: bool = False,
               n_samples: int = 1,
               seed: Optional[int] = None):
    """Initialize this featurizer.

    Parameters
    ----------
    max_atoms: int
      The maximum number of atoms expected for molecules this featurizer will
      process.
    remove_hydrogens: bool, optional (default False)
      If True, remove hydrogens before processing them.
    randomize: bool, optional (default False)
      If True, use method `randomize_coulomb_matrices` to randomize Coulomb matrices.
    n_samples: int, optional (default 1)
      If `randomize` is set to True, the number of random samples to draw.
    seed: int, optional (default None)
      Random seed to use.
    """
    self.max_atoms = int(max_atoms)
    self.remove_hydrogens = remove_hydrogens
    self.randomize = randomize
    self.n_samples = n_samples
    if seed is not None:
      seed = int(seed)
    self.seed = seed

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 83:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1036')" href="javascript:;">
deepchem-2.4.0/deepchem/feat/tests/test_one_hot_featurizer.py: 14-29
</a>
<div class="mid" id="frag1036" style="display:none"><pre>
  def test_onehot_featurizer(self):
    """
    Test simple one hot encoding.
    """
    from rdkit import Chem
    length = len(ZINC_CHARSET) + 1
    smiles = 'CC(=O)Oc1ccccc1C(=O)O'
    mol = Chem.MolFromSmiles(smiles)
    featurizer = OneHotFeaturizer()
    feature = featurizer([mol])
    assert feature.shape == (1, 100, length)

    # untranform
    undo_smiles = featurizer.untransform(feature[0])
    assert smiles == undo_smiles

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1037')" href="javascript:;">
deepchem-2.4.0/deepchem/feat/tests/test_one_hot_featurizer.py: 30-45
</a>
<div class="mid" id="frag1037" style="display:none"><pre>
  def test_onehot_featurizer_with_max_length(self):
    """
    Test one hot encoding with max_length.
    """
    from rdkit import Chem
    length = len(ZINC_CHARSET) + 1
    smiles = 'CC(=O)Oc1ccccc1C(=O)O'
    mol = Chem.MolFromSmiles(smiles)
    featurizer = OneHotFeaturizer(max_length=120)
    feature = featurizer([mol])
    assert feature.shape == (1, 120, length)

    # untranform
    undo_smiles = featurizer.untransform(feature[0])
    assert smiles == undo_smiles

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 84:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1055')" href="javascript:;">
deepchem-2.4.0/deepchem/feat/tests/test_rdkit_grid_features.py: 34-45
</a>
<div class="mid" id="frag1055" style="display:none"><pre>
  def test_load_molecule(self):
    # adding hydrogens and charges is tested in dc.utils
    from rdkit.Chem.AllChem import Mol
    for add_hydrogens in (True, False):
      for calc_charges in (True, False):
        mol_xyz, mol_rdk = rgf.load_molecule(self.ligand_file, add_hydrogens,
                                             calc_charges)
        num_atoms = mol_rdk.GetNumAtoms()
        self.assertIsInstance(mol_xyz, np.ndarray)
        self.assertIsInstance(mol_rdk, Mol)
        self.assertEqual(mol_xyz.shape, (num_atoms, 3))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1664')" href="javascript:;">
deepchem-2.4.0/deepchem/utils/test/test_rdkit_utils.py: 27-38
</a>
<div class="mid" id="frag1664" style="display:none"><pre>
  def test_load_molecule(self):
    # adding hydrogens and charges is tested in dc.utils
    from rdkit.Chem.AllChem import Mol
    for add_hydrogens in (True, False):
      for calc_charges in (True, False):
        mol_xyz, mol_rdk = rdkit_utils.load_molecule(
            self.ligand_file, add_hydrogens, calc_charges)
        num_atoms = mol_rdk.GetNumAtoms()
        self.assertIsInstance(mol_xyz, np.ndarray)
        self.assertIsInstance(mol_rdk, Mol)
        self.assertEqual(mol_xyz.shape, (num_atoms, 3))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 85:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 84%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1059')" href="javascript:;">
deepchem-2.4.0/deepchem/feat/tests/test_rdkit_grid_features.py: 72-89
</a>
<div class="mid" id="frag1059" style="display:none"><pre>
  def test_compute_pairwise_distances(self):
    n1 = 10
    n2 = 50
    coords1 = np.random.rand(n1, 3)
    coords2 = np.random.rand(n2, 3)

    distance = rgf.compute_pairwise_distances(coords1, coords2)
    self.assertEqual(distance.shape, (n1, n2))
    self.assertTrue((distance &gt;= 0).all())
    # random coords between 0 and 1, so the max possible distance in sqrt(2)
    self.assertTrue((distance &lt;= 2.0**0.5).all())

    # check if correct distance metric was used
    coords1 = np.array([[0, 0, 0], [1, 0, 0]])
    coords2 = np.array([[1, 0, 0], [2, 0, 0], [3, 0, 0]])
    distance = rgf.compute_pairwise_distances(coords1, coords2)
    self.assertTrue((distance == [[1, 2, 3], [0, 1, 2]]).all())

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1590')" href="javascript:;">
deepchem-2.4.0/deepchem/utils/test/test_geometry_utils.py: 49-66
</a>
<div class="mid" id="frag1590" style="display:none"><pre>
  def test_compute_pairwise_distances(self):
    n1 = 10
    n2 = 50
    coords1 = np.random.rand(n1, 3)
    coords2 = np.random.rand(n2, 3)

    distance = compute_pairwise_distances(coords1, coords2)
    self.assertEqual(distance.shape, (n1, n2))
    self.assertTrue((distance &gt;= 0).all())
    # random coords between 0 and 1, so the max possible distance in sqrt(2)
    self.assertTrue((distance &lt;= 2.0**0.5).all())

    # check if correct distance metric was used
    coords1 = np.array([[0, 0, 0], [1, 0, 0]])
    coords2 = np.array([[1, 0, 0], [2, 0, 0], [3, 0, 0]])
    distance = compute_pairwise_distances(coords1, coords2)
    self.assertTrue((distance == [[1, 2, 3], [0, 1, 2]]).all())

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 86:</b> &nbsp; 4 fragments, nominal size 16 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1070')" href="javascript:;">
deepchem-2.4.0/deepchem/feat/tests/test_rdkit_grid_features.py: 220-240
</a>
<div class="mid" id="frag1070" style="display:none"><pre>
  def test_is_pi_parallel(self):
    ring1_center = np.array([0.0, 0.0, 0.0])
    ring2_center_true = np.array([4.0, 0.0, 0.0])
    ring2_center_false = np.array([10.0, 0.0, 0.0])
    ring1_normal_true = np.array([1.0, 0.0, 0.0])
    ring1_normal_false = np.array([0.0, 1.0, 0.0])

    for ring2_normal in (np.array([2.0, 0, 0]), np.array([-3.0, 0, 0])):
      # parallel normals
      self.assertTrue(
          rgf.is_pi_parallel(ring1_center, ring1_normal_true, ring2_center_true,
                             ring2_normal))
      # perpendicular normals
      self.assertFalse(
          rgf.is_pi_parallel(ring1_center, ring1_normal_false,
                             ring2_center_true, ring2_normal))
      # too far away
      self.assertFalse(
          rgf.is_pi_parallel(ring1_center, ring1_normal_true,
                             ring2_center_false, ring2_normal))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1071')" href="javascript:;">
deepchem-2.4.0/deepchem/feat/tests/test_rdkit_grid_features.py: 241-261
</a>
<div class="mid" id="frag1071" style="display:none"><pre>
  def test_is_pi_t(self):
    ring1_center = np.array([0.0, 0.0, 0.0])
    ring2_center_true = np.array([4.0, 0.0, 0.0])
    ring2_center_false = np.array([10.0, 0.0, 0.0])
    ring1_normal_true = np.array([0.0, 1.0, 0.0])
    ring1_normal_false = np.array([1.0, 0.0, 0.0])

    for ring2_normal in (np.array([2.0, 0, 0]), np.array([-3.0, 0, 0])):
      # perpendicular normals
      self.assertTrue(
          rgf.is_pi_t(ring1_center, ring1_normal_true, ring2_center_true,
                      ring2_normal))
      # parallel normals
      self.assertFalse(
          rgf.is_pi_t(ring1_center, ring1_normal_false, ring2_center_true,
                      ring2_normal))
      # too far away
      self.assertFalse(
          rgf.is_pi_t(ring1_center, ring1_normal_true, ring2_center_false,
                      ring2_normal))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1612')" href="javascript:;">
deepchem-2.4.0/deepchem/utils/test/test_noncovalent_utils.py: 49-69
</a>
<div class="mid" id="frag1612" style="display:none"><pre>
  def test_is_pi_parallel(self):
    ring1_center = np.array([0.0, 0.0, 0.0])
    ring2_center_true = np.array([4.0, 0.0, 0.0])
    ring2_center_false = np.array([10.0, 0.0, 0.0])
    ring1_normal_true = np.array([1.0, 0.0, 0.0])
    ring1_normal_false = np.array([0.0, 1.0, 0.0])

    for ring2_normal in (np.array([2.0, 0, 0]), np.array([-3.0, 0, 0])):
      # parallel normals
      self.assertTrue(
          is_pi_parallel(ring1_center, ring1_normal_true, ring2_center_true,
                         ring2_normal))
      # perpendicular normals
      self.assertFalse(
          is_pi_parallel(ring1_center, ring1_normal_false, ring2_center_true,
                         ring2_normal))
      # too far away
      self.assertFalse(
          is_pi_parallel(ring1_center, ring1_normal_true, ring2_center_false,
                         ring2_normal))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1613')" href="javascript:;">
deepchem-2.4.0/deepchem/utils/test/test_noncovalent_utils.py: 70-90
</a>
<div class="mid" id="frag1613" style="display:none"><pre>
  def test_is_pi_t(self):
    ring1_center = np.array([0.0, 0.0, 0.0])
    ring2_center_true = np.array([4.0, 0.0, 0.0])
    ring2_center_false = np.array([10.0, 0.0, 0.0])
    ring1_normal_true = np.array([0.0, 1.0, 0.0])
    ring1_normal_false = np.array([1.0, 0.0, 0.0])

    for ring2_normal in (np.array([2.0, 0, 0]), np.array([-3.0, 0, 0])):
      # perpendicular normals
      self.assertTrue(
          is_pi_t(ring1_center, ring1_normal_true, ring2_center_true,
                  ring2_normal))
      # parallel normals
      self.assertFalse(
          is_pi_t(ring1_center, ring1_normal_false, ring2_center_true,
                  ring2_normal))
      # too far away
      self.assertFalse(
          is_pi_t(ring1_center, ring1_normal_true, ring2_center_false,
                  ring2_normal))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 87:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1073')" href="javascript:;">
deepchem-2.4.0/deepchem/feat/tests/test_rdkit_grid_features.py: 275-291
</a>
<div class="mid" id="frag1073" style="display:none"><pre>
  def test_is_cation_pi(self):
    cation_position = np.array([[2.0, 0.0, 0.0]])
    ring_center_true = np.array([4.0, 0.0, 0.0])
    ring_center_false = np.array([10.0, 0.0, 0.0])
    ring_normal_true = np.array([1.0, 0.0, 0.0])
    ring_normal_false = np.array([0.0, 1.0, 0.0])

    # parallel normals
    self.assertTrue(
        rgf.is_cation_pi(cation_position, ring_center_true, ring_normal_true))
    # perpendicular normals
    self.assertFalse(
        rgf.is_cation_pi(cation_position, ring_center_true, ring_normal_false))
    # too far away
    self.assertFalse(
        rgf.is_cation_pi(cation_position, ring_center_false, ring_normal_true))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1615')" href="javascript:;">
deepchem-2.4.0/deepchem/utils/test/test_noncovalent_utils.py: 104-125
</a>
<div class="mid" id="frag1615" style="display:none"><pre>
  def test_is_cation_pi(self):
    cation_position = np.array([[2.0, 0.0, 0.0]])
    ring_center_true = np.array([4.0, 0.0, 0.0])
    ring_center_false = np.array([10.0, 0.0, 0.0])
    ring_normal_true = np.array([1.0, 0.0, 0.0])
    ring_normal_false = np.array([0.0, 1.0, 0.0])

    # parallel normals
    self.assertTrue(
        is_cation_pi(cation_position, ring_center_true, ring_normal_true))
    # perpendicular normals
    self.assertFalse(
        is_cation_pi(cation_position, ring_center_true, ring_normal_false))
    # too far away
    self.assertFalse(
        is_cation_pi(cation_position, ring_center_false, ring_normal_true))

  # def test_compute_cation_pi(self):
  #   # TODO(rbharath): find better example, currently dicts are empty
  #   dicts1 = compute_cation_pi(self.prot, self.lig)
  #   dicts2 = compute_cation_pi(self.lig, self.prot)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 88:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1075')" href="javascript:;">
deepchem-2.4.0/deepchem/feat/tests/test_rdkit_grid_features.py: 297-315
</a>
<div class="mid" id="frag1075" style="display:none"><pre>
  def test_compute_binding_pocket_cation_pi(self):
    # TODO find better example, currently dicts are empty
    prot_dict, lig_dict = rgf.compute_binding_pocket_cation_pi(
        self.prot, self.lig)

    exp_prot_dict, exp_lig_dict = rgf.compute_cation_pi(self.prot, self.lig)
    add_lig, add_prot = rgf.compute_cation_pi(self.lig, self.prot)
    for exp_dict, to_add in ((exp_prot_dict, add_prot), (exp_lig_dict,
                                                         add_lig)):
      for atom_idx, count in to_add.items():
        if atom_idx not in exp_dict:
          exp_dict[atom_idx] = count
        else:
          exp_dict[atom_idx] += count

    self.assertEqual(prot_dict, exp_prot_dict)
    self.assertEqual(lig_dict, exp_lig_dict)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1616')" href="javascript:;">
deepchem-2.4.0/deepchem/utils/test/test_noncovalent_utils.py: 126-142
</a>
<div class="mid" id="frag1616" style="display:none"><pre>
  def test_compute_binding_pocket_cation_pi(self):
    # TODO find better example, currently dicts are empty
    prot_dict, lig_dict = compute_binding_pocket_cation_pi(self.prot, self.lig)

    exp_prot_dict, exp_lig_dict = compute_cation_pi(self.prot, self.lig)
    add_lig, add_prot = compute_cation_pi(self.lig, self.prot)
    for exp_dict, to_add in ((exp_prot_dict, add_prot), (exp_lig_dict,
                                                         add_lig)):
      for atom_idx, count in to_add.items():
        if atom_idx not in exp_dict:
          exp_dict[atom_idx] = count
        else:
          exp_dict[atom_idx] += count

    self.assertEqual(prot_dict, exp_prot_dict)
    self.assertEqual(lig_dict, exp_lig_dict)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 89:</b> &nbsp; 3 fragments, nominal size 17 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1089')" href="javascript:;">
deepchem-2.4.0/deepchem/feat/tests/test_graph_features.py: 17-44
</a>
<div class="mid" id="frag1089" style="display:none"><pre>
  def test_carbon_nitrogen(self):
    """Test on carbon nitrogen molecule"""
    # Note there is a central nitrogen of degree 4, with 4 carbons
    # of degree 1 (connected only to central nitrogen).
    raw_smiles = ['C[N+](C)(C)C']
    import rdkit.Chem
    mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
    featurizer = ConvMolFeaturizer()
    mols = featurizer.featurize(mols)
    mol = mols[0]

    # 5 atoms in compound
    assert mol.get_num_atoms() == 5

    # Get the adjacency lists grouped by degree
    deg_adj_lists = mol.get_deg_adjacency_lists()
    assert np.array_equal(deg_adj_lists[0], np.zeros([0, 0], dtype=np.int32))
    # The 4 outer atoms connected to central nitrogen
    assert np.array_equal(deg_adj_lists[1],
                          np.array([[4], [4], [4], [4]], dtype=np.int32))
    assert np.array_equal(deg_adj_lists[2], np.zeros([0, 2], dtype=np.int32))
    assert np.array_equal(deg_adj_lists[3], np.zeros([0, 3], dtype=np.int32))
    # Central nitrogen connected to everything else.
    assert np.array_equal(deg_adj_lists[4],
                          np.array([[0, 1, 2, 3]], dtype=np.int32))
    assert np.array_equal(deg_adj_lists[5], np.zeros([0, 5], dtype=np.int32))
    assert np.array_equal(deg_adj_lists[6], np.zeros([0, 6], dtype=np.int32))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1091')" href="javascript:;">
deepchem-2.4.0/deepchem/feat/tests/test_graph_features.py: 67-91
</a>
<div class="mid" id="frag1091" style="display:none"><pre>
  def test_alkane(self):
    """Test on simple alkane"""
    raw_smiles = ['CCC']
    import rdkit.Chem
    mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
    featurizer = ConvMolFeaturizer()
    mol_list = featurizer.featurize(mols)
    mol = mol_list[0]

    # 3 carbonds in alkane
    assert mol.get_num_atoms() == 3

    deg_adj_lists = mol.get_deg_adjacency_lists()
    assert np.array_equal(deg_adj_lists[0], np.zeros([0, 0], dtype=np.int32))
    # Outer two carbonds are connected to central carbon
    assert np.array_equal(deg_adj_lists[1], np.array(
        [[2], [2]], dtype=np.int32))
    # Central carbon connected to outer two
    assert np.array_equal(deg_adj_lists[2], np.array([[0, 1]], dtype=np.int32))
    assert np.array_equal(deg_adj_lists[3], np.zeros([0, 3], dtype=np.int32))
    assert np.array_equal(deg_adj_lists[4], np.zeros([0, 4], dtype=np.int32))
    assert np.array_equal(deg_adj_lists[5], np.zeros([0, 5], dtype=np.int32))
    assert np.array_equal(deg_adj_lists[6], np.zeros([0, 6], dtype=np.int32))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1090')" href="javascript:;">
deepchem-2.4.0/deepchem/feat/tests/test_graph_features.py: 45-66
</a>
<div class="mid" id="frag1090" style="display:none"><pre>
  def test_single_carbon(self):
    """Test that single carbon atom is featurized properly."""
    raw_smiles = ['C']
    import rdkit
    mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
    featurizer = ConvMolFeaturizer()
    mol_list = featurizer.featurize(mols)
    mol = mol_list[0]

    # Only one carbon
    assert mol.get_num_atoms() == 1

    # No bonds, so degree adjacency lists are empty
    deg_adj_lists = mol.get_deg_adjacency_lists()
    assert np.array_equal(deg_adj_lists[0], np.zeros([1, 0], dtype=np.int32))
    assert np.array_equal(deg_adj_lists[1], np.zeros([0, 1], dtype=np.int32))
    assert np.array_equal(deg_adj_lists[2], np.zeros([0, 2], dtype=np.int32))
    assert np.array_equal(deg_adj_lists[3], np.zeros([0, 3], dtype=np.int32))
    assert np.array_equal(deg_adj_lists[4], np.zeros([0, 4], dtype=np.int32))
    assert np.array_equal(deg_adj_lists[5], np.zeros([0, 5], dtype=np.int32))
    assert np.array_equal(deg_adj_lists[6], np.zeros([0, 6], dtype=np.int32))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 90:</b> &nbsp; 4 fragments, nominal size 11 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1099')" href="javascript:;">
deepchem-2.4.0/deepchem/feat/tests/test_mol_graph_conv_featurizer.py: 8-23
</a>
<div class="mid" id="frag1099" style="display:none"><pre>
  def test_default_featurizer(self):
    smiles = ["C1=CC=CN=C1", "O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"]
    featurizer = MolGraphConvFeaturizer()
    graph_feat = featurizer.featurize(smiles)
    assert len(graph_feat) == 2

    # assert "C1=CC=CN=C1"
    assert graph_feat[0].num_nodes == 6
    assert graph_feat[0].num_node_features == 30
    assert graph_feat[0].num_edges == 12

    # assert "O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"
    assert graph_feat[1].num_nodes == 22
    assert graph_feat[1].num_node_features == 30
    assert graph_feat[1].num_edges == 44

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1102')" href="javascript:;">
deepchem-2.4.0/deepchem/feat/tests/test_mol_graph_conv_featurizer.py: 58-72
</a>
<div class="mid" id="frag1102" style="display:none"><pre>
  def test_featurizer_with_use_partial_charge(self):
    smiles = ["C1=CC=CN=C1", "O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"]
    featurizer = MolGraphConvFeaturizer(use_partial_charge=True)
    graph_feat = featurizer.featurize(smiles)
    assert len(graph_feat) == 2

    # assert "C1=CC=CN=C1"
    assert graph_feat[0].num_nodes == 6
    assert graph_feat[0].num_node_features == 31
    assert graph_feat[0].num_edges == 12

    # assert "O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"
    assert graph_feat[1].num_nodes == 22
    assert graph_feat[1].num_node_features == 31
    assert graph_feat[1].num_edges == 44
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1100')" href="javascript:;">
deepchem-2.4.0/deepchem/feat/tests/test_mol_graph_conv_featurizer.py: 24-41
</a>
<div class="mid" id="frag1100" style="display:none"><pre>
  def test_featurizer_with_use_edge(self):
    smiles = ["C1=CC=CN=C1", "O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"]
    featurizer = MolGraphConvFeaturizer(use_edges=True)
    graph_feat = featurizer.featurize(smiles)
    assert len(graph_feat) == 2

    # assert "C1=CC=CN=C1"
    assert graph_feat[0].num_nodes == 6
    assert graph_feat[0].num_node_features == 30
    assert graph_feat[0].num_edges == 12
    assert graph_feat[0].num_edge_features == 11

    # assert "O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"
    assert graph_feat[1].num_nodes == 22
    assert graph_feat[1].num_node_features == 30
    assert graph_feat[1].num_edges == 44
    assert graph_feat[1].num_edge_features == 11

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1101')" href="javascript:;">
deepchem-2.4.0/deepchem/feat/tests/test_mol_graph_conv_featurizer.py: 42-57
</a>
<div class="mid" id="frag1101" style="display:none"><pre>
  def test_featurizer_with_use_chirality(self):
    smiles = ["C1=CC=CN=C1", "O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"]
    featurizer = MolGraphConvFeaturizer(use_chirality=True)
    graph_feat = featurizer.featurize(smiles)
    assert len(graph_feat) == 2

    # assert "C1=CC=CN=C1"
    assert graph_feat[0].num_nodes == 6
    assert graph_feat[0].num_node_features == 32
    assert graph_feat[0].num_edges == 12

    # assert "O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"
    assert graph_feat[1].num_nodes == 22
    assert graph_feat[1].num_node_features == 32
    assert graph_feat[1].num_edges == 44

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 91:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1105')" href="javascript:;">
deepchem-2.4.0/deepchem/feat/tests/test_contact_fingerprints.py: 23-33
</a>
<div class="mid" id="frag1105" style="display:none"><pre>
  def test_contact_voxels_shape(self):
    box_width = 48
    voxel_width = 2
    voxels_per_edge = box_width / voxel_width
    size = 8
    voxelizer = dc.feat.ContactCircularVoxelizer(
        box_width=box_width, voxel_width=voxel_width, size=size)
    features = voxelizer.featurize(self.complex_files)
    assert features.shape == (1, voxels_per_edge, voxels_per_edge,
                              voxels_per_edge, size)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1136')" href="javascript:;">
deepchem-2.4.0/deepchem/feat/tests/test_splif_fingerprints.py: 23-32
</a>
<div class="mid" id="frag1136" style="display:none"><pre>
  def test_splif_voxels_shape(self):
    box_width = 48
    voxel_width = 2
    voxels_per_edge = int(box_width / voxel_width)
    size = 8
    voxelizer = dc.feat.SplifVoxelizer(
        box_width=box_width, voxel_width=voxel_width, size=size)
    features = voxelizer.featurize(self.complex_files)
    assert features.shape == (1, voxels_per_edge, voxels_per_edge,
                              voxels_per_edge, size * 3)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 92:</b> &nbsp; 4 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1116')" href="javascript:;">
deepchem-2.4.0/deepchem/feat/tests/test_grid_featurizers.py: 5-19
</a>
<div class="mid" id="frag1116" style="display:none"><pre>
def test_charge_voxelizer():
  current_dir = os.path.dirname(os.path.realpath(__file__))
  protein_file = os.path.join(current_dir, 'data',
                              '3ws9_protein_fixer_rdkit.pdb')
  ligand_file = os.path.join(current_dir, 'data', '3ws9_ligand.sdf')

  cutoff = 4.5
  box_width = 20
  voxel_width = 1.0
  voxelizer = dc.feat.ChargeVoxelizer(
      cutoff=cutoff, box_width=box_width, voxel_width=voxel_width)
  features = voxelizer.featurize([(ligand_file, protein_file)])
  assert features.shape == (1, box_width, box_width, box_width, 1)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1119')" href="javascript:;">
deepchem-2.4.0/deepchem/feat/tests/test_grid_featurizers.py: 50-64
</a>
<div class="mid" id="frag1119" style="display:none"><pre>
def test_pi_stack_voxelizer():
  current_dir = os.path.dirname(os.path.realpath(__file__))
  protein_file = os.path.join(current_dir, 'data',
                              '3ws9_protein_fixer_rdkit.pdb')
  ligand_file = os.path.join(current_dir, 'data', '3ws9_ligand.sdf')

  cutoff = 4.5
  box_width = 20
  voxel_width = 1.0
  voxelizer = dc.feat.PiStackVoxelizer(
      cutoff=cutoff, box_width=box_width, voxel_width=voxel_width)
  features = voxelizer.featurize([(ligand_file, protein_file)])
  assert features.shape == (1, box_width, box_width, box_width, 2)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1118')" href="javascript:;">
deepchem-2.4.0/deepchem/feat/tests/test_grid_featurizers.py: 35-49
</a>
<div class="mid" id="frag1118" style="display:none"><pre>
def test_cation_pi_voxelizer():
  current_dir = os.path.dirname(os.path.realpath(__file__))
  protein_file = os.path.join(current_dir, 'data',
                              '3ws9_protein_fixer_rdkit.pdb')
  ligand_file = os.path.join(current_dir, 'data', '3ws9_ligand.sdf')

  cutoff = 4.5
  box_width = 20
  voxel_width = 1.0
  voxelizer = dc.feat.CationPiVoxelizer(
      cutoff=cutoff, box_width=box_width, voxel_width=voxel_width)
  features = voxelizer.featurize([(ligand_file, protein_file)])
  assert features.shape == (1, box_width, box_width, box_width, 1)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1117')" href="javascript:;">
deepchem-2.4.0/deepchem/feat/tests/test_grid_featurizers.py: 20-34
</a>
<div class="mid" id="frag1117" style="display:none"><pre>
def test_salt_bridge_voxelizer():
  current_dir = os.path.dirname(os.path.realpath(__file__))
  protein_file = os.path.join(current_dir, 'data',
                              '3ws9_protein_fixer_rdkit.pdb')
  ligand_file = os.path.join(current_dir, 'data', '3ws9_ligand.sdf')

  cutoff = 4.5
  box_width = 20
  voxel_width = 1.0
  voxelizer = dc.feat.SaltBridgeVoxelizer(
      cutoff=cutoff, box_width=box_width, voxel_width=voxel_width)
  features = voxelizer.featurize([(ligand_file, protein_file)])
  assert features.shape == (1, box_width, box_width, box_width, 1)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 93:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1139')" href="javascript:;">
deepchem-2.4.0/deepchem/feat/tests/test_coulomb_matrices.py: 16-41
</a>
<div class="mid" id="frag1139" style="display:none"><pre>
  def setUp(self):
    """
    Set up tests.
    """
    from rdkit import Chem
    from rdkit.Chem import AllChem
    smiles = 'CC(=O)OC1=CC=CC=C1C(=O)O'
    mol = Chem.MolFromSmiles(smiles)
    self.mol_with_no_conf = mol

    # with one conformer
    mol_with_one_conf = Chem.AddHs(mol)
    AllChem.EmbedMolecule(mol_with_one_conf, AllChem.ETKDG())
    self.mol_with_one_conf = mol_with_one_conf

    # with multiple conformers
    self.num_confs = 4
    engine = conformers.ConformerGenerator(max_conformers=self.num_confs)
    self.mol_with_multi_conf = engine.generate_conformers(mol)

    # include explicit hydrogens
    self.num_atoms = mol_with_one_conf.GetNumAtoms()
    assert self.num_atoms == 21
    assert self.mol_with_one_conf.GetNumConformers() == 1
    assert self.mol_with_multi_conf.GetNumConformers() == self.num_confs

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1146')" href="javascript:;">
deepchem-2.4.0/deepchem/feat/tests/test_coulomb_matrices.py: 130-155
</a>
<div class="mid" id="frag1146" style="display:none"><pre>
  def setUp(self):
    """
    Set up tests.
    """
    from rdkit import Chem
    from rdkit.Chem import AllChem
    smiles = 'CC(=O)OC1=CC=CC=C1C(=O)O'
    mol = Chem.MolFromSmiles(smiles)
    self.mol_with_no_conf = mol

    # with one conformer
    mol_with_one_conf = Chem.AddHs(mol)
    AllChem.EmbedMolecule(mol_with_one_conf, AllChem.ETKDG())
    self.mol_with_one_conf = mol_with_one_conf

    # with multiple conformers
    self.num_confs = 4
    engine = conformers.ConformerGenerator(max_conformers=self.num_confs)
    self.mol_with_multi_conf = engine.generate_conformers(mol)

    # include explicit hydrogens
    self.num_atoms = mol_with_one_conf.GetNumAtoms()
    assert self.num_atoms == 21
    assert self.mol_with_one_conf.GetNumConformers() == 1
    assert self.mol_with_multi_conf.GetNumConformers() == self.num_confs

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 94:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1143')" href="javascript:;">
deepchem-2.4.0/deepchem/feat/tests/test_coulomb_matrices.py: 80-93
</a>
<div class="mid" id="frag1143" style="display:none"><pre>
  def test_upper_tri_coulomb_matrix_padding(self):
    """
        Test upper triangular CoulombMatrix with padding.
        """
    max_atoms = self.num_atoms * 2
    f = CoulombMatrix(max_atoms=max_atoms, upper_tri=True)
    size = np.triu_indices(max_atoms)[0].size
    rval = f([self.mol_with_no_conf])
    assert rval.shape == (1, size)
    rval = f([self.mol_with_one_conf])
    assert rval.shape == (1, size)
    rval = f([self.mol_with_multi_conf])
    assert rval.shape == (1, self.num_confs, size)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1145')" href="javascript:;">
deepchem-2.4.0/deepchem/feat/tests/test_coulomb_matrices.py: 110-124
</a>
<div class="mid" id="frag1145" style="display:none"><pre>
  def test_coulomb_matrix_hydrogens(self):
    """
    Test no hydrogen removal.
    """
    f = CoulombMatrix(
        max_atoms=self.num_atoms, remove_hydrogens=False, upper_tri=True)
    size = np.triu_indices(self.num_atoms)[0].size
    rval = f([self.mol_with_no_conf])
    assert rval.shape == (1, size)
    rval = f([self.mol_with_one_conf])
    assert rval.shape == (1, size)
    rval = f([self.mol_with_multi_conf])
    assert rval.shape == (1, self.num_confs, size)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 95:</b> &nbsp; 4 fragments, nominal size 31 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1163')" href="javascript:;">
deepchem-2.4.0/deepchem/feat/complex_featurizers/grid_featurizers.py: 84-127
</a>
<div class="mid" id="frag1163" style="display:none"><pre>
  def _featurize(self, complex: Tuple[str, str]) -&gt; np.ndarray:
    """
    Compute featurization for a single mol/protein complex

    Parameters
    ----------
    complex: Tuple[str, str]
      Filenames for molecule and protein.
    """
    try:
      fragments = rdkit_utils.load_complex(complex, add_hydrogens=False)

    except MoleculeLoadException:
      logger.warning("This molecule cannot be loaded by Rdkit. Returning None")
      return None
    pairwise_features = []
    # We compute pairwise contact fingerprints
    centroid = compute_contact_centroid(fragments, cutoff=self.cutoff)
    if self.reduce_to_contacts:
      fragments = reduce_molecular_complex_to_contacts(fragments, self.cutoff)
    # We compute pairwise contact fingerprints
    for (frag1_ind, frag2_ind) in itertools.combinations(
        range(len(fragments)), 2):
      frag1, frag2 = fragments[frag1_ind], fragments[frag2_ind]
      frag1_xyz = subtract_centroid(frag1[0], centroid)
      frag2_xyz = subtract_centroid(frag2[0], centroid)
      xyzs = [frag1_xyz, frag2_xyz]
      rdks = [frag1[1], frag2[1]]
      pairwise_features.append(
          sum([
              voxelize(
                  convert_atom_to_voxel,
                  hash_function=None,
                  coordinates=xyz,
                  box_width=self.box_width,
                  voxel_width=self.voxel_width,
                  feature_dict=compute_charge_dictionary(mol),
                  nb_channel=1,
                  dtype="np.float16") for xyz, mol in zip(xyzs, rdks)
          ]))
    # Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis.
    return np.concatenate(pairwise_features, axis=-1)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1165')" href="javascript:;">
deepchem-2.4.0/deepchem/feat/complex_featurizers/grid_featurizers.py: 170-213
</a>
<div class="mid" id="frag1165" style="display:none"><pre>
  def _featurize(self, complex: Tuple[str, str]) -&gt; np.ndarray:
    """
    Compute featurization for a single mol/protein complex

    Parameters
    ----------
    complex: Tuple[str, str]
      Filenames for molecule and protein.
    """
    try:
      fragments = rdkit_utils.load_complex(complex, add_hydrogens=False)

    except MoleculeLoadException:
      logger.warning("This molecule cannot be loaded by Rdkit. Returning None")
      return None
    pairwise_features = []
    # We compute pairwise contact fingerprints
    centroid = compute_contact_centroid(fragments, cutoff=self.cutoff)
    if self.reduce_to_contacts:
      fragments = reduce_molecular_complex_to_contacts(fragments, self.cutoff)
    for (frag1_ind, frag2_ind) in itertools.combinations(
        range(len(fragments)), 2):
      frag1, frag2 = fragments[frag1_ind], fragments[frag2_ind]
      distances = compute_pairwise_distances(frag1[0], frag2[0])
      frag1_xyz = subtract_centroid(frag1[0], centroid)
      frag2_xyz = subtract_centroid(frag2[0], centroid)
      xyzs = [frag1_xyz, frag2_xyz]
      # rdks = [frag1[1], frag2[1]]
      pairwise_features.append(
          sum([
              voxelize(
                  convert_atom_pair_to_voxel,
                  hash_function=None,
                  coordinates=xyz,
                  box_width=self.box_width,
                  voxel_width=self.voxel_width,
                  feature_list=compute_salt_bridges(
                      frag1[1], frag2[1], distances, cutoff=self.cutoff),
                  nb_channel=1) for xyz in xyzs
          ]))
    # Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis.
    return np.concatenate(pairwise_features, axis=-1)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1167')" href="javascript:;">
deepchem-2.4.0/deepchem/feat/complex_featurizers/grid_featurizers.py: 255-302
</a>
<div class="mid" id="frag1167" style="display:none"><pre>
  def _featurize(self, complex: Tuple[str, str]) -&gt; np.ndarray:
    """
    Compute featurization for a single mol/protein complex

    Parameters
    ----------
    complex: Tuple[str, str]
      Filenames for molecule and protein.
    """
    try:
      fragments = rdkit_utils.load_complex(complex, add_hydrogens=False)

    except MoleculeLoadException:
      logger.warning("This molecule cannot be loaded by Rdkit. Returning None")
      return None
    pairwise_features = []
    # We compute pairwise contact fingerprints
    centroid = compute_contact_centroid(fragments, cutoff=self.cutoff)
    for (frag1_ind, frag2_ind) in itertools.combinations(
        range(len(fragments)), 2):
      frag1, frag2 = fragments[frag1_ind], fragments[frag2_ind]
      # distances = compute_pairwise_distances(frag1[0], frag2[0])
      frag1_xyz = subtract_centroid(frag1[0], centroid)
      frag2_xyz = subtract_centroid(frag2[0], centroid)
      xyzs = [frag1_xyz, frag2_xyz]
      # rdks = [frag1[1], frag2[1]]
      pairwise_features.append(
          sum([
              voxelize(
                  convert_atom_to_voxel,
                  hash_function=None,
                  box_width=self.box_width,
                  voxel_width=self.voxel_width,
                  coordinates=xyz,
                  feature_dict=cation_pi_dict,
                  nb_channel=1) for xyz, cation_pi_dict in zip(
                      xyzs,
                      compute_binding_pocket_cation_pi(
                          frag1[1],
                          frag2[1],
                          dist_cutoff=self.cutoff,
                          angle_cutoff=self.angle_cutoff,
                      ))
          ]))
    # Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis.
    return np.concatenate(pairwise_features, axis=-1)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1173')" href="javascript:;">
deepchem-2.4.0/deepchem/feat/complex_featurizers/grid_featurizers.py: 563-609
</a>
<div class="mid" id="frag1173" style="display:none"><pre>
  def _featurize(self, complex: Tuple[str, str]) -&gt; np.ndarray:
    """
    Compute featurization for a single mol/protein complex

    Parameters
    ----------
    complex: Tuple[str, str]
      Filenames for molecule and protein.
    """
    try:
      fragments = rdkit_utils.load_complex(complex, add_hydrogens=False)

    except MoleculeLoadException:
      logger.warning("This molecule cannot be loaded by Rdkit. Returning None")
      return None
    pairwise_features = []
    # We compute pairwise contact fingerprints
    centroid = compute_contact_centroid(fragments, cutoff=self.cutoff)
    if self.reduce_to_contacts:
      fragments = reduce_molecular_complex_to_contacts(fragments, self.cutoff)
    for (frag1_ind, frag2_ind) in itertools.combinations(
        range(len(fragments)), 2):
      frag1, frag2 = fragments[frag1_ind], fragments[frag2_ind]
      distances = compute_pairwise_distances(frag1[0], frag2[0])
      frag1_xyz = subtract_centroid(frag1[0], centroid)
      frag2_xyz = subtract_centroid(frag2[0], centroid)
      xyzs = [frag1_xyz, frag2_xyz]
      # rdks = [frag1[1], frag2[1]]
      pairwise_features.append(
          np.concatenate(
              [
                  sum([
                      voxelize(
                          convert_atom_pair_to_voxel,
                          hash_function=None,
                          box_width=self.box_width,
                          voxel_width=self.voxel_width,
                          coordinates=xyz,
                          feature_list=hbond_list,
                          nb_channel=1) for xyz in xyzs
                  ]) for hbond_list in compute_hydrogen_bonds(
                      frag1, frag2, distances, self.distance_bins,
                      self.angle_cutoffs)
              ],
              axis=-1))
    # Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis.
    return np.concatenate(pairwise_features, axis=-1)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 96:</b> &nbsp; 2 fragments, nominal size 19 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1170')" href="javascript:;">
deepchem-2.4.0/deepchem/feat/complex_featurizers/grid_featurizers.py: 422-457
</a>
<div class="mid" id="frag1170" style="display:none"><pre>
  def __init__(
      self,
      cutoff: float = 4.5,
      reduce_to_contacts: bool = True,
      distance_bins: Optional[List[Tuple[float, float]]] = None,
      angle_cutoffs: Optional[List[float]] = None,
  ):
    """
    Parameters
    ----------
    cutoff: float (default 4.5)
      Distance cutoff in angstroms for molecules in complex.
    reduce_to_contacts: bool, optional
      If True, reduce the atoms in the complex to those near a contact
      region.
    distance_bins: list[tuple]
      List of hydgrogen bond distance bins. If not specified is
      set to default
      `[(2.2, 2.5), (2.5, 3.2), (3.2, 4.0)]`.
    angle_cutoffs: list[float]
      List of hydrogen bond angle cutoffs. Max allowed
      deviation from the ideal (180 deg) angle between
      hydrogen-atom1, hydrogen-atom2 vectors.If not specified
      is set to default `[5, 50, 90]`
    """
    self.cutoff = cutoff
    if distance_bins is None:
      self.distance_bins = HBOND_DIST_BINS
    else:
      self.distance_bins = distance_bins
    if angle_cutoffs is None:
      self.angle_cutoffs = HBOND_ANGLE_CUTOFFS
    else:
      self.angle_cutoffs = angle_cutoffs
    self.reduce_to_contacts = reduce_to_contacts

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1172')" href="javascript:;">
deepchem-2.4.0/deepchem/feat/complex_featurizers/grid_featurizers.py: 518-562
</a>
<div class="mid" id="frag1172" style="display:none"><pre>
  def __init__(
      self,
      cutoff: float = 4.5,
      box_width: float = 16.0,
      voxel_width: float = 1.0,
      reduce_to_contacts: bool = True,
      distance_bins: Optional[List[Tuple[float, float]]] = None,
      angle_cutoffs: Optional[List[float]] = None,
  ):
    """
    Parameters
    ----------
    cutoff: float (default 4.5)
      Distance cutoff in angstroms for contact atoms in complex.
    box_width: float, optional (default 16.0)
      Size of a box in which voxel features are calculated. Box
      is centered on a ligand centroid.
    voxel_width: float, optional (default 1.0)
      Size of a 3D voxel in a grid.
    reduce_to_contacts: bool, optional
      If True, reduce the atoms in the complex to those near a contact
      region.
    distance_bins: list[tuple]
      List of hydgrogen bond distance bins. If not specified is
      set to default
      `[(2.2, 2.5), (2.5, 3.2), (3.2, 4.0)]`.
    angle_cutoffs: list[float]
      List of hydrogen bond angle cutoffs. Max allowed
      deviation from the ideal (180 deg) angle between
      hydrogen-atom1, hydrogen-atom2 vectors.If not specified
      is set to default `[5, 50, 90]`
    """
    self.cutoff = cutoff
    if distance_bins is None:
      self.distance_bins = HBOND_DIST_BINS
    else:
      self.distance_bins = distance_bins
    if angle_cutoffs is None:
      self.angle_cutoffs = HBOND_ANGLE_CUTOFFS
    else:
      self.angle_cutoffs = angle_cutoffs
    self.box_width = box_width
    self.voxel_width = voxel_width
    self.reduce_to_contacts = reduce_to_contacts

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 97:</b> &nbsp; 4 fragments, nominal size 18 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1188')" href="javascript:;">
deepchem-2.4.0/deepchem/feat/base_classes.py: 26-58
</a>
<div class="mid" id="frag1188" style="display:none"><pre>
  def featurize(self, datapoints: Iterable[Any],
                log_every_n: int = 1000) -&gt; np.ndarray:
    """Calculate features for datapoints.

    Parameters
    ----------
    datapoints: Iterable[Any]
      A sequence of objects that you'd like to featurize. Subclassses of
      `Featurizer` should instantiate the `_featurize` method that featurizes
      objects in the sequence.
    log_every_n: int, default 1000
      Logs featurization progress every `log_every_n` steps.

    Returns
    -------
    np.ndarray
      A numpy array containing a featurized representation of `datapoints`.
    """
    datapoints = list(datapoints)
    features = []
    for i, point in enumerate(datapoints):
      if i % log_every_n == 0:
        logger.info("Featurizing datapoint %i" % i)
      try:
        features.append(self._featurize(point))
      except:
        logger.warning(
            "Failed to featurize datapoint %d. Appending empty array")
        features.append(np.array([]))

    features = np.asarray(features)
    return features

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1197')" href="javascript:;">
deepchem-2.4.0/deepchem/feat/base_classes.py: 369-407
</a>
<div class="mid" id="frag1197" style="display:none"><pre>
  def featurize(self, compositions: Iterable[str],
                log_every_n: int = 1000) -&gt; np.ndarray:
    """Calculate features for crystal compositions.

    Parameters
    ----------
    compositions: Iterable[str]
      Iterable sequence of composition strings, e.g. "MoS2".
    log_every_n: int, default 1000
      Logging messages reported every `log_every_n` samples.

    Returns
    -------
    features: np.ndarray
      A numpy array containing a featurized representation of
      `compositions`.
    """
    try:
      from pymatgen import Composition
    except ModuleNotFoundError:
      raise ImportError("This class requires pymatgen to be installed.")

    compositions = list(compositions)
    features = []
    for idx, composition in enumerate(compositions):
      if idx % log_every_n == 0:
        logger.info("Featurizing datapoint %i" % idx)
      try:
        c = Composition(composition)
        features.append(self._featurize(c))
      except:
        logger.warning(
            "Failed to featurize datapoint %i. Appending empty array" % idx)
        features.append(np.array([]))

    features = np.asarray(features)
    return features


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1193')" href="javascript:;">
deepchem-2.4.0/deepchem/feat/base_classes.py: 157-191
</a>
<div class="mid" id="frag1193" style="display:none"><pre>
  def featurize(self,
                complexes: Iterable[Tuple[str, str]],
                log_every_n: int = 100) -&gt; np.ndarray:
    """
    Calculate features for mol/protein complexes.

    Parameters
    ----------
    complexes: Iterable[Tuple[str, str]]
      List of filenames (PDB, SDF, etc.) for ligand molecules and proteins.
      Each element should be a tuple of the form (ligand_filename,
      protein_filename).

    Returns
    -------
    features: np.ndarray
      Array of features
    """

    if not isinstance(complexes, Iterable):
      complexes = [cast(Tuple[str, str], complexes)]
    features = []
    for i, point in enumerate(complexes):
      if i % log_every_n == 0:
        logger.info("Featurizing datapoint %i" % i)
      try:
        features.append(self._featurize(point))
      except:
        logger.warning(
            "Failed to featurize datapoint %i. Appending empty array." % i)
        features.append(np.array([]))

    features = np.asarray(features)
    return features

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1196')" href="javascript:;">
deepchem-2.4.0/deepchem/feat/base_classes.py: 303-345
</a>
<div class="mid" id="frag1196" style="display:none"><pre>
  def featurize(self,
                structures: Iterable[Union[Dict[str, Any], PymatgenStructure]],
                log_every_n: int = 1000) -&gt; np.ndarray:
    """Calculate features for crystal structures.

    Parameters
    ----------
    structures: Iterable[Union[Dict, pymatgen.Structure]]
      Iterable sequence of pymatgen structure dictionaries
      or pymatgen.Structure. Please confirm the dictionary representations
      of pymatgen.Structure from https://pymatgen.org/pymatgen.core.structure.html.
    log_every_n: int, default 1000
      Logging messages reported every `log_every_n` samples.

    Returns
    -------
    features: np.ndarray
      A numpy array containing a featurized representation of
      `structures`.
    """
    try:
      from pymatgen import Structure
    except ModuleNotFoundError:
      raise ImportError("This class requires pymatgen to be installed.")

    structures = list(structures)
    features = []
    for idx, structure in enumerate(structures):
      if idx % log_every_n == 0:
        logger.info("Featurizing datapoint %i" % idx)
      try:
        if isinstance(structure, Dict):
          structure = Structure.from_dict(structure)
        features.append(self._featurize(structure))
      except:
        logger.warning(
            "Failed to featurize datapoint %i. Appending empty array" % idx)
        features.append(np.array([]))

    features = np.asarray(features)
    return features


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 98:</b> &nbsp; 2 fragments, nominal size 43 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1233')" href="javascript:;">
deepchem-2.4.0/deepchem/rl/a2c.py: 130-197
</a>
<div class="mid" id="frag1233" style="display:none"><pre>
  def __init__(self,
               env,
               policy,
               max_rollout_length=20,
               discount_factor=0.99,
               advantage_lambda=0.98,
               value_weight=1.0,
               entropy_weight=0.01,
               optimizer=None,
               model_dir=None,
               use_hindsight=False):
    """Create an object for optimizing a policy.

    Parameters
    ----------
    env: Environment
      the Environment to interact with
    policy: Policy
      the Policy to optimize.  It must have outputs with the names 'action_prob'
      and 'value' (for discrete action spaces) or 'action_mean', 'action_std',
      and 'value' (for continuous action spaces)
    max_rollout_length: int
      the maximum length of rollouts to generate
    discount_factor: float
      the discount factor to use when computing rewards
    advantage_lambda: float
      the parameter for trading bias vs. variance in Generalized Advantage Estimation
    value_weight: float
      a scale factor for the value loss term in the loss function
    entropy_weight: float
      a scale factor for the entropy term in the loss function
    optimizer: Optimizer
      the optimizer to use.  If None, a default optimizer is used.
    model_dir: str
      the directory in which the model will be saved.  If None, a temporary directory will be created.
    use_hindsight: bool
      if True, use Hindsight Experience Replay
    """
    self._env = env
    self._policy = policy
    self.max_rollout_length = max_rollout_length
    self.discount_factor = discount_factor
    self.advantage_lambda = advantage_lambda
    self.value_weight = value_weight
    self.entropy_weight = entropy_weight
    self.use_hindsight = use_hindsight
    self._state_is_list = isinstance(env.state_shape[0], SequenceCollection)
    if optimizer is None:
      self._optimizer = Adam(learning_rate=0.001, beta1=0.9, beta2=0.999)
    else:
      self._optimizer = optimizer
    output_names = policy.output_names
    self.continuous = ('action_mean' in output_names)
    self._value_index = output_names.index('value')
    if self.continuous:
      self._action_mean_index = output_names.index('action_mean')
      self._action_std_index = output_names.index('action_std')
    else:
      self._action_prob_index = output_names.index('action_prob')
    self._rnn_final_state_indices = [
        i for i, n in enumerate(output_names) if n == 'rnn_state'
    ]
    self._rnn_states = policy.rnn_initial_states
    self._model = self._build_model(model_dir)
    self._checkpoint = tf.train.Checkpoint()
    self._checkpoint.save_counter  # Ensure the variable has been created
    self._checkpoint.listed = self._model.model.trainable_variables

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1321')" href="javascript:;">
deepchem-2.4.0/deepchem/rl/ppo.py: 92-175
</a>
<div class="mid" id="frag1321" style="display:none"><pre>
  def __init__(self,
               env,
               policy,
               max_rollout_length=20,
               optimization_rollouts=8,
               optimization_epochs=4,
               batch_size=64,
               clipping_width=0.2,
               discount_factor=0.99,
               advantage_lambda=0.98,
               value_weight=1.0,
               entropy_weight=0.01,
               optimizer=None,
               model_dir=None,
               use_hindsight=False):
    """Create an object for optimizing a policy.

    Parameters
    ----------
    env: Environment
      the Environment to interact with
    policy: Policy
      the Policy to optimize.  It must have outputs with the names 'action_prob'
      and 'value', corresponding to the action probabilities and value estimate
    max_rollout_length: int
      the maximum length of rollouts to generate
    optimization_rollouts: int
      the number of rollouts to generate for each iteration of optimization
    optimization_epochs: int
      the number of epochs of optimization to perform within each iteration
    batch_size: int
      the batch size to use during optimization.  If this is 0, each rollout will be used as a
      separate batch.
    clipping_width: float
      in computing the PPO loss function, the probability ratio is clipped to the range
      (1-clipping_width, 1+clipping_width)
    discount_factor: float
      the discount factor to use when computing rewards
    advantage_lambda: float
      the parameter for trading bias vs. variance in Generalized Advantage Estimation
    value_weight: float
      a scale factor for the value loss term in the loss function
    entropy_weight: float
      a scale factor for the entropy term in the loss function
    optimizer: Optimizer
      the optimizer to use.  If None, a default optimizer is used.
    model_dir: str
      the directory in which the model will be saved.  If None, a temporary directory will be created.
    use_hindsight: bool
      if True, use Hindsight Experience Replay
    """
    self._env = env
    self._policy = policy
    self.max_rollout_length = max_rollout_length
    self.optimization_rollouts = optimization_rollouts
    self.optimization_epochs = optimization_epochs
    self.batch_size = batch_size
    self.clipping_width = clipping_width
    self.discount_factor = discount_factor
    self.advantage_lambda = advantage_lambda
    self.value_weight = value_weight
    self.entropy_weight = entropy_weight
    self.use_hindsight = use_hindsight
    self._state_is_list = isinstance(env.state_shape[0], SequenceCollection)
    if optimizer is None:
      self._optimizer = Adam(learning_rate=0.001, beta1=0.9, beta2=0.999)
    else:
      self._optimizer = optimizer
    output_names = policy.output_names
    self._value_index = output_names.index('value')
    self._action_prob_index = output_names.index('action_prob')
    self._rnn_final_state_indices = [
        i for i, n in enumerate(output_names) if n == 'rnn_state'
    ]
    self._rnn_states = policy.rnn_initial_states
    if len(self._rnn_states) &gt; 0 and batch_size != 0:
      raise ValueError(
          'Cannot batch rollouts when the policy contains a recurrent layer.  Set batch_size to 0.'
      )
    self._model = self._build_model(model_dir)
    self._checkpoint = tf.train.Checkpoint()
    self._checkpoint.save_counter  # Ensure the variable has been created
    self._checkpoint.listed = self._model.model.trainable_variables

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 99:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1239')" href="javascript:;">
deepchem-2.4.0/deepchem/rl/a2c.py: 345-361
</a>
<div class="mid" id="frag1239" style="display:none"><pre>
  def _predict_outputs(self, state, use_saved_states, save_states):
    """Compute a set of outputs for a state. """
    if not self._state_is_list:
      state = [state]
    if use_saved_states:
      state = state + list(self._rnn_states)
    else:
      state = state + list(self._policy.rnn_initial_states)
    inputs = [np.expand_dims(s, axis=0) for s in state]
    results = self._compute_model(inputs)
    results = [r.numpy() for r in results]
    if save_states:
      self._rnn_states = [
          np.squeeze(results[i], 0) for i in self._rnn_final_state_indices
      ]
    return results

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1329')" href="javascript:;">
deepchem-2.4.0/deepchem/rl/ppo.py: 367-383
</a>
<div class="mid" id="frag1329" style="display:none"><pre>
  def _predict_outputs(self, state, use_saved_states, save_states):
    """Compute a set of outputs for a state. """
    if not self._state_is_list:
      state = [state]
    if use_saved_states:
      state = state + list(self._rnn_states)
    else:
      state = state + list(self._policy.rnn_initial_states)
    inputs = [np.expand_dims(s, axis=0) for s in state]
    results = self._compute_model(inputs)
    results = [r.numpy() for r in results]
    if save_states:
      self._rnn_states = [
          np.squeeze(results[i], 0) for i in self._rnn_final_state_indices
      ]
    return results

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 100:</b> &nbsp; 3 fragments, nominal size 11 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1251')" href="javascript:;">
deepchem-2.4.0/deepchem/rl/tests/test_rl_reload.py: 39-55
</a>
<div class="mid" id="frag1251" style="display:none"><pre>
  def create_model(self, **kwargs):
    env = self.env

    class TestModel(tf.keras.Model):

      def __init__(self):
        super(TestModel, self).__init__(**kwargs)
        self.action = tf.Variable(np.ones(env.n_actions, np.float32))
        self.value = tf.Variable([0.0], tf.float32)

      def call(self, inputs, **kwargs):
        prob = tf.nn.softmax(tf.reshape(self.action, (-1, env.n_actions)))
        return (prob, self.value)

    return TestModel()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1282')" href="javascript:;">
deepchem-2.4.0/deepchem/rl/tests/test_a2c.py: 55-71
</a>
<div class="mid" id="frag1282" style="display:none"><pre>
      def create_model(self, **kwargs):

        class TestModel(tf.keras.Model):

          def __init__(self):
            super(TestModel, self).__init__(**kwargs)
            self.action = tf.Variable(np.ones(env.n_actions, np.float32))
            self.value = tf.Variable([0.0], tf.float32)

          def call(self, inputs, **kwargs):
            prob = tf.nn.softmax(tf.reshape(self.action, (-1, env.n_actions)))
            return (prob, self.value)

        return TestModel()

    # Optimize it.

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1261')" href="javascript:;">
deepchem-2.4.0/deepchem/rl/tests/test_ppo.py: 55-71
</a>
<div class="mid" id="frag1261" style="display:none"><pre>
      def create_model(self, **kwargs):

        class TestModel(tf.keras.Model):

          def __init__(self):
            super(TestModel, self).__init__(**kwargs)
            self.action = tf.Variable(np.ones(env.n_actions, np.float32))
            self.value = tf.Variable([0.0], tf.float32)

          def call(self, inputs, **kwargs):
            prob = tf.nn.softmax(tf.reshape(self.action, (-1, env.n_actions)))
            return (prob, self.value)

        return TestModel()

    # Optimize it.

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 101:</b> &nbsp; 2 fragments, nominal size 42 lines, similarity 97%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1264')" href="javascript:;">
deepchem-2.4.0/deepchem/rl/tests/test_ppo.py: 103-166
</a>
<div class="mid" id="frag1264" style="display:none"><pre>
  def test_recurrent_states(self):
    """Test a policy that involves recurrent layers."""

    # The environment just has a constant state.

    class TestEnvironment(dc.rl.Environment):

      def __init__(self):
        super(TestEnvironment, self).__init__((10,), 10)
        self._state = np.random.random(10).astype(np.float32)

      def step(self, action):
        self._state = np.random.random(10).astype(np.float32)
        return 0.0

      def reset(self):
        pass

    # The policy includes a single recurrent layer.

    class TestPolicy(dc.rl.Policy):

      def __init__(self):
        super(TestPolicy, self).__init__(['action_prob', 'value', 'rnn_state'],
                                         [np.zeros(10)])

      def create_model(self, **kwargs):
        state = Input(shape=(10,))
        rnn_state = Input(shape=(10,))
        reshaped = Reshape((1, 10))(state)
        gru, rnn_final_state = GRU(
            10, return_state=True, return_sequences=True, time_major=True)(
                reshaped, initial_state=rnn_state)
        output = Softmax()(Reshape((10,))(gru))
        value = dc.models.layers.Variable([0.0])([state])
        return tf.keras.Model(
            inputs=[state, rnn_state], outputs=[output, value, rnn_final_state])

    # We don't care about actually optimizing it, so just run a few rollouts to make
    # sure fit() doesn't crash, then check the behavior of the GRU state.

    env = TestEnvironment()
    ppo = dc.rl.PPO(env, TestPolicy(), batch_size=0)
    ppo.fit(100)
    # On the first call, the initial state should be all zeros.
    prob1, value1 = ppo.predict(
        env.state, use_saved_states=True, save_states=False)
    # It should still be zeros since we didn't save it last time.
    prob2, value2 = ppo.predict(
        env.state, use_saved_states=True, save_states=True)
    # It should be different now.
    prob3, value3 = ppo.predict(
        env.state, use_saved_states=True, save_states=False)
    # This should be the same as the previous one.
    prob4, value4 = ppo.predict(
        env.state, use_saved_states=True, save_states=False)
    # Now we reset it, so we should get the same result as initially.
    prob5, value5 = ppo.predict(
        env.state, use_saved_states=False, save_states=True)
    assert np.array_equal(prob1, prob2)
    assert np.array_equal(prob1, prob5)
    assert np.array_equal(prob3, prob4)
    assert not np.array_equal(prob2, prob3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1285')" href="javascript:;">
deepchem-2.4.0/deepchem/rl/tests/test_a2c.py: 102-165
</a>
<div class="mid" id="frag1285" style="display:none"><pre>
  def test_recurrent_states(self):
    """Test a policy that involves recurrent layers."""

    # The environment just has a constant state.

    class TestEnvironment(dc.rl.Environment):

      def __init__(self):
        super(TestEnvironment, self).__init__((10,), 10)
        self._state = np.random.random(10).astype(np.float32)

      def step(self, action):
        self._state = np.random.random(10).astype(np.float32)
        return 0.0

      def reset(self):
        pass

    # The policy includes a single recurrent layer.

    class TestPolicy(dc.rl.Policy):

      def __init__(self):
        super(TestPolicy, self).__init__(['action_prob', 'value', 'rnn_state'],
                                         [np.zeros(10)])

      def create_model(self, **kwargs):
        state = Input(shape=(10,))
        rnn_state = Input(shape=(10,))
        reshaped = Reshape((1, 10))(state)
        gru, rnn_final_state = GRU(
            10, return_state=True, return_sequences=True, time_major=True)(
                reshaped, initial_state=rnn_state)
        output = Softmax()(Reshape((10,))(gru))
        value = dc.models.layers.Variable([0.0])([state])
        return tf.keras.Model(
            inputs=[state, rnn_state], outputs=[output, value, rnn_final_state])

    # We don't care about actually optimizing it, so just run a few rollouts to make
    # sure fit() doesn't crash, then check the behavior of the GRU state.

    env = TestEnvironment()
    a2c = dc.rl.A2C(env, TestPolicy())
    a2c.fit(100)
    # On the first call, the initial state should be all zeros.
    prob1, value1 = a2c.predict(
        env.state, use_saved_states=True, save_states=False)
    # It should still be zeros since we didn't save it last time.
    prob2, value2 = a2c.predict(
        env.state, use_saved_states=True, save_states=True)
    # It should be different now.
    prob3, value3 = a2c.predict(
        env.state, use_saved_states=True, save_states=False)
    # This should be the same as the previous one.
    prob4, value4 = a2c.predict(
        env.state, use_saved_states=True, save_states=False)
    # Now we reset it, so we should get the same result as initially.
    prob5, value5 = a2c.predict(
        env.state, use_saved_states=False, save_states=True)
    assert np.array_equal(prob1, prob2)
    assert np.array_equal(prob1, prob5)
    assert np.array_equal(prob3, prob4)
    assert not np.array_equal(prob2, prob3)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 102:</b> &nbsp; 2 fragments, nominal size 62 lines, similarity 96%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1270')" href="javascript:;">
deepchem-2.4.0/deepchem/rl/tests/test_ppo.py: 168-252
</a>
<div class="mid" id="frag1270" style="display:none"><pre>
  def test_hindsight(self):
    """Test Hindsight Experience Replay."""

    # The environment is a plane in which the agent moves by steps until it reaches a randomly
    # positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
    # to learn by standard methods, since it may take a very long time to receive any feedback
    # at all.  Using hindsight makes it much easier.

    class TestEnvironment(dc.rl.Environment):

      def __init__(self):
        super(TestEnvironment, self).__init__((4,), 4)
        self.moves = [(-1, 0), (1, 0), (0, -1), (0, 1)]

      def reset(self):
        self._state = np.concatenate([[0, 0], np.random.randint(-50, 50, 2)])
        self._terminated = False
        self.count = 0

      def step(self, action):
        new_state = self._state.copy()
        new_state[:2] += self.moves[action]
        self._state = new_state
        self.count += 1
        reward = 0
        if np.array_equal(new_state[:2], new_state[2:]):
          self._terminated = True
          reward = 1
        elif self.count == 1000:
          self._terminated = True
        return reward

      def apply_hindsight(self, states, actions, goal):
        new_states = []
        rewards = []
        goal_pos = goal[:2]
        for state, action in zip(states, actions):
          new_state = state.copy()
          new_state[2:] = goal_pos
          new_states.append(new_state)
          pos_after_action = new_state[:2] + self.moves[action]
          if np.array_equal(pos_after_action, goal_pos):
            rewards.append(1)
            break
          else:
            rewards.append(0)
        return new_states, rewards

    # A simple policy with two hidden layers.

    class TestPolicy(dc.rl.Policy):

      def __init__(self):
        super(TestPolicy, self).__init__(['action_prob', 'value'])

      def create_model(self, **kwargs):
        state = Input(shape=(4,))
        dense1 = Dense(8, activation=tf.nn.relu)(state)
        dense2 = Dense(8, activation=tf.nn.relu)(dense1)
        output = Dense(4, activation=tf.nn.softmax, use_bias=False)(dense2)
        value = Dense(1)(dense2)
        return tf.keras.Model(inputs=state, outputs=[output, value])

    # Optimize it.

    env = TestEnvironment()
    ppo = dc.rl.PPO(
        env,
        TestPolicy(),
        use_hindsight=True,
        optimization_epochs=1,
        batch_size=0,
        optimizer=Adam(learning_rate=0.001))
    ppo.fit(1500000)

    # Try running it a few times and see if it succeeds.

    pass_count = 0
    for i in range(5):
      env.reset()
      while not env.terminated:
        env.step(ppo.select_action(env.state))
      if np.array_equal(env.state[:2], env.state[2:]):
        pass_count += 1
    assert pass_count &gt;= 3
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1291')" href="javascript:;">
deepchem-2.4.0/deepchem/rl/tests/test_a2c.py: 167-250
</a>
<div class="mid" id="frag1291" style="display:none"><pre>
  def test_hindsight(self):
    """Test Hindsight Experience Replay."""

    # The environment is a plane in which the agent moves by steps until it reaches a randomly
    # positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
    # to learn by standard methods, since it may take a very long time to receive any feedback
    # at all.  Using hindsight makes it much easier.

    class TestEnvironment(dc.rl.Environment):

      def __init__(self):
        super(TestEnvironment, self).__init__((4,), 4)
        self.moves = [(-1, 0), (1, 0), (0, -1), (0, 1)]

      def reset(self):
        self._state = np.concatenate([[0, 0], np.random.randint(-50, 50, 2)])
        self._terminated = False
        self.count = 0

      def step(self, action):
        new_state = self._state.copy()
        new_state[:2] += self.moves[action]
        self._state = new_state
        self.count += 1
        reward = 0
        if np.array_equal(new_state[:2], new_state[2:]):
          self._terminated = True
          reward = 1
        elif self.count == 1000:
          self._terminated = True
        return reward

      def apply_hindsight(self, states, actions, goal):
        new_states = []
        rewards = []
        goal_pos = goal[:2]
        for state, action in zip(states, actions):
          new_state = state.copy()
          new_state[2:] = goal_pos
          new_states.append(new_state)
          pos_after_action = new_state[:2] + self.moves[action]
          if np.array_equal(pos_after_action, goal_pos):
            rewards.append(1)
            break
          else:
            rewards.append(0)
        return new_states, rewards

    # A simple policy with two hidden layers.

    class TestPolicy(dc.rl.Policy):

      def __init__(self):
        super(TestPolicy, self).__init__(['action_prob', 'value'])

      def create_model(self, **kwargs):
        state = Input(shape=(4,))
        dense1 = Dense(6, activation=tf.nn.relu)(state)
        dense2 = Dense(6, activation=tf.nn.relu)(dense1)
        output = Dense(4, activation=tf.nn.softmax, use_bias=False)(dense2)
        value = Dense(1)(dense2)
        return tf.keras.Model(inputs=state, outputs=[output, value])

    # Optimize it.

    env = TestEnvironment()
    a2c = dc.rl.A2C(
        env,
        TestPolicy(),
        use_hindsight=True,
        optimizer=Adam(learning_rate=0.001))
    a2c.fit(1000000)

    # Try running it a few times and see if it succeeds.

    pass_count = 0
    for i in range(5):
      env.reset()
      while not env.terminated:
        env.step(a2c.select_action(env.state))
      if np.array_equal(env.state[:2], env.state[2:]):
        pass_count += 1
    assert pass_count &gt;= 3

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 103:</b> &nbsp; 2 fragments, nominal size 26 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1356')" href="javascript:;">
deepchem-2.4.0/deepchem/data/pytorch_datasets.py: 33-60
</a>
<div class="mid" id="frag1356" style="display:none"><pre>
  def __iter__(self):
    n_samples = self.numpy_dataset._X.shape[0]
    worker_info = torch.utils.data.get_worker_info()
    if worker_info is None:
      first_sample = 0
      last_sample = n_samples
    else:
      first_sample = worker_info.id * n_samples // worker_info.num_workers
      last_sample = (worker_info.id + 1) * n_samples // worker_info.num_workers
    for epoch in range(self.epochs):
      if self.deterministic:
        order = first_sample + np.arange(last_sample - first_sample)
      else:
        # Ensure that every worker will pick the same random order for each epoch.
        random = np.random.RandomState(epoch)
        order = random.permutation(n_samples)[first_sample:last_sample]
      if self.batch_size is None:
        for i in order:
          yield (self.numpy_dataset._X[i], self.numpy_dataset._y[i],
                 self.numpy_dataset._w[i], self.numpy_dataset._ids[i])
      else:
        for i in range(0, len(order), self.batch_size):
          indices = order[i:i + self.batch_size]
          yield (self.numpy_dataset._X[indices], self.numpy_dataset._y[indices],
                 self.numpy_dataset._w[indices],
                 self.numpy_dataset._ids[indices])


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1360')" href="javascript:;">
deepchem-2.4.0/deepchem/data/pytorch_datasets.py: 138-165
</a>
<div class="mid" id="frag1360" style="display:none"><pre>
  def __iter__(self):
    n_samples = self.image_dataset._X_shape[0]
    worker_info = torch.utils.data.get_worker_info()
    if worker_info is None:
      first_sample = 0
      last_sample = n_samples
    else:
      first_sample = worker_info.id * n_samples // worker_info.num_workers
      last_sample = (worker_info.id + 1) * n_samples // worker_info.num_workers
    for epoch in range(self.epochs):
      if self.deterministic:
        order = first_sample + np.arange(last_sample - first_sample)
      else:
        # Ensure that every worker will pick the same random order for each epoch.
        random = np.random.RandomState(epoch)
        order = random.permutation(n_samples)[first_sample:last_sample]
      if self.batch_size is None:
        for i in order:
          yield (self.image_dataset._get_image(self.image_dataset._X, i),
                 self.image_dataset._get_image(self.image_dataset._y, i),
                 self.image_dataset._w[i], self.image_dataset._ids[i])
      else:
        for i in range(0, len(order), self.batch_size):
          indices = order[i:i + self.batch_size]
          yield (self.image_dataset._get_image(self.image_dataset._X, indices),
                 self.image_dataset._get_image(self.image_dataset._y, indices),
                 self.image_dataset._w[indices],
                 self.image_dataset._ids[indices])
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 104:</b> &nbsp; 3 fragments, nominal size 14 lines, similarity 86%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1361')" href="javascript:;">
deepchem-2.4.0/deepchem/data/tests/test_shape.py: 6-25
</a>
<div class="mid" id="frag1361" style="display:none"><pre>
def test_numpy_dataset_get_shape():
  """Test that get_shape works for numpy datasets."""
  num_datapoints = 100
  num_features = 10
  num_tasks = 10
  # Generate data
  X = np.random.rand(num_datapoints, num_features)
  y = np.random.randint(2, size=(num_datapoints, num_tasks))
  w = np.random.randint(2, size=(num_datapoints, num_tasks))
  ids = np.array(["id"] * num_datapoints)

  dataset = dc.data.NumpyDataset(X, y, w, ids)

  X_shape, y_shape, w_shape, ids_shape = dataset.get_shape()
  assert X_shape == X.shape
  assert y_shape == y.shape
  assert w_shape == w.shape
  assert ids_shape == ids.shape


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1363')" href="javascript:;">
deepchem-2.4.0/deepchem/data/tests/test_shape.py: 46-67
</a>
<div class="mid" id="frag1363" style="display:none"><pre>
def test_disk_dataset_get_shape_multishard():
  """Test that get_shape works for multisharded disk dataset."""
  num_datapoints = 100
  num_features = 10
  num_tasks = 10
  # Generate data
  X = np.random.rand(num_datapoints, num_features)
  y = np.random.randint(2, size=(num_datapoints, num_tasks))
  w = np.random.randint(2, size=(num_datapoints, num_tasks))
  ids = np.array(["id"] * num_datapoints)

  dataset = dc.data.DiskDataset.from_numpy(X, y, w, ids)
  # Should now have 10 shards
  dataset.reshard(shard_size=10)

  X_shape, y_shape, w_shape, ids_shape = dataset.get_shape()
  assert X_shape == X.shape
  assert y_shape == y.shape
  assert w_shape == w.shape
  assert ids_shape == ids.shape


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1362')" href="javascript:;">
deepchem-2.4.0/deepchem/data/tests/test_shape.py: 26-45
</a>
<div class="mid" id="frag1362" style="display:none"><pre>
def test_disk_dataset_get_shape_single_shard():
  """Test that get_shape works for disk dataset."""
  num_datapoints = 100
  num_features = 10
  num_tasks = 10
  # Generate data
  X = np.random.rand(num_datapoints, num_features)
  y = np.random.randint(2, size=(num_datapoints, num_tasks))
  w = np.random.randint(2, size=(num_datapoints, num_tasks))
  ids = np.array(["id"] * num_datapoints)

  dataset = dc.data.DiskDataset.from_numpy(X, y, w, ids)

  X_shape, y_shape, w_shape, ids_shape = dataset.get_shape()
  assert X_shape == X.shape
  assert y_shape == y.shape
  assert w_shape == w.shape
  assert ids_shape == ids.shape


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 105:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 92%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1364')" href="javascript:;">
deepchem-2.4.0/deepchem/data/tests/test_shape.py: 68-87
</a>
<div class="mid" id="frag1364" style="display:none"><pre>
def test_disk_dataset_get_legacy_shape_single_shard():
  """Test that get_shape works for legacy disk dataset."""
  # This is the shape of legacy_data
  num_datapoints = 100
  num_features = 10
  num_tasks = 10

  current_dir = os.path.dirname(os.path.abspath(__file__))
  # legacy_dataset is a dataset in the legacy format kept around for testing
  # purposes.
  data_dir = os.path.join(current_dir, "legacy_dataset")
  dataset = dc.data.DiskDataset(data_dir)

  X_shape, y_shape, w_shape, ids_shape = dataset.get_shape()
  assert X_shape == (num_datapoints, num_features)
  assert y_shape == (num_datapoints, num_tasks)
  assert w_shape == (num_datapoints, num_tasks)
  assert ids_shape == (num_datapoints,)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1365')" href="javascript:;">
deepchem-2.4.0/deepchem/data/tests/test_shape.py: 88-108
</a>
<div class="mid" id="frag1365" style="display:none"><pre>
def test_disk_dataset_get_legacy_shape_multishard():
  """Test that get_shape works for multisharded legacy disk dataset."""
  # This is the shape of legacy_data_reshard
  num_datapoints = 100
  num_features = 10
  num_tasks = 10

  # legacy_dataset_reshard is a sharded dataset in the legacy format kept
  # around for testing
  current_dir = os.path.dirname(os.path.abspath(__file__))
  data_dir = os.path.join(current_dir, "legacy_dataset_reshard")
  dataset = dc.data.DiskDataset(data_dir)

  # Should now have 10 shards
  assert dataset.get_number_shards() == 10

  X_shape, y_shape, w_shape, ids_shape = dataset.get_shape()
  assert X_shape == (num_datapoints, num_features)
  assert y_shape == (num_datapoints, num_tasks)
  assert w_shape == (num_datapoints, num_tasks)
  assert ids_shape == (num_datapoints,)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 106:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1384')" href="javascript:;">
deepchem-2.4.0/deepchem/data/tests/test_property.py: 5-18
</a>
<div class="mid" id="frag1384" style="display:none"><pre>
def test_y_property():
  """Test that dataset.y works."""
  num_datapoints = 10
  num_features = 10
  num_tasks = 1
  X = np.random.rand(num_datapoints, num_features)
  y = np.random.randint(2, size=(num_datapoints, num_tasks))
  w = np.ones((num_datapoints, num_tasks))
  ids = np.array(["id"] * num_datapoints)
  dataset = dc.data.DiskDataset.from_numpy(X, y, w, ids)
  y_out = dataset.y
  np.testing.assert_array_equal(y, y_out)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1385')" href="javascript:;">
deepchem-2.4.0/deepchem/data/tests/test_property.py: 19-30
</a>
<div class="mid" id="frag1385" style="display:none"><pre>
def test_w_property():
  """Test that dataset.y works."""
  num_datapoints = 10
  num_features = 10
  num_tasks = 1
  X = np.random.rand(num_datapoints, num_features)
  y = np.random.randint(2, size=(num_datapoints, num_tasks))
  w = np.ones((num_datapoints, num_tasks))
  ids = np.array(["id"] * num_datapoints)
  dataset = dc.data.DiskDataset.from_numpy(X, y, w, ids)
  w_out = dataset.w
  np.testing.assert_array_equal(w, w_out)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 107:</b> &nbsp; 3 fragments, nominal size 19 lines, similarity 94%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1389')" href="javascript:;">
deepchem-2.4.0/deepchem/data/tests/test_shuffle.py: 9-29
</a>
<div class="mid" id="frag1389" style="display:none"><pre>
def test_complete_shuffle_one_shard():
  """Test that complete shuffle works with only one shard."""
  X = np.random.rand(10, 10)
  dataset = dc.data.DiskDataset.from_numpy(X)
  shuffled = dataset.complete_shuffle()
  assert len(shuffled) == len(dataset)
  assert not np.array_equal(shuffled.ids, dataset.ids)
  assert sorted(shuffled.ids) == sorted(dataset.ids)
  assert shuffled.X.shape == dataset.X.shape
  assert shuffled.y.shape == dataset.y.shape
  assert shuffled.w.shape == dataset.w.shape
  original_indices = dict((id, i) for i, id in enumerate(dataset.ids))
  shuffled_indices = dict((id, i) for i, id in enumerate(shuffled.ids))
  for id in dataset.ids:
    i = original_indices[id]
    j = shuffled_indices[id]
    assert np.array_equal(dataset.X[i], shuffled.X[j])
    assert np.array_equal(dataset.y[i], shuffled.y[j])
    assert np.array_equal(dataset.w[i], shuffled.w[j])


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1390')" href="javascript:;">
deepchem-2.4.0/deepchem/data/tests/test_shuffle.py: 30-51
</a>
<div class="mid" id="frag1390" style="display:none"><pre>
def test_complete_shuffle_multiple_shard():
  """Test that complete shuffle works with multiple shards."""
  X = np.random.rand(100, 10)
  dataset = dc.data.DiskDataset.from_numpy(X)
  dataset.reshard(shard_size=10)
  shuffled = dataset.complete_shuffle()
  assert len(shuffled) == len(dataset)
  assert not np.array_equal(shuffled.ids, dataset.ids)
  assert sorted(shuffled.ids) == sorted(dataset.ids)
  assert shuffled.X.shape == dataset.X.shape
  assert shuffled.y.shape == dataset.y.shape
  assert shuffled.w.shape == dataset.w.shape
  original_indices = dict((id, i) for i, id in enumerate(dataset.ids))
  shuffled_indices = dict((id, i) for i, id in enumerate(shuffled.ids))
  for id in dataset.ids:
    i = original_indices[id]
    j = shuffled_indices[id]
    assert np.array_equal(dataset.X[i], shuffled.X[j])
    assert np.array_equal(dataset.y[i], shuffled.y[j])
    assert np.array_equal(dataset.w[i], shuffled.w[j])


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1391')" href="javascript:;">
deepchem-2.4.0/deepchem/data/tests/test_shuffle.py: 52-73
</a>
<div class="mid" id="frag1391" style="display:none"><pre>
def test_complete_shuffle_multiple_shard_uneven():
  """Test that complete shuffle works with multiple shards and some shards not full size."""
  X = np.random.rand(57, 10)
  dataset = dc.data.DiskDataset.from_numpy(X)
  dataset.reshard(shard_size=10)
  shuffled = dataset.complete_shuffle()
  assert len(shuffled) == len(dataset)
  assert not np.array_equal(shuffled.ids, dataset.ids)
  assert sorted(shuffled.ids) == sorted(dataset.ids)
  assert shuffled.X.shape == dataset.X.shape
  assert shuffled.y.shape == dataset.y.shape
  assert shuffled.w.shape == dataset.w.shape
  original_indices = dict((id, i) for i, id in enumerate(dataset.ids))
  shuffled_indices = dict((id, i) for i, id in enumerate(shuffled.ids))
  for id in dataset.ids:
    i = original_indices[id]
    j = shuffled_indices[id]
    assert np.array_equal(dataset.X[i], shuffled.X[j])
    assert np.array_equal(dataset.y[i], shuffled.y[j])
    assert np.array_equal(dataset.w[i], shuffled.w[j])


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 108:</b> &nbsp; 2 fragments, nominal size 22 lines, similarity 95%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1394')" href="javascript:;">
deepchem-2.4.0/deepchem/data/tests/test_shuffle.py: 143-172
</a>
<div class="mid" id="frag1394" style="display:none"><pre>
def test_shuffle_each_shard():
  """Test that shuffle_each_shard works."""
  n_samples = 100
  n_tasks = 10
  n_features = 10

  X = np.random.rand(n_samples, n_features)
  y = np.random.randint(2, size=(n_samples, n_tasks))
  w = np.random.randint(2, size=(n_samples, n_tasks))
  ids = np.arange(n_samples)
  dataset = dc.data.DiskDataset.from_numpy(X, y, w, ids)
  dataset.reshard(shard_size=10)

  dataset.shuffle_each_shard()
  X_s, y_s, w_s, ids_s = (dataset.X, dataset.y, dataset.w, dataset.ids)
  assert X_s.shape == X.shape
  assert y_s.shape == y.shape
  assert ids_s.shape == ids.shape
  assert w_s.shape == w.shape
  assert not (ids_s == ids).all()

  # The ids should now store the performed permutation. Check that the
  # original dataset is recoverable.
  for i in range(n_samples):
    np.testing.assert_array_equal(X_s[i], X[ids_s[i]])
    np.testing.assert_array_equal(y_s[i], y[ids_s[i]])
    np.testing.assert_array_equal(w_s[i], w[ids_s[i]])
    np.testing.assert_array_equal(ids_s[i], ids[ids_s[i]])


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1395')" href="javascript:;">
deepchem-2.4.0/deepchem/data/tests/test_shuffle.py: 173-200
</a>
<div class="mid" id="frag1395" style="display:none"><pre>
def test_shuffle_shards():
  """Test that shuffle_shards works."""
  n_samples = 100
  n_tasks = 10
  n_features = 10

  X = np.random.rand(n_samples, n_features)
  y = np.random.randint(2, size=(n_samples, n_tasks))
  w = np.random.randint(2, size=(n_samples, n_tasks))
  ids = np.arange(n_samples)
  dataset = dc.data.DiskDataset.from_numpy(X, y, w, ids)
  dataset.reshard(shard_size=10)
  dataset.shuffle_shards()

  X_s, y_s, w_s, ids_s = (dataset.X, dataset.y, dataset.w, dataset.ids)

  assert X_s.shape == X.shape
  assert y_s.shape == y.shape
  assert ids_s.shape == ids.shape
  assert w_s.shape == w.shape

  # The ids should now store the performed permutation. Check that the
  # original dataset is recoverable.
  for i in range(n_samples):
    np.testing.assert_array_equal(X_s[i], X[ids_s[i]])
    np.testing.assert_array_equal(y_s[i], y[ids_s[i]])
    np.testing.assert_array_equal(w_s[i], w[ids_s[i]])
    np.testing.assert_array_equal(ids_s[i], ids[ids_s[i]])
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 109:</b> &nbsp; 3 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1408')" href="javascript:;">
deepchem-2.4.0/deepchem/data/tests/test_datasets.py: 280-293
</a>
<div class="mid" id="frag1408" style="display:none"><pre>
  def shard_generator():
    for sz in shard_sizes:
      X_b = np.random.rand(sz, 1)
      y_b = np.random.rand(sz, 1)
      w_b = np.random.rand(sz, 1)
      ids_b = np.random.rand(sz)

      all_Xs.append(X_b)
      all_ys.append(y_b)
      all_ws.append(w_b)
      all_ids.append(ids_b)

      yield X_b, y_b, w_b, ids_b

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1422')" href="javascript:;">
deepchem-2.4.0/deepchem/data/tests/test_datasets.py: 562-575
</a>
<div class="mid" id="frag1422" style="display:none"><pre>

    def shard_generator():
      for sz in shard_sizes:
        X_b = np.random.rand(sz, 1)
        y_b = np.random.rand(sz, 1)
        w_b = np.random.rand(sz, 1)
        ids_b = np.random.rand(sz)

        all_Xs.append(X_b)
        all_ys.append(y_b)
        all_ws.append(w_b)
        all_ids.append(ids_b)

        yield X_b, y_b, w_b, ids_b
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1418')" href="javascript:;">
deepchem-2.4.0/deepchem/data/tests/test_datasets.py: 442-455
</a>
<div class="mid" id="frag1418" style="display:none"><pre>

  def shard_generator():
    for sz in shard_sizes:
      X_b = np.random.rand(sz, 1)
      y_b = np.random.rand(sz, 1)
      w_b = np.random.rand(sz, 1)
      ids_b = np.random.rand(sz)

      all_Xs.append(X_b)
      all_ys.append(y_b)
      all_ws.append(w_b)
      all_ids.append(ids_b)

      yield X_b, y_b, w_b, ids_b
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 110:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 93%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1433')" href="javascript:;">
deepchem-2.4.0/deepchem/data/tests/test_reload.py: 63-80
</a>
<div class="mid" id="frag1433" style="display:none"><pre>

  def test_reload_after_gen(self):
    """Check num samples for loaded and reloaded datasets is equal."""
    reload = False
    current_dir = os.path.dirname(os.path.abspath(__file__))
    dataset_file = os.path.join(current_dir,
                                "../../../datasets/mini_muv.csv.gz")
    logger.info("Running experiment for first time without reload.")
    (len_train, len_valid, len_test) = self._run_muv_experiment(
        dataset_file, reload)

    logger.info("Running experiment for second time with reload.")
    reload = True
    (len_reload_train, len_reload_valid,
     len_reload_test) = (self._run_muv_experiment(dataset_file, reload))
    assert len_train == len_reload_train
    assert len_valid == len_reload_valid
    assert len_test == len_reload_valid
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1434')" href="javascript:;">
deepchem-2.4.0/deepchem/data/tests/test_reload.py: 81-96
</a>
<div class="mid" id="frag1434" style="display:none"><pre>

  def test_reload_twice(self):
    """Check ability to repeatedly run experiments with reload set True."""
    reload = True
    current_dir = os.path.dirname(os.path.abspath(__file__))
    dataset_file = os.path.join(current_dir,
                                "../../../datasets/mini_muv.csv.gz")
    logger.info("Running experiment for first time with reload.")
    (len_train, len_valid, len_test) = self._run_muv_experiment(
        dataset_file, reload)

    logger.info("Running experiment for second time with reload.")
    (len_reload_train, len_reload_valid,
     len_reload_test) = (self._run_muv_experiment(dataset_file, reload))
    assert len_train == len_reload_train
    assert len_valid == len_reload_valid
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 111:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1438')" href="javascript:;">
deepchem-2.4.0/deepchem/data/tests/test_reshard.py: 42-56
</a>
<div class="mid" id="frag1438" style="display:none"><pre>
def test_reshard_with_X_y_w():
  """Test resharding on a simple example"""
  X = np.random.rand(100, 10)
  y = np.random.rand(100,)
  w = np.ones_like(y)
  dataset = dc.data.DiskDataset.from_numpy(X, y, w)
  assert dataset.get_number_shards() == 1
  dataset.reshard(shard_size=10)
  assert (dataset.X == X).all()
  # This is necessary since from_numpy adds in shape information
  assert (dataset.y.flatten() == y).all()
  assert (dataset.w.flatten() == w).all()
  assert dataset.get_number_shards() == 10


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1439')" href="javascript:;">
deepchem-2.4.0/deepchem/data/tests/test_reshard.py: 57-71
</a>
<div class="mid" id="frag1439" style="display:none"><pre>
def test_reshard_with_X_y_w_ids():
  """Test resharding on a simple example"""
  X = np.random.rand(100, 10)
  y = np.random.rand(100,)
  w = np.ones_like(y)
  ids = np.arange(100)
  dataset = dc.data.DiskDataset.from_numpy(X, y, w, ids)
  assert dataset.get_number_shards() == 1
  dataset.reshard(shard_size=10)
  assert (dataset.X == X).all()
  # This is necessary since from_numpy adds in shape information
  assert (dataset.y.flatten() == y).all()
  assert (dataset.w.flatten() == w).all()
  assert (dataset.ids == ids).all()
  assert dataset.get_number_shards() == 10
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 112:</b> &nbsp; 5 fragments, nominal size 18 lines, similarity 88%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1444')" href="javascript:;">
deepchem-2.4.0/deepchem/data/tests/test_select.py: 6-27
</a>
<div class="mid" id="frag1444" style="display:none"><pre>
def test_select():
  """Test that dataset select works."""
  num_datapoints = 10
  num_features = 10
  num_tasks = 1
  X = np.random.rand(num_datapoints, num_features)
  y = np.random.randint(2, size=(num_datapoints, num_tasks))
  w = np.ones((num_datapoints, num_tasks))
  ids = np.array(["id"] * num_datapoints)
  dataset = dc.data.DiskDataset.from_numpy(X, y, w, ids)

  indices = [0, 4, 5, 8]
  select_dataset = dataset.select(indices)
  assert isinstance(select_dataset, dc.data.DiskDataset)
  X_sel, y_sel, w_sel, ids_sel = (select_dataset.X, select_dataset.y,
                                  select_dataset.w, select_dataset.ids)
  np.testing.assert_array_equal(X[indices], X_sel)
  np.testing.assert_array_equal(y[indices], y_sel)
  np.testing.assert_array_equal(w[indices], w_sel)
  np.testing.assert_array_equal(ids[indices], ids_sel)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1447')" href="javascript:;">
deepchem-2.4.0/deepchem/data/tests/test_select.py: 66-88
</a>
<div class="mid" id="frag1447" style="display:none"><pre>
def test_select_multishard():
  """Test that dataset select works with multiple shards."""
  num_datapoints = 100
  num_features = 10
  num_tasks = 1
  X = np.random.rand(num_datapoints, num_features)
  y = np.random.randint(2, size=(num_datapoints, num_tasks))
  w = np.ones((num_datapoints, num_tasks))
  ids = np.array(["id"] * num_datapoints)
  dataset = dc.data.DiskDataset.from_numpy(X, y, w, ids)
  dataset.reshard(shard_size=10)

  indices = [10, 42, 51, 82, 2, 4, 6]
  select_dataset = dataset.select(indices)
  assert isinstance(select_dataset, dc.data.DiskDataset)
  X_sel, y_sel, w_sel, ids_sel = (select_dataset.X, select_dataset.y,
                                  select_dataset.w, select_dataset.ids)
  np.testing.assert_array_equal(X[indices], X_sel)
  np.testing.assert_array_equal(y[indices], y_sel)
  np.testing.assert_array_equal(w[indices], w_sel)
  np.testing.assert_array_equal(ids[indices], ids_sel)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1446')" href="javascript:;">
deepchem-2.4.0/deepchem/data/tests/test_select.py: 44-65
</a>
<div class="mid" id="frag1446" style="display:none"><pre>
def test_numpy_dataset_select():
  """Test that dataset select works with numpy dataset."""
  num_datapoints = 10
  num_features = 10
  num_tasks = 1
  X = np.random.rand(num_datapoints, num_features)
  y = np.random.randint(2, size=(num_datapoints, num_tasks))
  w = np.ones((num_datapoints, num_tasks))
  ids = np.array(["id"] * num_datapoints)
  dataset = dc.data.NumpyDataset(X, y, w, ids)

  indices = [0, 4, 5, 8, 2]
  select_dataset = dataset.select(indices)
  assert isinstance(select_dataset, dc.data.NumpyDataset)
  X_sel, y_sel, w_sel, ids_sel = (select_dataset.X, select_dataset.y,
                                  select_dataset.w, select_dataset.ids)
  np.testing.assert_array_equal(X[indices], X_sel)
  np.testing.assert_array_equal(y[indices], y_sel)
  np.testing.assert_array_equal(w[indices], w_sel)
  np.testing.assert_array_equal(ids[indices], ids_sel)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1449')" href="javascript:;">
deepchem-2.4.0/deepchem/data/tests/test_select.py: 111-130
</a>
<div class="mid" id="frag1449" style="display:none"><pre>
def test_select_to_numpy():
  """Test that dataset select works."""
  num_datapoints = 10
  num_features = 10
  num_tasks = 1
  X = np.random.rand(num_datapoints, num_features)
  y = np.random.randint(2, size=(num_datapoints, num_tasks))
  w = np.ones((num_datapoints, num_tasks))
  ids = np.array(["id"] * num_datapoints)
  dataset = dc.data.DiskDataset.from_numpy(X, y, w, ids)

  indices = [0, 4, 5, 8]
  select_dataset = dataset.select(indices, output_numpy_dataset=True)
  assert isinstance(select_dataset, dc.data.NumpyDataset)
  X_sel, y_sel, w_sel, ids_sel = (select_dataset.X, select_dataset.y,
                                  select_dataset.w, select_dataset.ids)
  np.testing.assert_array_equal(X[indices], X_sel)
  np.testing.assert_array_equal(y[indices], y_sel)
  np.testing.assert_array_equal(w[indices], w_sel)
  np.testing.assert_array_equal(ids[indices], ids_sel)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1448')" href="javascript:;">
deepchem-2.4.0/deepchem/data/tests/test_select.py: 89-110
</a>
<div class="mid" id="frag1448" style="display:none"><pre>
def test_select_not_sorted():
  """Test that dataset select with ids not in sorted order."""
  num_datapoints = 10
  num_features = 10
  num_tasks = 1
  X = np.random.rand(num_datapoints, num_features)
  y = np.random.randint(2, size=(num_datapoints, num_tasks))
  w = np.ones((num_datapoints, num_tasks))
  ids = np.array(["id"] * num_datapoints)
  dataset = dc.data.DiskDataset.from_numpy(X, y, w, ids)

  indices = [4, 2, 8, 5, 0]
  select_dataset = dataset.select(indices)
  assert isinstance(select_dataset, dc.data.DiskDataset)
  X_sel, y_sel, w_sel, ids_sel = (select_dataset.X, select_dataset.y,
                                  select_dataset.w, select_dataset.ids)
  np.testing.assert_array_equal(X[indices], X_sel)
  np.testing.assert_array_equal(y[indices], y_sel)
  np.testing.assert_array_equal(w[indices], w_sel)
  np.testing.assert_array_equal(ids[indices], ids_sel)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 113:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1457')" href="javascript:;">
deepchem-2.4.0/deepchem/data/tests/test_inmemory.py: 29-43
</a>
<div class="mid" id="frag1457" style="display:none"><pre>
def test_inmemory_features_and_labels_and_weights():
  smiles = ["C", "CC", "CCC", "CCCC"]
  labels = [1, 0, 1, 0]
  weights = [1.5, 1.5, 1, 1]
  featurizer = dc.feat.CircularFingerprint(size=1024)
  loader = dc.data.InMemoryLoader(tasks=["task1"], featurizer=featurizer)
  dataset = loader.create_dataset(zip(smiles, labels, weights), shard_size=2)
  assert len(dataset) == 4
  assert dataset.X.shape == (4, 1024)
  assert (dataset.y == np.array(labels)).all()
  assert (dataset.w == np.array(weights)).all()
  assert (dataset.ids == np.arange(4)).all()
  assert dataset.get_number_shards() == 2


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1458')" href="javascript:;">
deepchem-2.4.0/deepchem/data/tests/test_inmemory.py: 44-58
</a>
<div class="mid" id="frag1458" style="display:none"><pre>
def test_inmemory_features_and_labels_and_weights_and_ids():
  smiles = ["C", "CC", "CCC", "CCCC"]
  labels = [1, 0, 1, 0]
  weights = [1.5, 1.5, 1, 1]
  ids = smiles
  featurizer = dc.feat.CircularFingerprint(size=1024)
  loader = dc.data.InMemoryLoader(tasks=["task1"], featurizer=featurizer)
  dataset = loader.create_dataset(
      zip(smiles, labels, weights, ids), shard_size=2)
  assert len(dataset) == 4
  assert dataset.X.shape == (4, 1024)
  assert (dataset.y == np.array(labels)).all()
  assert (dataset.w == np.array(weights)).all()
  assert (dataset.ids == np.array(ids)).all()
  assert dataset.get_number_shards() == 2
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 114:</b> &nbsp; 3 fragments, nominal size 20 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1460')" href="javascript:;">
deepchem-2.4.0/deepchem/data/tests/test_support_generator.py: 38-62
</a>
<div class="mid" id="frag1460" style="display:none"><pre>
  def test_get_task_support_simple(self):
    """Tests that get_task_support samples correctly."""
    n_samples = 20
    n_features = 3
    n_tasks = 1

    # Generate dummy dataset
    np.random.seed(123)
    ids = np.arange(n_samples)
    X = np.random.rand(n_samples, n_features)
    y = np.random.randint(2, size=(n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))
    dataset = dc.data.NumpyDataset(X, y, w, ids)

    n_episodes = 20
    n_pos = 1
    n_neg = 5
    supports = dc.data.get_task_support(
        dataset, n_episodes, n_pos, n_neg, task=0, log_every_n=10)
    assert len(supports) == n_episodes

    for support in supports:
      assert len(support) == n_pos + n_neg
      assert np.count_nonzero(support.y) == n_pos

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1461')" href="javascript:;">
deepchem-2.4.0/deepchem/data/tests/test_support_generator.py: 63-92
</a>
<div class="mid" id="frag1461" style="display:none"><pre>
  def test_get_task_support_missing(self):
    """Test that task support works in presence of missing data."""
    n_samples = 20
    n_features = 3
    n_tasks = 1

    # Generate dummy dataset
    np.random.seed(123)
    ids = np.arange(n_samples)
    X = np.random.rand(n_samples, n_features)
    y = np.random.randint(2, size=(n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))
    # Set last n_samples/2 weights to 0
    w[n_samples // 2:] = 0
    dataset = dc.data.NumpyDataset(X, y, w, ids)

    n_episodes = 20
    n_pos = 1
    n_neg = 2
    supports = dc.data.get_task_support(
        dataset, n_episodes, n_pos, n_neg, task=0, log_every_n=10)
    assert len(supports) == n_episodes

    for support in supports:
      assert len(support) == n_pos + n_neg
      assert np.count_nonzero(support.y) == n_pos
      # Check that no support elements are sample from zero-weight samples
      for identifier in support.ids:
        assert identifier &lt; n_samples / 2

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1462')" href="javascript:;">
deepchem-2.4.0/deepchem/data/tests/test_support_generator.py: 93-115
</a>
<div class="mid" id="frag1462" style="display:none"><pre>
  def test_get_task_test(self):
    """Tests that get_task_testsamples correctly."""
    n_samples = 20
    n_features = 3
    n_tasks = 1

    # Generate dummy dataset
    np.random.seed(123)
    ids = np.arange(n_samples)
    X = np.random.rand(n_samples, n_features)
    y = np.random.randint(2, size=(n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))
    dataset = dc.data.NumpyDataset(X, y, w, ids)

    n_episodes = 20
    n_test = 10
    tests = dc.data.get_task_test(
        dataset, n_episodes, n_test, task=0, log_every_n=10)

    assert len(tests) == n_episodes
    for test in tests:
      assert len(test) == n_test

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 115:</b> &nbsp; 2 fragments, nominal size 20 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1465')" href="javascript:;">
deepchem-2.4.0/deepchem/data/tests/test_support_generator.py: 168-195
</a>
<div class="mid" id="frag1465" style="display:none"><pre>
  def test_get_task_minus_support_simple(self):
    """Test that fixed index support can be removed from dataset."""
    n_samples = 20
    n_support = 5
    n_features = 3
    n_tasks = 1

    # Generate dummy dataset
    np.random.seed(123)
    ids = np.arange(n_samples)
    X = np.random.rand(n_samples, n_features)
    y = np.random.randint(2, size=(n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))
    dataset = dc.data.NumpyDataset(X, y, w, ids)

    support_dataset = dc.data.NumpyDataset(X[:n_support], y[:n_support],
                                           w[:n_support], ids[:n_support])

    task_dataset = dc.data.get_task_dataset_minus_support(
        dataset, support_dataset, task=0)

    # Assert all support elements have been removed
    assert len(task_dataset) == n_samples - n_support
    np.testing.assert_array_equal(task_dataset.X, X[n_support:])
    np.testing.assert_array_equal(task_dataset.y, y[n_support:])
    np.testing.assert_array_equal(task_dataset.w, w[n_support:])
    np.testing.assert_array_equal(task_dataset.ids, ids[n_support:])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1466')" href="javascript:;">
deepchem-2.4.0/deepchem/data/tests/test_support_generator.py: 196-222
</a>
<div class="mid" id="frag1466" style="display:none"><pre>
  def test_dataset_difference_simple(self):
    """Test that fixed index can be removed from dataset."""
    n_samples = 20
    n_remove = 5
    n_features = 3
    n_tasks = 1

    # Generate dummy dataset
    np.random.seed(123)
    ids = np.arange(n_samples)
    X = np.random.rand(n_samples, n_features)
    y = np.random.randint(2, size=(n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))
    dataset = dc.data.NumpyDataset(X, y, w, ids)

    remove_dataset = dc.data.NumpyDataset(X[:n_remove], y[:n_remove],
                                          w[:n_remove], ids[:n_remove])

    out_dataset = dc.data.dataset_difference(dataset, remove_dataset)

    # Assert all remove elements have been removed
    assert len(out_dataset) == n_samples - n_remove
    np.testing.assert_array_equal(out_dataset.X, X[n_remove:])
    np.testing.assert_array_equal(out_dataset.y, y[n_remove:])
    np.testing.assert_array_equal(out_dataset.w, w[n_remove:])
    np.testing.assert_array_equal(out_dataset.ids, ids[n_remove:])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 116:</b> &nbsp; 2 fragments, nominal size 23 lines, similarity 91%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1467')" href="javascript:;">
deepchem-2.4.0/deepchem/data/tests/test_support_generator.py: 223-253
</a>
<div class="mid" id="frag1467" style="display:none"><pre>
  def test_get_task_minus_support(self):
    """Test that random index support can be removed from dataset."""
    n_samples = 10
    n_support = 4
    n_features = 3
    n_tasks = 1

    # Generate dummy dataset
    np.random.seed(123)
    ids = np.arange(n_samples)
    X = np.random.rand(n_samples, n_features)
    y = np.random.randint(2, size=(n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))
    dataset = dc.data.NumpyDataset(X, y, w, ids)

    support_inds = sorted(
        np.random.choice(np.arange(n_samples), (n_support,), replace=False))
    support_dataset = dc.data.NumpyDataset(X[support_inds], y[support_inds],
                                           w[support_inds], ids[support_inds])

    task_dataset = dc.data.get_task_dataset_minus_support(
        dataset, support_dataset, task=0)

    # Assert all support elements have been removed
    data_inds = sorted(list(set(range(n_samples)) - set(support_inds)))
    assert len(task_dataset) == n_samples - n_support
    np.testing.assert_array_equal(task_dataset.X, X[data_inds])
    np.testing.assert_array_equal(task_dataset.y, y[data_inds])
    np.testing.assert_array_equal(task_dataset.w, w[data_inds])
    np.testing.assert_array_equal(task_dataset.ids, ids[data_inds])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1468')" href="javascript:;">
deepchem-2.4.0/deepchem/data/tests/test_support_generator.py: 254-283
</a>
<div class="mid" id="frag1468" style="display:none"><pre>
  def test_dataset_difference(self):
    """Test that random index can be removed from dataset."""
    n_samples = 10
    n_remove = 4
    n_features = 3
    n_tasks = 1

    # Generate dummy dataset
    np.random.seed(123)
    ids = np.arange(n_samples)
    X = np.random.rand(n_samples, n_features)
    y = np.random.randint(2, size=(n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))
    dataset = dc.data.NumpyDataset(X, y, w, ids)

    remove_inds = sorted(
        np.random.choice(np.arange(n_samples), (n_remove,), replace=False))
    remove_dataset = dc.data.NumpyDataset(X[remove_inds], y[remove_inds],
                                          w[remove_inds], ids[remove_inds])

    out_dataset = dc.data.dataset_difference(dataset, remove_dataset)

    # Assert all remove elements have been removed
    data_inds = sorted(list(set(range(n_samples)) - set(remove_inds)))
    assert len(out_dataset) == n_samples - n_remove
    np.testing.assert_array_equal(out_dataset.X, X[data_inds])
    np.testing.assert_array_equal(out_dataset.y, y[data_inds])
    np.testing.assert_array_equal(out_dataset.w, w[data_inds])
    np.testing.assert_array_equal(out_dataset.ids, ids[data_inds])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 117:</b> &nbsp; 4 fragments, nominal size 14 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1473')" href="javascript:;">
deepchem-2.4.0/deepchem/data/tests/test_data_loader.py: 21-42
</a>
<div class="mid" id="frag1473" style="display:none"><pre>
def test_scaffold_test_train_valid_test_split():
  """Test of singletask RF ECFP regression API."""
  current_dir = os.path.dirname(os.path.abspath(__file__))
  tasks = ["log-solubility"]
  input_file = os.path.join(current_dir, "../../models/tests/example.csv")
  featurizer = dc.feat.CircularFingerprint(size=1024)

  input_file = os.path.join(current_dir, input_file)
  loader = dc.data.CSVLoader(
      tasks=tasks, feature_field="smiles", featurizer=featurizer)

  dataset = loader.create_dataset(input_file)

  # Splits featurized samples into train/test
  splitter = dc.splits.ScaffoldSplitter()
  train_dataset, valid_dataset, test_dataset = splitter.train_valid_test_split(
      dataset)
  assert len(train_dataset) == 8
  assert len(valid_dataset) == 1
  assert len(test_dataset) == 1


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1474')" href="javascript:;">
deepchem-2.4.0/deepchem/data/tests/test_data_loader.py: 43-62
</a>
<div class="mid" id="frag1474" style="display:none"><pre>
def test_scaffold_test_train_test_split():
  """Test of singletask RF ECFP regression API."""
  current_dir = os.path.dirname(os.path.abspath(__file__))
  tasks = ["log-solubility"]
  input_file = os.path.join(current_dir, "../../models/tests/example.csv")
  featurizer = dc.feat.CircularFingerprint(size=1024)

  input_file = os.path.join(current_dir, input_file)
  loader = dc.data.CSVLoader(
      tasks=tasks, feature_field="smiles", featurizer=featurizer)

  dataset = loader.create_dataset(input_file)

  # Splits featurized samples into train/test
  splitter = dc.splits.ScaffoldSplitter()
  train_dataset, test_dataset = splitter.train_test_split(dataset)
  assert len(train_dataset) == 8
  assert len(test_dataset) == 2


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1475')" href="javascript:;">
deepchem-2.4.0/deepchem/data/tests/test_data_loader.py: 63-84
</a>
<div class="mid" id="frag1475" style="display:none"><pre>
def test_random_test_train_valid_test_split():
  """Test of singletask RF ECFP regression API."""
  current_dir = os.path.dirname(os.path.abspath(__file__))
  tasks = ["log-solubility"]
  input_file = os.path.join(current_dir, "../../models/tests/example.csv")
  featurizer = dc.feat.CircularFingerprint(size=1024)

  input_file = os.path.join(current_dir, input_file)
  loader = dc.data.CSVLoader(
      tasks=tasks, feature_field="smiles", featurizer=featurizer)

  dataset = loader.create_dataset(input_file)

  # Splits featurized samples into train/test
  splitter = dc.splits.RandomSplitter()
  train_dataset, valid_dataset, test_dataset = splitter.train_valid_test_split(
      dataset)
  assert len(train_dataset) == 8
  assert len(valid_dataset) == 1
  assert len(test_dataset) == 1


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1476')" href="javascript:;">
deepchem-2.4.0/deepchem/data/tests/test_data_loader.py: 85-102
</a>
<div class="mid" id="frag1476" style="display:none"><pre>
def test_random_test_train_test_split():
  """Test of singletask RF ECFP regression API."""
  current_dir = os.path.dirname(os.path.abspath(__file__))
  tasks = ["log-solubility"]
  input_file = os.path.join(current_dir, "../../models/tests/example.csv")
  featurizer = dc.feat.CircularFingerprint(size=1024)
  loader = dc.data.CSVLoader(
      tasks=tasks, feature_field="smiles", featurizer=featurizer)

  dataset = loader.create_dataset(input_file)

  # Splits featurized samples into train/test
  splitter = dc.splits.RandomSplitter()
  train_dataset, test_dataset = splitter.train_test_split(dataset)
  assert len(train_dataset) == 8
  assert len(test_dataset) == 2


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 118:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1550')" href="javascript:;">
deepchem-2.4.0/deepchem/utils/data_utils.py: 289-319
</a>
<div class="mid" id="frag1550" style="display:none"><pre>
def load_csv_files(input_files: List[str],
                   shard_size: Optional[int] = None) -&gt; Iterator[pd.DataFrame]:
  """Load data as pandas dataframe from CSV files.

  Parameters
  ----------
  input_files: List[str]
    List of filenames
  shard_size: int, default None
    The shard size to yield at one time.

  Returns
  -------
  Iterator[pd.DataFrame]
    Generator which yields the dataframe which is the same shard size.
  """
  # First line of user-specified CSV *must* be header.
  shard_num = 1
  for input_file in input_files:
    if shard_size is None:
      yield pd.read_csv(input_file)
    else:
      logger.info("About to start loading CSV from %s" % input_file)
      for df in pd.read_csv(input_file, chunksize=shard_size):
        logger.info(
            "Loading shard %d of size %s." % (shard_num, str(shard_size)))
        df = df.replace(np.nan, str(""), regex=True)
        shard_num += 1
        yield df


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1551')" href="javascript:;">
deepchem-2.4.0/deepchem/utils/data_utils.py: 320-355
</a>
<div class="mid" id="frag1551" style="display:none"><pre>
def load_json_files(input_files: List[str],
                    shard_size: Optional[int] = None) -&gt; Iterator[pd.DataFrame]:
  """Load data as pandas dataframe.

  Parameters
  ----------
  input_files: List[str]
    List of json filenames.
  shard_size: int, default None
    Chunksize for reading json files.

  Returns
  -------
  Iterator[pd.DataFrame]
    Generator which yields the dataframe which is the same shard size.

  Notes
  -----
  To load shards from a json file into a Pandas dataframe, the file
  must be originally saved with ``df.to_json('filename.json', orient='records', lines=True)``
  """
  shard_num = 1
  for input_file in input_files:
    if shard_size is None:
      yield pd.read_json(input_file, orient='records', lines=True)
    else:
      logger.info("About to start loading json from %s." % input_file)
      for df in pd.read_json(
          input_file, orient='records', chunksize=shard_size, lines=True):
        logger.info(
            "Loading shard %d of size %s." % (shard_num, str(shard_size)))
        df = df.replace(np.nan, str(""), regex=True)
        shard_num += 1
        yield df


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 119:</b> &nbsp; 2 fragments, nominal size 24 lines, similarity 88%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1572')" href="javascript:;">
deepchem-2.4.0/deepchem/utils/test/test_coordinate_box_utils.py: 20-50
</a>
<div class="mid" id="frag1572" style="display:none"><pre>
  def test_union(self):
    x_range = (-10, 10)
    y_range = (-20, 20)
    z_range = (-30, 30)
    box = box_utils.CoordinateBox(x_range, y_range, z_range)

    x_range = (-1, 1)
    y_range = (-2, 2)
    z_range = (-3, 3)
    interior_box = box_utils.CoordinateBox(x_range, y_range, z_range)

    merged_box = box_utils.union(box, interior_box)
    assert merged_box.x_range == box.x_range
    assert merged_box.y_range == box.y_range
    assert merged_box.z_range == box.z_range

    x_range = (-10, 10)
    y_range = (-20, 20)
    z_range = (-30, 30)
    box1 = box_utils.CoordinateBox(x_range, y_range, z_range)

    x_range = (-11, 9)
    y_range = (-20, 20)
    z_range = (-30, 30)
    box2 = box_utils.CoordinateBox(x_range, y_range, z_range)

    merged_box = box_utils.union(box1, box2)
    assert merged_box.x_range == (-11, 10)
    assert merged_box.y_range == (-20, 20)
    assert merged_box.z_range == (-30, 30)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1579')" href="javascript:;">
deepchem-2.4.0/deepchem/utils/test/test_coordinate_box_utils.py: 101-129
</a>
<div class="mid" id="frag1579" style="display:none"><pre>
  def test_intersection(self):
    x_range = (-10, 10)
    y_range = (-20, 20)
    z_range = (-30, 30)
    box = box_utils.CoordinateBox(x_range, y_range, z_range)

    x_range = (-1, 1)
    y_range = (-2, 2)
    z_range = (-3, 3)
    interior_box = box_utils.CoordinateBox(x_range, y_range, z_range)

    int_box = box_utils.intersection(box, interior_box)
    assert int_box == interior_box

    x_range = (-10, 10)
    y_range = (-20, 20)
    z_range = (-30, 30)
    box1 = box_utils.CoordinateBox(x_range, y_range, z_range)

    x_range = (-11, 9)
    y_range = (-20, 20)
    z_range = (-30, 30)
    box2 = box_utils.CoordinateBox(x_range, y_range, z_range)

    int_box = box_utils.intersection(box1, box2)
    assert int_box.x_range == (-10, 9)
    assert int_box.y_range == (-20, 20)
    assert int_box.z_range == (-30, 30)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 120:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1596')" href="javascript:;">
deepchem-2.4.0/deepchem/utils/test/test_genomics_utils.py: 39-52
</a>
<div class="mid" id="frag1596" style="display:none"><pre>
  def test_encode_fasta_sequence(self):
    # Test it's possible to load a sequence with an aribrary alphabet from a fasta file.
    fname = os.path.join(self.current_dir, "./data/example.fasta")

    encoded_seqs = dc.utils.genomics_utils.encode_bio_sequence(
        fname, letters=LETTERS)
    expected = np.expand_dims(
        np.array([
            [[1, 0], [0, 1], [0, 0]],
            [[0, 1], [0, 0], [1, 0]],
        ]), -1)

    np.testing.assert_array_equal(expected, encoded_seqs)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1597')" href="javascript:;">
deepchem-2.4.0/deepchem/utils/test/test_genomics_utils.py: 53-65
</a>
<div class="mid" id="frag1597" style="display:none"><pre>
  def test_encode_fastq_sequence(self):
    fname = os.path.join(self.current_dir, "./data/example.fastq")

    encoded_seqs = dc.utils.genomics_utils.encode_bio_sequence(
        fname, file_type="fastq", letters=LETTERS)

    expected = np.expand_dims(
        np.array([
            [[1, 0], [0, 1], [0, 0]],
            [[0, 1], [0, 0], [1, 0]],
        ]), -1)

    np.testing.assert_array_equal(expected, encoded_seqs)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 121:</b> &nbsp; 4 fragments, nominal size 17 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1606')" href="javascript:;">
deepchem-2.4.0/deepchem/utils/test/test_pdbqt_utils.py: 17-40
</a>
<div class="mid" id="frag1606" style="display:none"><pre>
  def test_pdbqt_to_pdb(self):
    """Test that a PDBQT molecule can be converted back in to PDB."""
    xyz, mol = rdkit_utils.load_molecule(
        self.protein_file, calc_charges=False, add_hydrogens=False)
    with tempfile.TemporaryDirectory() as tmp:
      out_pdb = os.path.join(tmp, "mol.pdb")
      out_pdbqt = os.path.join(tmp, "mol.pdbqt")

      rdkit_utils.write_molecule(mol, out_pdb, is_protein=True)
      rdkit_utils.write_molecule(mol, out_pdbqt, is_protein=True)

      pdb_block = pdbqt_utils.pdbqt_to_pdb(out_pdbqt)
      from rdkit import Chem
      pdb_mol = Chem.MolFromPDBBlock(pdb_block, sanitize=False, removeHs=False)

      xyz, pdbqt_mol = rdkit_utils.load_molecule(
          out_pdbqt, add_hydrogens=False, calc_charges=False)

    assert pdb_mol.GetNumAtoms() == pdbqt_mol.GetNumAtoms()
    for atom_idx in range(pdb_mol.GetNumAtoms()):
      atom1 = pdb_mol.GetAtoms()[atom_idx]
      atom2 = pdbqt_mol.GetAtoms()[atom_idx]
      assert atom1.GetAtomicNum() == atom2.GetAtomicNum()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1608')" href="javascript:;">
deepchem-2.4.0/deepchem/utils/test/test_pdbqt_utils.py: 60-77
</a>
<div class="mid" id="frag1608" style="display:none"><pre>
  def test_convert_protein_to_pdbqt(self):
    """Test a protein in a PDB can be converted to PDBQT."""
    from rdkit import Chem
    xyz, mol = rdkit_utils.load_molecule(
        self.protein_file, calc_charges=False, add_hydrogens=False)
    with tempfile.TemporaryDirectory() as tmp:
      outfile = os.path.join(tmp, "mol.pdbqt")
      writer = Chem.PDBWriter(outfile)
      writer.write(mol)
      writer.close()
      pdbqt_utils.convert_protein_to_pdbqt(mol, outfile)
      pdbqt_xyz, pdbqt_mol = rdkit_utils.load_molecule(
          outfile, add_hydrogens=False, calc_charges=False)
    assert pdbqt_mol.GetNumAtoms() == pdbqt_mol.GetNumAtoms()
    for atom_idx in range(pdbqt_mol.GetNumAtoms()):
      atom1 = pdbqt_mol.GetAtoms()[atom_idx]
      atom2 = pdbqt_mol.GetAtoms()[atom_idx]
      assert atom1.GetAtomicNum() == atom2.GetAtomicNum()
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1607')" href="javascript:;">
deepchem-2.4.0/deepchem/utils/test/test_pdbqt_utils.py: 41-59
</a>
<div class="mid" id="frag1607" style="display:none"><pre>
  def test_convert_mol_to_pdbqt(self):
    """Test that a ligand molecule can be coverted to PDBQT."""
    from rdkit import Chem
    xyz, mol = rdkit_utils.load_molecule(
        self.ligand_file, calc_charges=False, add_hydrogens=False)
    with tempfile.TemporaryDirectory() as tmp:
      outfile = os.path.join(tmp, "mol.pdbqt")
      writer = Chem.PDBWriter(outfile)
      writer.write(mol)
      writer.close()
      pdbqt_utils.convert_mol_to_pdbqt(mol, outfile)
      pdbqt_xyz, pdbqt_mol = rdkit_utils.load_molecule(
          outfile, add_hydrogens=False, calc_charges=False)
    assert pdbqt_mol.GetNumAtoms() == pdbqt_mol.GetNumAtoms()
    for atom_idx in range(pdbqt_mol.GetNumAtoms()):
      atom1 = pdbqt_mol.GetAtoms()[atom_idx]
      atom2 = pdbqt_mol.GetAtoms()[atom_idx]
      assert atom1.GetAtomicNum() == atom2.GetAtomicNum()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1670')" href="javascript:;">
deepchem-2.4.0/deepchem/utils/test/test_rdkit_utils.py: 115-133
</a>
<div class="mid" id="frag1670" style="display:none"><pre>
  def test_write_molecule(self):
    current_dir = os.path.dirname(os.path.realpath(__file__))
    ligand_file = os.path.join(current_dir, "../../dock/tests/1jld_ligand.sdf")
    xyz, mol = rdkit_utils.load_molecule(
        ligand_file, calc_charges=False, add_hydrogens=False)

    with tempfile.TemporaryDirectory() as tmp:
      outfile = os.path.join(tmp, "mol.sdf")
      rdkit_utils.write_molecule(mol, outfile)

      xyz, mol2 = rdkit_utils.load_molecule(
          outfile, calc_charges=False, add_hydrogens=False)

    assert mol.GetNumAtoms() == mol2.GetNumAtoms()
    for atom_idx in range(mol.GetNumAtoms()):
      atom1 = mol.GetAtoms()[atom_idx]
      atom2 = mol.GetAtoms()[atom_idx]
      assert atom1.GetAtomicNum() == atom2.GetAtomicNum()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 122:</b> &nbsp; 11 fragments, nominal size 11 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1620')" href="javascript:;">
deepchem-2.4.0/deepchem/utils/test/test_evaluate.py: 31-44
</a>
<div class="mid" id="frag1620" style="display:none"><pre>
def test_evaluator_dc_metric():
  """Test an evaluator on a dataset."""
  X = np.random.rand(10, 5)
  y = np.random.rand(10, 1)
  dataset = dc.data.NumpyDataset(X, y)
  model = dc.models.MultitaskRegressor(1, 5)
  evaluator = Evaluator(model, dataset, [])
  metric = dc.metrics.Metric(dc.metrics.mae_score)
  multitask_scores = evaluator.compute_model_performance(metric)
  assert isinstance(multitask_scores, dict)
  assert len(multitask_scores) == 1
  assert multitask_scores['mae_score'] &gt; 0


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1629')" href="javascript:;">
deepchem-2.4.0/deepchem/utils/test/test_evaluate.py: 161-175
</a>
<div class="mid" id="frag1629" style="display:none"><pre>
def test_model_evaluate_dc_multi_metric():
  """Test an evaluator on a dataset."""
  X = np.random.rand(10, 5)
  y = np.random.rand(10, 1)
  dataset = dc.data.NumpyDataset(X, y)
  model = dc.models.MultitaskRegressor(1, 5)
  metric1 = dc.metrics.Metric(dc.metrics.mae_score)
  metric2 = dc.metrics.Metric(dc.metrics.r2_score)
  multitask_scores = model.evaluate(dataset, [metric1, metric2])
  assert isinstance(multitask_scores, dict)
  assert len(multitask_scores) == 2
  assert multitask_scores['mae_score'] &gt; 0
  assert "r2_score" in multitask_scores


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1625')" href="javascript:;">
deepchem-2.4.0/deepchem/utils/test/test_evaluate.py: 102-114
</a>
<div class="mid" id="frag1625" style="display:none"><pre>
def test_model_evaluate_dc_metric():
  """Test a model evaluate on a dataset."""
  X = np.random.rand(10, 5)
  y = np.random.rand(10, 1)
  dataset = dc.data.NumpyDataset(X, y)
  model = dc.models.MultitaskRegressor(1, 5)
  metric = dc.metrics.Metric(dc.metrics.mae_score)
  multitask_scores = model.evaluate(dataset, metric, [])
  assert isinstance(multitask_scores, dict)
  assert len(multitask_scores) == 1
  assert multitask_scores['mae_score'] &gt; 0


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1630')" href="javascript:;">
deepchem-2.4.0/deepchem/utils/test/test_evaluate.py: 176-191
</a>
<div class="mid" id="frag1630" style="display:none"><pre>
def test_generator_evaluator_dc_metric_multitask_single_point():
  """Test generator evaluator on a generator."""
  X = np.random.rand(10, 5)
  y = np.random.rand(10, 1)
  dataset = dc.data.NumpyDataset(X, y)
  model = dc.models.MultitaskRegressor(1, 5)
  generator = model.default_generator(dataset, pad_batches=False)
  evaluator = GeneratorEvaluator(model, generator, [])
  metric = dc.metrics.Metric(dc.metrics.mae_score)
  multitask_scores = evaluator.compute_model_performance(metric)
  assert isinstance(multitask_scores, dict)
  assert len(multitask_scores) == 1
  assert multitask_scores['mae_score'] &gt; 0
  assert len(multitask_scores) == 1


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1632')" href="javascript:;">
deepchem-2.4.0/deepchem/utils/test/test_evaluate.py: 208-222
</a>
<div class="mid" id="frag1632" style="display:none"><pre>
def test_generator_evaluator_dc_metric_multitask():
  """Test generator evaluator on a generator."""
  X = np.random.rand(10, 5)
  y = np.random.rand(10, 1)
  dataset = dc.data.NumpyDataset(X, y)
  model = dc.models.MultitaskRegressor(1, 5)
  generator = model.default_generator(dataset, pad_batches=False)
  evaluator = GeneratorEvaluator(model, generator, [])
  metric = dc.metrics.Metric(dc.metrics.mae_score)
  multitask_scores = evaluator.compute_model_performance(metric)
  assert isinstance(multitask_scores, dict)
  assert len(multitask_scores) == 1
  assert multitask_scores['mae_score'] &gt; 0


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1631')" href="javascript:;">
deepchem-2.4.0/deepchem/utils/test/test_evaluate.py: 192-207
</a>
<div class="mid" id="frag1631" style="display:none"><pre>
def test_evaluator_sklearn_metric():
  """Test an evaluator on a dataset."""
  X = np.random.rand(10, 5)
  y = np.random.rand(10, 1)
  dataset = dc.data.NumpyDataset(X, y)
  model = dc.models.MultitaskRegressor(1, 5)
  evaluator = Evaluator(model, dataset, [])
  multitask_scores = evaluator.compute_model_performance(
      dc.metrics.mean_absolute_error)
  assert isinstance(multitask_scores, dict)
  assert len(multitask_scores) == 1
  # Note that since no name as provided, metrics are index by order
  # given.
  assert multitask_scores['metric-1'] &gt; 0


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1627')" href="javascript:;">
deepchem-2.4.0/deepchem/utils/test/test_evaluate.py: 131-144
</a>
<div class="mid" id="frag1627" style="display:none"><pre>
def test_multitask_model_evaluate():
  """Test evaluation of a multitask metric."""
  X = np.random.rand(10, 5)
  y = np.random.rand(10, 2)
  dataset = dc.data.NumpyDataset(X, y)
  model = dc.models.MultitaskRegressor(2, 5)
  multitask_scores, all_task_scores = model.evaluate(
      dataset, dc.metrics.mean_absolute_error, per_task_metrics=True)
  assert isinstance(multitask_scores, dict)
  assert len(multitask_scores) == 1
  assert multitask_scores["metric-1"] &gt; 0
  assert isinstance(all_task_scores, dict)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1626')" href="javascript:;">
deepchem-2.4.0/deepchem/utils/test/test_evaluate.py: 115-130
</a>
<div class="mid" id="frag1626" style="display:none"><pre>
def test_multitask_model_evaluate_sklearn():
  """Test evaluation of a multitask metric."""
  X = np.random.rand(10, 5)
  y = np.random.rand(10, 2)
  dataset = dc.data.NumpyDataset(X, y)
  model = dc.models.MultitaskRegressor(2, 5)
  evaluator = Evaluator(model, dataset, [])
  multitask_scores, all_task_scores = evaluator.compute_model_performance(
      dc.metrics.mean_absolute_error, per_task_metrics=True)
  assert isinstance(multitask_scores, dict)
  assert len(multitask_scores) == 1
  assert multitask_scores['metric-1'] &gt; 0
  assert isinstance(all_task_scores, dict)
  assert len(multitask_scores) == 1


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1624')" href="javascript:;">
deepchem-2.4.0/deepchem/utils/test/test_evaluate.py: 85-101
</a>
<div class="mid" id="frag1624" style="display:none"><pre>
def test_multitask_evaluator():
  """Test evaluation of a multitask metric."""
  X = np.random.rand(10, 5)
  y = np.random.rand(10, 2, 1)
  dataset = dc.data.NumpyDataset(X, y)
  model = dc.models.MultitaskRegressor(2, 5)
  evaluator = Evaluator(model, dataset, [])
  metric = dc.metrics.Metric(dc.metrics.mae_score)
  multitask_scores, all_task_scores = evaluator.compute_model_performance(
      metric, per_task_metrics=True)
  assert isinstance(multitask_scores, dict)
  assert len(multitask_scores) == 1
  assert multitask_scores['mae_score'] &gt; 0
  assert isinstance(all_task_scores, dict)
  assert len(multitask_scores) == 1


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1635')" href="javascript:;">
deepchem-2.4.0/deepchem/utils/test/test_evaluate.py: 254-269
</a>
<div class="mid" id="frag1635" style="display:none"><pre>
def test_model_evaluate_sklearn_multi_metric():
  """Test an evaluator on a dataset."""
  X = np.random.rand(10, 5)
  y = np.random.rand(10, 1)
  dataset = dc.data.NumpyDataset(X, y)
  model = dc.models.MultitaskRegressor(1, 5)
  multitask_scores = model.evaluate(
      dataset, [dc.metrics.mean_absolute_error, dc.metrics.r2_score])
  assert isinstance(multitask_scores, dict)
  assert len(multitask_scores.keys()) == 2
  # Note that since no name as provided, metrics are index by order
  # given.
  assert multitask_scores['metric-1'] &gt; 0
  assert "metric-2" in multitask_scores


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1634')" href="javascript:;">
deepchem-2.4.0/deepchem/utils/test/test_evaluate.py: 237-253
</a>
<div class="mid" id="frag1634" style="display:none"><pre>
def test_evaluator_sklearn_multi_metric():
  """Test an evaluator on a dataset."""
  X = np.random.rand(10, 5)
  y = np.random.rand(10, 1)
  dataset = dc.data.NumpyDataset(X, y)
  model = dc.models.MultitaskRegressor(1, 5)
  evaluator = Evaluator(model, dataset, [])
  multitask_scores = evaluator.compute_model_performance(
      [dc.metrics.mean_absolute_error, dc.metrics.r2_score])
  assert isinstance(multitask_scores, dict)
  assert len(multitask_scores.keys()) == 2
  # Note that since no name as provided, metrics are index by order
  # given.
  assert multitask_scores['metric-1'] &gt; 0
  assert "metric-2" in multitask_scores


</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 123:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1621')" href="javascript:;">
deepchem-2.4.0/deepchem/utils/test/test_evaluate.py: 45-57
</a>
<div class="mid" id="frag1621" style="display:none"><pre>
def test_multiclass_classification_singletask():
  """Test multiclass classification evaluation."""
  X = np.random.rand(100, 5)
  y = np.random.randint(5, size=(100,))
  dataset = dc.data.NumpyDataset(X, y)
  model = dc.models.MultitaskClassifier(1, 5, n_classes=5)
  evaluator = Evaluator(model, dataset, [])
  multitask_scores = evaluator.compute_model_performance(
      dc.metrics.roc_auc_score, n_classes=5)
  assert len(multitask_scores) == 1
  assert multitask_scores["metric-1"] &gt;= 0


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1622')" href="javascript:;">
deepchem-2.4.0/deepchem/utils/test/test_evaluate.py: 58-72
</a>
<div class="mid" id="frag1622" style="display:none"><pre>
def test_sklearn_multiclass_classification_singletask():
  """Test multiclass classification evaluation."""
  X = np.random.rand(100, 5)
  y = np.random.randint(5, size=(100,))
  dataset = dc.data.NumpyDataset(X, y)
  rf = sklearn.ensemble.RandomForestClassifier(50)
  model = dc.models.SklearnModel(rf)
  model.fit(dataset)
  evaluator = Evaluator(model, dataset, [])
  multitask_scores = evaluator.compute_model_performance(
      dc.metrics.roc_auc_score, n_classes=5)
  assert len(multitask_scores) == 1
  assert multitask_scores["metric-1"] &gt;= 0


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 124:</b> &nbsp; 3 fragments, nominal size 13 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1636')" href="javascript:;">
deepchem-2.4.0/deepchem/utils/test/test_evaluate.py: 270-285
</a>
<div class="mid" id="frag1636" style="display:none"><pre>
def test_gc_binary_classification():
  """Test multiclass classification evaluation."""
  smiles = ["C", "CC"]
  featurizer = dc.feat.ConvMolFeaturizer()
  X = featurizer.featurize(smiles)
  y = np.random.randint(2, size=(len(smiles),))
  dataset = dc.data.NumpyDataset(X, y)
  model = dc.models.GraphConvModel(1, mode="classification")
  # TODO: Fix this case with correct thresholding
  evaluator = Evaluator(model, dataset, [])
  multitask_scores = evaluator.compute_model_performance(
      dc.metrics.accuracy_score, n_classes=2)
  assert len(multitask_scores) == 1
  assert multitask_scores["metric-1"] &gt;= 0


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1638')" href="javascript:;">
deepchem-2.4.0/deepchem/utils/test/test_evaluate.py: 304-317
</a>
<div class="mid" id="frag1638" style="display:none"><pre>
def test_gc_multiclass_classification():
  """Test multiclass classification evaluation."""
  np.random.seed(1234)
  smiles = ["C", "CC"]
  featurizer = dc.feat.ConvMolFeaturizer()
  X = featurizer.featurize(smiles)
  y = np.random.randint(5, size=(len(smiles),))
  dataset = dc.data.NumpyDataset(X, y)
  model = dc.models.GraphConvModel(1, mode="classification", n_classes=5)
  evaluator = Evaluator(model, dataset, [])
  multitask_scores = evaluator.compute_model_performance(
      dc.metrics.accuracy_score, n_classes=5)
  assert len(multitask_scores) == 1
  assert multitask_scores["metric-1"] &gt;= 0
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1637')" href="javascript:;">
deepchem-2.4.0/deepchem/utils/test/test_evaluate.py: 286-303
</a>
<div class="mid" id="frag1637" style="display:none"><pre>
def test_gc_binary_kappa_classification():
  """Test multiclass classification evaluation."""
  np.random.seed(1234)
  smiles = ["C", "CC", "CO", "CCC", "CCCC"]
  featurizer = dc.feat.ConvMolFeaturizer()
  X = featurizer.featurize(smiles)
  y = np.random.randint(2, size=(len(smiles),))
  dataset = dc.data.NumpyDataset(X, y)
  model = dc.models.GraphConvModel(1, mode="classification")
  # TODO: Fix this case with correct thresholding
  evaluator = Evaluator(model, dataset, [])
  multitask_scores = evaluator.compute_model_performance(
      dc.metrics.kappa_score, n_classes=2)
  assert len(multitask_scores) == 1
  assert multitask_scores["metric-1"] &lt;= 1
  assert multitask_scores["metric-1"] &gt;= -1


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 125:</b> &nbsp; 2 fragments, nominal size 19 lines, similarity 94%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1666')" href="javascript:;">
deepchem-2.4.0/deepchem/utils/test/test_rdkit_utils.py: 50-70
</a>
<div class="mid" id="frag1666" style="display:none"><pre>
  def test_add_hydrogens_to_mol(self):
    current_dir = os.path.dirname(os.path.realpath(__file__))
    ligand_file = os.path.join(current_dir, "../../dock/tests/1jld_ligand.sdf")
    xyz, mol = rdkit_utils.load_molecule(
        ligand_file, calc_charges=False, add_hydrogens=False)
    original_hydrogen_count = 0
    for atom_idx in range(mol.GetNumAtoms()):
      atom = mol.GetAtoms()[atom_idx]
      if atom.GetAtomicNum() == 1:
        original_hydrogen_count += 1

    assert mol is not None
    mol = rdkit_utils.add_hydrogens_to_mol(mol, is_protein=False)
    assert mol is not None
    after_hydrogen_count = 0
    for atom_idx in range(mol.GetNumAtoms()):
      atom = mol.GetAtoms()[atom_idx]
      if atom.GetAtomicNum() == 1:
        after_hydrogen_count += 1
    assert after_hydrogen_count &gt;= original_hydrogen_count

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1667')" href="javascript:;">
deepchem-2.4.0/deepchem/utils/test/test_rdkit_utils.py: 71-91
</a>
<div class="mid" id="frag1667" style="display:none"><pre>
  def test_apply_pdbfixer(self):
    current_dir = os.path.dirname(os.path.realpath(__file__))
    ligand_file = os.path.join(current_dir, "../../dock/tests/1jld_ligand.sdf")
    xyz, mol = rdkit_utils.load_molecule(
        ligand_file, calc_charges=False, add_hydrogens=False)
    original_hydrogen_count = 0
    for atom_idx in range(mol.GetNumAtoms()):
      atom = mol.GetAtoms()[atom_idx]
      if atom.GetAtomicNum() == 1:
        original_hydrogen_count += 1

    assert mol is not None
    mol = rdkit_utils.apply_pdbfixer(mol, hydrogenate=True, is_protein=False)
    assert mol is not None
    after_hydrogen_count = 0
    for atom_idx in range(mol.GetNumAtoms()):
      atom = mol.GetAtoms()[atom_idx]
      if atom.GetAtomicNum() == 1:
        after_hydrogen_count += 1
    assert after_hydrogen_count &gt;= original_hydrogen_count

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 126:</b> &nbsp; 2 fragments, nominal size 22 lines, similarity 82%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1677')" href="javascript:;">
deepchem-2.4.0/deepchem/utils/test/test_voxel_utils.py: 31-51
</a>
<div class="mid" id="frag1677" style="display:none"><pre>
  def test_voxelize_convert_atom(self):
    N = 5
    coordinates = np.random.rand(N, 3)
    box_width = 16
    voxel_width = 1
    voxels_per_edge = int(box_width / voxel_width)
    get_voxels = voxel_utils.convert_atom_to_voxel
    hash_function = hash_utils.hash_ecfp
    feature_dict = {1: "C", 2: "CC"}
    nb_channel = 16
    features = voxel_utils.voxelize(
        get_voxels,
        coordinates,
        box_width,
        voxel_width,
        hash_function,
        feature_dict,
        nb_channel=nb_channel)
    assert features.shape == (voxels_per_edge, voxels_per_edge, voxels_per_edge,
                              nb_channel)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1678')" href="javascript:;">
deepchem-2.4.0/deepchem/utils/test/test_voxel_utils.py: 52-74
</a>
<div class="mid" id="frag1678" style="display:none"><pre>
  def test_voxelize_convert_atom_pair(self):
    N = 5
    M = 6
    coordinates1 = np.random.rand(N, 3)
    coordinates2 = np.random.rand(M, 3)
    coordinates = [coordinates1, coordinates2]
    box_width = 16
    voxel_width = 1
    voxels_per_edge = int(box_width / voxel_width)
    get_voxels = voxel_utils.convert_atom_pair_to_voxel
    hash_function = hash_utils.hash_ecfp_pair
    feature_dict = {(1, 2): ("C", "O"), (2, 3): ("CC", "OH")}
    nb_channel = 16
    features = voxel_utils.voxelize(
        get_voxels,
        coordinates,
        box_width,
        voxel_width,
        hash_function,
        feature_dict,
        nb_channel=nb_channel)
    assert features.shape == (voxels_per_edge, voxels_per_edge, voxels_per_edge,
                              nb_channel)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 127:</b> &nbsp; 3 fragments, nominal size 10 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1684')" href="javascript:;">
deepchem-2.4.0/deepchem/trans/tests/test_cdf_transform.py: 7-21
</a>
<div class="mid" id="frag1684" style="display:none"><pre>
def load_gaussian_cdf_data():
  """Load example with numbers sampled from Gaussian normal distribution.
     Each feature and task is a column of values that is sampled
     from a normal distribution of mean 0, stdev 1."""
  current_dir = os.path.dirname(os.path.abspath(__file__))
  features = ["feat0", "feat1"]
  featurizer = dc.feat.UserDefinedFeaturizer(features)
  tasks = ["task0", "task1"]
  input_file = os.path.join(current_dir,
                            "../../models/tests/gaussian_cdf_example.csv")
  loader = dc.data.UserCSVLoader(
      tasks=tasks, featurizer=featurizer, id_field="id")
  return loader.create_dataset(input_file)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1731')" href="javascript:;">
deepchem-2.4.0/deepchem/trans/tests/test_power.py: 6-20
</a>
<div class="mid" id="frag1731" style="display:none"><pre>
def load_gaussian_cdf_data():
  """Load example with numbers sampled from Gaussian normal distribution.
     Each feature and task is a column of values that is sampled
     from a normal distribution of mean 0, stdev 1."""
  current_dir = os.path.dirname(os.path.abspath(__file__))
  features = ["feat0", "feat1"]
  featurizer = dc.feat.UserDefinedFeaturizer(features)
  tasks = ["task0", "task1"]
  input_file = os.path.join(current_dir,
                            "../../models/tests/gaussian_cdf_example.csv")
  loader = dc.data.UserCSVLoader(
      tasks=tasks, featurizer=featurizer, id_field="id")
  return loader.create_dataset(input_file)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1715')" href="javascript:;">
deepchem-2.4.0/deepchem/trans/tests/test_log_transform.py: 7-19
</a>
<div class="mid" id="frag1715" style="display:none"><pre>
def load_feat_multitask_data():
  """Load example with numerical features, tasks."""
  current_dir = os.path.dirname(os.path.abspath(__file__))
  features = ["feat0", "feat1", "feat2", "feat3", "feat4", "feat5"]
  featurizer = dc.feat.UserDefinedFeaturizer(features)
  tasks = ["task0", "task1", "task2", "task3", "task4", "task5"]
  input_file = os.path.join(current_dir,
                            "../../models/tests/feat_multitask_example.csv")
  loader = dc.data.UserCSVLoader(
      tasks=tasks, featurizer=featurizer, id_field="id")
  return loader.create_dataset(input_file)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 128:</b> &nbsp; 2 fragments, nominal size 19 lines, similarity 85%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1685')" href="javascript:;">
deepchem-2.4.0/deepchem/trans/tests/test_cdf_transform.py: 22-47
</a>
<div class="mid" id="frag1685" style="display:none"><pre>
def test_cdf_X_transformer():
  """Test CDF transformer on Gaussian normal dataset."""
  target = np.array(np.transpose(np.linspace(0., 1., 1001)))
  target = np.transpose(np.array(np.append([target], [target], axis=0)))
  gaussian_dataset = load_gaussian_cdf_data()
  bins = 1001
  cdf_transformer = dc.trans.CDFTransformer(
      transform_X=True, dataset=gaussian_dataset, bins=bins)
  _, y, w, ids = (gaussian_dataset.X, gaussian_dataset.y, gaussian_dataset.w,
                  gaussian_dataset.ids)
  gaussian_dataset = cdf_transformer.transform(gaussian_dataset)
  X_t, y_t, w_t, ids_t = (gaussian_dataset.X, gaussian_dataset.y,
                          gaussian_dataset.w, gaussian_dataset.ids)

  # Check ids are unchanged.
  for id_elt, id_t_elt in zip(ids, ids_t):
    assert id_elt == id_t_elt
  # Check y is unchanged since this is an X transformer
  np.testing.assert_allclose(y, y_t)
  # Check w is unchanged since this is an X transformer
  np.testing.assert_allclose(w, w_t)
  # Check X is now holding the proper values when sorted.
  sorted = np.sort(X_t, axis=0)
  np.testing.assert_allclose(sorted, target)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1687')" href="javascript:;">
deepchem-2.4.0/deepchem/trans/tests/test_cdf_transform.py: 61-88
</a>
<div class="mid" id="frag1687" style="display:none"><pre>
def test_cdf_y_transformer():
  """Test CDF transformer on Gaussian normal dataset."""
  target = np.array(np.transpose(np.linspace(0., 1., 1001)))
  target = np.transpose(np.array(np.append([target], [target], axis=0)))
  gaussian_dataset = load_gaussian_cdf_data()
  bins = 1001
  cdf_transformer = dc.trans.CDFTransformer(
      transform_y=True, dataset=gaussian_dataset, bins=bins)
  X, y, w, ids = (gaussian_dataset.X, gaussian_dataset.y, gaussian_dataset.w,
                  gaussian_dataset.ids)
  gaussian_dataset = cdf_transformer.transform(gaussian_dataset, bins=bins)
  X_t, y_t, w_t, ids_t = (gaussian_dataset.X, gaussian_dataset.y,
                          gaussian_dataset.w, gaussian_dataset.ids)

  # Check ids are unchanged.
  for id_elt, id_t_elt in zip(ids, ids_t):
    assert id_elt == id_t_elt
  # Check X is unchanged since this is an y transformer
  np.testing.assert_allclose(X, X_t)
  # Check w is unchanged since this is an y transformer
  np.testing.assert_allclose(w, w_t)
  # Check y is now holding the proper values when sorted.
  sorted = np.sort(y_t, axis=0)
  np.testing.assert_allclose(sorted, target)

  # Check that untransform does the right thing.
  y_restored = cdf_transformer.untransform(y_t)
  assert np.max(y_restored - y) &lt; 1e-5
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 129:</b> &nbsp; 4 fragments, nominal size 19 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1688')" href="javascript:;">
deepchem-2.4.0/deepchem/trans/tests/test_duplicate_balancing.py: 6-32
</a>
<div class="mid" id="frag1688" style="display:none"><pre>
def test_binary_1d():
  """Test balancing transformer on single-task dataset without explicit task dimension."""
  n_samples = 6
  n_features = 3
  np.random.seed(123)
  X = np.random.rand(n_samples, n_features)
  y = np.array([1, 1, 0, 0, 0, 0])
  w = np.ones((n_samples,))
  dataset = dc.data.NumpyDataset(X, y, w)

  duplicator = dc.trans.DuplicateBalancingTransformer(dataset=dataset)
  dataset = duplicator.transform(dataset)
  # Check that we have length 8 now with duplication
  assert len(dataset) == 8
  X_t, y_t, w_t, ids_t = (dataset.X, dataset.y, dataset.w, dataset.ids)
  # Check shapes
  assert X_t.shape == (8, n_features)
  assert y_t.shape == (8,)
  assert w_t.shape == (8,)
  assert ids_t.shape == (8,)
  # Check that we have 4 positives and 4 negatives
  assert np.sum(y_t == 0) == 4
  assert np.sum(y_t == 1) == 4
  # Check that sum of 0s equals sum of 1s in transformed for each task
  assert np.isclose(np.sum(w_t[y_t == 0]), np.sum(w_t[y_t == 1]))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1689')" href="javascript:;">
deepchem-2.4.0/deepchem/trans/tests/test_duplicate_balancing.py: 33-60
</a>
<div class="mid" id="frag1689" style="display:none"><pre>
def test_binary_weighted_1d():
  """Test balancing transformer on a weighted single-task dataset without explicit task dimension."""
  n_samples = 6
  n_features = 3
  np.random.seed(123)
  X = np.random.rand(n_samples, n_features)
  # Note that nothing should change in this dataset since weights balance!
  y = np.array([1, 1, 0, 0, 0, 0])
  w = np.array([2, 2, 1, 1, 1, 1])
  dataset = dc.data.NumpyDataset(X, y, w)

  duplicator = dc.trans.DuplicateBalancingTransformer(dataset=dataset)
  dataset = duplicator.transform(dataset)
  # Check that still we have length 6
  assert len(dataset) == 6
  X_t, y_t, w_t, ids_t = (dataset.X, dataset.y, dataset.w, dataset.ids)
  # Check shapes
  assert X_t.shape == (6, n_features)
  assert y_t.shape == (6,)
  assert w_t.shape == (6,)
  assert ids_t.shape == (6,)
  # Check that we have 2 positives and 4 negatives
  assert np.sum(y_t == 0) == 4
  assert np.sum(y_t == 1) == 2
  # Check that sum of 0s equals sum of 1s in transformed for each task
  assert np.isclose(np.sum(w_t[y_t == 0]), np.sum(w_t[y_t == 1]))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1692')" href="javascript:;">
deepchem-2.4.0/deepchem/trans/tests/test_duplicate_balancing.py: 125-151
</a>
<div class="mid" id="frag1692" style="display:none"><pre>
def test_transform_to_directory():
  """Test that output can be written to a directory."""
  n_samples = 10
  n_features = 3
  np.random.seed(123)
  X = np.random.rand(n_samples, n_features)
  # Note class imbalance. This will round to 2x duplication for 1
  y = np.array([1, 1, 1, 0, 0, 0, 0, 0, 0, 0])
  w = np.ones((n_samples,))
  dataset = dc.data.NumpyDataset(X, y, w)

  duplicator = dc.trans.DuplicateBalancingTransformer(dataset=dataset)
  with tempfile.TemporaryDirectory() as tmpdirname:
    dataset = duplicator.transform(dataset, out_dir=tmpdirname)
    balanced_dataset = dc.data.DiskDataset(tmpdirname)
    X_t, y_t, w_t, ids_t = (balanced_dataset.X, balanced_dataset.y,
                            balanced_dataset.w, balanced_dataset.ids)
    # Check that we have length 13 now with duplication
    assert len(balanced_dataset) == 13
  # Check shapes
  assert X_t.shape == (13, n_features)
  assert y_t.shape == (13,)
  assert w_t.shape == (13,)
  assert ids_t.shape == (13,)
  # Check that we have 6 positives and 7 negatives
  assert np.sum(y_t == 0) == 7
  assert np.sum(y_t == 1) == 6
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1690')" href="javascript:;">
deepchem-2.4.0/deepchem/trans/tests/test_duplicate_balancing.py: 61-89
</a>
<div class="mid" id="frag1690" style="display:none"><pre>
def test_binary_singletask():
  """Test duplicate balancing transformer on single-task dataset."""
  n_samples = 6
  n_features = 3
  n_tasks = 1
  np.random.seed(123)
  X = np.random.rand(n_samples, n_features)
  y = np.reshape(np.array([1, 1, 0, 0, 0, 0]), (n_samples, n_tasks))
  w = np.ones((n_samples, n_tasks))
  dataset = dc.data.NumpyDataset(X, y, w)

  duplicator = dc.trans.DuplicateBalancingTransformer(dataset=dataset)
  dataset = duplicator.transform(dataset)
  X_t, y_t, w_t, ids_t = (dataset.X, dataset.y, dataset.w, dataset.ids)
  # Check that we have length 8 now with duplication
  assert len(dataset) == 8
  X_t, y_t, w_t, ids_t = (dataset.X, dataset.y, dataset.w, dataset.ids)
  # Check shapes
  assert X_t.shape == (8, n_features)
  assert y_t.shape == (8,)
  assert w_t.shape == (8,)
  assert ids_t.shape == (8,)
  # Check that we have 4 positives and 4 negatives
  assert np.sum(y_t == 0) == 4
  assert np.sum(y_t == 1) == 4
  # Check that sum of 0s equals sum of 1s in transformed for each task
  assert np.isclose(np.sum(w_t[y_t == 0]), np.sum(w_t[y_t == 1]))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 130:</b> &nbsp; 5 fragments, nominal size 16 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1708')" href="javascript:;">
deepchem-2.4.0/deepchem/trans/tests/test_normalization.py: 44-68
</a>
<div class="mid" id="frag1708" style="display:none"><pre>
def test_y_normalization_transformer():
  """Tests normalization transformer."""
  solubility_dataset = load_solubility_data()
  normalization_transformer = dc.trans.NormalizationTransformer(
      transform_y=True, dataset=solubility_dataset)
  X, y, w, ids = (solubility_dataset.X, solubility_dataset.y,
                  solubility_dataset.w, solubility_dataset.ids)
  solubility_dataset = normalization_transformer.transform(solubility_dataset)
  X_t, y_t, w_t, ids_t = (solubility_dataset.X, solubility_dataset.y,
                          solubility_dataset.w, solubility_dataset.ids)
  # Check ids are unchanged.
  for id_elt, id_t_elt in zip(ids, ids_t):
    assert id_elt == id_t_elt
  # Check X is unchanged since this is a y transformer
  np.testing.assert_allclose(X, X_t)
  # Check w is unchanged since this is a y transformer
  np.testing.assert_allclose(w, w_t)
  # Check that y_t has zero mean, unit std.
  assert np.isclose(y_t.mean(), 0.)
  assert np.isclose(y_t.std(), 1.)

  # Check that untransform does the right thing.
  np.testing.assert_allclose(normalization_transformer.untransform(y_t), y)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1711')" href="javascript:;">
deepchem-2.4.0/deepchem/trans/tests/test_minmax.py: 18-46
</a>
<div class="mid" id="frag1711" style="display:none"><pre>
def test_y_minmax_transformer():
  """Tests MinMax transformer."""
  solubility_dataset = load_solubility_data()
  minmax_transformer = dc.trans.MinMaxTransformer(
      transform_y=True, dataset=solubility_dataset)
  X, y, w, ids = (solubility_dataset.X, solubility_dataset.y,
                  solubility_dataset.w, solubility_dataset.ids)
  solubility_dataset = minmax_transformer.transform(solubility_dataset)
  X_t, y_t, w_t, ids_t = (solubility_dataset.X, solubility_dataset.y,
                          solubility_dataset.w, solubility_dataset.ids)

  # Check ids are unchanged before and after transformation
  for id_elt, id_t_elt in zip(ids, ids_t):
    assert id_elt == id_t_elt

  # Check X is unchanged since transform_y is true
  np.testing.assert_allclose(X, X_t)
  # Check w is unchanged since transform_y is true
  np.testing.assert_allclose(w, w_t)

  # Check minimum and maximum values of transformed y are 0 and 1
  np.testing.assert_allclose(y_t.min(), 0.)
  np.testing.assert_allclose(y_t.max(), 1.)

  # Check untransform works correctly
  y_restored = minmax_transformer.untransform(y_t)
  assert np.max(y_restored - y) &lt; 1e-5


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1717')" href="javascript:;">
deepchem-2.4.0/deepchem/trans/tests/test_log_transform.py: 32-56
</a>
<div class="mid" id="frag1717" style="display:none"><pre>
def test_y_log_transformer():
  """Tests logarithmic data transformer."""
  solubility_dataset = load_solubility_data()
  log_transformer = dc.trans.LogTransformer(
      transform_y=True, dataset=solubility_dataset)
  X, y, w, ids = (solubility_dataset.X, solubility_dataset.y,
                  solubility_dataset.w, solubility_dataset.ids)
  solubility_dataset = log_transformer.transform(solubility_dataset)
  X_t, y_t, w_t, ids_t = (solubility_dataset.X, solubility_dataset.y,
                          solubility_dataset.w, solubility_dataset.ids)

  # Check ids are unchanged.
  for id_elt, id_t_elt in zip(ids, ids_t):
    assert id_elt == id_t_elt
  # Check X is unchanged since this is a y transformer
  np.testing.assert_allclose(X, X_t)
  # Check w is unchanged since this is a y transformer
  np.testing.assert_allclose(w, w_t)
  # Check y is now a logarithmic version of itself
  np.testing.assert_allclose(y_t, np.log(y + 1))

  # Check that untransform does the right thing.
  np.testing.assert_allclose(log_transformer.untransform(y_t), y)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1713')" href="javascript:;">
deepchem-2.4.0/deepchem/trans/tests/test_minmax.py: 83-107
</a>
<div class="mid" id="frag1713" style="display:none"><pre>
def test_X_minmax_transformer():
  solubility_dataset = load_solubility_data()
  minmax_transformer = dc.trans.MinMaxTransformer(
      transform_X=True, dataset=solubility_dataset)
  X, y, w, ids = (solubility_dataset.X, solubility_dataset.y,
                  solubility_dataset.w, solubility_dataset.ids)
  solubility_dataset = minmax_transformer.transform(solubility_dataset)
  X_t, y_t, w_t, ids_t = (solubility_dataset.X, solubility_dataset.y,
                          solubility_dataset.w, solubility_dataset.ids)

  # Check ids are unchanged before and after transformation
  for id_elt, id_t_elt in zip(ids, ids_t):
    assert id_elt == id_t_elt

  # Check X is unchanged since transform_y is true
  np.testing.assert_allclose(y, y_t)
  # Check w is unchanged since transform_y is true
  np.testing.assert_allclose(w, w_t)

  # Check minimum and maximum values of transformed y are 0 and 1
  np.testing.assert_allclose(X_t.min(), 0.)
  np.testing.assert_allclose(X_t.max(), 1.)

  # Check untransform works correctly
  np.testing.assert_allclose(minmax_transformer.untransform(X_t), X)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1718')" href="javascript:;">
deepchem-2.4.0/deepchem/trans/tests/test_log_transform.py: 57-81
</a>
<div class="mid" id="frag1718" style="display:none"><pre>
def test_X_log_transformer():
  """Tests logarithmic data transformer."""
  solubility_dataset = load_solubility_data()
  log_transformer = dc.trans.LogTransformer(
      transform_X=True, dataset=solubility_dataset)
  X, y, w, ids = (solubility_dataset.X, solubility_dataset.y,
                  solubility_dataset.w, solubility_dataset.ids)
  solubility_dataset = log_transformer.transform(solubility_dataset)
  X_t, y_t, w_t, ids_t = (solubility_dataset.X, solubility_dataset.y,
                          solubility_dataset.w, solubility_dataset.ids)

  # Check ids are unchanged.
  for id_elt, id_t_elt in zip(ids, ids_t):
    assert id_elt == id_t_elt
  # Check y is unchanged since this is a X transformer
  np.testing.assert_allclose(y, y_t)
  # Check w is unchanged since this is a y transformer
  np.testing.assert_allclose(w, w_t)
  # Check y is now a logarithmic version of itself
  np.testing.assert_allclose(X_t, np.log(X + 1))

  # Check that untransform does the right thing.
  np.testing.assert_allclose(log_transformer.untransform(X_t), X)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 131:</b> &nbsp; 2 fragments, nominal size 26 lines, similarity 96%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1719')" href="javascript:;">
deepchem-2.4.0/deepchem/trans/tests/test_log_transform.py: 82-117
</a>
<div class="mid" id="frag1719" style="display:none"><pre>
def test_y_log_transformer_select():
  """Tests logarithmic data transformer with selection."""
  current_dir = os.path.dirname(os.path.abspath(__file__))
  multitask_dataset = load_feat_multitask_data()
  dfe = pd.read_csv(
      os.path.join(current_dir,
                   "../../models/tests/feat_multitask_example.csv"))
  tid = []
  tasklist = ["task0", "task3", "task4", "task5"]
  first_task = "task0"
  for task in tasklist:
    tiid = dfe.columns.get_loc(task) - dfe.columns.get_loc(first_task)
    tid = np.concatenate((tid, np.array([tiid])))
  tasks = tid.astype(int)
  log_transformer = dc.trans.LogTransformer(
      transform_y=True, tasks=tasks, dataset=multitask_dataset)
  X, y, w, ids = (multitask_dataset.X, multitask_dataset.y, multitask_dataset.w,
                  multitask_dataset.ids)
  multitask_dataset = log_transformer.transform(multitask_dataset)
  X_t, y_t, w_t, ids_t = (multitask_dataset.X, multitask_dataset.y,
                          multitask_dataset.w, multitask_dataset.ids)

  # Check ids are unchanged.
  for id_elt, id_t_elt in zip(ids, ids_t):
    assert id_elt == id_t_elt
  # Check X is unchanged since this is a y transformer
  np.testing.assert_allclose(X, X_t)
  # Check w is unchanged since this is a y transformer
  np.testing.assert_allclose(w, w_t)
  # Check y is now a logarithmic version of itself
  np.testing.assert_allclose(y_t[:, tasks], np.log(y[:, tasks] + 1))

  # Check that untransform does the right thing.
  np.testing.assert_allclose(log_transformer.untransform(y_t), y)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1720')" href="javascript:;">
deepchem-2.4.0/deepchem/trans/tests/test_log_transform.py: 118-151
</a>
<div class="mid" id="frag1720" style="display:none"><pre>
def test_X_log_transformer_select():
  # Tests logarithmic data transformer with selection.
  current_dir = os.path.dirname(os.path.abspath(__file__))
  multitask_dataset = load_feat_multitask_data()
  dfe = pd.read_csv(
      os.path.join(current_dir,
                   "../../models/tests/feat_multitask_example.csv"))
  fid = []
  featurelist = ["feat0", "feat1", "feat2", "feat3", "feat5"]
  first_feature = "feat0"
  for feature in featurelist:
    fiid = dfe.columns.get_loc(feature) - dfe.columns.get_loc(first_feature)
    fid = np.concatenate((fid, np.array([fiid])))
  features = fid.astype(int)
  log_transformer = dc.trans.LogTransformer(
      transform_X=True, features=features, dataset=multitask_dataset)
  X, y, w, ids = (multitask_dataset.X, multitask_dataset.y, multitask_dataset.w,
                  multitask_dataset.ids)
  multitask_dataset = log_transformer.transform(multitask_dataset)
  X_t, y_t, w_t, ids_t = (multitask_dataset.X, multitask_dataset.y,
                          multitask_dataset.w, multitask_dataset.ids)

  # Check ids are unchanged.
  for id_elt, id_t_elt in zip(ids, ids_t):
    assert id_elt == id_t_elt
  # Check y is unchanged since this is a X transformer
  np.testing.assert_allclose(y, y_t)
  # Check w is unchanged since this is a y transformer
  np.testing.assert_allclose(w, w_t)
  # Check y is now a logarithmic version of itself
  np.testing.assert_allclose(X_t[:, features], np.log(X[:, features] + 1))

  # Check that untransform does the right thing.
  np.testing.assert_allclose(log_transformer.untransform(X_t), X)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 132:</b> &nbsp; 5 fragments, nominal size 25 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1721')" href="javascript:;">
deepchem-2.4.0/deepchem/trans/tests/test_balancing.py: 9-40
</a>
<div class="mid" id="frag1721" style="display:none"><pre>
def test_binary_1d():
  """Test balancing transformer on single-task dataset without explicit task dimension."""
  n_samples = 20
  n_features = 3
  n_classes = 2
  np.random.seed(123)
  ids = np.arange(n_samples)
  X = np.random.rand(n_samples, n_features)
  y = np.random.randint(n_classes, size=(n_samples,))
  w = np.ones((n_samples,))
  dataset = dc.data.NumpyDataset(X, y, w)

  balancing_transformer = dc.trans.BalancingTransformer(dataset=dataset)
  dataset = balancing_transformer.transform(dataset)
  X_t, y_t, w_t, ids_t = (dataset.X, dataset.y, dataset.w, dataset.ids)
  # Check ids are unchanged.
  for id_elt, id_t_elt in zip(ids, ids_t):
    assert id_elt == id_t_elt
  # Check X is unchanged since this is a w transformer
  np.testing.assert_allclose(X, X_t)
  # Check y is unchanged since this is a w transformer
  np.testing.assert_allclose(y, y_t)
  y_task = y_t
  w_task = w_t
  w_orig_task = w
  # Assert that entries with zero weight retain zero weight
  np.testing.assert_allclose(w_task[w_orig_task == 0],
                             np.zeros_like(w_task[w_orig_task == 0]))
  # Check that sum of 0s equals sum of 1s in transformed for each task
  assert np.isclose(np.sum(w_task[y_task == 0]), np.sum(w_task[y_task == 1]))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1725')" href="javascript:;">
deepchem-2.4.0/deepchem/trans/tests/test_balancing.py: 142-176
</a>
<div class="mid" id="frag1725" style="display:none"><pre>
def test_transform_to_directory():
  """Test that output can be written to a directory."""
  n_samples = 20
  n_features = 3
  n_classes = 2
  np.random.seed(123)
  ids = np.arange(n_samples)
  X = np.random.rand(n_samples, n_features)
  y = np.random.randint(n_classes, size=(n_samples,))
  w = np.ones((n_samples,))
  dataset = dc.data.NumpyDataset(X, y, w)

  balancing_transformer = dc.trans.BalancingTransformer(dataset=dataset)
  with tempfile.TemporaryDirectory() as tmpdirname:
    dataset = balancing_transformer.transform(dataset, out_dir=tmpdirname)
    balanced_dataset = dc.data.DiskDataset(tmpdirname)
    X_t, y_t, w_t, ids_t = (balanced_dataset.X, balanced_dataset.y,
                            balanced_dataset.w, balanced_dataset.ids)
  # Check ids are unchanged.
  for id_elt, id_t_elt in zip(ids, ids_t):
    assert id_elt == id_t_elt
  # Check X is unchanged since this is a w transformer
  np.testing.assert_allclose(X, X_t)
  # Check y is unchanged since this is a w transformer
  np.testing.assert_allclose(y, y_t)
  y_task = y_t
  w_task = w_t
  w_orig_task = w
  # Assert that entries with zero weight retain zero weight
  np.testing.assert_allclose(w_task[w_orig_task == 0],
                             np.zeros_like(w_task[w_orig_task == 0]))
  # Check that sum of 0s equals sum of 1s in transformed for each task
  assert np.isclose(np.sum(w_task[y_task == 0]), np.sum(w_task[y_task == 1]))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1722')" href="javascript:;">
deepchem-2.4.0/deepchem/trans/tests/test_balancing.py: 41-74
</a>
<div class="mid" id="frag1722" style="display:none"><pre>
def test_binary_singletask():
  """Test balancing transformer on single-task dataset."""
  n_samples = 20
  n_features = 3
  n_tasks = 1
  n_classes = 2
  np.random.seed(123)
  ids = np.arange(n_samples)
  X = np.random.rand(n_samples, n_features)
  y = np.random.randint(n_classes, size=(n_samples, n_tasks))
  w = np.ones((n_samples, n_tasks))
  dataset = dc.data.NumpyDataset(X, y, w)

  balancing_transformer = dc.trans.BalancingTransformer(dataset=dataset)
  dataset = balancing_transformer.transform(dataset)
  X_t, y_t, w_t, ids_t = (dataset.X, dataset.y, dataset.w, dataset.ids)
  # Check ids are unchanged.
  for id_elt, id_t_elt in zip(ids, ids_t):
    assert id_elt == id_t_elt
  # Check X is unchanged since this is a w transformer
  np.testing.assert_allclose(X, X_t)
  # Check y is unchanged since this is a w transformer
  np.testing.assert_allclose(y, y_t)
  for ind, task in enumerate(dataset.get_task_names()):
    y_task = y_t[:, ind]
    w_task = w_t[:, ind]
    w_orig_task = w[:, ind]
    # Assert that entries with zero weight retain zero weight
    np.testing.assert_allclose(w_task[w_orig_task == 0],
                               np.zeros_like(w_task[w_orig_task == 0]))
    # Check that sum of 0s equals sum of 1s in transformed for each task
    assert np.isclose(np.sum(w_task[y_task == 0]), np.sum(w_task[y_task == 1]))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1723')" href="javascript:;">
deepchem-2.4.0/deepchem/trans/tests/test_balancing.py: 75-108
</a>
<div class="mid" id="frag1723" style="display:none"><pre>
def test_binary_multitask():
  """Test balancing transformer on multitask dataset."""
  n_samples = 10
  n_features = 3
  n_tasks = 5
  n_classes = 2
  ids = np.arange(n_samples)
  X = np.random.rand(n_samples, n_features)
  y = np.random.randint(n_classes, size=(n_samples, n_tasks))
  w = np.ones((n_samples, n_tasks))
  multitask_dataset = dc.data.NumpyDataset(X, y, w)
  balancing_transformer = dc.trans.BalancingTransformer(
      dataset=multitask_dataset)
  multitask_dataset = balancing_transformer.transform(multitask_dataset)
  X_t, y_t, w_t, ids_t = (multitask_dataset.X, multitask_dataset.y,
                          multitask_dataset.w, multitask_dataset.ids)
  # Check ids are unchanged.
  for id_elt, id_t_elt in zip(ids, ids_t):
    assert id_elt == id_t_elt
  # Check X is unchanged since this is a w transformer
  np.testing.assert_allclose(X, X_t)
  # Check y is unchanged since this is a w transformer
  np.testing.assert_allclose(y, y_t)
  for ind, task in enumerate(multitask_dataset.get_task_names()):
    y_task = y_t[:, ind]
    w_task = w_t[:, ind]
    w_orig_task = w[:, ind]
    # Assert that entries with zero weight retain zero weight
    np.testing.assert_allclose(w_task[w_orig_task == 0],
                               np.zeros_like(w_task[w_orig_task == 0]))
    # Check that sum of 0s equals sum of 1s in transformed for each task
    assert np.isclose(np.sum(w_task[y_task == 0]), np.sum(w_task[y_task == 1]))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1724')" href="javascript:;">
deepchem-2.4.0/deepchem/trans/tests/test_balancing.py: 109-141
</a>
<div class="mid" id="frag1724" style="display:none"><pre>
def test_multiclass_singletask():
  """Test balancing transformer on single-task dataset."""
  n_samples = 50
  n_features = 3
  n_tasks = 1
  n_classes = 5
  ids = np.arange(n_samples)
  X = np.random.rand(n_samples, n_features)
  y = np.random.randint(n_classes, size=(n_samples, n_tasks))
  w = np.ones((n_samples, n_tasks))
  dataset = dc.data.NumpyDataset(X, y, w)

  balancing_transformer = dc.trans.BalancingTransformer(dataset=dataset)
  dataset = balancing_transformer.transform(dataset)
  X_t, y_t, w_t, ids_t = (dataset.X, dataset.y, dataset.w, dataset.ids)
  # Check ids are unchanged.
  for id_elt, id_t_elt in zip(ids, ids_t):
    assert id_elt == id_t_elt
  # Check X is unchanged since this is a w transformer
  np.testing.assert_allclose(X, X_t)
  # Check y is unchanged since this is a w transformer
  np.testing.assert_allclose(y, y_t)
  for ind, task in enumerate(dataset.get_task_names()):
    y_task = y_t[:, ind]
    w_task = w_t[:, ind]
    # Check that sum of 0s equals sum of 1s in transformed for each task
    for i, j in itertools.product(range(n_classes), range(n_classes)):
      if i == j:
        continue
      assert np.isclose(
          np.sum(w_task[y_task == i]), np.sum(w_task[y_task == j]))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 133:</b> &nbsp; 2 fragments, nominal size 20 lines, similarity 95%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1728')" href="javascript:;">
deepchem-2.4.0/deepchem/trans/tests/test_clipping.py: 5-31
</a>
<div class="mid" id="frag1728" style="display:none"><pre>
def test_clipping_X_transformer():
  """Test clipping transformer on X of singletask dataset."""
  n_samples = 10
  n_features = 3
  n_tasks = 1
  ids = np.arange(n_samples)
  X = np.ones((n_samples, n_features))
  target = 5. * X
  X *= 6.
  y = np.zeros((n_samples, n_tasks))
  w = np.ones((n_samples, n_tasks))
  dataset = dc.data.NumpyDataset(X, y, w, ids)
  transformer = dc.trans.ClippingTransformer(transform_X=True, x_max=5.)
  clipped_dataset = transformer.transform(dataset)
  X_t, y_t, w_t, ids_t = (clipped_dataset.X, clipped_dataset.y,
                          clipped_dataset.w, clipped_dataset.ids)
  # Check ids are unchanged.
  for id_elt, id_t_elt in zip(ids, ids_t):
    assert id_elt == id_t_elt
  # Check y is unchanged since this is an X transformer
  np.testing.assert_allclose(y, y_t)
  # Check w is unchanged since this is an X transformer
  np.testing.assert_allclose(w, w_t)
  # Check X is now holding the proper values when sorted.
  np.testing.assert_allclose(X_t, target)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1729')" href="javascript:;">
deepchem-2.4.0/deepchem/trans/tests/test_clipping.py: 32-56
</a>
<div class="mid" id="frag1729" style="display:none"><pre>
def test_clipping_y_transformer():
  """Test clipping transformer on y of singletask dataset."""
  n_samples = 10
  n_features = 3
  n_tasks = 1
  ids = np.arange(n_samples)
  X = np.zeros((n_samples, n_features))
  y = np.ones((n_samples, n_tasks))
  target = 5. * y
  y *= 6.
  w = np.ones((n_samples, n_tasks))
  dataset = dc.data.NumpyDataset(X, y, w, ids)
  transformer = dc.trans.ClippingTransformer(transform_y=True, y_max=5.)
  clipped_dataset = transformer.transform(dataset)
  X_t, y_t, w_t, ids_t = (clipped_dataset.X, clipped_dataset.y,
                          clipped_dataset.w, clipped_dataset.ids)
  # Check ids are unchanged.
  for id_elt, id_t_elt in zip(ids, ids_t):
    assert id_elt == id_t_elt
  # Check X is unchanged since this is a y transformer
  np.testing.assert_allclose(X, X_t)
  # Check w is unchanged since this is a y transformer
  np.testing.assert_allclose(w, w_t)
  # Check y is now holding the proper values when sorted.
  np.testing.assert_allclose(y_t, target)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 134:</b> &nbsp; 30 fragments, nominal size 12 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1736')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/bbbp_datasets.py: 25-84
</a>
<div class="mid" id="frag1736" style="display:none"><pre>
def load_bbbp(
    featurizer: Union[dc.feat.Featurizer, str] = 'ECFP',
    splitter: Union[dc.splits.Splitter, str, None] = 'scaffold',
    transformers: List[Union[TransformerGenerator, str]] = ['balancing'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load BBBP dataset

  The blood-brain barrier penetration (BBBP) dataset is designed for the
  modeling and prediction of barrier permeability. As a membrane separating
  circulating blood and brain extracellular fluid, the blood-brain barrier
  blocks most drugs, hormones and neurotransmitters. Thus penetration of the
  barrier forms a long-standing issue in development of drugs targeting
  central nervous system.

  This dataset includes binary labels for over 2000 compounds on their
  permeability properties.

  Scaffold splitting is recommended for this dataset.

  The raw data csv file contains columns below:

  - "name" - Name of the compound
  - "smiles" - SMILES representation of the molecular structure
  - "p_np" - Binary labels for penetration/non-penetration

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in

  References
  ----------
  .. [1] Martins, Ines Filipa, et al. "A Bayesian approach to in silico
     blood-brain barrier penetration modeling." Journal of chemical
     information and modeling 52.6 (2012): 1686-1697.
  """
  loader = _BBBPLoader(featurizer, splitter, transformers, BBBP_TASKS, data_dir,
                       save_dir, **kwargs)
  return loader.load_dataset('bbbp', reload)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1782')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/clearance_datasets.py: 26-62
</a>
<div class="mid" id="frag1782" style="display:none"><pre>
def load_clearance(
    featurizer: Union[dc.feat.Featurizer, str] = 'ECFP',
    splitter: Union[dc.splits.Splitter, str, None] = 'scaffold',
    transformers: List[Union[TransformerGenerator, str]] = ['normalization'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """
  Load clearance datasets.

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in
  """
  loader = _ClearanceLoader(featurizer, splitter, transformers, CLEARANCE_TASKS,
                            data_dir, save_dir, **kwargs)
  return loader.load_dataset('clearance', reload)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1750')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/material_datasets/load_perovskite.py: 32-107
</a>
<div class="mid" id="frag1750" style="display:none"><pre>
def load_perovskite(
    featurizer: Union[dc.feat.Featurizer, str] = dc.feat.SineCoulombMatrix(),
    splitter: Union[dc.splits.Splitter, str, None] = 'random',
    transformers: List[Union[TransformerGenerator, str]] = ['normalization'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load perovskite dataset.

  Contains 18928 perovskite structures and their formation energies.
  In benchmark studies, random forest models and crystal graph
  neural networks achieved mean average error of 0.23 and 0.05 eV/atom,
  respectively, during five-fold nested cross validation on this
  dataset.

  For more details on the dataset see [1]_. For more details
  on previous benchmarks for this dataset, see [2]_.

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in

  Returns
  -------
  tasks, datasets, transformers : tuple
    tasks : list
      Column names corresponding to machine learning target variables.
    datasets : tuple
      train, validation, test splits of data as
      ``deepchem.data.datasets.Dataset`` instances.
    transformers : list
      ``deepchem.trans.transformers.Transformer`` instances applied
      to dataset.

  References
  ----------
  .. [1] Castelli, I. et al. "New cubic perovskites for one- and two-photon water splitting
     using the computational materials repository." Energy Environ. Sci., (2012), 5,
     9034-9043 DOI: 10.1039/C2EE22341D.
  .. [2] Dunn, A. et al. "Benchmarking Materials Property Prediction Methods:
     The Matbench Test Set and Automatminer Reference Algorithm." https://arxiv.org/abs/2005.00707 (2020)

  Examples
  --------
  &gt;&gt;&gt;
  &gt;&gt; import deepchem as dc
  &gt;&gt; tasks, datasets, transformers = dc.molnet.load_perovskite()
  &gt;&gt; train_dataset, val_dataset, test_dataset = datasets
  &gt;&gt; n_tasks = len(tasks)
  &gt;&gt; n_features = train_dataset.get_data_shape()[0]
  &gt;&gt; model = dc.models.MultitaskRegressor(n_tasks, n_features)

  """
  loader = _PerovskiteLoader(featurizer, splitter, transformers,
                             PEROVSKITE_TASKS, data_dir, save_dir, **kwargs)
  return loader.load_dataset('perovskite', reload)
</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1811')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/tox21_datasets.py: 28-83
</a>
<div class="mid" id="frag1811" style="display:none"><pre>
def load_tox21(
    featurizer: Union[dc.feat.Featurizer, str] = 'ECFP',
    splitter: Union[dc.splits.Splitter, str, None] = 'scaffold',
    transformers: List[Union[TransformerGenerator, str]] = ['balancing'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load Tox21 dataset

  The "Toxicology in the 21st Century" (Tox21) initiative created a public
  database measuring toxicity of compounds, which has been used in the 2014
  Tox21 Data Challenge. This dataset contains qualitative toxicity measurements
  for 8k compounds on 12 different targets, including nuclear receptors and
  stress response pathways.

  Random splitting is recommended for this dataset.

  The raw data csv file contains columns below:

  - "smiles" - SMILES representation of the molecular structure
  - "NR-XXX" - Nuclear receptor signaling bioassays results
  - "SR-XXX" - Stress response bioassays results

  please refer to https://tripod.nih.gov/tox21/challenge/data.jsp for details.

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in

  References
  ----------
  .. [1] Tox21 Challenge. https://tripod.nih.gov/tox21/challenge/
  """
  loader = _Tox21Loader(featurizer, splitter, transformers, TOX21_TASKS,
                        data_dir, save_dir, **kwargs)
  return loader.load_dataset('tox21', reload)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1769')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/delaney_datasets.py: 25-80
</a>
<div class="mid" id="frag1769" style="display:none"><pre>
def load_delaney(
    featurizer: Union[dc.feat.Featurizer, str] = 'ECFP',
    splitter: Union[dc.splits.Splitter, str, None] = 'scaffold',
    transformers: List[Union[TransformerGenerator, str]] = ['normalization'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load Delaney dataset

  The Delaney (ESOL) dataset a regression dataset containing structures and
  water solubility data for 1128 compounds. The dataset is widely used to
  validate machine learning models on estimating solubility directly from
  molecular structures (as encoded in SMILES strings).

  Scaffold splitting is recommended for this dataset.

  The raw data csv file contains columns below:

  - "Compound ID" - Name of the compound
  - "smiles" - SMILES representation of the molecular structure
  - "measured log solubility in mols per litre" - Log-scale water solubility
    of the compound, used as label

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in

  References
  ----------
  .. [1] Delaney, John S. "ESOL: estimating aqueous solubility directly from
     molecular structure." Journal of chemical information and computer
     sciences 44.3 (2004): 1000-1005.
  """
  loader = _DelaneyLoader(featurizer, splitter, transformers, DELANEY_TASKS,
                          data_dir, save_dir, **kwargs)
  return loader.load_dataset('delaney', reload)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1767')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/bace_datasets.py: 86-123
</a>
<div class="mid" id="frag1767" style="display:none"><pre>
def load_bace_classification(
    featurizer: Union[dc.feat.Featurizer, str] = 'ECFP',
    splitter: Union[dc.splits.Splitter, str, None] = 'scaffold',
    transformers: List[Union[TransformerGenerator, str]] = ['balancing'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """ Load BACE dataset, classification labels

  BACE dataset with classification labels ("class").

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in
  """
  loader = _BaceLoader(featurizer, splitter, transformers,
                       BACE_CLASSIFICATION_TASKS, data_dir, save_dir, **kwargs)
  return loader.load_dataset('bace_c', reload)
</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1758')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/muv_datasets.py: 29-85
</a>
<div class="mid" id="frag1758" style="display:none"><pre>
def load_muv(
    featurizer: Union[dc.feat.Featurizer, str] = 'ECFP',
    splitter: Union[dc.splits.Splitter, str, None] = 'scaffold',
    transformers: List[Union[TransformerGenerator, str]] = ['balancing'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load MUV dataset

  The Maximum Unbiased Validation (MUV) group is a benchmark dataset selected
  from PubChem BioAssay by applying a refined nearest neighbor analysis.

  The MUV dataset contains 17 challenging tasks for around 90 thousand
  compounds and is specifically designed for validation of virtual screening
  techniques.

  Scaffold splitting is recommended for this dataset.

  The raw data csv file contains columns below:

  - "mol_id" - PubChem CID of the compound
  - "smiles" - SMILES representation of the molecular structure
  - "MUV-XXX" - Measured results (Active/Inactive) for bioassays

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in

  References
  ----------
  .. [1] Rohrer, Sebastian G., and Knut Baumann. "Maximum unbiased validation
     (MUV) data sets for virtual screening based on PubChem bioactivity data."
     Journal of chemical information and modeling 49.2 (2009): 169-184.
  """
  loader = _MuvLoader(featurizer, splitter, transformers, MUV_TASKS, data_dir,
                      save_dir, **kwargs)
  return loader.load_dataset('muv', reload)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1744')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/material_datasets/load_mp_formation_energy.py: 32-107
</a>
<div class="mid" id="frag1744" style="display:none"><pre>
def load_mp_formation_energy(
    featurizer: Union[dc.feat.Featurizer, str] = dc.feat.SineCoulombMatrix(),
    splitter: Union[dc.splits.Splitter, str, None] = 'random',
    transformers: List[Union[TransformerGenerator, str]] = ['normalization'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load mp formation energy dataset.

  Contains 132752 calculated formation energies and inorganic
  crystal structures from the Materials Project database. In benchmark
  studies, random forest models achieved a mean average error of
  0.116 eV/atom during five-folded nested cross validation on this
  dataset.

  For more details on the dataset see [1]_. For more details
  on previous benchmarks for this dataset, see [2]_.

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in

  Returns
  -------
  tasks, datasets, transformers : tuple
    tasks : list
      Column names corresponding to machine learning target variables.
    datasets : tuple
      train, validation, test splits of data as
      ``deepchem.data.datasets.Dataset`` instances.
    transformers : list
      ``deepchem.trans.transformers.Transformer`` instances applied
      to dataset.

  References
  ----------
  .. [1] A. Jain*, S.P. Ong*, et al. (*=equal contributions) The Materials Project:
     A materials genome approach to accelerating materials innovation APL Materials,
     2013, 1(1), 011002. doi:10.1063/1.4812323 (2013).
  .. [2] Dunn, A. et al. "Benchmarking Materials Property Prediction Methods: The Matbench
     Test Set and Automatminer Reference Algorithm." https://arxiv.org/abs/2005.00707 (2020)

  Examples
  --------
  &gt;&gt;&gt;
  &gt;&gt; import deepchem as dc
  &gt;&gt; tasks, datasets, transformers = dc.molnet.load_mp_formation_energy()
  &gt;&gt; train_dataset, val_dataset, test_dataset = datasets
  &gt;&gt; n_tasks = len(tasks)
  &gt;&gt; n_features = train_dataset.get_data_shape()[0]
  &gt;&gt; model = dc.models.MultitaskRegressor(n_tasks, n_features)

  """
  loader = _MPFormationLoader(featurizer, splitter, transformers, MPFORME_TASKS,
                              data_dir, save_dir, **kwargs)
  return loader.load_dataset('mp-forme', reload)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1738')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/clintox_datasets.py: 26-97
</a>
<div class="mid" id="frag1738" style="display:none"><pre>
def load_clintox(
    featurizer: Union[dc.feat.Featurizer, str] = 'ECFP',
    splitter: Union[dc.splits.Splitter, str, None] = 'scaffold',
    transformers: List[Union[TransformerGenerator, str]] = ['balancing'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load ClinTox dataset

  The ClinTox dataset compares drugs approved by the FDA and
  drugs that have failed clinical trials for toxicity reasons.
  The dataset includes two classification tasks for 1491 drug
  compounds with known chemical structures:

  #. clinical trial toxicity (or absence of toxicity)
  #. FDA approval status.

  List of FDA-approved drugs are compiled from the SWEETLEAD
  database, and list of drugs that failed clinical trials for
  toxicity reasons are compiled from the Aggregate Analysis of
  ClinicalTrials.gov(AACT) database.

  Random splitting is recommended for this dataset.

  The raw data csv file contains columns below:

  - "smiles" - SMILES representation of the molecular structure
  - "FDA_APPROVED" - FDA approval status
  - "CT_TOX" - Clinical trial results

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in

  References
  ----------
  .. [1] Gayvert, Kaitlyn M., Neel S. Madhukar, and Olivier Elemento.
     "A data-driven approach to predicting successes and failures of clinical
     trials."
     Cell chemical biology 23.10 (2016): 1294-1301.
  .. [2] Artemov, Artem V., et al. "Integrated deep learned transcriptomic and
     structure-based predictor of clinical trials outcomes." bioRxiv (2016):
     095653.
  .. [3] Novick, Paul A., et al. "SWEETLEAD: an in silico database of approved
     drugs, regulated chemicals, and herbal isolates for computer-aided drug
     discovery." PloS one 8.11 (2013): e79568.
  .. [4] Aggregate Analysis of ClincalTrials.gov (AACT) Database.
     https://www.ctti-clinicaltrials.org/aact-database
  """
  loader = _ClintoxLoader(featurizer, splitter, transformers, CLINTOX_TASKS,
                          data_dir, save_dir, **kwargs)
  return loader.load_dataset('clintox', reload)
</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1740')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/toxcast_datasets.py: 248-305
</a>
<div class="mid" id="frag1740" style="display:none"><pre>
def load_toxcast(
    featurizer: Union[dc.feat.Featurizer, str] = 'ECFP',
    splitter: Union[dc.splits.Splitter, str, None] = 'scaffold',
    transformers: List[Union[TransformerGenerator, str]] = ['balancing'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load Toxcast dataset

  ToxCast is an extended data collection from the same
  initiative as Tox21, providing toxicology data for a large
  library of compounds based on in vitro high-throughput
  screening. The processed collection includes qualitative
  results of over 600 experiments on 8k compounds.

  Random splitting is recommended for this dataset.

  The raw data csv file contains columns below:

  - "smiles": SMILES representation of the molecular structure
  - "ACEA_T47D_80hr_Negative" ~ "Tanguay_ZF_120hpf_YSE_up": Bioassays results.
    Please refer to the section "high-throughput assay information" at
    https://www.epa.gov/chemical-research/toxicity-forecaster-toxcasttm-data
    for details.

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in

  References
  ----------
  .. [1] Richard, Ann M., et al. "ToxCast chemical landscape: paving the road
     to 21st century toxicology." Chemical research in toxicology 29.8 (2016):
     1225-1251.
  """
  loader = _ToxcastLoader(featurizer, splitter, transformers, TOXCAST_TASKS,
                          data_dir, save_dir, **kwargs)
  return loader.load_dataset('toxcast', reload)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1780')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/sampl_datasets.py: 25-82
</a>
<div class="mid" id="frag1780" style="display:none"><pre>
def load_sampl(
    featurizer: Union[dc.feat.Featurizer, str] = 'ECFP',
    splitter: Union[dc.splits.Splitter, str, None] = 'scaffold',
    transformers: List[Union[TransformerGenerator, str]] = ['normalization'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load SAMPL(FreeSolv) dataset

  The Free Solvation Database, FreeSolv(SAMPL), provides experimental and
  calculated hydration free energy of small molecules in water. The calculated
  values are derived from alchemical free energy calculations using molecular
  dynamics simulations. The experimental values are included in the benchmark
  collection.

  Random splitting is recommended for this dataset.

  The raw data csv file contains columns below:

  - "iupac" - IUPAC name of the compound
  - "smiles" - SMILES representation of the molecular structure
  - "expt" - Measured solvation energy (unit: kcal/mol) of the compound,
    used as label
  - "calc" - Calculated solvation energy (unit: kcal/mol) of the compound

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in

  References
  ----------
  .. [1] Mobley, David L., and J. Peter Guthrie. "FreeSolv: a database of
     experimental and calculated hydration free energies, with input files."
     Journal of computer-aided molecular design 28.7 (2014): 711-720.
  """
  loader = _SAMPLLoader(featurizer, splitter, transformers, SAMPL_TASKS,
                        data_dir, save_dir, **kwargs)
  return loader.load_dataset('sampl', reload)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1805')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/ppb_datasets.py: 25-60
</a>
<div class="mid" id="frag1805" style="display:none"><pre>
def load_ppb(
    featurizer: Union[dc.feat.Featurizer, str] = 'ECFP',
    splitter: Union[dc.splits.Splitter, str, None] = 'scaffold',
    transformers: List[Union[TransformerGenerator, str]] = ['normalization'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load PPB datasets.

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in
  """
  loader = _PPBLoader(featurizer, splitter, transformers, PPB_TASKS, data_dir,
                      save_dir, **kwargs)
  return loader.load_dataset('ppb', reload)
</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1746')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/material_datasets/load_mp_metallicity.py: 32-107
</a>
<div class="mid" id="frag1746" style="display:none"><pre>
def load_mp_metallicity(
    featurizer: Union[dc.feat.Featurizer, str] = dc.feat.SineCoulombMatrix(),
    splitter: Union[dc.splits.Splitter, str, None] = 'random',
    transformers: List[Union[TransformerGenerator, str]] = ['balancing'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load mp formation energy dataset.

  Contains 106113 inorganic crystal structures from the Materials
  Project database labeled as metals or nonmetals. In benchmark
  studies, random forest models achieved a mean ROC-AUC of
  0.9 during five-folded nested cross validation on this
  dataset.

  For more details on the dataset see [1]_. For more details
  on previous benchmarks for this dataset, see [2]_.

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in

  Returns
  -------
  tasks, datasets, transformers : tuple
    tasks : list
      Column names corresponding to machine learning target variables.
    datasets : tuple
      train, validation, test splits of data as
      ``deepchem.data.datasets.Dataset`` instances.
    transformers : list
      ``deepchem.trans.transformers.Transformer`` instances applied
      to dataset.

  References
  ----------
  .. [1] A. Jain*, S.P. Ong*, et al. (*=equal contributions) The Materials Project:
     A materials genome approach to accelerating materials innovation APL Materials,
     2013, 1(1), 011002. doi:10.1063/1.4812323 (2013).
  .. [2] Dunn, A. et al. "Benchmarking Materials Property Prediction Methods: The Matbench
     Test Set and Automatminer Reference Algorithm." https://arxiv.org/abs/2005.00707 (2020)

  Examples
  --------
  &gt;&gt;&gt;
  &gt;&gt; import deepchem as dc
  &gt;&gt; tasks, datasets, transformers = dc.molnet.load_mp_metallicity()
  &gt;&gt; train_dataset, val_dataset, test_dataset = datasets
  &gt;&gt; n_tasks = len(tasks)
  &gt;&gt; n_features = train_dataset.get_data_shape()[0]
  &gt;&gt; model = dc.models.MultitaskRegressor(n_tasks, n_features)

  """
  loader = _MPMetallicityLoader(featurizer, splitter, transformers,
                                MPMETAL_TASKS, data_dir, save_dir, **kwargs)
  return loader.load_dataset('mp-metallicity', reload)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1771')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/nci_datasets.py: 37-72
</a>
<div class="mid" id="frag1771" style="display:none"><pre>
def load_nci(
    featurizer: Union[dc.feat.Featurizer, str] = 'ECFP',
    splitter: Union[dc.splits.Splitter, str, None] = 'random',
    transformers: List[Union[TransformerGenerator, str]] = ['normalization'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load NCI dataset.

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in
  """
  loader = _NCILoader(featurizer, splitter, transformers, NCI_TASKS, data_dir,
                      save_dir, **kwargs)
  return loader.load_dataset('nci', reload)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1778')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/sweetlead_datasets.py: 26-71
</a>
<div class="mid" id="frag1778" style="display:none"><pre>
def load_sweet(
    featurizer: Union[dc.feat.Featurizer, str] = 'ECFP',
    splitter: Union[dc.splits.Splitter, str, None] = 'scaffold',
    transformers: List[Union[TransformerGenerator, str]] = ['balancing'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load sweet datasets.

  Sweetlead is a dataset of chemical structures for approved drugs, chemical isolates
  from traditional medicinal herbs, and regulated chemicals. Resulting structures are
  filtered for the active pharmaceutical ingredient, standardized, and differing
  formulations of the same drug were combined in the final database.

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in

  References
  ----------
  Novick, Paul A., et al. "SWEETLEAD: an in silico database of approved drugs, regulated
  chemicals, and herbal isolates for computer-aided drug discovery." PLoS One 8.11 (2013).
  """
  loader = _SweetLoader(featurizer, splitter, transformers, SWEETLEAD_TASKS,
                        data_dir, save_dir, **kwargs)
  return loader.load_dataset('sweet', reload)
</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1776')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/pcba_datasets.py: 62-149
</a>
<div class="mid" id="frag1776" style="display:none"><pre>
def load_pcba(
    featurizer: Union[dc.feat.Featurizer, str] = 'ECFP',
    splitter: Union[dc.splits.Splitter, str, None] = 'scaffold',
    transformers: List[Union[TransformerGenerator, str]] = ['balancing'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load PCBA dataset

  PubChem BioAssay (PCBA) is a database consisting of biological activities of
  small molecules generated by high-throughput screening. We use a subset of
  PCBA, containing 128 bioassays measured over 400 thousand compounds,
  used by previous work to benchmark machine learning methods.

  Random splitting is recommended for this dataset.

  The raw data csv file contains columns below:

  - "mol_id" - PubChem CID of the compound
  - "smiles" - SMILES representation of the molecular structure
  - "PCBA-XXX" - Measured results (Active/Inactive) for bioassays:
        search for the assay ID at
        https://pubchem.ncbi.nlm.nih.gov/search/#collection=bioassays
        for details

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in

  References
  ----------
  .. [1] Wang, Yanli, et al. "PubChem's BioAssay database."
     Nucleic acids research 40.D1 (2011): D400-D412.
  """
  loader = _PCBALoader('pcba.csv.gz', featurizer, splitter, transformers,
                       PCBA_TASKS, data_dir, save_dir, **kwargs)
  return loader.load_dataset('pcba', reload)


# def load_pcba_146(featurizer='ECFP',
#                   split='random',
#                   reload=True,
#                   data_dir=None,
#                   save_dir=None,
#                   **kwargs):
#   return load_pcba_dataset(
#       featurizer=featurizer,
#       split=split,
#       reload=reload,
#       assay_file_name="pcba_146.csv.gz",
#       data_dir=data_dir,
#       save_dir=save_dir,
#       **kwargs)

# def load_pcba_2475(featurizer='ECFP',
#                    split='random',
#                    reload=True,
#                    data_dir=None,
#                    save_dir=None,
#                    **kwargs):
#   return load_pcba_dataset(
#       featurizer=featurizer,
#       split=split,
#       reload=reload,
#       assay_file_name="pcba_2475.csv.gz",
#       data_dir=data_dir,
#       save_dir=save_dir,
#       **kwargs)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1803')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/chembl25_datasets.py: 47-82
</a>
<div class="mid" id="frag1803" style="display:none"><pre>
def load_chembl25(
    featurizer: Union[dc.feat.Featurizer, str] = 'ECFP',
    splitter: Union[dc.splits.Splitter, str, None] = 'scaffold',
    transformers: List[Union[TransformerGenerator, str]] = ['normalization'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Loads the ChEMBL25 dataset, featurizes it, and does a split.

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in
  """
  loader = _Chembl25Loader(featurizer, splitter, transformers, CHEMBL25_TASKS,
                           data_dir, save_dir, **kwargs)
  return loader.load_dataset('chembl25', reload)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1784')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/lipo_datasets.py: 25-78
</a>
<div class="mid" id="frag1784" style="display:none"><pre>
def load_lipo(
    featurizer: Union[dc.feat.Featurizer, str] = 'ECFP',
    splitter: Union[dc.splits.Splitter, str, None] = 'scaffold',
    transformers: List[Union[TransformerGenerator, str]] = ['normalization'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load Lipophilicity dataset

  Lipophilicity is an important feature of drug molecules that affects both
  membrane permeability and solubility. The lipophilicity dataset, curated
  from ChEMBL database, provides experimental results of octanol/water
  distribution coefficient (logD at pH 7.4) of 4200 compounds.

  Scaffold splitting is recommended for this dataset.

  The raw data csv file contains columns below:

  - "smiles" - SMILES representation of the molecular structure
  - "exp" - Measured octanol/water distribution coefficient (logD) of the
    compound, used as label

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in

  References
  ----------
  .. [1] Hersey, A. ChEMBL Deposited Data Set - AZ dataset; 2015.
     https://doi.org/10.6019/chembl3301361
  """
  loader = _LipoLoader(featurizer, splitter, transformers, LIPO_TASKS, data_dir,
                       save_dir, **kwargs)
  return loader.load_dataset('lipo', reload)
</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1773')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/qm7_datasets.py: 30-107
</a>
<div class="mid" id="frag1773" style="display:none"><pre>
def load_qm7(
    featurizer: Union[dc.feat.Featurizer, str] = dc.feat.CoulombMatrix(23),
    splitter: Union[dc.splits.Splitter, str, None] = 'random',
    transformers: List[Union[TransformerGenerator, str]] = ['normalization'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load QM7 dataset

  QM7 is a subset of GDB-13 (a database of nearly 1 billion
  stable and synthetically accessible organic molecules)
  containing up to 7 heavy atoms C, N, O, and S. The 3D
  Cartesian coordinates of the most stable conformations and
  their atomization energies were determined using ab-initio
  density functional theory (PBE0/tier2 basis set). This dataset
  also provided Coulomb matrices as calculated in [Rupp et al.
  PRL, 2012]:

  Stratified splitting is recommended for this dataset.

  The data file (.mat format, we recommend using `scipy.io.loadmat`
  for python users to load this original data) contains five arrays:

  - "X" - (7165 x 23 x 23), Coulomb matrices
  - "T" - (7165), atomization energies (unit: kcal/mol)
  - "P" - (5 x 1433), cross-validation splits as used in [Montavon et al.
    NIPS, 2012]
  - "Z" - (7165 x 23), atomic charges
  - "R" - (7165 x 23 x 3), cartesian coordinate (unit: Bohr) of each atom in
    the molecules

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in

  Note
  ----
  DeepChem 2.4.0 has turned on sanitization for this dataset by
  default.  For the QM7 dataset, this means that calling this
  function will return 6838 compounds instead of 7160 in the source
  dataset file.  This appears to be due to valence specification
  mismatches in the dataset that weren't caught in earlier more lax
  versions of RDKit.  Note that this may subtly affect benchmarking
  results on this
  dataset.

  References
  ----------
  .. [1] Rupp, Matthias, et al. "Fast and accurate modeling of molecular
     atomization energies with machine learning." Physical review letters
     108.5 (2012): 058301.
  .. [2] Montavon, Grégoire, et al. "Learning invariant representations of
     molecules for atomization energy prediction." Advances in Neural
     Information Proccessing Systems. 2012.
  """
  loader = _QM7Loader(featurizer, splitter, transformers, QM7_TASKS, data_dir,
                      save_dir, **kwargs)
  return loader.load_dataset('qm7', reload)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1764')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/hiv_datasets.py: 25-82
</a>
<div class="mid" id="frag1764" style="display:none"><pre>
def load_hiv(
    featurizer: Union[dc.feat.Featurizer, str] = 'ECFP',
    splitter: Union[dc.splits.Splitter, str, None] = 'scaffold',
    transformers: List[Union[TransformerGenerator, str]] = ['balancing'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load HIV dataset

  The HIV dataset was introduced by the Drug Therapeutics
  Program (DTP) AIDS Antiviral Screen, which tested the ability
  to inhibit HIV replication for over 40,000 compounds.
  Screening results were evaluated and placed into three
  categories: confirmed inactive (CI),confirmed active (CA) and
  confirmed moderately active (CM). We further combine the
  latter two labels, making it a classification task between
  inactive (CI) and active (CA and CM).

  Scaffold splitting is recommended for this dataset.

  The raw data csv file contains columns below:

  - "smiles": SMILES representation of the molecular structure
  - "activity": Three-class labels for screening results: CI/CM/CA
  - "HIV_active": Binary labels for screening results: 1 (CA/CM) and 0 (CI)

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in

  References
  ----------
  .. [1] AIDS Antiviral Screen Data.
     https://wiki.nci.nih.gov/display/NCIDTPdata/AIDS+Antiviral+Screen+Data
  """
  loader = _HIVLoader(featurizer, splitter, transformers, HIV_TASKS, data_dir,
                      save_dir, **kwargs)
  return loader.load_dataset('hiv', reload)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1797')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/qm8_datasets.py: 32-116
</a>
<div class="mid" id="frag1797" style="display:none"><pre>
def load_qm8(
    featurizer: Union[dc.feat.Featurizer, str] = dc.feat.CoulombMatrix(26),
    splitter: Union[dc.splits.Splitter, str, None] = 'random',
    transformers: List[Union[TransformerGenerator, str]] = ['normalization'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load QM8 dataset

  QM8 is the dataset used in a study on modeling quantum
  mechanical calculations of electronic spectra and excited
  state energy of small molecules. Multiple methods, including
  time-dependent density functional theories (TDDFT) and
  second-order approximate coupled-cluster (CC2), are applied to
  a collection of molecules that include up to eight heavy atoms
  (also a subset of the GDB-17 database). In our collection,
  there are four excited state properties calculated by four
  different methods on 22 thousand samples:

  S0 -&gt; S1 transition energy E1 and the corresponding oscillator strength f1

  S0 -&gt; S2 transition energy E2 and the corresponding oscillator strength f2

  E1, E2, f1, f2 are in atomic units. f1, f2 are in length representation

  Random splitting is recommended for this dataset.

  The source data contain:

  - qm8.sdf: molecular structures
  - qm8.sdf.csv: tables for molecular properties

    - Column 1: Molecule ID (gdb9 index) mapping to the .sdf file
    - Columns 2-5: RI-CC2/def2TZVP
    - Columns 6-9: LR-TDPBE0/def2SVP
    - Columns 10-13: LR-TDPBE0/def2TZVP
    - Columns 14-17: LR-TDCAM-B3LYP/def2TZVP

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in

  Note
  ----
  DeepChem 2.4.0 has turned on sanitization for this dataset by
  default.  For the QM8 dataset, this means that calling this
  function will return 21747 compounds instead of 21786 in the source
  dataset file.  This appears to be due to valence specification
  mismatches in the dataset that weren't caught in earlier more lax
  versions of RDKit.  Note that this may subtly affect benchmarking
  results on this dataset.

  References
  ----------
  .. [1] Blum, Lorenz C., and Jean-Louis Reymond. "970 million druglike
     small molecules for virtual screening in the chemical universe database
     GDB-13." Journal of the American Chemical Society 131.25 (2009):
     8732-8733.
  .. [2] Ramakrishnan, Raghunathan, et al. "Electronic spectra from TDDFT
     and machine learning in chemical space." The Journal of chemical physics
     143.8 (2015): 084111.
  """
  loader = _QM8Loader(featurizer, splitter, transformers, QM8_TASKS, data_dir,
                      save_dir, **kwargs)
  return loader.load_dataset('qm8', reload)
</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1766')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/bace_datasets.py: 27-85
</a>
<div class="mid" id="frag1766" style="display:none"><pre>
def load_bace_regression(
    featurizer: Union[dc.feat.Featurizer, str] = 'ECFP',
    splitter: Union[dc.splits.Splitter, str, None] = 'scaffold',
    transformers: List[Union[TransformerGenerator, str]] = ['normalization'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """ Load BACE dataset, regression labels

  The BACE dataset provides quantitative IC50 and qualitative (binary label)
  binding results for a set of inhibitors of human beta-secretase 1 (BACE-1).

  All data are experimental values reported in scientific literature over the
  past decade, some with detailed crystal structures available. A collection
  of 1522 compounds is provided, along with the regression labels of IC50.

  Scaffold splitting is recommended for this dataset.

  The raw data csv file contains columns below:

  - "mol" - SMILES representation of the molecular structure
  - "pIC50" - Negative log of the IC50 binding affinity
  - "class" - Binary labels for inhibitor

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in

  References
  ----------
  .. [1] Subramanian, Govindan, et al. "Computational modeling of β-secretase 1
     (BACE-1) inhibitors using ligand based approaches." Journal of chemical
     information and modeling 56.10 (2016): 1936-1949.
  """
  loader = _BaceLoader(featurizer, splitter, transformers,
                       BACE_REGRESSION_TASKS, data_dir, save_dir, **kwargs)
  return loader.load_dataset('bace_r', reload)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1786')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/hopv_datasets.py: 30-77
</a>
<div class="mid" id="frag1786" style="display:none"><pre>
def load_hopv(
    featurizer: Union[dc.feat.Featurizer, str] = 'ECFP',
    splitter: Union[dc.splits.Splitter, str, None] = 'scaffold',
    transformers: List[Union[TransformerGenerator, str]] = ['normalization'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load HOPV datasets. Does not do train/test split

  The HOPV datasets consist of the "Harvard Organic
  Photovoltaic Dataset. This dataset includes 350 small
  molecules and polymers that were utilized as p-type materials
  in OPVs. Experimental properties include: HOMO [a.u.], LUMO
  [a.u.], Electrochemical gap [a.u.], Optical gap [a.u.], Power
  conversion efficiency [%], Open circuit potential [V], Short
  circuit current density [mA/cm^2], and fill factor [%].
  Theoretical calculations in the original dataset have been
  removed (for now).

  Lopez, Steven A., et al. "The Harvard organic photovoltaic dataset." Scientific data 3.1 (2016): 1-7.

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in
  """
  loader = _HOPVLoader(featurizer, splitter, transformers, HOPV_TASKS, data_dir,
                       save_dir, **kwargs)
  return loader.load_dataset('hopv', reload)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1752')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/qm9_datasets.py: 31-123
</a>
<div class="mid" id="frag1752" style="display:none"><pre>
def load_qm9(
    featurizer: Union[dc.feat.Featurizer, str] = dc.feat.CoulombMatrix(29),
    splitter: Union[dc.splits.Splitter, str, None] = 'random',
    transformers: List[Union[TransformerGenerator, str]] = ['normalization'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load QM9 dataset

  QM9 is a comprehensive dataset that provides geometric, energetic,
  electronic and thermodynamic properties for a subset of GDB-17
  database, comprising 134 thousand stable organic molecules with up
  to 9 heavy atoms.  All molecules are modeled using density
  functional theory (B3LYP/6-31G(2df,p) based DFT).

  Random splitting is recommended for this dataset.

  The source data contain:

  - qm9.sdf: molecular structures
  - qm9.sdf.csv: tables for molecular properties

    - "mol_id" - Molecule ID (gdb9 index) mapping to the .sdf file
    - "A" - Rotational constant (unit: GHz)
    - "B" - Rotational constant (unit: GHz)
    - "C" - Rotational constant (unit: GHz)
    - "mu" - Dipole moment (unit: D)
    - "alpha" - Isotropic polarizability (unit: Bohr^3)
    - "homo" - Highest occupied molecular orbital energy (unit: Hartree)
    - "lumo" - Lowest unoccupied molecular orbital energy (unit: Hartree)
    - "gap" - Gap between HOMO and LUMO (unit: Hartree)
    - "r2" - Electronic spatial extent (unit: Bohr^2)
    - "zpve" - Zero point vibrational energy (unit: Hartree)
    - "u0" - Internal energy at 0K (unit: Hartree)
    - "u298" - Internal energy at 298.15K (unit: Hartree)
    - "h298" - Enthalpy at 298.15K (unit: Hartree)
    - "g298" - Free energy at 298.15K (unit: Hartree)
    - "cv" - Heat capavity at 298.15K (unit: cal/(mol*K))
    - "u0_atom" - Atomization energy at 0K (unit: kcal/mol)
    - "u298_atom" - Atomization energy at 298.15K (unit: kcal/mol)
    - "h298_atom" - Atomization enthalpy at 298.15K (unit: kcal/mol)
    - "g298_atom" - Atomization free energy at 298.15K (unit: kcal/mol)

  "u0_atom" ~ "g298_atom" (used in MoleculeNet) are calculated from the
  differences between "u0" ~ "g298" and sum of reference energies of all
  atoms in the molecules, as given in
  https://figshare.com/articles/Atomref%3A_Reference_thermochemical_energies_of_H%2C_C%2C_N%2C_O%2C_F_atoms./1057643

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in

  Note
  ----
  DeepChem 2.4.0 has turned on sanitization for this dataset by
  default.  For the QM9 dataset, this means that calling this
  function will return 132480 compounds instead of 133885 in the
  source dataset file. This appears to be due to valence
  specification mismatches in the dataset that weren't caught in
  earlier more lax versions of RDKit. Note that this may subtly
  affect benchmarking results on this dataset.

  References
  ----------
  .. [1] Blum, Lorenz C., and Jean-Louis Reymond. "970 million druglike small
     molecules for virtual screening in the chemical universe database GDB-13."
     Journal of the American Chemical Society 131.25 (2009): 8732-8733.
  .. [2] Ramakrishnan, Raghunathan, et al. "Quantum chemistry structures and
     properties of 134 kilo molecules." Scientific data 1 (2014): 140022.
  """
  loader = _QM9Loader(featurizer, splitter, transformers, QM9_TASKS, data_dir,
                      save_dir, **kwargs)
  return loader.load_dataset('qm9', reload)
</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1742')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/thermosol_datasets.py: 26-61
</a>
<div class="mid" id="frag1742" style="display:none"><pre>
def load_thermosol(
    featurizer: Union[dc.feat.Featurizer, str] = 'ECFP',
    splitter: Union[dc.splits.Splitter, str, None] = 'scaffold',
    transformers: List[Union[TransformerGenerator, str]] = [],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Loads the thermodynamic solubility datasets.

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in
  """
  loader = _ThermosolLoader(featurizer, splitter, transformers, THERMOSOL_TASKS,
                            data_dir, save_dir, **kwargs)
  return loader.load_dataset('thermosol', reload)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1789')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/hppb_datasets.py: 42-77
</a>
<div class="mid" id="frag1789" style="display:none"><pre>
def load_hppb(
    featurizer: Union[dc.feat.Featurizer, str] = 'ECFP',
    splitter: Union[dc.splits.Splitter, str, None] = 'scaffold',
    transformers: List[Union[TransformerGenerator, str]] = [],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Loads the thermodynamic solubility datasets.

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in
  """
  loader = _HPPBLoader(featurizer, splitter, transformers, HPPB_TASKS, data_dir,
                       save_dir, **kwargs)
  return loader.load_dataset('hppb', reload)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1791')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/sider_datasets.py: 42-100
</a>
<div class="mid" id="frag1791" style="display:none"><pre>
def load_sider(
    featurizer: Union[dc.feat.Featurizer, str] = 'ECFP',
    splitter: Union[dc.splits.Splitter, str, None] = 'scaffold',
    transformers: List[Union[TransformerGenerator, str]] = ['balancing'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load SIDER dataset

  The Side Effect Resource (SIDER) is a database of marketed
  drugs and adverse drug reactions (ADR). The version of the
  SIDER dataset in DeepChem has grouped drug side effects into
  27 system organ classes following MedDRA classifications
  measured for 1427 approved drugs.

  Random splitting is recommended for this dataset.

  The raw data csv file contains columns below:

  - "smiles": SMILES representation of the molecular structure
  - "Hepatobiliary disorders" ~ "Injury, poisoning and procedural
    complications": Recorded side effects for the drug. Please refer
    to http://sideeffects.embl.de/se/?page=98 for details on ADRs.

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in

  References
  ----------
  .. [1] Kuhn, Michael, et al. "The SIDER database of drugs and side effects."
     Nucleic acids research 44.D1 (2015): D1075-D1079.
  .. [2] Altae-Tran, Han, et al. "Low data drug discovery with one-shot
     learning." ACS central science 3.4 (2017): 283-293.
  .. [3] Medical Dictionary for Regulatory Activities. http://www.meddra.org/
  """
  loader = _SiderLoader(featurizer, splitter, transformers, SIDER_TASKS,
                        data_dir, save_dir, **kwargs)
  return loader.load_dataset('sider', reload)
</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1762')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/bbbc_datasets.py: 91-129
</a>
<div class="mid" id="frag1762" style="display:none"><pre>
def load_bbbc002(
    splitter: Union[dc.splits.Splitter, str, None] = 'index',
    transformers: List[Union[TransformerGenerator, str]] = [],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load BBBC002 dataset

  This dataset contains data corresponding to 5 samples of Drosophilia Kc167
  cells. There are 10 fields of view for each sample, each an image of size
  512x512. Ground truth labels contain cell counts for this dataset. Full
  details about this dataset are present at
  https://data.broadinstitute.org/bbbc/BBBC002/.

  Parameters
  ----------
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in
  """
  featurizer = dc.feat.UserDefinedFeaturizer([])  # Not actually used
  loader = _BBBC002Loader(featurizer, splitter, transformers, BBBC2_TASKS,
                          data_dir, save_dir, **kwargs)
  return loader.load_dataset('bbbc002', reload)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1748')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/material_datasets/load_bandgap.py: 32-106
</a>
<div class="mid" id="frag1748" style="display:none"><pre>
def load_bandgap(
    featurizer: Union[dc.feat.Featurizer,
                      str] = dc.feat.ElementPropertyFingerprint(),
    splitter: Union[dc.splits.Splitter, str, None] = 'random',
    transformers: List[Union[TransformerGenerator, str]] = ['normalization'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load band gap dataset.

  Contains 4604 experimentally measured band gaps for inorganic
  crystal structure compositions. In benchmark studies, random forest
  models achieved a mean average error of 0.45 eV during five-fold
  nested cross validation on this dataset.

  For more details on the dataset see [1]_. For more details
  on previous benchmarks for this dataset, see [2]_.

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in

  Returns
  -------
  tasks, datasets, transformers : tuple
    tasks : list
      Column names corresponding to machine learning target variables.
    datasets : tuple
      train, validation, test splits of data as
      ``deepchem.data.datasets.Dataset`` instances.
    transformers : list
      ``deepchem.trans.transformers.Transformer`` instances applied
      to dataset.

  References
  ----------
  .. [1] Zhuo, Y. et al. "Predicting the Band Gaps of Inorganic Solids by Machine Learning."
     J. Phys. Chem. Lett. (2018) DOI: 10.1021/acs.jpclett.8b00124.
  .. [2] Dunn, A. et al. "Benchmarking Materials Property Prediction Methods: The Matbench Test Set
     and Automatminer Reference Algorithm." https://arxiv.org/abs/2005.00707 (2020)

  Examples
  --------
  &gt;&gt;&gt;
  &gt;&gt; import deepchem as dc
  &gt;&gt; tasks, datasets, transformers = dc.molnet.load_bandgap()
  &gt;&gt; train_dataset, val_dataset, test_dataset = datasets
  &gt;&gt; n_tasks = len(tasks)
  &gt;&gt; n_features = train_dataset.get_data_shape()[0]
  &gt;&gt; model = dc.models.MultitaskRegressor(n_tasks, n_features)

  """
  loader = _BandgapLoader(featurizer, splitter, transformers, BANDGAP_TASKS,
                          data_dir, save_dir, **kwargs)
  return loader.load_dataset('bandgap', reload)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1760')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/bbbc_datasets.py: 36-75
</a>
<div class="mid" id="frag1760" style="display:none"><pre>
def load_bbbc001(
    splitter: Union[dc.splits.Splitter, str, None] = 'index',
    transformers: List[Union[TransformerGenerator, str]] = [],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load BBBC001 dataset

  This dataset contains 6 images of human HT29 colon cancer cells. The task is
  to learn to predict the cell counts in these images. This dataset is too small
  to serve to train algorithms, but might serve as a good test dataset.
  https://data.broadinstitute.org/bbbc/BBBC001/

  Parameters
  ----------
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in
  """
  featurizer = dc.feat.UserDefinedFeaturizer([])  # Not actually used
  loader = _BBBC001Loader(featurizer, splitter, transformers, BBBC1_TASKS,
                          data_dir, save_dir, **kwargs)
  return loader.load_dataset('bbbc001', reload)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 135:</b> &nbsp; 4 fragments, nominal size 14 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1743')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/material_datasets/load_mp_formation_energy.py: 16-31
</a>
<div class="mid" id="frag1743" style="display:none"><pre>
  def create_dataset(self) -&gt; Dataset:
    dataset_file = os.path.join(self.data_dir, 'mp_formation_energy.json')
    targz_file = os.path.join(self.data_dir, 'mp_formation_energy.tar.gz')
    if not os.path.exists(dataset_file):
      if not os.path.exists(targz_file):
        dc.utils.data_utils.download_url(
            url=MPFORME_URL, dest_dir=self.data_dir)
      dc.utils.data_utils.untargz_file(targz_file, self.data_dir)
    loader = dc.data.JsonLoader(
        tasks=self.tasks,
        feature_field="structure",
        label_field="formation_energy",
        featurizer=self.featurizer)
    return loader.create_dataset(dataset_file)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1749')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/material_datasets/load_perovskite.py: 16-31
</a>
<div class="mid" id="frag1749" style="display:none"><pre>
  def create_dataset(self) -&gt; Dataset:
    dataset_file = os.path.join(self.data_dir, 'perovskite.json')
    targz_file = os.path.join(self.data_dir, 'perovskite.tar.gz')
    if not os.path.exists(dataset_file):
      if not os.path.exists(targz_file):
        dc.utils.data_utils.download_url(
            url=PEROVSKITE_URL, dest_dir=self.data_dir)
      dc.utils.data_utils.untargz_file(targz_file, self.data_dir)
    loader = dc.data.JsonLoader(
        tasks=self.tasks,
        feature_field="structure",
        label_field="formation_energy",
        featurizer=self.featurizer)
    return loader.create_dataset(dataset_file)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1745')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/material_datasets/load_mp_metallicity.py: 16-31
</a>
<div class="mid" id="frag1745" style="display:none"><pre>
  def create_dataset(self) -&gt; Dataset:
    dataset_file = os.path.join(self.data_dir, 'mp_is_metal.json')
    targz_file = os.path.join(self.data_dir, 'mp_is_metal.tar.gz')
    if not os.path.exists(dataset_file):
      if not os.path.exists(targz_file):
        dc.utils.data_utils.download_url(
            url=MPMETAL_URL, dest_dir=self.data_dir)
      dc.utils.data_utils.untargz_file(targz_file, self.data_dir)
    loader = dc.data.JsonLoader(
        tasks=self.tasks,
        feature_field="structure",
        label_field="is_metal",
        featurizer=self.featurizer)
    return loader.create_dataset(dataset_file)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1747')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/material_datasets/load_bandgap.py: 16-31
</a>
<div class="mid" id="frag1747" style="display:none"><pre>
  def create_dataset(self) -&gt; Dataset:
    dataset_file = os.path.join(self.data_dir, 'expt_gap.json')
    targz_file = os.path.join(self.data_dir, 'expt_gap.tar.gz')
    if not os.path.exists(dataset_file):
      if not os.path.exists(targz_file):
        dc.utils.data_utils.download_url(
            url=BANDGAP_URL, dest_dir=self.data_dir)
      dc.utils.data_utils.untargz_file(targz_file, self.data_dir)
    loader = dc.data.JsonLoader(
        tasks=self.tasks,
        feature_field="composition",
        label_field="experimental_bandgap",
        featurizer=self.featurizer)
    return loader.create_dataset(dataset_file)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 136:</b> &nbsp; 7 fragments, nominal size 10 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1753')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/factors_datasets.py: 23-39
</a>
<div class="mid" id="frag1753" style="display:none"><pre>
def remove_missing_entries(dataset):
  """Remove missing entries.

  Some of the datasets have missing entries that sneak in as zero'd out
  feature vectors. Get rid of them.
  """
  for i, (X, y, w, ids) in enumerate(dataset.itershards()):
    available_rows = X.any(axis=1)
    logger.info("Shard %d has %d missing entries." %
                (i, np.count_nonzero(~available_rows)))
    X = X[available_rows]
    y = y[available_rows]
    w = w[available_rows]
    ids = ids[available_rows]
    dataset.set_shard(i, X, y, w, ids)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1992')" href="javascript:;">
deepchem-2.4.0/examples/uv/UV_datasets.py: 15-30
</a>
<div class="mid" id="frag1992" style="display:none"><pre>
def remove_missing_entries(dataset):
  """Remove missing entries.

  Some of the datasets have missing entries that sneak in as zero'd out
  feature vectors. Get rid of them.
  """
  for i, (X, y, w, ids) in enumerate(dataset.itershards()):
    available_rows = X.any(axis=1)
    print("Shard %d has %d missing entries."
        % (i, np.count_nonzero(~available_rows)))
    X = X[available_rows]
    y = y[available_rows]
    w = w[available_rows]
    ids = ids[available_rows]
    dataset.set_shard(i, X, y, w, ids)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1792')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/kaggle_datasets.py: 15-31
</a>
<div class="mid" id="frag1792" style="display:none"><pre>
def remove_missing_entries(dataset):
  """Remove missing entries.

  Some of the datasets have missing entries that sneak in as zero'd out
  feature vectors. Get rid of them.
  """
  for i, (X, y, w, ids) in enumerate(dataset.itershards()):
    available_rows = X.any(axis=1)
    logger.info("Shard %d has %d missing entries." %
                (i, np.count_nonzero(~available_rows)))
    X = X[available_rows]
    y = y[available_rows]
    w = w[available_rows]
    ids = ids[available_rows]
    dataset.set_shard(i, X, y, w, ids)


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1985')" href="javascript:;">
deepchem-2.4.0/examples/kinase/KINASE_datasets.py: 15-30
</a>
<div class="mid" id="frag1985" style="display:none"><pre>
def remove_missing_entries(dataset):
  """Remove missing entries.

  Some of the datasets have missing entries that sneak in as zero'd out
  feature vectors. Get rid of them.
  """
  for i, (X, y, w, ids) in enumerate(dataset.itershards()):
    available_rows = X.any(axis=1)
    print("Shard %d has %d missing entries."
        % (i, np.count_nonzero(~available_rows)))
    X = X[available_rows]
    y = y[available_rows]
    w = w[available_rows]
    ids = ids[available_rows]
    dataset.set_shard(i, X, y, w, ids)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1798')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/uv_datasets.py: 24-40
</a>
<div class="mid" id="frag1798" style="display:none"><pre>
def remove_missing_entries(dataset):
  """Remove missing entries.

  Some of the datasets have missing entries that sneak in as zero'd out
  feature vectors. Get rid of them.
  """
  for i, (X, y, w, ids) in enumerate(dataset.itershards()):
    available_rows = X.any(axis=1)
    logger.info("Shard %d has %d missing entries." %
                (i, np.count_nonzero(~available_rows)))
    X = X[available_rows]
    y = y[available_rows]
    w = w[available_rows]
    ids = ids[available_rows]
    dataset.set_shard(i, X, y, w, ids)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1806')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/kinase_datasets.py: 23-39
</a>
<div class="mid" id="frag1806" style="display:none"><pre>
def remove_missing_entries(dataset):
  """Remove missing entries.

  Some of the datasets have missing entries that sneak in as zero'd out
  feature vectors. Get rid of them.
  """
  for i, (X, y, w, ids) in enumerate(dataset.itershards()):
    available_rows = X.any(axis=1)
    logger.info("Shard %d has %d missing entries." %
                (i, np.count_nonzero(~available_rows)))
    X = X[available_rows]
    y = y[available_rows]
    w = w[available_rows]
    ids = ids[available_rows]
    dataset.set_shard(i, X, y, w, ids)


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2012')" href="javascript:;">
deepchem-2.4.0/examples/factors/FACTORS_datasets.py: 15-30
</a>
<div class="mid" id="frag2012" style="display:none"><pre>
def remove_missing_entries(dataset):
  """Remove missing entries.

  Some of the datasets have missing entries that sneak in as zero'd out
  feature vectors. Get rid of them.
  """
  for i, (X, y, w, ids) in enumerate(dataset.itershards()):
    available_rows = X.any(axis=1)
    print("Shard %d has %d missing entries."
        % (i, np.count_nonzero(~available_rows)))
    X = X[available_rows]
    y = y[available_rows]
    w = w[available_rows]
    ids = ids[available_rows]
    dataset.set_shard(i, X, y, w, ids)

</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 137:</b> &nbsp; 3 fragments, nominal size 58 lines, similarity 84%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1755')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/factors_datasets.py: 49-136
</a>
<div class="mid" id="frag1755" style="display:none"><pre>
def gen_factors(FACTORS_tasks,
                data_dir,
                train_dir,
                valid_dir,
                test_dir,
                shard_size=2000):
  """Loads the FACTORS dataset; does not do train/test split"""

  time1 = time.time()

  train_files = os.path.join(data_dir, TRAIN_FILENAME)
  valid_files = os.path.join(data_dir, VALID_FILENAME)
  test_files = os.path.join(data_dir, TEST_FILENAME)

  if not os.path.exists(train_files):
    logger.info("Downloading train file...")
    deepchem.utils.data_utils.download_url(url=TRAIN_URL, dest_dir=data_dir)
    logger.info("Training file download complete.")

    logger.info("Downloading validation file...")
    deepchem.utils.data_utils.download_url(url=VALID_URL, dest_dir=data_dir)
    logger.info("Validation file download complete.")

    logger.info("Downloading test file...")
    deepchem.utils.data_utils.download_url(url=TEST_URL, dest_dir=data_dir)
    logger.info("Test file download complete")

  # Featurize the FACTORS dataset
  logger.info("About to featurize the FACTORS dataset")
  featurizer = deepchem.feat.UserDefinedFeaturizer(merck_descriptors)
  loader = deepchem.data.UserCSVLoader(
      tasks=FACTORS_tasks, id_field="Molecule", featurizer=featurizer)

  logger.info("Featurizing the train dataset...")
  train_dataset = loader.featurize(train_files, shard_size=shard_size)

  logger.info("Featurizing the validation dataset...")
  valid_dataset = loader.featurize(valid_files, shard_size=shard_size)

  logger.info("Featurizing the test dataset...")
  test_dataset = loader.featurize(test_files, shard_size=shard_size)

  logger.info("Remove missing entries from dataset")
  remove_missing_entries(train_dataset)
  remove_missing_entries(valid_dataset)
  remove_missing_entries(test_dataset)

  # Shuffle the training data
  logger.info("Shuffling the training dataset")
  train_dataset.sparse_shuffle()

  # Apply transformations
  logger.info("Transforming datasets with transformers")
  transformers = get_transformers(train_dataset)

  for transformer in transformers:
    logger.info("Performing transformations with {}".format(
        transformer.__class__.__name__))

    logger.info("Transforming the training dataset...")
    train_dataset = transformer.transform(train_dataset)

    logger.info("Transforming the validation dataset...")
    valid_dataset = transformer.transform(valid_dataset)

    logger.info("Transforming the test dataset...")
    test_dataset = transformer.transform(test_dataset)

  logger.info("Transformations complete.")
  logger.info("Moving datasets to corresponding directories")

  train_dataset.move(train_dir)
  logger.info("Train dataset moved.")

  valid_dataset.move(valid_dir)
  logger.info("Validation dataset moved.")

  test_dataset.move(test_dir)
  logger.info("Test dataset moved.")

  time2 = time.time()

  ########## TIMING ################
  logger.info("TIMING: FACTORS fitting took %0.3f s" % (time2 - time1))

  return train_dataset, valid_dataset, test_dataset


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1808')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/kinase_datasets.py: 49-142
</a>
<div class="mid" id="frag1808" style="display:none"><pre>
def gen_kinase(KINASE_tasks,
               train_dir,
               valid_dir,
               test_dir,
               data_dir,
               shard_size=2000):

  time1 = time.time()

  train_files = os.path.join(data_dir, TRAIN_FILENAME)
  valid_files = os.path.join(data_dir, VALID_FILENAME)
  test_files = os.path.join(data_dir, TEST_FILENAME)

  # Download files if they don't exist

  if not os.path.exists(train_files):

    logger.info("Downloading training file...")
    deepchem.utils.data_utils.download_url(url=TRAIN_URL, dest_dir=data_dir)
    logger.info("Training file download complete.")

    logger.info("Downloading validation file...")
    deepchem.utils.data_utils.download_url(url=VALID_URL, dest_dir=data_dir)
    logger.info("Validation file download complete.")

    logger.info("Downloading test file...")
    deepchem.utils.data_utils.download_url(url=TEST_URL, dest_dir=data_dir)
    logger.info("Test file download complete")

  # Featurize the KINASE dataset
  logger.info("About to featurize KINASE dataset.")
  featurizer = deepchem.feat.UserDefinedFeaturizer(merck_descriptors)

  loader = deepchem.data.UserCSVLoader(
      tasks=KINASE_tasks, id_field="Molecule", featurizer=featurizer)

  logger.info("Featurizing train datasets...")
  train_dataset = loader.featurize(
      input_files=train_files, shard_size=shard_size)

  logger.info("Featurizing validation datasets...")
  valid_dataset = loader.featurize(
      input_files=valid_files, shard_size=shard_size)

  logger.info("Featurizing test datasets....")
  test_dataset = loader.featurize(input_files=test_files, shard_size=shard_size)

  logger.info("Remove missing entries from dataset")
  remove_missing_entries(train_dataset)
  remove_missing_entries(valid_dataset)
  remove_missing_entries(test_dataset)

  # Shuffle the training data
  logger.info("Shuffling the training dataset")
  train_dataset.sparse_shuffle()

  # Apply transformations
  logger.info("Transformating datasets with transformers")
  transformers = get_transformers(train_dataset)

  for transformer in transformers:
    logger.info("Performing transformations with {}".format(
        transformer.__class__.__name__))

    logger.info("Transforming the training dataset...")
    train_dataset = transformer.transform(train_dataset)

    logger.info("Transforming the validation dataset...")
    valid_dataset = transformer.transform(valid_dataset)

    logger.info("Transforming the test dataset...")
    test_dataset = transformer.transform(test_dataset)

  logger.info("Transformations complete.")
  logger.info("Moving datasets to corresponding directories")

  train_dataset.move(train_dir)
  logger.info("Train dataset moved.")

  valid_dataset.move(valid_dir)
  logger.info("Validation dataset moved.")

  test_dataset.move(test_dir)
  logger.info("Test dataset moved.")

  time2 = time.time()

  ##### TIMING ######

  logger.info("TIMING: KINASE fitting took %0.3f s" % (time2 - time1))

  return train_dataset, valid_dataset, test_dataset


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1800')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/uv_datasets.py: 50-138
</a>
<div class="mid" id="frag1800" style="display:none"><pre>
def gen_uv(UV_tasks, data_dir, train_dir, valid_dir, test_dir, shard_size=2000):
  """Loading the UV dataset; does not do train/test split"""

  time1 = time.time()

  train_files = os.path.join(data_dir, TRAIN_FILENAME)
  valid_files = os.path.join(data_dir, VALID_FILENAME)
  test_files = os.path.join(data_dir, TEST_FILENAME)

  # Download files if they don't exist

  if not os.path.exists(train_files):

    logger.info("Downloading training file...")
    deepchem.utils.data_utils.download_url(url=TRAIN_URL, dest_dir=data_dir)
    logger.info("Training file download complete.")

    logger.info("Downloading validation file...")
    deepchem.utils.data_utils.download_url(url=VALID_URL, dest_dir=data_dir)
    logger.info("Validation file download complete.")

    logger.info("Downloading test file...")
    deepchem.utils.data_utils.download_url(url=TEST_URL, dest_dir=data_dir)
    logger.info("Test file download complete")

  # Featurizing datasets
  logger.info("About to featurize UV dataset.")
  featurizer = deepchem.feat.UserDefinedFeaturizer(merck_descriptors)
  loader = deepchem.data.UserCSVLoader(
      tasks=UV_tasks, id_field="Molecule", featurizer=featurizer)

  logger.info("Featurizing train datasets...")
  train_dataset = loader.featurize(
      input_files=train_files, shard_size=shard_size)

  logger.info("Featurizing validation datasets...")
  valid_dataset = loader.featurize(
      input_files=valid_files, shard_size=shard_size)

  logger.info("Featurizing test datasets....")
  test_dataset = loader.featurize(input_files=test_files, shard_size=shard_size)

  # Missing entry removal
  logger.info("Removing missing entries from dataset.")
  remove_missing_entries(train_dataset)
  remove_missing_entries(valid_dataset)
  remove_missing_entries(test_dataset)

  # Shuffle the training data
  logger.info("Shuffling the training dataset")
  train_dataset.sparse_shuffle()

  # Apply transformations
  logger.info("Starting transformations")
  transformers = get_transformers(train_dataset)

  for transformer in transformers:
    logger.info("Performing transformations with {}".format(
        transformer.__class__.__name__))

    logger.info("Transforming the training dataset...")
    train_dataset = transformer.transform(train_dataset)

    logger.info("Transforming the validation dataset...")
    valid_dataset = transformer.transform(valid_dataset)

    logger.info("Transforming the test dataset...")
    test_dataset = transformer.transform(test_dataset)

  logger.info("Transformations complete.")
  logger.info("Moving datasets to corresponding directories")

  train_dataset.move(train_dir)
  logger.info("Train dataset moved.")

  valid_dataset.move(valid_dir)
  logger.info("Validation dataset moved.")

  test_dataset.move(test_dir)
  logger.info("Test dataset moved.")

  time2 = time.time()

  ##### TIMING ###########
  logger.info("TIMING: UV fitting took %0.3f s" % (time2 - time1))

  return train_dataset, valid_dataset, test_dataset


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 138:</b> &nbsp; 3 fragments, nominal size 22 lines, similarity 79%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1756')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/factors_datasets.py: 137-201
</a>
<div class="mid" id="frag1756" style="display:none"><pre>
def load_factors(shard_size=2000, featurizer=None, split=None, reload=True):
  """Loads FACTOR dataset; does not do train/test split

  The Factors dataset is an in-house dataset from Merck that was first introduced in the following paper:
  Ramsundar, Bharath, et al. "Is multitask deep learning practical for pharma?." Journal of chemical information and modeling 57.8 (2017): 2068-2076.

  It contains 1500 Merck in-house compounds that were measured
  for IC50 of inhibition on 12 serine proteases. Unlike most of
  the other datasets featured in MoleculeNet, the Factors 
  collection does not have structures for the compounds tested
  since they were proprietary Merck compounds. However, the
  collection does feature pre-computed descriptors for these
  compounds.

  Note that the original train/valid/test split from the source
  data was preserved here, so this function doesn't allow for
  alternate modes of splitting. Similarly, since the source data
  came pre-featurized, it is not possible to apply alternative
  featurizations.

  Parameters
  ----------
  shard_size: int, optional
    Size of the DiskDataset shards to write on disk
  featurizer: optional
    Ignored since featurization pre-computed
  split: optional
    Ignored since split pre-computed
  reload: bool, optional
    Whether to automatically re-load from disk

  """

  FACTORS_tasks = [
      'T_00001', 'T_00002', 'T_00003', 'T_00004', 'T_00005', 'T_00006',
      'T_00007', 'T_00008', 'T_00009', 'T_00010', 'T_00011', 'T_00012'
  ]

  data_dir = deepchem.utils.data_utils.get_data_dir()
  data_dir = os.path.join(data_dir, "factors")

  if not os.path.exists(data_dir):
    os.mkdir(data_dir)

  train_dir = os.path.join(data_dir, "train_dir")
  valid_dir = os.path.join(data_dir, "valid_dir")
  test_dir = os.path.join(data_dir, "test_dir")

  if (os.path.exists(train_dir) and os.path.exists(valid_dir) and
      os.path.exists(test_dir)):

    logger.info("Reloading existing datasets")
    train_dataset = deepchem.data.DiskDataset(train_dir)
    valid_dataset = deepchem.data.DiskDataset(valid_dir)
    test_dataset = deepchem.data.DiskDataset(test_dir)

  else:
    logger.info("Featurizing datasets")
    train_dataset, valid_dataset, test_dataset = \
    gen_factors(FACTORS_tasks=FACTORS_tasks, data_dir=data_dir, train_dir=train_dir,
                valid_dir=valid_dir, test_dir=test_dir, shard_size=shard_size)

  transformers = get_transformers(train_dataset)

  return FACTORS_tasks, (train_dataset, valid_dataset,
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1801')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/uv_datasets.py: 139-196
</a>
<div class="mid" id="frag1801" style="display:none"><pre>
def load_uv(shard_size=2000, featurizer=None, split=None, reload=True):
  """Load UV dataset; does not do train/test split

  The UV dataset is an in-house dataset from Merck that was first introduced in the following paper:
  Ramsundar, Bharath, et al. "Is multitask deep learning practical for pharma?." Journal of chemical information and modeling 57.8 (2017): 2068-2076.

  The UV dataset tests 10,000 of Merck's internal compounds on
  190 absorption wavelengths between 210 and 400 nm. Unlike
  most of the other datasets featured in MoleculeNet, the UV
  collection does not have structures for the compounds tested
  since they were proprietary Merck compounds. However, the
  collection does feature pre-computed descriptors for these
  compounds.

  Note that the original train/valid/test split from the source
  data was preserved here, so this function doesn't allow for
  alternate modes of splitting. Similarly, since the source data
  came pre-featurized, it is not possible to apply alternative
  featurizations.

  Parameters
  ----------
  shard_size: int, optional
    Size of the DiskDataset shards to write on disk
  featurizer: optional
    Ignored since featurization pre-computed
  split: optional
    Ignored since split pre-computed
  reload: bool, optional
    Whether to automatically re-load from disk
  """

  data_dir = deepchem.utils.data_utils.get_data_dir()
  data_dir = os.path.join(data_dir, "UV")

  if not os.path.exists(data_dir):
    os.mkdir(data_dir)

  train_dir = os.path.join(data_dir, "train_dir")
  valid_dir = os.path.join(data_dir, "valid_dir")
  test_dir = os.path.join(data_dir, "test_dir")

  if (os.path.exists(train_dir) and os.path.exists(valid_dir) and
      os.path.exists(test_dir)):

    logger.info("Reloading existing datasets")
    train_dataset = deepchem.data.DiskDataset(train_dir)
    valid_dataset = deepchem.data.DiskDataset(valid_dir)
    test_dataset = deepchem.data.DiskDataset(test_dir)

  else:
    logger.info("Featurizing datasets")
    train_dataset, valid_dataset, test_dataset = \
    gen_uv(UV_tasks=UV_tasks, data_dir=data_dir, train_dir=train_dir,
           valid_dir=valid_dir, test_dir=test_dir, shard_size=shard_size)

  transformers = get_transformers(train_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1795')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/kaggle_datasets.py: 119-178
</a>
<div class="mid" id="frag1795" style="display:none"><pre>
def load_kaggle(shard_size=2000, featurizer=None, split=None, reload=True):
  """Loads kaggle datasets. Generates if not stored already.

  The Kaggle dataset is an in-house dataset from Merck that was first introduced in the following paper:

  Ma, Junshui, et al. "Deep neural nets as a method for quantitative structure–activity relationships." Journal of chemical information and modeling 55.2 (2015): 263-274.

  It contains 100,000 unique Merck in-house compounds that were
  measured on 15 enzyme inhibition and ADME/TOX datasets.
  Unlike most of the other datasets featured in MoleculeNet,
  the Kaggle collection does not have structures for the
  compounds tested since they were proprietary Merck compounds.
  However, the collection does feature pre-computed descriptors
  for these compounds.

  Note that the original train/valid/test split from the source
  data was preserved here, so this function doesn't allow for
  alternate modes of splitting. Similarly, since the source data
  came pre-featurized, it is not possible to apply alternative
  featurizations.

  Parameters
  ----------
  shard_size: int, optional
    Size of the DiskDataset shards to write on disk
  featurizer: optional
    Ignored since featurization pre-computed
  split: optional
    Ignored since split pre-computed
  reload: bool, optional
    Whether to automatically re-load from disk

  """
  KAGGLE_tasks = [
      '3A4', 'CB1', 'DPP4', 'HIVINT', 'HIV_PROT', 'LOGD', 'METAB', 'NK1', 'OX1',
      'OX2', 'PGP', 'PPB', 'RAT_F', 'TDI', 'THROMBIN'
  ]
  data_dir = deepchem.utils.data_utils.get_data_dir()

  data_dir = os.path.join(data_dir, "kaggle")
  if not os.path.exists(data_dir):
    os.mkdir(data_dir)
  train_dir = os.path.join(data_dir, "train_dir")
  valid_dir = os.path.join(data_dir, "valid_dir")
  test_dir = os.path.join(data_dir, "test_dir")

  if (os.path.exists(train_dir) and os.path.exists(valid_dir) and
      os.path.exists(test_dir)):
    logger.info("Reloading existing datasets")
    train_dataset = deepchem.data.DiskDataset(train_dir)
    valid_dataset = deepchem.data.DiskDataset(valid_dir)
    test_dataset = deepchem.data.DiskDataset(test_dir)
  else:
    logger.info("Featurizing datasets")
    train_dataset, valid_dataset, test_dataset = \
      gen_kaggle(KAGGLE_tasks, train_dir, valid_dir, test_dir, data_dir,
                  shard_size=shard_size)

  transformers = get_transformers(train_dataset)
  return KAGGLE_tasks, (train_dataset, valid_dataset,
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 139:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1759')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/bbbc_datasets.py: 23-35
</a>
<div class="mid" id="frag1759" style="display:none"><pre>
  def create_dataset(self) -&gt; Dataset:
    dataset_file = os.path.join(self.data_dir, "BBBC001_v1_images_tif.zip")
    labels_file = os.path.join(self.data_dir, "BBBC001_v1_counts.txt")
    if not os.path.exists(dataset_file):
      dc.utils.data_utils.download_url(
          url=BBBC1_IMAGE_URL, dest_dir=self.data_dir)
    if not os.path.exists(labels_file):
      dc.utils.data_utils.download_url(
          url=BBBC1_LABEL_URL, dest_dir=self.data_dir)
    loader = dc.data.ImageLoader()
    return loader.create_dataset(dataset_file, in_memory=False)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1761')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/load_function/bbbc_datasets.py: 78-90
</a>
<div class="mid" id="frag1761" style="display:none"><pre>
  def create_dataset(self) -&gt; Dataset:
    dataset_file = os.path.join(self.data_dir, "BBBC002_v1_images.zip")
    labels_file = os.path.join(self.data_dir, "BBBC002_v1_counts.txt.txt")
    if not os.path.exists(dataset_file):
      dc.utils.data_utils.download_url(
          url=BBBC2_IMAGE_URL, dest_dir=self.data_dir)
    if not os.path.exists(labels_file):
      dc.utils.data_utils.download_url(
          url=BBBC2_LABEL_URL, dest_dir=self.data_dir)
    loader = dc.data.ImageLoader()
    return loader.create_dataset(dataset_file, in_memory=False)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 140:</b> &nbsp; 3 fragments, nominal size 17 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1824')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/tests/test_molnet.py: 26-42
</a>
<div class="mid" id="frag1824" style="display:none"><pre>
  def test_delaney_graphconvreg(self):
    """Tests molnet benchmarking on delaney with graphconvreg."""
    datasets = ['delaney']
    model = 'graphconvreg'
    split = 'random'
    out_path = tempfile.mkdtemp()
    metric = [dc.metrics.Metric(dc.metrics.pearson_r2_score, np.mean)]
    run_benchmark(
        datasets, str(model), metric=metric, split=split, out_path=out_path)
    with open(os.path.join(out_path, 'results.csv'), newline='\n') as f:
      reader = csv.reader(f)
      for lastrow in reader:
        pass
      assert lastrow[-4] == 'valid'
      assert float(lastrow[-3]) &gt; 0.65
    os.remove(os.path.join(out_path, 'results.csv'))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1826')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/tests/test_molnet.py: 61-81
</a>
<div class="mid" id="frag1826" style="display:none"><pre>
  def test_clintox_multitask(self):
    """Tests molnet benchmarking on clintox with multitask network."""
    datasets = ['clintox']
    model = 'tf'
    split = 'random'
    out_path = tempfile.mkdtemp()
    metric = [dc.metrics.Metric(dc.metrics.roc_auc_score, np.mean)]
    run_benchmark(
        datasets,
        str(model),
        metric=metric,
        split=split,
        out_path=out_path,
        test=True)
    with open(os.path.join(out_path, 'results.csv'), newline='\n') as f:
      reader = csv.reader(f)
      for lastrow in reader:
        pass
      assert lastrow[-4] == 'test'
      assert float(lastrow[-3]) &gt; 0.7
    os.remove(os.path.join(out_path, 'results.csv'))
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1825')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/tests/test_molnet.py: 44-60
</a>
<div class="mid" id="frag1825" style="display:none"><pre>
  def test_qm7_multitask(self):
    """Tests molnet benchmarking on qm7 with multitask network."""
    datasets = ['qm7']
    model = 'tf_regression_ft'
    split = 'random'
    out_path = tempfile.mkdtemp()
    metric = [dc.metrics.Metric(dc.metrics.pearson_r2_score, np.mean)]
    run_benchmark(
        datasets, str(model), metric=metric, split=split, out_path=out_path)
    with open(os.path.join(out_path, 'results.csv'), newline='\n') as f:
      reader = csv.reader(f)
      for lastrow in reader:
        pass
      assert lastrow[-4] == 'valid'
      assert float(lastrow[-3]) &gt; 0.75
    os.remove(os.path.join(out_path, 'results.csv'))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 141:</b> &nbsp; 3 fragments, nominal size 11 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1829')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/tests/test_dnasim.py: 25-39
</a>
<div class="mid" id="frag1829" style="display:none"><pre>
  def test_motif_counting_simulation(self):
    "Test motif counting"
    params = {
        "motif_name": "TAL1_known4",
        "seq_length": 1000,
        "pos_counts": [5, 10],
        "neg_counts": [1, 2],
        "num_pos": 30,
        "num_neg": 30,
        "GC_fraction": 0.4
    }
    sequences, y, embed = dc.molnet.simulate_motif_counting(**params)
    assert sequences.shape == (60,)
    assert y.shape == (60, 1)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1832')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/tests/test_dnasim.py: 64-75
</a>
<div class="mid" id="frag1832" style="display:none"><pre>
  def test_single_motif_detection(self):
    "Test single motif detection"
    params = {
        "motif_name": "TAL1_known4",
        "seq_length": 1000,
        "num_pos": 30,
        "num_neg": 30,
        "GC_fraction": 0.4
    }
    sequences, y, embed = dc.molnet.simulate_single_motif_detection(**params)
    assert sequences.shape == (60,)
    assert y.shape == (60, 1)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1831')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/tests/test_dnasim.py: 51-63
</a>
<div class="mid" id="frag1831" style="display:none"><pre>
  def test_motif_density(self):
    "Test motif density"
    params = {
        "motif_name": "TAL1_known4",
        "seq_length": 1000,
        "num_seqs": 30,
        "min_counts": 2,
        "max_counts": 4,
        "GC_fraction": 0.4
    }
    sequences, embed = dc.molnet.motif_density(**params)
    assert sequences.shape == (30,)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 142:</b> &nbsp; 2 fragments, nominal size 288 lines, similarity 79%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1833')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/run_benchmark_models.py: 19-382
</a>
<div class="mid" id="frag1833" style="display:none"><pre>
def benchmark_classification(train_dataset,
                             valid_dataset,
                             test_dataset,
                             tasks,
                             transformers,
                             n_features,
                             metric,
                             model,
                             test=False,
                             hyper_parameters=None,
                             seed=123):
  """
  Calculate performance of different models on the specific dataset &amp; tasks

  Parameters
  ----------
  train_dataset: dataset struct
      dataset used for model training and evaluation
  valid_dataset: dataset struct
      dataset only used for model evaluation (and hyperparameter tuning)
  test_dataset: dataset struct
      dataset only used for model evaluation
  tasks: list of string
      list of targets(tasks, datasets)
  transformers: dc.trans.Transformer struct
      transformer used for model evaluation
  n_features: integer
      number of features, or length of binary fingerprints
  metric: list of dc.metrics.Metric objects
      metrics used for evaluation
  model: string,  optional
      choice of model
      'rf', 'tf', 'tf_robust', 'logreg', 'irv', 'graphconv', 'dag', 'xgb',
      'weave', 'kernelsvm', 'textcnn', 'mpnn'
  test: boolean, optional
      whether to calculate test_set performance
  hyper_parameters: dict, optional (default=None)
      hyper parameters for designated model, None = use preset values


  Returns
  -------
  train_scores : dict
	predicting results(AUC) on training set
  valid_scores : dict
	predicting results(AUC) on valid set
  test_scores : dict
	predicting results(AUC) on test set


  """
  train_scores = {}
  valid_scores = {}
  test_scores = {}

  assert model in [
      'rf', 'tf', 'tf_robust', 'logreg', 'irv', 'graphconv', 'dag', 'xgb',
      'weave', 'kernelsvm', 'textcnn', 'mpnn'
  ]
  if hyper_parameters is None:
    hyper_parameters = hps[model]
  model_name = model

  if model_name == 'tf':
    layer_sizes = hyper_parameters['layer_sizes']
    weight_init_stddevs = hyper_parameters['weight_init_stddevs']
    bias_init_consts = hyper_parameters['bias_init_consts']
    dropouts = hyper_parameters['dropouts']
    penalty = hyper_parameters['penalty']
    penalty_type = hyper_parameters['penalty_type']
    batch_size = hyper_parameters['batch_size']
    nb_epoch = hyper_parameters['nb_epoch']
    learning_rate = hyper_parameters['learning_rate']

    # Building tensorflow MultitaskDNN model
    model = deepchem.models.MultitaskClassifier(
        len(tasks),
        n_features,
        layer_sizes=layer_sizes,
        weight_init_stddevs=weight_init_stddevs,
        bias_init_consts=bias_init_consts,
        dropouts=dropouts,
        weight_decay_penalty=penalty,
        weight_decay_penalty_type=penalty_type,
        batch_size=batch_size,
        learning_rate=learning_rate,
        random_seed=seed)

  elif model_name == 'tf_robust':
    layer_sizes = hyper_parameters['layer_sizes']
    weight_init_stddevs = hyper_parameters['weight_init_stddevs']
    bias_init_consts = hyper_parameters['bias_init_consts']
    dropouts = hyper_parameters['dropouts']

    bypass_layer_sizes = hyper_parameters['bypass_layer_sizes']
    bypass_weight_init_stddevs = hyper_parameters['bypass_weight_init_stddevs']
    bypass_bias_init_consts = hyper_parameters['bypass_bias_init_consts']
    bypass_dropouts = hyper_parameters['bypass_dropouts']

    penalty = hyper_parameters['penalty']
    penalty_type = hyper_parameters['penalty_type']
    batch_size = hyper_parameters['batch_size']
    nb_epoch = hyper_parameters['nb_epoch']
    learning_rate = hyper_parameters['learning_rate']

    # Building tensorflow robust MultitaskDNN model
    model = deepchem.models.RobustMultitaskClassifier(
        len(tasks),
        n_features,
        layer_sizes=layer_sizes,
        weight_init_stddevs=weight_init_stddevs,
        bias_init_consts=bias_init_consts,
        dropouts=dropouts,
        bypass_layer_sizes=bypass_layer_sizes,
        bypass_weight_init_stddevs=bypass_weight_init_stddevs,
        bypass_bias_init_consts=bypass_bias_init_consts,
        bypass_dropouts=bypass_dropouts,
        weight_decay_penalty=penalty,
        weight_decay_penalty_type=penalty_type,
        batch_size=batch_size,
        learning_rate=learning_rate,
        random_seed=seed)

  elif model_name == 'logreg':
    penalty = hyper_parameters['penalty']
    penalty_type = hyper_parameters['penalty_type']
    nb_epoch = None

    # Building scikit logistic regression model
    def model_builder(model_dir):
      sklearn_model = LogisticRegression(
          penalty=penalty_type,
          C=1. / penalty,
          class_weight="balanced",
          n_jobs=-1)
      return deepchem.models.sklearn_models.SklearnModel(
          sklearn_model, model_dir)

    model = deepchem.models.multitask.SingletaskToMultitask(
        tasks, model_builder)

  elif model_name == 'irv':
    penalty = hyper_parameters['penalty']
    batch_size = hyper_parameters['batch_size']
    nb_epoch = hyper_parameters['nb_epoch']
    learning_rate = hyper_parameters['learning_rate']
    n_K = hyper_parameters['n_K']

    # Transform fingerprints to IRV features
    transformer = deepchem.trans.IRVTransformer(n_K, len(tasks), train_dataset)
    train_dataset = transformer.transform(train_dataset)
    valid_dataset = transformer.transform(valid_dataset)
    if test:
      test_dataset = transformer.transform(test_dataset)

    # Building tensorflow IRV model
    model = deepchem.models.TensorflowMultitaskIRVClassifier(
        len(tasks),
        K=n_K,
        penalty=penalty,
        batch_size=batch_size,
        learning_rate=learning_rate,
        random_seed=seed,
        mode='classification')

  elif model_name == 'graphconv':
    batch_size = hyper_parameters['batch_size']
    nb_epoch = hyper_parameters['nb_epoch']
    learning_rate = hyper_parameters['learning_rate']
    n_filters = hyper_parameters['n_filters']
    n_fully_connected_nodes = hyper_parameters['n_fully_connected_nodes']

    model = deepchem.models.GraphConvModel(
        len(tasks),
        graph_conv_layers=[n_filters] * 2,
        dense_layer_size=n_fully_connected_nodes,
        batch_size=batch_size,
        learning_rate=learning_rate,
        random_seed=seed,
        mode='classification')

  elif model_name == 'dag':
    batch_size = hyper_parameters['batch_size']
    nb_epoch = hyper_parameters['nb_epoch']
    learning_rate = hyper_parameters['learning_rate']
    n_graph_feat = hyper_parameters['n_graph_feat']
    default_max_atoms = hyper_parameters['default_max_atoms']

    max_atoms_train = max([mol.get_num_atoms() for mol in train_dataset.X])
    max_atoms_valid = max([mol.get_num_atoms() for mol in valid_dataset.X])
    max_atoms_test = max([mol.get_num_atoms() for mol in test_dataset.X])
    max_atoms = max([max_atoms_train, max_atoms_valid, max_atoms_test])
    max_atoms = min([max_atoms, default_max_atoms])
    print('Maximum number of atoms: %i' % max_atoms)
    reshard_size = 256
    transformer = deepchem.trans.DAGTransformer(max_atoms=max_atoms)
    train_dataset.reshard(reshard_size)
    train_dataset = transformer.transform(train_dataset)
    valid_dataset.reshard(reshard_size)
    valid_dataset = transformer.transform(valid_dataset)
    if test:
      test_dataset.reshard(reshard_size)
      test_dataset = transformer.transform(test_dataset)

    model = deepchem.models.DAGModel(
        len(tasks),
        max_atoms=max_atoms,
        n_atom_feat=n_features,
        n_graph_feat=n_graph_feat,
        n_outputs=30,
        batch_size=batch_size,
        learning_rate=learning_rate,
        random_seed=seed,
        use_queue=False,
        mode='classification')

  elif model_name == 'weave':
    batch_size = hyper_parameters['batch_size']
    nb_epoch = hyper_parameters['nb_epoch']
    learning_rate = hyper_parameters['learning_rate']
    n_graph_feat = hyper_parameters['n_graph_feat']
    n_pair_feat = hyper_parameters['n_pair_feat']

    model = deepchem.models.WeaveModel(
        len(tasks),
        n_atom_feat=n_features,
        n_pair_feat=n_pair_feat,
        n_hidden=50,
        n_graph_feat=n_graph_feat,
        batch_size=batch_size,
        learning_rate=learning_rate,
        use_queue=False,
        random_seed=seed,
        mode='classification')

  elif model_name == 'textcnn':
    batch_size = hyper_parameters['batch_size']
    nb_epoch = hyper_parameters['nb_epoch']
    learning_rate = hyper_parameters['learning_rate']
    n_embedding = hyper_parameters['n_embedding']
    filter_sizes = hyper_parameters['filter_sizes']
    num_filters = hyper_parameters['num_filters']

    all_data = deepchem.data.DiskDataset.merge(
        [train_dataset, valid_dataset, test_dataset])
    char_dict, length = deepchem.models.TextCNNModel.build_char_dict(all_data)

    model = deepchem.models.TextCNNModel(
        len(tasks),
        char_dict,
        seq_length=length,
        n_embedding=n_embedding,
        filter_sizes=filter_sizes,
        num_filters=num_filters,
        learning_rate=learning_rate,
        batch_size=batch_size,
        use_queue=False,
        random_seed=seed,
        mode='classification')

  elif model_name == 'mpnn':
    batch_size = hyper_parameters['batch_size']
    nb_epoch = hyper_parameters['nb_epoch']
    learning_rate = hyper_parameters['learning_rate']
    T = hyper_parameters['T']
    M = hyper_parameters['M']

    model = deepchem.models.MPNNModel(
        len(tasks),
        n_atom_feat=n_features[0],
        n_pair_feat=n_features[1],
        n_hidden=n_features[0],
        T=T,
        M=M,
        batch_size=batch_size,
        learning_rate=learning_rate,
        use_queue=False,
        mode="classification")

  elif model_name == 'rf':
    n_estimators = hyper_parameters['n_estimators']
    nb_epoch = None

    # Building scikit random forest model
    def model_builder(model_dir):
      sklearn_model = RandomForestClassifier(
          class_weight="balanced", n_estimators=n_estimators, n_jobs=-1)
      return deepchem.models.sklearn_models.SklearnModel(
          sklearn_model, model_dir)

    model = deepchem.models.multitask.SingletaskToMultitask(
        tasks, model_builder)

  elif model_name == 'kernelsvm':
    C = hyper_parameters['C']
    gamma = hyper_parameters['gamma']
    nb_epoch = None

    # Building scikit learn Kernel SVM model
    def model_builder(model_dir):
      sklearn_model = SVC(
          C=C, gamma=gamma, class_weight="balanced", probability=True)
      return deepchem.models.SklearnModel(sklearn_model, model_dir)

    model = deepchem.models.multitask.SingletaskToMultitask(
        tasks, model_builder)

  elif model_name == 'xgb':
    max_depth = hyper_parameters['max_depth']
    learning_rate = hyper_parameters['learning_rate']
    n_estimators = hyper_parameters['n_estimators']
    gamma = hyper_parameters['gamma']
    min_child_weight = hyper_parameters['min_child_weight']
    max_delta_step = hyper_parameters['max_delta_step']
    subsample = hyper_parameters['subsample']
    colsample_bytree = hyper_parameters['colsample_bytree']
    colsample_bylevel = hyper_parameters['colsample_bylevel']
    reg_alpha = hyper_parameters['reg_alpha']
    reg_lambda = hyper_parameters['reg_lambda']
    scale_pos_weight = hyper_parameters['scale_pos_weight']
    base_score = hyper_parameters['base_score']
    seed = hyper_parameters['seed']
    early_stopping_rounds = hyper_parameters['early_stopping_rounds']
    nb_epoch = None

    esr = {'early_stopping_rounds': early_stopping_rounds}

    # Building xgboost classification model
    def model_builder(model_dir):
      import xgboost
      xgboost_model = xgboost.XGBClassifier(
          max_depth=max_depth,
          learning_rate=learning_rate,
          n_estimators=n_estimators,
          gamma=gamma,
          min_child_weight=min_child_weight,
          max_delta_step=max_delta_step,
          subsample=subsample,
          colsample_bytree=colsample_bytree,
          colsample_bylevel=colsample_bylevel,
          reg_alpha=reg_alpha,
          reg_lambda=reg_lambda,
          scale_pos_weight=scale_pos_weight,
          base_score=base_score,
          seed=seed)
      return deepchem.models.xgboost_models.XGBoostModel(
          xgboost_model, model_dir, **esr)

    model = deepchem.models.multitask.SingletaskToMultitask(
        tasks, model_builder)

  if nb_epoch is None:
    model.fit(train_dataset)
  else:
    model.fit(train_dataset, nb_epoch=nb_epoch)

  train_scores[model_name] = model.evaluate(train_dataset, metric, transformers)
  valid_scores[model_name] = model.evaluate(valid_dataset, metric, transformers)
  if test:
    test_scores[model_name] = model.evaluate(test_dataset, metric, transformers)

  return train_scores, valid_scores, test_scores


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1838')" href="javascript:;">
deepchem-2.4.0/deepchem/molnet/run_benchmark_models.py: 383-771
</a>
<div class="mid" id="frag1838" style="display:none"><pre>
def benchmark_regression(train_dataset,
                         valid_dataset,
                         test_dataset,
                         tasks,
                         transformers,
                         n_features,
                         metric,
                         model,
                         test=False,
                         hyper_parameters=None,
                         seed=123):
  """
  Calculate performance of different models on the specific dataset &amp; tasks

  Parameters
  ----------
  train_dataset: dataset struct
      dataset used for model training and evaluation
  valid_dataset: dataset struct
      dataset only used for model evaluation (and hyperparameter tuning)
  test_dataset: dataset struct
      dataset only used for model evaluation
  tasks: list of string
      list of targets(tasks, datasets)
  transformers: dc.trans.Transformer struct
      transformer used for model evaluation
  n_features: integer
      number of features, or length of binary fingerprints
  metric: list of dc.metrics.Metric objects
      metrics used for evaluation
  model: string, optional
      choice of model
      'tf_regression', 'tf_regression_ft', 'rf_regression', 'graphconvreg',
      'dtnn', 'dag_regression', 'xgb_regression', 'weave_regression',
      'textcnn_regression', 'krr', 'ani', 'krr_ft', 'mpnn'
  test: boolean, optional
      whether to calculate test_set performance
  hyper_parameters: dict, optional (default=None)
      hyper parameters for designated model, None = use preset values


  Returns
  -------
  train_scores : dict
	predicting results(R2) on training set
  valid_scores : dict
	predicting results(R2) on valid set
  test_scores : dict
	predicting results(R2) on test set

  """
  train_scores = {}
  valid_scores = {}
  test_scores = {}

  assert model in [
      'tf_regression', 'tf_regression_ft', 'rf_regression', 'graphconvreg',
      'dtnn', 'dag_regression', 'xgb_regression', 'weave_regression',
      'textcnn_regression', 'krr', 'ani', 'krr_ft', 'mpnn'
  ]
  import xgboost
  if hyper_parameters is None:
    hyper_parameters = hps[model]
  model_name = model

  if model_name == 'tf_regression':
    layer_sizes = hyper_parameters['layer_sizes']
    weight_init_stddevs = hyper_parameters['weight_init_stddevs']
    bias_init_consts = hyper_parameters['bias_init_consts']
    dropouts = hyper_parameters['dropouts']
    penalty = hyper_parameters['penalty']
    penalty_type = hyper_parameters['penalty_type']
    batch_size = hyper_parameters['batch_size']
    nb_epoch = hyper_parameters['nb_epoch']
    learning_rate = hyper_parameters['learning_rate']

    model = deepchem.models.MultitaskRegressor(
        len(tasks),
        n_features,
        layer_sizes=layer_sizes,
        weight_init_stddevs=weight_init_stddevs,
        bias_init_consts=bias_init_consts,
        dropouts=dropouts,
        weight_decay_penalty=penalty,
        weight_decay_penalty_type=penalty_type,
        batch_size=batch_size,
        learning_rate=learning_rate,
        seed=seed)

  elif model_name == 'tf_regression_ft':
    layer_sizes = hyper_parameters['layer_sizes']
    weight_init_stddevs = hyper_parameters['weight_init_stddevs']
    bias_init_consts = hyper_parameters['bias_init_consts']
    dropouts = hyper_parameters['dropouts']
    penalty = hyper_parameters['penalty']
    penalty_type = hyper_parameters['penalty_type']
    batch_size = hyper_parameters['batch_size']
    nb_epoch = hyper_parameters['nb_epoch']
    learning_rate = hyper_parameters['learning_rate']
    fit_transformers = [hyper_parameters['fit_transformers'](train_dataset)]

    model = deepchem.models.MultitaskFitTransformRegressor(
        n_tasks=len(tasks),
        n_features=n_features,
        layer_sizes=layer_sizes,
        weight_init_stddevs=weight_init_stddevs,
        bias_init_consts=bias_init_consts,
        dropouts=dropouts,
        weight_decay_penalty=penalty,
        weight_decay_penalty_type=penalty_type,
        batch_size=batch_size,
        learning_rate=learning_rate,
        fit_transformers=fit_transformers,
        n_eval=10,
        seed=seed)

  elif model_name == 'graphconvreg':
    batch_size = hyper_parameters['batch_size']
    nb_epoch = hyper_parameters['nb_epoch']
    learning_rate = hyper_parameters['learning_rate']
    n_filters = hyper_parameters['n_filters']
    n_fully_connected_nodes = hyper_parameters['n_fully_connected_nodes']

    model = deepchem.models.GraphConvModel(
        len(tasks),
        graph_conv_layers=[n_filters] * 2,
        dense_layer_size=n_fully_connected_nodes,
        batch_size=batch_size,
        learning_rate=learning_rate,
        random_seed=seed,
        mode='regression')

  elif model_name == 'dtnn':
    batch_size = hyper_parameters['batch_size']
    nb_epoch = hyper_parameters['nb_epoch']
    learning_rate = hyper_parameters['learning_rate']
    n_embedding = hyper_parameters['n_embedding']
    n_distance = hyper_parameters['n_distance']
    assert len(n_features) == 2, 'DTNN is only applicable to qm datasets'

    model = deepchem.models.DTNNModel(
        len(tasks),
        n_embedding=n_embedding,
        n_distance=n_distance,
        batch_size=batch_size,
        learning_rate=learning_rate,
        random_seed=seed,
        output_activation=False,
        use_queue=False,
        mode='regression')

  elif model_name == 'dag_regression':
    batch_size = hyper_parameters['batch_size']
    nb_epoch = hyper_parameters['nb_epoch']
    learning_rate = hyper_parameters['learning_rate']
    n_graph_feat = hyper_parameters['n_graph_feat']
    default_max_atoms = hyper_parameters['default_max_atoms']

    max_atoms_train = max([mol.get_num_atoms() for mol in train_dataset.X])
    max_atoms_valid = max([mol.get_num_atoms() for mol in valid_dataset.X])
    max_atoms_test = max([mol.get_num_atoms() for mol in test_dataset.X])
    max_atoms = max([max_atoms_train, max_atoms_valid, max_atoms_test])
    max_atoms = min([max_atoms, default_max_atoms])
    print('Maximum number of atoms: %i' % max_atoms)
    reshard_size = 256
    transformer = deepchem.trans.DAGTransformer(max_atoms=max_atoms)
    train_dataset.reshard(reshard_size)
    train_dataset = transformer.transform(train_dataset)
    valid_dataset.reshard(reshard_size)
    valid_dataset = transformer.transform(valid_dataset)
    if test:
      test_dataset.reshard(reshard_size)
      test_dataset = transformer.transform(test_dataset)

    model = deepchem.models.DAGModel(
        len(tasks),
        max_atoms=max_atoms,
        n_atom_feat=n_features,
        n_graph_feat=n_graph_feat,
        n_outputs=30,
        batch_size=batch_size,
        learning_rate=learning_rate,
        random_seed=seed,
        use_queue=False,
        mode='regression')

  elif model_name == 'weave_regression':
    batch_size = hyper_parameters['batch_size']
    nb_epoch = hyper_parameters['nb_epoch']
    learning_rate = hyper_parameters['learning_rate']
    n_graph_feat = hyper_parameters['n_graph_feat']
    n_pair_feat = hyper_parameters['n_pair_feat']

    model = deepchem.models.WeaveModel(
        len(tasks),
        n_atom_feat=n_features,
        n_pair_feat=n_pair_feat,
        n_hidden=50,
        n_graph_feat=n_graph_feat,
        batch_size=batch_size,
        learning_rate=learning_rate,
        use_queue=False,
        random_seed=seed,
        mode='regression')

  elif model_name == 'textcnn_regression':
    batch_size = hyper_parameters['batch_size']
    nb_epoch = hyper_parameters['nb_epoch']
    learning_rate = hyper_parameters['learning_rate']
    n_embedding = hyper_parameters['n_embedding']
    filter_sizes = hyper_parameters['filter_sizes']
    num_filters = hyper_parameters['num_filters']

    char_dict, length = deepchem.models.TextCNNModel.build_char_dict(
        train_dataset)

    model = deepchem.models.TextCNNModel(
        len(tasks),
        char_dict,
        seq_length=length,
        n_embedding=n_embedding,
        filter_sizes=filter_sizes,
        num_filters=num_filters,
        learning_rate=learning_rate,
        batch_size=batch_size,
        use_queue=False,
        random_seed=seed,
        mode='regression')

  elif model_name == 'ani':
    batch_size = hyper_parameters['batch_size']
    nb_epoch = hyper_parameters['nb_epoch']
    learning_rate = hyper_parameters['learning_rate']
    layer_structures = hyper_parameters['layer_structures']

    assert len(n_features) == 2, 'ANI is only applicable to qm datasets'
    max_atoms = n_features[0]
    atom_number_cases = np.unique(
        np.concatenate([
            train_dataset.X[:, :, 0], valid_dataset.X[:, :, 0],
            test_dataset.X[:, :, 0]
        ]))

    atom_number_cases = atom_number_cases.astype(int).tolist()
    try:
      # Remove token for paddings
      atom_number_cases.remove(0)
    except:
      pass
    ANItransformer = deepchem.trans.ANITransformer(
        max_atoms=max_atoms, atom_cases=atom_number_cases)
    train_dataset = ANItransformer.transform(train_dataset)
    valid_dataset = ANItransformer.transform(valid_dataset)
    if test:
      test_dataset = ANItransformer.transform(test_dataset)
    n_feat = ANItransformer.get_num_feats() - 1

    model = deepchem.models.ANIRegression(
        len(tasks),
        max_atoms,
        n_feat,
        layer_structures=layer_structures,
        atom_number_cases=atom_number_cases,
        batch_size=batch_size,
        learning_rate=learning_rate,
        use_queue=False,
        mode="regression",
        random_seed=seed)

  elif model_name == 'mpnn':
    batch_size = hyper_parameters['batch_size']
    nb_epoch = hyper_parameters['nb_epoch']
    learning_rate = hyper_parameters['learning_rate']
    T = hyper_parameters['T']
    M = hyper_parameters['M']

    model = deepchem.models.MPNNModel(
        len(tasks),
        n_atom_feat=n_features[0],
        n_pair_feat=n_features[1],
        n_hidden=n_features[0],
        T=T,
        M=M,
        batch_size=batch_size,
        learning_rate=learning_rate,
        use_queue=False,
        mode="regression")

  elif model_name == 'rf_regression':
    n_estimators = hyper_parameters['n_estimators']
    nb_epoch = None

    # Building scikit random forest model
    def model_builder(model_dir):
      sklearn_model = RandomForestRegressor(
          n_estimators=n_estimators, n_jobs=-1)
      return deepchem.models.sklearn_models.SklearnModel(
          sklearn_model, model_dir)

    model = deepchem.models.multitask.SingletaskToMultitask(
        tasks, model_builder)

  elif model_name == 'krr':
    alpha = hyper_parameters['alpha']
    nb_epoch = None

    # Building scikit learn Kernel Ridge Regression model
    def model_builder(model_dir):
      sklearn_model = KernelRidge(kernel="rbf", alpha=alpha)
      return deepchem.models.SklearnModel(sklearn_model, model_dir)

    model = deepchem.models.multitask.SingletaskToMultitask(
        tasks, model_builder)

  elif model_name == 'krr_ft':
    alpha = hyper_parameters['alpha']
    nb_epoch = None

    ft_transformer = deepchem.trans.CoulombFitTransformer(train_dataset)
    train_dataset = ft_transformer.transform(train_dataset)
    valid_dataset = ft_transformer.transform(valid_dataset)
    test_dataset = ft_transformer.transform(test_dataset)

    # Building scikit learn Kernel Ridge Regression model
    def model_builder(model_dir):
      sklearn_model = KernelRidge(kernel="rbf", alpha=alpha)
      return deepchem.models.SklearnModel(sklearn_model, model_dir)

    model = deepchem.models.multitask.SingletaskToMultitask(
        tasks, model_builder)

  elif model_name == 'xgb_regression':
    max_depth = hyper_parameters['max_depth']
    learning_rate = hyper_parameters['learning_rate']
    n_estimators = hyper_parameters['n_estimators']
    gamma = hyper_parameters['gamma']
    min_child_weight = hyper_parameters['min_child_weight']
    max_delta_step = hyper_parameters['max_delta_step']
    subsample = hyper_parameters['subsample']
    colsample_bytree = hyper_parameters['colsample_bytree']
    colsample_bylevel = hyper_parameters['colsample_bylevel']
    reg_alpha = hyper_parameters['reg_alpha']
    reg_lambda = hyper_parameters['reg_lambda']
    scale_pos_weight = hyper_parameters['scale_pos_weight']
    base_score = hyper_parameters['base_score']
    seed = hyper_parameters['seed']
    early_stopping_rounds = hyper_parameters['early_stopping_rounds']
    nb_epoch = None

    esr = {'early_stopping_rounds': early_stopping_rounds}

    # Building xgboost regression model
    def model_builder(model_dir):
      xgboost_model = xgboost.XGBRegressor(
          max_depth=max_depth,
          learning_rate=learning_rate,
          n_estimators=n_estimators,
          gamma=gamma,
          min_child_weight=min_child_weight,
          max_delta_step=max_delta_step,
          subsample=subsample,
          colsample_bytree=colsample_bytree,
          colsample_bylevel=colsample_bylevel,
          reg_alpha=reg_alpha,
          reg_lambda=reg_lambda,
          scale_pos_weight=scale_pos_weight,
          base_score=base_score,
          seed=seed)
      return deepchem.models.xgboost_models.XGBoostModel(
          xgboost_model, model_dir, **esr)

    model = deepchem.models.multitask.SingletaskToMultitask(
        tasks, model_builder)

  print('-----------------------------')
  print('Start fitting: %s' % model_name)
  if nb_epoch is None:
    model.fit(train_dataset)
  else:
    model.fit(train_dataset, nb_epoch=nb_epoch)

  train_scores[model_name] = model.evaluate(train_dataset, metric, transformers)
  valid_scores[model_name] = model.evaluate(valid_dataset, metric, transformers)
  if test:
    test_scores[model_name] = model.evaluate(test_dataset, metric, transformers)

  return train_scores, valid_scores, test_scores


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 143:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 92%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1875')" href="javascript:;">
deepchem-2.4.0/deepchem/metrics/tests/test_genomics.py: 57-74
</a>
<div class="mid" id="frag1875" style="display:none"><pre>
  def test_in_silico_mutagenesis_shape(self):
    """Test in-silico mutagenesis returns correct shape."""
    # Construct and train SequenceDNN model
    sequences = np.array(["ACGTA", "GATAG", "CGCGC"])
    sequences = dc.utils.genomics_utils.seq_one_hot_encode(
        sequences, letters=LETTERS)
    labels = np.array([1, 0, 0])
    labels = np.reshape(labels, (3, 1))
    self.assertEqual(sequences.shape, (3, 4, 5, 1))

    dataset = dc.data.NumpyDataset(sequences, labels)
    model = self.create_model_for_mutagenesis()
    model.fit(dataset, nb_epoch=1)

    # Call in-silico mutagenesis
    mutagenesis_scores = in_silico_mutagenesis(model, sequences)
    self.assertEqual(mutagenesis_scores.shape, (1, 3, 4, 5, 1))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1876')" href="javascript:;">
deepchem-2.4.0/deepchem/metrics/tests/test_genomics.py: 75-94
</a>
<div class="mid" id="frag1876" style="display:none"><pre>
  def test_in_silico_mutagenesis_nonzero(self):
    """Test in-silico mutagenesis returns nonzero output."""
    # Construct and train SequenceDNN model
    sequences = np.array(["ACGTA", "GATAG", "CGCGC"])
    sequences = dc.utils.genomics_utils.seq_one_hot_encode(
        sequences, letters=LETTERS)
    labels = np.array([1, 0, 0])
    labels = np.reshape(labels, (3, 1))
    self.assertEqual(sequences.shape, (3, 4, 5, 1))

    dataset = dc.data.NumpyDataset(sequences, labels)
    model = self.create_model_for_mutagenesis()
    model.fit(dataset, nb_epoch=1)

    # Call in-silico mutagenesis
    mutagenesis_scores = in_silico_mutagenesis(model, sequences)
    self.assertEqual(mutagenesis_scores.shape, (1, 3, 4, 5, 1))

    # Check nonzero elements exist
    assert np.count_nonzero(mutagenesis_scores) &gt; 0
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 144:</b> &nbsp; 2 fragments, nominal size 29 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1938')" href="javascript:;">
deepchem-2.4.0/examples/sider/sider_datasets.py: 14-54
</a>
<div class="mid" id="frag1938" style="display:none"><pre>
def load_sider(featurizer='ECFP', split='index'):
  current_dir = os.path.dirname(os.path.realpath(__file__))

  # Load SIDER dataset
  print("About to load SIDER dataset.")
  dataset_file = os.path.join(current_dir, "./sider.csv.gz")
  dataset = dc.utils.save.load_from_disk(dataset_file)
  print("Columns of dataset: %s" % str(dataset.columns.values))
  print("Number of examples in dataset: %s" % str(dataset.shape[0]))

  # Featurize SIDER dataset
  print("About to featurize SIDER dataset.")
  if featurizer == 'ECFP':
    featurizer = dc.feat.CircularFingerprint(size=1024)
  elif featurizer == 'GraphConv':
    featurizer = dc.feat.ConvMolFeaturizer()

  SIDER_tasks = dataset.columns.values[1:].tolist()
  print("SIDER tasks: %s" % str(SIDER_tasks))
  print("%d tasks in total" % len(SIDER_tasks))

  loader = dc.data.CSVLoader(
      tasks=SIDER_tasks, smiles_field="smiles", featurizer=featurizer)
  dataset = loader.featurize(dataset_file)
  print("%d datapoints in SIDER dataset" % len(dataset))

  # Initialize transformers
  transformers = [dc.trans.BalancingTransformer(dataset=dataset)]
  print("About to transform data")
  for transformer in transformers:
    dataset = transformer.transform(dataset)

  splitters = {
      'index': dc.splits.IndexSplitter(),
      'random': dc.splits.RandomSplitter(),
      'scaffold': dc.splits.ScaffoldSplitter()
  }
  splitter = splitters[split]
  train, valid, test = splitter.train_valid_test_split(dataset)

  return SIDER_tasks, (train, valid, test), transformers
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1941')" href="javascript:;">
deepchem-2.4.0/examples/toxcast/toxcast_datasets.py: 14-54
</a>
<div class="mid" id="frag1941" style="display:none"><pre>
def load_toxcast(featurizer='ECFP', split='index'):

  current_dir = os.path.dirname(os.path.realpath(__file__))

  # Load TOXCAST dataset
  print("About to load TOXCAST dataset.")
  dataset_file = os.path.join(current_dir, "./processing/toxcast_data.csv.gz")
  dataset = dc.utils.save.load_from_disk(dataset_file)
  print("Columns of dataset: %s" % str(dataset.columns.values))
  print("Number of examples in dataset: %s" % str(dataset.shape[0]))

  # Featurize TOXCAST dataset
  print("About to featurize TOXCAST dataset.")

  if featurizer == 'ECFP':
    featurizer = dc.feat.CircularFingerprint(size=1024)
  elif featurizer == 'GraphConv':
    featurizer = dc.feat.ConvMolFeaturizer()

  TOXCAST_tasks = dataset.columns.values[1:].tolist()

  loader = dc.data.CSVLoader(
      tasks=TOXCAST_tasks, smiles_field="smiles", featurizer=featurizer)
  dataset = loader.featurize(dataset_file)

  # Initialize transformers
  transformers = [dc.trans.BalancingTransformer(dataset=dataset)]
  print("About to transform data")
  for transformer in transformers:
    dataset = transformer.transform(dataset)

  splitters = {
      'index': dc.splits.IndexSplitter(),
      'random': dc.splits.RandomSplitter(),
      'scaffold': dc.splits.ScaffoldSplitter()
  }
  splitter = splitters[split]

  train, valid, test = splitter.train_valid_test_split(dataset)

  return TOXCAST_tasks, (train, valid, test), transformers
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 145:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 87%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1949')" href="javascript:;">
deepchem-2.4.0/examples/low_data/datasets.py: 21-47
</a>
<div class="mid" id="frag1949" style="display:none"><pre>
def load_tox21_ecfp(num_train=7200):
  """Load Tox21 datasets. Does not do train/test split"""
  # Set some global variables up top
  current_dir = os.path.dirname(os.path.realpath(__file__))
  dataset_file = os.path.join(current_dir, "../../datasets/tox21.csv.gz")
  # Featurize Tox21 dataset
  print("About to featurize Tox21 dataset.")
  featurizer = dc.feat.CircularFingerprint(size=1024)
  tox21_tasks = [
      'NR-AR', 'NR-AR-LBD', 'NR-AhR', 'NR-Aromatase', 'NR-ER', 'NR-ER-LBD',
      'NR-PPAR-gamma', 'SR-ARE', 'SR-ATAD5', 'SR-HSE', 'SR-MMP', 'SR-p53'
  ]

  loader = dc.data.CSVLoader(
      tasks=tox21_tasks, smiles_field="smiles", featurizer=featurizer)
  dataset = loader.featurize(dataset_file, shard_size=8192)

  # Initialize transformers
  transformers = [dc.trans.BalancingTransformer(dataset=dataset)]

  print("About to transform data")
  for transformer in transformers:
    dataset = transformer.transform(dataset)

  return tox21_tasks, dataset, transformers


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1950')" href="javascript:;">
deepchem-2.4.0/examples/low_data/datasets.py: 48-75
</a>
<div class="mid" id="frag1950" style="display:none"><pre>
def load_tox21_convmol(base_dir=None, num_train=7200):
  """Load Tox21 datasets. Does not do train/test split"""
  # Set some global variables up top
  current_dir = os.path.dirname(os.path.realpath(__file__))
  dataset_file = os.path.join(current_dir, "../../datasets/tox21.csv.gz")

  # Featurize Tox21 dataset
  print("About to featurize Tox21 dataset.")
  featurizer = dc.feat.ConvMolFeaturizer()
  tox21_tasks = [
      'NR-AR', 'NR-AR-LBD', 'NR-AhR', 'NR-Aromatase', 'NR-ER', 'NR-ER-LBD',
      'NR-PPAR-gamma', 'SR-ARE', 'SR-ATAD5', 'SR-HSE', 'SR-MMP', 'SR-p53'
  ]

  loader = dc.data.CSVLoader(
      tasks=tox21_tasks, smiles_field="smiles", featurizer=featurizer)
  dataset = loader.featurize(dataset_file, shard_size=8192)

  # Initialize transformers
  transformers = [dc.trans.BalancingTransformer(dataset=dataset)]

  print("About to transform data")
  for transformer in transformers:
    dataset = transformer.transform(dataset)

  return tox21_tasks, dataset, transformers


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 146:</b> &nbsp; 4 fragments, nominal size 18 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1951')" href="javascript:;">
deepchem-2.4.0/examples/low_data/datasets.py: 76-103
</a>
<div class="mid" id="frag1951" style="display:none"><pre>
def load_muv_ecfp():
  """Load MUV datasets. Does not do train/test split"""
  # Load MUV dataset
  print("About to load MUV dataset.")
  current_dir = os.path.dirname(os.path.realpath(__file__))
  dataset_file = os.path.join(current_dir, "../../datasets/muv.csv.gz")
  # Featurize MUV dataset
  print("About to featurize MUV dataset.")
  featurizer = dc.feat.CircularFingerprint(size=1024)
  MUV_tasks = sorted([
      'MUV-692', 'MUV-689', 'MUV-846', 'MUV-859', 'MUV-644', 'MUV-548',
      'MUV-852', 'MUV-600', 'MUV-810', 'MUV-712', 'MUV-737', 'MUV-858',
      'MUV-713', 'MUV-733', 'MUV-652', 'MUV-466', 'MUV-832'
  ])

  loader = dc.data.CSVLoader(
      tasks=MUV_tasks, smiles_field="smiles", featurizer=featurizer)
  dataset = loader.featurize(dataset_file)

  # Initialize transformers
  transformers = [dc.trans.BalancingTransformer(dataset=dataset)]
  print("About to transform data")
  for transformer in transformers:
    dataset = transformer.transform(dataset)

  return MUV_tasks, dataset, transformers


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1953')" href="javascript:;">
deepchem-2.4.0/examples/low_data/datasets.py: 132-158
</a>
<div class="mid" id="frag1953" style="display:none"><pre>
def load_sider_ecfp():
  """Load SIDER datasets. Does not do train/test split"""
  # Featurize SIDER dataset
  print("About to featurize SIDER dataset.")
  current_dir = os.path.dirname(os.path.realpath(__file__))
  dataset_file = os.path.join(current_dir, "../sider/sider.csv.gz")
  featurizer = dc.feat.CircularFingerprint(size=1024)

  dataset = dc.utils.save.load_from_disk(dataset_file)
  SIDER_tasks = dataset.columns.values[1:].tolist()
  print("SIDER tasks: %s" % str(SIDER_tasks))
  print("%d tasks in total" % len(SIDER_tasks))

  loader = dc.data.CSVLoader(
      tasks=SIDER_tasks, smiles_field="smiles", featurizer=featurizer)
  dataset = loader.featurize(dataset_file)
  print("%d datapoints in SIDER dataset" % len(dataset))

  # Initialize transformers
  transformers = [dc.trans.BalancingTransformer(dataset=dataset)]
  print("About to transform data")
  for transformer in transformers:
    dataset = transformer.transform(dataset)

  return SIDER_tasks, dataset, transformers


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1952')" href="javascript:;">
deepchem-2.4.0/examples/low_data/datasets.py: 104-131
</a>
<div class="mid" id="frag1952" style="display:none"><pre>
def load_muv_convmol():
  """Load MUV datasets. Does not do train/test split"""
  # Load MUV dataset
  print("About to load MUV dataset.")
  current_dir = os.path.dirname(os.path.realpath(__file__))
  dataset_file = os.path.join(current_dir, "../../datasets/muv.csv.gz")
  # Featurize MUV dataset
  print("About to featurize MUV dataset.")
  featurizer = dc.feat.ConvMolFeaturizer()
  MUV_tasks = sorted([
      'MUV-692', 'MUV-689', 'MUV-846', 'MUV-859', 'MUV-644', 'MUV-548',
      'MUV-852', 'MUV-600', 'MUV-810', 'MUV-712', 'MUV-737', 'MUV-858',
      'MUV-713', 'MUV-733', 'MUV-652', 'MUV-466', 'MUV-832'
  ])

  loader = dc.data.CSVLoader(
      tasks=MUV_tasks, smiles_field="smiles", featurizer=featurizer)
  dataset = loader.featurize(dataset_file)

  # Initialize transformers
  transformers = [dc.trans.BalancingTransformer(dataset=dataset)]
  print("About to transform data")
  for transformer in transformers:
    dataset = transformer.transform(dataset)

  return MUV_tasks, dataset, transformers


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1954')" href="javascript:;">
deepchem-2.4.0/examples/low_data/datasets.py: 159-183
</a>
<div class="mid" id="frag1954" style="display:none"><pre>
def load_sider_convmol():
  """Load SIDER datasets. Does not do train/test split"""
  # Featurize SIDER dataset
  print("About to featurize SIDER dataset.")
  current_dir = os.path.dirname(os.path.realpath(__file__))
  dataset_file = os.path.join(current_dir, "../sider/sider.csv.gz")
  featurizer = dc.feat.ConvMolFeaturizer()

  dataset = dc.utils.save.load_from_disk(dataset_file)
  SIDER_tasks = dataset.columns.values[1:].tolist()
  print("SIDER tasks: %s" % str(SIDER_tasks))
  print("%d tasks in total" % len(SIDER_tasks))

  loader = dc.data.CSVLoader(
      tasks=SIDER_tasks, smiles_field="smiles", featurizer=featurizer)
  dataset = loader.featurize(dataset_file)
  print("%d datapoints in SIDER dataset" % len(dataset))

  # Initialize transformers
  transformers = [dc.trans.BalancingTransformer(dataset=dataset)]
  print("About to transform data")
  for transformer in transformers:
    dataset = transformer.transform(dataset)

  return SIDER_tasks, dataset, transformers
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 147:</b> &nbsp; 3 fragments, nominal size 39 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1987')" href="javascript:;">
deepchem-2.4.0/examples/kinase/KINASE_datasets.py: 36-87
</a>
<div class="mid" id="frag1987" style="display:none"><pre>
def gen_kinase(KINASE_tasks, raw_train_dir, train_dir, valid_dir, test_dir,
                shard_size=10000):
  """Load Kinase datasets."""
  train_files = ("KINASE_training_disguised_combined_full.csv.gz")
  valid_files = ("KINASE_test1_disguised_combined_full.csv.gz")
  test_files = ("KINASE_test2_disguised_combined_full.csv.gz")

  # Featurize Kinase dataset
  print("About to featurize KINASE dataset.")
  featurizer = dc.feat.UserDefinedFeaturizer(kinase_descriptors)

  loader = dc.data.UserCSVLoader(
      tasks=KINASE_tasks, id_field="Molecule", featurizer=featurizer)

  train_datasets, valid_datasets, test_datasets = [], [], []
  print("Featurizing train datasets")
  train_dataset = loader.featurize(train_files, shard_size=shard_size)

  print("Featurizing valid datasets")
  valid_dataset = loader.featurize(valid_files, shard_size=shard_size)

  print("Featurizing test datasets")
  test_dataset = loader.featurize(test_files, shard_size=shard_size)

  print("Remove missing entries from datasets.")
  remove_missing_entries(train_dataset)
  remove_missing_entries(valid_dataset)
  remove_missing_entries(test_dataset)

  print("Transforming datasets with transformers.")
  transformers = get_transformers(train_dataset)
  raw_train_dataset = train_dataset

  for transformer in transformers:
    print("Performing transformations with %s"
          % transformer.__class__.__name__)
    print("Transforming datasets")
    train_dataset = transformer.transform(train_dataset)
    valid_dataset = transformer.transform(valid_dataset)
    test_dataset = transformer.transform(test_dataset)

  print("Shuffling order of train dataset.")
  train_dataset.sparse_shuffle()

  print("Moving directories")
  raw_train_dataset.move(raw_train_dir)
  train_dataset.move(train_dir)
  valid_dataset.move(valid_dir)
  test_dataset.move(test_dir)
  
  return (raw_train_dataset, train_dataset, valid_dataset, test_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1995')" href="javascript:;">
deepchem-2.4.0/examples/uv/UV_datasets.py: 51-107
</a>
<div class="mid" id="frag1995" style="display:none"><pre>
def gen_uv(UV_tasks, raw_train_dir, train_dir, valid_dir, test_dir,
           shard_size=10000):
  """Load UV datasets."""
  train_files = ("UV_training_disguised_combined_full.csv.gz")
  valid_files = ("UV_test1_disguised_combined_full.csv.gz")
  test_files = ("UV_test2_disguised_combined_full.csv.gz")

  # Featurize UV dataset
  print("About to featurize UV dataset.")
  featurizer = dc.feat.UserDefinedFeaturizer(uv_descriptors)

  loader = dc.data.UserCSVLoader(
      tasks=UV_tasks, id_field="Molecule", featurizer=featurizer)

  train_datasets, valid_datasets, test_datasets = [], [], []
  print("Featurizing train datasets")
  train_dataset = loader.featurize(train_files, shard_size=shard_size)

  print("Featurizing valid datasets")
  valid_dataset = loader.featurize(valid_files, shard_size=shard_size)

  print("Featurizing test datasets")
  test_dataset = loader.featurize(test_files, shard_size=shard_size)

  print("Remove missing entries from datasets.")
  remove_missing_entries(train_dataset)
  remove_missing_entries(valid_dataset)
  remove_missing_entries(test_dataset)

  print("Remove malformed datapoints from UV dataset.")
  remove_UV_negative_entries(train_dataset)
  remove_UV_negative_entries(valid_dataset)
  remove_UV_negative_entries(test_dataset)

  print("Transforming datasets with transformers.")
  transformers = get_transformers(train_dataset)
  raw_train_dataset = train_dataset

  for transformer in transformers:
    print("Performing transformations with %s"
          % transformer.__class__.__name__)
    print("Transforming dataset")
    train_dataset = transformer.transform(train_dataset)
    valid_dataset = transformer.transform(valid_dataset)
    test_dataset = transformer.transform(test_dataset)

  print("Shuffling order of train dataset.")
  train_dataset.sparse_shuffle()

  print("Moving directories")
  raw_train_dataset.move(raw_train_dir)
  train_dataset.move(train_dir)
  valid_dataset.move(valid_dir)
  test_dataset.move(test_dir)
  
  return (raw_train_dataset, train_dataset, valid_dataset, test_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2014')" href="javascript:;">
deepchem-2.4.0/examples/factors/FACTORS_datasets.py: 36-87
</a>
<div class="mid" id="frag2014" style="display:none"><pre>
def gen_factors(FACTORS_tasks, raw_train_dir, train_dir, valid_dir, test_dir,
                shard_size=10000):
  """Load Factor datasets."""
  train_files = ("FACTORS_training_disguised_combined_full.csv.gz")
  valid_files = ("FACTORS_test1_disguised_combined_full.csv.gz")
  test_files = ("FACTORS_test2_disguised_combined_full.csv.gz")

  # Featurize FACTORS dataset
  print("About to featurize FACTORS dataset.")
  featurizer = dc.feat.UserDefinedFeaturizer(factors_descriptors)

  loader = dc.data.UserCSVLoader(
      tasks=FACTORS_tasks, id_field="Molecule", featurizer=featurizer)

  train_datasets, valid_datasets, test_datasets = [], [], []
  print("Featurizing train datasets")
  train_dataset = loader.featurize(train_files, shard_size=shard_size)

  print("Featurizing valid datasets")
  valid_dataset = loader.featurize(valid_files, shard_size=shard_size)

  print("Featurizing test datasets")
  test_dataset = loader.featurize(test_files, shard_size=shard_size)

  print("Remove missing entries from datasets.")
  remove_missing_entries(train_dataset)
  remove_missing_entries(valid_dataset)
  remove_missing_entries(test_dataset)

  print("Transforming datasets with transformers.")
  transformers = get_transformers(train_dataset)
  raw_train_dataset = train_dataset

  for transformer in transformers:
    print("Performing transformations with %s"
          % transformer.__class__.__name__)
    print("Transforming datasets")
    train_dataset = transformer.transform(train_dataset)
    valid_dataset = transformer.transform(valid_dataset)
    test_dataset = transformer.transform(test_dataset)

  print("Shuffling order of train dataset.")
  train_dataset.sparse_shuffle()

  print("Moving directories")
  raw_train_dataset.move(raw_train_dir)
  train_dataset.move(train_dir)
  valid_dataset.move(valid_dir)
  test_dataset.move(test_dir)
  
  return (raw_train_dataset, train_dataset, valid_dataset, test_dataset)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 148:</b> &nbsp; 3 fragments, nominal size 23 lines, similarity 91%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1988')" href="javascript:;">
deepchem-2.4.0/examples/kinase/KINASE_datasets.py: 88-114
</a>
<div class="mid" id="frag1988" style="display:none"><pre>
def load_kinase(shard_size):
  """Loads kinase datasets. Generates if not stored already."""
  KINASE_tasks = (['T_000%d' % i for i in range(13, 100)]
                  + ['T_00%d' % i for i in range(100, 112)])

  current_dir = os.path.dirname(os.path.realpath(__file__))
  raw_train_dir = os.path.join(current_dir, "raw_train_dir")
  train_dir = os.path.join(current_dir, "train_dir") 
  valid_dir = os.path.join(current_dir, "valid_dir") 
  test_dir = os.path.join(current_dir, "test_dir") 

  if (os.path.exists(raw_train_dir) and
      os.path.exists(train_dir) and
      os.path.exists(valid_dir) and
      os.path.exists(test_dir)):
    print("Reloading existing datasets")
    raw_train_dataset = dc.data.DiskDataset(raw_train_dir)
    train_dataset = dc.data.DiskDataset(train_dir)
    valid_dataset = dc.data.DiskDataset(valid_dir)
    test_dataset = dc.data.DiskDataset(test_dir)
  else:
    print("Featurizing datasets")
    (raw_train_dataset, train_dataset, valid_dataset, test_dataset) = \
      gen_kinase(KINASE_tasks, raw_train_dir, train_dir, valid_dir, test_dir,
                  shard_size=shard_size)

  transformers = get_transformers(raw_train_dataset)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1996')" href="javascript:;">
deepchem-2.4.0/examples/uv/UV_datasets.py: 108-134
</a>
<div class="mid" id="frag1996" style="display:none"><pre>
def load_uv(shard_size):
  """Loads uv datasets. Generates if not stored already."""
  UV_tasks = (['logTIC'] +
                  ['w__%d' % i for i in range(210, 401)])

  current_dir = os.path.dirname(os.path.realpath(__file__))
  raw_train_dir = os.path.join(current_dir, "raw_train_dir")
  train_dir = os.path.join(current_dir, "train_dir") 
  valid_dir = os.path.join(current_dir, "valid_dir") 
  test_dir = os.path.join(current_dir, "test_dir") 

  if (os.path.exists(raw_train_dir) and
      os.path.exists(train_dir) and
      os.path.exists(valid_dir) and
      os.path.exists(test_dir)):
    print("Reloading existing datasets")
    raw_train_dataset = dc.data.DiskDataset(raw_train_dir)
    train_dataset = dc.data.DiskDataset(train_dir)
    valid_dataset = dc.data.DiskDataset(valid_dir)
    test_dataset = dc.data.DiskDataset(test_dir)
  else:
    print("Featurizing datasets")
    (raw_train_dataset, train_dataset, valid_dataset, test_dataset) = \
      gen_uv(UV_tasks, raw_train_dir, train_dir, valid_dir, test_dir,
                  shard_size=shard_size)

  transformers = get_transformers(raw_train_dataset)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2015')" href="javascript:;">
deepchem-2.4.0/examples/factors/FACTORS_datasets.py: 88-114
</a>
<div class="mid" id="frag2015" style="display:none"><pre>
def load_factors(shard_size):
  """Loads factors datasets. Generates if not stored already."""
  FACTORS_tasks = (['T_0000%d' % i for i in range(1, 10)]
                   + ['T_000%d' % i for i in range(10, 13)])

  current_dir = os.path.dirname(os.path.realpath(__file__))
  raw_train_dir = os.path.join(current_dir, "raw_train_dir")
  train_dir = os.path.join(current_dir, "train_dir") 
  valid_dir = os.path.join(current_dir, "valid_dir") 
  test_dir = os.path.join(current_dir, "test_dir") 

  if (os.path.exists(raw_train_dir) and
      os.path.exists(train_dir) and
      os.path.exists(valid_dir) and
      os.path.exists(test_dir)):
    print("Reloading existing datasets")
    raw_train_dataset = dc.data.DiskDataset(raw_train_dir)
    train_dataset = dc.data.DiskDataset(train_dir)
    valid_dataset = dc.data.DiskDataset(valid_dir)
    test_dataset = dc.data.DiskDataset(test_dir)
  else:
    print("Featurizing datasets")
    (raw_train_dataset, train_dataset, valid_dataset, test_dataset) = \
      gen_factors(FACTORS_tasks, raw_train_dir, train_dir, valid_dir, test_dir,
                  shard_size=shard_size)

  transformers = get_transformers(raw_train_dataset)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 149:</b> &nbsp; 3 fragments, nominal size 14 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1989')" href="javascript:;">
deepchem-2.4.0/examples/kinase/KINASE_tf_singletask.py: 38-53
</a>
<div class="mid" id="frag1989" style="display:none"><pre>
def task_model_builder(m_dir):
  return dc.models.TensorflowMultitaskRegressor(
      n_tasks=1,
      n_features=n_features,
      logdir=m_dir,
      layer_sizes=[1000] * n_layers,
      dropouts=[.25] * n_layers,
      weight_init_stddevs=[.02] * n_layers,
      bias_init_consts=[1.] * n_layers,
      learning_rate=.0003,
      penalty=.0001,
      penalty_type="l2",
      optimizer="adam",
      batch_size=100)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1997')" href="javascript:;">
deepchem-2.4.0/examples/uv/UV_tf_singletask.py: 37-52
</a>
<div class="mid" id="frag1997" style="display:none"><pre>
def task_model_builder(m_dir):
  return dc.models.TensorflowMultitaskRegressor(
      n_tasks=1,
      n_features=n_features,
      logdir=m_dir,
      layer_sizes=[1000] * n_layers,
      dropouts=[.25] * n_layers,
      weight_init_stddevs=[.02] * n_layers,
      bias_init_consts=[1.] * n_layers,
      learning_rate=.0003,
      penalty=.0001,
      penalty_type="l2",
      optimizer="adam",
      batch_size=100)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2011')" href="javascript:;">
deepchem-2.4.0/examples/factors/FACTORS_tf_singletask.py: 38-53
</a>
<div class="mid" id="frag2011" style="display:none"><pre>
def task_model_builder(m_dir):
  return dc.models.TensorflowMultitaskRegressor(
      n_tasks=1,
      n_features=n_features,
      logdir=m_dir,
      layer_sizes=[1000] * n_layers,
      dropouts=[.25] * n_layers,
      weight_init_stddevs=[.02] * n_layers,
      bias_init_consts=[1.] * n_layers,
      learning_rate=.0003,
      penalty=.0001,
      penalty_type="l2",
      optimizer="adam",
      batch_size=100)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<script language="JavaScript">
function ShowHide(divId) { 
    if(document.getElementById(divId).style.display == 'none') {
        document.getElementById(divId).style.display='block';
    } else { 
        document.getElementById(divId).style.display = 'none';
    } 
}
</script>
</body>
</html>

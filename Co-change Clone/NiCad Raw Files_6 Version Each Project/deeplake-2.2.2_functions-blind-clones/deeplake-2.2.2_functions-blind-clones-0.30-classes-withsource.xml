<clones>
<systeminfo processor="nicad6" system="deeplake-2.2.2" granularity="functions-blind" threshold="30%" minlines="10" maxlines="2500"/>
<cloneinfo npcs="541" npairs="33"/>
<runinfo ncompares="9018" cputime="34544"/>
<classinfo nclasses="16"/>

<class classid="1" nclones="2" nlines="25" similarity="73">
<source file="systems/deeplake-2.2.2/hub/auto/tests/test_ingestion.py" startline="52" endline="79" pcid="4">
def test_image_classification_sets(memory_ds: Dataset):
    path = get_dummy_data_path("tests_auto/image_classification_with_sets")
    ds = hub.ingest(
        src=path,
        dest=memory_ds.path,
        images_compression="auto",
        progress_bar=False,
        summary=False,
        overwrite=False,
    )

    assert list(ds.tensors) == [
        "test/images",
        "test/labels",
        "train/images",
        "train/labels",
    ]

    assert ds["train/images"].meta.sample_compression == "jpeg"
    assert ds["test/images"].numpy().shape == (3, 200, 200, 3)
    assert ds["test/labels"].numpy().shape == (3, 1)
    assert ds["test/labels"].info.class_names == ("class0", "class1", "class2")

    assert ds["train/images"].numpy().shape == (3, 200, 200, 3)
    assert ds["train/labels"].numpy().shape == (3, 1)
    assert ds["train/labels"].info.class_names == ("class0", "class1", "class2")


</source>
<source file="systems/deeplake-2.2.2/hub/auto/tests/test_kaggle.py" startline="35" endline="65" pcid="8">
def test_ingestion_sets(local_ds: Dataset, hub_kaggle_credentials):
    with CliRunner().isolated_filesystem():
        kaggle_path = os.path.join(local_ds.path, "unstructured_kaggle_data_sets")
        username, key = hub_kaggle_credentials

        ds = hub.ingest_kaggle(
            tag="thisiseshan/bird-classes",
            src=kaggle_path,
            dest=local_ds.path,
            images_compression="jpeg",
            kaggle_credentials={"username": username, "key": key},
            progress_bar=False,
            summary=False,
            overwrite=False,
        )

        assert list(ds.tensors.keys()) == [
            "test/images",
            "test/labels",
            "train/images",
            "train/labels",
        ]
        assert ds["test/images"].numpy().shape == (3, 200, 200, 3)
        assert ds["test/labels"].numpy().shape == (3, 1)
        assert ds["test/labels"].info.class_names == ("class0", "class1", "class2")

        assert ds["train/images"].numpy().shape == (3, 200, 200, 3)
        assert ds["train/labels"].numpy().shape == (3, 1)
        assert ds["train/labels"].info.class_names == ("class0", "class1", "class2")


</source>
</class>

<class classid="2" nclones="2" nlines="20" similarity="85">
<source file="systems/deeplake-2.2.2/hub/auto/tests/test_ingestion.py" startline="80" endline="102" pcid="5">
def test_ingestion_exception(memory_ds: Dataset):
    path = get_dummy_data_path("tests_auto/image_classification_with_sets")
    with pytest.raises(InvalidPathException):
        hub.ingest(
            src="tests_auto/invalid_path",
            dest=memory_ds.path,
            images_compression="auto",
            progress_bar=False,
            summary=False,
            overwrite=False,
        )

    with pytest.raises(SamePathException):
        hub.ingest(
            src=path,
            dest=path,
            images_compression="auto",
            progress_bar=False,
            summary=False,
            overwrite=False,
        )


</source>
<source file="systems/deeplake-2.2.2/hub/auto/tests/test_ingestion.py" startline="103" endline="123" pcid="6">
def test_overwrite(local_ds: Dataset):
    path = get_dummy_data_path("tests_auto/image_classification")

    hub.ingest(
        src=path,
        dest=local_ds.path,
        images_compression="auto",
        progress_bar=False,
        summary=False,
        overwrite=False,
    )

    with pytest.raises(TensorAlreadyExistsError):
        hub.ingest(
            src=path,
            dest=local_ds.path,
            images_compression="auto",
            progress_bar=False,
            summary=False,
            overwrite=False,
        )
</source>
</class>

<class classid="3" nclones="4" nlines="20" similarity="85">
<source file="systems/deeplake-2.2.2/hub/util/encoder.py" startline="22" endline="43" pcid="76">
def merge_all_tensor_metas(
    all_workers_tensor_metas: List[Dict[str, TensorMeta]],
    target_ds: hub.Dataset,
    storage: StorageProvider,
    overwrite: bool,
    tensors: List[str],
) -> None:
    """Merges tensor metas from all workers into a single one and stores it in target_ds."""
    commit_id = target_ds.version_state["commit_id"]
    for tensor in tensors:
        rel_path = posixpath.relpath(tensor, target_ds.group_index)
        tensor_meta = None if overwrite else target_ds[rel_path].meta
        for current_worker_metas in all_workers_tensor_metas:
            current_meta = current_worker_metas[tensor]
            if tensor_meta is None:
                tensor_meta = current_meta
            else:
                combine_metas(tensor_meta, current_meta)
        meta_key = get_tensor_meta_key(tensor, commit_id)
        storage[meta_key] = tensor_meta.tobytes()  # type: ignore


</source>
<source file="systems/deeplake-2.2.2/hub/util/encoder.py" startline="64" endline="88" pcid="78">
def merge_all_chunk_id_encoders(
    all_workers_chunk_id_encoders: List[Dict[str, ChunkIdEncoder]],
    target_ds: hub.Dataset,
    storage: StorageProvider,
    overwrite: bool,
    tensors: List[str],
) -> None:
    """Merges chunk_id_encoders from all workers into a single one and stores it in target_ds."""
    commit_id = target_ds.version_state["commit_id"]
    for tensor in tensors:
        rel_path = posixpath.relpath(tensor, target_ds.group_index)
        chunk_id_encoder = (
            None if overwrite else target_ds[rel_path].chunk_engine.chunk_id_encoder
        )
        for current_worker_chunk_id_encoders in all_workers_chunk_id_encoders:
            current_chunk_id_encoder = current_worker_chunk_id_encoders[tensor]
            if chunk_id_encoder is None:
                chunk_id_encoder = current_worker_chunk_id_encoders[tensor]
            else:
                combine_chunk_id_encoders(chunk_id_encoder, current_chunk_id_encoder)

        chunk_id_key = get_chunk_id_encoder_key(tensor, commit_id)
        storage[chunk_id_key] = chunk_id_encoder.tobytes()  # type: ignore


</source>
<source file="systems/deeplake-2.2.2/hub/util/encoder.py" startline="154" endline="178" pcid="82">
def merge_all_commit_chunk_sets(
    all_workers_commit_chunk_sets: List[Dict[str, CommitChunkSet]],
    target_ds: hub.Dataset,
    storage: StorageProvider,
    overwrite: bool,
    tensors: List[str],
) -> None:
    """Merges commit_chunk_sets from all workers into a single one and stores it in target_ds."""
    commit_id = target_ds.version_state["commit_id"]
    for tensor in tensors:
        rel_path = posixpath.relpath(tensor, target_ds.group_index)
        commit_chunk_set = (
            None if overwrite else target_ds[rel_path].chunk_engine.commit_chunk_set
        )
        for current_worker_commit_chunk_set in all_workers_commit_chunk_sets:
            current_commit_chunk_set = current_worker_commit_chunk_set[tensor]
            if commit_chunk_set is None:
                commit_chunk_set = current_commit_chunk_set
            else:
                combine_commit_chunk_sets(commit_chunk_set, current_commit_chunk_set)

        commit_chunk_key = get_tensor_commit_chunk_set_key(tensor, commit_id)
        storage[commit_chunk_key] = commit_chunk_set.tobytes()  # type: ignore


</source>
<source file="systems/deeplake-2.2.2/hub/util/encoder.py" startline="187" endline="210" pcid="84">
def merge_all_commit_diffs(
    all_workers_commit_diffs: List[Dict[str, CommitDiff]],
    target_ds: hub.Dataset,
    storage: StorageProvider,
    overwrite: bool,
    tensors: List[str],
) -> None:
    """Merges commit_diffs from all workers into a single one and stores it in target_ds."""
    commit_id = target_ds.version_state["commit_id"]
    for tensor in tensors:
        rel_path = posixpath.relpath(tensor, target_ds.group_index)  # type: ignore
        commit_diff = None if overwrite else target_ds[rel_path].chunk_engine.commit_diff  # type: ignore
        for current_worker_commit_diffs in all_workers_commit_diffs:
            current_commit_diff = current_worker_commit_diffs[tensor]
            if commit_diff is None:
                commit_diff = current_commit_diff
                commit_diff.transform_data()
            else:
                combine_commit_diffs(commit_diff, current_commit_diff)

        commit_chunk_key = get_tensor_commit_diff_key(tensor, commit_id)
        storage[commit_chunk_key] = commit_diff.tobytes()  # type: ignore


</source>
</class>

<class classid="4" nclones="2" nlines="30" similarity="93">
<source file="systems/deeplake-2.2.2/hub/integrations/pytorch/pytorch.py" startline="11" endline="46" pcid="198">
def create_dataloader_nesteddataloader(
    dataset,
    tensors,
    use_local_cache,
    transform,
    num_workers,
    buffer_size,
    batch_size,
    collate_fn,
    pin_memory,
    drop_last,
):
    import torch
    import torch.utils.data
    from hub.integrations.pytorch.dataset import SubIterableDataset

    return torch.utils.data.DataLoader(
        # this data set is more efficient also shuffles
        # using threads race conditions as source of entropy
        SubIterableDataset(
            dataset,
            tensors=tensors,
            use_local_cache=use_local_cache,
            transform=transform,
            num_workers=num_workers,
            buffer_size=buffer_size,
            batch_size=batch_size,
            collate_fn=collate_fn,
        ),
        batch_size=batch_size,
        collate_fn=collate_fn,
        pin_memory=pin_memory,
        drop_last=drop_last,
    )


</source>
<source file="systems/deeplake-2.2.2/hub/integrations/pytorch/pytorch.py" startline="47" endline="80" pcid="199">
def create_dataloader_shufflingdataloader(
    dataset,
    tensors,
    use_local_cache,
    transform,
    num_workers,
    buffer_size,
    batch_size,
    collate_fn,
    pin_memory,
    drop_last,
):
    import torch
    import torch.utils.data
    from hub.integrations.pytorch.dataset import ShufflingIterableDataset

    return torch.utils.data.DataLoader(
        # this data set is more efficient also shuffles
        # using threads race conditions as source of entropy
        ShufflingIterableDataset(
            dataset,
            tensors=tensors,
            use_local_cache=use_local_cache,
            transform=transform,
            num_workers=num_workers,
            buffer_size=buffer_size,
        ),
        batch_size=batch_size,
        collate_fn=collate_fn,
        pin_memory=pin_memory,
        drop_last=drop_last,
    )


</source>
</class>

<class classid="5" nclones="2" nlines="16" similarity="76">
<source file="systems/deeplake-2.2.2/hub/core/chunk/test_chunk_compressed.py" startline="31" endline="49" pcid="202">
def test_read_write_sequence(compression):
    tensor_meta = create_tensor_meta()
    common_args["tensor_meta"] = tensor_meta
    common_args["compression"] = compression
    dtype = tensor_meta.dtype
    data_in = [
        np.random.randint(0, 255, size=(1000, 500)).astype(dtype) for _ in range(10)
    ]
    data_in2 = data_in.copy()
    while data_in:
        chunk = ChunkCompressedChunk(**common_args)
        num_samples = int(chunk.extend_if_has_space(data_in))
        chunk._decompressed_samples = None
        data_out = [chunk.read_sample(i) for i in range(num_samples)]
        np.testing.assert_array_equal(data_out, data_in2[:num_samples])
        data_in = data_in[num_samples:]
        data_in2 = data_in2[num_samples:]


</source>
<source file="systems/deeplake-2.2.2/hub/core/chunk/test_sample_compressed.py" startline="32" endline="47" pcid="206">
def test_read_write_sequence(compression):
    tensor_meta = create_tensor_meta()
    common_args["tensor_meta"] = tensor_meta
    common_args["compression"] = compression
    dtype = tensor_meta.dtype
    data_in = [np.random.rand(1000, 500, 3).astype(dtype) for _ in range(10)]
    data_in2 = data_in.copy()
    while data_in:
        chunk = SampleCompressedChunk(**common_args)
        num_samples = int(chunk.extend_if_has_space(data_in))
        data_out = [chunk.read_sample(i) for i in range(num_samples)]
        np.testing.assert_array_equal(data_out, data_in2[:num_samples])
        data_in = data_in[num_samples:]
        data_in2 = data_in2[num_samples:]


</source>
</class>

<class classid="6" nclones="3" nlines="46" similarity="76">
<source file="systems/deeplake-2.2.2/hub/core/chunk/test_chunk_compressed.py" startline="52" endline="107" pcid="203">
def test_read_write_sequence_big(cat_path, compression, random):
    tensor_meta = create_tensor_meta()
    common_args["tensor_meta"] = tensor_meta
    common_args["compression"] = compression
    dtype = tensor_meta.dtype
    data_in = []
    for i in range(50):
        if i % 10 == 0:
            data_in.append(
                np.random.randint(0, 255, size=(6001, 3000, 3)).astype(dtype) * random
            )
        elif i % 3 == 0:
            data_in.append(
                hub.read(cat_path) if random else np.zeros((900, 900, 3), dtype=dtype)
            )
        else:
            data_in.append(
                np.random.randint(0, 255, size=(1000, 500, 3)).astype(dtype) * random
            )
    data_in2 = data_in.copy()
    tiles = []
    original_length = len(data_in)
    tiled = False
    while data_in:
        chunk = ChunkCompressedChunk(**common_args)
        chunk._compression_ratio = 10  # start with a bad compression ratio to hit exponential back off code path
        num_samples = chunk.extend_if_has_space(data_in)
        if num_samples == PARTIAL_NUM_SAMPLES:
            tiled = True
            tiles.append(chunk.read_sample(0))
            sample = data_in[0]
            assert isinstance(sample, SampleTiles)
            if sample.is_last_write:
                current_length = len(data_in)
                index = original_length - current_length
                full_data_out = np_list_to_sample(
                    tiles,
                    sample.sample_shape,
                    sample.tile_shape,
                    sample.layout_shape,
                    dtype,
                )
                np.testing.assert_array_equal(full_data_out, data_in2[index])
                data_in = data_in[1:]
                tiles = []

        elif num_samples > 0:
            data_out = [chunk.read_sample(i) for i in range(num_samples)]
            for i, item in enumerate(data_out):
                if isinstance(item, Sample):
                    item = item.array
                np.testing.assert_array_equal(item, data_in[i])
            data_in = data_in[num_samples:]
    assert tiled


</source>
<source file="systems/deeplake-2.2.2/hub/core/chunk/test_sample_compressed.py" startline="49" endline="95" pcid="207">
def test_read_write_sequence_big(cat_path, compression):
    tensor_meta = create_tensor_meta()
    common_args["tensor_meta"] = tensor_meta
    common_args["compression"] = compression
    dtype = tensor_meta.dtype
    data_in = []
    for i in range(50):
        if i % 10 == 0:
            data_in.append(np.random.rand(6001, 3000, 3).astype(dtype))
        elif i % 3 == 0:
            data_in.append(hub.read(cat_path))
        else:
            data_in.append(np.random.rand(1000, 500, 3).astype(dtype))
    data_in2 = data_in.copy()
    tiles = []
    original_length = len(data_in)

    while data_in:
        chunk = SampleCompressedChunk(**common_args)
        num_samples = chunk.extend_if_has_space(data_in)
        if num_samples == PARTIAL_NUM_SAMPLES:
            tiles.append(chunk.read_sample(0))
            sample = data_in[0]
            assert isinstance(sample, SampleTiles)
            if sample.is_last_write:
                current_length = len(data_in)
                index = original_length - current_length
                full_data_out = np_list_to_sample(
                    tiles,
                    sample.sample_shape,
                    sample.tile_shape,
                    sample.layout_shape,
                    dtype,
                )
                np.testing.assert_array_equal(full_data_out, data_in2[index])
                data_in = data_in[1:]
                tiles = []

        elif num_samples > 0:
            data_out = [chunk.read_sample(i) for i in range(num_samples)]
            for i, item in enumerate(data_out):
                if isinstance(item, Sample):
                    item = item.array
                np.testing.assert_array_equal(item, data_in[i])
            data_in = data_in[num_samples:]


</source>
<source file="systems/deeplake-2.2.2/hub/core/chunk/test_uncompressed.py" startline="43" endline="87" pcid="227">
def test_read_write_sequence_big(cat_path):
    tensor_meta = create_tensor_meta()
    common_args["tensor_meta"] = tensor_meta
    dtype = tensor_meta.dtype
    data_in = []
    for i in range(50):
        if i % 10 == 0:
            data_in.append(np.random.rand(3001, 3000, 3).astype(dtype))
        elif i % 3 == 0:
            data_in.append(hub.read(cat_path))
        else:
            data_in.append(np.random.rand(500, 500, 3).astype(dtype))
    data_in2 = data_in.copy()
    tiles = []
    original_length = len(data_in)

    while data_in:
        chunk = UncompressedChunk(**common_args)
        num_samples = chunk.extend_if_has_space(data_in)
        if num_samples == PARTIAL_NUM_SAMPLES:
            tiles.append(chunk.read_sample(0))
            sample = data_in[0]
            assert isinstance(sample, SampleTiles)
            if sample.is_last_write:
                current_length = len(data_in)
                index = original_length - current_length
                full_data_out = np_list_to_sample(
                    tiles,
                    sample.sample_shape,
                    sample.tile_shape,
                    sample.layout_shape,
                    dtype,
                )
                np.testing.assert_array_equal(full_data_out, data_in2[index])
                data_in = data_in[1:]
                tiles = []
        elif num_samples > 0:
            data_out = [chunk.read_sample(i) for i in range(num_samples)]
            for i, item in enumerate(data_out):
                if isinstance(item, Sample):
                    item = item.array
                np.testing.assert_array_equal(item, data_in[i])
            data_in = data_in[num_samples:]


</source>
</class>

<class classid="7" nclones="3" nlines="21" similarity="72">
<source file="systems/deeplake-2.2.2/hub/core/chunk/test_chunk_compressed.py" startline="109" endline="133" pcid="204">
def test_update(compression):
    tensor_meta = create_tensor_meta()
    common_args["tensor_meta"] = tensor_meta
    common_args["compression"] = compression
    dtype = tensor_meta.dtype
    arr = np.random.randint(0, 255, size=(7, 300, 200, 3)).astype(dtype)
    data_in = list(arr)
    chunk = ChunkCompressedChunk(**common_args)
    chunk.extend_if_has_space(data_in)

    data_out = np.array([chunk.read_sample(i) for i in range(7)])
    np.testing.assert_array_equal(data_out, data_in)

    data_3 = np.random.randint(0, 255, size=(1400, 700, 3)).astype(dtype)
    data_5 = np.random.randint(0, 255, size=(2000, 3000, 3)).astype(dtype)

    chunk.update_sample(3, data_3)
    chunk.update_sample(5, data_5)
    for i in range(7):
        if i == 3:
            np.testing.assert_array_equal(chunk.read_sample(i), data_3)
        elif i == 5:
            np.testing.assert_array_equal(chunk.read_sample(i), data_5)
        else:
            np.testing.assert_array_equal(chunk.read_sample(i), data_in[i])
</source>
<source file="systems/deeplake-2.2.2/hub/core/chunk/test_uncompressed.py" startline="123" endline="145" pcid="230">
def test_update():
    tensor_meta = create_tensor_meta()
    common_args["tensor_meta"] = tensor_meta
    dtype = tensor_meta.dtype
    data_in = np.random.rand(7, 500, 500).astype(dtype)
    chunk = UncompressedChunk(**common_args)
    chunk.extend_if_has_space(data_in)

    data_out = np.array([chunk.read_sample(i) for i in range(7)])
    np.testing.assert_array_equal(data_out, data_in)

    data_3 = np.random.rand(700, 700).astype(dtype)
    data_5 = np.random.rand(3000, 3000).astype(dtype)

    chunk.update_sample(3, data_3)
    chunk.update_sample(5, data_5)
    for i in range(7):
        if i == 3:
            np.testing.assert_array_equal(chunk.read_sample(i), data_3)
        elif i == 5:
            np.testing.assert_array_equal(chunk.read_sample(i), data_5)
        else:
            np.testing.assert_array_equal(chunk.read_sample(i), data_in[i])
</source>
<source file="systems/deeplake-2.2.2/hub/core/chunk/test_sample_compressed.py" startline="97" endline="120" pcid="208">
def test_update(compression):
    tensor_meta = create_tensor_meta()
    common_args["tensor_meta"] = tensor_meta
    common_args["compression"] = compression
    dtype = tensor_meta.dtype
    arr = np.random.rand(7, 100, 500, 3).astype(dtype)
    data_in = list(arr)
    chunk = SampleCompressedChunk(**common_args)
    chunk.extend_if_has_space(data_in)
    data_out = np.array([chunk.read_sample(i) for i in range(7)])
    np.testing.assert_array_equal(data_out, data_in)

    data_3 = np.random.rand(1400, 700, 3).astype(dtype)
    data_5 = np.random.rand(6000, 3000, 3).astype(dtype)

    chunk.update_sample(3, data_3)
    chunk.update_sample(5, data_5)
    for i in range(7):
        if i == 3:
            np.testing.assert_array_equal(chunk.read_sample(i), data_3)
        elif i == 5:
            np.testing.assert_array_equal(chunk.read_sample(i), data_5)
        else:
            np.testing.assert_array_equal(chunk.read_sample(i), arr[i])
</source>
</class>

<class classid="8" nclones="2" nlines="18" similarity="73">
<source file="systems/deeplake-2.2.2/hub/core/test_serialize.py" startline="49" endline="75" pcid="232">
def test_sample_img_compression(cat_path, compression="png"):
    sample = hub.read(cat_path)
    arr = sample.array

    # reloaded to get rid of cached array in sample
    sample = hub.read(cat_path)
    out, shape = serialize_sample_object(
        sample, compression, None, "uint16", "generic", 16 * MB
    )
    arr_deserialized = decompress_array(out, compression=compression).reshape(shape)
    np.testing.assert_array_equal(arr, arr_deserialized)

    # reloaded to get rid of cached array in sample
    sample = hub.read(cat_path)
    out, shape = serialize_sample_object(
        sample, compression, None, "uint16", "generic", 100 * KB
    )
    assert isinstance(out, SampleTiles)
    out_list = [out.yield_tile() for _ in range(out.num_tiles)]
    np_list = [
        decompress_array(b[0], compression=compression).reshape(b[1]) for b in out_list
    ]
    tile_shape, layout_shape = out.tile_shape, out.layout_shape
    out = np_list_to_sample(np_list, shape, tile_shape, layout_shape, "uint16")
    np.testing.assert_array_equal(arr, out)


</source>
<source file="systems/deeplake-2.2.2/hub/core/test_serialize.py" startline="102" endline="124" pcid="234">
def test_sample_no_compression(cat_path):
    sample = hub.read(cat_path)
    arr = sample.array

    # reloaded to get rid of cached array in sample
    sample = hub.read(cat_path)
    out, shape = serialize_sample_object(
        sample, None, None, "uint16", "generic", 16 * MB
    )
    arr_deserialized = np.frombuffer(out, dtype="uint16").reshape(shape)
    np.testing.assert_array_equal(arr, arr_deserialized)

    # reloaded to get rid of cached array in sample
    sample = hub.read(cat_path)
    out, shape = serialize_sample_object(
        sample, None, None, "uint16", "generic", 100 * KB
    )
    assert isinstance(out, SampleTiles)
    out_list = [out.yield_tile() for _ in range(out.num_tiles)]
    np_list = [np.frombuffer(b[0], dtype="uint16").reshape(b[1]) for b in out_list]
    tile_shape, layout_shape = out.tile_shape, out.layout_shape
    out = np_list_to_sample(np_list, shape, tile_shape, layout_shape, "uint16")
    np.testing.assert_array_equal(arr, out)
</source>
</class>

<class classid="9" nclones="3" nlines="18" similarity="83">
<source file="systems/deeplake-2.2.2/hub/core/dataset/hub_cloud_dataset.py" startline="105" endline="124" pcid="271">
    def _send_query_progress(
        self,
        query_id: str = "",
        query_text: str = "",
        start: bool = False,
        end: bool = False,
        progress: int = 0,
        status="",
    ):
        hub_meta = {
            "query_id": query_id,
            "query_text": query_text,
            "progress": progress,
            "start": start,
            "end": end,
            "status": status,
        }
        event_id = f"{self.org_id}/{self.ds_name}.query"
        self._send_event(event_id=event_id, event_group="query", hub_meta=hub_meta)

</source>
<source file="systems/deeplake-2.2.2/hub/core/dataset/hub_cloud_dataset.py" startline="125" endline="144" pcid="272">
    def _send_compute_progress(
        self,
        compute_id: str = "",
        start: bool = False,
        end: bool = False,
        progress: int = 0,
        status="",
    ):
        hub_meta = {
            "compute_id": compute_id,
            "progress": progress,
            "start": start,
            "end": end,
            "status": status,
        }
        event_id = f"{self.org_id}/{self.ds_name}.compute"
        self._send_event(
            event_id=event_id, event_group="hub_compute", hub_meta=hub_meta
        )

</source>
<source file="systems/deeplake-2.2.2/hub/core/dataset/hub_cloud_dataset.py" startline="145" endline="162" pcid="273">
    def _send_pytorch_progress(
        self,
        pytorch_id: str = "",
        start: bool = False,
        end: bool = False,
        progress: int = 0,
        status="",
    ):
        hub_meta = {
            "pytorch_id": pytorch_id,
            "progress": progress,
            "start": start,
            "end": end,
            "status": status,
        }
        event_id = f"{self.org_id}/{self.ds_name}.pytorch"
        self._send_event(event_id=event_id, event_group="pytorch", hub_meta=hub_meta)

</source>
</class>

<class classid="10" nclones="2" nlines="17" similarity="83">
<source file="systems/deeplake-2.2.2/hub/core/meta/encode/tests/test_shape_encoder.py" startline="15" endline="34" pcid="305">
def test_fixed():
    enc = ShapeEncoder()

    enc.register_samples((28, 28, 3), 1000)
    enc.register_samples((28, 28, 3), 1000)
    enc.register_samples((28, 28, 3), 3)
    enc.register_samples((28, 28, 3), 1000)
    enc.register_samples((28, 28, 3), 1000)

    assert enc.num_samples == 4003
    assert len(enc._encoded) == 1
    assert enc.num_samples_at(0) == 4003

    assert enc[0] == (28, 28, 3)
    assert enc[1999] == (28, 28, 3)
    assert enc[2000] == (28, 28, 3)
    assert enc[3000] == (28, 28, 3)
    assert enc[-1] == (28, 28, 3)


</source>
<source file="systems/deeplake-2.2.2/hub/core/meta/encode/tests/test_shape_encoder.py" startline="35" endline="57" pcid="306">
def test_dynamic():
    enc = ShapeEncoder()

    enc.register_samples((28, 28, 3), 1000)
    enc.register_samples((28, 28, 3), 1000)
    enc.register_samples((30, 28, 3), 1000)
    enc.register_samples((28, 28, 4), 1000)
    enc.register_samples((28, 28, 3), 1)

    assert enc.num_samples == 4001
    assert len(enc._encoded) == 4
    assert enc.num_samples_at(0) == 2000
    assert enc.num_samples_at(1) == 1000
    assert enc.num_samples_at(2) == 1000
    assert enc.num_samples_at(3) == 1

    assert enc[0] == (28, 28, 3)
    assert enc[1999] == (28, 28, 3)
    assert enc[2000] == (30, 28, 3)
    assert enc[3000] == (28, 28, 4)
    assert enc[-1] == (28, 28, 3)


</source>
</class>

<class classid="11" nclones="4" nlines="38" similarity="70">
<source file="systems/deeplake-2.2.2/hub/core/transform/test_transform.py" startline="107" endline="154" pcid="346">
def test_single_transform_hub_dataset(ds, scheduler):
    data_in = hub.dataset("./test/single_transform_hub_dataset", overwrite=True)
    with data_in:
        data_in.create_tensor("image")
        data_in.create_tensor("label")
        for i in range(1, 100):
            data_in.image.append(i * np.ones((i, i)))
            data_in.label.append(i * np.ones((1,)))
    ds_out = ds
    ds_out.create_tensor("image")
    ds_out.create_tensor("label")
    if (
        isinstance(remove_memory_cache(ds.storage), MemoryProvider)
        and scheduler != "threaded"
    ):
        # any scheduler other than `threaded` will not work with a dataset stored in memory
        with pytest.raises(InvalidOutputDatasetError):
            fn2(copy=1, mul=2).eval(
                data_in,
                ds_out,
                num_workers=TRANSFORM_TEST_NUM_WORKERS,
                progressbar=False,
                scheduler=scheduler,
            )
        data_in.delete()
        return

    fn2(copy=1, mul=2).eval(
        data_in,
        ds_out,
        num_workers=TRANSFORM_TEST_NUM_WORKERS,
        scheduler=scheduler,
        progressbar=False,
    )
    assert len(ds_out) == 99
    for index in range(1, 100):
        np.testing.assert_array_equal(
            ds_out[index - 1].image.numpy(), 2 * index * np.ones((index, index))
        )
        np.testing.assert_array_equal(
            ds_out[index - 1].label.numpy(), 2 * index * np.ones((1,))
        )

    assert ds_out.image.shape_interval.lower == (99, 1, 1)
    assert ds_out.image.shape_interval.upper == (99, 99, 99)
    data_in.delete()


</source>
<source file="systems/deeplake-2.2.2/hub/core/transform/test_transform.py" startline="468" endline="511" pcid="355">
def test_hub_like(ds, scheduler="threaded"):
    with CliRunner().isolated_filesystem():
        data_in = ds
        with data_in:
            data_in.create_tensor("image", htype="image", sample_compression="png")
            data_in.create_tensor("label", htype="class_label")
            for i in range(1, 100):
                data_in.image.append(i * np.ones((i, i), dtype="uint8"))
                data_in.label.append(i * np.ones((1,), dtype="uint32"))
        ds_out = hub.like("./transform_hub_like", data_in)
        if (
            isinstance(remove_memory_cache(ds.storage), MemoryProvider)
            and scheduler != "threaded"
        ):
            # any scheduler other than `threaded` will not work with a dataset stored in memory
            with pytest.raises(InvalidOutputDatasetError):
                fn2(copy=1, mul=2).eval(
                    data_in,
                    ds_out,
                    num_workers=TRANSFORM_TEST_NUM_WORKERS,
                    progressbar=False,
                    scheduler=scheduler,
                )
            return
        fn2(copy=1, mul=2).eval(
            data_in,
            ds_out,
            num_workers=TRANSFORM_TEST_NUM_WORKERS,
            progressbar=False,
            scheduler=scheduler,
        )
        assert len(ds_out) == 99
        for index in range(1, 100):
            np.testing.assert_array_equal(
                ds_out[index - 1].image.numpy(), 2 * index * np.ones((index, index))
            )
            np.testing.assert_array_equal(
                ds_out[index - 1].label.numpy(), 2 * index * np.ones((1,))
            )

        assert ds_out.image.shape_interval.lower == (99, 1, 1)
        assert ds_out.image.shape_interval.upper == (99, 99, 99)


</source>
<source file="systems/deeplake-2.2.2/hub/core/transform/test_transform.py" startline="225" endline="269" pcid="349">
def test_single_transform_hub_dataset_htypes(ds, num_workers, scheduler):
    data_in = hub.dataset("./test/single_transform_hub_dataset_htypes", overwrite=True)
    with data_in:
        data_in.create_tensor("image", htype="image", sample_compression="png")
        data_in.create_tensor("label", htype="class_label")
        for i in range(1, 100):
            data_in.image.append(i * np.ones((i, i), dtype="uint8"))
            data_in.label.append(i * np.ones((1,), dtype="uint32"))
    ds_out = ds
    ds_out.create_tensor("image")
    ds_out.create_tensor("label")
    if (
        isinstance(remove_memory_cache(ds.storage), MemoryProvider)
        and scheduler != "threaded"
        and num_workers > 0
    ):
        # any scheduler other than `threaded` will not work with a dataset stored in memory
        # num_workers = 0 automatically does single threaded irrespective of the scheduler
        with pytest.raises(InvalidOutputDatasetError):
            fn2(copy=1, mul=2).eval(
                data_in,
                ds_out,
                num_workers=num_workers,
                progressbar=False,
                scheduler=scheduler,
            )
        data_in.delete()
        return
    fn2(copy=1, mul=2).eval(
        data_in, ds_out, num_workers=num_workers, progressbar=False, scheduler=scheduler
    )
    assert len(ds_out) == 99
    for index in range(1, 100):
        np.testing.assert_array_equal(
            ds_out[index - 1].image.numpy(), 2 * index * np.ones((index, index))
        )
        np.testing.assert_array_equal(
            ds_out[index - 1].label.numpy(), 2 * index * np.ones((1,))
        )

    assert ds_out.image.shape_interval.lower == (99, 1, 1)
    assert ds_out.image.shape_interval.upper == (99, 99, 99)
    data_in.delete()


</source>
<source file="systems/deeplake-2.2.2/hub/core/transform/test_transform.py" startline="554" endline="605" pcid="360">
def test_transform_persistance(local_ds_generator, num_workers=2, scheduler="threaded"):
    data_in = hub.dataset("./test/single_transform_hub_dataset_htypes", overwrite=True)
    with data_in:
        data_in.create_tensor("image", htype="image", sample_compression="png")
        data_in.create_tensor("label", htype="class_label")
        for i in range(1, 100):
            data_in.image.append(i * np.ones((i, i), dtype="uint8"))
            data_in.label.append(i * np.ones((1,), dtype="uint32"))
    ds_out = local_ds_generator()
    ds_out.create_tensor("image")
    ds_out.create_tensor("label")
    if (
        isinstance(remove_memory_cache(ds_out.storage), MemoryProvider)
        and scheduler != "threaded"
        and num_workers > 0
    ):
        # any scheduler other than `threaded` will not work with a dataset stored in memory
        # num_workers = 0 automatically does single threaded irrespective of the scheduler
        with pytest.raises(InvalidOutputDatasetError):
            fn2(copy=1, mul=2).eval(
                data_in,
                ds_out,
                num_workers=num_workers,
                scheduler=scheduler,
                progressbar=False,
            )
        data_in.delete()
        return
    fn2(copy=1, mul=2).eval(
        data_in, ds_out, num_workers=num_workers, scheduler=scheduler, progressbar=False
    )

    def test_ds_out():
        assert len(ds_out) == 99
        for index in range(1, 100):
            np.testing.assert_array_equal(
                ds_out[index - 1].image.numpy(), 2 * index * np.ones((index, index))
            )
            np.testing.assert_array_equal(
                ds_out[index - 1].label.numpy(), 2 * index * np.ones((1,))
            )

        assert ds_out.image.shape_interval.lower == (99, 1, 1)
        assert ds_out.image.shape_interval.upper == (99, 99, 99)

    test_ds_out()
    ds_out = local_ds_generator()
    test_ds_out()

    data_in.delete()


</source>
</class>

<class classid="12" nclones="2" nlines="28" similarity="89">
<source file="systems/deeplake-2.2.2/hub/core/transform/test_transform.py" startline="399" endline="431" pcid="353">
def test_transform_hub_read(ds, cat_path, sample_compression, scheduler):
    data_in = [cat_path] * 10
    ds_out = ds
    ds_out.create_tensor("image", htype="image", sample_compression=sample_compression)

    if (
        isinstance(remove_memory_cache(ds.storage), MemoryProvider)
        and scheduler != "threaded"
    ):
        # any scheduler other than `threaded` will not work with a dataset stored in memory
        with pytest.raises(InvalidOutputDatasetError):
            read_image().eval(
                data_in,
                ds_out,
                num_workers=TRANSFORM_TEST_NUM_WORKERS,
                progressbar=False,
                scheduler=scheduler,
            )
        return

    read_image().eval(
        data_in,
        ds_out,
        num_workers=TRANSFORM_TEST_NUM_WORKERS,
        progressbar=False,
        scheduler=scheduler,
    )
    assert len(ds_out) == 10
    for i in range(10):
        assert ds_out.image[i].numpy().shape == (900, 900, 3)
        np.testing.assert_array_equal(ds_out.image[i].numpy(), ds_out.image[0].numpy())


</source>
<source file="systems/deeplake-2.2.2/hub/core/transform/test_transform.py" startline="435" endline="466" pcid="354">
def test_transform_hub_read_pipeline(ds, cat_path, sample_compression, scheduler):
    data_in = [cat_path] * 10
    ds_out = ds
    ds_out.create_tensor("image", htype="image", sample_compression=sample_compression)
    pipeline = hub.compose([read_image(), crop_image(copy=2)])
    if (
        isinstance(remove_memory_cache(ds.storage), MemoryProvider)
        and scheduler != "threaded"
    ):
        # any scheduler other than `threaded` will not work with a dataset stored in memory
        with pytest.raises(InvalidOutputDatasetError):
            pipeline.eval(
                data_in,
                ds_out,
                num_workers=TRANSFORM_TEST_NUM_WORKERS,
                progressbar=False,
                scheduler=scheduler,
            )
        return
    pipeline.eval(
        data_in,
        ds_out,
        num_workers=TRANSFORM_TEST_NUM_WORKERS,
        progressbar=False,
        scheduler=scheduler,
    )
    assert len(ds_out) == 20
    for i in range(20):
        assert ds_out.image[i].numpy().shape == (100, 100, 3)
        np.testing.assert_array_equal(ds_out.image[i].numpy(), ds_out.image[0].numpy())


</source>
</class>

<class classid="13" nclones="2" nlines="15" similarity="75">
<source file="systems/deeplake-2.2.2/hub/core/tiling/test_serialize.py" startline="26" endline="45" pcid="412">
def test_serialize_tiles():
    sample = _get_arange_sample((2, 5))
    tile_shape = (3, 3)

    tiles = break_into_tiles(sample, tile_shape)
    shapes = [t.shape for _, t in np.ndenumerate(tiles)]
    serialized_tiles = serialize_tiles(tiles, lambda x: memoryview(x.tobytes()))
    assert serialized_tiles.shape == tiles.shape

    flattened_tiles = serialized_tiles.reshape((serialized_tiles.size,))
    tiled_arrays = [
        np.frombuffer(x, dtype=sample.dtype).reshape(sh)
        for x, sh in zip(flattened_tiles, shapes)
    ]
    coalesced_sample = np_list_to_sample(
        tiled_arrays, sample.shape, tile_shape, tiles.shape, sample.dtype
    )
    np.testing.assert_array_equal(sample, coalesced_sample)


</source>
<source file="systems/deeplake-2.2.2/hub/core/tiling/test_serialize.py" startline="46" endline="65" pcid="413">
def test_serialize_tiles_gzip():
    sample = _get_arange_sample((2, 5))
    tile_shape = (3, 3)

    tiles = break_into_tiles(sample, tile_shape)
    shapes = [t.shape for _, t in np.ndenumerate(tiles)]
    gzip_compress = lambda x: memoryview(gzip.compress(x.tobytes()))
    serialized_tiles = serialize_tiles(tiles, gzip_compress)
    assert serialized_tiles.shape == tiles.shape

    flattened_tiles = serialized_tiles.reshape((serialized_tiles.size,))
    gzip_decompress = lambda x: np.frombuffer(gzip.decompress(x), dtype=sample.dtype)
    tiled_arrays = [
        gzip_decompress(x).reshape(sh) for x, sh in zip(flattened_tiles, shapes)
    ]

    coalesced_sample = np_list_to_sample(
        tiled_arrays, sample.shape, tile_shape, tiles.shape, sample.dtype
    )
    np.testing.assert_array_equal(sample, coalesced_sample)
</source>
</class>

<class classid="14" nclones="3" nlines="13" similarity="81">
<source file="systems/deeplake-2.2.2/hub/api/tests/test_chunk_sizes.py" startline="33" endline="54" pcid="500">
def test_append(memory_ds):
    ds = memory_ds
    images, labels = _create_tensors(ds)

    _append_tensors(images, labels)

    _assert_num_chunks(labels, 1)
    _assert_num_chunks(images, 5)

    _append_tensors(images, labels)

    _assert_num_chunks(labels, 1)
    _assert_num_chunks(images, 10)

    _append_tensors(images, labels)

    _assert_num_chunks(labels, 1)
    _assert_num_chunks(images, 15)

    assert len(ds) == 300


</source>
<source file="systems/deeplake-2.2.2/hub/api/tests/test_chunk_sizes.py" startline="55" endline="76" pcid="501">
def test_extend(memory_ds):
    ds = memory_ds
    images, labels = _create_tensors(ds)

    _extend_tensors(images, labels)

    _assert_num_chunks(labels, 1)
    _assert_num_chunks(images, 5)

    _extend_tensors(images, labels)

    _assert_num_chunks(labels, 1)
    _assert_num_chunks(images, 10)

    _extend_tensors(images, labels)

    _assert_num_chunks(labels, 1)
    _assert_num_chunks(images, 15)

    assert len(ds) == 300


</source>
<source file="systems/deeplake-2.2.2/hub/api/tests/test_chunk_sizes.py" startline="77" endline="101" pcid="502">
def test_extend_and_append(memory_ds):
    ds = memory_ds
    images, labels = _create_tensors(ds)

    _extend_tensors(images, labels)

    _assert_num_chunks(labels, 1)
    _assert_num_chunks(images, 5)

    _append_tensors(images, labels)

    _assert_num_chunks(labels, 1)
    _assert_num_chunks(images, 10)

    _extend_tensors(images, labels)

    _assert_num_chunks(labels, 1)
    _assert_num_chunks(images, 15)

    _append_tensors(images, labels)

    _assert_num_chunks(labels, 1)
    _assert_num_chunks(images, 20)

    assert len(ds) == 400
</source>
</class>

<class classid="15" nclones="2" nlines="21" similarity="72">
<source file="systems/deeplake-2.2.2/hub/api/tests/test_api_with_compression.py" startline="37" endline="67" pcid="504">
def test_populate_compressed_samples(ds: Dataset, cat_path, flower_path):
    images = ds.create_tensor(
        TENSOR_KEY, htype="image", sample_compression="png", max_chunk_size=2 * MB
    )

    assert images.meta.dtype == "uint8"
    assert images.meta.sample_compression == "png"

    _populate_compressed_samples(images, cat_path, flower_path)

    expected_shapes = [
        (900, 900, 3),
        (513, 464, 4),
        (100, 100, 4),
        (100, 100, 4),
        (513, 464, 4),
        (900, 900, 3),
    ]

    assert len(images) == 6

    for img, exp_shape in zip(images, expected_shapes):
        arr = img.numpy()
        assert arr.shape == exp_shape
        assert arr.dtype == "uint8"

    assert images.shape == (6, None, None, None)
    assert images.shape_interval.lower == (6, 100, 100, 3)
    assert images.shape_interval.upper == (6, 900, 900, 4)


</source>
<source file="systems/deeplake-2.2.2/hub/api/tests/test_api_with_compression.py" startline="69" endline="96" pcid="505">
def test_iterate_compressed_samples(ds: Dataset, cat_path, flower_path):
    images = ds.create_tensor(TENSOR_KEY, htype="image", sample_compression="png")

    assert images.meta.dtype == "uint8"
    assert images.meta.sample_compression == "png"

    _populate_compressed_samples(images, cat_path, flower_path)

    expected_shapes = [
        (900, 900, 3),
        (513, 464, 4),
        (100, 100, 4),
        (100, 100, 4),
        (513, 464, 4),
        (900, 900, 3),
    ]

    assert len(images) == len(expected_shapes)
    for image, expected_shape in zip(images, expected_shapes):
        x = image.numpy()

        assert (
            type(x) == np.ndarray
        ), "Check is necessary in case a `PIL` object is returned instead of an array."
        assert x.shape == expected_shape
        assert x.dtype == "uint8"


</source>
</class>

<class classid="16" nclones="3" nlines="14" similarity="71">
<source file="systems/deeplake-2.2.2/hub/api/tests/test_json.py" startline="9" endline="24" pcid="530">
def test_json_basic(memory_ds):
    ds = memory_ds
    ds.create_tensor("json", htype="json")
    items = [
        {"x": [1, 2, 3], "y": [4, [5, 6]]},
        {"x": [1, 2, 3], "y": [4, {"z": [0.1, 0.2, []]}]},
    ]
    with ds:
        for x in items:
            ds.json.append(x)
        ds.json.extend(items)
    assert ds.json.shape == (4, 1)
    for i in range(4):
        assert ds.json[i].data() == items[i % 2]


</source>
<source file="systems/deeplake-2.2.2/hub/api/tests/test_json.py" startline="25" endline="40" pcid="531">
def test_json_with_numpy(memory_ds):
    ds = memory_ds
    ds.create_tensor("json", htype="json")
    items = [
        {"x": np.array([1, 2, 3], dtype=np.float32), "y": [4, [5, 6]]},
        {"x": np.array([1, 2, 3], dtype=np.uint8), "y": [4, {"z": [0.1, 0.2, []]}]},
    ]
    with ds:
        for x in items:
            ds.json.append(x)
        ds.json.extend(items)
    for i in range(4):
        assert ds.json[i].data()["y"] == items[i % 2]["y"]
        np.testing.assert_array_equal(ds.json[i].data()["x"], items[i % 2]["x"])


</source>
<source file="systems/deeplake-2.2.2/hub/api/tests/test_json.py" startline="65" endline="82" pcid="533">
def test_json_list_basic(memory_ds):
    ds = memory_ds
    ds.create_tensor("list", htype="list")
    items = [
        [{"x": [1, 2, 3], "y": [4, [5, 6]]}, [[]], [None, 0.1]],
        [[], [[[]]], {"a": [0.1, 1, "a", []]}],
    ]
    with ds:
        for x in items:
            ds.list.append(x)
        ds.list.extend(items)
    assert ds.list.shape == (4, 3)
    for i in range(4):
        assert ds.list[i].data() == items[i % 2]
    for i, x in enumerate(ds.list.data()):
        assert x == items[i % 2]


</source>
</class>

</clones>

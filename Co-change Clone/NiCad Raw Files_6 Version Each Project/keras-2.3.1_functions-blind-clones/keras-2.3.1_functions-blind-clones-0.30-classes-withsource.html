<html>
<head>
    <title>NiCad6 Clone Report</title>
    <style type="text/css">
        body {font-family:sans-serif;}
        table {background-color:white; border:0px; padding:0px; border-spacing:4px; width:auto; margin-left:30px; margin-right:auto;}
        td {background-color:rgba(192,212,238,0.8); border:0px; padding:8px; width:auto; vertical-align:top; border-radius:8px}
        pre {background-color:white; padding:4px;}
        a {color:darkblue;}
    </style>
</head>
<body>
<h2>NiCad6 Clone Report</h2>
<table>
<tr style="font-size:14pt">
<td><b>System:</b> &nbsp; keras-2.3.1</td>
<td><b>Clone pairs:</b> &nbsp; 241</td>
<td><b>Clone classes:</b> &nbsp; 104</td>
</tr>
<tr style="font-size:12pt">
<td style="background-color:white">Clone type: &nbsp; 3-2</td>
<td style="background-color:white">Granularity: &nbsp; functions-blind</td>
<td style="background-color:white">Max diff threshold: &nbsp; 30%</td>
<td style="background-color:white">Clone size: &nbsp; 10 - 2500 lines</td>
<td style="background-color:white">Total functions-blind: &nbsp; 2310</td>
</tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 1:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag58')" href="javascript:;">
keras-2.3.1/keras/regularizers.py: 76-88
</a>
<div class="mid" id="frag58" style="display:none"><pre>
def get(identifier):
    if identifier is None:
        return None
    if isinstance(identifier, dict):
        return deserialize(identifier)
    elif isinstance(identifier, six.string_types):
        config = {'class_name': str(identifier), 'config': {}}
        return deserialize(config)
    elif callable(identifier):
        return identifier
    else:
        raise ValueError('Could not interpret regularizer identifier: ' +
                         str(identifier))
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag194')" href="javascript:;">
keras-2.3.1/keras/constraints.py: 171-183
</a>
<div class="mid" id="frag194" style="display:none"><pre>
def get(identifier):
    if identifier is None:
        return None
    if isinstance(identifier, dict):
        return deserialize(identifier)
    elif isinstance(identifier, six.string_types):
        config = {'class_name': str(identifier), 'config': {}}
        return deserialize(config)
    elif callable(identifier):
        return identifier
    else:
        raise ValueError('Could not interpret constraint identifier: ' +
                         str(identifier))
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 2:</b> &nbsp; 2 fragments, nominal size 29 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag59')" href="javascript:;">
keras-2.3.1/keras/legacy/layers.py: 62-95
</a>
<div class="mid" id="frag59" style="display:none"><pre>
    def __init__(self, output_dim,
                 nb_feature=4,
                 init='glorot_uniform',
                 weights=None,
                 W_regularizer=None,
                 b_regularizer=None,
                 activity_regularizer=None,
                 W_constraint=None,
                 b_constraint=None,
                 bias=True,
                 input_dim=None,
                 **kwargs):
        warnings.warn('The `MaxoutDense` layer is deprecated '
                      'and will be removed after 06/2017.')
        self.output_dim = output_dim
        self.nb_feature = nb_feature
        self.init = initializers.get(init)

        self.W_regularizer = regularizers.get(W_regularizer)
        self.b_regularizer = regularizers.get(b_regularizer)
        self.activity_regularizer = regularizers.get(activity_regularizer)

        self.W_constraint = constraints.get(W_constraint)
        self.b_constraint = constraints.get(b_constraint)

        self.bias = bias
        self.initial_weights = weights
        self.input_spec = InputSpec(ndim=2)

        self.input_dim = input_dim
        if self.input_dim:
            kwargs['input_shape'] = (self.input_dim,)
        super(MaxoutDense, self).__init__(**kwargs)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag64')" href="javascript:;">
keras-2.3.1/keras/legacy/layers.py: 188-224
</a>
<div class="mid" id="frag64" style="display:none"><pre>
    def __init__(self,
                 init='glorot_uniform',
                 activation=None,
                 weights=None,
                 W_regularizer=None,
                 b_regularizer=None,
                 activity_regularizer=None,
                 W_constraint=None,
                 b_constraint=None,
                 bias=True,
                 input_dim=None,
                 **kwargs):
        warnings.warn('The `Highway` layer is deprecated '
                      'and will be removed after 06/2017.')
        if 'transform_bias' in kwargs:
            kwargs.pop('transform_bias')
            warnings.warn('`transform_bias` argument is deprecated and '
                          'has been removed.')
        self.init = initializers.get(init)
        self.activation = activations.get(activation)

        self.W_regularizer = regularizers.get(W_regularizer)
        self.b_regularizer = regularizers.get(b_regularizer)
        self.activity_regularizer = regularizers.get(activity_regularizer)

        self.W_constraint = constraints.get(W_constraint)
        self.b_constraint = constraints.get(b_constraint)

        self.bias = bias
        self.initial_weights = weights
        self.input_spec = InputSpec(ndim=2)

        self.input_dim = input_dim
        if self.input_dim:
            kwargs['input_shape'] = (self.input_dim,)
        super(Highway, self).__init__(**kwargs)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 3:</b> &nbsp; 2 fragments, nominal size 24 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag60')" href="javascript:;">
keras-2.3.1/keras/legacy/layers.py: 96-119
</a>
<div class="mid" id="frag60" style="display:none"><pre>
    def build(self, input_shape):
        input_dim = input_shape[1]
        self.input_spec = InputSpec(dtype=K.floatx(),
                                    shape=(None, input_dim))

        self.W = self.add_weight(shape=(self.nb_feature, input_dim, self.output_dim),
                                 initializer=self.init,
                                 name='W',
                                 regularizer=self.W_regularizer,
                                 constraint=self.W_constraint)
        if self.bias:
            self.b = self.add_weight(shape=(self.nb_feature, self.output_dim,),
                                     initializer='zero',
                                     name='b',
                                     regularizer=self.b_regularizer,
                                     constraint=self.b_constraint)
        else:
            self.b = None

        if self.initial_weights is not None:
            self.set_weights(self.initial_weights)
            del self.initial_weights
        self.built = True

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag65')" href="javascript:;">
keras-2.3.1/keras/legacy/layers.py: 225-254
</a>
<div class="mid" id="frag65" style="display:none"><pre>
    def build(self, input_shape):
        input_dim = input_shape[1]
        self.input_spec = InputSpec(dtype=K.floatx(),
                                    shape=(None, input_dim))

        self.W = self.add_weight(shape=(input_dim, input_dim),
                                 initializer=self.init,
                                 name='W',
                                 regularizer=self.W_regularizer,
                                 constraint=self.W_constraint)
        self.W_carry = self.add_weight(shape=(input_dim, input_dim),
                                       initializer=self.init,
                                       name='W_carry')
        if self.bias:
            self.b = self.add_weight(shape=(input_dim,),
                                     initializer='zero',
                                     name='b',
                                     regularizer=self.b_regularizer,
                                     constraint=self.b_constraint)
            self.b_carry = self.add_weight(shape=(input_dim,),
                                           initializer='one',
                                           name='b_carry')
        else:
            self.b_carry = None

        if self.initial_weights is not None:
            self.set_weights(self.initial_weights)
            del self.initial_weights
        self.built = True

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 4:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 85%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag63')" href="javascript:;">
keras-2.3.1/keras/legacy/layers.py: 132-147
</a>
<div class="mid" id="frag63" style="display:none"><pre>
    def get_config(self):
        config = {'output_dim': self.output_dim,
                  'init': initializers.serialize(self.init),
                  'nb_feature': self.nb_feature,
                  'W_regularizer': regularizers.serialize(self.W_regularizer),
                  'b_regularizer': regularizers.serialize(self.b_regularizer),
                  'activity_regularizer':
                      regularizers.serialize(self.activity_regularizer),
                  'W_constraint': constraints.serialize(self.W_constraint),
                  'b_constraint': constraints.serialize(self.b_constraint),
                  'bias': self.bias,
                  'input_dim': self.input_dim}
        base_config = super(MaxoutDense, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag67')" href="javascript:;">
keras-2.3.1/keras/legacy/layers.py: 268-282
</a>
<div class="mid" id="frag67" style="display:none"><pre>
    def get_config(self):
        config = {'init': initializers.serialize(self.init),
                  'activation': activations.serialize(self.activation),
                  'W_regularizer': regularizers.serialize(self.W_regularizer),
                  'b_regularizer': regularizers.serialize(self.b_regularizer),
                  'activity_regularizer':
                      regularizers.serialize(self.activity_regularizer),
                  'W_constraint': constraints.serialize(self.W_constraint),
                  'b_constraint': constraints.serialize(self.b_constraint),
                  'bias': self.bias,
                  'input_dim': self.input_dim}
        base_config = super(Highway, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 5:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag86')" href="javascript:;">
keras-2.3.1/keras/backend/numpy_backend.py: 58-71
</a>
<div class="mid" id="frag86" style="display:none"><pre>
def conv(x, w, padding, data_format):
    y = []
    for i in range(x.shape[0]):
        _y = []
        for j in range(w.shape[1]):
            __y = []
            for k in range(w.shape[0]):
                __y.append(signal.convolve(x[i, k], w[k, j], mode=padding))
            _y.append(np.sum(np.stack(__y, axis=-1), axis=-1))
        y.append(_y)
    y = np.array(y)
    return y


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag87')" href="javascript:;">
keras-2.3.1/keras/backend/numpy_backend.py: 73-86
</a>
<div class="mid" id="frag87" style="display:none"><pre>
def depthwise_conv(x, w, padding, data_format):
    y = []
    for i in range(x.shape[0]):
        _y = []
        for j in range(w.shape[0]):
            __y = []
            for k in range(w.shape[1]):
                __y.append(signal.convolve(x[i, j], w[j, k], mode=padding))
            _y.append(np.stack(__y, axis=0))
        y.append(np.concatenate(_y, axis=0))
    y = np.array(y)
    return y


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 6:</b> &nbsp; 2 fragments, nominal size 39 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag301')" href="javascript:;">
keras-2.3.1/keras/datasets/imdb.py: 14-106
</a>
<div class="mid" id="frag301" style="display:none"><pre>
def load_data(path='imdb.npz', num_words=None, skip_top=0,
              maxlen=None, seed=113,
              start_char=1, oov_char=2, index_from=3, **kwargs):
    """Loads the IMDB dataset.

    # Arguments
        path: where to cache the data (relative to `~/.keras/dataset`).
        num_words: max number of words to include. Words are ranked
            by how often they occur (in the training set) and only
            the most frequent words are kept
        skip_top: skip the top N most frequently occurring words
            (which may not be informative).
        maxlen: sequences longer than this will be filtered out.
        seed: random seed for sample shuffling.
        start_char: The start of a sequence will be marked with this character.
            Set to 1 because 0 is usually the padding character.
        oov_char: words that were cut out because of the `num_words`
            or `skip_top` limit will be replaced with this character.
        index_from: index actual words with this index and higher.

    # Returns
        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.

    # Raises
        ValueError: in case `maxlen` is so low
            that no input sequence could be kept.

    Note that the 'out of vocabulary' character is only used for
    words that were present in the training set but are not included
    because they're not making the `num_words` cut here.
    Words that were not seen in the training set but are in the test set
    have simply been skipped.
    """
    # Legacy support
    if 'nb_words' in kwargs:
        warnings.warn('The `nb_words` argument in `load_data` '
                      'has been renamed `num_words`.')
        num_words = kwargs.pop('nb_words')
    if kwargs:
        raise TypeError('Unrecognized keyword arguments: ' + str(kwargs))

    path = get_file(path,
                    origin='https://s3.amazonaws.com/text-datasets/imdb.npz',
                    file_hash='599dadb1135973df5b59232a0e9a887c')
    with np.load(path, allow_pickle=True) as f:
        x_train, labels_train = f['x_train'], f['y_train']
        x_test, labels_test = f['x_test'], f['y_test']

    rng = np.random.RandomState(seed)
    indices = np.arange(len(x_train))
    rng.shuffle(indices)
    x_train = x_train[indices]
    labels_train = labels_train[indices]

    indices = np.arange(len(x_test))
    rng.shuffle(indices)
    x_test = x_test[indices]
    labels_test = labels_test[indices]

    xs = np.concatenate([x_train, x_test])
    labels = np.concatenate([labels_train, labels_test])

    if start_char is not None:
        xs = [[start_char] + [w + index_from for w in x] for x in xs]
    elif index_from:
        xs = [[w + index_from for w in x] for x in xs]

    if maxlen:
        xs, labels = _remove_long_seq(maxlen, xs, labels)
        if not xs:
            raise ValueError('After filtering for sequences shorter than maxlen=' +
                             str(maxlen) + ', no sequence was kept. '
                             'Increase maxlen.')
    if not num_words:
        num_words = max([max(x) for x in xs])

    # by convention, use 2 as OOV word
    # reserve 'index_from' (=3 by default) characters:
    # 0 (padding), 1 (start), 2 (OOV)
    if oov_char is not None:
        xs = [[w if (skip_top &lt;= w &lt; num_words) else oov_char for w in x]
              for x in xs]
    else:
        xs = [[w for w in x if skip_top &lt;= w &lt; num_words]
              for x in xs]

    idx = len(x_train)
    x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])
    x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])

    return (x_train, y_train), (x_test, y_test)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag303')" href="javascript:;">
keras-2.3.1/keras/datasets/reuters.py: 15-90
</a>
<div class="mid" id="frag303" style="display:none"><pre>
def load_data(path='reuters.npz', num_words=None, skip_top=0,
              maxlen=None, test_split=0.2, seed=113,
              start_char=1, oov_char=2, index_from=3, **kwargs):
    """Loads the Reuters newswire classification dataset.

    # Arguments
        path: where to cache the data (relative to `~/.keras/dataset`).
        num_words: max number of words to include. Words are ranked
            by how often they occur (in the training set) and only
            the most frequent words are kept
        skip_top: skip the top N most frequently occurring words
            (which may not be informative).
        maxlen: truncate sequences after this length.
        test_split: Fraction of the dataset to be used as test data.
        seed: random seed for sample shuffling.
        start_char: The start of a sequence will be marked with this character.
            Set to 1 because 0 is usually the padding character.
        oov_char: words that were cut out because of the `num_words`
            or `skip_top` limit will be replaced with this character.
        index_from: index actual words with this index and higher.

    # Returns
        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.

    Note that the 'out of vocabulary' character is only used for
    words that were present in the training set but are not included
    because they're not making the `num_words` cut here.
    Words that were not seen in the training set but are in the test set
    have simply been skipped.
    """
    # Legacy support
    if 'nb_words' in kwargs:
        warnings.warn('The `nb_words` argument in `load_data` '
                      'has been renamed `num_words`.')
        num_words = kwargs.pop('nb_words')
    if kwargs:
        raise TypeError('Unrecognized keyword arguments: ' + str(kwargs))

    path = get_file(path,
                    origin='https://s3.amazonaws.com/text-datasets/reuters.npz',
                    file_hash='87aedbeb0cb229e378797a632c1997b6')
    with np.load(path, allow_pickle=True) as f:
        xs, labels = f['x'], f['y']

    rng = np.random.RandomState(seed)
    indices = np.arange(len(xs))
    rng.shuffle(indices)
    xs = xs[indices]
    labels = labels[indices]

    if start_char is not None:
        xs = [[start_char] + [w + index_from for w in x] for x in xs]
    elif index_from:
        xs = [[w + index_from for w in x] for x in xs]

    if maxlen:
        xs, labels = _remove_long_seq(maxlen, xs, labels)

    if not num_words:
        num_words = max([max(x) for x in xs])

    # by convention, use 2 as OOV word
    # reserve 'index_from' (=3 by default) characters:
    # 0 (padding), 1 (start), 2 (OOV)
    if oov_char is not None:
        xs = [[w if skip_top &lt;= w &lt; num_words else oov_char for w in x] for x in xs]
    else:
        xs = [[w for w in x if skip_top &lt;= w &lt; num_words] for x in xs]

    idx = int(len(xs) * (1 - test_split))
    x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])
    x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])

    return (x_train, y_train), (x_test, y_test)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 7:</b> &nbsp; 2 fragments, nominal size 99 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag395')" href="javascript:;">
keras-2.3.1/keras/engine/training_generator.py: 296-425
</a>
<div class="mid" id="frag395" style="display:none"><pre>
def evaluate_generator(model, generator,
                       steps=None,
                       callbacks=None,
                       max_queue_size=10,
                       workers=1,
                       use_multiprocessing=False,
                       verbose=0):
    """See docstring for `Model.evaluate_generator`."""
    model._make_test_function()
    model.reset_metrics()

    steps_done = 0
    outs_per_batch = []
    batch_sizes = []
    use_sequence_api = is_sequence(generator)
    if not use_sequence_api and use_multiprocessing and workers &gt; 1:
        warnings.warn(
            UserWarning('Using a generator with `use_multiprocessing=True`'
                        ' and multiple workers may duplicate your data.'
                        ' Please consider using the `keras.utils.Sequence'
                        ' class.'))
    if steps is None:
        if use_sequence_api:
            steps = len(generator)
        else:
            raise ValueError('`steps=None` is only valid for a generator'
                             ' based on the `keras.utils.Sequence` class.'
                             ' Please specify `steps` or use the'
                             ' `keras.utils.Sequence` class.')
    enqueuer = None

    # Check if callbacks have not been already configured
    if not isinstance(callbacks, cbks.CallbackList):
        callbacks = cbks.CallbackList(callbacks)
        callback_model = model._get_callback_model()
        callbacks.set_model(callback_model)
        callback_metrics = list(model.metrics_names)
        callback_params = {
            'steps': steps,
            'verbose': verbose,
            'metrics': callback_metrics,
        }
        callbacks.set_params(callback_params)

    callbacks.model.stop_training = False
    callbacks._call_begin_hook('test')

    try:
        if workers &gt; 0:
            if use_sequence_api:
                enqueuer = OrderedEnqueuer(
                    generator,
                    use_multiprocessing=use_multiprocessing)
            else:
                enqueuer = GeneratorEnqueuer(
                    generator,
                    use_multiprocessing=use_multiprocessing)
            enqueuer.start(workers=workers, max_queue_size=max_queue_size)
            output_generator = enqueuer.get()
        else:
            if use_sequence_api:
                output_generator = iter_sequence_infinite(generator)
            else:
                output_generator = generator

        if verbose == 1:
            progbar = Progbar(target=steps)

        while steps_done &lt; steps:
            generator_output = next(output_generator)
            if not hasattr(generator_output, '__len__'):
                raise ValueError('Output of generator should be a tuple '
                                 '(x, y, sample_weight) '
                                 'or (x, y). Found: ' +
                                 str(generator_output))
            if len(generator_output) == 2:
                x, y = generator_output
                sample_weight = None
            elif len(generator_output) == 3:
                x, y, sample_weight = generator_output
            else:
                raise ValueError('Output of generator should be a tuple '
                                 '(x, y, sample_weight) '
                                 'or (x, y). Found: ' +
                                 str(generator_output))

            if x is None or len(x) == 0:
                # Handle data tensors support when no input given
                # step-size = 1 for data tensors
                batch_size = 1
            elif isinstance(x, list):
                batch_size = x[0].shape[0]
            elif isinstance(x, dict):
                batch_size = list(x.values())[0].shape[0]
            else:
                batch_size = x.shape[0]
            if batch_size == 0:
                raise ValueError('Received an empty batch. '
                                 'Batches should contain '
                                 'at least one item.')

            batch_logs = {'batch': steps_done, 'size': batch_size}
            callbacks._call_batch_hook('test', 'begin', steps_done, batch_logs)
            outs = model.test_on_batch(x, y,
                                       sample_weight=sample_weight,
                                       reset_metrics=False)
            outs = to_list(outs)
            outs_per_batch.append(outs)

            for l, o in zip(model.metrics_names, outs):
                batch_logs[l] = o
            callbacks._call_batch_hook('test', 'end', steps_done, batch_logs)

            steps_done += 1
            batch_sizes.append(batch_size)

            if verbose == 1:
                progbar.update(steps_done)
        callbacks._call_end_hook('test')

    finally:
        if enqueuer is not None:
            enqueuer.stop()

    averages = [float(outs_per_batch[-1][0])]  # index 0 = 'loss'
    for i in range(1, len(outs)):
        averages.append(np.float64(outs_per_batch[-1][i]))
    return unpack_singleton(averages)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag396')" href="javascript:;">
keras-2.3.1/keras/engine/training_generator.py: 426-556
</a>
<div class="mid" id="frag396" style="display:none"><pre>
def predict_generator(model, generator,
                      steps=None,
                      callbacks=None,
                      max_queue_size=10,
                      workers=1,
                      use_multiprocessing=False,
                      verbose=0):
    """See docstring for `Model.predict_generator`."""
    model._make_predict_function()

    steps_done = 0
    all_outs = []
    use_sequence_api = is_sequence(generator)
    if not use_sequence_api and use_multiprocessing and workers &gt; 1:
        warnings.warn(
            UserWarning('Using a generator with `use_multiprocessing=True`'
                        ' and multiple workers may duplicate your data.'
                        ' Please consider using the `keras.utils.Sequence'
                        ' class.'))
    if steps is None:
        if use_sequence_api:
            steps = len(generator)
        else:
            raise ValueError('`steps=None` is only valid for a generator'
                             ' based on the `keras.utils.Sequence` class.'
                             ' Please specify `steps` or use the'
                             ' `keras.utils.Sequence` class.')
    enqueuer = None

    # Check if callbacks have not been already configured
    if not isinstance(callbacks, cbks.CallbackList):
        callbacks = cbks.CallbackList(callbacks)
        callback_model = model._get_callback_model()
        callbacks.set_model(callback_model)
        callback_params = {
            'steps': steps,
            'verbose': verbose,
        }
        callbacks.set_params(callback_params)

    callbacks.model.stop_training = False
    callbacks._call_begin_hook('predict')

    try:
        if workers &gt; 0:
            if use_sequence_api:
                enqueuer = OrderedEnqueuer(
                    generator,
                    use_multiprocessing=use_multiprocessing)
            else:
                enqueuer = GeneratorEnqueuer(
                    generator,
                    use_multiprocessing=use_multiprocessing)
            enqueuer.start(workers=workers, max_queue_size=max_queue_size)
            output_generator = enqueuer.get()
        else:
            if use_sequence_api:
                output_generator = iter_sequence_infinite(generator)
            else:
                output_generator = generator

        if verbose == 1:
            progbar = Progbar(target=steps)

        while steps_done &lt; steps:
            generator_output = next(output_generator)
            if isinstance(generator_output, tuple):
                # Compatibility with the generators
                # used for training.
                if len(generator_output) == 2:
                    x, _ = generator_output
                elif len(generator_output) == 3:
                    x, _, _ = generator_output
                else:
                    raise ValueError('Output of generator should be '
                                     'a tuple `(x, y, sample_weight)` '
                                     'or `(x, y)`. Found: ' +
                                     str(generator_output))
            else:
                # Assumes a generator that only
                # yields inputs (not targets and sample weights).
                x = generator_output

            if x is None or len(x) == 0:
                # Handle data tensors support when no input given
                # step-size = 1 for data tensors
                batch_size = 1
            elif isinstance(x, list):
                batch_size = x[0].shape[0]
            elif isinstance(x, dict):
                batch_size = list(x.values())[0].shape[0]
            else:
                batch_size = x.shape[0]
            if batch_size == 0:
                raise ValueError('Received an empty batch. '
                                 'Batches should contain '
                                 'at least one item.')

            batch_logs = {'batch': steps_done, 'size': batch_size}
            callbacks._call_batch_hook('predict', 'begin', steps_done, batch_logs)

            outs = model.predict_on_batch(x)
            outs = to_list(outs)

            if not all_outs:
                for out in outs:
                    all_outs.append([])

            for i, out in enumerate(outs):
                all_outs[i].append(out)

            batch_logs['outputs'] = outs
            callbacks._call_batch_hook('predict', 'end', steps_done, batch_logs)

            steps_done += 1
            if verbose == 1:
                progbar.update(steps_done)
        callbacks._call_end_hook('predict')
    finally:
        if enqueuer is not None:
            enqueuer.stop()

    if len(all_outs) == 1:
        if steps_done == 1:
            return all_outs[0][0]
        else:
            return np.concatenate(all_outs[0])
    if steps_done == 1:
        return [out[0] for out in all_outs]
    else:
        return [np.concatenate(out) for out in all_outs]
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 8:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag415')" href="javascript:;">
keras-2.3.1/keras/engine/network.py: 555-585
</a>
<div class="mid" id="frag415" style="display:none"><pre>
    def call(self, inputs, mask=None):
        """Calls the model on new inputs.

        In this case `call` just reapplies
        all ops in the graph to the new inputs
        (e.g. build a new computational graph from the provided inputs).

        A model is callable on non-Keras tensors.

        # Arguments
            inputs: A tensor or list of tensors.
            mask: A mask or list of masks. A mask can be
                either a tensor or None (no mask).

        # Returns
            A tensor if there is a single output, or
            a list of tensors if there are more than one outputs.
        """
        inputs = to_list(inputs)
        if mask is None:
            masks = [None for _ in range(len(inputs))]
        else:
            masks = to_list(mask)
        cache_key = object_list_uid(inputs)
        cache_key += '_' + object_list_uid(masks)
        if cache_key in self._output_tensor_cache:
            return self._output_tensor_cache[cache_key]
        else:
            output_tensors, _, _ = self.run_internal_graph(inputs, masks)
            return output_tensors

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag416')" href="javascript:;">
keras-2.3.1/keras/engine/network.py: 586-602
</a>
<div class="mid" id="frag416" style="display:none"><pre>
    def compute_mask(self, inputs, mask):
        if not self._is_graph_network:
            return None

        inputs = to_list(inputs)
        if mask is None:
            masks = [None for _ in range(len(inputs))]
        else:
            masks = to_list(mask)
        cache_key = object_list_uid(inputs)
        cache_key += '_' + object_list_uid(masks)
        if cache_key in self._output_mask_cache:
            return self._output_mask_cache[cache_key]
        else:
            _, output_masks, _ = self.run_internal_graph(inputs, masks)
            return output_masks

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 9:</b> &nbsp; 2 fragments, nominal size 26 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag442')" href="javascript:;">
keras-2.3.1/keras/engine/training.py: 301-334
</a>
<div class="mid" id="frag442" style="display:none"><pre>
    def _make_train_function(self):
        if not hasattr(self, 'train_function'):
            raise RuntimeError('You must compile your model before using it.')
        self._check_trainable_weights_consistency()
        if self.train_function is None:
            inputs = (self._feed_inputs +
                      self._feed_targets +
                      self._feed_sample_weights)
            if self._uses_dynamic_learning_phase():
                inputs += [K.learning_phase()]

            with K.name_scope('training'):
                with K.name_scope(self.optimizer.__class__.__name__):
                    training_updates = self.optimizer.get_updates(
                        params=self._collected_trainable_weights,
                        loss=self.total_loss)
                updates = self.updates + training_updates

                metrics = self._get_training_eval_metrics()
                metrics_tensors = [
                    m._call_result for m in metrics if hasattr(m, '_call_result')
                ]
                metrics_updates = []
                for m in metrics:
                    metrics_updates.extend(m.updates)

                # Gets loss and metrics. Updates weights at each call.
                self.train_function = K.function(
                    inputs,
                    [self.total_loss] + metrics_tensors,
                    updates=updates + metrics_updates,
                    name='train_function',
                    **self._function_kwargs)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag443')" href="javascript:;">
keras-2.3.1/keras/engine/training.py: 335-362
</a>
<div class="mid" id="frag443" style="display:none"><pre>
    def _make_test_function(self):
        if not hasattr(self, 'test_function'):
            raise RuntimeError('You must compile your model before using it.')
        if self.test_function is None:
            inputs = (self._feed_inputs +
                      self._feed_targets +
                      self._feed_sample_weights)
            if self._uses_dynamic_learning_phase():
                inputs += [K.learning_phase()]

            metrics = self._get_training_eval_metrics()
            metrics_tensors = [
                m._call_result for m in metrics if hasattr(m, '_call_result')
            ]

            metrics_updates = []
            for m in metrics:
                metrics_updates.extend(m.updates)

            # Return loss and metrics, no gradient updates.
            # Does update the network states.
            self.test_function = K.function(
                inputs,
                [self.total_loss] + metrics_tensors,
                updates=self.state_updates + metrics_updates,
                name='test_function',
                **self._function_kwargs)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 10:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag462')" href="javascript:;">
keras-2.3.1/keras/engine/training.py: 1464-1519
</a>
<div class="mid" id="frag462" style="display:none"><pre>
    def train_on_batch(self, x, y,
                       sample_weight=None,
                       class_weight=None,
                       reset_metrics=True):
        """Runs a single gradient update on a single batch of data.

        # Arguments
            x: Numpy array of training data,
                or list of Numpy arrays if the model has multiple inputs.
                If all inputs in the model are named,
                you can also pass a dictionary
                mapping input names to Numpy arrays.
            y: Numpy array of target data,
                or list of Numpy arrays if the model has multiple outputs.
                If all outputs in the model are named,
                you can also pass a dictionary
                mapping output names to Numpy arrays.
            sample_weight: Optional array of the same length as x, containing
                weights to apply to the model's loss for each sample.
                In the case of temporal data, you can pass a 2D array
                with shape (samples, sequence_length),
                to apply a different weight to every timestep of every sample.
                In this case you should make sure to specify
                sample_weight_mode="temporal" in compile().
            class_weight: Optional dictionary mapping
                class indices (integers) to
                a weight (float) to apply to the model's loss for the samples
                from this class during training.
                This can be useful to tell the model to "pay more attention" to
                samples from an under-represented class.
            reset_metrics: If `True`, the metrics returned will be only for this
                batch. If `False`, the metrics will be statefully accumulated across
                batches.

        # Returns
            Scalar training loss
            (if the model has a single output and no metrics)
            or list of scalars (if the model has multiple outputs
            and/or metrics). The attribute `model.metrics_names` will give you
            the display labels for the scalar outputs.
        """
        x, y, sample_weights = self._standardize_user_data(
            x, y,
            sample_weight=sample_weight,
            class_weight=class_weight)
        if self._uses_dynamic_learning_phase():
            ins = x + y + sample_weights + [1]
        else:
            ins = x + y + sample_weights
        self._make_train_function()
        outputs = self.train_function(ins)

        if reset_metrics:
            self.reset_metrics()
        return unpack_singleton(outputs)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag463')" href="javascript:;">
keras-2.3.1/keras/engine/training.py: 1520-1564
</a>
<div class="mid" id="frag463" style="display:none"><pre>
    def test_on_batch(self, x, y, sample_weight=None, reset_metrics=True):
        """Test the model on a single batch of samples.

        # Arguments
            x: Numpy array of test data,
                or list of Numpy arrays if the model has multiple inputs.
                If all inputs in the model are named,
                you can also pass a dictionary
                mapping input names to Numpy arrays.
            y: Numpy array of target data,
                or list of Numpy arrays if the model has multiple outputs.
                If all outputs in the model are named,
                you can also pass a dictionary
                mapping output names to Numpy arrays.
            sample_weight: Optional array of the same length as x, containing
                weights to apply to the model's loss for each sample.
                In the case of temporal data, you can pass a 2D array
                with shape (samples, sequence_length),
                to apply a different weight to every timestep of every sample.
                In this case you should make sure to specify
                sample_weight_mode="temporal" in compile().
            reset_metrics: If `True`, the metrics returned will be only for this
                batch. If `False`, the metrics will be statefully accumulated across
                batches.

        # Returns
            Scalar test loss (if the model has a single output and no metrics)
            or list of scalars (if the model has multiple outputs
            and/or metrics). The attribute `model.metrics_names` will give you
            the display labels for the scalar outputs.
        """
        x, y, sample_weights = self._standardize_user_data(
            x, y,
            sample_weight=sample_weight)
        if self._uses_dynamic_learning_phase():
            ins = x + y + sample_weights + [0]
        else:
            ins = x + y + sample_weights
        self._make_test_function()
        outputs = self.test_function(ins)

        if reset_metrics:
            self.reset_metrics()
        return unpack_singleton(outputs)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 11:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag466')" href="javascript:;">
keras-2.3.1/keras/engine/training.py: 1735-1792
</a>
<div class="mid" id="frag466" style="display:none"><pre>
    def evaluate_generator(self, generator,
                           steps=None,
                           callbacks=None,
                           max_queue_size=10,
                           workers=1,
                           use_multiprocessing=False,
                           verbose=0):
        """Evaluates the model on a data generator.

        The generator should return the same kind of data
        as accepted by `test_on_batch`.

        # Arguments
            generator: Generator yielding tuples (inputs, targets)
                or (inputs, targets, sample_weights)
                or an instance of Sequence (keras.utils.Sequence)
                object in order to avoid duplicate data
                when using multiprocessing.
            steps: Total number of steps (batches of samples)
                to yield from `generator` before stopping.
                Optional for `Sequence`: if unspecified, will use
                the `len(generator)` as a number of steps.
            callbacks: List of `keras.callbacks.Callback` instances.
                List of callbacks to apply during training.
                See [callbacks](/callbacks).
            max_queue_size: maximum size for the generator queue
            workers: Integer. Maximum number of processes to spin up
                when using process based threading.
                If unspecified, `workers` will default to 1. If 0, will
                execute the generator on the main thread.
            use_multiprocessing: if True, use process based threading.
                Note that because
                this implementation relies on multiprocessing,
                you should not pass
                non picklable arguments to the generator
                as they can't be passed
                easily to children processes.
            verbose: verbosity mode, 0 or 1.

        # Returns
            Scalar test loss (if the model has a single output and no metrics)
            or list of scalars (if the model has multiple outputs
            and/or metrics). The attribute `model.metrics_names` will give you
            the display labels for the scalar outputs.

        # Raises
            ValueError: In case the generator yields
                data in an invalid format.
        """
        return training_generator.evaluate_generator(
            self, generator,
            steps=steps,
            callbacks=callbacks,
            max_queue_size=max_queue_size,
            workers=workers,
            use_multiprocessing=use_multiprocessing,
            verbose=verbose)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag467')" href="javascript:;">
keras-2.3.1/keras/engine/training.py: 1794-1848
</a>
<div class="mid" id="frag467" style="display:none"><pre>
    def predict_generator(self, generator,
                          steps=None,
                          callbacks=None,
                          max_queue_size=10,
                          workers=1,
                          use_multiprocessing=False,
                          verbose=0):
        """Generates predictions for the input samples from a data generator.

        The generator should return the same kind of data as accepted by
        `predict_on_batch`.

        # Arguments
            generator: Generator yielding batches of input samples
                or an instance of Sequence (keras.utils.Sequence)
                object in order to avoid duplicate data
                when using multiprocessing.
            steps: Total number of steps (batches of samples)
                to yield from `generator` before stopping.
                Optional for `Sequence`: if unspecified, will use
                the `len(generator)` as a number of steps.
            callbacks: List of `keras.callbacks.Callback` instances.
                List of callbacks to apply during training.
                See [callbacks](/callbacks).
            max_queue_size: Maximum size for the generator queue.
            workers: Integer. Maximum number of processes to spin up
                when using process based threading.
                If unspecified, `workers` will default to 1. If 0, will
                execute the generator on the main thread.
            use_multiprocessing: If `True`, use process based threading.
                Note that because
                this implementation relies on multiprocessing,
                you should not pass
                non picklable arguments to the generator
                as they can't be passed
                easily to children processes.
            verbose: verbosity mode, 0 or 1.

        # Returns
            Numpy array(s) of predictions.

        # Raises
            ValueError: In case the generator yields
                data in an invalid format.
        """
        return training_generator.predict_generator(
            self, generator,
            steps=steps,
            callbacks=callbacks,
            max_queue_size=max_queue_size,
            workers=workers,
            use_multiprocessing=use_multiprocessing,
            verbose=verbose)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 12:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 91%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag535')" href="javascript:;">
keras-2.3.1/keras/losses.py: 301-314
</a>
<div class="mid" id="frag535" style="display:none"><pre>
    def __init__(self,
                 from_logits=False,
                 label_smoothing=0,
                 reduction=losses_utils.Reduction.SUM_OVER_BATCH_SIZE,
                 name='binary_crossentropy'):
        super(BinaryCrossentropy, self).__init__(
            binary_crossentropy,
            name=name,
            reduction=reduction,
            from_logits=from_logits,
            label_smoothing=label_smoothing)
        self.from_logits = from_logits


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag536')" href="javascript:;">
keras-2.3.1/keras/losses.py: 357-369
</a>
<div class="mid" id="frag536" style="display:none"><pre>
    def __init__(self,
                 from_logits=False,
                 label_smoothing=0,
                 reduction=losses_utils.Reduction.SUM_OVER_BATCH_SIZE,
                 name='categorical_crossentropy'):
        super(CategoricalCrossentropy, self).__init__(
            categorical_crossentropy,
            name=name,
            reduction=reduction,
            from_logits=from_logits,
            label_smoothing=label_smoothing)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 13:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag567')" href="javascript:;">
keras-2.3.1/keras/losses.py: 779-802
</a>
<div class="mid" id="frag567" style="display:none"><pre>
def get(identifier):
    """Get the `identifier` loss function.

    # Arguments
        identifier: None or str, name of the function.

    # Returns
        The loss function or None if `identifier` is None.

    # Raises
        ValueError if unknown identifier.
    """
    if identifier is None:
        return None
    if isinstance(identifier, six.string_types):
        identifier = str(identifier)
        return deserialize(identifier)
    if isinstance(identifier, dict):
        return deserialize(identifier)
    elif callable(identifier):
        return identifier
    else:
        raise ValueError('Could not interpret '
                         'loss function identifier:', identifier)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1062')" href="javascript:;">
keras-2.3.1/keras/activations.py: 211-239
</a>
<div class="mid" id="frag1062" style="display:none"><pre>
def get(identifier):
    """Get the `identifier` activation function.

    # Arguments
        identifier: None or str, name of the function.

    # Returns
        The activation function, `linear` if `identifier` is None.

    # Raises
        ValueError if unknown identifier
    """
    if identifier is None:
        return linear
    if isinstance(identifier, six.string_types):
        identifier = str(identifier)
        return deserialize(identifier)
    elif callable(identifier):
        if isinstance(identifier, Layer):
            warnings.warn(
                'Do not pass a layer instance (such as {identifier}) as the '
                'activation argument of another layer. Instead, advanced '
                'activation layers should be used just like any other '
                'layer in a model.'.format(
                    identifier=identifier.__class__.__name__))
        return identifier
    else:
        raise ValueError('Could not interpret '
                         'activation function identifier:', identifier)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 14:</b> &nbsp; 3 fragments, nominal size 42 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag573')" href="javascript:;">
keras-2.3.1/keras/preprocessing/image.py: 188-237
</a>
<div class="mid" id="frag573" style="display:none"><pre>
    def __init__(self,
                 dataframe,
                 directory=None,
                 image_data_generator=None,
                 x_col='filename',
                 y_col='class',
                 weight_col=None,
                 target_size=(256, 256),
                 color_mode='rgb',
                 classes=None,
                 class_mode='categorical',
                 batch_size=32,
                 shuffle=True,
                 seed=None,
                 data_format='channels_last',
                 save_to_dir=None,
                 save_prefix='',
                 save_format='png',
                 subset=None,
                 interpolation='nearest',
                 dtype='float32',
                 validate_filenames=True):
        if data_format is None:
            data_format = backend.image_data_format()
        if dtype is None:
            dtype = backend.floatx()
        super(DataFrameIterator, self).__init__(
            dataframe,
            directory=directory,
            image_data_generator=image_data_generator,
            x_col=x_col,
            y_col=y_col,
            weight_col=weight_col,
            target_size=target_size,
            color_mode=color_mode,
            classes=classes,
            class_mode=class_mode,
            batch_size=batch_size,
            shuffle=shuffle,
            seed=seed,
            data_format=data_format,
            save_to_dir=save_to_dir,
            save_prefix=save_prefix,
            save_format=save_format,
            subset=subset,
            interpolation=interpolation,
            dtype=dtype,
            validate_filenames=validate_filenames)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag577')" href="javascript:;">
keras-2.3.1/keras/preprocessing/image.py: 470-597
</a>
<div class="mid" id="frag577" style="display:none"><pre>
    def flow_from_dataframe(self,
                            dataframe,
                            directory=None,
                            x_col="filename",
                            y_col="class",
                            weight_col=None,
                            target_size=(256, 256),
                            color_mode='rgb',
                            classes=None,
                            class_mode='categorical',
                            batch_size=32,
                            shuffle=True,
                            seed=None,
                            save_to_dir=None,
                            save_prefix='',
                            save_format='png',
                            subset=None,
                            interpolation='nearest',
                            validate_filenames=True,
                            **kwargs):
        """Takes the dataframe and the path to a directory
         and generates batches of augmented/normalized data.

        **A simple tutorial can be found **[here](
                                    http://bit.ly/keras_flow_from_dataframe).

        # Arguments
            dataframe: Pandas dataframe containing the filepaths relative to
                `directory` (or absolute paths if `directory` is None) of the
                images in a string column. It should include other column/s
                depending on the `class_mode`:
                - if `class_mode` is `"categorical"` (default value) it must
                    include the `y_col` column with the class/es of each image.
                    Values in column can be string/list/tuple if a single class
                    or list/tuple if multiple classes.
                - if `class_mode` is `"binary"` or `"sparse"` it must include
                    the given `y_col` column with class values as strings.
                - if `class_mode` is `"raw"` or `"multi_output"` it should contain
                the columns specified in `y_col`.
                - if `class_mode` is `"input"` or `None` no extra column is needed.
            directory: string, path to the directory to read images from. If `None`,
                data in `x_col` column should be absolute paths.
            x_col: string, column in `dataframe` that contains the filenames (or
                absolute paths if `directory` is `None`).
            y_col: string or list, column/s in `dataframe` that has the target data.
            weight_col: string, column in `dataframe` that contains the sample
                weights. Default: `None`.
            target_size: tuple of integers `(height, width)`, default: `(256, 256)`.
                The dimensions to which all images found will be resized.
            color_mode: one of "grayscale", "rgb", "rgba". Default: "rgb".
                Whether the images will be converted to have 1 or 3 color channels.
            classes: optional list of classes (e.g. `['dogs', 'cats']`).
                Default: None. If not provided, the list of classes will be
                automatically inferred from the `y_col`,
                which will map to the label indices, will be alphanumeric).
                The dictionary containing the mapping from class names to class
                indices can be obtained via the attribute `class_indices`.
            class_mode: one of "binary", "categorical", "input", "multi_output",
                "raw", sparse" or None. Default: "categorical".
                Mode for yielding the targets:
                - `"binary"`: 1D numpy array of binary labels,
                - `"categorical"`: 2D numpy array of one-hot encoded labels.
                    Supports multi-label output.
                - `"input"`: images identical to input images (mainly used to
                    work with autoencoders),
                - `"multi_output"`: list with the values of the different columns,
                - `"raw"`: numpy array of values in `y_col` column(s),
                - `"sparse"`: 1D numpy array of integer labels,
                - `None`, no targets are returned (the generator will only yield
                    batches of image data, which is useful to use in
                    `model.predict_generator()`).
            batch_size: size of the batches of data (default: 32).
            shuffle: whether to shuffle the data (default: True)
            seed: optional random seed for shuffling and transformations.
            save_to_dir: None or str (default: None).
                This allows you to optionally specify a directory
                to which to save the augmented pictures being generated
                (useful for visualizing what you are doing).
            save_prefix: str. Prefix to use for filenames of saved pictures
                (only relevant if `save_to_dir` is set).
            save_format: one of "png", "jpeg"
                (only relevant if `save_to_dir` is set). Default: "png".
            follow_links: whether to follow symlinks inside class subdirectories
                (default: False).
            subset: Subset of data (`"training"` or `"validation"`) if
                `validation_split` is set in `ImageDataGenerator`.
            interpolation: Interpolation method used to resample the image if the
                target size is different from that of the loaded image.
                Supported methods are `"nearest"`, `"bilinear"`, and `"bicubic"`.
                If PIL version 1.1.3 or newer is installed, `"lanczos"` is also
                supported. If PIL version 3.4.0 or newer is installed, `"box"` and
                `"hamming"` are also supported. By default, `"nearest"` is used.
            validate_filenames: Boolean, whether to validate image filenames in
                `x_col`. If `True`, invalid images will be ignored. Disabling this
                option can lead to speed-up in the execution of this function.
                Default: `True`.

        # Returns
            A `DataFrameIterator` yielding tuples of `(x, y)`
            where `x` is a numpy array containing a batch
            of images with shape `(batch_size, *target_size, channels)`
            and `y` is a numpy array of corresponding labels.
        """
        return DataFrameIterator(
            dataframe,
            directory,
            self,
            x_col=x_col,
            y_col=y_col,
            weight_col=weight_col,
            target_size=target_size,
            color_mode=color_mode,
            classes=classes,
            class_mode=class_mode,
            data_format=self.data_format,
            batch_size=batch_size,
            shuffle=shuffle,
            seed=seed,
            save_to_dir=save_to_dir,
            save_prefix=save_prefix,
            save_format=save_format,
            subset=subset,
            interpolation=interpolation,
            validate_filenames=validate_filenames,
            **kwargs
        )


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag576')" href="javascript:;">
keras-2.3.1/keras/preprocessing/image.py: 362-469
</a>
<div class="mid" id="frag576" style="display:none"><pre>
    def flow_from_directory(self,
                            directory,
                            target_size=(256, 256),
                            color_mode='rgb',
                            classes=None,
                            class_mode='categorical',
                            batch_size=32,
                            shuffle=True,
                            seed=None,
                            save_to_dir=None,
                            save_prefix='',
                            save_format='png',
                            follow_links=False,
                            subset=None,
                            interpolation='nearest'):
        """Takes the path to a directory &amp; generates batches of augmented data.

        # Arguments
            directory: string, path to the target directory.
                It should contain one subdirectory per class.
                Any PNG, JPG, BMP, PPM or TIF images
                inside each of the subdirectories directory tree
                will be included in the generator.
                See [this script](
                https://gist.github.com/fchollet/0830affa1f7f19fd47b06d4cf89ed44d)
                for more details.
            target_size: Tuple of integers `(height, width)`,
                default: `(256, 256)`.
                The dimensions to which all images found will be resized.
            color_mode: One of "grayscale", "rgb", "rgba". Default: "rgb".
                Whether the images will be converted to
                have 1, 3, or 4 channels.
            classes: Optional list of class subdirectories
                (e.g. `['dogs', 'cats']`). Default: None.
                If not provided, the list of classes will be automatically
                inferred from the subdirectory names/structure
                under `directory`, where each subdirectory will
                be treated as a different class
                (and the order of the classes, which will map to the label
                indices, will be alphanumeric).
                The dictionary containing the mapping from class names to class
                indices can be obtained via the attribute `class_indices`.
            class_mode: One of "categorical", "binary", "sparse",
                "input", or None. Default: "categorical".
                Determines the type of label arrays that are returned:
                - "categorical" will be 2D one-hot encoded labels,
                - "binary" will be 1D binary labels,
                    "sparse" will be 1D integer labels,
                - "input" will be images identical
                    to input images (mainly used to work with autoencoders).
                - If None, no labels are returned
                  (the generator will only yield batches of image data,
                  which is useful to use with `model.predict_generator()`).
                  Please note that in case of class_mode None,
                  the data still needs to reside in a subdirectory
                  of `directory` for it to work correctly.
            batch_size: Size of the batches of data (default: 32).
            shuffle: Whether to shuffle the data (default: True)
                If set to False, sorts the data in alphanumeric order.
            seed: Optional random seed for shuffling and transformations.
            save_to_dir: None or str (default: None).
                This allows you to optionally specify
                a directory to which to save
                the augmented pictures being generated
                (useful for visualizing what you are doing).
            save_prefix: Str. Prefix to use for filenames of saved pictures
                (only relevant if `save_to_dir` is set).
            save_format: One of "png", "jpeg"
                (only relevant if `save_to_dir` is set). Default: "png".
            follow_links: Whether to follow symlinks inside
                class subdirectories (default: False).
            subset: Subset of data (`"training"` or `"validation"`) if
                `validation_split` is set in `ImageDataGenerator`.
            interpolation: Interpolation method used to
                resample the image if the
                target size is different from that of the loaded image.
                Supported methods are `"nearest"`, `"bilinear"`,
                and `"bicubic"`.
                If PIL version 1.1.3 or newer is installed, `"lanczos"` is also
                supported. If PIL version 3.4.0 or newer is installed,
                `"box"` and `"hamming"` are also supported.
                By default, `"nearest"` is used.

        # Returns
            A `DirectoryIterator` yielding tuples of `(x, y)`
                where `x` is a numpy array containing a batch
                of images with shape `(batch_size, *target_size, channels)`
                and `y` is a numpy array of corresponding labels.
        """
        return DirectoryIterator(
            directory,
            self,
            target_size=target_size,
            color_mode=color_mode,
            classes=classes,
            class_mode=class_mode,
            data_format=self.data_format,
            batch_size=batch_size,
            shuffle=shuffle,
            seed=seed,
            save_to_dir=save_to_dir,
            save_prefix=save_prefix,
            save_format=save_format,
            follow_links=follow_links,
            subset=subset,
            interpolation=interpolation
        )

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 15:</b> &nbsp; 4 fragments, nominal size 17 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag626')" href="javascript:;">
keras-2.3.1/keras/layers/core.py: 922-939
</a>
<div class="mid" id="frag626" style="display:none"><pre>
    def get_config(self):
        config = {
            'units': self.units,
            'activation': activations.serialize(self.activation),
            'use_bias': self.use_bias,
            'kernel_initializer': initializers.serialize(self.kernel_initializer),
            'bias_initializer': initializers.serialize(self.bias_initializer),
            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),
            'bias_regularizer': regularizers.serialize(self.bias_regularizer),
            'activity_regularizer':
                regularizers.serialize(self.activity_regularizer),
            'kernel_constraint': constraints.serialize(self.kernel_constraint),
            'bias_constraint': constraints.serialize(self.bias_constraint)
        }
        base_config = super(Dense, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag716')" href="javascript:;">
keras-2.3.1/keras/layers/local.py: 374-393
</a>
<div class="mid" id="frag716" style="display:none"><pre>
    def get_config(self):
        config = {
            'filters': self.filters,
            'kernel_size': self.kernel_size,
            'strides': self.strides,
            'padding': self.padding,
            'data_format': self.data_format,
            'activation': activations.serialize(self.activation),
            'use_bias': self.use_bias,
            'kernel_initializer': initializers.serialize(self.kernel_initializer),
            'bias_initializer': initializers.serialize(self.bias_initializer),
            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),
            'bias_regularizer': regularizers.serialize(self.bias_regularizer),
            'activity_regularizer':
                regularizers.serialize(self.activity_regularizer),
            'kernel_constraint': constraints.serialize(self.kernel_constraint),
            'bias_constraint': constraints.serialize(self.bias_constraint)
        }
        base_config = super(LocallyConnected2D, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag711')" href="javascript:;">
keras-2.3.1/keras/layers/local.py: 162-182
</a>
<div class="mid" id="frag711" style="display:none"><pre>
    def get_config(self):
        config = {
            'filters': self.filters,
            'kernel_size': self.kernel_size,
            'strides': self.strides,
            'padding': self.padding,
            'activation': activations.serialize(self.activation),
            'use_bias': self.use_bias,
            'kernel_initializer': initializers.serialize(self.kernel_initializer),
            'bias_initializer': initializers.serialize(self.bias_initializer),
            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),
            'bias_regularizer': regularizers.serialize(self.bias_regularizer),
            'activity_regularizer':
                regularizers.serialize(self.activity_regularizer),
            'kernel_constraint': constraints.serialize(self.kernel_constraint),
            'bias_constraint': constraints.serialize(self.bias_constraint)
        }
        base_config = super(LocallyConnected1D, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag988')" href="javascript:;">
keras-2.3.1/keras/layers/convolutional.py: 210-233
</a>
<div class="mid" id="frag988" style="display:none"><pre>
    def get_config(self):
        config = {
            'rank': self.rank,
            'filters': self.filters,
            'kernel_size': self.kernel_size,
            'strides': self.strides,
            'padding': self.padding,
            'data_format': self.data_format,
            'dilation_rate': self.dilation_rate,
            'activation': activations.serialize(self.activation),
            'use_bias': self.use_bias,
            'kernel_initializer': initializers.serialize(self.kernel_initializer),
            'bias_initializer': initializers.serialize(self.bias_initializer),
            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),
            'bias_regularizer': regularizers.serialize(self.bias_regularizer),
            'activity_regularizer':
                regularizers.serialize(self.activity_regularizer),
            'kernel_constraint': constraints.serialize(self.kernel_constraint),
            'bias_constraint': constraints.serialize(self.bias_constraint)
        }
        base_config = super(_Conv, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 16:</b> &nbsp; 3 fragments, nominal size 34 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag707')" href="javascript:;">
keras-2.3.1/keras/layers/local.py: 82-116
</a>
<div class="mid" id="frag707" style="display:none"><pre>
    def __init__(self, filters,
                 kernel_size,
                 strides=1,
                 padding='valid',
                 data_format=None,
                 activation=None,
                 use_bias=True,
                 kernel_initializer='glorot_uniform',
                 bias_initializer='zeros',
                 kernel_regularizer=None,
                 bias_regularizer=None,
                 activity_regularizer=None,
                 kernel_constraint=None,
                 bias_constraint=None,
                 **kwargs):
        super(LocallyConnected1D, self).__init__(**kwargs)
        self.filters = filters
        self.kernel_size = conv_utils.normalize_tuple(kernel_size, 1, 'kernel_size')
        self.strides = conv_utils.normalize_tuple(strides, 1, 'strides')
        self.padding = conv_utils.normalize_padding(padding)
        if self.padding != 'valid':
            raise ValueError('Invalid border mode for LocallyConnected1D '
                             '(only "valid" is supported): ' + padding)
        self.data_format = K.normalize_data_format(data_format)
        self.activation = activations.get(activation)
        self.use_bias = use_bias
        self.kernel_initializer = initializers.get(kernel_initializer)
        self.bias_initializer = initializers.get(bias_initializer)
        self.kernel_regularizer = regularizers.get(kernel_regularizer)
        self.bias_regularizer = regularizers.get(bias_regularizer)
        self.activity_regularizer = regularizers.get(activity_regularizer)
        self.kernel_constraint = constraints.get(kernel_constraint)
        self.bias_constraint = constraints.get(bias_constraint)
        self.input_spec = InputSpec(ndim=3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag984')" href="javascript:;">
keras-2.3.1/keras/layers/convolutional.py: 88-125
</a>
<div class="mid" id="frag984" style="display:none"><pre>
    def __init__(self, rank,
                 filters,
                 kernel_size,
                 strides=1,
                 padding='valid',
                 data_format=None,
                 dilation_rate=1,
                 activation=None,
                 use_bias=True,
                 kernel_initializer='glorot_uniform',
                 bias_initializer='zeros',
                 kernel_regularizer=None,
                 bias_regularizer=None,
                 activity_regularizer=None,
                 kernel_constraint=None,
                 bias_constraint=None,
                 **kwargs):
        super(_Conv, self).__init__(**kwargs)
        self.rank = rank
        self.filters = filters
        self.kernel_size = conv_utils.normalize_tuple(kernel_size, rank,
                                                      'kernel_size')
        self.strides = conv_utils.normalize_tuple(strides, rank, 'strides')
        self.padding = conv_utils.normalize_padding(padding)
        self.data_format = K.normalize_data_format(data_format)
        self.dilation_rate = conv_utils.normalize_tuple(dilation_rate, rank,
                                                        'dilation_rate')
        self.activation = activations.get(activation)
        self.use_bias = use_bias
        self.kernel_initializer = initializers.get(kernel_initializer)
        self.bias_initializer = initializers.get(bias_initializer)
        self.kernel_regularizer = regularizers.get(kernel_regularizer)
        self.bias_regularizer = regularizers.get(bias_regularizer)
        self.activity_regularizer = regularizers.get(activity_regularizer)
        self.kernel_constraint = constraints.get(kernel_constraint)
        self.bias_constraint = constraints.get(bias_constraint)
        self.input_spec = InputSpec(ndim=self.rank + 2)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag712')" href="javascript:;">
keras-2.3.1/keras/layers/local.py: 266-300
</a>
<div class="mid" id="frag712" style="display:none"><pre>
    def __init__(self, filters,
                 kernel_size,
                 strides=(1, 1),
                 padding='valid',
                 data_format=None,
                 activation=None,
                 use_bias=True,
                 kernel_initializer='glorot_uniform',
                 bias_initializer='zeros',
                 kernel_regularizer=None,
                 bias_regularizer=None,
                 activity_regularizer=None,
                 kernel_constraint=None,
                 bias_constraint=None,
                 **kwargs):
        super(LocallyConnected2D, self).__init__(**kwargs)
        self.filters = filters
        self.kernel_size = conv_utils.normalize_tuple(kernel_size, 2, 'kernel_size')
        self.strides = conv_utils.normalize_tuple(strides, 2, 'strides')
        self.padding = conv_utils.normalize_padding(padding)
        if self.padding != 'valid':
            raise ValueError('Invalid border mode for LocallyConnected2D '
                             '(only "valid" is supported): ' + padding)
        self.data_format = K.normalize_data_format(data_format)
        self.activation = activations.get(activation)
        self.use_bias = use_bias
        self.kernel_initializer = initializers.get(kernel_initializer)
        self.bias_initializer = initializers.get(bias_initializer)
        self.kernel_regularizer = regularizers.get(kernel_regularizer)
        self.bias_regularizer = regularizers.get(bias_regularizer)
        self.activity_regularizer = regularizers.get(activity_regularizer)
        self.kernel_constraint = constraints.get(kernel_constraint)
        self.bias_constraint = constraints.get(bias_constraint)
        self.input_spec = InputSpec(ndim=4)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 17:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 86%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag714')" href="javascript:;">
keras-2.3.1/keras/layers/local.py: 342-359
</a>
<div class="mid" id="frag714" style="display:none"><pre>
    def compute_output_shape(self, input_shape):
        if self.data_format == 'channels_first':
            rows = input_shape[2]
            cols = input_shape[3]
        elif self.data_format == 'channels_last':
            rows = input_shape[1]
            cols = input_shape[2]

        rows = conv_utils.conv_output_length(rows, self.kernel_size[0],
                                             self.padding, self.strides[0])
        cols = conv_utils.conv_output_length(cols, self.kernel_size[1],
                                             self.padding, self.strides[1])

        if self.data_format == 'channels_first':
            return (input_shape[0], self.filters, rows, cols)
        elif self.data_format == 'channels_last':
            return (input_shape[0], rows, cols, self.filters)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag905')" href="javascript:;">
keras-2.3.1/keras/layers/pooling.py: 180-195
</a>
<div class="mid" id="frag905" style="display:none"><pre>
    def compute_output_shape(self, input_shape):
        if self.data_format == 'channels_first':
            rows = input_shape[2]
            cols = input_shape[3]
        elif self.data_format == 'channels_last':
            rows = input_shape[1]
            cols = input_shape[2]
        rows = conv_utils.conv_output_length(rows, self.pool_size[0],
                                             self.padding, self.strides[0])
        cols = conv_utils.conv_output_length(cols, self.pool_size[1],
                                             self.padding, self.strides[1])
        if self.data_format == 'channels_first':
            return (input_shape[0], input_shape[1], rows, cols)
        elif self.data_format == 'channels_last':
            return (input_shape[0], rows, cols, input_shape[3])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 18:</b> &nbsp; 2 fragments, nominal size 37 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag721')" href="javascript:;">
keras-2.3.1/keras/layers/convolutional_recurrent.py: 283-335
</a>
<div class="mid" id="frag721" style="display:none"><pre>
    def __call__(self, inputs, initial_state=None, constants=None, **kwargs):
        inputs, initial_state, constants = _standardize_args(
            inputs, initial_state, constants, self._num_constants)

        if initial_state is None and constants is None:
            return super(ConvRNN2D, self).__call__(inputs, **kwargs)

        # If any of `initial_state` or `constants` are specified and are Keras
        # tensors, then add them to the inputs and temporarily modify the
        # input_spec to include them.

        additional_inputs = []
        additional_specs = []
        if initial_state is not None:
            kwargs['initial_state'] = initial_state
            additional_inputs += initial_state
            self.state_spec = []
            for state in initial_state:
                try:
                    shape = K.int_shape(state)
                # Fix for Theano
                except TypeError:
                    shape = tuple(None for _ in range(K.ndim(state)))
                self.state_spec.append(InputSpec(shape=shape))

            additional_specs += self.state_spec
        if constants is not None:
            kwargs['constants'] = constants
            additional_inputs += constants
            self.constants_spec = [InputSpec(shape=K.int_shape(constant))
                                   for constant in constants]
            self._num_constants = len(constants)
            additional_specs += self.constants_spec
        # at this point additional_inputs cannot be empty
        for tensor in additional_inputs:
            if K.is_keras_tensor(tensor) != K.is_keras_tensor(additional_inputs[0]):
                raise ValueError('The initial state or constants of an RNN'
                                 ' layer cannot be specified with a mix of'
                                 ' Keras tensors and non-Keras tensors')

        if K.is_keras_tensor(additional_inputs[0]):
            # Compute the full input spec, including state and constants
            full_input = [inputs] + additional_inputs
            full_input_spec = self.input_spec + additional_specs
            # Perform the call with temporarily replaced input_spec
            original_input_spec = self.input_spec
            self.input_spec = full_input_spec
            output = super(ConvRNN2D, self).__call__(full_input, **kwargs)
            self.input_spec = original_input_spec
            return output
        else:
            return super(ConvRNN2D, self).__call__(inputs, **kwargs)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag805')" href="javascript:;">
keras-2.3.1/keras/layers/recurrent.py: 536-589
</a>
<div class="mid" id="frag805" style="display:none"><pre>
    def __call__(self, inputs, initial_state=None, constants=None, **kwargs):
        inputs, initial_state, constants = _standardize_args(
            inputs, initial_state, constants, self._num_constants)

        if initial_state is None and constants is None:
            return super(RNN, self).__call__(inputs, **kwargs)

        # If any of `initial_state` or `constants` are specified and are Keras
        # tensors, then add them to the inputs and temporarily modify the
        # input_spec to include them.

        additional_inputs = []
        additional_specs = []

        if initial_state is not None:
            kwargs['initial_state'] = initial_state
            additional_inputs += initial_state
            self.state_spec = [InputSpec(shape=K.int_shape(state))
                               for state in initial_state]
            additional_specs += self.state_spec
        if constants is not None:
            kwargs['constants'] = constants
            additional_inputs += constants
            self.constants_spec = [InputSpec(shape=K.int_shape(constant))
                                   for constant in constants]
            self._num_constants = len(constants)
            additional_specs += self.constants_spec
        # at this point additional_inputs cannot be empty
        is_keras_tensor = K.is_keras_tensor(additional_inputs[0])
        for tensor in additional_inputs:
            if K.is_keras_tensor(tensor) != is_keras_tensor:
                raise ValueError('The initial state or constants of an RNN'
                                 ' layer cannot be specified with a mix of'
                                 ' Keras tensors and non-Keras tensors'
                                 ' (a "Keras tensor" is a tensor that was'
                                 ' returned by a Keras layer, or by `Input`)')

        if is_keras_tensor:
            # Compute the full input spec, including state and constants
            full_input = [inputs] + additional_inputs
            full_input_spec = self.input_spec + additional_specs
            # Perform the call with temporarily replaced input_spec
            original_input_spec = self.input_spec
            self.input_spec = full_input_spec
            if 'initial_state' in kwargs:
                kwargs.pop('initial_state')
            if 'constants' in kwargs:
                kwargs.pop('constants')
            output = super(RNN, self).__call__(full_input, **kwargs)
            self.input_spec = original_input_spec
            return output
        else:
            return super(RNN, self).__call__(inputs, **kwargs)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 19:</b> &nbsp; 8 fragments, nominal size 29 lines, similarity 74%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag733')" href="javascript:;">
keras-2.3.1/keras/layers/convolutional_recurrent.py: 762-794
</a>
<div class="mid" id="frag733" style="display:none"><pre>
    def get_config(self):
        config = {'filters': self.filters,
                  'kernel_size': self.kernel_size,
                  'strides': self.strides,
                  'padding': self.padding,
                  'data_format': self.data_format,
                  'dilation_rate': self.dilation_rate,
                  'activation': activations.serialize(self.activation),
                  'recurrent_activation':
                      activations.serialize(self.recurrent_activation),
                  'use_bias': self.use_bias,
                  'kernel_initializer':
                      initializers.serialize(self.kernel_initializer),
                  'recurrent_initializer':
                      initializers.serialize(self.recurrent_initializer),
                  'bias_initializer': initializers.serialize(self.bias_initializer),
                  'unit_forget_bias': self.unit_forget_bias,
                  'kernel_regularizer':
                      regularizers.serialize(self.kernel_regularizer),
                  'recurrent_regularizer':
                      regularizers.serialize(self.recurrent_regularizer),
                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),
                  'kernel_constraint':
                      constraints.serialize(self.kernel_constraint),
                  'recurrent_constraint':
                      constraints.serialize(self.recurrent_constraint),
                  'bias_constraint': constraints.serialize(self.bias_constraint),
                  'dropout': self.dropout,
                  'recurrent_dropout': self.recurrent_dropout}
        base_config = super(ConvLSTM2DCell, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag862')" href="javascript:;">
keras-2.3.1/keras/layers/recurrent.py: 1761-1790
</a>
<div class="mid" id="frag862" style="display:none"><pre>
    def get_config(self):
        config = {'units': self.units,
                  'activation': activations.serialize(self.activation),
                  'recurrent_activation':
                      activations.serialize(self.recurrent_activation),
                  'use_bias': self.use_bias,
                  'kernel_initializer':
                      initializers.serialize(self.kernel_initializer),
                  'recurrent_initializer':
                      initializers.serialize(self.recurrent_initializer),
                  'bias_initializer': initializers.serialize(self.bias_initializer),
                  'kernel_regularizer':
                      regularizers.serialize(self.kernel_regularizer),
                  'recurrent_regularizer':
                      regularizers.serialize(self.recurrent_regularizer),
                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),
                  'activity_regularizer':
                      regularizers.serialize(self.activity_regularizer),
                  'kernel_constraint': constraints.serialize(self.kernel_constraint),
                  'recurrent_constraint':
                      constraints.serialize(self.recurrent_constraint),
                  'bias_constraint': constraints.serialize(self.bias_constraint),
                  'dropout': self.dropout,
                  'recurrent_dropout': self.recurrent_dropout,
                  'implementation': self.implementation,
                  'reset_after': self.reset_after}
        base_config = super(GRU, self).get_config()
        del base_config['cell']
        return dict(list(base_config.items()) + list(config.items()))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag889')" href="javascript:;">
keras-2.3.1/keras/layers/recurrent.py: 2317-2346
</a>
<div class="mid" id="frag889" style="display:none"><pre>
    def get_config(self):
        config = {'units': self.units,
                  'activation': activations.serialize(self.activation),
                  'recurrent_activation':
                      activations.serialize(self.recurrent_activation),
                  'use_bias': self.use_bias,
                  'kernel_initializer':
                      initializers.serialize(self.kernel_initializer),
                  'recurrent_initializer':
                      initializers.serialize(self.recurrent_initializer),
                  'bias_initializer': initializers.serialize(self.bias_initializer),
                  'unit_forget_bias': self.unit_forget_bias,
                  'kernel_regularizer':
                      regularizers.serialize(self.kernel_regularizer),
                  'recurrent_regularizer':
                      regularizers.serialize(self.recurrent_regularizer),
                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),
                  'activity_regularizer':
                      regularizers.serialize(self.activity_regularizer),
                  'kernel_constraint': constraints.serialize(self.kernel_constraint),
                  'recurrent_constraint':
                      constraints.serialize(self.recurrent_constraint),
                  'bias_constraint': constraints.serialize(self.bias_constraint),
                  'dropout': self.dropout,
                  'recurrent_dropout': self.recurrent_dropout,
                  'implementation': self.implementation}
        base_config = super(LSTM, self).get_config()
        del base_config['cell']
        return dict(list(base_config.items()) + list(config.items()))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag842')" href="javascript:;">
keras-2.3.1/keras/layers/recurrent.py: 1496-1523
</a>
<div class="mid" id="frag842" style="display:none"><pre>
    def get_config(self):
        config = {'units': self.units,
                  'activation': activations.serialize(self.activation),
                  'recurrent_activation':
                      activations.serialize(self.recurrent_activation),
                  'use_bias': self.use_bias,
                  'kernel_initializer':
                      initializers.serialize(self.kernel_initializer),
                  'recurrent_initializer':
                      initializers.serialize(self.recurrent_initializer),
                  'bias_initializer': initializers.serialize(self.bias_initializer),
                  'kernel_regularizer':
                      regularizers.serialize(self.kernel_regularizer),
                  'recurrent_regularizer':
                      regularizers.serialize(self.recurrent_regularizer),
                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),
                  'kernel_constraint': constraints.serialize(self.kernel_constraint),
                  'recurrent_constraint':
                      constraints.serialize(self.recurrent_constraint),
                  'bias_constraint': constraints.serialize(self.bias_constraint),
                  'dropout': self.dropout,
                  'recurrent_dropout': self.recurrent_dropout,
                  'implementation': self.implementation,
                  'reset_after': self.reset_after}
        base_config = super(GRUCell, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag836')" href="javascript:;">
keras-2.3.1/keras/layers/recurrent.py: 1163-1188
</a>
<div class="mid" id="frag836" style="display:none"><pre>
    def get_config(self):
        config = {'units': self.units,
                  'activation': activations.serialize(self.activation),
                  'use_bias': self.use_bias,
                  'kernel_initializer':
                      initializers.serialize(self.kernel_initializer),
                  'recurrent_initializer':
                      initializers.serialize(self.recurrent_initializer),
                  'bias_initializer': initializers.serialize(self.bias_initializer),
                  'kernel_regularizer':
                      regularizers.serialize(self.kernel_regularizer),
                  'recurrent_regularizer':
                      regularizers.serialize(self.recurrent_regularizer),
                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),
                  'activity_regularizer':
                      regularizers.serialize(self.activity_regularizer),
                  'kernel_constraint': constraints.serialize(self.kernel_constraint),
                  'recurrent_constraint':
                      constraints.serialize(self.recurrent_constraint),
                  'bias_constraint': constraints.serialize(self.bias_constraint),
                  'dropout': self.dropout,
                  'recurrent_dropout': self.recurrent_dropout}
        base_config = super(SimpleRNN, self).get_config()
        del base_config['cell']
        return dict(list(base_config.items()) + list(config.items()))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag757')" href="javascript:;">
keras-2.3.1/keras/layers/convolutional_recurrent.py: 1059-1093
</a>
<div class="mid" id="frag757" style="display:none"><pre>
    def get_config(self):
        config = {'filters': self.filters,
                  'kernel_size': self.kernel_size,
                  'strides': self.strides,
                  'padding': self.padding,
                  'data_format': self.data_format,
                  'dilation_rate': self.dilation_rate,
                  'activation': activations.serialize(self.activation),
                  'recurrent_activation':
                      activations.serialize(self.recurrent_activation),
                  'use_bias': self.use_bias,
                  'kernel_initializer':
                      initializers.serialize(self.kernel_initializer),
                  'recurrent_initializer':
                      initializers.serialize(self.recurrent_initializer),
                  'bias_initializer': initializers.serialize(self.bias_initializer),
                  'unit_forget_bias': self.unit_forget_bias,
                  'kernel_regularizer':
                      regularizers.serialize(self.kernel_regularizer),
                  'recurrent_regularizer':
                      regularizers.serialize(self.recurrent_regularizer),
                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),
                  'activity_regularizer':
                      regularizers.serialize(self.activity_regularizer),
                  'kernel_constraint':
                      constraints.serialize(self.kernel_constraint),
                  'recurrent_constraint':
                      constraints.serialize(self.recurrent_constraint),
                  'bias_constraint': constraints.serialize(self.bias_constraint),
                  'dropout': self.dropout,
                  'recurrent_dropout': self.recurrent_dropout}
        base_config = super(ConvLSTM2D, self).get_config()
        del base_config['cell']
        return dict(list(base_config.items()) + list(config.items()))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag869')" href="javascript:;">
keras-2.3.1/keras/layers/recurrent.py: 2058-2085
</a>
<div class="mid" id="frag869" style="display:none"><pre>
    def get_config(self):
        config = {'units': self.units,
                  'activation': activations.serialize(self.activation),
                  'recurrent_activation':
                      activations.serialize(self.recurrent_activation),
                  'use_bias': self.use_bias,
                  'kernel_initializer':
                      initializers.serialize(self.kernel_initializer),
                  'recurrent_initializer':
                      initializers.serialize(self.recurrent_initializer),
                  'bias_initializer': initializers.serialize(self.bias_initializer),
                  'unit_forget_bias': self.unit_forget_bias,
                  'kernel_regularizer':
                      regularizers.serialize(self.kernel_regularizer),
                  'recurrent_regularizer':
                      regularizers.serialize(self.recurrent_regularizer),
                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),
                  'kernel_constraint': constraints.serialize(self.kernel_constraint),
                  'recurrent_constraint':
                      constraints.serialize(self.recurrent_constraint),
                  'bias_constraint': constraints.serialize(self.bias_constraint),
                  'dropout': self.dropout,
                  'recurrent_dropout': self.recurrent_dropout,
                  'implementation': self.implementation}
        base_config = super(LSTMCell, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag819')" href="javascript:;">
keras-2.3.1/keras/layers/recurrent.py: 953-976
</a>
<div class="mid" id="frag819" style="display:none"><pre>
    def get_config(self):
        config = {'units': self.units,
                  'activation': activations.serialize(self.activation),
                  'use_bias': self.use_bias,
                  'kernel_initializer':
                      initializers.serialize(self.kernel_initializer),
                  'recurrent_initializer':
                      initializers.serialize(self.recurrent_initializer),
                  'bias_initializer': initializers.serialize(self.bias_initializer),
                  'kernel_regularizer':
                      regularizers.serialize(self.kernel_regularizer),
                  'recurrent_regularizer':
                      regularizers.serialize(self.recurrent_regularizer),
                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),
                  'kernel_constraint': constraints.serialize(self.kernel_constraint),
                  'recurrent_constraint':
                      constraints.serialize(self.recurrent_constraint),
                  'bias_constraint': constraints.serialize(self.bias_constraint),
                  'dropout': self.dropout,
                  'recurrent_dropout': self.recurrent_dropout}
        base_config = super(SimpleRNNCell, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 20:</b> &nbsp; 13 fragments, nominal size 44 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag734')" href="javascript:;">
keras-2.3.1/keras/layers/convolutional_recurrent.py: 915-968
</a>
<div class="mid" id="frag734" style="display:none"><pre>
    def __init__(self, filters,
                 kernel_size,
                 strides=(1, 1),
                 padding='valid',
                 data_format=None,
                 dilation_rate=(1, 1),
                 activation='tanh',
                 recurrent_activation='hard_sigmoid',
                 use_bias=True,
                 kernel_initializer='glorot_uniform',
                 recurrent_initializer='orthogonal',
                 bias_initializer='zeros',
                 unit_forget_bias=True,
                 kernel_regularizer=None,
                 recurrent_regularizer=None,
                 bias_regularizer=None,
                 activity_regularizer=None,
                 kernel_constraint=None,
                 recurrent_constraint=None,
                 bias_constraint=None,
                 return_sequences=False,
                 go_backwards=False,
                 stateful=False,
                 dropout=0.,
                 recurrent_dropout=0.,
                 **kwargs):
        cell = ConvLSTM2DCell(filters=filters,
                              kernel_size=kernel_size,
                              strides=strides,
                              padding=padding,
                              data_format=data_format,
                              dilation_rate=dilation_rate,
                              activation=activation,
                              recurrent_activation=recurrent_activation,
                              use_bias=use_bias,
                              kernel_initializer=kernel_initializer,
                              recurrent_initializer=recurrent_initializer,
                              bias_initializer=bias_initializer,
                              unit_forget_bias=unit_forget_bias,
                              kernel_regularizer=kernel_regularizer,
                              recurrent_regularizer=recurrent_regularizer,
                              bias_regularizer=bias_regularizer,
                              kernel_constraint=kernel_constraint,
                              recurrent_constraint=recurrent_constraint,
                              bias_constraint=bias_constraint,
                              dropout=dropout,
                              recurrent_dropout=recurrent_dropout)
        super(ConvLSTM2D, self).__init__(cell,
                                         return_sequences=return_sequences,
                                         go_backwards=go_backwards,
                                         stateful=stateful,
                                         **kwargs)
        self.activity_regularizer = regularizers.get(activity_regularizer)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag820')" href="javascript:;">
keras-2.3.1/keras/layers/recurrent.py: 1041-1098
</a>
<div class="mid" id="frag820" style="display:none"><pre>
    def __init__(self, units,
                 activation='tanh',
                 use_bias=True,
                 kernel_initializer='glorot_uniform',
                 recurrent_initializer='orthogonal',
                 bias_initializer='zeros',
                 kernel_regularizer=None,
                 recurrent_regularizer=None,
                 bias_regularizer=None,
                 activity_regularizer=None,
                 kernel_constraint=None,
                 recurrent_constraint=None,
                 bias_constraint=None,
                 dropout=0.,
                 recurrent_dropout=0.,
                 return_sequences=False,
                 return_state=False,
                 go_backwards=False,
                 stateful=False,
                 unroll=False,
                 **kwargs):
        if 'implementation' in kwargs:
            kwargs.pop('implementation')
            warnings.warn('The `implementation` argument '
                          'in `SimpleRNN` has been deprecated. '
                          'Please remove it from your layer call.')
        if K.backend() == 'theano' and (dropout or recurrent_dropout):
            warnings.warn(
                'RNN dropout is no longer supported with the Theano backend '
                'due to technical limitations. '
                'You can either set `dropout` and `recurrent_dropout` to 0, '
                'or use the TensorFlow backend.')
            dropout = 0.
            recurrent_dropout = 0.

        cell = SimpleRNNCell(units,
                             activation=activation,
                             use_bias=use_bias,
                             kernel_initializer=kernel_initializer,
                             recurrent_initializer=recurrent_initializer,
                             bias_initializer=bias_initializer,
                             kernel_regularizer=kernel_regularizer,
                             recurrent_regularizer=recurrent_regularizer,
                             bias_regularizer=bias_regularizer,
                             kernel_constraint=kernel_constraint,
                             recurrent_constraint=recurrent_constraint,
                             bias_constraint=bias_constraint,
                             dropout=dropout,
                             recurrent_dropout=recurrent_dropout)
        super(SimpleRNN, self).__init__(cell,
                                        return_sequences=return_sequences,
                                        return_state=return_state,
                                        go_backwards=go_backwards,
                                        stateful=stateful,
                                        unroll=unroll,
                                        **kwargs)
        self.activity_regularizer = regularizers.get(activity_regularizer)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag843')" href="javascript:;">
keras-2.3.1/keras/layers/recurrent.py: 1622-1684
</a>
<div class="mid" id="frag843" style="display:none"><pre>
    def __init__(self, units,
                 activation='tanh',
                 recurrent_activation='sigmoid',
                 use_bias=True,
                 kernel_initializer='glorot_uniform',
                 recurrent_initializer='orthogonal',
                 bias_initializer='zeros',
                 kernel_regularizer=None,
                 recurrent_regularizer=None,
                 bias_regularizer=None,
                 activity_regularizer=None,
                 kernel_constraint=None,
                 recurrent_constraint=None,
                 bias_constraint=None,
                 dropout=0.,
                 recurrent_dropout=0.,
                 implementation=2,
                 return_sequences=False,
                 return_state=False,
                 go_backwards=False,
                 stateful=False,
                 unroll=False,
                 reset_after=False,
                 **kwargs):
        if implementation == 0:
            warnings.warn('`implementation=0` has been deprecated, '
                          'and now defaults to `implementation=1`.'
                          'Please update your layer call.')
        if K.backend() == 'theano' and (dropout or recurrent_dropout):
            warnings.warn(
                'RNN dropout is no longer supported with the Theano backend '
                'due to technical limitations. '
                'You can either set `dropout` and `recurrent_dropout` to 0, '
                'or use the TensorFlow backend.')
            dropout = 0.
            recurrent_dropout = 0.

        cell = GRUCell(units,
                       activation=activation,
                       recurrent_activation=recurrent_activation,
                       use_bias=use_bias,
                       kernel_initializer=kernel_initializer,
                       recurrent_initializer=recurrent_initializer,
                       bias_initializer=bias_initializer,
                       kernel_regularizer=kernel_regularizer,
                       recurrent_regularizer=recurrent_regularizer,
                       bias_regularizer=bias_regularizer,
                       kernel_constraint=kernel_constraint,
                       recurrent_constraint=recurrent_constraint,
                       bias_constraint=bias_constraint,
                       dropout=dropout,
                       recurrent_dropout=recurrent_dropout,
                       implementation=implementation,
                       reset_after=reset_after)
        super(GRU, self).__init__(cell,
                                  return_sequences=return_sequences,
                                  return_state=return_state,
                                  go_backwards=go_backwards,
                                  stateful=stateful,
                                  unroll=unroll,
                                  **kwargs)
        self.activity_regularizer = regularizers.get(activity_regularizer)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1010')" href="javascript:;">
keras-2.3.1/keras/layers/convolutional.py: 1643-1686
</a>
<div class="mid" id="frag1010" style="display:none"><pre>
    def __init__(self, filters,
                 kernel_size,
                 strides=(1, 1),
                 padding='valid',
                 data_format=None,
                 dilation_rate=(1, 1),
                 depth_multiplier=1,
                 activation=None,
                 use_bias=True,
                 depthwise_initializer='glorot_uniform',
                 pointwise_initializer='glorot_uniform',
                 bias_initializer='zeros',
                 depthwise_regularizer=None,
                 pointwise_regularizer=None,
                 bias_regularizer=None,
                 activity_regularizer=None,
                 depthwise_constraint=None,
                 pointwise_constraint=None,
                 bias_constraint=None,
                 **kwargs):
        super(SeparableConv2D, self).__init__(
            rank=2,
            filters=filters,
            kernel_size=kernel_size,
            strides=strides,
            padding=padding,
            data_format=data_format,
            dilation_rate=dilation_rate,
            depth_multiplier=depth_multiplier,
            activation=activation,
            use_bias=use_bias,
            depthwise_initializer=depthwise_initializer,
            pointwise_initializer=pointwise_initializer,
            bias_initializer=bias_initializer,
            depthwise_regularizer=depthwise_regularizer,
            pointwise_regularizer=pointwise_regularizer,
            bias_regularizer=bias_regularizer,
            activity_regularizer=activity_regularizer,
            depthwise_constraint=depthwise_constraint,
            pointwise_constraint=pointwise_constraint,
            bias_constraint=bias_constraint,
            **kwargs)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag870')" href="javascript:;">
keras-2.3.1/keras/layers/recurrent.py: 2178-2240
</a>
<div class="mid" id="frag870" style="display:none"><pre>
    def __init__(self, units,
                 activation='tanh',
                 recurrent_activation='sigmoid',
                 use_bias=True,
                 kernel_initializer='glorot_uniform',
                 recurrent_initializer='orthogonal',
                 bias_initializer='zeros',
                 unit_forget_bias=True,
                 kernel_regularizer=None,
                 recurrent_regularizer=None,
                 bias_regularizer=None,
                 activity_regularizer=None,
                 kernel_constraint=None,
                 recurrent_constraint=None,
                 bias_constraint=None,
                 dropout=0.,
                 recurrent_dropout=0.,
                 implementation=2,
                 return_sequences=False,
                 return_state=False,
                 go_backwards=False,
                 stateful=False,
                 unroll=False,
                 **kwargs):
        if implementation == 0:
            warnings.warn('`implementation=0` has been deprecated, '
                          'and now defaults to `implementation=1`.'
                          'Please update your layer call.')
        if K.backend() == 'theano' and (dropout or recurrent_dropout):
            warnings.warn(
                'RNN dropout is no longer supported with the Theano backend '
                'due to technical limitations. '
                'You can either set `dropout` and `recurrent_dropout` to 0, '
                'or use the TensorFlow backend.')
            dropout = 0.
            recurrent_dropout = 0.

        cell = LSTMCell(units,
                        activation=activation,
                        recurrent_activation=recurrent_activation,
                        use_bias=use_bias,
                        kernel_initializer=kernel_initializer,
                        recurrent_initializer=recurrent_initializer,
                        unit_forget_bias=unit_forget_bias,
                        bias_initializer=bias_initializer,
                        kernel_regularizer=kernel_regularizer,
                        recurrent_regularizer=recurrent_regularizer,
                        bias_regularizer=bias_regularizer,
                        kernel_constraint=kernel_constraint,
                        recurrent_constraint=recurrent_constraint,
                        bias_constraint=bias_constraint,
                        dropout=dropout,
                        recurrent_dropout=recurrent_dropout,
                        implementation=implementation)
        super(LSTM, self).__init__(cell,
                                   return_sequences=return_sequences,
                                   return_state=return_state,
                                   go_backwards=go_backwards,
                                   stateful=stateful,
                                   unroll=unroll,
                                   **kwargs)
        self.activity_regularizer = regularizers.get(activity_regularizer)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag989')" href="javascript:;">
keras-2.3.1/keras/layers/convolutional.py: 315-354
</a>
<div class="mid" id="frag989" style="display:none"><pre>
    def __init__(self, filters,
                 kernel_size,
                 strides=1,
                 padding='valid',
                 data_format='channels_last',
                 dilation_rate=1,
                 activation=None,
                 use_bias=True,
                 kernel_initializer='glorot_uniform',
                 bias_initializer='zeros',
                 kernel_regularizer=None,
                 bias_regularizer=None,
                 activity_regularizer=None,
                 kernel_constraint=None,
                 bias_constraint=None,
                 **kwargs):
        if padding == 'causal':
            if data_format != 'channels_last':
                raise ValueError('When using causal padding in `Conv1D`, '
                                 '`data_format` must be "channels_last" '
                                 '(temporal data).')
        super(Conv1D, self).__init__(
            rank=1,
            filters=filters,
            kernel_size=kernel_size,
            strides=strides,
            padding=padding,
            data_format=data_format,
            dilation_rate=dilation_rate,
            activation=activation,
            use_bias=use_bias,
            kernel_initializer=kernel_initializer,
            bias_initializer=bias_initializer,
            kernel_regularizer=kernel_regularizer,
            bias_regularizer=bias_regularizer,
            activity_regularizer=activity_regularizer,
            kernel_constraint=kernel_constraint,
            bias_constraint=bias_constraint,
            **kwargs)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1009')" href="javascript:;">
keras-2.3.1/keras/layers/convolutional.py: 1502-1545
</a>
<div class="mid" id="frag1009" style="display:none"><pre>
    def __init__(self, filters,
                 kernel_size,
                 strides=1,
                 padding='valid',
                 data_format='channels_last',
                 dilation_rate=1,
                 depth_multiplier=1,
                 activation=None,
                 use_bias=True,
                 depthwise_initializer='glorot_uniform',
                 pointwise_initializer='glorot_uniform',
                 bias_initializer='zeros',
                 depthwise_regularizer=None,
                 pointwise_regularizer=None,
                 bias_regularizer=None,
                 activity_regularizer=None,
                 depthwise_constraint=None,
                 pointwise_constraint=None,
                 bias_constraint=None,
                 **kwargs):
        super(SeparableConv1D, self).__init__(
            rank=1,
            filters=filters,
            kernel_size=kernel_size,
            strides=strides,
            padding=padding,
            data_format=data_format,
            dilation_rate=dilation_rate,
            depth_multiplier=depth_multiplier,
            activation=activation,
            use_bias=use_bias,
            depthwise_initializer=depthwise_initializer,
            pointwise_initializer=pointwise_initializer,
            bias_initializer=bias_initializer,
            depthwise_regularizer=depthwise_regularizer,
            pointwise_regularizer=pointwise_regularizer,
            bias_regularizer=bias_regularizer,
            activity_regularizer=activity_regularizer,
            depthwise_constraint=depthwise_constraint,
            pointwise_constraint=pointwise_constraint,
            bias_constraint=bias_constraint,
            **kwargs)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag995')" href="javascript:;">
keras-2.3.1/keras/layers/convolutional.py: 731-775
</a>
<div class="mid" id="frag995" style="display:none"><pre>
    def __init__(self, filters,
                 kernel_size,
                 strides=(1, 1),
                 padding='valid',
                 output_padding=None,
                 data_format=None,
                 dilation_rate=(1, 1),
                 activation=None,
                 use_bias=True,
                 kernel_initializer='glorot_uniform',
                 bias_initializer='zeros',
                 kernel_regularizer=None,
                 bias_regularizer=None,
                 activity_regularizer=None,
                 kernel_constraint=None,
                 bias_constraint=None,
                 **kwargs):
        super(Conv2DTranspose, self).__init__(
            filters,
            kernel_size,
            strides=strides,
            padding=padding,
            data_format=data_format,
            dilation_rate=dilation_rate,
            activation=activation,
            use_bias=use_bias,
            kernel_initializer=kernel_initializer,
            bias_initializer=bias_initializer,
            kernel_regularizer=kernel_regularizer,
            bias_regularizer=bias_regularizer,
            activity_regularizer=activity_regularizer,
            kernel_constraint=kernel_constraint,
            bias_constraint=bias_constraint,
            **kwargs)

        self.output_padding = output_padding
        if self.output_padding is not None:
            self.output_padding = conv_utils.normalize_tuple(
                self.output_padding, 2, 'output_padding')
            for stride, out_pad in zip(self.strides, self.output_padding):
                if out_pad &gt;= stride:
                    raise ValueError('Stride ' + str(self.strides) + ' must be '
                                     'greater than output padding ' +
                                     str(self.output_padding))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag993')" href="javascript:;">
keras-2.3.1/keras/layers/convolutional.py: 580-614
</a>
<div class="mid" id="frag993" style="display:none"><pre>
    def __init__(self, filters,
                 kernel_size,
                 strides=(1, 1, 1),
                 padding='valid',
                 data_format=None,
                 dilation_rate=(1, 1, 1),
                 activation=None,
                 use_bias=True,
                 kernel_initializer='glorot_uniform',
                 bias_initializer='zeros',
                 kernel_regularizer=None,
                 bias_regularizer=None,
                 activity_regularizer=None,
                 kernel_constraint=None,
                 bias_constraint=None,
                 **kwargs):
        super(Conv3D, self).__init__(
            rank=3,
            filters=filters,
            kernel_size=kernel_size,
            strides=strides,
            padding=padding,
            data_format=data_format,
            dilation_rate=dilation_rate,
            activation=activation,
            use_bias=use_bias,
            kernel_initializer=kernel_initializer,
            bias_initializer=bias_initializer,
            kernel_regularizer=kernel_regularizer,
            bias_regularizer=bias_regularizer,
            activity_regularizer=activity_regularizer,
            kernel_constraint=kernel_constraint,
            bias_constraint=bias_constraint,
            **kwargs)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1005')" href="javascript:;">
keras-2.3.1/keras/layers/convolutional.py: 1277-1320
</a>
<div class="mid" id="frag1005" style="display:none"><pre>
    def __init__(self, rank,
                 filters,
                 kernel_size,
                 strides=1,
                 padding='valid',
                 data_format=None,
                 dilation_rate=1,
                 depth_multiplier=1,
                 activation=None,
                 use_bias=True,
                 depthwise_initializer='glorot_uniform',
                 pointwise_initializer='glorot_uniform',
                 bias_initializer='zeros',
                 depthwise_regularizer=None,
                 pointwise_regularizer=None,
                 bias_regularizer=None,
                 activity_regularizer=None,
                 depthwise_constraint=None,
                 pointwise_constraint=None,
                 bias_constraint=None,
                 **kwargs):
        super(_SeparableConv, self).__init__(
            rank=rank,
            filters=filters,
            kernel_size=kernel_size,
            strides=strides,
            padding=padding,
            data_format=data_format,
            dilation_rate=dilation_rate,
            activation=activation,
            use_bias=use_bias,
            bias_initializer=bias_initializer,
            bias_regularizer=bias_regularizer,
            activity_regularizer=activity_regularizer,
            bias_constraint=bias_constraint,
            **kwargs)
        self.depth_multiplier = depth_multiplier
        self.depthwise_initializer = initializers.get(depthwise_initializer)
        self.pointwise_initializer = initializers.get(pointwise_initializer)
        self.depthwise_regularizer = regularizers.get(depthwise_regularizer)
        self.pointwise_regularizer = regularizers.get(pointwise_regularizer)
        self.depthwise_constraint = constraints.get(depthwise_constraint)
        self.pointwise_constraint = constraints.get(pointwise_constraint)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag991')" href="javascript:;">
keras-2.3.1/keras/layers/convolutional.py: 451-485
</a>
<div class="mid" id="frag991" style="display:none"><pre>
    def __init__(self, filters,
                 kernel_size,
                 strides=(1, 1),
                 padding='valid',
                 data_format=None,
                 dilation_rate=(1, 1),
                 activation=None,
                 use_bias=True,
                 kernel_initializer='glorot_uniform',
                 bias_initializer='zeros',
                 kernel_regularizer=None,
                 bias_regularizer=None,
                 activity_regularizer=None,
                 kernel_constraint=None,
                 bias_constraint=None,
                 **kwargs):
        super(Conv2D, self).__init__(
            rank=2,
            filters=filters,
            kernel_size=kernel_size,
            strides=strides,
            padding=padding,
            data_format=data_format,
            dilation_rate=dilation_rate,
            activation=activation,
            use_bias=use_bias,
            kernel_initializer=kernel_initializer,
            bias_initializer=bias_initializer,
            kernel_regularizer=kernel_regularizer,
            bias_regularizer=bias_regularizer,
            activity_regularizer=activity_regularizer,
            kernel_constraint=kernel_constraint,
            bias_constraint=bias_constraint,
            **kwargs)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1011')" href="javascript:;">
keras-2.3.1/keras/layers/convolutional.py: 1770-1805
</a>
<div class="mid" id="frag1011" style="display:none"><pre>
    def __init__(self,
                 kernel_size,
                 strides=(1, 1),
                 padding='valid',
                 depth_multiplier=1,
                 data_format=None,
                 dilation_rate=(1, 1),
                 activation=None,
                 use_bias=True,
                 depthwise_initializer='glorot_uniform',
                 bias_initializer='zeros',
                 depthwise_regularizer=None,
                 bias_regularizer=None,
                 activity_regularizer=None,
                 depthwise_constraint=None,
                 bias_constraint=None,
                 **kwargs):
        super(DepthwiseConv2D, self).__init__(
            filters=None,
            kernel_size=kernel_size,
            strides=strides,
            padding=padding,
            data_format=data_format,
            dilation_rate=dilation_rate,
            activation=activation,
            use_bias=use_bias,
            bias_regularizer=bias_regularizer,
            activity_regularizer=activity_regularizer,
            bias_constraint=bias_constraint,
            **kwargs)
        self.depth_multiplier = depth_multiplier
        self.depthwise_initializer = initializers.get(depthwise_initializer)
        self.depthwise_regularizer = regularizers.get(depthwise_regularizer)
        self.depthwise_constraint = constraints.get(depthwise_constraint)
        self.bias_initializer = initializers.get(bias_initializer)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1000')" href="javascript:;">
keras-2.3.1/keras/layers/convolutional.py: 1005-1047
</a>
<div class="mid" id="frag1000" style="display:none"><pre>
    def __init__(self, filters,
                 kernel_size,
                 strides=(1, 1, 1),
                 padding='valid',
                 output_padding=None,
                 data_format=None,
                 activation=None,
                 use_bias=True,
                 kernel_initializer='glorot_uniform',
                 bias_initializer='zeros',
                 kernel_regularizer=None,
                 bias_regularizer=None,
                 activity_regularizer=None,
                 kernel_constraint=None,
                 bias_constraint=None,
                 **kwargs):
        super(Conv3DTranspose, self).__init__(
            filters,
            kernel_size,
            strides=strides,
            padding=padding,
            data_format=data_format,
            activation=activation,
            use_bias=use_bias,
            kernel_initializer=kernel_initializer,
            bias_initializer=bias_initializer,
            kernel_regularizer=kernel_regularizer,
            bias_regularizer=bias_regularizer,
            activity_regularizer=activity_regularizer,
            kernel_constraint=kernel_constraint,
            bias_constraint=bias_constraint,
            **kwargs)

        self.output_padding = output_padding
        if self.output_padding is not None:
            self.output_padding = conv_utils.normalize_tuple(
                self.output_padding, 3, 'output_padding')
            for stride, out_pad in zip(self.strides, self.output_padding):
                if out_pad &gt;= stride:
                    raise ValueError('Stride ' + str(self.strides) + ' must be '
                                     'greater than output padding ' +
                                     str(self.output_padding))

</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 21:</b> &nbsp; 3 fragments, nominal size 37 lines, similarity 82%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag816')" href="javascript:;">
keras-2.3.1/keras/layers/recurrent.py: 856-894
</a>
<div class="mid" id="frag816" style="display:none"><pre>
    def __init__(self, units,
                 activation='tanh',
                 use_bias=True,
                 kernel_initializer='glorot_uniform',
                 recurrent_initializer='orthogonal',
                 bias_initializer='zeros',
                 kernel_regularizer=None,
                 recurrent_regularizer=None,
                 bias_regularizer=None,
                 kernel_constraint=None,
                 recurrent_constraint=None,
                 bias_constraint=None,
                 dropout=0.,
                 recurrent_dropout=0.,
                 **kwargs):
        super(SimpleRNNCell, self).__init__(**kwargs)
        self.units = units
        self.activation = activations.get(activation)
        self.use_bias = use_bias

        self.kernel_initializer = initializers.get(kernel_initializer)
        self.recurrent_initializer = initializers.get(recurrent_initializer)
        self.bias_initializer = initializers.get(bias_initializer)

        self.kernel_regularizer = regularizers.get(kernel_regularizer)
        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)
        self.bias_regularizer = regularizers.get(bias_regularizer)

        self.kernel_constraint = constraints.get(kernel_constraint)
        self.recurrent_constraint = constraints.get(recurrent_constraint)
        self.bias_constraint = constraints.get(bias_constraint)

        self.dropout = min(1., max(0., dropout))
        self.recurrent_dropout = min(1., max(0., recurrent_dropout))
        self.state_size = self.units
        self.output_size = self.units
        self._dropout_mask = None
        self._recurrent_dropout_mask = None

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag864')" href="javascript:;">
keras-2.3.1/keras/layers/recurrent.py: 1859-1903
</a>
<div class="mid" id="frag864" style="display:none"><pre>
    def __init__(self, units,
                 activation='tanh',
                 recurrent_activation='sigmoid',
                 use_bias=True,
                 kernel_initializer='glorot_uniform',
                 recurrent_initializer='orthogonal',
                 bias_initializer='zeros',
                 unit_forget_bias=True,
                 kernel_regularizer=None,
                 recurrent_regularizer=None,
                 bias_regularizer=None,
                 kernel_constraint=None,
                 recurrent_constraint=None,
                 bias_constraint=None,
                 dropout=0.,
                 recurrent_dropout=0.,
                 implementation=2,
                 **kwargs):
        super(LSTMCell, self).__init__(**kwargs)
        self.units = units
        self.activation = activations.get(activation)
        self.recurrent_activation = activations.get(recurrent_activation)
        self.use_bias = use_bias

        self.kernel_initializer = initializers.get(kernel_initializer)
        self.recurrent_initializer = initializers.get(recurrent_initializer)
        self.bias_initializer = initializers.get(bias_initializer)
        self.unit_forget_bias = unit_forget_bias

        self.kernel_regularizer = regularizers.get(kernel_regularizer)
        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)
        self.bias_regularizer = regularizers.get(bias_regularizer)

        self.kernel_constraint = constraints.get(kernel_constraint)
        self.recurrent_constraint = constraints.get(recurrent_constraint)
        self.bias_constraint = constraints.get(bias_constraint)

        self.dropout = min(1., max(0., dropout))
        self.recurrent_dropout = min(1., max(0., recurrent_dropout))
        self.implementation = implementation
        self.state_size = (self.units, self.units)
        self.output_size = self.units
        self._dropout_mask = None
        self._recurrent_dropout_mask = None

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag838')" href="javascript:;">
keras-2.3.1/keras/layers/recurrent.py: 1255-1299
</a>
<div class="mid" id="frag838" style="display:none"><pre>
    def __init__(self, units,
                 activation='tanh',
                 recurrent_activation='sigmoid',
                 use_bias=True,
                 kernel_initializer='glorot_uniform',
                 recurrent_initializer='orthogonal',
                 bias_initializer='zeros',
                 kernel_regularizer=None,
                 recurrent_regularizer=None,
                 bias_regularizer=None,
                 kernel_constraint=None,
                 recurrent_constraint=None,
                 bias_constraint=None,
                 dropout=0.,
                 recurrent_dropout=0.,
                 implementation=2,
                 reset_after=False,
                 **kwargs):
        super(GRUCell, self).__init__(**kwargs)
        self.units = units
        self.activation = activations.get(activation)
        self.recurrent_activation = activations.get(recurrent_activation)
        self.use_bias = use_bias

        self.kernel_initializer = initializers.get(kernel_initializer)
        self.recurrent_initializer = initializers.get(recurrent_initializer)
        self.bias_initializer = initializers.get(bias_initializer)

        self.kernel_regularizer = regularizers.get(kernel_regularizer)
        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)
        self.bias_regularizer = regularizers.get(bias_regularizer)

        self.kernel_constraint = constraints.get(kernel_constraint)
        self.recurrent_constraint = constraints.get(recurrent_constraint)
        self.bias_constraint = constraints.get(bias_constraint)

        self.dropout = min(1., max(0., dropout))
        self.recurrent_dropout = min(1., max(0., recurrent_dropout))
        self.implementation = implementation
        self.reset_after = reset_after
        self.state_size = self.units
        self.output_size = self.units
        self._dropout_mask = None
        self._recurrent_dropout_mask = None

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 22:</b> &nbsp; 2 fragments, nominal size 57 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag865')" href="javascript:;">
keras-2.3.1/keras/layers/recurrent.py: 1904-1969
</a>
<div class="mid" id="frag865" style="display:none"><pre>
    def build(self, input_shape):
        input_dim = input_shape[-1]

        if type(self.recurrent_initializer).__name__ == 'Identity':
            def recurrent_identity(shape, gain=1., dtype=None):
                del dtype
                return gain * np.concatenate(
                    [np.identity(shape[0])] * (shape[1] // shape[0]), axis=1)

            self.recurrent_initializer = recurrent_identity

        self.kernel = self.add_weight(shape=(input_dim, self.units * 4),
                                      name='kernel',
                                      initializer=self.kernel_initializer,
                                      regularizer=self.kernel_regularizer,
                                      constraint=self.kernel_constraint)
        self.recurrent_kernel = self.add_weight(
            shape=(self.units, self.units * 4),
            name='recurrent_kernel',
            initializer=self.recurrent_initializer,
            regularizer=self.recurrent_regularizer,
            constraint=self.recurrent_constraint)

        if self.use_bias:
            if self.unit_forget_bias:
                @K.eager
                def bias_initializer(_, *args, **kwargs):
                    return K.concatenate([
                        self.bias_initializer((self.units,), *args, **kwargs),
                        initializers.Ones()((self.units,), *args, **kwargs),
                        self.bias_initializer((self.units * 2,), *args, **kwargs),
                    ])
            else:
                bias_initializer = self.bias_initializer
            self.bias = self.add_weight(shape=(self.units * 4,),
                                        name='bias',
                                        initializer=bias_initializer,
                                        regularizer=self.bias_regularizer,
                                        constraint=self.bias_constraint)
        else:
            self.bias = None

        self.kernel_i = self.kernel[:, :self.units]
        self.kernel_f = self.kernel[:, self.units: self.units * 2]
        self.kernel_c = self.kernel[:, self.units * 2: self.units * 3]
        self.kernel_o = self.kernel[:, self.units * 3:]

        self.recurrent_kernel_i = self.recurrent_kernel[:, :self.units]
        self.recurrent_kernel_f = (
            self.recurrent_kernel[:, self.units: self.units * 2])
        self.recurrent_kernel_c = (
            self.recurrent_kernel[:, self.units * 2: self.units * 3])
        self.recurrent_kernel_o = self.recurrent_kernel[:, self.units * 3:]

        if self.use_bias:
            self.bias_i = self.bias[:self.units]
            self.bias_f = self.bias[self.units: self.units * 2]
            self.bias_c = self.bias[self.units * 2: self.units * 3]
            self.bias_o = self.bias[self.units * 3:]
        else:
            self.bias_i = None
            self.bias_f = None
            self.bias_c = None
            self.bias_o = None
        self.built = True

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag974')" href="javascript:;">
keras-2.3.1/keras/layers/cudnn_recurrent.py: 419-481
</a>
<div class="mid" id="frag974" style="display:none"><pre>
    def build(self, input_shape):
        super(CuDNNLSTM, self).build(input_shape)
        if isinstance(input_shape, list):
            input_shape = input_shape[0]
        input_dim = input_shape[-1]

        from tensorflow.contrib.cudnn_rnn.python.ops import cudnn_rnn_ops
        self._cudnn_lstm = cudnn_rnn_ops.CudnnLSTM(
            num_layers=1,
            num_units=self.units,
            input_size=input_dim,
            input_mode='linear_input')

        self.kernel = self.add_weight(shape=(input_dim, self.units * 4),
                                      name='kernel',
                                      initializer=self.kernel_initializer,
                                      regularizer=self.kernel_regularizer,
                                      constraint=self.kernel_constraint)
        self.recurrent_kernel = self.add_weight(
            shape=(self.units, self.units * 4),
            name='recurrent_kernel',
            initializer=self.recurrent_initializer,
            regularizer=self.recurrent_regularizer,
            constraint=self.recurrent_constraint)

        if self.unit_forget_bias:
            def bias_initializer(shape, *args, **kwargs):
                return K.concatenate([
                    self.bias_initializer((self.units * 5,), *args, **kwargs),
                    initializers.Ones()((self.units,), *args, **kwargs),
                    self.bias_initializer((self.units * 2,), *args, **kwargs),
                ])
        else:
            bias_initializer = self.bias_initializer
        self.bias = self.add_weight(shape=(self.units * 8,),
                                    name='bias',
                                    initializer=bias_initializer,
                                    regularizer=self.bias_regularizer,
                                    constraint=self.bias_constraint)

        self.kernel_i = self.kernel[:, :self.units]
        self.kernel_f = self.kernel[:, self.units: self.units * 2]
        self.kernel_c = self.kernel[:, self.units * 2: self.units * 3]
        self.kernel_o = self.kernel[:, self.units * 3:]

        self.recurrent_kernel_i = self.recurrent_kernel[:, :self.units]
        self.recurrent_kernel_f = (
            self.recurrent_kernel[:, self.units: self.units * 2])
        self.recurrent_kernel_c = (
            self.recurrent_kernel[:, self.units * 2: self.units * 3])
        self.recurrent_kernel_o = self.recurrent_kernel[:, self.units * 3:]

        self.bias_i_i = self.bias[:self.units]
        self.bias_f_i = self.bias[self.units: self.units * 2]
        self.bias_c_i = self.bias[self.units * 2: self.units * 3]
        self.bias_o_i = self.bias[self.units * 3: self.units * 4]
        self.bias_i = self.bias[self.units * 4: self.units * 5]
        self.bias_f = self.bias[self.units * 5: self.units * 6]
        self.bias_c = self.bias[self.units * 6: self.units * 7]
        self.bias_o = self.bias[self.units * 7:]

        self.built = True

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 23:</b> &nbsp; 3 fragments, nominal size 10 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag895')" href="javascript:;">
keras-2.3.1/keras/layers/pooling.py: 19-29
</a>
<div class="mid" id="frag895" style="display:none"><pre>
    def __init__(self, pool_size=2, strides=None,
                 padding='valid', data_format='channels_last', **kwargs):
        super(_Pooling1D, self).__init__(**kwargs)
        if strides is None:
            strides = pool_size
        self.pool_size = conv_utils.normalize_tuple(pool_size, 1, 'pool_size')
        self.strides = conv_utils.normalize_tuple(strides, 1, 'strides')
        self.padding = conv_utils.normalize_padding(padding)
        self.data_format = K.normalize_data_format(data_format)
        self.input_spec = InputSpec(ndim=3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag913')" href="javascript:;">
keras-2.3.1/keras/layers/pooling.py: 330-340
</a>
<div class="mid" id="frag913" style="display:none"><pre>
    def __init__(self, pool_size=(2, 2, 2), strides=None, padding='valid',
                 data_format=None, **kwargs):
        super(_Pooling3D, self).__init__(**kwargs)
        if strides is None:
            strides = pool_size
        self.pool_size = conv_utils.normalize_tuple(pool_size, 3, 'pool_size')
        self.strides = conv_utils.normalize_tuple(strides, 3, 'strides')
        self.padding = conv_utils.normalize_padding(padding)
        self.data_format = K.normalize_data_format(data_format)
        self.input_spec = InputSpec(ndim=5)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag904')" href="javascript:;">
keras-2.3.1/keras/layers/pooling.py: 169-179
</a>
<div class="mid" id="frag904" style="display:none"><pre>
    def __init__(self, pool_size=(2, 2), strides=None, padding='valid',
                 data_format=None, **kwargs):
        super(_Pooling2D, self).__init__(**kwargs)
        if strides is None:
            strides = pool_size
        self.pool_size = conv_utils.normalize_tuple(pool_size, 2, 'pool_size')
        self.strides = conv_utils.normalize_tuple(strides, 2, 'strides')
        self.padding = conv_utils.normalize_padding(padding)
        self.data_format = K.normalize_data_format(data_format)
        self.input_spec = InputSpec(ndim=4)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 24:</b> &nbsp; 2 fragments, nominal size 32 lines, similarity 93%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag967')" href="javascript:;">
keras-2.3.1/keras/layers/cudnn_recurrent.py: 179-213
</a>
<div class="mid" id="frag967" style="display:none"><pre>
    def __init__(self, units,
                 kernel_initializer='glorot_uniform',
                 recurrent_initializer='orthogonal',
                 bias_initializer='zeros',
                 kernel_regularizer=None,
                 recurrent_regularizer=None,
                 bias_regularizer=None,
                 activity_regularizer=None,
                 kernel_constraint=None,
                 recurrent_constraint=None,
                 bias_constraint=None,
                 return_sequences=False,
                 return_state=False,
                 stateful=False,
                 **kwargs):
        self.units = units
        super(CuDNNGRU, self).__init__(
            return_sequences=return_sequences,
            return_state=return_state,
            stateful=stateful,
            **kwargs)

        self.kernel_initializer = initializers.get(kernel_initializer)
        self.recurrent_initializer = initializers.get(recurrent_initializer)
        self.bias_initializer = initializers.get(bias_initializer)

        self.kernel_regularizer = regularizers.get(kernel_regularizer)
        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)
        self.bias_regularizer = regularizers.get(bias_regularizer)
        self.activity_regularizer = regularizers.get(activity_regularizer)

        self.kernel_constraint = constraints.get(kernel_constraint)
        self.recurrent_constraint = constraints.get(recurrent_constraint)
        self.bias_constraint = constraints.get(bias_constraint)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag972')" href="javascript:;">
keras-2.3.1/keras/layers/cudnn_recurrent.py: 376-412
</a>
<div class="mid" id="frag972" style="display:none"><pre>
    def __init__(self, units,
                 kernel_initializer='glorot_uniform',
                 recurrent_initializer='orthogonal',
                 bias_initializer='zeros',
                 unit_forget_bias=True,
                 kernel_regularizer=None,
                 recurrent_regularizer=None,
                 bias_regularizer=None,
                 activity_regularizer=None,
                 kernel_constraint=None,
                 recurrent_constraint=None,
                 bias_constraint=None,
                 return_sequences=False,
                 return_state=False,
                 stateful=False,
                 **kwargs):
        self.units = units
        super(CuDNNLSTM, self).__init__(
            return_sequences=return_sequences,
            return_state=return_state,
            stateful=stateful,
            **kwargs)

        self.kernel_initializer = initializers.get(kernel_initializer)
        self.recurrent_initializer = initializers.get(recurrent_initializer)
        self.bias_initializer = initializers.get(bias_initializer)
        self.unit_forget_bias = unit_forget_bias

        self.kernel_regularizer = regularizers.get(kernel_regularizer)
        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)
        self.bias_regularizer = regularizers.get(bias_regularizer)
        self.activity_regularizer = regularizers.get(activity_regularizer)

        self.kernel_constraint = constraints.get(kernel_constraint)
        self.recurrent_constraint = constraints.get(recurrent_constraint)
        self.bias_constraint = constraints.get(bias_constraint)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 25:</b> &nbsp; 2 fragments, nominal size 39 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag970')" href="javascript:;">
keras-2.3.1/keras/layers/cudnn_recurrent.py: 269-306
</a>
<div class="mid" id="frag970" style="display:none"><pre>
    def _process_batch(self, inputs, initial_state):
        import tensorflow as tf
        inputs = tf.transpose(inputs, (1, 0, 2))
        input_h = initial_state[0]
        input_h = tf.expand_dims(input_h, axis=0)

        params = self._canonical_to_params(
            weights=[
                self.kernel_r,
                self.kernel_z,
                self.kernel_h,
                self.recurrent_kernel_r,
                self.recurrent_kernel_z,
                self.recurrent_kernel_h,
            ],
            biases=[
                self.bias_r_i,
                self.bias_z_i,
                self.bias_h_i,
                self.bias_r,
                self.bias_z,
                self.bias_h,
            ],
        )
        outputs, h = self._cudnn_gru(
            inputs,
            input_h=input_h,
            params=params,
            is_training=True)

        if self.stateful or self.return_state:
            h = h[0]
        if self.return_sequences:
            output = tf.transpose(outputs, (1, 0, 2))
        else:
            output = outputs[-1]
        return output, [h]

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag976')" href="javascript:;">
keras-2.3.1/keras/layers/cudnn_recurrent.py: 482-527
</a>
<div class="mid" id="frag976" style="display:none"><pre>
    def _process_batch(self, inputs, initial_state):
        import tensorflow as tf
        inputs = tf.transpose(inputs, (1, 0, 2))
        input_h = initial_state[0]
        input_c = initial_state[1]
        input_h = tf.expand_dims(input_h, axis=0)
        input_c = tf.expand_dims(input_c, axis=0)

        params = self._canonical_to_params(
            weights=[
                self.kernel_i,
                self.kernel_f,
                self.kernel_c,
                self.kernel_o,
                self.recurrent_kernel_i,
                self.recurrent_kernel_f,
                self.recurrent_kernel_c,
                self.recurrent_kernel_o,
            ],
            biases=[
                self.bias_i_i,
                self.bias_f_i,
                self.bias_c_i,
                self.bias_o_i,
                self.bias_i,
                self.bias_f,
                self.bias_c,
                self.bias_o,
            ],
        )
        outputs, h, c = self._cudnn_lstm(
            inputs,
            input_h=input_h,
            input_c=input_c,
            params=params,
            is_training=True)

        if self.stateful or self.return_state:
            h = h[0]
            c = c[0]
        if self.return_sequences:
            output = tf.transpose(outputs, (1, 0, 2))
        else:
            output = outputs[-1]
        return output, [h, c]

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 26:</b> &nbsp; 2 fragments, nominal size 19 lines, similarity 89%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag971')" href="javascript:;">
keras-2.3.1/keras/layers/cudnn_recurrent.py: 307-327
</a>
<div class="mid" id="frag971" style="display:none"><pre>
    def get_config(self):
        config = {
            'units': self.units,
            'kernel_initializer': initializers.serialize(self.kernel_initializer),
            'recurrent_initializer':
                initializers.serialize(self.recurrent_initializer),
            'bias_initializer': initializers.serialize(self.bias_initializer),
            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),
            'recurrent_regularizer':
                regularizers.serialize(self.recurrent_regularizer),
            'bias_regularizer': regularizers.serialize(self.bias_regularizer),
            'activity_regularizer':
                regularizers.serialize(self.activity_regularizer),
            'kernel_constraint': constraints.serialize(self.kernel_constraint),
            'recurrent_constraint':
                constraints.serialize(self.recurrent_constraint),
            'bias_constraint': constraints.serialize(self.bias_constraint)}
        base_config = super(CuDNNGRU, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag977')" href="javascript:;">
keras-2.3.1/keras/layers/cudnn_recurrent.py: 528-546
</a>
<div class="mid" id="frag977" style="display:none"><pre>
    def get_config(self):
        config = {
            'units': self.units,
            'kernel_initializer': initializers.serialize(self.kernel_initializer),
            'recurrent_initializer':
                initializers.serialize(self.recurrent_initializer),
            'bias_initializer': initializers.serialize(self.bias_initializer),
            'unit_forget_bias': self.unit_forget_bias,
            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),
            'recurrent_regularizer':
                regularizers.serialize(self.recurrent_regularizer),
            'bias_regularizer': regularizers.serialize(self.bias_regularizer),
            'activity_regularizer':
                regularizers.serialize(self.activity_regularizer),
            'kernel_constraint': constraints.serialize(self.kernel_constraint),
            'recurrent_constraint': constraints.serialize(self.recurrent_constraint),
            'bias_constraint': constraints.serialize(self.bias_constraint)}
        base_config = super(CuDNNLSTM, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 27:</b> &nbsp; 3 fragments, nominal size 27 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag985')" href="javascript:;">
keras-2.3.1/keras/layers/convolutional.py: 126-154
</a>
<div class="mid" id="frag985" style="display:none"><pre>
    def build(self, input_shape):
        if self.data_format == 'channels_first':
            channel_axis = 1
        else:
            channel_axis = -1
        if input_shape[channel_axis] is None:
            raise ValueError('The channel dimension of the inputs '
                             'should be defined. Found `None`.')
        input_dim = input_shape[channel_axis]
        kernel_shape = self.kernel_size + (input_dim, self.filters)

        self.kernel = self.add_weight(shape=kernel_shape,
                                      initializer=self.kernel_initializer,
                                      name='kernel',
                                      regularizer=self.kernel_regularizer,
                                      constraint=self.kernel_constraint)
        if self.use_bias:
            self.bias = self.add_weight(shape=(self.filters,),
                                        initializer=self.bias_initializer,
                                        name='bias',
                                        regularizer=self.bias_regularizer,
                                        constraint=self.bias_constraint)
        else:
            self.bias = None
        # Set input spec.
        self.input_spec = InputSpec(ndim=self.rank + 2,
                                    axes={channel_axis: input_dim})
        self.built = True

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1001')" href="javascript:;">
keras-2.3.1/keras/layers/convolutional.py: 1048-1079
</a>
<div class="mid" id="frag1001" style="display:none"><pre>
    def build(self, input_shape):
        if len(input_shape) != 5:
            raise ValueError('Inputs should have rank ' +
                             str(5) +
                             '; Received input shape:', str(input_shape))
        if self.data_format == 'channels_first':
            channel_axis = 1
        else:
            channel_axis = -1
        if input_shape[channel_axis] is None:
            raise ValueError('The channel dimension of the inputs '
                             'should be defined. Found `None`.')
        input_dim = input_shape[channel_axis]
        kernel_shape = self.kernel_size + (self.filters, input_dim)

        self.kernel = self.add_weight(shape=kernel_shape,
                                      initializer=self.kernel_initializer,
                                      name='kernel',
                                      regularizer=self.kernel_regularizer,
                                      constraint=self.kernel_constraint)
        if self.use_bias:
            self.bias = self.add_weight(shape=(self.filters,),
                                        initializer=self.bias_initializer,
                                        name='bias',
                                        regularizer=self.bias_regularizer,
                                        constraint=self.bias_constraint)
        else:
            self.bias = None
        # Set input spec.
        self.input_spec = InputSpec(ndim=5, axes={channel_axis: input_dim})
        self.built = True

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag996')" href="javascript:;">
keras-2.3.1/keras/layers/convolutional.py: 776-807
</a>
<div class="mid" id="frag996" style="display:none"><pre>
    def build(self, input_shape):
        if len(input_shape) != 4:
            raise ValueError('Inputs should have rank ' +
                             str(4) +
                             '; Received input shape:', str(input_shape))
        if self.data_format == 'channels_first':
            channel_axis = 1
        else:
            channel_axis = -1
        if input_shape[channel_axis] is None:
            raise ValueError('The channel dimension of the inputs '
                             'should be defined. Found `None`.')
        input_dim = input_shape[channel_axis]
        kernel_shape = self.kernel_size + (self.filters, input_dim)

        self.kernel = self.add_weight(shape=kernel_shape,
                                      initializer=self.kernel_initializer,
                                      name='kernel',
                                      regularizer=self.kernel_regularizer,
                                      constraint=self.kernel_constraint)
        if self.use_bias:
            self.bias = self.add_weight(shape=(self.filters,),
                                        initializer=self.bias_initializer,
                                        name='bias',
                                        regularizer=self.bias_regularizer,
                                        constraint=self.bias_constraint)
        else:
            self.bias = None
        # Set input spec.
        self.input_spec = InputSpec(ndim=4, axes={channel_axis: input_dim})
        self.built = True

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 28:</b> &nbsp; 2 fragments, nominal size 30 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag986')" href="javascript:;">
keras-2.3.1/keras/layers/convolutional.py: 155-190
</a>
<div class="mid" id="frag986" style="display:none"><pre>
    def call(self, inputs):
        if self.rank == 1:
            outputs = K.conv1d(
                inputs,
                self.kernel,
                strides=self.strides[0],
                padding=self.padding,
                data_format=self.data_format,
                dilation_rate=self.dilation_rate[0])
        if self.rank == 2:
            outputs = K.conv2d(
                inputs,
                self.kernel,
                strides=self.strides,
                padding=self.padding,
                data_format=self.data_format,
                dilation_rate=self.dilation_rate)
        if self.rank == 3:
            outputs = K.conv3d(
                inputs,
                self.kernel,
                strides=self.strides,
                padding=self.padding,
                data_format=self.data_format,
                dilation_rate=self.dilation_rate)

        if self.use_bias:
            outputs = K.bias_add(
                outputs,
                self.bias,
                data_format=self.data_format)

        if self.activation is not None:
            return self.activation(outputs)
        return outputs

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1007')" href="javascript:;">
keras-2.3.1/keras/layers/convolutional.py: 1362-1391
</a>
<div class="mid" id="frag1007" style="display:none"><pre>
    def call(self, inputs):
        if self.rank == 1:
            outputs = K.separable_conv1d(
                inputs,
                self.depthwise_kernel,
                self.pointwise_kernel,
                data_format=self.data_format,
                strides=self.strides,
                padding=self.padding,
                dilation_rate=self.dilation_rate)
        if self.rank == 2:
            outputs = K.separable_conv2d(
                inputs,
                self.depthwise_kernel,
                self.pointwise_kernel,
                data_format=self.data_format,
                strides=self.strides,
                padding=self.padding,
                dilation_rate=self.dilation_rate)

        if self.use_bias:
            outputs = K.bias_add(
                outputs,
                self.bias,
                data_format=self.data_format)

        if self.activation is not None:
            return self.activation(outputs)
        return outputs

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 29:</b> &nbsp; 2 fragments, nominal size 19 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag987')" href="javascript:;">
keras-2.3.1/keras/layers/convolutional.py: 191-209
</a>
<div class="mid" id="frag987" style="display:none"><pre>
    def compute_output_shape(self, input_shape):
        if self.data_format == 'channels_last':
            space = input_shape[1:-1]
        elif self.data_format == 'channels_first':
            space = input_shape[2:]
        new_space = []
        for i in range(len(space)):
            new_dim = conv_utils.conv_output_length(
                space[i],
                self.kernel_size[i],
                padding=self.padding,
                stride=self.strides[i],
                dilation=self.dilation_rate[i])
            new_space.append(new_dim)
        if self.data_format == 'channels_last':
            return (input_shape[0],) + tuple(new_space) + (self.filters,)
        elif self.data_format == 'channels_first':
            return (input_shape[0], self.filters) + tuple(new_space)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1014')" href="javascript:;">
keras-2.3.1/keras/layers/convolutional.py: 1863-1883
</a>
<div class="mid" id="frag1014" style="display:none"><pre>
    def compute_output_shape(self, input_shape):
        if self.data_format == 'channels_last':
            space = input_shape[1:-1]
            out_filters = input_shape[3] * self.depth_multiplier
        elif self.data_format == 'channels_first':
            space = input_shape[2:]
            out_filters = input_shape[1] * self.depth_multiplier
        new_space = []
        for i in range(len(space)):
            new_dim = conv_utils.conv_output_length(
                space[i],
                self.kernel_size[i],
                padding=self.padding,
                stride=self.strides[i],
                dilation=self.dilation_rate[i])
            new_space.append(new_dim)
        if self.data_format == 'channels_last':
            return (input_shape[0], new_space[0], new_space[1], out_filters)
        elif self.data_format == 'channels_first':
            return (input_shape[0], out_filters, new_space[0], new_space[1])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 30:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1008')" href="javascript:;">
keras-2.3.1/keras/layers/convolutional.py: 1392-1413
</a>
<div class="mid" id="frag1008" style="display:none"><pre>
    def get_config(self):
        config = super(_SeparableConv, self).get_config()
        config.pop('rank')
        config.pop('kernel_initializer')
        config.pop('kernel_regularizer')
        config.pop('kernel_constraint')
        config['depth_multiplier'] = self.depth_multiplier
        config['depthwise_initializer'] = (
            initializers.serialize(self.depthwise_initializer))
        config['pointwise_initializer'] = (
            initializers.serialize(self.pointwise_initializer))
        config['depthwise_regularizer'] = (
            regularizers.serialize(self.depthwise_regularizer))
        config['pointwise_regularizer'] = (
            regularizers.serialize(self.pointwise_regularizer))
        config['depthwise_constraint'] = (
            constraints.serialize(self.depthwise_constraint))
        config['pointwise_constraint'] = (
            constraints.serialize(self.pointwise_constraint))
        return config


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1015')" href="javascript:;">
keras-2.3.1/keras/layers/convolutional.py: 1884-1899
</a>
<div class="mid" id="frag1015" style="display:none"><pre>
    def get_config(self):
        config = super(DepthwiseConv2D, self).get_config()
        config.pop('filters')
        config.pop('kernel_initializer')
        config.pop('kernel_regularizer')
        config.pop('kernel_constraint')
        config['depth_multiplier'] = self.depth_multiplier
        config['depthwise_initializer'] = (
            initializers.serialize(self.depthwise_initializer))
        config['depthwise_regularizer'] = (
            regularizers.serialize(self.depthwise_regularizer))
        config['depthwise_constraint'] = (
            constraints.serialize(self.depthwise_constraint))
        return config


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 31:</b> &nbsp; 3 fragments, nominal size 11 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1018')" href="javascript:;">
keras-2.3.1/keras/layers/convolutional.py: 1926-1937
</a>
<div class="mid" id="frag1018" style="display:none"><pre>
    def compute_output_shape(self, input_shape):
        size_all_dims = (1,) + self.size + (1,)
        spatial_axes = list(range(1, 1 + self.rank))
        size_all_dims = transpose_shape(size_all_dims,
                                        self.data_format,
                                        spatial_axes)
        output_shape = list(input_shape)
        for dim in range(len(output_shape)):
            if output_shape[dim] is not None:
                output_shape[dim] *= size_all_dims[dim]
        return tuple(output_shape)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1030')" href="javascript:;">
keras-2.3.1/keras/layers/convolutional.py: 2106-2117
</a>
<div class="mid" id="frag1030" style="display:none"><pre>
    def compute_output_shape(self, input_shape):
        padding_all_dims = ((0, 0),) + self.padding + ((0, 0),)
        spatial_axes = list(range(1, 1 + self.rank))
        padding_all_dims = transpose_shape(padding_all_dims,
                                           self.data_format,
                                           spatial_axes)
        output_shape = list(input_shape)
        for dim in range(len(output_shape)):
            if output_shape[dim] is not None:
                output_shape[dim] += sum(padding_all_dims[dim])
        return tuple(output_shape)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1041')" href="javascript:;">
keras-2.3.1/keras/layers/convolutional.py: 2358-2369
</a>
<div class="mid" id="frag1041" style="display:none"><pre>
    def compute_output_shape(self, input_shape):
        cropping_all_dims = ((0, 0),) + self.cropping + ((0, 0),)
        spatial_axes = list(range(1, 1 + self.rank))
        cropping_all_dims = transpose_shape(cropping_all_dims,
                                            self.data_format,
                                            spatial_axes)
        output_shape = list(input_shape)
        for dim in range(len(output_shape)):
            if output_shape[dim] is not None:
                output_shape[dim] -= sum(cropping_all_dims[dim])
        return tuple(output_shape)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 32:</b> &nbsp; 2 fragments, nominal size 21 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1037')" href="javascript:;">
keras-2.3.1/keras/layers/convolutional.py: 2283-2309
</a>
<div class="mid" id="frag1037" style="display:none"><pre>
    def __init__(self, padding=(1, 1, 1), data_format=None, **kwargs):
        if isinstance(padding, int):
            normalized_padding = 3 * ((padding, padding),)
        elif hasattr(padding, '__len__'):
            if len(padding) != 3:
                raise ValueError('`padding` should have 3 elements. '
                                 'Found: ' + str(padding))
            dim1_padding = conv_utils.normalize_tuple(padding[0], 2,
                                                      '1st entry of padding')
            dim2_padding = conv_utils.normalize_tuple(padding[1], 2,
                                                      '2nd entry of padding')
            dim3_padding = conv_utils.normalize_tuple(padding[2], 2,
                                                      '3rd entry of padding')
            normalized_padding = (dim1_padding, dim2_padding, dim3_padding)
        else:
            raise ValueError(
                '`padding` should be either an int, a tuple of 3 ints '
                '(symmetric_dim1_pad, symmetric_dim2_pad, symmetric_dim3_pad), '
                'or a tuple of 3 tuples of 2 ints '
                '((left_dim1_pad, right_dim1_pad),'
                ' (left_dim2_pad, right_dim2_pad),'
                ' (left_dim3_pad, right_dim2_pad)). '
                'Found: ' + str(padding))
        super(ZeroPadding3D, self).__init__(normalized_padding,
                                            data_format,
                                            **kwargs)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1046')" href="javascript:;">
keras-2.3.1/keras/layers/convolutional.py: 2539-2572
</a>
<div class="mid" id="frag1046" style="display:none"><pre>
    def __init__(self, cropping=((1, 1), (1, 1), (1, 1)),
                 data_format=None, **kwargs):
        self.data_format = K.normalize_data_format(data_format)
        if isinstance(cropping, int):
            normalized_cropping = ((cropping, cropping),
                                   (cropping, cropping),
                                   (cropping, cropping))
        elif hasattr(cropping, '__len__'):
            if len(cropping) != 3:
                raise ValueError('`cropping` should have 3 elements. '
                                 'Found: ' + str(cropping))
            dim1_cropping = conv_utils.normalize_tuple(cropping[0], 2,
                                                       '1st entry of cropping')
            dim2_cropping = conv_utils.normalize_tuple(cropping[1], 2,
                                                       '2nd entry of cropping')
            dim3_cropping = conv_utils.normalize_tuple(cropping[2], 2,
                                                       '3rd entry of cropping')
            normalized_cropping = (dim1_cropping, dim2_cropping, dim3_cropping)
        else:
            raise ValueError(
                '`cropping` should be either an int, a tuple of 3 ints '
                '(symmetric_dim1_crop, symmetric_dim2_crop, symmetric_dim3_crop), '
                'or a tuple of 3 tuples of 2 ints '
                '((left_dim1_crop, right_dim1_crop),'
                ' (left_dim2_crop, right_dim2_crop),'
                ' (left_dim3_crop, right_dim2_crop)). '
                'Found: ' + str(cropping))
        super(Cropping3D, self).__init__(normalized_cropping,
                                         data_format,
                                         **kwargs)


# Aliases

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 33:</b> &nbsp; 2 fragments, nominal size 22 lines, similarity 91%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1111')" href="javascript:;">
keras-2.3.1/tests/keras/legacy/interface_test.py: 175-198
</a>
<div class="mid" id="frag1111" style="display:none"><pre>
def test_simplernn_legacy_interface():
    old_layer = keras.layers.SimpleRNN(input_shape=[3, 5], output_dim=2, name='d')
    new_layer = keras.layers.SimpleRNN(2, input_shape=[3, 5], name='d')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())

    old_layer = keras.layers.SimpleRNN(2, init='normal',
                                       inner_init='glorot_uniform',
                                       W_regularizer='l1',
                                       U_regularizer='l1',
                                       b_regularizer='l1',
                                       dropout_W=0.1,
                                       dropout_U=0.1,
                                       name='SimpleRNN')
    new_layer = keras.layers.SimpleRNN(2, kernel_initializer='normal',
                                       recurrent_initializer='glorot_uniform',
                                       kernel_regularizer='l1',
                                       recurrent_regularizer='l1',
                                       bias_regularizer='l1',
                                       dropout=0.1,
                                       recurrent_dropout=0.1,
                                       name='SimpleRNN')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1112')" href="javascript:;">
keras-2.3.1/tests/keras/legacy/interface_test.py: 199-224
</a>
<div class="mid" id="frag1112" style="display:none"><pre>
def test_gru_legacy_interface():
    old_layer = keras.layers.GRU(input_shape=[3, 5], output_dim=2, name='d')
    new_layer = keras.layers.GRU(2, input_shape=[3, 5], name='d')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())

    old_layer = keras.layers.GRU(2, init='normal',
                                 inner_init='glorot_uniform',
                                 inner_activation='hard_sigmoid',
                                 W_regularizer='l1',
                                 U_regularizer='l1',
                                 b_regularizer='l1',
                                 dropout_W=0.1,
                                 dropout_U=0.1,
                                 name='GRU')
    new_layer = keras.layers.GRU(2, kernel_initializer='normal',
                                 recurrent_initializer='glorot_uniform',
                                 recurrent_activation='hard_sigmoid',
                                 kernel_regularizer='l1',
                                 recurrent_regularizer='l1',
                                 bias_regularizer='l1',
                                 dropout=0.1,
                                 recurrent_dropout=0.1,
                                 name='GRU')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 34:</b> &nbsp; 2 fragments, nominal size 27 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1114')" href="javascript:;">
keras-2.3.1/tests/keras/legacy/interface_test.py: 233-264
</a>
<div class="mid" id="frag1114" style="display:none"><pre>
def test_maxpooling2d_legacy_interface():
    old_layer = keras.layers.MaxPooling2D(
        pool_size=(2, 2), border_mode='valid', name='maxpool2d')
    new_layer = keras.layers.MaxPool2D(
        pool_size=2, padding='valid', name='maxpool2d')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())

    old_layer = keras.layers.MaxPooling2D((2, 2), 2, 'valid', name='maxpool2d')
    new_layer = keras.layers.MaxPool2D(
        pool_size=2, strides=2, padding='valid', name='maxpool2d')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())

    old_layer = keras.layers.MaxPooling2D(
        (2, 2), padding='valid', dim_ordering='tf', name='maxpool2d')
    new_layer = keras.layers.MaxPool2D(
        pool_size=2, padding='valid', data_format='channels_last', name='maxpool2d')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())

    old_layer = keras.layers.MaxPooling2D(
        (2, 2), padding='valid', dim_ordering='th', name='maxpool2d')
    new_layer = keras.layers.MaxPool2D(
        pool_size=2, padding='valid', data_format='channels_first',
        name='maxpool2d')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())

    old_layer = keras.layers.MaxPooling2D(
        (2, 2), padding='valid', dim_ordering='default', name='maxpool2d')
    new_layer = keras.layers.MaxPool2D(
        pool_size=2, padding='valid', name='maxpool2d')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1115')" href="javascript:;">
keras-2.3.1/tests/keras/legacy/interface_test.py: 265-298
</a>
<div class="mid" id="frag1115" style="display:none"><pre>
def test_avgpooling2d_legacy_interface():
    old_layer = keras.layers.AveragePooling2D(
        pool_size=(2, 2), border_mode='valid', name='avgpooling2d')
    new_layer = keras.layers.AvgPool2D(
        pool_size=(2, 2), padding='valid', name='avgpooling2d')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())

    old_layer = keras.layers.AveragePooling2D(
        (2, 2), (2, 2), 'valid', name='avgpooling2d')
    new_layer = keras.layers.AvgPool2D(
        pool_size=(2, 2), strides=(2, 2), padding='valid', name='avgpooling2d')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())

    old_layer = keras.layers.AveragePooling2D(
        (2, 2), padding='valid', dim_ordering='tf', name='avgpooling2d')
    new_layer = keras.layers.AvgPool2D(
        pool_size=2, padding='valid', data_format='channels_last',
        name='avgpooling2d')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())

    old_layer = keras.layers.AveragePooling2D(
        (2, 2), padding='valid', dim_ordering='th', name='avgpooling2d')
    new_layer = keras.layers.AvgPool2D(
        pool_size=2, padding='valid', data_format='channels_first',
        name='avgpooling2d')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())

    old_layer = keras.layers.AveragePooling2D(
        (2, 2), padding='valid', dim_ordering='default', name='avgpooling2d')
    new_layer = keras.layers.AvgPool2D(
        pool_size=2, padding='valid', name='avgpooling2d')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 35:</b> &nbsp; 2 fragments, nominal size 29 lines, similarity 93%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1116')" href="javascript:;">
keras-2.3.1/tests/keras/legacy/interface_test.py: 299-332
</a>
<div class="mid" id="frag1116" style="display:none"><pre>
def test_maxpooling3d_legacy_interface():
    old_layer = keras.layers.MaxPooling3D(
        pool_size=(2, 2, 2), border_mode='valid', name='maxpool3d')
    new_layer = keras.layers.MaxPool3D(
        pool_size=(2, 2, 2), padding='valid', name='maxpool3d')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())

    old_layer = keras.layers.MaxPooling3D(
        (2, 2, 2), (2, 2, 2), 'valid', name='maxpool3d')
    new_layer = keras.layers.MaxPool3D(
        pool_size=(2, 2, 2), strides=(2, 2, 2), padding='valid', name='maxpool3d')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())

    old_layer = keras.layers.MaxPooling3D(
        (2, 2, 2), padding='valid', dim_ordering='tf', name='maxpool3d')
    new_layer = keras.layers.MaxPool3D(
        pool_size=(2, 2, 2), padding='valid', data_format='channels_last',
        name='maxpool3d')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())

    old_layer = keras.layers.MaxPooling3D(
        (2, 2, 2), padding='valid', dim_ordering='th', name='maxpool3d')
    new_layer = keras.layers.MaxPool3D(
        pool_size=(2, 2, 2), padding='valid', data_format='channels_first',
        name='maxpool3d')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())

    old_layer = keras.layers.MaxPooling3D(
        (2, 2, 2), padding='valid', dim_ordering='default', name='maxpool3d')
    new_layer = keras.layers.MaxPool3D(
        pool_size=(2, 2, 2), padding='valid', name='maxpool3d')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1117')" href="javascript:;">
keras-2.3.1/tests/keras/legacy/interface_test.py: 333-367
</a>
<div class="mid" id="frag1117" style="display:none"><pre>
def test_avgpooling3d_legacy_interface():
    old_layer = keras.layers.AveragePooling3D(
        pool_size=(2, 2, 2), border_mode='valid', name='avgpooling3d')
    new_layer = keras.layers.AvgPool3D(
        pool_size=(2, 2, 2), padding='valid', name='avgpooling3d')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())

    old_layer = keras.layers.AveragePooling3D(
        (2, 2, 2), (2, 2, 2), 'valid', name='avgpooling3d')
    new_layer = keras.layers.AvgPool3D(
        pool_size=(2, 2, 2), strides=(2, 2, 2), padding='valid',
        name='avgpooling3d')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())

    old_layer = keras.layers.AveragePooling3D(
        (2, 2, 2), padding='valid', dim_ordering='tf', name='avgpooling3d')
    new_layer = keras.layers.AvgPool3D(
        pool_size=(2, 2, 2), padding='valid', data_format='channels_last',
        name='avgpooling3d')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())

    old_layer = keras.layers.AveragePooling3D(
        (2, 2, 2), padding='valid', dim_ordering='th', name='avgpooling3d')
    new_layer = keras.layers.AvgPool3D(
        pool_size=(2, 2, 2), padding='valid', data_format='channels_first',
        name='avgpooling3d')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())

    old_layer = keras.layers.AveragePooling3D(
        (2, 2, 2), padding='valid', dim_ordering='default', name='avgpooling3d')
    new_layer = keras.layers.AvgPool3D(
        pool_size=(2, 2, 2), padding='valid', name='avgpooling3d')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 36:</b> &nbsp; 4 fragments, nominal size 15 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1118')" href="javascript:;">
keras-2.3.1/tests/keras/legacy/interface_test.py: 368-386
</a>
<div class="mid" id="frag1118" style="display:none"><pre>
def test_global_maxpooling2d_legacy_interface():
    old_layer = keras.layers.GlobalMaxPooling2D(dim_ordering='tf',
                                                name='global_maxpool2d')
    new_layer = keras.layers.GlobalMaxPool2D(data_format='channels_last',
                                             name='global_maxpool2d')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())

    old_layer = keras.layers.GlobalMaxPooling2D(dim_ordering='th',
                                                name='global_maxpool2d')
    new_layer = keras.layers.GlobalMaxPool2D(data_format='channels_first',
                                             name='global_maxpool2d')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())

    old_layer = keras.layers.GlobalMaxPooling2D(dim_ordering='default',
                                                name='global_maxpool2d')
    new_layer = keras.layers.GlobalMaxPool2D(name='global_maxpool2d')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1119')" href="javascript:;">
keras-2.3.1/tests/keras/legacy/interface_test.py: 387-405
</a>
<div class="mid" id="frag1119" style="display:none"><pre>
def test_global_avgpooling2d_legacy_interface():
    old_layer = keras.layers.GlobalAveragePooling2D(dim_ordering='tf',
                                                    name='global_avgpool2d')
    new_layer = keras.layers.GlobalAvgPool2D(data_format='channels_last',
                                             name='global_avgpool2d')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())

    old_layer = keras.layers.GlobalAveragePooling2D(dim_ordering='th',
                                                    name='global_avgpool2d')
    new_layer = keras.layers.GlobalAvgPool2D(data_format='channels_first',
                                             name='global_avgpool2d')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())

    old_layer = keras.layers.GlobalAveragePooling2D(dim_ordering='default',
                                                    name='global_avgpool2d')
    new_layer = keras.layers.GlobalAvgPool2D(name='global_avgpool2d')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1120')" href="javascript:;">
keras-2.3.1/tests/keras/legacy/interface_test.py: 406-424
</a>
<div class="mid" id="frag1120" style="display:none"><pre>
def test_global_maxpooling3d_legacy_interface():
    old_layer = keras.layers.GlobalMaxPooling3D(dim_ordering='tf',
                                                name='global_maxpool3d')
    new_layer = keras.layers.GlobalMaxPool3D(data_format='channels_last',
                                             name='global_maxpool3d')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())

    old_layer = keras.layers.GlobalMaxPooling3D(dim_ordering='th',
                                                name='global_maxpool3d')
    new_layer = keras.layers.GlobalMaxPool3D(data_format='channels_first',
                                             name='global_maxpool3d')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())

    old_layer = keras.layers.GlobalMaxPooling3D(dim_ordering='default',
                                                name='global_maxpool3d')
    new_layer = keras.layers.GlobalMaxPool3D(name='global_maxpool3d')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1121')" href="javascript:;">
keras-2.3.1/tests/keras/legacy/interface_test.py: 425-443
</a>
<div class="mid" id="frag1121" style="display:none"><pre>
def test_global_avgpooling3d_legacy_interface():
    old_layer = keras.layers.GlobalAveragePooling3D(dim_ordering='tf',
                                                    name='global_avgpool3d')
    new_layer = keras.layers.GlobalAvgPool3D(data_format='channels_last',
                                             name='global_avgpool3d')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())

    old_layer = keras.layers.GlobalAveragePooling3D(dim_ordering='th',
                                                    name='global_avgpool3d')
    new_layer = keras.layers.GlobalAvgPool3D(data_format='channels_first',
                                             name='global_avgpool3d')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())

    old_layer = keras.layers.GlobalAveragePooling3D(dim_ordering='default',
                                                    name='global_avgpool3d')
    new_layer = keras.layers.GlobalAvgPool3D(name='global_avgpool3d')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 37:</b> &nbsp; 3 fragments, nominal size 34 lines, similarity 74%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1125')" href="javascript:;">
keras-2.3.1/tests/keras/legacy/interface_test.py: 469-504
</a>
<div class="mid" id="frag1125" style="display:none"><pre>
def test_conv2d_legacy_interface():
    old_layer = keras.layers.Convolution2D(5, 3, 3, name='conv')
    new_layer = keras.layers.Conv2D(5, (3, 3), name='conv')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())

    old_layer = keras.layers.Convolution2D(5, 3, nb_col=3, name='conv')
    new_layer = keras.layers.Conv2D(5, (3, 3), name='conv')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())

    old_layer = keras.layers.Convolution2D(5, nb_row=3, nb_col=3, name='conv')
    new_layer = keras.layers.Conv2D(5, (3, 3), name='conv')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())

    old_layer = keras.layers.Convolution2D(5, 3, 3,
                                           init='normal',
                                           subsample=(2, 2),
                                           border_mode='valid',
                                           dim_ordering='th',
                                           W_regularizer='l1',
                                           b_regularizer='l2',
                                           W_constraint='maxnorm',
                                           b_constraint='unitnorm',
                                           name='conv')
    new_layer = keras.layers.Conv2D(5, (3, 3),
                                    kernel_initializer='normal',
                                    strides=(2, 2),
                                    padding='valid',
                                    kernel_regularizer='l1',
                                    bias_regularizer='l2',
                                    kernel_constraint='max_norm',
                                    bias_constraint='unit_norm',
                                    data_format='channels_first',
                                    name='conv')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1130')" href="javascript:;">
keras-2.3.1/tests/keras/legacy/interface_test.py: 666-709
</a>
<div class="mid" id="frag1130" style="display:none"><pre>
def test_convlstm2d_legacy_interface():
    old_layer = keras.layers.ConvLSTM2D(5, 3, 3, name='conv')
    new_layer = keras.layers.ConvLSTM2D(5, (3, 3), name='conv')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())

    old_layer = keras.layers.ConvLSTM2D(5, 3, nb_col=3, name='conv')
    new_layer = keras.layers.ConvLSTM2D(5, (3, 3), name='conv')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())

    old_layer = keras.layers.ConvLSTM2D(5, nb_row=3, nb_col=3, name='conv')
    new_layer = keras.layers.ConvLSTM2D(5, (3, 3), name='conv')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())

    old_layer = keras.layers.ConvLSTM2D(5, 3, 3,
                                        init='normal',
                                        inner_init='uniform',
                                        forget_bias_init='one',
                                        inner_activation='relu',
                                        subsample=(2, 2),
                                        border_mode='valid',
                                        dim_ordering='th',
                                        W_regularizer='l1',
                                        U_regularizer='l2',
                                        b_regularizer='l2',
                                        dropout_W=0.2,
                                        dropout_U=0.1,
                                        name='conv')
    new_layer = keras.layers.ConvLSTM2D(5, (3, 3),
                                        kernel_initializer='normal',
                                        recurrent_initializer='uniform',
                                        unit_forget_bias=True,
                                        recurrent_activation='relu',
                                        strides=(2, 2),
                                        padding='valid',
                                        kernel_regularizer='l1',
                                        recurrent_regularizer='l2',
                                        bias_regularizer='l2',
                                        data_format='channels_first',
                                        dropout=0.2,
                                        recurrent_dropout=0.1,
                                        name='conv')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1128')" href="javascript:;">
keras-2.3.1/tests/keras/legacy/interface_test.py: 580-618
</a>
<div class="mid" id="frag1128" style="display:none"><pre>
def test_separable_conv2d_legacy_interface():
    old_layer = keras.layers.SeparableConv2D(5, 3, 3, name='conv')
    new_layer = keras.layers.SeparableConv2D(5, (3, 3), name='conv')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())

    old_layer = keras.layers.SeparableConv2D(5, 3, nb_col=3, name='conv')
    new_layer = keras.layers.SeparableConv2D(5, (3, 3), name='conv')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())

    old_layer = keras.layers.SeparableConv2D(5, nb_row=3, nb_col=3, name='conv')
    new_layer = keras.layers.SeparableConv2D(5, (3, 3), name='conv')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())

    old_layer = keras.layers.SeparableConv2D(5, 3, 3,
                                             init='normal',
                                             subsample=(2, 2),
                                             border_mode='valid',
                                             dim_ordering='th',
                                             depthwise_regularizer='l1',
                                             b_regularizer='l2',
                                             depthwise_constraint='maxnorm',
                                             b_constraint='unitnorm',
                                             name='conv')
    new_layer = keras.layers.SeparableConv2D(5, (3, 3),
                                             depthwise_initializer='normal',
                                             pointwise_initializer='normal',
                                             strides=(2, 2),
                                             padding='valid',
                                             depthwise_regularizer='l1',
                                             bias_regularizer='l2',
                                             depthwise_constraint='max_norm',
                                             bias_constraint='unit_norm',
                                             data_format='channels_first',
                                             name='conv')
    old_config = json.dumps(old_layer.get_config())
    new_config = json.dumps(new_layer.get_config())
    assert old_config == new_config


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 38:</b> &nbsp; 2 fragments, nominal size 25 lines, similarity 74%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1127')" href="javascript:;">
keras-2.3.1/tests/keras/legacy/interface_test.py: 550-579
</a>
<div class="mid" id="frag1127" style="display:none"><pre>
def test_conv1d_legacy_interface():
    old_layer = keras.layers.Convolution1D(5,
                                           filter_length=3,
                                           input_dim=3,
                                           input_length=4,
                                           name='conv')
    new_layer = keras.layers.Conv1D(5, 3, name='conv', input_shape=(4, 3))
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())

    old_layer = keras.layers.Convolution1D(5, 3,
                                           init='normal',
                                           subsample_length=2,
                                           border_mode='valid',
                                           W_regularizer='l1',
                                           b_regularizer='l2',
                                           W_constraint='maxnorm',
                                           b_constraint='unitnorm',
                                           name='conv')
    new_layer = keras.layers.Conv1D(5, 3,
                                    kernel_initializer='normal',
                                    strides=2,
                                    padding='valid',
                                    kernel_regularizer='l1',
                                    bias_regularizer='l2',
                                    kernel_constraint='max_norm',
                                    bias_constraint='unit_norm',
                                    name='conv')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1132')" href="javascript:;">
keras-2.3.1/tests/keras/legacy/interface_test.py: 725-748
</a>
<div class="mid" id="frag1132" style="display:none"><pre>
def test_atrousconv1d_legacy_interface():
    old_layer = keras.layers.AtrousConvolution1D(5, 3,
                                                 init='normal',
                                                 subsample_length=2,
                                                 border_mode='valid',
                                                 W_regularizer='l1',
                                                 b_regularizer='l2',
                                                 W_constraint='maxnorm',
                                                 b_constraint='unitnorm',
                                                 atrous_rate=2,
                                                 name='conv')
    new_layer = keras.layers.Conv1D(5, 3,
                                    kernel_initializer='normal',
                                    strides=2,
                                    padding='valid',
                                    kernel_regularizer='l1',
                                    bias_regularizer='l2',
                                    kernel_constraint='max_norm',
                                    bias_constraint='unit_norm',
                                    dilation_rate=2,
                                    name='conv')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 39:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1143')" href="javascript:;">
keras-2.3.1/tests/keras/legacy/interface_test.py: 860-873
</a>
<div class="mid" id="frag1143" style="display:none"><pre>
def test_spatialdropout2d_legacy_interface():
    old_layer = keras.layers.SpatialDropout2D(p=0.5,
                                              dim_ordering='tf',
                                              name='sd2d')
    new_layer_1 = keras.layers.SpatialDropout2D(rate=0.5,
                                                data_format='channels_last',
                                                name='sd2d')
    new_layer_2 = keras.layers.SpatialDropout2D(0.5,
                                                data_format='channels_last',
                                                name='sd2d')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer_1.get_config())
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer_2.get_config())


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1144')" href="javascript:;">
keras-2.3.1/tests/keras/legacy/interface_test.py: 874-887
</a>
<div class="mid" id="frag1144" style="display:none"><pre>
def test_spatialdropout3d_legacy_interface():
    old_layer = keras.layers.SpatialDropout3D(p=0.5,
                                              dim_ordering='tf',
                                              name='sd3d')
    new_layer_1 = keras.layers.SpatialDropout3D(rate=0.5,
                                                data_format='channels_last',
                                                name='sd3d')
    new_layer_2 = keras.layers.SpatialDropout3D(0.5,
                                                data_format='channels_last',
                                                name='sd3d')
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer_1.get_config())
    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer_2.get_config())


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 40:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1145')" href="javascript:;">
keras-2.3.1/tests/keras/legacy/layers_test.py: 9-22
</a>
<div class="mid" id="frag1145" style="display:none"><pre>
def test_highway():
    layer_test(legacy_layers.Highway,
               kwargs={},
               input_shape=(3, 2))

    layer_test(legacy_layers.Highway,
               kwargs={'W_regularizer': regularizers.l2(0.01),
                       'b_regularizer': regularizers.l1(0.01),
                       'activity_regularizer': regularizers.l2(0.01),
                       'W_constraint': constraints.MaxNorm(1),
                       'b_constraint': constraints.MaxNorm(1)},
               input_shape=(3, 2))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1146')" href="javascript:;">
keras-2.3.1/tests/keras/legacy/layers_test.py: 23-37
</a>
<div class="mid" id="frag1146" style="display:none"><pre>
def test_maxout_dense():
    layer_test(legacy_layers.MaxoutDense,
               kwargs={'output_dim': 3},
               input_shape=(3, 2))

    layer_test(legacy_layers.MaxoutDense,
               kwargs={'output_dim': 3,
                       'W_regularizer': regularizers.l2(0.01),
                       'b_regularizer': regularizers.l1(0.01),
                       'activity_regularizer': regularizers.l2(0.01),
                       'W_constraint': constraints.MaxNorm(1),
                       'b_constraint': constraints.MaxNorm(1)},
               input_shape=(3, 2))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 41:</b> &nbsp; 4 fragments, nominal size 31 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1187')" href="javascript:;">
keras-2.3.1/tests/keras/backend/backend_test.py: 766-805
</a>
<div class="mid" id="frag1187" style="display:none"><pre>
    def test_rnn(self):
        # implement a simple RNN
        num_samples = 4
        input_dim = 5
        output_dim = 3
        timesteps = 6

        _, x = parse_shape_or_val((num_samples, timesteps, input_dim))
        _, h0 = parse_shape_or_val((num_samples, output_dim))
        _, wi = parse_shape_or_val((input_dim, output_dim))
        _, wh = parse_shape_or_val((output_dim, output_dim))
        mask = np.random.randint(2, size=(num_samples, timesteps))

        wi_k = K.variable(wi)
        wh_k = K.variable(wh)

        def get_step_function(backend, w_i, w_h):

            def simple_rnn(inputs, states):
                assert len(states) == 1
                h = states[0]
                y = backend.dot(inputs, w_i) + backend.dot(h, w_h)
                return y, [y]

            return simple_rnn

        kwargs_list = [
            {'go_backwards': False, 'mask': None},
            {'go_backwards': True, 'mask': None},
            {'go_backwards': False, 'mask': mask},
            {'go_backwards': True, 'mask': mask},
        ]
        for kwargs in kwargs_list:
            check_rnn_operation(step_function_k=get_step_function(K, wi_k, wh_k),
                                step_function_np=get_step_function(KNP, wi, wh),
                                inputs_np=x,
                                initial_states_np=[h0],
                                mask_np=kwargs.pop('mask', None),
                                **kwargs)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1190')" href="javascript:;">
keras-2.3.1/tests/keras/backend/backend_test.py: 807-841
</a>
<div class="mid" id="frag1190" style="display:none"><pre>
    def test_rnn_unroll_with_len_1(self):
        num_samples = 4
        input_dim = 5
        output_dim = 3

        _, x = parse_shape_or_val((num_samples, 1, input_dim))
        _, h0 = parse_shape_or_val((num_samples, output_dim))
        _, wi = parse_shape_or_val((input_dim, output_dim))
        _, wh = parse_shape_or_val((output_dim, output_dim))

        wi_k = K.variable(wi)
        wh_k = K.variable(wh)

        def get_step_function(backend, w_i, w_h):

            def simple_rnn(inputs, states):
                assert len(states) == 1
                h = states[0]
                y = backend.dot(inputs, w_i) + backend.dot(h, w_h)
                return y, [y]

            return simple_rnn

        kwargs_list = [
            {'go_backwards': False},
            {'go_backwards': True},
        ]
        for kwargs in kwargs_list:
            check_rnn_operation(step_function_k=get_step_function(K, wi_k, wh_k),
                                step_function_np=get_step_function(KNP, wi, wh),
                                inputs_np=x,
                                initial_states_np=[h0],
                                unroll=True,
                                **kwargs)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1193')" href="javascript:;">
keras-2.3.1/tests/keras/backend/backend_test.py: 842-883
</a>
<div class="mid" id="frag1193" style="display:none"><pre>
    def test_rnn_additional_states(self):
        # implement a simple RNN with an additional state
        # whose shape is different from that of the output
        num_samples = 4
        input_dim = 5
        output_dim = 3
        timesteps = 6

        _, x = parse_shape_or_val((num_samples, timesteps, input_dim))
        _, h0 = parse_shape_or_val((num_samples, output_dim))
        h1 = np.concatenate([h0, h0], axis=-1)
        _, wi = parse_shape_or_val((input_dim, output_dim))
        _, wh = parse_shape_or_val((output_dim, output_dim))
        mask = np.random.randint(2, size=(num_samples, timesteps))

        wi_k = K.variable(wi)
        wh_k = K.variable(wh)

        def get_step_function(backend, w_i, w_h):

            def simple_rnn_with_extra_mock_state(inputs, states):
                assert len(states) == 2
                h = states[0]
                y = backend.dot(inputs, w_i) + backend.dot(h, w_h)
                return y, [y, backend.concatenate([y, y], axis=-1)]

            return simple_rnn_with_extra_mock_state

        kwargs_list = [
            {'go_backwards': False, 'mask': None},
            {'go_backwards': True, 'mask': None},
            {'go_backwards': False, 'mask': mask},
            {'go_backwards': True, 'mask': mask},
        ]
        for kwargs in kwargs_list:
            check_rnn_operation(step_function_k=get_step_function(K, wi_k, wh_k),
                                step_function_np=get_step_function(KNP, wi, wh),
                                inputs_np=x,
                                initial_states_np=[h0, h1],
                                mask_np=kwargs.pop('mask', None),
                                **kwargs)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1199')" href="javascript:;">
keras-2.3.1/tests/keras/backend/backend_test.py: 918-959
</a>
<div class="mid" id="frag1199" style="display:none"><pre>
    def test_rnn_constants(self):
        # implement a simple RNN
        num_samples = 4
        input_dim = 5
        output_dim = 3
        timesteps = 6

        _, x = parse_shape_or_val((num_samples, timesteps, input_dim))
        _, h0 = parse_shape_or_val((num_samples, output_dim))
        _, c = parse_shape_or_val((num_samples, output_dim))
        _, wi = parse_shape_or_val((input_dim, output_dim))
        _, wh = parse_shape_or_val((output_dim, output_dim))
        mask = np.random.randint(2, size=(num_samples, timesteps))

        wi_k = K.variable(wi)
        wh_k = K.variable(wh)

        def get_step_function(backend, w_i, w_h):

            def simple_rnn_add_constant(inputs, states_and_constants):
                # constants are appended to states in K.rnn
                [h, c] = states_and_constants
                y = backend.dot(inputs, w_i) + backend.dot(h, w_h) + c
                return y, [y]

            return simple_rnn_add_constant

        kwargs_list = [
            {'go_backwards': False, 'mask': None},
            {'go_backwards': True, 'mask': None},
            {'go_backwards': False, 'mask': mask},
            {'go_backwards': True, 'mask': mask},
        ]
        for kwargs in kwargs_list:
            check_rnn_operation(step_function_k=get_step_function(K, wi_k, wh_k),
                                step_function_np=get_step_function(KNP, wi, wh),
                                inputs_np=x,
                                initial_states_np=[h0],
                                mask_np=kwargs.pop('mask', None),
                                constants_np=[c],
                                **kwargs)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 42:</b> &nbsp; 5 fragments, nominal size 11 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1217')" href="javascript:;">
keras-2.3.1/tests/keras/backend/backend_test.py: 1259-1270
</a>
<div class="mid" id="frag1217" style="display:none"><pre>
    def test_conv_transpose(self,
                            op,
                            input_shape,
                            kernel_shape,
                            output_shape,
                            padding,
                            data_format):
        check_two_tensor_operation(
            op, input_shape, kernel_shape, WITH_NP,
            output_shape=output_shape, padding=padding, data_format=data_format,
            cntk_dynamicity=True)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1219')" href="javascript:;">
keras-2.3.1/tests/keras/backend/backend_test.py: 1308-1320
</a>
<div class="mid" id="frag1219" style="display:none"><pre>
    def test_dilated_conv_transpose(self,
                                    op,
                                    input_shape,
                                    kernel_shape,
                                    output_shape,
                                    padding,
                                    data_format,
                                    dilation_rate):
        check_two_tensor_operation(
            op, input_shape, kernel_shape, WITH_NP, output_shape=output_shape,
            padding=padding, data_format=data_format, dilation_rate=dilation_rate,
            cntk_dynamicity=True)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1221')" href="javascript:;">
keras-2.3.1/tests/keras/backend/backend_test.py: 1357-1370
</a>
<div class="mid" id="frag1221" style="display:none"><pre>
    def test_pool(self,
                  op,
                  input_shape,
                  pool_size,
                  strides,
                  padding,
                  data_format,
                  pool_mode):
        check_single_tensor_operation(
            op, input_shape, WITH_NP,
            pool_size=pool_size, strides=strides,
            padding=padding, data_format=data_format, pool_mode=pool_mode,
            cntk_dynamicity=True)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1218')" href="javascript:;">
keras-2.3.1/tests/keras/backend/backend_test.py: 1286-1297
</a>
<div class="mid" id="frag1218" style="display:none"><pre>
    def test_dilated_conv(self,
                          op,
                          input_shape,
                          kernel_shape,
                          padding,
                          data_format,
                          dilation_rate):
        check_two_tensor_operation(
            op, input_shape, kernel_shape, WITH_NP,
            padding=padding, data_format=data_format,
            dilation_rate=dilation_rate, cntk_dynamicity=True)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1220')" href="javascript:;">
keras-2.3.1/tests/keras/backend/backend_test.py: 1327-1337
</a>
<div class="mid" id="frag1220" style="display:none"><pre>
    def test_depthwise_conv(self,
                            op,
                            input_shape,
                            kernel_shape,
                            padding,
                            data_format):
        check_two_tensor_operation(
            op, input_shape, kernel_shape, WITH_NP,
            padding=padding, data_format=data_format,
            cntk_dynamicity=True)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 43:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 82%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1229')" href="javascript:;">
keras-2.3.1/tests/keras/backend/backend_test.py: 1512-1530
</a>
<div class="mid" id="frag1229" style="display:none"><pre>
    def test_resize_images(self):
        for data_format in ['channels_first', 'channels_last']:
            shape = (5, 5)
            if data_format == 'channels_first':
                x_shape = (2, 3) + shape
            elif data_format == 'channels_last':
                x_shape = (2,) + shape + (3,)
            check_single_tensor_operation('resize_images', x_shape,
                                          WITH_NP, cntk_dynamicity=True,
                                          height_factor=2,
                                          width_factor=2,
                                          data_format=data_format)

        # Test invalid use cases
        xval = np.random.random(x_shape)
        with pytest.raises(ValueError):
            K.resize_images(K.variable(xval), 2, 2,
                            data_format='channels_middle')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1232')" href="javascript:;">
keras-2.3.1/tests/keras/backend/backend_test.py: 1548-1567
</a>
<div class="mid" id="frag1232" style="display:none"><pre>
    def test_resize_volumes(self):
        for data_format in ['channels_first', 'channels_last']:
            shape = (5, 5, 5)
            if data_format == 'channels_first':
                x_shape = (2, 3) + shape
            elif data_format == 'channels_last':
                x_shape = (2,) + shape + (3,)
            check_single_tensor_operation('resize_volumes', x_shape,
                                          WITH_NP, cntk_dynamicity=True,
                                          depth_factor=2,
                                          height_factor=2,
                                          width_factor=2,
                                          data_format=data_format)

        # Test invalid use cases
        xval = np.random.random(x_shape)
        with pytest.raises(ValueError):
            K.resize_volumes(K.variable(xval), 2, 2, 2,
                             data_format='channels_middle')

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 44:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 77%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1234')" href="javascript:;">
keras-2.3.1/tests/keras/backend/backend_test.py: 1574-1595
</a>
<div class="mid" id="frag1234" style="display:none"><pre>
    def test_spatial_2d_padding(self):
        padding = ((1, 2), (2, 1))
        for data_format in ['channels_first', 'channels_last']:
            shape = (5, 5)
            if data_format == 'channels_first':
                x_shape = (1, 3) + shape
            else:
                x_shape = (1,) + shape + (3,)
            check_single_tensor_operation('spatial_2d_padding', x_shape, WITH_NP,
                                          padding=padding, data_format=data_format)
        # Check handling of dynamic shapes.
        if K in [KTF, KTH]:
            x = K.placeholder(shape=(1, None, None, 1))
            y = K.spatial_2d_padding(x, padding=padding, data_format='channels_last')
            assert K.int_shape(y) == (1, None, None, 1)

        # Test invalid use cases
        xval = np.random.random(x_shape)
        with pytest.raises(ValueError):
            K.spatial_2d_padding(K.variable(xval), padding=padding,
                                 data_format='channels_middle')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1235')" href="javascript:;">
keras-2.3.1/tests/keras/backend/backend_test.py: 1596-1617
</a>
<div class="mid" id="frag1235" style="display:none"><pre>
    def test_spatial_3d_padding(self):
        padding = ((1, 2), (2, 1), (1, 2))
        for data_format in ['channels_first', 'channels_last']:
            shape = (5, 5, 5)
            if data_format == 'channels_first':
                x_shape = (1, 3) + shape
            else:
                x_shape = (1,) + shape + (3,)
            check_single_tensor_operation('spatial_3d_padding', x_shape, WITH_NP,
                                          padding=padding, data_format=data_format)
        # Check handling of dynamic shapes.
        if K in [KTF, KTH]:
            x = K.placeholder(shape=(1, None, None, None, 1))
            y = K.spatial_3d_padding(x, padding=padding, data_format='channels_last')
            assert K.int_shape(y) == (1, None, None, None, 1)

        # Test invalid use cases
        xval = np.random.random(x_shape)
        with pytest.raises(ValueError):
            K.spatial_3d_padding(K.variable(xval), padding=padding,
                                 data_format='channels_middle')

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 45:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1270')" href="javascript:;">
keras-2.3.1/tests/keras/wrappers/scikit_learn_test.py: 27-39
</a>
<div class="mid" id="frag1270" style="display:none"><pre>
def build_fn_clf(hidden_dims):
    model = Sequential()
    model.add(Dense(input_dim, input_shape=(input_dim,)))
    model.add(Activation('relu'))
    model.add(Dense(hidden_dims))
    model.add(Activation('relu'))
    model.add(Dense(num_classes))
    model.add(Activation('softmax'))
    model.compile(optimizer='sgd', loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1278')" href="javascript:;">
keras-2.3.1/tests/keras/wrappers/scikit_learn_test.py: 113-125
</a>
<div class="mid" id="frag1278" style="display:none"><pre>
def build_fn_reg(hidden_dims=50):
    model = Sequential()
    model.add(Dense(input_dim, input_shape=(input_dim,)))
    model.add(Activation('relu'))
    model.add(Dense(hidden_dims))
    model.add(Activation('relu'))
    model.add(Dense(1))
    model.add(Activation('linear'))
    model.compile(optimizer='sgd', loss='mean_absolute_error',
                  metrics=['accuracy'])
    return model


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 46:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1315')" href="javascript:;">
keras-2.3.1/tests/keras/utils/conv_utils_test.py: 34-48
</a>
<div class="mid" id="frag1315" style="display:none"><pre>
def test_conv_output_length():
    assert conv_utils.conv_output_length(None, 7, 'same', 1) is None
    assert conv_utils.conv_output_length(224, 7, 'same', 1) == 224
    assert conv_utils.conv_output_length(224, 7, 'same', 2) == 112
    assert conv_utils.conv_output_length(32, 5, 'valid', 1) == 28
    assert conv_utils.conv_output_length(32, 5, 'valid', 2) == 14
    assert conv_utils.conv_output_length(32, 5, 'causal', 1) == 32
    assert conv_utils.conv_output_length(32, 5, 'causal', 2) == 16
    assert conv_utils.conv_output_length(32, 5, 'full', 1) == 36
    assert conv_utils.conv_output_length(32, 5, 'full', 2) == 18

    with pytest.raises(AssertionError):
        conv_utils.conv_output_length(32, 5, 'diagonal', 2)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1316')" href="javascript:;">
keras-2.3.1/tests/keras/utils/conv_utils_test.py: 49-61
</a>
<div class="mid" id="frag1316" style="display:none"><pre>
def test_conv_input_length():
    assert conv_utils.conv_input_length(None, 7, 'same', 1) is None
    assert conv_utils.conv_input_length(112, 7, 'same', 1) == 112
    assert conv_utils.conv_input_length(112, 7, 'same', 2) == 223
    assert conv_utils.conv_input_length(28, 5, 'valid', 1) == 32
    assert conv_utils.conv_input_length(14, 5, 'valid', 2) == 31
    assert conv_utils.conv_input_length(36, 5, 'full', 1) == 32
    assert conv_utils.conv_input_length(18, 5, 'full', 2) == 31

    with pytest.raises(AssertionError):
        conv_utils.conv_output_length(18, 5, 'diagonal', 2)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 47:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1330')" href="javascript:;">
keras-2.3.1/tests/keras/utils/io_utils_test.py: 289-304
</a>
<div class="mid" id="frag1330" style="display:none"><pre>
def test_load_from_binary_h5py_direct_from_file():
    data = np.random.random((3, 5))

    def load_function(h5file_):
        return h5file_['data'][:]

    with temp_filename('.h5') as fname:
        with h5py.File(fname, 'w') as h5file:
            h5file['data'] = data

        with open(fname, 'rb') as f:
            data_rec = load_from_binary_h5py(load_function, f)

    assert_array_equal(data_rec, data)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1332')" href="javascript:;">
keras-2.3.1/tests/keras/utils/io_utils_test.py: 305-324
</a>
<div class="mid" id="frag1332" style="display:none"><pre>
def test_load_from_binary_h5py_from_bytes_io():
    data = np.random.random((3, 5))

    def load_function(h5file_):
        return h5file_['data'][:]

    with temp_filename('.h5') as fname:
        with h5py.File(fname, 'w') as h5file:
            h5file['data'] = data

        file_like = io.BytesIO()
        with open(fname, 'rb') as f:
            file_like.write(f.read())

    file_like.seek(0)
    data_rec = load_from_binary_h5py(load_function, file_like)

    assert_array_equal(data_rec, data)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 48:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1357')" href="javascript:;">
keras-2.3.1/tests/keras/datasets/datasets_test.py: 12-26
</a>
<div class="mid" id="frag1357" style="display:none"><pre>
def fake_downloaded_boston_path(monkeypatch):
    num_rows = 100
    num_cols = 10
    rng = np.random.RandomState(123)

    x = rng.randint(1, 100, size=(num_rows, num_cols))
    y = rng.normal(loc=100, scale=15, size=num_rows)

    with tempfile.NamedTemporaryFile('wb', delete=True) as f:
        np.savez(f, x=x, y=y)
        monkeypatch.setattr(boston_housing, 'get_file',
                            lambda *args, **kwargs: f.name)
        yield f.name


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1359')" href="javascript:;">
keras-2.3.1/tests/keras/datasets/datasets_test.py: 46-59
</a>
<div class="mid" id="frag1359" style="display:none"><pre>
def fake_downloaded_reuters_path(monkeypatch):
    num_rows = 100
    seq_length = 10
    rng = np.random.RandomState(123)

    x = rng.randint(1, 100, size=(num_rows, seq_length))
    y = rng.binomial(n=1, p=0.5, size=num_rows)

    with tempfile.NamedTemporaryFile('wb', delete=True) as f:
        np.savez(f, x=x, y=y)
        monkeypatch.setattr(reuters, 'get_file', lambda *args, **kwargs: f.name)
        yield f.name


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 49:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1363')" href="javascript:;">
keras-2.3.1/tests/keras/callbacks/tensorboard_test.py: 28-44
</a>
<div class="mid" id="frag1363" style="display:none"><pre>
def data_generator(x, y, batch_size):
    x = to_list(x)
    y = to_list(y)
    max_batch_index = len(x[0]) // batch_size
    i = 0
    while 1:
        x_batch = [array[i * batch_size: (i + 1) * batch_size] for array in x]
        x_batch = unpack_singleton(x_batch)

        y_batch = [array[i * batch_size: (i + 1) * batch_size] for array in y]
        y_batch = unpack_singleton(y_batch)
        yield x_batch, y_batch
        i += 1
        i = i % max_batch_index


# Changing the default arguments of get_test_data.
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1375')" href="javascript:;">
keras-2.3.1/tests/keras/callbacks/callbacks_test.py: 41-57
</a>
<div class="mid" id="frag1375" style="display:none"><pre>
def data_generator(x, y, batch_size):
    x = to_list(x)
    y = to_list(y)
    max_batch_index = len(x[0]) // batch_size
    i = 0
    while 1:
        x_batch = [array[i * batch_size: (i + 1) * batch_size] for array in x]
        x_batch = unpack_singleton(x_batch)

        y_batch = [array[i * batch_size: (i + 1) * batch_size] for array in y]
        y_batch = unpack_singleton(y_batch)
        yield x_batch, y_batch
        i += 1
        i = i % max_batch_index


# Changing the default arguments of get_test_data.
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 50:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1364')" href="javascript:;">
keras-2.3.1/tests/keras/callbacks/tensorboard_test.py: 45-56
</a>
<div class="mid" id="frag1364" style="display:none"><pre>
def get_data_callbacks(num_train=train_samples,
                       num_test=test_samples,
                       input_shape=(input_dim,),
                       classification=True,
                       num_classes=num_classes):
    return get_test_data(num_train=num_train,
                         num_test=num_test,
                         input_shape=input_shape,
                         classification=classification,
                         num_classes=num_classes)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1376')" href="javascript:;">
keras-2.3.1/tests/keras/callbacks/callbacks_test.py: 58-69
</a>
<div class="mid" id="frag1376" style="display:none"><pre>
def get_data_callbacks(num_train=train_samples,
                       num_test=test_samples,
                       input_shape=(input_dim,),
                       classification=True,
                       num_classes=num_classes):
    return get_test_data(num_train=num_train,
                         num_test=num_test,
                         input_shape=input_shape,
                         classification=classification,
                         num_classes=num_classes)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 51:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 88%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1369')" href="javascript:;">
keras-2.3.1/tests/keras/callbacks/tensorboard_test.py: 90-109
</a>
<div class="mid" id="frag1369" style="display:none"><pre>
    def callbacks_factory(histogram_freq=0,
                          embeddings_freq=0,
                          write_images=False,
                          write_grads=False):
        if embeddings_freq:
            embeddings_layer_names = ['dense_1']
            embeddings_data = X_test
        else:
            embeddings_layer_names = None
            embeddings_data = None
        return [callbacks.TensorBoard(log_dir=filepath,
                                      histogram_freq=histogram_freq,
                                      write_images=write_images,
                                      write_grads=write_grads,
                                      embeddings_freq=embeddings_freq,
                                      embeddings_layer_names=embeddings_layer_names,
                                      embeddings_data=embeddings_data,
                                      update_freq=update_freq)]

    # fit without validation data
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1371')" href="javascript:;">
keras-2.3.1/tests/keras/callbacks/tensorboard_test.py: 163-181
</a>
<div class="mid" id="frag1371" style="display:none"><pre>
    def callbacks_factory(histogram_freq=0,
                          embeddings_freq=0,
                          write_images=False,
                          write_grads=False):
        if embeddings_freq:
            embeddings_layer_names = ['dense_1']
            embeddings_data = [X_test] * 2
        else:
            embeddings_layer_names = None
            embeddings_data = None
        return [callbacks.TensorBoard(log_dir=filepath,
                                      histogram_freq=histogram_freq,
                                      write_images=write_images,
                                      write_grads=write_grads,
                                      embeddings_freq=embeddings_freq,
                                      embeddings_layer_names=embeddings_layer_names,
                                      embeddings_data=embeddings_data)]

    # fit without validation data
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 52:</b> &nbsp; 6 fragments, nominal size 28 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1382')" href="javascript:;">
keras-2.3.1/tests/keras/callbacks/callbacks_test.py: 121-152
</a>
<div class="mid" id="frag1382" style="display:none"><pre>

    def test_callback_hooks_are_called_in_fit(self):
        np.random.seed(1337)
        (X_train, y_train), (X_test, y_test) = get_data_callbacks(num_train=10,
                                                                  num_test=4)
        y_train = np_utils.to_categorical(y_train)
        y_test = np_utils.to_categorical(y_test)

        model = self._get_model()
        counter = Counter()
        model.fit(X_train, y_train, validation_data=(X_test, y_test),
                  batch_size=2, epochs=5, callbacks=[counter])

        self._check_counts(
            counter, {
                'on_batch_begin': 25,
                'on_batch_end': 25,
                'on_epoch_begin': 5,
                'on_epoch_end': 5,
                'on_predict_batch_begin': 0,
                'on_predict_batch_end': 0,
                'on_predict_begin': 0,
                'on_predict_end': 0,
                'on_test_batch_begin': 10,
                'on_test_batch_end': 10,
                'on_test_begin': 5,
                'on_test_end': 5,
                'on_train_batch_begin': 25,
                'on_train_batch_end': 25,
                'on_train_begin': 1,
                'on_train_end': 1,
            })
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1383')" href="javascript:;">
keras-2.3.1/tests/keras/callbacks/callbacks_test.py: 153-181
</a>
<div class="mid" id="frag1383" style="display:none"><pre>

    def test_callback_hooks_are_called_in_evaluate(self):
        np.random.seed(1337)
        (_, _), (X_test, y_test) = get_data_callbacks(num_test=10)

        y_test = np_utils.to_categorical(y_test)

        model = self._get_model()
        counter = Counter()
        model.evaluate(X_test, y_test, batch_size=2, callbacks=[counter])
        self._check_counts(
            counter, {
                'on_test_batch_begin': 5,
                'on_test_batch_end': 5,
                'on_test_begin': 1,
                'on_test_end': 1,
                'on_batch_begin': 0,
                'on_batch_end': 0,
                'on_epoch_begin': 0,
                'on_epoch_end': 0,
                'on_predict_batch_begin': 0,
                'on_predict_batch_end': 0,
                'on_predict_begin': 0,
                'on_predict_end': 0,
                'on_train_batch_begin': 0,
                'on_train_batch_end': 0,
                'on_train_begin': 0,
                'on_train_end': 0,
            })
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1389')" href="javascript:;">
keras-2.3.1/tests/keras/callbacks/callbacks_test.py: 317-347
</a>
<div class="mid" id="frag1389" style="display:none"><pre>

    def test_callback_list_methods(self):
        counter = Counter()
        callback_list = callbacks.CallbackList([counter])

        batch = 0
        callback_list.on_test_batch_begin(batch)
        callback_list.on_test_batch_end(batch)
        callback_list.on_predict_batch_begin(batch)
        callback_list.on_predict_batch_end(batch)

        self._check_counts(
            counter, {
                'on_test_batch_begin': 1,
                'on_test_batch_end': 1,
                'on_predict_batch_begin': 1,
                'on_predict_batch_end': 1,
                'on_predict_begin': 0,
                'on_predict_end': 0,
                'on_batch_begin': 0,
                'on_batch_end': 0,
                'on_epoch_begin': 0,
                'on_epoch_end': 0,
                'on_test_begin': 0,
                'on_test_end': 0,
                'on_train_batch_begin': 0,
                'on_train_batch_end': 0,
                'on_train_begin': 0,
                'on_train_end': 0,
            })

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1386')" href="javascript:;">
keras-2.3.1/tests/keras/callbacks/callbacks_test.py: 247-275
</a>
<div class="mid" id="frag1386" style="display:none"><pre>

    def test_callback_hooks_are_called_in_evaluate_generator(self):
        np.random.seed(1337)
        (_, _), (X_test, y_test) = get_data_callbacks(num_test=10)
        y_test = np_utils.to_categorical(y_test)

        model = self._get_model()
        counter = Counter()
        model.evaluate_generator(data_generator(X_test, y_test, batch_size=2),
                                 steps=len(X_test) // 2, callbacks=[counter])
        self._check_counts(
            counter, {
                'on_test_batch_begin': 5,
                'on_test_batch_end': 5,
                'on_test_begin': 1,
                'on_test_end': 1,
                'on_batch_begin': 0,
                'on_batch_end': 0,
                'on_epoch_begin': 0,
                'on_epoch_end': 0,
                'on_predict_batch_begin': 0,
                'on_predict_batch_end': 0,
                'on_predict_begin': 0,
                'on_predict_end': 0,
                'on_train_batch_begin': 0,
                'on_train_batch_end': 0,
                'on_train_begin': 0,
                'on_train_end': 0,
            })
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1385')" href="javascript:;">
keras-2.3.1/tests/keras/callbacks/callbacks_test.py: 209-246
</a>
<div class="mid" id="frag1385" style="display:none"><pre>

    def test_callback_hooks_are_called_in_fit_generator(self):
        np.random.seed(1337)
        (X_train, y_train), (X_test, y_test) = get_data_callbacks(num_train=10,
                                                                  num_test=4)
        y_train = np_utils.to_categorical(y_train)
        y_test = np_utils.to_categorical(y_test)
        train_generator = data_generator(X_train, y_train, batch_size=2)
        validation_generator = data_generator(X_test, y_test, batch_size=2)

        model = self._get_model()
        counter = Counter()
        model.fit_generator(train_generator,
                            steps_per_epoch=len(X_train) // 2,
                            epochs=5,
                            validation_data=validation_generator,
                            validation_steps=len(X_test) // 2,
                            callbacks=[counter])

        self._check_counts(
            counter, {
                'on_batch_begin': 25,
                'on_batch_end': 25,
                'on_epoch_begin': 5,
                'on_epoch_end': 5,
                'on_predict_batch_begin': 0,
                'on_predict_batch_end': 0,
                'on_predict_begin': 0,
                'on_predict_end': 0,
                'on_test_batch_begin': 10,
                'on_test_batch_end': 10,
                'on_test_begin': 5,
                'on_test_end': 5,
                'on_train_batch_begin': 25,
                'on_train_batch_end': 25,
                'on_train_begin': 1,
                'on_train_end': 1,
            })
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1384')" href="javascript:;">
keras-2.3.1/tests/keras/callbacks/callbacks_test.py: 182-208
</a>
<div class="mid" id="frag1384" style="display:none"><pre>

    def test_callback_hooks_are_called_in_predict(self):
        np.random.seed(1337)
        (_, _), (X_test, _) = get_data_callbacks(num_test=10)

        model = self._get_model()
        counter = Counter()
        model.predict(X_test, batch_size=2, callbacks=[counter])
        self._check_counts(
            counter, {
                'on_predict_batch_begin': 5,
                'on_predict_batch_end': 5,
                'on_predict_begin': 1,
                'on_predict_end': 1,
                'on_batch_begin': 0,
                'on_batch_end': 0,
                'on_epoch_begin': 0,
                'on_epoch_end': 0,
                'on_test_batch_begin': 0,
                'on_test_batch_end': 0,
                'on_test_begin': 0,
                'on_test_end': 0,
                'on_train_batch_begin': 0,
                'on_train_batch_end': 0,
                'on_train_begin': 0,
                'on_train_end': 0,
            })
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 53:</b> &nbsp; 2 fragments, nominal size 24 lines, similarity 91%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1405')" href="javascript:;">
keras-2.3.1/tests/keras/callbacks/callbacks_test.py: 635-670
</a>
<div class="mid" id="frag1405" style="display:none"><pre>

def test_EarlyStopping_final_weights():
    class DummyModel(object):
        def __init__(self):
            self.stop_training = False
            self.weights = -1

        def get_weights(self):
            return self.weights

        def set_weights(self, weights):
            self.weights = weights

        def set_weight_to_epoch(self, epoch):
            self.weights = epoch

    early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=2)
    early_stop.model = DummyModel()

    losses = [0.2, 0.15, 0.1, 0.11, 0.12]

    epochs_trained = 0
    early_stop.on_train_begin()

    for epoch in range(len(losses)):
        epochs_trained += 1
        early_stop.model.set_weight_to_epoch(epoch=epoch)
        early_stop.on_epoch_end(epoch, logs={'val_loss': losses[epoch]})

        if early_stop.model.stop_training:
            break

    # The best configuration is in the epoch 2 (loss = 0.1000),
    # so with patience=2 we need to end up at epoch 4
    assert early_stop.model.get_weights() == 4

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1410')" href="javascript:;">
keras-2.3.1/tests/keras/callbacks/callbacks_test.py: 671-710
</a>
<div class="mid" id="frag1410" style="display:none"><pre>

def test_EarlyStopping_final_weights_when_restoring_model_weights():
    class DummyModel(object):
        def __init__(self):
            self.stop_training = False
            self.weights = -1

        def get_weights(self):
            return self.weights

        def set_weights(self, weights):
            self.weights = weights

        def set_weight_to_epoch(self, epoch):
            self.weights = epoch

    early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=2,
                                         restore_best_weights=True)
    early_stop.model = DummyModel()

    losses = [0.2, 0.15, 0.1, 0.11, 0.12]

    # The best configuration is in the epoch 2 (loss = 0.1000).

    epochs_trained = 0
    early_stop.on_train_begin()

    for epoch in range(len(losses)):
        epochs_trained += 1
        early_stop.model.set_weight_to_epoch(epoch=epoch)
        early_stop.on_epoch_end(epoch, logs={'val_loss': losses[epoch]})

        if early_stop.model.stop_training:
            break

    # The best configuration is in epoch 2 (loss = 0.1000),
    # and while patience = 2, we're restoring the best weights,
    # so we end up at the epoch with the best weights, i.e. epoch 2
    assert early_stop.model.get_weights() == 2

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 54:</b> &nbsp; 3 fragments, nominal size 15 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1415')" href="javascript:;">
keras-2.3.1/tests/keras/callbacks/callbacks_test.py: 711-728
</a>
<div class="mid" id="frag1415" style="display:none"><pre>

def test_LearningRateScheduler():
    np.random.seed(1337)
    (X_train, y_train), (X_test, y_test) = get_data_callbacks()
    y_test = np_utils.to_categorical(y_test)
    y_train = np_utils.to_categorical(y_train)
    model = Sequential()
    model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(loss='categorical_crossentropy',
                  optimizer='sgd',
                  metrics=['accuracy'])

    cbks = [callbacks.LearningRateScheduler(lambda x: 1. / (1. + x))]
    model.fit(X_train, y_train, batch_size=batch_size,
              validation_data=(X_test, y_test), callbacks=cbks, epochs=5)
    assert (float(K.get_value(model.optimizer.lr)) - 0.2) &lt; K.epsilon()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1428')" href="javascript:;">
keras-2.3.1/tests/keras/callbacks/callbacks_test.py: 960-976
</a>
<div class="mid" id="frag1428" style="display:none"><pre>

def tests_RemoteMonitor():
    (X_train, y_train), (X_test, y_test) = get_data_callbacks()
    y_test = np_utils.to_categorical(y_test)
    y_train = np_utils.to_categorical(y_train)
    model = Sequential()
    model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(loss='categorical_crossentropy',
                  optimizer='rmsprop',
                  metrics=['accuracy'])
    cbks = [callbacks.RemoteMonitor()]

    with patch('requests.post'):
        model.fit(X_train, y_train, batch_size=batch_size,
                  validation_data=(X_test, y_test), callbacks=cbks, epochs=1)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1429')" href="javascript:;">
keras-2.3.1/tests/keras/callbacks/callbacks_test.py: 977-993
</a>
<div class="mid" id="frag1429" style="display:none"><pre>

def tests_RemoteMonitorWithJsonPayload():
    (X_train, y_train), (X_test, y_test) = get_data_callbacks()
    y_test = np_utils.to_categorical(y_test)
    y_train = np_utils.to_categorical(y_train)
    model = Sequential()
    model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(loss='categorical_crossentropy',
                  optimizer='rmsprop',
                  metrics=['accuracy'])
    cbks = [callbacks.RemoteMonitor(send_as_json=True)]

    with patch('requests.post'):
        model.fit(X_train, y_train, batch_size=batch_size,
                  validation_data=(X_test, y_test), callbacks=cbks, epochs=1)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 55:</b> &nbsp; 3 fragments, nominal size 15 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1438')" href="javascript:;">
keras-2.3.1/tests/keras/metrics_test.py: 243-264
</a>
<div class="mid" id="frag1438" style="display:none"><pre>
    def test_categorical_accuracy(self):
        acc_obj = metrics.CategoricalAccuracy(name='my_acc')

        # check config
        assert acc_obj.name == 'my_acc'
        assert acc_obj.stateful
        assert len(acc_obj.weights) == 2
        assert acc_obj.dtype == 'float32'

        # verify that correct value is returned
        result_t = acc_obj([[0, 0, 1], [0, 1, 0]],
                           [[0.1, 0.1, 0.8], [0.05, 0.95, 0]])
        result = K.eval(result_t)
        assert result == 1  # 2/2

        # check with sample_weight
        result_t = acc_obj([[0, 0, 1], [0, 1, 0]],
                           [[0.1, 0.1, 0.8], [0.05, 0, 0.95]],
                           [[0.5], [0.2]])
        result = K.eval(result_t)
        assert np.isclose(result, 2.5 / 2.7, atol=1e-3)  # 2.5/2.7

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1439')" href="javascript:;">
keras-2.3.1/tests/keras/metrics_test.py: 265-287
</a>
<div class="mid" id="frag1439" style="display:none"><pre>
    def test_sparse_categorical_accuracy(self):
        acc_obj = metrics.SparseCategoricalAccuracy(name='my_acc')

        # check config
        assert acc_obj.name == 'my_acc'
        assert acc_obj.stateful
        assert len(acc_obj.weights) == 2
        assert acc_obj.dtype == 'float32'

        # verify that correct value is returned
        result_t = acc_obj([[2], [1]],
                           [[0.1, 0.1, 0.8],
                           [0.05, 0.95, 0]])
        result = K.eval(result_t)
        assert result == 1  # 2/2

        # check with sample_weight
        result_t = acc_obj([[2], [1]],
                           [[0.1, 0.1, 0.8], [0.05, 0, 0.95]],
                           [[0.5], [0.2]])
        result = K.eval(result_t)
        assert np.isclose(result, 2.5 / 2.7, atol=1e-3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1440')" href="javascript:;">
keras-2.3.1/tests/keras/metrics_test.py: 288-308
</a>
<div class="mid" id="frag1440" style="display:none"><pre>
    def test_sparse_categorical_accuracy_mismatched_dims(self):
        acc_obj = metrics.SparseCategoricalAccuracy(name='my_acc')

        # check config
        assert acc_obj.name == 'my_acc'
        assert acc_obj.stateful
        assert len(acc_obj.weights) == 2
        assert acc_obj.dtype == 'float32'

        # verify that correct value is returned
        result_t = acc_obj([2, 1], [[0.1, 0.1, 0.8], [0.05, 0.95, 0]])
        result = K.eval(result_t)
        assert result == 1  # 2/2

        # check with sample_weight
        result_t = acc_obj([2, 1], [[0.1, 0.1, 0.8], [0.05, 0, 0.95]],
                           [[0.5], [0.2]])
        result = K.eval(result_t)
        assert np.isclose(result, 2.5 / 2.7, atol=1e-3)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 56:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 86%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1454')" href="javascript:;">
keras-2.3.1/tests/keras/metrics_test.py: 453-473
</a>
<div class="mid" id="frag1454" style="display:none"><pre>
    def test_correctness(self):
        a_obj = metrics.TopKCategoricalAccuracy()
        y_true = [[0, 0, 1], [0, 1, 0]]
        y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]

        result = a_obj(y_true, y_pred)
        assert 1 == K.eval(result)  # both the samples match

        # With `k` &lt; 5.
        a_obj = metrics.TopKCategoricalAccuracy(k=1)
        result = a_obj(y_true, y_pred)
        assert 0.5 == K.eval(result)  # only sample #2 matches

        # With `k` &gt; 5.
        y_true = ([[0, 0, 1, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0]])
        y_pred = [[0.5, 0.9, 0.1, 0.7, 0.6, 0.5, 0.4],
                  [0.05, 0.95, 0, 0, 0, 0, 0]]
        a_obj = metrics.TopKCategoricalAccuracy(k=6)
        result = a_obj(y_true, y_pred)
        assert 0.5 == K.eval(result)  # only 1 sample matches.

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1457')" href="javascript:;">
keras-2.3.1/tests/keras/metrics_test.py: 496-515
</a>
<div class="mid" id="frag1457" style="display:none"><pre>
    def test_correctness(self):
        a_obj = metrics.SparseTopKCategoricalAccuracy()
        y_true = [2, 1]
        y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]

        result = a_obj(y_true, y_pred)
        assert 1 == K.eval(result)  # both the samples match

        # With `k` &lt; 5.
        a_obj = metrics.SparseTopKCategoricalAccuracy(k=1)
        result = a_obj(y_true, y_pred)
        assert 0.5 == K.eval(result)  # only sample #2 matches

        # With `k` &gt; 5.
        y_pred = [[0.5, 0.9, 0.1, 0.7, 0.6, 0.5, 0.4],
                  [0.05, 0.95, 0, 0, 0, 0, 0]]
        a_obj = metrics.SparseTopKCategoricalAccuracy(k=6)
        result = a_obj(y_true, y_pred)
        assert 0.5 == K.eval(result)  # only 1 sample matches.

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 57:</b> &nbsp; 8 fragments, nominal size 11 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1566')" href="javascript:;">
keras-2.3.1/tests/keras/losses_test.py: 620-632
</a>
<div class="mid" id="frag1566" style="display:none"><pre>
    def test_all_correct_unweighted(self):
        y_true = K.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]])
        y_pred = K.constant([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])
        cce_obj = losses.CategoricalCrossentropy()
        loss = cce_obj(y_true, y_pred)
        assert np.isclose(K.eval(loss), 0.0, atol=1e-3)

        # Test with logits.
        logits = K.constant([[10., 0., 0.], [0., 10., 0.], [0., 0., 10.]])
        cce_obj = losses.CategoricalCrossentropy(from_logits=True)
        loss = cce_obj(y_true, logits)
        assert np.isclose(K.eval(loss), 0.0, atol=1e-3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1567')" href="javascript:;">
keras-2.3.1/tests/keras/losses_test.py: 633-646
</a>
<div class="mid" id="frag1567" style="display:none"><pre>
    def test_unweighted(self):
        cce_obj = losses.CategoricalCrossentropy()
        y_true = K.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]])
        y_pred = K.constant(
            [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]])
        loss = cce_obj(y_true, y_pred)
        assert np.isclose(K.eval(loss), .3239, atol=1e-3)

        # Test with logits.
        logits = K.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])
        cce_obj = losses.CategoricalCrossentropy(from_logits=True)
        loss = cce_obj(y_true, logits)
        assert np.isclose(K.eval(loss), .05737, atol=1e-3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1574')" href="javascript:;">
keras-2.3.1/tests/keras/losses_test.py: 729-742
</a>
<div class="mid" id="frag1574" style="display:none"><pre>
    def test_unweighted(self):
        cce_obj = losses.SparseCategoricalCrossentropy()
        y_true = K.constant([0, 1, 2])
        y_pred = K.constant(
            [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]])
        loss = cce_obj(y_true, y_pred)
        assert np.isclose(K.eval(loss), .3239, atol=1e-3)

        # Test with logits.
        logits = K.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])
        cce_obj = losses.SparseCategoricalCrossentropy(from_logits=True)
        loss = cce_obj(y_true, logits)
        assert np.isclose(K.eval(loss), .0573, atol=1e-3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1573')" href="javascript:;">
keras-2.3.1/tests/keras/losses_test.py: 716-728
</a>
<div class="mid" id="frag1573" style="display:none"><pre>
    def test_all_correct_unweighted(self):
        y_true = K.constant([[0], [1], [2]])
        y_pred = K.constant([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])
        cce_obj = losses.SparseCategoricalCrossentropy()
        loss = cce_obj(y_true, y_pred)
        assert np.isclose(K.eval(loss), 0.0, atol=1e-3)

        # Test with logits.
        logits = K.constant([[10., 0., 0.], [0., 10., 0.], [0., 0., 10.]])
        cce_obj = losses.SparseCategoricalCrossentropy(from_logits=True)
        loss = cce_obj(y_true, logits)
        assert np.isclose(K.eval(loss), 0.0, atol=1e-3)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1575')" href="javascript:;">
keras-2.3.1/tests/keras/losses_test.py: 743-756
</a>
<div class="mid" id="frag1575" style="display:none"><pre>
    def test_scalar_weighted(self):
        cce_obj = losses.SparseCategoricalCrossentropy()
        y_true = K.constant([[0], [1], [2]])
        y_pred = K.constant(
            [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]])
        loss = cce_obj(y_true, y_pred, sample_weight=2.3)
        assert np.isclose(K.eval(loss), .7449, atol=1e-3)

        # Test with logits.
        logits = K.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])
        cce_obj = losses.SparseCategoricalCrossentropy(from_logits=True)
        loss = cce_obj(y_true, logits, sample_weight=2.3)
        assert np.isclose(K.eval(loss), .1317, atol=1e-3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1568')" href="javascript:;">
keras-2.3.1/tests/keras/losses_test.py: 647-660
</a>
<div class="mid" id="frag1568" style="display:none"><pre>
    def test_scalar_weighted(self):
        cce_obj = losses.CategoricalCrossentropy()
        y_true = K.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]])
        y_pred = K.constant(
            [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]])
        loss = cce_obj(y_true, y_pred, sample_weight=2.3)
        assert np.isclose(K.eval(loss), .7449, atol=1e-3)

        # Test with logits.
        logits = K.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])
        cce_obj = losses.CategoricalCrossentropy(from_logits=True)
        loss = cce_obj(y_true, logits, sample_weight=2.3)
        assert np.isclose(K.eval(loss), .132, atol=1e-3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1569')" href="javascript:;">
keras-2.3.1/tests/keras/losses_test.py: 661-675
</a>
<div class="mid" id="frag1569" style="display:none"><pre>
    def test_sample_weighted(self):
        cce_obj = losses.CategoricalCrossentropy()
        y_true = K.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]])
        y_pred = K.constant(
            [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]])
        sample_weight = K.constant([[1.2], [3.4], [5.6]], shape=(3, 1))
        loss = cce_obj(y_true, y_pred, sample_weight=sample_weight)
        assert np.isclose(K.eval(loss), 1.0696, atol=1e-3)

        # Test with logits.
        logits = K.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])
        cce_obj = losses.CategoricalCrossentropy(from_logits=True)
        loss = cce_obj(y_true, logits, sample_weight=sample_weight)
        assert np.isclose(K.eval(loss), 0.31829, atol=1e-3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1576')" href="javascript:;">
keras-2.3.1/tests/keras/losses_test.py: 757-771
</a>
<div class="mid" id="frag1576" style="display:none"><pre>
    def test_sample_weighted(self):
        cce_obj = losses.SparseCategoricalCrossentropy()
        y_true = K.constant([[0], [1], [2]])
        y_pred = K.constant(
            [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]])
        sample_weight = K.constant([[1.2], [3.4], [5.6]], shape=(3, 1))
        loss = cce_obj(y_true, y_pred, sample_weight=sample_weight)
        assert np.isclose(K.eval(loss), 1.0696, atol=1e-3)

        # Test with logits.
        logits = K.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])
        cce_obj = losses.SparseCategoricalCrossentropy(from_logits=True)
        loss = cce_obj(y_true, logits, sample_weight=sample_weight)
        assert np.isclose(K.eval(loss), 0.31829, atol=1e-3)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 58:</b> &nbsp; 2 fragments, nominal size 58 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1674')" href="javascript:;">
keras-2.3.1/tests/keras/engine/test_training.py: 636-699
</a>
<div class="mid" id="frag1674" style="display:none"><pre>
def test_fit_generator_dynamic_size_sequence_with_workers():
    model = get_model(num_outputs=2)
    optimizer = 'rmsprop'
    loss = 'mse'
    loss_weights = [1., 0.5]

    model.compile(optimizer, loss, metrics=[], loss_weights=loss_weights,
                  sample_weight_mode=None)
    tracker_cb = TrackerCallback()
    val_seq = RandomSequence(4)
    train_seq = IncreaseBatchSizeRandomSequence(3, 20)
    out = model.fit_generator(generator=train_seq,
                              epochs=5,
                              initial_epoch=0,
                              validation_data=val_seq,
                              validation_steps=3,
                              max_queue_size=1,
                              callbacks=[tracker_cb])
    assert tracker_cb.trained_epochs == [0, 1, 2, 3, 4]
    assert tracker_cb.trained_batches == [
        0, 1, 2, 3, 4, 5, 6,  # 1st epoch -&gt; ceil(20 / 3) = 7 batches
        0, 1, 2, 3,           # 2nd epoch -&gt; ceil(20 / 5) = 4 batches
        0, 1, 2,              # 3d  epoch -&gt; ceil(20 / 7) = 3 batches
        0, 1, 2,              # 4th epoch -&gt; ceil(20 / 9) = 3 batches
        0, 1,                 # 5th epoch -&gt; ceil(20 /11) = 2 batches
    ]
    assert tracker_cb.steps_per_epoch_log[0:5] == [7, 4, 3, 3, 2]

    tracker_cb = TrackerCallback()
    val_seq = RandomSequence(4)
    train_seq = IncreaseBatchSizeRandomSequence(3, 30)
    out = model.fit_generator(generator=train_seq,
                              epochs=5,
                              initial_epoch=0,
                              validation_data=val_seq,
                              validation_steps=3,
                              max_queue_size=1,
                              callbacks=[tracker_cb])
    assert tracker_cb.trained_epochs == [0, 1, 2, 3, 4]
    assert tracker_cb.trained_batches == [
        0, 1, 2, 3, 4, 5, 6, 7, 8, 9,  # 1st epoch -&gt; ceil(30 / 3) = 10 batches
        0, 1, 2, 3, 4, 5,              # 2nd epoch -&gt; ceil(30 / 5) =  6 batches
        0, 1, 2, 3, 4,                 # 3d  epoch -&gt; ceil(30 / 7) =  5 batches
        0, 1, 2, 3,                    # 4th epoch -&gt; ceil(30 / 9) =  4 batches
        0, 1, 2,                       # 5th epoch -&gt; ceil(30 /11) =  3 batches
    ]
    assert tracker_cb.steps_per_epoch_log[0:5] == [10, 6, 5, 4, 3]

    tracker_cb = TrackerCallback()
    val_seq = RandomSequence(4)
    train_seq = IncreaseBatchSizeRandomSequence(2, 404, lambda x: x * 2)
    out = model.fit_generator(generator=train_seq,
                              epochs=5,
                              initial_epoch=0,
                              validation_data=val_seq,
                              validation_steps=3,
                              max_queue_size=1,
                              callbacks=[tracker_cb])
    assert tracker_cb.trained_epochs == [0, 1, 2, 3, 4]
    # number of trained batches should match sum of steps per each epoch
    assert len(tracker_cb.trained_batches) == 202 + 101 + 51 + 26 + 13
    assert tracker_cb.steps_per_epoch_log[0:5] == [202, 101, 51, 26, 13]


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1675')" href="javascript:;">
keras-2.3.1/tests/keras/engine/test_training.py: 700-763
</a>
<div class="mid" id="frag1675" style="display:none"><pre>
def test_fit_generator_dynamic_size_sequence_main_thread():
    model = get_model(num_outputs=2)
    optimizer = 'rmsprop'
    loss = 'mse'
    loss_weights = [1., 0.5]

    model.compile(optimizer, loss, metrics=[], loss_weights=loss_weights,
                  sample_weight_mode=None)
    tracker_cb = TrackerCallback()
    val_seq = RandomSequence(4)
    train_seq = IncreaseBatchSizeRandomSequence(3, 20)
    out = model.fit_generator(generator=train_seq,
                              epochs=5,
                              initial_epoch=0,
                              validation_data=val_seq,
                              validation_steps=3,
                              workers=0,
                              callbacks=[tracker_cb])
    assert tracker_cb.trained_epochs == [0, 1, 2, 3, 4]
    assert tracker_cb.trained_batches == [
        0, 1, 2, 3, 4, 5, 6,  # 1st epoch -&gt; ceil(20 / 3) = 7 batches
        0, 1, 2, 3,           # 2nd epoch -&gt; ceil(20 / 5) = 4 batches
        0, 1, 2,              # 3d  epoch -&gt; ceil(20 / 7) = 3 batches
        0, 1, 2,              # 4th epoch -&gt; ceil(20 / 9) = 3 batches
        0, 1,                 # 5th epoch -&gt; ceil(20 /11) = 2 batches
    ]
    assert tracker_cb.steps_per_epoch_log[0:5] == [7, 4, 3, 3, 2]

    tracker_cb = TrackerCallback()
    val_seq = RandomSequence(4)
    train_seq = IncreaseBatchSizeRandomSequence(3, 30)
    out = model.fit_generator(generator=train_seq,
                              epochs=5,
                              initial_epoch=0,
                              validation_data=val_seq,
                              validation_steps=3,
                              workers=0,
                              callbacks=[tracker_cb])
    assert tracker_cb.trained_epochs == [0, 1, 2, 3, 4]
    assert tracker_cb.trained_batches == [
        0, 1, 2, 3, 4, 5, 6, 7, 8, 9,  # 1st epoch -&gt; ceil(30 / 3) = 10 batches
        0, 1, 2, 3, 4, 5,              # 2nd epoch -&gt; ceil(30 / 5) =  6 batches
        0, 1, 2, 3, 4,                 # 3d  epoch -&gt; ceil(30 / 7) =  5 batches
        0, 1, 2, 3,                    # 4th epoch -&gt; ceil(30 / 9) =  4 batches
        0, 1, 2,                       # 5th epoch -&gt; ceil(30 /11) =  3 batches
    ]
    assert tracker_cb.steps_per_epoch_log[0:5] == [10, 6, 5, 4, 3]

    tracker_cb = TrackerCallback()
    val_seq = RandomSequence(4)
    train_seq = IncreaseBatchSizeRandomSequence(2, 404, lambda x: x * 2)
    out = model.fit_generator(generator=train_seq,
                              epochs=5,
                              initial_epoch=0,
                              validation_data=val_seq,
                              validation_steps=3,
                              workers=0,
                              callbacks=[tracker_cb])
    assert tracker_cb.trained_epochs == [0, 1, 2, 3, 4]
    # number of trained batches should match sum of steps per each epoch
    assert len(tracker_cb.trained_batches) == 202 + 101 + 51 + 26 + 13
    assert tracker_cb.steps_per_epoch_log[0:5] == [202, 101, 51, 26, 13]


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 59:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 92%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1681')" href="javascript:;">
keras-2.3.1/tests/keras/engine/test_training.py: 901-915
</a>
<div class="mid" id="frag1681" style="display:none"><pre>
def test_sparse_inputs_targets():
    test_inputs = [sparse.random(6, 3, density=0.25).tocsr() for _ in range(2)]
    test_outputs = [sparse.random(6, i, density=0.25).tocsr() for i in range(3, 5)]
    in1 = Input(shape=(3,))
    in2 = Input(shape=(3,))
    out1 = Dropout(0.5, name='dropout')(in1)
    out2 = Dense(4, name='dense_1')(in2)
    model = Model([in1, in2], [out1, out2])
    model.predict(test_inputs, batch_size=2)
    model.compile('rmsprop', 'mse')
    model.fit(test_inputs, test_outputs,
              epochs=1, batch_size=2, validation_split=0.5)
    model.evaluate(test_inputs, test_outputs, batch_size=2)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1682')" href="javascript:;">
keras-2.3.1/tests/keras/engine/test_training.py: 918-933
</a>
<div class="mid" id="frag1682" style="display:none"><pre>
def DISABLED_test_sparse_placeholder_fit():
    """Must wait for tf.keras to support sparse operations."""
    test_inputs = [sparse.random(6, 3, density=0.25).tocsr() for _ in range(2)]
    test_outputs = [sparse.random(6, i, density=0.25).tocsr() for i in range(3, 5)]
    in1 = Input(shape=(3,))
    in2 = Input(shape=(3,), sparse=True)
    out1 = Dropout(0.5, name='dropout')(in1)
    out2 = Dense(4, name='dense_1')(in2)
    model = Model([in1, in2], [out1, out2])
    model.predict(test_inputs, batch_size=2)
    model.compile('rmsprop', 'mse')
    model.fit(test_inputs, test_outputs,
              epochs=1, batch_size=2, validation_split=0.5)
    model.evaluate(test_inputs, test_outputs, batch_size=2)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 60:</b> &nbsp; 2 fragments, nominal size 25 lines, similarity 88%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1721')" href="javascript:;">
keras-2.3.1/tests/keras/engine/test_training.py: 2044-2079
</a>
<div class="mid" id="frag1721" style="display:none"><pre>
def test_add_metric_in_model_call():

    class TestModel(Model):

        def __init__(self):
            super(TestModel, self).__init__(name='test_model')
            self.dense1 = keras.layers.Dense(2, kernel_initializer='ones')
            self.mean = metrics.Mean(name='metric_1')

        def call(self, x):
            self.add_metric(K.sum(x), name='metric_2')
            # Provide same name as in the instance created in __init__
            # for eager mode
            self.add_metric(self.mean(x), name='metric_1')
            return self.dense1(x)

    model = TestModel()
    model.compile(loss='mse', optimizer='sgd')

    x = np.ones(shape=(10, 1))
    y = np.ones(shape=(10, 2))
    history = model.fit(x, y, epochs=2, batch_size=5, validation_data=(x, y))
    assert np.isclose(history.history['metric_1'][-1], 1, 0)
    assert np.isclose(history.history['val_metric_1'][-1], 1, 0)
    assert np.isclose(history.history['metric_2'][-1], 5, 0)
    assert np.isclose(history.history['val_metric_2'][-1], 5, 0)

    eval_results = model.evaluate(x, y, batch_size=5)
    assert np.isclose(eval_results[1], 1, 0)
    assert np.isclose(eval_results[2], 5, 0)

    model.predict(x, batch_size=5)
    model.train_on_batch(x, y)
    model.test_on_batch(x, y)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1724')" href="javascript:;">
keras-2.3.1/tests/keras/engine/test_training.py: 2080-2113
</a>
<div class="mid" id="frag1724" style="display:none"><pre>
def test_multiple_add_metric_calls():

    class TestModel(Model):

        def __init__(self):
            super(TestModel, self).__init__(name='test_model')
            self.dense1 = keras.layers.Dense(2, kernel_initializer='ones')
            self.mean1 = metrics.Mean(name='metric_1')
            self.mean2 = metrics.Mean(name='metric_2')

        def call(self, x):
            self.add_metric(self.mean2(x), name='metric_2')
            self.add_metric(self.mean1(x), name='metric_1')
            self.add_metric(K.sum(x), name='metric_3')
            return self.dense1(x)

    model = TestModel()
    model.compile(loss='mse', optimizer='sgd')

    x = np.ones(shape=(10, 1))
    y = np.ones(shape=(10, 2))
    history = model.fit(x, y, epochs=2, batch_size=5, validation_data=(x, y))
    assert np.isclose(history.history['metric_1'][-1], 1, 0)
    assert np.isclose(history.history['metric_2'][-1], 1, 0)
    assert np.isclose(history.history['metric_3'][-1], 5, 0)

    eval_results = model.evaluate(x, y, batch_size=5)
    assert np.allclose(eval_results[1:4], [1, 1, 5], 0.1)

    model.predict(x, batch_size=5)
    model.train_on_batch(x, y)
    model.test_on_batch(x, y)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 61:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1747')" href="javascript:;">
keras-2.3.1/tests/keras/activations_test.py: 54-70
</a>
<div class="mid" id="frag1747" style="display:none"><pre>
def test_softmax_valid():
    """Test using a reference implementation of softmax.
    """
    def softmax(values):
        m = np.max(values)
        e = np.exp(values - m)
        return e / np.sum(e)

    x = K.placeholder(ndim=2)
    f = K.function([x], [activations.softmax(x)])
    test_values = get_standard_values()

    result = f([test_values])[0]
    expected = softmax(test_values)
    assert_allclose(result, expected, rtol=1e-05)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1759')" href="javascript:;">
keras-2.3.1/tests/keras/activations_test.py: 157-174
</a>
<div class="mid" id="frag1759" style="display:none"><pre>
def test_hard_sigmoid():
    """Test using a reference hard sigmoid implementation.
    """
    def ref_hard_sigmoid(x):
        x = (x * 0.2) + 0.5
        z = 0.0 if x &lt;= 0 else (1.0 if x &gt;= 1 else x)
        return z
    hard_sigmoid = np.vectorize(ref_hard_sigmoid)

    x = K.placeholder(ndim=2)
    f = K.function([x], [activations.hard_sigmoid(x)])
    test_values = get_standard_values()

    result = f([test_values])[0]
    expected = hard_sigmoid(test_values)
    assert_allclose(result, expected, rtol=1e-05)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 62:</b> &nbsp; 2 fragments, nominal size 19 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1771')" href="javascript:;">
keras-2.3.1/tests/keras/metrics_correctness_test.py: 168-190
</a>
<div class="mid" id="frag1771" style="display:none"><pre>
    def test_fit_with_sample_weight(self):
        self.setUp()
        model = self._get_compiled_multi_io_model()
        history = model.fit([self.x, self.x], [self.y1, self.y2],
                            sample_weight={
                                'output_1': self.sample_weight_1,
                                'output_2': self.sample_weight_2},
                            batch_size=2,
                            epochs=2,
                            shuffle=False)
        for key, value in self.expected_fit_result_with_weights.items():
            np.allclose(history.history[key], value, 1e-3)

        # Set weights for one output (use batch size).
        history = model.fit([self.x, self.x], [self.y1, self.y2],
                            sample_weight={'output_2': self.sample_weight_2},
                            batch_size=2,
                            epochs=2,
                            shuffle=False)

        for key, value in self.expected_fit_result_with_weights_output_2.items():
            np.allclose(history.history[key], value, 1e-3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1772')" href="javascript:;">
keras-2.3.1/tests/keras/metrics_correctness_test.py: 191-213
</a>
<div class="mid" id="frag1772" style="display:none"><pre>
    def test_fit_with_class_weight(self):
        self.setUp()
        model = self._get_compiled_multi_io_model()
        history = model.fit([self.x, self.x], [self.y1, self.y2],
                            class_weight={
                                'output_1': self.class_weight_1,
                                'output_2': self.class_weight_2},
                            batch_size=2,
                            epochs=2,
                            shuffle=False)
        for key, value in self.expected_fit_result_with_weights.items():
            np.allclose(history.history[key], value, 1e-3)

        # Set weights for one output.
        history = model.fit([self.x, self.x], [self.y1, self.y2],
                            class_weight={'output_2': self.class_weight_2},
                            batch_size=2,
                            epochs=2,
                            shuffle=False)

        for key, value in self.expected_fit_result_with_weights_output_2.items():
            np.allclose(history.history[key], value, 1e-3)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 63:</b> &nbsp; 3 fragments, nominal size 13 lines, similarity 84%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1776')" href="javascript:;">
keras-2.3.1/tests/keras/metrics_correctness_test.py: 256-270
</a>
<div class="mid" id="frag1776" style="display:none"><pre>
    def test_train_on_batch_with_sample_weight(self):
        self.setUp()
        model = self._get_compiled_multi_io_model()
        result = model.train_on_batch([self.x, self.x], [self.y1, self.y2],
                                      sample_weight={
                                          'output_1': self.sample_weight_1,
                                          'output_2': self.sample_weight_2})
        np.allclose(result, self.expected_batch_result_with_weights, 1e-3)

        # Set weights for one output.
        result = model.train_on_batch([self.x, self.x], [self.y1, self.y2],
                                      sample_weight={
                                          'output_2': self.sample_weight_2})
        np.allclose(result, self.expected_batch_result_with_weights_output_2, 1e-3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1779')" href="javascript:;">
keras-2.3.1/tests/keras/metrics_correctness_test.py: 293-308
</a>
<div class="mid" id="frag1779" style="display:none"><pre>
    def test_test_on_batch_with_sample_weight(self):
        self.setUp()
        model = self._get_compiled_multi_io_model()
        result = model.test_on_batch([self.x, self.x], [self.y1, self.y2],
                                     sample_weight={
                                         'output_1': self.sample_weight_1,
                                         'output_2': self.sample_weight_2})
        np.allclose(result, self.expected_batch_result_with_weights, 1e-3)

        # Set weights for one output.
        result = model.test_on_batch([self.x, self.x], [self.y1, self.y2],
                                     sample_weight={
                                         'output_2': self.sample_weight_2})
        np.allclose(result,
                    self.expected_batch_result_with_weights_output_2, 1e-3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1777')" href="javascript:;">
keras-2.3.1/tests/keras/metrics_correctness_test.py: 271-286
</a>
<div class="mid" id="frag1777" style="display:none"><pre>
    def test_train_on_batch_with_class_weight(self):
        self.setUp()
        model = self._get_compiled_multi_io_model()
        result = model.train_on_batch([self.x, self.x], [self.y1, self.y2],
                                      class_weight={
                                          'output_1': self.class_weight_1,
                                          'output_2': self.class_weight_2})
        np.allclose(result, self.expected_batch_result_with_weights, 1e-3)

        # Set weights for one output.
        result = model.train_on_batch([self.x, self.x], [self.y1, self.y2],
                                      class_weight={
                                          'output_2': self.class_weight_2})
        np.allclose(result,
                    self.expected_batch_result_with_weights_output_2, 1e-3)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 64:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1806')" href="javascript:;">
keras-2.3.1/tests/keras/metrics_confusion_matrix_test.py: 251-265
</a>
<div class="mid" id="frag1806" style="display:none"><pre>
    def test_config(self):
        s_obj = metrics.SensitivityAtSpecificity(
            0.4, num_thresholds=100, name='sensitivity_at_specificity_1')
        assert s_obj.name == 'sensitivity_at_specificity_1'
        assert len(s_obj.weights) == 4
        assert s_obj.specificity == 0.4
        assert s_obj.num_thresholds == 100

        # Check save and restore config
        s_obj2 = metrics.SensitivityAtSpecificity.from_config(s_obj.get_config())
        assert s_obj2.name == 'sensitivity_at_specificity_1'
        assert len(s_obj2.weights) == 4
        assert s_obj2.specificity == 0.4
        assert s_obj2.num_thresholds == 100

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1813')" href="javascript:;">
keras-2.3.1/tests/keras/metrics_confusion_matrix_test.py: 317-331
</a>
<div class="mid" id="frag1813" style="display:none"><pre>
    def test_config(self):
        s_obj = metrics.SpecificityAtSensitivity(
            0.4, num_thresholds=100, name='specificity_at_sensitivity_1')
        assert s_obj.name == 'specificity_at_sensitivity_1'
        assert len(s_obj.weights) == 4
        assert s_obj.sensitivity == 0.4
        assert s_obj.num_thresholds == 100

        # Check save and restore config
        s_obj2 = metrics.SpecificityAtSensitivity.from_config(s_obj.get_config())
        assert s_obj2.name == 'specificity_at_sensitivity_1'
        assert len(s_obj2.weights) == 4
        assert s_obj2.sensitivity == 0.4
        assert s_obj2.num_thresholds == 100

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 65:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1810')" href="javascript:;">
keras-2.3.1/tests/keras/metrics_confusion_matrix_test.py: 294-305
</a>
<div class="mid" id="frag1810" style="display:none"><pre>
    def test_weighted(self):
        s_obj = metrics.SensitivityAtSpecificity(0.4)
        pred_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.01, 0.02, 0.25, 0.26, 0.26]
        label_values = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]
        weight_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

        y_pred = K.constant(pred_values, dtype='float32')
        y_true = K.constant(label_values, dtype='float32')
        weights = K.constant(weight_values)
        result = s_obj(y_true, y_pred, sample_weight=weights)
        assert np.isclose(0.675, K.eval(result))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1817')" href="javascript:;">
keras-2.3.1/tests/keras/metrics_confusion_matrix_test.py: 360-371
</a>
<div class="mid" id="frag1817" style="display:none"><pre>
    def test_weighted(self):
        s_obj = metrics.SpecificityAtSensitivity(0.4)
        pred_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.01, 0.02, 0.25, 0.26, 0.26]
        label_values = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]
        weight_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

        y_pred = K.constant(pred_values, dtype='float32')
        y_true = K.constant(label_values, dtype='float32')
        weights = K.constant(weight_values)
        result = s_obj(y_true, y_pred, sample_weight=weights)
        assert np.isclose(0.4, K.eval(result))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 66:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1821')" href="javascript:;">
keras-2.3.1/tests/keras/metrics_confusion_matrix_test.py: 410-429
</a>
<div class="mid" id="frag1821" style="display:none"><pre>
    def test_config(self):
        auc_obj = metrics.AUC(
            num_thresholds=100,
            curve='PR',
            summation_method='majoring',
            name='auc_1')
        assert auc_obj.name == 'auc_1'
        assert len(auc_obj.weights) == 4
        assert auc_obj.num_thresholds == 100
        assert auc_obj.curve == metrics_utils.AUCCurve.PR
        assert auc_obj.summation_method == metrics_utils.AUCSummationMethod.MAJORING

        # Check save and restore config.
        auc_obj2 = metrics.AUC.from_config(auc_obj.get_config())
        assert auc_obj2.name == 'auc_1'
        assert len(auc_obj2.weights) == 4
        assert auc_obj2.num_thresholds == 100
        assert auc_obj2.curve == metrics_utils.AUCCurve.PR
        assert auc_obj2.summation_method == metrics_utils.AUCSummationMethod.MAJORING

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1822')" href="javascript:;">
keras-2.3.1/tests/keras/metrics_confusion_matrix_test.py: 430-451
</a>
<div class="mid" id="frag1822" style="display:none"><pre>
    def test_config_manual_thresholds(self):
        auc_obj = metrics.AUC(
            num_thresholds=None,
            curve='PR',
            summation_method='majoring',
            name='auc_1',
            thresholds=[0.3, 0.5])
        assert auc_obj.name == 'auc_1'
        assert len(auc_obj.weights) == 4
        assert auc_obj.num_thresholds == 4
        assert np.allclose(auc_obj.thresholds, [0.0, 0.3, 0.5, 1.0], atol=1e-3)
        assert auc_obj.curve == metrics_utils.AUCCurve.PR
        assert auc_obj.summation_method == metrics_utils.AUCSummationMethod.MAJORING

        # Check save and restore config.
        auc_obj2 = metrics.AUC.from_config(auc_obj.get_config())
        assert auc_obj2.name == 'auc_1'
        assert len(auc_obj2.weights) == 4
        assert auc_obj2.num_thresholds == 4
        assert auc_obj2.curve == metrics_utils.AUCCurve.PR
        assert auc_obj2.summation_method == metrics_utils.AUCSummationMethod.MAJORING

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 67:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1835')" href="javascript:;">
keras-2.3.1/tests/keras/metrics_confusion_matrix_test.py: 599-617
</a>
<div class="mid" id="frag1835" style="display:none"><pre>
    def test_config(self):
        p_obj = metrics.Precision(
            name='my_precision', thresholds=[0.4, 0.9], top_k=15, class_id=12)
        assert p_obj.name == 'my_precision'
        assert len(p_obj.weights) == 2
        assert ([v.name for v in p_obj.weights] ==
                ['true_positives:0', 'false_positives:0'])
        assert p_obj.thresholds == [0.4, 0.9]
        assert p_obj.top_k == 15
        assert p_obj.class_id == 12

        # Check save and restore config
        p_obj2 = metrics.Precision.from_config(p_obj.get_config())
        assert p_obj2.name == 'my_precision'
        assert len(p_obj2.weights) == 2
        assert p_obj2.thresholds == [0.4, 0.9]
        assert p_obj2.top_k == 15
        assert p_obj2.class_id == 12

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1846')" href="javascript:;">
keras-2.3.1/tests/keras/metrics_confusion_matrix_test.py: 748-766
</a>
<div class="mid" id="frag1846" style="display:none"><pre>
    def test_config(self):
        r_obj = metrics.Recall(
            name='my_recall', thresholds=[0.4, 0.9], top_k=15, class_id=12)
        assert r_obj.name == 'my_recall'
        assert len(r_obj.weights) == 2
        assert ([v.name for v in r_obj.weights] ==
                ['true_positives:0', 'false_negatives:0'])
        assert r_obj.thresholds == [0.4, 0.9]
        assert r_obj.top_k == 15
        assert r_obj.class_id == 12

        # Check save and restore config
        r_obj2 = metrics.Recall.from_config(r_obj.get_config())
        assert r_obj2.name == 'my_recall'
        assert len(r_obj2.weights) == 2
        assert r_obj2.thresholds == [0.4, 0.9]
        assert r_obj2.top_k == 15
        assert r_obj2.class_id == 12

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 68:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1838')" href="javascript:;">
keras-2.3.1/tests/keras/metrics_confusion_matrix_test.py: 633-645
</a>
<div class="mid" id="frag1838" style="display:none"><pre>
    def test_weighted(self):
        p_obj = metrics.Precision()
        y_pred = K.constant([[1, 0, 1, 0], [1, 0, 1, 0]])
        y_true = K.constant([[0, 1, 1, 0], [1, 0, 0, 1]])
        result = p_obj(
            y_true,
            y_pred,
            sample_weight=K.constant([[1, 2, 3, 4], [4, 3, 2, 1]]))
        weighted_tp = 3.0 + 4.0
        weighted_positives = (1.0 + 3.0) + (4.0 + 2.0)
        expected_precision = weighted_tp / weighted_positives
        assert np.isclose(expected_precision, K.eval(result))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1849')" href="javascript:;">
keras-2.3.1/tests/keras/metrics_confusion_matrix_test.py: 782-794
</a>
<div class="mid" id="frag1849" style="display:none"><pre>
    def test_weighted(self):
        r_obj = metrics.Recall()
        y_pred = K.constant([[1, 0, 1, 0], [0, 1, 0, 1]])
        y_true = K.constant([[0, 1, 1, 0], [1, 0, 0, 1]])
        result = r_obj(
            y_true,
            y_pred,
            sample_weight=K.constant([[1, 2, 3, 4], [4, 3, 2, 1]]))
        weighted_tp = 3.0 + 1.0
        weighted_t = (2.0 + 3.0) + (4.0 + 1.0)
        expected_recall = weighted_tp / weighted_t
        assert np.isclose(expected_recall, K.eval(result))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 69:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1840')" href="javascript:;">
keras-2.3.1/tests/keras/metrics_confusion_matrix_test.py: 653-667
</a>
<div class="mid" id="frag1840" style="display:none"><pre>
    def test_weighted_with_threshold(self):
        p_obj = metrics.Precision(thresholds=[0.5, 1.])
        y_true = K.constant([[0, 1], [1, 0]], shape=(2, 2))
        y_pred = K.constant([[1, 0], [0.6, 0]],
                            shape=(2, 2),
                            dtype='float32')
        weights = K.constant([[4, 0], [3, 1]],
                             shape=(2, 2),
                             dtype='float32')
        result = p_obj(y_true, y_pred, sample_weight=weights)
        weighted_tp = 0 + 3.
        weighted_positives = (0 + 3.) + (4. + 0.)
        expected_precision = weighted_tp / weighted_positives
        assert np.allclose([expected_precision, 0], K.eval(result), 1e-3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1851')" href="javascript:;">
keras-2.3.1/tests/keras/metrics_confusion_matrix_test.py: 802-816
</a>
<div class="mid" id="frag1851" style="display:none"><pre>
    def test_weighted_with_threshold(self):
        r_obj = metrics.Recall(thresholds=[0.5, 1.])
        y_true = K.constant([[0, 1], [1, 0]], shape=(2, 2))
        y_pred = K.constant([[1, 0], [0.6, 0]],
                            shape=(2, 2),
                            dtype='float32')
        weights = K.constant([[1, 4], [3, 2]],
                             shape=(2, 2),
                             dtype='float32')
        result = r_obj(y_true, y_pred, sample_weight=weights)
        weighted_tp = 0 + 3.
        weighted_positives = (0 + 3.) + (4. + 0.)
        expected_recall = weighted_tp / weighted_positives
        assert np.allclose([expected_recall, 0], K.eval(result), 1e-3)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 70:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 93%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1842')" href="javascript:;">
keras-2.3.1/tests/keras/metrics_confusion_matrix_test.py: 675-693
</a>
<div class="mid" id="frag1842" style="display:none"><pre>
    def test_weighted_top_k(self):
        p_obj = metrics.Precision(top_k=3)
        y_pred1 = K.constant([0.2, 0.1, 0.4, 0, 0.2], shape=(1, 5))
        y_true1 = K.constant([0, 1, 1, 0, 1], shape=(1, 5))
        K.eval(
            p_obj(
                y_true1,
                y_pred1,
                sample_weight=K.constant([[1, 4, 2, 3, 5]])))

        y_pred2 = K.constant([0.2, 0.6, 0.4, 0.2, 0.2], shape=(1, 5))
        y_true2 = K.constant([1, 0, 1, 1, 1], shape=(1, 5))
        result = p_obj(y_true2, y_pred2, sample_weight=K.constant(3))

        tp = (2 + 5) + (3 + 3)
        predicted_positives = (1 + 2 + 5) + (3 + 3 + 3)
        expected_precision = float(tp) / predicted_positives
        assert np.isclose(expected_precision, K.eval(result))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1853')" href="javascript:;">
keras-2.3.1/tests/keras/metrics_confusion_matrix_test.py: 824-842
</a>
<div class="mid" id="frag1853" style="display:none"><pre>
    def test_weighted_top_k(self):
        r_obj = metrics.Recall(top_k=3)
        y_pred1 = K.constant([0.2, 0.1, 0.4, 0, 0.2], shape=(1, 5))
        y_true1 = K.constant([0, 1, 1, 0, 1], shape=(1, 5))
        K.eval(
            r_obj(
                y_true1,
                y_pred1,
                sample_weight=K.constant([[1, 4, 2, 3, 5]])))

        y_pred2 = K.constant([0.2, 0.6, 0.4, 0.2, 0.2], shape=(1, 5))
        y_true2 = K.constant([1, 0, 1, 1, 1], shape=(1, 5))
        result = r_obj(y_true2, y_pred2, sample_weight=K.constant(3))

        tp = (2 + 5) + (3 + 3)
        positives = (4 + 2 + 5) + (3 + 3 + 3 + 3)
        expected_recall = float(tp) / positives
        assert np.isclose(expected_recall, K.eval(result))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 71:</b> &nbsp; 2 fragments, nominal size 20 lines, similarity 95%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1843')" href="javascript:;">
keras-2.3.1/tests/keras/metrics_confusion_matrix_test.py: 694-717
</a>
<div class="mid" id="frag1843" style="display:none"><pre>
    def test_unweighted_class_id(self):
        p_obj = metrics.Precision(class_id=2)

        y_pred = K.constant([0.2, 0.1, 0.6, 0, 0.2], shape=(1, 5))
        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))
        result = p_obj(y_true, y_pred)
        assert np.isclose(1, K.eval(result))
        assert np.isclose(1, K.eval(p_obj.true_positives))
        assert np.isclose(0, K.eval(p_obj.false_positives))

        y_pred = K.constant([0.2, 0.1, 0, 0, 0.2], shape=(1, 5))
        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))
        result = p_obj(y_true, y_pred)
        assert np.isclose(1, K.eval(result))
        assert np.isclose(1, K.eval(p_obj.true_positives))
        assert np.isclose(0, K.eval(p_obj.false_positives))

        y_pred = K.constant([0.2, 0.1, 0.6, 0, 0.2], shape=(1, 5))
        y_true = K.constant([0, 1, 0, 0, 0], shape=(1, 5))
        result = p_obj(y_true, y_pred)
        assert np.isclose(0.5, K.eval(result))
        assert np.isclose(1, K.eval(p_obj.true_positives))
        assert np.isclose(1, K.eval(p_obj.false_positives))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1854')" href="javascript:;">
keras-2.3.1/tests/keras/metrics_confusion_matrix_test.py: 843-866
</a>
<div class="mid" id="frag1854" style="display:none"><pre>
    def test_unweighted_class_id(self):
        r_obj = metrics.Recall(class_id=2)

        y_pred = K.constant([0.2, 0.1, 0.6, 0, 0.2], shape=(1, 5))
        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))
        result = r_obj(y_true, y_pred)
        assert np.isclose(1, K.eval(result))
        assert np.isclose(1, K.eval(r_obj.true_positives))
        assert np.isclose(0, K.eval(r_obj.false_negatives))

        y_pred = K.constant([0.2, 0.1, 0, 0, 0.2], shape=(1, 5))
        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))
        result = r_obj(y_true, y_pred)
        assert np.isclose(0.5, K.eval(result))
        assert np.isclose(1, K.eval(r_obj.true_positives))
        assert np.isclose(1, K.eval(r_obj.false_negatives))

        y_pred = K.constant([0.2, 0.1, 0.6, 0, 0.2], shape=(1, 5))
        y_true = K.constant([0, 1, 0, 0, 0], shape=(1, 5))
        result = r_obj(y_true, y_pred)
        assert np.isclose(0.5, K.eval(result))
        assert np.isclose(1, K.eval(r_obj.true_positives))
        assert np.isclose(1, K.eval(r_obj.false_negatives))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 72:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 92%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1844')" href="javascript:;">
keras-2.3.1/tests/keras/metrics_confusion_matrix_test.py: 718-734
</a>
<div class="mid" id="frag1844" style="display:none"><pre>
    def test_unweighted_top_k_and_class_id(self):
        p_obj = metrics.Precision(class_id=2, top_k=2)

        y_pred = K.constant([0.2, 0.6, 0.3, 0, 0.2], shape=(1, 5))
        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))
        result = p_obj(y_true, y_pred)
        assert np.isclose(1, K.eval(result))
        assert np.isclose(1, K.eval(p_obj.true_positives))
        assert np.isclose(0, K.eval(p_obj.false_positives))

        y_pred = K.constant([1, 1, 0.9, 1, 1], shape=(1, 5))
        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))
        result = p_obj(y_true, y_pred)
        assert np.isclose(1, K.eval(result))
        assert np.isclose(1, K.eval(p_obj.true_positives))
        assert np.isclose(0, K.eval(p_obj.false_positives))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1855')" href="javascript:;">
keras-2.3.1/tests/keras/metrics_confusion_matrix_test.py: 867-883
</a>
<div class="mid" id="frag1855" style="display:none"><pre>
    def test_unweighted_top_k_and_class_id(self):
        r_obj = metrics.Recall(class_id=2, top_k=2)

        y_pred = K.constant([0.2, 0.6, 0.3, 0, 0.2], shape=(1, 5))
        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))
        result = r_obj(y_true, y_pred)
        assert np.isclose(1, K.eval(result))
        assert np.isclose(1, K.eval(r_obj.true_positives))
        assert np.isclose(0, K.eval(r_obj.false_negatives))

        y_pred = K.constant([1, 1, 0.9, 1, 1], shape=(1, 5))
        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))
        result = r_obj(y_true, y_pred)
        assert np.isclose(0.5, K.eval(result))
        assert np.isclose(1, K.eval(r_obj.true_positives))
        assert np.isclose(1, K.eval(r_obj.false_negatives))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 73:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1872')" href="javascript:;">
keras-2.3.1/tests/keras/layers/normalization_test.py: 121-139
</a>
<div class="mid" id="frag1872" style="display:none"><pre>
def test_batchnorm_convnet():
    np.random.seed(1337)
    model = Sequential()
    norm = normalization.BatchNormalization(axis=1, input_shape=(3, 4, 4),
                                            momentum=0.8)
    model.add(norm)
    model.compile(loss='mse', optimizer='sgd')

    # centered on 5.0, variance 10.0
    x = np.random.normal(loc=5.0, scale=10.0, size=(1000, 3, 4, 4))
    model.fit(x, x, epochs=4, verbose=0)
    out = model.predict(x)
    out -= np.reshape(K.eval(norm.beta), (1, 3, 1, 1))
    out /= np.reshape(K.eval(norm.gamma), (1, 3, 1, 1))

    assert_allclose(np.mean(out, axis=(0, 2, 3)), 0.0, atol=1e-1)
    assert_allclose(np.std(out, axis=(0, 2, 3)), 1.0, atol=1e-1)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1873')" href="javascript:;">
keras-2.3.1/tests/keras/layers/normalization_test.py: 142-158
</a>
<div class="mid" id="frag1873" style="display:none"><pre>
def test_batchnorm_convnet_no_center_no_scale():
    np.random.seed(1337)
    model = Sequential()
    norm = normalization.BatchNormalization(axis=-1, center=False, scale=False,
                                            input_shape=(3, 4, 4), momentum=0.8)
    model.add(norm)
    model.compile(loss='mse', optimizer='sgd')

    # centered on 5.0, variance 10.0
    x = np.random.normal(loc=5.0, scale=10.0, size=(1000, 3, 4, 4))
    model.fit(x, x, epochs=4, verbose=0)
    out = model.predict(x)

    assert_allclose(np.mean(out, axis=(0, 2, 3)), 0.0, atol=1e-1)
    assert_allclose(np.std(out, axis=(0, 2, 3)), 1.0, atol=1e-1)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 74:</b> &nbsp; 5 fragments, nominal size 13 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1880')" href="javascript:;">
keras-2.3.1/tests/keras/layers/convolutional_test.py: 81-96
</a>
<div class="mid" id="frag1880" style="display:none"><pre>
def test_conv_1d_dilation():
    batch_size = 2
    steps = 8
    input_dim = 2
    kernel_size = 3
    filters = 3
    padding = _convolution_paddings[-1]

    layer_test(convolutional.Conv1D,
               kwargs={'filters': filters,
                       'kernel_size': kernel_size,
                       'padding': padding,
                       'dilation_rate': 2},
               input_shape=(batch_size, steps, input_dim))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1881')" href="javascript:;">
keras-2.3.1/tests/keras/layers/convolutional_test.py: 97-110
</a>
<div class="mid" id="frag1881" style="display:none"><pre>
def test_conv_1d_channels_first():
    batch_size = 2
    steps = 8
    input_dim = 2
    kernel_size = 3
    filters = 3

    layer_test(convolutional.Conv1D,
               kwargs={'filters': filters,
                       'kernel_size': kernel_size,
                       'data_format': 'channels_first'},
               input_shape=(batch_size, input_dim, steps))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1884')" href="javascript:;">
keras-2.3.1/tests/keras/layers/convolutional_test.py: 161-177
</a>
<div class="mid" id="frag1884" style="display:none"><pre>
def test_convolution_2d_dilation():
    num_samples = 2
    filters = 2
    stack_size = 3
    kernel_size = (3, 2)
    num_row = 7
    num_col = 6
    padding = 'valid'

    layer_test(convolutional.Conv2D,
               kwargs={'filters': filters,
                       'kernel_size': kernel_size,
                       'padding': padding,
                       'dilation_rate': (2, 2)},
               input_shape=(num_samples, num_row, num_col, stack_size))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1882')" href="javascript:;">
keras-2.3.1/tests/keras/layers/convolutional_test.py: 118-134
</a>
<div class="mid" id="frag1882" style="display:none"><pre>
def test_convolution_2d(strides, padding):
    num_samples = 2
    filters = 2
    stack_size = 3
    kernel_size = (3, 2)
    num_row = 7
    num_col = 6

    layer_test(convolutional.Conv2D,
               kwargs={'filters': filters,
                       'kernel_size': kernel_size,
                       'padding': padding,
                       'strides': strides,
                       'data_format': 'channels_first'},
               input_shape=(num_samples, stack_size, num_row, num_col))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1886')" href="javascript:;">
keras-2.3.1/tests/keras/layers/convolutional_test.py: 198-215
</a>
<div class="mid" id="frag1886" style="display:none"><pre>
def test_conv2d_transpose(padding, out_padding, strides):
    num_samples = 2
    filters = 2
    stack_size = 3
    num_row = 5
    num_col = 6

    layer_test(convolutional.Conv2DTranspose,
               kwargs={'filters': filters,
                       'kernel_size': 3,
                       'padding': padding,
                       'output_padding': out_padding,
                       'strides': strides,
                       'data_format': 'channels_last'},
               input_shape=(num_samples, num_row, num_col, stack_size),
               fixed_batch_size=True)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 75:</b> &nbsp; 8 fragments, nominal size 21 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1883')" href="javascript:;">
keras-2.3.1/tests/keras/layers/convolutional_test.py: 135-158
</a>
<div class="mid" id="frag1883" style="display:none"><pre>
def test_convolution_2d_channels_last():
    num_samples = 2
    filters = 2
    stack_size = 3
    num_row = 7
    num_col = 6
    padding = 'valid'
    strides = (2, 2)

    layer_test(convolutional.Conv2D,
               kwargs={'filters': filters,
                       'kernel_size': 3,
                       'padding': padding,
                       'data_format': 'channels_last',
                       'activation': None,
                       'kernel_regularizer': 'l2',
                       'bias_regularizer': 'l2',
                       'activity_regularizer': 'l2',
                       'kernel_constraint': 'max_norm',
                       'bias_constraint': 'max_norm',
                       'strides': strides},
               input_shape=(num_samples, num_row, num_col, stack_size))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1888')" href="javascript:;">
keras-2.3.1/tests/keras/layers/convolutional_test.py: 246-270
</a>
<div class="mid" id="frag1888" style="display:none"><pre>
def test_conv2d_transpose_channels_first():
    num_samples = 2
    filters = 2
    stack_size = 3
    num_row = 5
    num_col = 6
    padding = 'valid'
    strides = (2, 2)

    layer_test(convolutional.Conv2DTranspose,
               kwargs={'filters': filters,
                       'kernel_size': 3,
                       'padding': padding,
                       'data_format': 'channels_first',
                       'activation': None,
                       'kernel_regularizer': 'l2',
                       'bias_regularizer': 'l2',
                       'activity_regularizer': 'l2',
                       'kernel_constraint': 'max_norm',
                       'bias_constraint': 'max_norm',
                       'strides': strides},
               input_shape=(num_samples, stack_size, num_row, num_col),
               fixed_batch_size=True)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1891')" href="javascript:;">
keras-2.3.1/tests/keras/layers/convolutional_test.py: 334-359
</a>
<div class="mid" id="frag1891" style="display:none"><pre>
def test_separable_conv_1d_additional_args():
    num_samples = 2
    filters = 6
    stack_size = 3
    num_step = 9
    padding = 'valid'
    multiplier = 2

    layer_test(convolutional.SeparableConv1D,
               kwargs={'filters': filters,
                       'kernel_size': 3,
                       'padding': padding,
                       'data_format': 'channels_first',
                       'activation': None,
                       'depthwise_regularizer': 'l2',
                       'pointwise_regularizer': 'l2',
                       'bias_regularizer': 'l2',
                       'activity_regularizer': 'l2',
                       'pointwise_constraint': 'unit_norm',
                       'depthwise_constraint': 'unit_norm',
                       'strides': 1,
                       'use_bias': True,
                       'depth_multiplier': multiplier},
               input_shape=(num_samples, stack_size, num_step))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1902')" href="javascript:;">
keras-2.3.1/tests/keras/layers/convolutional_test.py: 580-605
</a>
<div class="mid" id="frag1902" style="display:none"><pre>
def test_conv3d_transpose_additional_args():
    filters = 2
    stack_size = 3
    num_depth = 7
    num_row = 5
    num_col = 6
    padding = 'valid'
    strides = (2, 2, 2)

    layer_test(convolutional.Conv3DTranspose,
               kwargs={'filters': filters,
                       'kernel_size': 3,
                       'padding': padding,
                       'data_format': 'channels_first',
                       'activation': None,
                       'kernel_regularizer': 'l2',
                       'bias_regularizer': 'l2',
                       'activity_regularizer': 'l2',
                       'kernel_constraint': 'max_norm',
                       'bias_constraint': 'max_norm',
                       'use_bias': True,
                       'strides': strides},
               input_shape=(None, stack_size, num_depth, num_row, num_col),
               fixed_batch_size=True)


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1897')" href="javascript:;">
keras-2.3.1/tests/keras/layers/convolutional_test.py: 465-488
</a>
<div class="mid" id="frag1897" style="display:none"><pre>
def test_depthwise_conv_2d_additional_args():
    num_samples = 2
    stack_size = 3
    num_row = 7
    num_col = 6
    padding = 'valid'
    strides = (2, 2)
    multiplier = 2

    layer_test(convolutional.DepthwiseConv2D,
               kwargs={'kernel_size': 3,
                       'padding': padding,
                       'data_format': 'channels_first',
                       'activation': None,
                       'depthwise_regularizer': 'l2',
                       'bias_regularizer': 'l2',
                       'activity_regularizer': 'l2',
                       'depthwise_constraint': 'unit_norm',
                       'use_bias': True,
                       'strides': strides,
                       'depth_multiplier': multiplier},
               input_shape=(num_samples, stack_size, num_row, num_col))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1932')" href="javascript:;">
keras-2.3.1/tests/keras/layers/local_test.py: 7-26
</a>
<div class="mid" id="frag1932" style="display:none"><pre>
def test_locallyconnected_1d():
    num_samples = 2
    num_steps = 8
    input_dim = 5
    filter_length = 3
    filters = 4
    padding = 'valid'
    strides = 1

    layer_test(local.LocallyConnected1D,
               kwargs={'filters': filters,
                       'kernel_size': filter_length,
                       'padding': padding,
                       'kernel_regularizer': 'l2',
                       'bias_regularizer': 'l2',
                       'activity_regularizer': 'l2',
                       'strides': strides},
               input_shape=(num_samples, num_steps, input_dim))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1894')" href="javascript:;">
keras-2.3.1/tests/keras/layers/convolutional_test.py: 399-425
</a>
<div class="mid" id="frag1894" style="display:none"><pre>
def test_separable_conv_2d_additional_args():
    num_samples = 2
    filters = 6
    stack_size = 3
    num_row = 7
    num_col = 6
    padding = 'valid'
    strides = (2, 2)
    multiplier = 2

    layer_test(convolutional.SeparableConv2D,
               kwargs={'filters': filters,
                       'kernel_size': 3,
                       'padding': padding,
                       'data_format': 'channels_first',
                       'activation': None,
                       'depthwise_regularizer': 'l2',
                       'pointwise_regularizer': 'l2',
                       'bias_regularizer': 'l2',
                       'activity_regularizer': 'l2',
                       'pointwise_constraint': 'unit_norm',
                       'depthwise_constraint': 'unit_norm',
                       'strides': strides,
                       'depth_multiplier': multiplier},
               input_shape=(num_samples, stack_size, num_row, num_col))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1900')" href="javascript:;">
keras-2.3.1/tests/keras/layers/convolutional_test.py: 524-550
</a>
<div class="mid" id="frag1900" style="display:none"><pre>
def test_convolution_3d_additional_args():
    num_samples = 2
    filters = 2
    stack_size = 3
    padding = 'valid'
    strides = (2, 2, 2)

    input_len_dim1 = 9
    input_len_dim2 = 8
    input_len_dim3 = 8

    layer_test(convolutional.Convolution3D,
               kwargs={'filters': filters,
                       'kernel_size': (1, 2, 3),
                       'padding': padding,
                       'activation': None,
                       'kernel_regularizer': 'l2',
                       'bias_regularizer': 'l2',
                       'activity_regularizer': 'l2',
                       'kernel_constraint': 'max_norm',
                       'bias_constraint': 'max_norm',
                       'strides': strides},
               input_shape=(num_samples,
                            input_len_dim1, input_len_dim2, input_len_dim3,
                            stack_size))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 76:</b> &nbsp; 2 fragments, nominal size 29 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1889')" href="javascript:;">
keras-2.3.1/tests/keras/layers/convolutional_test.py: 271-306
</a>
<div class="mid" id="frag1889" style="display:none"><pre>
def test_conv2d_transpose_invalid():
    filters = 2
    stack_size = 3
    num_row = 5
    num_col = 6
    padding = 'valid'

    with pytest.raises(ValueError):
        model = Sequential([convolutional.Conv2DTranspose(
            filters=filters,
            kernel_size=3,
            padding=padding,
            use_bias=True,
            batch_input_shape=(None, None, 5, None))])

    # Test invalid output padding for given stride. Output padding equal to stride
    with pytest.raises(ValueError):
        model = Sequential([convolutional.Conv2DTranspose(
            filters=filters,
            kernel_size=3,
            padding=padding,
            output_padding=(0, 3),
            strides=(1, 3),
            batch_input_shape=(None, num_row, num_col, stack_size))])

    # Output padding greater than stride
    with pytest.raises(ValueError):
        model = Sequential([convolutional.Conv2DTranspose(
            filters=filters,
            kernel_size=3,
            padding=padding,
            output_padding=(2, 2),
            strides=(1, 3),
            batch_input_shape=(None, num_row, num_col, stack_size))])


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1903')" href="javascript:;">
keras-2.3.1/tests/keras/layers/convolutional_test.py: 606-643
</a>
<div class="mid" id="frag1903" style="display:none"><pre>
def test_conv3d_transpose_invalid():
    filters = 2
    stack_size = 3
    num_depth = 7
    num_row = 5
    num_col = 6
    padding = 'valid'

    # Test invalid use case
    with pytest.raises(ValueError):
        model = Sequential([convolutional.Conv3DTranspose(
            filters=filters,
            kernel_size=3,
            padding=padding,
            batch_input_shape=(None, None, 5, None, None))])

    # Test invalid output padding for given stride. Output padding equal
    # to stride
    with pytest.raises(ValueError):
        model = Sequential([convolutional.Conv3DTranspose(
            filters=filters,
            kernel_size=3,
            padding=padding,
            output_padding=(0, 3, 3),
            strides=(1, 3, 4),
            batch_input_shape=(None, num_depth, num_row, num_col, stack_size))])

    # Output padding greater than stride
    with pytest.raises(ValueError):
        model = Sequential([convolutional.Conv3DTranspose(
            filters=filters,
            kernel_size=3,
            padding=padding,
            output_padding=(2, 2, 3),
            strides=(1, 3, 4),
            batch_input_shape=(None, num_depth, num_row, num_col, stack_size))])


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 77:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1893')" href="javascript:;">
keras-2.3.1/tests/keras/layers/convolutional_test.py: 381-398
</a>
<div class="mid" id="frag1893" style="display:none"><pre>
def test_separable_conv_2d(padding, strides, multiplier, dilation_rate):
    num_samples = 2
    filters = 6
    stack_size = 3
    num_row = 7
    num_col = 6

    layer_test(
        convolutional.SeparableConv2D,
        kwargs={'filters': filters,
                'kernel_size': (3, 3),
                'padding': padding,
                'strides': strides,
                'depth_multiplier': multiplier,
                'dilation_rate': dilation_rate},
        input_shape=(num_samples, num_row, num_col, stack_size))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1901')" href="javascript:;">
keras-2.3.1/tests/keras/layers/convolutional_test.py: 561-579
</a>
<div class="mid" id="frag1901" style="display:none"><pre>
def test_conv3d_transpose(padding, out_padding, strides, data_format):
    filters = 2
    stack_size = 3
    num_depth = 7
    num_row = 5
    num_col = 6

    layer_test(
        convolutional.Conv3DTranspose,
        kwargs={'filters': filters,
                'kernel_size': 3,
                'padding': padding,
                'output_padding': out_padding,
                'strides': strides,
                'data_format': data_format},
        input_shape=(None, num_depth, num_row, num_col, stack_size),
        fixed_batch_size=True)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 78:</b> &nbsp; 2 fragments, nominal size 33 lines, similarity 77%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1910')" href="javascript:;">
keras-2.3.1/tests/keras/layers/convolutional_test.py: 836-879
</a>
<div class="mid" id="frag1910" style="display:none"><pre>
def test_upsampling_2d(data_format):
    num_samples = 2
    stack_size = 2
    input_num_row = 11
    input_num_col = 12

    if data_format == 'channels_first':
        inputs = np.random.rand(num_samples, stack_size, input_num_row,
                                input_num_col)
    else:  # tf
        inputs = np.random.rand(num_samples, input_num_row, input_num_col,
                                stack_size)

    # basic test
    layer_test(convolutional.UpSampling2D,
               kwargs={'size': (2, 2), 'data_format': data_format},
               input_shape=inputs.shape)

    for length_row in [2]:
        for length_col in [2, 3]:
            layer = convolutional.UpSampling2D(
                size=(length_row, length_col),
                data_format=data_format)
            layer.build(inputs.shape)
            outputs = layer(K.variable(inputs))
            np_output = K.eval(outputs)
            if data_format == 'channels_first':
                assert np_output.shape[2] == length_row * input_num_row
                assert np_output.shape[3] == length_col * input_num_col
            else:  # tf
                assert np_output.shape[1] == length_row * input_num_row
                assert np_output.shape[2] == length_col * input_num_col

            # compare with numpy
            if data_format == 'channels_first':
                expected_out = np.repeat(inputs, length_row, axis=2)
                expected_out = np.repeat(expected_out, length_col, axis=3)
            else:  # tf
                expected_out = np.repeat(inputs, length_row, axis=1)
                expected_out = np.repeat(expected_out, length_col, axis=2)

            assert_allclose(np_output, expected_out)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1911')" href="javascript:;">
keras-2.3.1/tests/keras/layers/convolutional_test.py: 884-919
</a>
<div class="mid" id="frag1911" style="display:none"><pre>
def test_upsampling_2d_bilinear(data_format):
    num_samples = 2
    stack_size = 2
    input_num_row = 11
    input_num_col = 12

    if data_format == 'channels_first':
        inputs = np.random.rand(num_samples, stack_size, input_num_row,
                                input_num_col)
    else:  # tf
        inputs = np.random.rand(num_samples, input_num_row, input_num_col,
                                stack_size)

    # basic test
    layer_test(convolutional.UpSampling2D,
               kwargs={'size': (2, 2),
                       'data_format': data_format,
                       'interpolation': 'bilinear'},
               input_shape=inputs.shape)

    for length_row in [2]:
        for length_col in [2, 3]:
            layer = convolutional.UpSampling2D(
                size=(length_row, length_col),
                data_format=data_format)
            layer.build(inputs.shape)
            outputs = layer(K.variable(inputs))
            np_output = K.eval(outputs)
            if data_format == 'channels_first':
                assert np_output.shape[2] == length_row * input_num_row
                assert np_output.shape[3] == length_col * input_num_col
            else:  # tf
                assert np_output.shape[1] == length_row * input_num_row
                assert np_output.shape[2] == length_col * input_num_col


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 79:</b> &nbsp; 2 fragments, nominal size 55 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1914')" href="javascript:;">
keras-2.3.1/tests/keras/layers/convolutional_test.py: 989-1052
</a>
<div class="mid" id="frag1914" style="display:none"><pre>
def test_cropping_2d():
    num_samples = 2
    stack_size = 2
    input_len_dim1 = 9
    input_len_dim2 = 9
    cropping = ((2, 2), (3, 3))

    for data_format in ['channels_first', 'channels_last']:
        if data_format == 'channels_first':
            inputs = np.random.rand(num_samples, stack_size,
                                    input_len_dim1, input_len_dim2)
        else:
            inputs = np.random.rand(num_samples,
                                    input_len_dim1, input_len_dim2,
                                    stack_size)
        # basic test
        layer_test(convolutional.Cropping2D,
                   kwargs={'cropping': cropping,
                           'data_format': data_format},
                   input_shape=inputs.shape)
        # correctness test
        layer = convolutional.Cropping2D(cropping=cropping,
                                         data_format=data_format)
        layer.build(inputs.shape)
        outputs = layer(K.variable(inputs))
        np_output = K.eval(outputs)
        # compare with numpy
        if data_format == 'channels_first':
            expected_out = inputs[:,
                                  :,
                                  cropping[0][0]: -cropping[0][1],
                                  cropping[1][0]: -cropping[1][1]]
        else:
            expected_out = inputs[:,
                                  cropping[0][0]: -cropping[0][1],
                                  cropping[1][0]: -cropping[1][1],
                                  :]
        assert_allclose(np_output, expected_out)

    for data_format in ['channels_first', 'channels_last']:
        if data_format == 'channels_first':
            inputs = np.random.rand(num_samples, stack_size,
                                    input_len_dim1, input_len_dim2)
        else:
            inputs = np.random.rand(num_samples,
                                    input_len_dim1, input_len_dim2,
                                    stack_size)
        # another correctness test (no cropping)
        cropping = ((0, 0), (0, 0))
        layer = convolutional.Cropping2D(cropping=cropping,
                                         data_format=data_format)
        layer.build(inputs.shape)
        outputs = layer(K.variable(inputs))
        np_output = K.eval(outputs)
        # compare with input
        assert_allclose(np_output, inputs)

    # Test invalid use cases
    with pytest.raises(ValueError):
        layer = convolutional.Cropping2D(cropping=((1, 1),))
    with pytest.raises(ValueError):
        layer = convolutional.Cropping2D(cropping=lambda x: x)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1915')" href="javascript:;">
keras-2.3.1/tests/keras/layers/convolutional_test.py: 1053-1119
</a>
<div class="mid" id="frag1915" style="display:none"><pre>
def test_cropping_3d():
    num_samples = 2
    stack_size = 2
    input_len_dim1 = 8
    input_len_dim2 = 8
    input_len_dim3 = 8
    cropping = ((2, 2), (3, 3), (2, 3))

    for data_format in ['channels_last', 'channels_first']:
        if data_format == 'channels_first':
            inputs = np.random.rand(num_samples, stack_size,
                                    input_len_dim1, input_len_dim2, input_len_dim3)
        else:
            inputs = np.random.rand(num_samples,
                                    input_len_dim1, input_len_dim2,
                                    input_len_dim3, stack_size)
        # basic test
        layer_test(convolutional.Cropping3D,
                   kwargs={'cropping': cropping,
                           'data_format': data_format},
                   input_shape=inputs.shape)
        # correctness test
        layer = convolutional.Cropping3D(cropping=cropping,
                                         data_format=data_format)
        layer.build(inputs.shape)
        outputs = layer(K.variable(inputs))
        np_output = K.eval(outputs)
        # compare with numpy
        if data_format == 'channels_first':
            expected_out = inputs[:,
                                  :,
                                  cropping[0][0]: -cropping[0][1],
                                  cropping[1][0]: -cropping[1][1],
                                  cropping[2][0]: -cropping[2][1]]
        else:
            expected_out = inputs[:,
                                  cropping[0][0]: -cropping[0][1],
                                  cropping[1][0]: -cropping[1][1],
                                  cropping[2][0]: -cropping[2][1],
                                  :]
        assert_allclose(np_output, expected_out)

    for data_format in ['channels_last', 'channels_first']:
        if data_format == 'channels_first':
            inputs = np.random.rand(num_samples, stack_size,
                                    input_len_dim1, input_len_dim2, input_len_dim3)
        else:
            inputs = np.random.rand(num_samples,
                                    input_len_dim1, input_len_dim2,
                                    input_len_dim3, stack_size)
        # another correctness test (no cropping)
        cropping = ((0, 0), (0, 0), (0, 0))
        layer = convolutional.Cropping3D(cropping=cropping,
                                         data_format=data_format)
        layer.build(inputs.shape)
        outputs = layer(K.variable(inputs))
        np_output = K.eval(outputs)
        # compare with input
        assert_allclose(np_output, inputs)

    # Test invalid use cases
    with pytest.raises(ValueError):
        layer = convolutional.Cropping3D(cropping=((1, 1),))
    with pytest.raises(ValueError):
        layer = convolutional.Cropping3D(cropping=lambda x: x)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 80:</b> &nbsp; 3 fragments, nominal size 14 lines, similarity 85%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1940')" href="javascript:;">
keras-2.3.1/tests/keras/layers/core_test.py: 92-109
</a>
<div class="mid" id="frag1940" style="display:none"><pre>
    def test_4d():
        np_inp_channels_last = np.arange(24, dtype='float32').reshape(
                                        (1, 4, 3, 2))

        np_output_cl = layer_test(layers.Flatten,
                                  kwargs={'data_format':
                                          'channels_last'},
                                  input_data=np_inp_channels_last)

        np_inp_channels_first = np.transpose(np_inp_channels_last,
                                             [0, 3, 1, 2])

        np_output_cf = layer_test(layers.Flatten,
                                  kwargs={'data_format':
                                          'channels_first'},
                                  input_data=np_inp_channels_first,
                                  expected_output=np_output_cl)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1942')" href="javascript:;">
keras-2.3.1/tests/keras/layers/core_test.py: 128-144
</a>
<div class="mid" id="frag1942" style="display:none"><pre>
    def test_5d():
        np_inp_channels_last = np.arange(120, dtype='float32').reshape(
            (1, 5, 4, 3, 2))

        np_output_cl = layer_test(layers.Flatten,
                                  kwargs={'data_format':
                                          'channels_last'},
                                  input_data=np_inp_channels_last)

        np_inp_channels_first = np.transpose(np_inp_channels_last,
                                             [0, 4, 1, 2, 3])

        np_output_cf = layer_test(layers.Flatten,
                                  kwargs={'data_format':
                                          'channels_first'},
                                  input_data=np_inp_channels_first,
                                  expected_output=np_output_cl)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1941')" href="javascript:;">
keras-2.3.1/tests/keras/layers/core_test.py: 110-127
</a>
<div class="mid" id="frag1941" style="display:none"><pre>
    def test_3d():
        np_inp_channels_last = np.arange(12, dtype='float32').reshape(
            (1, 4, 3))

        np_output_cl = layer_test(layers.Flatten,
                                  kwargs={'data_format':
                                          'channels_last'},
                                  input_data=np_inp_channels_last)

        np_inp_channels_first = np.transpose(np_inp_channels_last,
                                             [0, 2, 1])

        np_output_cf = layer_test(layers.Flatten,
                                  kwargs={'data_format':
                                          'channels_first'},
                                  input_data=np_inp_channels_first,
                                  expected_output=np_output_cl)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 81:</b> &nbsp; 2 fragments, nominal size 38 lines, similarity 89%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1969')" href="javascript:;">
keras-2.3.1/tests/keras/layers/recurrent_test.py: 169-224
</a>
<div class="mid" id="frag1969" style="display:none"><pre>
def test_masking_correctness_output_not_equal_to_first_state():

    class Cell(keras.layers.Layer):

        def __init__(self):
            self.state_size = None
            self.output_size = None
            super(Cell, self).__init__()

        def build(self, input_shape):
            self.state_size = input_shape[-1]
            self.output_size = input_shape[-1]

        def call(self, inputs, states):
            return inputs, [s + 1 for s in states]

    num_samples = 5
    num_timesteps = 4
    state_size = input_size = 3  # also equal to `output_size`

    # random inputs and state values
    x_vals = np.random.random((num_samples, num_timesteps, input_size))
    # last timestep masked for first sample (all zero inputs masked by Masking layer)
    x_vals[0, -1, :] = 0
    s_initial_vals = np.random.random((num_samples, state_size))

    # final outputs equal to last inputs
    y_vals_expected = x_vals[:, -1].copy()
    # except for first sample, where it is equal to second to last value due to mask
    y_vals_expected[0] = x_vals[0, -2]

    s_final_vals_expected = s_initial_vals.copy()
    # states are incremented `num_timesteps - 1` times for first sample
    s_final_vals_expected[0] += (num_timesteps - 1)
    # and `num_timesteps - 1` times for remaining samples
    s_final_vals_expected[1:] += num_timesteps

    for unroll in [True, False]:
        x = Input((num_timesteps, input_size), name="x")
        x_masked = Masking()(x)
        s_initial = Input((state_size,), name="s_initial")
        y, s_final = recurrent.RNN(Cell(),
                                   return_state=True,
                                   unroll=unroll)(x_masked, initial_state=s_initial)
        model = Model([x, s_initial], [y, s_final])
        model.compile(optimizer='sgd', loss='mse')

        y_vals, s_final_vals = model.predict([x_vals, s_initial_vals])
        assert_allclose(y_vals,
                        y_vals_expected,
                        err_msg="Unexpected output for unroll={}".format(unroll))
        assert_allclose(s_final_vals,
                        s_final_vals_expected,
                        err_msg="Unexpected state for unroll={}".format(unroll))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1973')" href="javascript:;">
keras-2.3.1/tests/keras/layers/recurrent_test.py: 226-281
</a>
<div class="mid" id="frag1973" style="display:none"><pre>
def test_masking_correctness_output_size_not_equal_to_first_state_size():

    class Cell(keras.layers.Layer):

        def __init__(self):
            self.state_size = None
            self.output_size = None
            super(Cell, self).__init__()

        def build(self, input_shape):
            self.state_size = input_shape[-1]
            self.output_size = input_shape[-1] * 2

        def call(self, inputs, states):
            return keras.layers.concatenate([inputs] * 2), [s + 1 for s in states]

    num_samples = 5
    num_timesteps = 6
    input_size = state_size = 7

    # random inputs and state values
    x_vals = np.random.random((num_samples, num_timesteps, input_size))
    # last timestep masked for first sample (all zero inputs masked by Masking layer)
    x_vals[0, -1, :] = 0
    s_initial_vals = np.random.random((num_samples, state_size))

    # final outputs equal to last inputs concatenated
    y_vals_expected = np.concatenate([x_vals[:, -1]] * 2, axis=-1)
    # except for first sample, where it is equal to second to last value due to mask
    y_vals_expected[0] = np.concatenate([x_vals[0, -2]] * 2, axis=-1)

    s_final_vals_expected = s_initial_vals.copy()
    # states are incremented `num_timesteps - 1` times for first sample
    s_final_vals_expected[0] += (num_timesteps - 1)
    # and `num_timesteps - 1` times for remaining samples
    s_final_vals_expected[1:] += num_timesteps

    for unroll in [True, False]:
        x = Input((num_timesteps, input_size), name="x")
        x_masked = Masking()(x)
        s_initial = Input((state_size,), name="s_initial")
        y, s_final = recurrent.RNN(Cell(),
                                   return_state=True,
                                   unroll=unroll)(x_masked, initial_state=s_initial)
        model = Model([x, s_initial], [y, s_final])
        model.compile(optimizer='sgd', loss='mse')

        y_vals, s_final_vals = model.predict([x_vals, s_initial_vals])
        assert_allclose(y_vals,
                        y_vals_expected,
                        err_msg="Unexpected output for unroll={}".format(unroll))
        assert_allclose(s_final_vals,
                        s_final_vals_expected,
                        err_msg="Unexpected state for unroll={}".format(unroll))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 82:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1979')" href="javascript:;">
keras-2.3.1/tests/keras/layers/recurrent_test.py: 327-342
</a>
<div class="mid" id="frag1979" style="display:none"><pre>
def test_trainability(layer_class):
    layer = layer_class(units)
    layer.build((None, None, embedding_dim))
    assert len(layer.weights) == 3
    assert len(layer.trainable_weights) == 3
    assert len(layer.non_trainable_weights) == 0
    layer.trainable = False
    assert len(layer.weights) == 3
    assert len(layer.trainable_weights) == 0
    assert len(layer.non_trainable_weights) == 3
    layer.trainable = True
    assert len(layer.weights) == 3
    assert len(layer.trainable_weights) == 3
    assert len(layer.non_trainable_weights) == 0


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2025')" href="javascript:;">
keras-2.3.1/tests/keras/layers/cudnn_recurrent_test.py: 188-206
</a>
<div class="mid" id="frag2025" style="display:none"><pre>
def test_trainability():
    input_size = 10
    units = 2
    for layer_class in [keras.layers.CuDNNGRU, keras.layers.CuDNNLSTM]:
        layer = layer_class(units)
        layer.build((None, None, input_size))
        assert len(layer.weights) == 3
        assert len(layer.trainable_weights) == 3
        assert len(layer.non_trainable_weights) == 0
        layer.trainable = False
        assert len(layer.weights) == 3
        assert len(layer.non_trainable_weights) == 3
        assert len(layer.trainable_weights) == 0
        layer.trainable = True
        assert len(layer.weights) == 3
        assert len(layer.trainable_weights) == 3
        assert len(layer.non_trainable_weights) == 0


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 83:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1982')" href="javascript:;">
keras-2.3.1/tests/keras/layers/recurrent_test.py: 374-397
</a>
<div class="mid" id="frag1982" style="display:none"><pre>
def test_specify_initial_state_keras_tensor(layer_class):
    num_states = 2 if layer_class is recurrent.LSTM else 1

    # Test with Keras tensor
    inputs = Input((timesteps, embedding_dim))
    initial_state = [Input((units,)) for _ in range(num_states)]
    layer = layer_class(units)
    if len(initial_state) == 1:
        output = layer(inputs, initial_state=initial_state[0])
    else:
        output = layer(inputs, initial_state=initial_state)
    assert id(initial_state[0]) in [
        id(x) for x in layer._inbound_nodes[0].input_tensors]

    model = Model([inputs] + initial_state, output)
    model.compile(loss='categorical_crossentropy', optimizer='adam')

    inputs = np.random.random((num_samples, timesteps, embedding_dim))
    initial_state = [np.random.random((num_samples, units))
                     for _ in range(num_states)]
    targets = np.random.random((num_samples, units))
    model.fit([inputs] + initial_state, targets)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1985')" href="javascript:;">
keras-2.3.1/tests/keras/layers/recurrent_test.py: 444-466
</a>
<div class="mid" id="frag1985" style="display:none"><pre>
def test_initial_states_as_other_inputs(layer_class):
    num_states = 2 if layer_class is recurrent.LSTM else 1

    # Test with Keras tensor
    main_inputs = Input((timesteps, embedding_dim))
    initial_state = [Input((units,)) for _ in range(num_states)]
    inputs = [main_inputs] + initial_state

    layer = layer_class(units)
    output = layer(inputs)
    assert id(initial_state[0]) in [
        id(x) for x in layer._inbound_nodes[0].input_tensors]

    model = Model(inputs, output)
    model.compile(loss='categorical_crossentropy', optimizer='adam')

    main_inputs = np.random.random((num_samples, timesteps, embedding_dim))
    initial_state = [np.random.random((num_samples, units))
                     for _ in range(num_states)]
    targets = np.random.random((num_samples, units))
    model.train_on_batch([main_inputs] + initial_state, targets)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 84:</b> &nbsp; 2 fragments, nominal size 28 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1990')" href="javascript:;">
keras-2.3.1/tests/keras/layers/recurrent_test.py: 535-569
</a>
<div class="mid" id="frag1990" style="display:none"><pre>
def test_minimal_rnn_cell_non_layer():

    class MinimalRNNCell(object):

        def __init__(self, units, input_dim):
            self.units = units
            self.state_size = units
            self.kernel = keras.backend.variable(
                np.random.random((input_dim, units)))

        def call(self, inputs, states):
            prev_output = states[0]
            output = keras.backend.dot(inputs, self.kernel) + prev_output
            return output, [output]

    # Basic test case.
    cell = MinimalRNNCell(32, 5)
    x = keras.Input((None, 5))
    layer = recurrent.RNN(cell)
    y = layer(x)
    model = keras.models.Model(x, y)
    model.compile(optimizer='rmsprop', loss='mse')
    model.train_on_batch(np.zeros((6, 5, 5)), np.zeros((6, 32)))

    # Test stacking.
    cells = [MinimalRNNCell(8, 5),
             MinimalRNNCell(32, 8),
             MinimalRNNCell(32, 32)]
    layer = recurrent.RNN(cells)
    y = layer(x)
    model = keras.models.Model(x, y)
    model.compile(optimizer='rmsprop', loss='mse')
    model.train_on_batch(np.zeros((6, 5, 5)), np.zeros((6, 32)))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1993')" href="javascript:;">
keras-2.3.1/tests/keras/layers/recurrent_test.py: 570-608
</a>
<div class="mid" id="frag1993" style="display:none"><pre>
def test_minimal_rnn_cell_non_layer_multiple_states():

    class MinimalRNNCell(object):

        def __init__(self, units, input_dim):
            self.units = units
            self.state_size = (units, units)
            self.kernel = keras.backend.variable(
                np.random.random((input_dim, units)))

        def call(self, inputs, states):
            prev_output_1 = states[0]
            prev_output_2 = states[1]
            output = keras.backend.dot(inputs, self.kernel)
            output += prev_output_1
            output -= prev_output_2
            return output, [output * 2, output * 3]

    # Basic test case.
    cell = MinimalRNNCell(32, 5)
    x = keras.Input((None, 5))
    layer = recurrent.RNN(cell)
    y = layer(x)
    model = keras.models.Model(x, y)
    model.compile(optimizer='rmsprop', loss='mse')
    model.train_on_batch(np.zeros((6, 5, 5)), np.zeros((6, 32)))

    # Test stacking.
    cells = [MinimalRNNCell(8, 5),
             MinimalRNNCell(16, 8),
             MinimalRNNCell(32, 16)]
    layer = recurrent.RNN(cells)
    assert layer.cell.state_size == (8, 8, 16, 16, 32, 32)
    y = layer(x)
    model = keras.models.Model(x, y)
    model.compile(optimizer='rmsprop', loss='mse')
    model.train_on_batch(np.zeros((6, 5, 5)), np.zeros((6, 32)))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 85:</b> &nbsp; 4 fragments, nominal size 17 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2008')" href="javascript:;">
keras-2.3.1/tests/keras/layers/recurrent_test.py: 819-838
</a>
<div class="mid" id="frag2008" style="display:none"><pre>
        def build(self, input_shape):
            if not isinstance(input_shape, list):
                raise TypeError('expects constants shape')
            [input_shape, constant_shape] = input_shape
            # will (and should) raise if more than one constant passed

            self.input_kernel = self.add_weight(
                shape=(input_shape[-1], self.units),
                initializer='uniform',
                name='kernel')
            self.recurrent_kernel = self.add_weight(
                shape=(self.units, self.units),
                initializer='uniform',
                name='recurrent_kernel')
            self.constant_kernel = self.add_weight(
                shape=(constant_shape[-1], self.units),
                initializer='uniform',
                name='constant_kernel')
            self.built = True

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2013')" href="javascript:;">
keras-2.3.1/tests/keras/layers/recurrent_test.py: 927-946
</a>
<div class="mid" id="frag2013" style="display:none"><pre>
        def build(self, input_shape):
            if not isinstance(input_shape, list):
                raise TypeError('expects constants shape')
            [input_shape, constant_shape] = input_shape
            # will (and should) raise if more than one constant passed

            self.input_kernel = self.add_weight(
                shape=(input_shape[-1], self.units),
                initializer='uniform',
                name='kernel')
            self.recurrent_kernel = self.add_weight(
                shape=(self.units, self.units),
                initializer='uniform',
                name='recurrent_kernel')
            self.constant_kernel = self.add_weight(
                shape=(constant_shape[-1], self.units),
                initializer='uniform',
                name='constant_kernel')
            self.built = True

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2069')" href="javascript:;">
keras-2.3.1/tests/keras/layers/wrappers_test.py: 500-519
</a>
<div class="mid" id="frag2069" style="display:none"><pre>
        def build(self, input_shape):
            if not isinstance(input_shape, list):
                raise TypeError('expects constants shape')
            [input_shape, constant_shape] = input_shape
            # will (and should) raise if more than one constant passed

            self.input_kernel = self.add_weight(
                shape=(input_shape[-1], self.units),
                initializer='uniform',
                name='kernel')
            self.recurrent_kernel = self.add_weight(
                shape=(self.units, self.units),
                initializer='uniform',
                name='recurrent_kernel')
            self.constant_kernel = self.add_weight(
                shape=(constant_shape[-1], self.units),
                initializer='uniform',
                name='constant_kernel')
            self.built = True

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2064')" href="javascript:;">
keras-2.3.1/tests/keras/layers/wrappers_test.py: 420-439
</a>
<div class="mid" id="frag2064" style="display:none"><pre>
        def build(self, input_shape):
            if not isinstance(input_shape, list):
                raise TypeError('expects constants shape')
            [input_shape, constant_shape] = input_shape
            # will (and should) raise if more than one constant passed

            self.input_kernel = self.add_weight(
                shape=(input_shape[-1], self.units),
                initializer='uniform',
                name='kernel')
            self.recurrent_kernel = self.add_weight(
                shape=(self.units, self.units),
                initializer='uniform',
                name='recurrent_kernel')
            self.constant_kernel = self.add_weight(
                shape=(constant_shape[-1], self.units),
                initializer='uniform',
                name='constant_kernel')
            self.built = True

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 86:</b> &nbsp; 2 fragments, nominal size 49 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2021')" href="javascript:;">
keras-2.3.1/tests/keras/layers/cudnn_recurrent_test.py: 17-72
</a>
<div class="mid" id="frag2021" style="display:none"><pre>
def test_cudnn_rnn_canonical_to_params_lstm():
    units = 1
    input_size = 1
    layer = keras.layers.CuDNNLSTM(units)
    layer.build((None, None, input_size))

    params = layer._canonical_to_params(
        weights=[
            layer.kernel_i,
            layer.kernel_f,
            layer.kernel_c,
            layer.kernel_o,
            layer.recurrent_kernel_i,
            layer.recurrent_kernel_f,
            layer.recurrent_kernel_c,
            layer.recurrent_kernel_o,
        ],
        biases=[
            layer.bias_i_i,
            layer.bias_f_i,
            layer.bias_c_i,
            layer.bias_o_i,
            layer.bias_i,
            layer.bias_f,
            layer.bias_c,
            layer.bias_o,
        ],
    )
    ref_params = layer._cudnn_lstm.canonical_to_params(
        weights=[
            layer.kernel_i,
            layer.kernel_f,
            layer.kernel_c,
            layer.kernel_o,
            layer.recurrent_kernel_i,
            layer.recurrent_kernel_f,
            layer.recurrent_kernel_c,
            layer.recurrent_kernel_o,
        ],
        biases=[
            layer.bias_i_i,
            layer.bias_f_i,
            layer.bias_c_i,
            layer.bias_o_i,
            layer.bias_i,
            layer.bias_f,
            layer.bias_c,
            layer.bias_o,
        ],
    )
    ref_params_value = keras.backend.get_value(ref_params)
    params_value = keras.backend.get_value(params)
    diff = np.mean(ref_params_value - params_value)
    assert diff &lt; 1e-8


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2022')" href="javascript:;">
keras-2.3.1/tests/keras/layers/cudnn_recurrent_test.py: 74-121
</a>
<div class="mid" id="frag2022" style="display:none"><pre>
def test_cudnn_rnn_canonical_to_params_gru():
    units = 7
    input_size = 9
    layer = keras.layers.CuDNNGRU(units)
    layer.build((None, None, input_size))

    ref_params = layer._cudnn_gru.canonical_to_params(
        weights=[
            layer.kernel_r,
            layer.kernel_z,
            layer.kernel_h,
            layer.recurrent_kernel_r,
            layer.recurrent_kernel_z,
            layer.recurrent_kernel_h,
        ],
        biases=[
            layer.bias_r_i,
            layer.bias_z_i,
            layer.bias_h_i,
            layer.bias_r,
            layer.bias_z,
            layer.bias_h,
        ],
    )
    params = layer._canonical_to_params(
        weights=[
            layer.kernel_r,
            layer.kernel_z,
            layer.kernel_h,
            layer.recurrent_kernel_r,
            layer.recurrent_kernel_z,
            layer.recurrent_kernel_h,
        ],
        biases=[
            layer.bias_r_i,
            layer.bias_z_i,
            layer.bias_h_i,
            layer.bias_r,
            layer.bias_z,
            layer.bias_h,
        ],
    )
    ref_params_value = keras.backend.get_value(ref_params)
    params_value = keras.backend.get_value(params)
    diff = np.mean(ref_params_value - params_value)
    assert diff &lt; 1e-8


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 87:</b> &nbsp; 3 fragments, nominal size 14 lines, similarity 92%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2044')" href="javascript:;">
keras-2.3.1/tests/keras/layers/merge_test.py: 97-114
</a>
<div class="mid" id="frag2044" style="display:none"><pre>
def test_merge_average():
    i1 = layers.Input(shape=(4, 5))
    i2 = layers.Input(shape=(4, 5))
    o = layers.average([i1, i2])
    assert o._keras_shape == (None, 4, 5)
    model = models.Model([i1, i2], o)

    avg_layer = layers.Average()
    o2 = avg_layer([i1, i2])
    assert avg_layer.output_shape == (None, 4, 5)

    x1 = np.random.random((2, 4, 5))
    x2 = np.random.random((2, 4, 5))
    out = model.predict([x1, x2])
    assert out.shape == (2, 4, 5)
    assert_allclose(out, 0.5 * (x1 + x2), atol=1e-4)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2045')" href="javascript:;">
keras-2.3.1/tests/keras/layers/merge_test.py: 115-132
</a>
<div class="mid" id="frag2045" style="display:none"><pre>
def test_merge_maximum():
    i1 = layers.Input(shape=(4, 5))
    i2 = layers.Input(shape=(4, 5))
    o = layers.maximum([i1, i2])
    assert o._keras_shape == (None, 4, 5)
    model = models.Model([i1, i2], o)

    max_layer = layers.Maximum()
    o2 = max_layer([i1, i2])
    assert max_layer.output_shape == (None, 4, 5)

    x1 = np.random.random((2, 4, 5))
    x2 = np.random.random((2, 4, 5))
    out = model.predict([x1, x2])
    assert out.shape == (2, 4, 5)
    assert_allclose(out, np.maximum(x1, x2), atol=1e-4)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2046')" href="javascript:;">
keras-2.3.1/tests/keras/layers/merge_test.py: 133-150
</a>
<div class="mid" id="frag2046" style="display:none"><pre>
def test_merge_minimum():
    i1 = layers.Input(shape=(4, 5))
    i2 = layers.Input(shape=(4, 5))
    o = layers.minimum([i1, i2])
    assert o._keras_shape == (None, 4, 5)
    model = models.Model([i1, i2], o)

    max_layer = layers.Minimum()
    o2 = max_layer([i1, i2])
    assert max_layer.output_shape == (None, 4, 5)

    x1 = np.random.random((2, 4, 5))
    x2 = np.random.random((2, 4, 5))
    out = model.predict([x1, x2])
    assert out.shape == (2, 4, 5)
    assert_allclose(out, np.minimum(x1, x2), atol=1e-4)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 88:</b> &nbsp; 2 fragments, nominal size 70 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2062')" href="javascript:;">
keras-2.3.1/tests/keras/layers/wrappers_test.py: 413-492
</a>
<div class="mid" id="frag2062" style="display:none"><pre>
def test_Bidirectional_with_constants():
    class RNNCellWithConstants(Layer):
        def __init__(self, units, **kwargs):
            self.units = units
            self.state_size = units
            super(RNNCellWithConstants, self).__init__(**kwargs)

        def build(self, input_shape):
            if not isinstance(input_shape, list):
                raise TypeError('expects constants shape')
            [input_shape, constant_shape] = input_shape
            # will (and should) raise if more than one constant passed

            self.input_kernel = self.add_weight(
                shape=(input_shape[-1], self.units),
                initializer='uniform',
                name='kernel')
            self.recurrent_kernel = self.add_weight(
                shape=(self.units, self.units),
                initializer='uniform',
                name='recurrent_kernel')
            self.constant_kernel = self.add_weight(
                shape=(constant_shape[-1], self.units),
                initializer='uniform',
                name='constant_kernel')
            self.built = True

        def call(self, inputs, states, constants):
            [prev_output] = states
            [constant] = constants
            h_input = K.dot(inputs, self.input_kernel)
            h_state = K.dot(prev_output, self.recurrent_kernel)
            h_const = K.dot(constant, self.constant_kernel)
            output = h_input + h_state + h_const
            return output, [output]

        def get_config(self):
            config = {'units': self.units}
            base_config = super(RNNCellWithConstants, self).get_config()
            return dict(list(base_config.items()) + list(config.items()))

    # Test basic case.
    x = Input((5, 5))
    c = Input((3,))
    cell = RNNCellWithConstants(32)
    custom_objects = {'RNNCellWithConstants': RNNCellWithConstants}
    with CustomObjectScope(custom_objects):
        layer = wrappers.Bidirectional(RNN(cell))
    y = layer(x, constants=c)
    model = Model([x, c], y)
    model.compile(optimizer='rmsprop', loss='mse')
    model.train_on_batch(
        [np.zeros((6, 5, 5)), np.zeros((6, 3))],
        np.zeros((6, 64))
    )

    # Test basic case serialization.
    x_np = np.random.random((6, 5, 5))
    c_np = np.random.random((6, 3))
    y_np = model.predict([x_np, c_np])
    weights = model.get_weights()
    config = layer.get_config()
    with CustomObjectScope(custom_objects):
        layer = wrappers.Bidirectional.from_config(copy.deepcopy(config))
    y = layer(x, constants=c)
    model = Model([x, c], y)
    model.set_weights(weights)
    y_np_2 = model.predict([x_np, c_np])
    assert_allclose(y_np, y_np_2, atol=1e-4)

    # test flat list inputs
    with CustomObjectScope(custom_objects):
        layer = wrappers.Bidirectional.from_config(copy.deepcopy(config))
    y = layer([x, c])
    model = Model([x, c], y)
    model.set_weights(weights)
    y_np_3 = model.predict([x_np, c_np])
    assert_allclose(y_np, y_np_3, atol=1e-4)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2067')" href="javascript:;">
keras-2.3.1/tests/keras/layers/wrappers_test.py: 493-582
</a>
<div class="mid" id="frag2067" style="display:none"><pre>
def test_Bidirectional_with_constants_layer_passing_initial_state():
    class RNNCellWithConstants(Layer):
        def __init__(self, units, **kwargs):
            self.units = units
            self.state_size = units
            super(RNNCellWithConstants, self).__init__(**kwargs)

        def build(self, input_shape):
            if not isinstance(input_shape, list):
                raise TypeError('expects constants shape')
            [input_shape, constant_shape] = input_shape
            # will (and should) raise if more than one constant passed

            self.input_kernel = self.add_weight(
                shape=(input_shape[-1], self.units),
                initializer='uniform',
                name='kernel')
            self.recurrent_kernel = self.add_weight(
                shape=(self.units, self.units),
                initializer='uniform',
                name='recurrent_kernel')
            self.constant_kernel = self.add_weight(
                shape=(constant_shape[-1], self.units),
                initializer='uniform',
                name='constant_kernel')
            self.built = True

        def call(self, inputs, states, constants):
            [prev_output] = states
            [constant] = constants
            h_input = K.dot(inputs, self.input_kernel)
            h_state = K.dot(prev_output, self.recurrent_kernel)
            h_const = K.dot(constant, self.constant_kernel)
            output = h_input + h_state + h_const
            return output, [output]

        def get_config(self):
            config = {'units': self.units}
            base_config = super(RNNCellWithConstants, self).get_config()
            return dict(list(base_config.items()) + list(config.items()))

    # Test basic case.
    x = Input((5, 5))
    c = Input((3,))
    s_for = Input((32,))
    s_bac = Input((32,))
    cell = RNNCellWithConstants(32)
    custom_objects = {'RNNCellWithConstants': RNNCellWithConstants}
    with CustomObjectScope(custom_objects):
        layer = wrappers.Bidirectional(RNN(cell))
    y = layer(x, initial_state=[s_for, s_bac], constants=c)
    model = Model([x, s_for, s_bac, c], y)
    model.compile(optimizer='rmsprop', loss='mse')
    model.train_on_batch(
        [np.zeros((6, 5, 5)), np.zeros((6, 32)),
         np.zeros((6, 32)), np.zeros((6, 3))],
        np.zeros((6, 64))
    )

    # Test basic case serialization.
    x_np = np.random.random((6, 5, 5))
    s_fw_np = np.random.random((6, 32))
    s_bk_np = np.random.random((6, 32))
    c_np = np.random.random((6, 3))
    y_np = model.predict([x_np, s_fw_np, s_bk_np, c_np])
    weights = model.get_weights()
    config = layer.get_config()
    with CustomObjectScope(custom_objects):
        layer = wrappers.Bidirectional.from_config(copy.deepcopy(config))
    y = layer(x, initial_state=[s_for, s_bac], constants=c)
    model = Model([x, s_for, s_bac, c], y)
    model.set_weights(weights)
    y_np_2 = model.predict([x_np, s_fw_np, s_bk_np, c_np])
    assert_allclose(y_np, y_np_2, atol=1e-4)

    # verify that state is used
    y_np_2_different_s = model.predict([x_np, s_fw_np + 10., s_bk_np + 10., c_np])
    with pytest.raises(AssertionError):
        assert_allclose(y_np, y_np_2_different_s, atol=1e-4)

    # test flat list inputs
    with CustomObjectScope(custom_objects):
        layer = wrappers.Bidirectional.from_config(copy.deepcopy(config))
    y = layer([x, s_for, s_bac, c])
    model = Model([x, s_for, s_bac, c], y)
    model.set_weights(weights)
    y_np_3 = model.predict([x_np, s_fw_np, s_bk_np, c_np])
    assert_allclose(y_np, y_np_3, atol=1e-4)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 89:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2073')" href="javascript:;">
keras-2.3.1/tests/keras/layers/wrappers_test.py: 595-609
</a>
<div class="mid" id="frag2073" style="display:none"><pre>
def test_Bidirectional_updates():
    x = Input(shape=(3, 2))
    layer = wrappers.Bidirectional(layers.SimpleRNN(3))
    assert len(layer.updates) == 0
    assert len(layer.get_updates_for(None)) == 0
    assert len(layer.get_updates_for(x)) == 0
    layer.forward_layer.add_update(0, inputs=x)
    layer.forward_layer.add_update(1, inputs=None)
    layer.backward_layer.add_update(0, inputs=x)
    layer.backward_layer.add_update(1, inputs=None)
    assert len(layer.updates) == 4
    assert len(layer.get_updates_for(None)) == 2
    assert len(layer.get_updates_for(x)) == 2


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2074')" href="javascript:;">
keras-2.3.1/tests/keras/layers/wrappers_test.py: 610-626
</a>
<div class="mid" id="frag2074" style="display:none"><pre>
def test_Bidirectional_losses():
    x = Input(shape=(3, 2))
    layer = wrappers.Bidirectional(
        layers.SimpleRNN(3, kernel_regularizer='l1', bias_regularizer='l1'))
    _ = layer(x)
    assert len(layer.losses) == 4
    assert len(layer.get_losses_for(None)) == 4
    assert len(layer.get_losses_for(x)) == 0
    layer.forward_layer.add_loss(0, inputs=x)
    layer.forward_layer.add_loss(1, inputs=None)
    layer.backward_layer.add_loss(0, inputs=x)
    layer.backward_layer.add_loss(1, inputs=None)
    assert len(layer.losses) == 8
    assert len(layer.get_losses_for(None)) == 6
    assert len(layer.get_losses_for(x)) == 2


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 90:</b> &nbsp; 2 fragments, nominal size 30 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2091')" href="javascript:;">
keras-2.3.1/tests/integration_tests/test_image_data_tasks.py: 12-44
</a>
<div class="mid" id="frag2091" style="display:none"><pre>
def test_image_classification():
    np.random.seed(1337)
    input_shape = (16, 16, 3)
    (x_train, y_train), (x_test, y_test) = get_test_data(num_train=500,
                                                         num_test=200,
                                                         input_shape=input_shape,
                                                         classification=True,
                                                         num_classes=4)
    y_train = to_categorical(y_train)
    y_test = to_categorical(y_test)

    model = Sequential([
        layers.Conv2D(filters=8, kernel_size=3,
                      activation='relu',
                      input_shape=input_shape),
        layers.MaxPooling2D(pool_size=2),
        layers.Conv2D(filters=4, kernel_size=(3, 3),
                      activation='relu', padding='same'),
        layers.GlobalAveragePooling2D(),
        layers.Dense(y_test.shape[-1], activation='softmax')
    ])
    model.compile(loss='categorical_crossentropy',
                  optimizer='rmsprop',
                  metrics=['accuracy'])
    model.summary()
    history = model.fit(x_train, y_train, epochs=12, batch_size=16,
                        validation_data=(x_test, y_test),
                        verbose=0)
    assert history.history['val_accuracy'][-1] &gt; 0.75
    config = model.get_config()
    model = Sequential.from_config(config)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2092')" href="javascript:;">
keras-2.3.1/tests/integration_tests/test_image_data_tasks.py: 45-78
</a>
<div class="mid" id="frag2092" style="display:none"><pre>
def test_image_data_generator_training():
    np.random.seed(1337)
    img_gen = ImageDataGenerator(rescale=1.)  # Dummy ImageDataGenerator
    input_shape = (16, 16, 3)
    (x_train, y_train), (x_test, y_test) = get_test_data(num_train=500,
                                                         num_test=200,
                                                         input_shape=input_shape,
                                                         classification=True,
                                                         num_classes=4)
    y_train = to_categorical(y_train)
    y_test = to_categorical(y_test)

    model = Sequential([
        layers.Conv2D(filters=8, kernel_size=3,
                      activation='relu',
                      input_shape=input_shape),
        layers.MaxPooling2D(pool_size=2),
        layers.Conv2D(filters=4, kernel_size=(3, 3),
                      activation='relu', padding='same'),
        layers.GlobalAveragePooling2D(),
        layers.Dense(y_test.shape[-1], activation='softmax')
    ])
    model.compile(loss='categorical_crossentropy',
                  optimizer='rmsprop',
                  metrics=['accuracy'])
    history = model.fit_generator(img_gen.flow(x_train, y_train, batch_size=16),
                                  epochs=15,
                                  validation_data=img_gen.flow(x_test, y_test,
                                                               batch_size=16),
                                  verbose=0)
    assert history.history['val_accuracy'][-1] &gt; 0.70
    model.evaluate_generator(img_gen.flow(x_train, y_train, batch_size=16))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 91:</b> &nbsp; 2 fragments, nominal size 23 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2102')" href="javascript:;">
keras-2.3.1/tests/integration_tests/test_temporal_data_tasks.py: 14-45
</a>
<div class="mid" id="frag2102" style="display:none"><pre>
def test_temporal_classification():
    '''
    Classify temporal sequences of float numbers
    of length 3 into 2 classes using
    single layer of GRU units and softmax applied
    to the last activations of the units
    '''
    np.random.seed(1337)
    (x_train, y_train), (x_test, y_test) = get_test_data(num_train=200,
                                                         num_test=20,
                                                         input_shape=(3, 4),
                                                         classification=True,
                                                         num_classes=2)
    y_train = to_categorical(y_train)
    y_test = to_categorical(y_test)

    model = Sequential()
    model.add(layers.GRU(8,
                         input_shape=(x_train.shape[1], x_train.shape[2])))
    model.add(layers.Dense(y_train.shape[-1], activation='softmax'))
    model.compile(loss='categorical_crossentropy',
                  optimizer='rmsprop',
                  metrics=['accuracy'])
    model.summary()
    history = model.fit(x_train, y_train, epochs=5, batch_size=10,
                        validation_data=(x_test, y_test),
                        verbose=0)
    assert(history.history['accuracy'][-1] &gt;= 0.8)
    config = model.get_config()
    model = Sequential.from_config(config)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2103')" href="javascript:;">
keras-2.3.1/tests/integration_tests/test_temporal_data_tasks.py: 46-74
</a>
<div class="mid" id="frag2103" style="display:none"><pre>
def test_temporal_classification_functional():
    '''
    Classify temporal sequences of float numbers
    of length 3 into 2 classes using
    single layer of GRU units and softmax applied
    to the last activations of the units
    '''
    np.random.seed(1337)
    (x_train, y_train), (x_test, y_test) = get_test_data(num_train=200,
                                                         num_test=20,
                                                         input_shape=(3, 4),
                                                         classification=True,
                                                         num_classes=2)
    y_train = to_categorical(y_train)
    y_test = to_categorical(y_test)

    inputs = layers.Input(shape=(x_train.shape[1], x_train.shape[2]))
    x = layers.SimpleRNN(8)(inputs)
    outputs = layers.Dense(y_train.shape[-1], activation='softmax')(x)
    model = keras.models.Model(inputs, outputs)
    model.compile(loss='categorical_crossentropy',
                  optimizer='rmsprop',
                  metrics=['accuracy'])
    history = model.fit(x_train, y_train, epochs=5, batch_size=10,
                        validation_data=(x_test, y_test),
                        verbose=0)
    assert(history.history['accuracy'][-1] &gt;= 0.75)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 92:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2104')" href="javascript:;">
keras-2.3.1/tests/integration_tests/test_temporal_data_tasks.py: 75-94
</a>
<div class="mid" id="frag2104" style="display:none"><pre>
def test_temporal_regression():
    '''
    Predict float numbers (regression) based on sequences
    of float numbers of length 3 using a single layer of GRU units
    '''
    np.random.seed(1337)
    (x_train, y_train), (x_test, y_test) = get_test_data(num_train=200,
                                                         num_test=20,
                                                         input_shape=(3, 5),
                                                         output_shape=(2,),
                                                         classification=False)
    model = Sequential()
    model.add(layers.LSTM(y_train.shape[-1],
                          input_shape=(x_train.shape[1], x_train.shape[2])))
    model.compile(loss='hinge', optimizer='adam')
    history = model.fit(x_train, y_train, epochs=5, batch_size=16,
                        validation_data=(x_test, y_test), verbose=0)
    assert(history.history['loss'][-1] &lt; 1.)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2105')" href="javascript:;">
keras-2.3.1/tests/integration_tests/test_temporal_data_tasks.py: 95-117
</a>
<div class="mid" id="frag2105" style="display:none"><pre>
def test_3d_to_3d():
    '''
    Apply a same Dense layer for each element of time dimension of the input
    and make predictions of the output sequence elements.
    This does not make use of the temporal structure of the sequence
    (see TimeDistributedDense for more details)
    '''
    np.random.seed(1337)
    (x_train, y_train), (x_test, y_test) = get_test_data(num_train=100,
                                                         num_test=20,
                                                         input_shape=(3, 5),
                                                         output_shape=(3, 5),
                                                         classification=False)

    model = Sequential()
    model.add(layers.TimeDistributed(
        layers.Dense(y_train.shape[-1]), input_shape=x_train.shape[1:3]))
    model.compile(loss='hinge', optimizer='rmsprop')
    history = model.fit(x_train, y_train, epochs=20, batch_size=16,
                        validation_data=(x_test, y_test), verbose=0)
    assert(history.history['loss'][-1] &lt; 1.)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 93:</b> &nbsp; 2 fragments, nominal size 19 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2153')" href="javascript:;">
keras-2.3.1/tests/test_loss_weighting.py: 74-98
</a>
<div class="mid" id="frag2153" style="display:none"><pre>
def test_sequential_class_weights():
    model = create_sequential_model()
    model.compile(loss=loss, optimizer='rmsprop')

    ((x_train, y_train), (x_test, y_test),
     (sample_weight, class_weight, test_ids)) = _get_test_data()

    model.fit(x_train, y_train, batch_size=batch_size,
              epochs=epochs // 3, verbose=0,
              class_weight=class_weight,
              validation_data=(x_train, y_train, sample_weight))
    model.fit(x_train, y_train, batch_size=batch_size,
              epochs=epochs // 2, verbose=0,
              class_weight=class_weight)
    model.fit(x_train, y_train, batch_size=batch_size,
              epochs=epochs // 2, verbose=0,
              class_weight=class_weight,
              validation_split=0.1)

    model.train_on_batch(x_train[:32], y_train[:32],
                         class_weight=class_weight)
    score = model.evaluate(x_test[test_ids, :], y_test[test_ids, :], verbose=0)
    assert(score &lt; standard_score_sequential)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2154')" href="javascript:;">
keras-2.3.1/tests/test_loss_weighting.py: 99-121
</a>
<div class="mid" id="frag2154" style="display:none"><pre>
def test_sequential_sample_weights():
    model = create_sequential_model()
    model.compile(loss=loss, optimizer='rmsprop')

    ((x_train, y_train), (x_test, y_test),
     (sample_weight, class_weight, test_ids)) = _get_test_data()

    model.fit(x_train, y_train, batch_size=batch_size,
              epochs=epochs // 3, verbose=0,
              sample_weight=sample_weight)
    model.fit(x_train, y_train, batch_size=batch_size,
              epochs=epochs // 3, verbose=0,
              sample_weight=sample_weight,
              validation_split=0.1)

    model.train_on_batch(x_train[:32], y_train[:32],
                         sample_weight=sample_weight[:32])
    model.test_on_batch(x_train[:32], y_train[:32],
                        sample_weight=sample_weight[:32])
    score = model.evaluate(x_test[test_ids, :], y_test[test_ids, :], verbose=0)
    assert(score &lt; standard_score_sequential)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 94:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2163')" href="javascript:;">
keras-2.3.1/tests/test_model_saving.py: 185-199
</a>
<div class="mid" id="frag2163" style="display:none"><pre>
def test_model_saving_to_binary_stream():
    model, x = _get_sample_model_and_input()
    out = model.predict(x)

    with temp_filename('h5') as fname:
        # save directly to binary file
        with open(fname, 'wb') as raw_file:
            save_model(model, raw_file)
        # Load the data the usual way, and make sure the model is intact.
        with h5py.File(fname, mode='r') as h5file:
            loaded_model = load_model(h5file)
    out2 = loaded_model.predict(x)
    assert_allclose(out, out2, atol=1e-05)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2164')" href="javascript:;">
keras-2.3.1/tests/test_model_saving.py: 200-214
</a>
<div class="mid" id="frag2164" style="display:none"><pre>
def test_model_loading_from_binary_stream():
    model, x = _get_sample_model_and_input()
    out = model.predict(x)

    with temp_filename('h5') as fname:
        # save the model the usual way
        with h5py.File(fname, mode='w') as h5file:
            save_model(model, h5file)
        # Load the data binary, and make sure the model is intact.
        with open(fname, 'rb') as raw_file:
            loaded_model = load_model(raw_file)
    out2 = loaded_model.predict(x)
    assert_allclose(out, out2, atol=1e-05)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 95:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2168')" href="javascript:;">
keras-2.3.1/tests/test_model_saving.py: 268-280
</a>
<div class="mid" id="frag2168" style="display:none"><pre>
def test_saving_right_after_compilation():
    model = Sequential()
    model.add(Dense(2, input_shape=(3,)))
    model.add(Dense(3))
    model.compile(loss='mse', optimizer='sgd', metrics=['acc'])
    model._make_train_function()

    _, fname = tempfile.mkstemp('.h5')
    save_model(model, fname)
    model = load_model(fname)
    os.remove(fname)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2182')" href="javascript:;">
keras-2.3.1/tests/test_model_saving.py: 696-710
</a>
<div class="mid" id="frag2182" style="display:none"><pre>
def test_saving_constant_initializer_with_numpy():
    """Test saving and loading model of constant initializer with numpy inputs.
    """
    model = Sequential()
    model.add(Dense(2, input_shape=(3,),
                    kernel_initializer=Constant(np.ones((3, 2)))))
    model.add(Dense(3))
    model.compile(loss='mse', optimizer='sgd', metrics=['acc'])

    _, fname = tempfile.mkstemp('.h5')
    save_model(model, fname)
    model = load_model(fname)
    os.remove(fname)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 96:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2174')" href="javascript:;">
keras-2.3.1/tests/test_model_saving.py: 485-508
</a>
<div class="mid" id="frag2174" style="display:none"><pre>
def test_saving_lambda_custom_objects():
    inputs = Input(shape=(3,))
    x = Lambda(lambda x: square_fn(x), output_shape=(3,))(inputs)
    outputs = Dense(3)(x)

    model = Model(inputs, outputs)
    model.compile(loss=losses.MSE,
                  optimizer=optimizers.RMSprop(lr=0.0001),
                  metrics=[metrics.categorical_accuracy])
    x = np.random.random((1, 3))
    y = np.random.random((1, 3))
    model.train_on_batch(x, y)

    out = model.predict(x)
    _, fname = tempfile.mkstemp('.h5')
    save_model(model, fname)

    model = load_model(fname, custom_objects={'square_fn': square_fn})
    os.remove(fname)

    out2 = model.predict(x)
    assert_allclose(out, out2, atol=1e-05)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2176')" href="javascript:;">
keras-2.3.1/tests/test_model_saving.py: 528-550
</a>
<div class="mid" id="frag2176" style="display:none"><pre>
def test_saving_custom_activation_function():
    x = Input(shape=(3,))
    output = Dense(3, activation=K.cos)(x)

    model = Model(x, output)
    model.compile(loss=losses.MSE,
                  optimizer=optimizers.RMSprop(lr=0.0001),
                  metrics=[metrics.categorical_accuracy])
    x = np.random.random((1, 3))
    y = np.random.random((1, 3))
    model.train_on_batch(x, y)

    out = model.predict(x)
    _, fname = tempfile.mkstemp('.h5')
    save_model(model, fname)

    model = load_model(fname, custom_objects={'cos': K.cos})
    os.remove(fname)

    out2 = model.predict(x)
    assert_allclose(out, out2, atol=1e-05)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 97:</b> &nbsp; 2 fragments, nominal size 23 lines, similarity 87%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2184')" href="javascript:;">
keras-2.3.1/tests/test_model_saving.py: 732-759
</a>
<div class="mid" id="frag2184" style="display:none"><pre>
def test_saving_overwrite_option():
    model = Sequential()
    model.add(Dense(2, input_shape=(3,)))
    org_weights = model.get_weights()
    new_weights = [np.random.random(w.shape) for w in org_weights]

    _, fname = tempfile.mkstemp('.h5')
    save_model(model, fname)
    model.set_weights(new_weights)

    with patch('keras.engine.saving.ask_to_proceed_with_overwrite') as ask:
        ask.return_value = False
        save_model(model, fname, overwrite=False)
        ask.assert_called_once()
        new_model = load_model(fname)
        for w, org_w in zip(new_model.get_weights(), org_weights):
            assert_allclose(w, org_w)

        ask.return_value = True
        save_model(model, fname, overwrite=False)
        assert ask.call_count == 2
        new_model = load_model(fname)
        for w, new_w in zip(new_model.get_weights(), new_weights):
            assert_allclose(w, new_w)

    os.remove(fname)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2185')" href="javascript:;">
keras-2.3.1/tests/test_model_saving.py: 760-791
</a>
<div class="mid" id="frag2185" style="display:none"><pre>
def test_saving_overwrite_option_gcs():
    model = Sequential()
    model.add(Dense(2, input_shape=(3,)))
    org_weights = model.get_weights()
    new_weights = [np.random.random(w.shape) for w in org_weights]

    with tf_file_io_proxy('keras.engine.saving.tf_file_io') as file_io_proxy:
        gcs_filepath = file_io_proxy.get_filepath(
            filename='test_saving_overwrite_option_gcs.h5')
        # we should not use same filename in several tests to allow for parallel
        # execution
        save_model(model, gcs_filepath)
        model.set_weights(new_weights)

        with patch('keras.engine.saving.ask_to_proceed_with_overwrite') as ask:
            ask.return_value = False
            save_model(model, gcs_filepath, overwrite=False)
            ask.assert_called_once()
            new_model = load_model(gcs_filepath)
            for w, org_w in zip(new_model.get_weights(), org_weights):
                assert_allclose(w, org_w)

            ask.return_value = True
            save_model(model, gcs_filepath, overwrite=False)
            assert ask.call_count == 2
            new_model = load_model(gcs_filepath)
            for w, new_w in zip(new_model.get_weights(), new_weights):
                assert_allclose(w, new_w)

        file_io_proxy.delete_file(gcs_filepath)  # cleanup


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 98:</b> &nbsp; 2 fragments, nominal size 43 lines, similarity 97%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2200')" href="javascript:;">
keras-2.3.1/examples/variational_autoencoder_deconv.py: 56-118
</a>
<div class="mid" id="frag2200" style="display:none"><pre>
def plot_results(models,
                 data,
                 batch_size=128,
                 model_name="vae_mnist"):
    """Plots labels and MNIST digits as function of 2-dim latent vector

    # Arguments
        models (tuple): encoder and decoder models
        data (tuple): test data and label
        batch_size (int): prediction batch size
        model_name (string): which model is using this function
    """

    encoder, decoder = models
    x_test, y_test = data
    os.makedirs(model_name, exist_ok=True)

    filename = os.path.join(model_name, "vae_mean.png")
    # display a 2D plot of the digit classes in the latent space
    z_mean, _, _ = encoder.predict(x_test,
                                   batch_size=batch_size)
    plt.figure(figsize=(12, 10))
    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test)
    plt.colorbar()
    plt.xlabel("z[0]")
    plt.ylabel("z[1]")
    plt.savefig(filename)
    plt.show()

    filename = os.path.join(model_name, "digits_over_latent.png")
    # display a 30x30 2D manifold of digits
    n = 30
    digit_size = 28
    figure = np.zeros((digit_size * n, digit_size * n))
    # linearly spaced coordinates corresponding to the 2D plot
    # of digit classes in the latent space
    grid_x = np.linspace(-4, 4, n)
    grid_y = np.linspace(-4, 4, n)[::-1]

    for i, yi in enumerate(grid_y):
        for j, xi in enumerate(grid_x):
            z_sample = np.array([[xi, yi]])
            x_decoded = decoder.predict(z_sample)
            digit = x_decoded[0].reshape(digit_size, digit_size)
            figure[i * digit_size: (i + 1) * digit_size,
                   j * digit_size: (j + 1) * digit_size] = digit

    plt.figure(figsize=(10, 10))
    start_range = digit_size // 2
    end_range = n * digit_size + start_range + 1
    pixel_range = np.arange(start_range, end_range, digit_size)
    sample_range_x = np.round(grid_x, 1)
    sample_range_y = np.round(grid_y, 1)
    plt.xticks(pixel_range, sample_range_x)
    plt.yticks(pixel_range, sample_range_y)
    plt.xlabel("z[0]")
    plt.ylabel("z[1]")
    plt.imshow(figure, cmap='Greys_r')
    plt.savefig(filename)
    plt.show()


# MNIST dataset
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2269')" href="javascript:;">
keras-2.3.1/examples/variational_autoencoder.py: 54-116
</a>
<div class="mid" id="frag2269" style="display:none"><pre>
def plot_results(models,
                 data,
                 batch_size=128,
                 model_name="vae_mnist"):
    """Plots labels and MNIST digits as a function of the 2D latent vector

    # Arguments
        models (tuple): encoder and decoder models
        data (tuple): test data and label
        batch_size (int): prediction batch size
        model_name (string): which model is using this function
    """

    encoder, decoder = models
    x_test, y_test = data
    os.makedirs(model_name, exist_ok=True)

    filename = os.path.join(model_name, "vae_mean.png")
    # display a 2D plot of the digit classes in the latent space
    z_mean, _, _ = encoder.predict(x_test,
                                   batch_size=batch_size)
    plt.figure(figsize=(12, 10))
    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test)
    plt.colorbar()
    plt.xlabel("z[0]")
    plt.ylabel("z[1]")
    plt.savefig(filename)
    plt.show()

    filename = os.path.join(model_name, "digits_over_latent.png")
    # display a 30x30 2D manifold of digits
    n = 30
    digit_size = 28
    figure = np.zeros((digit_size * n, digit_size * n))
    # linearly spaced coordinates corresponding to the 2D plot
    # of digit classes in the latent space
    grid_x = np.linspace(-4, 4, n)
    grid_y = np.linspace(-4, 4, n)[::-1]

    for i, yi in enumerate(grid_y):
        for j, xi in enumerate(grid_x):
            z_sample = np.array([[xi, yi]])
            x_decoded = decoder.predict(z_sample)
            digit = x_decoded[0].reshape(digit_size, digit_size)
            figure[i * digit_size: (i + 1) * digit_size,
                   j * digit_size: (j + 1) * digit_size] = digit

    plt.figure(figsize=(10, 10))
    start_range = digit_size // 2
    end_range = (n - 1) * digit_size + start_range + 1
    pixel_range = np.arange(start_range, end_range, digit_size)
    sample_range_x = np.round(grid_x, 1)
    sample_range_y = np.round(grid_y, 1)
    plt.xticks(pixel_range, sample_range_x)
    plt.yticks(pixel_range, sample_range_y)
    plt.xlabel("z[0]")
    plt.ylabel("z[1]")
    plt.imshow(figure, cmap='Greys_r')
    plt.savefig(filename)
    plt.show()


# MNIST dataset
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 99:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2217')" href="javascript:;">
keras-2.3.1/examples/neural_style_transfer.py: 110-125
</a>
<div class="mid" id="frag2217" style="display:none"><pre>
def deprocess_image(x):
    if K.image_data_format() == 'channels_first':
        x = x.reshape((3, img_nrows, img_ncols))
        x = x.transpose((1, 2, 0))
    else:
        x = x.reshape((img_nrows, img_ncols, 3))
    # Remove zero-center by mean pixel
    x[:, :, 0] += 103.939
    x[:, :, 1] += 116.779
    x[:, :, 2] += 123.68
    # 'BGR'-&gt;'RGB'
    x = x[:, :, ::-1]
    x = np.clip(x, 0, 255).astype('uint8')
    return x

# get tensor representations of our images
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2252')" href="javascript:;">
keras-2.3.1/examples/neural_doodle.py: 114-129
</a>
<div class="mid" id="frag2252" style="display:none"><pre>
def deprocess_image(x):
    if K.image_data_format() == 'channels_first':
        x = x.reshape((3, img_nrows, img_ncols))
        x = x.transpose((1, 2, 0))
    else:
        x = x.reshape((img_nrows, img_ncols, 3))
    # Remove zero-center by mean pixel
    x[:, :, 0] += 103.939
    x[:, :, 1] += 116.779
    x[:, :, 2] += 123.68
    # 'BGR'-&gt;'RGB'
    x = x[:, :, ::-1]
    x = np.clip(x, 0, 255).astype('uint8')
    return x


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 100:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2222')" href="javascript:;">
keras-2.3.1/examples/neural_style_transfer.py: 238-258
</a>
<div class="mid" id="frag2222" style="display:none"><pre>
def eval_loss_and_grads(x):
    if K.image_data_format() == 'channels_first':
        x = x.reshape((1, 3, img_nrows, img_ncols))
    else:
        x = x.reshape((1, img_nrows, img_ncols, 3))
    outs = f_outputs([x])
    loss_value = outs[0]
    if len(outs[1:]) == 1:
        grad_values = outs[1].flatten().astype('float64')
    else:
        grad_values = np.array(outs[1:]).flatten().astype('float64')
    return loss_value, grad_values

# this Evaluator class makes it possible
# to compute loss and gradients in one pass
# while retrieving them via two separate functions,
# "loss" and "grads". This is done because scipy.optimize
# requires separate functions for loss and gradients,
# but computing them separately would be inefficient.


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2260')" href="javascript:;">
keras-2.3.1/examples/neural_doodle.py: 327-340
</a>
<div class="mid" id="frag2260" style="display:none"><pre>
def eval_loss_and_grads(x):
    if K.image_data_format() == 'channels_first':
        x = x.reshape((1, 3, img_nrows, img_ncols))
    else:
        x = x.reshape((1, img_nrows, img_ncols, 3))
    outs = f_outputs([x])
    loss_value = outs[0]
    if len(outs[1:]) == 1:
        grad_values = outs[1].flatten().astype('float64')
    else:
        grad_values = np.array(outs[1:]).flatten().astype('float64')
    return loss_value, grad_values


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 101:</b> &nbsp; 2 fragments, nominal size 19 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2226')" href="javascript:;">
keras-2.3.1/examples/lstm_seq2seq_restore.py: 106-143
</a>
<div class="mid" id="frag2226" style="display:none"><pre>
def decode_sequence(input_seq):
    # Encode the input as state vectors.
    states_value = encoder_model.predict(input_seq)

    # Generate empty target sequence of length 1.
    target_seq = np.zeros((1, 1, num_decoder_tokens))
    # Populate the first character of target sequence with the start character.
    target_seq[0, 0, target_token_index['\t']] = 1.

    # Sampling loop for a batch of sequences
    # (to simplify, here we assume a batch of size 1).
    stop_condition = False
    decoded_sentence = ''
    while not stop_condition:
        output_tokens, h, c = decoder_model.predict(
            [target_seq] + states_value)

        # Sample a token
        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        sampled_char = reverse_target_char_index[sampled_token_index]
        decoded_sentence += sampled_char

        # Exit condition: either hit max length
        # or find stop character.
        if (sampled_char == '\n' or
           len(decoded_sentence) &gt; max_decoder_seq_length):
            stop_condition = True

        # Update the target sequence (of length 1).
        target_seq = np.zeros((1, 1, num_decoder_tokens))
        target_seq[0, 0, sampled_token_index] = 1.

        # Update states
        states_value = [h, c]

    return decoded_sentence


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2302')" href="javascript:;">
keras-2.3.1/examples/lstm_seq2seq.py: 189-226
</a>
<div class="mid" id="frag2302" style="display:none"><pre>
def decode_sequence(input_seq):
    # Encode the input as state vectors.
    states_value = encoder_model.predict(input_seq)

    # Generate empty target sequence of length 1.
    target_seq = np.zeros((1, 1, num_decoder_tokens))
    # Populate the first character of target sequence with the start character.
    target_seq[0, 0, target_token_index['\t']] = 1.

    # Sampling loop for a batch of sequences
    # (to simplify, here we assume a batch of size 1).
    stop_condition = False
    decoded_sentence = ''
    while not stop_condition:
        output_tokens, h, c = decoder_model.predict(
            [target_seq] + states_value)

        # Sample a token
        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        sampled_char = reverse_target_char_index[sampled_token_index]
        decoded_sentence += sampled_char

        # Exit condition: either hit max length
        # or find stop character.
        if (sampled_char == '\n' or
           len(decoded_sentence) &gt; max_decoder_seq_length):
            stop_condition = True

        # Update the target sequence (of length 1).
        target_seq = np.zeros((1, 1, num_decoder_tokens))
        target_seq[0, 0, sampled_token_index] = 1.

        # Update states
        states_value = [h, c]

    return decoded_sentence


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 102:</b> &nbsp; 2 fragments, nominal size 31 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2234')" href="javascript:;">
keras-2.3.1/examples/mnist_net2net.py: 100-151
</a>
<div class="mid" id="frag2234" style="display:none"><pre>
def wider2net_conv2d(teacher_w1, teacher_b1, teacher_w2, new_width, init):
    '''Get initial weights for a wider conv2d layer with a bigger filters,
    by 'random-padding' or 'net2wider'.

    # Arguments
        teacher_w1: `weight` of conv2d layer to become wider,
          of shape (filters1, num_channel1, kh1, kw1)
        teacher_b1: `bias` of conv2d layer to become wider,
          of shape (filters1, )
        teacher_w2: `weight` of next connected conv2d layer,
          of shape (filters2, num_channel2, kh2, kw2)
        new_width: new `filters` for the wider conv2d layer
        init: initialization algorithm for new weights,
          either 'random-pad' or 'net2wider'
    '''
    assert teacher_w1.shape[0] == teacher_w2.shape[1], (
        'successive layers from teacher model should have compatible shapes')
    assert teacher_w1.shape[3] == teacher_b1.shape[0], (
        'weight and bias from same layer should have compatible shapes')
    assert new_width &gt; teacher_w1.shape[3], (
        'new width (filters) should be bigger than the existing one')

    n = new_width - teacher_w1.shape[3]
    if init == 'random-pad':
        new_w1 = np.random.normal(0, 0.1, size=teacher_w1.shape[:3] + (n,))
        new_b1 = np.ones(n) * 0.1
        new_w2 = np.random.normal(
            0, 0.1,
            size=teacher_w2.shape[:2] + (n, teacher_w2.shape[3]))
    elif init == 'net2wider':
        index = np.random.randint(teacher_w1.shape[3], size=n)
        factors = np.bincount(index)[index] + 1.
        new_w1 = teacher_w1[:, :, :, index]
        new_b1 = teacher_b1[index]
        new_w2 = teacher_w2[:, :, index, :] / factors.reshape((1, 1, -1, 1))
    else:
        raise ValueError('Unsupported weight initializer: %s' % init)

    student_w1 = np.concatenate((teacher_w1, new_w1), axis=3)
    if init == 'random-pad':
        student_w2 = np.concatenate((teacher_w2, new_w2), axis=2)
    elif init == 'net2wider':
        # add small noise to break symmetry, so that student model will have
        # full capacity later
        noise = np.random.normal(0, 5e-2 * new_w2.std(), size=new_w2.shape)
        student_w2 = np.concatenate((teacher_w2, new_w2 + noise), axis=2)
        student_w2[:, :, index, :] = new_w2
    student_b1 = np.concatenate((teacher_b1, new_b1), axis=0)

    return student_w1, student_b1, student_w2


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2235')" href="javascript:;">
keras-2.3.1/examples/mnist_net2net.py: 152-201
</a>
<div class="mid" id="frag2235" style="display:none"><pre>
def wider2net_fc(teacher_w1, teacher_b1, teacher_w2, new_width, init):
    '''Get initial weights for a wider fully connected (dense) layer
       with a bigger nout, by 'random-padding' or 'net2wider'.

    # Arguments
        teacher_w1: `weight` of fc layer to become wider,
          of shape (nin1, nout1)
        teacher_b1: `bias` of fc layer to become wider,
          of shape (nout1, )
        teacher_w2: `weight` of next connected fc layer,
          of shape (nin2, nout2)
        new_width: new `nout` for the wider fc layer
        init: initialization algorithm for new weights,
          either 'random-pad' or 'net2wider'
    '''
    assert teacher_w1.shape[1] == teacher_w2.shape[0], (
        'successive layers from teacher model should have compatible shapes')
    assert teacher_w1.shape[1] == teacher_b1.shape[0], (
        'weight and bias from same layer should have compatible shapes')
    assert new_width &gt; teacher_w1.shape[1], (
        'new width (nout) should be bigger than the existing one')

    n = new_width - teacher_w1.shape[1]
    if init == 'random-pad':
        new_w1 = np.random.normal(0, 0.1, size=(teacher_w1.shape[0], n))
        new_b1 = np.ones(n) * 0.1
        new_w2 = np.random.normal(0, 0.1, size=(n, teacher_w2.shape[1]))
    elif init == 'net2wider':
        index = np.random.randint(teacher_w1.shape[1], size=n)
        factors = np.bincount(index)[index] + 1.
        new_w1 = teacher_w1[:, index]
        new_b1 = teacher_b1[index]
        new_w2 = teacher_w2[index, :] / factors[:, np.newaxis]
    else:
        raise ValueError('Unsupported weight initializer: %s' % init)

    student_w1 = np.concatenate((teacher_w1, new_w1), axis=1)
    if init == 'random-pad':
        student_w2 = np.concatenate((teacher_w2, new_w2), axis=0)
    elif init == 'net2wider':
        # add small noise to break symmetry, so that student model will have
        # full capacity later
        noise = np.random.normal(0, 5e-2 * new_w2.std(), size=new_w2.shape)
        student_w2 = np.concatenate((teacher_w2, new_w2 + noise), axis=0)
        student_w2[index, :] = new_w2
    student_b1 = np.concatenate((teacher_b1, new_b1), axis=0)

    return student_w1, student_b1, student_w2


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 103:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2241')" href="javascript:;">
keras-2.3.1/examples/mnist_net2net.py: 352-372
</a>
<div class="mid" id="frag2241" style="display:none"><pre>
def net2wider_experiment():
    '''Benchmark performances of
    (1) a wider student model with `random_pad` initializer
    (2) a wider student model with `Net2WiderNet` initializer
    '''
    print('\nExperiment of Net2WiderNet ...')

    print('\n(1) building wider student model by random padding ...')
    make_wider_student_model(teacher_model,
                             x_train, y_train,
                             x_test, y_test,
                             init='random-pad',
                             epochs=epochs)
    print('\n(2) building wider student model by net2wider ...')
    make_wider_student_model(teacher_model,
                             x_train, y_train,
                             x_test, y_test,
                             init='net2wider',
                             epochs=epochs)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2242')" href="javascript:;">
keras-2.3.1/examples/mnist_net2net.py: 373-393
</a>
<div class="mid" id="frag2242" style="display:none"><pre>
def net2deeper_experiment():
    '''Benchmark performances of
    (3) a deeper student model with `random_init` initializer
    (4) a deeper student model with `Net2DeeperNet` initializer
    '''
    print('\nExperiment of Net2DeeperNet ...')

    print('\n(3) building deeper student model by random init ...')
    make_deeper_student_model(teacher_model,
                              x_train, y_train,
                              x_test, y_test,
                              init='random-init',
                              epochs=epochs)
    print('\n(4) building deeper student model by net2deeper ...')
    make_deeper_student_model(teacher_model,
                              x_train, y_train,
                              x_test, y_test,
                              init='net2deeper',
                              epochs=epochs)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 104:</b> &nbsp; 2 fragments, nominal size 24 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2248')" href="javascript:;">
keras-2.3.1/examples/babi_rnn.py: 85-116
</a>
<div class="mid" id="frag2248" style="display:none"><pre>
def parse_stories(lines, only_supporting=False):
    '''Parse stories provided in the bAbi tasks format

    If only_supporting is true,
    only the sentences that support the answer are kept.
    '''
    data = []
    story = []
    for line in lines:
        line = line.decode('utf-8').strip()
        nid, line = line.split(' ', 1)
        nid = int(nid)
        if nid == 1:
            story = []
        if '\t' in line:
            q, a, supporting = line.split('\t')
            q = tokenize(q)
            if only_supporting:
                # Only select the related substory
                supporting = map(int, supporting.split())
                substory = [story[i - 1] for i in supporting]
            else:
                # Provide all the substories
                substory = [x for x in story if x]
            data.append((substory, q, a))
            story.append('')
        else:
            sent = tokenize(line)
            story.append(sent)
    return data


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2306')" href="javascript:;">
keras-2.3.1/examples/babi_memnn.py: 40-71
</a>
<div class="mid" id="frag2306" style="display:none"><pre>
def parse_stories(lines, only_supporting=False):
    '''Parse stories provided in the bAbi tasks format

    If only_supporting is true, only the sentences
    that support the answer are kept.
    '''
    data = []
    story = []
    for line in lines:
        line = line.decode('utf-8').strip()
        nid, line = line.split(' ', 1)
        nid = int(nid)
        if nid == 1:
            story = []
        if '\t' in line:
            q, a, supporting = line.split('\t')
            q = tokenize(q)
            if only_supporting:
                # Only select the related substory
                supporting = map(int, supporting.split())
                substory = [story[i - 1] for i in supporting]
            else:
                # Provide all the substories
                substory = [x for x in story if x]
            data.append((substory, q, a))
            story.append('')
        else:
            sent = tokenize(line)
            story.append(sent)
    return data


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<script language="JavaScript">
function ShowHide(divId) { 
    if(document.getElementById(divId).style.display == 'none') {
        document.getElementById(divId).style.display='block';
    } else { 
        document.getElementById(divId).style.display = 'none';
    } 
}
</script>
</body>
</html>

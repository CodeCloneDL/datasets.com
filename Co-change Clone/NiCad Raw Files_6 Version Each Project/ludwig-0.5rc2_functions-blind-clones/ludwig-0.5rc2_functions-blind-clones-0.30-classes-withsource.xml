<clones>
<systeminfo processor="nicad6" system="ludwig-0.5rc2" granularity="functions-blind" threshold="30%" minlines="10" maxlines="2500"/>
<cloneinfo npcs="1475" npairs="587"/>
<runinfo ncompares="27883" cputime="71797"/>
<classinfo nclasses="29"/>

<class classid="1" nclones="3" nlines="20" similarity="75">
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_kfold_cv.py" startline="136" endline="177" pcid="2">
def test_kfold_cv_api_from_file():
    # k-fold_cross_validate api with config file
    num_folds = 3

    # setup temporary directory to run test
    with tempfile.TemporaryDirectory() as tmpdir:

        # setup required data structures for test
        training_data_fp = os.path.join(tmpdir, "train.csv")
        config_fp = os.path.join(tmpdir, "config.yaml")

        # generate synthetic data for the test
        input_features = [number_feature(normalization="zscore"), number_feature(normalization="zscore")]

        output_features = [category_feature(vocab_size=3, reduce_input="sum")]

        generate_data(input_features, output_features, training_data_fp)

        # generate config file
        config = {
            "input_features": input_features,
            "output_features": output_features,
            "combiner": {"type": "concat", "output_size": 14},
            TRAINER: {"epochs": 2},
        }

        with open(config_fp, "w") as f:
            yaml.dump(config, f)

        # test kfold_cross_validate api with config file

        # execute k-fold cross validation run
        (kfold_cv_stats, kfold_split_indices) = kfold_cross_validate(3, config=config_fp, dataset=training_data_fp)

        # correct structure for results from kfold cv
        for key in ["fold_" + str(i + 1) for i in range(num_folds)] + ["overall"]:
            assert key in kfold_cv_stats

        for key in ["fold_" + str(i + 1) for i in range(num_folds)]:
            assert key in kfold_split_indices


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_kfold_cv.py" startline="236" endline="272" pcid="4">
def test_kfold_cv_dataset_formats(data_format):
    # k-fold_cross_validate api with in-memory config
    num_folds = 3

    # setup temporary directory to run test
    with tempfile.TemporaryDirectory() as tmpdir:

        # setup required data structures for test
        training_data_fp = os.path.join(tmpdir, "train.csv")

        # generate synthetic data for the test
        input_features = [number_feature(normalization="zscore"), number_feature(normalization="zscore")]

        output_features = [number_feature()]

        generate_data(input_features, output_features, training_data_fp)
        dataset_to_use = create_data_set_to_use(data_format, training_data_fp)

        # generate config file
        config = {
            "input_features": input_features,
            "output_features": output_features,
            "combiner": {"type": "concat", "output_size": 14},
            TRAINER: {"epochs": 2},
        }

        # test kfold_cross_validate api with config in-memory

        # execute k-fold cross validation run
        (kfold_cv_stats, kfold_split_indices) = kfold_cross_validate(3, config=config, dataset=dataset_to_use)

        # correct structure for results from kfold cv
        for key in ["fold_" + str(i + 1) for i in range(num_folds)] + ["overall"]:
            assert key in kfold_cv_stats

        for key in ["fold_" + str(i + 1) for i in range(num_folds)]:
            assert key in kfold_split_indices
</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_kfold_cv.py" startline="179" endline="216" pcid="3">
def test_kfold_cv_api_in_memory():
    # k-fold_cross_validate api with in-memory config
    num_folds = 3

    # setup temporary directory to run test
    with tempfile.TemporaryDirectory() as tmpdir:

        # setup required data structures for test
        training_data_fp = os.path.join(tmpdir, "train.csv")

        # generate synthetic data for the test
        input_features = [number_feature(normalization="zscore"), number_feature(normalization="zscore")]

        output_features = [number_feature()]

        generate_data(input_features, output_features, training_data_fp)

        # generate config file
        config = {
            "input_features": input_features,
            "output_features": output_features,
            "combiner": {"type": "concat", "output_size": 14},
            TRAINER: {"epochs": 2},
        }

        # test kfold_cross_validate api with config in-memory

        # execute k-fold cross validation run
        (kfold_cv_stats, kfold_split_indices) = kfold_cross_validate(3, config=config, dataset=training_data_fp)

        # correct structure for results from kfold cv
        for key in ["fold_" + str(i + 1) for i in range(num_folds)] + ["overall"]:
            assert key in kfold_cv_stats

        for key in ["fold_" + str(i + 1) for i in range(num_folds)]:
            assert key in kfold_split_indices


</source>
</class>

<class classid="2" nclones="25" nlines="40" similarity="73">
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization.py" startline="87" endline="129" pcid="7">
def test_visualization_learning_curves_output_saved(csv_filename):
    """Ensure pdf and png figures from the experiments can be saved.

    :param csv_filename: csv fixture from tests.conftest.csv_filename
    :return: None
    """
    input_features = [text_feature(encoder="parallel_cnn")]
    output_features = [category_feature()]

    # Generate test data
    rel_path = generate_data(input_features, output_features, csv_filename)
    input_features[0]["encoder"] = "parallel_cnn"
    exp_dir_name = run_experiment_with_visualization(input_features, output_features, dataset=rel_path)

    vis_output_pattern_pdf = os.path.join(exp_dir_name, "*.pdf")
    vis_output_pattern_png = os.path.join(exp_dir_name, "*.png")
    train_stats = os.path.join(exp_dir_name, "training_statistics.json")
    test_cmd_pdf = [
        "python",
        "-m",
        "ludwig.visualize",
        "--visualization",
        "learning_curves",
        "--training_statistics",
        train_stats,
        "-od",
        exp_dir_name,
    ]
    test_cmd_png = test_cmd_pdf.copy() + ["-ff", "png"]

    commands = [test_cmd_pdf, test_cmd_png]
    vis_patterns = [vis_output_pattern_pdf, vis_output_pattern_png]

    for command, viz_pattern in zip(commands, vis_patterns):
        result = subprocess.run(
            command,
        )
        figure_cnt = glob.glob(viz_pattern)

        assert 0 == result.returncode
        assert 4 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization.py" startline="173" endline="220" pcid="9">
def test_visualization_compare_performance_output_saved(csv_filename):
    """Ensure pdf and png figures from the experiments can be saved.

    Compare performance between two models. To reduce test complexity
    one model is compared to it self.

    :param csv_filename: csv fixture from tests.conftest.csv_filename
    :return: None
    """
    input_features = [text_feature(encoder="parallel_cnn")]
    output_features = [category_feature()]

    # Generate test data
    rel_path = generate_data(input_features, output_features, csv_filename)
    input_features[0]["encoder"] = "parallel_cnn"
    exp_dir_name = run_experiment_with_visualization(input_features, output_features, dataset=rel_path)
    vis_output_pattern_pdf = os.path.join(exp_dir_name, "*.pdf")
    vis_output_pattern_png = os.path.join(exp_dir_name, "*.png")
    test_stats = os.path.join(exp_dir_name, TEST_STATISTICS_FILE_NAME)

    test_cmd_pdf = [
        "python",
        "-m",
        "ludwig.visualize",
        "--visualization",
        "compare_performance",
        "--test_statistics",
        test_stats,
        test_stats,
        "-m",
        "Model1",
        "Model2",
        "-od",
        exp_dir_name,
    ]
    test_cmd_png = test_cmd_pdf.copy() + ["-ff", "png"]

    commands = [test_cmd_pdf, test_cmd_png]
    vis_patterns = [vis_output_pattern_pdf, vis_output_pattern_png]

    for command, viz_pattern in zip(commands, vis_patterns):
        result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        figure_cnt = glob.glob(viz_pattern)

        assert 0 == result.returncode
        assert 1 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization.py" startline="1254" endline="1297" pcid="28">
def test_visualization_roc_curves_from_test_statistics_output_saved(csv_filename):
    """Ensure pdf and png figures from the experiments can be saved.

    :param csv_filename: csv fixture from tests.conftest.csv_filename
    :return: None
    """
    input_features = [binary_feature(), bag_feature()]
    output_features = [binary_feature()]
    # Generate test data
    rel_path = generate_data(input_features, output_features, csv_filename)

    exp_dir_name = run_experiment_with_visualization(input_features, output_features, dataset=rel_path)
    vis_output_pattern_pdf = os.path.join(exp_dir_name, "*.pdf")
    vis_output_pattern_png = os.path.join(exp_dir_name, "*.png")
    output_feature_name = get_output_feature_name(exp_dir_name)
    test_stats = os.path.join(exp_dir_name, TEST_STATISTICS_FILE_NAME)
    test_cmd_pdf = [
        "python",
        "-m",
        "ludwig.visualize",
        "--visualization",
        "roc_curves_from_test_statistics",
        "--output_feature_name",
        output_feature_name,
        "--test_statistics",
        test_stats,
        "--model_names",
        "Model1",
        "-od",
        exp_dir_name,
    ]
    test_cmd_png = test_cmd_pdf.copy() + ["-ff", "png"]

    commands = [test_cmd_pdf, test_cmd_png]
    vis_patterns = [vis_output_pattern_pdf, vis_output_pattern_png]

    for command, viz_pattern in zip(commands, vis_patterns):
        result = subprocess.run(command)
        figure_cnt = glob.glob(viz_pattern)

        assert 0 == result.returncode
        assert 1 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization.py" startline="130" endline="172" pcid="8">
def test_visualization_confusion_matrix_output_saved(csv_filename):
    """Ensure pdf and png figures from the experiments can be saved.

    :param csv_filename: csv fixture from tests.conftest.csv_filename
    :return: None
    """
    input_features = [text_feature(encoder="parallel_cnn")]
    output_features = [category_feature()]

    # Generate test data
    rel_path = generate_data(input_features, output_features, csv_filename)
    input_features[0]["encoder"] = "parallel_cnn"
    exp_dir_name = run_experiment_with_visualization(input_features, output_features, dataset=rel_path)
    vis_output_pattern_pdf = os.path.join(exp_dir_name, "*.pdf")
    vis_output_pattern_png = os.path.join(exp_dir_name, "*.png")
    experiment_source_data_name = csv_filename.split(".")[0]
    ground_truth_metadata = experiment_source_data_name + ".meta.json"
    test_stats = os.path.join(exp_dir_name, TEST_STATISTICS_FILE_NAME)
    test_cmd_pdf = [
        "python",
        "-m",
        "ludwig.visualize",
        "--visualization",
        "confusion_matrix",
        "--test_statistics",
        test_stats,
        "--ground_truth_metadata",
        ground_truth_metadata,
        "-od",
        exp_dir_name,
    ]
    test_cmd_png = test_cmd_pdf.copy() + ["-ff", "png"]
    commands = [test_cmd_pdf, test_cmd_png]
    vis_patterns = [vis_output_pattern_pdf, vis_output_pattern_png]

    for command, viz_pattern in zip(commands, vis_patterns):
        result = subprocess.run(command)
        figure_cnt = glob.glob(viz_pattern)

        assert 0 == result.returncode
        assert 2 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization.py" startline="1412" endline="1461" pcid="31">
def test_visualization_frequency_vs_f1_output_saved(csv_filename):
    """Ensure pdf and png figures from the experiments can be saved.

    :param csv_filename: csv fixture from tests.conftest.csv_filename
    :return: None
    """
    input_features = [category_feature(vocab_size=10)]
    output_features = [category_feature(vocab_size=2, reduce_input="sum")]

    # Generate test data
    rel_path = generate_data(input_features, output_features, csv_filename)
    exp_dir_name = run_experiment_with_visualization(input_features, output_features, dataset=rel_path)
    vis_output_pattern_pdf = os.path.join(exp_dir_name, "*.pdf")
    vis_output_pattern_png = os.path.join(exp_dir_name, "*.png")
    output_feature_name = get_output_feature_name(exp_dir_name)
    test_stats = os.path.join(exp_dir_name, TEST_STATISTICS_FILE_NAME)
    experiment_source_data_name = csv_filename.split(".")[0]
    ground_truth_metadata = experiment_source_data_name + ".meta.json"
    test_cmd_pdf = [
        "python",
        "-m",
        "ludwig.visualize",
        "--visualization",
        "frequency_vs_f1",
        "--ground_truth_metadata",
        ground_truth_metadata,
        "--output_feature_name",
        output_feature_name,
        "--test_statistics",
        test_stats,
        test_stats,
        "--model_names",
        "Model1",
        "Model2",
        "-od",
        exp_dir_name,
    ]
    test_cmd_png = test_cmd_pdf.copy() + ["-ff", "png"]

    commands = [test_cmd_pdf, test_cmd_png]
    vis_patterns = [vis_output_pattern_pdf, vis_output_pattern_png]

    for command, viz_pattern in zip(commands, vis_patterns):
        result = subprocess.run(command)
        figure_cnt = glob.glob(viz_pattern)

        assert 0 == result.returncode
        assert 2 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization.py" startline="559" endline="605" pcid="16">
def test_visualization_compare_classifiers_multiclass_multimetric_output_saved(csv_filename):
    """Ensure pdf and png figures from the experiments can be saved.

    :param csv_filename: csv fixture from tests.conftest.csv_filename
    :return: None
    """
    input_features = [category_feature(vocab_size=10)]
    output_features = [category_feature(vocab_size=2, reduce_input="sum")]

    # Generate test data
    rel_path = generate_data(input_features, output_features, csv_filename)
    exp_dir_name = run_experiment_with_visualization(input_features, output_features, dataset=rel_path)
    vis_output_pattern_pdf = os.path.join(exp_dir_name, "*.pdf")
    vis_output_pattern_png = os.path.join(exp_dir_name, "*.png")
    output_feature_name = get_output_feature_name(exp_dir_name)
    test_stats = os.path.join(exp_dir_name, TEST_STATISTICS_FILE_NAME)
    experiment_source_data_name = csv_filename.split(".")[0]
    ground_truth_metadata = experiment_source_data_name + ".meta.json"
    test_cmd_pdf = [
        "python",
        "-m",
        "ludwig.visualize",
        "--visualization",
        "compare_classifiers_multiclass_multimetric",
        "--output_feature_name",
        output_feature_name,
        "--test_statistics",
        test_stats,
        test_stats,
        "--ground_truth_metadata",
        ground_truth_metadata,
        "-od",
        exp_dir_name,
    ]
    test_cmd_png = test_cmd_pdf.copy() + ["-ff", "png"]

    commands = [test_cmd_pdf, test_cmd_png]
    vis_patterns = [vis_output_pattern_pdf, vis_output_pattern_png]

    for command, viz_pattern in zip(commands, vis_patterns):
        result = subprocess.run(command)
        figure_cnt = glob.glob(viz_pattern)

        assert 0 == result.returncode
        assert 4 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization.py" startline="506" endline="558" pcid="15">
def test_visualization_compare_classifiers_changing_k_output_pdf(csv_filename):
    """It should be possible to save figures as pdf in the specified directory."""
    input_features = [category_feature(vocab_size=10)]
    output_features = [category_feature(vocab_size=2, reduce_input="sum")]

    # Generate test data
    rel_path = generate_data(input_features, output_features, csv_filename)
    exp_dir_name = run_experiment_with_visualization(input_features, output_features, dataset=rel_path)
    vis_output_pattern_pdf = os.path.join(exp_dir_name, "*.pdf")
    vis_output_pattern_png = os.path.join(exp_dir_name, "*.png")
    output_feature_name = get_output_feature_name(exp_dir_name)
    probability = os.path.join(exp_dir_name, PREDICTIONS_PARQUET_FILE_NAME)
    experiment_source_data_name = csv_filename.split(".")[0]
    ground_truth = experiment_source_data_name + ".csv"
    split_file = experiment_source_data_name + ".split.csv"
    ground_truth_metadata = exp_dir_name + "/model/training_set_metadata.json"
    test_cmd_pdf = [
        "python",
        "-m",
        "ludwig.visualize",
        "--visualization",
        "compare_classifiers_performance_changing_k",
        "--output_feature_name",
        output_feature_name,
        "--split_file",
        split_file,
        "--ground_truth_metadata",
        ground_truth_metadata,
        "--probabilities",
        probability,
        probability,
        "--model_names",
        "Model1",
        "Model2",
        "--ground_truth",
        ground_truth,
        "--top_n_classes",
        "6",
        "-od",
        exp_dir_name,
    ]
    test_cmd_png = test_cmd_pdf.copy() + ["-ff", "png"]

    commands = [test_cmd_pdf, test_cmd_png]
    vis_patterns = [vis_output_pattern_pdf, vis_output_pattern_png]
    for command, viz_pattern in zip(commands, vis_patterns):
        result = subprocess.run(command)
        figure_cnt = glob.glob(viz_pattern)

        assert 0 == result.returncode
        assert 1 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization.py" startline="449" endline="505" pcid="14">
def test_visualization_compare_classifiers_subset_output_saved(csv_filename):
    """Ensure pdf and png figures from the experiments can be saved.

    :param csv_filename: csv fixture from tests.conftest.csv_filename
    :return: None
    """
    input_features = [category_feature(vocab_size=10)]
    output_features = [category_feature(vocab_size=2, reduce_input="sum")]

    # Generate test data
    rel_path = generate_data(input_features, output_features, csv_filename)
    exp_dir_name = run_experiment_with_visualization(input_features, output_features, dataset=rel_path)
    vis_output_pattern_pdf = os.path.join(exp_dir_name, "*.pdf")
    vis_output_pattern_png = os.path.join(exp_dir_name, "*.png")
    output_feature_name = get_output_feature_name(exp_dir_name)
    probability = os.path.join(exp_dir_name, PREDICTIONS_PARQUET_FILE_NAME)
    experiment_source_data_name = csv_filename.split(".")[0]
    ground_truth = experiment_source_data_name + ".csv"
    split_file = experiment_source_data_name + ".split.csv"
    test_cmd_pdf = [
        "python",
        "-m",
        "ludwig.visualize",
        "--visualization",
        "compare_classifiers_performance_subset",
        "--output_feature_name",
        output_feature_name,
        "--split_file",
        split_file,
        "--ground_truth_metadata",
        exp_dir_name + "/model/training_set_metadata.json",
        "--probabilities",
        probability,
        probability,
        "--model_names",
        "Model1",
        "Model2",
        "--ground_truth",
        ground_truth,
        "--top_n_classes",
        "6",
        "-od",
        exp_dir_name,
    ]
    test_cmd_png = test_cmd_pdf.copy() + ["-ff", "png"]

    commands = [test_cmd_pdf, test_cmd_png]
    vis_patterns = [vis_output_pattern_pdf, vis_output_pattern_png]

    for command, viz_pattern in zip(commands, vis_patterns):
        result = subprocess.run(command)
        figure_cnt = glob.glob(viz_pattern)

        assert 0 == result.returncode
        assert 1 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization.py" startline="940" endline="998" pcid="23">
def test_vis_confidence_thresholding_data_vs_acc_subset_per_class_output_saved(csv_filename):
    """Ensure pdf and png figures from the experiments can be saved.

    :param csv_filename: csv fixture from tests.conftest.csv_filename
    :return: None
    """
    input_features = [category_feature(vocab_size=10)]
    output_features = [category_feature(vocab_size=5, reduce_input="sum")]

    # Generate test data
    rel_path = generate_data(input_features, output_features, csv_filename)
    exp_dir_name = run_experiment_with_visualization(input_features, output_features, dataset=rel_path)
    vis_output_pattern_pdf = os.path.join(exp_dir_name, "*.pdf")
    vis_output_pattern_png = os.path.join(exp_dir_name, "*.png")
    output_feature_name = get_output_feature_name(exp_dir_name)
    probability = os.path.join(exp_dir_name, PREDICTIONS_PARQUET_FILE_NAME)
    experiment_source_data_name = csv_filename.split(".")[0]
    ground_truth = experiment_source_data_name + ".csv"
    split_file = experiment_source_data_name + ".split.csv"
    test_cmd_pdf = [
        "python",
        "-m",
        "ludwig.visualize",
        "--visualization",
        "confidence_thresholding_data_vs_acc_subset_per_class",
        "--ground_truth",
        ground_truth,
        "--output_feature_name",
        output_feature_name,
        "--split_file",
        split_file,
        "--ground_truth_metadata",
        exp_dir_name + "/model/training_set_metadata.json",
        "--probabilities",
        probability,
        probability,
        "--model_names",
        "Model1",
        "Model2",
        "--top_n_classes",
        "3",
        "-od",
        exp_dir_name,
    ]
    test_cmd_png = test_cmd_pdf.copy() + ["-ff", "png"]

    commands = [test_cmd_pdf, test_cmd_png]
    vis_patterns = [vis_output_pattern_pdf, vis_output_pattern_png]

    for command, viz_pattern in zip(commands, vis_patterns):
        result = subprocess.run(command)
        figure_cnt = glob.glob(viz_pattern)

        assert 0 == result.returncode
        # 3 figures should be saved because experiment setting top_n_classes = 3
        # hence one figure per class
        assert 3 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization.py" startline="392" endline="448" pcid="13">
def test_visualization_compare_classifiers_from_pred_csv_output_saved(csv_filename):
    """Ensure pdf and png figures from the experiments can be saved.

    Predictions are loaded from csv file.
    :param csv_filename: csv fixture from tests.fixtures.filenames.csv_filename
    :return: None
    """
    input_features = [category_feature(vocab_size=10)]
    output_features = [category_feature(vocab_size=2, reduce_input="sum")]

    # Generate test data
    rel_path = generate_data(input_features, output_features, csv_filename)
    exp_dir_name = run_experiment_with_visualization(input_features, output_features, dataset=rel_path)
    vis_output_pattern_pdf = os.path.join(exp_dir_name, "*.pdf")
    vis_output_pattern_png = os.path.join(exp_dir_name, "*.png")
    output_feature_name = get_output_feature_name(exp_dir_name)
    prediction = os.path.join(exp_dir_name, PREDICTIONS_PARQUET_FILE_NAME)
    experiment_source_data_name = csv_filename.split(".")[0]
    ground_truth = experiment_source_data_name + ".csv"
    split_file = experiment_source_data_name + ".split.csv"
    ground_truth_metadata = experiment_source_data_name + ".meta.json"
    test_cmd_pdf = [
        "python",
        "-m",
        "ludwig.visualize",
        "--visualization",
        "compare_classifiers_performance_from_pred",
        "--ground_truth_metadata",
        ground_truth_metadata,
        "--ground_truth",
        ground_truth,
        "--output_feature_name",
        output_feature_name,
        "--split_file",
        split_file,
        "--predictions",
        prediction,
        prediction,
        "--model_names",
        "Model1",
        "Model2",
        "-od",
        exp_dir_name,
    ]
    test_cmd_png = test_cmd_pdf.copy() + ["-ff", "png"]

    commands = [test_cmd_pdf, test_cmd_png]
    vis_patterns = [vis_output_pattern_pdf, vis_output_pattern_png]

    for command, viz_pattern in zip(commands, vis_patterns):
        result = subprocess.run(command)
        figure_cnt = glob.glob(viz_pattern)

        assert 0 == result.returncode
        assert 1 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization.py" startline="883" endline="939" pcid="22">
def test_visualization_confidence_thresholding_data_vs_acc_subset_output_saved(csv_filename):
    """Ensure pdf and png figures from the experiments can be saved.

    :param csv_filename: csv fixture from tests.conftest.csv_filename
    :return: None
    """
    input_features = [category_feature(vocab_size=10)]
    output_features = [category_feature(vocab_size=2, reduce_input="sum")]

    # Generate test data
    rel_path = generate_data(input_features, output_features, csv_filename)
    exp_dir_name = run_experiment_with_visualization(input_features, output_features, dataset=rel_path)
    vis_output_pattern_pdf = os.path.join(exp_dir_name, "*.pdf")
    vis_output_pattern_png = os.path.join(exp_dir_name, "*.png")
    output_feature_name = get_output_feature_name(exp_dir_name)
    probability = os.path.join(exp_dir_name, PREDICTIONS_PARQUET_FILE_NAME)
    experiment_source_data_name = csv_filename.split(".")[0]
    ground_truth = experiment_source_data_name + ".csv"
    split_file = experiment_source_data_name + ".split.csv"
    test_cmd_pdf = [
        "python",
        "-m",
        "ludwig.visualize",
        "--visualization",
        "confidence_thresholding_data_vs_acc_subset",
        "--ground_truth",
        ground_truth,
        "--output_feature_name",
        output_feature_name,
        "--split_file",
        split_file,
        "--ground_truth_metadata",
        exp_dir_name + "/model/training_set_metadata.json",
        "--probabilities",
        probability,
        probability,
        "--model_names",
        "Model1",
        "Model2",
        "--top_n_classes",
        "3",
        "-od",
        exp_dir_name,
    ]
    test_cmd_png = test_cmd_pdf.copy() + ["-ff", "png"]

    commands = [test_cmd_pdf, test_cmd_png]
    vis_patterns = [vis_output_pattern_pdf, vis_output_pattern_png]

    for command, viz_pattern in zip(commands, vis_patterns):
        result = subprocess.run(command)
        figure_cnt = glob.glob(viz_pattern)

        assert 0 == result.returncode
        assert 1 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization.py" startline="278" endline="334" pcid="11">
def test_visualization_compare_classifiers_from_prob_npy_output_saved(csv_filename):
    """Ensure pdf and png figures from the experiments can be saved.

    Probabilities are loaded from npy file.
    :param csv_filename: csv fixture from tests.fixtures.filenames.csv_filename
    :return: None
    """
    input_features = [category_feature(vocab_size=10)]
    output_features = [category_feature(vocab_size=2, reduce_input="sum")]

    # Generate test data
    rel_path = generate_data(input_features, output_features, csv_filename)
    exp_dir_name = run_experiment_with_visualization(input_features, output_features, dataset=rel_path)

    vis_output_pattern_pdf = os.path.join(exp_dir_name, "*.pdf")
    vis_output_pattern_png = os.path.join(exp_dir_name, "*.png")
    output_feature_name = get_output_feature_name(exp_dir_name)
    probability = os.path.join(exp_dir_name, PREDICTIONS_PARQUET_FILE_NAME)
    experiment_source_data_name = csv_filename.split(".")[0]
    ground_truth = experiment_source_data_name + ".csv"
    split_file = experiment_source_data_name + ".split.csv"
    test_cmd_pdf = [
        "python",
        "-m",
        "ludwig.visualize",
        "--visualization",
        "compare_classifiers_performance_from_prob",
        "--ground_truth",
        ground_truth,
        "--output_feature_name",
        output_feature_name,
        "--split_file",
        split_file,
        "--ground_truth_metadata",
        exp_dir_name + "/model/training_set_metadata.json",
        "--probabilities",
        probability,
        probability,
        "--model_names",
        "Model1",
        "Model2",
        "-od",
        exp_dir_name,
    ]
    test_cmd_png = test_cmd_pdf.copy() + ["-ff", "png"]

    commands = [test_cmd_pdf, test_cmd_png]
    vis_patterns = [vis_output_pattern_pdf, vis_output_pattern_png]

    for command, viz_pattern in zip(commands, vis_patterns):
        result = subprocess.run(command)
        figure_cnt = glob.glob(viz_pattern)

        assert 0 == result.returncode
        assert 1 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization.py" startline="1357" endline="1411" pcid="30">
def test_visualization_calibration_multiclass_output_saved(csv_filename):
    """Ensure pdf and png figures from the experiments can be saved.

    :param csv_filename: csv fixture from tests.conftest.csv_filename
    :return: None
    """
    input_features = [category_feature(vocab_size=10)]
    output_features = [category_feature(vocab_size=2, reduce_input="sum")]

    # Generate test data
    rel_path = generate_data(input_features, output_features, csv_filename)
    exp_dir_name = run_experiment_with_visualization(input_features, output_features, dataset=rel_path)
    vis_output_pattern_pdf = os.path.join(exp_dir_name, "*.pdf")
    vis_output_pattern_png = os.path.join(exp_dir_name, "*.png")
    output_feature_name = get_output_feature_name(exp_dir_name)
    probability = os.path.join(exp_dir_name, PREDICTIONS_PARQUET_FILE_NAME)
    experiment_source_data_name = csv_filename.split(".")[0]
    ground_truth = experiment_source_data_name + ".csv"
    split_file = experiment_source_data_name + ".split.csv"
    test_cmd_pdf = [
        "python",
        "-m",
        "ludwig.visualize",
        "--visualization",
        "calibration_multiclass",
        "--ground_truth",
        ground_truth,
        "--output_feature_name",
        output_feature_name,
        "--split_file",
        split_file,
        "--ground_truth_metadata",
        exp_dir_name + "/model/training_set_metadata.json",
        "--probabilities",
        probability,
        probability,
        "--model_names",
        "Model1",
        "Model2",
        "-od",
        exp_dir_name,
    ]
    test_cmd_png = test_cmd_pdf.copy() + ["-ff", "png"]

    commands = [test_cmd_pdf, test_cmd_png]
    vis_patterns = [vis_output_pattern_pdf, vis_output_pattern_png]

    for command, viz_pattern in zip(commands, vis_patterns):
        result = subprocess.run(command)
        figure_cnt = glob.glob(viz_pattern)

        assert 0 == result.returncode
        assert 2 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization.py" startline="335" endline="391" pcid="12">
def test_visualization_compare_classifiers_from_pred_npy_output_saved(csv_filename):
    """Ensure pdf and png figures from the experiments can be saved.

    Predictions are loaded from npy file.
    :param csv_filename: csv fixture from tests.fixtures.filenames.csv_filename
    :return: None
    """
    input_features = [category_feature(vocab_size=10)]
    output_features = [category_feature(vocab_size=2, reduce_input="sum")]

    # Generate test data
    rel_path = generate_data(input_features, output_features, csv_filename)
    exp_dir_name = run_experiment_with_visualization(input_features, output_features, dataset=rel_path)
    vis_output_pattern_pdf = os.path.join(exp_dir_name, "*.pdf")
    vis_output_pattern_png = os.path.join(exp_dir_name, "*.png")
    output_feature_name = get_output_feature_name(exp_dir_name)
    prediction = os.path.join(exp_dir_name, PREDICTIONS_PARQUET_FILE_NAME)
    experiment_source_data_name = csv_filename.split(".")[0]
    ground_truth = experiment_source_data_name + ".csv"
    split_file = experiment_source_data_name + ".split.csv"
    ground_truth_metadata = experiment_source_data_name + ".meta.json"
    test_cmd_pdf = [
        "python",
        "-m",
        "ludwig.visualize",
        "--visualization",
        "compare_classifiers_performance_from_pred",
        "--ground_truth_metadata",
        ground_truth_metadata,
        "--ground_truth",
        ground_truth,
        "--output_feature_name",
        output_feature_name,
        "--split_file",
        split_file,
        "--predictions",
        prediction,
        prediction,
        "--model_names",
        "Model1",
        "Model2",
        "-od",
        exp_dir_name,
    ]
    test_cmd_png = test_cmd_pdf.copy() + ["-ff", "png"]

    commands = [test_cmd_pdf, test_cmd_png]
    vis_patterns = [vis_output_pattern_pdf, vis_output_pattern_png]

    for command, viz_pattern in zip(commands, vis_patterns):
        result = subprocess.run(command)
        figure_cnt = glob.glob(viz_pattern)

        assert 0 == result.returncode
        assert 1 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization.py" startline="718" endline="772" pcid="19">
def test_visualization_cmp_classifiers_predictions_distribution_output_saved(csv_filename):
    """Ensure pdf and png figures from the experiments can be saved.

    :param csv_filename: csv fixture from tests.conftest.csv_filename
    :return: None
    """
    input_features = [category_feature(vocab_size=10)]
    output_features = [category_feature(vocab_size=2, reduce_input="sum")]

    # Generate test data
    rel_path = generate_data(input_features, output_features, csv_filename)
    exp_dir_name = run_experiment_with_visualization(input_features, output_features, dataset=rel_path)
    vis_output_pattern_pdf = os.path.join(exp_dir_name, "*.pdf")
    vis_output_pattern_png = os.path.join(exp_dir_name, "*.png")
    output_feature_name = get_output_feature_name(exp_dir_name)
    prediction = os.path.join(exp_dir_name, PREDICTIONS_PARQUET_FILE_NAME)
    experiment_source_data_name = csv_filename.split(".")[0]
    ground_truth = experiment_source_data_name + ".csv"
    split_file = experiment_source_data_name + ".split.csv"
    test_cmd_pdf = [
        "python",
        "-m",
        "ludwig.visualize",
        "--visualization",
        "compare_classifiers_predictions_distribution",
        "--ground_truth",
        ground_truth,
        "--output_feature_name",
        output_feature_name,
        "--split_file",
        split_file,
        "--ground_truth_metadata",
        exp_dir_name + "/model/training_set_metadata.json",
        "--predictions",
        prediction,
        prediction,
        "--model_names",
        "Model1",
        "Model2",
        "-od",
        exp_dir_name,
    ]
    test_cmd_png = test_cmd_pdf.copy() + ["-ff", "png"]

    commands = [test_cmd_pdf, test_cmd_png]
    vis_patterns = [vis_output_pattern_pdf, vis_output_pattern_png]

    for command, viz_pattern in zip(commands, vis_patterns):
        result = subprocess.run(command)
        figure_cnt = glob.glob(viz_pattern)

        assert 0 == result.returncode
        assert 1 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization.py" startline="773" endline="827" pcid="20">
def test_visualization_cconfidence_thresholding_output_saved(csv_filename):
    """Ensure pdf and png figures from the experiments can be saved.

    :param csv_filename: csv fixture from tests.conftest.csv_filename
    :return: None
    """
    input_features = [category_feature(vocab_size=10)]
    output_features = [category_feature(vocab_size=2, reduce_input="sum")]

    # Generate test data
    rel_path = generate_data(input_features, output_features, csv_filename)
    exp_dir_name = run_experiment_with_visualization(input_features, output_features, dataset=rel_path)
    vis_output_pattern_pdf = os.path.join(exp_dir_name, "*.pdf")
    vis_output_pattern_png = os.path.join(exp_dir_name, "*.png")
    output_feature_name = get_output_feature_name(exp_dir_name)
    probability = os.path.join(exp_dir_name, PREDICTIONS_PARQUET_FILE_NAME)
    experiment_source_data_name = csv_filename.split(".")[0]
    ground_truth = experiment_source_data_name + ".csv"
    split_file = experiment_source_data_name + ".split.csv"
    test_cmd_pdf = [
        "python",
        "-m",
        "ludwig.visualize",
        "--visualization",
        "confidence_thresholding",
        "--ground_truth",
        ground_truth,
        "--output_feature_name",
        output_feature_name,
        "--split_file",
        split_file,
        "--ground_truth_metadata",
        exp_dir_name + "/model/training_set_metadata.json",
        "--probabilities",
        probability,
        probability,
        "--model_names",
        "Model1",
        "Model2",
        "-od",
        exp_dir_name,
    ]
    test_cmd_png = test_cmd_pdf.copy() + ["-ff", "png"]

    commands = [test_cmd_pdf, test_cmd_png]
    vis_patterns = [vis_output_pattern_pdf, vis_output_pattern_png]

    for command, viz_pattern in zip(commands, vis_patterns):
        result = subprocess.run(command)
        figure_cnt = glob.glob(viz_pattern)

        assert 0 == result.returncode
        assert 1 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization.py" startline="606" endline="661" pcid="17">
def test_visualization_compare_classifiers_predictions_npy_output_saved(csv_filename):
    """Ensure pdf and png figures from the experiments can be saved.

    Predictions are loaded form npy file.
    :param csv_filename: csv fixture from tests.fixtures.filenames.csv_filename
    :return: None
    """
    input_features = [category_feature(vocab_size=10)]
    output_features = [category_feature(vocab_size=2, reduce_input="sum")]

    # Generate test data
    rel_path = generate_data(input_features, output_features, csv_filename)
    exp_dir_name = run_experiment_with_visualization(input_features, output_features, dataset=rel_path)
    vis_output_pattern_pdf = os.path.join(exp_dir_name, "*.pdf")
    vis_output_pattern_png = os.path.join(exp_dir_name, "*.png")
    output_feature_name = get_output_feature_name(exp_dir_name)
    prediction = os.path.join(exp_dir_name, PREDICTIONS_PARQUET_FILE_NAME)
    experiment_source_data_name = csv_filename.split(".")[0]
    ground_truth = experiment_source_data_name + ".csv"
    split_file = experiment_source_data_name + ".split.csv"
    test_cmd_pdf = [
        "python",
        "-m",
        "ludwig.visualize",
        "--visualization",
        "compare_classifiers_predictions",
        "--ground_truth",
        ground_truth,
        "--output_feature_name",
        output_feature_name,
        "--split_file",
        split_file,
        "--ground_truth_metadata",
        exp_dir_name + "/model/training_set_metadata.json",
        "--predictions",
        prediction,
        prediction,
        "--model_names",
        "Model1",
        "Model2",
        "-od",
        exp_dir_name,
    ]
    test_cmd_png = test_cmd_pdf.copy() + ["-ff", "png"]

    commands = [test_cmd_pdf, test_cmd_png]
    vis_patterns = [vis_output_pattern_pdf, vis_output_pattern_png]

    for command, viz_pattern in zip(commands, vis_patterns):
        result = subprocess.run(command)
        figure_cnt = glob.glob(viz_pattern)

        assert 0 == result.returncode
        assert 1 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization.py" startline="1298" endline="1356" pcid="29">
def test_visualization_calibration_1_vs_all_output_saved(csv_filename):
    """Ensure pdf and png figures from the experiments can be saved.

    :param csv_filename: csv fixture from tests.conftest.csv_filename
    :return: None
    """
    input_features = [category_feature(vocab_size=10)]
    output_features = [category_feature(vocab_size=2, reduce_input="sum")]

    # Generate test data
    rel_path = generate_data(input_features, output_features, csv_filename)
    exp_dir_name = run_experiment_with_visualization(input_features, output_features, dataset=rel_path)
    vis_output_pattern_pdf = os.path.join(exp_dir_name, "*.pdf")
    vis_output_pattern_png = os.path.join(exp_dir_name, "*.png")
    output_feature_name = get_output_feature_name(exp_dir_name)
    probability = os.path.join(exp_dir_name, PREDICTIONS_PARQUET_FILE_NAME)
    experiment_source_data_name = csv_filename.split(".")[0]
    ground_truth = experiment_source_data_name + ".csv"
    split_file = experiment_source_data_name + ".split.csv"
    test_cmd_pdf = [
        "python",
        "-m",
        "ludwig.visualize",
        "--visualization",
        "calibration_1_vs_all",
        "--metrics",
        "accuracy",
        "--ground_truth",
        ground_truth,
        "--output_feature_name",
        output_feature_name,
        "--split_file",
        split_file,
        "--ground_truth_metadata",
        exp_dir_name + "/model/training_set_metadata.json",
        "--probabilities",
        probability,
        probability,
        "--model_names",
        "Model1",
        "Model2",
        "--top_k",
        "6",
        "-od",
        exp_dir_name,
    ]
    test_cmd_png = test_cmd_pdf.copy() + ["-ff", "png"]

    commands = [test_cmd_pdf, test_cmd_png]
    vis_patterns = [vis_output_pattern_pdf, vis_output_pattern_png]

    for command, viz_pattern in zip(commands, vis_patterns):
        result = subprocess.run(command)
        figure_cnt = glob.glob(viz_pattern)

        assert 0 == result.returncode
        assert 5 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization.py" startline="221" endline="277" pcid="10">
def test_visualization_compare_classifiers_from_prob_csv_output_saved(csv_filename):
    """Ensure pdf and png figures from the experiments can be saved.

    Probabilities are loaded from csv file.
    :param csv_filename: csv fixture from tests.fixtures.filenames.csv_filename
    :return: None
    """
    input_features = [category_feature(vocab_size=10)]
    output_features = [category_feature(vocab_size=2, reduce_input="sum")]

    # Generate test data
    rel_path = generate_data(input_features, output_features, csv_filename)
    exp_dir_name = run_experiment_with_visualization(input_features, output_features, dataset=rel_path)

    vis_output_pattern_pdf = os.path.join(exp_dir_name, "*.pdf")
    vis_output_pattern_png = os.path.join(exp_dir_name, "*.png")
    output_feature_name = get_output_feature_name(exp_dir_name)
    probability = os.path.join(exp_dir_name, PREDICTIONS_PARQUET_FILE_NAME)
    experiment_source_data_name = csv_filename.split(".")[0]
    ground_truth = experiment_source_data_name + ".csv"
    split_file = get_split_path(csv_filename)
    test_cmd_pdf = [
        "python",
        "-m",
        "ludwig.visualize",
        "--visualization",
        "compare_classifiers_performance_from_prob",
        "--ground_truth",
        ground_truth,
        "--output_feature_name",
        output_feature_name,
        "--split_file",
        split_file,
        "--ground_truth_metadata",
        exp_dir_name + "/model/training_set_metadata.json",
        "--probabilities",
        probability,
        probability,
        "--model_names",
        "Model1",
        "Model2",
        "-od",
        exp_dir_name,
    ]
    test_cmd_png = test_cmd_pdf.copy() + ["-ff", "png"]

    commands = [test_cmd_pdf, test_cmd_png]
    vis_patterns = [vis_output_pattern_pdf, vis_output_pattern_png]

    for command, viz_pattern in zip(commands, vis_patterns):
        result = subprocess.run(command)
        figure_cnt = glob.glob(viz_pattern)

        assert 0 == result.returncode
        assert 1 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization.py" startline="1195" endline="1253" pcid="27">
def test_visualization_roc_curves_output_saved(csv_filename):
    """Ensure pdf and png figures from the experiments can be saved.

    :param csv_filename: csv fixture from tests.conftest.csv_filename
    :return: None
    """
    input_features = [category_feature(vocab_size=10)]
    output_features = [category_feature(vocab_size=2, reduce_input="sum")]

    # Generate test data
    rel_path = generate_data(input_features, output_features, csv_filename)
    exp_dir_name = run_experiment_with_visualization(input_features, output_features, dataset=rel_path)
    vis_output_pattern_pdf = os.path.join(exp_dir_name, "*.pdf")
    vis_output_pattern_png = os.path.join(exp_dir_name, "*.png")
    output_feature_name = get_output_feature_name(exp_dir_name)
    probability = os.path.join(exp_dir_name, PREDICTIONS_PARQUET_FILE_NAME)
    experiment_source_data_name = csv_filename.split(".")[0]
    ground_truth = experiment_source_data_name + ".csv"
    split_file = experiment_source_data_name + ".split.csv"
    test_cmd_pdf = [
        "python",
        "-m",
        "ludwig.visualize",
        "--visualization",
        "roc_curves",
        "--positive_label",
        "2",
        "--metrics",
        "accuracy",
        "--ground_truth",
        ground_truth,
        "--output_feature_name",
        output_feature_name,
        "--split_file",
        split_file,
        "--ground_truth_metadata",
        exp_dir_name + "/model/training_set_metadata.json",
        "--probabilities",
        probability,
        probability,
        "--model_names",
        "Model1",
        "Model2",
        "-od",
        exp_dir_name,
    ]
    test_cmd_png = test_cmd_pdf.copy() + ["-ff", "png"]

    commands = [test_cmd_pdf, test_cmd_png]
    vis_patterns = [vis_output_pattern_pdf, vis_output_pattern_png]

    for command, viz_pattern in zip(commands, vis_patterns):
        result = subprocess.run(command)
        figure_cnt = glob.glob(viz_pattern)

        assert 0 == result.returncode
        assert 1 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization.py" startline="662" endline="717" pcid="18">
def test_visualization_compare_classifiers_predictions_csv_output_saved(csv_filename):
    """Ensure pdf and png figures from the experiments can be saved.

    Predictions are loaded form csv file.
    :param csv_filename: csv fixture from tests.fixtures.filenames.csv_filename
    :return: None
    """
    input_features = [category_feature(vocab_size=10)]
    output_features = [category_feature(vocab_size=2, reduce_input="sum")]

    # Generate test data
    rel_path = generate_data(input_features, output_features, csv_filename)
    exp_dir_name = run_experiment_with_visualization(input_features, output_features, dataset=rel_path)
    vis_output_pattern_pdf = os.path.join(exp_dir_name, "*.pdf")
    vis_output_pattern_png = os.path.join(exp_dir_name, "*.png")
    output_feature_name = get_output_feature_name(exp_dir_name)
    prediction = os.path.join(exp_dir_name, PREDICTIONS_PARQUET_FILE_NAME)
    experiment_source_data_name = csv_filename.split(".")[0]
    ground_truth = experiment_source_data_name + ".csv"
    split_file = experiment_source_data_name + ".split.csv"
    test_cmd_pdf = [
        "python",
        "-m",
        "ludwig.visualize",
        "--visualization",
        "compare_classifiers_predictions",
        "--ground_truth",
        ground_truth,
        "--output_feature_name",
        output_feature_name,
        "--split_file",
        split_file,
        "--ground_truth_metadata",
        exp_dir_name + "/model/training_set_metadata.json",
        "--predictions",
        prediction,
        prediction,
        "--model_names",
        "Model1",
        "Model2",
        "-od",
        exp_dir_name,
    ]
    test_cmd_png = test_cmd_pdf.copy() + ["-ff", "png"]

    commands = [test_cmd_pdf, test_cmd_png]
    vis_patterns = [vis_output_pattern_pdf, vis_output_pattern_png]

    for command, viz_pattern in zip(commands, vis_patterns):
        result = subprocess.run(command)
        figure_cnt = glob.glob(viz_pattern)

        assert 0 == result.returncode
        assert 1 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization.py" startline="828" endline="882" pcid="21">
def test_visualization_confidence_thresholding_data_vs_acc_output_saved(csv_filename):
    """Ensure pdf and png figures from the experiments can be saved.

    :param csv_filename: csv fixture from tests.conftest.csv_filename
    :return: None
    """
    input_features = [category_feature(vocab_size=10)]
    output_features = [category_feature(vocab_size=2, reduce_input="sum")]

    # Generate test data
    rel_path = generate_data(input_features, output_features, csv_filename)
    exp_dir_name = run_experiment_with_visualization(input_features, output_features, dataset=rel_path)
    vis_output_pattern_pdf = os.path.join(exp_dir_name, "*.pdf")
    vis_output_pattern_png = os.path.join(exp_dir_name, "*.png")
    output_feature_name = get_output_feature_name(exp_dir_name)
    probability = os.path.join(exp_dir_name, PREDICTIONS_PARQUET_FILE_NAME)
    experiment_source_data_name = csv_filename.split(".")[0]
    ground_truth = experiment_source_data_name + ".csv"
    split_file = experiment_source_data_name + ".split.csv"
    test_cmd_pdf = [
        "python",
        "-m",
        "ludwig.visualize",
        "--visualization",
        "confidence_thresholding_data_vs_acc",
        "--ground_truth",
        ground_truth,
        "--output_feature_name",
        output_feature_name,
        "--split_file",
        split_file,
        "--ground_truth_metadata",
        exp_dir_name + "/model/training_set_metadata.json",
        "--probabilities",
        probability,
        probability,
        "--model_names",
        "Model1",
        "Model2",
        "-od",
        exp_dir_name,
    ]
    test_cmd_png = test_cmd_pdf.copy() + ["-ff", "png"]

    commands = [test_cmd_pdf, test_cmd_png]
    vis_patterns = [vis_output_pattern_pdf, vis_output_pattern_png]

    for command, viz_pattern in zip(commands, vis_patterns):
        result = subprocess.run(command)
        figure_cnt = glob.glob(viz_pattern)

        assert 0 == result.returncode
        assert 1 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization.py" startline="1129" endline="1194" pcid="26">
def test_visualization_binary_threshold_vs_metric_output_saved(csv_filename):
    """Ensure pdf and png figures from the experiments can be saved.

    :param csv_filename: csv fixture from tests.conftest.csv_filename
    :return: None
    """
    input_features = [
        text_feature(vocab_size=10, min_len=1, encoder="stacked_cnn"),
        number_feature(),
        category_feature(vocab_size=10, embedding_size=5),
        set_feature(),
        sequence_feature(vocab_size=10, max_len=10, encoder="embed"),
    ]
    output_features = [category_feature(vocab_size=4, reduce_input="sum")]

    # Generate test data
    rel_path = generate_data(input_features, output_features, csv_filename)
    input_features[0]["encoder"] = "parallel_cnn"
    exp_dir_name = run_experiment_with_visualization(input_features, output_features, dataset=rel_path)
    vis_output_pattern_pdf = os.path.join(exp_dir_name, "*.pdf")
    vis_output_pattern_png = os.path.join(exp_dir_name, "*.png")
    output_feature_name = get_output_feature_name(exp_dir_name)
    probability = os.path.join(exp_dir_name, PREDICTIONS_PARQUET_FILE_NAME)
    experiment_source_data_name = csv_filename.split(".")[0]
    ground_truth = experiment_source_data_name + ".csv"
    split_file = experiment_source_data_name + ".split.csv"
    test_cmd_pdf = [
        "python",
        "-m",
        "ludwig.visualize",
        "--visualization",
        "binary_threshold_vs_metric",
        "--positive_label",
        "2",
        "--metrics",
        "accuracy",
        "--ground_truth",
        ground_truth,
        "--output_feature_name",
        output_feature_name,
        "--split_file",
        split_file,
        "--ground_truth_metadata",
        exp_dir_name + "/model/training_set_metadata.json",
        "--probabilities",
        probability,
        probability,
        "--model_names",
        "Model1",
        "Model2",
        "-od",
        exp_dir_name,
    ]
    test_cmd_png = test_cmd_pdf.copy() + ["-ff", "png"]

    commands = [test_cmd_pdf, test_cmd_png]
    vis_patterns = [vis_output_pattern_pdf, vis_output_pattern_png]

    for command, viz_pattern in zip(commands, vis_patterns):
        result = subprocess.run(command)
        figure_cnt = glob.glob(viz_pattern)

        assert 0 == result.returncode
        assert 1 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization.py" startline="1065" endline="1128" pcid="25">
def test_vis_confidence_thresholding_2thresholds_3d_output_saved(csv_filename):
    """Ensure pdf and png figures from the experiments can be saved.

    :param csv_filename: csv fixture from tests.conftest.csv_filename
    :return: None
    """
    input_features = [
        text_feature(vocab_size=10, min_len=1, encoder="stacked_cnn"),
        number_feature(),
        category_feature(vocab_size=10, embedding_size=5),
        set_feature(),
        sequence_feature(vocab_size=10, max_len=10, encoder="embed"),
    ]
    output_features = [
        category_feature(vocab_size=2, reduce_input="sum"),
        category_feature(vocab_size=2, reduce_input="sum"),
    ]
    # Generate test data
    rel_path = generate_data(input_features, output_features, csv_filename)
    input_features[0]["encoder"] = "parallel_cnn"
    exp_dir_name = run_experiment_with_visualization(input_features, output_features, dataset=rel_path)
    vis_output_pattern_pdf = os.path.join(exp_dir_name, "*.pdf")
    vis_output_pattern_png = os.path.join(exp_dir_name, "*.png")
    threshold_output_feature_name1 = get_output_feature_name(exp_dir_name)
    threshold_output_feature_name2 = get_output_feature_name(exp_dir_name, output_feature=1)
    probability = os.path.join(exp_dir_name, PREDICTIONS_PARQUET_FILE_NAME)
    experiment_source_data_name = csv_filename.split(".")[0]
    ground_truth = experiment_source_data_name + ".csv"
    split_file = experiment_source_data_name + ".split.csv"
    test_cmd_pdf = [
        "python",
        "-m",
        "ludwig.visualize",
        "--visualization",
        "confidence_thresholding_2thresholds_3d",
        "--ground_truth",
        ground_truth,
        "--split_file",
        split_file,
        "--ground_truth_metadata",
        exp_dir_name + "/model/training_set_metadata.json",
        "--probabilities",
        probability,
        "--threshold_output_feature_names",
        threshold_output_feature_name1,
        threshold_output_feature_name2,
        "-od",
        exp_dir_name,
    ]
    test_cmd_png = test_cmd_pdf.copy() + ["-ff", "png"]

    commands = [test_cmd_pdf, test_cmd_png]
    vis_patterns = [vis_output_pattern_pdf, vis_output_pattern_png]

    for command, viz_pattern in zip(commands, vis_patterns):
        result = subprocess.run(
            command,
        )
        figure_cnt = glob.glob(viz_pattern)

        assert 0 == result.returncode
        assert 1 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization.py" startline="999" endline="1064" pcid="24">
def test_vis_confidence_thresholding_2thresholds_2d_output_saved(csv_filename):
    """Ensure pdf and png figures from the experiments can be saved.

    :param csv_filename: csv fixture from tests.conftest.csv_filename
    :return: None
    """
    input_features = [
        text_feature(vocab_size=10, min_len=1, encoder="stacked_cnn"),
        number_feature(),
        category_feature(vocab_size=10, embedding_size=5),
        set_feature(),
        sequence_feature(vocab_size=10, max_len=10, encoder="embed"),
    ]
    output_features = [
        category_feature(vocab_size=2, reduce_input="sum"),
        category_feature(vocab_size=2, reduce_input="sum"),
    ]
    # Generate test data
    rel_path = generate_data(input_features, output_features, csv_filename)
    input_features[0]["encoder"] = "parallel_cnn"
    exp_dir_name = run_experiment_with_visualization(input_features, output_features, dataset=rel_path)
    vis_output_pattern_pdf = os.path.join(exp_dir_name, "*.pdf")
    vis_output_pattern_png = os.path.join(exp_dir_name, "*.png")
    threshold_output_feature_name1 = get_output_feature_name(exp_dir_name)
    threshold_output_feature_name2 = get_output_feature_name(exp_dir_name, output_feature=1)
    probability = os.path.join(exp_dir_name, PREDICTIONS_PARQUET_FILE_NAME)
    experiment_source_data_name = csv_filename.split(".")[0]
    ground_truth = experiment_source_data_name + ".csv"
    split_file = experiment_source_data_name + ".split.csv"
    test_cmd_pdf = [
        "python",
        "-m",
        "ludwig.visualize",
        "--visualization",
        "confidence_thresholding_2thresholds_2d",
        "--ground_truth",
        ground_truth,
        "--split_file",
        split_file,
        "--ground_truth_metadata",
        exp_dir_name + "/model/training_set_metadata.json",
        "--probabilities",
        probability,
        "--threshold_output_feature_names",
        threshold_output_feature_name1,
        threshold_output_feature_name2,
        "--model_names",
        "Model1",
        "-od",
        exp_dir_name,
    ]
    test_cmd_png = test_cmd_pdf.copy() + ["-ff", "png"]

    commands = [test_cmd_pdf, test_cmd_png]
    vis_patterns = [vis_output_pattern_pdf, vis_output_pattern_png]

    for command, viz_pattern in zip(commands, vis_patterns):
        result = subprocess.run(
            command,
        )
        figure_cnt = glob.glob(viz_pattern)

        assert 0 == result.returncode
        assert 3 == len(figure_cnt)


</source>
</class>

<class classid="3" nclones="5" nlines="10" similarity="83">
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_cli.py" startline="162" endline="174" pcid="42">
def test_export_savedmodel_cli(csv_filename):
    """Test exporting Ludwig model to Tensorflows savedmodel format."""
    with tempfile.TemporaryDirectory() as tmpdir:
        config_filename = os.path.join(tmpdir, "config.yaml")
        dataset_filename = _prepare_data(csv_filename, config_filename)
        _run_ludwig("train", dataset=dataset_filename, config=config_filename, output_directory=tmpdir)
        _run_ludwig(
            "export_savedmodel",
            model=os.path.join(tmpdir, "experiment_run", "model"),
            output_path=os.path.join(tmpdir, "savedmodel"),
        )


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_cli.py" startline="215" endline="228" pcid="46">
def test_evaluate_cli(csv_filename):
    """Test evaluate cli."""
    with tempfile.TemporaryDirectory() as tmpdir:
        config_filename = os.path.join(tmpdir, "config.yaml")
        dataset_filename = _prepare_data(csv_filename, config_filename)
        _run_ludwig("train", dataset=dataset_filename, config=config_filename, output_directory=tmpdir)
        _run_ludwig(
            "evaluate",
            dataset=dataset_filename,
            model=os.path.join(tmpdir, "experiment_run", "model"),
            output_directory=os.path.join(tmpdir, "predictions"),
        )


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_cli.py" startline="200" endline="213" pcid="45">
def test_predict_cli(csv_filename):
    """Test predict cli."""
    with tempfile.TemporaryDirectory() as tmpdir:
        config_filename = os.path.join(tmpdir, "config.yaml")
        dataset_filename = _prepare_data(csv_filename, config_filename)
        _run_ludwig("train", dataset=dataset_filename, config=config_filename, output_directory=tmpdir)
        _run_ludwig(
            "predict",
            dataset=dataset_filename,
            model=os.path.join(tmpdir, "experiment_run", "model"),
            output_directory=os.path.join(tmpdir, "predictions"),
        )


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_cli.py" startline="177" endline="189" pcid="43">
def test_export_neuropod_cli(csv_filename):
    """Test exporting Ludwig model to neuropod format."""
    with tempfile.TemporaryDirectory() as tmpdir:
        config_filename = os.path.join(tmpdir, "config.yaml")
        dataset_filename = _prepare_data(csv_filename, config_filename)
        _run_ludwig("train", dataset=dataset_filename, config=config_filename, output_directory=tmpdir)
        _run_ludwig(
            "export_neuropod",
            model=os.path.join(tmpdir, "experiment_run", "model"),
            output_path=os.path.join(tmpdir, "neuropod"),
        )


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_cli.py" startline="239" endline="253" pcid="48">
def test_visualize_cli(csv_filename):
    """Test Ludwig 'visualize' cli."""
    with tempfile.TemporaryDirectory() as tmpdir:
        config_filename = os.path.join(tmpdir, "config.yaml")
        dataset_filename = _prepare_data(csv_filename, config_filename)
        _run_ludwig("train", dataset=dataset_filename, config=config_filename, output_directory=tmpdir)
        _run_ludwig(
            "visualize",
            visualization="learning_curves",
            model_names="run",
            training_statistics=os.path.join(tmpdir, "experiment_run", "training_statistics.json"),
            output_directory=os.path.join(tmpdir, "visualizations"),
        )


</source>
</class>

<class classid="4" nclones="2" nlines="44" similarity="88">
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_server.py" startline="120" endline="183" pcid="60">
def test_server_integration_with_images(tmpdir):
    # Image Inputs
    image_dest_folder = os.path.join(tmpdir, "generated_images")

    # Resnet encoder
    input_features = [
        image_feature(
            folder=image_dest_folder,
            preprocessing={"in_memory": True, "height": 8, "width": 8, "num_channels": 3},
            output_size=16,
            num_filters=8,
        ),
        text_feature(encoder="embed", min_len=1),
        number_feature(normalization="zscore"),
    ]
    output_features = [category_feature(vocab_size=4), number_feature()]

    np.random.seed(123)  # reproducible synthetic data
    rel_path = generate_data(input_features, output_features, os.path.join(tmpdir, "dataset.csv"))

    model = train_and_predict_model(input_features, output_features, data_csv=rel_path, output_directory=tmpdir)

    app = server(model)
    client = TestClient(app)
    response = client.get("/")
    assert response.status_code == 200

    response = client.post("/predict")
    # expect the HTTP 400 error code for this situation
    assert response.status_code == 400
    assert response.json() == ALL_FEATURES_PRESENT_ERROR

    data_df = read_csv(rel_path)

    # One-off prediction
    first_entry = data_df.T.to_dict()[0]
    data, files = convert_to_form(first_entry)
    server_response = client.post("/predict", data=data, files=files)
    assert server_response.status_code == 200
    server_response = server_response.json()

    server_response_keys = sorted(list(server_response.keys()))
    assert server_response_keys == sorted(output_keys_for(output_features))

    model_output, _ = model.predict(dataset=[first_entry], data_format=dict)
    model_output = model_output.to_dict("records")[0]
    assert model_output == server_response

    # Batch prediction
    assert len(data_df) > 1
    files = convert_to_batch_form(data_df)
    server_response = client.post("/batch_predict", files=files)
    assert server_response.status_code == 200
    server_response = server_response.json()

    server_response_keys = sorted(server_response["columns"])
    assert server_response_keys == sorted(output_keys_for(output_features))
    assert len(data_df) == len(server_response["data"])

    model_output, _ = model.predict(dataset=data_df)
    model_output = model_output.to_dict("split")
    assert model_output == server_response


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_server.py" startline="185" endline="243" pcid="61">
def test_server_integration_with_audio(single_record, tmpdir):
    # Audio Inputs
    audio_dest_folder = os.path.join(tmpdir, "generated_audio")

    # Resnet encoder
    input_features = [
        audio_feature(
            folder=audio_dest_folder,
        ),
        text_feature(encoder="embed", min_len=1),
        number_feature(normalization="zscore"),
    ]
    output_features = [category_feature(vocab_size=4), number_feature()]

    rel_path = generate_data(input_features, output_features, os.path.join(tmpdir, "dataset.csv"))

    model = train_and_predict_model(input_features, output_features, data_csv=rel_path, output_directory=tmpdir)

    app = server(model)
    client = TestClient(app)
    response = client.get("/")
    assert response.status_code == 200

    response = client.post("/predict")
    # expect the HTTP 400 error code for this situation
    assert response.status_code == 400
    assert response.json() == ALL_FEATURES_PRESENT_ERROR

    data_df = read_csv(rel_path)

    if single_record:
        # Single record prediction
        first_entry = data_df.T.to_dict()[0]
        data, files = convert_to_form(first_entry)
        server_response = client.post("/predict", data=data, files=files)
        assert server_response.status_code == 200
        server_response = server_response.json()

        server_response_keys = sorted(list(server_response.keys()))
        assert server_response_keys == sorted(output_keys_for(output_features))

        model_output, _ = model.predict(dataset=[first_entry], data_format=dict)
        model_output = model_output.to_dict("records")[0]
        assert model_output == server_response
    else:
        # Batch prediction
        assert len(data_df) > 1
        files = convert_to_batch_form(data_df)
        server_response = client.post("/batch_predict", files=files)
        assert server_response.status_code == 200
        server_response = server_response.json()

        server_response_keys = sorted(server_response["columns"])
        assert server_response_keys == sorted(output_keys_for(output_features))
        assert len(data_df) == len(server_response["data"])

        model_output, _ = model.predict(dataset=data_df)
        model_output = model_output.to_dict("split")
        assert model_output == server_response
</source>
</class>

<class classid="5" nclones="2" nlines="11" similarity="100">
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_graph_execution.py" startline="57" endline="68" pcid="80">
def test_experiment_multiple_seq_seq(csv_filename, output_features):
    input_features = [
        text_feature(vocab_size=100, min_len=1, encoder="stacked_cnn"),
        number_feature(normalization="zscore"),
        category_feature(vocab_size=10, embedding_size=5),
        set_feature(),
        sequence_feature(vocab_size=10, max_len=10, encoder="embed"),
    ]
    output_features = output_features

    rel_path = generate_data(input_features, output_features, csv_filename)
    run_experiment(input_features, output_features, dataset=rel_path)
</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_experiment.py" startline="245" endline="258" pcid="161">
def test_experiment_multiple_seq_seq(csv_filename, output_features):
    input_features = [
        text_feature(vocab_size=100, min_len=1, encoder="stacked_cnn"),
        number_feature(normalization="zscore"),
        category_feature(vocab_size=10, embedding_size=5),
        set_feature(),
        sequence_feature(vocab_size=10, max_len=10, encoder="embed"),
    ]
    output_features = output_features

    rel_path = generate_data(input_features, output_features, csv_filename)
    run_experiment(input_features, output_features, dataset=rel_path)


</source>
</class>

<class classid="6" nclones="18" nlines="18" similarity="70">
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization_api.py" startline="153" endline="177" pcid="101">
def test_compare_performance_vis_api(experiment_to_use):
    """Ensure pdf and png figures can be saved via visualization API call.

    :param experiment_to_use: Object containing trained model and results to
        test visualization
    :return: None
    """
    experiment = experiment_to_use
    # extract test stats only
    test_stats = experiment.test_stats_full
    viz_outputs = ("pdf", "png")
    with TemporaryDirectory() as tmpvizdir:
        for viz_output in viz_outputs:
            vis_output_pattern_pdf = tmpvizdir + f"/*.{viz_output}"
            visualize.compare_performance(
                [test_stats, test_stats],
                output_feature_name=None,
                model_names=["Model1", "Model2"],
                output_directory=tmpvizdir,
                file_format=viz_output,
            )
            figure_cnt = glob.glob(vis_output_pattern_pdf)
            assert 1 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization_api.py" startline="317" endline="343" pcid="107">
def test_compare_classifiers_predictions_vis_api(experiment_to_use):
    """Ensure pdf and png figures can be saved via visualization API call.

    :param experiment_to_use: Object containing trained model and results to
        test visualization
    :return: None
    """
    experiment = experiment_to_use
    predictions = experiment.predictions
    viz_outputs = ("pdf", "png")
    with TemporaryDirectory() as tmpvizdir:
        for viz_output in viz_outputs:
            vis_output_pattern_pdf = tmpvizdir + f"/*.{viz_output}"
            visualize.compare_classifiers_predictions(
                [predictions, predictions],
                experiment.ground_truth,
                experiment.ground_truth_metadata,
                experiment.output_feature_name,
                labels_limit=0,
                model_names=["Model1", "Model2"],
                output_directory=tmpvizdir,
                file_format=viz_output,
            )
            figure_cnt = glob.glob(vis_output_pattern_pdf)
            assert 1 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization_api.py" startline="805" endline="831" pcid="121">
def test_frequency_vs_f1_vis_api(experiment_to_use):
    """Ensure pdf and png figures can be saved via visualization API call.

    :param experiment_to_use: Object containing trained model and results to
        test visualization
    :return: None
    """
    experiment = experiment_to_use
    # extract test stats
    test_stats = experiment.test_stats_full
    viz_outputs = ("pdf", "png")
    with TemporaryDirectory() as tmpvizdir:
        for viz_output in viz_outputs:
            vis_output_pattern_pdf = tmpvizdir + f"/*.{viz_output}"
            visualize.frequency_vs_f1(
                [test_stats, test_stats],
                experiment.ground_truth_metadata,
                experiment.output_feature_name,
                top_n_classes=[0],
                model_names=["Model1", "Model2"],
                output_directory=tmpvizdir,
                file_format=viz_output,
            )
            figure_cnt = glob.glob(vis_output_pattern_pdf)
            assert 2 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization_api.py" startline="777" endline="804" pcid="120">
def test_confusion_matrix_vis_api(experiment_to_use):
    """Ensure pdf and png figures can be saved via visualization API call.

    :param experiment_to_use: Object containing trained model and results to
        test visualization
    :return: None
    """
    experiment = experiment_to_use
    # extract test stats only
    test_stats = experiment.test_stats_full
    viz_outputs = ("pdf", "png")
    with TemporaryDirectory() as tmpvizdir:
        for viz_output in viz_outputs:
            vis_output_pattern_pdf = tmpvizdir + f"/*.{viz_output}"
            visualize.confusion_matrix(
                [test_stats, test_stats],
                experiment.ground_truth_metadata,
                experiment.output_feature_name,
                top_n_classes=[0],
                normalize=False,
                model_names=["Model1", "Model2"],
                output_directory=tmpvizdir,
                file_format=viz_output,
            )
            figure_cnt = glob.glob(vis_output_pattern_pdf)
            assert 4 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization_api.py" startline="722" endline="749" pcid="118">
def test_calibration_1_vs_all_vis_api(experiment_to_use):
    """Ensure pdf and png figures can be saved via visualization API call.

    :param experiment_to_use: Object containing trained model and results to
        test visualization
    :return: None
    """
    experiment = experiment_to_use
    probabilities = experiment.probabilities
    viz_outputs = ("pdf", "png")
    with TemporaryDirectory() as tmpvizdir:
        for viz_output in viz_outputs:
            vis_output_pattern_pdf = os.path.join(tmpvizdir, f"*.{viz_output}")
            visualize.calibration_1_vs_all(
                [probabilities, probabilities],
                experiment.ground_truth,
                experiment.ground_truth_metadata,
                experiment.output_feature_name,
                top_n_classes=[6],
                labels_limit=0,
                model_namess=["Model1", "Model2"],
                output_directory=tmpvizdir,
                file_format=viz_output,
            )
            figure_cnt = glob.glob(vis_output_pattern_pdf)
            assert 5 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization_api.py" startline="660" endline="687" pcid="116">
def test_roc_curves_vis_api(experiment_to_use):
    """Ensure pdf and png figures can be saved via visualization API call.

    :param experiment_to_use: Object containing trained model and results to
        test visualization
    :return: None
    """
    experiment = experiment_to_use
    probabilities = experiment.probabilities
    viz_outputs = ("pdf", "png")
    positive_label = 2
    with TemporaryDirectory() as tmpvizdir:
        for viz_output in viz_outputs:
            vis_output_pattern_pdf = tmpvizdir + f"/*.{viz_output}"
            visualize.roc_curves(
                [probabilities, probabilities],
                experiment.ground_truth,
                experiment.ground_truth_metadata,
                experiment.output_feature_name,
                positive_label,
                model_names=["Model1", "Model2"],
                output_directory=tmpvizdir,
                file_format=viz_output,
            )
            figure_cnt = glob.glob(vis_output_pattern_pdf)
            assert 1 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization_api.py" startline="290" endline="316" pcid="106">
def test_compare_classifiers_multiclass_multimetric_vis_api(experiment_to_use):
    """Ensure pdf and png figures can be saved via visualization API call.

    :param experiment_to_use: Object containing trained model and results to
        test visualization
    :return: None
    """
    experiment = experiment_to_use
    # extract test stats only
    test_stats = experiment.test_stats_full
    viz_outputs = ("pdf", "png")
    with TemporaryDirectory() as tmpvizdir:
        for viz_output in viz_outputs:
            vis_output_pattern_pdf = tmpvizdir + f"/*.{viz_output}"
            visualize.compare_classifiers_multiclass_multimetric(
                [test_stats, test_stats],
                experiment.ground_truth_metadata,
                experiment.output_feature_name,
                top_n_classes=[6],
                model_namess=["Model1", "Model2"],
                output_directory=tmpvizdir,
                file_format=viz_output,
            )
            figure_cnt = glob.glob(vis_output_pattern_pdf)
            assert 4 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization_api.py" startline="206" endline="232" pcid="103">
def test_compare_classifier_performance_from_pred_vis_api(experiment_to_use):
    """Ensure pdf and png figures can be saved via visualization API call.

    :param experiment_to_use: Object containing trained model and results to
        test visualization
    :return: None
    """
    experiment = experiment_to_use
    prediction = experiment.predictions
    viz_outputs = ("pdf", "png")
    with TemporaryDirectory() as tmpvizdir:
        for viz_output in viz_outputs:
            vis_output_pattern_pdf = tmpvizdir + f"/*.{viz_output}"
            visualize.compare_classifiers_performance_from_pred(
                [prediction, prediction],
                experiment.ground_truth,
                experiment.ground_truth_metadata,
                experiment.output_feature_name,
                labels_limit=0,
                model_namess=["Model1", "Model2"],
                output_directory=tmpvizdir,
                file_format=viz_output,
            )
            figure_cnt = glob.glob(vis_output_pattern_pdf)
            assert 1 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization_api.py" startline="233" endline="261" pcid="104">
def test_compare_classifiers_performance_subset_vis_api(experiment_to_use):
    """Ensure pdf and png figures can be saved via visualization API call.

    :param experiment_to_use: Object containing trained model and results to
        test visualization
    :return: None
    """
    experiment = experiment_to_use
    probabilities = experiment.probabilities
    viz_outputs = ("pdf", "png")
    with TemporaryDirectory() as tmpvizdir:
        for viz_output in viz_outputs:
            vis_output_pattern_pdf = tmpvizdir + f"/*.{viz_output}"
            visualize.compare_classifiers_performance_subset(
                [probabilities, probabilities],
                experiment.ground_truth,
                experiment.ground_truth_metadata,
                experiment.output_feature_name,
                top_n_classes=[6],
                labels_limit=0,
                subset="ground_truth",
                model_namess=["Model1", "Model2"],
                output_directory=tmpvizdir,
                file_format=viz_output,
            )
            figure_cnt = glob.glob(vis_output_pattern_pdf)
            assert 1 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization_api.py" startline="262" endline="289" pcid="105">
def test_compare_classifiers_performance_changing_k_vis_api(experiment_to_use):
    """Ensure pdf and png figures can be saved via visualization API call.

    :param experiment_to_use: Object containing trained model and results to
        test visualization
    :return: None
    """
    experiment = experiment_to_use
    probabilities = experiment.probabilities
    viz_outputs = ("pdf", "png")
    with TemporaryDirectory() as tmpvizdir:
        for viz_output in viz_outputs:
            vis_output_pattern_pdf = tmpvizdir + f"/*.{viz_output}"
            visualize.compare_classifiers_performance_changing_k(
                [probabilities, probabilities],
                experiment.ground_truth,
                experiment.ground_truth_metadata,
                experiment.output_feature_name,
                top_k=3,
                labels_limit=0,
                model_namess=["Model1", "Model2"],
                output_directory=tmpvizdir,
                file_format=viz_output,
            )
            figure_cnt = glob.glob(vis_output_pattern_pdf)
            assert 1 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization_api.py" startline="398" endline="424" pcid="110">
def test_confidence_thresholding_data_vs_acc_vis_api(experiment_to_use):
    """Ensure pdf and png figures can be saved via visualization API call.

    :param experiment_to_use: Object containing trained model and results to
        test visualization
    :return: None
    """
    experiment = experiment_to_use
    probabilities = experiment.probabilities
    viz_outputs = ("pdf", "png")
    with TemporaryDirectory() as tmpvizdir:
        for viz_output in viz_outputs:
            vis_output_pattern_pdf = tmpvizdir + f"/*.{viz_output}"
            visualize.confidence_thresholding_data_vs_acc(
                [probabilities, probabilities],
                experiment.ground_truth,
                experiment.ground_truth_metadata,
                experiment.output_feature_name,
                labels_limit=0,
                model_names=["Model1", "Model2"],
                output_directory=tmpvizdir,
                file_format=viz_output,
            )
            figure_cnt = glob.glob(vis_output_pattern_pdf)
            assert 1 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization_api.py" startline="425" endline="453" pcid="111">
def test_confidence_thresholding_data_vs_acc_subset_vis_api(experiment_to_use):
    """Ensure pdf and png figures can be saved via visualization API call.

    :param experiment_to_use: Object containing trained model and results to
        test visualization
    :return: None
    """
    experiment = experiment_to_use
    probabilities = experiment.probabilities
    viz_outputs = ("pdf", "png")
    with TemporaryDirectory() as tmpvizdir:
        for viz_output in viz_outputs:
            vis_output_pattern_pdf = tmpvizdir + f"/*.{viz_output}"
            visualize.confidence_thresholding_data_vs_acc_subset(
                [probabilities, probabilities],
                experiment.ground_truth,
                experiment.ground_truth_metadata,
                experiment.output_feature_name,
                top_n_classes=[3],
                labels_limit=0,
                subset="ground_truth",
                model_names=["Model1", "Model2"],
                output_directory=tmpvizdir,
                file_format=viz_output,
            )
            figure_cnt = glob.glob(vis_output_pattern_pdf)
            assert 1 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization_api.py" startline="454" endline="484" pcid="112">
def test_confidence_thresholding_data_vs_acc_subset_per_class_vis_api(experiment_to_use):
    """Ensure pdf and png figures can be saved via visualization API call.

    :param experiment_to_use: Object containing trained model and results to
        test visualization
    :return: None
    """
    experiment = experiment_to_use
    probabilities = experiment.probabilities
    viz_outputs = ("pdf", "png")
    with TemporaryDirectory() as tmpvizdir:
        for viz_output in viz_outputs:
            vis_output_pattern_pdf = tmpvizdir + f"/*.{viz_output}"
            visualize.confidence_thresholding_data_vs_acc_subset_per_class(
                [probabilities, probabilities],
                experiment.ground_truth,
                experiment.ground_truth_metadata,
                experiment.output_feature_name,
                top_n_classes=[3],
                labels_limit=0,
                subset="ground_truth",
                model_names=["Model1", "Model2"],
                output_directory=tmpvizdir,
                file_format=viz_output,
            )
            figure_cnt = glob.glob(vis_output_pattern_pdf)
            # 3 figures should be saved because experiment setting top_n_classes = 3
            # hence one figure per class
            assert 3 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization_api.py" startline="178" endline="205" pcid="102">
def test_compare_classifier_performance_from_prob_vis_api(experiment_to_use):
    """Ensure pdf and png figures can be saved via visualization API call.

    :param experiment_to_use: Object containing trained model and results to
        test visualization
    :return: None
    """
    experiment = experiment_to_use
    probability = experiment.probabilities
    viz_outputs = ("pdf", "png")
    with TemporaryDirectory() as tmpvizdir:
        for viz_output in viz_outputs:
            vis_output_pattern_pdf = tmpvizdir + f"/*.{viz_output}"
            visualize.compare_classifiers_performance_from_prob(
                [probability, probability],
                experiment.ground_truth,
                experiment.ground_truth_metadata,
                experiment.output_feature_name,
                top_n_classes=[0],
                labels_limit=0,
                model_namess=["Model1", "Model2"],
                output_directory=tmpvizdir,
                file_format=viz_output,
            )
            figure_cnt = glob.glob(vis_output_pattern_pdf)
            assert 1 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization_api.py" startline="371" endline="397" pcid="109">
def test_confidence_thresholding_vis_api(experiment_to_use):
    """Ensure pdf and png figures can be saved via visualization API call.

    :param experiment_to_use: Object containing trained model and results to
        test visualization
    :return: None
    """
    experiment = experiment_to_use
    probabilities = experiment.probabilities
    viz_outputs = ("pdf", "png")
    with TemporaryDirectory() as tmpvizdir:
        for viz_output in viz_outputs:
            vis_output_pattern_pdf = tmpvizdir + f"/*.{viz_output}"
            visualize.confidence_thresholding(
                [probabilities, probabilities],
                experiment.ground_truth,
                experiment.ground_truth_metadata,
                experiment.output_feature_name,
                labels_limit=0,
                model_names=["Model1", "Model2"],
                output_directory=tmpvizdir,
                file_format=viz_output,
            )
            figure_cnt = glob.glob(vis_output_pattern_pdf)
            assert 1 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization_api.py" startline="750" endline="776" pcid="119">
def test_calibration_multiclass_vis_api(experiment_to_use):
    """Ensure pdf and png figures can be saved via visualization API call.

    :param experiment_to_use: Object containing trained model and results to
        test visualization
    :return: None
    """
    experiment = experiment_to_use
    probabilities = experiment.probabilities
    viz_outputs = ("pdf", "png")
    with TemporaryDirectory() as tmpvizdir:
        for viz_output in viz_outputs:
            vis_output_pattern_pdf = tmpvizdir + f"/*.{viz_output}"
            visualize.calibration_multiclass(
                [probabilities, probabilities],
                experiment.ground_truth,
                experiment.ground_truth_metadata,
                experiment.output_feature_name,
                labels_limit=0,
                model_names=["Model1", "Model2"],
                output_directory=tmpvizdir,
                file_format=viz_output,
            )
            figure_cnt = glob.glob(vis_output_pattern_pdf)
            assert 2 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization_api.py" startline="344" endline="370" pcid="108">
def test_compare_classifiers_predictions_distribution_vis_api(experiment_to_use):
    """Ensure pdf and png figures can be saved via visualization API call.

    :param experiment_to_use: Object containing trained model and results to
        test visualization
    :return: None
    """
    experiment = experiment_to_use
    predictions = experiment.predictions_num
    viz_outputs = ("pdf", "png")
    with TemporaryDirectory() as tmpvizdir:
        for viz_output in viz_outputs:
            vis_output_pattern_pdf = tmpvizdir + f"/*.{viz_output}"
            visualize.compare_classifiers_predictions_distribution(
                [predictions, predictions],
                experiment.ground_truth,
                experiment.ground_truth_metadata,
                experiment.output_feature_name,
                labels_limit=0,
                model_names=["Model1", "Model2"],
                output_directory=tmpvizdir,
                file_format=viz_output,
            )
            figure_cnt = glob.glob(vis_output_pattern_pdf)
            assert 1 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization_api.py" startline="630" endline="659" pcid="115">
def test_binary_threshold_vs_metric_vis_api(experiment_to_use):
    """Ensure pdf and png figures can be saved via visualization API call.

    :param experiment_to_use: Object containing trained model and results to
        test visualization
    :return: None
    """
    experiment = experiment_to_use
    probabilities = experiment.probabilities
    viz_outputs = ("pdf", "png")
    metrics = ["accuracy"]
    positive_label = 2
    with TemporaryDirectory() as tmpvizdir:
        for viz_output in viz_outputs:
            vis_output_pattern_pdf = tmpvizdir + f"/*.{viz_output}"
            visualize.binary_threshold_vs_metric(
                [probabilities, probabilities],
                experiment.ground_truth,
                experiment.ground_truth_metadata,
                experiment.output_feature_name,
                metrics,
                positive_label,
                model_names=["Model1", "Model2"],
                output_directory=tmpvizdir,
                file_format=viz_output,
            )
            figure_cnt = glob.glob(vis_output_pattern_pdf)
            assert 1 == len(figure_cnt)


</source>
</class>

<class classid="7" nclones="2" nlines="53" similarity="94">
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization_api.py" startline="485" endline="556" pcid="113">
def test_confidence_thresholding_2thresholds_2d_vis_api(csv_filename):
    """Ensure pdf and png figures can be saved via visualization API call.

    :param csv_filename: csv fixture from tests.fixtures.filenames.csv_filename
    :return: None
    """
    input_features = [
        text_feature(vocab_size=10, min_len=1, encoder="stacked_cnn"),
        number_feature(),
        category_feature(vocab_size=10, embedding_size=5),
        set_feature(),
        sequence_feature(vocab_size=10, max_len=10, encoder="embed"),
    ]
    output_features = [
        category_feature(vocab_size=2, reduce_input="sum"),
        category_feature(vocab_size=2, reduce_input="sum"),
    ]
    encoder = "parallel_cnn"
    with TemporaryDirectory() as tmpvizdir:
        # Generate test data
        data_csv = generate_data(input_features, output_features, os.path.join(tmpvizdir, csv_filename))
        input_features[0]["encoder"] = encoder
        model = run_api_experiment(input_features, output_features)
        test_df, train_df, val_df = obtain_df_splits(data_csv)
        _, _, output_dir = model.train(
            training_set=train_df, validation_set=val_df, output_directory=os.path.join(tmpvizdir, "results")
        )
        test_stats, predictions, _ = model.evaluate(dataset=test_df, collect_predictions=True, output_dir=output_dir)

        output_feature_name1 = output_features[0]["name"]
        output_feature_name2 = output_features[1]["name"]

        ground_truth_metadata = model.training_set_metadata
        feature1_cols = [
            f"{output_feature_name1}_probabilities_{label}"
            for label in ground_truth_metadata[output_feature_name1]["idx2str"]
        ]
        feature2_cols = [
            f"{output_feature_name2}_probabilities_{label}"
            for label in ground_truth_metadata[output_feature_name2]["idx2str"]
        ]

        # probabilities need to be list of lists containing each row data from the
        # probability columns ref: https://ludwig-ai.github.io/ludwig-docs/api/#test - Return
        probability1 = predictions.loc[:, feature1_cols].values
        probability2 = predictions.loc[:, feature2_cols].values

        target_predictions1 = test_df[output_feature_name1]
        target_predictions2 = test_df[output_feature_name2]
        ground_truth1 = np.asarray(
            [ground_truth_metadata[output_feature_name1]["str2idx"][prediction] for prediction in target_predictions1]
        )
        ground_truth2 = np.asarray(
            [ground_truth_metadata[output_feature_name2]["str2idx"][prediction] for prediction in target_predictions2]
        )
        viz_outputs = ("pdf", "png")
        for viz_output in viz_outputs:
            vis_output_pattern_pdf = os.path.join(output_dir, "*.{}").format(viz_output)
            visualize.confidence_thresholding_2thresholds_2d(
                [probability1, probability2],
                [ground_truth1, ground_truth2],
                model.training_set_metadata,
                [output_feature_name1, output_feature_name2],
                labels_limit=0,
                model_names=["Model1"],
                output_directory=output_dir,
                file_format=viz_output,
            )
            figure_cnt = glob.glob(vis_output_pattern_pdf)
            assert 3 == len(figure_cnt)


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_visualization_api.py" startline="557" endline="629" pcid="114">
def test_confidence_thresholding_2thresholds_3d_vis_api(csv_filename):
    """Ensure pdf and png figures can be saved via visualization API call.

    :param csv_filename: csv fixture from tests.fixtures.filenames.csv_filename
    :return: None
    """
    input_features = [
        text_feature(vocab_size=10, min_len=1, encoder="stacked_cnn"),
        number_feature(),
        category_feature(vocab_size=10, embedding_size=5),
        set_feature(),
        sequence_feature(vocab_size=10, max_len=10, encoder="embed"),
    ]
    output_features = [
        category_feature(vocab_size=2, reduce_input="sum"),
        category_feature(vocab_size=2, reduce_input="sum"),
    ]
    encoder = "parallel_cnn"
    with TemporaryDirectory() as tmpvizdir:
        # Generate test data
        data_csv = generate_data(input_features, output_features, os.path.join(tmpvizdir, csv_filename))
        input_features[0]["encoder"] = encoder
        model = run_api_experiment(input_features, output_features)
        test_df, train_df, val_df = obtain_df_splits(data_csv)
        _, _, output_dir = model.train(
            training_set=train_df, validation_set=val_df, output_directory=os.path.join(tmpvizdir, "results")
        )
        test_stats, predictions, _ = model.evaluate(
            dataset=test_df, collect_predictions=True, output_directory=output_dir
        )

        output_feature_name1 = output_features[0]["name"]
        output_feature_name2 = output_features[1]["name"]

        ground_truth_metadata = model.training_set_metadata
        feature1_cols = [
            f"{output_feature_name1}_probabilities_{label}"
            for label in ground_truth_metadata[output_feature_name1]["idx2str"]
        ]
        feature2_cols = [
            f"{output_feature_name2}_probabilities_{label}"
            for label in ground_truth_metadata[output_feature_name2]["idx2str"]
        ]

        # probabilities need to be list of lists containing each row data from the
        # probability columns ref: https://ludwig-ai.github.io/ludwig-docs/api/#test - Return
        probability1 = predictions.loc[:, feature1_cols].values
        probability2 = predictions.loc[:, feature2_cols].values

        target_predictions1 = test_df[output_feature_name1]
        target_predictions2 = test_df[output_feature_name2]
        ground_truth1 = np.asarray(
            [ground_truth_metadata[output_feature_name1]["str2idx"][prediction] for prediction in target_predictions1]
        )
        ground_truth2 = np.asarray(
            [ground_truth_metadata[output_feature_name2]["str2idx"][prediction] for prediction in target_predictions2]
        )
        viz_outputs = ("pdf", "png")
        for viz_output in viz_outputs:
            vis_output_pattern_pdf = os.path.join(output_dir, f"*.{viz_output}")
            visualize.confidence_thresholding_2thresholds_3d(
                [probability1, probability2],
                [ground_truth1, ground_truth2],
                model.training_set_metadata,
                [output_feature_name1, output_feature_name2],
                labels_limit=0,
                output_directory=output_dir,
                file_format=viz_output,
            )
            figure_cnt = glob.glob(vis_output_pattern_pdf)
            assert 1 == len(figure_cnt)


</source>
</class>

<class classid="8" nclones="3" nlines="22" similarity="73">
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_api.py" startline="337" endline="366" pcid="144">
def test_api_skip_parameters_train(
    csv_filename,
    skip_save_training_description,
    skip_save_training_statistics,
    skip_save_model,
    skip_save_progress,
    skip_save_log,
    skip_save_processed_input,
):
    # Single sequence input, single category output
    input_features = [category_feature(vocab_size=5)]
    output_features = [category_feature(vocab_size=5)]

    with tempfile.TemporaryDirectory() as output_dir:
        # Generate test data
        rel_path = generate_data(input_features, output_features, os.path.join(output_dir, csv_filename))
        run_api_commands(
            input_features,
            output_features,
            data_csv=rel_path,
            output_dir=output_dir,
            skip_save_training_description=skip_save_training_description,
            skip_save_training_statistics=skip_save_training_statistics,
            skip_save_model=skip_save_model,
            skip_save_progress=skip_save_progress,
            skip_save_log=skip_save_log,
            skip_save_processed_input=skip_save_processed_input,
        )


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_api.py" startline="396" endline="423" pcid="146">
def test_api_skip_parameters_evaluate(
    csv_filename,
    skip_save_unprocessed_output,
    skip_save_predictions,
    skip_save_eval_stats,
    skip_collect_predictions,
    skip_collect_overall_stats,
):
    # Single sequence input, single category output
    input_features = [category_feature(vocab_size=5)]
    output_features = [category_feature(vocab_size=5)]

    with tempfile.TemporaryDirectory() as output_dir:
        # Generate test data
        rel_path = generate_data(input_features, output_features, os.path.join(output_dir, csv_filename))
        run_api_commands(
            input_features,
            output_features,
            data_csv=rel_path,
            output_dir=output_dir,
            skip_save_unprocessed_output=skip_save_unprocessed_output,
            skip_save_predictions=skip_save_predictions,
            skip_save_eval_stats=skip_save_eval_stats,
            skip_collect_predictions=skip_collect_predictions,
            skip_collect_overall_stats=skip_collect_overall_stats,
        )


</source>
<source file="systems/ludwig-0.5rc2/tests/integration_tests/test_api.py" startline="369" endline="390" pcid="145">
def test_api_skip_parameters_predict(
    csv_filename,
    skip_save_unprocessed_output,
    skip_save_predictions,
):
    # Single sequence input, single category output
    input_features = [category_feature(vocab_size=5)]
    output_features = [category_feature(vocab_size=5)]

    with tempfile.TemporaryDirectory() as output_dir:
        # Generate test data
        rel_path = generate_data(input_features, output_features, os.path.join(output_dir, csv_filename))
        run_api_commands(
            input_features,
            output_features,
            data_csv=rel_path,
            output_dir=output_dir,
            skip_save_unprocessed_output=skip_save_unprocessed_output,
            skip_save_predictions=skip_save_predictions,
        )


</source>
</class>

<class classid="9" nclones="2" nlines="11" similarity="75">
<source file="systems/ludwig-0.5rc2/tests/ludwig/decoders/test_sequence_decoder_utils.py" startline="10" endline="25" pcid="189">
def test_get_rnn_init_state_uses_hidden(num_layers):
    batch_size = 16
    sequence_length = 32
    state_size = 64
    combiner_outputs = {}
    combiner_outputs[HIDDEN] = torch.rand([batch_size, sequence_length, state_size])

    # With sequence reduction.
    result = sequence_decoder_utils.get_rnn_init_state(combiner_outputs, SequenceReducer(reduce_mode="sum"), num_layers)
    assert list(result.size()) == [num_layers, batch_size, state_size]

    # Without sequence reduction.
    with pytest.raises(ValueError):
        sequence_decoder_utils.get_rnn_init_state(combiner_outputs, SequenceReducer(reduce_mode="none"), num_layers)


</source>
<source file="systems/ludwig-0.5rc2/tests/ludwig/decoders/test_sequence_decoder_utils.py" startline="40" endline="58" pcid="191">
def test_get_lstm_init_state_uses_hidden(num_layers):
    batch_size = 16
    sequence_length = 32
    state_size = 64
    combiner_outputs = {}
    combiner_outputs[HIDDEN] = torch.rand([batch_size, sequence_length, state_size])

    # With sequence reduction.
    decoder_hidden_state, decoder_cell_state = sequence_decoder_utils.get_lstm_init_state(
        combiner_outputs, SequenceReducer(reduce_mode="sum"), num_layers
    )
    assert list(decoder_hidden_state.size()) == [num_layers, batch_size, state_size]
    assert list(decoder_cell_state.size()) == [num_layers, batch_size, state_size]

    # Without sequence reduction.
    with pytest.raises(ValueError):
        sequence_decoder_utils.get_lstm_init_state(combiner_outputs, SequenceReducer(reduce_mode="none"), num_layers)


</source>
</class>

<class classid="10" nclones="3" nlines="11" similarity="100">
<source file="systems/ludwig-0.5rc2/tests/ludwig/encoders/test_h3_encoders.py" startline="11" endline="23" pcid="231">
def test_h3_embed():
    embed = h3_encoders.H3Embed().to(DEVICE)
    inputs = torch.tensor(
        [
            [2, 0, 14, 102, 7, 0, 3, 5, 0, 5, 5, 0, 5, 7, 7, 7, 7, 7, 7],
            [2, 0, 14, 102, 7, 0, 3, 5, 0, 5, 5, 0, 5, 7, 7, 7, 7, 7, 7],
        ],
        dtype=torch.int32,
    ).to(DEVICE)
    outputs = embed(inputs)
    assert outputs["encoder_output"].size()[1:] == embed.output_shape


</source>
<source file="systems/ludwig-0.5rc2/tests/ludwig/encoders/test_h3_encoders.py" startline="24" endline="36" pcid="232">
def test_h3_weighted_sum():
    embed = h3_encoders.H3WeightedSum().to(DEVICE)
    inputs = torch.tensor(
        [
            [2, 0, 14, 102, 7, 0, 3, 5, 0, 5, 5, 0, 5, 7, 7, 7, 7, 7, 7],
            [2, 0, 14, 102, 7, 0, 3, 5, 0, 5, 5, 0, 5, 7, 7, 7, 7, 7, 7],
        ],
        dtype=torch.int32,
    ).to(DEVICE)
    outputs = embed(inputs)
    assert outputs["encoder_output"].size()[1:] == embed.output_shape


</source>
<source file="systems/ludwig-0.5rc2/tests/ludwig/encoders/test_h3_encoders.py" startline="37" endline="47" pcid="233">
def test_h3_rnn_embed():
    embed = h3_encoders.H3RNN().to(DEVICE)
    inputs = torch.tensor(
        [
            [2, 0, 14, 102, 7, 0, 3, 5, 0, 5, 5, 0, 5, 7, 7, 7, 7, 7, 7],
            [2, 0, 14, 102, 7, 0, 3, 5, 0, 5, 5, 0, 5, 7, 7, 7, 7, 7, 7],
        ],
        dtype=torch.int32,
    ).to(DEVICE)
    outputs = embed(inputs)
    assert outputs["encoder_output"].size()[1:] == embed.output_shape
</source>
</class>

<class classid="11" nclones="2" nlines="23" similarity="79">
<source file="systems/ludwig-0.5rc2/tests/ludwig/utils/test_class_balancing.py" startline="54" endline="79" pcid="238">
def test_non_binary_failure():
    config = {
        "input_features": [
            {"name": "Index", "proc_column": "Index", "type": "number"},
            {"name": "random_1", "proc_column": "random_1", "type": "number"},
            {"name": "random_2", "proc_column": "random_2", "type": "number"},
        ],
        "output_features": [{"name": "Label", "proc_column": "Label", "type": "number"}],
        "preprocessing": {},
    }
    input_df = pd.DataFrame(
        {
            "Index": np.arange(0, 200, 1),
            "random_1": np.random.randint(0, 50, 200),
            "random_2": np.random.choice(["Type A", "Type B", "Type C", "Type D"], 200),
            "Label": np.concatenate((np.zeros(180), np.ones(20))),
            "split": np.zeros(200),
        }
    )

    backend = LocalBackend()

    with pytest.raises(ValueError):
        balance_data(input_df, config["output_features"], config["preprocessing"], backend)


</source>
<source file="systems/ludwig-0.5rc2/tests/ludwig/utils/test_class_balancing.py" startline="80" endline="107" pcid="239">
def test_multiple_class_failure():
    config = {
        "input_features": [
            {"name": "Index", "proc_column": "Index", "type": "number"},
            {"name": "random_1", "proc_column": "random_1", "type": "number"},
            {"name": "random_2", "proc_column": "random_2", "type": "number"},
        ],
        "output_features": [
            {"name": "Label", "proc_column": "Label", "type": "binary"},
            {"name": "Label2", "proc_column": "Label2", "type": "binary"},
        ],
        "preprocessing": {},
    }
    input_df = pd.DataFrame(
        {
            "Index": np.arange(0, 200, 1),
            "random_1": np.random.randint(0, 50, 200),
            "random_2": np.random.choice(["Type A", "Type B", "Type C", "Type D"], 200),
            "Label": np.concatenate((np.zeros(180), np.ones(20))),
            "Label2": np.concatenate((np.zeros(180), np.ones(20))),
            "split": np.zeros(200),
        }
    )

    backend = LocalBackend()

    with pytest.raises(ValueError):
        balance_data(input_df, config["output_features"], config["preprocessing"], backend)
</source>
</class>

<class classid="12" nclones="2" nlines="10" similarity="70">
<source file="systems/ludwig-0.5rc2/tests/ludwig/utils/test_torch_utils.py" startline="109" endline="123" pcid="248">
def test_initialize_pytorch_with_horovod(mock_torch):
    mock_torch.cuda.is_available.return_value = True
    mock_torch.cuda.device_count.return_value = 4

    mock_hvd = Mock()
    mock_hvd.local_rank.return_value = 1
    mock_hvd.local_size.return_value = 4

    with clean_params():
        initialize_pytorch(horovod=mock_hvd)

    mock_torch.cuda.set_device.assert_called_with(1)
    assert "CUDA_VISIBLE_DEVICES" not in os.environ


</source>
<source file="systems/ludwig-0.5rc2/tests/ludwig/utils/test_torch_utils.py" startline="126" endline="141" pcid="249">
def test_initialize_pytorch_with_horovod_bad_local_rank(mock_torch, mock_warnings):
    """In this scenario, the local_size 5 is out of the bounds of the GPU indices."""
    mock_torch.cuda.is_available.return_value = True
    mock_torch.cuda.device_count.return_value = 4

    mock_hvd = Mock()
    mock_hvd.local_rank.return_value = 1
    mock_hvd.local_size.return_value = 5

    with clean_params():
        initialize_pytorch(horovod=mock_hvd)

    assert os.environ["CUDA_VISIBLE_DEVICES"] == ""
    mock_warnings.warn.assert_called()


</source>
</class>

<class classid="13" nclones="4" nlines="18" similarity="72">
<source file="systems/ludwig-0.5rc2/tests/ludwig/utils/test_strings_utils.py" startline="55" endline="76" pcid="286">
def test_create_vocabulary_chars():
    data = pd.DataFrame(["Hello, I'm a single sentence!", "And another sentence", "And the very very last one"])
    column = data[0]
    preprocessing_parameters = TextFeatureMixin.preprocessing_defaults()

    vocabulary_output = strings_utils.create_vocabulary(
        column,
        tokenizer_type="characters",
        num_most_frequent=preprocessing_parameters["char_most_common"],
        lowercase=preprocessing_parameters["lowercase"],
        unknown_symbol=preprocessing_parameters["unknown_symbol"],
        padding_symbol=preprocessing_parameters["padding_symbol"],
        pretrained_model_name_or_path=preprocessing_parameters["pretrained_model_name_or_path"],
    )

    assert len(vocabulary_output[0]) == 24
    assert vocabulary_output[0][strings_utils.SpecialSymbol.START.value] == strings_utils.START_SYMBOL
    assert vocabulary_output[0][strings_utils.SpecialSymbol.STOP.value] == strings_utils.STOP_SYMBOL
    assert vocabulary_output[0][strings_utils.SpecialSymbol.PADDING.value] == strings_utils.PADDING_SYMBOL
    assert vocabulary_output[0][strings_utils.SpecialSymbol.UNKNOWN.value] == strings_utils.UNKNOWN_SYMBOL


</source>
<source file="systems/ludwig-0.5rc2/tests/ludwig/utils/test_strings_utils.py" startline="77" endline="99" pcid="287">
def test_create_vocabulary_word():
    data = pd.DataFrame(["Hello, I'm a single sentence!", "And another sentence", "And the very very last one"])
    column = data[0]
    preprocessing_parameters = TextFeatureMixin.preprocessing_defaults()

    vocabulary_output = strings_utils.create_vocabulary(
        column,
        tokenizer_type=preprocessing_parameters["word_tokenizer"],
        num_most_frequent=preprocessing_parameters["word_most_common"],
        lowercase=preprocessing_parameters["lowercase"],
        vocab_file=preprocessing_parameters["word_vocab_file"],
        unknown_symbol=preprocessing_parameters["unknown_symbol"],
        padding_symbol=preprocessing_parameters["padding_symbol"],
        pretrained_model_name_or_path=preprocessing_parameters["pretrained_model_name_or_path"],
    )

    assert len(vocabulary_output[0]) == 19
    assert vocabulary_output[0][strings_utils.SpecialSymbol.UNKNOWN.value] == strings_utils.UNKNOWN_SYMBOL
    assert vocabulary_output[0][strings_utils.SpecialSymbol.STOP.value] == strings_utils.STOP_SYMBOL
    assert vocabulary_output[0][strings_utils.SpecialSymbol.PADDING.value] == strings_utils.PADDING_SYMBOL
    assert vocabulary_output[0][strings_utils.SpecialSymbol.UNKNOWN.value] == strings_utils.UNKNOWN_SYMBOL


</source>
<source file="systems/ludwig-0.5rc2/tests/ludwig/utils/test_strings_utils.py" startline="121" endline="138" pcid="289">
def test_create_vocabulary_from_hf():
    data = pd.DataFrame(["Hello, I'm a single sentence!", "And another sentence", "And the very very last one"])
    column = data[0]
    preprocessing_parameters = TextFeatureMixin.preprocessing_defaults()

    vocabulary_output = strings_utils.create_vocabulary(
        column,
        tokenizer_type="hf_tokenizer",
        num_most_frequent=preprocessing_parameters["char_most_common"],
        lowercase=preprocessing_parameters["lowercase"],
        unknown_symbol=preprocessing_parameters["unknown_symbol"],
        padding_symbol=preprocessing_parameters["padding_symbol"],
        pretrained_model_name_or_path="albert-base-v2",
    )

    assert len(vocabulary_output[0]) == 30000


</source>
<source file="systems/ludwig-0.5rc2/tests/ludwig/utils/test_strings_utils.py" startline="100" endline="120" pcid="288">
def test_create_vocabulary_no_special_symbols():
    data = pd.DataFrame(["Hello, I'm a single sentence!", "And another sentence", "And the very very last one"])
    column = data[0]
    preprocessing_parameters = TextFeatureMixin.preprocessing_defaults()

    vocabulary_output = strings_utils.create_vocabulary(
        column,
        tokenizer_type=preprocessing_parameters["word_tokenizer"],
        num_most_frequent=preprocessing_parameters["word_most_common"],
        lowercase=preprocessing_parameters["lowercase"],
        vocab_file=preprocessing_parameters["word_vocab_file"],
        unknown_symbol=preprocessing_parameters["unknown_symbol"],
        padding_symbol=preprocessing_parameters["padding_symbol"],
        pretrained_model_name_or_path=preprocessing_parameters["pretrained_model_name_or_path"],
        add_special_symbols=False,
    )

    assert len(vocabulary_output[0]) == 16
    assert vocabulary_output[0][strings_utils.SpecialSymbol.UNKNOWN.value] == strings_utils.UNKNOWN_SYMBOL


</source>
</class>

<class classid="14" nclones="3" nlines="20" similarity="78">
<source file="systems/ludwig-0.5rc2/tests/ludwig/utils/test_hyperopt_utils.py" startline="52" endline="77" pcid="291">
def test_grid_strategy(key):
    hyperopt_test_params = HYPEROPT_PARAMS[key]
    goal = hyperopt_test_params["goal"]
    grid_sampler_params = hyperopt_test_params["parameters"]

    grid_sampler = GridSampler(goal=goal, parameters=grid_sampler_params)

    actual_params_keys = grid_sampler.sample().keys()
    expected_params_keys = grid_sampler_params.keys()

    for sample in grid_sampler.samples:
        for param in actual_params_keys:
            value = sample[param]
            param_type = grid_sampler_params[param]["type"]
            if param_type == "int" or param_type == "float":
                low = grid_sampler_params[param]["low"]
                high = grid_sampler_params[param]["high"]
                assert value >= low and value <= high
            else:
                assert value in set(grid_sampler_params[param]["values"])

    assert actual_params_keys == expected_params_keys
    assert grid_sampler.search_space == hyperopt_test_params["expected_search_space"]
    assert len(grid_sampler.samples) == hyperopt_test_params["expected_len_grids"]


</source>
<source file="systems/ludwig-0.5rc2/tests/ludwig/utils/test_hyperopt_utils.py" startline="79" endline="104" pcid="292">
def test_random_sampler(key):
    hyperopt_test_params = HYPEROPT_PARAMS[key]
    goal = hyperopt_test_params["goal"]
    random_sampler_params = hyperopt_test_params["parameters"]
    num_samples = hyperopt_test_params["num_samples"]

    random_sampler = RandomSampler(goal=goal, parameters=random_sampler_params, num_samples=num_samples)

    actual_params_keys = random_sampler.sample().keys()
    expected_params_keys = random_sampler_params.keys()

    for sample in random_sampler.samples:
        for param in actual_params_keys:
            value = sample[param]
            param_type = random_sampler_params[param]["type"]
            if param_type == "int" or param_type == "float":
                low = random_sampler_params[param]["low"]
                high = random_sampler_params[param]["high"]
                assert value >= low and value <= high
            else:
                assert value in set(random_sampler_params[param]["values"])

    assert actual_params_keys == expected_params_keys
    assert len(random_sampler.samples) == num_samples


</source>
<source file="systems/ludwig-0.5rc2/tests/ludwig/utils/test_hyperopt_utils.py" startline="106" endline="133" pcid="293">
def test_pysot_sampler(key):
    hyperopt_test_params = HYPEROPT_PARAMS[key]
    goal = hyperopt_test_params["goal"]
    pysot_sampler_params = hyperopt_test_params["parameters"]
    num_samples = hyperopt_test_params["num_samples"]

    pysot_sampler = PySOTSampler(goal=goal, parameters=pysot_sampler_params, num_samples=num_samples)

    actual_params_keys = pysot_sampler.sample().keys()
    expected_params_keys = pysot_sampler_params.keys()

    pysot_sampler_samples = 1

    for _ in range(num_samples - 1):
        sample = pysot_sampler.sample()
        for param in actual_params_keys:
            value = sample[param]
            param_type = pysot_sampler_params[param]["type"]
            if param_type == "int" or param_type == "float":
                low = pysot_sampler_params[param]["low"]
                high = pysot_sampler_params[param]["high"]
                assert value >= low and value <= high
            else:
                assert value in set(pysot_sampler_params[param]["values"])
        pysot_sampler_samples += 1

    assert actual_params_keys == expected_params_keys
    assert pysot_sampler_samples == num_samples
</source>
</class>

<class classid="15" nclones="2" nlines="20" similarity="95">
<source file="systems/ludwig-0.5rc2/tests/ludwig/features/test_binary_feature.py" startline="32" endline="55" pcid="310">
def test_binary_output_feature():
    binary_output_feature = BinaryOutputFeature(
        {
            "name": "binary_feature",
            "type": "binary",
            "input_size": 1,
            "loss": {
                "positive_class_weight": 1,
                "robust_lambda": 0,
                "confidence_penalty": 0,
            },
        },
        {},
    ).to(DEVICE)
    combiner_outputs = {}
    combiner_outputs["combiner_output"] = torch.randn([BATCH_SIZE, BINARY_W_SIZE], dtype=torch.float32).to(DEVICE)

    binary_output = binary_output_feature(combiner_outputs, {})

    assert "last_hidden" in binary_output
    assert "logits" in binary_output
    assert binary_output["logits"].size() == torch.Size([BATCH_SIZE])


</source>
<source file="systems/ludwig-0.5rc2/tests/ludwig/features/test_binary_feature.py" startline="56" endline="77" pcid="311">
def test_binary_output_feature_without_positive_class_weight():
    binary_output_feature = BinaryOutputFeature(
        {
            "name": "binary_feature",
            "type": "binary",
            "input_size": 1,
            "loss": {
                "positive_class_weight": None,
                "robust_lambda": 0,
                "confidence_penalty": 0,
            },
        },
        {},
    ).to(DEVICE)
    combiner_outputs = {}
    combiner_outputs["combiner_output"] = torch.randn([BATCH_SIZE, BINARY_W_SIZE], dtype=torch.float32).to(DEVICE)

    binary_output = binary_output_feature(combiner_outputs, {})

    assert "last_hidden" in binary_output
    assert "logits" in binary_output
    assert binary_output["logits"].size() == torch.Size([BATCH_SIZE])
</source>
</class>

<class classid="16" nclones="2" nlines="13" similarity="75">
<source file="systems/ludwig-0.5rc2/tests/ludwig/features/test_timeseries_feature.py" startline="16" endline="29" pcid="312">
def timeseries_config():
    return {
        "name": "timeseries_12",
        "type": "timeseries",
        "max_len": MAX_LEN,
        "embedding_size": EMBEDDING_SIZE,
        "max_sequence_length": SEQ_SIZE,
        "output_size": 8,
        "state_size": 8,
        "num_filters": 8,
        "hidden_size": 8,
    }


</source>
<source file="systems/ludwig-0.5rc2/ludwig/features/image_feature.py" startline="80" endline="93" pcid="1351">
    def preprocessing_defaults():
        return {
            "missing_value_strategy": BACKFILL,
            "in_memory": True,
            "resize_method": "interpolate",
            "scaling": "pixel_normalization",
            "num_processes": 1,
            "infer_image_num_channels": True,
            "infer_image_dimensions": True,
            "infer_image_max_height": 256,
            "infer_image_max_width": 256,
            "infer_image_sample_size": 100,
        }

</source>
</class>

<class classid="17" nclones="4" nlines="13" similarity="85">
<source file="systems/ludwig-0.5rc2/tests/ludwig/modules/test_embedding_modules.py" startline="14" endline="28" pcid="359">
def test_embed(
    vocab: List[str],
    embedding_size: int,
    representation: str,
):
    embed = Embed(
        vocab=vocab,
        embedding_size=embedding_size,
        representation=representation,
    ).to(DEVICE)
    inputs = torch.randint(0, 2, size=(2, 1)).bool().to(DEVICE)
    outputs = embed(inputs)
    assert outputs.shape[1:] == embed.output_shape


</source>
<source file="systems/ludwig-0.5rc2/tests/ludwig/modules/test_embedding_modules.py" startline="64" endline="79" pcid="362">
def test_embed_sequence(
    vocab: List[str],
    embedding_size: int,
    representation: str,
):
    embed = EmbedSequence(
        vocab=vocab,
        embedding_size=embedding_size,
        max_sequence_length=10,
        representation=representation,
    ).to(DEVICE)
    inputs = torch.randint(0, 2, size=(2, 10)).to(DEVICE)
    outputs = embed(inputs)
    assert outputs.shape[1:] == embed.output_shape


</source>
<source file="systems/ludwig-0.5rc2/tests/ludwig/modules/test_embedding_modules.py" startline="32" endline="46" pcid="360">
def test_embed_set(
    vocab: List[str],
    embedding_size: int,
    representation: str,
):
    embed = EmbedSet(
        vocab=vocab,
        embedding_size=embedding_size,
        representation=representation,
    ).to(DEVICE)
    inputs = torch.randint(0, 2, size=(2, len(vocab))).bool().to(DEVICE)
    outputs = embed(inputs)
    assert outputs.shape[1:] == embed.output_shape


</source>
<source file="systems/ludwig-0.5rc2/tests/ludwig/modules/test_embedding_modules.py" startline="83" endline="96" pcid="363">
def test_token_and_position_embedding(
    vocab: List[str],
    embedding_size: int,
    representation: str,
):
    embed = TokenAndPositionEmbedding(
        vocab=vocab,
        embedding_size=embedding_size,
        max_sequence_length=10,
        representation=representation,
    ).to(DEVICE)
    inputs = torch.randint(0, 2, size=(2, 10)).to(DEVICE)
    outputs = embed(inputs)
    assert outputs.shape[1:] == embed.output_shape
</source>
</class>

<class classid="18" nclones="3" nlines="12" similarity="76">
<source file="systems/ludwig-0.5rc2/ludwig/decoders/registry.py" startline="9" endline="25" pcid="373">
def register_decoder(name: str, features: Union[str, List[str]], default=False):
    if isinstance(features, str):
        features = [features]

    def wrap(cls):
        for feature in features:
            feature_registry = decoder_registry.get(feature, {})
            feature_registry[name] = cls
            if default:
                for key in DEFAULT_KEYS:
                    feature_registry[key] = cls
            decoder_registry[feature] = feature_registry
        return cls

    return wrap


</source>
<source file="systems/ludwig-0.5rc2/ludwig/modules/metric_registry.py" startline="9" endline="23" pcid="1460">
def register_metric(name: str, features: Union[str, List[str]]):
    if isinstance(features, str):
        features = [features]

    def wrap(cls):
        for feature in features:
            feature_registry = metric_feature_registry.get(feature, {})
            feature_registry[name] = cls
            metric_feature_registry[feature] = feature_registry
        metric_registry[name] = cls
        return cls

    return wrap


</source>
<source file="systems/ludwig-0.5rc2/ludwig/encoders/registry.py" startline="9" endline="25" pcid="466">
def register_encoder(name: str, features: Union[str, List[str]], default=False):
    if isinstance(features, str):
        features = [features]

    def wrap(cls):
        for feature in features:
            feature_registry = encoder_registry.get(feature, {})
            feature_registry[name] = cls
            if default:
                for key in DEFAULT_KEYS:
                    feature_registry[key] = cls
            encoder_registry[feature] = feature_registry
        return cls

    return wrap


</source>
</class>

<class classid="19" nclones="14" nlines="67" similarity="70">
<source file="systems/ludwig-0.5rc2/ludwig/encoders/text_encoders.py" startline="41" endline="117" pcid="474">
    def __init__(
        self,
        max_sequence_length,
        use_pretrained: bool = True,
        pretrained_model_name_or_path: str = "albert-base-v2",
        trainable: bool = True,
        reduce_output: str = "cls_pooled",
        vocab_size: int = 30000,
        embedding_size: int = 128,
        hidden_size: int = 4096,
        num_hidden_layers: int = 12,
        num_hidden_groups: int = 1,
        num_attention_heads: int = 64,
        intermediate_size: int = 16384,
        inner_group_num: int = 1,
        hidden_act: str = "gelu_new",
        hidden_dropout_prob: float = 0,
        attention_probs_dropout_prob: float = 0,
        max_position_embeddings: int = 512,
        type_vocab_size: int = 2,
        initializer_range: float = 0.02,
        layer_norm_eps: float = 1e-12,
        classifier_dropout_prob: float = 0.1,
        position_embedding_type: str = "absolute",
        pad_token_id: int = 0,
        bos_token_id: int = 2,
        eos_token_id: int = 3,
        pretrained_kwargs: Dict = None,
        **kwargs
    ):
        super().__init__()
        try:
            from transformers import AlbertConfig, AlbertModel
        except ModuleNotFoundError:
            logger.error(
                " transformers is not installed. "
                "In order to install all text feature dependencies run "
                "pip install ludwig[text]"
            )
            sys.exit(-1)

        if use_pretrained:
            pretrained_kwargs = pretrained_kwargs or {}
            self.transformer = AlbertModel.from_pretrained(pretrained_model_name_or_path, **pretrained_kwargs)
        else:
            config = AlbertConfig(
                vocab_size=vocab_size,
                embedding_size=embedding_size,
                hidden_size=hidden_size,
                num_hidden_layers=num_hidden_layers,
                num_hidden_groups=num_hidden_groups,
                num_attention_heads=num_attention_heads,
                intermediate_size=intermediate_size,
                inner_group_num=inner_group_num,
                hidden_act=hidden_act,
                hidden_dropout_prob=hidden_dropout_prob,
                attention_probs_dropout_prob=attention_probs_dropout_prob,
                max_position_embeddings=max_position_embeddings,
                type_vocab_size=type_vocab_size,
                initializer_range=initializer_range,
                layer_norm_eps=layer_norm_eps,
                classifier_dropout_prob=classifier_dropout_prob,
                position_embedding_type=position_embedding_type,
                pad_token_id=pad_token_id,
                bos_token_id=bos_token_id,
                eos_token_id=eos_token_id,
            )
            self.transformer = AlbertModel(config)

        self.reduce_output = reduce_output
        if not self.reduce_output == "cls_pooled":
            self.reduce_sequence = SequenceReducer(reduce_mode=reduce_output)
        if trainable:
            self.transformer.train()
        self.transformer.resize_token_embeddings(vocab_size)
        self.max_sequence_length = max_sequence_length

</source>
<source file="systems/ludwig-0.5rc2/ludwig/encoders/text_encoders.py" startline="1370" endline="1438" pcid="534">
    def __init__(
        self,
        max_sequence_length: int,
        use_pretrained: bool = True,
        pretrained_model_name_or_path: str = "ctrl",
        reduce_output: str = "cls-pooled",
        trainable: bool = True,
        vocab_size: int = 30522,
        hidden_size: int = 768,
        num_hidden_layers: int = 12,
        num_attention_heads: int = 12,
        intermediate_size: int = 3072,
        hidden_act: Union[str, Callable] = "gelu",
        hidden_dropout_prob: float = 0.1,
        attention_probs_dropout_prob: float = 0.1,
        max_position_embeddings: int = 512,
        type_vocab_size: int = 2,
        initializer_range: float = 0.02,
        layer_norm_eps: float = 1e-12,
        pad_token_id: int = 0,
        gradient_checkpointing: bool = False,
        position_embedding_type: str = "absolute",
        classifier_dropout: float = None,
        pretrained_kwargs: Dict = None,
        **kwargs
    ):
        super().__init__()
        try:
            from transformers import CamembertConfig, CamembertModel
        except ModuleNotFoundError:
            logger.error(
                " transformers is not installed. "
                "In order to install all text feature dependencies run "
                "pip install ludwig[text]"
            )
            sys.exit(-1)

        if use_pretrained:
            pretrained_kwargs = pretrained_kwargs or {}
            self.transformer = CamembertModel.from_pretrained(pretrained_model_name_or_path, **pretrained_kwargs)
        else:
            config = CamembertConfig(
                vocab_size=vocab_size,
                hidden_size=hidden_size,
                num_hidden_layers=num_hidden_layers,
                num_attention_heads=num_attention_heads,
                intermediate_size=intermediate_size,
                hidden_act=hidden_act,
                hidden_dropout_prob=hidden_dropout_prob,
                attention_probs_dropout_prob=attention_probs_dropout_prob,
                max_position_embeddings=max_position_embeddings,
                type_vocab_size=type_vocab_size,
                initializer_range=initializer_range,
                layer_norm_eps=layer_norm_eps,
                pad_token_id=pad_token_id,
                gradient_checkpointing=gradient_checkpointing,
                position_embedding_type=position_embedding_type,
                classifier_dropout=classifier_dropout,
            )
            self.transformer = CamembertModel(config)

        if trainable:
            self.transformer.train()
        self.reduce_output = reduce_output
        if not self.reduce_output == "cls_pooled":
            self.reduce_sequence = SequenceReducer(reduce_mode=reduce_output)
        self.transformer.resize_token_embeddings(vocab_size)
        self.max_sequence_length = max_sequence_length

</source>
<source file="systems/ludwig-0.5rc2/ludwig/encoders/text_encoders.py" startline="166" endline="240" pcid="479">
    def __init__(
        self,
        max_sequence_length: int,
        use_pretrained: bool = True,
        pretrained_model_name_or_path: str = "google/mt5-base",
        trainable: bool = True,
        reduce_output: str = "cls_pooled",
        vocab_size: int = 250112,
        d_model: int = 512,
        d_kv: int = 64,
        d_ff: int = 1024,
        num_layers: int = 8,
        num_decoder_layers: int = None,
        num_heads: int = 6,
        relative_attention_num_buckets: int = 32,
        dropout_rate: float = 0.1,
        layer_norm_epsilon: float = 1e-06,
        initializer_factor: float = 1.0,
        feed_forward_proj: str = "gated-gelu",
        is_encoder_decoder: bool = True,
        use_cache: bool = True,
        tokenizer_class: str = "T5Tokenizer",
        tie_word_embeddings: bool = False,
        pad_token_id: int = 0,
        eos_token_id: int = 1,
        decoder_start_token_id: int = 0,
        pretrained_kwargs: Dict = None,
        **kwargs
    ):
        super().__init__()
        try:
            from transformers import MT5Config, MT5EncoderModel
        except ModuleNotFoundError:
            logger.error(
                " transformers is not installed. "
                "In order to install all text feature dependencies run "
                "pip install ludwig[text]"
            )
            sys.exit(-1)

        if use_pretrained:
            pretrained_kwargs = pretrained_kwargs or {}
            self.transformer = MT5EncoderModel.from_pretrained(pretrained_model_name_or_path, **pretrained_kwargs)
        else:
            config = MT5Config(
                vocab_size=vocab_size,
                d_model=d_model,
                d_kv=d_kv,
                d_ff=d_ff,
                num_layers=num_layers,
                num_decoder_layers=num_decoder_layers,
                num_heads=num_heads,
                relative_attention_num_buckets=relative_attention_num_buckets,
                dropout_rate=dropout_rate,
                layer_norm_epsilon=layer_norm_epsilon,
                initializer_factor=initializer_factor,
                feed_forward_proj=feed_forward_proj,
                is_encoder_decoder=is_encoder_decoder,
                use_cache=use_cache,
                tokenizer_class=tokenizer_class,
                tie_word_embeddings=tie_word_embeddings,
                pad_token_id=pad_token_id,
                eos_token_id=eos_token_id,
                decoder_start_token_id=decoder_start_token_id,
            )
            self.transformer = MT5EncoderModel(config)

        self.reduce_output = reduce_output
        if not self.reduce_output == "cls_pooled":
            self.reduce_sequence = SequenceReducer(reduce_mode=reduce_output)
        if trainable:
            self.transformer.train()
        self.transformer.resize_token_embeddings(vocab_size)
        self.max_sequence_length = max_sequence_length

</source>
<source file="systems/ludwig-0.5rc2/ludwig/encoders/text_encoders.py" startline="383" endline="451" pcid="489">
    def __init__(
        self,
        max_sequence_length: int,
        use_pretrained: bool = True,
        pretrained_model_name_or_path: str = "bert-base-uncased",
        trainable: bool = True,
        reduce_output: str = "cls_pooled",
        vocab_size: int = 30522,
        hidden_size: int = 768,
        num_hidden_layers: int = 12,
        num_attention_heads: int = 12,
        intermediate_size: int = 3072,
        hidden_act: Union[str, Callable] = "gelu",
        hidden_dropout_prob: float = 0.1,
        attention_probs_dropout_prob: float = 0.1,
        max_position_embeddings: int = 512,
        type_vocab_size: int = 2,
        initializer_range: float = 0.02,
        layer_norm_eps: float = 1e-12,
        pad_token_id: int = 0,
        gradient_checkpointing: bool = False,
        position_embedding_type: str = "absolute",
        classifier_dropout: float = None,
        pretrained_kwargs: Dict = None,
        **kwargs
    ):
        super().__init__()
        try:
            from transformers import BertConfig, BertModel
        except ModuleNotFoundError:
            logger.error(
                " transformers is not installed. "
                "In order to install all text feature dependencies run "
                "pip install ludwig[text]"
            )
            sys.exit(-1)

        if use_pretrained:
            pretrained_kwargs = pretrained_kwargs or {}
            self.transformer = BertModel.from_pretrained(pretrained_model_name_or_path, **pretrained_kwargs)
        else:
            config = BertConfig(
                vocab_size=vocab_size,
                hidden_size=hidden_size,
                num_hidden_layers=num_hidden_layers,
                num_attention_heads=num_attention_heads,
                intermediate_size=intermediate_size,
                hidden_act=hidden_act,
                hidden_dropout_prob=hidden_dropout_prob,
                attention_probs_dropout_prob=attention_probs_dropout_prob,
                max_position_embeddings=max_position_embeddings,
                type_vocab_size=type_vocab_size,
                initializer_range=initializer_range,
                layer_norm_eps=layer_norm_eps,
                pad_token_id=pad_token_id,
                gradient_checkpointing=gradient_checkpointing,
                position_embedding_type=position_embedding_type,
                classifier_dropout=classifier_dropout,
            )
            self.transformer = BertModel(config)

        self.reduce_output = reduce_output
        if not self.reduce_output == "cls_pooled":
            self.reduce_sequence = SequenceReducer(reduce_mode=reduce_output)
        if trainable:
            self.transformer.train()
        self.transformer.resize_token_embeddings(vocab_size)
        self.max_sequence_length = max_sequence_length

</source>
<source file="systems/ludwig-0.5rc2/ludwig/encoders/text_encoders.py" startline="1723" endline="1788" pcid="549">
    def __init__(
        self,
        max_sequence_length: int,
        use_pretrained: bool = True,
        pretrained_model_name_or_path: str = "google/electra-small-discriminator",
        reduce_output: str = "sum",
        trainable: bool = True,
        vocab_size: int = 30522,
        embedding_size: int = 128,
        hidden_size: int = 256,
        num_hidden_layers: int = 12,
        num_attention_heads: int = 4,
        intermediate_size: int = 1024,
        hidden_act: Union[str, Callable] = "gelu",
        hidden_dropout_prob: float = 0.1,
        attention_probs_dropout_prob: float = 0.1,
        max_position_embeddings: int = 512,
        type_vocab_size: int = 2,
        initializer_range: float = 0.02,
        layer_norm_eps: float = 1e-12,
        position_embedding_type: str = "absolute",
        classifier_dropout: Optional[float] = None,
        pretrained_kwargs: Dict = None,
        **kwargs
    ):
        super().__init__()
        try:
            from transformers import ElectraConfig, ElectraModel
        except ModuleNotFoundError:
            logger.error(
                " transformers is not installed. "
                "In order to install all text feature dependencies run "
                "pip install ludwig[text]"
            )
            sys.exit(-1)

        if use_pretrained:
            pretrained_kwargs = pretrained_kwargs or {}
            self.transformer = ElectraModel.from_pretrained(pretrained_model_name_or_path, **pretrained_kwargs)
        else:
            config = ElectraConfig(
                vocab_size=vocab_size,
                embedding_size=embedding_size,
                hidden_size=hidden_size,
                num_hidden_layers=num_hidden_layers,
                num_attention_heads=num_attention_heads,
                intermediate_size=intermediate_size,
                hidden_act=hidden_act,
                hidden_dropout_prob=hidden_dropout_prob,
                attention_probs_dropout_prob=attention_probs_dropout_prob,
                max_position_embeddings=max_position_embeddings,
                type_vocab_size=type_vocab_size,
                initializer_range=initializer_range,
                layer_norm_eps=layer_norm_eps,
                position_embedding_type=position_embedding_type,
                classifier_dropout=classifier_dropout,
            )
            self.transformer = ElectraModel(config)

        self.max_sequence_length = max_sequence_length
        self.reduce_output = reduce_output
        self.reduce_sequence = SequenceReducer(reduce_mode=reduce_output)
        if trainable:
            self.transformer.train()
        self.transformer.resize_token_embeddings(vocab_size)

</source>
<source file="systems/ludwig-0.5rc2/ludwig/encoders/text_encoders.py" startline="501" endline="592" pcid="494">
    def __init__(
        self,
        max_sequence_length: int,
        use_pretrained: bool = True,
        pretrained_model_name_or_path: str = "xlm-mlm-en-2048",
        trainable: bool = True,
        reduce_output: str = "cls_pooled",
        vocab_size: int = 30145,
        emb_dim: int = 2048,
        n_layers: int = 12,
        n_heads: int = 16,
        dropout: float = 0.1,
        attention_dropout: float = 0.1,
        gelu_activation: bool = True,
        sinusoidal_embeddings: bool = False,
        causal: bool = False,
        asm: bool = False,
        n_langs: int = 1,
        use_lang_emb: bool = True,
        max_position_embeddings: int = 512,
        embed_init_std: float = 2048**-0.5,
        layer_norm_eps: float = 1e-12,
        init_std: float = 0.02,
        bos_index: int = 0,
        eos_index: int = 1,
        pad_index: int = 2,
        unk_index: int = 3,
        mask_index: int = 5,
        is_encoder: bool = True,
        start_n_top: int = 5,
        end_n_top: int = 5,
        mask_token_id: int = 0,
        lang_id: int = 0,
        pad_token_id: int = 2,
        bos_token_id: int = 0,
        pretrained_kwargs: Dict = None,
        **kwargs
    ):
        super().__init__()
        try:
            from transformers import XLMConfig, XLMModel
        except ModuleNotFoundError:
            logger.error(
                " transformers is not installed. "
                "In order to install all text feature dependencies run "
                "pip install ludwig[text]"
            )
            sys.exit(-1)

        if use_pretrained:
            pretrained_kwargs = pretrained_kwargs or {}
            self.transformer = XLMModel.from_pretrained(pretrained_model_name_or_path, **pretrained_kwargs)
            if trainable:
                self.transformer.train()
        else:
            config = XLMConfig(
                vocab_size=vocab_size,
                emb_dim=emb_dim,
                n_layers=n_layers,
                n_heads=n_heads,
                dropout=dropout,
                attention_dropout=attention_dropout,
                gelu_activation=gelu_activation,
                sinusoidal_embeddings=sinusoidal_embeddings,
                causal=causal,
                asm=asm,
                n_langs=n_langs,
                use_lang_emb=use_lang_emb,
                max_position_embeddings=max_position_embeddings,
                embed_init_std=embed_init_std,
                layer_norm_eps=layer_norm_eps,
                init_std=init_std,
                bos_index=bos_index,
                eos_index=eos_index,
                pad_index=pad_index,
                unk_index=unk_index,
                mask_index=mask_index,
                is_encoder=is_encoder,
                start_n_top=start_n_top,
                end_n_top=end_n_top,
                mask_token_id=mask_token_id,
                lang_id=lang_id,
                pad_token_id=pad_token_id,
                bos_token_id=bos_token_id,
            )
            self.transformer = XLMModel(config)

        self.reduce_output = reduce_output
        self.reduce_sequence = SequenceReducer(reduce_mode=reduce_output)
        self.transformer.resize_token_embeddings(vocab_size)
        self.max_sequence_length = max_sequence_length

</source>
<source file="systems/ludwig-0.5rc2/ludwig/encoders/text_encoders.py" startline="639" endline="698" pcid="499">
    def __init__(
        self,
        max_sequence_length: int,
        reduce_output: str = "sum",
        use_pretrained: bool = True,
        pretrained_model_name_or_path: str = "openai-gpt",
        trainable: bool = True,
        vocab_size: int = 30522,
        n_positions: int = 40478,
        n_ctx: int = 512,
        n_embd: int = 768,
        n_layer: int = 12,
        n_head: int = 12,
        afn: str = "gelu",
        resid_pdrop: float = 0.1,
        embd_pdrop: float = 0.1,
        attn_pdrop: float = 0.1,
        layer_norm_epsilon: float = 1e-5,
        initializer_range: float = 0.02,
        pretrained_kwargs: Dict = None,
        **kwargs
    ):
        super().__init__()
        try:
            from transformers import OpenAIGPTConfig, OpenAIGPTModel
        except ModuleNotFoundError:
            logger.error(
                " transformers is not installed. "
                "In order to install all text feature dependencies run "
                "pip install ludwig[text]"
            )
            sys.exit(-1)

        if use_pretrained:
            pretrained_kwargs = pretrained_kwargs or {}
            self.transformer = OpenAIGPTModel.from_pretrained(pretrained_model_name_or_path, **pretrained_kwargs)
        else:
            config = OpenAIGPTConfig(
                vocab_size=vocab_size,
                n_positions=n_positions,
                n_ctx=n_ctx,
                n_embd=n_embd,
                n_layer=n_layer,
                n_head=n_head,
                afn=afn,
                resid_pdrop=resid_pdrop,
                embd_pdrop=embd_pdrop,
                attn_pdrop=attn_pdrop,
                layer_norm_epsilon=layer_norm_epsilon,
                initializer_range=initializer_range,
            )
            self.transformer = OpenAIGPTModel(config)

        self.reduce_output = reduce_output
        self.reduce_sequence = SequenceReducer(reduce_mode=reduce_output)
        if trainable:
            self.transformer.train()
        self.transformer.resize_token_embeddings(vocab_size)
        self.max_sequence_length = max_sequence_length

</source>
<source file="systems/ludwig-0.5rc2/ludwig/encoders/text_encoders.py" startline="923" endline="1008" pcid="514">
    def __init__(
        self,
        max_sequence_length: int,
        use_pretrained: bool = True,
        pretrained_model_name_or_path: str = "transfo-xl-wt103",
        reduce_output: str = "sum",
        trainable: bool = True,
        vocab_size: int = 267735,
        cutoffs: List[int] = [20000, 40000, 200000],
        d_model: int = 1024,
        d_embed: int = 1024,
        n_head: int = 16,
        d_head: int = 64,
        d_inner: int = 4096,
        div_val: int = 4,
        pre_lnorm: bool = False,
        n_layer: int = 18,
        mem_len: int = 1600,
        clamp_len: int = 1000,
        same_length: bool = True,
        proj_share_all_but_first: bool = True,
        attn_type: int = 0,
        sample_softmax: int = -1,
        adaptive: bool = True,
        dropout: float = 0.1,
        dropatt: float = 0.0,
        untie_r: bool = True,
        init: str = "normal",
        init_range: float = 0.01,
        proj_init_std: float = 0.01,
        init_std: float = 0.02,
        layer_norm_epsilon: float = 1e-5,
        eos_token_id: int = 0,
        pretrained_kwargs: Dict = None,
        **kwargs
    ):
        super().__init__()
        try:
            from transformers import TransfoXLConfig, TransfoXLModel
        except ModuleNotFoundError:
            logger.error(
                " transformers is not installed. "
                "In order to install all text feature dependencies run "
                "pip install ludwig[text]"
            )
            sys.exit(-1)

        if use_pretrained:
            pretrained_kwargs = pretrained_kwargs or {}
            self.transformer = TransfoXLModel.from_pretrained(pretrained_model_name_or_path, **pretrained_kwargs)
        else:
            config = TransfoXLConfig(
                vocab_size=vocab_size,
                cutoffs=cutoffs,
                d_model=d_model,
                d_embed=d_embed,
                n_head=n_head,
                d_head=d_head,
                d_inner=d_inner,
                div_val=div_val,
                pre_lnorm=pre_lnorm,
                n_layer=n_layer,
                mem_len=mem_len,
                clamp_len=clamp_len,
                same_length=same_length,
                proj_share_all_but_first=proj_share_all_but_first,
                attn_type=attn_type,
                sample_softmax=sample_softmax,
                adaptive=adaptive,
                dropout=dropout,
                dropatt=dropatt,
                untie_r=untie_r,
                init=init,
                init_range=init_range,
                proj_init_std=proj_init_std,
                init_std=init_std,
                layer_norm_epsilon=layer_norm_epsilon,
                eos_token_id=eos_token_id,
            )
            self.transformer = TransfoXLModel(config)
        self.reduce_output = reduce_output
        self.reduce_sequence = SequenceReducer(reduce_mode=reduce_output)
        if trainable:
            self.transformer.train()
        self.max_sequence_length = max_sequence_length

</source>
<source file="systems/ludwig-0.5rc2/ludwig/encoders/text_encoders.py" startline="1043" endline="1131" pcid="519">
    def __init__(
        self,
        max_sequence_length: int,
        use_pretrained: bool = True,
        pretrained_model_name_or_path: str = "xlnet-base-cased",
        reduce_output: str = "sum",
        trainable: bool = True,
        vocab_size: int = 32000,
        d_model: int = 1024,
        n_layer: int = 24,
        n_head: int = 16,
        d_inner: int = 4096,
        ff_activation: str = "gelu",
        untie_r: bool = True,
        attn_type: str = "bi",
        initializer_range: float = 0.02,
        layer_norm_eps: float = 1e-12,
        dropout: float = 0.1,
        mem_len: Optional[int] = 512,
        reuse_len: Optional[int] = None,
        use_mems_eval: bool = True,
        use_mems_train: bool = False,
        bi_data: bool = False,
        clamp_len: int = -1,
        same_length: bool = False,
        summary_type: str = "last",
        summary_use_proj: bool = True,
        summary_activation: str = "tanh",
        summary_last_dropout: float = 0.1,
        start_n_top: int = 5,
        end_n_top: int = 5,
        pad_token_id: int = 5,
        bos_token_id: int = 1,
        eos_token_id: int = 2,
        pretrained_kwargs: Dict = None,
        **kwargs
    ):
        super().__init__()
        try:
            from transformers import XLNetConfig, XLNetModel
        except ModuleNotFoundError:
            logger.error(
                " transformers is not installed. "
                "In order to install all text feature dependencies run "
                "pip install ludwig[text]"
            )
            sys.exit(-1)

        if use_pretrained:
            pretrained_kwargs = pretrained_kwargs or {}
            self.transformer = XLNetModel.from_pretrained(pretrained_model_name_or_path, **pretrained_kwargs)
        else:
            config = XLNetConfig(
                vocab_size=vocab_size,
                d_model=d_model,
                n_layer=n_layer,
                n_head=n_head,
                d_inner=d_inner,
                ff_activation=ff_activation,
                untie_r=untie_r,
                attn_type=attn_type,
                initializer_range=initializer_range,
                layer_norm_eps=layer_norm_eps,
                dropout=dropout,
                mem_len=mem_len,
                reuse_len=reuse_len,
                use_mems_eval=use_mems_eval,
                use_mems_train=use_mems_train,
                bi_data=bi_data,
                clamp_len=clamp_len,
                same_length=same_length,
                summary_type=summary_type,
                summary_use_proj=summary_use_proj,
                summary_activation=summary_activation,
                summary_last_dropout=summary_last_dropout,
                start_n_top=start_n_top,
                end_n_top=end_n_top,
                pad_token_id=pad_token_id,
                bos_token_id=bos_token_id,
                eos_token_id=eos_token_id,
            )
            self.transformer = XLNetModel(config)
        self.max_sequence_length = max_sequence_length
        self.reduce_output = reduce_output
        self.reduce_sequence = SequenceReducer(reduce_mode=reduce_output)
        if trainable:
            self.transformer.train()
        self.transformer.resize_token_embeddings(vocab_size)

</source>
<source file="systems/ludwig-0.5rc2/ludwig/encoders/text_encoders.py" startline="1591" endline="1678" pcid="544">
    def __init__(
        self,
        max_sequence_length: int,
        use_pretrained: bool,
        pretrained_model_name_or_path: str = "flaubert/flaubert_small_cased",
        reduce_output: str = "sum",
        trainable: bool = True,
        vocab_size: int = 30145,
        pre_norm: bool = False,
        layerdrop: float = 0.0,
        emb_dim: int = 2048,
        n_layer: int = 12,
        n_head: int = 16,
        dropout: float = 0.1,
        attention_dropout: float = 0.1,
        gelu_activation: bool = True,
        sinusoidal_embeddings: bool = False,
        causal: bool = False,
        asm: bool = False,
        n_langs: int = 1,
        use_lang_emb: bool = True,
        max_position_embeddings: int = 512,
        embed_init_std: float = 2048**-0.5,
        init_std: int = 50257,
        layer_norm_eps: float = 1e-12,
        bos_index: int = 0,
        eos_index: int = 1,
        pad_index: int = 2,
        unk_index: int = 3,
        mask_index: int = 5,
        is_encoder: bool = True,
        mask_token_id: int = 0,
        lang_id: int = 1,
        pretrained_kwargs: Dict = None,
        **kwargs
    ):
        super().__init__()
        try:
            from transformers import FlaubertConfig, FlaubertModel
        except ModuleNotFoundError:
            logger.error(
                " transformers is not installed. "
                "In order to install all text feature dependencies run "
                "pip install ludwig[text]"
            )
            sys.exit(-1)

        if use_pretrained:
            pretrained_kwargs = pretrained_kwargs or {}
            self.transformer = FlaubertModel.from_pretrained(pretrained_model_name_or_path, **pretrained_kwargs)
        else:
            config = FlaubertConfig(
                vocab_size=vocab_size,
                pre_norm=pre_norm,
                layerdrop=layerdrop,
                emb_dim=emb_dim,
                n_layer=n_layer,
                n_head=n_head,
                dropout=dropout,
                attention_dropout=dropout,
                gelu_activation=gelu_activation,
                sinusoidal_embeddings=sinusoidal_embeddings,
                causal=causal,
                asm=asm,
                n_langs=n_langs,
                use_lang_emb=use_lang_emb,
                max_position_embeddings=max_position_embeddings,
                embed_init_std=embed_init_std,
                init_std=init_std,
                layer_norm_eps=layer_norm_eps,
                bos_index=bos_index,
                eos_index=eos_index,
                pad_index=pad_index,
                unk_index=unk_index,
                mask_index=mask_index,
                is_encoder=is_encoder,
                mask_token_id=mask_token_id,
                lang_id=lang_id,
            )
            self.transformer = FlaubertModel(config)

        self.max_sequence_length = max_sequence_length
        self.reduce_output = reduce_output
        self.reduce_sequence = SequenceReducer(reduce_mode=reduce_output)
        if trainable:
            self.transformer.train()
        self.transformer.resize_token_embeddings(vocab_size)

</source>
<source file="systems/ludwig-0.5rc2/ludwig/encoders/text_encoders.py" startline="1271" endline="1331" pcid="529">
    def __init__(
        self,
        max_sequence_length: int,
        use_pretrained: bool = True,
        pretrained_model_name_or_path: str = "ctrl",
        reduce_output: str = "sum",
        trainable: bool = True,
        vocab_size: int = 246534,
        n_positions: int = 256,
        n_ctx: int = 256,
        n_embd: int = 1280,
        dff: int = 8192,
        n_layer: int = 48,
        n_head: int = 16,
        resid_pdrop: float = 0.1,
        embd_pdrop: float = 0.1,
        attn_pdrop: float = 0.1,
        layer_norm_epsilon: float = 1e-6,
        initializer_range: float = 0.02,
        pretrained_kwargs: Dict = None,
        **kwargs
    ):
        super().__init__()
        try:
            from transformers import CTRLConfig, CTRLModel
        except ModuleNotFoundError:
            logger.error(
                " transformers is not installed. "
                "In order to install all text feature dependencies run "
                "pip install ludwig[text]"
            )
            sys.exit(-1)

        if use_pretrained:
            pretrained_kwargs = pretrained_kwargs or {}
            self.transformer = CTRLModel.from_pretrained(pretrained_model_name_or_path, **pretrained_kwargs)
        else:
            config = CTRLConfig(
                vocab_size=vocab_size,
                n_positions=n_positions,
                n_ctx=n_ctx,
                n_embd=n_embd,
                dff=dff,
                n_layer=n_layer,
                n_head=n_head,
                resid_pdrop=resid_pdrop,
                embd_pdrop=embd_pdrop,
                attn_pdrop=attn_pdrop,
                layer_norm_epsilon=layer_norm_epsilon,
                initializer_range=initializer_range,
            )
            self.transformer = CTRLModel(config)

        self.vocab_size = vocab_size
        self.max_sequence_length = max_sequence_length
        if trainable:
            self.transformer.train()
        self.reduce_output = reduce_output
        self.reduce_sequence = SequenceReducer(reduce_mode=reduce_output)
        self.transformer.resize_token_embeddings(self.vocab_size)

</source>
<source file="systems/ludwig-0.5rc2/ludwig/encoders/text_encoders.py" startline="1171" endline="1232" pcid="524">
    def __init__(
        self,
        max_sequence_length: int,
        pretrained_model_name_or_path: str = "distilbert-base-uncased",
        reduce_output: str = "sum",
        trainable: bool = True,
        use_pretrained: bool = True,
        vocab_size: int = 30522,
        max_position_embeddings: int = 512,
        sinusoidal_pos_embds: bool = False,
        n_layers: int = 6,
        n_heads: int = 12,
        dim: int = 768,
        hidden_dim: int = 3072,
        dropout: float = 0.1,
        attention_dropout: float = 0.1,
        activation: Union[str, Callable] = "gelu",
        initializer_range: float = 0.02,
        qa_dropout: float = 0.1,
        seq_classif_dropout: float = 0.2,
        pretrained_kwargs: Dict = None,
        **kwargs
    ):
        super().__init__()
        try:
            from transformers import DistilBertConfig, DistilBertModel
        except ModuleNotFoundError:
            logger.error(
                " transformers is not installed. "
                "In order to install all text feature dependencies run "
                "pip install ludwig[text]"
            )
            sys.exit(-1)

        if use_pretrained:
            pretrained_kwargs = pretrained_kwargs or {}
            self.transformer = DistilBertModel.from_pretrained(pretrained_model_name_or_path, **pretrained_kwargs)
        else:
            config = DistilBertConfig(
                vocab_size=vocab_size,
                max_position_embeddings=max_position_embeddings,
                sinusoidal_pos_embds=sinusoidal_pos_embds,
                n_layers=n_layers,
                n_heads=n_heads,
                dim=dim,
                hidden_dim=hidden_dim,
                dropout=dropout,
                attention_dropout=attention_dropout,
                activation=activation,
                initializer_range=initializer_range,
                qa_dropout=qa_dropout,
                seq_classif_dropout=seq_classif_dropout,
            )
            self.transformer = DistilBertModel(config)

        if trainable:
            self.transformer.train()
        self.reduce_output = reduce_output
        self.max_sequence_length = max_sequence_length
        self.reduce_sequence = SequenceReducer(reduce_mode=reduce_output)
        self.transformer.resize_token_embeddings(vocab_size)

</source>
<source file="systems/ludwig-0.5rc2/ludwig/encoders/text_encoders.py" startline="1487" endline="1546" pcid="539">
    def __init__(
        self,
        max_sequence_length: int,
        use_pretrained: bool = True,
        pretrained_model_name_or_path: str = "t5-small",
        reduce_output: str = "sum",
        trainable: bool = True,
        vocab_size: int = 32128,
        d_model: int = 512,
        d_kv: int = 64,
        d_ff: int = 2048,
        num_layers: int = 6,
        num_decoder_layers: Optional[int] = None,
        num_heads: int = 8,
        relative_attention_num_buckets: int = 32,
        dropout_rate: float = 0.1,
        layer_norm_eps: float = 1e-6,
        initializer_factor: float = 1,
        feed_forward_proj: str = "relu",
        pretrained_kwargs: Dict = None,
        **kwargs
    ):
        super().__init__()
        try:
            from transformers import T5Config, T5Model
        except ModuleNotFoundError:
            logger.error(
                " transformers is not installed. "
                "In order to install all text feature dependencies run "
                "pip install ludwig[text]"
            )
            sys.exit(-1)

        if use_pretrained:
            pretrained_kwargs = pretrained_kwargs or {}
            self.transformer = T5Model.from_pretrained(pretrained_model_name_or_path, **pretrained_kwargs)
        else:
            config = T5Config(
                vocab_size=vocab_size,
                d_model=d_model,
                d_kv=d_kv,
                d_ff=d_ff,
                num_layers=num_layers,
                num_decoder_layers=num_decoder_layers,
                num_heads=num_heads,
                relative_attention_num_buckets=relative_attention_num_buckets,
                dropout_rate=dropout_rate,
                layer_norm_eps=layer_norm_eps,
                initializer_factor=initializer_factor,
                feed_forward_proj=feed_forward_proj,
            )
            self.transformer = T5Model(config)

        self.max_sequence_length = max_sequence_length
        self.reduce_output = reduce_output
        self.reduce_sequence = SequenceReducer(reduce_mode=reduce_output)
        if trainable:
            self.transformer.train()
        self.transformer.resize_token_embeddings(vocab_size)

</source>
<source file="systems/ludwig-0.5rc2/ludwig/encoders/text_encoders.py" startline="737" endline="800" pcid="504">
    def __init__(
        self,
        max_sequence_length: int,
        use_pretrained: bool = True,
        pretrained_model_name_or_path: str = "gpt2",
        reduce_output: str = "sum",
        trainable: bool = True,
        vocab_size: int = 50257,
        n_positions: int = 1024,
        n_ctx: int = 1024,
        n_embd: int = 768,
        n_layer: int = 12,
        n_head: int = 12,
        n_inner: Optional[int] = None,
        activation_function: str = "gelu",
        resid_pdrop: float = 0.1,
        embd_pdrop: float = 0.1,
        attn_pdrop: float = 0.1,
        layer_norm_epsilon: float = 1e-5,
        initializer_range: float = 0.02,
        scale_attn_weights: bool = True,
        pretrained_kwargs: Dict = None,
        **kwargs
    ):
        super().__init__()
        try:
            from transformers import GPT2Config, GPT2Model
        except ModuleNotFoundError:
            logger.error(
                " transformers is not installed. "
                "In order to install all text feature dependencies run "
                "pip install ludwig[text]"
            )
            sys.exit(-1)

        if use_pretrained:
            pretrained_kwargs = pretrained_kwargs or {}
            self.transformer = GPT2Model.from_pretrained(pretrained_model_name_or_path, **pretrained_kwargs)
        else:
            config = GPT2Config(
                vocab_size=vocab_size,
                n_positions=n_positions,
                n_ctx=n_ctx,
                n_embd=n_embd,
                n_layer=n_layer,
                n_head=n_head,
                n_inner=n_inner,
                activation_function=activation_function,
                resid_pdrop=resid_pdrop,
                embd_pdrop=embd_pdrop,
                attn_pdrop=attn_pdrop,
                layer_norm_epsilon=layer_norm_epsilon,
                initializer_range=initializer_range,
                scale_attn_weights=scale_attn_weights,
            )
            self.transformer = GPT2Model(config)

        if trainable:
            self.transformer.train()
        self.max_sequence_length = max_sequence_length
        self.reduce_output = reduce_output
        self.reduce_sequence = SequenceReducer(reduce_mode=reduce_output)
        self.transformer.resize_token_embeddings(vocab_size)

</source>
</class>

<class classid="20" nclones="17" nlines="13" similarity="71">
<source file="systems/ludwig-0.5rc2/ludwig/encoders/text_encoders.py" startline="118" endline="133" pcid="475">
    def forward(self, inputs: torch.Tensor, mask: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        if mask is not None:
            mask = mask.to(torch.int32)
        transformer_outputs = self.transformer(
            input_ids=inputs,
            attention_mask=mask,
            token_type_ids=torch.zeros_like(inputs),
        )
        if self.reduce_output == "cls_pooled":
            hidden = transformer_outputs[1]
        else:
            hidden = transformer_outputs[0][:, 1:-1, :]
            hidden = self.reduce_sequence(hidden, self.reduce_output)

        return {"encoder_output": hidden}

</source>
<source file="systems/ludwig-0.5rc2/ludwig/encoders/text_encoders.py" startline="1439" endline="1454" pcid="535">
    def forward(self, inputs: torch.Tensor, mask: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        if mask is not None:
            mask = mask.to(torch.int32)
        transformer_outputs = self.transformer(
            input_ids=inputs,
            attention_mask=mask,
            token_type_ids=torch.zeros_like(inputs),
        )
        if self.reduce_output == "cls_pooled":
            hidden = transformer_outputs[1]
        else:
            hidden = transformer_outputs[0][:, 1:-1, :]
            hidden = self.reduce_sequence(hidden, self.reduce_output)

        return {"encoder_output": hidden}

</source>
<source file="systems/ludwig-0.5rc2/ludwig/encoders/text_encoders.py" startline="882" endline="896" pcid="510">
    def forward(self, inputs: torch.Tensor, mask: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        if mask is not None:
            mask = mask.to(torch.int32)
        transformer_outputs = self.transformer(
            input_ids=inputs,
            attention_mask=mask,
            token_type_ids=torch.zeros_like(inputs),
        )
        if self.reduce_output == "cls_pooled":
            hidden = transformer_outputs[1]
        else:
            hidden = transformer_outputs[0][:, 1:-1, :]  # bos + [sent] + sep
            hidden = self.reduce_sequence(hidden, self.reduce_output)
        return {"encoder_output": hidden}

</source>
<source file="systems/ludwig-0.5rc2/ludwig/encoders/text_encoders.py" startline="452" endline="467" pcid="490">
    def forward(self, inputs: torch.Tensor, mask: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        if mask is not None:
            mask = mask.to(torch.int32)
        transformer_outputs = self.transformer(
            input_ids=inputs,
            attention_mask=mask,
            token_type_ids=torch.zeros_like(inputs),
        )
        if self.reduce_output == "cls_pooled":
            hidden = transformer_outputs[1]
        else:
            hidden = transformer_outputs[0][:, 1:-1, :]
            hidden = self.reduce_sequence(hidden, self.reduce_output)

        return {"encoder_output": hidden}

</source>
<source file="systems/ludwig-0.5rc2/ludwig/encoders/text_encoders.py" startline="1946" endline="1963" pcid="560">
    def forward(self, inputs: torch.Tensor, mask: Optional[torch.Tensor] = None):
        if mask is not None:
            mask = mask.to(torch.int32)
        transformer_outputs = self.transformer(
            input_ids=inputs,
            attention_mask=mask,
            token_type_ids=torch.zeros_like(inputs),
        )
        if self.reduce_output == "cls_pooled":
            # this works only if the user know that the specific model
            # they want to use has the same outputs of
            # the BERT base class call() function
            hidden = transformer_outputs["pooler_output"]
        else:
            hidden = transformer_outputs["last_hidden_state"]
            hidden = self.reduce_sequence(hidden, self.reduce_output)
        return {"encoder_output": hidden}

</source>
<source file="systems/ludwig-0.5rc2/ludwig/encoders/text_encoders.py" startline="1132" endline="1143" pcid="520">
    def forward(self, inputs: torch.Tensor, mask: torch.Tensor = None) -> Dict[str, torch.Tensor]:
        if mask is not None:
            mask = mask.to(torch.int32)
        transformer_outputs = self.transformer(
            input_ids=inputs,
            attention_mask=mask,
            token_type_ids=torch.zeros_like(inputs),
        )
        hidden = transformer_outputs[0]
        hidden = self.reduce_sequence(hidden, self.reduce_output)
        return {"encoder_output": hidden}

</source>
<source file="systems/ludwig-0.5rc2/ludwig/encoders/text_encoders.py" startline="1871" endline="1885" pcid="555">
    def forward(self, inputs: torch.Tensor, mask: Optional[torch.Tensor] = None):
        if mask is not None:
            mask = mask.to(torch.int32)
        transformer_outputs = self.transformer(
            input_ids=inputs,
            attention_mask=mask,
            token_type_ids=torch.zeros_like(inputs),
        )
        if self.reduce_output == "cls_pooled":
            hidden = transformer_outputs[1]
        else:
            hidden = transformer_outputs[0][:, 1:-1, :]  # bos + [sent] + sep
            hidden = self.reduce_sequence(hidden, self.reduce_output)
        return {"encoder_output": hidden}

</source>
<source file="systems/ludwig-0.5rc2/ludwig/encoders/text_encoders.py" startline="1233" endline="1243" pcid="525">
    def forward(self, inputs: torch.Tensor, mask: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        if mask is not None:
            mask = mask.to(torch.int32)
        transformer_outputs = self.transformer(
            input_ids=inputs,
            attention_mask=mask,
        )
        hidden = transformer_outputs[0][:, 1:-1, :]
        hidden = self.reduce_sequence(hidden, self.reduce_output)
        return {"encoder_output": hidden}

</source>
<source file="systems/ludwig-0.5rc2/ludwig/encoders/text_encoders.py" startline="1332" endline="1343" pcid="530">
    def forward(self, inputs: torch.Tensor, mask: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        if mask is not None:
            mask = mask.to(torch.int32)
        transformer_outputs = self.transformer(
            input_ids=inputs,
            attention_mask=mask,
            token_type_ids=torch.zeros_like(inputs),
        )
        hidden = transformer_outputs[0]
        hidden = self.reduce_sequence(hidden, self.reduce_output)
        return {"encoder_output": hidden}

</source>
<source file="systems/ludwig-0.5rc2/ludwig/encoders/text_encoders.py" startline="1547" endline="1558" pcid="540">
    def forward(self, inputs: torch.Tensor, mask: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        if mask is not None:
            mask = mask.to(torch.int32)
        transformer_outputs = self.transformer(
            inputs,
            decoder_input_ids=inputs,
            attention_mask=mask,
        )
        hidden = transformer_outputs[0][:, 0:-1, :]  # [eos token]
        hidden = self.reduce_sequence(hidden, self.reduce_output)
        return {"encoder_output": hidden}

</source>
<source file="systems/ludwig-0.5rc2/ludwig/encoders/text_encoders.py" startline="1679" endline="1690" pcid="545">
    def forward(self, inputs: torch.Tensor, mask: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        if mask is not None:
            mask = mask.to(torch.int32)
        transformer_outputs = self.transformer(
            input_ids=inputs,
            attention_mask=mask,
            token_type_ids=torch.zeros_like(inputs),
        )
        hidden = transformer_outputs[0][:, 1:-1, :]
        hidden = self.reduce_sequence(hidden, self.reduce_output)
        return {"encoder_output": hidden}

</source>
<source file="systems/ludwig-0.5rc2/ludwig/encoders/text_encoders.py" startline="1789" endline="1800" pcid="550">
    def forward(self, inputs: torch.Tensor, mask: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        if mask is not None:
            mask = mask.to(torch.int32)
        transformer_outputs = self.transformer(
            input_ids=inputs,
            attention_mask=mask,
            token_type_ids=torch.zeros_like(inputs),
        )
        hidden = transformer_outputs[0][:, 1:-1, :]
        hidden = self.reduce_sequence(hidden, self.reduce_output)
        return {"encoder_output": hidden}

</source>
<source file="systems/ludwig-0.5rc2/ludwig/encoders/text_encoders.py" startline="334" endline="349" pcid="485">
    def forward(self, inputs: torch.Tensor, mask: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        if mask is not None:
            mask = mask.to(torch.int32)
        transformer_outputs = self.transformer(
            input_ids=inputs,
            attention_mask=mask,
            token_type_ids=torch.zeros_like(inputs),
        )
        if self.reduce_output == "cls_pooled":
            hidden = transformer_outputs[1]
        else:
            hidden = transformer_outputs[0][:, 1:-1, :]
            hidden = self.reduce_sequence(hidden, self.reduce_output)

        return {"encoder_output": hidden}

</source>
<source file="systems/ludwig-0.5rc2/ludwig/encoders/text_encoders.py" startline="699" endline="710" pcid="500">
    def forward(self, inputs: torch.Tensor, mask: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        if mask is not None:
            mask = mask.to(torch.int32)
        transformer_outputs = self.transformer(
            input_ids=inputs,
            attention_mask=mask,
            token_type_ids=torch.zeros_like(inputs),
        )
        hidden = transformer_outputs[0]
        hidden = self.reduce_sequence(hidden, self.reduce_output)
        return {"encoder_output": hidden}

</source>
<source file="systems/ludwig-0.5rc2/ludwig/encoders/text_encoders.py" startline="241" endline="255" pcid="480">
    def forward(self, inputs: torch.Tensor, mask: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        if mask is not None:
            mask = mask.to(torch.int32)
        transformer_outputs = self.transformer(
            input_ids=inputs,
            attention_mask=mask,
        )
        if self.reduce_output == "cls_pooled":
            hidden = transformer_outputs[1]
        else:
            hidden = transformer_outputs[0][:, 1:-1, :]
            hidden = self.reduce_sequence(hidden, self.reduce_output)

        return {"encoder_output": hidden}

</source>
<source file="systems/ludwig-0.5rc2/ludwig/encoders/text_encoders.py" startline="801" endline="812" pcid="505">
    def forward(self, inputs: torch.Tensor, mask: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        if mask is not None:
            mask = mask.to(torch.int32)
        transformer_outputs = self.transformer(
            input_ids=inputs,
            attention_mask=mask,
            token_type_ids=torch.zeros_like(inputs),
        )
        hidden = transformer_outputs[0]
        hidden = self.reduce_sequence(hidden, self.reduce_output)
        return {"encoder_output": hidden}

</source>
<source file="systems/ludwig-0.5rc2/ludwig/encoders/text_encoders.py" startline="593" endline="605" pcid="495">
    def forward(self, inputs: torch.Tensor, mask: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:

        if mask is not None:
            mask = mask.to(torch.int32)
        transformer_outputs = self.transformer(
            input_ids=inputs,
            attention_mask=mask,
            token_type_ids=torch.zeros_like(inputs),
        )
        hidden = transformer_outputs[0]
        hidden = self.reduce_sequence(hidden, self.reduce_output)
        return {"encoder_output": hidden}

</source>
</class>

<class classid="21" nclones="2" nlines="39" similarity="84">
<source file="systems/ludwig-0.5rc2/ludwig/encoders/text_encoders.py" startline="288" endline="333" pcid="484">
    def __init__(
        self,
        max_sequence_length: int,
        use_pretrained: bool = True,
        pretrained_model_name_or_path: str = "xlm-roberta-base",
        reduce_output: str = "cls_pooled",
        trainable: bool = True,
        vocab_size: int = None,
        pad_token_id: int = 1,
        bos_token_id: int = 0,
        eos_token_id: int = 2,
        add_pooling_layer: bool = True,
        pretrained_kwargs: Dict = None,
        **kwargs
    ):
        super().__init__()
        try:
            from transformers import XLMRobertaConfig, XLMRobertaModel
        except ModuleNotFoundError:
            logger.error(
                " transformers is not installed. "
                "In order to install all text feature dependencies run "
                "pip install ludwig[text]"
            )
            sys.exit(-1)

        if use_pretrained:
            pretrained_kwargs = pretrained_kwargs or {}
            self.transformer = XLMRobertaModel.from_pretrained(pretrained_model_name_or_path, **pretrained_kwargs)
        else:
            config = XLMRobertaConfig(
                pad_token_id=pad_token_id,
                bos_token_id=bos_token_id,
                eos_token_id=eos_token_id,
            )

            self.transformer = XLMRobertaModel(config, add_pooling_layer)

        self.reduce_output = reduce_output
        if not self.reduce_output == "cls_pooled":
            self.reduce_sequence = SequenceReducer(reduce_mode=reduce_output)
        if trainable:
            self.transformer.train()
        self.transformer.resize_token_embeddings(vocab_size)
        self.max_sequence_length = max_sequence_length

</source>
<source file="systems/ludwig-0.5rc2/ludwig/encoders/text_encoders.py" startline="839" endline="881" pcid="509">
    def __init__(
        self,
        max_sequence_length,
        use_pretrained: bool = True,
        pretrained_model_name_or_path: str = "roberta-base",
        reduce_output: str = "cls_pooled",
        trainable: bool = True,
        vocab_size: int = None,
        pad_token_id: int = 1,
        bos_token_id: int = 0,
        eos_token_id: int = 2,
        pretrained_kwargs: Dict = None,
        **kwargs
    ):
        super().__init__()
        try:
            from transformers import RobertaConfig, RobertaModel
        except ModuleNotFoundError:
            logger.error(
                " transformers is not installed. "
                "In order to install all text feature dependencies run "
                "pip install ludwig[text]"
            )
            sys.exit(-1)

        if use_pretrained:
            pretrained_kwargs = pretrained_kwargs or {}
            self.transformer = RobertaModel.from_pretrained(pretrained_model_name_or_path, **pretrained_kwargs)
        else:
            config = RobertaConfig(
                pad_token_id=pad_token_id,
                bos_token_id=bos_token_id,
                eos_token_id=eos_token_id,
            )
            self.transformer = RobertaModel(config)
        if trainable:
            self.transformer.train()
        self.reduce_output = reduce_output
        if not self.reduce_output == "cls_pooled":
            self.reduce_sequence = SequenceReducer(reduce_mode=reduce_output)
        self.transformer.trainable = trainable
        self.transformer.resize_token_embeddings(vocab_size)

</source>
</class>

<class classid="22" nclones="4" nlines="25" similarity="70">
<source file="systems/ludwig-0.5rc2/ludwig/data/dataset_synthesizer.py" startline="464" endline="524" pcid="677">
def cli(sys_argv):
    parser = argparse.ArgumentParser(
        description="This script generates a synthetic dataset.",
        prog="ludwig synthesize_dataset",
        usage="%(prog)s [options]",
    )
    parser.add_argument("-od", "--output_path", type=str, help="output CSV file path")
    parser.add_argument("-d", "--dataset_size", help="size of the dataset", type=int, default=100)
    parser.add_argument(
        "-f",
        "--features",
        default="[\
          {name: text_1, type: text, vocab_size: 20, max_len: 20}, \
          {name: text_2, type: text, vocab_size: 20, max_len: 20}, \
          {name: category_1, type: category, vocab_size: 10}, \
          {name: category_2, type: category, vocab_size: 15}, \
          {name: number_1, type: number}, \
          {name: number_2, type: number}, \
          {name: binary_1, type: binary}, \
          {name: binary_2, type: binary}, \
          {name: set_1, type: set, vocab_size: 20, max_len: 20}, \
          {name: set_2, type: set, vocab_size: 20, max_len: 20}, \
          {name: bag_1, type: bag, vocab_size: 20, max_len: 10}, \
          {name: bag_2, type: bag, vocab_size: 20, max_len: 10}, \
          {name: sequence_1, type: sequence, vocab_size: 20, max_len: 20}, \
          {name: sequence_2, type: sequence, vocab_size: 20, max_len: 20}, \
          {name: timeseries_1, type: timeseries, max_len: 20}, \
          {name: timeseries_2, type: timeseries, max_len: 20}, \
          {name: date_1, type: date}, \
          {name: date_2, type: date}, \
          {name: h3_1, type: h3}, \
          {name: h3_2, type: h3}, \
          {name: vector_1, type: vector}, \
          {name: vector_2, type: vector}, \
        ]",
        type=yaml.safe_load,
        help="list of features to generate in YAML format. "
        "Provide a list containing one dictionary for each feature, "
        "each dictionary must include a name, a type "
        "and can include some generation parameters depending on the type",
    )
    add_contrib_callback_args(parser)
    args = parser.parse_args(sys_argv)

    args.callbacks = args.callbacks or []
    for callback in args.callbacks:
        callback.on_cmdline("synthesize_dataset", *sys_argv)

    # No log level parameter this is placeholder if we add at later date
    # args.logging_level = logging_level_registry[args.logging_level]
    # logging.getLogger('ludwig').setLevel(
    #     args.logging_level
    # )
    # global logger
    # logger = logging.getLogger('ludwig.data.dataset_synthesizer')

    print_ludwig("Synthesize Dataset", LUDWIG_VERSION)

    cli_synthesize_dataset(**vars(args))


</source>
<source file="systems/ludwig-0.5rc2/ludwig/export.py" startline="147" endline="192" pcid="1423">
def cli_export_neuropod(sys_argv):
    parser = argparse.ArgumentParser(
        description="This script loads a pretrained model " "and saves it as a Neuropod.",
        prog="ludwig export_neuropod",
        usage="%(prog)s [options]",
    )

    # ----------------
    # Model parameters
    # ----------------
    parser.add_argument("-m", "--model_path", help="model to load", required=True)
    parser.add_argument("-mn", "--model_name", help="model name", default="neuropod")

    # -----------------
    # Output parameters
    # -----------------
    parser.add_argument("-od", "--output_path", type=str, help="path where to save the export model", required=True)

    # ------------------
    # Runtime parameters
    # ------------------
    parser.add_argument(
        "-l",
        "--logging_level",
        default="info",
        help="the level of logging to use",
        choices=["critical", "error", "warning", "info", "debug", "notset"],
    )

    add_contrib_callback_args(parser)
    args = parser.parse_args(sys_argv)

    args.callbacks = args.callbacks or []
    for callback in args.callbacks:
        callback.on_cmdline("export_neuropod", *sys_argv)

    args.logging_level = logging_level_registry[args.logging_level]
    logging.getLogger("ludwig").setLevel(args.logging_level)
    global logger
    logger = logging.getLogger("ludwig.export")

    print_ludwig("Export Neuropod", LUDWIG_VERSION)

    export_neuropod(**vars(args))


</source>
<source file="systems/ludwig-0.5rc2/ludwig/export.py" startline="102" endline="146" pcid="1422">
def cli_export_torchscript(sys_argv):
    parser = argparse.ArgumentParser(
        description="This script loads a pretrained model " "and saves it as torchscript.",
        prog="ludwig export_torchscript",
        usage="%(prog)s [options]",
    )

    # ----------------
    # Model parameters
    # ----------------
    parser.add_argument("-m", "--model_path", help="model to load", required=True)

    # -----------------
    # Output parameters
    # -----------------
    parser.add_argument("-od", "--output_path", type=str, help="path where to save the export model", required=True)

    # ------------------
    # Runtime parameters
    # ------------------
    parser.add_argument(
        "-l",
        "--logging_level",
        default="info",
        help="the level of logging to use",
        choices=["critical", "error", "warning", "info", "debug", "notset"],
    )

    add_contrib_callback_args(parser)
    args = parser.parse_args(sys_argv)

    args.callbacks = args.callbacks or []
    for callback in args.callbacks:
        callback.on_cmdline("export_torchscript", *sys_argv)

    args.logging_level = logging_level_registry[args.logging_level]
    logging.getLogger("ludwig").setLevel(args.logging_level)
    global logger
    logger = logging.getLogger("ludwig.export")

    print_ludwig("Export Torchscript", LUDWIG_VERSION)

    export_torchscript(**vars(args))


</source>
<source file="systems/ludwig-0.5rc2/ludwig/export.py" startline="193" endline="240" pcid="1424">
def cli_export_mlflow(sys_argv):
    parser = argparse.ArgumentParser(
        description="This script loads a pretrained model " "and saves it as an MLFlow model.",
        prog="ludwig export_mlflow",
        usage="%(prog)s [options]",
    )

    # ----------------
    # Model parameters
    # ----------------
    parser.add_argument("-m", "--model_path", help="model to load", required=True)
    parser.add_argument(
        "-mn", "--registered_model_name", help="model name to upload to in MLflow model registry", default="mlflow"
    )

    # -----------------
    # Output parameters
    # -----------------
    parser.add_argument("-od", "--output_path", type=str, help="path where to save the exported model", required=True)

    # ------------------
    # Runtime parameters
    # ------------------
    parser.add_argument(
        "-l",
        "--logging_level",
        default="info",
        help="the level of logging to use",
        choices=["critical", "error", "warning", "info", "debug", "notset"],
    )

    add_contrib_callback_args(parser)
    args = parser.parse_args(sys_argv)

    args.callbacks = args.callbacks or []
    for callback in args.callbacks:
        callback.on_cmdline("export_mlflow", *sys_argv)

    args.logging_level = logging_level_registry[args.logging_level]
    logging.getLogger("ludwig").setLevel(args.logging_level)
    global logger
    logger = logging.getLogger("ludwig.export")

    print_ludwig("Export MLFlow", LUDWIG_VERSION)

    export_mlflow(**vars(args))


</source>
</class>

<class classid="23" nclones="5" nlines="30" similarity="70">
<source file="systems/ludwig-0.5rc2/ludwig/utils/visualization_utils.py" startline="796" endline="843" pcid="814">
def threshold_vs_metric_plot(
    thresholds,
    scores,
    algorithm_names=None,
    title=None,
    filename=None,
    callbacks=None,
):
    sns.set_style("whitegrid")

    colors = plt.get_cmap("tab10").colors

    # y_ticks_minor = np.linspace(0.0, 1.0, num=21)
    # y_ticks_major = np.linspace(0.0, 1.0, num=11)
    # y_ticks_major_labels = ['{:3.0f}%'.format(y * 100) for y in y_ticks_major]

    fig, ax1 = plt.subplots()

    if title is not None:
        ax1.set_title(title)

    ax1.grid(which="both")
    ax1.grid(which="minor", alpha=0.5)
    ax1.grid(which="major", alpha=0.75)
    ax1.set_xticks([x for idx, x in enumerate(thresholds) if idx % 2 == 0])
    ax1.set_xticks(thresholds, minor=True)

    # ax1.set_xlim(0, 1)
    ax1.set_xlabel("confidence threshold")

    # ax1.set_ylim(0, 1)
    # ax1.set_yticks(y_ticks_major)
    # ax1.set_yticklabels(y_ticks_major_labels)
    # ax1.set_yticks(y_ticks_minor, minor=True)

    for i in range(len(scores)):
        algorithm_name = algorithm_names[i] + " " if algorithm_names is not None and i < len(algorithm_names) else ""
        ax1.plot(thresholds, scores[i], label=algorithm_name, color=colors[i], linewidth=3, marker="o")

    ax1.legend(frameon=True)
    plt.tight_layout()
    visualize_callbacks(callbacks, plt.gcf())
    if filename:
        plt.savefig(filename)
    else:
        plt.show()


</source>
<source file="systems/ludwig-0.5rc2/ludwig/utils/visualization_utils.py" startline="1130" endline="1171" pcid="822">
def plot_distributions(
    distributions,
    labels=None,
    title=None,
    filename=None,
    callbacks=None,
):
    sns.set_style("whitegrid")

    colors = plt.get_cmap("tab10").colors

    fig, ax1 = plt.subplots()

    if title is not None:
        ax1.set_title(title)

    ax1.grid(which="both")
    ax1.grid(which="minor", alpha=0.5)
    ax1.grid(which="major", alpha=0.75)

    ax1.set_xlabel("class")

    ax1.set_ylabel("p")
    ax1.tick_params("y")

    for i, distribution in enumerate(distributions):
        ax1.plot(
            distribution,
            color=colors[i],
            alpha=0.6,
            label=labels[i] if labels is not None and i < len(labels) else f"Distribution {i}",
        )

    ax1.legend(frameon=True)
    fig.tight_layout()
    visualize_callbacks(callbacks, plt.gcf())
    if filename:
        plt.savefig(filename)
    else:
        plt.show()


</source>
<source file="systems/ludwig-0.5rc2/ludwig/utils/visualization_utils.py" startline="1172" endline="1206" pcid="823">
def plot_distributions_difference(
    distribution,
    labels=None,
    title=None,
    filename=None,
    callbacks=None,
):
    sns.set_style("whitegrid")

    colors = plt.get_cmap("tab10").colors

    fig, ax1 = plt.subplots()

    if title is not None:
        ax1.set_title(title)

    ax1.grid(which="both")
    ax1.grid(which="minor", alpha=0.5)
    ax1.grid(which="major", alpha=0.75)

    ax1.set_xlabel("class")

    ax1.set_ylabel("p")
    ax1.tick_params("y")

    ax1.plot(distribution, color=colors[0])

    fig.tight_layout()
    visualize_callbacks(callbacks, plt.gcf())
    if filename:
        plt.savefig(filename)
    else:
        plt.show()


</source>
<source file="systems/ludwig-0.5rc2/ludwig/utils/visualization_utils.py" startline="844" endline="892" pcid="815">
def roc_curves(
    fpr_tprs,
    algorithm_names=None,
    title=None,
    graded_color=False,
    filename=None,
    callbacks=None,
):
    sns.set_style("whitegrid")

    colors = plt.get_cmap("tab10").colors
    colormap = plt.get_cmap("RdYlGn")

    y_ticks_minor = np.linspace(0.0, 1.0, num=21)
    y_ticks_major = np.linspace(0.0, 1.0, num=11)

    fig, ax = plt.subplots()

    if title is not None:
        ax.set_title(title)

    ax.grid(which="both")
    ax.grid(which="minor", alpha=0.5)
    ax.grid(which="major", alpha=0.75)

    ax.set_xlim(0, 1)
    ax.set_xlabel("False positive rate")

    ax.set_ylim(0, 1)
    ax.set_yticks(y_ticks_major)
    ax.set_yticks(y_ticks_minor, minor=True)
    ax.set_ylabel("True positive rate")

    plt.plot([0, 1], [0, 1], color="black", linewidth=3, linestyle="--")

    for i in range(len(fpr_tprs)):
        algorithm_name = algorithm_names[i] + " " if algorithm_names is not None and i < len(algorithm_names) else ""
        color = colormap(i / len(fpr_tprs)) if graded_color else colors[i]
        ax.plot(fpr_tprs[i][0], fpr_tprs[i][1], label=algorithm_name, color=color, linewidth=3)

    ax.legend(frameon=True)
    plt.tight_layout()
    visualize_callbacks(callbacks, plt.gcf())
    if filename:
        plt.savefig(filename)
    else:
        plt.show()


</source>
<source file="systems/ludwig-0.5rc2/ludwig/utils/visualization_utils.py" startline="953" endline="989" pcid="817">
def brier_plot(
    brier_scores,
    algorithm_names=None,
    title=None,
    filename=None,
    callbacks=None,
):
    sns.set_style("whitegrid")

    if title is not None:
        plt.title(title)

    colors = plt.get_cmap("tab10").colors

    plt.grid(which="both")
    plt.grid(which="minor", alpha=0.5)
    plt.grid(which="major", alpha=0.75)
    plt.xlabel("class")
    plt.ylabel("brier")

    for i in range(brier_scores.shape[1]):
        plt.plot(
            brier_scores[:, i],
            label=algorithm_names[i] + " " if algorithm_names is not None and i < len(algorithm_names) else "",
            color=colors[i],
            linewidth=3,
        )

    plt.legend()
    plt.tight_layout()
    visualize_callbacks(callbacks, plt.gcf())
    if filename:
        plt.savefig(filename)
    else:
        plt.show()


</source>
</class>

<class classid="24" nclones="2" nlines="16" similarity="82">
<source file="systems/ludwig-0.5rc2/ludwig/utils/visualization_utils.py" startline="1321" endline="1339" pcid="826">
def hyperopt_int_plot(hyperopt_results_df, hp_name, metric, title, filename, log_scale_x=False, log_scale_y=True):
    sns.set_style("whitegrid")
    plt.figure()
    seaborn_figure = sns.scatterplot(x=hp_name, y=metric, data=hyperopt_results_df)
    seaborn_figure.set_title(title)
    if log_scale_x:
        seaborn_figure.set(xscale="log")
    if log_scale_y:
        seaborn_figure.set(yscale="log")
    seaborn_figure.xaxis.set_major_locator(ticker.MaxNLocator(integer=True))
    seaborn_figure.xaxis.set_major_formatter(ticker.ScalarFormatter())
    seaborn_figure.xaxis.set_minor_formatter(ticker.NullFormatter())
    seaborn_figure.figure.tight_layout()
    if filename:
        seaborn_figure.figure.savefig(filename)
    else:
        seaborn_figure.figure.show()


</source>
<source file="systems/ludwig-0.5rc2/ludwig/utils/visualization_utils.py" startline="1340" endline="1356" pcid="827">
def hyperopt_float_plot(hyperopt_results_df, hp_name, metric, title, filename, log_scale_x=False, log_scale_y=True):
    sns.set_style("whitegrid")
    plt.figure()
    seaborn_figure = sns.scatterplot(x=hp_name, y=metric, data=hyperopt_results_df)
    seaborn_figure.set_title(title)
    seaborn_figure.set(ylabel=metric)
    if log_scale_x:
        seaborn_figure.set(xscale="log")
    if log_scale_y:
        seaborn_figure.set(yscale="log")
    seaborn_figure.figure.tight_layout()
    if filename:
        seaborn_figure.figure.savefig(filename)
    else:
        seaborn_figure.figure.show()


</source>
</class>

<class classid="25" nclones="2" nlines="29" similarity="74">
<source file="systems/ludwig-0.5rc2/ludwig/datasets/mushroom_edibility/__init__.py" startline="44" endline="74" pcid="1105">
    def process_downloaded_dataset(self):
        super().process_downloaded_dataset(header=None)
        processed_df = pd.read_csv(os.path.join(self.processed_dataset_path, self.csv_filename))
        columns = [
            "class",
            "cap-shape",
            "cap-surface",
            "cap-color",
            "bruises?",
            "odor",
            "gill-attachment",
            "gill-spacing",
            "gill-size",
            "gill-color",
            "stalk-shape",
            "stalk-root",
            "stalk-surface-above-ring",
            "stalk-surface-below-ring",
            "stalk-color-above-ring",
            "stalk-color-below-ring",
            "veil-type",
            "veil-color",
            "ring-number",
            "ring-type",
            "spore-print-color",
            "population",
            "habitat",
            "split",
        ]
        processed_df.columns = columns
        processed_df.to_csv(os.path.join(self.processed_dataset_path, self.csv_filename), index=False)
</source>
<source file="systems/ludwig-0.5rc2/ludwig/datasets/naval/__init__.py" startline="44" endline="71" pcid="1193">
    def process_downloaded_dataset(self):
        df = pd.read_csv(os.path.join(self.raw_dataset_path, "UCI CBM Dataset", "data.txt"), header=None, sep="   ")

        columns = [
            "lp",
            "v",
            "gtt",
            "gtn",
            "ggn",
            "ts",
            "tp",
            "t48",
            "t1",
            "t2",
            "p48",
            "p1",
            "p2",
            "pexh",
            "tic",
            "mf",
            "gtcdsc",
            "gttdsc",
        ]
        df.columns = columns

        makedirs(self.processed_temp_path, exist_ok=True)
        df.to_csv(os.path.join(self.processed_temp_path, self.csv_filename), index=False)
        rename(self.processed_temp_path, self.processed_dataset_path)
</source>
</class>

<class classid="26" nclones="4" nlines="14" similarity="78">
<source file="systems/ludwig-0.5rc2/ludwig/datasets/sst3/__init__.py" startline="43" endline="57" pcid="1125">
    def __init__(
        self,
        cache_dir=DEFAULT_CACHE_LOCATION,
        include_subtrees=False,
        convert_parentheses=True,
        remove_duplicates=False,
    ):
        super().__init__(
            dataset_name="sst3",
            cache_dir=cache_dir,
            include_subtrees=include_subtrees,
            convert_parentheses=convert_parentheses,
            remove_duplicates=False,
        )

</source>
<source file="systems/ludwig-0.5rc2/ludwig/datasets/sst5/__init__.py" startline="43" endline="57" pcid="1168">
    def __init__(
        self,
        cache_dir=DEFAULT_CACHE_LOCATION,
        include_subtrees=False,
        convert_parentheses=True,
        remove_duplicates=False,
    ):
        super().__init__(
            dataset_name="sst5",
            cache_dir=cache_dir,
            include_subtrees=include_subtrees,
            convert_parentheses=convert_parentheses,
            remove_duplicates=False,
        )

</source>
<source file="systems/ludwig-0.5rc2/ludwig/datasets/sst2/__init__.py" startline="59" endline="74" pcid="1186">
    def __init__(
        self,
        cache_dir=DEFAULT_CACHE_LOCATION,
        include_subtrees=False,
        convert_parentheses=True,
        remove_duplicates=False,
    ):
        super().__init__(
            dataset_name="sst2",
            cache_dir=cache_dir,
            include_subtrees=include_subtrees,
            discard_neutral=True,
            convert_parentheses=convert_parentheses,
            remove_duplicates=remove_duplicates,
        )

</source>
<source file="systems/ludwig-0.5rc2/ludwig/datasets/sst2/__init__.py" startline="21" endline="36" pcid="1185">
def load(
    cache_dir=DEFAULT_CACHE_LOCATION,
    split=False,
    include_subtrees=False,
    convert_parentheses=True,
    remove_duplicates=False,
):
    dataset = SST2(
        cache_dir=cache_dir,
        include_subtrees=include_subtrees,
        convert_parentheses=convert_parentheses,
        remove_duplicates=remove_duplicates,
    )
    return dataset.load(split=split)


</source>
</class>

<class classid="27" nclones="4" nlines="11" similarity="75">
<source file="systems/ludwig-0.5rc2/ludwig/features/text_feature.py" startline="240" endline="253" pcid="1246">
    def add_feature_data(
        feature_config, input_df, proc_df, metadata, preprocessing_parameters, backend, skip_save_processed_input
    ):
        chars_data, words_data = TextFeatureMixin.feature_data(
            input_df[feature_config[COLUMN]].astype(str),
            metadata[feature_config[NAME]],
            preprocessing_parameters,
            backend,
        )
        proc_df[f"{feature_config[PROC_COLUMN]}_char"] = chars_data
        proc_df[f"{feature_config[PROC_COLUMN]}_word"] = words_data
        return proc_df


</source>
<source file="systems/ludwig-0.5rc2/ludwig/features/bag_feature.py" startline="91" endline="102" pcid="1321">
    def add_feature_data(
        feature_config, input_df, proc_df, metadata, preprocessing_parameters, backend, skip_save_processed_input
    ):
        proc_df[feature_config[PROC_COLUMN]] = BagFeatureMixin.feature_data(
            input_df[feature_config[COLUMN]].astype(str),
            metadata[feature_config[NAME]],
            preprocessing_parameters,
            backend,
        )
        return proc_df


</source>
<source file="systems/ludwig-0.5rc2/ludwig/features/timeseries_feature.py" startline="117" endline="128" pcid="1374">
    def add_feature_data(
        feature_config, input_df, proc_df, metadata, preprocessing_parameters, backend, skip_save_processed_input
    ):
        proc_df[feature_config[PROC_COLUMN]] = TimeseriesFeatureMixin.feature_data(
            input_df[feature_config[COLUMN]].astype(str),
            metadata[feature_config[NAME]],
            preprocessing_parameters,
            backend,
        )
        return proc_df


</source>
<source file="systems/ludwig-0.5rc2/ludwig/features/sequence_feature.py" startline="153" endline="165" pcid="1274">
    def add_feature_data(
        feature_config, input_df, proc_df, metadata, preprocessing_parameters, backend, skip_save_processed_input
    ):
        sequence_data = SequenceInputFeature.feature_data(
            input_df[feature_config[COLUMN]].astype(str),
            metadata[feature_config[NAME]],
            preprocessing_parameters,
            backend,
        )
        proc_df[feature_config[PROC_COLUMN]] = sequence_data
        return proc_df


</source>
</class>

<class classid="28" nclones="2" nlines="18" similarity="73">
<source file="systems/ludwig-0.5rc2/ludwig/features/sequence_feature.py" startline="115" endline="135" pcid="1272">
    def get_feature_meta(column, preprocessing_parameters, backend):
        column = column.astype(str)
        idx2str, str2idx, str2freq, max_length, _, _, _ = create_vocabulary(
            column,
            preprocessing_parameters["tokenizer"],
            lowercase=preprocessing_parameters["lowercase"],
            num_most_frequent=preprocessing_parameters["most_common"],
            vocab_file=preprocessing_parameters["vocab_file"],
            unknown_symbol=preprocessing_parameters["unknown_symbol"],
            padding_symbol=preprocessing_parameters["padding_symbol"],
            processor=backend.df_engine,
        )
        max_length = min(preprocessing_parameters["sequence_length_limit"], max_length)
        return {
            "idx2str": idx2str,
            "str2idx": str2idx,
            "str2freq": str2freq,
            "vocab_size": len(idx2str),
            "max_sequence_length": max_length + 2,  # For start and end symbol.
        }

</source>
<source file="systems/ludwig-0.5rc2/ludwig/features/bag_feature.py" startline="62" endline="78" pcid="1318">
    def get_feature_meta(column, preprocessing_parameters, backend):
        column = column.astype(str)
        idx2str, str2idx, str2freq, max_size, _, _, _ = create_vocabulary(
            column,
            preprocessing_parameters["tokenizer"],
            num_most_frequent=preprocessing_parameters["most_common"],
            lowercase=preprocessing_parameters["lowercase"],
            processor=backend.df_engine,
        )
        return {
            "idx2str": idx2str,
            "str2idx": str2idx,
            "str2freq": str2freq,
            "vocab_size": len(str2idx),
            "max_set_size": max_size,
        }

</source>
</class>

<class classid="29" nclones="4" nlines="31" similarity="78">
<source file="systems/ludwig-0.5rc2/ludwig/modules/embedding_modules.py" startline="117" endline="148" pcid="1433">
    def __init__(
        self,
        vocab: List[str],
        embedding_size: int,
        representation: str = "dense",
        embeddings_trainable: bool = True,
        pretrained_embeddings: Optional[str] = None,
        force_embedding_size: bool = False,
        embeddings_on_cpu: bool = False,
        dropout: float = 0.0,
        embedding_initializer: Optional[Union[str, Dict]] = None,
    ):
        super().__init__()
        self.supports_masking = True

        self.vocab_size = len(vocab)
        self.embeddings, self.embedding_size = embedding_matrix_on_device(
            vocab,
            embedding_size,
            representation=representation,
            embeddings_trainable=embeddings_trainable,
            pretrained_embeddings=pretrained_embeddings,
            force_embedding_size=force_embedding_size,
            embeddings_on_cpu=embeddings_on_cpu,
            embedding_initializer=embedding_initializer,
        )

        if dropout > 0:
            self.dropout = torch.nn.Dropout(p=dropout)
        else:
            self.dropout = None

</source>
<source file="systems/ludwig-0.5rc2/ludwig/modules/embedding_modules.py" startline="172" endline="213" pcid="1437">
    def __init__(
        self,
        vocab: List[str],
        embedding_size: int,
        representation: str = "dense",
        embeddings_trainable: bool = True,
        pretrained_embeddings: Optional[str] = None,
        force_embedding_size: bool = False,
        embeddings_on_cpu: bool = False,
        dropout: float = 0.0,
        embedding_initializer: Optional[Union[str, Dict]] = None,
        aggregation_function: str = "sum",
    ):
        super().__init__()
        self.supports_masking = True

        self.vocab_size = len(vocab)
        self.embeddings, self.embedding_size = embedding_matrix_on_device(
            vocab,
            embedding_size,
            representation=representation,
            embeddings_trainable=embeddings_trainable,
            pretrained_embeddings=pretrained_embeddings,
            force_embedding_size=force_embedding_size,
            embeddings_on_cpu=embeddings_on_cpu,
            embedding_initializer=embedding_initializer,
        )

        if dropout > 0:
            self.dropout = torch.nn.Dropout(p=dropout)
        else:
            self.dropout = None

        if aggregation_function == "sum":
            self.aggregation_function = torch.sum
        elif aggregation_function == "avg":
            self.aggregation_function = torch.mean
        else:
            raise ValueError(f"Unsupported aggregation function {aggregation_function}")

        self.register_buffer("vocab_indices", torch.arange(self.vocab_size))

</source>
<source file="systems/ludwig-0.5rc2/ludwig/modules/embedding_modules.py" startline="248" endline="280" pcid="1442">
    def __init__(
        self,
        vocab: List[str],
        embedding_size: int,
        representation: str = "dense",
        embeddings_trainable: bool = True,
        pretrained_embeddings: Optional[str] = None,
        force_embedding_size: bool = False,
        embeddings_on_cpu: bool = False,
        dropout: float = 0.0,
        embedding_initializer: Optional[str] = None,
    ):
        super().__init__()

        self.embeddings, self.embedding_size = embedding_matrix_on_device(
            vocab,
            embedding_size,
            representation=representation,
            embeddings_trainable=embeddings_trainable,
            pretrained_embeddings=pretrained_embeddings,
            force_embedding_size=force_embedding_size,
            embeddings_on_cpu=embeddings_on_cpu,
            embedding_initializer=embedding_initializer,
        )
        self.vocab_size = len(vocab)

        if dropout > 0:
            self.dropout = nn.Dropout(dropout)
        else:
            self.dropout = None

        self.register_buffer("vocab_indices", torch.arange(self.vocab_size, dtype=torch.int32))

</source>
<source file="systems/ludwig-0.5rc2/ludwig/modules/embedding_modules.py" startline="374" endline="407" pcid="1446">
    def __init__(
        self,
        vocab: List[str],
        embedding_size: int,
        max_sequence_length: int,
        representation: str = "dense",
        embeddings_trainable: bool = True,
        pretrained_embeddings: Optional[str] = None,
        force_embedding_size: bool = False,
        embeddings_on_cpu: bool = False,
        dropout: float = 0.0,
        embedding_initializer: Optional[str] = None,
    ):
        super().__init__()
        self.supports_masking = True

        self.vocab_size = len(vocab)
        self.max_sequence_length = max_sequence_length
        self.embeddings, self.embedding_size = embedding_matrix_on_device(
            vocab,
            embedding_size,
            representation=representation,
            embeddings_trainable=embeddings_trainable,
            pretrained_embeddings=pretrained_embeddings,
            force_embedding_size=force_embedding_size,
            embeddings_on_cpu=embeddings_on_cpu,
            embedding_initializer=embedding_initializer,
        )

        if dropout > 0:
            self.dropout = nn.Dropout(dropout)
        else:
            self.dropout = None

</source>
</class>

</clones>

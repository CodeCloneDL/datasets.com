<html>
<head>
    <title>NiCad6 Clone Report</title>
    <style type="text/css">
        body {font-family:sans-serif;}
        table {background-color:white; border:0px; padding:0px; border-spacing:4px; width:auto; margin-left:30px; margin-right:auto;}
        td {background-color:rgba(192,212,238,0.8); border:0px; padding:8px; width:auto; vertical-align:top; border-radius:8px}
        pre {background-color:white; padding:4px;}
        a {color:darkblue;}
    </style>
</head>
<body>
<h2>NiCad6 Clone Report</h2>
<table>
<tr style="font-size:14pt">
<td><b>System:</b> &nbsp; deepchem-2.6.1</td>
<td><b>Clone pairs:</b> &nbsp; 135</td>
<td><b>Clone classes:</b> &nbsp; 50</td>
</tr>
<tr style="font-size:12pt">
<td style="background-color:white">Clone type: &nbsp; 2</td>
<td style="background-color:white">Granularity: &nbsp; functions-blind</td>
<td style="background-color:white">Max diff threshold: &nbsp; 0%</td>
<td style="background-color:white">Clone size: &nbsp; 10 - 2500 lines</td>
<td style="background-color:white">Total functions-blind: &nbsp; 1958</td>
</tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 1:</b> &nbsp; 2 fragments, nominal size 24 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag48')" href="javascript:;">
deepchem-2.6.1/contrib/torch/pytorch_graphconv.py: 177-216
</a>
<div class="mid" id="frag48" style="display:none"><pre>
    def train_epoch(self, train_features, y_train, batch_size=32,
                    shuffle_train_inds=True):
        """
        train_features: list of dictionaries. each dictionary represents one sample feature. 
            key "x" maps to max_n_atoms x p feature matrix. key "g" maps to square adjacency matrix 
        y_train: numpy array of labels 
        """

        train_inds = range(0, len(train_features))
        if shuffle_train_inds:
            random.shuffle(train_inds)

        for b in range(0, len(train_inds)/batch_size):
            batch_inds = [train_inds[idx] for idx in range(b*batch_size, (b+1)*batch_size)]
            
            train_x_batch = np.concatenate([np.expand_dims(train_features[idx]["x"], 0) for idx in batch_inds], axis=0)
            train_g_batch = np.concatenate([np.expand_dims(train_features[idx]["g"], 0) for idx in batch_inds], axis=0)

            xb = torch.from_numpy(train_x_batch.astype(np.float32)).cuda()
            gb = torch.from_numpy(train_g_batch.astype(np.float32)).cuda()
            yb = torch.from_numpy(y_train[batch_inds].astype(np.float32)).cuda()

            self.net.train()
            self.net.zero_grad()
            
            self.input_x.resize_as_(xb).copy_(xb)
            self.input_g.resize_as_(gb).copy_(gb)
            self.label.resize_as_(yb).copy_(yb)
            
            input_xv = Variable(self.input_x)
            input_gv = Variable(self.input_g)
            label_v = Variable(self.label)

            output = self.net(input_gv, input_xv)
            
            err = self.criterion(output, label_v)
            err.backward()
            
            self.optimizer.step()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag52')" href="javascript:;">
deepchem-2.6.1/contrib/torch/pytorch_graphconv.py: 359-392
</a>
<div class="mid" id="frag52" style="display:none"><pre>
    def train_epoch(self, train_features, y_train, batch_size=32,
                    shuffle_train_inds=True):
        train_inds = range(0, len(train_features))
        if shuffle_train_inds:
            random.shuffle(train_inds)

        for b in range(0, len(train_inds)/batch_size):
            batch_inds = [train_inds[idx] for idx in range(b*batch_size, (b+1)*batch_size)]
            
            train_x_batch = np.concatenate([np.expand_dims(train_features[idx]["x"], 0) for idx in batch_inds], axis=0)
            train_g_batch = np.concatenate([np.expand_dims(train_features[idx]["g"], 0) for idx in batch_inds], axis=0)

            xb = torch.from_numpy(train_x_batch.astype(np.float32)).cuda()
            gb = torch.from_numpy(train_g_batch.astype(np.float32)).cuda()
            yb = torch.from_numpy(y_train[batch_inds].astype(np.float32)).cuda()

            self.net.train()
            self.net.zero_grad()
            
            self.input_x.resize_as_(xb).copy_(xb)
            self.input_g.resize_as_(gb).copy_(gb)
            self.label.resize_as_(yb).copy_(yb)
            
            input_xv = Variable(self.input_x)
            input_gv = Variable(self.input_g)
            label_v = Variable(self.label)

            output = self.net(input_gv, input_xv)
            
            err = self.multitask_loss(output, label_v)
            err.backward()
            
            self.optimizer.step()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 2:</b> &nbsp; 2 fragments, nominal size 61 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag49')" href="javascript:;">
deepchem-2.6.1/contrib/torch/pytorch_graphconv.py: 217-300
</a>
<div class="mid" id="frag49" style="display:none"><pre>
    def evaluate(self, train_features,
                       test_features,
                       y_train,
                       y_test, 
                       transformer,
                       batch_size=32):
        
        self.net.eval()
        print("TRAIN:")
        
        o = []
        l = []

        train_inds = range(0, len(train_features))

        for b in range(0, len(train_features)/batch_size):
            batch_inds = [train_inds[idx] for idx in range(b*batch_size, (b+1)*batch_size)]
            
            train_x_batch = np.concatenate([np.expand_dims(train_features[idx]["x"], 0) for idx in batch_inds], axis=0)
            train_g_batch = np.concatenate([np.expand_dims(train_features[idx]["g"], 0) for idx in batch_inds], axis=0)

            xb = torch.from_numpy(train_x_batch.astype(np.float32)).cuda()
            gb = torch.from_numpy(train_g_batch.astype(np.float32)).cuda()
            
            self.input_x.resize_as_(xb).copy_(xb)
            self.input_g.resize_as_(gb).copy_(gb)
            
            input_xv = Variable(self.input_x)
            input_gv = Variable(self.input_g)

            output = self.net(input_gv, input_xv)
            
            if transformer is not None:
                o.append(transformer.inverse_transform(output.data.cpu().numpy().reshape((-1,1))).flatten())
                l.append(transformer.inverse_transform(y_train[batch_inds].reshape((-1,1))).flatten())
            else:
                o.append(output.data.cpu().numpy().reshape((-1,1)).flatten())
                l.append(y_train[batch_inds].reshape((-1,1)).flatten())

        o = np.concatenate(o)
        l = np.concatenate(l)
        print("RMSE:")
        print(np.sqrt(np.mean(np.square(l-o))))
        print("ROC AUC:")
        print(roc_auc_score(l, o))
        
        o = []
        l = []

        print("TEST:")
        test_inds = range(0, len(test_features))

        for b in range(0, len(test_features)/batch_size):
            batch_inds = [test_inds[idx] for idx in range(b*batch_size, (b+1)*batch_size)]
            
            test_x_batch = np.concatenate([np.expand_dims(test_features[idx]["x"], 0) for idx in batch_inds], axis=0)
            test_g_batch = np.concatenate([np.expand_dims(test_features[idx]["g"], 0) for idx in batch_inds], axis=0)

            xb = torch.from_numpy(test_x_batch.astype(np.float32)).cuda()
            gb = torch.from_numpy(test_g_batch.astype(np.float32)).cuda()
            
            self.input_x.resize_as_(xb).copy_(xb)
            self.input_g.resize_as_(gb).copy_(gb)
            
            input_xv = Variable(self.input_x)
            input_gv = Variable(self.input_g)

            output = self.net(input_gv, input_xv)
            
            if transformer is not None:
                o.append(transformer.inverse_transform(output.data.cpu().numpy().reshape((-1,1))).flatten())
                l.append(transformer.inverse_transform(y_test[batch_inds].reshape((-1,1))).flatten())
            else:
                o.append(output.data.cpu().numpy().reshape((-1,1)).flatten())
                l.append(y_test[batch_inds].reshape((-1,1)).flatten())

        o = np.concatenate(o)
        l = np.concatenate(l)
        print("RMSE:")
        print(np.sqrt(np.mean(np.square(l-o))))
        print("ROC AUC:")
        print(roc_auc_score(l, o))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag53')" href="javascript:;">
deepchem-2.6.1/contrib/torch/pytorch_graphconv.py: 393-474
</a>
<div class="mid" id="frag53" style="display:none"><pre>
    def evaluate(self, train_features,
                       test_features,
                       y_train,
                       y_test, 
                       transformer,
                       batch_size=32):
        
        self.net.eval()
        print("TRAIN:")
        
        o = []
        l = []

        train_inds = range(0, len(train_features))

        for b in range(0, len(train_features)/batch_size):
            batch_inds = [train_inds[idx] for idx in range(b*batch_size, (b+1)*batch_size)]
            
            train_x_batch = np.concatenate([np.expand_dims(train_features[idx]["x"], 0) for idx in batch_inds], axis=0)
            train_g_batch = np.concatenate([np.expand_dims(train_features[idx]["g"], 0) for idx in batch_inds], axis=0)

            xb = torch.from_numpy(train_x_batch.astype(np.float32)).cuda()
            gb = torch.from_numpy(train_g_batch.astype(np.float32)).cuda()
            
            self.input_x.resize_as_(xb).copy_(xb)
            self.input_g.resize_as_(gb).copy_(gb)
            
            input_xv = Variable(self.input_x)
            input_gv = Variable(self.input_g)

            output = self.net(input_gv, input_xv)
            
            if transformer is not None:
                o.append(transformer.inverse_transform(output.data.cpu().numpy().reshape((-1,1))).flatten())
                l.append(transformer.inverse_transform(y_train[batch_inds].reshape((-1,1))).flatten())
            else:
                o.append(output.data.cpu().numpy().reshape((-1,1)).flatten())
                l.append(y_train[batch_inds].reshape((-1,1)).flatten())

        o = np.concatenate(o)
        l = np.concatenate(l)
        print("RMSE:")
        print(np.sqrt(np.mean(np.square(l-o))))
        print("ROC AUC:")
        print(roc_auc_score(l, o))
        
        o = []
        l = []

        print("TEST:")
        test_inds = range(0, len(test_features))

        for b in range(0, len(test_features)/batch_size):
            batch_inds = [test_inds[idx] for idx in range(b*batch_size, (b+1)*batch_size)]
            
            test_x_batch = np.concatenate([np.expand_dims(test_features[idx]["x"], 0) for idx in batch_inds], axis=0)
            test_g_batch = np.concatenate([np.expand_dims(test_features[idx]["g"], 0) for idx in batch_inds], axis=0)

            xb = torch.from_numpy(test_x_batch.astype(np.float32)).cuda()
            gb = torch.from_numpy(test_g_batch.astype(np.float32)).cuda()
            
            self.input_x.resize_as_(xb).copy_(xb)
            self.input_g.resize_as_(gb).copy_(gb)
            
            input_xv = Variable(self.input_x)
            input_gv = Variable(self.input_g)

            output = self.net(input_gv, input_xv)
            
            if transformer is not None:
                o.append(transformer.inverse_transform(output.data.cpu().numpy().reshape((-1,1))).flatten())
                l.append(transformer.inverse_transform(y_test[batch_inds].reshape((-1,1))).flatten())
            else:
                o.append(output.data.cpu().numpy().reshape((-1,1)).flatten())
                l.append(y_test[batch_inds].reshape((-1,1)).flatten())

        o = np.concatenate(o)
        l = np.concatenate(l)
        print("RMSE:")
        print(np.sqrt(np.mean(np.square(l-o))))
        print("ROC AUC:")
        print(roc_auc_score(l, o))
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 3:</b> &nbsp; 4 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag119')" href="javascript:;">
deepchem-2.6.1/contrib/rl/test_mcts.py: 29-41
</a>
<div class="mid" id="frag119" style="display:none"><pre>
      def step(self, action):
        if action == 37:
          self._terminated = True  # Walk away.
          return 0.0
        wheel = np.random.randint(37)
        if wheel == 0:
          if action == 0:
            return 35.0
          return -1.0
        if action != 0 and wheel % 2 == action % 2:
          return 1.0
        return -1.0

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1187')" href="javascript:;">
deepchem-2.6.1/deepchem/rl/tests/test_a2c.py: 35-47
</a>
<div class="mid" id="frag1187" style="display:none"><pre>
      def step(self, action):
        if action == 37:
          self._terminated = True  # Walk away.
          return 0.0
        wheel = np.random.randint(37)
        if wheel == 0:
          if action == 0:
            return 35.0
          return -1.0
        if action != 0 and wheel % 2 == action % 2:
          return 1.0
        return -1.0

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1166')" href="javascript:;">
deepchem-2.6.1/deepchem/rl/tests/test_ppo.py: 35-47
</a>
<div class="mid" id="frag1166" style="display:none"><pre>
      def step(self, action):
        if action == 37:
          self._terminated = True  # Walk away.
          return 0.0
        wheel = np.random.randint(37)
        if wheel == 0:
          if action == 0:
            return 35.0
          return -1.0
        if action != 0 and wheel % 2 == action % 2:
          return 1.0
        return -1.0

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1156')" href="javascript:;">
deepchem-2.6.1/deepchem/rl/tests/test_rl_reload.py: 14-26
</a>
<div class="mid" id="frag1156" style="display:none"><pre>
    def step(self, action):
      if action == 37:
        self._terminated = True  # Walk away.
        return 0.0
      wheel = np.random.randint(37)
      if wheel == 0:
        if action == 0:
          return 35.0
        return -1.0
      if action != 0 and wheel % 2 == action % 2:
        return 1.0
      return -1.0

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 4:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag171')" href="javascript:;">
deepchem-2.6.1/contrib/atomicconv/splits/pdbbind_temporal_split.py: 8-42
</a>
<div class="mid" id="frag171" style="display:none"><pre>
def load_pdbbind_labels(labels_file):
  """Loads pdbbind labels as dataframe

  Parameters
  ----------
  labels_file: str
    Location of PDBbind datafile.

  Returns
  -------
  contents_df: pd.DataFrame
    Dataframe containing contents of PDBbind datafile.

  """

  contents = []
  with open(labels_file) as f:
    for line in f:
      if line.startswith("#"):
        continue
      else:
        splitline = line.split()
        if len(splitline) == 8:
          contents.append(splitline)
        else:
          print("Incorrect data format")
          print(splitline)

  contents_df = pd.DataFrame(
      contents,
      columns=("PDB code", "resolution", "release year", "-logKd/Ki", "Kd/Ki",
               "ignore-this-field", "reference", "ligand name"))
  return contents_df


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag183')" href="javascript:;">
deepchem-2.6.1/contrib/atomicconv/feat/atomicnet_pdbbind_datasets.py: 15-49
</a>
<div class="mid" id="frag183" style="display:none"><pre>
def load_pdbbind_labels(labels_file):
  """Loads pdbbind labels as dataframe

  Parameters
  ----------
  labels_file: str
    Location of PDBbind datafile.

  Returns
  -------
  contents_df: pd.DataFrame
    Dataframe containing contents of PDBbind datafile.

  """

  contents = []
  with open(labels_file) as f:
    for line in f:
      if line.startswith("#"):
        continue
      else:
        splitline = line.split()
        if len(splitline) == 8:
          contents.append(splitline)
        else:
          print("Incorrect data format")
          print(splitline)

  contents_df = pd.DataFrame(
      contents,
      columns=("PDB code", "resolution", "release year", "-logKd/Ki", "Kd/Ki",
               "ignore-this-field", "reference", "ligand name"))
  return contents_df


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 5:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag214')" href="javascript:;">
deepchem-2.6.1/deepchem/splits/tests/test_task_splitter.py: 14-33
</a>
<div class="mid" id="frag214" style="display:none"><pre>
  def test_multitask_train_valid_test_split(self):
    """
    Test TaskSplitter train/valid/test split on multitask dataset.
    """
    n_samples = 100
    n_features = 10
    n_tasks = 10
    X = np.random.rand(n_samples, n_features)
    p = .05  # proportion actives
    y = np.random.binomial(1, p, size=(n_samples, n_tasks))
    dataset = dc.data.NumpyDataset(X, y)

    task_splitter = dc.splits.TaskSplitter()
    train, valid, test = task_splitter.train_valid_test_split(
        dataset, frac_train=.4, frac_valid=.3, frac_test=.3)

    assert len(train.get_task_names()) == 4
    assert len(valid.get_task_names()) == 3
    assert len(test.get_task_names()) == 3

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag217')" href="javascript:;">
deepchem-2.6.1/deepchem/splits/tests/test_task_splitter.py: 73-93
</a>
<div class="mid" id="frag217" style="display:none"><pre>
  def test_uneven_train_valid_test_split(self):
    """
    Test train/valid/test split works when proportions don't divide n_tasks.
    """
    n_samples = 100
    n_features = 10
    n_tasks = 11
    X = np.random.rand(n_samples, n_features)
    p = .05  # proportion actives
    y = np.random.binomial(1, p, size=(n_samples, n_tasks))
    dataset = dc.data.NumpyDataset(X, y)

    task_splitter = dc.splits.TaskSplitter()
    train, valid, test = task_splitter.train_valid_test_split(
        dataset, frac_train=.4, frac_valid=.3, frac_test=.3)

    assert len(train.get_task_names()) == 4
    assert len(valid.get_task_names()) == 3
    # Note that the extra task goes to test
    assert len(test.get_task_names()) == 4

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 6:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag220')" href="javascript:;">
deepchem-2.6.1/deepchem/splits/tests/test_splitter.py: 27-42
</a>
<div class="mid" id="frag220" style="display:none"><pre>
def load_multitask_data():
  """Load example multitask data."""
  current_dir = os.path.dirname(os.path.abspath(__file__))
  featurizer = dc.feat.CircularFingerprint(size=1024)
  tasks = [
      "task0", "task1", "task2", "task3", "task4", "task5", "task6", "task7",
      "task8", "task9", "task10", "task11", "task12", "task13", "task14",
      "task15", "task16"
  ]
  input_file = os.path.join(current_dir,
                            "../../models/tests/multitask_example.csv")
  loader = dc.data.CSVLoader(
      tasks=tasks, feature_field="smiles", featurizer=featurizer)
  return loader.create_dataset(input_file)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1306')" href="javascript:;">
deepchem-2.6.1/deepchem/data/tests/test_datasets.py: 31-46
</a>
<div class="mid" id="frag1306" style="display:none"><pre>
def load_multitask_data():
  """Load example multitask data."""
  current_dir = os.path.dirname(os.path.abspath(__file__))
  featurizer = dc.feat.CircularFingerprint(size=1024)
  tasks = [
      "task0", "task1", "task2", "task3", "task4", "task5", "task6", "task7",
      "task8", "task9", "task10", "task11", "task12", "task13", "task14",
      "task15", "task16"
  ]
  input_file = os.path.join(current_dir,
                            "../../models/tests/multitask_example.csv")
  loader = dc.data.CSVLoader(
      tasks=tasks, feature_field="smiles", featurizer=featurizer)
  return loader.create_dataset(input_file)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 7:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag224')" href="javascript:;">
deepchem-2.6.1/deepchem/splits/tests/test_splitter.py: 100-115
</a>
<div class="mid" id="frag224" style="display:none"><pre>
  def test_singletask_random_split(self):
    """
    Test singletask RandomSplitter class.
    """
    solubility_dataset = load_solubility_data()
    random_splitter = dc.splits.RandomSplitter()
    train_data, valid_data, test_data = \
      random_splitter.train_valid_test_split(
        solubility_dataset, frac_train=0.8, frac_valid=0.1, frac_test=0.1)
    assert len(train_data) == 8
    assert len(valid_data) == 1
    assert len(test_data) == 1

    merged_dataset = dc.data.DiskDataset.merge(
        [train_data, valid_data, test_data])
    assert sorted(merged_dataset.ids) == (sorted(solubility_dataset.ids))
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag228')" href="javascript:;">
deepchem-2.6.1/deepchem/splits/tests/test_splitter.py: 164-179
</a>
<div class="mid" id="frag228" style="display:none"><pre>
    s1 = set(train_data.ids)
    assert valid_data.ids[0] not in s1
    assert test_data.ids[0] not in s1

  def test_singletask_stratified_split(self):
    """
    Test singletask SingletaskStratifiedSplitter class.
    """
    solubility_dataset = load_solubility_data()
    stratified_splitter = dc.splits.ScaffoldSplitter()
    train_data, valid_data, test_data = \
      stratified_splitter.train_valid_test_split(
        solubility_dataset, frac_train=0.8, frac_valid=0.1, frac_test=0.1)
    assert len(train_data) == 8
    assert len(valid_data) == 1
    assert len(test_data) == 1
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 8:</b> &nbsp; 2 fragments, nominal size 20 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag233')" href="javascript:;">
deepchem-2.6.1/deepchem/splits/tests/test_splitter.py: 247-276
</a>
<div class="mid" id="frag233" style="display:none"><pre>
      for other_fold in range(K):
        if fold == other_fold:
          continue
        other_fold_dataset = fold_datasets[other_fold][1]
        other_fold_ids_set = set(other_fold_dataset.ids)
        assert fold_ids_set.isdisjoint(other_fold_ids_set)

  def test_singletask_index_k_fold_split(self):
    """
    Test singletask IndexSplitter class.
    """
    solubility_dataset = load_solubility_data()
    index_splitter = dc.splits.IndexSplitter()
    ids_set = set(solubility_dataset.ids)

    K = 5
    fold_datasets = index_splitter.k_fold_split(solubility_dataset, K)

    for fold in range(K):
      fold_dataset = fold_datasets[fold][1]
      # Verify lengths is 10/k == 2
      assert len(fold_dataset) == 2
      # Verify that compounds in this fold are subset of original compounds
      fold_ids_set = set(fold_dataset.ids)
      assert fold_ids_set.issubset(ids_set)
      # Verify that no two folds have overlapping compounds.
      for other_fold in range(K):
        if fold == other_fold:
          continue
        other_fold_dataset = fold_datasets[other_fold][1]
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag234')" href="javascript:;">
deepchem-2.6.1/deepchem/splits/tests/test_splitter.py: 277-306
</a>
<div class="mid" id="frag234" style="display:none"><pre>
        other_fold_ids_set = set(other_fold_dataset.ids)
        assert fold_ids_set.isdisjoint(other_fold_ids_set)

    merged_dataset = dc.data.DiskDataset.merge([x[1] for x in fold_datasets])
    assert len(merged_dataset) == len(solubility_dataset)
    assert sorted(merged_dataset.ids) == (sorted(solubility_dataset.ids))

  def test_singletask_scaffold_k_fold_split(self):
    """
    Test singletask ScaffoldSplitter class.
    """
    solubility_dataset = load_solubility_data()
    scaffold_splitter = dc.splits.ScaffoldSplitter()
    ids_set = set(solubility_dataset.ids)

    K = 5
    fold_datasets = scaffold_splitter.k_fold_split(solubility_dataset, K)

    for fold in range(K):
      fold_dataset = fold_datasets[fold][1]
      # Verify lengths is 10/k == 2
      assert len(fold_dataset) == 2
      # Verify that compounds in this fold are subset of original compounds
      fold_ids_set = set(fold_dataset.ids)
      assert fold_ids_set.issubset(ids_set)
      # Verify that no two folds have overlapping compounds.
      for other_fold in range(K):
        if fold == other_fold:
          continue
        other_fold_dataset = fold_datasets[other_fold][1]
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 9:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag322')" href="javascript:;">
deepchem-2.6.1/deepchem/models/optimizers.py: 212-223
</a>
<div class="mid" id="frag322" style="display:none"><pre>
  def _create_tf_optimizer(self, global_step):
    import tensorflow as tf
    if isinstance(self.learning_rate, LearningRateSchedule):
      learning_rate = self.learning_rate._create_tf_tensor(global_step)
    else:
      learning_rate = self.learning_rate
    return tf.keras.optimizers.Adam(
        learning_rate=learning_rate,
        beta_1=self.beta1,
        beta_2=self.beta2,
        epsilon=self.epsilon)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag333')" href="javascript:;">
deepchem-2.6.1/deepchem/models/optimizers.py: 407-418
</a>
<div class="mid" id="frag333" style="display:none"><pre>
  def _create_tf_optimizer(self, global_step):
    import tensorflow as tf
    if isinstance(self.learning_rate, LearningRateSchedule):
      learning_rate = self.learning_rate._create_tf_tensor(global_step)
    else:
      learning_rate = self.learning_rate
    return tf.keras.optimizers.RMSprop(
        learning_rate=learning_rate,
        momentum=self.momentum,
        rho=self.decay,
        epsilon=self.epsilon)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 10:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag409')" href="javascript:;">
deepchem-2.6.1/deepchem/models/chemnet_layers.py: 240-260
</a>
<div class="mid" id="frag409" style="display:none"><pre>
  def call(self, inputs):
    """Invoked when __call__ method of the layer is used."""
    conv1 = inputs
    for layer in self.conv_block1:
      conv1 = layer(conv1)

    conv2 = inputs
    for layer in self.conv_block2:
      conv2 = layer(conv2)

    concat_conv = self.concat_layer([conv1, conv2])

    conv3 = concat_conv
    for layer in self.conv_block3:
      conv3 = layer(conv3)

    output = self.add_layer([conv3, inputs])
    output = self.activation_layer(output)
    return output


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag412')" href="javascript:;">
deepchem-2.6.1/deepchem/models/chemnet_layers.py: 337-357
</a>
<div class="mid" id="frag412" style="display:none"><pre>
  def call(self, inputs):
    """Invoked when __call__ method of the layer is used."""
    conv1 = inputs
    for layer in self.conv_block1:
      conv1 = layer(conv1)

    conv2 = inputs
    for layer in self.conv_block2:
      conv2 = layer(conv2)

    concat_conv = self.concat_layer([conv1, conv2])

    conv3 = concat_conv
    for layer in self.conv_block3:
      conv3 = layer(conv3)

    output = self.add_layer([conv3, inputs])
    output = self.activation_layer(output)
    return output


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 11:</b> &nbsp; 2 fragments, nominal size 30 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag424')" href="javascript:;">
deepchem-2.6.1/deepchem/models/tests/test_gat.py: 95-130
</a>
<div class="mid" id="frag424" style="display:none"><pre>
def test_gat_reload():
  # load datasets
  featurizer = MolGraphConvFeaturizer()
  tasks, dataset, transformers, metric = get_dataset(
      'classification', featurizer=featurizer)

  # initialize models
  n_tasks = len(tasks)
  model_dir = tempfile.mkdtemp()
  model = GATModel(
      mode='classification',
      n_tasks=n_tasks,
      number_atom_features=30,
      model_dir=model_dir,
      batch_size=10,
      learning_rate=0.001)

  model.fit(dataset, nb_epoch=100)
  scores = model.evaluate(dataset, [metric], transformers)
  assert scores['mean-roc_auc_score'] &gt;= 0.85

  reloaded_model = GATModel(
      mode='classification',
      n_tasks=n_tasks,
      number_atom_features=30,
      model_dir=model_dir,
      batch_size=10,
      learning_rate=0.001)
  reloaded_model.restore()

  pred_mols = ["CCCC", "CCCCCO", "CCCCC"]
  X_pred = featurizer(pred_mols)
  random_dataset = dc.data.NumpyDataset(X_pred)
  original_pred = model.predict(random_dataset)
  reload_pred = reloaded_model.predict(random_dataset)
  assert np.all(original_pred == reload_pred)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag497')" href="javascript:;">
deepchem-2.6.1/deepchem/models/tests/test_gcn.py: 93-128
</a>
<div class="mid" id="frag497" style="display:none"><pre>
def test_gcn_reload():
  # load datasets
  featurizer = MolGraphConvFeaturizer()
  tasks, dataset, transformers, metric = get_dataset(
      'classification', featurizer=featurizer)

  # initialize models
  n_tasks = len(tasks)
  model_dir = tempfile.mkdtemp()
  model = GCNModel(
      mode='classification',
      n_tasks=n_tasks,
      number_atom_features=30,
      model_dir=model_dir,
      batch_size=10,
      learning_rate=0.0003)

  model.fit(dataset, nb_epoch=70)
  scores = model.evaluate(dataset, [metric], transformers)
  assert scores['mean-roc_auc_score'] &gt;= 0.85

  reloaded_model = GCNModel(
      mode='classification',
      n_tasks=n_tasks,
      number_atom_features=30,
      model_dir=model_dir,
      batch_size=10,
      learning_rate=0.0003)
  reloaded_model.restore()

  pred_mols = ["CCCC", "CCCCCO", "CCCCC"]
  X_pred = featurizer(pred_mols)
  random_dataset = dc.data.NumpyDataset(X_pred)
  original_pred = model.predict(random_dataset)
  reload_pred = reloaded_model.predict(random_dataset)
  assert np.all(original_pred == reload_pred)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 12:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag436')" href="javascript:;">
deepchem-2.6.1/deepchem/models/tests/test_layers_from_config.py: 103-119
</a>
<div class="mid" id="frag436" style="display:none"><pre>
def test_attn_lstm_embedding():
  n_test = 10
  n_support = 100
  n_feat = 20
  max_depth = 3

  layer = dc.models.layers.AttnLSTMEmbedding(n_test, n_support, n_feat,
                                             max_depth)
  config = layer.get_config()
  layer_copied = dc.models.layers.AttnLSTMEmbedding.from_config(config)

  assert layer_copied.n_test == layer.n_test
  assert layer_copied.n_support == layer.n_support
  assert layer_copied.n_feat == layer.n_feat
  assert layer_copied.max_depth == layer.max_depth


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag437')" href="javascript:;">
deepchem-2.6.1/deepchem/models/tests/test_layers_from_config.py: 121-137
</a>
<div class="mid" id="frag437" style="display:none"><pre>
def test_iterref_lstm_embedding():
  n_test = 10
  n_support = 100
  n_feat = 20
  max_depth = 3

  layer = dc.models.layers.IterRefLSTMEmbedding(n_test, n_support, n_feat,
                                                max_depth)
  config = layer.get_config()
  layer_copied = dc.models.layers.IterRefLSTMEmbedding.from_config(config)

  assert layer_copied.n_test == layer.n_test
  assert layer_copied.n_support == layer.n_support
  assert layer_copied.n_feat == layer.n_feat
  assert layer_copied.max_depth == layer.max_depth


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 13:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag619')" href="javascript:;">
deepchem-2.6.1/deepchem/models/tests/test_gbdt_model.py: 22-48
</a>
<div class="mid" id="frag619" style="display:none"><pre>
def test_singletask_regression_with_xgboost():
  np.random.seed(123)

  # prepare dataset
  dataset = load_diabetes()
  X, y = dataset.data, dataset.target
  frac_train = .7
  X_train, X_test, y_train, y_test = \
    train_test_split(X, y, train_size=frac_train)
  train_dataset = dc.data.NumpyDataset(X_train, y_train)
  test_dataset = dc.data.NumpyDataset(X_test, y_test)

  # global setting
  regression_metric = dc.metrics.Metric(dc.metrics.mae_score)
  params = {'early_stopping_rounds': 25}

  # xgboost test
  xgb_model = xgboost.XGBRegressor(
      n_estimators=50, random_state=123, verbose=False)
  model = dc.models.GBDTModel(xgb_model, **params)
  # fit trained model
  model.fit(train_dataset)
  model.save()
  # eval model on test
  scores = model.evaluate(test_dataset, [regression_metric])
  assert scores[regression_metric.name] &lt; 55

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag620')" href="javascript:;">
deepchem-2.6.1/deepchem/models/tests/test_gbdt_model.py: 51-77
</a>
<div class="mid" id="frag620" style="display:none"><pre>
                 'xgboost or lightgbm are not installed')
def test_singletask_regression_with_lightgbm():
  np.random.seed(123)

  # prepare dataset
  dataset = load_diabetes()
  X, y = dataset.data, dataset.target
  frac_train = .7
  X_train, X_test, y_train, y_test = \
    train_test_split(X, y, train_size=frac_train)
  train_dataset = dc.data.NumpyDataset(X_train, y_train)
  test_dataset = dc.data.NumpyDataset(X_test, y_test)

  # global setting
  regression_metric = dc.metrics.Metric(dc.metrics.mae_score)
  params = {'early_stopping_rounds': 25}

  # lightgbm test
  lgbm_model = lightgbm.LGBMRegressor(
      n_estimators=50, random_state=123, silent=True)
  model = dc.models.GBDTModel(lgbm_model, **params)
  # fit trained model
  model.fit(train_dataset)
  model.save()
  # eval model on test
  scores = model.evaluate(test_dataset, [regression_metric])
  assert scores[regression_metric.name] &lt; 55
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 14:</b> &nbsp; 2 fragments, nominal size 23 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag621')" href="javascript:;">
deepchem-2.6.1/deepchem/models/tests/test_gbdt_model.py: 80-113
</a>
<div class="mid" id="frag621" style="display:none"><pre>
@unittest.skipIf(not has_xgboost_and_lightgbm,
                 'xgboost or lightgbm are not installed')
def test_multitask_regression_with_xgboost():
  np.random.seed(123)

  # prepare dataset
  n_tasks = 4
  tasks = range(n_tasks)
  dataset = load_diabetes()
  X, y = dataset.data, dataset.target
  y = np.reshape(y, (len(y), 1))
  y = np.hstack([y] * n_tasks)
  frac_train = .7
  X_train, X_test, y_train, y_test = \
    train_test_split(X, y, train_size=frac_train)
  train_dataset = dc.data.DiskDataset.from_numpy(X_train, y_train)
  test_dataset = dc.data.DiskDataset.from_numpy(X_test, y_test)

  # global setting
  regression_metric = dc.metrics.Metric(dc.metrics.mae_score)
  params = {'early_stopping_rounds': 25}

  # xgboost test
  def xgboost_builder(model_dir):
    xgb_model = xgboost.XGBRegressor(n_estimators=50, seed=123, verbose=False)
    return dc.models.GBDTModel(xgb_model, model_dir, **params)

  model = dc.models.SingletaskToMultitask(tasks, xgboost_builder)
  # fit trained model
  model.fit(train_dataset)
  model.save()
  # eval model on test
  scores = model.evaluate(test_dataset, [regression_metric])
  score = scores[regression_metric.name]
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag623')" href="javascript:;">
deepchem-2.6.1/deepchem/models/tests/test_gbdt_model.py: 116-149
</a>
<div class="mid" id="frag623" style="display:none"><pre>

@unittest.skipIf(not has_xgboost_and_lightgbm,
                 'xgboost or lightgbm are not installed')
def test_multitask_regression_with_lightgbm():
  np.random.seed(123)

  # prepare dataset
  n_tasks = 4
  tasks = range(n_tasks)
  dataset = load_diabetes()
  X, y = dataset.data, dataset.target
  y = np.reshape(y, (len(y), 1))
  y = np.hstack([y] * n_tasks)
  frac_train = .7
  X_train, X_test, y_train, y_test = \
    train_test_split(X, y, train_size=frac_train)
  train_dataset = dc.data.DiskDataset.from_numpy(X_train, y_train)
  test_dataset = dc.data.DiskDataset.from_numpy(X_test, y_test)

  # global setting
  regression_metric = dc.metrics.Metric(dc.metrics.mae_score)
  params = {'early_stopping_rounds': 25}

  # lightgbm test
  def lightgbm_builder(model_dir):
    lgbm_model = lightgbm.LGBMRegressor(n_estimators=50, seed=123, silent=False)
    return dc.models.GBDTModel(lgbm_model, model_dir, **params)

  model = dc.models.SingletaskToMultitask(tasks, lightgbm_builder)
  # fit trained model
  model.fit(train_dataset)
  model.save()
  # eval model on test
  scores = model.evaluate(test_dataset, [regression_metric])
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 15:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag625')" href="javascript:;">
deepchem-2.6.1/deepchem/models/tests/test_gbdt_model.py: 152-178
</a>
<div class="mid" id="frag625" style="display:none"><pre>


@unittest.skipIf(not has_xgboost_and_lightgbm,
                 'xgboost or lightgbm are not installed')
def test_classification_with_xgboost():
  """Test that sklearn models can learn on simple classification datasets."""
  np.random.seed(123)

  # prepare dataset
  dataset = load_digits(n_class=2)
  X, y = dataset.data, dataset.target
  frac_train = .7
  X_train, X_test, y_train, y_test = \
    train_test_split(X, y, train_size=frac_train)
  train_dataset = dc.data.NumpyDataset(X_train, y_train)
  test_dataset = dc.data.NumpyDataset(X_test, y_test)

  # global setting
  classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
  params = {'early_stopping_rounds': 25}

  # xgboost test
  xgb_model = xgboost.XGBClassifier(n_estimators=50, seed=123, verbose=False)
  model = dc.models.GBDTModel(xgb_model, **params)
  # fit trained model
  model.fit(train_dataset)
  model.save()
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag626')" href="javascript:;">
deepchem-2.6.1/deepchem/models/tests/test_gbdt_model.py: 181-207
</a>
<div class="mid" id="frag626" style="display:none"><pre>
  assert scores[classification_metric.name] &gt; .9


@unittest.skipIf(not has_xgboost_and_lightgbm,
                 'xgboost or lightgbm are not installed')
def test_classification_with_lightgbm():
  """Test that sklearn models can learn on simple classification datasets."""
  np.random.seed(123)

  # prepare dataset
  dataset = load_digits(n_class=2)
  X, y = dataset.data, dataset.target
  frac_train = .7
  X_train, X_test, y_train, y_test = \
    train_test_split(X, y, train_size=frac_train)
  train_dataset = dc.data.NumpyDataset(X_train, y_train)
  test_dataset = dc.data.NumpyDataset(X_test, y_test)

  # global setting
  classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
  params = {'early_stopping_rounds': 25}

  # lightgbm test
  lgbm_model = lightgbm.LGBMClassifier(n_estimators=50, seed=123, silent=True)
  model = dc.models.GBDTModel(lgbm_model, **params)
  # fit trained model
  model.fit(train_dataset)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 16:</b> &nbsp; 2 fragments, nominal size 23 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag627')" href="javascript:;">
deepchem-2.6.1/deepchem/models/tests/test_gbdt_model.py: 210-244
</a>
<div class="mid" id="frag627" style="display:none"><pre>
  scores = model.evaluate(test_dataset, [classification_metric])
  assert scores[classification_metric.name] &gt; .9


@unittest.skipIf(not has_xgboost_and_lightgbm,
                 'xgboost or lightgbm are not installed')
def test_reload_with_xgboost():
  np.random.seed(123)

  # prepare dataset
  dataset = load_diabetes()
  X, y = dataset.data, dataset.target
  frac_train = .7
  X_train, X_test, y_train, y_test = \
    train_test_split(X, y, train_size=frac_train)
  train_dataset = dc.data.NumpyDataset(X_train, y_train)
  test_dataset = dc.data.NumpyDataset(X_test, y_test)

  # global setting
  regression_metric = dc.metrics.Metric(dc.metrics.mae_score)
  model_dir = tempfile.mkdtemp()
  params = {'early_stopping_rounds': 25, 'model_dir': model_dir}

  # xgboost test
  xgb_model = xgboost.XGBRegressor(
      n_estimators=50, random_state=123, verbose=False)
  model = dc.models.GBDTModel(xgb_model, **params)
  # fit trained model
  model.fit(train_dataset)
  model.save()
  # reload
  reloaded_model = dc.models.GBDTModel(None, model_dir)
  reloaded_model.reload()
  # check predictions match on test dataset
  original_pred = model.predict(test_dataset)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag628')" href="javascript:;">
deepchem-2.6.1/deepchem/models/tests/test_gbdt_model.py: 247-279
</a>
<div class="mid" id="frag628" style="display:none"><pre>
  # eval model on test
  scores = reloaded_model.evaluate(test_dataset, [regression_metric])
  assert scores[regression_metric.name] &lt; 55


@unittest.skipIf(not has_xgboost_and_lightgbm,
                 'xgboost or lightgbm are not installed')
def test_reload_with_lightgbm():
  np.random.seed(123)

  # prepare dataset
  dataset = load_diabetes()
  X, y = dataset.data, dataset.target
  frac_train = .7
  X_train, X_test, y_train, y_test = \
    train_test_split(X, y, train_size=frac_train)
  train_dataset = dc.data.NumpyDataset(X_train, y_train)
  test_dataset = dc.data.NumpyDataset(X_test, y_test)

  # global setting
  regression_metric = dc.metrics.Metric(dc.metrics.mae_score)
  model_dir = tempfile.mkdtemp()
  params = {'early_stopping_rounds': 25, 'model_dir': model_dir}

  # lightgbm test
  lgbm_model = lightgbm.LGBMRegressor(
      n_estimators=50, random_state=123, silent=True)
  model = dc.models.GBDTModel(lgbm_model, **params)
  # fit trained model
  model.fit(train_dataset)
  model.save()
  # reload
  reloaded_model = dc.models.GBDTModel(None, model_dir)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 17:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag652')" href="javascript:;">
deepchem-2.6.1/deepchem/models/tests/test_layers.py: 326-340
</a>
<div class="mid" id="frag652" style="display:none"><pre>
def test_attn_lstm_embedding():
  """Test invoking AttnLSTMEmbedding."""
  max_depth = 5
  n_test = 5
  n_support = 11
  n_feat = 10
  test = np.random.rand(n_test, n_feat).astype(np.float32)
  support = np.random.rand(n_support, n_feat).astype(np.float32)
  layer = layers.AttnLSTMEmbedding(n_test, n_support, n_feat, max_depth)
  test_out, support_out = layer([test, support])
  assert test_out.shape == (n_test, n_feat)
  assert support_out.shape == (n_support, n_feat)
  assert len(layer.trainable_variables) == 4


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag653')" href="javascript:;">
deepchem-2.6.1/deepchem/models/tests/test_layers.py: 342-356
</a>
<div class="mid" id="frag653" style="display:none"><pre>
def test_iter_ref_lstm_embedding():
  """Test invoking IterRefLSTMEmbedding."""
  max_depth = 5
  n_test = 5
  n_support = 11
  n_feat = 10
  test = np.random.rand(n_test, n_feat).astype(np.float32)
  support = np.random.rand(n_support, n_feat).astype(np.float32)
  layer = layers.IterRefLSTMEmbedding(n_test, n_support, n_feat, max_depth)
  test_out, support_out = layer([test, support])
  assert test_out.shape == (n_test, n_feat)
  assert support_out.shape == (n_support, n_feat)
  assert len(layer.trainable_variables) == 8


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 18:</b> &nbsp; 2 fragments, nominal size 28 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag682')" href="javascript:;">
deepchem-2.6.1/deepchem/models/tests/test_mpnn.py: 87-120
</a>
<div class="mid" id="frag682" style="display:none"><pre>
def test_mpnn_reload():
  # load datasets
  featurizer = MolGraphConvFeaturizer(use_edges=True)
  tasks, dataset, transformers, metric = get_dataset(
      'classification', featurizer=featurizer)

  # initialize models
  n_tasks = len(tasks)
  model_dir = tempfile.mkdtemp()
  model = MPNNModel(
      mode='classification',
      n_tasks=n_tasks,
      model_dir=model_dir,
      batch_size=10,
      learning_rate=0.001)

  model.fit(dataset, nb_epoch=200)
  scores = model.evaluate(dataset, [metric], transformers)
  assert scores['mean-roc_auc_score'] &gt;= 0.80

  reloaded_model = MPNNModel(
      mode='classification',
      n_tasks=n_tasks,
      model_dir=model_dir,
      batch_size=10,
      learning_rate=0.001)
  reloaded_model.restore()

  pred_mols = ["CCCC", "CCCCCO", "CCCCC"]
  X_pred = featurizer(pred_mols)
  random_dataset = dc.data.NumpyDataset(X_pred)
  original_pred = model.predict(random_dataset)
  reload_pred = reloaded_model.predict(random_dataset)
  assert np.all(original_pred == reload_pred)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag770')" href="javascript:;">
deepchem-2.6.1/deepchem/models/tests/test_attentivefp.py: 86-119
</a>
<div class="mid" id="frag770" style="display:none"><pre>
def test_attentivefp_reload():
  # load datasets
  featurizer = MolGraphConvFeaturizer(use_edges=True)
  tasks, dataset, transformers, metric = get_dataset(
      'classification', featurizer=featurizer)

  # initialize models
  n_tasks = len(tasks)
  model_dir = tempfile.mkdtemp()
  model = AttentiveFPModel(
      mode='classification',
      n_tasks=n_tasks,
      model_dir=model_dir,
      batch_size=10,
      learning_rate=0.001)

  model.fit(dataset, nb_epoch=100)
  scores = model.evaluate(dataset, [metric], transformers)
  assert scores['mean-roc_auc_score'] &gt;= 0.85

  reloaded_model = AttentiveFPModel(
      mode='classification',
      n_tasks=n_tasks,
      model_dir=model_dir,
      batch_size=10,
      learning_rate=0.001)
  reloaded_model.restore()

  pred_mols = ["CCCC", "CCCCCO", "CCCCC"]
  X_pred = featurizer(pred_mols)
  random_dataset = dc.data.NumpyDataset(X_pred)
  original_pred = model.predict(random_dataset)
  reload_pred = reloaded_model.predict(random_dataset)
  assert np.all(original_pred == reload_pred)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 19:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1000')" href="javascript:;">
deepchem-2.6.1/deepchem/feat/tests/test_mol_graph_conv_featurizer.py: 43-58
</a>
<div class="mid" id="frag1000" style="display:none"><pre>
  def test_featurizer_with_use_chirality(self):
    smiles = ["C1=CC=CN=C1", "O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"]
    featurizer = MolGraphConvFeaturizer(use_chirality=True)
    graph_feat = featurizer.featurize(smiles)
    assert len(graph_feat) == 2

    # assert "C1=CC=CN=C1"
    assert graph_feat[0].num_nodes == 6
    assert graph_feat[0].num_node_features == 32
    assert graph_feat[0].num_edges == 12

    # assert "O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"
    assert graph_feat[1].num_nodes == 22
    assert graph_feat[1].num_node_features == 32
    assert graph_feat[1].num_edges == 44

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1001')" href="javascript:;">
deepchem-2.6.1/deepchem/feat/tests/test_mol_graph_conv_featurizer.py: 59-75
</a>
<div class="mid" id="frag1001" style="display:none"><pre>
  def test_featurizer_with_use_partial_charge(self):
    smiles = ["C1=CC=CN=C1", "O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"]
    featurizer = MolGraphConvFeaturizer(use_partial_charge=True)
    graph_feat = featurizer.featurize(smiles)
    assert len(graph_feat) == 2

    # assert "C1=CC=CN=C1"
    assert graph_feat[0].num_nodes == 6
    assert graph_feat[0].num_node_features == 31
    assert graph_feat[0].num_edges == 12

    # assert "O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"
    assert graph_feat[1].num_nodes == 22
    assert graph_feat[1].num_node_features == 31
    assert graph_feat[1].num_edges == 44


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 20:</b> &nbsp; 4 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1018')" href="javascript:;">
deepchem-2.6.1/deepchem/feat/tests/test_grid_featurizers.py: 5-19
</a>
<div class="mid" id="frag1018" style="display:none"><pre>
def test_charge_voxelizer():
  current_dir = os.path.dirname(os.path.realpath(__file__))
  protein_file = os.path.join(current_dir, 'data',
                              '3ws9_protein_fixer_rdkit.pdb')
  ligand_file = os.path.join(current_dir, 'data', '3ws9_ligand.sdf')

  cutoff = 4.5
  box_width = 20
  voxel_width = 1.0
  voxelizer = dc.feat.ChargeVoxelizer(
      cutoff=cutoff, box_width=box_width, voxel_width=voxel_width)
  features = voxelizer.featurize([(ligand_file, protein_file)])
  assert features.shape == (1, box_width, box_width, box_width, 1)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1019')" href="javascript:;">
deepchem-2.6.1/deepchem/feat/tests/test_grid_featurizers.py: 20-34
</a>
<div class="mid" id="frag1019" style="display:none"><pre>
def test_salt_bridge_voxelizer():
  current_dir = os.path.dirname(os.path.realpath(__file__))
  protein_file = os.path.join(current_dir, 'data',
                              '3ws9_protein_fixer_rdkit.pdb')
  ligand_file = os.path.join(current_dir, 'data', '3ws9_ligand.sdf')

  cutoff = 4.5
  box_width = 20
  voxel_width = 1.0
  voxelizer = dc.feat.SaltBridgeVoxelizer(
      cutoff=cutoff, box_width=box_width, voxel_width=voxel_width)
  features = voxelizer.featurize([(ligand_file, protein_file)])
  assert features.shape == (1, box_width, box_width, box_width, 1)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1021')" href="javascript:;">
deepchem-2.6.1/deepchem/feat/tests/test_grid_featurizers.py: 50-64
</a>
<div class="mid" id="frag1021" style="display:none"><pre>
def test_pi_stack_voxelizer():
  current_dir = os.path.dirname(os.path.realpath(__file__))
  protein_file = os.path.join(current_dir, 'data',
                              '3ws9_protein_fixer_rdkit.pdb')
  ligand_file = os.path.join(current_dir, 'data', '3ws9_ligand.sdf')

  cutoff = 4.5
  box_width = 20
  voxel_width = 1.0
  voxelizer = dc.feat.PiStackVoxelizer(
      cutoff=cutoff, box_width=box_width, voxel_width=voxel_width)
  features = voxelizer.featurize([(ligand_file, protein_file)])
  assert features.shape == (1, box_width, box_width, box_width, 2)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1020')" href="javascript:;">
deepchem-2.6.1/deepchem/feat/tests/test_grid_featurizers.py: 35-49
</a>
<div class="mid" id="frag1020" style="display:none"><pre>
def test_cation_pi_voxelizer():
  current_dir = os.path.dirname(os.path.realpath(__file__))
  protein_file = os.path.join(current_dir, 'data',
                              '3ws9_protein_fixer_rdkit.pdb')
  ligand_file = os.path.join(current_dir, 'data', '3ws9_ligand.sdf')

  cutoff = 4.5
  box_width = 20
  voxel_width = 1.0
  voxelizer = dc.feat.CationPiVoxelizer(
      cutoff=cutoff, box_width=box_width, voxel_width=voxel_width)
  features = voxelizer.featurize([(ligand_file, protein_file)])
  assert features.shape == (1, box_width, box_width, box_width, 1)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 21:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1043')" href="javascript:;">
deepchem-2.6.1/deepchem/feat/tests/test_coulomb_matrices.py: 16-41
</a>
<div class="mid" id="frag1043" style="display:none"><pre>
  def setUp(self):
    """
    Set up tests.
    """
    from rdkit import Chem
    from rdkit.Chem import AllChem
    smiles = 'CC(=O)OC1=CC=CC=C1C(=O)O'
    mol = Chem.MolFromSmiles(smiles)
    self.mol_with_no_conf = mol

    # with one conformer
    mol_with_one_conf = Chem.AddHs(mol)
    AllChem.EmbedMolecule(mol_with_one_conf, AllChem.ETKDG())
    self.mol_with_one_conf = mol_with_one_conf

    # with multiple conformers
    self.num_confs = 4
    engine = conformers.ConformerGenerator(max_conformers=self.num_confs)
    self.mol_with_multi_conf = engine.generate_conformers(mol)

    # include explicit hydrogens
    self.num_atoms = mol_with_one_conf.GetNumAtoms()
    assert self.num_atoms == 21
    assert self.mol_with_one_conf.GetNumConformers() == 1
    assert self.mol_with_multi_conf.GetNumConformers() == self.num_confs

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1050')" href="javascript:;">
deepchem-2.6.1/deepchem/feat/tests/test_coulomb_matrices.py: 130-155
</a>
<div class="mid" id="frag1050" style="display:none"><pre>
  def setUp(self):
    """
    Set up tests.
    """
    from rdkit import Chem
    from rdkit.Chem import AllChem
    smiles = 'CC(=O)OC1=CC=CC=C1C(=O)O'
    mol = Chem.MolFromSmiles(smiles)
    self.mol_with_no_conf = mol

    # with one conformer
    mol_with_one_conf = Chem.AddHs(mol)
    AllChem.EmbedMolecule(mol_with_one_conf, AllChem.ETKDG())
    self.mol_with_one_conf = mol_with_one_conf

    # with multiple conformers
    self.num_confs = 4
    engine = conformers.ConformerGenerator(max_conformers=self.num_confs)
    self.mol_with_multi_conf = engine.generate_conformers(mol)

    # include explicit hydrogens
    self.num_atoms = mol_with_one_conf.GetNumAtoms()
    assert self.num_atoms == 21
    assert self.mol_with_one_conf.GetNumConformers() == 1
    assert self.mol_with_multi_conf.GetNumConformers() == self.num_confs

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 22:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1147')" href="javascript:;">
deepchem-2.6.1/deepchem/rl/a2c.py: 345-361
</a>
<div class="mid" id="frag1147" style="display:none"><pre>
  def _predict_outputs(self, state, use_saved_states, save_states):
    """Compute a set of outputs for a state. """
    if not self._state_is_list:
      state = [state]
    if use_saved_states:
      state = state + list(self._rnn_states)
    else:
      state = state + list(self._policy.rnn_initial_states)
    inputs = [np.expand_dims(s, axis=0) for s in state]
    results = self._compute_model(inputs)
    results = [r.numpy() for r in results]
    if save_states:
      self._rnn_states = [
          np.squeeze(results[i], 0) for i in self._rnn_final_state_indices
      ]
    return results

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1237')" href="javascript:;">
deepchem-2.6.1/deepchem/rl/ppo.py: 367-383
</a>
<div class="mid" id="frag1237" style="display:none"><pre>
  def _predict_outputs(self, state, use_saved_states, save_states):
    """Compute a set of outputs for a state. """
    if not self._state_is_list:
      state = [state]
    if use_saved_states:
      state = state + list(self._rnn_states)
    else:
      state = state + list(self._policy.rnn_initial_states)
    inputs = [np.expand_dims(s, axis=0) for s in state]
    results = self._compute_model(inputs)
    results = [r.numpy() for r in results]
    if save_states:
      self._rnn_states = [
          np.squeeze(results[i], 0) for i in self._rnn_final_state_indices
      ]
    return results

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 23:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1169')" href="javascript:;">
deepchem-2.6.1/deepchem/rl/tests/test_ppo.py: 60-76
</a>
<div class="mid" id="frag1169" style="display:none"><pre>
      def create_model(self, **kwargs):

        class TestModel(tf.keras.Model):

          def __init__(self):
            super(TestModel, self).__init__(**kwargs)
            self.action = tf.Variable(np.ones(env.n_actions, np.float32))
            self.value = tf.Variable([0.0], tf.float32)

          def call(self, inputs, **kwargs):
            prob = tf.nn.softmax(tf.reshape(self.action, (-1, env.n_actions)))
            return (prob, self.value)

        return TestModel()

    # Optimize it.

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1190')" href="javascript:;">
deepchem-2.6.1/deepchem/rl/tests/test_a2c.py: 60-76
</a>
<div class="mid" id="frag1190" style="display:none"><pre>
      def create_model(self, **kwargs):

        class TestModel(tf.keras.Model):

          def __init__(self):
            super(TestModel, self).__init__(**kwargs)
            self.action = tf.Variable(np.ones(env.n_actions, np.float32))
            self.value = tf.Variable([0.0], tf.float32)

          def call(self, inputs, **kwargs):
            prob = tf.nn.softmax(tf.reshape(self.action, (-1, env.n_actions)))
            return (prob, self.value)

        return TestModel()

    # Optimize it.

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 24:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1177')" href="javascript:;">
deepchem-2.6.1/deepchem/rl/tests/test_ppo.py: 137-151
</a>
<div class="mid" id="frag1177" style="display:none"><pre>
      def create_model(self, **kwargs):
        state = Input(shape=(10,))
        rnn_state = Input(shape=(10,))
        reshaped = Reshape((1, 10))(state)
        gru, rnn_final_state = GRU(
            10, return_state=True, return_sequences=True, time_major=True)(
                reshaped, initial_state=rnn_state)
        output = Softmax()(Reshape((10,))(gru))
        value = dc.models.layers.Variable([0.0])([state])
        return tf.keras.Model(
            inputs=[state, rnn_state], outputs=[output, value, rnn_final_state])

    # We don't care about actually optimizing it, so just run a few rollouts to make
    # sure fit() doesn't crash, then check the behavior of the GRU state.

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1198')" href="javascript:;">
deepchem-2.6.1/deepchem/rl/tests/test_a2c.py: 136-150
</a>
<div class="mid" id="frag1198" style="display:none"><pre>
      def create_model(self, **kwargs):
        state = Input(shape=(10,))
        rnn_state = Input(shape=(10,))
        reshaped = Reshape((1, 10))(state)
        gru, rnn_final_state = GRU(
            10, return_state=True, return_sequences=True, time_major=True)(
                reshaped, initial_state=rnn_state)
        output = Softmax()(Reshape((10,))(gru))
        value = dc.models.layers.Variable([0.0])([state])
        return tf.keras.Model(
            inputs=[state, rnn_state], outputs=[output, value, rnn_final_state])

    # We don't care about actually optimizing it, so just run a few rollouts to make
    # sure fit() doesn't crash, then check the behavior of the GRU state.

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 25:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1181')" href="javascript:;">
deepchem-2.6.1/deepchem/rl/tests/test_ppo.py: 197-209
</a>
<div class="mid" id="frag1181" style="display:none"><pre>
      def step(self, action):
        new_state = self._state.copy()
        new_state[:2] += self.moves[action]
        self._state = new_state
        self.count += 1
        reward = 0
        if np.array_equal(new_state[:2], new_state[2:]):
          self._terminated = True
          reward = 1
        elif self.count == 1000:
          self._terminated = True
        return reward

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1202')" href="javascript:;">
deepchem-2.6.1/deepchem/rl/tests/test_a2c.py: 196-208
</a>
<div class="mid" id="frag1202" style="display:none"><pre>
      def step(self, action):
        new_state = self._state.copy()
        new_state[:2] += self.moves[action]
        self._state = new_state
        self.count += 1
        reward = 0
        if np.array_equal(new_state[:2], new_state[2:]):
          self._terminated = True
          reward = 1
        elif self.count == 1000:
          self._terminated = True
        return reward

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 26:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1182')" href="javascript:;">
deepchem-2.6.1/deepchem/rl/tests/test_ppo.py: 210-227
</a>
<div class="mid" id="frag1182" style="display:none"><pre>
      def apply_hindsight(self, states, actions, goal):
        new_states = []
        rewards = []
        goal_pos = goal[:2]
        for state, action in zip(states, actions):
          new_state = state.copy()
          new_state[2:] = goal_pos
          new_states.append(new_state)
          pos_after_action = new_state[:2] + self.moves[action]
          if np.array_equal(pos_after_action, goal_pos):
            rewards.append(1)
            break
          else:
            rewards.append(0)
        return new_states, rewards

    # A simple policy with two hidden layers.

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1203')" href="javascript:;">
deepchem-2.6.1/deepchem/rl/tests/test_a2c.py: 209-226
</a>
<div class="mid" id="frag1203" style="display:none"><pre>
      def apply_hindsight(self, states, actions, goal):
        new_states = []
        rewards = []
        goal_pos = goal[:2]
        for state, action in zip(states, actions):
          new_state = state.copy()
          new_state[2:] = goal_pos
          new_states.append(new_state)
          pos_after_action = new_state[:2] + self.moves[action]
          if np.array_equal(pos_after_action, goal_pos):
            rewards.append(1)
            break
          else:
            rewards.append(0)
        return new_states, rewards

    # A simple policy with two hidden layers.

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 27:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1293')" href="javascript:;">
deepchem-2.6.1/deepchem/data/tests/test_property.py: 5-18
</a>
<div class="mid" id="frag1293" style="display:none"><pre>
def test_y_property():
  """Test that dataset.y works."""
  num_datapoints = 10
  num_features = 10
  num_tasks = 1
  X = np.random.rand(num_datapoints, num_features)
  y = np.random.randint(2, size=(num_datapoints, num_tasks))
  w = np.ones((num_datapoints, num_tasks))
  ids = np.array(["id"] * num_datapoints)
  dataset = dc.data.DiskDataset.from_numpy(X, y, w, ids)
  y_out = dataset.y
  np.testing.assert_array_equal(y, y_out)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1294')" href="javascript:;">
deepchem-2.6.1/deepchem/data/tests/test_property.py: 19-30
</a>
<div class="mid" id="frag1294" style="display:none"><pre>
def test_w_property():
  """Test that dataset.y works."""
  num_datapoints = 10
  num_features = 10
  num_tasks = 1
  X = np.random.rand(num_datapoints, num_features)
  y = np.random.randint(2, size=(num_datapoints, num_tasks))
  w = np.ones((num_datapoints, num_tasks))
  ids = np.array(["id"] * num_datapoints)
  dataset = dc.data.DiskDataset.from_numpy(X, y, w, ids)
  w_out = dataset.w
  np.testing.assert_array_equal(w, w_out)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 28:</b> &nbsp; 2 fragments, nominal size 19 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1299')" href="javascript:;">
deepchem-2.6.1/deepchem/data/tests/test_shuffle.py: 30-51
</a>
<div class="mid" id="frag1299" style="display:none"><pre>
def test_complete_shuffle_multiple_shard():
  """Test that complete shuffle works with multiple shards."""
  X = np.random.rand(100, 10)
  dataset = dc.data.DiskDataset.from_numpy(X)
  dataset.reshard(shard_size=10)
  shuffled = dataset.complete_shuffle()
  assert len(shuffled) == len(dataset)
  assert not np.array_equal(shuffled.ids, dataset.ids)
  assert sorted(shuffled.ids) == sorted(dataset.ids)
  assert shuffled.X.shape == dataset.X.shape
  assert shuffled.y.shape == dataset.y.shape
  assert shuffled.w.shape == dataset.w.shape
  original_indices = dict((id, i) for i, id in enumerate(dataset.ids))
  shuffled_indices = dict((id, i) for i, id in enumerate(shuffled.ids))
  for id in dataset.ids:
    i = original_indices[id]
    j = shuffled_indices[id]
    assert np.array_equal(dataset.X[i], shuffled.X[j])
    assert np.array_equal(dataset.y[i], shuffled.y[j])
    assert np.array_equal(dataset.w[i], shuffled.w[j])


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1300')" href="javascript:;">
deepchem-2.6.1/deepchem/data/tests/test_shuffle.py: 52-73
</a>
<div class="mid" id="frag1300" style="display:none"><pre>
def test_complete_shuffle_multiple_shard_uneven():
  """Test that complete shuffle works with multiple shards and some shards not full size."""
  X = np.random.rand(57, 10)
  dataset = dc.data.DiskDataset.from_numpy(X)
  dataset.reshard(shard_size=10)
  shuffled = dataset.complete_shuffle()
  assert len(shuffled) == len(dataset)
  assert not np.array_equal(shuffled.ids, dataset.ids)
  assert sorted(shuffled.ids) == sorted(dataset.ids)
  assert shuffled.X.shape == dataset.X.shape
  assert shuffled.y.shape == dataset.y.shape
  assert shuffled.w.shape == dataset.w.shape
  original_indices = dict((id, i) for i, id in enumerate(dataset.ids))
  shuffled_indices = dict((id, i) for i, id in enumerate(shuffled.ids))
  for id in dataset.ids:
    i = original_indices[id]
    j = shuffled_indices[id]
    assert np.array_equal(dataset.X[i], shuffled.X[j])
    assert np.array_equal(dataset.y[i], shuffled.y[j])
    assert np.array_equal(dataset.w[i], shuffled.w[j])


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 29:</b> &nbsp; 3 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1317')" href="javascript:;">
deepchem-2.6.1/deepchem/data/tests/test_datasets.py: 281-294
</a>
<div class="mid" id="frag1317" style="display:none"><pre>
  def shard_generator():
    for sz in shard_sizes:
      X_b = np.random.rand(sz, 1)
      y_b = np.random.rand(sz, 1)
      w_b = np.random.rand(sz, 1)
      ids_b = np.random.rand(sz)

      all_Xs.append(X_b)
      all_ys.append(y_b)
      all_ws.append(w_b)
      all_ids.append(ids_b)

      yield X_b, y_b, w_b, ids_b

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1327')" href="javascript:;">
deepchem-2.6.1/deepchem/data/tests/test_datasets.py: 443-456
</a>
<div class="mid" id="frag1327" style="display:none"><pre>

  def shard_generator():
    for sz in shard_sizes:
      X_b = np.random.rand(sz, 1)
      y_b = np.random.rand(sz, 1)
      w_b = np.random.rand(sz, 1)
      ids_b = np.random.rand(sz)

      all_Xs.append(X_b)
      all_ys.append(y_b)
      all_ws.append(w_b)
      all_ids.append(ids_b)

      yield X_b, y_b, w_b, ids_b
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1331')" href="javascript:;">
deepchem-2.6.1/deepchem/data/tests/test_datasets.py: 563-576
</a>
<div class="mid" id="frag1331" style="display:none"><pre>

    def shard_generator():
      for sz in shard_sizes:
        X_b = np.random.rand(sz, 1)
        y_b = np.random.rand(sz, 1)
        w_b = np.random.rand(sz, 1)
        ids_b = np.random.rand(sz)

        all_Xs.append(X_b)
        all_ys.append(y_b)
        all_ws.append(w_b)
        all_ids.append(ids_b)

        yield X_b, y_b, w_b, ids_b
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 30:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1387')" href="javascript:;">
deepchem-2.6.1/deepchem/data/tests/test_data_loader.py: 21-42
</a>
<div class="mid" id="frag1387" style="display:none"><pre>
def test_scaffold_test_train_valid_test_split():
  """Test of singletask RF ECFP regression API."""
  current_dir = os.path.dirname(os.path.abspath(__file__))
  tasks = ["log-solubility"]
  input_file = os.path.join(current_dir, "../../models/tests/example.csv")
  featurizer = dc.feat.CircularFingerprint(size=1024)

  input_file = os.path.join(current_dir, input_file)
  loader = dc.data.CSVLoader(
      tasks=tasks, feature_field="smiles", featurizer=featurizer)

  dataset = loader.create_dataset(input_file)

  # Splits featurized samples into train/test
  splitter = dc.splits.ScaffoldSplitter()
  train_dataset, valid_dataset, test_dataset = splitter.train_valid_test_split(
      dataset)
  assert len(train_dataset) == 8
  assert len(valid_dataset) == 1
  assert len(test_dataset) == 1


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1389')" href="javascript:;">
deepchem-2.6.1/deepchem/data/tests/test_data_loader.py: 63-84
</a>
<div class="mid" id="frag1389" style="display:none"><pre>
def test_random_test_train_valid_test_split():
  """Test of singletask RF ECFP regression API."""
  current_dir = os.path.dirname(os.path.abspath(__file__))
  tasks = ["log-solubility"]
  input_file = os.path.join(current_dir, "../../models/tests/example.csv")
  featurizer = dc.feat.CircularFingerprint(size=1024)

  input_file = os.path.join(current_dir, input_file)
  loader = dc.data.CSVLoader(
      tasks=tasks, feature_field="smiles", featurizer=featurizer)

  dataset = loader.create_dataset(input_file)

  # Splits featurized samples into train/test
  splitter = dc.splits.RandomSplitter()
  train_dataset, valid_dataset, test_dataset = splitter.train_valid_test_split(
      dataset)
  assert len(train_dataset) == 8
  assert len(valid_dataset) == 1
  assert len(test_dataset) == 1


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 31:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1534')" href="javascript:;">
deepchem-2.6.1/deepchem/utils/test/test_pdbqt_utils.py: 41-59
</a>
<div class="mid" id="frag1534" style="display:none"><pre>
  def test_convert_mol_to_pdbqt(self):
    """Test that a ligand molecule can be coverted to PDBQT."""
    from rdkit import Chem
    xyz, mol = rdkit_utils.load_molecule(
        self.ligand_file, calc_charges=False, add_hydrogens=False)
    with tempfile.TemporaryDirectory() as tmp:
      outfile = os.path.join(tmp, "mol.pdbqt")
      writer = Chem.PDBWriter(outfile)
      writer.write(mol)
      writer.close()
      pdbqt_utils.convert_mol_to_pdbqt(mol, outfile)
      pdbqt_xyz, pdbqt_mol = rdkit_utils.load_molecule(
          outfile, add_hydrogens=False, calc_charges=False)
    assert pdbqt_mol.GetNumAtoms() == pdbqt_mol.GetNumAtoms()
    for atom_idx in range(pdbqt_mol.GetNumAtoms()):
      atom1 = pdbqt_mol.GetAtoms()[atom_idx]
      atom2 = pdbqt_mol.GetAtoms()[atom_idx]
      assert atom1.GetAtomicNum() == atom2.GetAtomicNum()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1535')" href="javascript:;">
deepchem-2.6.1/deepchem/utils/test/test_pdbqt_utils.py: 60-77
</a>
<div class="mid" id="frag1535" style="display:none"><pre>
  def test_convert_protein_to_pdbqt(self):
    """Test a protein in a PDB can be converted to PDBQT."""
    from rdkit import Chem
    xyz, mol = rdkit_utils.load_molecule(
        self.protein_file, calc_charges=False, add_hydrogens=False)
    with tempfile.TemporaryDirectory() as tmp:
      outfile = os.path.join(tmp, "mol.pdbqt")
      writer = Chem.PDBWriter(outfile)
      writer.write(mol)
      writer.close()
      pdbqt_utils.convert_protein_to_pdbqt(mol, outfile)
      pdbqt_xyz, pdbqt_mol = rdkit_utils.load_molecule(
          outfile, add_hydrogens=False, calc_charges=False)
    assert pdbqt_mol.GetNumAtoms() == pdbqt_mol.GetNumAtoms()
    for atom_idx in range(pdbqt_mol.GetNumAtoms()):
      atom1 = pdbqt_mol.GetAtoms()[atom_idx]
      atom2 = pdbqt_mol.GetAtoms()[atom_idx]
      assert atom1.GetAtomicNum() == atom2.GetAtomicNum()
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 32:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1539')" href="javascript:;">
deepchem-2.6.1/deepchem/utils/test/test_noncovalent_utils.py: 49-69
</a>
<div class="mid" id="frag1539" style="display:none"><pre>
  def test_is_pi_parallel(self):
    ring1_center = np.array([0.0, 0.0, 0.0])
    ring2_center_true = np.array([4.0, 0.0, 0.0])
    ring2_center_false = np.array([10.0, 0.0, 0.0])
    ring1_normal_true = np.array([1.0, 0.0, 0.0])
    ring1_normal_false = np.array([0.0, 1.0, 0.0])

    for ring2_normal in (np.array([2.0, 0, 0]), np.array([-3.0, 0, 0])):
      # parallel normals
      self.assertTrue(
          is_pi_parallel(ring1_center, ring1_normal_true, ring2_center_true,
                         ring2_normal))
      # perpendicular normals
      self.assertFalse(
          is_pi_parallel(ring1_center, ring1_normal_false, ring2_center_true,
                         ring2_normal))
      # too far away
      self.assertFalse(
          is_pi_parallel(ring1_center, ring1_normal_true, ring2_center_false,
                         ring2_normal))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1540')" href="javascript:;">
deepchem-2.6.1/deepchem/utils/test/test_noncovalent_utils.py: 70-90
</a>
<div class="mid" id="frag1540" style="display:none"><pre>
  def test_is_pi_t(self):
    ring1_center = np.array([0.0, 0.0, 0.0])
    ring2_center_true = np.array([4.0, 0.0, 0.0])
    ring2_center_false = np.array([10.0, 0.0, 0.0])
    ring1_normal_true = np.array([0.0, 1.0, 0.0])
    ring1_normal_false = np.array([1.0, 0.0, 0.0])

    for ring2_normal in (np.array([2.0, 0, 0]), np.array([-3.0, 0, 0])):
      # perpendicular normals
      self.assertTrue(
          is_pi_t(ring1_center, ring1_normal_true, ring2_center_true,
                  ring2_normal))
      # parallel normals
      self.assertFalse(
          is_pi_t(ring1_center, ring1_normal_false, ring2_center_true,
                  ring2_normal))
      # too far away
      self.assertFalse(
          is_pi_t(ring1_center, ring1_normal_true, ring2_center_false,
                  ring2_normal))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 33:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1617')" href="javascript:;">
deepchem-2.6.1/deepchem/trans/tests/test_cdf_transform.py: 7-21
</a>
<div class="mid" id="frag1617" style="display:none"><pre>
def load_gaussian_cdf_data():
  """Load example with numbers sampled from Gaussian normal distribution.
     Each feature and task is a column of values that is sampled
     from a normal distribution of mean 0, stdev 1."""
  current_dir = os.path.dirname(os.path.abspath(__file__))
  features = ["feat0", "feat1"]
  featurizer = dc.feat.UserDefinedFeaturizer(features)
  tasks = ["task0", "task1"]
  input_file = os.path.join(current_dir,
                            "../../models/tests/gaussian_cdf_example.csv")
  loader = dc.data.UserCSVLoader(
      tasks=tasks, featurizer=featurizer, id_field="id")
  return loader.create_dataset(input_file)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1668')" href="javascript:;">
deepchem-2.6.1/deepchem/trans/tests/test_power.py: 6-20
</a>
<div class="mid" id="frag1668" style="display:none"><pre>
def load_gaussian_cdf_data():
  """Load example with numbers sampled from Gaussian normal distribution.
     Each feature and task is a column of values that is sampled
     from a normal distribution of mean 0, stdev 1."""
  current_dir = os.path.dirname(os.path.abspath(__file__))
  features = ["feat0", "feat1"]
  featurizer = dc.feat.UserDefinedFeaturizer(features)
  tasks = ["task0", "task1"]
  input_file = os.path.join(current_dir,
                            "../../models/tests/gaussian_cdf_example.csv")
  loader = dc.data.UserCSVLoader(
      tasks=tasks, featurizer=featurizer, id_field="id")
  return loader.create_dataset(input_file)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 34:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1650')" href="javascript:;">
deepchem-2.6.1/deepchem/trans/tests/test_log_transform.py: 32-56
</a>
<div class="mid" id="frag1650" style="display:none"><pre>
def test_y_log_transformer():
  """Tests logarithmic data transformer."""
  solubility_dataset = load_solubility_data()
  log_transformer = dc.trans.LogTransformer(
      transform_y=True, dataset=solubility_dataset)
  X, y, w, ids = (solubility_dataset.X, solubility_dataset.y,
                  solubility_dataset.w, solubility_dataset.ids)
  solubility_dataset = log_transformer.transform(solubility_dataset)
  X_t, y_t, w_t, ids_t = (solubility_dataset.X, solubility_dataset.y,
                          solubility_dataset.w, solubility_dataset.ids)

  # Check ids are unchanged.
  for id_elt, id_t_elt in zip(ids, ids_t):
    assert id_elt == id_t_elt
  # Check X is unchanged since this is a y transformer
  np.testing.assert_allclose(X, X_t)
  # Check w is unchanged since this is a y transformer
  np.testing.assert_allclose(w, w_t)
  # Check y is now a logarithmic version of itself
  np.testing.assert_allclose(y_t, np.log(y + 1))

  # Check that untransform does the right thing.
  np.testing.assert_allclose(log_transformer.untransform(y_t), y)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1651')" href="javascript:;">
deepchem-2.6.1/deepchem/trans/tests/test_log_transform.py: 57-81
</a>
<div class="mid" id="frag1651" style="display:none"><pre>
def test_X_log_transformer():
  """Tests logarithmic data transformer."""
  solubility_dataset = load_solubility_data()
  log_transformer = dc.trans.LogTransformer(
      transform_X=True, dataset=solubility_dataset)
  X, y, w, ids = (solubility_dataset.X, solubility_dataset.y,
                  solubility_dataset.w, solubility_dataset.ids)
  solubility_dataset = log_transformer.transform(solubility_dataset)
  X_t, y_t, w_t, ids_t = (solubility_dataset.X, solubility_dataset.y,
                          solubility_dataset.w, solubility_dataset.ids)

  # Check ids are unchanged.
  for id_elt, id_t_elt in zip(ids, ids_t):
    assert id_elt == id_t_elt
  # Check y is unchanged since this is a X transformer
  np.testing.assert_allclose(y, y_t)
  # Check w is unchanged since this is a y transformer
  np.testing.assert_allclose(w, w_t)
  # Check y is now a logarithmic version of itself
  np.testing.assert_allclose(X_t, np.log(X + 1))

  # Check that untransform does the right thing.
  np.testing.assert_allclose(log_transformer.untransform(X_t), X)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 35:</b> &nbsp; 8 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1673')" href="javascript:;">
deepchem-2.6.1/deepchem/molnet/load_function/bbbp_datasets.py: 25-84
</a>
<div class="mid" id="frag1673" style="display:none"><pre>
def load_bbbp(
    featurizer: Union[dc.feat.Featurizer, str] = 'ECFP',
    splitter: Union[dc.splits.Splitter, str, None] = 'scaffold',
    transformers: List[Union[TransformerGenerator, str]] = ['balancing'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load BBBP dataset

  The blood-brain barrier penetration (BBBP) dataset is designed for the
  modeling and prediction of barrier permeability. As a membrane separating
  circulating blood and brain extracellular fluid, the blood-brain barrier
  blocks most drugs, hormones and neurotransmitters. Thus penetration of the
  barrier forms a long-standing issue in development of drugs targeting
  central nervous system.

  This dataset includes binary labels for over 2000 compounds on their
  permeability properties.

  Scaffold splitting is recommended for this dataset.

  The raw data csv file contains columns below:

  - "name" - Name of the compound
  - "smiles" - SMILES representation of the molecular structure
  - "p_np" - Binary labels for penetration/non-penetration

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in

  References
  ----------
  .. [1] Martins, Ines Filipa, et al. "A Bayesian approach to in silico
     blood-brain barrier penetration modeling." Journal of chemical
     information and modeling 52.6 (2012): 1686-1697.
  """
  loader = _BBBPLoader(featurizer, splitter, transformers, BBBP_TASKS, data_dir,
                       save_dir, **kwargs)
  return loader.load_dataset('bbbp', reload)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1702')" href="javascript:;">
deepchem-2.6.1/deepchem/molnet/load_function/hiv_datasets.py: 25-82
</a>
<div class="mid" id="frag1702" style="display:none"><pre>
def load_hiv(
    featurizer: Union[dc.feat.Featurizer, str] = 'ECFP',
    splitter: Union[dc.splits.Splitter, str, None] = 'scaffold',
    transformers: List[Union[TransformerGenerator, str]] = ['balancing'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load HIV dataset

  The HIV dataset was introduced by the Drug Therapeutics
  Program (DTP) AIDS Antiviral Screen, which tested the ability
  to inhibit HIV replication for over 40,000 compounds.
  Screening results were evaluated and placed into three
  categories: confirmed inactive (CI),confirmed active (CA) and
  confirmed moderately active (CM). We further combine the
  latter two labels, making it a classification task between
  inactive (CI) and active (CA and CM).

  Scaffold splitting is recommended for this dataset.

  The raw data csv file contains columns below:

  - "smiles": SMILES representation of the molecular structure
  - "activity": Three-class labels for screening results: CI/CM/CA
  - "HIV_active": Binary labels for screening results: 1 (CA/CM) and 0 (CI)

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in

  References
  ----------
  .. [1] AIDS Antiviral Screen Data.
     https://wiki.nci.nih.gov/display/NCIDTPdata/AIDS+Antiviral+Screen+Data
  """
  loader = _HIVLoader(featurizer, splitter, transformers, HIV_TASKS, data_dir,
                      save_dir, **kwargs)
  return loader.load_dataset('hiv', reload)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1696')" href="javascript:;">
deepchem-2.6.1/deepchem/molnet/load_function/muv_datasets.py: 29-85
</a>
<div class="mid" id="frag1696" style="display:none"><pre>
def load_muv(
    featurizer: Union[dc.feat.Featurizer, str] = 'ECFP',
    splitter: Union[dc.splits.Splitter, str, None] = 'scaffold',
    transformers: List[Union[TransformerGenerator, str]] = ['balancing'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load MUV dataset

  The Maximum Unbiased Validation (MUV) group is a benchmark dataset selected
  from PubChem BioAssay by applying a refined nearest neighbor analysis.

  The MUV dataset contains 17 challenging tasks for around 90 thousand
  compounds and is specifically designed for validation of virtual screening
  techniques.

  Scaffold splitting is recommended for this dataset.

  The raw data csv file contains columns below:

  - "mol_id" - PubChem CID of the compound
  - "smiles" - SMILES representation of the molecular structure
  - "MUV-XXX" - Measured results (Active/Inactive) for bioassays

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in

  References
  ----------
  .. [1] Rohrer, Sebastian G., and Knut Baumann. "Maximum unbiased validation
     (MUV) data sets for virtual screening based on PubChem bioactivity data."
     Journal of chemical information and modeling 49.2 (2009): 169-184.
  """
  loader = _MuvLoader(featurizer, splitter, transformers, MUV_TASKS, data_dir,
                      save_dir, **kwargs)
  return loader.load_dataset('muv', reload)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1709')" href="javascript:;">
deepchem-2.6.1/deepchem/molnet/load_function/nci_datasets.py: 37-72
</a>
<div class="mid" id="frag1709" style="display:none"><pre>
def load_nci(
    featurizer: Union[dc.feat.Featurizer, str] = 'ECFP',
    splitter: Union[dc.splits.Splitter, str, None] = 'random',
    transformers: List[Union[TransformerGenerator, str]] = ['normalization'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load NCI dataset.

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in
  """
  loader = _NCILoader(featurizer, splitter, transformers, NCI_TASKS, data_dir,
                      save_dir, **kwargs)
  return loader.load_dataset('nci', reload)
</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1742')" href="javascript:;">
deepchem-2.6.1/deepchem/molnet/load_function/ppb_datasets.py: 25-60
</a>
<div class="mid" id="frag1742" style="display:none"><pre>
def load_ppb(
    featurizer: Union[dc.feat.Featurizer, str] = 'ECFP',
    splitter: Union[dc.splits.Splitter, str, None] = 'scaffold',
    transformers: List[Union[TransformerGenerator, str]] = ['normalization'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load PPB datasets.

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in
  """
  loader = _PPBLoader(featurizer, splitter, transformers, PPB_TASKS, data_dir,
                      save_dir, **kwargs)
  return loader.load_dataset('ppb', reload)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1728')" href="javascript:;">
deepchem-2.6.1/deepchem/molnet/load_function/hppb_datasets.py: 28-63
</a>
<div class="mid" id="frag1728" style="display:none"><pre>
def load_hppb(
    featurizer: Union[dc.feat.Featurizer, str] = 'ECFP',
    splitter: Union[dc.splits.Splitter, str, None] = 'scaffold',
    transformers: List[Union[TransformerGenerator, str]] = ['log'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Loads the thermodynamic solubility datasets.

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in
  """
  loader = _HPPBLoader(featurizer, splitter, transformers, HPPB_TASKS, data_dir,
                       save_dir, **kwargs)
  return loader.load_dataset('hppb', reload)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1724')" href="javascript:;">
deepchem-2.6.1/deepchem/molnet/load_function/lipo_datasets.py: 25-78
</a>
<div class="mid" id="frag1724" style="display:none"><pre>
def load_lipo(
    featurizer: Union[dc.feat.Featurizer, str] = 'ECFP',
    splitter: Union[dc.splits.Splitter, str, None] = 'scaffold',
    transformers: List[Union[TransformerGenerator, str]] = ['normalization'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load Lipophilicity dataset

  Lipophilicity is an important feature of drug molecules that affects both
  membrane permeability and solubility. The lipophilicity dataset, curated
  from ChEMBL database, provides experimental results of octanol/water
  distribution coefficient (logD at pH 7.4) of 4200 compounds.

  Scaffold splitting is recommended for this dataset.

  The raw data csv file contains columns below:

  - "smiles" - SMILES representation of the molecular structure
  - "exp" - Measured octanol/water distribution coefficient (logD) of the
    compound, used as label

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in

  References
  ----------
  .. [1] Hersey, A. ChEMBL Deposited Data Set - AZ dataset; 2015.
     https://doi.org/10.6019/chembl3301361
  """
  loader = _LipoLoader(featurizer, splitter, transformers, LIPO_TASKS, data_dir,
                       save_dir, **kwargs)
  return loader.load_dataset('lipo', reload)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1726')" href="javascript:;">
deepchem-2.6.1/deepchem/molnet/load_function/hopv_datasets.py: 30-77
</a>
<div class="mid" id="frag1726" style="display:none"><pre>
def load_hopv(
    featurizer: Union[dc.feat.Featurizer, str] = 'ECFP',
    splitter: Union[dc.splits.Splitter, str, None] = 'scaffold',
    transformers: List[Union[TransformerGenerator, str]] = ['normalization'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load HOPV datasets. Does not do train/test split

  The HOPV datasets consist of the "Harvard Organic
  Photovoltaic Dataset. This dataset includes 350 small
  molecules and polymers that were utilized as p-type materials
  in OPVs. Experimental properties include: HOMO [a.u.], LUMO
  [a.u.], Electrochemical gap [a.u.], Optical gap [a.u.], Power
  conversion efficiency [%], Open circuit potential [V], Short
  circuit current density [mA/cm^2], and fill factor [%].
  Theoretical calculations in the original dataset have been
  removed (for now).

  Lopez, Steven A., et al. "The Harvard organic photovoltaic dataset." Scientific data 3.1 (2016): 1-7.

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in
  """
  loader = _HOPVLoader(featurizer, splitter, transformers, HOPV_TASKS, data_dir,
                       save_dir, **kwargs)
  return loader.load_dataset('hopv', reload)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 36:</b> &nbsp; 9 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1675')" href="javascript:;">
deepchem-2.6.1/deepchem/molnet/load_function/clintox_datasets.py: 26-97
</a>
<div class="mid" id="frag1675" style="display:none"><pre>
def load_clintox(
    featurizer: Union[dc.feat.Featurizer, str] = 'ECFP',
    splitter: Union[dc.splits.Splitter, str, None] = 'scaffold',
    transformers: List[Union[TransformerGenerator, str]] = ['balancing'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load ClinTox dataset

  The ClinTox dataset compares drugs approved by the FDA and
  drugs that have failed clinical trials for toxicity reasons.
  The dataset includes two classification tasks for 1491 drug
  compounds with known chemical structures:

  #. clinical trial toxicity (or absence of toxicity)
  #. FDA approval status.

  List of FDA-approved drugs are compiled from the SWEETLEAD
  database, and list of drugs that failed clinical trials for
  toxicity reasons are compiled from the Aggregate Analysis of
  ClinicalTrials.gov(AACT) database.

  Random splitting is recommended for this dataset.

  The raw data csv file contains columns below:

  - "smiles" - SMILES representation of the molecular structure
  - "FDA_APPROVED" - FDA approval status
  - "CT_TOX" - Clinical trial results

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in

  References
  ----------
  .. [1] Gayvert, Kaitlyn M., Neel S. Madhukar, and Olivier Elemento.
     "A data-driven approach to predicting successes and failures of clinical
     trials."
     Cell chemical biology 23.10 (2016): 1294-1301.
  .. [2] Artemov, Artem V., et al. "Integrated deep learned transcriptomic and
     structure-based predictor of clinical trials outcomes." bioRxiv (2016):
     095653.
  .. [3] Novick, Paul A., et al. "SWEETLEAD: an in silico database of approved
     drugs, regulated chemicals, and herbal isolates for computer-aided drug
     discovery." PloS one 8.11 (2013): e79568.
  .. [4] Aggregate Analysis of ClincalTrials.gov (AACT) Database.
     https://www.ctti-clinicaltrials.org/aact-database
  """
  loader = _ClintoxLoader(featurizer, splitter, transformers, CLINTOX_TASKS,
                          data_dir, save_dir, **kwargs)
  return loader.load_dataset('clintox', reload)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1730')" href="javascript:;">
deepchem-2.6.1/deepchem/molnet/load_function/sider_datasets.py: 42-100
</a>
<div class="mid" id="frag1730" style="display:none"><pre>
def load_sider(
    featurizer: Union[dc.feat.Featurizer, str] = 'ECFP',
    splitter: Union[dc.splits.Splitter, str, None] = 'scaffold',
    transformers: List[Union[TransformerGenerator, str]] = ['balancing'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load SIDER dataset

  The Side Effect Resource (SIDER) is a database of marketed
  drugs and adverse drug reactions (ADR). The version of the
  SIDER dataset in DeepChem has grouped drug side effects into
  27 system organ classes following MedDRA classifications
  measured for 1427 approved drugs.

  Random splitting is recommended for this dataset.

  The raw data csv file contains columns below:

  - "smiles": SMILES representation of the molecular structure
  - "Hepatobiliary disorders" ~ "Injury, poisoning and procedural
    complications": Recorded side effects for the drug. Please refer
    to http://sideeffects.embl.de/se/?page=98 for details on ADRs.

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in

  References
  ----------
  .. [1] Kuhn, Michael, et al. "The SIDER database of drugs and side effects."
     Nucleic acids research 44.D1 (2015): D1075-D1079.
  .. [2] Altae-Tran, Han, et al. "Low data drug discovery with one-shot
     learning." ACS central science 3.4 (2017): 283-293.
  .. [3] Medical Dictionary for Regulatory Activities. http://www.meddra.org/
  """
  loader = _SiderLoader(featurizer, splitter, transformers, SIDER_TASKS,
                        data_dir, save_dir, **kwargs)
  return loader.load_dataset('sider', reload)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1716')" href="javascript:;">
deepchem-2.6.1/deepchem/molnet/load_function/sweetlead_datasets.py: 26-71
</a>
<div class="mid" id="frag1716" style="display:none"><pre>
def load_sweet(
    featurizer: Union[dc.feat.Featurizer, str] = 'ECFP',
    splitter: Union[dc.splits.Splitter, str, None] = 'scaffold',
    transformers: List[Union[TransformerGenerator, str]] = ['balancing'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load sweet datasets.

  Sweetlead is a dataset of chemical structures for approved drugs, chemical isolates
  from traditional medicinal herbs, and regulated chemicals. Resulting structures are
  filtered for the active pharmaceutical ingredient, standardized, and differing
  formulations of the same drug were combined in the final database.

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in

  References
  ----------
  Novick, Paul A., et al. "SWEETLEAD: an in silico database of approved drugs, regulated
  chemicals, and herbal isolates for computer-aided drug discovery." PLoS One 8.11 (2013).
  """
  loader = _SweetLoader(featurizer, splitter, transformers, SWEETLEAD_TASKS,
                        data_dir, save_dir, **kwargs)
  return loader.load_dataset('sweet', reload)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1722')" href="javascript:;">
deepchem-2.6.1/deepchem/molnet/load_function/clearance_datasets.py: 26-62
</a>
<div class="mid" id="frag1722" style="display:none"><pre>
def load_clearance(
    featurizer: Union[dc.feat.Featurizer, str] = 'ECFP',
    splitter: Union[dc.splits.Splitter, str, None] = 'scaffold',
    transformers: List[Union[TransformerGenerator, str]] = ['log'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """
  Load clearance datasets.

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in
  """
  loader = _ClearanceLoader(featurizer, splitter, transformers, CLEARANCE_TASKS,
                            data_dir, save_dir, **kwargs)
  return loader.load_dataset('clearance', reload)
</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1740')" href="javascript:;">
deepchem-2.6.1/deepchem/molnet/load_function/chembl25_datasets.py: 47-82
</a>
<div class="mid" id="frag1740" style="display:none"><pre>
def load_chembl25(
    featurizer: Union[dc.feat.Featurizer, str] = 'ECFP',
    splitter: Union[dc.splits.Splitter, str, None] = 'scaffold',
    transformers: List[Union[TransformerGenerator, str]] = ['normalization'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Loads the ChEMBL25 dataset, featurizes it, and does a split.

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in
  """
  loader = _Chembl25Loader(featurizer, splitter, transformers, CHEMBL25_TASKS,
                           data_dir, save_dir, **kwargs)
  return loader.load_dataset('chembl25', reload)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1677')" href="javascript:;">
deepchem-2.6.1/deepchem/molnet/load_function/toxcast_datasets.py: 248-305
</a>
<div class="mid" id="frag1677" style="display:none"><pre>
def load_toxcast(
    featurizer: Union[dc.feat.Featurizer, str] = 'ECFP',
    splitter: Union[dc.splits.Splitter, str, None] = 'scaffold',
    transformers: List[Union[TransformerGenerator, str]] = ['balancing'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load Toxcast dataset

  ToxCast is an extended data collection from the same
  initiative as Tox21, providing toxicology data for a large
  library of compounds based on in vitro high-throughput
  screening. The processed collection includes qualitative
  results of over 600 experiments on 8k compounds.

  Random splitting is recommended for this dataset.

  The raw data csv file contains columns below:

  - "smiles": SMILES representation of the molecular structure
  - "ACEA_T47D_80hr_Negative" ~ "Tanguay_ZF_120hpf_YSE_up": Bioassays results.
    Please refer to the section "high-throughput assay information" at
    https://www.epa.gov/chemical-research/toxicity-forecaster-toxcasttm-data
    for details.

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in

  References
  ----------
  .. [1] Richard, Ann M., et al. "ToxCast chemical landscape: paving the road
     to 21st century toxicology." Chemical research in toxicology 29.8 (2016):
     1225-1251.
  """
  loader = _ToxcastLoader(featurizer, splitter, transformers, TOXCAST_TASKS,
                          data_dir, save_dir, **kwargs)
  return loader.load_dataset('toxcast', reload)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1747')" href="javascript:;">
deepchem-2.6.1/deepchem/molnet/load_function/tox21_datasets.py: 28-83
</a>
<div class="mid" id="frag1747" style="display:none"><pre>
def load_tox21(
    featurizer: Union[dc.feat.Featurizer, str] = 'ECFP',
    splitter: Union[dc.splits.Splitter, str, None] = 'scaffold',
    transformers: List[Union[TransformerGenerator, str]] = ['balancing'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load Tox21 dataset

  The "Toxicology in the 21st Century" (Tox21) initiative created a public
  database measuring toxicity of compounds, which has been used in the 2014
  Tox21 Data Challenge. This dataset contains qualitative toxicity measurements
  for 8k compounds on 12 different targets, including nuclear receptors and
  stress response pathways.

  Random splitting is recommended for this dataset.

  The raw data csv file contains columns below:

  - "smiles" - SMILES representation of the molecular structure
  - "NR-XXX" - Nuclear receptor signaling bioassays results
  - "SR-XXX" - Stress response bioassays results

  please refer to https://tripod.nih.gov/tox21/challenge/data.jsp for details.

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in

  References
  ----------
  .. [1] Tox21 Challenge. https://tripod.nih.gov/tox21/challenge/
  """
  loader = _Tox21Loader(featurizer, splitter, transformers, TOX21_TASKS,
                        data_dir, save_dir, **kwargs)
  return loader.load_dataset('tox21', reload)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1720')" href="javascript:;">
deepchem-2.6.1/deepchem/molnet/load_function/sampl_datasets.py: 25-82
</a>
<div class="mid" id="frag1720" style="display:none"><pre>
def load_sampl(
    featurizer: Union[dc.feat.Featurizer, str] = 'ECFP',
    splitter: Union[dc.splits.Splitter, str, None] = 'scaffold',
    transformers: List[Union[TransformerGenerator, str]] = ['normalization'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load SAMPL(FreeSolv) dataset

  The Free Solvation Database, FreeSolv(SAMPL), provides experimental and
  calculated hydration free energy of small molecules in water. The calculated
  values are derived from alchemical free energy calculations using molecular
  dynamics simulations. The experimental values are included in the benchmark
  collection.

  Random splitting is recommended for this dataset.

  The raw data csv file contains columns below:

  - "iupac" - IUPAC name of the compound
  - "smiles" - SMILES representation of the molecular structure
  - "expt" - Measured solvation energy (unit: kcal/mol) of the compound,
    used as label
  - "calc" - Calculated solvation energy (unit: kcal/mol) of the compound

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in

  References
  ----------
  .. [1] Mobley, David L., and J. Peter Guthrie. "FreeSolv: a database of
     experimental and calculated hydration free energies, with input files."
     Journal of computer-aided molecular design 28.7 (2014): 711-720.
  """
  loader = _SAMPLLoader(featurizer, splitter, transformers, SAMPL_TASKS,
                        data_dir, save_dir, **kwargs)
  return loader.load_dataset('sampl', reload)
</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1707')" href="javascript:;">
deepchem-2.6.1/deepchem/molnet/load_function/delaney_datasets.py: 25-80
</a>
<div class="mid" id="frag1707" style="display:none"><pre>
def load_delaney(
    featurizer: Union[dc.feat.Featurizer, str] = 'ECFP',
    splitter: Union[dc.splits.Splitter, str, None] = 'scaffold',
    transformers: List[Union[TransformerGenerator, str]] = ['normalization'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load Delaney dataset

  The Delaney (ESOL) dataset a regression dataset containing structures and
  water solubility data for 1128 compounds. The dataset is widely used to
  validate machine learning models on estimating solubility directly from
  molecular structures (as encoded in SMILES strings).

  Scaffold splitting is recommended for this dataset.

  The raw data csv file contains columns below:

  - "Compound ID" - Name of the compound
  - "smiles" - SMILES representation of the molecular structure
  - "measured log solubility in mols per litre" - Log-scale water solubility
    of the compound, used as label

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in

  References
  ----------
  .. [1] Delaney, John S. "ESOL: estimating aqueous solubility directly from
     molecular structure." Journal of chemical information and computer
     sciences 44.3 (2004): 1000-1005.
  """
  loader = _DelaneyLoader(featurizer, splitter, transformers, DELANEY_TASKS,
                          data_dir, save_dir, **kwargs)
  return loader.load_dataset('delaney', reload)
</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 37:</b> &nbsp; 4 fragments, nominal size 14 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1680')" href="javascript:;">
deepchem-2.6.1/deepchem/molnet/load_function/material_datasets/load_mp_formation_energy.py: 16-31
</a>
<div class="mid" id="frag1680" style="display:none"><pre>
  def create_dataset(self) -&gt; Dataset:
    dataset_file = os.path.join(self.data_dir, 'mp_formation_energy.json')
    targz_file = os.path.join(self.data_dir, 'mp_formation_energy.tar.gz')
    if not os.path.exists(dataset_file):
      if not os.path.exists(targz_file):
        dc.utils.data_utils.download_url(
            url=MPFORME_URL, dest_dir=self.data_dir)
      dc.utils.data_utils.untargz_file(targz_file, self.data_dir)
    loader = dc.data.JsonLoader(
        tasks=self.tasks,
        feature_field="structure",
        label_field="formation_energy",
        featurizer=self.featurizer)
    return loader.create_dataset(dataset_file)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1686')" href="javascript:;">
deepchem-2.6.1/deepchem/molnet/load_function/material_datasets/load_perovskite.py: 16-31
</a>
<div class="mid" id="frag1686" style="display:none"><pre>
  def create_dataset(self) -&gt; Dataset:
    dataset_file = os.path.join(self.data_dir, 'perovskite.json')
    targz_file = os.path.join(self.data_dir, 'perovskite.tar.gz')
    if not os.path.exists(dataset_file):
      if not os.path.exists(targz_file):
        dc.utils.data_utils.download_url(
            url=PEROVSKITE_URL, dest_dir=self.data_dir)
      dc.utils.data_utils.untargz_file(targz_file, self.data_dir)
    loader = dc.data.JsonLoader(
        tasks=self.tasks,
        feature_field="structure",
        label_field="formation_energy",
        featurizer=self.featurizer)
    return loader.create_dataset(dataset_file)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1684')" href="javascript:;">
deepchem-2.6.1/deepchem/molnet/load_function/material_datasets/load_bandgap.py: 16-31
</a>
<div class="mid" id="frag1684" style="display:none"><pre>
  def create_dataset(self) -&gt; Dataset:
    dataset_file = os.path.join(self.data_dir, 'expt_gap.json')
    targz_file = os.path.join(self.data_dir, 'expt_gap.tar.gz')
    if not os.path.exists(dataset_file):
      if not os.path.exists(targz_file):
        dc.utils.data_utils.download_url(
            url=BANDGAP_URL, dest_dir=self.data_dir)
      dc.utils.data_utils.untargz_file(targz_file, self.data_dir)
    loader = dc.data.JsonLoader(
        tasks=self.tasks,
        feature_field="composition",
        label_field="experimental_bandgap",
        featurizer=self.featurizer)
    return loader.create_dataset(dataset_file)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1682')" href="javascript:;">
deepchem-2.6.1/deepchem/molnet/load_function/material_datasets/load_mp_metallicity.py: 16-31
</a>
<div class="mid" id="frag1682" style="display:none"><pre>
  def create_dataset(self) -&gt; Dataset:
    dataset_file = os.path.join(self.data_dir, 'mp_is_metal.json')
    targz_file = os.path.join(self.data_dir, 'mp_is_metal.tar.gz')
    if not os.path.exists(dataset_file):
      if not os.path.exists(targz_file):
        dc.utils.data_utils.download_url(
            url=MPMETAL_URL, dest_dir=self.data_dir)
      dc.utils.data_utils.untargz_file(targz_file, self.data_dir)
    loader = dc.data.JsonLoader(
        tasks=self.tasks,
        feature_field="structure",
        label_field="is_metal",
        featurizer=self.featurizer)
    return loader.create_dataset(dataset_file)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 38:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1681')" href="javascript:;">
deepchem-2.6.1/deepchem/molnet/load_function/material_datasets/load_mp_formation_energy.py: 32-107
</a>
<div class="mid" id="frag1681" style="display:none"><pre>
def load_mp_formation_energy(
    featurizer: Union[dc.feat.Featurizer, str] = dc.feat.SineCoulombMatrix(),
    splitter: Union[dc.splits.Splitter, str, None] = 'random',
    transformers: List[Union[TransformerGenerator, str]] = ['normalization'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load mp formation energy dataset.

  Contains 132752 calculated formation energies and inorganic
  crystal structures from the Materials Project database. In benchmark
  studies, random forest models achieved a mean average error of
  0.116 eV/atom during five-folded nested cross validation on this
  dataset.

  For more details on the dataset see [1]_. For more details
  on previous benchmarks for this dataset, see [2]_.

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in

  Returns
  -------
  tasks, datasets, transformers : tuple
    tasks : list
      Column names corresponding to machine learning target variables.
    datasets : tuple
      train, validation, test splits of data as
      ``deepchem.data.datasets.Dataset`` instances.
    transformers : list
      ``deepchem.trans.transformers.Transformer`` instances applied
      to dataset.

  References
  ----------
  .. [1] A. Jain*, S.P. Ong*, et al. (*=equal contributions) The Materials Project:
     A materials genome approach to accelerating materials innovation APL Materials,
     2013, 1(1), 011002. doi:10.1063/1.4812323 (2013).
  .. [2] Dunn, A. et al. "Benchmarking Materials Property Prediction Methods: The Matbench
     Test Set and Automatminer Reference Algorithm." https://arxiv.org/abs/2005.00707 (2020)

  Examples
  --------
  &gt;&gt;&gt;
  &gt;&gt; import deepchem as dc
  &gt;&gt; tasks, datasets, transformers = dc.molnet.load_mp_formation_energy()
  &gt;&gt; train_dataset, val_dataset, test_dataset = datasets
  &gt;&gt; n_tasks = len(tasks)
  &gt;&gt; n_features = train_dataset.get_data_shape()[0]
  &gt;&gt; model = dc.models.MultitaskRegressor(n_tasks, n_features)

  """
  loader = _MPFormationLoader(featurizer, splitter, transformers, MPFORME_TASKS,
                              data_dir, save_dir, **kwargs)
  return loader.load_dataset('mp-forme', reload)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1718')" href="javascript:;">
deepchem-2.6.1/deepchem/molnet/load_function/freesolv_dataset.py: 25-81
</a>
<div class="mid" id="frag1718" style="display:none"><pre>
def load_freesolv(
    featurizer: Union[dc.feat.Featurizer, str] = dc.feat.MATFeaturizer(),
    splitter: Union[dc.splits.Splitter, str, None] = 'random',
    transformers: List[Union[TransformerGenerator, str]] = ['normalization'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load Freesolv dataset

  The FreeSolv dataset is a collection of experimental and calculated hydration
  free energies for small molecules in water, along with their experiemental values.
  Here, we are using a modified version of the dataset with the molecule smile string
  and the corresponding experimental hydration free energies.


  Random splitting is recommended for this dataset.

  The raw data csv file contains columns below:

  - "mol" - SMILES representation of the molecular structure
  - "y" - Experimental hydration free energy

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in

  References
  ----------
  .. [1] ukasz Maziarka, et al. "Molecule Attention Transformer." NeurIPS 2019
     arXiv:2002.08264v1 [cs.LG].
  .. [2] Mobley DL, Guthrie JP. FreeSolv:
     a database of experimental and calculated hydration free energies, with input files.
     J Comput Aided Mol Des. 2014;28(7):711-720. doi:10.1007/s10822-014-9747-x
  """
  loader = _FreesolvLoader(featurizer, splitter, transformers, FREESOLV_TASKS,
                           data_dir, save_dir, **kwargs)
  return loader.load_dataset('freesolv', reload)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 39:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1683')" href="javascript:;">
deepchem-2.6.1/deepchem/molnet/load_function/material_datasets/load_mp_metallicity.py: 32-107
</a>
<div class="mid" id="frag1683" style="display:none"><pre>
def load_mp_metallicity(
    featurizer: Union[dc.feat.Featurizer, str] = dc.feat.SineCoulombMatrix(),
    splitter: Union[dc.splits.Splitter, str, None] = 'random',
    transformers: List[Union[TransformerGenerator, str]] = ['balancing'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load mp formation energy dataset.

  Contains 106113 inorganic crystal structures from the Materials
  Project database labeled as metals or nonmetals. In benchmark
  studies, random forest models achieved a mean ROC-AUC of
  0.9 during five-folded nested cross validation on this
  dataset.

  For more details on the dataset see [1]_. For more details
  on previous benchmarks for this dataset, see [2]_.

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in

  Returns
  -------
  tasks, datasets, transformers : tuple
    tasks : list
      Column names corresponding to machine learning target variables.
    datasets : tuple
      train, validation, test splits of data as
      ``deepchem.data.datasets.Dataset`` instances.
    transformers : list
      ``deepchem.trans.transformers.Transformer`` instances applied
      to dataset.

  References
  ----------
  .. [1] A. Jain*, S.P. Ong*, et al. (*=equal contributions) The Materials Project:
     A materials genome approach to accelerating materials innovation APL Materials,
     2013, 1(1), 011002. doi:10.1063/1.4812323 (2013).
  .. [2] Dunn, A. et al. "Benchmarking Materials Property Prediction Methods: The Matbench
     Test Set and Automatminer Reference Algorithm." https://arxiv.org/abs/2005.00707 (2020)

  Examples
  --------
  &gt;&gt;&gt;
  &gt;&gt; import deepchem as dc
  &gt;&gt; tasks, datasets, transformers = dc.molnet.load_mp_metallicity()
  &gt;&gt; train_dataset, val_dataset, test_dataset = datasets
  &gt;&gt; n_tasks = len(tasks)
  &gt;&gt; n_features = train_dataset.get_data_shape()[0]
  &gt;&gt; model = dc.models.MultitaskRegressor(n_tasks, n_features)

  """
  loader = _MPMetallicityLoader(featurizer, splitter, transformers,
                                MPMETAL_TASKS, data_dir, save_dir, **kwargs)
  return loader.load_dataset('mp-metallicity', reload)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1687')" href="javascript:;">
deepchem-2.6.1/deepchem/molnet/load_function/material_datasets/load_perovskite.py: 32-104
</a>
<div class="mid" id="frag1687" style="display:none"><pre>
def load_perovskite(
    featurizer: Union[dc.feat.Featurizer, str] = dc.feat.CGCNNFeaturizer(),
    splitter: Union[dc.splits.Splitter, str, None] = 'random',
    transformers: List[Union[TransformerGenerator, str]] = ['normalization'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load perovskite dataset.

  Contains 18928 perovskite structures and their formation energies.
  In benchmark studies, random forest models and crystal graph
  neural networks achieved mean average error of 0.23 and 0.05 eV/atom,
  respectively, during five-fold nested cross validation on this
  dataset.

  For more details on the dataset see [1]_. For more details
  on previous benchmarks for this dataset, see [2]_.

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in

  Returns
  -------
  tasks, datasets, transformers : tuple
    tasks : list
      Column names corresponding to machine learning target variables.
    datasets : tuple
      train, validation, test splits of data as
      ``deepchem.data.datasets.Dataset`` instances.
    transformers : list
      ``deepchem.trans.transformers.Transformer`` instances applied
      to dataset.

  References
  ----------
  .. [1] Castelli, I. et al. "New cubic perovskites for one- and two-photon water splitting
     using the computational materials repository." Energy Environ. Sci., (2012), 5,
     9034-9043DOI: 10.1039/C2EE22341D.
  .. [2] Dunn, A. et al. "Benchmarking Materials Property Prediction Methods:
     The Matbench Test Set and Automatminer Reference Algorithm." https://arxiv.org/abs/2005.00707 (2020)

  Examples
  --------
  &gt;&gt;&gt; import deepchem as dc
  &gt;&gt;&gt; tasks, datasets, transformers = dc.molnet.load_perovskite()
  &gt;&gt;&gt; train_dataset, val_dataset, test_dataset = datasets
  &gt;&gt;&gt; model = dc.models.CGCNNModel(mode='regression', batch_size=32, learning_rate=0.001)

  """
  loader = _PerovskiteLoader(featurizer, splitter, transformers,
                             PEROVSKITE_TASKS, data_dir, save_dir, **kwargs)
  return loader.load_dataset('perovskite', reload)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 40:</b> &nbsp; 3 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1691')" href="javascript:;">
deepchem-2.6.1/deepchem/molnet/load_function/qm9_datasets.py: 31-123
</a>
<div class="mid" id="frag1691" style="display:none"><pre>
def load_qm9(
    featurizer: Union[dc.feat.Featurizer, str] = dc.feat.CoulombMatrix(29),
    splitter: Union[dc.splits.Splitter, str, None] = 'random',
    transformers: List[Union[TransformerGenerator, str]] = ['normalization'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load QM9 dataset

  QM9 is a comprehensive dataset that provides geometric, energetic,
  electronic and thermodynamic properties for a subset of GDB-17
  database, comprising 134 thousand stable organic molecules with up
  to 9 heavy atoms.  All molecules are modeled using density
  functional theory (B3LYP/6-31G(2df,p) based DFT).

  Random splitting is recommended for this dataset.

  The source data contain:

  - qm9.sdf: molecular structures
  - qm9.sdf.csv: tables for molecular properties

    - "mol_id" - Molecule ID (gdb9 index) mapping to the .sdf file
    - "A" - Rotational constant (unit: GHz)
    - "B" - Rotational constant (unit: GHz)
    - "C" - Rotational constant (unit: GHz)
    - "mu" - Dipole moment (unit: D)
    - "alpha" - Isotropic polarizability (unit: Bohr^3)
    - "homo" - Highest occupied molecular orbital energy (unit: Hartree)
    - "lumo" - Lowest unoccupied molecular orbital energy (unit: Hartree)
    - "gap" - Gap between HOMO and LUMO (unit: Hartree)
    - "r2" - Electronic spatial extent (unit: Bohr^2)
    - "zpve" - Zero point vibrational energy (unit: Hartree)
    - "u0" - Internal energy at 0K (unit: Hartree)
    - "u298" - Internal energy at 298.15K (unit: Hartree)
    - "h298" - Enthalpy at 298.15K (unit: Hartree)
    - "g298" - Free energy at 298.15K (unit: Hartree)
    - "cv" - Heat capavity at 298.15K (unit: cal/(mol*K))
    - "u0_atom" - Atomization energy at 0K (unit: kcal/mol)
    - "u298_atom" - Atomization energy at 298.15K (unit: kcal/mol)
    - "h298_atom" - Atomization enthalpy at 298.15K (unit: kcal/mol)
    - "g298_atom" - Atomization free energy at 298.15K (unit: kcal/mol)

  "u0_atom" ~ "g298_atom" (used in MoleculeNet) are calculated from the
  differences between "u0" ~ "g298" and sum of reference energies of all
  atoms in the molecules, as given in
  https://figshare.com/articles/Atomref%3A_Reference_thermochemical_energies_of_H%2C_C%2C_N%2C_O%2C_F_atoms./1057643

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in

  Note
  ----
  DeepChem 2.4.0 has turned on sanitization for this dataset by
  default.  For the QM9 dataset, this means that calling this
  function will return 132480 compounds instead of 133885 in the
  source dataset file. This appears to be due to valence
  specification mismatches in the dataset that weren't caught in
  earlier more lax versions of RDKit. Note that this may subtly
  affect benchmarking results on this dataset.

  References
  ----------
  .. [1] Blum, Lorenz C., and Jean-Louis Reymond. "970 million druglike small
     molecules for virtual screening in the chemical universe database GDB-13."
     Journal of the American Chemical Society 131.25 (2009): 8732-8733.
  .. [2] Ramakrishnan, Raghunathan, et al. "Quantum chemistry structures and
     properties of 134 kilo molecules." Scientific data 1 (2014): 140022.
  """
  loader = _QM9Loader(featurizer, splitter, transformers, QM9_TASKS, data_dir,
                      save_dir, **kwargs)
  return loader.load_dataset('qm9', reload)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1735')" href="javascript:;">
deepchem-2.6.1/deepchem/molnet/load_function/qm8_datasets.py: 32-116
</a>
<div class="mid" id="frag1735" style="display:none"><pre>
def load_qm8(
    featurizer: Union[dc.feat.Featurizer, str] = dc.feat.CoulombMatrix(26),
    splitter: Union[dc.splits.Splitter, str, None] = 'random',
    transformers: List[Union[TransformerGenerator, str]] = ['normalization'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load QM8 dataset

  QM8 is the dataset used in a study on modeling quantum
  mechanical calculations of electronic spectra and excited
  state energy of small molecules. Multiple methods, including
  time-dependent density functional theories (TDDFT) and
  second-order approximate coupled-cluster (CC2), are applied to
  a collection of molecules that include up to eight heavy atoms
  (also a subset of the GDB-17 database). In our collection,
  there are four excited state properties calculated by four
  different methods on 22 thousand samples:

  S0 -&gt; S1 transition energy E1 and the corresponding oscillator strength f1

  S0 -&gt; S2 transition energy E2 and the corresponding oscillator strength f2

  E1, E2, f1, f2 are in atomic units. f1, f2 are in length representation

  Random splitting is recommended for this dataset.

  The source data contain:

  - qm8.sdf: molecular structures
  - qm8.sdf.csv: tables for molecular properties

    - Column 1: Molecule ID (gdb9 index) mapping to the .sdf file
    - Columns 2-5: RI-CC2/def2TZVP
    - Columns 6-9: LR-TDPBE0/def2SVP
    - Columns 10-13: LR-TDPBE0/def2TZVP
    - Columns 14-17: LR-TDCAM-B3LYP/def2TZVP

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in

  Note
  ----
  DeepChem 2.4.0 has turned on sanitization for this dataset by
  default.  For the QM8 dataset, this means that calling this
  function will return 21747 compounds instead of 21786 in the source
  dataset file.  This appears to be due to valence specification
  mismatches in the dataset that weren't caught in earlier more lax
  versions of RDKit.  Note that this may subtly affect benchmarking
  results on this dataset.

  References
  ----------
  .. [1] Blum, Lorenz C., and Jean-Louis Reymond. "970 million druglike
     small molecules for virtual screening in the chemical universe database
     GDB-13." Journal of the American Chemical Society 131.25 (2009):
     8732-8733.
  .. [2] Ramakrishnan, Raghunathan, et al. "Electronic spectra from TDDFT
     and machine learning in chemical space." The Journal of chemical physics
     143.8 (2015): 084111.
  """
  loader = _QM8Loader(featurizer, splitter, transformers, QM8_TASKS, data_dir,
                      save_dir, **kwargs)
  return loader.load_dataset('qm8', reload)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1711')" href="javascript:;">
deepchem-2.6.1/deepchem/molnet/load_function/qm7_datasets.py: 30-107
</a>
<div class="mid" id="frag1711" style="display:none"><pre>
def load_qm7(
    featurizer: Union[dc.feat.Featurizer, str] = dc.feat.CoulombMatrix(23),
    splitter: Union[dc.splits.Splitter, str, None] = 'random',
    transformers: List[Union[TransformerGenerator, str]] = ['normalization'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load QM7 dataset

  QM7 is a subset of GDB-13 (a database of nearly 1 billion
  stable and synthetically accessible organic molecules)
  containing up to 7 heavy atoms C, N, O, and S. The 3D
  Cartesian coordinates of the most stable conformations and
  their atomization energies were determined using ab-initio
  density functional theory (PBE0/tier2 basis set). This dataset
  also provided Coulomb matrices as calculated in [Rupp et al.
  PRL, 2012]:

  Stratified splitting is recommended for this dataset.

  The data file (.mat format, we recommend using `scipy.io.loadmat`
  for python users to load this original data) contains five arrays:

  - "X" - (7165 x 23 x 23), Coulomb matrices
  - "T" - (7165), atomization energies (unit: kcal/mol)
  - "P" - (5 x 1433), cross-validation splits as used in [Montavon et al.
    NIPS, 2012]
  - "Z" - (7165 x 23), atomic charges
  - "R" - (7165 x 23 x 3), cartesian coordinate (unit: Bohr) of each atom in
    the molecules

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in

  Note
  ----
  DeepChem 2.4.0 has turned on sanitization for this dataset by
  default.  For the QM7 dataset, this means that calling this
  function will return 6838 compounds instead of 7160 in the source
  dataset file.  This appears to be due to valence specification
  mismatches in the dataset that weren't caught in earlier more lax
  versions of RDKit.  Note that this may subtly affect benchmarking
  results on this
  dataset.

  References
  ----------
  .. [1] Rupp, Matthias, et al. "Fast and accurate modeling of molecular
     atomization energies with machine learning." Physical review letters
     108.5 (2012): 058301.
  .. [2] Montavon, Grgoire, et al. "Learning invariant representations of
     molecules for atomization energy prediction." Advances in Neural
     Information Proccessing Systems. 2012.
  """
  loader = _QM7Loader(featurizer, splitter, transformers, QM7_TASKS, data_dir,
                      save_dir, **kwargs)
  return loader.load_dataset('qm7', reload)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 41:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1697')" href="javascript:;">
deepchem-2.6.1/deepchem/molnet/load_function/bbbc_datasets.py: 23-35
</a>
<div class="mid" id="frag1697" style="display:none"><pre>
  def create_dataset(self) -&gt; Dataset:
    dataset_file = os.path.join(self.data_dir, "BBBC001_v1_images_tif.zip")
    labels_file = os.path.join(self.data_dir, "BBBC001_v1_counts.txt")
    if not os.path.exists(dataset_file):
      dc.utils.data_utils.download_url(
          url=BBBC1_IMAGE_URL, dest_dir=self.data_dir)
    if not os.path.exists(labels_file):
      dc.utils.data_utils.download_url(
          url=BBBC1_LABEL_URL, dest_dir=self.data_dir)
    loader = dc.data.ImageLoader()
    return loader.create_dataset(dataset_file, in_memory=False)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1699')" href="javascript:;">
deepchem-2.6.1/deepchem/molnet/load_function/bbbc_datasets.py: 78-90
</a>
<div class="mid" id="frag1699" style="display:none"><pre>
  def create_dataset(self) -&gt; Dataset:
    dataset_file = os.path.join(self.data_dir, "BBBC002_v1_images.zip")
    labels_file = os.path.join(self.data_dir, "BBBC002_v1_counts.txt.txt")
    if not os.path.exists(dataset_file):
      dc.utils.data_utils.download_url(
          url=BBBC2_IMAGE_URL, dest_dir=self.data_dir)
    if not os.path.exists(labels_file):
      dc.utils.data_utils.download_url(
          url=BBBC2_LABEL_URL, dest_dir=self.data_dir)
    loader = dc.data.ImageLoader()
    return loader.create_dataset(dataset_file, in_memory=False)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 42:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1698')" href="javascript:;">
deepchem-2.6.1/deepchem/molnet/load_function/bbbc_datasets.py: 36-75
</a>
<div class="mid" id="frag1698" style="display:none"><pre>
def load_bbbc001(
    splitter: Union[dc.splits.Splitter, str, None] = 'index',
    transformers: List[Union[TransformerGenerator, str]] = [],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load BBBC001 dataset

  This dataset contains 6 images of human HT29 colon cancer cells. The task is
  to learn to predict the cell counts in these images. This dataset is too small
  to serve to train algorithms, but might serve as a good test dataset.
  https://data.broadinstitute.org/bbbc/BBBC001/

  Parameters
  ----------
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in
  """
  featurizer = dc.feat.UserDefinedFeaturizer([])  # Not actually used
  loader = _BBBC001Loader(featurizer, splitter, transformers, BBBC1_TASKS,
                          data_dir, save_dir, **kwargs)
  return loader.load_dataset('bbbc001', reload)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1700')" href="javascript:;">
deepchem-2.6.1/deepchem/molnet/load_function/bbbc_datasets.py: 91-129
</a>
<div class="mid" id="frag1700" style="display:none"><pre>
def load_bbbc002(
    splitter: Union[dc.splits.Splitter, str, None] = 'index',
    transformers: List[Union[TransformerGenerator, str]] = [],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """Load BBBC002 dataset

  This dataset contains data corresponding to 5 samples of Drosophilia Kc167
  cells. There are 10 fields of view for each sample, each an image of size
  512x512. Ground truth labels contain cell counts for this dataset. Full
  details about this dataset are present at
  https://data.broadinstitute.org/bbbc/BBBC002/.

  Parameters
  ----------
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in
  """
  featurizer = dc.feat.UserDefinedFeaturizer([])  # Not actually used
  loader = _BBBC002Loader(featurizer, splitter, transformers, BBBC2_TASKS,
                          data_dir, save_dir, **kwargs)
  return loader.load_dataset('bbbc002', reload)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 43:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1704')" href="javascript:;">
deepchem-2.6.1/deepchem/molnet/load_function/bace_datasets.py: 26-84
</a>
<div class="mid" id="frag1704" style="display:none"><pre>
def load_bace_regression(
    featurizer: Union[dc.feat.Featurizer, str] = 'ECFP',
    splitter: Union[dc.splits.Splitter, str, None] = 'scaffold',
    transformers: List[Union[TransformerGenerator, str]] = ['normalization'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """ Load BACE dataset, regression labels

  The BACE dataset provides quantitative IC50 and qualitative (binary label)
  binding results for a set of inhibitors of human beta-secretase 1 (BACE-1).

  All data are experimental values reported in scientific literature over the
  past decade, some with detailed crystal structures available. A collection
  of 1522 compounds is provided, along with the regression labels of IC50.

  Scaffold splitting is recommended for this dataset.

  The raw data csv file contains columns below:

  - "mol" - SMILES representation of the molecular structure
  - "pIC50" - Negative log of the IC50 binding affinity
  - "class" - Binary labels for inhibitor

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in

  References
  ----------
  .. [1] Subramanian, Govindan, et al. "Computational modeling of -secretase 1
     (BACE-1) inhibitors using ligand based approaches." Journal of chemical
     information and modeling 56.10 (2016): 1936-1949.
  """
  loader = _BaceLoader(featurizer, splitter, transformers,
                       BACE_REGRESSION_TASKS, data_dir, save_dir, **kwargs)
  return loader.load_dataset('bace_r', reload)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1705')" href="javascript:;">
deepchem-2.6.1/deepchem/molnet/load_function/bace_datasets.py: 85-122
</a>
<div class="mid" id="frag1705" style="display:none"><pre>
def load_bace_classification(
    featurizer: Union[dc.feat.Featurizer, str] = 'ECFP',
    splitter: Union[dc.splits.Splitter, str, None] = 'scaffold',
    transformers: List[Union[TransformerGenerator, str]] = ['balancing'],
    reload: bool = True,
    data_dir: Optional[str] = None,
    save_dir: Optional[str] = None,
    **kwargs
) -&gt; Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:
  """ Load BACE dataset, classification labels

  BACE dataset with classification labels ("class").

  Parameters
  ----------
  featurizer: Featurizer or str
    the featurizer to use for processing the data.  Alternatively you can pass
    one of the names from dc.molnet.featurizers as a shortcut.
  splitter: Splitter or str
    the splitter to use for splitting the data into training, validation, and
    test sets.  Alternatively you can pass one of the names from
    dc.molnet.splitters as a shortcut.  If this is None, all the data
    will be included in a single dataset.
  transformers: list of TransformerGenerators or strings
    the Transformers to apply to the data.  Each one is specified by a
    TransformerGenerator or, as a shortcut, one of the names from
    dc.molnet.transformers.
  reload: bool
    if True, the first call for a particular featurizer and splitter will cache
    the datasets to disk, and subsequent calls will reload the cached datasets.
  data_dir: str
    a directory to save the raw data in
  save_dir: str
    a directory to save the dataset in
  """
  loader = _BaceLoader(featurizer, splitter, transformers,
                       BACE_CLASSIFICATION_TASKS, data_dir, save_dir, **kwargs)
  return loader.load_dataset('bace_c', reload)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 44:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1759')" href="javascript:;">
deepchem-2.6.1/deepchem/molnet/tests/test_molnet.py: 32-48
</a>
<div class="mid" id="frag1759" style="display:none"><pre>
  def test_delaney_graphconvreg(self):
    """Tests molnet benchmarking on delaney with graphconvreg."""
    datasets = ['delaney']
    model = 'graphconvreg'
    split = 'random'
    out_path = tempfile.mkdtemp()
    metric = [dc.metrics.Metric(dc.metrics.pearson_r2_score, np.mean)]
    run_benchmark(
        datasets, str(model), metric=metric, split=split, out_path=out_path)
    with open(os.path.join(out_path, 'results.csv'), newline='\n') as f:
      reader = csv.reader(f)
      for lastrow in reader:
        pass
      assert lastrow[-4] == 'valid'
      assert float(lastrow[-3]) &gt; 0.65
    os.remove(os.path.join(out_path, 'results.csv'))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1760')" href="javascript:;">
deepchem-2.6.1/deepchem/molnet/tests/test_molnet.py: 51-70
</a>
<div class="mid" id="frag1760" style="display:none"><pre>
  def test_qm7_multitask(self):
    """Tests molnet benchmarking on qm7 with multitask network."""
    datasets = ['qm7']
    model = 'tf_regression_ft'
    split = 'random'
    out_path = tempfile.mkdtemp()
    metric = [dc.metrics.Metric(dc.metrics.pearson_r2_score, np.mean)]
    run_benchmark(
        datasets, str(model), metric=metric, split=split, out_path=out_path)
    with open(os.path.join(out_path, 'results.csv'), newline='\n') as f:
      reader = csv.reader(f)
      for lastrow in reader:
        pass
      assert lastrow[-4] == 'valid'
      # TODO For this dataset and model, the R2-scores are less than 0.3.
      # This has to be improved.
      # See: https://github.com/deepchem/deepchem/issues/2776
      assert float(lastrow[-3]) &gt; 0.15
    os.remove(os.path.join(out_path, 'results.csv'))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 45:</b> &nbsp; 2 fragments, nominal size 19 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1772')" href="javascript:;">
deepchem-2.6.1/deepchem/molnet/run_benchmark_models.py: 346-365
</a>
<div class="mid" id="frag1772" style="display:none"><pre>
    def model_builder(model_dir):
      import xgboost
      xgboost_model = xgboost.XGBClassifier(
          max_depth=max_depth,
          learning_rate=learning_rate,
          n_estimators=n_estimators,
          gamma=gamma,
          min_child_weight=min_child_weight,
          max_delta_step=max_delta_step,
          subsample=subsample,
          colsample_bytree=colsample_bytree,
          colsample_bylevel=colsample_bylevel,
          reg_alpha=reg_alpha,
          reg_lambda=reg_lambda,
          scale_pos_weight=scale_pos_weight,
          base_score=base_score,
          seed=seed)
      return deepchem.models.xgboost_models.XGBoostModel(
          xgboost_model, model_dir, **esr)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1777')" href="javascript:;">
deepchem-2.6.1/deepchem/molnet/run_benchmark_models.py: 734-753
</a>
<div class="mid" id="frag1777" style="display:none"><pre>
    def model_builder(model_dir):
      import xgboost
      xgboost_model = xgboost.XGBRegressor(
          max_depth=max_depth,
          learning_rate=learning_rate,
          n_estimators=n_estimators,
          gamma=gamma,
          min_child_weight=min_child_weight,
          max_delta_step=max_delta_step,
          subsample=subsample,
          colsample_bytree=colsample_bytree,
          colsample_bylevel=colsample_bylevel,
          reg_alpha=reg_alpha,
          reg_lambda=reg_lambda,
          scale_pos_weight=scale_pos_weight,
          base_score=base_score,
          seed=seed)
      return deepchem.models.xgboost_models.XGBoostModel(
          xgboost_model, model_dir, **esr)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 46:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1792')" href="javascript:;">
deepchem-2.6.1/deepchem/metalearning/tests/test_maml_reload.py: 12-22
</a>
<div class="mid" id="frag1792" style="display:none"><pre>
    def __init__(self):
      self.batch_size = 10
      self.w1 = tf.Variable(np.random.normal(size=[1, 40], scale=1.0))
      self.w2 = tf.Variable(
          np.random.normal(size=[40, 40], scale=np.sqrt(1 / 40)))
      self.w3 = tf.Variable(
          np.random.normal(size=[40, 1], scale=np.sqrt(1 / 40)))
      self.b1 = tf.Variable(np.zeros(40))
      self.b2 = tf.Variable(np.zeros(40))
      self.b3 = tf.Variable(np.zeros(1))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1799')" href="javascript:;">
deepchem-2.6.1/deepchem/metalearning/tests/test_maml.py: 26-36
</a>
<div class="mid" id="frag1799" style="display:none"><pre>
      def __init__(self):
        self.batch_size = 10
        self.w1 = tf.Variable(np.random.normal(size=[1, 40], scale=1.0))
        self.w2 = tf.Variable(
            np.random.normal(size=[40, 40], scale=np.sqrt(1 / 40)))
        self.w3 = tf.Variable(
            np.random.normal(size=[40, 1], scale=np.sqrt(1 / 40)))
        self.b1 = tf.Variable(np.zeros(40))
        self.b2 = tf.Variable(np.zeros(40))
        self.b3 = tf.Variable(np.zeros(1))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 47:</b> &nbsp; 3 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1922')" href="javascript:;">
deepchem-2.6.1/examples/kinase/KINASE_datasets.py: 15-30
</a>
<div class="mid" id="frag1922" style="display:none"><pre>
def remove_missing_entries(dataset):
  """Remove missing entries.

  Some of the datasets have missing entries that sneak in as zero'd out
  feature vectors. Get rid of them.
  """
  for i, (X, y, w, ids) in enumerate(dataset.itershards()):
    available_rows = X.any(axis=1)
    print("Shard %d has %d missing entries."
        % (i, np.count_nonzero(~available_rows)))
    X = X[available_rows]
    y = y[available_rows]
    w = w[available_rows]
    ids = ids[available_rows]
    dataset.set_shard(i, X, y, w, ids)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1929')" href="javascript:;">
deepchem-2.6.1/examples/uv/UV_datasets.py: 15-30
</a>
<div class="mid" id="frag1929" style="display:none"><pre>
def remove_missing_entries(dataset):
  """Remove missing entries.

  Some of the datasets have missing entries that sneak in as zero'd out
  feature vectors. Get rid of them.
  """
  for i, (X, y, w, ids) in enumerate(dataset.itershards()):
    available_rows = X.any(axis=1)
    print("Shard %d has %d missing entries."
        % (i, np.count_nonzero(~available_rows)))
    X = X[available_rows]
    y = y[available_rows]
    w = w[available_rows]
    ids = ids[available_rows]
    dataset.set_shard(i, X, y, w, ids)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1949')" href="javascript:;">
deepchem-2.6.1/examples/factors/FACTORS_datasets.py: 15-30
</a>
<div class="mid" id="frag1949" style="display:none"><pre>
def remove_missing_entries(dataset):
  """Remove missing entries.

  Some of the datasets have missing entries that sneak in as zero'd out
  feature vectors. Get rid of them.
  """
  for i, (X, y, w, ids) in enumerate(dataset.itershards()):
    available_rows = X.any(axis=1)
    print("Shard %d has %d missing entries."
        % (i, np.count_nonzero(~available_rows)))
    X = X[available_rows]
    y = y[available_rows]
    w = w[available_rows]
    ids = ids[available_rows]
    dataset.set_shard(i, X, y, w, ids)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 48:</b> &nbsp; 2 fragments, nominal size 38 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1924')" href="javascript:;">
deepchem-2.6.1/examples/kinase/KINASE_datasets.py: 36-87
</a>
<div class="mid" id="frag1924" style="display:none"><pre>
def gen_kinase(KINASE_tasks, raw_train_dir, train_dir, valid_dir, test_dir,
                shard_size=10000):
  """Load Kinase datasets."""
  train_files = ("KINASE_training_disguised_combined_full.csv.gz")
  valid_files = ("KINASE_test1_disguised_combined_full.csv.gz")
  test_files = ("KINASE_test2_disguised_combined_full.csv.gz")

  # Featurize Kinase dataset
  print("About to featurize KINASE dataset.")
  featurizer = dc.feat.UserDefinedFeaturizer(kinase_descriptors)

  loader = dc.data.UserCSVLoader(
      tasks=KINASE_tasks, id_field="Molecule", featurizer=featurizer)

  train_datasets, valid_datasets, test_datasets = [], [], []
  print("Featurizing train datasets")
  train_dataset = loader.featurize(train_files, shard_size=shard_size)

  print("Featurizing valid datasets")
  valid_dataset = loader.featurize(valid_files, shard_size=shard_size)

  print("Featurizing test datasets")
  test_dataset = loader.featurize(test_files, shard_size=shard_size)

  print("Remove missing entries from datasets.")
  remove_missing_entries(train_dataset)
  remove_missing_entries(valid_dataset)
  remove_missing_entries(test_dataset)

  print("Transforming datasets with transformers.")
  transformers = get_transformers(train_dataset)
  raw_train_dataset = train_dataset

  for transformer in transformers:
    print("Performing transformations with %s"
          % transformer.__class__.__name__)
    print("Transforming datasets")
    train_dataset = transformer.transform(train_dataset)
    valid_dataset = transformer.transform(valid_dataset)
    test_dataset = transformer.transform(test_dataset)

  print("Shuffling order of train dataset.")
  train_dataset.sparse_shuffle()

  print("Moving directories")
  raw_train_dataset.move(raw_train_dir)
  train_dataset.move(train_dir)
  valid_dataset.move(valid_dir)
  test_dataset.move(test_dir)
  
  return (raw_train_dataset, train_dataset, valid_dataset, test_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1951')" href="javascript:;">
deepchem-2.6.1/examples/factors/FACTORS_datasets.py: 36-87
</a>
<div class="mid" id="frag1951" style="display:none"><pre>
def gen_factors(FACTORS_tasks, raw_train_dir, train_dir, valid_dir, test_dir,
                shard_size=10000):
  """Load Factor datasets."""
  train_files = ("FACTORS_training_disguised_combined_full.csv.gz")
  valid_files = ("FACTORS_test1_disguised_combined_full.csv.gz")
  test_files = ("FACTORS_test2_disguised_combined_full.csv.gz")

  # Featurize FACTORS dataset
  print("About to featurize FACTORS dataset.")
  featurizer = dc.feat.UserDefinedFeaturizer(factors_descriptors)

  loader = dc.data.UserCSVLoader(
      tasks=FACTORS_tasks, id_field="Molecule", featurizer=featurizer)

  train_datasets, valid_datasets, test_datasets = [], [], []
  print("Featurizing train datasets")
  train_dataset = loader.featurize(train_files, shard_size=shard_size)

  print("Featurizing valid datasets")
  valid_dataset = loader.featurize(valid_files, shard_size=shard_size)

  print("Featurizing test datasets")
  test_dataset = loader.featurize(test_files, shard_size=shard_size)

  print("Remove missing entries from datasets.")
  remove_missing_entries(train_dataset)
  remove_missing_entries(valid_dataset)
  remove_missing_entries(test_dataset)

  print("Transforming datasets with transformers.")
  transformers = get_transformers(train_dataset)
  raw_train_dataset = train_dataset

  for transformer in transformers:
    print("Performing transformations with %s"
          % transformer.__class__.__name__)
    print("Transforming datasets")
    train_dataset = transformer.transform(train_dataset)
    valid_dataset = transformer.transform(valid_dataset)
    test_dataset = transformer.transform(test_dataset)

  print("Shuffling order of train dataset.")
  train_dataset.sparse_shuffle()

  print("Moving directories")
  raw_train_dataset.move(raw_train_dir)
  train_dataset.move(train_dir)
  valid_dataset.move(valid_dir)
  test_dataset.move(test_dir)
  
  return (raw_train_dataset, train_dataset, valid_dataset, test_dataset)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 49:</b> &nbsp; 2 fragments, nominal size 23 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1925')" href="javascript:;">
deepchem-2.6.1/examples/kinase/KINASE_datasets.py: 88-114
</a>
<div class="mid" id="frag1925" style="display:none"><pre>
def load_kinase(shard_size):
  """Loads kinase datasets. Generates if not stored already."""
  KINASE_tasks = (['T_000%d' % i for i in range(13, 100)]
                  + ['T_00%d' % i for i in range(100, 112)])

  current_dir = os.path.dirname(os.path.realpath(__file__))
  raw_train_dir = os.path.join(current_dir, "raw_train_dir")
  train_dir = os.path.join(current_dir, "train_dir") 
  valid_dir = os.path.join(current_dir, "valid_dir") 
  test_dir = os.path.join(current_dir, "test_dir") 

  if (os.path.exists(raw_train_dir) and
      os.path.exists(train_dir) and
      os.path.exists(valid_dir) and
      os.path.exists(test_dir)):
    print("Reloading existing datasets")
    raw_train_dataset = dc.data.DiskDataset(raw_train_dir)
    train_dataset = dc.data.DiskDataset(train_dir)
    valid_dataset = dc.data.DiskDataset(valid_dir)
    test_dataset = dc.data.DiskDataset(test_dir)
  else:
    print("Featurizing datasets")
    (raw_train_dataset, train_dataset, valid_dataset, test_dataset) = \
      gen_kinase(KINASE_tasks, raw_train_dir, train_dir, valid_dir, test_dir,
                  shard_size=shard_size)

  transformers = get_transformers(raw_train_dataset)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1952')" href="javascript:;">
deepchem-2.6.1/examples/factors/FACTORS_datasets.py: 88-114
</a>
<div class="mid" id="frag1952" style="display:none"><pre>
def load_factors(shard_size):
  """Loads factors datasets. Generates if not stored already."""
  FACTORS_tasks = (['T_0000%d' % i for i in range(1, 10)]
                   + ['T_000%d' % i for i in range(10, 13)])

  current_dir = os.path.dirname(os.path.realpath(__file__))
  raw_train_dir = os.path.join(current_dir, "raw_train_dir")
  train_dir = os.path.join(current_dir, "train_dir") 
  valid_dir = os.path.join(current_dir, "valid_dir") 
  test_dir = os.path.join(current_dir, "test_dir") 

  if (os.path.exists(raw_train_dir) and
      os.path.exists(train_dir) and
      os.path.exists(valid_dir) and
      os.path.exists(test_dir)):
    print("Reloading existing datasets")
    raw_train_dataset = dc.data.DiskDataset(raw_train_dir)
    train_dataset = dc.data.DiskDataset(train_dir)
    valid_dataset = dc.data.DiskDataset(valid_dir)
    test_dataset = dc.data.DiskDataset(test_dir)
  else:
    print("Featurizing datasets")
    (raw_train_dataset, train_dataset, valid_dataset, test_dataset) = \
      gen_factors(FACTORS_tasks, raw_train_dir, train_dir, valid_dir, test_dir,
                  shard_size=shard_size)

  transformers = get_transformers(raw_train_dataset)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 50:</b> &nbsp; 3 fragments, nominal size 14 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1926')" href="javascript:;">
deepchem-2.6.1/examples/kinase/KINASE_tf_singletask.py: 38-53
</a>
<div class="mid" id="frag1926" style="display:none"><pre>
def task_model_builder(m_dir):
  return dc.models.TensorflowMultitaskRegressor(
      n_tasks=1,
      n_features=n_features,
      logdir=m_dir,
      layer_sizes=[1000] * n_layers,
      dropouts=[.25] * n_layers,
      weight_init_stddevs=[.02] * n_layers,
      bias_init_consts=[1.] * n_layers,
      learning_rate=.0003,
      penalty=.0001,
      penalty_type="l2",
      optimizer="adam",
      batch_size=100)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1934')" href="javascript:;">
deepchem-2.6.1/examples/uv/UV_tf_singletask.py: 37-52
</a>
<div class="mid" id="frag1934" style="display:none"><pre>
def task_model_builder(m_dir):
  return dc.models.TensorflowMultitaskRegressor(
      n_tasks=1,
      n_features=n_features,
      logdir=m_dir,
      layer_sizes=[1000] * n_layers,
      dropouts=[.25] * n_layers,
      weight_init_stddevs=[.02] * n_layers,
      bias_init_consts=[1.] * n_layers,
      learning_rate=.0003,
      penalty=.0001,
      penalty_type="l2",
      optimizer="adam",
      batch_size=100)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1948')" href="javascript:;">
deepchem-2.6.1/examples/factors/FACTORS_tf_singletask.py: 38-53
</a>
<div class="mid" id="frag1948" style="display:none"><pre>
def task_model_builder(m_dir):
  return dc.models.TensorflowMultitaskRegressor(
      n_tasks=1,
      n_features=n_features,
      logdir=m_dir,
      layer_sizes=[1000] * n_layers,
      dropouts=[.25] * n_layers,
      weight_init_stddevs=[.02] * n_layers,
      bias_init_consts=[1.] * n_layers,
      learning_rate=.0003,
      penalty=.0001,
      penalty_type="l2",
      optimizer="adam",
      batch_size=100)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<script language="JavaScript">
function ShowHide(divId) { 
    if(document.getElementById(divId).style.display == 'none') {
        document.getElementById(divId).style.display='block';
    } else { 
        document.getElementById(divId).style.display = 'none';
    } 
}
</script>
</body>
</html>

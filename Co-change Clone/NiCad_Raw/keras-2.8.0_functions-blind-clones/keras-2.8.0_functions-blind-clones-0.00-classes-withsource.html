<html>
<head>
    <title>NiCad6 Clone Report</title>
    <style type="text/css">
        body {font-family:sans-serif;}
        table {background-color:white; border:0px; padding:0px; border-spacing:4px; width:auto; margin-left:30px; margin-right:auto;}
        td {background-color:rgba(192,212,238,0.8); border:0px; padding:8px; width:auto; vertical-align:top; border-radius:8px}
        pre {background-color:white; padding:4px;}
        a {color:darkblue;}
    </style>
</head>
<body>
<h2>NiCad6 Clone Report</h2>
<table>
<tr style="font-size:14pt">
<td><b>System:</b> &nbsp; keras-2.8.0</td>
<td><b>Clone pairs:</b> &nbsp; 281</td>
<td><b>Clone classes:</b> &nbsp; 126</td>
</tr>
<tr style="font-size:12pt">
<td style="background-color:white">Clone type: &nbsp; 2</td>
<td style="background-color:white">Granularity: &nbsp; functions-blind</td>
<td style="background-color:white">Max diff threshold: &nbsp; 0%</td>
<td style="background-color:white">Clone size: &nbsp; 10 - 2500 lines</td>
<td style="background-color:white">Total functions-blind: &nbsp; 5640</td>
</tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 1:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4')" href="javascript:;">
keras-2.8.0/keras/optimizer_v2/adam.py: 97-112
</a>
<div class="mid" id="frag4" style="display:none"><pre>
  def __init__(self,
               learning_rate=0.001,
               beta_1=0.9,
               beta_2=0.999,
               epsilon=1e-7,
               amsgrad=False,
               name='Adam',
               **kwargs):
    super(Adam, self).__init__(name, **kwargs)
    self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))
    self._set_hyper('decay', self._initial_decay)
    self._set_hyper('beta_1', beta_1)
    self._set_hyper('beta_2', beta_2)
    self.epsilon = epsilon or backend_config.epsilon()
    self.amsgrad = amsgrad

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag11')" href="javascript:;">
keras-2.8.0/keras/optimizer_v2/adam.py: 321-364
</a>
<div class="mid" id="frag11" style="display:none"><pre>
  def __init__(self,
               learning_rate=0.001,
               beta_1=0.9,
               beta_2=0.999,
               epsilon=1e-7,
               amsgrad=False,
               name='Adam',
               **kwargs):
    """Construct a new Adam optimizer.

    Args:
      learning_rate: A `Tensor`, floating point value, or a schedule that is a
        `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable that
        takes no arguments and returns the actual value to use, The learning
        rate. Defaults to 0.001.
      beta_1: A float value or a constant float tensor, or a callable that takes
        no arguments and returns the actual value to use. The exponential decay
        rate for the 1st moment estimates. Defaults to 0.9.
      beta_2: A float value or a constant float tensor, or a callable that takes
        no arguments and returns the actual value to use, The exponential decay
        rate for the 2nd moment estimates. Defaults to 0.999.
      epsilon: A small constant for numerical stability. This epsilon is
        "epsilon hat" in the Kingma and Ba paper (in the formula just before
        Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to
        1e-7.
      amsgrad: Boolean. Whether to apply AMSGrad variant of this algorithm from
        the paper "On the Convergence of Adam and beyond". Defaults to `False`.
      name: Optional name for the operations created when applying gradients.
        Defaults to "Adam".
      **kwargs: keyword arguments. Allowed to be {`clipnorm`, `clipvalue`, `lr`,
        `decay`}. `clipnorm` is clip gradients by norm; `clipvalue` is clip
        gradients by value, `decay` is included for backward compatibility to
        allow time inverse decay of learning rate. `lr` is included for backward
        compatibility, recommended to use `learning_rate` instead.
    """

    super(NonFusedAdam, self).__init__(name, **kwargs)
    self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))
    self._set_hyper('decay', self._initial_decay)
    self._set_hyper('beta_1', beta_1)
    self._set_hyper('beta_2', beta_2)
    self.epsilon = epsilon or backend_config.epsilon()
    self.amsgrad = amsgrad

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 2:</b> &nbsp; 3 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10')" href="javascript:;">
keras-2.8.0/keras/optimizer_v2/adam.py: 233-245
</a>
<div class="mid" id="frag10" style="display:none"><pre>
  def get_config(self):
    config = super(Adam, self).get_config()
    config.update({
        'learning_rate': self._serialize_hyperparameter('learning_rate'),
        'decay': self._initial_decay,
        'beta_1': self._serialize_hyperparameter('beta_1'),
        'beta_2': self._serialize_hyperparameter('beta_2'),
        'epsilon': self.epsilon,
        'amsgrad': self.amsgrad,
    })
    return config


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag199')" href="javascript:;">
keras-2.8.0/keras/optimizer_v2/rmsprop.py: 282-294
</a>
<div class="mid" id="frag199" style="display:none"><pre>
  def get_config(self):
    config = super(RMSprop, self).get_config()
    config.update({
        "learning_rate": self._serialize_hyperparameter("learning_rate"),
        "decay": self._initial_decay,
        "rho": self._serialize_hyperparameter("rho"),
        "momentum": self._serialize_hyperparameter("momentum"),
        "epsilon": self.epsilon,
        "centered": self.centered,
    })
    return config


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag17')" href="javascript:;">
keras-2.8.0/keras/optimizer_v2/adam.py: 457-467
</a>
<div class="mid" id="frag17" style="display:none"><pre>
  def get_config(self):
    config = super(NonFusedAdam, self).get_config()
    config.update({
        'learning_rate': self._serialize_hyperparameter('learning_rate'),
        'decay': self._initial_decay,
        'beta_1': self._serialize_hyperparameter('beta_1'),
        'beta_2': self._serialize_hyperparameter('beta_2'),
        'epsilon': self.epsilon,
        'amsgrad': self.amsgrad,
    })
    return config
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 3:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag51')" href="javascript:;">
keras-2.8.0/keras/optimizer_v2/learning_rate_schedule_test.py: 228-238
</a>
<div class="mid" id="frag51" style="display:none"><pre>
  def testEnd(self, serialize):
    step = 10
    lr = 0.05
    end_lr = 0.001
    power = 0.5
    decayed_lr = learning_rate_schedule.PolynomialDecay(
        lr, 10, end_lr, power=power)
    decayed_lr = _maybe_serialized(decayed_lr, serialize)
    expected = end_lr
    self.assertAllClose(self.evaluate(decayed_lr(step)), expected, 1e-6)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag53')" href="javascript:;">
keras-2.8.0/keras/optimizer_v2/learning_rate_schedule_test.py: 250-260
</a>
<div class="mid" id="frag53" style="display:none"><pre>
  def testBeyondEnd(self, serialize):
    step = 15
    lr = 0.05
    end_lr = 0.001
    power = 0.5
    decayed_lr = learning_rate_schedule.PolynomialDecay(
        lr, 10, end_lr, power=power)
    decayed_lr = _maybe_serialized(decayed_lr, serialize)
    expected = end_lr
    self.assertAllClose(self.evaluate(decayed_lr(step)), expected, 1e-6)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 4:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag62')" href="javascript:;">
keras-2.8.0/keras/optimizer_v2/learning_rate_schedule_test.py: 378-389
</a>
<div class="mid" id="frag62" style="display:none"><pre>
  def np_cosine_decay_restarts(self, step, decay_steps, t_mul=2.0, m_mul=1.0,
                               alpha=0.0):
    fac = 1.0
    while step &gt;= decay_steps:
      step -= decay_steps
      decay_steps *= t_mul
      fac *= m_mul

    completed_fraction = step / decay_steps
    decay = fac * 0.5 * (1.0 + math.cos(math.pi * completed_fraction))
    return (1.0 - alpha) * decay + alpha

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag183')" href="javascript:;">
keras-2.8.0/keras/optimizer_v2/legacy_learning_rate_decay_test.py: 347-358
</a>
<div class="mid" id="frag183" style="display:none"><pre>
  def np_cosine_decay_restarts(self, step, decay_steps, t_mul=2.0, m_mul=1.0,
                               alpha=0.0):
    fac = 1.0
    while step &gt;= decay_steps:
      step -= decay_steps
      decay_steps *= t_mul
      fac *= m_mul

    completed_fraction = step / decay_steps
    decay = fac * 0.5 * (1.0 + math.cos(math.pi * completed_fraction))
    return (1.0 - alpha) * decay + alpha

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 5:</b> &nbsp; 3 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag65')" href="javascript:;">
keras-2.8.0/keras/optimizer_v2/learning_rate_schedule_test.py: 410-421
</a>
<div class="mid" id="frag65" style="display:none"><pre>
  def testAlpha(self, serialize):
    num_training_steps = 1000
    initial_lr = 1.0
    alpha = 0.1
    for step in range(0, 1500, 250):
      decayed_lr = learning_rate_schedule.CosineDecayRestarts(
          initial_lr, num_training_steps, alpha=alpha)
      decayed_lr = _maybe_serialized(decayed_lr, serialize)
      expected = self.np_cosine_decay_restarts(
          step, num_training_steps, alpha=alpha)
      self.assertAllClose(self.evaluate(decayed_lr(step)), expected, 1e-6)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag67')" href="javascript:;">
keras-2.8.0/keras/optimizer_v2/learning_rate_schedule_test.py: 434-446
</a>
<div class="mid" id="frag67" style="display:none"><pre>
  def testTMul(self, serialize):
    num_training_steps = 1000
    initial_lr = 1.0
    t_mul = 1.0
    for step in range(0, 1500, 250):
      decayed_lr = learning_rate_schedule.CosineDecayRestarts(
          initial_lr, num_training_steps, t_mul=t_mul)
      decayed_lr = _maybe_serialized(decayed_lr, serialize)
      expected = self.np_cosine_decay_restarts(
          step, num_training_steps, t_mul=t_mul)
      self.assertAllClose(self.evaluate(decayed_lr(step)), expected, 1e-6)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag66')" href="javascript:;">
keras-2.8.0/keras/optimizer_v2/learning_rate_schedule_test.py: 422-433
</a>
<div class="mid" id="frag66" style="display:none"><pre>
  def testMMul(self, serialize):
    num_training_steps = 1000
    initial_lr = 1.0
    m_mul = 0.9
    for step in range(0, 1500, 250):
      decayed_lr = learning_rate_schedule.CosineDecayRestarts(
          initial_lr, num_training_steps, m_mul=m_mul)
      decayed_lr = _maybe_serialized(decayed_lr, serialize)
      expected = self.np_cosine_decay_restarts(
          step, num_training_steps, m_mul=m_mul)
      self.assertAllClose(self.evaluate(decayed_lr(step)), expected, 1e-6)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 6:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag74')" href="javascript:;">
keras-2.8.0/keras/optimizer_v2/nadam.py: 204-213
</a>
<div class="mid" id="frag74" style="display:none"><pre>
  def get_config(self):
    config = super(Nadam, self).get_config()
    config.update({
        'learning_rate': self._serialize_hyperparameter('learning_rate'),
        'decay': self._initial_decay,
        'beta_1': self._serialize_hyperparameter('beta_1'),
        'beta_2': self._serialize_hyperparameter('beta_2'),
        'epsilon': self.epsilon,
    })
    return config
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag219')" href="javascript:;">
keras-2.8.0/keras/optimizer_v2/adamax.py: 170-179
</a>
<div class="mid" id="frag219" style="display:none"><pre>
  def get_config(self):
    config = super(Adamax, self).get_config()
    config.update({
        'learning_rate': self._serialize_hyperparameter('learning_rate'),
        'decay': self._initial_decay,
        'beta_1': self._serialize_hyperparameter('beta_1'),
        'beta_2': self._serialize_hyperparameter('beta_2'),
        'epsilon': self.epsilon,
    })
    return config
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 7:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag106')" href="javascript:;">
keras-2.8.0/keras/optimizer_v2/optimizer_v2_test.py: 640-653
</a>
<div class="mid" id="frag106" style="display:none"><pre>
  def testAggregationTrue(self):
    # Test that experimental_aggregate_gradients=True works without distributed
    # strategy.
    var = tf.Variable([1., 2.])
    opt = gradient_descent.SGD(3.0)

    self.evaluate(tf.compat.v1.global_variables_initializer())
    self.assertAllClose([1., 2.], self.evaluate(var))
    opt_op = opt.apply_gradients([([0.1, 0.1], var)],
                                 experimental_aggregate_gradients=True)
    self.evaluate(tf.compat.v1.global_variables_initializer())
    self.evaluate(opt_op)
    self.assertAllClose([0.7, 1.7], self.evaluate(var))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag107')" href="javascript:;">
keras-2.8.0/keras/optimizer_v2/optimizer_v2_test.py: 655-668
</a>
<div class="mid" id="frag107" style="display:none"><pre>
  def testAggregationFalse(self):
    # Test that experimental_aggregate_gradients=False works without distributed
    # strategy.
    var = tf.Variable([1., 2.])
    opt = gradient_descent.SGD(3.0)

    self.evaluate(tf.compat.v1.global_variables_initializer())
    self.assertAllClose([1., 2.], self.evaluate(var))
    opt_op = opt.apply_gradients([([0.1, 0.1], var)],
                                 experimental_aggregate_gradients=False)
    self.evaluate(tf.compat.v1.global_variables_initializer())
    self.evaluate(opt_op)
    self.assertAllClose([0.7, 1.7], self.evaluate(var))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 8:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag125')" href="javascript:;">
keras-2.8.0/keras/optimizer_v2/optimizer_v2_test.py: 941-953
</a>
<div class="mid" id="frag125" style="display:none"><pre>
  def testBasic(self):
    var = tf.Variable([1.0, 2.0], dtype=tf.float32)
    loss = lambda: 3 * var
    opt = adam.Adam(learning_rate=1.0)

    @tf.function
    def fn():
      opt.minimize(loss, [var])
      return var

    self.assertAllClose([0., 1.], fn(), atol=1e-4)
    self.assertAllClose([-1, 0.], fn(), atol=1e-4)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag127')" href="javascript:;">
keras-2.8.0/keras/optimizer_v2/optimizer_v2_test.py: 954-966
</a>
<div class="mid" id="frag127" style="display:none"><pre>
  def testBasicWithConstantDecay(self):
    var = tf.Variable([1.0, 2.0], dtype=tf.float32)
    loss = lambda: 3 * var
    opt = adam.Adam(learning_rate=1.0)

    @tf.function
    def fn():
      opt.minimize(loss, [var])
      return var

    self.assertAllClose([0., 1.], fn(), atol=1e-4)
    self.assertAllClose([-1, 0.], fn(), atol=1e-4)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 9:</b> &nbsp; 3 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag185')" href="javascript:;">
keras-2.8.0/keras/optimizer_v2/legacy_learning_rate_decay_test.py: 368-378
</a>
<div class="mid" id="frag185" style="display:none"><pre>
  def testAlpha(self):
    num_training_steps = 1000
    initial_lr = 1.0
    alpha = 0.1
    for step in range(0, 1500, 250):
      decayed_lr = tf.compat.v1.train.cosine_decay_restarts(
          initial_lr, step, num_training_steps, alpha=alpha)
      expected = self.np_cosine_decay_restarts(
          step, num_training_steps, alpha=alpha)
      self.assertAllClose(self.evaluate(decayed_lr), expected, 1e-6)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag186')" href="javascript:;">
keras-2.8.0/keras/optimizer_v2/legacy_learning_rate_decay_test.py: 379-389
</a>
<div class="mid" id="frag186" style="display:none"><pre>
  def testMMul(self):
    num_training_steps = 1000
    initial_lr = 1.0
    m_mul = 0.9
    for step in range(0, 1500, 250):
      decayed_lr = tf.compat.v1.train.cosine_decay_restarts(
          initial_lr, step, num_training_steps, m_mul=m_mul)
      expected = self.np_cosine_decay_restarts(
          step, num_training_steps, m_mul=m_mul)
      self.assertAllClose(self.evaluate(decayed_lr), expected, 1e-6)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag187')" href="javascript:;">
keras-2.8.0/keras/optimizer_v2/legacy_learning_rate_decay_test.py: 390-401
</a>
<div class="mid" id="frag187" style="display:none"><pre>
  def testTMul(self):
    num_training_steps = 1000
    initial_lr = 1.0
    t_mul = 1.0
    for step in range(0, 1500, 250):
      decayed_lr = tf.compat.v1.train.cosine_decay_restarts(
          initial_lr, step, num_training_steps, t_mul=t_mul)
      expected = self.np_cosine_decay_restarts(
          step, num_training_steps, t_mul=t_mul)
      self.assertAllClose(self.evaluate(decayed_lr), expected, 1e-6)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 10:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag223')" href="javascript:;">
keras-2.8.0/keras/optimizer_v2/learning_rate_schedule.py: 143-170
</a>
<div class="mid" id="frag223" style="display:none"><pre>
  def __init__(
      self,
      initial_learning_rate,
      decay_steps,
      decay_rate,
      staircase=False,
      name=None):
    """Applies exponential decay to the learning rate.

    Args:
      initial_learning_rate: A scalar `float32` or `float64` `Tensor` or a
        Python number.  The initial learning rate.
      decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number.
        Must be positive.  See the decay computation above.
      decay_rate: A scalar `float32` or `float64` `Tensor` or a
        Python number.  The decay rate.
      staircase: Boolean.  If `True` decay the learning rate at discrete
        intervals
      name: String.  Optional name of the operation.  Defaults to
        'ExponentialDecay'.
    """
    super(ExponentialDecay, self).__init__()
    self.initial_learning_rate = initial_learning_rate
    self.decay_steps = decay_steps
    self.decay_rate = decay_rate
    self.staircase = staircase
    self.name = name

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag232')" href="javascript:;">
keras-2.8.0/keras/optimizer_v2/learning_rate_schedule.py: 497-523
</a>
<div class="mid" id="frag232" style="display:none"><pre>
  def __init__(
      self,
      initial_learning_rate,
      decay_steps,
      decay_rate,
      staircase=False,
      name=None):
    """Applies inverse time decay to the initial learning rate.

    Args:
      initial_learning_rate: A scalar `float32` or `float64` `Tensor` or a
        Python number.  The initial learning rate.
      decay_steps: How often to apply decay.
      decay_rate: A Python number.  The decay rate.
      staircase: Whether to apply decay in a discrete staircase, as opposed to
        continuous, fashion.
      name: String.  Optional name of the operation.  Defaults to
        'InverseTimeDecay'.
    """
    super(InverseTimeDecay, self).__init__()

    self.initial_learning_rate = initial_learning_rate
    self.decay_steps = decay_steps
    self.decay_rate = decay_rate
    self.staircase = staircase
    self.name = name

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 11:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag238')" href="javascript:;">
keras-2.8.0/keras/optimizer_v2/learning_rate_schedule.py: 687-718
</a>
<div class="mid" id="frag238" style="display:none"><pre>
  def __init__(
      self,
      initial_learning_rate,
      first_decay_steps,
      t_mul=2.0,
      m_mul=1.0,
      alpha=0.0,
      name=None):
    """Applies cosine decay with restarts to the learning rate.

    Args:
      initial_learning_rate: A scalar `float32` or `float64` Tensor or a Python
        number. The initial learning rate.
      first_decay_steps: A scalar `int32` or `int64` `Tensor` or a Python
        number. Number of steps to decay over.
      t_mul: A scalar `float32` or `float64` `Tensor` or a Python number.
        Used to derive the number of iterations in the i-th period.
      m_mul: A scalar `float32` or `float64` `Tensor` or a Python number.
        Used to derive the initial learning rate of the i-th period.
      alpha: A scalar `float32` or `float64` Tensor or a Python number.
        Minimum learning rate value as a fraction of the initial_learning_rate.
      name: String. Optional name of the operation.  Defaults to 'SGDRDecay'.
    """
    super(CosineDecayRestarts, self).__init__()

    self.initial_learning_rate = initial_learning_rate
    self.first_decay_steps = first_decay_steps
    self._t_mul = t_mul
    self._m_mul = m_mul
    self.alpha = alpha
    self.name = name

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag242')" href="javascript:;">
keras-2.8.0/keras/optimizer_v2/learning_rate_schedule.py: 825-855
</a>
<div class="mid" id="frag242" style="display:none"><pre>
  def __init__(
      self,
      initial_learning_rate,
      decay_steps,
      num_periods=0.5,
      alpha=0.0,
      beta=0.001,
      name=None):
    """Applies linear cosine decay to the learning rate.

    Args:
      initial_learning_rate: A scalar `float32` or `float64` Tensor or a Python
        number. The initial learning rate.
      decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number.
        Number of steps to decay over.
      num_periods: Number of periods in the cosine part of the decay.
        See computation above.
      alpha: See computation above.
      beta: See computation above.
      name: String.  Optional name of the operation.  Defaults to
        'LinearCosineDecay'.
    """
    super(LinearCosineDecay, self).__init__()

    self.initial_learning_rate = initial_learning_rate
    self.decay_steps = decay_steps
    self.num_periods = num_periods
    self.alpha = alpha
    self.beta = beta
    self.name = name

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 12:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag250')" href="javascript:;">
keras-2.8.0/keras/optimizer_v2/legacy_learning_rate_decay.py: 25-99
</a>
<div class="mid" id="frag250" style="display:none"><pre>
def exponential_decay(learning_rate,
                      global_step,
                      decay_steps,
                      decay_rate,
                      staircase=False,
                      name=None):
  """Applies exponential decay to the learning rate.

  When training a model, it is often recommended to lower the learning rate as
  the training progresses.  This function applies an exponential decay function
  to a provided initial learning rate.  It requires a `global_step` value to
  compute the decayed learning rate.  You can just pass a TensorFlow variable
  that you increment at each training step.

  The function returns the decayed learning rate.  It is computed as:

  ```python
  decayed_learning_rate = learning_rate *
                          decay_rate ^ (global_step / decay_steps)
  ```

  If the argument `staircase` is `True`, then `global_step / decay_steps` is an
  integer division and the decayed learning rate follows a staircase function.

  Example: decay every 100000 steps with a base of 0.96:

  ```python
  ...
  global_step = tf.Variable(0, trainable=False)
  starter_learning_rate = 0.1
  learning_rate = tf.compat.v1.train.exponential_decay(starter_learning_rate,
  global_step,
                                             100000, 0.96, staircase=True)
  # Passing global_step to minimize() will increment it at each step.
  learning_step = (
      tf.compat.v1.train.GradientDescentOptimizer(learning_rate)
      .minimize(...my loss..., global_step=global_step)
  )
  ```

  Args:
    learning_rate: A scalar `float32` or `float64` `Tensor` or a Python number.
      The initial learning rate.
    global_step: A scalar `int32` or `int64` `Tensor` or a Python number. Global
      step to use for the decay computation.  Must not be negative.
    decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number. Must
      be positive.  See the decay computation above.
    decay_rate: A scalar `float32` or `float64` `Tensor` or a Python number.
      The decay rate.
    staircase: Boolean.  If `True` decay the learning rate at discrete intervals
    name: String.  Optional name of the operation.  Defaults to
      'ExponentialDecay'.

  Returns:
    A scalar `Tensor` of the same type as `learning_rate`.  The decayed
    learning rate.

  Raises:
    ValueError: if `global_step` is not supplied.

  @compatibility(eager)
  When eager execution is enabled, this function returns a function which in
  turn returns the decayed learning rate Tensor. This can be useful for changing
  the learning rate value across different invocations of optimizer functions.
  @end_compatibility
  """
  decayed_lr = learning_rate_schedule.ExponentialDecay(
      learning_rate, decay_steps, decay_rate, staircase=staircase, name=name)
  if not tf.executing_eagerly():
    decayed_lr = decayed_lr(global_step)
  else:
    decayed_lr = functools.partial(decayed_lr, global_step)
  return decayed_lr


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag254')" href="javascript:;">
keras-2.8.0/keras/optimizer_v2/legacy_learning_rate_decay.py: 368-449
</a>
<div class="mid" id="frag254" style="display:none"><pre>
def inverse_time_decay(learning_rate,
                       global_step,
                       decay_steps,
                       decay_rate,
                       staircase=False,
                       name=None):
  """Applies inverse time decay to the initial learning rate.

  When training a model, it is often recommended to lower the learning rate as
  the training progresses.  This function applies an inverse decay function
  to a provided initial learning rate.  It requires an `global_step` value to
  compute the decayed learning rate.  You can just pass a TensorFlow variable
  that you increment at each training step.

  The function returns the decayed learning rate.  It is computed as:

  ```python
  decayed_learning_rate = learning_rate / (1 + decay_rate * global_step /
  decay_step)
  ```

  or, if `staircase` is `True`, as:

  ```python
  decayed_learning_rate = learning_rate / (1 + decay_rate * floor(global_step /
  decay_step))
  ```

  Example: decay 1/t with a rate of 0.5:

  ```python
  ...
  global_step = tf.Variable(0, trainable=False)
  learning_rate = 0.1
  decay_steps = 1.0
  decay_rate = 0.5
  learning_rate = tf.compat.v1.train.inverse_time_decay(learning_rate,
  global_step,
  decay_steps, decay_rate)

  # Passing global_step to minimize() will increment it at each step.
  learning_step = (
      tf.compat.v1.train.GradientDescentOptimizer(learning_rate)
      .minimize(...my loss..., global_step=global_step)
  )
  ```

  Args:
    learning_rate: A scalar `float32` or `float64` `Tensor` or a Python number.
      The initial learning rate.
    global_step: A Python number. Global step to use for the decay computation.
      Must not be negative.
    decay_steps: How often to apply decay.
    decay_rate: A Python number.  The decay rate.
    staircase: Whether to apply decay in a discrete staircase, as opposed to
      continuous, fashion.
    name: String.  Optional name of the operation.  Defaults to
      'InverseTimeDecay'.

  Returns:
    A scalar `Tensor` of the same type as `learning_rate`.  The decayed
    learning rate.

  Raises:
    ValueError: if `global_step` is not supplied.

  @compatibility(eager)
  When eager execution is enabled, this function returns a function which in
  turn returns the decayed learning rate Tensor. This can be useful for changing
  the learning rate value across different invocations of optimizer functions.
  @end_compatibility
  """
  decayed_lr = learning_rate_schedule.InverseTimeDecay(
      learning_rate, decay_steps, decay_rate, staircase=staircase, name=name)

  if not tf.executing_eagerly():
    decayed_lr = decayed_lr(global_step)
  else:
    decayed_lr = functools.partial(decayed_lr, global_step)
  return decayed_lr


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 13:</b> &nbsp; 2 fragments, nominal size 19 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag256')" href="javascript:;">
keras-2.8.0/keras/optimizer_v2/legacy_learning_rate_decay.py: 514-589
</a>
<div class="mid" id="frag256" style="display:none"><pre>
def cosine_decay_restarts(learning_rate,
                          global_step,
                          first_decay_steps,
                          t_mul=2.0,
                          m_mul=1.0,
                          alpha=0.0,
                          name=None):
  """Applies cosine decay with restarts to the learning rate.

  When training a model, it is often recommended to lower the learning rate as
  the training progresses.  This function applies a cosine decay function with
  restarts to a provided initial learning rate.  It requires a `global_step`
  value to compute the decayed learning rate.  You can just pass a TensorFlow
  variable that you increment at each training step.

  The function returns the decayed learning rate while taking into account
  possible warm restarts. The learning rate multiplier first decays
  from 1 to `alpha` for `first_decay_steps` steps. Then, a warm
  restart is performed. Each new warm restart runs for `t_mul` times more steps
  and with `m_mul` times smaller initial learning rate.

  Example usage:
  ```python
  first_decay_steps = 1000
  lr_decayed = cosine_decay_restarts(learning_rate, global_step,
                                     first_decay_steps)
  ```

  Args:
    learning_rate: A scalar `float32` or `float64` Tensor or a Python number.
      The initial learning rate.
    global_step: A scalar `int32` or `int64` `Tensor` or a Python number. Global
      step to use for the decay computation.
    first_decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number.
      Number of steps to decay over.
    t_mul: A scalar `float32` or `float64` `Tensor` or a Python number. Used to
      derive the number of iterations in the i-th period
    m_mul: A scalar `float32` or `float64` `Tensor` or a Python number.
      Used to derive the initial learning rate of the i-th period:
    alpha: A scalar `float32` or `float64` Tensor or a Python number. Minimum
      learning rate value as a fraction of the learning_rate.
    name: String. Optional name of the operation.  Defaults to 'SGDRDecay'.

  Returns:
    A scalar `Tensor` of the same type as `learning_rate`.  The decayed
    learning rate.
  Raises:
    ValueError: if `global_step` is not supplied.

  References:
    Stochastic Gradient Descent with Warm Restarts:
      [Loshchilov et al., 2017]
      (https://openreview.net/forum?id=Skq89Scxx&amp;noteId=Skq89Scxx)
      ([pdf](https://openreview.net/pdf?id=Skq89Scxx))

  @compatibility(eager)
  When eager execution is enabled, this function returns a function which in
  turn returns the decayed learning rate Tensor. This can be useful for changing
  the learning rate value across different invocations of optimizer functions.
  @end_compatibility
  """
  decayed_lr = learning_rate_schedule.CosineDecayRestarts(
      learning_rate,
      first_decay_steps,
      t_mul=t_mul,
      m_mul=m_mul,
      alpha=alpha,
      name=name)

  if not tf.executing_eagerly():
    decayed_lr = decayed_lr(global_step)
  else:
    decayed_lr = functools.partial(decayed_lr, global_step)
  return decayed_lr


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag257')" href="javascript:;">
keras-2.8.0/keras/optimizer_v2/legacy_learning_rate_decay.py: 591-674
</a>
<div class="mid" id="frag257" style="display:none"><pre>
def linear_cosine_decay(learning_rate,
                        global_step,
                        decay_steps,
                        num_periods=0.5,
                        alpha=0.0,
                        beta=0.001,
                        name=None):
  """Applies linear cosine decay to the learning rate.

  Note that linear cosine decay is more aggressive than cosine decay and
  larger initial learning rates can typically be used.

  When training a model, it is often recommended to lower the learning rate as
  the training progresses.  This function applies a linear cosine decay function
  to a provided initial learning rate.  It requires a `global_step` value to
  compute the decayed learning rate.  You can just pass a TensorFlow variable
  that you increment at each training step.

  The function returns the decayed learning rate.  It is computed as:
  ```python
  global_step = min(global_step, decay_steps)
  linear_decay = (decay_steps - global_step) / decay_steps)
  cosine_decay = 0.5 * (
      1 + cos(pi * 2 * num_periods * global_step / decay_steps))
  decayed = (alpha + linear_decay) * cosine_decay + beta
  decayed_learning_rate = learning_rate * decayed
  ```

  Example usage:
  ```python
  decay_steps = 1000
  lr_decayed = linear_cosine_decay(learning_rate, global_step, decay_steps)
  ```

  Args:
    learning_rate: A scalar `float32` or `float64` Tensor or a Python number.
      The initial learning rate.
    global_step: A scalar `int32` or `int64` `Tensor` or a Python number. Global
      step to use for the decay computation.
    decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number. Number
      of steps to decay over.
    num_periods: Number of periods in the cosine part of the decay. See
      computation above.
    alpha: See computation above.
    beta: See computation above.
    name: String.  Optional name of the operation.  Defaults to
      'LinearCosineDecay'.

  Returns:
    A scalar `Tensor` of the same type as `learning_rate`.  The decayed
    learning rate.
  Raises:
    ValueError: if `global_step` is not supplied.

  References:
    Neural Optimizer Search with Reinforcement Learning:
      [Bello et al., 2017](http://proceedings.mlr.press/v70/bello17a.html)
      ([pdf](http://proceedings.mlr.press/v70/bello17a/bello17a.pdf))
    Stochastic Gradient Descent with Warm Restarts:
      [Loshchilov et al., 2017]
      (https://openreview.net/forum?id=Skq89Scxx&amp;noteId=Skq89Scxx)
      ([pdf](https://openreview.net/pdf?id=Skq89Scxx))

  @compatibility(eager)
  When eager execution is enabled, this function returns a function which in
  turn returns the decayed learning rate Tensor. This can be useful for changing
  the learning rate value across different invocations of optimizer functions.
  @end_compatibility
  """
  decayed_lr = learning_rate_schedule.LinearCosineDecay(
      learning_rate,
      decay_steps,
      num_periods=num_periods,
      alpha=alpha,
      beta=beta,
      name=name)

  if not tf.executing_eagerly():
    decayed_lr = decayed_lr(global_step)
  else:
    decayed_lr = functools.partial(decayed_lr, global_step)
  return decayed_lr


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 14:</b> &nbsp; 3 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag393')" href="javascript:;">
keras-2.8.0/keras/distribute/optimizer_combinations.py: 74-86
</a>
<div class="mid" id="frag393" style="display:none"><pre>
def distributions_and_v1_optimizers():
  """A common set of combination with DistributionStrategies and Optimizers."""
  return tf.__internal__.test.combinations.combine(
      distribution=[
          tf.__internal__.distribute.combinations.one_device_strategy,
          tf.__internal__.distribute.combinations.mirrored_strategy_with_gpu_and_cpu,
          tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus,
          tf.__internal__.distribute.combinations
          .mirrored_strategy_with_two_gpus_no_merge_call,
      ],
      optimizer_fn=optimizers_v1)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag394')" href="javascript:;">
keras-2.8.0/keras/distribute/optimizer_combinations.py: 87-99
</a>
<div class="mid" id="frag394" style="display:none"><pre>
def distributions_and_v2_optimizers():
  """A common set of combination with DistributionStrategies and Optimizers."""
  return tf.__internal__.test.combinations.combine(
      distribution=[
          tf.__internal__.distribute.combinations.one_device_strategy,
          tf.__internal__.distribute.combinations.mirrored_strategy_with_gpu_and_cpu,
          tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus,
          tf.__internal__.distribute.combinations
          .mirrored_strategy_with_two_gpus_no_merge_call,
      ],
      optimizer_fn=optimizers_v2)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag395')" href="javascript:;">
keras-2.8.0/keras/distribute/optimizer_combinations.py: 100-110
</a>
<div class="mid" id="frag395" style="display:none"><pre>
def distributions_and_v1_and_v2_optimizers():
  """A common set of combination with DistributionStrategies and Optimizers."""
  return tf.__internal__.test.combinations.combine(
      distribution=[
          tf.__internal__.distribute.combinations.one_device_strategy,
          tf.__internal__.distribute.combinations.mirrored_strategy_with_gpu_and_cpu,
          tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus,
          tf.__internal__.distribute.combinations
          .mirrored_strategy_with_two_gpus_no_merge_call,
      ],
      optimizer_fn=optimizers_v1_and_v2)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 15:</b> &nbsp; 3 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag441')" href="javascript:;">
keras-2.8.0/keras/distribute/custom_training_loop_models_test.py: 63-76
</a>
<div class="mid" id="frag441" style="display:none"><pre>
    def train_step(iterator):
      def step_fn(inputs):
        images, targets = inputs
        with tf.GradientTape() as tape:
          outputs = model(images)
          loss = keras.losses.mean_squared_error(targets, outputs)
        grads = tape.gradient(loss, model.variables)
        return grads

      outputs = distribution.run(
          step_fn, args=(next(iterator),))
      return tf.nest.map_structure(distribution.experimental_local_results,
                                outputs)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag481')" href="javascript:;">
keras-2.8.0/keras/distribute/custom_training_loop_models_test.py: 398-412
</a>
<div class="mid" id="frag481" style="display:none"><pre>
    def train_step(iterator):

      def step_fn(inputs):
        images, targets = inputs
        with tf.GradientTape() as tape:
          outputs = model(images)
          loss = keras.losses.mean_squared_error(targets, outputs)
        grads = tape.gradient(loss, model.variables)
        return grads

      outputs = distribution.run(
          step_fn, args=(next(iterator),))
      return tf.nest.map_structure(distribution.experimental_local_results,
                                outputs)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag493')" href="javascript:;">
keras-2.8.0/keras/distribute/custom_training_loop_models_test.py: 503-516
</a>
<div class="mid" id="frag493" style="display:none"><pre>
    def train_step(iterator):
      def step_fn(inputs):
        images, targets = inputs
        with tf.GradientTape() as tape:
          outputs = model(images)
          loss = keras.losses.mean_squared_error(targets, outputs)
        grads = tape.gradient(loss, model.variables)
        return grads

      outputs = distribution.run(
          step_fn, args=(next(iterator),))
      return tf.nest.map_structure(distribution.experimental_local_results,
                                outputs)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 16:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag591')" href="javascript:;">
keras-2.8.0/keras/distribute/dataset_creator_model_fit_ps_only_test.py: 54-69
</a>
<div class="mid" id="frag591" style="display:none"><pre>
  def testModelFitErrorOnBatchLevelCallbacks(self, strategy,
                                             use_dataset_creator):

    class BatchLevelCallback(callbacks_lib.Callback):

      def on_train_batch_end(self, batch, logs=None):
        pass

    with self.assertRaisesRegex(ValueError,
                                "Batch-level `Callback`s are not supported"):
      callbacks = [BatchLevelCallback()]
      self._model_fit(
          strategy,
          callbacks=callbacks,
          use_dataset_creator=use_dataset_creator)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag599')" href="javascript:;">
keras-2.8.0/keras/distribute/dataset_creator_model_fit_ps_only_test.py: 117-133
</a>
<div class="mid" id="frag599" style="display:none"><pre>
  def testModelEvaluateErrorOnBatchLevelCallbacks(self, strategy,
                                                  use_dataset_creator):

    class BatchLevelCallback(callbacks_lib.Callback):

      def on_train_batch_end(self, batch, logs=None):
        pass

    with self.assertRaisesRegex(ValueError,
                                "Batch-level `Callback`s are not supported"):
      callbacks = [BatchLevelCallback()]
      self._model_evaluate(
          strategy,
          callbacks=callbacks,
          use_dataset_creator=use_dataset_creator)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 17:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag715')" href="javascript:;">
keras-2.8.0/keras/distribute/distributed_file_utils_test.py: 74-85
</a>
<div class="mid" id="frag715" style="display:none"><pre>
  def testChiefDoesNotRemoveDirAndFilePath(self):
    temp_dir = self.get_temp_dir()
    strategy = DistributedFileUtilsTest.MockedChiefStrategy()
    dir_to_write = distributed_file_utils.write_dirpath(temp_dir, strategy)
    file_to_write = os.path.join(dir_to_write, 'tmp')
    self.assertFalse(os.path.exists(file_to_write))
    self._write_dummy_file(file_to_write)
    self.assertTrue(os.path.exists(file_to_write))
    distributed_file_utils.remove_temp_dir_with_filepath(
        file_to_write, strategy)
    self.assertTrue(os.path.exists(file_to_write))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag716')" href="javascript:;">
keras-2.8.0/keras/distribute/distributed_file_utils_test.py: 86-97
</a>
<div class="mid" id="frag716" style="display:none"><pre>
  def testWorkerDoesRemoveFilePath(self):
    temp_dir = self.get_temp_dir()
    strategy = DistributedFileUtilsTest.MockedWorkerStrategy()
    dir_to_write = distributed_file_utils.write_dirpath(temp_dir, strategy)
    file_to_write = os.path.join(dir_to_write, 'tmp')
    self.assertFalse(os.path.exists(file_to_write))
    self._write_dummy_file(file_to_write)
    self.assertTrue(os.path.exists(file_to_write))
    distributed_file_utils.remove_temp_dir_with_filepath(
        file_to_write, strategy)
    self.assertFalse(os.path.exists(file_to_write))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 18:</b> &nbsp; 3 fragments, nominal size 25 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag768')" href="javascript:;">
keras-2.8.0/keras/applications/resnet_v2.py: 30-58
</a>
<div class="mid" id="frag768" style="display:none"><pre>
def ResNet50V2(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'):
  """Instantiates the ResNet50V2 architecture."""
  def stack_fn(x):
    x = resnet.stack2(x, 64, 3, name='conv2')
    x = resnet.stack2(x, 128, 4, name='conv3')
    x = resnet.stack2(x, 256, 6, name='conv4')
    return resnet.stack2(x, 512, 3, stride1=1, name='conv5')

  return resnet.ResNet(
      stack_fn,
      True,
      True,
      'resnet50v2',
      include_top,
      weights,
      input_tensor,
      input_shape,
      pooling,
      classes,
      classifier_activation=classifier_activation)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag770')" href="javascript:;">
keras-2.8.0/keras/applications/resnet_v2.py: 61-89
</a>
<div class="mid" id="frag770" style="display:none"><pre>
def ResNet101V2(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'):
  """Instantiates the ResNet101V2 architecture."""
  def stack_fn(x):
    x = resnet.stack2(x, 64, 3, name='conv2')
    x = resnet.stack2(x, 128, 4, name='conv3')
    x = resnet.stack2(x, 256, 23, name='conv4')
    return resnet.stack2(x, 512, 3, stride1=1, name='conv5')

  return resnet.ResNet(
      stack_fn,
      True,
      True,
      'resnet101v2',
      include_top,
      weights,
      input_tensor,
      input_shape,
      pooling,
      classes,
      classifier_activation=classifier_activation)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag772')" href="javascript:;">
keras-2.8.0/keras/applications/resnet_v2.py: 92-120
</a>
<div class="mid" id="frag772" style="display:none"><pre>
def ResNet152V2(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'):
  """Instantiates the ResNet152V2 architecture."""
  def stack_fn(x):
    x = resnet.stack2(x, 64, 3, name='conv2')
    x = resnet.stack2(x, 128, 8, name='conv3')
    x = resnet.stack2(x, 256, 36, name='conv4')
    return resnet.stack2(x, 512, 3, stride1=1, name='conv5')

  return resnet.ResNet(
      stack_fn,
      True,
      True,
      'resnet152v2',
      include_top,
      weights,
      input_tensor,
      input_shape,
      pooling,
      classes,
      classifier_activation=classifier_activation)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 19:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag840')" href="javascript:;">
keras-2.8.0/keras/wrappers/scikit_learn_test.py: 114-129
</a>
<div class="mid" id="frag840" style="display:none"><pre>
  def test_classify_class_build_fn(self):

    class ClassBuildFnClf:

      def __call__(self, hidden_dim):
        return build_fn_clf(hidden_dim)

    with self.cached_session():
      clf = scikit_learn.KerasClassifier(
          build_fn=ClassBuildFnClf(),
          hidden_dim=HIDDEN_DIM,
          batch_size=BATCH_SIZE,
          epochs=EPOCHS)

      assert_classification_works(clf)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag845')" href="javascript:;">
keras-2.8.0/keras/wrappers/scikit_learn_test.py: 156-171
</a>
<div class="mid" id="frag845" style="display:none"><pre>
  def test_regression_class_build_fn(self):

    class ClassBuildFnReg:

      def __call__(self, hidden_dim):
        return build_fn_reg(hidden_dim)

    with self.cached_session():
      reg = scikit_learn.KerasRegressor(
          build_fn=ClassBuildFnReg(),
          hidden_dim=HIDDEN_DIM,
          batch_size=BATCH_SIZE,
          epochs=EPOCHS)

      assert_regression_works(reg)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 20:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag842')" href="javascript:;">
keras-2.8.0/keras/wrappers/scikit_learn_test.py: 130-145
</a>
<div class="mid" id="frag842" style="display:none"><pre>
  def test_classify_inherit_class_build_fn(self):

    class InheritClassBuildFnClf(scikit_learn.KerasClassifier):

      def __call__(self, hidden_dim):
        return build_fn_clf(hidden_dim)

    with self.cached_session():
      clf = InheritClassBuildFnClf(
          build_fn=None,
          hidden_dim=HIDDEN_DIM,
          batch_size=BATCH_SIZE,
          epochs=EPOCHS)

      assert_classification_works(clf)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag847')" href="javascript:;">
keras-2.8.0/keras/wrappers/scikit_learn_test.py: 172-187
</a>
<div class="mid" id="frag847" style="display:none"><pre>
  def test_regression_inherit_class_build_fn(self):

    class InheritClassBuildFnReg(scikit_learn.KerasRegressor):

      def __call__(self, hidden_dim):
        return build_fn_reg(hidden_dim)

    with self.cached_session():
      reg = InheritClassBuildFnReg(
          build_fn=None,
          hidden_dim=HIDDEN_DIM,
          batch_size=BATCH_SIZE,
          epochs=EPOCHS)

      assert_regression_works(reg)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 21:</b> &nbsp; 2 fragments, nominal size 30 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1147')" href="javascript:;">
keras-2.8.0/keras/optimizer_experimental/adam.py: 93-123
</a>
<div class="mid" id="frag1147" style="display:none"><pre>
  def __init__(self,
               learning_rate=0.001,
               beta_1=0.9,
               beta_2=0.999,
               epsilon=1e-7,
               amsgrad=False,
               clipnorm=None,
               clipvalue=None,
               global_clipnorm=None,
               use_ema=False,
               ema_momentum=0.99,
               ema_overwrite_frequency=100,
               jit_compile=False,
               name='Adam',
               **kwargs):
    super(Adam, self).__init__(
        name=name,
        clipnorm=clipnorm,
        clipvalue=clipvalue,
        global_clipnorm=global_clipnorm,
        use_ema=use_ema,
        ema_momentum=ema_momentum,
        ema_overwrite_frequency=ema_overwrite_frequency,
        jit_compile=jit_compile,
        **kwargs)
    self._learning_rate = self._build_learning_rate(learning_rate)
    self.beta_1 = beta_1
    self.beta_2 = beta_2
    self.epsilon = epsilon
    self.amsgrad = amsgrad

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1185')" href="javascript:;">
keras-2.8.0/keras/optimizer_experimental/rmsprop.py: 83-113
</a>
<div class="mid" id="frag1185" style="display:none"><pre>
  def __init__(self,
               learning_rate=0.001,
               rho=0.9,
               momentum=0.0,
               epsilon=1e-7,
               centered=False,
               clipnorm=None,
               clipvalue=None,
               global_clipnorm=None,
               use_ema=False,
               ema_momentum=0.99,
               ema_overwrite_frequency=100,
               jit_compile=False,
               name='RMSprop',
               **kwargs):
    super(RMSprop, self).__init__(
        clipnorm=clipnorm,
        clipvalue=clipvalue,
        global_clipnorm=global_clipnorm,
        use_ema=use_ema,
        ema_momentum=ema_momentum,
        ema_overwrite_frequency=ema_overwrite_frequency,
        jit_compile=jit_compile,
        name=name,
        **kwargs)
    self._learning_rate = self._build_learning_rate(learning_rate)
    self.rho = rho
    self.momentum = momentum
    self.epsilon = epsilon
    self.centered = centered

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 22:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1150')" href="javascript:;">
keras-2.8.0/keras/optimizer_experimental/adam.py: 198-208
</a>
<div class="mid" id="frag1150" style="display:none"><pre>
  def get_config(self):
    config = super(Adam, self).get_config()

    config.update({
        'learning_rate': self._serialize_hyperparameter(self._learning_rate),
        'beta_1': self.beta_1,
        'beta_2': self.beta_2,
        'epsilon': self.epsilon,
        'amsgrad': self.amsgrad,
    })
    return config
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1188')" href="javascript:;">
keras-2.8.0/keras/optimizer_experimental/rmsprop.py: 195-205
</a>
<div class="mid" id="frag1188" style="display:none"><pre>
  def get_config(self):
    config = super(RMSprop, self).get_config()

    config.update({
        'learning_rate': self._serialize_hyperparameter(self._learning_rate),
        'rho': self.rho,
        'momentum': self.momentum,
        'epsilon': self.epsilon,
        'centered': self.centered,
    })
    return config
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 23:</b> &nbsp; 2 fragments, nominal size 26 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1151')" href="javascript:;">
keras-2.8.0/keras/optimizer_experimental/adagrad.py: 65-91
</a>
<div class="mid" id="frag1151" style="display:none"><pre>
  def __init__(self,
               learning_rate=0.001,
               initial_accumulator_value=0.1,
               epsilon=1e-7,
               clipnorm=None,
               clipvalue=None,
               global_clipnorm=None,
               use_ema=False,
               ema_momentum=0.99,
               ema_overwrite_frequency=100,
               jit_compile=False,
               name='Adagrad',
               **kwargs):
    super(Adagrad, self).__init__(
        clipnorm=clipnorm,
        clipvalue=clipvalue,
        global_clipnorm=global_clipnorm,
        use_ema=use_ema,
        ema_momentum=ema_momentum,
        ema_overwrite_frequency=ema_overwrite_frequency,
        jit_compile=jit_compile,
        name=name,
        **kwargs)
    self._learning_rate = self._build_learning_rate(learning_rate)
    self.initial_accumulator_value = initial_accumulator_value
    self.epsilon = epsilon

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1193')" href="javascript:;">
keras-2.8.0/keras/optimizer_experimental/adadelta.py: 70-96
</a>
<div class="mid" id="frag1193" style="display:none"><pre>
  def __init__(self,
               learning_rate=0.001,
               rho=0.95,
               epsilon=1e-7,
               clipnorm=None,
               clipvalue=None,
               global_clipnorm=None,
               use_ema=False,
               ema_momentum=0.99,
               ema_overwrite_frequency=100,
               jit_compile=False,
               name='Adadelta',
               **kwargs):
    super(Adadelta, self).__init__(
        clipnorm=clipnorm,
        clipvalue=clipvalue,
        global_clipnorm=global_clipnorm,
        use_ema=use_ema,
        ema_momentum=ema_momentum,
        ema_overwrite_frequency=ema_overwrite_frequency,
        jit_compile=jit_compile,
        name=name,
        **kwargs)
    self._learning_rate = self._build_learning_rate(learning_rate)
    self.rho = rho
    self.epsilon = epsilon

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 24:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1295')" href="javascript:;">
keras-2.8.0/keras/utils/data_utils_test.py: 213-223
</a>
<div class="mid" id="frag1295" style="display:none"><pre>
  def test_ordered_enqueuer_threads(self):
    enqueuer = keras.utils.data_utils.OrderedEnqueuer(
        TestSequence([3, 200, 200, 3]), use_multiprocessing=False)
    enqueuer.start(3, 10)
    gen_output = enqueuer.get()
    acc = []
    for _ in range(100):
      acc.append(next(gen_output)[0, 0, 0, 0])
    self.assertEqual(acc, list(range(100)))
    enqueuer.stop()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1296')" href="javascript:;">
keras-2.8.0/keras/utils/data_utils_test.py: 225-235
</a>
<div class="mid" id="frag1296" style="display:none"><pre>
  def test_ordered_enqueuer_processes(self):
    enqueuer = keras.utils.data_utils.OrderedEnqueuer(
        TestSequence([3, 200, 200, 3]), use_multiprocessing=True)
    enqueuer.start(3, 10)
    gen_output = enqueuer.get()
    acc = []
    for _ in range(100):
      acc.append(next(gen_output)[0, 0, 0, 0])
    self.assertEqual(acc, list(range(100)))
    enqueuer.stop()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 25:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1705')" href="javascript:;">
keras-2.8.0/keras/keras_parameterized.py: 134-150
</a>
<div class="mid" id="frag1705" style="display:none"><pre>
  def single_method_decorator(f):
    """Decorator that constructs the test cases."""
    # Use named_parameters so it can be individually run from the command line
    @parameterized.named_parameters(*params)
    @functools.wraps(f)
    def decorated(self, saved_format, *args, **kwargs):
      """A run of a single test case w/ the specified model type."""
      if saved_format == 'h5':
        _test_h5_saved_model_format(f, self, *args, **kwargs)
      elif saved_format == 'tf':
        _test_tf_saved_model_format(f, self, *args, **kwargs)
      elif saved_format == 'tf_no_traces':
        _test_tf_saved_model_format_no_traces(f, self, *args, **kwargs)
      else:
        raise ValueError('Unknown model type: %s' % (saved_format,))
    return decorated

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1712')" href="javascript:;">
keras-2.8.0/keras/keras_parameterized.py: 274-290
</a>
<div class="mid" id="frag1712" style="display:none"><pre>
  def single_method_decorator(f):
    """Decorator that constructs the test cases."""
    # Use named_parameters so it can be individually run from the command line
    @parameterized.named_parameters(*params)
    @functools.wraps(f)
    def decorated(self, model_type, *args, **kwargs):
      """A run of a single test case w/ the specified model type."""
      if model_type == 'functional':
        _test_functional_model_type(f, self, *args, **kwargs)
      elif model_type == 'subclass':
        _test_subclass_model_type(f, self, *args, **kwargs)
      elif model_type == 'sequential':
        _test_sequential_model_type(f, self, *args, **kwargs)
      else:
        raise ValueError('Unknown model type: %s' % (model_type,))
    return decorated

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 26:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1727')" href="javascript:;">
keras-2.8.0/keras/integration_test/central_storage_strategy_test.py: 49-66
</a>
<div class="mid" id="frag1727" style="display:none"><pre>
      def train_step(iterator):
        """The step function for one training step."""

        def step_fn(inputs):
          """The computation to run on each replica."""
          features, labels = inputs
          with tf.GradientTape() as tape:
            pred = model(features, training=True)
            loss = tf.keras.losses.binary_crossentropy(labels, pred)
            loss = tf.nn.compute_average_loss(loss)
          grads = tape.gradient(loss, model.trainable_variables)
          optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))

          actual_pred = tf.cast(tf.math.greater(pred, 0.5), tf.dtypes.int64)
          accuracy.update_state(labels, actual_pred)

        distribution.run(step_fn, args=(next(iterator),))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1742')" href="javascript:;">
keras-2.8.0/keras/integration_test/tpu_strategy_test.py: 167-184
</a>
<div class="mid" id="frag1742" style="display:none"><pre>
      def train_step(iterator):
        """The step function for one training step."""

        def step_fn(inputs):
          """The computation to run on each TPU device."""
          features, labels = inputs
          with tf.GradientTape() as tape:
            pred = model(features, training=True)
            loss = tf.keras.losses.binary_crossentropy(labels, pred)
            loss = tf.nn.compute_average_loss(loss)
          grads = tape.gradient(loss, model.trainable_variables)
          optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))

          actual_pred = tf.cast(tf.math.greater(pred, 0.5), tf.dtypes.int64)
          accuracy.update_state(labels, actual_pred)

        strategy.run(step_fn, args=(next(iterator),))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 27:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1748')" href="javascript:;">
keras-2.8.0/keras/integration_test/parameter_server_keras_preprocessing_test.py: 71-82
</a>
<div class="mid" id="frag1748" style="display:none"><pre>
  def setUp(self):
    super(KPLTest, self).setUp()

    cluster_spec = create_in_process_cluster(num_workers=3, num_ps=2)
    cluster_resolver = tf.distribute.cluster_resolver.SimpleClusterResolver(
        cluster_spec, rpc_layer="grpc")
    self.strategy = tf.distribute.experimental.ParameterServerStrategy(
        cluster_resolver)
    self.coordinator = (
        tf.distribute.experimental.coordinator.ClusterCoordinator(
            self.strategy))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1758')" href="javascript:;">
keras-2.8.0/keras/integration_test/parameter_server_keras_preprocessing_test.py: 268-279
</a>
<div class="mid" id="frag1758" style="display:none"><pre>
  def setUp(self):
    super(KPLCreatedInDatasetsFromFunctionTest, self).setUp()

    cluster_spec = create_in_process_cluster(num_workers=3, num_ps=2)
    cluster_resolver = tf.distribute.cluster_resolver.SimpleClusterResolver(
        cluster_spec, rpc_layer="grpc")
    self.strategy = tf.distribute.experimental.ParameterServerStrategy(
        cluster_resolver)
    self.coordinator = (
        tf.distribute.experimental.coordinator.ClusterCoordinator(
            self.strategy))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 28:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1978')" href="javascript:;">
keras-2.8.0/keras/losses_test.py: 1696-1709
</a>
<div class="mid" id="frag1978" style="display:none"><pre>
  def test_scalar_weighted(self):
    self.setup()
    logcosh_obj = losses.LogCosh()
    sample_weight = 2.3

    loss = logcosh_obj(self.y_true, self.y_pred, sample_weight=sample_weight)
    expected_loss = sample_weight * np.sum(
        self.expected_losses) / self.batch_size
    self.assertAlmostEqual(self.evaluate(loss), expected_loss, 3)

    # Verify we get the same output when the same input is given
    loss_2 = logcosh_obj(self.y_true, self.y_pred, sample_weight=sample_weight)
    self.assertAlmostEqual(self.evaluate(loss), self.evaluate(loss_2), 3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1992')" href="javascript:;">
keras-2.8.0/keras/losses_test.py: 1854-1867
</a>
<div class="mid" id="frag1992" style="display:none"><pre>
  def test_scalar_weighted(self):
    self.setup()
    k_obj = losses.KLDivergence()
    sample_weight = 2.3

    loss = k_obj(self.y_true, self.y_pred, sample_weight=sample_weight)
    expected_loss = sample_weight * np.sum(
        self.expected_losses) / self.batch_size
    self.assertAlmostEqual(self.evaluate(loss), expected_loss, 3)

    # Verify we get the same output when the same input is given
    loss_2 = k_obj(self.y_true, self.y_pred, sample_weight=sample_weight)
    self.assertAlmostEqual(self.evaluate(loss), self.evaluate(loss_2), 3)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 29:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1979')" href="javascript:;">
keras-2.8.0/keras/losses_test.py: 1710-1722
</a>
<div class="mid" id="frag1979" style="display:none"><pre>
  def test_sample_weighted(self):
    self.setup()
    logcosh_obj = losses.LogCosh()

    sample_weight = tf.constant([1.2, 3.4], shape=(2, 1))
    loss = logcosh_obj(self.y_true, self.y_pred, sample_weight=sample_weight)

    expected_loss = np.multiply(
        self.expected_losses,
        np.asarray([1.2, 1.2, 1.2, 3.4, 3.4, 3.4]).reshape((2, 3)))
    expected_loss = np.sum(expected_loss) / self.batch_size
    self.assertAlmostEqual(self.evaluate(loss), expected_loss, 3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1986')" href="javascript:;">
keras-2.8.0/keras/losses_test.py: 1790-1802
</a>
<div class="mid" id="frag1986" style="display:none"><pre>
  def test_sample_weighted(self):
    self.setup()
    poisson_obj = losses.Poisson()

    sample_weight = tf.constant([1.2, 3.4], shape=(2, 1))
    loss = poisson_obj(self.y_true, self.y_pred, sample_weight=sample_weight)

    expected_loss = np.multiply(
        self.expected_losses,
        np.asarray([1.2, 1.2, 1.2, 3.4, 3.4, 3.4]).reshape((2, 3)))
    expected_loss = np.sum(expected_loss) / self.batch_size
    self.assertAlmostEqual(self.evaluate(loss), expected_loss, 3)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 30:</b> &nbsp; 2 fragments, nominal size 19 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2150')" href="javascript:;">
keras-2.8.0/keras/tests/model_subclassing_compiled_test.py: 299-322
</a>
<div class="mid" id="frag2150" style="display:none"><pre>
  def test_subclass_nested_in_subclass(self):
    num_classes = 2
    num_samples = 100
    input_dim = 50

    model = model_util.NestedTestModel1(num_classes=num_classes)
    model.compile(
        loss='mse',
        optimizer='rmsprop',
        metrics=['acc'],
        run_eagerly=testing_utils.should_run_eagerly())

    x = np.ones((num_samples, input_dim))
    y = np.zeros((num_samples, num_classes))

    model.fit(x, y, epochs=2, batch_size=32, verbose=0)
    _ = model.evaluate(x, y, verbose=0)

    self.assertEqual(len(model.weights), 8 + len(model.test_net.weights))
    self.assertEqual(len(model.non_trainable_weights),
                     2 + len(model.test_net.non_trainable_weights))
    self.assertEqual(len(model.trainable_weights),
                     6 + len(model.test_net.trainable_weights))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2151')" href="javascript:;">
keras-2.8.0/keras/tests/model_subclassing_compiled_test.py: 323-346
</a>
<div class="mid" id="frag2151" style="display:none"><pre>
  def test_graph_nested_in_subclass(self):
    num_classes = 2
    num_samples = 100
    input_dim = 50

    model = model_util.NestedTestModel2(num_classes=num_classes)
    model.compile(
        loss='mse',
        optimizer='rmsprop',
        metrics=['acc'],
        run_eagerly=testing_utils.should_run_eagerly())

    x = np.ones((num_samples, input_dim))
    y = np.zeros((num_samples, num_classes))

    model.fit(x, y, epochs=2, batch_size=32, verbose=0)
    _ = model.evaluate(x, y, verbose=0)

    self.assertEqual(len(model.weights), 8 + len(model.test_net.weights))
    self.assertEqual(len(model.non_trainable_weights),
                     2 + len(model.test_net.non_trainable_weights))
    self.assertEqual(len(model.trainable_weights),
                     6 + len(model.test_net.trainable_weights))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 31:</b> &nbsp; 4 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2303')" href="javascript:;">
keras-2.8.0/keras/initializers/initializers_test.py: 115-125
</a>
<div class="mid" id="frag2303" style="display:none"><pre>
  def test_lecun_uniform(self):
    tensor_shape = (5, 6, 4, 2)
    with self.cached_session():
      fan_in, _ = _compute_fans(tensor_shape)
      std = np.sqrt(1. / fan_in)
      self._runner(
          initializers.LecunUniformV2(seed=123),
          tensor_shape,
          target_mean=0.,
          target_std=std)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2308')" href="javascript:;">
keras-2.8.0/keras/initializers/initializers_test.py: 170-180
</a>
<div class="mid" id="frag2308" style="display:none"><pre>
  def test_he_normal(self):
    tensor_shape = (5, 6, 4, 2)
    with self.cached_session():
      fan_in, _ = _compute_fans(tensor_shape)
      std = np.sqrt(2. / fan_in)
      self._runner(
          initializers.HeNormalV2(seed=123),
          tensor_shape,
          target_mean=0.,
          target_std=std)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2306')" href="javascript:;">
keras-2.8.0/keras/initializers/initializers_test.py: 148-158
</a>
<div class="mid" id="frag2306" style="display:none"><pre>
  def test_lecun_normal(self):
    tensor_shape = (5, 6, 4, 2)
    with self.cached_session():
      fan_in, _ = _compute_fans(tensor_shape)
      std = np.sqrt(1. / fan_in)
      self._runner(
          initializers.LecunNormalV2(seed=123),
          tensor_shape,
          target_mean=0.,
          target_std=std)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2305')" href="javascript:;">
keras-2.8.0/keras/initializers/initializers_test.py: 137-147
</a>
<div class="mid" id="frag2305" style="display:none"><pre>
  def test_he_uniform(self):
    tensor_shape = (5, 6, 4, 2)
    with self.cached_session():
      fan_in, _ = _compute_fans(tensor_shape)
      std = np.sqrt(2. / fan_in)
      self._runner(
          initializers.HeUniformV2(seed=123),
          tensor_shape,
          target_mean=0.,
          target_std=std)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 32:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2304')" href="javascript:;">
keras-2.8.0/keras/initializers/initializers_test.py: 126-136
</a>
<div class="mid" id="frag2304" style="display:none"><pre>
  def test_glorot_uniform(self):
    tensor_shape = (5, 6, 4, 2)
    with self.cached_session():
      fan_in, fan_out = _compute_fans(tensor_shape)
      std = np.sqrt(2. / (fan_in + fan_out))
      self._runner(
          initializers.GlorotUniformV2(seed=123),
          tensor_shape,
          target_mean=0.,
          target_std=std)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2307')" href="javascript:;">
keras-2.8.0/keras/initializers/initializers_test.py: 159-169
</a>
<div class="mid" id="frag2307" style="display:none"><pre>
  def test_glorot_normal(self):
    tensor_shape = (5, 6, 4, 2)
    with self.cached_session():
      fan_in, fan_out = _compute_fans(tensor_shape)
      std = np.sqrt(2. / (fan_in + fan_out))
      self._runner(
          initializers.GlorotNormalV2(seed=123),
          tensor_shape,
          target_mean=0.,
          target_std=std)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 33:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2641')" href="javascript:;">
keras-2.8.0/keras/engine/ragged_keras_tensor_test.py: 216-231
</a>
<div class="mid" id="frag2641" style="display:none"><pre>
  def test_from_row_lengths(self):
    inp = layers.Input(shape=[None])
    out = tf.RaggedTensor.from_row_lengths(
        inp, row_lengths=[4, 0, 3, 1, 0])
    model = training.Model(inp, out)

    x = tf.constant([3, 1, 4, 1, 5, 9, 2, 6])
    expected = tf.RaggedTensor.from_row_lengths(
        x, row_lengths=[4, 0, 3, 1, 0])
    self.assertAllEqual(model(x), expected)

    # Test that the model can serialize and deserialize as well
    model_config = model.get_config()
    model2 = training.Model.from_config(model_config)
    self.assertAllEqual(model2(x), expected)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2642')" href="javascript:;">
keras-2.8.0/keras/engine/ragged_keras_tensor_test.py: 232-247
</a>
<div class="mid" id="frag2642" style="display:none"><pre>
  def test_from_row_starts(self):
    inp = layers.Input(shape=[None])
    out = tf.RaggedTensor.from_row_starts(
        inp, row_starts=[0, 4, 4, 7, 8])
    model = training.Model(inp, out)

    x = tf.constant([3, 1, 4, 1, 5, 9, 2, 6])
    expected = tf.RaggedTensor.from_row_starts(
        x, row_starts=[0, 4, 4, 7, 8])
    self.assertAllEqual(model(x), expected)

    # Test that the model can serialize and deserialize as well
    model_config = model.get_config()
    model2 = training.Model.from_config(model_config)
    self.assertAllEqual(model2(x), expected)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 34:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2807')" href="javascript:;">
keras-2.8.0/keras/engine/data_adapter_test.py: 489-502
</a>
<div class="mid" id="frag2807" style="display:none"><pre>
  def test_training_numpy_target(self):
    self.model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd',
                       run_eagerly=testing_utils.should_run_eagerly())
    self.model.fit(self.arraylike_input,
                   self.numpy_target, batch_size=5)
    self.model.fit(self.arraylike_input,
                   self.numpy_target, shuffle=True,
                   batch_size=5)
    self.model.fit(self.arraylike_input,
                   self.numpy_target, shuffle='batch',
                   batch_size=5)
    self.model.evaluate(self.arraylike_input,
                        self.numpy_target, batch_size=5)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2808')" href="javascript:;">
keras-2.8.0/keras/engine/data_adapter_test.py: 504-517
</a>
<div class="mid" id="frag2808" style="display:none"><pre>
  def test_training_tensor_target(self):
    self.model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd',
                       run_eagerly=testing_utils.should_run_eagerly())
    self.model.fit(self.arraylike_input,
                   self.tensor_target, batch_size=5)
    self.model.fit(self.arraylike_input,
                   self.tensor_target, shuffle=True,
                   batch_size=5)
    self.model.fit(self.arraylike_input,
                   self.tensor_target, shuffle='batch',
                   batch_size=5)
    self.model.evaluate(self.arraylike_input,
                        self.tensor_target, batch_size=5)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 35:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3084')" href="javascript:;">
keras-2.8.0/keras/feature_column/dense_features_test.py: 263-280
</a>
<div class="mid" id="frag3084" style="display:none"><pre>
  def test_compute_output_shape(self):
    price1 = tf.feature_column.numeric_column('price1', shape=2)
    price2 = tf.feature_column.numeric_column('price2', shape=4)
    with tf.Graph().as_default():
      features = {
          'price1': [[1., 2.], [5., 6.]],
          'price2': [[3., 4., 5., 6.], [7., 8., 9., 10.]]
      }
      dense_features = df.DenseFeatures([price1, price2])
      self.assertEqual((None, 6), dense_features.compute_output_shape((None,)))
      net = dense_features(features)

      self.evaluate(tf.compat.v1.global_variables_initializer())
      self.evaluate(tf.compat.v1.tables_initializer())

      self.assertAllClose([[1., 2., 3., 4., 5., 6.], [5., 6., 7., 8., 9., 10.]],
                          self.evaluate(net))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3135')" href="javascript:;">
keras-2.8.0/keras/feature_column/dense_features_v2_test.py: 249-266
</a>
<div class="mid" id="frag3135" style="display:none"><pre>
  def test_compute_output_shape(self):
    price1 = tf.feature_column.numeric_column('price1', shape=2)
    price2 = tf.feature_column.numeric_column('price2', shape=4)
    with tf.Graph().as_default():
      features = {
          'price1': [[1., 2.], [5., 6.]],
          'price2': [[3., 4., 5., 6.], [7., 8., 9., 10.]]
      }
      dense_features = df.DenseFeatures([price1, price2])
      self.assertEqual((None, 6), dense_features.compute_output_shape((None,)))
      net = dense_features(features)

      self.evaluate(tf.compat.v1.global_variables_initializer())
      self.evaluate(tf.compat.v1.tables_initializer())

      self.assertAllClose([[1., 2., 3., 4., 5., 6.], [5., 6., 7., 8., 9., 10.]],
                          self.evaluate(net))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 36:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3088')" href="javascript:;">
keras-2.8.0/keras/feature_column/dense_features_test.py: 313-329
</a>
<div class="mid" id="frag3088" style="display:none"><pre>
  def test_cols_to_output_tensors(self):
    price1 = tf.feature_column.numeric_column('price1', shape=2)
    price2 = tf.feature_column.numeric_column('price2')
    with tf.Graph().as_default():
      cols_dict = {}
      features = {'price1': [[1., 2.], [5., 6.]], 'price2': [[3.], [4.]]}
      dense_features = df.DenseFeatures([price1, price2])
      net = dense_features(features, cols_dict)

      self.evaluate(tf.compat.v1.global_variables_initializer())
      self.evaluate(tf.compat.v1.tables_initializer())

      self.assertAllClose([[1., 2.], [5., 6.]],
                          self.evaluate(cols_dict[price1]))
      self.assertAllClose([[3.], [4.]], self.evaluate(cols_dict[price2]))
      self.assertAllClose([[1., 2., 3.], [5., 6., 4.]], self.evaluate(net))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3139')" href="javascript:;">
keras-2.8.0/keras/feature_column/dense_features_v2_test.py: 299-315
</a>
<div class="mid" id="frag3139" style="display:none"><pre>
  def test_cols_to_output_tensors(self):
    price1 = tf.feature_column.numeric_column('price1', shape=2)
    price2 = tf.feature_column.numeric_column('price2')
    with tf.Graph().as_default():
      cols_dict = {}
      features = {'price1': [[1., 2.], [5., 6.]], 'price2': [[3.], [4.]]}
      dense_features = df.DenseFeatures([price1, price2])
      net = dense_features(features, cols_dict)

      self.evaluate(tf.compat.v1.global_variables_initializer())
      self.evaluate(tf.compat.v1.tables_initializer())

      self.assertAllClose([[1., 2.], [5., 6.]],
                          self.evaluate(cols_dict[price1]))
      self.assertAllClose([[3.], [4.]], self.evaluate(cols_dict[price2]))
      self.assertAllClose([[1., 2., 3.], [5., 6., 4.]], self.evaluate(net))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 37:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3089')" href="javascript:;">
keras-2.8.0/keras/feature_column/dense_features_test.py: 330-346
</a>
<div class="mid" id="frag3089" style="display:none"><pre>
  def test_column_order(self):
    price_a = tf.feature_column.numeric_column('price_a')
    price_b = tf.feature_column.numeric_column('price_b')
    with tf.Graph().as_default():
      features = {
          'price_a': [[1.]],
          'price_b': [[3.]],
      }
      net1 = df.DenseFeatures([price_a, price_b])(features)
      net2 = df.DenseFeatures([price_b, price_a])(features)

      self.evaluate(tf.compat.v1.global_variables_initializer())
      self.evaluate(tf.compat.v1.tables_initializer())

      self.assertAllClose([[1., 3.]], self.evaluate(net1))
      self.assertAllClose([[1., 3.]], self.evaluate(net2))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3140')" href="javascript:;">
keras-2.8.0/keras/feature_column/dense_features_v2_test.py: 316-332
</a>
<div class="mid" id="frag3140" style="display:none"><pre>
  def test_column_order(self):
    price_a = tf.feature_column.numeric_column('price_a')
    price_b = tf.feature_column.numeric_column('price_b')
    with tf.Graph().as_default():
      features = {
          'price_a': [[1.]],
          'price_b': [[3.]],
      }
      net1 = df.DenseFeatures([price_a, price_b])(features)
      net2 = df.DenseFeatures([price_b, price_a])(features)

      self.evaluate(tf.compat.v1.global_variables_initializer())
      self.evaluate(tf.compat.v1.tables_initializer())

      self.assertAllClose([[1., 3.]], self.evaluate(net1))
      self.assertAllClose([[1., 3.]], self.evaluate(net2))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 38:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3091')" href="javascript:;">
keras-2.8.0/keras/feature_column/dense_features_test.py: 359-371
</a>
<div class="mid" id="frag3091" style="display:none"><pre>
  def test_static_batch_size_mismatch(self):
    price1 = tf.feature_column.numeric_column('price1')
    price2 = tf.feature_column.numeric_column('price2')
    with tf.Graph().as_default():
      features = {
          'price1': [[1.], [5.], [7.]],  # batchsize = 3
          'price2': [[3.], [4.]]  # batchsize = 2
      }
      with self.assertRaisesRegex(
          ValueError,
          r'Batch size \(first dimension\) of each feature must be same.'):  # pylint: disable=anomalous-backslash-in-string
        df.DenseFeatures([price1, price2])(features)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3142')" href="javascript:;">
keras-2.8.0/keras/feature_column/dense_features_v2_test.py: 344-356
</a>
<div class="mid" id="frag3142" style="display:none"><pre>
  def test_static_batch_size_mismatch(self):
    price1 = tf.feature_column.numeric_column('price1')
    price2 = tf.feature_column.numeric_column('price2')
    with tf.Graph().as_default():
      features = {
          'price1': [[1.], [5.], [7.]],  # batchsize = 3
          'price2': [[3.], [4.]]  # batchsize = 2
      }
      with self.assertRaisesRegex(
          ValueError,
          r'Batch size \(first dimension\) of each feature must be same.'):  # pylint: disable=anomalous-backslash-in-string
        df.DenseFeatures([price1, price2])(features)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 39:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3092')" href="javascript:;">
keras-2.8.0/keras/feature_column/dense_features_test.py: 372-386
</a>
<div class="mid" id="frag3092" style="display:none"><pre>
  def test_subset_of_static_batch_size_mismatch(self):
    price1 = tf.feature_column.numeric_column('price1')
    price2 = tf.feature_column.numeric_column('price2')
    price3 = tf.feature_column.numeric_column('price3')
    with tf.Graph().as_default():
      features = {
          'price1': tf.compat.v1.placeholder(dtype=tf.int64),  # batchsize = 3
          'price2': [[3.], [4.]],  # batchsize = 2
          'price3': [[3.], [4.], [5.]]  # batchsize = 3
      }
      with self.assertRaisesRegex(
          ValueError,
          r'Batch size \(first dimension\) of each feature must be same.'):  # pylint: disable=anomalous-backslash-in-string
        df.DenseFeatures([price1, price2, price3])(features)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3143')" href="javascript:;">
keras-2.8.0/keras/feature_column/dense_features_v2_test.py: 357-371
</a>
<div class="mid" id="frag3143" style="display:none"><pre>
  def test_subset_of_static_batch_size_mismatch(self):
    price1 = tf.feature_column.numeric_column('price1')
    price2 = tf.feature_column.numeric_column('price2')
    price3 = tf.feature_column.numeric_column('price3')
    with tf.Graph().as_default():
      features = {
          'price1': tf.compat.v1.placeholder(dtype=tf.int64),  # batchsize = 3
          'price2': [[3.], [4.]],  # batchsize = 2
          'price3': [[3.], [4.], [5.]]  # batchsize = 3
      }
      with self.assertRaisesRegex(
          ValueError,
          r'Batch size \(first dimension\) of each feature must be same.'):  # pylint: disable=anomalous-backslash-in-string
        df.DenseFeatures([price1, price2, price3])(features)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 40:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3093')" href="javascript:;">
keras-2.8.0/keras/feature_column/dense_features_test.py: 387-401
</a>
<div class="mid" id="frag3093" style="display:none"><pre>
  def test_runtime_batch_size_mismatch(self):
    price1 = tf.feature_column.numeric_column('price1')
    price2 = tf.feature_column.numeric_column('price2')
    with tf.Graph().as_default():
      features = {
          'price1': tf.compat.v1.placeholder(dtype=tf.int64),  # batchsize = 3
          'price2': [[3.], [4.]]  # batchsize = 2
      }
      net = df.DenseFeatures([price1, price2])(features)
      with _initialized_session() as sess:
        with self.assertRaisesRegex(tf.errors.OpError,
                                    'Dimension 0 in both shapes must be equal|'
                                    'Dimensions of inputs should match'):
          sess.run(net, feed_dict={features['price1']: [[1.], [5.], [7.]]})

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3144')" href="javascript:;">
keras-2.8.0/keras/feature_column/dense_features_v2_test.py: 372-386
</a>
<div class="mid" id="frag3144" style="display:none"><pre>
  def test_runtime_batch_size_mismatch(self):
    price1 = tf.feature_column.numeric_column('price1')
    price2 = tf.feature_column.numeric_column('price2')
    with tf.Graph().as_default():
      features = {
          'price1': tf.compat.v1.placeholder(dtype=tf.int64),  # batchsize = 3
          'price2': [[3.], [4.]]  # batchsize = 2
      }
      net = df.DenseFeatures([price1, price2])(features)
      with _initialized_session() as sess:
        with self.assertRaisesRegex(tf.errors.OpError,
                                    'Dimension 0 in both shapes must be equal|'
                                    'Dimensions of inputs should match'):
          sess.run(net, feed_dict={features['price1']: [[1.], [5.], [7.]]})

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 41:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3094')" href="javascript:;">
keras-2.8.0/keras/feature_column/dense_features_test.py: 402-418
</a>
<div class="mid" id="frag3094" style="display:none"><pre>
  def test_runtime_batch_size_matches(self):
    price1 = tf.feature_column.numeric_column('price1')
    price2 = tf.feature_column.numeric_column('price2')
    with tf.Graph().as_default():
      features = {
          'price1': tf.compat.v1.placeholder(dtype=tf.int64),  # batchsize = 2
          'price2': tf.compat.v1.placeholder(dtype=tf.int64),  # batchsize = 2
      }
      net = df.DenseFeatures([price1, price2])(features)
      with _initialized_session() as sess:
        sess.run(
            net,
            feed_dict={
                features['price1']: [[1.], [5.]],
                features['price2']: [[1.], [5.]],
            })

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3145')" href="javascript:;">
keras-2.8.0/keras/feature_column/dense_features_v2_test.py: 387-403
</a>
<div class="mid" id="frag3145" style="display:none"><pre>
  def test_runtime_batch_size_matches(self):
    price1 = tf.feature_column.numeric_column('price1')
    price2 = tf.feature_column.numeric_column('price2')
    with tf.Graph().as_default():
      features = {
          'price1': tf.compat.v1.placeholder(dtype=tf.int64),  # batchsize = 2
          'price2': tf.compat.v1.placeholder(dtype=tf.int64),  # batchsize = 2
      }
      net = df.DenseFeatures([price1, price2])(features)
      with _initialized_session() as sess:
        sess.run(
            net,
            feed_dict={
                features['price1']: [[1.], [5.]],
                features['price2']: [[1.], [5.]],
            })

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 42:</b> &nbsp; 2 fragments, nominal size 31 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3182')" href="javascript:;">
keras-2.8.0/keras/legacy_tf_layers/normalization_test.py: 307-349
</a>
<div class="mid" id="frag3182" style="display:none"><pre>
  def test3DInputAxis1(self):
    epsilon = 1e-3
    bn = normalization_layers.BatchNormalization(
        axis=1, epsilon=epsilon, momentum=0.9)
    inputs = tf.Variable(
        np.random.random((5, 4, 3)) + 100, dtype=tf.float32)
    training = tf.compat.v1.placeholder(dtype='bool')
    outputs = bn.apply(inputs, training=training)

    with self.cached_session() as sess:
      # Test training with placeholder learning phase.
      self.evaluate(tf.compat.v1.global_variables_initializer())

      np_gamma, np_beta = self.evaluate([bn.gamma, bn.beta])
      np_gamma = np.reshape(np_gamma, (1, 4, 1))
      np_beta = np.reshape(np_beta, (1, 4, 1))

      for _ in range(100):
        np_output, _, _ = sess.run([outputs] + bn.updates,
                                   feed_dict={training: True})
        # Verify that the axis is normalized during training.
        normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
        self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
        self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

      # Verify that the statistics are updated during training.
      moving_mean, moving_var = self.evaluate(
          [bn.moving_mean, bn.moving_variance])
      np_inputs = self.evaluate(inputs)
      mean = np.mean(np_inputs, axis=(0, 2))
      std = np.std(np_inputs, axis=(0, 2))
      variance = np.square(std)
      self.assertAllClose(mean, moving_mean, atol=1e-2)
      self.assertAllClose(variance, moving_var, atol=1e-2)

      # Test inference with placeholder learning phase.
      np_output = sess.run(outputs, feed_dict={training: False})

      # Verify that the axis is normalized during inference.
      normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
      self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
      self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3183')" href="javascript:;">
keras-2.8.0/keras/legacy_tf_layers/normalization_test.py: 350-390
</a>
<div class="mid" id="frag3183" style="display:none"><pre>
  def test3DInputAxis2(self):
    epsilon = 1e-3
    bn = normalization_layers.BatchNormalization(
        axis=2, epsilon=epsilon, momentum=0.9)
    inputs = tf.Variable(
        np.random.random((5, 4, 3)) + 100, dtype=tf.float32)
    training = tf.compat.v1.placeholder(dtype='bool')
    outputs = bn.apply(inputs, training=training)

    with self.cached_session() as sess:
      # Test training with placeholder learning phase.
      self.evaluate(tf.compat.v1.global_variables_initializer())
      np_gamma, np_beta = self.evaluate([bn.gamma, bn.beta])
      np_gamma = np.reshape(np_gamma, (1, 1, 3))
      np_beta = np.reshape(np_beta, (1, 1, 3))
      for _ in range(100):
        np_output, _, _ = sess.run([outputs] + bn.updates,
                                   feed_dict={training: True})
        # Verify that the axis is normalized during training.
        normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
        self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
        self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

      # Verify that the statistics are updated during training.
      moving_mean, moving_var = self.evaluate(
          [bn.moving_mean, bn.moving_variance])
      np_inputs = self.evaluate(inputs)
      mean = np.mean(np_inputs, axis=(0, 1))
      std = np.std(np_inputs, axis=(0, 1))
      variance = np.square(std)
      self.assertAllClose(mean, moving_mean, atol=1e-2)
      self.assertAllClose(variance, moving_var, atol=1e-2)

      # Test inference with placeholder learning phase.
      np_output = sess.run(outputs, feed_dict={training: False})

      # Verify that the axis is normalized during inference.
      normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
      self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
      self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 43:</b> &nbsp; 2 fragments, nominal size 31 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3185')" href="javascript:;">
keras-2.8.0/keras/legacy_tf_layers/normalization_test.py: 433-473
</a>
<div class="mid" id="frag3185" style="display:none"><pre>
  def test4DInputAxis2(self):
    epsilon = 1e-3
    bn = normalization_layers.BatchNormalization(
        axis=2, epsilon=epsilon, momentum=0.9)
    inputs = tf.Variable(
        np.random.random((5, 4, 3, 6)) + 100, dtype=tf.float32)
    training = tf.compat.v1.placeholder(dtype='bool')
    outputs = bn.apply(inputs, training=training)

    with self.cached_session() as sess:
      # Test training with placeholder learning phase.
      self.evaluate(tf.compat.v1.global_variables_initializer())
      np_gamma, np_beta = self.evaluate([bn.gamma, bn.beta])
      np_gamma = np.reshape(np_gamma, (1, 1, 3, 1))
      np_beta = np.reshape(np_beta, (1, 1, 3, 1))
      for _ in range(100):
        np_output, _, _ = sess.run([outputs] + bn.updates,
                                   feed_dict={training: True})
        # Verify that the axis is normalized during training.
        normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
        self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
        self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

      # Verify that the statistics are updated during training.
      moving_mean, moving_var = self.evaluate(
          [bn.moving_mean, bn.moving_variance])
      np_inputs = self.evaluate(inputs)
      mean = np.mean(np_inputs, axis=(0, 1, 3))
      std = np.std(np_inputs, axis=(0, 1, 3))
      variance = np.square(std)
      self.assertAllClose(mean, moving_mean, atol=1e-2)
      self.assertAllClose(variance, moving_var, atol=1e-2)

      # Test inference with placeholder learning phase.
      np_output = sess.run(outputs, feed_dict={training: False})

      # Verify that the axis is normalized during inference.
      normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
      self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
      self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3186')" href="javascript:;">
keras-2.8.0/keras/legacy_tf_layers/normalization_test.py: 474-514
</a>
<div class="mid" id="frag3186" style="display:none"><pre>
  def test4DInputAxis3(self):
    epsilon = 1e-3
    bn = normalization_layers.BatchNormalization(
        axis=3, epsilon=epsilon, momentum=0.9)
    inputs = tf.Variable(
        np.random.random((5, 4, 3, 6)) + 100, dtype=tf.float32)
    training = tf.compat.v1.placeholder(dtype='bool')
    outputs = bn.apply(inputs, training=training)

    with self.cached_session() as sess:
      # Test training with placeholder learning phase.
      self.evaluate(tf.compat.v1.global_variables_initializer())
      np_gamma, np_beta = self.evaluate([bn.gamma, bn.beta])
      np_gamma = np.reshape(np_gamma, (1, 1, 1, 6))
      np_beta = np.reshape(np_beta, (1, 1, 1, 6))
      for _ in range(100):
        np_output, _, _ = sess.run([outputs] + bn.updates,
                                   feed_dict={training: True})
        # Verify that the axis is normalized during training.
        normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
        self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
        self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

      # Verify that the statistics are updated during training.
      moving_mean, moving_var = self.evaluate(
          [bn.moving_mean, bn.moving_variance])
      np_inputs = self.evaluate(inputs)
      mean = np.mean(np_inputs, axis=(0, 1, 2))
      std = np.std(np_inputs, axis=(0, 1, 2))
      variance = np.square(std)
      self.assertAllClose(mean, moving_mean, atol=1e-2)
      self.assertAllClose(variance, moving_var, atol=1e-2)

      # Test inference with placeholder learning phase.
      np_output = sess.run(outputs, feed_dict={training: False})

      # Verify that the axis is normalized during inference.
      normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
      self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
      self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 44:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3194')" href="javascript:;">
keras-2.8.0/keras/legacy_tf_layers/normalization_test.py: 810-824
</a>
<div class="mid" id="frag3194" style="display:none"><pre>
  def testNoCenter(self):
    bn = normalization_layers.BatchNormalization(axis=1, center=False)
    inputs = tf.random.uniform((5, 4, 3), seed=1)
    training = tf.compat.v1.placeholder(dtype='bool')
    outputs = bn.apply(inputs, training=training)

    # Verify shape.
    self.assertListEqual(outputs.get_shape().as_list(), [5, 4, 3])

    # Verify layer attributes.
    self.assertEqual(len(bn.updates), 2)
    self.assertEqual(len(bn.variables), 3)
    self.assertEqual(len(bn.trainable_variables), 1)
    self.assertEqual(len(bn.non_trainable_variables), 2)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3195')" href="javascript:;">
keras-2.8.0/keras/legacy_tf_layers/normalization_test.py: 825-839
</a>
<div class="mid" id="frag3195" style="display:none"><pre>
  def testNoScale(self):
    bn = normalization_layers.BatchNormalization(axis=1, scale=False)
    inputs = tf.random.uniform((5, 4, 3), seed=1)
    training = tf.compat.v1.placeholder(dtype='bool')
    outputs = bn.apply(inputs, training=training)

    # Verify shape.
    self.assertListEqual(outputs.get_shape().as_list(), [5, 4, 3])

    # Verify layer attributes.
    self.assertEqual(len(bn.updates), 2)
    self.assertEqual(len(bn.variables), 3)
    self.assertEqual(len(bn.trainable_variables), 1)
    self.assertEqual(len(bn.non_trainable_variables), 2)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 45:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3241')" href="javascript:;">
keras-2.8.0/keras/legacy_tf_layers/pooling.py: 80-93
</a>
<div class="mid" id="frag3241" style="display:none"><pre>
  def __init__(self, pool_size, strides,
               padding='valid', data_format='channels_last',
               name=None, **kwargs):
    if strides is None:
      raise ValueError('Argument `strides` must not be None.')
    super(AveragePooling1D, self).__init__(
        pool_size=pool_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        name=name,
        **kwargs)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3243')" href="javascript:;">
keras-2.8.0/keras/legacy_tf_layers/pooling.py: 222-235
</a>
<div class="mid" id="frag3243" style="display:none"><pre>
  def __init__(self, pool_size, strides,
               padding='valid', data_format='channels_last',
               name=None, **kwargs):
    if strides is None:
      raise ValueError('Argument `strides` must not be None.')
    super(MaxPooling1D, self).__init__(
        pool_size=pool_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        name=name,
        **kwargs)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 46:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3242')" href="javascript:;">
keras-2.8.0/keras/legacy_tf_layers/pooling.py: 96-170
</a>
<div class="mid" id="frag3242" style="display:none"><pre>
def average_pooling1d(inputs, pool_size, strides,
                      padding='valid', data_format='channels_last',
                      name=None):
  """Average Pooling layer for 1D inputs.

  Args:
    inputs: The tensor over which to pool. Must have rank 3.
    pool_size: An integer or tuple/list of a single integer,
      representing the size of the pooling window.
    strides: An integer or tuple/list of a single integer, specifying the
      strides of the pooling operation.
    padding: A string. The padding method, either 'valid' or 'same'.
      Case-insensitive.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, length, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, length)`.
    name: A string, the name of the layer.

  Returns:
    The output tensor, of rank 3.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.AveragePooling1D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.average_pooling1d(x, pool_size=2, strides=2)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.AveragePooling1D(pool_size=2, strides=2)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.average_pooling1d` is deprecated and '
      'will be removed in a future version. '
      'Please use `tf.keras.layers.AveragePooling1D` instead.',
      stacklevel=2)
  layer = AveragePooling1D(pool_size=pool_size,
                           strides=strides,
                           padding=padding,
                           data_format=data_format,
                           name=name)
  return layer.apply(inputs)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3244')" href="javascript:;">
keras-2.8.0/keras/legacy_tf_layers/pooling.py: 238-312
</a>
<div class="mid" id="frag3244" style="display:none"><pre>
def max_pooling1d(inputs, pool_size, strides,
                  padding='valid', data_format='channels_last',
                  name=None):
  """Max Pooling layer for 1D inputs.

  Args:
    inputs: The tensor over which to pool. Must have rank 3.
    pool_size: An integer or tuple/list of a single integer,
      representing the size of the pooling window.
    strides: An integer or tuple/list of a single integer, specifying the
      strides of the pooling operation.
    padding: A string. The padding method, either 'valid' or 'same'.
      Case-insensitive.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, length, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, length)`.
    name: A string, the name of the layer.

  Returns:
    The output tensor, of rank 3.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.MaxPooling1D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.max_pooling1d(x, pool_size=2, strides=2)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.MaxPooling1D(pool_size=2, strides=2)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.max_pooling1d` is deprecated and '
      'will be removed in a future version. '
      'Please use `tf.keras.layers.MaxPooling1D` instead.',
      stacklevel=2)
  layer = MaxPooling1D(pool_size=pool_size,
                       strides=strides,
                       padding=padding,
                       data_format=data_format,
                       name=name)
  return layer.apply(inputs)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 47:</b> &nbsp; 4 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3246')" href="javascript:;">
keras-2.8.0/keras/legacy_tf_layers/pooling.py: 380-457
</a>
<div class="mid" id="frag3246" style="display:none"><pre>
def average_pooling2d(inputs,
                      pool_size, strides,
                      padding='valid', data_format='channels_last',
                      name=None):
  """Average pooling layer for 2D inputs (e.g. images).

  Args:
    inputs: The tensor over which to pool. Must have rank 4.
    pool_size: An integer or tuple/list of 2 integers: (pool_height, pool_width)
      specifying the size of the pooling window.
      Can be a single integer to specify the same value for
      all spatial dimensions.
    strides: An integer or tuple/list of 2 integers,
      specifying the strides of the pooling operation.
      Can be a single integer to specify the same value for
      all spatial dimensions.
    padding: A string. The padding method, either 'valid' or 'same'.
      Case-insensitive.
    data_format: A string. The ordering of the dimensions in the inputs.
      `channels_last` (default) and `channels_first` are supported.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, height, width)`.
    name: A string, the name of the layer.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.AveragePooling2D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.average_pooling2d(x, pool_size=2, strides=2)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.AveragePooling2D(pool_size=2, strides=2)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.average_pooling2d` is deprecated and '
      'will be removed in a future version. '
      'Please use `tf.keras.layers.AveragePooling2D` instead.',
      stacklevel=2)
  layer = AveragePooling2D(pool_size=pool_size, strides=strides,
                           padding=padding, data_format=data_format,
                           name=name)
  return layer.apply(inputs)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3250')" href="javascript:;">
keras-2.8.0/keras/legacy_tf_layers/pooling.py: 672-751
</a>
<div class="mid" id="frag3250" style="display:none"><pre>
def average_pooling3d(inputs,
                      pool_size, strides,
                      padding='valid', data_format='channels_last',
                      name=None):
  """Average pooling layer for 3D inputs (e.g. volumes).

  Args:
    inputs: The tensor over which to pool. Must have rank 5.
    pool_size: An integer or tuple/list of 3 integers:
      (pool_depth, pool_height, pool_width)
      specifying the size of the pooling window.
      Can be a single integer to specify the same value for
      all spatial dimensions.
    strides: An integer or tuple/list of 3 integers,
      specifying the strides of the pooling operation.
      Can be a single integer to specify the same value for
      all spatial dimensions.
    padding: A string. The padding method, either 'valid' or 'same'.
      Case-insensitive.
    data_format: A string. The ordering of the dimensions in the inputs.
      `channels_last` (default) and `channels_first` are supported.
      `channels_last` corresponds to inputs with shape
      `(batch, depth, height, width, channels)` while `channels_first`
      corresponds to inputs with shape
      `(batch, channels, depth, height, width)`.
    name: A string, the name of the layer.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.AveragePooling3D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.average_pooling3d(x, pool_size=2, strides=2)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.AveragePooling3D(pool_size=2, strides=2)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.average_pooling3d` is deprecated and '
      'will be removed in a future version. '
      'Please use `tf.keras.layers.AveragePooling3D` instead.',
      stacklevel=2)
  layer = AveragePooling3D(pool_size=pool_size, strides=strides,
                           padding=padding, data_format=data_format,
                           name=name)
  return layer.apply(inputs)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3252')" href="javascript:;">
keras-2.8.0/keras/legacy_tf_layers/pooling.py: 821-899
</a>
<div class="mid" id="frag3252" style="display:none"><pre>
def max_pooling3d(inputs,
                  pool_size, strides,
                  padding='valid', data_format='channels_last',
                  name=None):
  """Max pooling layer for 3D inputs (e.g.

  volumes).

  Args:
    inputs: The tensor over which to pool. Must have rank 5.
    pool_size: An integer or tuple/list of 3 integers: (pool_depth, pool_height,
      pool_width) specifying the size of the pooling window. Can be a single
      integer to specify the same value for all spatial dimensions.
    strides: An integer or tuple/list of 3 integers, specifying the strides of
      the pooling operation. Can be a single integer to specify the same value
      for all spatial dimensions.
    padding: A string. The padding method, either 'valid' or 'same'.
      Case-insensitive.
    data_format: A string. The ordering of the dimensions in the inputs.
      `channels_last` (default) and `channels_first` are supported.
      `channels_last` corresponds to inputs with shape `(batch, depth, height,
      width, channels)` while `channels_first` corresponds to inputs with shape
      `(batch, channels, depth, height, width)`.
    name: A string, the name of the layer.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.MaxPooling3D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.max_pooling3d(x, pool_size=2, strides=2)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.MaxPooling3D(pool_size=2, strides=2)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.max_pooling3d` is deprecated and '
      'will be removed in a future version. '
      'Please use `tf.keras.layers.MaxPooling3D` instead.',
      stacklevel=2)
  layer = MaxPooling3D(pool_size=pool_size, strides=strides,
                       padding=padding, data_format=data_format,
                       name=name)
  return layer.apply(inputs)

# Aliases

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3248')" href="javascript:;">
keras-2.8.0/keras/legacy_tf_layers/pooling.py: 525-602
</a>
<div class="mid" id="frag3248" style="display:none"><pre>
def max_pooling2d(inputs,
                  pool_size, strides,
                  padding='valid', data_format='channels_last',
                  name=None):
  """Max pooling layer for 2D inputs (e.g. images).

  Args:
    inputs: The tensor over which to pool. Must have rank 4.
    pool_size: An integer or tuple/list of 2 integers: (pool_height, pool_width)
      specifying the size of the pooling window.
      Can be a single integer to specify the same value for
      all spatial dimensions.
    strides: An integer or tuple/list of 2 integers,
      specifying the strides of the pooling operation.
      Can be a single integer to specify the same value for
      all spatial dimensions.
    padding: A string. The padding method, either 'valid' or 'same'.
      Case-insensitive.
    data_format: A string. The ordering of the dimensions in the inputs.
      `channels_last` (default) and `channels_first` are supported.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, height, width)`.
    name: A string, the name of the layer.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.MaxPooling2D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.max_pooling2d(x, pool_size=2, strides=2)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.max_pooling2d` is deprecated and '
      'will be removed in a future version. '
      'Please use `tf.keras.layers.MaxPooling2D` instead.',
      stacklevel=2)
  layer = MaxPooling2D(pool_size=pool_size, strides=strides,
                       padding=padding, data_format=data_format,
                       name=name)
  return layer.apply(inputs)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 48:</b> &nbsp; 9 fragments, nominal size 14 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3399')" href="javascript:;">
keras-2.8.0/keras/benchmarks/keras_examples_benchmarks/text_classification_transformer_benchmark_test.py: 68-84
</a>
<div class="mid" id="frag3399" style="display:none"><pre>
  def benchmark_text_classification_bs_128(self):
    """Measure performance with batch_size=128."""
    batch_size = 128
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.imdb_x,
        y=self.imdb_y,
        batch_size=batch_size,
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata(
        'transformer', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3464')" href="javascript:;">
keras-2.8.0/keras/benchmarks/keras_examples_benchmarks/bidirectional_lstm_benchmark_test.py: 58-74
</a>
<div class="mid" id="frag3464" style="display:none"><pre>
  def benchmark_bidirect_lstm_imdb_bs_128(self):
    """Measure performance with batch_size=128."""
    batch_size = 128
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.imdb_x,
        y=self.imdb_y,
        batch_size=batch_size,
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata(
        'bidirectional_lstm', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3419')" href="javascript:;">
keras-2.8.0/keras/benchmarks/keras_examples_benchmarks/mnist_hierarchical_rnn_benchmark_test.py: 64-80
</a>
<div class="mid" id="frag3419" style="display:none"><pre>
  def benchmark_hrnn_mnist_bs_256(self):
    """Measure performance with batch_size=256."""
    batch_size = 256
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        optimizer='rmsprop',
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata(
        'hierarchical_rnn', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3421')" href="javascript:;">
keras-2.8.0/keras/benchmarks/keras_examples_benchmarks/mnist_hierarchical_rnn_benchmark_test.py: 98-114
</a>
<div class="mid" id="frag3421" style="display:none"><pre>
  def benchmark_hrnn_mnist_bs_1024(self):
    """Measure performance with batch_size=1024."""
    batch_size = 1024
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        optimizer='rmsprop',
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata(
        'hierarchical_rnn', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3465')" href="javascript:;">
keras-2.8.0/keras/benchmarks/keras_examples_benchmarks/bidirectional_lstm_benchmark_test.py: 75-91
</a>
<div class="mid" id="frag3465" style="display:none"><pre>
  def benchmark_bidirect_lstm_imdb_bs_256(self):
    """Measure performance with batch_size=256."""
    batch_size = 256
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.imdb_x,
        y=self.imdb_y,
        batch_size=batch_size,
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata(
        'bidirectional_lstm', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3466')" href="javascript:;">
keras-2.8.0/keras/benchmarks/keras_examples_benchmarks/bidirectional_lstm_benchmark_test.py: 92-108
</a>
<div class="mid" id="frag3466" style="display:none"><pre>
  def benchmark_bidirect_lstm_imdb_bs_512(self):
    """Measure performance with batch_size=512."""
    batch_size = 512
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.imdb_x,
        y=self.imdb_y,
        batch_size=batch_size,
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata(
        'bidirectional_lstm', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3420')" href="javascript:;">
keras-2.8.0/keras/benchmarks/keras_examples_benchmarks/mnist_hierarchical_rnn_benchmark_test.py: 81-97
</a>
<div class="mid" id="frag3420" style="display:none"><pre>
  def benchmark_hrnn_mnist_bs_512(self):
    """Measure performance with batch_size=512."""
    batch_size = 512
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        optimizer='rmsprop',
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata(
        'hierarchical_rnn', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3400')" href="javascript:;">
keras-2.8.0/keras/benchmarks/keras_examples_benchmarks/text_classification_transformer_benchmark_test.py: 85-101
</a>
<div class="mid" id="frag3400" style="display:none"><pre>
  def benchmark_text_classification_bs_256(self):
    """Measure performance with batch_size=256."""
    batch_size = 256
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.imdb_x,
        y=self.imdb_y,
        batch_size=batch_size,
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata(
        'transformer', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3401')" href="javascript:;">
keras-2.8.0/keras/benchmarks/keras_examples_benchmarks/text_classification_transformer_benchmark_test.py: 102-118
</a>
<div class="mid" id="frag3401" style="display:none"><pre>
  def benchmark_text_classification_bs_512(self):
    """Measure performance with batch_size=512."""
    batch_size = 512
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.imdb_x,
        y=self.imdb_y,
        batch_size=batch_size,
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata(
        'transformer', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 49:</b> &nbsp; 3 fragments, nominal size 16 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3402')" href="javascript:;">
keras-2.8.0/keras/benchmarks/keras_examples_benchmarks/text_classification_transformer_benchmark_test.py: 119-141
</a>
<div class="mid" id="frag3402" style="display:none"><pre>
  def benchmark_text_classification_bs_512_gpu_2(self):
    """Measure performance with batch_size=512, gpu=1 and

    distribution_strategy='mirrored'
    """
    batch_size = 512
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.imdb_x,
        y=self.imdb_y,
        batch_size=batch_size,
        num_gpus=2,
        distribution_strategy='mirrored',
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata(
        'transformer', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3467')" href="javascript:;">
keras-2.8.0/keras/benchmarks/keras_examples_benchmarks/bidirectional_lstm_benchmark_test.py: 109-131
</a>
<div class="mid" id="frag3467" style="display:none"><pre>
  def benchmark_bidirect_lstm_imdb_bs_512_gpu_2(self):
    """Measure performance with batch_size=512, gpu=2 and

    distribution_strategy=`mirrored`.
    """
    batch_size = 512
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.imdb_x,
        y=self.imdb_y,
        batch_size=batch_size,
        num_gpus=2,
        distribution_strategy='mirrored',
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata(
        'bidirectional_lstm', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3422')" href="javascript:;">
keras-2.8.0/keras/benchmarks/keras_examples_benchmarks/mnist_hierarchical_rnn_benchmark_test.py: 115-137
</a>
<div class="mid" id="frag3422" style="display:none"><pre>
  def benchmark_hrnn_mnist_bs_1024_gpu_2(self):
    """Measure performance with batch_size=1024, gpu=2 and

    distribution_strategy='mirrored'
    """
    batch_size = 1024
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        num_gpus=2,
        distribution_strategy='mirrored',
        optimizer='rmsprop',
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata(
        'hierarchical_rnn', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 50:</b> &nbsp; 3 fragments, nominal size 13 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3413')" href="javascript:;">
keras-2.8.0/keras/benchmarks/keras_examples_benchmarks/mnist_irnn_benchmark_test.py: 64-79
</a>
<div class="mid" id="frag3413" style="display:none"><pre>
  def benchmark_irnn_mnist_bs_256(self):
    """Measure performance with batch_size=256."""
    batch_size = 256
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        optimizer=tf.keras.optimizers.RMSprop(learning_rate=self.learning_rate),
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('irnn', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3415')" href="javascript:;">
keras-2.8.0/keras/benchmarks/keras_examples_benchmarks/mnist_irnn_benchmark_test.py: 96-111
</a>
<div class="mid" id="frag3415" style="display:none"><pre>
  def benchmark_irnn_mnist_bs_1024(self):
    """Measure performance with batch_size=1024."""
    batch_size = 1024
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        optimizer=tf.keras.optimizers.RMSprop(learning_rate=self.learning_rate),
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('irnn', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3414')" href="javascript:;">
keras-2.8.0/keras/benchmarks/keras_examples_benchmarks/mnist_irnn_benchmark_test.py: 80-95
</a>
<div class="mid" id="frag3414" style="display:none"><pre>
  def benchmark_irnn_mnist_bs_512(self):
    """Measure performance with batch_size=512."""
    batch_size = 512
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        optimizer=tf.keras.optimizers.RMSprop(learning_rate=self.learning_rate),
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('irnn', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 51:</b> &nbsp; 3 fragments, nominal size 14 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3425')" href="javascript:;">
keras-2.8.0/keras/benchmarks/keras_examples_benchmarks/antirectifier_benchmark_test.py: 56-72
</a>
<div class="mid" id="frag3425" style="display:none"><pre>
  def benchmark_antirectifier_bs_128(self):
    """Measure performance with batch_size=128."""
    batch_size = 128
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        optimizer="rmsprop",
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=["sparse_categorical_accuracy"])

    metadata = benchmark_util.get_keras_examples_metadata(
        "antirectifier", batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3426')" href="javascript:;">
keras-2.8.0/keras/benchmarks/keras_examples_benchmarks/antirectifier_benchmark_test.py: 73-89
</a>
<div class="mid" id="frag3426" style="display:none"><pre>
  def benchmark_antirectifier_bs_256(self):
    """Measure performance with batch_size=256."""
    batch_size = 256
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        optimizer="rmsprop",
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=["sparse_categorical_accuracy"])

    metadata = benchmark_util.get_keras_examples_metadata(
        "antirectifier", batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3427')" href="javascript:;">
keras-2.8.0/keras/benchmarks/keras_examples_benchmarks/antirectifier_benchmark_test.py: 90-106
</a>
<div class="mid" id="frag3427" style="display:none"><pre>
  def benchmark_antirectifier_bs_512(self):
    """Measure performance with batch_size=512."""
    batch_size = 512
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        optimizer="rmsprop",
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=["sparse_categorical_accuracy"])

    metadata = benchmark_util.get_keras_examples_metadata(
        "antirectifier", batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 52:</b> &nbsp; 3 fragments, nominal size 14 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3435')" href="javascript:;">
keras-2.8.0/keras/benchmarks/keras_examples_benchmarks/cifar10_cnn_benchmark_test.py: 72-88
</a>
<div class="mid" id="frag3435" style="display:none"><pre>
  def benchmark_cnn_cifar10_bs_256(self):
    """Measure performance with batch_size=256."""
    batch_size = 256
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        epochs=self.epochs,
        optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6),
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('cnn', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3436')" href="javascript:;">
keras-2.8.0/keras/benchmarks/keras_examples_benchmarks/cifar10_cnn_benchmark_test.py: 89-105
</a>
<div class="mid" id="frag3436" style="display:none"><pre>
  def benchmark_cnn_cifar10_bs_512(self):
    """Measure performance with batch_size=512."""
    batch_size = 512
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        epochs=self.epochs,
        optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6),
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('cnn', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3437')" href="javascript:;">
keras-2.8.0/keras/benchmarks/keras_examples_benchmarks/cifar10_cnn_benchmark_test.py: 106-122
</a>
<div class="mid" id="frag3437" style="display:none"><pre>
  def benchmark_cnn_cifar10_bs_1024(self):
    """Measure performance with batch_size=1024."""
    batch_size = 1024
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        epochs=self.epochs,
        optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6),
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('cnn', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 53:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3440')" href="javascript:;">
keras-2.8.0/keras/benchmarks/keras_examples_benchmarks/mnist_conv_custom_training_benchmark_test.py: 45-59
</a>
<div class="mid" id="frag3440" style="display:none"><pre>
  def _build_model(self):
    """Model from https://keras.io/examples/vision/mnist_convnet/."""
    model = tf.keras.Sequential([
        tf.keras.Input(shape=self.input_shape),
        tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(self.num_classes, activation='softmax'),
    ])

    return model

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3457')" href="javascript:;">
keras-2.8.0/keras/benchmarks/keras_examples_benchmarks/mnist_conv_benchmark_test.py: 40-62
</a>
<div class="mid" id="frag3457" style="display:none"><pre>
  def _build_model(self):
    """Model from https://keras.io/examples/vision/mnist_convnet/."""
    model = tf.keras.Sequential([
        tf.keras.Input(shape=self.input_shape),
        tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(self.num_classes, activation='softmax'),
    ])
    return model

  # In each benchmark test, the required arguments for the
  # method `measure_performance` include:
  #   x: Input data, it could be Numpy or loaded from tfds.
  #   y: Target data. If `x` is a dataset or generator instance,
  #      `y` should not be specified.
  #   loss: Loss function for model.
  #   optimizer: Optimizer for model.
  #   Check more details in `measure_performance()` method of
  #   benchmark_util.
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 54:</b> &nbsp; 3 fragments, nominal size 16 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3446')" href="javascript:;">
keras-2.8.0/keras/benchmarks/keras_examples_benchmarks/mnist_conv_custom_training_benchmark_test.py: 266-287
</a>
<div class="mid" id="frag3446" style="display:none"><pre>
    return metrics, wall_time

  def benchmark_custom_training_mnist_bs_128(self):
    """Measure performance with batch_size=128 and run_iters=5."""
    batch_size = 128
    run_iters = 5
    train_dataset = self.train_dataset.shuffle(
        buffer_size=1024).batch(batch_size)

    # Instantiate a loss function.
    loss_fn = tf.keras.losses.CategoricalCrossentropy(
        reduction=tf.keras.losses.Reduction.NONE)
    # Instantiate an optimizer to train the model.
    optimizer = tf.keras.optimizers.Adam()
    model = self._build_model()

    metrics, wall_time = self.measure_performance(model, train_dataset, loss_fn,
                                                  optimizer, batch_size,
                                                  run_iters, self.epochs)
    extras = benchmark_util.get_keras_examples_metadata('conv', batch_size,
                                                        '.keras.ctl_graph')
    self.report_benchmark(
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3448')" href="javascript:;">
keras-2.8.0/keras/benchmarks/keras_examples_benchmarks/mnist_conv_custom_training_benchmark_test.py: 310-331
</a>
<div class="mid" id="frag3448" style="display:none"><pre>
        iters=run_iters, wall_time=wall_time, metrics=metrics, extras=extras)

  def benchmark_custom_training_mnist_bs_512(self):
    """Measure performance with batch_size=512 and run_iters=10."""
    batch_size = 512
    run_iters = 5
    train_dataset = self.train_dataset.shuffle(
        buffer_size=1024).batch(batch_size)

    # Instantiate a loss function.
    loss_fn = tf.keras.losses.CategoricalCrossentropy(
        reduction=tf.keras.losses.Reduction.NONE)
    # Instantiate an optimizer to train the model.
    optimizer = tf.keras.optimizers.Adam()
    model = self._build_model()

    metrics, wall_time = self.measure_performance(model, train_dataset, loss_fn,
                                                  optimizer, batch_size,
                                                  run_iters, self.epochs)
    extras = benchmark_util.get_keras_examples_metadata('conv', batch_size,
                                                        '.keras.ctl_graph')
    self.report_benchmark(
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3447')" href="javascript:;">
keras-2.8.0/keras/benchmarks/keras_examples_benchmarks/mnist_conv_custom_training_benchmark_test.py: 288-309
</a>
<div class="mid" id="frag3447" style="display:none"><pre>
        iters=run_iters, wall_time=wall_time, metrics=metrics, extras=extras)

  def benchmark_custom_training_mnist_bs_256(self):
    """Measure performance with batch_size=256 and run_iters=5."""
    batch_size = 256
    run_iters = 5
    train_dataset = self.train_dataset.shuffle(
        buffer_size=1024).batch(batch_size)

    # Instantiate a loss function.
    loss_fn = tf.keras.losses.CategoricalCrossentropy(
        reduction=tf.keras.losses.Reduction.NONE)
    # Instantiate an optimizer to train the model.
    optimizer = tf.keras.optimizers.Adam()
    model = self._build_model()

    metrics, wall_time = self.measure_performance(model, train_dataset, loss_fn,
                                                  optimizer, batch_size,
                                                  run_iters, self.epochs)
    extras = benchmark_util.get_keras_examples_metadata('conv', batch_size,
                                                        '.keras.ctl_graph')
    self.report_benchmark(
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 55:</b> &nbsp; 6 fragments, nominal size 14 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3452')" href="javascript:;">
keras-2.8.0/keras/benchmarks/keras_examples_benchmarks/reuters_mlp_benchmark_test.py: 63-79
</a>
<div class="mid" id="frag3452" style="display:none"><pre>
  def benchmark_mlp_reuters_bs_128(self):
    """Measure performance with batch_size=128."""
    batch_size = 128
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        epochs=self.epochs,
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('mlp', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3454')" href="javascript:;">
keras-2.8.0/keras/benchmarks/keras_examples_benchmarks/reuters_mlp_benchmark_test.py: 97-113
</a>
<div class="mid" id="frag3454" style="display:none"><pre>
  def benchmark_mlp_reuters_bs_512(self):
    """Measure performance with batch_size=512."""
    batch_size = 512
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        epochs=self.epochs,
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('mlp', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3458')" href="javascript:;">
keras-2.8.0/keras/benchmarks/keras_examples_benchmarks/mnist_conv_benchmark_test.py: 63-79
</a>
<div class="mid" id="frag3458" style="display:none"><pre>
  def benchmark_conv_mnist_bs_128(self):
    """Measure performance with batch_size=128."""
    batch_size = 128
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        epochs=self.epochs,
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('conv', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3459')" href="javascript:;">
keras-2.8.0/keras/benchmarks/keras_examples_benchmarks/mnist_conv_benchmark_test.py: 80-96
</a>
<div class="mid" id="frag3459" style="display:none"><pre>
  def benchmark_conv_mnist_bs_256(self):
    """Measure performance with batch_size=256."""
    batch_size = 256
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        epochs=self.epochs,
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('conv', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3460')" href="javascript:;">
keras-2.8.0/keras/benchmarks/keras_examples_benchmarks/mnist_conv_benchmark_test.py: 97-113
</a>
<div class="mid" id="frag3460" style="display:none"><pre>
  def benchmark_conv_mnist_bs_512(self):
    """Measure performance with batch_size=512."""
    batch_size = 512
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        epochs=self.epochs,
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('conv', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3453')" href="javascript:;">
keras-2.8.0/keras/benchmarks/keras_examples_benchmarks/reuters_mlp_benchmark_test.py: 80-96
</a>
<div class="mid" id="frag3453" style="display:none"><pre>
  def benchmark_mlp_reuters_bs_256(self):
    """Measure performance with batch_size=256."""
    batch_size = 256
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        epochs=self.epochs,
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('mlp', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 56:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3455')" href="javascript:;">
keras-2.8.0/keras/benchmarks/keras_examples_benchmarks/reuters_mlp_benchmark_test.py: 114-136
</a>
<div class="mid" id="frag3455" style="display:none"><pre>
  def benchmark_mlp_reuters_bs_512_gpu_2(self):
    """Measure performance with batch_size=512, gpu=2 and

    distribution_strategy='mirrored'
    """
    batch_size = 512
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        num_gpus=2,
        distribution_strategy='mirrored',
        epochs=self.epochs,
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('mlp', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3461')" href="javascript:;">
keras-2.8.0/keras/benchmarks/keras_examples_benchmarks/mnist_conv_benchmark_test.py: 114-136
</a>
<div class="mid" id="frag3461" style="display:none"><pre>
  def benchmark_conv_mnist_bs_512_gpu_2(self):
    """Measure performance with batch_size=512, gpu=2 and

    distribution_strategy='mirrored'
    """
    batch_size = 512
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        num_gpus=2,
        distribution_strategy='mirrored',
        epochs=self.epochs,
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('conv', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 57:</b> &nbsp; 8 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3488')" href="javascript:;">
keras-2.8.0/keras/benchmarks/saved_model_benchmarks/densenet_benchmark_test.py: 27-41
</a>
<div class="mid" id="frag3488" style="display:none"><pre>
  def benchmark_save_and_load_densenet_201(self):
    app = tf.keras.applications.DenseNet201
    save_result, load_result = (
        saved_model_benchmark_util.save_and_load_benchmark(app))

    self.report_benchmark(
        iters=save_result['iters'],
        wall_time=save_result['wall_time'],
        name=save_result['name'])

    self.report_benchmark(
        iters=load_result['iters'],
        wall_time=load_result['wall_time'],
        name=load_result['name'])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3489')" href="javascript:;">
keras-2.8.0/keras/benchmarks/saved_model_benchmarks/xception_benchmark_test.py: 27-42
</a>
<div class="mid" id="frag3489" style="display:none"><pre>
  def benchmark_save_and_load_xception(self):
    app = tf.keras.applications.Xception
    save_result, load_result = (
        saved_model_benchmark_util.save_and_load_benchmark(app))

    self.report_benchmark(
        iters=save_result['iters'],
        wall_time=save_result['wall_time'],
        name=save_result['name'])

    self.report_benchmark(
        iters=load_result['iters'],
        wall_time=load_result['wall_time'],
        name=load_result['name'])


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3496')" href="javascript:;">
keras-2.8.0/keras/benchmarks/saved_model_benchmarks/inception_resnet_v2_benchmark_test.py: 27-42
</a>
<div class="mid" id="frag3496" style="display:none"><pre>
  def benchmark_save_and_load_inception_resnet_v2(self):
    app = tf.keras.applications.InceptionResNetV2
    save_result, load_result = (
        saved_model_benchmark_util.save_and_load_benchmark(app))

    self.report_benchmark(
        iters=save_result['iters'],
        wall_time=save_result['wall_time'],
        name=save_result['name'])

    self.report_benchmark(
        iters=load_result['iters'],
        wall_time=load_result['wall_time'],
        name=load_result['name'])


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3495')" href="javascript:;">
keras-2.8.0/keras/benchmarks/saved_model_benchmarks/vgg_benchmark_test.py: 27-42
</a>
<div class="mid" id="frag3495" style="display:none"><pre>
  def benchmark_save_and_load_vgg19(self):
    app = tf.keras.applications.VGG19
    save_result, load_result = (
        saved_model_benchmark_util.save_and_load_benchmark(app))

    self.report_benchmark(
        iters=save_result['iters'],
        wall_time=save_result['wall_time'],
        name=save_result['name'])

    self.report_benchmark(
        iters=load_result['iters'],
        wall_time=load_result['wall_time'],
        name=load_result['name'])


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3494')" href="javascript:;">
keras-2.8.0/keras/benchmarks/saved_model_benchmarks/mobilenet_benchmark_test.py: 27-41
</a>
<div class="mid" id="frag3494" style="display:none"><pre>
  def benchmark_save_and_load_mobilenet_v2(self):
    app = tf.keras.applications.MobileNetV2
    save_result, load_result = (
        saved_model_benchmark_util.save_and_load_benchmark(app))

    self.report_benchmark(
        iters=save_result['iters'],
        wall_time=save_result['wall_time'],
        name=save_result['name'])

    self.report_benchmark(
        iters=load_result['iters'],
        wall_time=load_result['wall_time'],
        name=load_result['name'])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3490')" href="javascript:;">
keras-2.8.0/keras/benchmarks/saved_model_benchmarks/efficientnet_benchmark_test.py: 27-41
</a>
<div class="mid" id="frag3490" style="display:none"><pre>
  def benchmark_save_and_load_efficient_net_b7(self):
    app = tf.keras.applications.EfficientNetB7
    save_result, load_result = (
        saved_model_benchmark_util.save_and_load_benchmark(app))

    self.report_benchmark(
        iters=save_result['iters'],
        wall_time=save_result['wall_time'],
        name=save_result['name'])

    self.report_benchmark(
        iters=load_result['iters'],
        wall_time=load_result['wall_time'],
        name=load_result['name'])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3491')" href="javascript:;">
keras-2.8.0/keras/benchmarks/saved_model_benchmarks/nasnet_large_benchmark_test.py: 27-41
</a>
<div class="mid" id="frag3491" style="display:none"><pre>
  def benchmark_save_and_load_nasnet_large(self):
    app = tf.keras.applications.NASNetLarge
    save_result, load_result = (
        saved_model_benchmark_util.save_and_load_benchmark(app))

    self.report_benchmark(
        iters=save_result['iters'],
        wall_time=save_result['wall_time'],
        name=save_result['name'])

    self.report_benchmark(
        iters=load_result['iters'],
        wall_time=load_result['wall_time'],
        name=load_result['name'])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3492')" href="javascript:;">
keras-2.8.0/keras/benchmarks/saved_model_benchmarks/resnet152_v2_benchmark_test.py: 27-42
</a>
<div class="mid" id="frag3492" style="display:none"><pre>
  def benchmark_save_and_load_resnet152_v2(self):
    app = tf.keras.applications.ResNet152V2
    save_result, load_result = (
        saved_model_benchmark_util.save_and_load_benchmark(app))

    self.report_benchmark(
        iters=save_result['iters'],
        wall_time=save_result['wall_time'],
        name=save_result['name'])

    self.report_benchmark(
        iters=load_result['iters'],
        wall_time=load_result['wall_time'],
        name=load_result['name'])


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 58:</b> &nbsp; 4 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3535')" href="javascript:;">
keras-2.8.0/keras/metrics_confusion_matrix_test.py: 46-59
</a>
<div class="mid" id="frag3535" style="display:none"><pre>
  def test_unweighted(self):
    fp_obj = metrics.FalsePositives()
    self.evaluate(tf.compat.v1.variables_initializer(fp_obj.variables))

    y_true = tf.constant(((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),
                                   (1, 1, 1, 1, 0), (0, 0, 0, 0, 1)))
    y_pred = tf.constant(((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),
                                   (0, 1, 0, 1, 0), (1, 1, 1, 1, 1)))

    update_op = fp_obj.update_state(y_true, y_pred)
    self.evaluate(update_op)
    result = fp_obj.result()
    self.assertAllClose(7., result)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3541')" href="javascript:;">
keras-2.8.0/keras/metrics_confusion_matrix_test.py: 126-139
</a>
<div class="mid" id="frag3541" style="display:none"><pre>
  def test_unweighted(self):
    fn_obj = metrics.FalseNegatives()
    self.evaluate(tf.compat.v1.variables_initializer(fn_obj.variables))

    y_true = tf.constant(((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),
                                   (1, 1, 1, 1, 0), (0, 0, 0, 0, 1)))
    y_pred = tf.constant(((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),
                                   (0, 1, 0, 1, 0), (1, 1, 1, 1, 1)))

    update_op = fn_obj.update_state(y_true, y_pred)
    self.evaluate(update_op)
    result = fn_obj.result()
    self.assertAllClose(3., result)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3546')" href="javascript:;">
keras-2.8.0/keras/metrics_confusion_matrix_test.py: 194-207
</a>
<div class="mid" id="frag3546" style="display:none"><pre>
  def test_unweighted(self):
    tn_obj = metrics.TrueNegatives()
    self.evaluate(tf.compat.v1.variables_initializer(tn_obj.variables))

    y_true = tf.constant(((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),
                                   (1, 1, 1, 1, 0), (0, 0, 0, 0, 1)))
    y_pred = tf.constant(((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),
                                   (0, 1, 0, 1, 0), (1, 1, 1, 1, 1)))

    update_op = tn_obj.update_state(y_true, y_pred)
    self.evaluate(update_op)
    result = tn_obj.result()
    self.assertAllClose(3., result)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3551')" href="javascript:;">
keras-2.8.0/keras/metrics_confusion_matrix_test.py: 262-275
</a>
<div class="mid" id="frag3551" style="display:none"><pre>
  def test_unweighted(self):
    tp_obj = metrics.TruePositives()
    self.evaluate(tf.compat.v1.variables_initializer(tp_obj.variables))

    y_true = tf.constant(((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),
                                   (1, 1, 1, 1, 0), (0, 0, 0, 0, 1)))
    y_pred = tf.constant(((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),
                                   (0, 1, 0, 1, 0), (1, 1, 1, 1, 1)))

    update_op = tp_obj.update_state(y_true, y_pred)
    self.evaluate(update_op)
    result = tp_obj.result()
    self.assertAllClose(7., result)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 59:</b> &nbsp; 4 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3536')" href="javascript:;">
keras-2.8.0/keras/metrics_confusion_matrix_test.py: 60-70
</a>
<div class="mid" id="frag3536" style="display:none"><pre>
  def test_weighted(self):
    fp_obj = metrics.FalsePositives()
    self.evaluate(tf.compat.v1.variables_initializer(fp_obj.variables))
    y_true = tf.constant(((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),
                                   (1, 1, 1, 1, 0), (0, 0, 0, 0, 1)))
    y_pred = tf.constant(((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),
                                   (0, 1, 0, 1, 0), (1, 1, 1, 1, 1)))
    sample_weight = tf.constant((1., 1.5, 2., 2.5))
    result = fp_obj(y_true, y_pred, sample_weight=sample_weight)
    self.assertAllClose(14., self.evaluate(result))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3552')" href="javascript:;">
keras-2.8.0/keras/metrics_confusion_matrix_test.py: 276-286
</a>
<div class="mid" id="frag3552" style="display:none"><pre>
  def test_weighted(self):
    tp_obj = metrics.TruePositives()
    self.evaluate(tf.compat.v1.variables_initializer(tp_obj.variables))
    y_true = tf.constant(((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),
                                   (1, 1, 1, 1, 0), (0, 0, 0, 0, 1)))
    y_pred = tf.constant(((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),
                                   (0, 1, 0, 1, 0), (1, 1, 1, 1, 1)))
    sample_weight = tf.constant((1., 1.5, 2., 2.5))
    result = tp_obj(y_true, y_pred, sample_weight=sample_weight)
    self.assertAllClose(12., self.evaluate(result))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3547')" href="javascript:;">
keras-2.8.0/keras/metrics_confusion_matrix_test.py: 208-218
</a>
<div class="mid" id="frag3547" style="display:none"><pre>
  def test_weighted(self):
    tn_obj = metrics.TrueNegatives()
    self.evaluate(tf.compat.v1.variables_initializer(tn_obj.variables))
    y_true = tf.constant(((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),
                                   (1, 1, 1, 1, 0), (0, 0, 0, 0, 1)))
    y_pred = tf.constant(((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),
                                   (0, 1, 0, 1, 0), (1, 1, 1, 1, 1)))
    sample_weight = tf.constant((1., 1.5, 2., 2.5))
    result = tn_obj(y_true, y_pred, sample_weight=sample_weight)
    self.assertAllClose(4., self.evaluate(result))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3542')" href="javascript:;">
keras-2.8.0/keras/metrics_confusion_matrix_test.py: 140-150
</a>
<div class="mid" id="frag3542" style="display:none"><pre>
  def test_weighted(self):
    fn_obj = metrics.FalseNegatives()
    self.evaluate(tf.compat.v1.variables_initializer(fn_obj.variables))
    y_true = tf.constant(((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),
                                   (1, 1, 1, 1, 0), (0, 0, 0, 0, 1)))
    y_pred = tf.constant(((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),
                                   (0, 1, 0, 1, 0), (1, 1, 1, 1, 1)))
    sample_weight = tf.constant((1., 1.5, 2., 2.5))
    result = fn_obj(y_true, y_pred, sample_weight=sample_weight)
    self.assertAllClose(5., self.evaluate(result))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 60:</b> &nbsp; 4 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3537')" href="javascript:;">
keras-2.8.0/keras/metrics_confusion_matrix_test.py: 71-84
</a>
<div class="mid" id="frag3537" style="display:none"><pre>
  def test_unweighted_with_thresholds(self):
    fp_obj = metrics.FalsePositives(thresholds=[0.15, 0.5, 0.85])
    self.evaluate(tf.compat.v1.variables_initializer(fp_obj.variables))

    y_pred = tf.constant(((0.9, 0.2, 0.8, 0.1), (0.2, 0.9, 0.7, 0.6),
                                   (0.1, 0.2, 0.4, 0.3), (0, 1, 0.7, 0.3)))
    y_true = tf.constant(((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0),
                                   (1, 1, 1, 1)))

    update_op = fp_obj.update_state(y_true, y_pred)
    self.evaluate(update_op)
    result = fp_obj.result()
    self.assertAllClose([7., 4., 2.], result)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3548')" href="javascript:;">
keras-2.8.0/keras/metrics_confusion_matrix_test.py: 219-232
</a>
<div class="mid" id="frag3548" style="display:none"><pre>
  def test_unweighted_with_thresholds(self):
    tn_obj = metrics.TrueNegatives(thresholds=[0.15, 0.5, 0.85])
    self.evaluate(tf.compat.v1.variables_initializer(tn_obj.variables))

    y_pred = tf.constant(((0.9, 0.2, 0.8, 0.1), (0.2, 0.9, 0.7, 0.6),
                                   (0.1, 0.2, 0.4, 0.3), (0, 1, 0.7, 0.3)))
    y_true = tf.constant(((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0),
                                   (1, 1, 1, 1)))

    update_op = tn_obj.update_state(y_true, y_pred)
    self.evaluate(update_op)
    result = tn_obj.result()
    self.assertAllClose([2., 5., 7.], result)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3553')" href="javascript:;">
keras-2.8.0/keras/metrics_confusion_matrix_test.py: 287-300
</a>
<div class="mid" id="frag3553" style="display:none"><pre>
  def test_unweighted_with_thresholds(self):
    tp_obj = metrics.TruePositives(thresholds=[0.15, 0.5, 0.85])
    self.evaluate(tf.compat.v1.variables_initializer(tp_obj.variables))

    y_pred = tf.constant(((0.9, 0.2, 0.8, 0.1), (0.2, 0.9, 0.7, 0.6),
                                   (0.1, 0.2, 0.4, 0.3), (0, 1, 0.7, 0.3)))
    y_true = tf.constant(((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0),
                                   (1, 1, 1, 1)))

    update_op = tp_obj.update_state(y_true, y_pred)
    self.evaluate(update_op)
    result = tp_obj.result()
    self.assertAllClose([6., 3., 1.], result)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3543')" href="javascript:;">
keras-2.8.0/keras/metrics_confusion_matrix_test.py: 151-164
</a>
<div class="mid" id="frag3543" style="display:none"><pre>
  def test_unweighted_with_thresholds(self):
    fn_obj = metrics.FalseNegatives(thresholds=[0.15, 0.5, 0.85])
    self.evaluate(tf.compat.v1.variables_initializer(fn_obj.variables))

    y_pred = tf.constant(((0.9, 0.2, 0.8, 0.1), (0.2, 0.9, 0.7, 0.6),
                                   (0.1, 0.2, 0.4, 0.3), (0, 1, 0.7, 0.3)))
    y_true = tf.constant(((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0),
                                   (1, 1, 1, 1)))

    update_op = fn_obj.update_state(y_true, y_pred)
    self.evaluate(update_op)
    result = fn_obj.result()
    self.assertAllClose([1., 4., 6.], result)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 61:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3555')" href="javascript:;">
keras-2.8.0/keras/metrics_confusion_matrix_test.py: 317-335
</a>
<div class="mid" id="frag3555" style="display:none"><pre>
  def test_config(self):
    p_obj = metrics.Precision(
        name='my_precision', thresholds=[0.4, 0.9], top_k=15, class_id=12)
    self.assertEqual(p_obj.name, 'my_precision')
    self.assertLen(p_obj.variables, 2)
    self.assertEqual([v.name for v in p_obj.variables],
                     ['true_positives:0', 'false_positives:0'])
    self.assertEqual(p_obj.thresholds, [0.4, 0.9])
    self.assertEqual(p_obj.top_k, 15)
    self.assertEqual(p_obj.class_id, 12)

    # Check save and restore config
    p_obj2 = metrics.Precision.from_config(p_obj.get_config())
    self.assertEqual(p_obj2.name, 'my_precision')
    self.assertLen(p_obj2.variables, 2)
    self.assertEqual(p_obj2.thresholds, [0.4, 0.9])
    self.assertEqual(p_obj2.top_k, 15)
    self.assertEqual(p_obj2.class_id, 12)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3569')" href="javascript:;">
keras-2.8.0/keras/metrics_confusion_matrix_test.py: 522-540
</a>
<div class="mid" id="frag3569" style="display:none"><pre>
  def test_config(self):
    r_obj = metrics.Recall(
        name='my_recall', thresholds=[0.4, 0.9], top_k=15, class_id=12)
    self.assertEqual(r_obj.name, 'my_recall')
    self.assertLen(r_obj.variables, 2)
    self.assertEqual([v.name for v in r_obj.variables],
                     ['true_positives:0', 'false_negatives:0'])
    self.assertEqual(r_obj.thresholds, [0.4, 0.9])
    self.assertEqual(r_obj.top_k, 15)
    self.assertEqual(r_obj.class_id, 12)

    # Check save and restore config
    r_obj2 = metrics.Recall.from_config(r_obj.get_config())
    self.assertEqual(r_obj2.name, 'my_recall')
    self.assertLen(r_obj2.variables, 2)
    self.assertEqual(r_obj2.thresholds, [0.4, 0.9])
    self.assertEqual(r_obj2.top_k, 15)
    self.assertEqual(r_obj2.class_id, 12)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 62:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3559')" href="javascript:;">
keras-2.8.0/keras/metrics_confusion_matrix_test.py: 370-383
</a>
<div class="mid" id="frag3559" style="display:none"><pre>
  def test_weighted(self):
    p_obj = metrics.Precision()
    y_pred = tf.constant([[1, 0, 1, 0], [1, 0, 1, 0]])
    y_true = tf.constant([[0, 1, 1, 0], [1, 0, 0, 1]])
    self.evaluate(tf.compat.v1.variables_initializer(p_obj.variables))
    result = p_obj(
        y_true,
        y_pred,
        sample_weight=tf.constant([[1, 2, 3, 4], [4, 3, 2, 1]]))
    weighted_tp = 3.0 + 4.0
    weighted_positives = (1.0 + 3.0) + (4.0 + 2.0)
    expected_precision = weighted_tp / weighted_positives
    self.assertAlmostEqual(expected_precision, self.evaluate(result))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3573')" href="javascript:;">
keras-2.8.0/keras/metrics_confusion_matrix_test.py: 574-587
</a>
<div class="mid" id="frag3573" style="display:none"><pre>
  def test_weighted(self):
    r_obj = metrics.Recall()
    y_pred = tf.constant([[1, 0, 1, 0], [0, 1, 0, 1]])
    y_true = tf.constant([[0, 1, 1, 0], [1, 0, 0, 1]])
    self.evaluate(tf.compat.v1.variables_initializer(r_obj.variables))
    result = r_obj(
        y_true,
        y_pred,
        sample_weight=tf.constant([[1, 2, 3, 4], [4, 3, 2, 1]]))
    weighted_tp = 3.0 + 1.0
    weighted_t = (2.0 + 3.0) + (4.0 + 1.0)
    expected_recall = weighted_tp / weighted_t
    self.assertAlmostEqual(expected_recall, self.evaluate(result))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 63:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3562')" href="javascript:;">
keras-2.8.0/keras/metrics_confusion_matrix_test.py: 400-415
</a>
<div class="mid" id="frag3562" style="display:none"><pre>
  def test_weighted_with_threshold(self):
    p_obj = metrics.Precision(thresholds=[0.5, 1.])
    y_true = tf.constant([[0, 1], [1, 0]], shape=(2, 2))
    y_pred = tf.constant([[1, 0], [0.6, 0]],
                                  shape=(2, 2),
                                  dtype=tf.float32)
    weights = tf.constant([[4, 0], [3, 1]],
                                   shape=(2, 2),
                                   dtype=tf.float32)
    self.evaluate(tf.compat.v1.variables_initializer(p_obj.variables))
    result = p_obj(y_true, y_pred, sample_weight=weights)
    weighted_tp = 0 + 3.
    weighted_positives = (0 + 3.) + (4. + 0.)
    expected_precision = weighted_tp / weighted_positives
    self.assertArrayNear([expected_precision, 0], self.evaluate(result), 1e-3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3576')" href="javascript:;">
keras-2.8.0/keras/metrics_confusion_matrix_test.py: 604-619
</a>
<div class="mid" id="frag3576" style="display:none"><pre>
  def test_weighted_with_threshold(self):
    r_obj = metrics.Recall(thresholds=[0.5, 1.])
    y_true = tf.constant([[0, 1], [1, 0]], shape=(2, 2))
    y_pred = tf.constant([[1, 0], [0.6, 0]],
                                  shape=(2, 2),
                                  dtype=tf.float32)
    weights = tf.constant([[1, 4], [3, 2]],
                                   shape=(2, 2),
                                   dtype=tf.float32)
    self.evaluate(tf.compat.v1.variables_initializer(r_obj.variables))
    result = r_obj(y_true, y_pred, sample_weight=weights)
    weighted_tp = 0 + 3.
    weighted_positives = (0 + 3.) + (4. + 0.)
    expected_recall = weighted_tp / weighted_positives
    self.assertArrayNear([expected_recall, 0], self.evaluate(result), 1e-3)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 64:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3563')" href="javascript:;">
keras-2.8.0/keras/metrics_confusion_matrix_test.py: 416-435
</a>
<div class="mid" id="frag3563" style="display:none"><pre>
  def test_multiple_updates(self):
    p_obj = metrics.Precision(thresholds=[0.5, 1.])
    y_true = tf.constant([[0, 1], [1, 0]], shape=(2, 2))
    y_pred = tf.constant([[1, 0], [0.6, 0]],
                                  shape=(2, 2),
                                  dtype=tf.float32)
    weights = tf.constant([[4, 0], [3, 1]],
                                   shape=(2, 2),
                                   dtype=tf.float32)
    self.evaluate(tf.compat.v1.variables_initializer(p_obj.variables))
    update_op = p_obj.update_state(y_true, y_pred, sample_weight=weights)
    for _ in range(2):
      self.evaluate(update_op)

    weighted_tp = (0 + 3.) + (0 + 3.)
    weighted_positives = ((0 + 3.) + (4. + 0.)) + ((0 + 3.) + (4. + 0.))
    expected_precision = weighted_tp / weighted_positives
    self.assertArrayNear([expected_precision, 0], self.evaluate(p_obj.result()),
                         1e-3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3577')" href="javascript:;">
keras-2.8.0/keras/metrics_confusion_matrix_test.py: 620-639
</a>
<div class="mid" id="frag3577" style="display:none"><pre>
  def test_multiple_updates(self):
    r_obj = metrics.Recall(thresholds=[0.5, 1.])
    y_true = tf.constant([[0, 1], [1, 0]], shape=(2, 2))
    y_pred = tf.constant([[1, 0], [0.6, 0]],
                                  shape=(2, 2),
                                  dtype=tf.float32)
    weights = tf.constant([[1, 4], [3, 2]],
                                   shape=(2, 2),
                                   dtype=tf.float32)
    self.evaluate(tf.compat.v1.variables_initializer(r_obj.variables))
    update_op = r_obj.update_state(y_true, y_pred, sample_weight=weights)
    for _ in range(2):
      self.evaluate(update_op)

    weighted_tp = (0 + 3.) + (0 + 3.)
    weighted_positives = ((0 + 3.) + (4. + 0.)) + ((0 + 3.) + (4. + 0.))
    expected_recall = weighted_tp / weighted_positives
    self.assertArrayNear([expected_recall, 0], self.evaluate(r_obj.result()),
                         1e-3)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 65:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3583')" href="javascript:;">
keras-2.8.0/keras/metrics_confusion_matrix_test.py: 726-745
</a>
<div class="mid" id="frag3583" style="display:none"><pre>
  def test_config(self):
    s_obj = metrics.SensitivityAtSpecificity(
        0.4,
        num_thresholds=100,
        class_id=12,
        name='sensitivity_at_specificity_1')
    self.assertEqual(s_obj.name, 'sensitivity_at_specificity_1')
    self.assertLen(s_obj.variables, 4)
    self.assertEqual(s_obj.specificity, 0.4)
    self.assertEqual(s_obj.num_thresholds, 100)
    self.assertEqual(s_obj.class_id, 12)

    # Check save and restore config
    s_obj2 = metrics.SensitivityAtSpecificity.from_config(s_obj.get_config())
    self.assertEqual(s_obj2.name, 'sensitivity_at_specificity_1')
    self.assertLen(s_obj2.variables, 4)
    self.assertEqual(s_obj2.specificity, 0.4)
    self.assertEqual(s_obj2.num_thresholds, 100)
    self.assertEqual(s_obj.class_id, 12)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3592')" href="javascript:;">
keras-2.8.0/keras/metrics_confusion_matrix_test.py: 840-859
</a>
<div class="mid" id="frag3592" style="display:none"><pre>
  def test_config(self):
    s_obj = metrics.SpecificityAtSensitivity(
        0.4,
        num_thresholds=100,
        class_id=12,
        name='specificity_at_sensitivity_1')
    self.assertEqual(s_obj.name, 'specificity_at_sensitivity_1')
    self.assertLen(s_obj.variables, 4)
    self.assertEqual(s_obj.sensitivity, 0.4)
    self.assertEqual(s_obj.num_thresholds, 100)
    self.assertEqual(s_obj.class_id, 12)

    # Check save and restore config
    s_obj2 = metrics.SpecificityAtSensitivity.from_config(s_obj.get_config())
    self.assertEqual(s_obj2.name, 'specificity_at_sensitivity_1')
    self.assertLen(s_obj2.variables, 4)
    self.assertEqual(s_obj2.sensitivity, 0.4)
    self.assertEqual(s_obj2.num_thresholds, 100)
    self.assertEqual(s_obj.class_id, 12)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 66:</b> &nbsp; 4 fragments, nominal size 18 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3584')" href="javascript:;">
keras-2.8.0/keras/metrics_confusion_matrix_test.py: 746-768
</a>
<div class="mid" id="frag3584" style="display:none"><pre>
  def test_value_is_idempotent(self):
    s_obj = metrics.SensitivityAtSpecificity(0.7)
    y_pred = tf.random.uniform((10, 3),
                                       maxval=1,
                                       dtype=tf.float32,
                                       seed=1)
    y_true = tf.random.uniform((10, 3),
                                       maxval=2,
                                       dtype=tf.int64,
                                       seed=1)
    update_op = s_obj.update_state(y_true, y_pred)
    self.evaluate(tf.compat.v1.variables_initializer(s_obj.variables))

    # Run several updates.
    for _ in range(10):
      self.evaluate(update_op)

    # Then verify idempotency.
    initial_sensitivity = self.evaluate(s_obj.result())
    for _ in range(10):
      self.assertAlmostEqual(initial_sensitivity, self.evaluate(s_obj.result()),
                             1e-3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3602')" href="javascript:;">
keras-2.8.0/keras/metrics_confusion_matrix_test.py: 970-992
</a>
<div class="mid" id="frag3602" style="display:none"><pre>
  def test_value_is_idempotent(self):
    s_obj = metrics.PrecisionAtRecall(0.7)
    y_pred = tf.random.uniform((10, 3),
                                       maxval=1,
                                       dtype=tf.float32,
                                       seed=1)
    y_true = tf.random.uniform((10, 3),
                                       maxval=2,
                                       dtype=tf.int64,
                                       seed=1)
    update_op = s_obj.update_state(y_true, y_pred)
    self.evaluate(tf.compat.v1.variables_initializer(s_obj.variables))

    # Run several updates.
    for _ in range(10):
      self.evaluate(update_op)

    # Then verify idempotency.
    initial_precision = self.evaluate(s_obj.result())
    for _ in range(10):
      self.assertAlmostEqual(initial_precision, self.evaluate(s_obj.result()),
                             1e-3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3593')" href="javascript:;">
keras-2.8.0/keras/metrics_confusion_matrix_test.py: 860-882
</a>
<div class="mid" id="frag3593" style="display:none"><pre>
  def test_value_is_idempotent(self):
    s_obj = metrics.SpecificityAtSensitivity(0.7)
    y_pred = tf.random.uniform((10, 3),
                                       maxval=1,
                                       dtype=tf.float32,
                                       seed=1)
    y_true = tf.random.uniform((10, 3),
                                       maxval=2,
                                       dtype=tf.int64,
                                       seed=1)
    update_op = s_obj.update_state(y_true, y_pred)
    self.evaluate(tf.compat.v1.variables_initializer(s_obj.variables))

    # Run several updates.
    for _ in range(10):
      self.evaluate(update_op)

    # Then verify idempotency.
    initial_specificity = self.evaluate(s_obj.result())
    for _ in range(10):
      self.assertAlmostEqual(initial_specificity, self.evaluate(s_obj.result()),
                             1e-3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3611')" href="javascript:;">
keras-2.8.0/keras/metrics_confusion_matrix_test.py: 1084-1106
</a>
<div class="mid" id="frag3611" style="display:none"><pre>
  def test_value_is_idempotent(self):
    s_obj = metrics.RecallAtPrecision(0.7)
    y_pred = tf.random.uniform((10, 3),
                                       maxval=1,
                                       dtype=tf.float32,
                                       seed=1)
    y_true = tf.random.uniform((10, 3),
                                       maxval=2,
                                       dtype=tf.int64,
                                       seed=1)
    update_op = s_obj.update_state(y_true, y_pred)
    self.evaluate(tf.compat.v1.variables_initializer(s_obj.variables))

    # Run several updates.
    for _ in range(10):
      self.evaluate(update_op)

    # Then verify idempotency.
    initial_recall = self.evaluate(s_obj.result())
    for _ in range(10):
      self.assertAlmostEqual(initial_recall, self.evaluate(s_obj.result()),
                             1e-3)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 67:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3589')" href="javascript:;">
keras-2.8.0/keras/metrics_confusion_matrix_test.py: 813-825
</a>
<div class="mid" id="frag3589" style="display:none"><pre>
  def test_weighted(self, label_dtype):
    s_obj = metrics.SensitivityAtSpecificity(0.4)
    pred_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.01, 0.02, 0.25, 0.26, 0.26]
    label_values = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]
    weight_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

    y_pred = tf.constant(pred_values, dtype=tf.float32)
    y_true = tf.cast(label_values, dtype=label_dtype)
    weights = tf.constant(weight_values)
    self.evaluate(tf.compat.v1.variables_initializer(s_obj.variables))
    result = s_obj(y_true, y_pred, sample_weight=weights)
    self.assertAlmostEqual(0.675, self.evaluate(result))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3598')" href="javascript:;">
keras-2.8.0/keras/metrics_confusion_matrix_test.py: 926-938
</a>
<div class="mid" id="frag3598" style="display:none"><pre>
  def test_weighted(self, label_dtype):
    s_obj = metrics.SpecificityAtSensitivity(0.4)
    pred_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.01, 0.02, 0.25, 0.26, 0.26]
    label_values = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]
    weight_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

    y_pred = tf.constant(pred_values, dtype=tf.float32)
    y_true = tf.cast(label_values, dtype=label_dtype)
    weights = tf.constant(weight_values)
    self.evaluate(tf.compat.v1.variables_initializer(s_obj.variables))
    result = s_obj(y_true, y_pred, sample_weight=weights)
    self.assertAlmostEqual(0.4, self.evaluate(result))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 68:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3601')" href="javascript:;">
keras-2.8.0/keras/metrics_confusion_matrix_test.py: 953-969
</a>
<div class="mid" id="frag3601" style="display:none"><pre>
  def test_config(self):
    s_obj = metrics.PrecisionAtRecall(
        0.4, num_thresholds=100, class_id=12, name='precision_at_recall_1')
    self.assertEqual(s_obj.name, 'precision_at_recall_1')
    self.assertLen(s_obj.variables, 4)
    self.assertEqual(s_obj.recall, 0.4)
    self.assertEqual(s_obj.num_thresholds, 100)
    self.assertEqual(s_obj.class_id, 12)

    # Check save and restore config
    s_obj2 = metrics.PrecisionAtRecall.from_config(s_obj.get_config())
    self.assertEqual(s_obj2.name, 'precision_at_recall_1')
    self.assertLen(s_obj2.variables, 4)
    self.assertEqual(s_obj2.recall, 0.4)
    self.assertEqual(s_obj2.num_thresholds, 100)
    self.assertEqual(s_obj.class_id, 12)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3610')" href="javascript:;">
keras-2.8.0/keras/metrics_confusion_matrix_test.py: 1067-1083
</a>
<div class="mid" id="frag3610" style="display:none"><pre>
  def test_config(self):
    s_obj = metrics.RecallAtPrecision(
        0.4, num_thresholds=100, class_id=12, name='recall_at_precision_1')
    self.assertEqual(s_obj.name, 'recall_at_precision_1')
    self.assertLen(s_obj.variables, 4)
    self.assertEqual(s_obj.precision, 0.4)
    self.assertEqual(s_obj.num_thresholds, 100)
    self.assertEqual(s_obj.class_id, 12)

    # Check save and restore config
    s_obj2 = metrics.RecallAtPrecision.from_config(s_obj.get_config())
    self.assertEqual(s_obj2.name, 'recall_at_precision_1')
    self.assertLen(s_obj2.variables, 4)
    self.assertEqual(s_obj2.precision, 0.4)
    self.assertEqual(s_obj2.num_thresholds, 100)
    self.assertEqual(s_obj.class_id, 12)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 69:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3796')" href="javascript:;">
keras-2.8.0/keras/layers/lstm_v2_test.py: 89-101
</a>
<div class="mid" id="frag3796" style="display:none"><pre>
  def test_dynamic_behavior_LSTM(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    layer = rnn.LSTM(units, input_shape=(None, embedding_dim))
    model = keras.models.Sequential()
    model.add(layer)
    model.compile(tf.compat.v1.train.GradientDescentOptimizer(0.001), 'mse')
    x = np.random.random((num_samples, timesteps, embedding_dim))
    y = np.random.random((num_samples, units))
    model.train_on_batch(x, y)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4404')" href="javascript:;">
keras-2.8.0/keras/layers/gru_v2_test.py: 103-115
</a>
<div class="mid" id="frag4404" style="display:none"><pre>
  def test_dynamic_behavior_GRU(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    layer = rnn.GRU(units, input_shape=(None, embedding_dim))
    model = keras.models.Sequential()
    model.add(layer)
    model.compile(tf.compat.v1.train.GradientDescentOptimizer(0.001), 'mse')
    x = np.random.random((num_samples, timesteps, embedding_dim))
    y = np.random.random((num_samples, units))
    model.train_on_batch(x, y)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 70:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3797')" href="javascript:;">
keras-2.8.0/keras/layers/lstm_v2_test.py: 102-113
</a>
<div class="mid" id="frag3797" style="display:none"><pre>
  def test_stacking_LSTM(self):
    inputs = np.random.random((2, 3, 4))
    targets = np.abs(np.random.random((2, 3, 5)))
    targets /= targets.sum(axis=-1, keepdims=True)
    model = keras.models.Sequential()
    model.add(rnn.LSTM(10, return_sequences=True, unroll=False))
    model.add(rnn.LSTM(5, return_sequences=True, unroll=False))
    model.compile(
        loss='categorical_crossentropy',
        optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01))
    model.fit(inputs, targets, epochs=1, batch_size=2, verbose=1)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4405')" href="javascript:;">
keras-2.8.0/keras/layers/gru_v2_test.py: 116-127
</a>
<div class="mid" id="frag4405" style="display:none"><pre>
  def test_stacking_GRU(self):
    inputs = np.random.random((2, 3, 4))
    targets = np.abs(np.random.random((2, 3, 5)))
    targets /= targets.sum(axis=-1, keepdims=True)
    model = keras.models.Sequential()
    model.add(rnn.GRU(10, return_sequences=True, unroll=False))
    model.add(rnn.GRU(5, return_sequences=True, unroll=False))
    model.compile(
        loss='categorical_crossentropy',
        optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01))
    model.fit(inputs, targets, epochs=1, batch_size=2, verbose=1)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 71:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3808')" href="javascript:;">
keras-2.8.0/keras/layers/lstm_v2_test.py: 405-417
</a>
<div class="mid" id="frag3808" style="display:none"><pre>
  def test_masking_with_stacking_LSTM(self):
    inputs = np.random.random((2, 3, 4))
    targets = np.abs(np.random.random((2, 3, 5)))
    targets /= targets.sum(axis=-1, keepdims=True)
    model = keras.models.Sequential()
    model.add(keras.layers.Masking(input_shape=(3, 4)))
    model.add(rnn.LSTM(10, return_sequences=True, unroll=False))
    model.add(rnn.LSTM(5, return_sequences=True, unroll=False))
    model.compile(
        loss='categorical_crossentropy',
        optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01))
    model.fit(inputs, targets, epochs=1, batch_size=2, verbose=1)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4414')" href="javascript:;">
keras-2.8.0/keras/layers/gru_v2_test.py: 325-337
</a>
<div class="mid" id="frag4414" style="display:none"><pre>
  def test_masking_with_stacking_GRU(self):
    inputs = np.random.random((2, 3, 4))
    targets = np.abs(np.random.random((2, 3, 5)))
    targets /= targets.sum(axis=-1, keepdims=True)
    model = keras.models.Sequential()
    model.add(keras.layers.Masking(input_shape=(3, 4)))
    model.add(rnn.GRU(10, return_sequences=True, unroll=False))
    model.add(rnn.GRU(5, return_sequences=True, unroll=False))
    model.compile(
        loss='categorical_crossentropy',
        optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01))
    model.fit(inputs, targets, epochs=1, batch_size=2, verbose=1)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 72:</b> &nbsp; 2 fragments, nominal size 26 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3811')" href="javascript:;">
keras-2.8.0/keras/layers/lstm_v2_test.py: 493-525
</a>
<div class="mid" id="frag3811" style="display:none"><pre>
  def test_lstm_model_save_load(self, use_bias, bias_initializer):
    temp_dir = self.get_temp_dir()
    self.addCleanup(shutil.rmtree, temp_dir)
    h5_path = os.path.join(temp_dir, 'test.h5')

    batch = 10
    timestep = 3
    input_dim = 5
    units = 2

    x = np.random.random((batch, timestep, input_dim))

    def build_model():
      inputs = keras.layers.Input(
          shape=[timestep, input_dim], dtype=tf.float32)
      layer = rnn.LSTM(
          units,
          use_bias=use_bias,
          bias_initializer=bias_initializer)
      output = layer(inputs)
      return keras.models.Model(inputs, output), layer

    model, layer = build_model()
    y_ref = model.predict(x)
    model.save_weights(h5_path)

    cloned_model, new_layer = build_model()
    cloned_model.load_weights(h5_path)
    y = cloned_model.predict(x)

    self.assertAllClose(y, y_ref)
    self.assertAllClose(layer.get_weights(), new_layer.get_weights())

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4408')" href="javascript:;">
keras-2.8.0/keras/layers/gru_v2_test.py: 191-223
</a>
<div class="mid" id="frag4408" style="display:none"><pre>
  def test_gru_v2_model_save_load(self, use_bias, bias_initializer):
    temp_dir = self.get_temp_dir()
    self.addCleanup(shutil.rmtree, temp_dir)
    h5_path = os.path.join(temp_dir, 'test.h5')

    batch = 10
    timestep = 3
    input_dim = 5
    units = 2

    x = np.random.random((batch, timestep, input_dim))

    def build_model():
      inputs = keras.layers.Input(
          shape=[timestep, input_dim], dtype=tf.float32)
      layer = rnn.GRU(
          units,
          use_bias=use_bias,
          bias_initializer=bias_initializer)
      output = layer(inputs)
      return keras.models.Model(inputs, output), layer

    model, layer = build_model()
    y_ref = model.predict(x)
    model.save_weights(h5_path)

    cloned_model, new_layer = build_model()
    cloned_model.load_weights(h5_path)
    y = cloned_model.predict(x)

    self.assertAllClose(y, y_ref)
    self.assertAllClose(layer.get_weights(), new_layer.get_weights())

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 73:</b> &nbsp; 2 fragments, nominal size 20 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3816')" href="javascript:;">
keras-2.8.0/keras/layers/lstm_v2_test.py: 596-616
</a>
<div class="mid" id="frag3816" style="display:none"><pre>
  def test_regularizers_LSTM(self):
    embedding_dim = 4
    layer_class = rnn.LSTM
    layer = layer_class(
        5,
        return_sequences=False,
        weights=None,
        input_shape=(None, embedding_dim),
        kernel_regularizer=keras.regularizers.l1(0.01),
        recurrent_regularizer=keras.regularizers.l1(0.01),
        bias_regularizer='l2',
        activity_regularizer='l1')
    layer.build((None, None, 2))
    self.assertEqual(len(layer.losses), 3)
    x = keras.backend.variable(np.ones((2, 3, 2)))
    layer(x)
    if tf.executing_eagerly():
      self.assertEqual(len(layer.losses), 4)
    else:
      self.assertEqual(len(layer.get_losses_for(x)), 1)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4421')" href="javascript:;">
keras-2.8.0/keras/layers/gru_v2_test.py: 427-448
</a>
<div class="mid" id="frag4421" style="display:none"><pre>
  def test_regularizers_GRU(self):
    embedding_dim = 4
    layer_class = rnn.GRU
    layer = layer_class(
        5,
        return_sequences=False,
        weights=None,
        input_shape=(None, embedding_dim),
        kernel_regularizer=keras.regularizers.l1(0.01),
        recurrent_regularizer=keras.regularizers.l1(0.01),
        bias_regularizer='l2',
        activity_regularizer='l1')
    layer.build((None, None, 2))
    self.assertEqual(len(layer.losses), 3)

    x = keras.backend.variable(np.ones((2, 3, 2)))
    layer(x)
    if tf.executing_eagerly():
      self.assertEqual(len(layer.losses), 4)
    else:
      self.assertEqual(len(layer.get_losses_for(x)), 1)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 74:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3818')" href="javascript:;">
keras-2.8.0/keras/layers/lstm_v2_test.py: 693-714
</a>
<div class="mid" id="frag3818" style="display:none"><pre>
  def test_stateful_LSTM_training(self):
    # See b/123587692 for more context.
    vocab_size = 20
    embedding_dim = 10
    batch_size = 8
    timestep = 12
    units = 5
    x = np.random.randint(0, vocab_size, size=(batch_size, timestep))
    y = np.random.randint(0, vocab_size, size=(batch_size, timestep))

    model = keras.Sequential([
        keras.layers.Embedding(vocab_size, embedding_dim,
                               batch_input_shape=[batch_size, timestep]),
        rnn.LSTM(units, return_sequences=True, stateful=True),
        keras.layers.Dense(vocab_size)
    ])
    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        run_eagerly=testing_utils.should_run_eagerly())
    model.fit(x, y, epochs=1, shuffle=False)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4423')" href="javascript:;">
keras-2.8.0/keras/layers/gru_v2_test.py: 525-546
</a>
<div class="mid" id="frag4423" style="display:none"><pre>
  def test_stateful_GRU_training(self):
    # See b/123587692 for more context.
    vocab_size = 20
    embedding_dim = 10
    batch_size = 8
    timestep = 12
    units = 5
    x = np.random.randint(0, vocab_size, size=(batch_size, timestep))
    y = np.random.randint(0, vocab_size, size=(batch_size, timestep))

    model = keras.Sequential([
        keras.layers.Embedding(vocab_size, embedding_dim,
                               batch_input_shape=[batch_size, timestep]),
        rnn.GRU(units, return_sequences=True, stateful=True),
        keras.layers.Dense(vocab_size)
    ])
    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        run_eagerly=testing_utils.should_run_eagerly())
    model.fit(x, y, epochs=1, shuffle=False)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 75:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3821')" href="javascript:;">
keras-2.8.0/keras/layers/lstm_v2_test.py: 756-779
</a>
<div class="mid" id="frag3821" style="display:none"><pre>
  def test_explicit_device_with_go_backward_and_mask(self):
    batch_size = 8
    timestep = 7
    masksteps = 5
    units = 4

    inputs = np.random.randn(batch_size, timestep, units).astype(np.float32)
    mask = np.ones((batch_size, timestep)).astype(np.bool)
    mask[:, masksteps:] = 0

    # Test for V1 behavior.
    lstm_v1 = rnn_v1.LSTM(units, return_sequences=True, go_backwards=True)
    with testing_utils.device(should_use_gpu=True):
      outputs_masked_v1 = lstm_v1(inputs, mask=tf.constant(mask))
      outputs_trimmed_v1 = lstm_v1(inputs[:, :masksteps])
    self.assertAllClose(outputs_masked_v1[:, -masksteps:], outputs_trimmed_v1)

    # Test for V2 behavior.
    lstm = rnn.LSTM(units, return_sequences=True, go_backwards=True)
    with testing_utils.device(should_use_gpu=True):
      outputs_masked = lstm(inputs, mask=tf.constant(mask))
      outputs_trimmed = lstm(inputs[:, :masksteps])
    self.assertAllClose(outputs_masked[:, -masksteps:], outputs_trimmed)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4424')" href="javascript:;">
keras-2.8.0/keras/layers/gru_v2_test.py: 551-574
</a>
<div class="mid" id="frag4424" style="display:none"><pre>
  def test_explicit_device_with_go_backward_and_mask(self):
    batch_size = 8
    timestep = 7
    masksteps = 5
    units = 4

    inputs = np.random.randn(batch_size, timestep, units).astype(np.float32)
    mask = np.ones((batch_size, timestep)).astype(np.bool)
    mask[:, masksteps:] = 0

    # Test for V1 behavior.
    lstm_v1 = rnn_v1.GRU(units, return_sequences=True, go_backwards=True)
    with testing_utils.device(should_use_gpu=True):
      outputs_masked_v1 = lstm_v1(inputs, mask=tf.constant(mask))
      outputs_trimmed_v1 = lstm_v1(inputs[:, :masksteps])
    self.assertAllClose(outputs_masked_v1[:, -masksteps:], outputs_trimmed_v1)

    # Test for V2 behavior.
    lstm = rnn.GRU(units, return_sequences=True, go_backwards=True)
    with testing_utils.device(should_use_gpu=True):
      outputs_masked = lstm(inputs, mask=tf.constant(mask))
      outputs_trimmed = lstm(inputs[:, :masksteps])
    self.assertAllClose(outputs_masked[:, -masksteps:], outputs_trimmed)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 76:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3822')" href="javascript:;">
keras-2.8.0/keras/layers/lstm_v2_test.py: 781-797
</a>
<div class="mid" id="frag3822" style="display:none"><pre>
  def test_v1_session_behavior(self):
    with tf.compat.v1.get_default_graph().as_default():
      # See b/139132348 for more details.
      x = np.random.uniform(size=(100, 4, 8))
      y = np.random.uniform(size=(100, 1))
      dataset = tf.data.Dataset.from_tensor_slices(
          (x, y)).shuffle(100).batch(32)

      inp = keras.layers.Input(shape=(4, 8))
      layer = rnn.LSTM(1)(inp)
      layer = keras.layers.Dense(1)(layer)

      model = keras.models.Model(inp, layer)

      model.compile(loss='mse', optimizer='sgd')
      model.fit(dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4425')" href="javascript:;">
keras-2.8.0/keras/layers/gru_v2_test.py: 576-592
</a>
<div class="mid" id="frag4425" style="display:none"><pre>
  def test_v1_session_behavior(self):
    with tf.compat.v1.get_default_graph().as_default():
      # See b/139132348 for more details.
      x = np.random.uniform(size=(100, 4, 8))
      y = np.random.uniform(size=(100, 1))
      dataset = tf.data.Dataset.from_tensor_slices(
          (x, y)).shuffle(100).batch(32)

      inp = keras.layers.Input(shape=(4, 8))
      layer = rnn.GRU(1)(inp)
      layer = keras.layers.Dense(1)(layer)

      model = keras.models.Model(inp, layer)

      model.compile(loss='mse', optimizer='sgd')
      model.fit(dataset)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 77:</b> &nbsp; 2 fragments, nominal size 23 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3823')" href="javascript:;">
keras-2.8.0/keras/layers/lstm_v2_test.py: 798-826
</a>
<div class="mid" id="frag3823" style="display:none"><pre>
  def test_with_fully_masked_inputs(self):
    num_samples = 8
    timestep = 5
    embedding_dim = 4
    vocab_size = 20
    units = 2

    inputs = np.random.randint(0, vocab_size, size=(num_samples, timestep))
    # Set the first inputs to be fully zero.
    inputs[0, :] = 0.0

    model = keras.models.Sequential()
    model.add(
        keras.layers.Embedding(
            vocab_size,
            embedding_dim,
            mask_zero=True,
            input_length=timestep,
            batch_input_shape=(num_samples, timestep)))
    layer = rnn.LSTM(units)
    model.add(layer)
    model.compile(
        optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01),
        loss='mse',
        run_eagerly=testing_utils.should_run_eagerly())
    # Make sure it doesn't crash with cudnn kernel.
    model.predict(inputs)

  # TODO (b/169895267): test with xla_gpu is disabled.
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4426')" href="javascript:;">
keras-2.8.0/keras/layers/gru_v2_test.py: 593-621
</a>
<div class="mid" id="frag4426" style="display:none"><pre>
  def test_with_fully_masked_inputs(self):
    num_samples = 8
    timestep = 5
    embedding_dim = 4
    vocab_size = 20
    units = 2

    inputs = np.random.randint(0, vocab_size, size=(num_samples, timestep))
    # Set the first inputs to be fully zero.
    inputs[0, :] = 0.0

    model = keras.models.Sequential()
    model.add(
        keras.layers.Embedding(
            vocab_size,
            embedding_dim,
            mask_zero=True,
            input_length=timestep,
            batch_input_shape=(num_samples, timestep)))
    layer = rnn.GRU(units)
    model.add(layer)
    model.compile(
        optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01),
        loss='mse',
        run_eagerly=testing_utils.should_run_eagerly())
    # Make sure it doesn't crash with cudnn kernel.
    model.predict(inputs)

  # TODO (b/169895267): test with xla_gpu is disabled.
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 78:</b> &nbsp; 2 fragments, nominal size 19 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3824')" href="javascript:;">
keras-2.8.0/keras/layers/lstm_v2_test.py: 827-851
</a>
<div class="mid" id="frag3824" style="display:none"><pre>
  def test_deepcopy(self):
    if not tf.executing_eagerly():
      self.skipTest('v2-only test')
    original_layer = rnn.LSTM(5)
    copied_layer = copy.deepcopy(original_layer)
    self.assertEqual(copied_layer.units, 5)
    self.assertEqual(original_layer.get_config(), original_layer.get_config())

    # Copy layer before layer call on inputs without weight initialization.
    inputs = np.random.normal(size=[32, 10, 8]).astype(np.float32)
    original_layer = rnn.LSTM(4)
    copied_layer = copy.deepcopy(original_layer)
    outputs = original_layer(inputs)
    copied_outputs = copied_layer(inputs)
    self.assertNotAllClose(
        self.evaluate(outputs), self.evaluate(copied_outputs))

    # Copy layer after layer call on inputs with weight initialization.
    original_layer = rnn.LSTM(4)
    outputs = original_layer(inputs)
    copied_layer = copy.deepcopy(original_layer)
    copied_outputs = copied_layer(inputs)
    self.assertAllClose(self.evaluate(outputs), self.evaluate(copied_outputs))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4427')" href="javascript:;">
keras-2.8.0/keras/layers/gru_v2_test.py: 622-646
</a>
<div class="mid" id="frag4427" style="display:none"><pre>
  def test_deepcopy(self):
    if not tf.executing_eagerly():
      self.skipTest('v2-only test')
    original_layer = rnn.GRU(5)
    copied_layer = copy.deepcopy(original_layer)
    self.assertEqual(copied_layer.units, 5)
    self.assertEqual(original_layer.get_config(), original_layer.get_config())

    # Copy layer before layer call on inputs without weight initialization.
    inputs = np.random.normal(size=[32, 10, 8]).astype(np.float32)
    original_layer = rnn.GRU(4)
    copied_layer = copy.deepcopy(original_layer)
    outputs = original_layer(inputs)
    copied_outputs = copied_layer(inputs)
    self.assertNotAllClose(
        self.evaluate(outputs), self.evaluate(copied_outputs))

    # Copy layer after layer call on inputs with weight initialization.
    original_layer = rnn.GRU(4)
    outputs = original_layer(inputs)
    copied_layer = copy.deepcopy(original_layer)
    copied_outputs = copied_layer(inputs)
    self.assertAllClose(self.evaluate(outputs), self.evaluate(copied_outputs))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 79:</b> &nbsp; 2 fragments, nominal size 36 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3827')" href="javascript:;">
keras-2.8.0/keras/layers/lstm_v2_test.py: 910-963
</a>
<div class="mid" id="frag3827" style="display:none"><pre>
  def test_LSTM_runtime_with_mask(self):
    # Masking will affect which backend is selected based on whether the mask
    # is strictly right padded.
    layer = rnn.LSTM(self.rnn_state_size, return_runtime=True)

    inputs = keras.layers.Input(
        shape=[self.timestep, self.input_shape], dtype=tf.float32)
    masked_inputs = keras.layers.Masking()(inputs)

    outputs, runtime = layer(masked_inputs)
    # Expand the runtime so that it is a 1D tensor instead of scalar.
    # TF model does not work with scalar model output, specially during
    # aggregation.
    runtime = keras.layers.Lambda(
        lambda x: tf.expand_dims(x, axis=-1))(runtime)
    model = keras.models.Model(inputs=inputs, outputs=[outputs, runtime])

    (x_train, y_train), _ = testing_utils.get_test_data(
        train_samples=self.batch,
        test_samples=0,
        input_shape=(self.timestep, self.input_shape),
        num_classes=self.output_shape)
    y_train = np_utils.to_categorical(y_train, self.output_shape)

    model.compile(
        optimizer='sgd',
        loss=['categorical_crossentropy', None],
        run_eagerly=testing_utils.should_run_eagerly())

    model.fit(x_train, y_train)

    # Verify unpadded data.
    _, runtime_value = model.predict(x_train)
    if tf.test.is_gpu_available():
      self.assertEqual(runtime_value[0], rnn._RUNTIME_GPU)
    else:
      self.assertEqual(runtime_value[0], rnn._RUNTIME_CPU)

    # Update x/y to be right padded by setting the last timestep to 0
    x_train[:, -1, :] = 0
    y_train[:, -1] = 0
    _, runtime_value = model.predict(x_train)
    if tf.test.is_gpu_available():
      self.assertEqual(runtime_value[0], rnn._RUNTIME_GPU)
    else:
      self.assertEqual(runtime_value[0], rnn._RUNTIME_CPU)

    # Further update x/y to be mix padded (masks in the middle), and verify
    # only cpu kernel can be selected.
    x_train[:, -3, :] = 0
    y_train[:, -3] = 0
    _, runtime_value = model.predict(x_train)
    self.assertEqual(runtime_value[0], rnn._RUNTIME_CPU)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4431')" href="javascript:;">
keras-2.8.0/keras/layers/gru_v2_test.py: 732-785
</a>
<div class="mid" id="frag4431" style="display:none"><pre>
  def test_GRU_runtime_with_mask(self):
    # Masking will affect which backend is selected based on whether the mask
    # is strictly right padded.
    layer = rnn.GRU(self.rnn_state_size, return_runtime=True)

    inputs = keras.layers.Input(
        shape=[self.timestep, self.input_shape], dtype=tf.float32)
    masked_inputs = keras.layers.Masking()(inputs)

    outputs, runtime = layer(masked_inputs)
    # Expand the runtime so that it is a 1D tensor instead of scalar.
    # TF model does not work with scalar model output, specially during
    # aggregation.
    runtime = keras.layers.Lambda(
        lambda x: tf.expand_dims(x, axis=-1))(runtime)
    model = keras.models.Model(inputs=inputs, outputs=[outputs, runtime])

    (x_train, y_train), _ = testing_utils.get_test_data(
        train_samples=self.batch,
        test_samples=0,
        input_shape=(self.timestep, self.input_shape),
        num_classes=self.output_shape)
    y_train = np_utils.to_categorical(y_train, self.output_shape)

    model.compile(
        optimizer='sgd',
        loss=['categorical_crossentropy', None],
        run_eagerly=testing_utils.should_run_eagerly())

    model.fit(x_train, y_train)

    # Verify unpadded data.
    _, runtime_value = model.predict(x_train)
    if tf.test.is_gpu_available():
      self.assertEqual(runtime_value[0], rnn._RUNTIME_GPU)
    else:
      self.assertEqual(runtime_value[0], rnn._RUNTIME_CPU)

    # Update x/y to be right padded by setting the last timestep to 0
    x_train[:, -1, :] = 0
    y_train[:, -1] = 0
    _, runtime_value = model.predict(x_train)
    if tf.test.is_gpu_available():
      self.assertEqual(runtime_value[0], rnn._RUNTIME_GPU)
    else:
      self.assertEqual(runtime_value[0], rnn._RUNTIME_CPU)

    # Further update x/y to be mix padded (masks in the middle), and verify
    # only cpu kernel can be selected.
    x_train[:, -3, :] = 0
    y_train[:, -3] = 0
    _, runtime_value = model.predict(x_train)
    self.assertEqual(runtime_value[0], rnn._RUNTIME_CPU)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 80:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3828')" href="javascript:;">
keras-2.8.0/keras/layers/lstm_v2_test.py: 965-992
</a>
<div class="mid" id="frag3828" style="display:none"><pre>
  def test_LSTM_runtime_with_cond(self):
    # This test is to demonstrate the graph rewrite of grappler plugin under
    # the condition that the function returns different number of internal
    # states.
    layer = rnn.LSTM(self.rnn_state_size, return_runtime=True)

    inputs = keras.layers.Input(
        shape=[self.timestep, self.input_shape], dtype=tf.float32)

    zeros = tf.zeros([self.batch, self.output_shape])
    dummy_runtime = rnn._runtime(rnn._RUNTIME_UNKNOWN)
    a = tf.constant(0)
    b = tf.constant(1)
    # Will always run the lstm layer.
    outputs, runtime = tf.cond(
        tf.less(a, b),
        lambda: layer(inputs),
        lambda: (zeros, dummy_runtime))

    # Expand the runtime so that it is a 1D tensor instead of scalar.
    # TF model does not work with scalar model output, specially during
    # aggregation.
    runtime = keras.layers.Lambda(
        lambda x: tf.expand_dims(x, axis=-1))(runtime)
    model = keras.models.Model(inputs=inputs, outputs=[outputs, runtime])
    self._test_runtime_with_model(model)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4432')" href="javascript:;">
keras-2.8.0/keras/layers/gru_v2_test.py: 787-814
</a>
<div class="mid" id="frag4432" style="display:none"><pre>
  def test_GRU_runtime_with_cond(self):
    # This test is to demonstrate the graph rewrite of grappler plugin under
    # the condition that the function returns different number of internal
    # states.
    layer = rnn.GRU(self.rnn_state_size, return_runtime=True)

    inputs = keras.layers.Input(
        shape=[self.timestep, self.input_shape], dtype=tf.float32)

    zeros = tf.zeros([self.batch, self.output_shape])
    dummy_runtime = rnn._runtime(rnn._RUNTIME_UNKNOWN)
    a = tf.constant(0)
    b = tf.constant(1)
    # Will always run the GRU layer.
    outputs, runtime = tf.cond(
        tf.less(a, b),
        lambda: layer(inputs),
        lambda: (zeros, dummy_runtime))

    # Expand the runtime so that it is a 1D tensor instead of scalar.
    # TF model does not work with scalar model output, specially during
    # aggregation.
    runtime = keras.layers.Lambda(
        lambda x: tf.expand_dims(x, axis=-1))(runtime)
    model = keras.models.Model(inputs=inputs, outputs=[outputs, runtime])
    self._test_runtime_with_model(model)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 81:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3831')" href="javascript:;">
keras-2.8.0/keras/layers/lstm_v2_test.py: 1027-1047
</a>
<div class="mid" id="frag3831" style="display:none"><pre>
  def _time_performance_run_unifed_lstm_gpu(
      self, test_config, x_train, y_train):
    # Get performance number for lstm_v2 with grappler swap the impl
    input_shape = test_config['input_shape']
    rnn_state_size = test_config['rnn_state_size']
    timestep = test_config['timestep']

    layer = rnn.LSTM(rnn_state_size)
    inputs = keras.layers.Input(
        shape=[timestep, input_shape], dtype=tf.float32)

    outputs = layer(inputs)
    model = keras.models.Model(inputs, outputs)
    model.compile('sgd', 'mse')

    sec_per_epoch = self._measure_performance(
        test_config, model, x_train, y_train)
    logging.info('Average performance for %s per epoch is: %s',
                 'LSTM V2', sec_per_epoch)
    return sec_per_epoch

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3832')" href="javascript:;">
keras-2.8.0/keras/layers/lstm_v2_test.py: 1048-1068
</a>
<div class="mid" id="frag3832" style="display:none"><pre>
  def _time_performance_run_normal_lstm(
      self, test_config, x_train, y_train):
    # Get performance number for standard LSTM on GPU.
    input_shape = test_config['input_shape']
    rnn_state_size = test_config['rnn_state_size']
    timestep = test_config['timestep']

    layer = rnn_v1.LSTM(rnn_state_size)
    inputs = keras.layers.Input(
        shape=[timestep, input_shape], dtype=tf.float32)

    outputs = layer(inputs)
    model = keras.models.Model(inputs, outputs)
    model.compile('sgd', 'mse')

    sec_per_epoch = self._measure_performance(
        test_config, model, x_train, y_train)
    logging.info('Average performance for %s per epoch is: %s',
                 'Normal LSTM', sec_per_epoch)
    return sec_per_epoch

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 82:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3836')" href="javascript:;">
keras-2.8.0/keras/layers/convolutional_test.py: 31-42
</a>
<div class="mid" id="frag3836" style="display:none"><pre>
  def _run_test(self, kwargs, expected_output_shape):
    num_samples = 2
    stack_size = 3
    length = 7

    with self.cached_session():
      testing_utils.layer_test(
          keras.layers.Conv1D,
          kwargs=kwargs,
          input_shape=(num_samples, length, stack_size),
          expected_output_shape=expected_output_shape)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3862')" href="javascript:;">
keras-2.8.0/keras/layers/convolutional_test.py: 508-519
</a>
<div class="mid" id="frag3862" style="display:none"><pre>
  def _run_test(self, kwargs, expected_output_shape):
    num_samples = 2
    stack_size = 3
    num_col = 6

    with testing_utils.use_gpu():
      testing_utils.layer_test(
          keras.layers.Conv1DTranspose,
          kwargs=kwargs,
          input_shape=(num_samples, num_col, stack_size),
          expected_output_shape=expected_output_shape)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 83:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3848')" href="javascript:;">
keras-2.8.0/keras/layers/convolutional_test.py: 263-279
</a>
<div class="mid" id="frag3848" style="display:none"><pre>
  def test_conv2d_regularizers(self):
    kwargs = {
        'filters': 3,
        'kernel_size': 3,
        'padding': 'valid',
        'kernel_regularizer': 'l2',
        'bias_regularizer': 'l2',
        'activity_regularizer': 'l2',
        'strides': 1
    }
    with self.cached_session():
      layer = keras.layers.Conv2D(**kwargs)
      layer.build((None, 5, 5, 2))
      self.assertEqual(len(layer.losses), 2)
      layer(keras.backend.variable(np.ones((1, 5, 5, 2))))
      self.assertEqual(len(layer.losses), 3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4232')" href="javascript:;">
keras-2.8.0/keras/layers/convolutional_transpose_test.py: 57-73
</a>
<div class="mid" id="frag4232" style="display:none"><pre>
  def test_conv2d_transpose_regularizers(self):
    kwargs = {
        'filters': 3,
        'kernel_size': 3,
        'padding': 'valid',
        'kernel_regularizer': 'l2',
        'bias_regularizer': 'l2',
        'activity_regularizer': 'l2',
        'strides': 1
    }
    with self.cached_session():
      layer = keras.layers.Conv2DTranspose(**kwargs)
      layer.build((None, 5, 5, 2))
      self.assertEqual(len(layer.losses), 2)
      layer(keras.backend.variable(np.ones((1, 5, 5, 2))))
      self.assertEqual(len(layer.losses), 3)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 84:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3849')" href="javascript:;">
keras-2.8.0/keras/layers/convolutional_test.py: 280-297
</a>
<div class="mid" id="frag3849" style="display:none"><pre>
  def test_conv2d_constraints(self):
    k_constraint = lambda x: x
    b_constraint = lambda x: x

    kwargs = {
        'filters': 3,
        'kernel_size': 3,
        'padding': 'valid',
        'kernel_constraint': k_constraint,
        'bias_constraint': b_constraint,
        'strides': 1
    }
    with self.cached_session():
      layer = keras.layers.Conv2D(**kwargs)
      layer.build((None, 5, 5, 2))
      self.assertEqual(layer.kernel.constraint, k_constraint)
      self.assertEqual(layer.bias.constraint, b_constraint)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4233')" href="javascript:;">
keras-2.8.0/keras/layers/convolutional_transpose_test.py: 74-91
</a>
<div class="mid" id="frag4233" style="display:none"><pre>
  def test_conv2d_transpose_constraints(self):
    k_constraint = lambda x: x
    b_constraint = lambda x: x

    kwargs = {
        'filters': 3,
        'kernel_size': 3,
        'padding': 'valid',
        'kernel_constraint': k_constraint,
        'bias_constraint': b_constraint,
        'strides': 1
    }
    with self.cached_session():
      layer = keras.layers.Conv2DTranspose(**kwargs)
      layer.build((None, 5, 5, 2))
      self.assertEqual(layer.kernel.constraint, k_constraint)
      self.assertEqual(layer.bias.constraint, b_constraint)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 85:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3856')" href="javascript:;">
keras-2.8.0/keras/layers/convolutional_test.py: 403-420
</a>
<div class="mid" id="frag3856" style="display:none"><pre>
  def test_conv3d_constraints(self):
    k_constraint = lambda x: x
    b_constraint = lambda x: x

    kwargs = {
        'filters': 3,
        'kernel_size': 3,
        'padding': 'valid',
        'kernel_constraint': k_constraint,
        'bias_constraint': b_constraint,
        'strides': 1
    }
    with self.cached_session():
      layer = keras.layers.Conv3D(**kwargs)
      layer.build((None, 5, 5, 5, 2))
      self.assertEqual(layer.kernel.constraint, k_constraint)
      self.assertEqual(layer.bias.constraint, b_constraint)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4238')" href="javascript:;">
keras-2.8.0/keras/layers/convolutional_transpose_test.py: 169-186
</a>
<div class="mid" id="frag4238" style="display:none"><pre>
  def test_conv3d_transpose_constraints(self):
    k_constraint = lambda x: x
    b_constraint = lambda x: x

    kwargs = {
        'filters': 3,
        'kernel_size': 3,
        'padding': 'valid',
        'kernel_constraint': k_constraint,
        'bias_constraint': b_constraint,
        'strides': 1
    }
    with self.cached_session():
      layer = keras.layers.Conv3DTranspose(**kwargs)
      layer.build((None, 5, 5, 5, 2))
      self.assertEqual(layer.kernel.constraint, k_constraint)
      self.assertEqual(layer.bias.constraint, b_constraint)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 86:</b> &nbsp; 2 fragments, nominal size 20 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3857')" href="javascript:;">
keras-2.8.0/keras/layers/convolutional_test.py: 421-444
</a>
<div class="mid" id="frag3857" style="display:none"><pre>
  def test_conv3d_dynamic_shape(self):
    input_data = np.random.random((1, 3, 3, 3, 3)).astype(np.float32)
    with self.cached_session():
      # Won't raise error here.
      testing_utils.layer_test(
          keras.layers.Conv3D,
          kwargs={
              'data_format': 'channels_last',
              'filters': 3,
              'kernel_size': 3
          },
          input_shape=(None, None, None, None, 3),
          input_data=input_data)
      if tf.test.is_gpu_available(cuda_only=True):
        testing_utils.layer_test(
            keras.layers.Conv3D,
            kwargs={
                'data_format': 'channels_first',
                'filters': 3,
                'kernel_size': 3
            },
            input_shape=(None, 3, None, None, None),
            input_data=input_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4239')" href="javascript:;">
keras-2.8.0/keras/layers/convolutional_transpose_test.py: 187-210
</a>
<div class="mid" id="frag4239" style="display:none"><pre>
  def test_conv3d_transpose_dynamic_shape(self):
    input_data = np.random.random((1, 3, 3, 3, 3)).astype(np.float32)
    with self.cached_session():
      # Won't raise error here.
      testing_utils.layer_test(
          keras.layers.Conv3DTranspose,
          kwargs={
              'data_format': 'channels_last',
              'filters': 3,
              'kernel_size': 3
          },
          input_shape=(None, None, None, None, 3),
          input_data=input_data)
      if tf.test.is_gpu_available(cuda_only=True):
        testing_utils.layer_test(
            keras.layers.Conv3DTranspose,
            kwargs={
                'data_format': 'channels_first',
                'filters': 3,
                'kernel_size': 3
            },
            input_shape=(None, 3, None, None, None),
            input_data=input_data)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 87:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3932')" href="javascript:;">
keras-2.8.0/keras/layers/dense_attention_test.py: 238-252
</a>
<div class="mid" id="frag3932" style="display:none"><pre>
  def test_shape(self):
    # Query tensor of shape [1, 2, 4]
    q = np.array([[[1., 1.1, 1.2, 1.3], [2., 2.1, 2.2, 2.3]]], dtype=np.float32)
    # Value tensor of shape [1, 3, 4]
    v = np.array(
        [[[1.5, 1.6, 1.7, 1.8], [2.5, 2.6, 2.7, 2.8], [3.5, 3.6, 3.7, 3.8]]],
        dtype=np.float32)
    # Value mask tensor of shape [1, 3]
    v_mask = np.array([[True, True, False]], dtype=np.bool_)
    attention_layer = dense_attention.Attention()
    actual = attention_layer([q, v], mask=[None, v_mask])

    expected_shape = [1, 2, 4]
    self.assertAllEqual(expected_shape, tf.shape(actual))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3953')" href="javascript:;">
keras-2.8.0/keras/layers/dense_attention_test.py: 580-594
</a>
<div class="mid" id="frag3953" style="display:none"><pre>
  def test_shape(self):
    # Query tensor of shape [1, 2, 4]
    q = np.array([[[1., 1.1, 1.2, 1.3], [2., 2.1, 2.2, 2.3]]], dtype=np.float32)
    # Value tensor of shape [1, 3, 4]
    v = np.array(
        [[[1.5, 1.6, 1.7, 1.8], [2.5, 2.6, 2.7, 2.8], [3.5, 3.6, 3.7, 3.8]]],
        dtype=np.float32)
    # Value mask tensor of shape [1, 3]
    v_mask = np.array([[True, True, False]], dtype=np.bool_)
    attention_layer = dense_attention.AdditiveAttention()
    actual = attention_layer([q, v], mask=[None, v_mask])

    expected_shape = [1, 2, 4]
    self.assertAllEqual(expected_shape, tf.shape(actual))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 88:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3933')" href="javascript:;">
keras-2.8.0/keras/layers/dense_attention_test.py: 253-271
</a>
<div class="mid" id="frag3933" style="display:none"><pre>
  def test_shape_with_key(self):
    # Query tensor of shape [1, 2, 4]
    q = np.array([[[1., 1.1, 1.2, 1.3], [2., 2.1, 2.2, 2.3]]], dtype=np.float32)
    # Value tensor of shape [1, 3, 4]
    v = np.array(
        [[[1.5, 1.6, 1.7, 1.8], [2.5, 2.6, 2.7, 2.8], [3.5, 3.6, 3.7, 3.8]]],
        dtype=np.float32)
    # Key tensor of shape [1, 3, 4]
    k = np.array(
        [[[1.5, 1.6, 1.7, 1.8], [2.5, 2.6, 2.7, 2.8], [3.5, 3.6, 3.7, 3.8]]],
        dtype=np.float32)
    # Value mask tensor of shape [1, 3]
    v_mask = np.array([[True, True, False]], dtype=np.bool_)
    attention_layer = dense_attention.Attention()
    actual = attention_layer([q, v, k], mask=[None, v_mask])

    expected_shape = [1, 2, 4]
    self.assertAllEqual(expected_shape, tf.shape(actual))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3955')" href="javascript:;">
keras-2.8.0/keras/layers/dense_attention_test.py: 610-628
</a>
<div class="mid" id="frag3955" style="display:none"><pre>
  def test_shape_with_key(self):
    # Query tensor of shape [1, 2, 4]
    q = np.array([[[1., 1.1, 1.2, 1.3], [2., 2.1, 2.2, 2.3]]], dtype=np.float32)
    # Value tensor of shape [1, 3, 4]
    v = np.array(
        [[[1.5, 1.6, 1.7, 1.8], [2.5, 2.6, 2.7, 2.8], [3.5, 3.6, 3.7, 3.8]]],
        dtype=np.float32)
    # Key tensor of shape [1, 3, 4]
    k = np.array(
        [[[1.5, 1.6, 1.7, 1.8], [2.5, 2.6, 2.7, 2.8], [3.5, 3.6, 3.7, 3.8]]],
        dtype=np.float32)
    # Value mask tensor of shape [1, 3]
    v_mask = np.array([[True, True, False]], dtype=np.bool_)
    attention_layer = dense_attention.AdditiveAttention()
    actual = attention_layer([q, v, k], mask=[None, v_mask])

    expected_shape = [1, 2, 4]
    self.assertAllEqual(expected_shape, tf.shape(actual))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 89:</b> &nbsp; 2 fragments, nominal size 34 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3977')" href="javascript:;">
keras-2.8.0/keras/layers/local.py: 300-336
</a>
<div class="mid" id="frag3977" style="display:none"><pre>
  def get_config(self):
    config = {
        'filters':
            self.filters,
        'kernel_size':
            self.kernel_size,
        'strides':
            self.strides,
        'padding':
            self.padding,
        'data_format':
            self.data_format,
        'activation':
            activations.serialize(self.activation),
        'use_bias':
            self.use_bias,
        'kernel_initializer':
            initializers.serialize(self.kernel_initializer),
        'bias_initializer':
            initializers.serialize(self.bias_initializer),
        'kernel_regularizer':
            regularizers.serialize(self.kernel_regularizer),
        'bias_regularizer':
            regularizers.serialize(self.bias_regularizer),
        'activity_regularizer':
            regularizers.serialize(self.activity_regularizer),
        'kernel_constraint':
            constraints.serialize(self.kernel_constraint),
        'bias_constraint':
            constraints.serialize(self.bias_constraint),
        'implementation':
            self.implementation
    }
    base_config = super(LocallyConnected1D, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3983')" href="javascript:;">
keras-2.8.0/keras/layers/local.py: 625-661
</a>
<div class="mid" id="frag3983" style="display:none"><pre>
  def get_config(self):
    config = {
        'filters':
            self.filters,
        'kernel_size':
            self.kernel_size,
        'strides':
            self.strides,
        'padding':
            self.padding,
        'data_format':
            self.data_format,
        'activation':
            activations.serialize(self.activation),
        'use_bias':
            self.use_bias,
        'kernel_initializer':
            initializers.serialize(self.kernel_initializer),
        'bias_initializer':
            initializers.serialize(self.bias_initializer),
        'kernel_regularizer':
            regularizers.serialize(self.kernel_regularizer),
        'bias_regularizer':
            regularizers.serialize(self.bias_regularizer),
        'activity_regularizer':
            regularizers.serialize(self.activity_regularizer),
        'kernel_constraint':
            constraints.serialize(self.kernel_constraint),
        'bias_constraint':
            constraints.serialize(self.bias_constraint),
        'implementation':
            self.implementation
    }
    base_config = super(LocallyConnected2D, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 90:</b> &nbsp; 3 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4058')" href="javascript:;">
keras-2.8.0/keras/layers/lstm_test.py: 32-42
</a>
<div class="mid" id="frag4058" style="display:none"><pre>
  def test_return_sequences_LSTM(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        keras.layers.LSTM,
        kwargs={'units': units,
                'return_sequences': True},
        input_shape=(num_samples, timesteps, embedding_dim))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4079')" href="javascript:;">
keras-2.8.0/keras/layers/simplernn_test.py: 32-42
</a>
<div class="mid" id="frag4079" style="display:none"><pre>
  def test_return_sequences_SimpleRNN(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        keras.layers.SimpleRNN,
        kwargs={'units': units,
                'return_sequences': True},
        input_shape=(num_samples, timesteps, embedding_dim))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4614')" href="javascript:;">
keras-2.8.0/keras/layers/gru_test.py: 34-44
</a>
<div class="mid" id="frag4614" style="display:none"><pre>
  def test_return_sequences_GRU(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        keras.layers.GRU,
        kwargs={'units': units,
                'return_sequences': True},
        input_shape=(num_samples, timesteps, embedding_dim))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 91:</b> &nbsp; 3 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4059')" href="javascript:;">
keras-2.8.0/keras/layers/lstm_test.py: 47-59
</a>
<div class="mid" id="frag4059" style="display:none"><pre>
  def test_float64_LSTM(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        keras.layers.LSTM,
        kwargs={'units': units,
                'return_sequences': True,
                'dtype': 'float64'},
        input_shape=(num_samples, timesteps, embedding_dim),
        input_dtype='float64')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4615')" href="javascript:;">
keras-2.8.0/keras/layers/gru_test.py: 49-61
</a>
<div class="mid" id="frag4615" style="display:none"><pre>
  def test_float64_GRU(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        keras.layers.GRU,
        kwargs={'units': units,
                'return_sequences': True,
                'dtype': 'float64'},
        input_shape=(num_samples, timesteps, embedding_dim),
        input_dtype='float64')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4080')" href="javascript:;">
keras-2.8.0/keras/layers/simplernn_test.py: 44-56
</a>
<div class="mid" id="frag4080" style="display:none"><pre>
  def test_float64_SimpleRNN(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        keras.layers.SimpleRNN,
        kwargs={'units': units,
                'return_sequences': True,
                'dtype': 'float64'},
        input_shape=(num_samples, timesteps, embedding_dim),
        input_dtype='float64')

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 92:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4061')" href="javascript:;">
keras-2.8.0/keras/layers/lstm_test.py: 75-91
</a>
<div class="mid" id="frag4061" style="display:none"><pre>
  def test_dynamic_behavior_LSTM(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    layer = keras.layers.LSTM(units, input_shape=(None, embedding_dim))
    model = keras.models.Sequential()
    model.add(layer)
    model.compile(
        'rmsprop',
        'mse',
        run_eagerly=testing_utils.should_run_eagerly())

    x = np.random.random((num_samples, timesteps, embedding_dim))
    y = np.random.random((num_samples, units))
    model.train_on_batch(x, y)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4616')" href="javascript:;">
keras-2.8.0/keras/layers/gru_test.py: 62-77
</a>
<div class="mid" id="frag4616" style="display:none"><pre>
  def test_dynamic_behavior_GRU(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    layer = keras.layers.GRU(units, input_shape=(None, embedding_dim))
    model = keras.models.Sequential()
    model.add(layer)
    model.compile(
        'rmsprop',
        'mse',
        run_eagerly=testing_utils.should_run_eagerly())
    x = np.random.random((num_samples, timesteps, embedding_dim))
    y = np.random.random((num_samples, units))
    model.train_on_batch(x, y)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 93:</b> &nbsp; 3 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4062')" href="javascript:;">
keras-2.8.0/keras/layers/lstm_test.py: 92-103
</a>
<div class="mid" id="frag4062" style="display:none"><pre>
  def test_dropout_LSTM(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        keras.layers.LSTM,
        kwargs={'units': units,
                'dropout': 0.1,
                'recurrent_dropout': 0.1},
        input_shape=(num_samples, timesteps, embedding_dim))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4082')" href="javascript:;">
keras-2.8.0/keras/layers/simplernn_test.py: 70-81
</a>
<div class="mid" id="frag4082" style="display:none"><pre>
  def test_dropout_SimpleRNN(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        keras.layers.SimpleRNN,
        kwargs={'units': units,
                'dropout': 0.1,
                'recurrent_dropout': 0.1},
        input_shape=(num_samples, timesteps, embedding_dim))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4617')" href="javascript:;">
keras-2.8.0/keras/layers/gru_test.py: 78-89
</a>
<div class="mid" id="frag4617" style="display:none"><pre>
  def test_dropout_GRU(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        keras.layers.GRU,
        kwargs={'units': units,
                'dropout': 0.1,
                'recurrent_dropout': 0.1},
        input_shape=(num_samples, timesteps, embedding_dim))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 94:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4064')" href="javascript:;">
keras-2.8.0/keras/layers/lstm_test.py: 110-120
</a>
<div class="mid" id="frag4064" style="display:none"><pre>
  def test_implementation_mode_LSTM(self, implementation_mode):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        keras.layers.LSTM,
        kwargs={'units': units,
                'implementation': implementation_mode},
        input_shape=(num_samples, timesteps, embedding_dim))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4619')" href="javascript:;">
keras-2.8.0/keras/layers/gru_test.py: 96-106
</a>
<div class="mid" id="frag4619" style="display:none"><pre>
  def test_implementation_mode_GRU(self, implementation_mode):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        keras.layers.GRU,
        kwargs={'units': units,
                'implementation': implementation_mode},
        input_shape=(num_samples, timesteps, embedding_dim))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 95:</b> &nbsp; 3 fragments, nominal size 18 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4065')" href="javascript:;">
keras-2.8.0/keras/layers/lstm_test.py: 121-139
</a>
<div class="mid" id="frag4065" style="display:none"><pre>
  def test_constraints_LSTM(self):
    embedding_dim = 4
    layer_class = keras.layers.LSTM
    k_constraint = keras.constraints.max_norm(0.01)
    r_constraint = keras.constraints.max_norm(0.01)
    b_constraint = keras.constraints.max_norm(0.01)
    layer = layer_class(
        5,
        return_sequences=False,
        weights=None,
        input_shape=(None, embedding_dim),
        kernel_constraint=k_constraint,
        recurrent_constraint=r_constraint,
        bias_constraint=b_constraint)
    layer.build((None, None, embedding_dim))
    self.assertEqual(layer.cell.kernel.constraint, k_constraint)
    self.assertEqual(layer.cell.recurrent_kernel.constraint, r_constraint)
    self.assertEqual(layer.cell.bias.constraint, b_constraint)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4084')" href="javascript:;">
keras-2.8.0/keras/layers/simplernn_test.py: 94-112
</a>
<div class="mid" id="frag4084" style="display:none"><pre>
  def test_constraints_SimpleRNN(self):
    embedding_dim = 4
    layer_class = keras.layers.SimpleRNN
    k_constraint = keras.constraints.max_norm(0.01)
    r_constraint = keras.constraints.max_norm(0.01)
    b_constraint = keras.constraints.max_norm(0.01)
    layer = layer_class(
        5,
        return_sequences=False,
        weights=None,
        input_shape=(None, embedding_dim),
        kernel_constraint=k_constraint,
        recurrent_constraint=r_constraint,
        bias_constraint=b_constraint)
    layer.build((None, None, embedding_dim))
    self.assertEqual(layer.cell.kernel.constraint, k_constraint)
    self.assertEqual(layer.cell.recurrent_kernel.constraint, r_constraint)
    self.assertEqual(layer.cell.bias.constraint, b_constraint)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4624')" href="javascript:;">
keras-2.8.0/keras/layers/gru_test.py: 229-247
</a>
<div class="mid" id="frag4624" style="display:none"><pre>
  def test_constraints_GRU(self):
    embedding_dim = 4
    layer_class = keras.layers.GRU
    k_constraint = keras.constraints.max_norm(0.01)
    r_constraint = keras.constraints.max_norm(0.01)
    b_constraint = keras.constraints.max_norm(0.01)
    layer = layer_class(
        5,
        return_sequences=False,
        weights=None,
        input_shape=(None, embedding_dim),
        kernel_constraint=k_constraint,
        recurrent_constraint=r_constraint,
        bias_constraint=b_constraint)
    layer.build((None, None, embedding_dim))
    self.assertEqual(layer.cell.kernel.constraint, k_constraint)
    self.assertEqual(layer.cell.recurrent_kernel.constraint, r_constraint)
    self.assertEqual(layer.cell.bias.constraint, b_constraint)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 96:</b> &nbsp; 2 fragments, nominal size 20 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4077')" href="javascript:;">
keras-2.8.0/keras/layers/lstm_test.py: 362-382
</a>
<div class="mid" id="frag4077" style="display:none"><pre>
  def test_regularizers_LSTM(self):
    embedding_dim = 4
    layer_class = keras.layers.LSTM
    layer = layer_class(
        5,
        return_sequences=False,
        weights=None,
        input_shape=(None, embedding_dim),
        kernel_regularizer=keras.regularizers.l1(0.01),
        recurrent_regularizer=keras.regularizers.l1(0.01),
        bias_regularizer='l2',
        activity_regularizer='l1')
    layer.build((None, None, 2))
    self.assertEqual(len(layer.losses), 3)
    x = keras.backend.variable(np.ones((2, 3, 2)))
    layer(x)
    if tf.executing_eagerly():
      self.assertEqual(len(layer.losses), 4)
    else:
      self.assertEqual(len(layer.get_losses_for(x)), 1)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4627')" href="javascript:;">
keras-2.8.0/keras/layers/gru_test.py: 261-283
</a>
<div class="mid" id="frag4627" style="display:none"><pre>
  def test_regularizers_GRU(self):
    embedding_dim = 4
    layer_class = keras.layers.GRU
    layer = layer_class(
        5,
        return_sequences=False,
        weights=None,
        input_shape=(None, embedding_dim),
        kernel_regularizer=keras.regularizers.l1(0.01),
        recurrent_regularizer=keras.regularizers.l1(0.01),
        bias_regularizer='l2',
        activity_regularizer='l1')
    layer.build((None, None, 2))
    self.assertEqual(len(layer.losses), 3)

    x = keras.backend.variable(np.ones((2, 3, 2)))
    layer(x)
    if tf.executing_eagerly():
      self.assertEqual(len(layer.losses), 4)
    else:
      self.assertEqual(len(layer.get_losses_for(x)), 1)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 97:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4159')" href="javascript:;">
keras-2.8.0/keras/layers/pooling_test.py: 285-304
</a>
<div class="mid" id="frag4159" style="display:none"><pre>
  def test_maxpooling_3d(self):
    pool_size = (3, 3, 3)
    testing_utils.layer_test(
        keras.layers.MaxPooling3D,
        kwargs={
            'strides': 2,
            'padding': 'valid',
            'pool_size': pool_size
        },
        input_shape=(3, 11, 12, 10, 4))
    testing_utils.layer_test(
        keras.layers.MaxPooling3D,
        kwargs={
            'strides': 3,
            'padding': 'valid',
            'data_format': 'channels_first',
            'pool_size': pool_size
        },
        input_shape=(3, 4, 11, 12, 10))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4160')" href="javascript:;">
keras-2.8.0/keras/layers/pooling_test.py: 305-325
</a>
<div class="mid" id="frag4160" style="display:none"><pre>
  def test_averagepooling_3d(self):
    pool_size = (3, 3, 3)
    testing_utils.layer_test(
        keras.layers.AveragePooling3D,
        kwargs={
            'strides': 2,
            'padding': 'valid',
            'pool_size': pool_size
        },
        input_shape=(3, 11, 12, 10, 4))
    testing_utils.layer_test(
        keras.layers.AveragePooling3D,
        kwargs={
            'strides': 3,
            'padding': 'valid',
            'data_format': 'channels_first',
            'pool_size': pool_size
        },
        input_shape=(3, 4, 11, 12, 10))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 98:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4161')" href="javascript:;">
keras-2.8.0/keras/layers/pooling_test.py: 329-343
</a>
<div class="mid" id="frag4161" style="display:none"><pre>
  def test_maxpooling_1d(self):
    for padding in ['valid', 'same']:
      for stride in [1, 2]:
        testing_utils.layer_test(
            keras.layers.MaxPooling1D,
            kwargs={
                'strides': stride,
                'padding': padding
            },
            input_shape=(3, 5, 4))
    testing_utils.layer_test(
        keras.layers.MaxPooling1D,
        kwargs={'data_format': 'channels_first'},
        input_shape=(3, 2, 6))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4162')" href="javascript:;">
keras-2.8.0/keras/layers/pooling_test.py: 344-360
</a>
<div class="mid" id="frag4162" style="display:none"><pre>
  def test_averagepooling_1d(self):
    for padding in ['valid', 'same']:
      for stride in [1, 2]:
        testing_utils.layer_test(
            keras.layers.AveragePooling1D,
            kwargs={
                'strides': stride,
                'padding': padding
            },
            input_shape=(3, 5, 4))

    testing_utils.layer_test(
        keras.layers.AveragePooling1D,
        kwargs={'data_format': 'channels_first'},
        input_shape=(3, 2, 6))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 99:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4217')" href="javascript:;">
keras-2.8.0/keras/layers/serialization_test.py: 137-149
</a>
<div class="mid" id="frag4217" style="display:none"><pre>
  def test_serialize_deserialize_lstm(self, layer):
    lstm = layer(5, return_sequences=True)
    config = keras.layers.serialize(lstm)
    self.assertEqual(config['class_name'], 'LSTM')
    new_layer = keras.layers.deserialize(config)
    self.assertEqual(new_layer.units, 5)
    self.assertEqual(new_layer.return_sequences, True)
    if tf.__internal__.tf2.enabled():
      self.assertIsInstance(new_layer, rnn_v2.LSTM)
    else:
      self.assertIsInstance(new_layer, rnn_v1.LSTM)
      self.assertNotIsInstance(new_layer, rnn_v2.LSTM)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4218')" href="javascript:;">
keras-2.8.0/keras/layers/serialization_test.py: 151-164
</a>
<div class="mid" id="frag4218" style="display:none"><pre>
  def test_serialize_deserialize_gru(self, layer):
    gru = layer(5, return_sequences=True)
    config = keras.layers.serialize(gru)
    self.assertEqual(config['class_name'], 'GRU')
    new_layer = keras.layers.deserialize(config)
    self.assertEqual(new_layer.units, 5)
    self.assertEqual(new_layer.return_sequences, True)
    if tf.__internal__.tf2.enabled():
      self.assertIsInstance(new_layer, rnn_v2.GRU)
    else:
      self.assertIsInstance(new_layer, rnn_v1.GRU)
      self.assertNotIsInstance(new_layer, rnn_v2.GRU)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 100:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4230')" href="javascript:;">
keras-2.8.0/keras/layers/convolutional_transpose_test.py: 30-41
</a>
<div class="mid" id="frag4230" style="display:none"><pre>
  def _run_test(self, kwargs):
    num_samples = 2
    stack_size = 3
    num_row = 7
    num_col = 6

    with self.cached_session():
      testing_utils.layer_test(
          keras.layers.Conv2DTranspose,
          kwargs=kwargs,
          input_shape=(num_samples, num_row, num_col, stack_size))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4811')" href="javascript:;">
keras-2.8.0/keras/layers/separable_convolutional_test.py: 98-109
</a>
<div class="mid" id="frag4811" style="display:none"><pre>
  def _run_test(self, kwargs):
    num_samples = 2
    stack_size = 3
    num_row = 7
    num_col = 6

    with self.cached_session():
      testing_utils.layer_test(
          keras.layers.SeparableConv2D,
          kwargs=kwargs,
          input_shape=(num_samples, num_row, num_col, stack_size))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 101:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4433')" href="javascript:;">
keras-2.8.0/keras/layers/pooling.py: 50-65
</a>
<div class="mid" id="frag4433" style="display:none"><pre>
  def __init__(self, pool_function, pool_size, strides,
               padding='valid', data_format='channels_last',
               name=None, **kwargs):
    super(Pooling1D, self).__init__(name=name, **kwargs)
    if data_format is None:
      data_format = backend.image_data_format()
    if strides is None:
      strides = pool_size
    self.pool_function = pool_function
    self.pool_size = conv_utils.normalize_tuple(pool_size, 1, 'pool_size')
    self.strides = conv_utils.normalize_tuple(
        strides, 1, 'strides', allow_zero=True)
    self.padding = conv_utils.normalize_padding(padding)
    self.data_format = conv_utils.normalize_data_format(data_format)
    self.input_spec = InputSpec(ndim=3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4445')" href="javascript:;">
keras-2.8.0/keras/layers/pooling.py: 673-688
</a>
<div class="mid" id="frag4445" style="display:none"><pre>
  def __init__(self, pool_function, pool_size, strides,
               padding='valid', data_format='channels_last',
               name=None, **kwargs):
    super(Pooling3D, self).__init__(name=name, **kwargs)
    if data_format is None:
      data_format = backend.image_data_format()
    if strides is None:
      strides = pool_size
    self.pool_function = pool_function
    self.pool_size = conv_utils.normalize_tuple(pool_size, 3, 'pool_size')
    self.strides = conv_utils.normalize_tuple(
        strides, 3, 'strides', allow_zero=True)
    self.padding = conv_utils.normalize_padding(padding)
    self.data_format = conv_utils.normalize_data_format(data_format)
    self.input_spec = InputSpec(ndim=5)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 102:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4449')" href="javascript:;">
keras-2.8.0/keras/layers/pooling.py: 802-813
</a>
<div class="mid" id="frag4449" style="display:none"><pre>
  def __init__(self,
               pool_size=(2, 2, 2),
               strides=None,
               padding='valid',
               data_format=None,
               **kwargs):
    super(MaxPooling3D, self).__init__(
        tf.nn.max_pool3d,
        pool_size=pool_size, strides=strides,
        padding=padding, data_format=data_format, **kwargs)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4450')" href="javascript:;">
keras-2.8.0/keras/layers/pooling.py: 873-884
</a>
<div class="mid" id="frag4450" style="display:none"><pre>
  def __init__(self,
               pool_size=(2, 2, 2),
               strides=None,
               padding='valid',
               data_format=None,
               **kwargs):
    super(AveragePooling3D, self).__init__(
        tf.nn.avg_pool3d,
        pool_size=pool_size, strides=strides,
        padding=padding, data_format=data_format, **kwargs)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 103:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4628')" href="javascript:;">
keras-2.8.0/keras/layers/cudnn_recurrent_test.py: 41-51
</a>
<div class="mid" id="frag4628" style="display:none"><pre>
  def test_cudnn_rnn_return_sequence(self, layer_class, return_sequences):
    input_size = 10
    timesteps = 6
    units = 2
    num_samples = 32
    testing_utils.layer_test(
        layer_class,
        kwargs={'units': units,
                'return_sequences': return_sequences},
        input_shape=(num_samples, timesteps, input_size))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4629')" href="javascript:;">
keras-2.8.0/keras/layers/cudnn_recurrent_test.py: 57-67
</a>
<div class="mid" id="frag4629" style="display:none"><pre>
  def test_cudnn_rnn_go_backward(self, layer_class, go_backwards):
    input_size = 10
    timesteps = 6
    units = 2
    num_samples = 32
    testing_utils.layer_test(
        layer_class,
        kwargs={'units': units,
                'go_backwards': go_backwards},
        input_shape=(num_samples, timesteps, input_size))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 104:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4841')" href="javascript:;">
keras-2.8.0/keras/layers/recurrent_v2.py: 778-790
</a>
<div class="mid" id="frag4841" style="display:none"><pre>
    def standard_gru_fn():
      return standard_gru(
          inputs=inputs,
          init_h=init_h,
          kernel=kernel,
          recurrent_kernel=recurrent_kernel,
          bias=bias,
          mask=mask,
          time_major=time_major,
          go_backwards=go_backwards,
          sequence_lengths=sequence_lengths,
          zero_output_for_mask=zero_output_for_mask)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4853')" href="javascript:;">
keras-2.8.0/keras/layers/recurrent_v2.py: 1591-1603
</a>
<div class="mid" id="frag4853" style="display:none"><pre>
    def cudnn_lstm_fn():
      return gpu_lstm(
          inputs=inputs,
          init_h=init_h,
          init_c=init_c,
          kernel=kernel,
          recurrent_kernel=recurrent_kernel,
          bias=bias,
          mask=mask,
          time_major=time_major,
          go_backwards=go_backwards,
          sequence_lengths=sequence_lengths)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 105:</b> &nbsp; 3 fragments, nominal size 15 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4878')" href="javascript:;">
keras-2.8.0/keras/layers/advanced_activations_test.py: 91-109
</a>
<div class="mid" id="frag4878" style="display:none"><pre>
  def test_relu_with_invalid_negative_slope(self):
    with self.assertRaisesRegex(
        ValueError, 'negative_slope of a ReLU layer cannot be a negative '
        'value. Received: None'):
      testing_utils.layer_test(
          keras.layers.ReLU,
          kwargs={'negative_slope': None},
          input_shape=(2, 3, 4),
          supports_masking=True)

    with self.assertRaisesRegex(
        ValueError, 'negative_slope of a ReLU layer cannot be a negative '
        'value. Received: -10'):
      testing_utils.layer_test(
          keras.layers.ReLU,
          kwargs={'negative_slope': -10},
          input_shape=(2, 3, 4),
          supports_masking=True)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4879')" href="javascript:;">
keras-2.8.0/keras/layers/advanced_activations_test.py: 110-128
</a>
<div class="mid" id="frag4879" style="display:none"><pre>
  def test_relu_with_invalid_threshold(self):
    with self.assertRaisesRegex(
        ValueError, 'threshold of a ReLU layer cannot be a negative '
        'value. Received: None'):
      testing_utils.layer_test(
          keras.layers.ReLU,
          kwargs={'threshold': None},
          input_shape=(2, 3, 4),
          supports_masking=True)

    with self.assertRaisesRegex(
        ValueError, 'threshold of a ReLU layer cannot be a negative '
        'value. Received: -10'):
      testing_utils.layer_test(
          keras.layers.ReLU,
          kwargs={'threshold': -10},
          input_shape=(2, 3, 4),
          supports_masking=True)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4883')" href="javascript:;">
keras-2.8.0/keras/layers/advanced_activations_test.py: 161-180
</a>
<div class="mid" id="frag4883" style="display:none"><pre>
  def test_threshold_relu_with_invalid_theta(self):
    with self.assertRaisesRegex(
        ValueError, 'Theta of a Thresholded ReLU layer cannot '
        'be None, expecting a float. Received: None'):
      testing_utils.layer_test(
          keras.layers.ThresholdedReLU,
          kwargs={'theta': None},
          input_shape=(2, 3, 4),
          supports_masking=True)

    with self.assertRaisesRegex(
        ValueError, 'The theta value of a Thresholded ReLU '
        'layer should be &gt;=0. Received: -10'):
      testing_utils.layer_test(
          keras.layers.ThresholdedReLU,
          kwargs={'theta': -10},
          input_shape=(2, 3, 4),
          supports_masking=True)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 106:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4888')" href="javascript:;">
keras-2.8.0/keras/layers/merge_test.py: 130-143
</a>
<div class="mid" id="frag4888" style="display:none"><pre>
  def test_merge_maximum(self):
    i1 = keras.layers.Input(shape=(4, 5))
    i2 = keras.layers.Input(shape=(4, 5))
    o = keras.layers.maximum([i1, i2])
    self.assertListEqual(o.shape.as_list(), [None, 4, 5])
    model = keras.models.Model([i1, i2], o)
    model.run_eagerly = testing_utils.should_run_eagerly()

    x1 = np.random.random((2, 4, 5))
    x2 = np.random.random((2, 4, 5))
    out = model.predict([x1, x2])
    self.assertEqual(out.shape, (2, 4, 5))
    self.assertAllClose(out, np.maximum(x1, x2), atol=1e-4)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4889')" href="javascript:;">
keras-2.8.0/keras/layers/merge_test.py: 144-157
</a>
<div class="mid" id="frag4889" style="display:none"><pre>
  def test_merge_minimum(self):
    i1 = keras.layers.Input(shape=(4, 5))
    i2 = keras.layers.Input(shape=(4, 5))
    o = keras.layers.minimum([i1, i2])
    self.assertListEqual(o.shape.as_list(), [None, 4, 5])
    model = keras.models.Model([i1, i2], o)
    model.run_eagerly = testing_utils.should_run_eagerly()

    x1 = np.random.random((2, 4, 5))
    x2 = np.random.random((2, 4, 5))
    out = model.predict([x1, x2])
    self.assertEqual(out.shape, (2, 4, 5))
    self.assertAllClose(out, np.minimum(x1, x2), atol=1e-4)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 107:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4924')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/hashing.py: 259-268
</a>
<div class="mid" id="frag4924" style="display:none"><pre>
  def get_config(self):
    config = super().get_config()
    config.update({
        'num_bins': self.num_bins,
        'salt': self.salt,
        'mask_value': self.mask_value,
        'output_mode': self.output_mode,
        'sparse': self.sparse,
    })
    return config
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5178')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/discretization.py: 353-363
</a>
<div class="mid" id="frag5178" style="display:none"><pre>
  def get_config(self):
    config = super().get_config()
    config.update({
        "bin_boundaries": self.input_bin_boundaries,
        "num_bins": self.num_bins,
        "epsilon": self.epsilon,
        "output_mode": self.output_mode,
        "sparse": self.sparse,
    })
    return config

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 108:</b> &nbsp; 2 fragments, nominal size 21 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4970')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/integer_lookup_test.py: 76-111
</a>
<div class="mid" id="frag4970" style="display:none"><pre>
  def test_layer_end_to_end_with_adapt(self, vocab_data, input_data, kwargs,
                                       use_dataset, expected_output,
                                       input_dtype):
    cls = integer_lookup.IntegerLookup
    expected_output_dtype = tf.int64
    input_shape = input_data.shape

    if use_dataset:
      # Keras APIs expect batched datasets.
      # TODO(rachelim): `model.predict` predicts the result on each
      # dataset batch separately, then tries to concatenate the results
      # together. When the results have different shapes on the non-concat
      # axis (which can happen in the output_mode = INT case for
      # IntegerLookup), the concatenation fails. In real use cases, this may
      # not be an issue because users are likely to pipe the preprocessing layer
      # into other keras layers instead of predicting it directly. A workaround
      # for these unit tests is to have the dataset only contain one batch, so
      # no concatenation needs to happen with the result. For consistency with
      # numpy input, we should make `predict` join differently shaped results
      # together sensibly, with 0 padding.
      input_data = tf.data.Dataset.from_tensor_slices(input_data).batch(
          input_shape[0])
      vocab_data = tf.data.Dataset.from_tensor_slices(vocab_data).batch(
          input_shape[0])

    output_data = testing_utils.layer_test(
        cls,
        kwargs=kwargs,
        input_shape=input_shape,
        input_data=input_data,
        input_dtype=input_dtype,
        expected_output_dtype=expected_output_dtype,
        validate_training=False,
        adapt_data=vocab_data)
    self.assertAllClose(expected_output, output_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5191')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/string_lookup_test.py: 70-106
</a>
<div class="mid" id="frag5191" style="display:none"><pre>
  def test_layer_end_to_end_with_adapt(self, vocab_data, input_data, kwargs,
                                       use_dataset, expected_output,
                                       input_dtype):
    cls = string_lookup.StringLookup
    expected_output_dtype = tf.int64
    input_shape = input_data.shape

    if use_dataset:
      # Keras APIs expect batched datasets.
      # TODO(rachelim): `model.predict` predicts the result on each
      # dataset batch separately, then tries to concatenate the results
      # together. When the results have different shapes on the non-concat
      # axis (which can happen in the output_mode = INT case for
      # StringLookup), the concatenation fails. In real use cases, this may
      # not be an issue because users are likely to pipe the preprocessing layer
      # into other keras layers instead of predicting it directly. A workaround
      # for these unit tests is to have the dataset only contain one batch, so
      # no concatenation needs to happen with the result. For consistency with
      # numpy input, we should make `predict` join differently shaped results
      # together sensibly, with 0 padding.
      input_data = tf.data.Dataset.from_tensor_slices(input_data).batch(
          input_shape[0])
      vocab_data = tf.data.Dataset.from_tensor_slices(vocab_data).batch(
          input_shape[0])

    output_data = testing_utils.layer_test(
        cls,
        kwargs=kwargs,
        input_shape=input_shape,
        input_data=input_data,
        input_dtype=input_dtype,
        expected_output_dtype=expected_output_dtype,
        validate_training=False,
        adapt_data=vocab_data)
    self.assertAllClose(expected_output, output_data)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 109:</b> &nbsp; 4 fragments, nominal size 23 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4974')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/integer_lookup_test.py: 167-192
</a>
<div class="mid" id="frag4974" style="display:none"><pre>
  def test_sparse_int_input_multi_bucket(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.SparseTensor(
        indices=[[0, 0], [1, 2]],
        values=np.array([13, 133], dtype=np.int64),
        dense_shape=[3, 4])

    expected_indices = [[0, 0], [1, 2]]
    expected_values = [6, 2]
    expected_dense_shape = [3, 4]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, sparse=True)
    layer = integer_lookup.IntegerLookup(
        max_tokens=None,
        dtype=tf.int64,
        num_oov_indices=2,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array, steps=1)
    self.assertAllEqual(expected_indices, output_data.indices)
    self.assertAllEqual(expected_values, output_data.values)
    self.assertAllEqual(expected_dense_shape, output_data.dense_shape)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5515')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py: 380-405
</a>
<div class="mid" id="frag5515" style="display:none"><pre>
  def test_sparse_int_input(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.SparseTensor(
        indices=[[0, 0], [1, 2]],
        values=np.array([13, 32], dtype=np.int64),
        dense_shape=[3, 4])

    expected_indices = [[0, 0], [1, 2]]
    expected_values = [5, 1]
    expected_dense_shape = [3, 4]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, sparse=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        vocabulary_dtype=tf.int64,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array, steps=1)
    self.assertAllEqual(expected_indices, output_data.indices)
    self.assertAllEqual(expected_values, output_data.values)
    self.assertAllEqual(expected_dense_shape, output_data.dense_shape)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5525')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py: 595-620
</a>
<div class="mid" id="frag5525" style="display:none"><pre>
  def test_sparse_int_input(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.SparseTensor(
        indices=[[0, 0], [1, 2]],
        values=np.array([13, 32], dtype=np.int64),
        dense_shape=[3, 4])

    expected_indices = [[0, 0], [1, 2]]
    expected_values = [5, 1]
    expected_dense_shape = [3, 4]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, sparse=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        vocabulary_dtype=tf.int64,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array, steps=1)
    self.assertAllEqual(expected_indices, output_data.indices)
    self.assertAllEqual(expected_values, output_data.values)
    self.assertAllEqual(expected_dense_shape, output_data.dense_shape)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5520')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py: 493-518
</a>
<div class="mid" id="frag5520" style="display:none"><pre>
  def test_sparse_int_input_multi_bucket(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.SparseTensor(
        indices=[[0, 0], [1, 2]],
        values=np.array([13, 133], dtype=np.int64),
        dense_shape=[3, 4])

    expected_indices = [[0, 0], [1, 2]]
    expected_values = [6, 2]
    expected_dense_shape = [3, 4]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, sparse=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        vocabulary_dtype=tf.int64,
        num_oov_indices=2,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array, steps=1)
    self.assertAllEqual(expected_indices, output_data.indices)
    self.assertAllEqual(expected_values, output_data.values)
    self.assertAllEqual(expected_dense_shape, output_data.dense_shape)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 110:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4993')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/integer_lookup_test.py: 429-441
</a>
<div class="mid" id="frag4993" style="display:none"><pre>
  def test_multi_hot_output(self):
    vocab_data = [2, 3, 4, 5]
    input_array = np.array([[2, 2, 3, 4], [0, 1, 5, 2]])
    expected_output = [[0, 1, 1, 1, 0], [1, 1, 0, 0, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = integer_lookup.IntegerLookup(
        vocabulary=vocab_data, output_mode="multi_hot")
    res = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=res)
    output_data = model.predict(input_array)
    self.assertAllEqual(expected_output, output_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4994')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/integer_lookup_test.py: 442-454
</a>
<div class="mid" id="frag4994" style="display:none"><pre>
  def test_count_output(self):
    vocab_data = [2, 3, 4, 5]
    input_array = np.array([[2, 2, 3, 4], [0, 1, 5, 6]])
    expected_output = [[0, 2, 1, 1, 0], [3, 0, 0, 0, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = integer_lookup.IntegerLookup(
        vocabulary=vocab_data, output_mode="count")
    res = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=res)
    output_data = model.predict(input_array)
    self.assertAllEqual(expected_output, output_data)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 111:</b> &nbsp; 3 fragments, nominal size 16 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5030')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/text_vectorization_test.py: 512-529
</a>
<div class="mid" id="frag5030" style="display:none"><pre>
  def test_lower_and_strip_punctuation(self, data_fn):
    input_array = data_fn([["Earth", "wInD", "aNd", "firE"],
                           ["fire|", "an&lt;&gt;d", "{earth}", "michigan@%$"]])
    expected_output = data_fn([[b"earth", b"wind", b"and", b"fire"],
                               [b"fire", b"and", b"earth", b"michigan"]])

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=text_vectorization.LOWER_AND_STRIP_PUNCTUATION,
        split=None,
        ngrams=None,
        output_mode=None)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5031')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/text_vectorization_test.py: 531-548
</a>
<div class="mid" id="frag5031" style="display:none"><pre>
  def test_strip_punctuation(self, data_fn):
    input_array = data_fn([["Earth", "wInD", "aNd", "firE"],
                           ["fire|", "an&lt;&gt;d", "{earth}", "michigan@%$"]])
    expected_output = data_fn([[b"Earth", b"wInD", b"aNd", b"firE"],
                               [b"fire", b"and", b"earth", b"michigan"]])

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=text_vectorization.STRIP_PUNCTUATION,
        split=None,
        ngrams=None,
        output_mode=None)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5032')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/text_vectorization_test.py: 550-567
</a>
<div class="mid" id="frag5032" style="display:none"><pre>
  def test_lower(self, data_fn):
    input_array = data_fn([["Earth", "wInD", "aNd", "firE"],
                           ["fire|", "an&lt;&gt;d", "{earth}", "michigan@$"]])
    expected_output = data_fn([[b"earth", b"wind", b"and", b"fire"],
                               [b"fire|", b"an&lt;&gt;d", b"{earth}", b"michigan@$"]])

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=text_vectorization.LOWER,
        split=None,
        ngrams=None,
        output_mode=None)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 112:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5047')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/text_vectorization_test.py: 820-839
</a>
<div class="mid" id="frag5047" style="display:none"><pre>
  def test_vocab_setting_via_setter(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    vocab_path = self._write_to_temp_file("vocab_file", vocab_data)
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=None,
        output_mode=text_vectorization.INT)
    layer.set_vocabulary(vocab_path)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)

    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5048')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/text_vectorization_test.py: 840-860
</a>
<div class="mid" id="frag5048" style="display:none"><pre>
  def test_vocab_setting_with_oov_via_setter(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    vocab_path = self._write_to_temp_file("vocab_file", vocab_data)
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=None,
        output_mode=text_vectorization.INT)
    layer.set_vocabulary(vocab_path)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)

    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 113:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5111')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/discretization_test.py: 141-156
</a>
<div class="mid" id="frag5111" style="display:none"><pre>
  def test_multi_hot_output(self):
    input_data = np.array([-1.5, 1.0, 3.4, 3.5])

    expected_output = [1., 0., 1., 1.]
    expected_output_shape = [None, 4]

    inputs = keras.Input(shape=(4,))
    layer = discretization.Discretization(bin_boundaries=[0., 1., 2.],
                                          output_mode="multi_hot")
    outputs = layer(inputs)
    self.assertAllEqual(expected_output_shape, outputs.shape.as_list())

    model = keras.Model(inputs, outputs)
    output_data = model(input_data)
    self.assertAllEqual(expected_output, output_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5112')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/discretization_test.py: 157-172
</a>
<div class="mid" id="frag5112" style="display:none"><pre>
  def test_count_output(self):
    input_data = np.array([-1.5, 1.0, 3.4, 3.5])

    expected_output = [1., 0., 1., 2.]
    expected_output_shape = [None, 4]

    inputs = keras.Input(shape=(4,))
    layer = discretization.Discretization(bin_boundaries=[0., 1., 2.],
                                          output_mode="count")
    outputs = layer(inputs)
    self.assertAllEqual(expected_output_shape, outputs.shape.as_list())

    model = keras.Model(inputs, outputs)
    output_data = model(input_data)
    self.assertAllEqual(expected_output, output_data)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 114:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5198')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/string_lookup_test.py: 191-204
</a>
<div class="mid" id="frag5198" style="display:none"><pre>
  def test_multi_hot_output(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[0, 1, 1, 1, 1], [1, 1, 0, 1, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = string_lookup.StringLookup(
        vocabulary=vocab_data, output_mode="multi_hot")
    res = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=res)
    output_data = model.predict(input_array)
    self.assertAllEqual(expected_output, output_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5199')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/string_lookup_test.py: 205-218
</a>
<div class="mid" id="frag5199" style="display:none"><pre>
  def test_count_output(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "earth", "fire", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[0, 2, 0, 0, 2], [1, 1, 0, 1, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = string_lookup.StringLookup(
        vocabulary=vocab_data, output_mode="count")
    res = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=res)
    output_data = model.predict(input_array)
    self.assertAllEqual(expected_output, output_data)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 115:</b> &nbsp; 2 fragments, nominal size 25 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5287')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/benchmarks/normalization_adapt_benchmark.py: 81-113
</a>
<div class="mid" id="frag5287" style="display:none"><pre>
  def bm_adapt_implementation(self, num_elements, batch_size):
    """Test the KPL adapt implementation."""
    input_t = keras.Input(shape=(1,), dtype=tf.float32)
    layer = normalization.Normalization()
    _ = layer(input_t)

    num_repeats = 5
    starts = []
    ends = []
    for _ in range(num_repeats):
      ds = tf.data.Dataset.range(num_elements)
      ds = ds.map(
          lambda x: tf.expand_dims(tf.cast(x, tf.float32), -1))
      ds = ds.batch(batch_size)

      starts.append(time.time())
      # Benchmarked code begins here.
      layer.adapt(ds)
      # Benchmarked code ends here.
      ends.append(time.time())

    avg_time = np.mean(np.array(ends) - np.array(starts))
    name = "normalization_adapt|%s_elements|batch_%s" % (num_elements,
                                                         batch_size)
    baseline = self.run_dataset_implementation(num_elements, batch_size)
    extras = {
        "tf.data implementation baseline": baseline,
        "delta seconds": (baseline - avg_time),
        "delta percent": ((baseline - avg_time) / baseline) * 100
    }
    self.report_benchmark(
        iters=num_repeats, wall_time=avg_time, extras=extras, name=name)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5330')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/benchmarks/discretization_adapt_benchmark.py: 70-102
</a>
<div class="mid" id="frag5330" style="display:none"><pre>
  def bm_adapt_implementation(self, num_elements, batch_size):
    """Test the KPL adapt implementation."""
    input_t = keras.Input(shape=(1,), dtype=tf.float32)
    layer = discretization.Discretization()
    _ = layer(input_t)

    num_repeats = 5
    starts = []
    ends = []
    for _ in range(num_repeats):
      ds = tf.data.Dataset.range(num_elements)
      ds = ds.map(
          lambda x: tf.expand_dims(tf.cast(x, tf.float32), -1))
      ds = ds.batch(batch_size)

      starts.append(time.time())
      # Benchmarked code begins here.
      layer.adapt(ds)
      # Benchmarked code ends here.
      ends.append(time.time())

    avg_time = np.mean(np.array(ends) - np.array(starts))
    name = "discretization_adapt|%s_elements|batch_%s" % (num_elements,
                                                          batch_size)
    baseline = self.run_dataset_implementation(num_elements, batch_size)
    extras = {
        "tf.data implementation baseline": baseline,
        "delta seconds": (baseline - avg_time),
        "delta percent": ((baseline - avg_time) / baseline) * 100
    }
    self.report_benchmark(
        iters=num_repeats, wall_time=avg_time, extras=extras, name=name)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 116:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5359')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/hashing_test.py: 291-305
</a>
<div class="mid" id="frag5359" style="display:none"><pre>
  def test_multi_hot_output(self):
    input_array = np.array([0, 1, 2, 3, 4])

    expected_output = [1., 1., 1.]
    expected_output_shape = [None, 3]

    inputs = keras.Input(shape=(3,), dtype='int32')
    layer = hashing.Hashing(num_bins=3, output_mode='multi_hot')
    outputs = layer(inputs)
    self.assertAllEqual(expected_output_shape, outputs.shape.as_list())

    model = keras.Model(inputs, outputs)
    output_data = model(input_array)
    self.assertAllEqual(expected_output, output_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5360')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/hashing_test.py: 306-320
</a>
<div class="mid" id="frag5360" style="display:none"><pre>
  def test_count_output(self):
    input_array = np.array([0, 1, 2, 3, 4])

    expected_output = [2., 2., 1.]
    expected_output_shape = [None, 3]

    inputs = keras.Input(shape=(3,), dtype='int32')
    layer = hashing.Hashing(num_bins=3, output_mode='count')
    outputs = layer(inputs)
    self.assertAllEqual(expected_output_shape, outputs.shape.as_list())

    model = keras.Model(inputs, outputs)
    output_data = model(input_array)
    self.assertAllEqual(expected_output, output_data)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 117:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5410')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/image_preprocessing_test.py: 254-266
</a>
<div class="mid" id="frag5410" style="display:none"><pre>
  def test_input_smaller_than_crop_box(self):
    np.random.seed(1337)
    height, width = 10, 8
    inp = np.random.random((12, 3, 3, 3))
    with testing_utils.use_gpu():
      layer = image_preprocessing.CenterCrop(height, width)
      actual_output = layer(inp)
      # In this case, output should equal resizing with crop_to_aspect ratio.
      resize_layer = image_preprocessing.Resizing(
          height, width, crop_to_aspect_ratio=True)
      expected_output = resize_layer(inp)
      self.assertAllEqual(expected_output, actual_output)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5415')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/image_preprocessing_test.py: 312-324
</a>
<div class="mid" id="frag5415" style="display:none"><pre>
  def test_input_smaller_than_crop_box(self):
    np.random.seed(1337)
    height, width = 10, 8
    inp = np.random.random((12, 3, 3, 3))
    with testing_utils.use_gpu():
      layer = image_preprocessing.RandomCrop(height, width)
      actual_output = layer(inp)
      # In this case, output should equal resizing with crop_to_aspect ratio.
      resize_layer = image_preprocessing.Resizing(
          height, width, crop_to_aspect_ratio=True)
      expected_output = resize_layer(inp)
      self.assertAllEqual(expected_output, actual_output)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 118:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5451')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/image_preprocessing_test.py: 675-688
</a>
<div class="mid" id="frag5451" style="display:none"><pre>
  def _run_test(self, height_factor, width_factor):
    np.random.seed(1337)
    num_samples = 2
    orig_height = 5
    orig_width = 8
    channels = 3
    kwargs = {'height_factor': height_factor, 'width_factor': width_factor}
    with testing_utils.use_gpu():
      testing_utils.layer_test(
          image_preprocessing.RandomTranslation,
          kwargs=kwargs,
          input_shape=(num_samples, orig_height, orig_width, channels),
          expected_output_shape=(None, orig_height, orig_width, channels))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5478')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/image_preprocessing_test.py: 1308-1321
</a>
<div class="mid" id="frag5478" style="display:none"><pre>
  def _run_test(self, height_factor, width_factor):
    np.random.seed(1337)
    num_samples = 2
    orig_height = 5
    orig_width = 8
    channels = 3
    kwargs = {'height_factor': height_factor, 'width_factor': width_factor}
    with testing_utils.use_gpu():
      testing_utils.layer_test(
          image_preprocessing.RandomZoom,
          kwargs=kwargs,
          input_shape=(num_samples, orig_height, orig_width, channels),
          expected_output_shape=(None, orig_height, orig_width, channels))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 119:</b> &nbsp; 4 fragments, nominal size 37 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5465')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/image_preprocessing_test.py: 886-941
</a>
<div class="mid" id="frag5465" style="display:none"><pre>
  def test_random_translation_reflect(self):
    # reflected output is (dcba|abcd|dcba)

    # Test down shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[0., 1., 2.],
         [0., 1., 2.],
         [3., 4., 5.],
         [6., 7., 8],
         [9., 10., 11]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., 0., 0., 1., -1., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'reflect')

    # Test up shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[3., 4., 5.],
         [6., 7., 8],
         [9., 10., 11.],
         [12., 13., 14.],
         [12., 13., 14.]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., 0., 0., 1., 1., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'reflect')

    # Test left shift by 1.
    # reflected output is (dcba|abcd|dcba)
    # pyformat: disable
    expected_output = np.asarray(
        [[1., 2., 2.],
         [4., 5., 5.],
         [7., 8., 8.],
         [10., 11., 11.],
         [13., 14., 14.]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., 1., 0., 1., 0., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'reflect')

    # Test right shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[0., 0., 1.],
         [3., 3., 4],
         [6., 6., 7.],
         [9., 9., 10.],
         [12., 12., 13.]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., -1., 0., 1., 0., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'reflect')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5468')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/image_preprocessing_test.py: 1052-1106
</a>
<div class="mid" id="frag5468" style="display:none"><pre>
  def test_random_translation_constant_0(self):
    # constant output is (0000|abcd|0000)

    # Test down shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[0., 0., 0.],
         [0., 1., 2.],
         [3., 4., 5.],
         [6., 7., 8],
         [9., 10., 11]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., 0., 0., 1., -1., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'constant')

    # Test up shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[3., 4., 5.],
         [6., 7., 8],
         [9., 10., 11.],
         [12., 13., 14.],
         [0., 0., 0.]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., 0., 0., 1., 1., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'constant')

    # Test left shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[1., 2., 0.],
         [4., 5., 0.],
         [7., 8., 0.],
         [10., 11., 0.],
         [13., 14., 0.]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., 1., 0., 1., 0., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'constant')

    # Test right shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[0., 0., 1.],
         [0., 3., 4],
         [0., 6., 7.],
         [0., 9., 10.],
         [0., 12., 13.]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., -1., 0., 1., 0., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'constant')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5467')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/image_preprocessing_test.py: 997-1051
</a>
<div class="mid" id="frag5467" style="display:none"><pre>
  def test_random_translation_nearest(self):
    # nearest output is (aaaa|abcd|dddd)

    # Test down shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[0., 1., 2.],
         [0., 1., 2.],
         [3., 4., 5.],
         [6., 7., 8],
         [9., 10., 11]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., 0., 0., 1., -1., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'nearest')

    # Test up shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[3., 4., 5.],
         [6., 7., 8],
         [9., 10., 11.],
         [12., 13., 14.],
         [12., 13., 14.]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., 0., 0., 1., 1., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'nearest')

    # Test left shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[1., 2., 2.],
         [4., 5., 5.],
         [7., 8., 8.],
         [10., 11., 11.],
         [13., 14., 14.]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., 1., 0., 1., 0., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'nearest')

    # Test right shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[0., 0., 1.],
         [3., 3., 4],
         [6., 6., 7.],
         [9., 9., 10.],
         [12., 12., 13.]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., -1., 0., 1., 0., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'nearest')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5466')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/image_preprocessing_test.py: 942-996
</a>
<div class="mid" id="frag5466" style="display:none"><pre>
  def test_random_translation_wrap(self):
    # warpped output is (abcd|abcd|abcd)

    # Test down shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[12., 13., 14.],
         [0., 1., 2.],
         [3., 4., 5.],
         [6., 7., 8],
         [9., 10., 11]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., 0., 0., 1., -1., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'wrap')

    # Test up shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[3., 4., 5.],
         [6., 7., 8],
         [9., 10., 11.],
         [12., 13., 14.],
         [0., 1., 2.]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., 0., 0., 1., 1., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'wrap')

    # Test left shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[1., 2., 0.],
         [4., 5., 3.],
         [7., 8., 6.],
         [10., 11., 9.],
         [13., 14., 12.]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., 1., 0., 1., 0., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'wrap')

    # Test right shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[2., 0., 1.],
         [5., 3., 4],
         [8., 6., 7.],
         [11., 9., 10.],
         [14., 12., 13.]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., -1., 0., 1., 0., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'wrap')

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 120:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5488')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/image_preprocessing_test.py: 1430-1443
</a>
<div class="mid" id="frag5488" style="display:none"><pre>
  def _run_test(self, factor):
    np.random.seed(1337)
    num_samples = 2
    orig_height = 5
    orig_width = 8
    channels = 3
    with testing_utils.use_gpu():
      img = np.random.random((num_samples, orig_height, orig_width, channels))
      layer = image_preprocessing.RandomHeight(factor)
      img_out = layer(img, training=True)
      self.assertEqual(img_out.shape[0], 2)
      self.assertEqual(img_out.shape[2], 8)
      self.assertEqual(img_out.shape[3], 3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5498')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/image_preprocessing_test.py: 1539-1552
</a>
<div class="mid" id="frag5498" style="display:none"><pre>
  def _run_test(self, factor):
    np.random.seed(1337)
    num_samples = 2
    orig_height = 5
    orig_width = 8
    channels = 3
    with testing_utils.use_gpu():
      img = np.random.random((num_samples, orig_height, orig_width, channels))
      layer = image_preprocessing.RandomWidth(factor)
      img_out = layer(img, training=True)
      self.assertEqual(img_out.shape[0], 2)
      self.assertEqual(img_out.shape[1], 5)
      self.assertEqual(img_out.shape[3], 3)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 121:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5492')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/image_preprocessing_test.py: 1481-1496
</a>
<div class="mid" id="frag5492" style="display:none"><pre>
  def test_random_height_shorter_numeric(self):
    for dtype in (np.int64, np.float32):
      with testing_utils.use_gpu():
        input_image = np.reshape(np.arange(0, 8), (4, 2, 1)).astype(dtype)
        layer = image_preprocessing.RandomHeight(
            factor=(-.5, -.5), interpolation='nearest')
        output_image = layer(np.expand_dims(input_image, axis=0))
        # pyformat: disable
        expected_output = np.asarray([
            [2, 3],
            [6, 7]
        ]).astype(dtype)
        # pyformat: enable
        expected_output = np.reshape(expected_output, (1, 2, 2, 1))
        self.assertAllEqual(expected_output, output_image)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5502')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/image_preprocessing_test.py: 1589-1604
</a>
<div class="mid" id="frag5502" style="display:none"><pre>
  def test_random_width_shorter_numeric(self):
    for dtype in (np.int64, np.float32):
      with testing_utils.use_gpu():
        input_image = np.reshape(np.arange(0, 8), (2, 4, 1)).astype(dtype)
        layer = image_preprocessing.RandomWidth(
            factor=(-.5, -.5), interpolation='nearest')
        output_image = layer(np.expand_dims(input_image, axis=0))
        # pyformat: disable
        expected_output = np.asarray([
            [1, 3],
            [5, 7]
        ]).astype(dtype)
        # pyformat: enable
        expected_output = np.reshape(expected_output, (1, 2, 2, 1))
        self.assertAllEqual(expected_output, output_image)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 122:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5516')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py: 406-424
</a>
<div class="mid" id="frag5516" style="display:none"><pre>
  def test_ragged_string_input(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = tf.ragged.constant(
        [["earth", "wind", "fire"], ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5526')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py: 621-639
</a>
<div class="mid" id="frag5526" style="display:none"><pre>
  def test_ragged_string_input(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = tf.ragged.constant(
        [["earth", "wind", "fire"], ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 123:</b> &nbsp; 4 fragments, nominal size 17 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5517')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py: 425-443
</a>
<div class="mid" id="frag5517" style="display:none"><pre>
  def test_ragged_int_input(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.ragged.constant([[10, 11, 13], [13, 12, 10, 42]],
                                     dtype=np.int64)
    expected_output = [[2, 3, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        vocabulary_dtype=tf.int64,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5518')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py: 444-463
</a>
<div class="mid" id="frag5518" style="display:none"><pre>
  def test_int32_input_with_int64_keys(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.ragged.constant([[10, 11, 13], [13, 12, 10, 42]],
                                     dtype=np.int32)
    expected_output = [[2, 3, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.int32, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        vocabulary_dtype=tf.int64,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5522')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py: 538-557
</a>
<div class="mid" id="frag5522" style="display:none"><pre>
  def test_ragged_int_input_multi_bucket(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.ragged.constant([[10, 11, 13], [13, 12, 10, 133]],
                                     dtype=np.int64)
    expected_output = [[3, 4, 6], [6, 5, 3, 2]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        vocabulary_dtype=tf.int64,
        num_oov_indices=2,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5527')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py: 640-658
</a>
<div class="mid" id="frag5527" style="display:none"><pre>
  def test_ragged_int_input(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.ragged.constant([[10, 11, 13], [13, 12, 10, 42]],
                                     dtype=np.int64)
    expected_output = [[2, 3, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        vocabulary_dtype=tf.int64,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 124:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5539')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py: 860-878
</a>
<div class="mid" id="frag5539" style="display:none"><pre>
  def test_int_output_explicit_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5560')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py: 1365-1383
</a>
<div class="mid" id="frag5560" style="display:none"><pre>
  def test_int_output_explicit_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 125:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5570')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py: 1502-1512
</a>
<div class="mid" id="frag5570" style="display:none"><pre>
  def test_vocab_with_repeated_element_fails(self):
    vocab_data = ["earth", "earth", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    with self.assertRaisesRegex(ValueError, "repeated term.*earth"):
      layer.set_vocabulary(vocab_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5572')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py: 1525-1535
</a>
<div class="mid" id="frag5572" style="display:none"><pre>
  def test_vocab_with_reserved_mask_element_fails(self):
    vocab_data = ["earth", "mask_token", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="mask_token",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    with self.assertRaisesRegex(ValueError, "reserved mask"):
      layer.set_vocabulary(vocab_data)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 126:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5586')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py: 1717-1728
</a>
<div class="mid" id="frag5586" style="display:none"><pre>
  def test_vocab_with_repeated_element_fails(self):
    vocab_data = ["earth", "earth", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string,
        invert=True)
    with self.assertRaisesRegex(ValueError, "repeated term.*earth"):
      layer.set_vocabulary(vocab_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5587')" href="javascript:;">
keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py: 1729-1740
</a>
<div class="mid" id="frag5587" style="display:none"><pre>
  def test_vocab_with_reserved_mask_element_fails(self):
    vocab_data = ["earth", "mask_token", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="mask_token",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string,
        invert=True)
    with self.assertRaisesRegex(ValueError, "reserved mask"):
      layer.set_vocabulary(vocab_data)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<script language="JavaScript">
function ShowHide(divId) { 
    if(document.getElementById(divId).style.display == 'none') {
        document.getElementById(divId).style.display='block';
    } else { 
        document.getElementById(divId).style.display = 'none';
    } 
}
</script>
</body>
</html>

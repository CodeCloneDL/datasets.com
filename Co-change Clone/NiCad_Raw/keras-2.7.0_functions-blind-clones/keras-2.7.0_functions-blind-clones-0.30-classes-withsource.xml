<clones>
<systeminfo processor="nicad6" system="keras-2.7.0" granularity="functions-blind" threshold="30%" minlines="10" maxlines="2500"/>
<cloneinfo npcs="5503" npairs="2025"/>
<runinfo ncompares="1255002" cputime="678629"/>
<classinfo nclasses="307"/>

<class classid="1" nclones="7" nlines="15" similarity="73">
<source file="systems/keras-2.7.0/keras/optimizer_v2/adam.py" startline="97" endline="112" pcid="4">
  def __init__(self,
               learning_rate=0.001,
               beta_1=0.9,
               beta_2=0.999,
               epsilon=1e-7,
               amsgrad=False,
               name='Adam',
               **kwargs):
    super(Adam, self).__init__(name, **kwargs)
    self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))
    self._set_hyper('decay', self._initial_decay)
    self._set_hyper('beta_1', beta_1)
    self._set_hyper('beta_2', beta_2)
    self.epsilon = epsilon or backend_config.epsilon()
    self.amsgrad = amsgrad

</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/adamax.py" startline="83" endline="96" pcid="215">
  def __init__(self,
               learning_rate=0.001,
               beta_1=0.9,
               beta_2=0.999,
               epsilon=1e-7,
               name='Adamax',
               **kwargs):
    super(Adamax, self).__init__(name, **kwargs)
    self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))
    self._set_hyper('decay', self._initial_decay)
    self._set_hyper('beta_1', beta_1)
    self._set_hyper('beta_2', beta_2)
    self.epsilon = epsilon or backend_config.epsilon()

</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/adam.py" startline="321" endline="364" pcid="11">
  def __init__(self,
               learning_rate=0.001,
               beta_1=0.9,
               beta_2=0.999,
               epsilon=1e-7,
               amsgrad=False,
               name='Adam',
               **kwargs):
    """Construct a new Adam optimizer.

    Args:
      learning_rate: A `Tensor`, floating point value, or a schedule that is a
        `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable that
        takes no arguments and returns the actual value to use, The learning
        rate. Defaults to 0.001.
      beta_1: A float value or a constant float tensor, or a callable that takes
        no arguments and returns the actual value to use. The exponential decay
        rate for the 1st moment estimates. Defaults to 0.9.
      beta_2: A float value or a constant float tensor, or a callable that takes
        no arguments and returns the actual value to use, The exponential decay
        rate for the 2nd moment estimates. Defaults to 0.999.
      epsilon: A small constant for numerical stability. This epsilon is
        "epsilon hat" in the Kingma and Ba paper (in the formula just before
        Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to
        1e-7.
      amsgrad: Boolean. Whether to apply AMSGrad variant of this algorithm from
        the paper "On the Convergence of Adam and beyond". Defaults to `False`.
      name: Optional name for the operations created when applying gradients.
        Defaults to "Adam".
      **kwargs: keyword arguments. Allowed to be {`clipnorm`, `clipvalue`, `lr`,
        `decay`}. `clipnorm` is clip gradients by norm; `clipvalue` is clip
        gradients by value, `decay` is included for backward compatibility to
        allow time inverse decay of learning rate. `lr` is included for backward
        compatibility, recommended to use `learning_rate` instead.
    """

    super(NonFusedAdam, self).__init__(name, **kwargs)
    self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))
    self._set_hyper('decay', self._initial_decay)
    self._set_hyper('beta_1', beta_1)
    self._set_hyper('beta_2', beta_2)
    self.epsilon = epsilon or backend_config.epsilon()
    self.amsgrad = amsgrad

</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/rmsprop.py" startline="90" endline="145" pcid="194">
  def __init__(self,
               learning_rate=0.001,
               rho=0.9,
               momentum=0.0,
               epsilon=1e-7,
               centered=False,
               name="RMSprop",
               **kwargs):
    """Construct a new RMSprop optimizer.

    Args:
      learning_rate: A `Tensor`, floating point value, or a schedule that is a
        `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable
        that takes no arguments and returns the actual value to use. The
        learning rate. Defaults to 0.001.
      rho: Discounting factor for the history/coming gradient. Defaults to 0.9.
      momentum: A scalar or a scalar `Tensor`. Defaults to 0.0.
      epsilon: A small constant for numerical stability. This epsilon is
        "epsilon hat" in the Kingma and Ba paper (in the formula just before
        Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to
        1e-7.
      centered: Boolean. If `True`, gradients are normalized by the estimated
        variance of the gradient; if False, by the uncentered second moment.
        Setting this to `True` may help with training, but is slightly more
        expensive in terms of computation and memory. Defaults to `False`.
      name: Optional name prefix for the operations created when applying
        gradients. Defaults to "RMSprop".
      **kwargs: keyword arguments. Allowed to be {`clipnorm`, `clipvalue`, `lr`,
        `decay`}. `clipnorm` is clip gradients by norm; `clipvalue` is clip
        gradients by value, `decay` is included for backward compatibility to
        allow time inverse decay of learning rate. `lr` is included for backward
        compatibility, recommended to use `learning_rate` instead.

    @compatibility(eager)
    When eager execution is enabled, `learning_rate`, `decay`, `momentum`, and
    `epsilon` can each be a callable that takes no arguments and returns the
    actual value to use. This can be useful for changing these values across
    different invocations of optimizer functions.
    @end_compatibility
    """
    super(RMSprop, self).__init__(name, **kwargs)
    self._set_hyper("learning_rate", kwargs.get("lr", learning_rate))
    self._set_hyper("decay", self._initial_decay)
    self._set_hyper("rho", rho)

    self._momentum = False
    if isinstance(momentum, tf.Tensor) or callable(momentum) or momentum > 0:
      self._momentum = True
    if isinstance(momentum, (int, float)) and (momentum < 0 or momentum > 1):
      raise ValueError(f"`momentum` must be between [0, 1]. Received: "
                       f"momentum={momentum} (of type {type(momentum)}).")
    self._set_hyper("momentum", momentum)

    self.epsilon = epsilon or backend_config.epsilon()
    self.centered = centered

</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/adadelta.py" startline="68" endline="79" pcid="260">
  def __init__(self,
               learning_rate=0.001,
               rho=0.95,
               epsilon=1e-7,
               name='Adadelta',
               **kwargs):
    super(Adadelta, self).__init__(name, **kwargs)
    self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))
    self._set_hyper('decay', self._initial_decay)
    self._set_hyper('rho', rho)
    self.epsilon = epsilon or backend_config.epsilon()

</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/nadam.py" startline="58" endline="80" pcid="68">
  def __init__(self,
               learning_rate=0.001,
               beta_1=0.9,
               beta_2=0.999,
               epsilon=1e-7,
               name='Nadam',
               **kwargs):
    # Backwards compatibility with keras NAdam optimizer.
    kwargs['decay'] = kwargs.pop('schedule_decay', 0.004)
    learning_rate = kwargs.get('lr', learning_rate)
    if isinstance(learning_rate, learning_rate_schedule.LearningRateSchedule):
      raise ValueError('The Nadam optimizer does not support '
                       'tf.keras.optimizers.LearningRateSchedules as the '
                       'learning rate.')

    super(Nadam, self).__init__(name, **kwargs)
    self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))
    self._set_hyper('decay', self._initial_decay)
    self._set_hyper('beta_1', beta_1)
    self._set_hyper('beta_2', beta_2)
    self.epsilon = epsilon or backend_config.epsilon()
    self._m_cache = None

</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/gradient_descent.py" startline="96" endline="115" pcid="208">
  def __init__(self,
               learning_rate=0.01,
               momentum=0.0,
               nesterov=False,
               name="SGD",
               **kwargs):
    super(SGD, self).__init__(name, **kwargs)
    self._set_hyper("learning_rate", kwargs.get("lr", learning_rate))
    self._set_hyper("decay", self._initial_decay)

    self._momentum = False
    if isinstance(momentum, tf.Tensor) or callable(momentum) or momentum > 0:
      self._momentum = True
    if isinstance(momentum, (int, float)) and (momentum < 0 or momentum > 1):
      raise ValueError(f"`momentum` must be between [0, 1]. Received: "
                       f"momentum={momentum} (of type {type(momentum)}).")
    self._set_hyper("momentum", momentum)

    self.nesterov = nesterov

</source>
</class>

<class classid="2" nclones="3" nlines="19" similarity="70">
<source file="systems/keras-2.7.0/keras/optimizer_v2/adam.py" startline="124" endline="145" pcid="6">
  def _prepare_local(self, var_device, var_dtype, apply_state):
    super(Adam, self)._prepare_local(var_device, var_dtype, apply_state)

    local_step = tf.cast(self.iterations + 1, var_dtype)
    beta_1_t = tf.identity(self._get_hyper('beta_1', var_dtype))
    beta_2_t = tf.identity(self._get_hyper('beta_2', var_dtype))
    beta_1_power = tf.pow(beta_1_t, local_step)
    beta_2_power = tf.pow(beta_2_t, local_step)
    lr = (apply_state[(var_device, var_dtype)]['lr_t'] *
          (tf.sqrt(1 - beta_2_power) / (1 - beta_1_power)))
    apply_state[(var_device, var_dtype)].update(
        dict(
            lr=lr,
            epsilon=tf.convert_to_tensor(
                self.epsilon, var_dtype),
            beta_1_t=beta_1_t,
            beta_1_power=beta_1_power,
            one_minus_beta_1_t=1 - beta_1_t,
            beta_2_t=beta_2_t,
            beta_2_power=beta_2_power,
            one_minus_beta_2_t=1 - beta_2_t))

</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/adamax.py" startline="104" endline="123" pcid="217">
  def _prepare_local(self, var_device, var_dtype, apply_state):
    super(Adamax, self)._prepare_local(var_device, var_dtype, apply_state)

    local_step = tf.cast(self.iterations + 1, var_dtype)
    beta_1_t = tf.identity(self._get_hyper('beta_1', var_dtype))
    beta_2_t = tf.identity(self._get_hyper('beta_2', var_dtype))
    beta_1_power = tf.pow(beta_1_t, local_step)
    lr_t = apply_state[(var_device, var_dtype)]['lr_t']

    apply_state[(var_device, var_dtype)].update(
        dict(
            neg_scaled_lr=-lr_t / (1 - beta_1_power),
            epsilon=tf.convert_to_tensor(
                self.epsilon, var_dtype),
            beta_1_t=beta_1_t,
            beta_1_power=beta_1_power,
            one_minus_beta_1_t=1 - beta_1_t,
            beta_2_t=beta_2_t,
            zero=tf.zeros((), dtype=tf.int64)))

</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/adam.py" startline="376" endline="398" pcid="13">
  def _prepare_local(self, var_device, var_dtype, apply_state):
    super(NonFusedAdam, self)._prepare_local(var_device, var_dtype, apply_state)

    local_step = tf.cast(self.iterations + 1, var_dtype)
    beta_1_t = tf.identity(self._get_hyper('beta_1', var_dtype))
    beta_2_t = tf.identity(self._get_hyper('beta_2', var_dtype))
    beta_1_power = tf.pow(beta_1_t, local_step)
    beta_2_power = tf.pow(beta_2_t, local_step)
    lr = (
        apply_state[(var_device, var_dtype)]['lr_t'] *
        (tf.sqrt(1 - beta_2_power) / (1 - beta_1_power)))
    apply_state[(var_device, var_dtype)].update(
        dict(
            lr=lr,
            epsilon=tf.convert_to_tensor(
                self.epsilon, var_dtype),
            beta_1_t=beta_1_t,
            beta_1_power=beta_1_power,
            one_minus_beta_1_t=1 - beta_1_t,
            beta_2_t=beta_2_t,
            beta_2_power=beta_2_power,
            one_minus_beta_2_t=1 - beta_2_t))

</source>
</class>

<class classid="3" nclones="3" nlines="33" similarity="70">
<source file="systems/keras-2.7.0/keras/optimizer_v2/adam.py" startline="156" endline="192" pcid="8">
  def _resource_apply_dense(self, grad, var, apply_state=None):
    var_device, var_dtype = var.device, var.dtype.base_dtype
    coefficients = ((apply_state or {}).get((var_device, var_dtype))
                    or self._fallback_apply_state(var_device, var_dtype))

    m = self.get_slot(var, 'm')
    v = self.get_slot(var, 'v')

    if not self.amsgrad:
      return tf.raw_ops.ResourceApplyAdam(
          var=var.handle,
          m=m.handle,
          v=v.handle,
          beta1_power=coefficients['beta_1_power'],
          beta2_power=coefficients['beta_2_power'],
          lr=coefficients['lr_t'],
          beta1=coefficients['beta_1_t'],
          beta2=coefficients['beta_2_t'],
          epsilon=coefficients['epsilon'],
          grad=grad,
          use_locking=self._use_locking)
    else:
      vhat = self.get_slot(var, 'vhat')
      return tf.raw_ops.ResourceApplyAdamWithAmsgrad(
          var=var.handle,
          m=m.handle,
          v=v.handle,
          vhat=vhat.handle,
          beta1_power=coefficients['beta_1_power'],
          beta2_power=coefficients['beta_2_power'],
          lr=coefficients['lr_t'],
          beta1=coefficients['beta_1_t'],
          beta2=coefficients['beta_2_t'],
          epsilon=coefficients['epsilon'],
          grad=grad,
          use_locking=self._use_locking)

</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/ftrl.py" startline="167" endline="204" pcid="270">
  def _resource_apply_dense(self, grad, var, apply_state=None):
    var_device, var_dtype = var.device, var.dtype.base_dtype
    coefficients = ((apply_state or {}).get((var_device, var_dtype))
                    or self._fallback_apply_state(var_device, var_dtype))

    # Adjust L2 regularization strength to include beta to avoid the underlying
    # TensorFlow ops needing to include it.
    adjusted_l2_regularization_strength = (
        coefficients['l2_regularization_strength'] + coefficients['beta'] /
        (2. * coefficients['lr_t']))

    accum = self.get_slot(var, 'accumulator')
    linear = self.get_slot(var, 'linear')

    if self._l2_shrinkage_regularization_strength <= 0.0:
      return tf.raw_ops.ResourceApplyFtrl(
          var=var.handle,
          accum=accum.handle,
          linear=linear.handle,
          grad=grad,
          lr=coefficients['lr_t'],
          l1=coefficients['l1_regularization_strength'],
          l2=adjusted_l2_regularization_strength,
          lr_power=coefficients['learning_rate_power'],
          use_locking=self._use_locking)
    else:
      return tf.raw_ops.ResourceApplyFtrlV2(
          var=var.handle,
          accum=accum.handle,
          linear=linear.handle,
          grad=grad,
          lr=coefficients['lr_t'],
          l1=coefficients['l1_regularization_strength'],
          l2=adjusted_l2_regularization_strength,
          l2_shrinkage=coefficients['l2_shrinkage_regularization_strength'],
          lr_power=coefficients['learning_rate_power'],
          use_locking=self._use_locking)

</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/ftrl.py" startline="205" endline="244" pcid="271">
  def _resource_apply_sparse(self, grad, var, indices, apply_state=None):
    var_device, var_dtype = var.device, var.dtype.base_dtype
    coefficients = ((apply_state or {}).get((var_device, var_dtype))
                    or self._fallback_apply_state(var_device, var_dtype))

    # Adjust L2 regularization strength to include beta to avoid the underlying
    # TensorFlow ops needing to include it.
    adjusted_l2_regularization_strength = (
        coefficients['l2_regularization_strength'] + coefficients['beta'] /
        (2. * coefficients['lr_t']))

    accum = self.get_slot(var, 'accumulator')
    linear = self.get_slot(var, 'linear')

    if self._l2_shrinkage_regularization_strength <= 0.0:
      return tf.raw_ops.ResourceSparseApplyFtrl(
          var=var.handle,
          accum=accum.handle,
          linear=linear.handle,
          grad=grad,
          indices=indices,
          lr=coefficients['lr_t'],
          l1=coefficients['l1_regularization_strength'],
          l2=adjusted_l2_regularization_strength,
          lr_power=coefficients['learning_rate_power'],
          use_locking=self._use_locking)
    else:
      return tf.raw_ops.ResourceSparseApplyFtrlV2(
          var=var.handle,
          accum=accum.handle,
          linear=linear.handle,
          grad=grad,
          indices=indices,
          lr=coefficients['lr_t'],
          l1=coefficients['l1_regularization_strength'],
          l2=adjusted_l2_regularization_strength,
          l2_shrinkage=coefficients['l2_shrinkage_regularization_strength'],
          lr_power=coefficients['learning_rate_power'],
          use_locking=self._use_locking)

</source>
</class>

<class classid="4" nclones="5" nlines="11" similarity="90">
<source file="systems/keras-2.7.0/keras/optimizer_v2/adam.py" startline="233" endline="245" pcid="10">
  def get_config(self):
    config = super(Adam, self).get_config()
    config.update({
        'learning_rate': self._serialize_hyperparameter('learning_rate'),
        'decay': self._initial_decay,
        'beta_1': self._serialize_hyperparameter('beta_1'),
        'beta_2': self._serialize_hyperparameter('beta_2'),
        'epsilon': self.epsilon,
        'amsgrad': self.amsgrad,
    })
    return config


</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/rmsprop.py" startline="282" endline="294" pcid="200">
  def get_config(self):
    config = super(RMSprop, self).get_config()
    config.update({
        "learning_rate": self._serialize_hyperparameter("learning_rate"),
        "decay": self._initial_decay,
        "rho": self._serialize_hyperparameter("rho"),
        "momentum": self._serialize_hyperparameter("momentum"),
        "epsilon": self.epsilon,
        "centered": self.centered,
    })
    return config


</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/adam.py" startline="457" endline="467" pcid="17">
  def get_config(self):
    config = super(NonFusedAdam, self).get_config()
    config.update({
        'learning_rate': self._serialize_hyperparameter('learning_rate'),
        'decay': self._initial_decay,
        'beta_1': self._serialize_hyperparameter('beta_1'),
        'beta_2': self._serialize_hyperparameter('beta_2'),
        'epsilon': self.epsilon,
        'amsgrad': self.amsgrad,
    })
    return config
</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/adamax.py" startline="170" endline="179" pcid="220">
  def get_config(self):
    config = super(Adamax, self).get_config()
    config.update({
        'learning_rate': self._serialize_hyperparameter('learning_rate'),
        'decay': self._initial_decay,
        'beta_1': self._serialize_hyperparameter('beta_1'),
        'beta_2': self._serialize_hyperparameter('beta_2'),
        'epsilon': self.epsilon,
    })
    return config
</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/nadam.py" startline="204" endline="213" pcid="74">
  def get_config(self):
    config = super(Nadam, self).get_config()
    config.update({
        'learning_rate': self._serialize_hyperparameter('learning_rate'),
        'decay': self._initial_decay,
        'beta_1': self._serialize_hyperparameter('beta_1'),
        'beta_2': self._serialize_hyperparameter('beta_2'),
        'epsilon': self.epsilon,
    })
    return config
</source>
</class>

<class classid="5" nclones="3" nlines="34" similarity="77">
<source file="systems/keras-2.7.0/keras/optimizer_v2/adagrad_test.py" startline="62" endline="104" pcid="20">
  def doTestBasic(self, use_callable_params=False):
    for dtype in _DATA_TYPES:
      var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)
      var1_np = np.array([3.0, 4.0], dtype=dtype.as_numpy_dtype)
      grads0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)
      grads1_np = np.array([0.01, 0.01], dtype=dtype.as_numpy_dtype)
      var0 = tf.Variable(var0_np)
      var1 = tf.Variable(var1_np)
      grads0 = tf.constant(grads0_np)
      grads1 = tf.constant(grads1_np)

      learning_rate = lambda: 3.0
      if not use_callable_params:
        learning_rate = learning_rate()

      ada_opt = adagrad.Adagrad(learning_rate)

      accum0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)
      accum1_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)

      if not tf.executing_eagerly():
        ada_update = ada_opt.apply_gradients(
            zip([grads0, grads1], [var0, var1]))
        self.evaluate(tf.compat.v1.global_variables_initializer())

      # Fetch params to validate initial values
      v0_val, v1_val = self.evaluate([var0, var1])
      self.assertAllClose([1.0, 2.0], v0_val)
      self.assertAllClose([3.0, 4.0], v1_val)

      # Run 3 steps of adagrad
      for _ in range(3):
        if not tf.executing_eagerly():
          self.evaluate(ada_update)
        else:
          ada_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))
        var0_np, accum0_np = adagrad_update_numpy(var0_np, accum0_np, grads0_np,
                                                  3.0)
        var1_np, accum1_np = adagrad_update_numpy(var1_np, accum1_np, grads1_np,
                                                  3.0)
        self.assertAllCloseAccordingToType(var0_np, self.evaluate(var0))
        self.assertAllCloseAccordingToType(var1_np, self.evaluate(var1))

</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/adagrad_test.py" startline="113" endline="155" pcid="23">
  def testBasicWithLearningRateDecay(self):
    for dtype in _DATA_TYPES:
      var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)
      var1_np = np.array([3.0, 4.0], dtype=dtype.as_numpy_dtype)
      grads0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)
      grads1_np = np.array([0.01, 0.01], dtype=dtype.as_numpy_dtype)
      var0 = tf.Variable(var0_np)
      var1 = tf.Variable(var1_np)
      grads0 = tf.constant(grads0_np)
      grads1 = tf.constant(grads1_np)

      learning_rate = 3.0
      decay = 0.5

      ada_opt = adagrad.Adagrad(learning_rate, decay=decay)

      accum0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)
      accum1_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)

      if not tf.executing_eagerly():
        ada_update = ada_opt.apply_gradients(
            zip([grads0, grads1], [var0, var1]))
        self.evaluate(tf.compat.v1.global_variables_initializer())

      # Fetch params to validate initial values
      v0_val, v1_val = self.evaluate([var0, var1])
      self.assertAllClose([1.0, 2.0], v0_val)
      self.assertAllClose([3.0, 4.0], v1_val)

      # Run 3 steps of adagrad
      for t in range(3):
        if not tf.executing_eagerly():
          self.evaluate(ada_update)
        else:
          ada_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))
        lr_np = learning_rate / (1 + decay * t)
        var0_np, accum0_np = adagrad_update_numpy(var0_np, accum0_np, grads0_np,
                                                  lr_np)
        var1_np, accum1_np = adagrad_update_numpy(var1_np, accum1_np, grads1_np,
                                                  lr_np)
        self.assertAllCloseAccordingToType(var0_np, self.evaluate(var0))
        self.assertAllCloseAccordingToType(var1_np, self.evaluate(var1))

</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/adagrad_test.py" startline="195" endline="239" pcid="25">
  def testBasicWithLearningRateInverseTimeDecay(self):
    for dtype in _DATA_TYPES:
      var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)
      var1_np = np.array([3.0, 4.0], dtype=dtype.as_numpy_dtype)
      grads0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)
      grads1_np = np.array([0.01, 0.01], dtype=dtype.as_numpy_dtype)
      var0 = tf.Variable(var0_np)
      var1 = tf.Variable(var1_np)
      grads0 = tf.constant(grads0_np)
      grads1 = tf.constant(grads1_np)

      learning_rate = 3.0
      decay = 0.5
      lr_schedule = learning_rate_schedule.InverseTimeDecay(
          learning_rate, decay_steps=1.0, decay_rate=decay)

      ada_opt = adagrad.Adagrad(lr_schedule)

      accum0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)
      accum1_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)

      if not tf.executing_eagerly():
        ada_update = ada_opt.apply_gradients(
            zip([grads0, grads1], [var0, var1]))
        self.evaluate(tf.compat.v1.global_variables_initializer())

      # Fetch params to validate initial values
      v0_val, v1_val = self.evaluate([var0, var1])
      self.assertAllClose([1.0, 2.0], v0_val)
      self.assertAllClose([3.0, 4.0], v1_val)

      # Run 3 steps of adagrad
      for t in range(3):
        if not tf.executing_eagerly():
          self.evaluate(ada_update)
        else:
          ada_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))
        lr_np = learning_rate / (1 + decay * t)
        var0_np, accum0_np = adagrad_update_numpy(var0_np, accum0_np, grads0_np,
                                                  lr_np)
        var1_np, accum1_np = adagrad_update_numpy(var1_np, accum1_np, grads1_np,
                                                  lr_np)
        self.assertAllCloseAccordingToType(var0_np, self.evaluate(var0))
        self.assertAllCloseAccordingToType(var1_np, self.evaluate(var1))

</source>
</class>

<class classid="6" nclones="2" nlines="32" similarity="72">
<source file="systems/keras-2.7.0/keras/optimizer_v2/adagrad_test.py" startline="263" endline="295" pcid="28">
  def testTensorLearningRate(self):
    # TODO(tanzheny, omalleyt): Fix test in eager mode.
    with tf.Graph().as_default():
      for dtype in _DATA_TYPES:
        var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)
        var1_np = np.array([3.0, 4.0], dtype=dtype.as_numpy_dtype)
        grads0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)
        grads1_np = np.array([0.01, 0.01], dtype=dtype.as_numpy_dtype)
        var0 = tf.Variable(var0_np)
        var1 = tf.Variable(var1_np)
        grads0 = tf.constant(grads0_np)
        grads1 = tf.constant(grads1_np)

        learning_rate = tf.constant(3.0)
        ada_opt = adagrad.Adagrad(learning_rate)
        ada_update = ada_opt.apply_gradients(zip([grads0, grads1],
                                                 [var0, var1]))
        self.evaluate(tf.compat.v1.global_variables_initializer())
        # Fetch params to validate initial values
        self.assertAllClose([1.0, 2.0], self.evaluate(var0))
        self.assertAllClose([3.0, 4.0], self.evaluate(var1))
        accum0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)
        accum1_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)
        # Run 3 steps of adagrad
        for _ in range(3):
          self.evaluate(ada_update)
          var0_np, accum0_np = adagrad_update_numpy(
              var0_np, accum0_np, grads0_np, learning_rate)
          var1_np, accum1_np = adagrad_update_numpy(
              var1_np, accum1_np, grads1_np, learning_rate)
          self.assertAllCloseAccordingToType(var0_np, self.evaluate(var0))
          self.assertAllCloseAccordingToType(var1_np, self.evaluate(var1))

</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/adagrad_test.py" startline="464" endline="509" pcid="34">
  def testSharing(self):
    # TODO(tanzheny, omalleyt): Fix test in eager mode.
    with tf.Graph().as_default():
      for dtype in _DATA_TYPES:
        var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)
        grads0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)
        var1_np = np.array([3.0, 4.0], dtype=dtype.as_numpy_dtype)
        grads1_np = np.array([0.01, 0.01], dtype=dtype.as_numpy_dtype)

        var0 = tf.Variable(var0_np)
        var1 = tf.Variable(var1_np)
        grads0 = tf.constant(grads0_np)
        grads1 = tf.constant(grads1_np)

        learning_rate = 3.0
        ada_opt = adagrad.Adagrad(learning_rate)
        # Apply the optimizer twice.  Both applications will use
        # the same accums.
        ada_update1 = ada_opt.apply_gradients(zip([grads0, grads1],
                                                  [var0, var1]))
        ada_update2 = ada_opt.apply_gradients(zip([grads0, grads1],
                                                  [var0, var1]))
        slot0 = ada_opt.get_slot(var0, "accumulator")
        self.assertEqual(slot0.shape, var0.shape)
        slot1 = ada_opt.get_slot(var1, "accumulator")
        self.assertEqual(slot1.shape, var1.shape)
        self.evaluate(tf.compat.v1.global_variables_initializer())

        # Fetch params to validate initial values.
        self.assertAllClose([1.0, 2.0], self.evaluate(var0))
        self.assertAllClose([3.0, 4.0], self.evaluate(var1))
        # Mix the first and the second adagrad for 3 steps.
        self.evaluate(ada_update1)
        self.evaluate(ada_update2)
        self.evaluate(ada_update1)

        accum0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)
        accum1_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)
        for _ in range(3):
          var0_np, accum0_np = adagrad_update_numpy(
              var0_np, accum0_np, grads0_np, learning_rate)
          var1_np, accum1_np = adagrad_update_numpy(
              var1_np, accum1_np, grads1_np, learning_rate)
        self.assertAllCloseAccordingToType(var0_np, self.evaluate(var0))
        self.assertAllCloseAccordingToType(var1_np, self.evaluate(var1))

</source>
</class>

<class classid="7" nclones="2" nlines="11" similarity="72">
<source file="systems/keras-2.7.0/keras/optimizer_v2/adagrad_test.py" startline="510" endline="523" pcid="35">
  def testConstructAdagradWithLR(self):
    opt = adagrad.Adagrad(lr=1.0)
    opt_2 = adagrad.Adagrad(learning_rate=0.1, lr=1.0)
    opt_3 = adagrad.Adagrad(learning_rate=0.1)
    self.assertIsInstance(opt.lr, tf.Variable)
    self.assertIsInstance(opt_2.lr, tf.Variable)
    self.assertIsInstance(opt_3.lr, tf.Variable)

    self.evaluate(tf.compat.v1.global_variables_initializer())
    self.assertAllClose(self.evaluate(opt.lr), (1.0))
    self.assertAllClose(self.evaluate(opt_2.lr), (1.0))
    self.assertAllClose(self.evaluate(opt_3.lr), (0.1))


</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/adadelta_test.py" startline="164" endline="176" pcid="206">
  def testConstructAdadeltaWithLR(self):
    opt = adadelta.Adadelta(lr=1.0, rho=0.9, epsilon=1.)
    opt_2 = adadelta.Adadelta(learning_rate=0.1, rho=0.9, epsilon=1., lr=1.0)
    opt_3 = adadelta.Adadelta(learning_rate=0.1, rho=0.9, epsilon=1.)
    self.assertIsInstance(opt.lr, tf.Variable)
    self.assertIsInstance(opt_2.lr, tf.Variable)
    self.assertIsInstance(opt_3.lr, tf.Variable)

    self.evaluate(tf.compat.v1.global_variables_initializer())
    self.assertAllClose(self.evaluate(opt.lr), (1.0))
    self.assertAllClose(self.evaluate(opt_2.lr), (1.0))
    self.assertAllClose(self.evaluate(opt_3.lr), (0.1))

</source>
</class>

<class classid="8" nclones="2" nlines="17" similarity="76">
<source file="systems/keras-2.7.0/keras/optimizer_v2/learning_rate_schedule_test.py" startline="93" endline="112" pcid="40">
  def testPiecewiseConstant(self, serialize):
    x = tf.Variable(-999)
    decayed_lr = learning_rate_schedule.PiecewiseConstantDecay(
        [100, 110, 120], [1.0, 0.1, 0.01, 0.001])
    decayed_lr = _maybe_serialized(decayed_lr, serialize)

    self.evaluate(tf.compat.v1.global_variables_initializer())

    self.assertAllClose(self.evaluate(decayed_lr(x)), 1.0, 1e-6)
    self.evaluate(x.assign(100))
    self.assertAllClose(self.evaluate(decayed_lr(x)), 1.0, 1e-6)
    self.evaluate(x.assign(105))
    self.assertAllClose(self.evaluate(decayed_lr(x)), 0.1, 1e-6)
    self.evaluate(x.assign(110))
    self.assertAllClose(self.evaluate(decayed_lr(x)), 0.1, 1e-6)
    self.evaluate(x.assign(120))
    self.assertAllClose(self.evaluate(decayed_lr(x)), 0.01, 1e-6)
    self.evaluate(x.assign(999))
    self.assertAllClose(self.evaluate(decayed_lr(x)), 0.001, 1e-6)

</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/learning_rate_schedule_test.py" startline="135" endline="157" pcid="44">
  def testPiecewiseConstantEdgeCases(self, serialize):
    # Test casting boundaries from int32 to int64.
    x_int64 = tf.Variable(0, dtype=tf.int64)
    boundaries, values = [1, 2, 3], [0.4, 0.5, 0.6, 0.7]
    decayed_lr = learning_rate_schedule.PiecewiseConstantDecay(
        boundaries, values)
    decayed_lr = _maybe_serialized(decayed_lr, serialize)

    self.evaluate(tf.compat.v1.global_variables_initializer())
    self.assertAllClose(self.evaluate(decayed_lr(x_int64)), 0.4, 1e-6)
    self.evaluate(x_int64.assign(1))
    self.assertAllClose(self.evaluate(decayed_lr(x_int64)), 0.4, 1e-6)
    self.evaluate(x_int64.assign(2))
    self.assertAllClose(self.evaluate(decayed_lr(x_int64)), 0.5, 1e-6)
    self.evaluate(x_int64.assign(3))
    self.assertAllClose(self.evaluate(decayed_lr(x_int64)), 0.6, 1e-6)
    self.evaluate(x_int64.assign(4))
    self.assertAllClose(self.evaluate(decayed_lr(x_int64)), 0.7, 1e-6)


# @parameterized.named_parameters(
#     ("NotSerialized", False),
#     ("Serialized", True))
</source>
</class>

<class classid="9" nclones="5" nlines="10" similarity="80">
<source file="systems/keras-2.7.0/keras/optimizer_v2/learning_rate_schedule_test.py" startline="217" endline="227" pcid="50">
  def testHalfWay(self, serialize):
    step = 5
    lr = 0.05
    end_lr = 0.0
    power = 0.5
    decayed_lr = learning_rate_schedule.PolynomialDecay(
        lr, 10, end_lr, power=power)
    decayed_lr = _maybe_serialized(decayed_lr, serialize)
    expected = lr * 0.5**power
    self.assertAllClose(self.evaluate(decayed_lr(step)), expected, 1e-6)

</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/learning_rate_schedule_test.py" startline="250" endline="260" pcid="53">
  def testBeyondEnd(self, serialize):
    step = 15
    lr = 0.05
    end_lr = 0.001
    power = 0.5
    decayed_lr = learning_rate_schedule.PolynomialDecay(
        lr, 10, end_lr, power=power)
    decayed_lr = _maybe_serialized(decayed_lr, serialize)
    expected = end_lr
    self.assertAllClose(self.evaluate(decayed_lr(step)), expected, 1e-6)

</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/learning_rate_schedule_test.py" startline="228" endline="238" pcid="51">
  def testEnd(self, serialize):
    step = 10
    lr = 0.05
    end_lr = 0.001
    power = 0.5
    decayed_lr = learning_rate_schedule.PolynomialDecay(
        lr, 10, end_lr, power=power)
    decayed_lr = _maybe_serialized(decayed_lr, serialize)
    expected = end_lr
    self.assertAllClose(self.evaluate(decayed_lr(step)), expected, 1e-6)

</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/learning_rate_schedule_test.py" startline="261" endline="275" pcid="54">
  def testBeyondEndWithCycle(self, serialize):
    step = 15
    lr = 0.05
    end_lr = 0.001
    power = 0.5
    decayed_lr = learning_rate_schedule.PolynomialDecay(
        lr, 10, end_lr, power=power, cycle=True)
    decayed_lr = _maybe_serialized(decayed_lr, serialize)
    expected = (lr - end_lr) * 0.25**power + end_lr
    self.assertAllClose(self.evaluate(decayed_lr(step)), expected, 1e-6)


# @parameterized.named_parameters(
#     ("NotSerialized", False),
#     ("Serialized", True))
</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/learning_rate_schedule_test.py" startline="239" endline="249" pcid="52">
  def testHalfWayWithEnd(self, serialize):
    step = 5
    lr = 0.05
    end_lr = 0.001
    power = 0.5
    decayed_lr = learning_rate_schedule.PolynomialDecay(
        lr, 10, end_lr, power=power)
    decayed_lr = _maybe_serialized(decayed_lr, serialize)
    expected = (lr - end_lr) * 0.5**power + end_lr
    self.assertAllClose(self.evaluate(decayed_lr(step)), expected, 1e-6)

</source>
</class>

<class classid="10" nclones="2" nlines="13" similarity="76">
<source file="systems/keras-2.7.0/keras/optimizer_v2/learning_rate_schedule_test.py" startline="299" endline="313" pcid="56">
  def testDecay(self, serialize):
    initial_lr = 0.1
    k = 10
    decay_rate = 0.96
    step = tf.Variable(0)
    decayed_lr = learning_rate_schedule.InverseTimeDecay(initial_lr, k,
                                                         decay_rate)
    decayed_lr = _maybe_serialized(decayed_lr, serialize)

    self.evaluate(tf.compat.v1.global_variables_initializer())
    for i in range(k + 1):
      expected = initial_lr / (1 + i / k * decay_rate)
      self.assertAllClose(self.evaluate(decayed_lr(step)), expected, 1e-6)
      self.evaluate(step.assign_add(1))

</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/learning_rate_schedule_test.py" startline="314" endline="329" pcid="57">
  def testStaircase(self, serialize):
    initial_lr = 0.1
    k = 10
    decay_rate = 0.96
    step = tf.Variable(0)
    decayed_lr = learning_rate_schedule.InverseTimeDecay(
        initial_lr, k, decay_rate, staircase=True)
    decayed_lr = _maybe_serialized(decayed_lr, serialize)

    self.evaluate(tf.compat.v1.global_variables_initializer())
    for i in range(k + 1):
      expected = initial_lr / (1 + decay_rate * (i // k))
      self.assertAllClose(self.evaluate(decayed_lr(step)), expected, 1e-6)
      self.evaluate(step.assign_add(1))


</source>
</class>

<class classid="11" nclones="2" nlines="10" similarity="100">
<source file="systems/keras-2.7.0/keras/optimizer_v2/learning_rate_schedule_test.py" startline="378" endline="389" pcid="62">
  def np_cosine_decay_restarts(self, step, decay_steps, t_mul=2.0, m_mul=1.0,
                               alpha=0.0):
    fac = 1.0
    while step >= decay_steps:
      step -= decay_steps
      decay_steps *= t_mul
      fac *= m_mul

    completed_fraction = step / decay_steps
    decay = fac * 0.5 * (1.0 + math.cos(math.pi * completed_fraction))
    return (1.0 - alpha) * decay + alpha

</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/legacy_learning_rate_decay_test.py" startline="347" endline="358" pcid="184">
  def np_cosine_decay_restarts(self, step, decay_steps, t_mul=2.0, m_mul=1.0,
                               alpha=0.0):
    fac = 1.0
    while step >= decay_steps:
      step -= decay_steps
      decay_steps *= t_mul
      fac *= m_mul

    completed_fraction = step / decay_steps
    decay = fac * 0.5 * (1.0 + math.cos(math.pi * completed_fraction))
    return (1.0 - alpha) * decay + alpha

</source>
</class>

<class classid="12" nclones="3" nlines="11" similarity="100">
<source file="systems/keras-2.7.0/keras/optimizer_v2/learning_rate_schedule_test.py" startline="410" endline="421" pcid="65">
  def testAlpha(self, serialize):
    num_training_steps = 1000
    initial_lr = 1.0
    alpha = 0.1
    for step in range(0, 1500, 250):
      decayed_lr = learning_rate_schedule.CosineDecayRestarts(
          initial_lr, num_training_steps, alpha=alpha)
      decayed_lr = _maybe_serialized(decayed_lr, serialize)
      expected = self.np_cosine_decay_restarts(
          step, num_training_steps, alpha=alpha)
      self.assertAllClose(self.evaluate(decayed_lr(step)), expected, 1e-6)

</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/learning_rate_schedule_test.py" startline="434" endline="446" pcid="67">
  def testTMul(self, serialize):
    num_training_steps = 1000
    initial_lr = 1.0
    t_mul = 1.0
    for step in range(0, 1500, 250):
      decayed_lr = learning_rate_schedule.CosineDecayRestarts(
          initial_lr, num_training_steps, t_mul=t_mul)
      decayed_lr = _maybe_serialized(decayed_lr, serialize)
      expected = self.np_cosine_decay_restarts(
          step, num_training_steps, t_mul=t_mul)
      self.assertAllClose(self.evaluate(decayed_lr(step)), expected, 1e-6)


</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/learning_rate_schedule_test.py" startline="422" endline="433" pcid="66">
  def testMMul(self, serialize):
    num_training_steps = 1000
    initial_lr = 1.0
    m_mul = 0.9
    for step in range(0, 1500, 250):
      decayed_lr = learning_rate_schedule.CosineDecayRestarts(
          initial_lr, num_training_steps, m_mul=m_mul)
      decayed_lr = _maybe_serialized(decayed_lr, serialize)
      expected = self.np_cosine_decay_restarts(
          step, num_training_steps, m_mul=m_mul)
      self.assertAllClose(self.evaluate(decayed_lr(step)), expected, 1e-6)

</source>
</class>

<class classid="13" nclones="2" nlines="18" similarity="88">
<source file="systems/keras-2.7.0/keras/optimizer_v2/optimizer_v2_test.py" startline="426" endline="448" pcid="96">
  def testGettingHyperParameters(self):
    with self.test_session():
      opt = adam.Adam(learning_rate=1.0)
      var = tf.Variable([1.0, 2.0], dtype=tf.float32)
      loss = lambda: 3 * var
      opt_op = opt.minimize(loss, [var])
      self.evaluate(tf.compat.v1.global_variables_initializer())
      self.evaluate(opt_op)

      lr = self.evaluate(opt.lr)
      self.assertEqual(1.0, lr)

      opt.lr = 2.0
      lr = self.evaluate(opt.lr)
      self.assertEqual(2.0, lr)

      self.evaluate(opt.lr.assign(3.0))
      lr = self.evaluate(opt.lr)
      self.assertEqual(3.0, lr)

      with self.assertRaises(AttributeError):
        opt.not_an_attr += 3

</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/optimizer_v2_test.py" startline="450" endline="472" pcid="97">
  def testGettingHyperParametersWithLrInConstructor(self):
    with self.test_session():
      opt = gradient_descent.SGD(lr=3.0)
      var = tf.Variable([1.0, 2.0], dtype=tf.float32)
      loss = lambda: 3 * var
      opt_op = opt.minimize(loss, [var])
      self.evaluate(tf.compat.v1.global_variables_initializer())
      self.evaluate(opt_op)

      self.assertIsInstance(opt.lr, tf.Variable)
      self.assertIsInstance(opt.learning_rate, tf.Variable)

      lr = self.evaluate(opt.lr)
      self.assertEqual(3.0, lr)

      opt.lr = 2.0
      lr = self.evaluate(opt.lr)
      self.assertEqual(2.0, lr)

      self.evaluate(opt.lr.assign(4.0))
      lr = self.evaluate(opt.lr)
      self.assertEqual(4.0, lr)

</source>
</class>

<class classid="14" nclones="2" nlines="10" similarity="100">
<source file="systems/keras-2.7.0/keras/optimizer_v2/optimizer_v2_test.py" startline="640" endline="653" pcid="106">
  def testAggregationTrue(self):
    # Test that experimental_aggregate_gradients=True works without distributed
    # strategy.
    var = tf.Variable([1., 2.])
    opt = gradient_descent.SGD(3.0)

    self.evaluate(tf.compat.v1.global_variables_initializer())
    self.assertAllClose([1., 2.], self.evaluate(var))
    opt_op = opt.apply_gradients([([0.1, 0.1], var)],
                                 experimental_aggregate_gradients=True)
    self.evaluate(tf.compat.v1.global_variables_initializer())
    self.evaluate(opt_op)
    self.assertAllClose([0.7, 1.7], self.evaluate(var))

</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/optimizer_v2_test.py" startline="655" endline="668" pcid="107">
  def testAggregationFalse(self):
    # Test that experimental_aggregate_gradients=False works without distributed
    # strategy.
    var = tf.Variable([1., 2.])
    opt = gradient_descent.SGD(3.0)

    self.evaluate(tf.compat.v1.global_variables_initializer())
    self.assertAllClose([1., 2.], self.evaluate(var))
    opt_op = opt.apply_gradients([([0.1, 0.1], var)],
                                 experimental_aggregate_gradients=False)
    self.evaluate(tf.compat.v1.global_variables_initializer())
    self.evaluate(opt_op)
    self.assertAllClose([0.7, 1.7], self.evaluate(var))

</source>
</class>

<class classid="15" nclones="2" nlines="14" similarity="78">
<source file="systems/keras-2.7.0/keras/optimizer_v2/optimizer_v2_test.py" startline="707" endline="723" pcid="110">
  def test_gradient_aggregator(self):
    def gradient_aggregator(grads_and_vars):
      # Simulate an all-reduce where the other replica has zeros for gradients,
      # by dividing each gradient by 2.
      grads = [g for g, _ in grads_and_vars]
      vars = [v for _, v in grads_and_vars]  # pylint: disable=redefined-builtin
      all_reduced_grads = [g / 2 for g in grads]
      return list(zip(all_reduced_grads, vars))

    var = tf.Variable(2.0)
    sgd = gradient_descent.SGD(1.0, gradient_aggregator=gradient_aggregator)
    loss = lambda: 2 * var
    opt_op = sgd.minimize(loss, var_list=[var])
    self.evaluate(tf.compat.v1.global_variables_initializer())
    self.evaluate(opt_op)
    self.assertEqual(self.evaluate(var), 1.0)

</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/optimizer_v2_test.py" startline="725" endline="744" pcid="112">
  def test_override_aggregate_gradients(self):
    class MyOptimizer(gradient_descent.SGD):

      def _aggregate_gradients(self, grads_and_vars):
        # Simulate an all-reduce where the other replica has zeros for
        # gradients, by dividing each gradient by 2.
        grads = [g for g, _ in grads_and_vars]
        vars = [v for _, v in grads_and_vars]  # pylint: disable=redefined-builtin
        all_reduced_grads = [g / 2 for g in grads]
        return list(zip(all_reduced_grads, vars))

    var = tf.Variable(2.0)
    sgd = MyOptimizer(1.0)
    loss = lambda: 2 * var
    opt_op = sgd.minimize(loss, var_list=[var])
    self.evaluate(tf.compat.v1.global_variables_initializer())
    self.evaluate(opt_op)
    self.assertEqual(self.evaluate(var), 1.0)


</source>
</class>

<class classid="16" nclones="2" nlines="45" similarity="70">
<source file="systems/keras-2.7.0/keras/optimizer_v2/optimizer_v2_test.py" startline="834" endline="890" pcid="123">
  def testNumericEquivalenceForNesterovMomentum(self):
    if tf.executing_eagerly():
      self.skipTest(
          'v1 optimizer does not run in eager mode')
    np.random.seed(1331)
    with testing_utils.use_gpu():
      train_samples = 20
      input_dim = 3
      num_classes = 2
      (x, y), _ = testing_utils.get_test_data(
          train_samples=train_samples,
          test_samples=10,
          input_shape=(input_dim,),
          num_classes=num_classes)
      y = np_utils.to_categorical(y)

      num_hidden = 5
      model_k_v1 = testing_utils.get_small_sequential_mlp(
          num_hidden=num_hidden, num_classes=num_classes, input_dim=input_dim)
      model_k_v2 = testing_utils.get_small_sequential_mlp(
          num_hidden=num_hidden, num_classes=num_classes, input_dim=input_dim)
      model_k_v2.set_weights(model_k_v1.get_weights())
      model_tf = testing_utils.get_small_sequential_mlp(
          num_hidden=num_hidden, num_classes=num_classes, input_dim=input_dim)
      model_tf.set_weights(model_k_v2.get_weights())

      opt_k_v1 = optimizer_v1.SGD(momentum=0.9, nesterov=True)
      opt_k_v2 = gradient_descent.SGD(momentum=0.9, nesterov=True)
      opt_tf = tf.compat.v1.train.MomentumOptimizer(
          learning_rate=0.01, momentum=0.9, use_nesterov=True)

      model_k_v1.compile(
          opt_k_v1,
          loss='categorical_crossentropy',
          metrics=[],
          run_eagerly=testing_utils.should_run_eagerly())
      model_k_v2.compile(
          opt_k_v2,
          loss='categorical_crossentropy',
          metrics=[],
          run_eagerly=testing_utils.should_run_eagerly())
      model_tf.compile(
          opt_tf,
          loss='categorical_crossentropy',
          metrics=[],
          run_eagerly=testing_utils.should_run_eagerly())

      hist_k_v1 = model_k_v1.fit(x, y, batch_size=5, epochs=10, shuffle=False)
      hist_k_v2 = model_k_v2.fit(x, y, batch_size=5, epochs=10, shuffle=False)
      hist_tf = model_tf.fit(x, y, batch_size=5, epochs=10, shuffle=False)

      self.assertAllClose(model_k_v1.get_weights(), model_tf.get_weights())
      self.assertAllClose(model_k_v1.get_weights(), model_k_v2.get_weights())
      self.assertAllClose(opt_k_v1.get_weights(), opt_k_v2.get_weights())
      self.assertAllClose(hist_k_v1.history['loss'], hist_tf.history['loss'])
      self.assertAllClose(hist_k_v1.history['loss'], hist_k_v2.history['loss'])

</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/optimizer_v2_test.py" startline="891" endline="937" pcid="124">
  def testNumericEquivalenceForAmsgrad(self):
    if tf.executing_eagerly():
      self.skipTest(
          'v1 optimizer does not run in eager mode')
    np.random.seed(1331)
    with testing_utils.use_gpu():
      train_samples = 20
      input_dim = 3
      num_classes = 2
      (x, y), _ = testing_utils.get_test_data(
          train_samples=train_samples,
          test_samples=10,
          input_shape=(input_dim,),
          num_classes=num_classes)
      y = np_utils.to_categorical(y)

      num_hidden = 5
      model_k_v1 = testing_utils.get_small_sequential_mlp(
          num_hidden=num_hidden, num_classes=num_classes, input_dim=input_dim)
      model_k_v2 = testing_utils.get_small_sequential_mlp(
          num_hidden=num_hidden, num_classes=num_classes, input_dim=input_dim)
      model_k_v2.set_weights(model_k_v1.get_weights())

      opt_k_v1 = optimizer_v1.Adam(amsgrad=True)
      opt_k_v2 = adam.Adam(amsgrad=True)

      model_k_v1.compile(
          opt_k_v1,
          loss='categorical_crossentropy',
          metrics=[],
          run_eagerly=testing_utils.should_run_eagerly())
      model_k_v2.compile(
          opt_k_v2,
          loss='categorical_crossentropy',
          metrics=[],
          run_eagerly=testing_utils.should_run_eagerly())

      hist_k_v1 = model_k_v1.fit(x, y, batch_size=5, epochs=10, shuffle=False)
      hist_k_v2 = model_k_v2.fit(x, y, batch_size=5, epochs=10, shuffle=False)

      self.assertAllClose(model_k_v1.get_weights(), model_k_v2.get_weights())
      self.assertAllClose(opt_k_v1.get_weights(), opt_k_v2.get_weights())
      self.assertAllClose(hist_k_v1.history['loss'], hist_k_v2.history['loss'])


# Note: These tests are kept in a separate class to avoid bugs in some
# distributions of Python that break AutoGraph which is used by tf.function.
</source>
</class>

<class classid="17" nclones="2" nlines="10" similarity="100">
<source file="systems/keras-2.7.0/keras/optimizer_v2/optimizer_v2_test.py" startline="941" endline="953" pcid="125">
  def testBasic(self):
    var = tf.Variable([1.0, 2.0], dtype=tf.float32)
    loss = lambda: 3 * var
    opt = adam.Adam(learning_rate=1.0)

    @tf.function
    def fn():
      opt.minimize(loss, [var])
      return var

    self.assertAllClose([0., 1.], fn(), atol=1e-4)
    self.assertAllClose([-1, 0.], fn(), atol=1e-4)

</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/optimizer_v2_test.py" startline="954" endline="966" pcid="127">
  def testBasicWithConstantDecay(self):
    var = tf.Variable([1.0, 2.0], dtype=tf.float32)
    loss = lambda: 3 * var
    opt = adam.Adam(learning_rate=1.0)

    @tf.function
    def fn():
      opt.minimize(loss, [var])
      return var

    self.assertAllClose([0., 1.], fn(), atol=1e-4)
    self.assertAllClose([-1, 0.], fn(), atol=1e-4)

</source>
</class>

<class classid="18" nclones="6" nlines="14" similarity="70">
<source file="systems/keras-2.7.0/keras/optimizer_v2/adagrad.py" startline="128" endline="141" pcid="148">
  def _resource_apply_dense(self, grad, var, apply_state=None):
    var_device, var_dtype = var.device, var.dtype.base_dtype
    coefficients = ((apply_state or {}).get((var_device, var_dtype))
                    or self._fallback_apply_state(var_device, var_dtype))

    acc = self.get_slot(var, 'accumulator')
    return tf.raw_ops.ResourceApplyAdagradV2(
        var=var.handle,
        accum=acc.handle,
        lr=coefficients['lr_t'],
        epsilon=coefficients['epsilon'],
        grad=grad,
        use_locking=self._use_locking)

</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/adadelta.py" startline="104" endline="120" pcid="264">
  def _resource_apply_dense(self, grad, var, apply_state=None):
    var_device, var_dtype = var.device, var.dtype.base_dtype
    coefficients = ((apply_state or {}).get((var_device, var_dtype))
                    or self._fallback_apply_state(var_device, var_dtype))

    accum_grad = self.get_slot(var, 'accum_grad')
    accum_var = self.get_slot(var, 'accum_var')
    return tf.raw_ops.ResourceApplyAdadelta(
        var=var.handle,
        accum=accum_grad.handle,
        accum_update=accum_var.handle,
        lr=coefficients['lr_t'],
        rho=coefficients['rho'],
        epsilon=coefficients['epsilon'],
        grad=grad,
        use_locking=self._use_locking)

</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/adagrad.py" startline="142" endline="156" pcid="149">
  def _resource_apply_sparse(self, grad, var, indices, apply_state=None):
    var_device, var_dtype = var.device, var.dtype.base_dtype
    coefficients = ((apply_state or {}).get((var_device, var_dtype))
                    or self._fallback_apply_state(var_device, var_dtype))

    acc = self.get_slot(var, 'accumulator')
    return tf.raw_ops.ResourceSparseApplyAdagradV2(
        var=var.handle,
        accum=acc.handle,
        lr=coefficients['lr_t'],
        epsilon=coefficients['epsilon'],
        grad=grad,
        indices=indices,
        use_locking=self._use_locking)

</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/gradient_descent.py" startline="163" endline="179" pcid="213">
  def _resource_apply_sparse(self, grad, var, indices, apply_state=None):
    # This method is only needed for momentum optimization.
    var_device, var_dtype = var.device, var.dtype.base_dtype
    coefficients = ((apply_state or {}).get((var_device, var_dtype))
                    or self._fallback_apply_state(var_device, var_dtype))

    momentum_var = self.get_slot(var, "momentum")
    return tf.raw_ops.ResourceSparseApplyKerasMomentum(
        var=var.handle,
        accum=momentum_var.handle,
        lr=coefficients["lr_t"],
        grad=grad,
        indices=indices,
        momentum=coefficients["momentum"],
        use_locking=self._use_locking,
        use_nesterov=self.nesterov)

</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/adamax.py" startline="124" endline="142" pcid="218">
  def _resource_apply_dense(self, grad, var, apply_state=None):
    var_device, var_dtype = var.device, var.dtype.base_dtype
    coefficients = ((apply_state or {}).get((var_device, var_dtype))
                    or self._fallback_apply_state(var_device, var_dtype))

    m = self.get_slot(var, 'm')
    v = self.get_slot(var, 'v')
    return tf.raw_ops.ResourceApplyAdaMax(
        var=var.handle,
        m=m.handle,
        v=v.handle,
        beta1_power=coefficients['beta_1_power'],
        lr=coefficients['lr_t'],
        beta1=coefficients['beta_1_t'],
        beta2=coefficients['beta_2_t'],
        epsilon=coefficients['epsilon'],
        grad=grad,
        use_locking=self._use_locking)

</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/adadelta.py" startline="121" endline="138" pcid="265">
  def _resource_apply_sparse(self, grad, var, indices, apply_state=None):
    var_device, var_dtype = var.device, var.dtype.base_dtype
    coefficients = ((apply_state or {}).get((var_device, var_dtype))
                    or self._fallback_apply_state(var_device, var_dtype))

    accum_grad = self.get_slot(var, 'accum_grad')
    accum_var = self.get_slot(var, 'accum_var')
    return tf.raw_ops.ResourceSparseApplyAdadelta(
        var=var.handle,
        accum=accum_grad.handle,
        accum_update=accum_var.handle,
        lr=coefficients['lr_t'],
        rho=coefficients['rho'],
        epsilon=coefficients['epsilon'],
        grad=grad,
        indices=indices,
        use_locking=self._use_locking)

</source>
</class>

<class classid="19" nclones="3" nlines="14" similarity="78">
<source file="systems/keras-2.7.0/keras/optimizer_v2/utils.py" startline="84" endline="105" pcid="153">
def make_gradient_clipnorm_fn(clipnorm):
  """Creates a gradient transformation function for clipping by norm."""
  if clipnorm is None:
    return lambda grads_and_vars: grads_and_vars

  def gradient_clipnorm_fn(grads_and_vars):

    if isinstance(tf.distribute.get_strategy(),
                  (tf.distribute.experimental.CentralStorageStrategy,
                   tf.compat.v1.distribute.experimental.CentralStorageStrategy)):
      raise ValueError(
          "`clipnorm` is not supported with `CenteralStorageStrategy`. "
          f"The strategy used is {tf.distribute.get_strategy()}.")

    clipped_grads_and_vars = [
        (tf.clip_by_norm(g, clipnorm), v) for g, v in grads_and_vars
    ]
    return clipped_grads_and_vars

  return gradient_clipnorm_fn


</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/utils.py" startline="106" endline="127" pcid="155">
def make_global_gradient_clipnorm_fn(clipnorm):
  """Creates a gradient transformation function for clipping by norm."""
  if clipnorm is None:
    return lambda grads_and_vars: grads_and_vars

  def gradient_clipnorm_fn(grads_and_vars):

    if isinstance(tf.distribute.get_strategy(),
                  (tf.distribute.experimental.CentralStorageStrategy,
                   tf.compat.v1.distribute.experimental.CentralStorageStrategy)):
      raise ValueError(
          "`global_clipnorm` is not supported with `CenteralStorageStrategy`. "
          f"The strategy used is {tf.distribute.get_strategy()}.")

    grads, variables = zip(*grads_and_vars)
    clipped_grads, _ = tf.clip_by_global_norm(grads, clipnorm)
    clipped_grads_and_vars = list(zip(clipped_grads, variables))
    return clipped_grads_and_vars

  return gradient_clipnorm_fn


</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/utils.py" startline="128" endline="149" pcid="157">
def make_gradient_clipvalue_fn(clipvalue):
  """Creates a gradient transformation function for clipping by value."""
  if clipvalue is None:
    return lambda grads_and_vars: grads_and_vars

  def gradient_clipvalue_fn(grads_and_vars):

    if isinstance(tf.distribute.get_strategy(),
                  (tf.distribute.experimental.CentralStorageStrategy,
                   tf.compat.v1.distribute.experimental.CentralStorageStrategy)):
      raise ValueError(
          "`clipvalue` is not supported with `CenteralStorageStrategy`. "
          f"The strategy used is {tf.distribute.get_strategy()}.")

    clipped_grads_and_vars = [(tf.clip_by_value(g, -clipvalue,
                                                      clipvalue), v)
                              for g, v in grads_and_vars]
    return clipped_grads_and_vars

  return gradient_clipvalue_fn


</source>
</class>

<class classid="20" nclones="4" nlines="12" similarity="75">
<source file="systems/keras-2.7.0/keras/optimizer_v2/legacy_learning_rate_decay_test.py" startline="254" endline="267" pcid="177">
  def testDecay(self):
    initial_lr = 0.1
    k = 10
    decay_rate = 0.96
    step = tf.Variable(0)
    decayed_lr = tf.compat.v1.train.natural_exp_decay(initial_lr, step, k,
                                                       decay_rate)

    self.evaluate(tf.compat.v1.global_variables_initializer())
    for i in range(k + 1):
      expected = initial_lr * math.exp(-i / k * decay_rate)
      self.assertAllClose(self.evaluate(decayed_lr), expected, 1e-6)
      self.evaluate(step.assign_add(1))

</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/legacy_learning_rate_decay_test.py" startline="286" endline="299" pcid="179">
  def testDecay(self):
    initial_lr = 0.1
    k = 10
    decay_rate = 0.96
    step = tf.Variable(0)
    decayed_lr = tf.compat.v1.train.inverse_time_decay(initial_lr, step, k,
                                                        decay_rate)

    self.evaluate(tf.compat.v1.global_variables_initializer())
    for i in range(k + 1):
      expected = initial_lr / (1 + i / k * decay_rate)
      self.assertAllClose(self.evaluate(decayed_lr), expected, 1e-6)
      self.evaluate(step.assign_add(1))

</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/legacy_learning_rate_decay_test.py" startline="300" endline="314" pcid="180">
  def testStaircase(self):
    initial_lr = 0.1
    k = 10
    decay_rate = 0.96
    step = tf.Variable(0)
    decayed_lr = tf.compat.v1.train.inverse_time_decay(
        initial_lr, step, k, decay_rate, staircase=True)

    self.evaluate(tf.compat.v1.global_variables_initializer())
    for i in range(k + 1):
      expected = initial_lr / (1 + decay_rate * (i // k))
      self.assertAllClose(self.evaluate(decayed_lr), expected, 1e-6)
      self.evaluate(step.assign_add(1))


</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/legacy_learning_rate_decay_test.py" startline="268" endline="282" pcid="178">
  def testStaircase(self):
    initial_lr = 0.1
    k = 10
    decay_rate = 0.96
    step = tf.Variable(0)
    decayed_lr = tf.compat.v1.train.natural_exp_decay(
        initial_lr, step, k, decay_rate, staircase=True)

    self.evaluate(tf.compat.v1.global_variables_initializer())
    for i in range(k + 1):
      expected = initial_lr * math.exp(-decay_rate * (i // k))
      self.assertAllClose(self.evaluate(decayed_lr), expected, 1e-6)
      self.evaluate(step.assign_add(1))


</source>
</class>

<class classid="21" nclones="3" nlines="10" similarity="100">
<source file="systems/keras-2.7.0/keras/optimizer_v2/legacy_learning_rate_decay_test.py" startline="368" endline="378" pcid="186">
  def testAlpha(self):
    num_training_steps = 1000
    initial_lr = 1.0
    alpha = 0.1
    for step in range(0, 1500, 250):
      decayed_lr = tf.compat.v1.train.cosine_decay_restarts(
          initial_lr, step, num_training_steps, alpha=alpha)
      expected = self.np_cosine_decay_restarts(
          step, num_training_steps, alpha=alpha)
      self.assertAllClose(self.evaluate(decayed_lr), expected, 1e-6)

</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/legacy_learning_rate_decay_test.py" startline="390" endline="401" pcid="188">
  def testTMul(self):
    num_training_steps = 1000
    initial_lr = 1.0
    t_mul = 1.0
    for step in range(0, 1500, 250):
      decayed_lr = tf.compat.v1.train.cosine_decay_restarts(
          initial_lr, step, num_training_steps, t_mul=t_mul)
      expected = self.np_cosine_decay_restarts(
          step, num_training_steps, t_mul=t_mul)
      self.assertAllClose(self.evaluate(decayed_lr), expected, 1e-6)


</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/legacy_learning_rate_decay_test.py" startline="379" endline="389" pcid="187">
  def testMMul(self):
    num_training_steps = 1000
    initial_lr = 1.0
    m_mul = 0.9
    for step in range(0, 1500, 250):
      decayed_lr = tf.compat.v1.train.cosine_decay_restarts(
          initial_lr, step, num_training_steps, m_mul=m_mul)
      expected = self.np_cosine_decay_restarts(
          step, num_training_steps, m_mul=m_mul)
      self.assertAllClose(self.evaluate(decayed_lr), expected, 1e-6)

</source>
</class>

<class classid="22" nclones="2" nlines="14" similarity="78">
<source file="systems/keras-2.7.0/keras/optimizer_v2/legacy_learning_rate_decay_test.py" startline="426" endline="441" pcid="191">
  def testNonDefaultDecay(self):
    num_training_steps = 1000
    initial_lr = 1.0
    for step in range(0, 1500, 250):
      decayed_lr = tf.compat.v1.train.linear_cosine_decay(
          initial_lr,
          step,
          num_training_steps,
          alpha=0.1,
          beta=1e-4,
          num_periods=5)
      expected = self.np_linear_cosine_decay(
          step, num_training_steps, alpha=0.1, beta=1e-4, num_periods=5)
      self.assertAllClose(self.evaluate(decayed_lr), expected, 1e-6)


</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/legacy_learning_rate_decay_test.py" startline="455" endline="472" pcid="193">
  def testNonDefaultNoisyLinearCosine(self):
    num_training_steps = 1000
    initial_lr = 1.0
    for step in range(0, 1500, 250):
      # No numerical check because of noise
      decayed_lr = tf.compat.v1.train.noisy_linear_cosine_decay(
          initial_lr,
          step,
          num_training_steps,
          initial_variance=0.5,
          variance_decay=0.1,
          alpha=0.1,
          beta=1e-4,
          num_periods=5)
      # Cannot be deterministically tested
      self.evaluate(decayed_lr)


</source>
</class>

<class classid="23" nclones="7" nlines="13" similarity="73">
<source file="systems/keras-2.7.0/keras/optimizer_v2/learning_rate_schedule.py" startline="143" endline="170" pcid="224">
  def __init__(
      self,
      initial_learning_rate,
      decay_steps,
      decay_rate,
      staircase=False,
      name=None):
    """Applies exponential decay to the learning rate.

    Args:
      initial_learning_rate: A scalar `float32` or `float64` `Tensor` or a
        Python number.  The initial learning rate.
      decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number.
        Must be positive.  See the decay computation above.
      decay_rate: A scalar `float32` or `float64` `Tensor` or a
        Python number.  The decay rate.
      staircase: Boolean.  If `True` decay the learning rate at discrete
        intervals
      name: String.  Optional name of the operation.  Defaults to
        'ExponentialDecay'.
    """
    super(ExponentialDecay, self).__init__()
    self.initial_learning_rate = initial_learning_rate
    self.decay_steps = decay_steps
    self.decay_rate = decay_rate
    self.staircase = staircase
    self.name = name

</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/learning_rate_schedule.py" startline="687" endline="718" pcid="239">
  def __init__(
      self,
      initial_learning_rate,
      first_decay_steps,
      t_mul=2.0,
      m_mul=1.0,
      alpha=0.0,
      name=None):
    """Applies cosine decay with restarts to the learning rate.

    Args:
      initial_learning_rate: A scalar `float32` or `float64` Tensor or a Python
        number. The initial learning rate.
      first_decay_steps: A scalar `int32` or `int64` `Tensor` or a Python
        number. Number of steps to decay over.
      t_mul: A scalar `float32` or `float64` `Tensor` or a Python number.
        Used to derive the number of iterations in the i-th period.
      m_mul: A scalar `float32` or `float64` `Tensor` or a Python number.
        Used to derive the initial learning rate of the i-th period.
      alpha: A scalar `float32` or `float64` Tensor or a Python number.
        Minimum learning rate value as a fraction of the initial_learning_rate.
      name: String. Optional name of the operation.  Defaults to 'SGDRDecay'.
    """
    super(CosineDecayRestarts, self).__init__()

    self.initial_learning_rate = initial_learning_rate
    self.first_decay_steps = first_decay_steps
    self._t_mul = t_mul
    self._m_mul = m_mul
    self.alpha = alpha
    self.name = name

</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/learning_rate_schedule.py" startline="825" endline="855" pcid="243">
  def __init__(
      self,
      initial_learning_rate,
      decay_steps,
      num_periods=0.5,
      alpha=0.0,
      beta=0.001,
      name=None):
    """Applies linear cosine decay to the learning rate.

    Args:
      initial_learning_rate: A scalar `float32` or `float64` Tensor or a Python
        number. The initial learning rate.
      decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number.
        Number of steps to decay over.
      num_periods: Number of periods in the cosine part of the decay.
        See computation above.
      alpha: See computation above.
      beta: See computation above.
      name: String.  Optional name of the operation.  Defaults to
        'LinearCosineDecay'.
    """
    super(LinearCosineDecay, self).__init__()

    self.initial_learning_rate = initial_learning_rate
    self.decay_steps = decay_steps
    self.num_periods = num_periods
    self.alpha = alpha
    self.beta = beta
    self.name = name

</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/learning_rate_schedule.py" startline="372" endline="403" pcid="230">
  def __init__(
      self,
      initial_learning_rate,
      decay_steps,
      end_learning_rate=0.0001,
      power=1.0,
      cycle=False,
      name=None):
    """Applies a polynomial decay to the learning rate.

    Args:
      initial_learning_rate: A scalar `float32` or `float64` `Tensor` or a
        Python number.  The initial learning rate.
      decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number.
        Must be positive.  See the decay computation above.
      end_learning_rate: A scalar `float32` or `float64` `Tensor` or a
        Python number.  The minimal end learning rate.
      power: A scalar `float32` or `float64` `Tensor` or a
        Python number.  The power of the polynomial. Defaults to linear, 1.0.
      cycle: A boolean, whether or not it should cycle beyond decay_steps.
      name: String.  Optional name of the operation. Defaults to
        'PolynomialDecay'.
    """
    super(PolynomialDecay, self).__init__()

    self.initial_learning_rate = initial_learning_rate
    self.decay_steps = decay_steps
    self.end_learning_rate = end_learning_rate
    self.power = power
    self.cycle = cycle
    self.name = name

</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/learning_rate_schedule.py" startline="595" endline="618" pcid="236">
  def __init__(
      self,
      initial_learning_rate,
      decay_steps,
      alpha=0.0,
      name=None):
    """Applies cosine decay to the learning rate.

    Args:
      initial_learning_rate: A scalar `float32` or `float64` Tensor or a
        Python number. The initial learning rate.
      decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number.
        Number of steps to decay over.
      alpha: A scalar `float32` or `float64` Tensor or a Python number.
        Minimum learning rate value as a fraction of initial_learning_rate.
      name: String. Optional name of the operation.  Defaults to 'CosineDecay'.
    """
    super(CosineDecay, self).__init__()

    self.initial_learning_rate = initial_learning_rate
    self.decay_steps = decay_steps
    self.alpha = alpha
    self.name = name

</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/learning_rate_schedule.py" startline="497" endline="523" pcid="233">
  def __init__(
      self,
      initial_learning_rate,
      decay_steps,
      decay_rate,
      staircase=False,
      name=None):
    """Applies inverse time decay to the initial learning rate.

    Args:
      initial_learning_rate: A scalar `float32` or `float64` `Tensor` or a
        Python number.  The initial learning rate.
      decay_steps: How often to apply decay.
      decay_rate: A Python number.  The decay rate.
      staircase: Whether to apply decay in a discrete staircase, as opposed to
        continuous, fashion.
      name: String.  Optional name of the operation.  Defaults to
        'InverseTimeDecay'.
    """
    super(InverseTimeDecay, self).__init__()

    self.initial_learning_rate = initial_learning_rate
    self.decay_steps = decay_steps
    self.decay_rate = decay_rate
    self.staircase = staircase
    self.name = name

</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/learning_rate_schedule.py" startline="234" endline="266" pcid="227">
  def __init__(
      self,
      boundaries,
      values,
      name=None):
    """Piecewise constant from boundaries and interval values.

    Args:
      boundaries: A list of `Tensor`s or `int`s or `float`s with strictly
        increasing entries, and with all elements having the same type as the
        optimizer step.
      values: A list of `Tensor`s or `float`s or `int`s that specifies the
        values for the intervals defined by `boundaries`. It should have one
        more element than `boundaries`, and all elements should have the same
        type.
      name: A string. Optional name of the operation. Defaults to
        'PiecewiseConstant'.

    Raises:
      ValueError: if the number of elements in the lists do not match.
    """
    super(PiecewiseConstantDecay, self).__init__()

    if len(boundaries) != len(values) - 1:
      raise ValueError(
          "The length of boundaries should be 1 less than the length of "
          f"values. Received: boundaries={boundaries} of length "
          f"{len(boundaries)}, and values={values} of length {len(values)}.")

    self.boundaries = boundaries
    self.values = values
    self.name = name

</source>
</class>

<class classid="24" nclones="2" nlines="14" similarity="78">
<source file="systems/keras-2.7.0/keras/optimizer_v2/learning_rate_schedule.py" startline="171" endline="185" pcid="225">
  def __call__(self, step):
    with tf.name_scope(self.name or "ExponentialDecay") as name:
      initial_learning_rate = tf.convert_to_tensor(
          self.initial_learning_rate, name="initial_learning_rate")
      dtype = initial_learning_rate.dtype
      decay_steps = tf.cast(self.decay_steps, dtype)
      decay_rate = tf.cast(self.decay_rate, dtype)

      global_step_recomp = tf.cast(step, dtype)
      p = global_step_recomp / decay_steps
      if self.staircase:
        p = tf.floor(p)
      return tf.multiply(
          initial_learning_rate, tf.pow(decay_rate, p), name=name)

</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/learning_rate_schedule.py" startline="524" endline="539" pcid="234">
  def __call__(self, step):
    with tf.name_scope(self.name or "InverseTimeDecay") as name:
      initial_learning_rate = tf.convert_to_tensor(
          self.initial_learning_rate, name="initial_learning_rate")
      dtype = initial_learning_rate.dtype
      decay_steps = tf.cast(self.decay_steps, dtype)
      decay_rate = tf.cast(self.decay_rate, dtype)

      global_step_recomp = tf.cast(step, dtype)
      p = global_step_recomp / decay_steps
      if self.staircase:
        p = tf.floor(p)
      const = tf.cast(tf.constant(1), dtype)
      denom = tf.add(const, tf.multiply(decay_rate, p))
      return tf.divide(initial_learning_rate, denom, name=name)

</source>
</class>

<class classid="25" nclones="2" nlines="13" similarity="100">
<source file="systems/keras-2.7.0/keras/optimizer_v2/legacy_learning_rate_decay.py" startline="25" endline="99" pcid="251">
def exponential_decay(learning_rate,
                      global_step,
                      decay_steps,
                      decay_rate,
                      staircase=False,
                      name=None):
  """Applies exponential decay to the learning rate.

  When training a model, it is often recommended to lower the learning rate as
  the training progresses.  This function applies an exponential decay function
  to a provided initial learning rate.  It requires a `global_step` value to
  compute the decayed learning rate.  You can just pass a TensorFlow variable
  that you increment at each training step.

  The function returns the decayed learning rate.  It is computed as:

  ```python
  decayed_learning_rate = learning_rate *
                          decay_rate ^ (global_step / decay_steps)
  ```

  If the argument `staircase` is `True`, then `global_step / decay_steps` is an
  integer division and the decayed learning rate follows a staircase function.

  Example: decay every 100000 steps with a base of 0.96:

  ```python
  ...
  global_step = tf.Variable(0, trainable=False)
  starter_learning_rate = 0.1
  learning_rate = tf.compat.v1.train.exponential_decay(starter_learning_rate,
  global_step,
                                             100000, 0.96, staircase=True)
  # Passing global_step to minimize() will increment it at each step.
  learning_step = (
      tf.compat.v1.train.GradientDescentOptimizer(learning_rate)
      .minimize(...my loss..., global_step=global_step)
  )
  ```

  Args:
    learning_rate: A scalar `float32` or `float64` `Tensor` or a Python number.
      The initial learning rate.
    global_step: A scalar `int32` or `int64` `Tensor` or a Python number. Global
      step to use for the decay computation.  Must not be negative.
    decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number. Must
      be positive.  See the decay computation above.
    decay_rate: A scalar `float32` or `float64` `Tensor` or a Python number.
      The decay rate.
    staircase: Boolean.  If `True` decay the learning rate at discrete intervals
    name: String.  Optional name of the operation.  Defaults to
      'ExponentialDecay'.

  Returns:
    A scalar `Tensor` of the same type as `learning_rate`.  The decayed
    learning rate.

  Raises:
    ValueError: if `global_step` is not supplied.

  @compatibility(eager)
  When eager execution is enabled, this function returns a function which in
  turn returns the decayed learning rate Tensor. This can be useful for changing
  the learning rate value across different invocations of optimizer functions.
  @end_compatibility
  """
  decayed_lr = learning_rate_schedule.ExponentialDecay(
      learning_rate, decay_steps, decay_rate, staircase=staircase, name=name)
  if not tf.executing_eagerly():
    decayed_lr = decayed_lr(global_step)
  else:
    decayed_lr = functools.partial(decayed_lr, global_step)
  return decayed_lr


</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/legacy_learning_rate_decay.py" startline="368" endline="449" pcid="255">
def inverse_time_decay(learning_rate,
                       global_step,
                       decay_steps,
                       decay_rate,
                       staircase=False,
                       name=None):
  """Applies inverse time decay to the initial learning rate.

  When training a model, it is often recommended to lower the learning rate as
  the training progresses.  This function applies an inverse decay function
  to a provided initial learning rate.  It requires an `global_step` value to
  compute the decayed learning rate.  You can just pass a TensorFlow variable
  that you increment at each training step.

  The function returns the decayed learning rate.  It is computed as:

  ```python
  decayed_learning_rate = learning_rate / (1 + decay_rate * global_step /
  decay_step)
  ```

  or, if `staircase` is `True`, as:

  ```python
  decayed_learning_rate = learning_rate / (1 + decay_rate * floor(global_step /
  decay_step))
  ```

  Example: decay 1/t with a rate of 0.5:

  ```python
  ...
  global_step = tf.Variable(0, trainable=False)
  learning_rate = 0.1
  decay_steps = 1.0
  decay_rate = 0.5
  learning_rate = tf.compat.v1.train.inverse_time_decay(learning_rate,
  global_step,
  decay_steps, decay_rate)

  # Passing global_step to minimize() will increment it at each step.
  learning_step = (
      tf.compat.v1.train.GradientDescentOptimizer(learning_rate)
      .minimize(...my loss..., global_step=global_step)
  )
  ```

  Args:
    learning_rate: A scalar `float32` or `float64` `Tensor` or a Python number.
      The initial learning rate.
    global_step: A Python number. Global step to use for the decay computation.
      Must not be negative.
    decay_steps: How often to apply decay.
    decay_rate: A Python number.  The decay rate.
    staircase: Whether to apply decay in a discrete staircase, as opposed to
      continuous, fashion.
    name: String.  Optional name of the operation.  Defaults to
      'InverseTimeDecay'.

  Returns:
    A scalar `Tensor` of the same type as `learning_rate`.  The decayed
    learning rate.

  Raises:
    ValueError: if `global_step` is not supplied.

  @compatibility(eager)
  When eager execution is enabled, this function returns a function which in
  turn returns the decayed learning rate Tensor. This can be useful for changing
  the learning rate value across different invocations of optimizer functions.
  @end_compatibility
  """
  decayed_lr = learning_rate_schedule.InverseTimeDecay(
      learning_rate, decay_steps, decay_rate, staircase=staircase, name=name)

  if not tf.executing_eagerly():
    decayed_lr = decayed_lr(global_step)
  else:
    decayed_lr = functools.partial(decayed_lr, global_step)
  return decayed_lr


</source>
</class>

<class classid="26" nclones="5" nlines="19" similarity="78">
<source file="systems/keras-2.7.0/keras/optimizer_v2/legacy_learning_rate_decay.py" startline="179" endline="278" pcid="253">
def polynomial_decay(learning_rate,
                     global_step,
                     decay_steps,
                     end_learning_rate=0.0001,
                     power=1.0,
                     cycle=False,
                     name=None):
  """Applies a polynomial decay to the learning rate.

  It is commonly observed that a monotonically decreasing learning rate, whose
  degree of change is carefully chosen, results in a better performing model.
  This function applies a polynomial decay function to a provided initial
  `learning_rate` to reach an `end_learning_rate` in the given `decay_steps`.

  It requires a `global_step` value to compute the decayed learning rate.  You
  can just pass a TensorFlow variable that you increment at each training step.

  The function returns the decayed learning rate.  It is computed as:

  ```python
  global_step = min(global_step, decay_steps)
  decayed_learning_rate = (learning_rate - end_learning_rate) *
                          (1 - global_step / decay_steps) ^ (power) +
                          end_learning_rate

  ```

  If `cycle` is True then a multiple of `decay_steps` is used, the first one
  that is bigger than `global_steps`.

  ```python
  decay_steps = decay_steps * ceil(global_step / decay_steps)
  decayed_learning_rate = (learning_rate - end_learning_rate) *
                          (1 - global_step / decay_steps) ^ (power) +
                          end_learning_rate

  ```

  Example: decay from 0.1 to 0.01 in 10000 steps using sqrt (i.e. power=0.5):

  ```python
  ...
  global_step = tf.Variable(0, trainable=False)
  starter_learning_rate = 0.1
  end_learning_rate = 0.01
  decay_steps = 10000
  learning_rate = tf.compat.v1.train.polynomial_decay(starter_learning_rate,
  global_step,
                                            decay_steps, end_learning_rate,
                                            power=0.5)
  # Passing global_step to minimize() will increment it at each step.
  learning_step = (
      tf.compat.v1.train.GradientDescentOptimizer(learning_rate)
      .minimize(...my loss..., global_step=global_step)
  )
  ```

  Args:
    learning_rate: A scalar `float32` or `float64` `Tensor` or a Python number.
      The initial learning rate.
    global_step: A scalar `int32` or `int64` `Tensor` or a Python number. Global
      step to use for the decay computation.  Must not be negative.
    decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number. Must
      be positive.  See the decay computation above.
    end_learning_rate: A scalar `float32` or `float64` `Tensor` or a Python
      number.  The minimal end learning rate.
    power: A scalar `float32` or `float64` `Tensor` or a Python number.  The
      power of the polynomial. Defaults to linear, 1.0.
    cycle: A boolean, whether or not it should cycle beyond decay_steps.
    name: String.  Optional name of the operation. Defaults to
      'PolynomialDecay'.

  Returns:
    A scalar `Tensor` of the same type as `learning_rate`.  The decayed
    learning rate.

  Raises:
    ValueError: if `global_step` is not supplied.

  @compatibility(eager)
  When eager execution is enabled, this function returns a function which in
  turn returns the decayed learning rate Tensor. This can be useful for changing
  the learning rate value across different invocations of optimizer functions.
  @end_compatibility
  """
  decayed_lr = learning_rate_schedule.PolynomialDecay(
      learning_rate,
      decay_steps,
      end_learning_rate=end_learning_rate,
      power=power,
      cycle=cycle,
      name=name)

  if not tf.executing_eagerly():
    decayed_lr = decayed_lr(global_step)
  else:
    decayed_lr = functools.partial(decayed_lr, global_step)
  return decayed_lr


</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/legacy_learning_rate_decay.py" startline="280" endline="366" pcid="254">
def natural_exp_decay(learning_rate,
                      global_step,
                      decay_steps,
                      decay_rate,
                      staircase=False,
                      name=None):
  """Applies natural exponential decay to the initial learning rate.

  When training a model, it is often recommended to lower the learning rate as
  the training progresses.  This function applies an exponential decay function
  to a provided initial learning rate.  It requires an `global_step` value to
  compute the decayed learning rate.  You can just pass a TensorFlow variable
  that you increment at each training step.

  The function returns the decayed learning rate.  It is computed as:

  ```python
  decayed_learning_rate = learning_rate * exp(-decay_rate * global_step /
  decay_step)
  ```

  or, if `staircase` is `True`, as:

  ```python
  decayed_learning_rate = learning_rate * exp(-decay_rate * floor(global_step /
  decay_step))
  ```

  Example: decay exponentially with a base of 0.96:

  ```python
  ...
  global_step = tf.Variable(0, trainable=False)
  learning_rate = 0.1
  decay_steps = 5
  k = 0.5
  learning_rate = tf.compat.v1.train.natural_exp_decay(learning_rate,
  global_step,
                                             decay_steps, k)

  # Passing global_step to minimize() will increment it at each step.
  learning_step = (
      tf.compat.v1.train.GradientDescentOptimizer(learning_rate)
      .minimize(...my loss..., global_step=global_step)
  )
  ```

  Args:
    learning_rate: A scalar `float32` or `float64` `Tensor` or a Python number.
      The initial learning rate.
    global_step: A Python number. Global step to use for the decay computation.
      Must not be negative.
    decay_steps: How often to apply decay.
    decay_rate: A Python number.  The decay rate.
    staircase: Whether to apply decay in a discrete staircase, as opposed to
      continuous, fashion.
    name: String.  Optional name of the operation.  Defaults to
      'ExponentialTimeDecay'.

  Returns:
    A scalar `Tensor` of the same type as `learning_rate`.  The decayed
    learning rate.

  Raises:
    ValueError: if `global_step` is not supplied.

  @compatibility(eager)
  When eager execution is enabled, this function returns a function which in
  turn returns the decayed learning rate Tensor. This can be useful for changing
  the learning rate value across different invocations of optimizer functions.
  @end_compatibility
  """
  natural_exp_rate = tf.exp(tf.negative(decay_rate))
  decayed_lr = learning_rate_schedule.ExponentialDecay(
      learning_rate,
      decay_steps,
      natural_exp_rate,
      staircase=staircase,
      name=name)

  if not tf.executing_eagerly():
    decayed_lr = decayed_lr(global_step)
  else:
    decayed_lr = functools.partial(decayed_lr, global_step)
  return decayed_lr


</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/legacy_learning_rate_decay.py" startline="591" endline="674" pcid="258">
def linear_cosine_decay(learning_rate,
                        global_step,
                        decay_steps,
                        num_periods=0.5,
                        alpha=0.0,
                        beta=0.001,
                        name=None):
  """Applies linear cosine decay to the learning rate.

  Note that linear cosine decay is more aggressive than cosine decay and
  larger initial learning rates can typically be used.

  When training a model, it is often recommended to lower the learning rate as
  the training progresses.  This function applies a linear cosine decay function
  to a provided initial learning rate.  It requires a `global_step` value to
  compute the decayed learning rate.  You can just pass a TensorFlow variable
  that you increment at each training step.

  The function returns the decayed learning rate.  It is computed as:
  ```python
  global_step = min(global_step, decay_steps)
  linear_decay = (decay_steps - global_step) / decay_steps)
  cosine_decay = 0.5 * (
      1 + cos(pi * 2 * num_periods * global_step / decay_steps))
  decayed = (alpha + linear_decay) * cosine_decay + beta
  decayed_learning_rate = learning_rate * decayed
  ```

  Example usage:
  ```python
  decay_steps = 1000
  lr_decayed = linear_cosine_decay(learning_rate, global_step, decay_steps)
  ```

  Args:
    learning_rate: A scalar `float32` or `float64` Tensor or a Python number.
      The initial learning rate.
    global_step: A scalar `int32` or `int64` `Tensor` or a Python number. Global
      step to use for the decay computation.
    decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number. Number
      of steps to decay over.
    num_periods: Number of periods in the cosine part of the decay. See
      computation above.
    alpha: See computation above.
    beta: See computation above.
    name: String.  Optional name of the operation.  Defaults to
      'LinearCosineDecay'.

  Returns:
    A scalar `Tensor` of the same type as `learning_rate`.  The decayed
    learning rate.
  Raises:
    ValueError: if `global_step` is not supplied.

  References:
    Neural Optimizer Search with Reinforcement Learning:
      [Bello et al., 2017](http://proceedings.mlr.press/v70/bello17a.html)
      ([pdf](http://proceedings.mlr.press/v70/bello17a/bello17a.pdf))
    Stochastic Gradient Descent with Warm Restarts:
      [Loshchilov et al., 2017]
      (https://openreview.net/forum?id=Skq89Scxx&noteId=Skq89Scxx)
      ([pdf](https://openreview.net/pdf?id=Skq89Scxx))

  @compatibility(eager)
  When eager execution is enabled, this function returns a function which in
  turn returns the decayed learning rate Tensor. This can be useful for changing
  the learning rate value across different invocations of optimizer functions.
  @end_compatibility
  """
  decayed_lr = learning_rate_schedule.LinearCosineDecay(
      learning_rate,
      decay_steps,
      num_periods=num_periods,
      alpha=alpha,
      beta=beta,
      name=name)

  if not tf.executing_eagerly():
    decayed_lr = decayed_lr(global_step)
  else:
    decayed_lr = functools.partial(decayed_lr, global_step)
  return decayed_lr


</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/legacy_learning_rate_decay.py" startline="514" endline="589" pcid="257">
def cosine_decay_restarts(learning_rate,
                          global_step,
                          first_decay_steps,
                          t_mul=2.0,
                          m_mul=1.0,
                          alpha=0.0,
                          name=None):
  """Applies cosine decay with restarts to the learning rate.

  When training a model, it is often recommended to lower the learning rate as
  the training progresses.  This function applies a cosine decay function with
  restarts to a provided initial learning rate.  It requires a `global_step`
  value to compute the decayed learning rate.  You can just pass a TensorFlow
  variable that you increment at each training step.

  The function returns the decayed learning rate while taking into account
  possible warm restarts. The learning rate multiplier first decays
  from 1 to `alpha` for `first_decay_steps` steps. Then, a warm
  restart is performed. Each new warm restart runs for `t_mul` times more steps
  and with `m_mul` times smaller initial learning rate.

  Example usage:
  ```python
  first_decay_steps = 1000
  lr_decayed = cosine_decay_restarts(learning_rate, global_step,
                                     first_decay_steps)
  ```

  Args:
    learning_rate: A scalar `float32` or `float64` Tensor or a Python number.
      The initial learning rate.
    global_step: A scalar `int32` or `int64` `Tensor` or a Python number. Global
      step to use for the decay computation.
    first_decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number.
      Number of steps to decay over.
    t_mul: A scalar `float32` or `float64` `Tensor` or a Python number. Used to
      derive the number of iterations in the i-th period
    m_mul: A scalar `float32` or `float64` `Tensor` or a Python number.
      Used to derive the initial learning rate of the i-th period:
    alpha: A scalar `float32` or `float64` Tensor or a Python number. Minimum
      learning rate value as a fraction of the learning_rate.
    name: String. Optional name of the operation.  Defaults to 'SGDRDecay'.

  Returns:
    A scalar `Tensor` of the same type as `learning_rate`.  The decayed
    learning rate.
  Raises:
    ValueError: if `global_step` is not supplied.

  References:
    Stochastic Gradient Descent with Warm Restarts:
      [Loshchilov et al., 2017]
      (https://openreview.net/forum?id=Skq89Scxx&noteId=Skq89Scxx)
      ([pdf](https://openreview.net/pdf?id=Skq89Scxx))

  @compatibility(eager)
  When eager execution is enabled, this function returns a function which in
  turn returns the decayed learning rate Tensor. This can be useful for changing
  the learning rate value across different invocations of optimizer functions.
  @end_compatibility
  """
  decayed_lr = learning_rate_schedule.CosineDecayRestarts(
      learning_rate,
      first_decay_steps,
      t_mul=t_mul,
      m_mul=m_mul,
      alpha=alpha,
      name=name)

  if not tf.executing_eagerly():
    decayed_lr = decayed_lr(global_step)
  else:
    decayed_lr = functools.partial(decayed_lr, global_step)
  return decayed_lr


</source>
<source file="systems/keras-2.7.0/keras/optimizer_v2/legacy_learning_rate_decay.py" startline="676" endline="767" pcid="259">
def noisy_linear_cosine_decay(learning_rate,
                              global_step,
                              decay_steps,
                              initial_variance=1.0,
                              variance_decay=0.55,
                              num_periods=0.5,
                              alpha=0.0,
                              beta=0.001,
                              name=None):
  """Applies noisy linear cosine decay to the learning rate.

  Note that linear cosine decay is more aggressive than cosine decay and
  larger initial learning rates can typically be used.

  When training a model, it is often recommended to lower the learning rate as
  the training progresses.  This function applies a noisy linear
  cosine decay function to a provided initial learning rate.
  It requires a `global_step` value to compute the decayed learning rate.
  You can just pass a TensorFlow variable that you increment at each
  training step.

  The function returns the decayed learning rate.  It is computed as:
  ```python
  global_step = min(global_step, decay_steps)
  linear_decay = (decay_steps - global_step) / decay_steps)
  cosine_decay = 0.5 * (
      1 + cos(pi * 2 * num_periods * global_step / decay_steps))
  decayed = (alpha + linear_decay + eps_t) * cosine_decay + beta
  decayed_learning_rate = learning_rate * decayed
  ```
  where eps_t is 0-centered gaussian noise with variance
  initial_variance / (1 + global_step) ** variance_decay

  Example usage:
  ```python
  decay_steps = 1000
  lr_decayed = noisy_linear_cosine_decay(
    learning_rate, global_step, decay_steps)
  ```

  Args:
    learning_rate: A scalar `float32` or `float64` Tensor or a Python number.
      The initial learning rate.
    global_step: A scalar `int32` or `int64` `Tensor` or a Python number. Global
      step to use for the decay computation.
    decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number. Number
      of steps to decay over.
    initial_variance: initial variance for the noise. See computation above.
    variance_decay: decay for the noise's variance. See computation above.
    num_periods: Number of periods in the cosine part of the decay. See
      computation above.
    alpha: See computation above.
    beta: See computation above.
    name: String.  Optional name of the operation.  Defaults to
      'NoisyLinearCosineDecay'.

  Returns:
    A scalar `Tensor` of the same type as `learning_rate`.  The decayed
    learning rate.
  Raises:
    ValueError: if `global_step` is not supplied.

  References:
    Neural Optimizer Search with Reinforcement Learning:
      [Bello et al., 2017](http://proceedings.mlr.press/v70/bello17a.html)
      ([pdf](http://proceedings.mlr.press/v70/bello17a/bello17a.pdf))
    Stochastic Gradient Descent with Warm Restarts:
      [Loshchilov et al., 2017]
      (https://openreview.net/forum?id=Skq89Scxx&noteId=Skq89Scxx)
      ([pdf](https://openreview.net/pdf?id=Skq89Scxx))

  @compatibility(eager)
  When eager execution is enabled, this function returns a function which in
  turn returns the decayed learning rate Tensor. This can be useful for changing
  the learning rate value across different invocations of optimizer functions.
  @end_compatibility
  """
  decayed_lr = learning_rate_schedule.NoisyLinearCosineDecay(
      learning_rate,
      decay_steps,
      initial_variance=initial_variance,
      variance_decay=variance_decay,
      num_periods=num_periods,
      alpha=alpha,
      beta=beta,
      name=name)

  if not tf.executing_eagerly():
    decayed_lr = decayed_lr(global_step)
  else:
    decayed_lr = functools.partial(decayed_lr, global_step)
  return decayed_lr
</source>
</class>

<class classid="27" nclones="2" nlines="24" similarity="76">
<source file="systems/keras-2.7.0/keras/distribute/saved_model_test_base.py" startline="200" endline="229" pcid="328">
  def run_test_save_strategy_restore_no_strategy(self, model_and_input,
                                                 distribution, save_in_scope):
    """Save a model with DS, and restore it without DS."""

    saved_dir = os.path.join(self.get_temp_dir(), '1')

    with distribution.scope():
      model = model_and_input.get_model()
      x_train, y_train, x_predict = model_and_input.get_data()
      batch_size = model_and_input.get_batch_size()

      self._train_model(model, x_train, y_train, batch_size)
      predict_dataset = self._get_predict_dataset(x_predict, batch_size)
      result_before_save = self._predict_with_model(
          distribution, model, predict_dataset)

    if save_in_scope:
      with distribution.scope():
        self._save_model(model, saved_dir)
    else:
      self._save_model(model, saved_dir)

    load_result = self._load_and_run_model(
        distribution=None,
        saved_dir=saved_dir,
        predict_dataset=predict_dataset)

    tolerance = get_tolerance(distribution, None)
    self.assertAllClose(result_before_save, load_result, atol=tolerance)

</source>
<source file="systems/keras-2.7.0/keras/distribute/saved_model_test_base.py" startline="230" endline="263" pcid="329">
  def run_test_save_strategy_restore_strategy(self, model_and_input,
                                              distribution_for_saving,
                                              distribution_for_restoring,
                                              save_in_scope):
    """Save a model with DS, and restore it with potentially different DS."""
    saved_dir = os.path.join(self.get_temp_dir(), '2')

    with distribution_for_saving.scope():
      model = model_and_input.get_model()
      x_train, y_train, x_predict = model_and_input.get_data()
      batch_size = model_and_input.get_batch_size()

      self._train_model(model, x_train, y_train, batch_size)
      predict_dataset = self._get_predict_dataset(x_predict, batch_size)
      result_before_save = self._predict_with_model(
          distribution_for_saving, model, predict_dataset)

    if save_in_scope:
      with distribution_for_saving.scope():
        self._save_model(model, saved_dir)
    else:
      self._save_model(model, saved_dir)

    with distribution_for_restoring.scope():

      load_result = self._load_and_run_model(
          distribution=distribution_for_restoring,
          saved_dir=saved_dir,
          predict_dataset=predict_dataset)

    tolerance = get_tolerance(distribution_for_saving,
                              distribution_for_restoring)
    self.assertAllClose(result_before_save, load_result, atol=tolerance)

</source>
</class>

<class classid="28" nclones="2" nlines="13" similarity="84">
<source file="systems/keras-2.7.0/keras/distribute/sidecar_evaluator_test.py" startline="115" endline="131" pcid="354">
  def testIterationsNotSavedWillRaiseError(self, model_type):
    model = _test_model_builder(
        model_type=model_type, compile_model=False, build_model=True)

    checkpoint_dir = self.get_temp_dir()
    checkpoint = tf.train.Checkpoint(model=model)
    checkpoint_manager = tf.train.CheckpointManager(
        checkpoint, checkpoint_dir, max_to_keep=2)
    checkpoint_manager.save()

    sidecar_evaluator = sidecar_evaluator_lib.SidecarEvaluator(
        model, data=None, checkpoint_dir=checkpoint_dir)
    with self.assertRaisesRegex(
        RuntimeError, '`iterations` cannot be loaded '
        'from the checkpoint file.'):
      sidecar_evaluator.start()

</source>
<source file="systems/keras-2.7.0/keras/distribute/sidecar_evaluator_test.py" startline="136" endline="150" pcid="355">
  def testModelNotBuiltRaiseError(self, model_type):
    model = _test_model_builder(
        model_type=model_type, compile_model=False, build_model=False)

    checkpoint_dir = self.get_temp_dir()
    checkpoint = tf.train.Checkpoint(model=model)
    checkpoint_manager = tf.train.CheckpointManager(
        checkpoint, checkpoint_dir, max_to_keep=2)
    checkpoint_manager.save()

    sidecar_evaluator = sidecar_evaluator_lib.SidecarEvaluator(
        model, data=None, checkpoint_dir=checkpoint_dir)
    with self.assertRaisesRegex(AssertionError, 'Nothing to load.'):
      sidecar_evaluator.start()

</source>
</class>

<class classid="29" nclones="4" nlines="11" similarity="72">
<source file="systems/keras-2.7.0/keras/distribute/dataset_creator_model_fit_test.py" startline="72" endline="83" pcid="362">
  def testModelFitWithNumpyData(self, strategy):
    x = np.random.rand(100, 10)
    y = np.random.rand(100, 1)
    model = self._model_fit(
        strategy,
        x=x,
        y=y,
        batch_size=1,
        validation_data=(x, y),
    )
    self.assertEqual(model.optimizer.iterations, 100)

</source>
<source file="systems/keras-2.7.0/keras/distribute/dataset_creator_model_fit_test.py" startline="84" endline="95" pcid="363">
  def testModelFitWithTensorData(self, strategy):
    x = tf.random.uniform((100, 10))
    y = tf.random.uniform((100,))
    model = self._model_fit(
        strategy,
        x=x,
        y=y,
        batch_size=1,
        validation_data=(x, y),
    )
    self.assertEqual(model.optimizer.iterations, 100)

</source>
<source file="systems/keras-2.7.0/keras/distribute/dataset_creator_model_fit_test.py" startline="120" endline="130" pcid="369">
  def testModelEvaluateWithNumpyData(self, strategy):
    x = np.random.rand(100, 10)
    y = np.random.rand(100, 1)
    self._model_evaluate(
        strategy,
        x=x,
        y=y,
        batch_size=1,
    )
    self.assertGreaterEqual(self._accuracy_metric.result(), 0.0)

</source>
<source file="systems/keras-2.7.0/keras/distribute/dataset_creator_model_fit_test.py" startline="131" endline="141" pcid="370">
  def testModelEvaluateWithTensorData(self, strategy):
    x = tf.random.uniform((100, 10))
    y = tf.random.uniform((100,))
    self._model_evaluate(
        strategy,
        x=x,
        y=y,
        batch_size=1,
    )
    self.assertGreaterEqual(self._accuracy_metric.result(), 0.0)

</source>
</class>

<class classid="30" nclones="3" nlines="10" similarity="100">
<source file="systems/keras-2.7.0/keras/distribute/optimizer_combinations.py" startline="71" endline="83" pcid="392">
def distributions_and_v1_optimizers():
  """A common set of combination with DistributionStrategies and Optimizers."""
  return tf.__internal__.test.combinations.combine(
      distribution=[
          tf.__internal__.distribute.combinations.one_device_strategy,
          tf.__internal__.distribute.combinations.mirrored_strategy_with_gpu_and_cpu,
          tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus,
          tf.__internal__.distribute.combinations
          .mirrored_strategy_with_two_gpus_no_merge_call,
      ],
      optimizer_fn=optimizers_v1)


</source>
<source file="systems/keras-2.7.0/keras/distribute/optimizer_combinations.py" startline="84" endline="96" pcid="393">
def distributions_and_v2_optimizers():
  """A common set of combination with DistributionStrategies and Optimizers."""
  return tf.__internal__.test.combinations.combine(
      distribution=[
          tf.__internal__.distribute.combinations.one_device_strategy,
          tf.__internal__.distribute.combinations.mirrored_strategy_with_gpu_and_cpu,
          tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus,
          tf.__internal__.distribute.combinations
          .mirrored_strategy_with_two_gpus_no_merge_call,
      ],
      optimizer_fn=optimizers_v2)


</source>
<source file="systems/keras-2.7.0/keras/distribute/optimizer_combinations.py" startline="97" endline="107" pcid="394">
def distributions_and_v1_and_v2_optimizers():
  """A common set of combination with DistributionStrategies and Optimizers."""
  return tf.__internal__.test.combinations.combine(
      distribution=[
          tf.__internal__.distribute.combinations.one_device_strategy,
          tf.__internal__.distribute.combinations.mirrored_strategy_with_gpu_and_cpu,
          tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus,
          tf.__internal__.distribute.combinations
          .mirrored_strategy_with_two_gpus_no_merge_call,
      ],
      optimizer_fn=optimizers_v1_and_v2)
</source>
</class>

<class classid="31" nclones="2" nlines="23" similarity="73">
<source file="systems/keras-2.7.0/keras/distribute/keras_embedding_model_correctness_test.py" startline="30" endline="55" pcid="402">
  def get_model(self,
                max_words=10,
                initial_weights=None,
                distribution=None,
                input_shapes=None):
    del input_shapes
    with keras_correctness_test_base.MaybeDistributionScope(distribution):
      word_ids = keras.layers.Input(
          shape=(max_words,), dtype=np.int32, name='words')
      word_embed = keras.layers.Embedding(input_dim=20, output_dim=10)(word_ids)
      if self.use_distributed_dense:
        word_embed = keras.layers.TimeDistributed(keras.layers.Dense(4))(
            word_embed)
      avg = keras.layers.GlobalAveragePooling1D()(word_embed)
      preds = keras.layers.Dense(2, activation='softmax')(avg)
      model = keras.Model(inputs=[word_ids], outputs=[preds])

      if initial_weights:
        model.set_weights(initial_weights)

      model.compile(
          optimizer=gradient_descent_keras.SGD(learning_rate=0.1),
          loss='sparse_categorical_crossentropy',
          metrics=['sparse_categorical_accuracy'])
    return model

</source>
<source file="systems/keras-2.7.0/keras/distribute/keras_rnn_model_correctness_test.py" startline="37" endline="66" pcid="750">
  def get_model(self,
                max_words=10,
                initial_weights=None,
                distribution=None,
                input_shapes=None):
    del input_shapes
    rnn_cls = self._get_layer_class()

    with keras_correctness_test_base.MaybeDistributionScope(distribution):
      word_ids = keras.layers.Input(
          shape=(max_words,), dtype=np.int32, name='words')
      word_embed = keras.layers.Embedding(input_dim=20, output_dim=10)(word_ids)
      rnn_embed = rnn_cls(units=4, return_sequences=False)(word_embed)

      dense_output = keras.layers.Dense(2)(rnn_embed)
      preds = keras.layers.Softmax(dtype='float32')(dense_output)
      model = keras.Model(inputs=[word_ids], outputs=[preds])

      if initial_weights:
        model.set_weights(initial_weights)

      optimizer_fn = gradient_descent_keras.SGD

      model.compile(
          optimizer=optimizer_fn(learning_rate=0.1),
          loss='sparse_categorical_crossentropy',
          metrics=['sparse_categorical_accuracy'])
    return model


</source>
</class>

<class classid="32" nclones="2" nlines="26" similarity="81">
<source file="systems/keras-2.7.0/keras/distribute/keras_premade_models_test.py" startline="92" endline="118" pcid="434">
  def test_linear_model(self, distribution, use_dataset_creator, data_fn):
    if ((not use_dataset_creator) and isinstance(
        distribution, tf.distribute.experimental.ParameterServerStrategy)):
      self.skipTest(
          'Parameter Server strategy requires dataset creator to be used in '
          'model.fit.')
    if (not tf.__internal__.tf2.enabled() and use_dataset_creator
        and isinstance(distribution,
                       tf.distribute.experimental.ParameterServerStrategy)):
      self.skipTest(
          'Parameter Server strategy with dataset creator needs to be run when '
          'eager execution is enabled.')
    with distribution.scope():
      model = linear.LinearModel()
      opt = gradient_descent.SGD(learning_rate=0.1)
      model.compile(opt, 'mse')
      if use_dataset_creator:
        x = dataset_creator.DatasetCreator(dataset_fn)
        hist = model.fit(x, epochs=5, steps_per_epoch=INPUT_SIZE)
      else:
        if data_fn == 'numpy':
          inputs, output = get_numpy()
          hist = model.fit(inputs, output, epochs=5)
        else:
          hist = model.fit(get_dataset(), epochs=5)
        self.assertLess(hist.history['loss'][4], 0.2)

</source>
<source file="systems/keras-2.7.0/keras/distribute/keras_premade_models_test.py" startline="121" endline="152" pcid="435">
  def test_wide_deep_model(self, distribution, use_dataset_creator, data_fn):
    if ((not use_dataset_creator) and isinstance(
        distribution, tf.distribute.experimental.ParameterServerStrategy)):
      self.skipTest(
          'Parameter Server strategy requires dataset creator to be used in '
          'model.fit.')
    if (not tf.__internal__.tf2.enabled() and use_dataset_creator
        and isinstance(distribution,
                       tf.distribute.experimental.ParameterServerStrategy)):
      self.skipTest(
          'Parameter Server strategy with dataset creator needs to be run when '
          'eager execution is enabled.')
    with distribution.scope():
      linear_model = linear.LinearModel(units=1)
      dnn_model = sequential.Sequential([core.Dense(units=1)])
      wide_deep_model = wide_deep.WideDeepModel(linear_model, dnn_model)
      linear_opt = gradient_descent.SGD(learning_rate=0.05)
      dnn_opt = adagrad.Adagrad(learning_rate=0.1)
      wide_deep_model.compile(optimizer=[linear_opt, dnn_opt], loss='mse')

      if use_dataset_creator:
        x = dataset_creator.DatasetCreator(dataset_fn)
        hist = wide_deep_model.fit(x, epochs=5, steps_per_epoch=INPUT_SIZE)
      else:
        if data_fn == 'numpy':
          inputs, output = get_numpy()
          hist = wide_deep_model.fit(inputs, output, epochs=5)
        else:
          hist = wide_deep_model.fit(get_dataset(), epochs=5)
      self.assertLess(hist.history['loss'][4], 0.2)


</source>
</class>

<class classid="33" nclones="4" nlines="19" similarity="73">
<source file="systems/keras-2.7.0/keras/distribute/custom_training_loop_models_test.py" startline="55" endline="78" pcid="439">
  def test_single_keras_layer_run(self, distribution):
    dataset = _get_dataset()
    input_iterator = iter(distribution.experimental_distribute_dataset(dataset))

    with distribution.scope():
      model = keras.layers.Dense(4, name="dense")

    @tf.function
    def train_step(iterator):
      def step_fn(inputs):
        images, targets = inputs
        with tf.GradientTape() as tape:
          outputs = model(images)
          loss = keras.losses.mean_squared_error(targets, outputs)
        grads = tape.gradient(loss, model.variables)
        return grads

      outputs = distribution.run(
          step_fn, args=(next(iterator),))
      return tf.nest.map_structure(distribution.experimental_local_results,
                                outputs)

    train_step(input_iterator)

</source>
<source file="systems/keras-2.7.0/keras/distribute/custom_training_loop_models_test.py" startline="79" endline="104" pcid="442">
  def test_keras_model_optimizer_run(self, distribution):
    dataset = _get_dataset()
    input_iterator = iter(distribution.experimental_distribute_dataset(dataset))

    with distribution.scope():
      model = _get_model()
      optimizer = keras.optimizer_v2.rmsprop.RMSprop()

    @tf.function
    def train_step(replicated_inputs):
      def step_fn(inputs):
        images, targets = inputs
        with tf.GradientTape() as tape:
          outputs = model(images)
          loss = keras.losses.mean_squared_error(targets, outputs)
        grads = tape.gradient(loss, model.variables)
        optimizer.apply_gradients(zip(grads, model.variables))
        return loss

      outputs = distribution.run(step_fn, args=(replicated_inputs,))
      return tf.nest.map_structure(distribution.experimental_local_results,
                                outputs)

    for x in input_iterator:
      train_step(x)

</source>
<source file="systems/keras-2.7.0/keras/distribute/custom_training_loop_models_test.py" startline="142" endline="165" pcid="451">
  def test_keras_model_optimizer_run_loop(self, distribution):
    dataset = _get_dataset()
    input_iterator = iter(distribution.experimental_distribute_dataset(dataset))

    with distribution.scope():
      model = _get_model()
      optimizer = keras.optimizer_v2.rmsprop.RMSprop()

    @tf.function
    def train_step(iterator):
      def step_fn(inputs):
        images, targets = inputs
        with tf.GradientTape() as tape:
          outputs = model(images)
          loss = keras.losses.mean_squared_error(targets, outputs)
        grads = tape.gradient(loss, model.variables)
        optimizer.apply_gradients(zip(grads, model.variables))
        return loss

      for _ in tf.range(4):
        distribution.run(step_fn, args=(next(iterator),))

    train_step(input_iterator)

</source>
<source file="systems/keras-2.7.0/keras/distribute/custom_training_loop_models_test.py" startline="390" endline="414" pcid="479">
  def test_customized_tf_module_run(self, distribution):
    dataset = _get_dataset()
    input_iterator = iter(distribution.experimental_distribute_dataset(dataset))

    with distribution.scope():
      model = CustomModel()

    @tf.function
    def train_step(iterator):

      def step_fn(inputs):
        images, targets = inputs
        with tf.GradientTape() as tape:
          outputs = model(images)
          loss = keras.losses.mean_squared_error(targets, outputs)
        grads = tape.gradient(loss, model.variables)
        return grads

      outputs = distribution.run(
          step_fn, args=(next(iterator),))
      return tf.nest.map_structure(distribution.experimental_local_results,
                                outputs)

    train_step(input_iterator)

</source>
</class>

<class classid="34" nclones="7" nlines="11" similarity="75">
<source file="systems/keras-2.7.0/keras/distribute/custom_training_loop_models_test.py" startline="63" endline="76" pcid="440">
    def train_step(iterator):
      def step_fn(inputs):
        images, targets = inputs
        with tf.GradientTape() as tape:
          outputs = model(images)
          loss = keras.losses.mean_squared_error(targets, outputs)
        grads = tape.gradient(loss, model.variables)
        return grads

      outputs = distribution.run(
          step_fn, args=(next(iterator),))
      return tf.nest.map_structure(distribution.experimental_local_results,
                                outputs)

</source>
<source file="systems/keras-2.7.0/keras/distribute/custom_training_loop_models_test.py" startline="503" endline="516" pcid="492">
    def train_step(iterator):
      def step_fn(inputs):
        images, targets = inputs
        with tf.GradientTape() as tape:
          outputs = model(images)
          loss = keras.losses.mean_squared_error(targets, outputs)
        grads = tape.gradient(loss, model.variables)
        return grads

      outputs = distribution.run(
          step_fn, args=(next(iterator),))
      return tf.nest.map_structure(distribution.experimental_local_results,
                                outputs)

</source>
<source file="systems/keras-2.7.0/keras/distribute/custom_training_loop_models_test.py" startline="126" endline="139" pcid="449">
    def train_step(iterator):
      def step_fn(inputs):
        images, targets = inputs
        with tf.GradientTape() as tape:
          outputs = model(images)
          loss = keras.losses.mean_squared_error(targets, outputs)
        grads = tape.gradient(loss, model.variables)
        optimizer.apply_gradients(zip(grads, model.variables))
        return loss

      outputs = distribution.run(step_fn, args=(next(iterator),))
      return tf.nest.map_structure(distribution.experimental_local_results,
                                outputs)

</source>
<source file="systems/keras-2.7.0/keras/distribute/custom_training_loop_models_test.py" startline="398" endline="412" pcid="480">
    def train_step(iterator):

      def step_fn(inputs):
        images, targets = inputs
        with tf.GradientTape() as tape:
          outputs = model(images)
          loss = keras.losses.mean_squared_error(targets, outputs)
        grads = tape.gradient(loss, model.variables)
        return grads

      outputs = distribution.run(
          step_fn, args=(next(iterator),))
      return tf.nest.map_structure(distribution.experimental_local_results,
                                outputs)

</source>
<source file="systems/keras-2.7.0/keras/distribute/custom_training_loop_models_test.py" startline="88" endline="101" pcid="443">
    def train_step(replicated_inputs):
      def step_fn(inputs):
        images, targets = inputs
        with tf.GradientTape() as tape:
          outputs = model(images)
          loss = keras.losses.mean_squared_error(targets, outputs)
        grads = tape.gradient(loss, model.variables)
        optimizer.apply_gradients(zip(grads, model.variables))
        return loss

      outputs = distribution.run(step_fn, args=(replicated_inputs,))
      return tf.nest.map_structure(distribution.experimental_local_results,
                                outputs)

</source>
<source file="systems/keras-2.7.0/keras/distribute/custom_training_loop_models_test.py" startline="151" endline="163" pcid="452">
    def train_step(iterator):
      def step_fn(inputs):
        images, targets = inputs
        with tf.GradientTape() as tape:
          outputs = model(images)
          loss = keras.losses.mean_squared_error(targets, outputs)
        grads = tape.gradient(loss, model.variables)
        optimizer.apply_gradients(zip(grads, model.variables))
        return loss

      for _ in tf.range(4):
        distribution.run(step_fn, args=(next(iterator),))

</source>
<source file="systems/keras-2.7.0/keras/distribute/custom_training_loop_models_test.py" startline="183" endline="194" pcid="455">
    def train_step(iterator):
      def step_fn(inputs):
        images, targets = inputs
        with tf.GradientTape() as tape:
          outputs = model(images, training=True)
          loss = keras.losses.mean_squared_error(targets, outputs)
        grads = tape.gradient(loss, model.variables)
        optimizer.apply_gradients(zip(grads, model.variables))
        return loss

      distribution.run(step_fn, args=(next(iterator),))

</source>
</class>

<class classid="35" nclones="2" nlines="37" similarity="89">
<source file="systems/keras-2.7.0/keras/distribute/mirrored_strategy_test.py" startline="81" endline="130" pcid="509">
  def testTrainAndServeWithKPL(self, distribution):
    use_adapt = False
    test_utils_obj = kpl_test_utils.DistributeKplTestUtils()
    with distribution.scope():
      feature_mapper, label_mapper = test_utils_obj.define_kpls_for_training(
          use_adapt)
      model = test_utils_obj.define_model()
      optimizer = rmsprop.RMSprop(learning_rate=0.1)
      accuracy = keras.metrics.Accuracy()

      def dataset_fn(_):
        return test_utils_obj.dataset_fn(feature_mapper, label_mapper)

      @tf.function
      def train_step(iterator):
        """The step function for one training step."""

        def step_fn(inputs):
          """The computation to run on each replica(GPU)."""
          features, labels = inputs
          with tf.GradientTape() as tape:
            pred = model(features, training=True)
            loss = keras.losses.binary_crossentropy(labels, pred)
            loss = tf.nn.compute_average_loss(loss)
          grads = tape.gradient(loss, model.trainable_variables)
          optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))

          actual_pred = tf.cast(tf.greater(pred, 0.5), tf.int64)
          accuracy.update_state(labels, actual_pred)

        distribution.run(step_fn, args=(next(iterator),))

      distributed_dataset = distribution.distribute_datasets_from_function(
          dataset_fn)
      distributed_iterator = iter(distributed_dataset)
      num_epochs = 4
      num_steps = 7
      for _ in range(num_epochs):
        accuracy.reset_state()
        for _ in range(num_steps):
          train_step(distributed_iterator)

      self.assertGreater(accuracy.result().numpy(), 0.5)
      self.assertEqual(optimizer.iterations.numpy(), num_epochs * num_steps)

    # Test save/load/serving the trained model.
    test_utils_obj.test_save_load_serving_model(
        model, feature_mapper, test_utils_obj.define_reverse_lookup_layer())


</source>
<source file="systems/keras-2.7.0/keras/integration_test/central_storage_strategy_test.py" startline="35" endline="84" pcid="1640">
  def testTrainAndServeWithKPL(self, distribution):
    use_adapt = False
    test_utils_obj = kpl_test_utils.DistributeKplTestUtils()
    with distribution.scope():
      feature_mapper, label_mapper = test_utils_obj.define_kpls_for_training(
          use_adapt)
      model = test_utils_obj.define_model()
      optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.1)
      accuracy = tf.keras.metrics.Accuracy()

      def dataset_fn(_):
        return test_utils_obj.dataset_fn(feature_mapper, label_mapper)

      @tf.function
      def train_step(iterator):
        """The step function for one training step."""

        def step_fn(inputs):
          """The computation to run on each replica."""
          features, labels = inputs
          with tf.GradientTape() as tape:
            pred = model(features, training=True)
            loss = tf.keras.losses.binary_crossentropy(labels, pred)
            loss = tf.nn.compute_average_loss(loss)
          grads = tape.gradient(loss, model.trainable_variables)
          optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))

          actual_pred = tf.cast(tf.math.greater(pred, 0.5), tf.dtypes.int64)
          accuracy.update_state(labels, actual_pred)

        distribution.run(step_fn, args=(next(iterator),))

      distributed_dataset = distribution.distribute_datasets_from_function(
          dataset_fn)
      distributed_iterator = iter(distributed_dataset)
      num_epochs = 4
      num_steps = 7
      for _ in range(num_epochs):
        accuracy.reset_state()
        for _ in range(num_steps):
          train_step(distributed_iterator)

      self.assertGreater(accuracy.result().numpy(), 0.5)
      self.assertEqual(optimizer.iterations.numpy(), num_epochs * num_steps)

    # Test save/load/serving the trained model.
    test_utils_obj.test_save_load_serving_model(
        model, feature_mapper, test_utils_obj.define_reverse_lookup_layer())


</source>
</class>

<class classid="36" nclones="3" nlines="12" similarity="83">
<source file="systems/keras-2.7.0/keras/distribute/mirrored_strategy_test.py" startline="95" endline="112" pcid="511">
      def train_step(iterator):
        """The step function for one training step."""

        def step_fn(inputs):
          """The computation to run on each replica(GPU)."""
          features, labels = inputs
          with tf.GradientTape() as tape:
            pred = model(features, training=True)
            loss = keras.losses.binary_crossentropy(labels, pred)
            loss = tf.nn.compute_average_loss(loss)
          grads = tape.gradient(loss, model.trainable_variables)
          optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))

          actual_pred = tf.cast(tf.greater(pred, 0.5), tf.int64)
          accuracy.update_state(labels, actual_pred)

        distribution.run(step_fn, args=(next(iterator),))

</source>
<source file="systems/keras-2.7.0/keras/integration_test/tpu_strategy_test.py" startline="167" endline="184" pcid="1657">
      def train_step(iterator):
        """The step function for one training step."""

        def step_fn(inputs):
          """The computation to run on each TPU device."""
          features, labels = inputs
          with tf.GradientTape() as tape:
            pred = model(features, training=True)
            loss = tf.keras.losses.binary_crossentropy(labels, pred)
            loss = tf.nn.compute_average_loss(loss)
          grads = tape.gradient(loss, model.trainable_variables)
          optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))

          actual_pred = tf.cast(tf.math.greater(pred, 0.5), tf.dtypes.int64)
          accuracy.update_state(labels, actual_pred)

        strategy.run(step_fn, args=(next(iterator),))

</source>
<source file="systems/keras-2.7.0/keras/integration_test/central_storage_strategy_test.py" startline="49" endline="66" pcid="1642">
      def train_step(iterator):
        """The step function for one training step."""

        def step_fn(inputs):
          """The computation to run on each replica."""
          features, labels = inputs
          with tf.GradientTape() as tape:
            pred = model(features, training=True)
            loss = tf.keras.losses.binary_crossentropy(labels, pred)
            loss = tf.nn.compute_average_loss(loss)
          grads = tape.gradient(loss, model.trainable_variables)
          optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))

          actual_pred = tf.cast(tf.math.greater(pred, 0.5), tf.dtypes.int64)
          accuracy.update_state(labels, actual_pred)

        distribution.run(step_fn, args=(next(iterator),))

</source>
</class>

<class classid="37" nclones="2" nlines="12" similarity="100">
<source file="systems/keras-2.7.0/keras/distribute/dataset_creator_model_fit_ps_only_test.py" startline="54" endline="69" pcid="590">
  def testModelFitErrorOnBatchLevelCallbacks(self, strategy,
                                             use_dataset_creator):

    class BatchLevelCallback(callbacks_lib.Callback):

      def on_train_batch_end(self, batch, logs=None):
        pass

    with self.assertRaisesRegex(ValueError,
                                "Batch-level `Callback`s are not supported"):
      callbacks = [BatchLevelCallback()]
      self._model_fit(
          strategy,
          callbacks=callbacks,
          use_dataset_creator=use_dataset_creator)

</source>
<source file="systems/keras-2.7.0/keras/distribute/dataset_creator_model_fit_ps_only_test.py" startline="117" endline="133" pcid="598">
  def testModelEvaluateErrorOnBatchLevelCallbacks(self, strategy,
                                                  use_dataset_creator):

    class BatchLevelCallback(callbacks_lib.Callback):

      def on_train_batch_end(self, batch, logs=None):
        pass

    with self.assertRaisesRegex(ValueError,
                                "Batch-level `Callback`s are not supported"):
      callbacks = [BatchLevelCallback()]
      self._model_evaluate(
          strategy,
          callbacks=callbacks,
          use_dataset_creator=use_dataset_creator)


</source>
</class>

<class classid="38" nclones="2" nlines="18" similarity="78">
<source file="systems/keras-2.7.0/keras/distribute/keras_utils_test.py" startline="122" endline="142" pcid="633">
  def test_callbacks_in_eval(self, distribution):
    with distribution.scope():
      model = keras_test_lib.get_model()
      model.compile(
          optimizer='sgd',
          loss='mse',
          metrics=['mae'])

    dataset = keras_test_lib.get_dataset(distribution)
    counter = Counter()

    model.evaluate(dataset, steps=5, callbacks=[counter])

    self.assertDictEqual(
        counter.method_counts, {
            'on_test_batch_begin': 5,
            'on_test_batch_end': 5,
            'on_test_begin': 1,
            'on_test_end': 1
        })

</source>
<source file="systems/keras-2.7.0/keras/distribute/keras_utils_test.py" startline="146" endline="170" pcid="634">
  def test_callbacks_in_predict(self, distribution):
    with distribution.scope():
      model = keras_test_lib.get_model()
      model.compile(
          optimizer='sgd',
          loss='mse',
          metrics=['mae'])

    dataset = keras_test_lib.get_dataset(distribution)
    counter = Counter()

    model.predict(
        keras_test_lib.get_predict_dataset(dataset),
        steps=5,
        callbacks=[counter])

    self.assertDictEqual(
        counter.method_counts, {
            'on_predict_batch_begin': 5,
            'on_predict_batch_end': 5,
            'on_predict_begin': 1,
            'on_predict_end': 1
        })


</source>
</class>

<class classid="39" nclones="2" nlines="12" similarity="83">
<source file="systems/keras-2.7.0/keras/distribute/keras_utils_test.py" startline="179" endline="196" pcid="635">
  def test_validating_dataset_input_tensors_with_shape_mismatch(
      self, distribution):
    with self.cached_session():
      a = tf.constant([1, 2], shape=(1, 2))
      b = tf.constant([[1, 2], [1, 2]], shape=(2, 2))
      x = tf.distribute.DistributedValues((a, b))
      y = tf.distribute.DistributedValues((a, a))
      # Removed device and input tensor shape details from the error message
      # since the order of the device and the corresponding input tensor shape
      # is not deterministic over different runs.
      with self.assertRaisesRegex(
          ValueError, 'Input tensor shapes do not match for '
          'distributed tensor inputs '
          'DistributedValues:.+'):
        with distribution.scope():
          distributed_training_utils_v1.validate_distributed_dataset_inputs(
              distribution, x, y)

</source>
<source file="systems/keras-2.7.0/keras/distribute/keras_utils_test.py" startline="203" endline="220" pcid="636">
  def test_validating_dataset_input_tensors_with_dtype_mismatch(
      self, distribution):
    with self.cached_session():
      a = tf.constant([1, 2], shape=(1, 2), dtype=tf.int32)
      b = tf.constant([1, 2], shape=(1, 2), dtype=tf.float64)
      x = tf.distribute.DistributedValues((a, b))
      y = tf.distribute.DistributedValues((a, a))
      # Removed device and input tensor dtype details from the error message
      # since the order of the device and the corresponding input tensor dtype
      # is not deterministic over different runs.
      with self.assertRaisesRegex(
          ValueError, 'Input tensor dtypes do not match for '
          'distributed tensor inputs '
          'DistributedValues:.+'):
        with distribution.scope():
          distributed_training_utils_v1.validate_distributed_dataset_inputs(
              distribution, x, y)

</source>
</class>

<class classid="40" nclones="2" nlines="29" similarity="93">
<source file="systems/keras-2.7.0/keras/distribute/keras_utils_test.py" startline="390" endline="425" pcid="644">
  def test_batchnorm_correctness(self, distribution, fused, optimizer):
    with self.cached_session():
      with distribution.scope():
        model = keras.models.Sequential()
        norm = keras.layers.BatchNormalization(
            input_shape=(
                10,
                20,
                30,
            ), momentum=0.8, fused=fused)
        model.add(norm)
        model.compile(
            loss='mse',
            optimizer=optimizer())

      # centered on 5.0, variance 10.0
      x = np.random.normal(loc=5.0, scale=10.0, size=(1000, 10, 20, 30))
      x = x.astype('float32')
      dataset = tf.data.Dataset.from_tensor_slices((x, x))
      dataset = dataset.repeat(100)
      dataset = keras_test_lib.batch_wrapper(dataset, 32, distribution)

      predict_dataset = tf.data.Dataset.from_tensor_slices(x)
      predict_dataset = predict_dataset.repeat(100)
      predict_dataset = keras_test_lib.batch_wrapper(predict_dataset, 32,
                                                     distribution)

      model.fit(dataset, epochs=4, verbose=0, steps_per_epoch=10)
      out = model.predict(predict_dataset, steps=2)
      out -= keras.backend.eval(norm.beta)
      out /= keras.backend.eval(norm.gamma)
      np.testing.assert_allclose(out.mean(), 0.0, atol=1e-1)
      np.testing.assert_allclose(out.std(), 1.0, atol=1e-1)

# TODO(b/146181571): Enable this for all distribution strategies once
# DistributedVariable.assign() returns a variable for MirroredStrategy.
</source>
<source file="systems/keras-2.7.0/keras/distribute/keras_utils_test.py" startline="432" endline="466" pcid="645">
  def test_batchnorm_correctness_with_renorm(self, distribution, optimizer):
    with self.cached_session():
      with distribution.scope():
        model = keras.models.Sequential()
        norm = keras.layers.BatchNormalization(
            input_shape=(
                10,
                20,
                30,
            ), momentum=0.8, fused=False, renorm=True)
        model.add(norm)
        model.compile(
            loss='mse',
            optimizer=optimizer())

      # centered on 5.0, variance 10.0
      x = np.random.normal(loc=5.0, scale=10.0, size=(1000, 10, 20, 30))
      x = x.astype('float32')
      dataset = tf.data.Dataset.from_tensor_slices((x, x))
      dataset = dataset.repeat(100)
      dataset = keras_test_lib.batch_wrapper(dataset, 32, distribution)

      predict_dataset = tf.data.Dataset.from_tensor_slices(x)
      predict_dataset = predict_dataset.repeat(100)
      predict_dataset = keras_test_lib.batch_wrapper(predict_dataset, 32,
                                                     distribution)

      model.fit(dataset, epochs=4, verbose=0, steps_per_epoch=10)
      out = model.predict(predict_dataset, steps=2)
      out -= keras.backend.eval(norm.beta)
      out /= keras.backend.eval(norm.gamma)
      np.testing.assert_allclose(out.mean(), 0.0, atol=1e-1)
      np.testing.assert_allclose(out.std(), 1.0, atol=1e-1)


</source>
</class>

<class classid="41" nclones="2" nlines="21" similarity="78">
<source file="systems/keras-2.7.0/keras/distribute/keras_utils_test.py" startline="475" endline="496" pcid="646">
  def test_save_load_h5(self, distribution, optimizer):
    with self.cached_session():
      dataset = keras_test_lib.get_dataset(distribution)
      with distribution.scope():
        model = keras_test_lib.get_model()
        model.compile(
            optimizer(),
            'mse')
        model.fit(dataset, epochs=1, steps_per_epoch=1)

        weights_file = tempfile.mktemp('.h5')
        model.save_weights(weights_file)

        model_2 = keras_test_lib.get_model()
        model_2.compile(
            optimizer(),
            'mse')
        model_2.load_weights(weights_file)
        model_2.predict(
            keras_test_lib.get_predict_dataset(distribution), steps=2)
        model_2.fit(dataset, epochs=1, steps_per_epoch=1)

</source>
<source file="systems/keras-2.7.0/keras/distribute/keras_utils_test.py" startline="502" endline="529" pcid="647">
  def test_save_load_trackable(self, distribution, optimizer):
    # TODO(b/123533246): Enable the test for TPU once bug is fixed
    if (isinstance(distribution,
                   (tf.distribute.experimental.TPUStrategy, tf.compat.v1.distribute.experimental.TPUStrategy)) and
        distribution.extended.steps_per_run > 1):
      self.skipTest('MultiStep TPU Strategy deadlocks with optimizer restore.')
    with self.cached_session():
      dataset = keras_test_lib.get_dataset(distribution)
      with distribution.scope():
        model = keras_test_lib.get_model()
        model.compile(
            optimizer(),
            'mse')
        model.fit(dataset, epochs=1, steps_per_epoch=1)

        weights_file = tempfile.mktemp()
        model.save_weights(weights_file)

        model_2 = keras_test_lib.get_model()
        model_2.compile(
            optimizer(),
            'mse')
        model_2.load_weights(weights_file)
        model_2.predict(
            keras_test_lib.get_predict_dataset(distribution), steps=2)
        model_2.fit(dataset, epochs=1, steps_per_epoch=1)


</source>
</class>

<class classid="42" nclones="2" nlines="18" similarity="83">
<source file="systems/keras-2.7.0/keras/distribute/multi_worker_callback_tf2_test.py" startline="147" endline="171" pcid="661">
  def test_model_checkpoint_works_with_same_file_path(self, mode):

    def proc_model_checkpoint_works_with_same_file_path(
        test_obj, saving_filepath):
      model, _, train_ds, steps = _model_setup(test_obj, file_format='')
      num_epoch = 2

      # The saving_filepath shouldn't exist at the beginning (as it's unique).
      test_obj.assertFalse(tf.io.gfile.exists(saving_filepath))

      model.fit(
          x=train_ds,
          epochs=num_epoch,
          steps_per_epoch=steps,
          callbacks=[callbacks.ModelCheckpoint(filepath=saving_filepath)])

      test_obj.assertTrue(tf.io.gfile.exists(saving_filepath))

    saving_filepath = os.path.join(self.get_temp_dir(), 'checkpoint')

    tf.__internal__.distribute.multi_process_runner.run(
        proc_model_checkpoint_works_with_same_file_path,
        cluster_spec=tf.__internal__.distribute.multi_process_runner.create_cluster_spec(num_workers=2),
        args=(self, saving_filepath))

</source>
<source file="systems/keras-2.7.0/keras/distribute/multi_worker_callback_tf2_test.py" startline="344" endline="371" pcid="673">
  def test_tensorboard_works_with_same_file_path(self, mode):

    def proc_tensorboard_works_with_same_file_path(test_obj, saving_filepath):
      model, _, train_ds, steps = _model_setup(test_obj, file_format='')
      num_epoch = 2

      # The saving_filepath shouldn't exist at the beginning (as it's unique).
      test_obj.assertFalse(tf.io.gfile.exists(saving_filepath))

      tf.__internal__.distribute.multi_process_runner.get_barrier().wait()

      model.fit(
          x=train_ds,
          epochs=num_epoch,
          steps_per_epoch=steps,
          callbacks=[callbacks.TensorBoard(log_dir=saving_filepath)])

      tf.__internal__.distribute.multi_process_runner.get_barrier().wait()

      test_obj.assertTrue(tf.io.gfile.listdir(saving_filepath))

    saving_filepath = os.path.join(self.get_temp_dir(), 'logfile')

    tf.__internal__.distribute.multi_process_runner.run(
        proc_tensorboard_works_with_same_file_path,
        cluster_spec=tf.__internal__.distribute.multi_process_runner.create_cluster_spec(num_workers=2),
        args=(self, saving_filepath))

</source>
</class>

<class classid="43" nclones="3" nlines="22" similarity="71">
<source file="systems/keras-2.7.0/keras/distribute/multi_worker_callback_tf2_test.py" startline="239" endline="272" pcid="667">
  def test_profiler_saves_on_both_chief_and_non_chief(self, mode):

    def proc_profiler_saves_on_both_chief_and_non_chief(test_obj):
      model, _, train_ds, steps = _model_setup(test_obj, file_format='')
      num_epoch = 2

      task_config = get_tf_config_task()
      saving_filepath = os.path.join(
          test_obj.get_temp_dir(),
          'logfile_%s_%d' % (task_config['type'], task_config['index']))

      # The saving_filepath shouldn't exist at the beginning (as it's unique).
      test_obj.assertFalse(tf.io.gfile.exists(saving_filepath))

      model.fit(
          x=train_ds,
          epochs=num_epoch,
          steps_per_epoch=steps,
          callbacks=[
              callbacks.TensorBoard(
                  log_dir=saving_filepath, profile_batch=[2, 4])
          ])

      # Profiler dir should be created on both chief and non-chief node
      profiler_dir_path = os.path.join(saving_filepath, 'plugins', 'profile')
      test_obj.assertTrue(tf.io.gfile.exists(profiler_dir_path))

    tf.__internal__.distribute.multi_process_runner.run(
        proc_profiler_saves_on_both_chief_and_non_chief,
        cluster_spec=
        tf.__internal__.distribute.multi_process_runner.create_cluster_spec(
            num_workers=2),
        args=(self,))

</source>
<source file="systems/keras-2.7.0/keras/distribute/multi_worker_callback_tf2_test.py" startline="274" endline="312" pcid="669">
  def test_tensorboard_saves_on_chief_but_not_otherwise(self, mode):

    def proc_tensorboard_saves_on_chief_but_not_otherwise(test_obj):
      model, _, train_ds, steps = _model_setup(test_obj, file_format='')
      num_epoch = 2

      # Incorporate type/index information and thread id in saving_filepath to
      # ensure every worker has a unique path. Note that in normal use case the
      # saving_filepath will be the same for all workers, but we use different
      # ones here just to test out chief saves summaries but non-chief doesn't.
      task_config = get_tf_config_task()
      saving_filepath = os.path.join(
          test_obj.get_temp_dir(),
          'logfile_%s_%d' % (task_config['type'], task_config['index']))

      # The saving_filepath shouldn't exist at the beginning (as it's unique).
      test_obj.assertFalse(tf.io.gfile.exists(saving_filepath))

      model.fit(
          x=train_ds,
          epochs=num_epoch,
          steps_per_epoch=steps,
          # disabling profiler by setting profile_batch to zero
          callbacks=[
              callbacks.TensorBoard(log_dir=saving_filepath, profile_batch=0)
          ])

      # If it's chief, the summaries should be saved in the filepath; if not,
      # the directory should be empty (although created). Using
      # `file_io.list_directory()` since the directory may be created at this
      # point.
      test_obj.assertEqual(
          bool(tf.io.gfile.listdir(saving_filepath)), is_chief())

    tf.__internal__.distribute.multi_process_runner.run(
        proc_tensorboard_saves_on_chief_but_not_otherwise,
        cluster_spec=tf.__internal__.distribute.multi_process_runner.create_cluster_spec(num_workers=2),
        args=(self,))

</source>
<source file="systems/keras-2.7.0/keras/distribute/multi_worker_callback_tf2_test.py" startline="314" endline="342" pcid="671">
  def test_tensorboard_can_still_save_to_temp_even_if_it_exists(self, mode):

    def proc_tensorboard_can_still_save_to_temp_even_if_it_exists(test_obj):
      model, _, train_ds, steps = _model_setup(test_obj, file_format='')
      num_epoch = 2

      saving_filepath = os.path.join(
          test_obj.get_temp_dir(),
          'logfile_%s' % (get_tf_config_task()['type']))

      saving_filepath_for_temp = os.path.join(saving_filepath, 'workertemp_1')
      os.mkdir(saving_filepath)
      os.mkdir(saving_filepath_for_temp)

      # Verifies that even if `saving_filepath_for_temp` exists, tensorboard
      # can still save to temporary directory.
      test_obj.assertTrue(tf.io.gfile.exists(saving_filepath_for_temp))

      model.fit(
          x=train_ds,
          epochs=num_epoch,
          steps_per_epoch=steps,
          callbacks=[callbacks.TensorBoard(log_dir=saving_filepath)])

    tf.__internal__.distribute.multi_process_runner.run(
        proc_tensorboard_can_still_save_to_temp_even_if_it_exists,
        cluster_spec=tf.__internal__.distribute.multi_process_runner.create_cluster_spec(num_workers=2),
        args=(self,))

</source>
</class>

<class classid="44" nclones="3" nlines="11" similarity="81">
<source file="systems/keras-2.7.0/keras/distribute/distributed_file_utils_test.py" startline="74" endline="85" pcid="714">
  def testChiefDoesNotRemoveDirAndFilePath(self):
    temp_dir = self.get_temp_dir()
    strategy = DistributedFileUtilsTest.MockedChiefStrategy()
    dir_to_write = distributed_file_utils.write_dirpath(temp_dir, strategy)
    file_to_write = os.path.join(dir_to_write, 'tmp')
    self.assertFalse(os.path.exists(file_to_write))
    self._write_dummy_file(file_to_write)
    self.assertTrue(os.path.exists(file_to_write))
    distributed_file_utils.remove_temp_dir_with_filepath(
        file_to_write, strategy)
    self.assertTrue(os.path.exists(file_to_write))

</source>
<source file="systems/keras-2.7.0/keras/distribute/distributed_file_utils_test.py" startline="86" endline="97" pcid="715">
  def testWorkerDoesRemoveFilePath(self):
    temp_dir = self.get_temp_dir()
    strategy = DistributedFileUtilsTest.MockedWorkerStrategy()
    dir_to_write = distributed_file_utils.write_dirpath(temp_dir, strategy)
    file_to_write = os.path.join(dir_to_write, 'tmp')
    self.assertFalse(os.path.exists(file_to_write))
    self._write_dummy_file(file_to_write)
    self.assertTrue(os.path.exists(file_to_write))
    distributed_file_utils.remove_temp_dir_with_filepath(
        file_to_write, strategy)
    self.assertFalse(os.path.exists(file_to_write))

</source>
<source file="systems/keras-2.7.0/keras/distribute/distributed_file_utils_test.py" startline="98" endline="109" pcid="716">
  def testWorkerDoesRemoveDirPath(self):
    temp_dir = self.get_temp_dir()
    strategy = DistributedFileUtilsTest.MockedWorkerStrategy()
    dir_to_write = distributed_file_utils.write_dirpath(temp_dir, strategy)
    file_to_write = os.path.join(dir_to_write, 'tmp')
    self.assertFalse(os.path.exists(file_to_write))
    self._write_dummy_file(file_to_write)
    self.assertTrue(os.path.exists(file_to_write))
    distributed_file_utils.remove_temp_dirpath(temp_dir, strategy)
    self.assertFalse(os.path.exists(file_to_write))
    self.assertFalse(os.path.exists(os.path.dirname(file_to_write)))

</source>
</class>

<class classid="45" nclones="2" nlines="10" similarity="90">
<source file="systems/keras-2.7.0/keras/distribute/dataset_creator_model_fit_test_base.py" startline="46" endline="56" pcid="757">
      def dataset_fn(input_context):
        del input_context
        lookup_layer = string_lookup.StringLookup(
            num_oov_indices=1, vocabulary=filepath)
        x = np.array([["earth", "wind", "and", "fire"],
                      ["fire", "and", "earth", "michigan"]])
        y = np.array([0, 1])
        map_fn = lambda x, y: (lookup_layer(x), y)
        return tf.data.Dataset.from_tensor_slices(
            (x, y)).shuffle(10).repeat().batch(2).map(map_fn)

</source>
<source file="systems/keras-2.7.0/keras/integration_test/parameter_server_keras_preprocessing_test.py" startline="288" endline="298" pcid="1676">
      def dataset_fn(input_context):
        del input_context
        lookup_layer = tf.keras.layers.StringLookup(
            num_oov_indices=1, vocabulary=filepath)
        x = np.array([["earth", "wind", "and", "fire"],
                      ["fire", "and", "earth", "michigan"]])
        y = np.array([0, 1])
        map_fn = lambda x, y: (lookup_layer(x), y)
        return tf.data.Dataset.from_tensor_slices(
            (x, y)).shuffle(10).repeat().batch(2).map(map_fn)

</source>
</class>

<class classid="46" nclones="3" nlines="25" similarity="100">
<source file="systems/keras-2.7.0/keras/applications/resnet_v2.py" startline="30" endline="58" pcid="767">
def ResNet50V2(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'):
  """Instantiates the ResNet50V2 architecture."""
  def stack_fn(x):
    x = resnet.stack2(x, 64, 3, name='conv2')
    x = resnet.stack2(x, 128, 4, name='conv3')
    x = resnet.stack2(x, 256, 6, name='conv4')
    return resnet.stack2(x, 512, 3, stride1=1, name='conv5')

  return resnet.ResNet(
      stack_fn,
      True,
      True,
      'resnet50v2',
      include_top,
      weights,
      input_tensor,
      input_shape,
      pooling,
      classes,
      classifier_activation=classifier_activation)


</source>
<source file="systems/keras-2.7.0/keras/applications/resnet_v2.py" startline="92" endline="120" pcid="771">
def ResNet152V2(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'):
  """Instantiates the ResNet152V2 architecture."""
  def stack_fn(x):
    x = resnet.stack2(x, 64, 3, name='conv2')
    x = resnet.stack2(x, 128, 8, name='conv3')
    x = resnet.stack2(x, 256, 36, name='conv4')
    return resnet.stack2(x, 512, 3, stride1=1, name='conv5')

  return resnet.ResNet(
      stack_fn,
      True,
      True,
      'resnet152v2',
      include_top,
      weights,
      input_tensor,
      input_shape,
      pooling,
      classes,
      classifier_activation=classifier_activation)


</source>
<source file="systems/keras-2.7.0/keras/applications/resnet_v2.py" startline="61" endline="89" pcid="769">
def ResNet101V2(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'):
  """Instantiates the ResNet101V2 architecture."""
  def stack_fn(x):
    x = resnet.stack2(x, 64, 3, name='conv2')
    x = resnet.stack2(x, 128, 4, name='conv3')
    x = resnet.stack2(x, 256, 23, name='conv4')
    return resnet.stack2(x, 512, 3, stride1=1, name='conv5')

  return resnet.ResNet(
      stack_fn,
      True,
      True,
      'resnet101v2',
      include_top,
      weights,
      input_tensor,
      input_shape,
      pooling,
      classes,
      classifier_activation=classifier_activation)


</source>
</class>

<class classid="47" nclones="3" nlines="13" similarity="84">
<source file="systems/keras-2.7.0/keras/regularizers.py" startline="367" endline="379" pcid="805">
def get(identifier):
  """Retrieve a regularizer instance from a config or identifier."""
  if identifier is None:
    return None
  if isinstance(identifier, dict):
    return deserialize(identifier)
  elif isinstance(identifier, str):
    return deserialize(str(identifier))
  elif callable(identifier):
    return identifier
  else:
    raise ValueError(
        f'Could not interpret regularizer identifier: {identifier}')
</source>
<source file="systems/keras-2.7.0/keras/constraints.py" startline="335" endline="348" pcid="832">
def get(identifier):
  """Retrieves a Keras constraint function."""
  if identifier is None:
    return None
  if isinstance(identifier, dict):
    return deserialize(identifier)
  elif isinstance(identifier, str):
    config = {'class_name': str(identifier), 'config': {}}
    return deserialize(config)
  elif callable(identifier):
    return identifier
  else:
    raise ValueError(
        f'Could not interpret constraint function identifier: {identifier}')
</source>
<source file="systems/keras-2.7.0/keras/activations.py" startline="564" endline="603" pcid="5503">
def get(identifier):
  """Returns function.

  Args:
      identifier: Function or string

  Returns:
      Function corresponding to the input string or input function.

  For example:

  >>> tf.keras.activations.get('softmax')
   <function softmax at 0x1222a3d90>
  >>> tf.keras.activations.get(tf.keras.activations.softmax)
   <function softmax at 0x1222a3d90>
  >>> tf.keras.activations.get(None)
   <function linear at 0x1239596a8>
  >>> tf.keras.activations.get(abs)
   <built-in function abs>
  >>> tf.keras.activations.get('abcd')
  Traceback (most recent call last):
  ...
  ValueError: Unknown activation function:abcd

  Raises:
      ValueError: Input is an unknown function or string, i.e., the input does
      not denote any defined function.
  """
  if identifier is None:
    return linear
  if isinstance(identifier, str):
    identifier = str(identifier)
    return deserialize(identifier)
  elif isinstance(identifier, dict):
    return deserialize(identifier)
  elif callable(identifier):
    return identifier
  else:
    raise TypeError(
        f'Could not interpret activation function identifier: {identifier}')
</source>
</class>

<class classid="48" nclones="2" nlines="25" similarity="92">
<source file="systems/keras-2.7.0/keras/estimator/__init__.py" startline="32" endline="180" pcid="806">
def model_to_estimator(
    keras_model=None,
    keras_model_path=None,
    custom_objects=None,
    model_dir=None,
    config=None,
    checkpoint_format='saver',
    metric_names_map=None,
    export_outputs=None):
  """Constructs an `Estimator` instance from given keras model.

  If you use infrastructure or other tooling that relies on Estimators, you can
  still build a Keras model and use model_to_estimator to convert the Keras
  model to an Estimator for use with downstream systems.

  For usage example, please see:
  [Creating estimators from Keras Models](
    https://www.tensorflow.org/guide/estimator#create_an_estimator_from_a_keras_model).

  Sample Weights:
  Estimators returned by `model_to_estimator` are configured so that they can
  handle sample weights (similar to `keras_model.fit(x, y, sample_weights)`).

  To pass sample weights when training or evaluating the Estimator, the first
  item returned by the input function should be a dictionary with keys
  `features` and `sample_weights`. Example below:

  ```python
  keras_model = tf.keras.Model(...)
  keras_model.compile(...)

  estimator = tf.keras.estimator.model_to_estimator(keras_model)

  def input_fn():
    return dataset_ops.Dataset.from_tensors(
        ({'features': features, 'sample_weights': sample_weights},
         targets))

  estimator.train(input_fn, steps=1)
  ```

  Example with customized export signature:
  ```python
  inputs = {'a': tf.keras.Input(..., name='a'),
            'b': tf.keras.Input(..., name='b')}
  outputs = {'c': tf.keras.layers.Dense(..., name='c')(inputs['a']),
             'd': tf.keras.layers.Dense(..., name='d')(inputs['b'])}
  keras_model = tf.keras.Model(inputs, outputs)
  keras_model.compile(...)
  export_outputs = {'c': tf.estimator.export.RegressionOutput,
                    'd': tf.estimator.export.ClassificationOutput}

  estimator = tf.keras.estimator.model_to_estimator(
      keras_model, export_outputs=export_outputs)

  def input_fn():
    return dataset_ops.Dataset.from_tensors(
        ({'features': features, 'sample_weights': sample_weights},
         targets))

  estimator.train(input_fn, steps=1)
  ```

  Args:
    keras_model: A compiled Keras model object. This argument is mutually
      exclusive with `keras_model_path`. Estimator's `model_fn` uses the
      structure of the model to clone the model. Defaults to `None`.
    keras_model_path: Path to a compiled Keras model saved on disk, in HDF5
      format, which can be generated with the `save()` method of a Keras model.
      This argument is mutually exclusive with `keras_model`.
      Defaults to `None`.
    custom_objects: Dictionary for cloning customized objects. This is
      used with classes that is not part of this pip package. For example, if
      user maintains a `relu6` class that inherits from `tf.keras.layers.Layer`,
      then pass `custom_objects={'relu6': relu6}`. Defaults to `None`.
    model_dir: Directory to save `Estimator` model parameters, graph, summary
      files for TensorBoard, etc. If unset a directory will be created with
      `tempfile.mkdtemp`
    config: `RunConfig` to config `Estimator`. Allows setting up things in
      `model_fn` based on configuration such as `num_ps_replicas`, or
      `model_dir`. Defaults to `None`. If both `config.model_dir` and the
      `model_dir` argument (above) are specified the `model_dir` **argument**
      takes precedence.
    checkpoint_format: Sets the format of the checkpoint saved by the estimator
      when training. May be `saver` or `checkpoint`, depending on whether to
      save checkpoints from `tf.train.Saver` or `tf.train.Checkpoint`. This
      argument currently defaults to `saver`. When 2.0 is released, the default
      will be `checkpoint`. Estimators use name-based `tf.train.Saver`
      checkpoints, while Keras models use object-based checkpoints from
      `tf.train.Checkpoint`. Currently, saving object-based checkpoints from
      `model_to_estimator` is only supported by Functional and Sequential
      models. Defaults to 'saver'.
    metric_names_map: Optional dictionary mapping Keras model output metric
      names to custom names. This can be used to override the default Keras
      model output metrics names in a multi IO model use case and provide custom
      names for the `eval_metric_ops` in Estimator.
      The Keras model metric names can be obtained using `model.metrics_names`
      excluding any loss metrics such as total loss and output losses.
      For example, if your Keras model has two outputs `out_1` and `out_2`,
      with `mse` loss and `acc` metric, then `model.metrics_names` will be
      `['loss', 'out_1_loss', 'out_2_loss', 'out_1_acc', 'out_2_acc']`.
      The model metric names excluding the loss metrics will be
      `['out_1_acc', 'out_2_acc']`.
    export_outputs: Optional dictionary. This can be used to override the
      default Keras model output exports in a multi IO model use case and
      provide custom names for the `export_outputs` in
      `tf.estimator.EstimatorSpec`. Default is None, which is equivalent to
      {'serving_default': `tf.estimator.export.PredictOutput`}. If not None,
      the keys must match the keys of `model.output_names`.
      A dict `{name: output}` where:
        * name: An arbitrary name for this output.
        * output: an `ExportOutput` class such as `ClassificationOutput`,
          `RegressionOutput`, or `PredictOutput`. Single-headed models only need
          to specify one entry in this dictionary. Multi-headed models should
          specify one entry for each head, one of which must be named using
          `tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`
          If no entry is provided, a default `PredictOutput` mapping to
          `predictions` will be created.

  Returns:
    An Estimator from given keras model.

  Raises:
    ValueError: If neither keras_model nor keras_model_path was given.
    ValueError: If both keras_model and keras_model_path was given.
    ValueError: If the keras_model_path is a GCS URI.
    ValueError: If keras_model has not been compiled.
    ValueError: If an invalid checkpoint_format was given.
  """

  try:
    from tensorflow_estimator.python.estimator import keras_lib  # pylint: disable=g-import-not-at-top
  except ImportError:
    raise NotImplementedError(
        'tf.keras.estimator.model_to_estimator function not available in your '
        'installation.')
  _model_to_estimator_usage_gauge.get_cell('v1').set(True)
  return keras_lib.model_to_estimator(  # pylint:disable=unexpected-keyword-arg
      keras_model=keras_model,
      keras_model_path=keras_model_path,
      custom_objects=custom_objects,
      model_dir=model_dir,
      config=config,
      checkpoint_format=checkpoint_format,
      use_v2_estimator=False,
      metric_names_map=metric_names_map,
      export_outputs=export_outputs)


</source>
<source file="systems/keras-2.7.0/keras/estimator/__init__.py" startline="182" endline="367" pcid="807">
def model_to_estimator_v2(keras_model=None,
                          keras_model_path=None,
                          custom_objects=None,
                          model_dir=None,
                          config=None,
                          checkpoint_format='checkpoint',
                          metric_names_map=None,
                          export_outputs=None):
  """Constructs an `Estimator` instance from given keras model.

  If you use infrastructure or other tooling that relies on Estimators, you can
  still build a Keras model and use model_to_estimator to convert the Keras
  model to an Estimator for use with downstream systems.

  For usage example, please see:
  [Creating estimators from Keras Models](
    https://www.tensorflow.org/guide/estimators#creating_estimators_from_keras_models).

  Sample Weights:
  Estimators returned by `model_to_estimator` are configured so that they can
  handle sample weights (similar to `keras_model.fit(x, y, sample_weights)`).

  To pass sample weights when training or evaluating the Estimator, the first
  item returned by the input function should be a dictionary with keys
  `features` and `sample_weights`. Example below:

  ```python
  keras_model = tf.keras.Model(...)
  keras_model.compile(...)

  estimator = tf.keras.estimator.model_to_estimator(keras_model)

  def input_fn():
    return dataset_ops.Dataset.from_tensors(
        ({'features': features, 'sample_weights': sample_weights},
         targets))

  estimator.train(input_fn, steps=1)
  ```

  Example with customized export signature:
  ```python
  inputs = {'a': tf.keras.Input(..., name='a'),
            'b': tf.keras.Input(..., name='b')}
  outputs = {'c': tf.keras.layers.Dense(..., name='c')(inputs['a']),
             'd': tf.keras.layers.Dense(..., name='d')(inputs['b'])}
  keras_model = tf.keras.Model(inputs, outputs)
  keras_model.compile(...)
  export_outputs = {'c': tf.estimator.export.RegressionOutput,
                    'd': tf.estimator.export.ClassificationOutput}

  estimator = tf.keras.estimator.model_to_estimator(
      keras_model, export_outputs=export_outputs)

  def input_fn():
    return dataset_ops.Dataset.from_tensors(
        ({'features': features, 'sample_weights': sample_weights},
         targets))

  estimator.train(input_fn, steps=1)
  ```

  Note: We do not support creating weighted metrics in Keras and converting them
  to weighted metrics in the Estimator API using `model_to_estimator`.
  You will have to create these metrics directly on the estimator spec using the
  `add_metrics` function.

  To customize the estimator `eval_metric_ops` names, you can pass in the
  `metric_names_map` dictionary mapping the keras model output metric names
  to the custom names as follows:

  ```python
    input_a = tf.keras.layers.Input(shape=(16,), name='input_a')
    input_b = tf.keras.layers.Input(shape=(16,), name='input_b')
    dense = tf.keras.layers.Dense(8, name='dense_1')
    interm_a = dense(input_a)
    interm_b = dense(input_b)
    merged = tf.keras.layers.concatenate([interm_a, interm_b], name='merge')
    output_a = tf.keras.layers.Dense(3, activation='softmax', name='dense_2')(
            merged)
    output_b = tf.keras.layers.Dense(2, activation='softmax', name='dense_3')(
            merged)
    keras_model = tf.keras.models.Model(
        inputs=[input_a, input_b], outputs=[output_a, output_b])
    keras_model.compile(
        loss='categorical_crossentropy',
        optimizer='rmsprop',
        metrics={
            'dense_2': 'categorical_accuracy',
            'dense_3': 'categorical_accuracy'
        })

    metric_names_map = {
        'dense_2_categorical_accuracy': 'acc_1',
        'dense_3_categorical_accuracy': 'acc_2',
    }
    keras_est = tf.keras.estimator.model_to_estimator(
        keras_model=keras_model,
        config=config,
        metric_names_map=metric_names_map)
  ```

  Args:
    keras_model: A compiled Keras model object. This argument is mutually
      exclusive with `keras_model_path`. Estimator's `model_fn` uses the
      structure of the model to clone the model. Defaults to `None`.
    keras_model_path: Path to a compiled Keras model saved on disk, in HDF5
      format, which can be generated with the `save()` method of a Keras model.
      This argument is mutually exclusive with `keras_model`.
      Defaults to `None`.
    custom_objects: Dictionary for cloning customized objects. This is
      used with classes that is not part of this pip package. For example, if
      user maintains a `relu6` class that inherits from `tf.keras.layers.Layer`,
      then pass `custom_objects={'relu6': relu6}`. Defaults to `None`.
    model_dir: Directory to save `Estimator` model parameters, graph, summary
      files for TensorBoard, etc. If unset a directory will be created with
      `tempfile.mkdtemp`
    config: `RunConfig` to config `Estimator`. Allows setting up things in
      `model_fn` based on configuration such as `num_ps_replicas`, or
      `model_dir`. Defaults to `None`. If both `config.model_dir` and the
      `model_dir` argument (above) are specified the `model_dir` **argument**
      takes precedence.
    checkpoint_format: Sets the format of the checkpoint saved by the estimator
      when training. May be `saver` or `checkpoint`, depending on whether to
      save checkpoints from `tf.compat.v1.train.Saver` or `tf.train.Checkpoint`.
      The default is `checkpoint`. Estimators use name-based `tf.train.Saver`
      checkpoints, while Keras models use object-based checkpoints from
      `tf.train.Checkpoint`. Currently, saving object-based checkpoints from
      `model_to_estimator` is only supported by Functional and Sequential
      models. Defaults to 'checkpoint'.
    metric_names_map: Optional dictionary mapping Keras model output metric
      names to custom names. This can be used to override the default Keras
      model output metrics names in a multi IO model use case and provide custom
      names for the `eval_metric_ops` in Estimator.
      The Keras model metric names can be obtained using `model.metrics_names`
      excluding any loss metrics such as total loss and output losses.
      For example, if your Keras model has two outputs `out_1` and `out_2`,
      with `mse` loss and `acc` metric, then `model.metrics_names` will be
      `['loss', 'out_1_loss', 'out_2_loss', 'out_1_acc', 'out_2_acc']`.
      The model metric names excluding the loss metrics will be
      `['out_1_acc', 'out_2_acc']`.
    export_outputs: Optional dictionary. This can be used to override the
      default Keras model output exports in a multi IO model use case and
      provide custom names for the `export_outputs` in
      `tf.estimator.EstimatorSpec`. Default is None, which is equivalent to
      {'serving_default': `tf.estimator.export.PredictOutput`}. If not None,
      the keys must match the keys of `model.output_names`.
      A dict `{name: output}` where:
        * name: An arbitrary name for this output.
        * output: an `ExportOutput` class such as `ClassificationOutput`,
          `RegressionOutput`, or `PredictOutput`. Single-headed models only need
          to specify one entry in this dictionary. Multi-headed models should
          specify one entry for each head, one of which must be named using
          `tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`
          If no entry is provided, a default `PredictOutput` mapping to
          `predictions` will be created.

  Returns:
    An Estimator from given keras model.

  Raises:
    ValueError: If neither keras_model nor keras_model_path was given.
    ValueError: If both keras_model and keras_model_path was given.
    ValueError: If the keras_model_path is a GCS URI.
    ValueError: If keras_model has not been compiled.
    ValueError: If an invalid checkpoint_format was given.
  """

  try:
    from tensorflow_estimator.python.estimator import keras_lib  # pylint: disable=g-import-not-at-top
  except ImportError:
    raise NotImplementedError(
        'tf.keras.estimator.model_to_estimator function not available in your '
        'installation.')
  _model_to_estimator_usage_gauge.get_cell('v2').set(True)
  return keras_lib.model_to_estimator(  # pylint:disable=unexpected-keyword-arg
      keras_model=keras_model,
      keras_model_path=keras_model_path,
      custom_objects=custom_objects,
      model_dir=model_dir,
      config=config,
      checkpoint_format=checkpoint_format,
      use_v2_estimator=True,
      metric_names_map=metric_names_map,
      export_outputs=export_outputs)
# LINT.ThenChange(//tensorflow_estimator/python/estimator/keras_lib.py)
</source>
</class>

<class classid="49" nclones="2" nlines="11" similarity="90">
<source file="systems/keras-2.7.0/keras/wrappers/scikit_learn_test.py" startline="36" endline="48" pcid="833">
def build_fn_clf(hidden_dim):
  model = keras.models.Sequential()
  model.add(keras.layers.Dense(INPUT_DIM, input_shape=(INPUT_DIM,)))
  model.add(keras.layers.Activation('relu'))
  model.add(keras.layers.Dense(hidden_dim))
  model.add(keras.layers.Activation('relu'))
  model.add(keras.layers.Dense(NUM_CLASSES))
  model.add(keras.layers.Activation('softmax'))
  model.compile(
      optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])
  return model


</source>
<source file="systems/keras-2.7.0/keras/wrappers/scikit_learn_test.py" startline="72" endline="84" pcid="835">
def build_fn_reg(hidden_dim):
  model = keras.models.Sequential()
  model.add(keras.layers.Dense(INPUT_DIM, input_shape=(INPUT_DIM,)))
  model.add(keras.layers.Activation('relu'))
  model.add(keras.layers.Dense(hidden_dim))
  model.add(keras.layers.Activation('relu'))
  model.add(keras.layers.Dense(1))
  model.add(keras.layers.Activation('linear'))
  model.compile(
      optimizer='sgd', loss='mean_absolute_error', metrics=['accuracy'])
  return model


</source>
</class>

<class classid="50" nclones="2" nlines="15" similarity="70">
<source file="systems/keras-2.7.0/keras/wrappers/scikit_learn_test.py" startline="49" endline="71" pcid="834">
def assert_classification_works(clf):
  np.random.seed(42)
  (x_train, y_train), (x_test, _) = testing_utils.get_test_data(
      train_samples=TRAIN_SAMPLES,
      test_samples=TEST_SAMPLES,
      input_shape=(INPUT_DIM,),
      num_classes=NUM_CLASSES)

  clf.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS)

  score = clf.score(x_train, y_train, batch_size=BATCH_SIZE)
  assert np.isscalar(score) and np.isfinite(score)

  preds = clf.predict(x_test, batch_size=BATCH_SIZE)
  assert preds.shape == (TEST_SAMPLES,)
  for prediction in np.unique(preds):
    assert prediction in range(NUM_CLASSES)

  proba = clf.predict_proba(x_test, batch_size=BATCH_SIZE)
  assert proba.shape == (TEST_SAMPLES, NUM_CLASSES)
  assert np.allclose(np.sum(proba, axis=1), np.ones(TEST_SAMPLES))


</source>
<source file="systems/keras-2.7.0/keras/wrappers/scikit_learn_test.py" startline="85" endline="101" pcid="836">
def assert_regression_works(reg):
  np.random.seed(42)
  (x_train, y_train), (x_test, _) = testing_utils.get_test_data(
      train_samples=TRAIN_SAMPLES,
      test_samples=TEST_SAMPLES,
      input_shape=(INPUT_DIM,),
      num_classes=NUM_CLASSES)

  reg.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS)

  score = reg.score(x_train, y_train, batch_size=BATCH_SIZE)
  assert np.isscalar(score) and np.isfinite(score)

  preds = reg.predict(x_test, batch_size=BATCH_SIZE)
  assert preds.shape == (TEST_SAMPLES,)


</source>
</class>

<class classid="51" nclones="4" nlines="11" similarity="72">
<source file="systems/keras-2.7.0/keras/wrappers/scikit_learn_test.py" startline="114" endline="129" pcid="838">
  def test_classify_class_build_fn(self):

    class ClassBuildFnClf:

      def __call__(self, hidden_dim):
        return build_fn_clf(hidden_dim)

    with self.cached_session():
      clf = scikit_learn.KerasClassifier(
          build_fn=ClassBuildFnClf(),
          hidden_dim=HIDDEN_DIM,
          batch_size=BATCH_SIZE,
          epochs=EPOCHS)

      assert_classification_works(clf)

</source>
<source file="systems/keras-2.7.0/keras/wrappers/scikit_learn_test.py" startline="172" endline="187" pcid="845">
  def test_regression_inherit_class_build_fn(self):

    class InheritClassBuildFnReg(scikit_learn.KerasRegressor):

      def __call__(self, hidden_dim):
        return build_fn_reg(hidden_dim)

    with self.cached_session():
      reg = InheritClassBuildFnReg(
          build_fn=None,
          hidden_dim=HIDDEN_DIM,
          batch_size=BATCH_SIZE,
          epochs=EPOCHS)

      assert_regression_works(reg)

</source>
<source file="systems/keras-2.7.0/keras/wrappers/scikit_learn_test.py" startline="156" endline="171" pcid="843">
  def test_regression_class_build_fn(self):

    class ClassBuildFnReg:

      def __call__(self, hidden_dim):
        return build_fn_reg(hidden_dim)

    with self.cached_session():
      reg = scikit_learn.KerasRegressor(
          build_fn=ClassBuildFnReg(),
          hidden_dim=HIDDEN_DIM,
          batch_size=BATCH_SIZE,
          epochs=EPOCHS)

      assert_regression_works(reg)

</source>
<source file="systems/keras-2.7.0/keras/wrappers/scikit_learn_test.py" startline="130" endline="145" pcid="840">
  def test_classify_inherit_class_build_fn(self):

    class InheritClassBuildFnClf(scikit_learn.KerasClassifier):

      def __call__(self, hidden_dim):
        return build_fn_clf(hidden_dim)

    with self.cached_session():
      clf = InheritClassBuildFnClf(
          build_fn=None,
          hidden_dim=HIDDEN_DIM,
          batch_size=BATCH_SIZE,
          epochs=EPOCHS)

      assert_classification_works(clf)

</source>
</class>

<class classid="52" nclones="2" nlines="11" similarity="72">
<source file="systems/keras-2.7.0/keras/models_test.py" startline="288" endline="299" pcid="872">
  def test_functional_cloning_does_not_create_unnecessary_placeholders(self):
    with tf.Graph().as_default():
      x = keras.Input((4,))
      y = keras.layers.Dense(4)(x)
      model = keras.models.Model(x, y)
    graph = tf.Graph()
    with graph.as_default():
      x = tf.ones((10, 4))
      _ = keras.models.clone_model(model, input_tensors=[x])
      has_placeholder = _has_placeholder(graph)
      self.assertFalse(has_placeholder)

</source>
<source file="systems/keras-2.7.0/keras/models_test.py" startline="300" endline="310" pcid="873">
  def test_sequential_cloning_does_not_create_unnecessary_placeholders(self):
    with tf.Graph().as_default():
      model = keras.models.Sequential()
      model.add(keras.layers.Dense(4, input_shape=(4,)))
    graph = tf.Graph()
    with graph.as_default():
      x = tf.ones((10, 4))
      _ = keras.models.clone_model(model, input_tensors=[x])
      has_placeholder = _has_placeholder(graph)
      self.assertFalse(has_placeholder)

</source>
</class>

<class classid="53" nclones="5" nlines="10" similarity="72">
<source file="systems/keras-2.7.0/keras/utils/data_utils_test.py" startline="168" endline="180" pcid="1215">
  def test_generator_enqueuer_threads(self):
    enqueuer = keras.utils.data_utils.GeneratorEnqueuer(
        create_generator_from_sequence_threads(TestSequence([3, 200, 200, 3])),
        use_multiprocessing=False)
    enqueuer.start(3, 10)
    gen_output = enqueuer.get()
    acc = []
    for _ in range(100):
      acc.append(int(next(gen_output)[0, 0, 0, 0]))

    self.assertEqual(len(set(acc) - set(range(100))), 0)
    enqueuer.stop()

</source>
<source file="systems/keras-2.7.0/keras/utils/data_utils_test.py" startline="182" endline="193" pcid="1216">
  def test_generator_enqueuer_processes(self):
    enqueuer = keras.utils.data_utils.GeneratorEnqueuer(
        create_generator_from_sequence_threads(TestSequence([3, 200, 200, 3])),
        use_multiprocessing=True)
    enqueuer.start(4, 10)
    gen_output = enqueuer.get()
    acc = []
    for _ in range(300):
      acc.append(int(next(gen_output)[0, 0, 0, 0]))
    self.assertNotEqual(acc, list(range(100)))
    enqueuer.stop()

</source>
<source file="systems/keras-2.7.0/keras/utils/data_utils_test.py" startline="225" endline="235" pcid="1220">
  def test_ordered_enqueuer_processes(self):
    enqueuer = keras.utils.data_utils.OrderedEnqueuer(
        TestSequence([3, 200, 200, 3]), use_multiprocessing=True)
    enqueuer.start(3, 10)
    gen_output = enqueuer.get()
    acc = []
    for _ in range(100):
      acc.append(next(gen_output)[0, 0, 0, 0])
    self.assertEqual(acc, list(range(100)))
    enqueuer.stop()

</source>
<source file="systems/keras-2.7.0/keras/utils/data_utils_test.py" startline="213" endline="223" pcid="1219">
  def test_ordered_enqueuer_threads(self):
    enqueuer = keras.utils.data_utils.OrderedEnqueuer(
        TestSequence([3, 200, 200, 3]), use_multiprocessing=False)
    enqueuer.start(3, 10)
    gen_output = enqueuer.get()
    acc = []
    for _ in range(100):
      acc.append(next(gen_output)[0, 0, 0, 0])
    self.assertEqual(acc, list(range(100)))
    enqueuer.stop()

</source>
<source file="systems/keras-2.7.0/keras/utils/data_utils_test.py" startline="254" endline="265" pcid="1223">
  def test_on_epoch_end_processes(self):
    enqueuer = keras.utils.data_utils.OrderedEnqueuer(
        TestSequence([3, 200, 200, 3]), use_multiprocessing=True)
    enqueuer.start(3, 10)
    gen_output = enqueuer.get()
    acc = []
    for _ in range(200):
      acc.append(next(gen_output)[0, 0, 0, 0])
    # Check that order was keep in GeneratorEnqueuer with processes
    self.assertEqual(acc[100:], list([k * 5 for k in range(100)]))
    enqueuer.stop()

</source>
</class>

<class classid="54" nclones="4" nlines="14" similarity="71">
<source file="systems/keras-2.7.0/keras/utils/conv_utils_test.py" startline="191" endline="207" pcid="1307">
  def test_conv_kernel_mask_fc(self, *input_shape):
    padding = 'valid'
    kernel_shape = input_shape
    ndims = len(input_shape)
    strides = (1,) * ndims
    output_shape = _get_const_output_shape(input_shape, dim=1)
    mask = np.ones(input_shape + output_shape, np.bool)
    self.assertAllEqual(
        mask,
        conv_utils.conv_kernel_mask(
            input_shape,
            kernel_shape,
            strides,
            padding
        )
    )

</source>
<source file="systems/keras-2.7.0/keras/utils/conv_utils_test.py" startline="208" endline="225" pcid="1308">
  def test_conv_kernel_mask_diag(self, *input_shape):
    ndims = len(input_shape)
    kernel_shape = (1,) * ndims
    strides = (1,) * ndims

    for padding in ['valid', 'same']:
      mask = np.identity(int(np.prod(input_shape)), np.bool)
      mask = np.reshape(mask, input_shape * 2)
      self.assertAllEqual(
          mask,
          conv_utils.conv_kernel_mask(
              input_shape,
              kernel_shape,
              strides,
              padding
          )
      )

</source>
<source file="systems/keras-2.7.0/keras/utils/conv_utils_test.py" startline="226" endline="246" pcid="1309">
  def test_conv_kernel_mask_full_stride(self, *input_shape):
    padding = 'valid'
    ndims = len(input_shape)
    kernel_shape = (1,) * ndims
    strides = tuple([max(d, 1) for d in input_shape])
    output_shape = _get_const_output_shape(input_shape, dim=1)

    mask = np.zeros(input_shape + output_shape, np.bool)
    if all(d > 0 for d in mask.shape):  # pylint: disable=not-an-iterable
      mask[(0,) * len(output_shape)] = True

    self.assertAllEqual(
        mask,
        conv_utils.conv_kernel_mask(
            input_shape,
            kernel_shape,
            strides,
            padding
        )
    )

</source>
<source file="systems/keras-2.7.0/keras/utils/conv_utils_test.py" startline="247" endline="269" pcid="1310">
  def test_conv_kernel_mask_almost_full_stride(self, *input_shape):
    padding = 'valid'
    ndims = len(input_shape)
    kernel_shape = (1,) * ndims
    strides = tuple([max(d - 1, 1) for d in input_shape])
    output_shape = _get_const_output_shape(input_shape, dim=2)

    mask = np.zeros(input_shape + output_shape, np.bool)
    if all(d > 0 for d in mask.shape):  # pylint: disable=not-an-iterable
      for in_position in itertools.product(*[[0, d - 1] for d in input_shape]):
        out_position = tuple([min(p, 1) for p in in_position])
        mask[in_position + out_position] = True

    self.assertAllEqual(
        mask,
        conv_utils.conv_kernel_mask(
            input_shape,
            kernel_shape,
            strides,
            padding
        )
    )

</source>
</class>

<class classid="55" nclones="3" nlines="15" similarity="80">
<source file="systems/keras-2.7.0/keras/utils/kpl_test_utils.py" startline="77" endline="109" pcid="1368">
  def dataset_fn(self, feature_mapper, label_mapper):
    """Function that generates dataset for test of tf.distribute + KPL.

    Args:
      feature_mapper: a simple keras model with one keras StringLookup layer
        which maps feature to index.
      label_mapper: similar to feature_mapper, but maps label to index.

    Returns:
      Generated dataset for test of tf.distribute + KPL.

    """

    def feature_and_label_gen():
      # Generator of dataset.
      while True:
        features = random.sample(self.FEATURE_VOCAB, 3)
        label = ["yes"] if self.FEATURE_VOCAB[0] in features else ["no"]
        yield {"features": features, "label": label}

    raw_dataset = tf.data.Dataset.from_generator(
        feature_and_label_gen,
        output_signature={
            "features": tf.TensorSpec([3], tf.string),
            "label": tf.TensorSpec([1], tf.string)
        }).shuffle(100).batch(32)

    train_dataset = raw_dataset.map(lambda x: (  # pylint: disable=g-long-lambda
        {
            "features": feature_mapper(x["features"])
        }, label_mapper(x["label"])))
    return train_dataset

</source>
<source file="systems/keras-2.7.0/keras/integration_test/parameter_server_keras_preprocessing_test.py" startline="143" endline="165" pcid="1667">
      def dataset_fn():

        def feature_and_label_gen():
          while True:
            features = random.sample(FEATURE_VOCAB, 3)
            label = ["yes"] if "avenger" in features else ["no"]
            yield {"features": features, "label": label}

        # The dataset will be created on the coordinator.
        raw_dataset = tf.data.Dataset.from_generator(
            feature_and_label_gen,
            output_signature={
                "features": tf.TensorSpec([3], tf.string),
                "label": tf.TensorSpec([1], tf.string)
            }).shuffle(100).batch(32)

        train_dataset = raw_dataset.map(lambda x: (  # pylint: disable=g-long-lambda
            {
                "features": feature_ps(x["features"])
            }, label_ps(x["label"])))
        return train_dataset

      # Create the model. The input needs to be compatible with KPLs.
</source>
<source file="systems/keras-2.7.0/keras/integration_test/tpu_strategy_test.py" startline="127" endline="149" pcid="1655">
      def dataset_fn(_):

        def feature_and_label_gen():
          # Generator of dataset.
          while True:
            features = random.sample(FEATURE_VOCAB, 3)
            label = ["yes"] if "avenger" in features else ["no"]
            yield {"features": features, "label": label}

        raw_dataset = tf.data.Dataset.from_generator(
            feature_and_label_gen,
            output_signature={
                "features": tf.TensorSpec([3], tf.dtypes.string),
                "label": tf.TensorSpec([1], tf.dtypes.string)
            }).shuffle(100).batch(32)

        train_dataset = raw_dataset.map(lambda x: (  # pylint: disable=g-long-lambda
            {
                "features": feature_mapper(x["features"])
            }, label_mapper(x["label"])))
        return train_dataset

      # Create the model. The input needs to be compatible with KPLs.
</source>
</class>

<class classid="56" nclones="2" nlines="23" similarity="70">
<source file="systems/keras-2.7.0/keras/utils/tf_utils_test.py" startline="33" endline="52" pcid="1432">
  def test_default_behavior(self):
    if tf.executing_eagerly():
      self.assertFalse(tf_utils.is_symbolic_tensor(
          tf.Variable(name='blah', initial_value=0.)))
      self.assertFalse(
          tf_utils.is_symbolic_tensor(
              tf.convert_to_tensor(0.)))
      self.assertFalse(tf_utils.is_symbolic_tensor(
          tf.SparseTensor(
              indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4])))
    else:
      self.assertTrue(tf_utils.is_symbolic_tensor(
          tf.Variable(name='blah', initial_value=0.)))
      self.assertTrue(
          tf_utils.is_symbolic_tensor(
              tf.convert_to_tensor(0.)))
      self.assertTrue(tf_utils.is_symbolic_tensor(
          tf.SparseTensor(
              indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4])))

</source>
<source file="systems/keras-2.7.0/keras/utils/tf_utils_test.py" startline="53" endline="85" pcid="1433">
  def test_works_with_registered(self):

    class CustomClass:

      def value(self):
        return tf.convert_to_tensor(42.)

    tf.register_tensor_conversion_function(
        CustomClass, lambda value, **_: value.value())

    tf_utils.register_symbolic_tensor_type(CustomClass)

    if tf.executing_eagerly():
      self.assertFalse(tf_utils.is_symbolic_tensor(
          tf.Variable(name='blah', initial_value=0.)))
      self.assertFalse(
          tf_utils.is_symbolic_tensor(
              tf.convert_to_tensor(0.)))
      self.assertFalse(tf_utils.is_symbolic_tensor(
          tf.SparseTensor(
              indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4])))
      self.assertFalse(tf_utils.is_symbolic_tensor(CustomClass()))
    else:
      self.assertTrue(tf_utils.is_symbolic_tensor(
          tf.Variable(name='blah', initial_value=0.)))
      self.assertTrue(
          tf_utils.is_symbolic_tensor(
              tf.convert_to_tensor(0.)))
      self.assertTrue(tf_utils.is_symbolic_tensor(
          tf.SparseTensor(
              indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4])))
      self.assertTrue(tf_utils.is_symbolic_tensor(CustomClass()))

</source>
</class>

<class classid="57" nclones="2" nlines="25" similarity="70">
<source file="systems/keras-2.7.0/keras/utils/composite_tensor_support_test.py" startline="407" endline="437" pcid="1597">
  def test_sparse_scipy_predict_input_dicts_via_input_layer_args(self):
    # Create a model that accepts a sparse input and converts the sparse tensor
    # back to a dense tensor. Scipy sparse matrices are limited to 2D, so use
    # a one-dimensional shape; note also that scipy's default dtype is int64.
    if testing_utils.get_model_type() == "subclass":
      input_name = "input_1"  # Subclass models don"t support input names.
    else:
      input_name = "test_input_name"
    model_input = input_layer.Input(
        shape=(3,), sparse=True, name=input_name, dtype=tf.int64)
    layers = [ToDense(default_value=-1)]
    model = get_model_from_layers_with_input(layers, model_input=model_input)

    input_data = {
        input_name:
            scipy.sparse.coo_matrix(([1, 2, 3], ([0, 1, 1], [0, 0, 1])),
                                    shape=[2, 3])
    }
    expected_output = np.array([[1, -1, -1], [2, 3, -1]])
    output = model.predict(input_data, steps=1)
    self.assertAllEqual(expected_output, output)

    input_data_2 = {
        input_name:
            scipy.sparse.coo_matrix(
                ([5, 6, 7, 8], ([0, 1, 1, 2], [0, 0, 1, 1])), shape=[3, 3])
    }
    expected_output_2 = np.array([[5, -1, -1], [6, 7, -1], [-1, 8, -1]])
    output_2 = model.predict(input_data_2, steps=1)
    self.assertAllEqual(expected_output_2, output_2)

</source>
<source file="systems/keras-2.7.0/keras/utils/composite_tensor_support_test.py" startline="438" endline="473" pcid="1598">
  def test_sparse_scipy_eval_input_dicts(self):
    # Create a model that accepts a sparse input and converts the sparse tensor
    # back to a dense tensor. Scipy sparse matrices are limited to 2D, so use
    # a one-dimensional shape; note also that scipy's default dtype is int64.
    if testing_utils.get_model_type() == "subclass":
      input_name = "input_1"  # Subclass models don"t support input names.
    else:
      input_name = "test_input_name"
    model_input = input_layer.Input(
        shape=(3,), sparse=True, name=input_name, dtype=tf.int64)
    layers = [ToDense(default_value=-1)]
    model = get_model_from_layers_with_input(layers, model_input=model_input)
    model.compile(
        optimizer="sgd",
        loss="mse",
        metrics=["accuracy"])

    input_data = {
        input_name:
            scipy.sparse.coo_matrix(([1, 2, 3], ([0, 1, 1], [0, 0, 1])),
                                    shape=[2, 3])
    }
    expected_output = np.array([[1, -1, -1], [2, 3, -1]])
    output = model.evaluate(input_data, expected_output, steps=1)
    self.assertAllEqual(1.0, output[-1])

    input_data_2 = {
        input_name:
            scipy.sparse.coo_matrix(
                ([5, 6, 7, 8], ([0, 1, 1, 2], [0, 0, 1, 1])), shape=[3, 3])
    }
    expected_output_2 = np.array([[5, -1, -1], [6, 7, -1], [-1, 8, -1]])
    output_2 = model.evaluate(input_data_2, expected_output_2, steps=1)
    self.assertAllEqual(1.0, output_2[-1])


</source>
</class>

<class classid="58" nclones="2" nlines="25" similarity="88">
<source file="systems/keras-2.7.0/keras/utils/composite_tensor_support_test.py" startline="529" endline="557" pcid="1600">
  def test_ragged_tensor_input_with_one_none_dimension(self, use_dict,
                                                       use_dataset):
    # Define some input data.
    data = [(tf.ragged.constant([[[1, 0]], [[2, 3]]], ragged_rank=1),
             np.array([[[1, 0]], [[2, 3]]]))]

    # Prepare the model to test.
    input_shape = (None, 2)  # RaggedTensorInputTest uses (None, None).
    input_name = get_input_name(use_dict)
    model_input = input_layer.Input(
        shape=input_shape, ragged=True, name=input_name, dtype=tf.int32)
    layers = [ToDense(default_value=-1)]
    model = get_model_from_layers_with_input(layers, model_input=model_input)
    model.compile(
        optimizer="sgd",
        loss="mse",
        metrics=["accuracy"],
        **get_test_mode_kwargs())

    for data_element in data:
      input_data, expected_output = prepare_inputs(
          data_element,
          use_dict,
          use_dataset,
          action="predict",
          input_name=input_name)
      result = model.predict(input_data)
      self.assertAllEqual(expected_output, result)

</source>
<source file="systems/keras-2.7.0/keras/utils/composite_tensor_support_test.py" startline="558" endline="588" pcid="1601">
  def test_ragged_tensor_input_with_no_none_dimension(self, use_dict,
                                                      use_dataset):
    # Define some input data.
    data = [(tf.ragged.constant([[[1, 0]], [[2, 3]]], ragged_rank=0),
             np.array([[[1, 0]], [[2, 3]]]))]

    # Prepare the model to test.
    input_shape = (1, 2)  # RaggedTensorInputTest uses (None, None).
    input_name = get_input_name(use_dict)
    model_input = input_layer.Input(
        shape=input_shape, ragged=True, name=input_name, dtype=tf.int32)
    layers = [ToDense(default_value=-1)]
    model = get_model_from_layers_with_input(layers, model_input=model_input)
    model.compile(
        optimizer="sgd",
        loss="mse",
        metrics=["accuracy"],
        **get_test_mode_kwargs())
    kwargs = get_kwargs(use_dataset)

    for data_element in data:
      input_data, expected_output = prepare_inputs(
          data_element,
          use_dict,
          use_dataset,
          action="predict",
          input_name=input_name)
      result = model.predict(input_data, **kwargs)
      self.assertAllEqual(expected_output, result)


</source>
</class>

<class classid="59" nclones="2" nlines="21" similarity="90">
<source file="systems/keras-2.7.0/keras/keras_parameterized.py" startline="42" endline="153" pcid="1619">
def run_with_all_saved_model_formats(
    test_or_class=None,
    exclude_formats=None):
  """Execute the decorated test with all Keras saved model formats).

  This decorator is intended to be applied either to individual test methods in
  a `keras_parameterized.TestCase` class, or directly to a test class that
  extends it. Doing so will cause the contents of the individual test
  method (or all test methods in the class) to be executed multiple times - once
  for each Keras saved model format.

  The Keras saved model formats include:
  1. HDF5: 'h5'
  2. SavedModel: 'tf'

  Note: if stacking this decorator with absl.testing's parameterized decorators,
  those should be at the bottom of the stack.

  Various methods in `testing_utils` to get file path for saved models will
  auto-generate a string of the two saved model formats. This allows unittests
  to confirm the equivalence between the two Keras saved model formats.

  For example, consider the following unittest:

  ```python
  class MyTests(testing_utils.KerasTestCase):

    @testing_utils.run_with_all_saved_model_formats
    def test_foo(self):
      save_format = testing_utils.get_save_format()
      saved_model_dir = '/tmp/saved_model/'
      model = keras.models.Sequential()
      model.add(keras.layers.Dense(2, input_shape=(3,)))
      model.add(keras.layers.Dense(3))
      model.compile(loss='mse', optimizer='sgd', metrics=['acc'])

      keras.models.save_model(model, saved_model_dir, save_format=save_format)
      model = keras.models.load_model(saved_model_dir)

  if __name__ == "__main__":
    tf.test.main()
  ```

  This test tries to save the model into the formats of 'hdf5', 'h5', 'keras',
  'tensorflow', and 'tf'.

  We can also annotate the whole class if we want this to apply to all tests in
  the class:
  ```python
  @testing_utils.run_with_all_saved_model_formats
  class MyTests(testing_utils.KerasTestCase):

    def test_foo(self):
      save_format = testing_utils.get_save_format()
      saved_model_dir = '/tmp/saved_model/'
      model = keras.models.Sequential()
      model.add(keras.layers.Dense(2, input_shape=(3,)))
      model.add(keras.layers.Dense(3))
      model.compile(loss='mse', optimizer='sgd', metrics=['acc'])

      keras.models.save_model(model, saved_model_dir, save_format=save_format)
      model = tf.keras.models.load_model(saved_model_dir)

  if __name__ == "__main__":
    tf.test.main()
  ```

  Args:
    test_or_class: test method or class to be annotated. If None,
      this method returns a decorator that can be applied to a test method or
      test class. If it is not None this returns the decorator applied to the
      test or class.
    exclude_formats: A collection of Keras saved model formats to not run.
      (May also be a single format not wrapped in a collection).
      Defaults to None.

  Returns:
    Returns a decorator that will run the decorated test method multiple times:
    once for each desired Keras saved model format.

  Raises:
    ImportError: If abseil parameterized is not installed or not included as
      a target dependency.
  """
  # Exclude h5 save format if H5py isn't available.
  if h5py is None:
    exclude_formats.append(['h5'])
  saved_model_formats = ['h5', 'tf', 'tf_no_traces']
  params = [('_%s' % saved_format, saved_format)
            for saved_format in saved_model_formats
            if saved_format not in tf.nest.flatten(exclude_formats)]

  def single_method_decorator(f):
    """Decorator that constructs the test cases."""
    # Use named_parameters so it can be individually run from the command line
    @parameterized.named_parameters(*params)
    @functools.wraps(f)
    def decorated(self, saved_format, *args, **kwargs):
      """A run of a single test case w/ the specified model type."""
      if saved_format == 'h5':
        _test_h5_saved_model_format(f, self, *args, **kwargs)
      elif saved_format == 'tf':
        _test_tf_saved_model_format(f, self, *args, **kwargs)
      elif saved_format == 'tf_no_traces':
        _test_tf_saved_model_format_no_traces(f, self, *args, **kwargs)
      else:
        raise ValueError('Unknown model type: %s' % (saved_format,))
    return decorated

  return _test_or_class_decorator(test_or_class, single_method_decorator)


</source>
<source file="systems/keras-2.7.0/keras/keras_parameterized.py" startline="178" endline="293" pcid="1626">
def run_with_all_model_types(
    test_or_class=None,
    exclude_models=None):
  """Execute the decorated test with all Keras model types.

  This decorator is intended to be applied either to individual test methods in
  a `keras_parameterized.TestCase` class, or directly to a test class that
  extends it. Doing so will cause the contents of the individual test
  method (or all test methods in the class) to be executed multiple times - once
  for each Keras model type.

  The Keras model types are: ['functional', 'subclass', 'sequential']

  Note: if stacking this decorator with absl.testing's parameterized decorators,
  those should be at the bottom of the stack.

  Various methods in `testing_utils` to get models will auto-generate a model
  of the currently active Keras model type. This allows unittests to confirm
  the equivalence between different Keras models.

  For example, consider the following unittest:

  ```python
  class MyTests(testing_utils.KerasTestCase):

    @testing_utils.run_with_all_model_types(
      exclude_models = ['sequential'])
    def test_foo(self):
      model = testing_utils.get_small_mlp(1, 4, input_dim=3)
      optimizer = RMSPropOptimizer(learning_rate=0.001)
      loss = 'mse'
      metrics = ['mae']
      model.compile(optimizer, loss, metrics=metrics)

      inputs = np.zeros((10, 3))
      targets = np.zeros((10, 4))
      dataset = dataset_ops.Dataset.from_tensor_slices((inputs, targets))
      dataset = dataset.repeat(100)
      dataset = dataset.batch(10)

      model.fit(dataset, epochs=1, steps_per_epoch=2, verbose=1)

  if __name__ == "__main__":
    tf.test.main()
  ```

  This test tries building a small mlp as both a functional model and as a
  subclass model.

  We can also annotate the whole class if we want this to apply to all tests in
  the class:
  ```python
  @testing_utils.run_with_all_model_types(exclude_models = ['sequential'])
  class MyTests(testing_utils.KerasTestCase):

    def test_foo(self):
      model = testing_utils.get_small_mlp(1, 4, input_dim=3)
      optimizer = RMSPropOptimizer(learning_rate=0.001)
      loss = 'mse'
      metrics = ['mae']
      model.compile(optimizer, loss, metrics=metrics)

      inputs = np.zeros((10, 3))
      targets = np.zeros((10, 4))
      dataset = dataset_ops.Dataset.from_tensor_slices((inputs, targets))
      dataset = dataset.repeat(100)
      dataset = dataset.batch(10)

      model.fit(dataset, epochs=1, steps_per_epoch=2, verbose=1)

  if __name__ == "__main__":
    tf.test.main()
  ```


  Args:
    test_or_class: test method or class to be annotated. If None,
      this method returns a decorator that can be applied to a test method or
      test class. If it is not None this returns the decorator applied to the
      test or class.
    exclude_models: A collection of Keras model types to not run.
      (May also be a single model type not wrapped in a collection).
      Defaults to None.

  Returns:
    Returns a decorator that will run the decorated test method multiple times:
    once for each desired Keras model type.

  Raises:
    ImportError: If abseil parameterized is not installed or not included as
      a target dependency.
  """
  model_types = ['functional', 'subclass', 'sequential']
  params = [('_%s' % model, model) for model in model_types
            if model not in tf.nest.flatten(exclude_models)]

  def single_method_decorator(f):
    """Decorator that constructs the test cases."""
    # Use named_parameters so it can be individually run from the command line
    @parameterized.named_parameters(*params)
    @functools.wraps(f)
    def decorated(self, model_type, *args, **kwargs):
      """A run of a single test case w/ the specified model type."""
      if model_type == 'functional':
        _test_functional_model_type(f, self, *args, **kwargs)
      elif model_type == 'subclass':
        _test_subclass_model_type(f, self, *args, **kwargs)
      elif model_type == 'sequential':
        _test_sequential_model_type(f, self, *args, **kwargs)
      else:
        raise ValueError('Unknown model type: %s' % (model_type,))
    return decorated

  return _test_or_class_decorator(test_or_class, single_method_decorator)


</source>
</class>

<class classid="60" nclones="3" nlines="13" similarity="84">
<source file="systems/keras-2.7.0/keras/keras_parameterized.py" startline="134" endline="150" pcid="1620">
  def single_method_decorator(f):
    """Decorator that constructs the test cases."""
    # Use named_parameters so it can be individually run from the command line
    @parameterized.named_parameters(*params)
    @functools.wraps(f)
    def decorated(self, saved_format, *args, **kwargs):
      """A run of a single test case w/ the specified model type."""
      if saved_format == 'h5':
        _test_h5_saved_model_format(f, self, *args, **kwargs)
      elif saved_format == 'tf':
        _test_tf_saved_model_format(f, self, *args, **kwargs)
      elif saved_format == 'tf_no_traces':
        _test_tf_saved_model_format_no_traces(f, self, *args, **kwargs)
      else:
        raise ValueError('Unknown model type: %s' % (saved_format,))
    return decorated

</source>
<source file="systems/keras-2.7.0/keras/keras_parameterized.py" startline="391" endline="409" pcid="1633">
  def single_method_decorator(f):
    """Decorator that constructs the test cases."""

    # Use named_parameters so it can be individually run from the command line
    @parameterized.named_parameters(*params)
    @functools.wraps(f)
    def decorated(self, run_mode, *args, **kwargs):
      """A run of a single test case w/ specified run mode."""
      if run_mode == 'v1_session':
        _v1_session_test(f, self, config, *args, **kwargs)
      elif run_mode == 'v2_eager':
        _v2_eager_test(f, self, *args, **kwargs)
      elif run_mode == 'v2_function':
        _v2_function_test(f, self, *args, **kwargs)
      else:
        return ValueError('Unknown run mode %s' % run_mode)

    return decorated

</source>
<source file="systems/keras-2.7.0/keras/keras_parameterized.py" startline="274" endline="290" pcid="1627">
  def single_method_decorator(f):
    """Decorator that constructs the test cases."""
    # Use named_parameters so it can be individually run from the command line
    @parameterized.named_parameters(*params)
    @functools.wraps(f)
    def decorated(self, model_type, *args, **kwargs):
      """A run of a single test case w/ the specified model type."""
      if model_type == 'functional':
        _test_functional_model_type(f, self, *args, **kwargs)
      elif model_type == 'subclass':
        _test_subclass_model_type(f, self, *args, **kwargs)
      elif model_type == 'sequential':
        _test_sequential_model_type(f, self, *args, **kwargs)
      else:
        raise ValueError('Unknown model type: %s' % (model_type,))
    return decorated

</source>
</class>

<class classid="61" nclones="2" nlines="21" similarity="81">
<source file="systems/keras-2.7.0/keras/integration_test/preprocessing_applied_in_model_test.py" startline="49" endline="76" pcid="1644">
  def testDistributedModelFit(self, strategy):
    if (not tf.__internal__.tf2.enabled()
        and isinstance(strategy,
                       tf.distribute.experimental.ParameterServerStrategy)):
      self.skipTest(
          "Parameter Server strategy with dataset creator need to be run when "
          "eager execution is enabled.")
    with strategy.scope():
      preprocessing_model = utils.make_preprocessing_model(self.get_temp_dir())
      training_model = utils.make_training_model()
      # Merge the two separate models into a single model for training.
      inputs = preprocessing_model.inputs
      outputs = training_model(preprocessing_model(inputs))
      merged_model = tf.keras.Model(inputs, outputs)
      merged_model.compile(optimizer="sgd", loss="binary_crossentropy")

    def dataset_fn(input_context):
      dataset = utils.make_dataset()
      dataset = dataset.shard(input_context.num_input_pipelines,
                              input_context.input_pipeline_id)
      batch_size = input_context.get_per_replica_batch_size(
          global_batch_size=utils.BATCH_SIZE)
      return dataset.batch(batch_size).repeat().prefetch(2)

    dataset_creator = tf.keras.utils.experimental.DatasetCreator(dataset_fn)
    merged_model.fit(dataset_creator, epochs=2, steps_per_epoch=utils.STEPS)


</source>
<source file="systems/keras-2.7.0/keras/integration_test/preprocessing_applied_in_dataset_creator_test.py" startline="48" endline="72" pcid="1730">
  def testDistributedModelFit(self, strategy):
    if (not tf.__internal__.tf2.enabled()
        and isinstance(strategy,
                       tf.distribute.experimental.ParameterServerStrategy)):
      self.skipTest(
          "Parameter Server strategy with dataset creator need to be run when "
          "eager execution is enabled.")
    with strategy.scope():
      preprocessing_model = utils.make_preprocessing_model(self.get_temp_dir())
      training_model = utils.make_training_model()
      training_model.compile(optimizer="sgd", loss="binary_crossentropy")

    def dataset_fn(input_context):
      dataset = utils.make_dataset()
      dataset = dataset.shard(input_context.num_input_pipelines,
                              input_context.input_pipeline_id)
      batch_size = input_context.get_per_replica_batch_size(
          global_batch_size=utils.BATCH_SIZE)
      dataset = dataset.batch(batch_size).repeat().prefetch(2)
      return dataset.map(lambda x, y: (preprocessing_model(x), y))

    dataset_creator = tf.keras.utils.experimental.DatasetCreator(dataset_fn)
    training_model.fit(dataset_creator, epochs=2, steps_per_epoch=utils.STEPS)


</source>
</class>

<class classid="62" nclones="2" nlines="28" similarity="82">
<source file="systems/keras-2.7.0/keras/integration_test/tpu_strategy_test.py" startline="56" endline="86" pcid="1650">
  def define_kpls_for_training(self, use_adapt):
    if use_adapt:
      feature_lookup_layer = (
          tf.keras.layers.StringLookup(
              num_oov_indices=1))
      feature_lookup_layer.adapt(FEATURE_VOCAB)
      label_lookup_layer = (
          tf.keras.layers.StringLookup(
              num_oov_indices=0, mask_token=None))
      label_lookup_layer.adapt(LABEL_VOCAB)
    else:
      feature_lookup_layer = (
          tf.keras.layers.StringLookup(
              vocabulary=FEATURE_VOCAB, num_oov_indices=1))
      label_lookup_layer = (
          tf.keras.layers.StringLookup(
              vocabulary=LABEL_VOCAB, num_oov_indices=0, mask_token=None))

    raw_feature_input = tf.keras.layers.Input(
        shape=(3,), dtype=tf.dtypes.string, name="feature", ragged=True)
    feature_id_input = feature_lookup_layer(raw_feature_input)
    feature_mapper = tf.keras.Model({"features": raw_feature_input},
                                    feature_id_input)

    raw_label_input = tf.keras.layers.Input(
        shape=(1,), dtype=tf.dtypes.string, name="label")
    label_id_input = label_lookup_layer(raw_label_input)
    label_mapper = tf.keras.Model({"label": raw_label_input}, label_id_input)

    return feature_mapper, label_mapper

</source>
<source file="systems/keras-2.7.0/keras/integration_test/parameter_server_keras_preprocessing_test.py" startline="83" endline="121" pcid="1664">
  def define_kpls_for_training(self, use_adapt):
    # Define KPLs under strategy's scope. Right now, if they have look up
    # tables, they will be created on the client. Their variables will be
    # created on PS. Ideally they should be cached on each worker since they
    # will not be changed in a training step.
    if use_adapt:
      feature_lookup_layer = (
          tf.keras.layers.StringLookup(
              num_oov_indices=1))
      feature_lookup_layer.adapt(FEATURE_VOCAB)
      label_lookup_layer = (
          tf.keras.layers.StringLookup(
              num_oov_indices=0, mask_token=None))
      label_lookup_layer.adapt(LABEL_VOCAB)
    else:
      # Do vocab shuffling.
      shuffled_vocab = FEATURE_VOCAB.copy()
      random.shuffle(shuffled_vocab)
      feature_lookup_layer = (
          tf.keras.layers.StringLookup(
              vocabulary=shuffled_vocab, num_oov_indices=1))
      label_lookup_layer = (
          tf.keras.layers.StringLookup(
              vocabulary=LABEL_VOCAB, num_oov_indices=0, mask_token=None))

    raw_feature_input = tf.keras.Input(
        shape=(3,), dtype=tf.string, name="feature", ragged=True)
    feature_id_input = feature_lookup_layer(raw_feature_input)

    # Model creates variables as well.
    feature_ps = tf.keras.Model({"features": raw_feature_input},
                                feature_id_input)

    raw_label_input = tf.keras.Input(shape=(1,), dtype=tf.string, name="label")
    label_id_input = label_lookup_layer(raw_label_input)
    label_ps = tf.keras.Model({"label": raw_label_input}, label_id_input)

    return feature_ps, label_ps

</source>
</class>

<class classid="63" nclones="2" nlines="12" similarity="83">
<source file="systems/keras-2.7.0/keras/integration_test/tpu_strategy_test.py" startline="203" endline="218" pcid="1659">
      def create_serving_signature(model):

        @tf.function
        def serve_fn(raw_features):
          raw_features = tf.expand_dims(raw_features, axis=0)
          transformed_features = model.feature_mapper(raw_features)
          outputs = model(transformed_features)
          outputs = tf.squeeze(outputs, axis=0)
          outputs = tf.cast(tf.math.greater(outputs, 0.5), tf.dtypes.int64)
          decoded_outputs = model.label_inverse_lookup_layer(outputs)
          return tf.squeeze(decoded_outputs, axis=0)

        # Serving does NOT have batch dimension
        return serve_fn.get_concrete_function(
            tf.TensorSpec(shape=(3), dtype=tf.dtypes.string, name="example"))

</source>
<source file="systems/keras-2.7.0/keras/integration_test/parameter_server_keras_preprocessing_test.py" startline="215" endline="230" pcid="1671">
    def create_serving_signature(model):

      @tf.function
      def serve_fn(raw_features):
        raw_features = tf.expand_dims(raw_features, axis=0)
        transformed_features = model.feature_ps(raw_features)
        outputs = model(transformed_features)
        outputs = tf.squeeze(outputs, axis=0)
        outputs = tf.cast(tf.greater(outputs, 0.5), tf.int64)
        decoded_outputs = model.label_inverse_lookup_layer(outputs)
        return tf.squeeze(decoded_outputs, axis=0)

      # serving does NOT have batch dimension
      return serve_fn.get_concrete_function(
          tf.TensorSpec(shape=(3), dtype=tf.string, name="example"))

</source>
</class>

<class classid="64" nclones="2" nlines="22" similarity="95">
<source file="systems/keras-2.7.0/keras/integration_test/parameter_server_keras_preprocessing_test.py" startline="36" endline="68" pcid="1662">
def create_in_process_cluster(num_workers, num_ps):
  """Creates and starts local servers and returns the cluster_resolver."""

  worker_ports = [portpicker.pick_unused_port() for _ in range(num_workers)]
  ps_ports = [portpicker.pick_unused_port() for _ in range(num_ps)]

  cluster_dict = {}
  cluster_dict["worker"] = ["localhost:%s" % port for port in worker_ports]
  if num_ps > 0:
    cluster_dict["ps"] = ["localhost:%s" % port for port in ps_ports]

  cluster_spec = tf.train.ClusterSpec(cluster_dict)

  # Workers need some inter_ops threads to work properly.
  worker_config = tf.compat.v1.ConfigProto()
  if multiprocessing.cpu_count() < num_workers + 1:
    worker_config.inter_op_parallelism_threads = num_workers + 1

  for i in range(num_workers):
    tf.distribute.Server(
        cluster_spec,
        job_name="worker",
        task_index=i,
        config=worker_config,
        protocol="grpc")

  for i in range(num_ps):
    tf.distribute.Server(
        cluster_spec, job_name="ps", task_index=i, protocol="grpc")

  return cluster_spec


</source>
<source file="systems/keras-2.7.0/keras/integration_test/parameter_server_custom_training_loop_test.py" startline="32" endline="62" pcid="1738">
  def create_in_process_cluster(self, num_workers, num_ps):
    """Creates and starts local servers and returns the cluster_resolver."""
    worker_ports = [portpicker.pick_unused_port() for _ in range(num_workers)]
    ps_ports = [portpicker.pick_unused_port() for _ in range(num_ps)]

    cluster_dict = {}
    cluster_dict["worker"] = ["localhost:%s" % port for port in worker_ports]
    if num_ps > 0:
      cluster_dict["ps"] = ["localhost:%s" % port for port in ps_ports]

    cluster_spec = tf.train.ClusterSpec(cluster_dict)

    # Workers need some inter_ops threads to work properly.
    worker_config = tf.compat.v1.ConfigProto()
    if multiprocessing.cpu_count() < num_workers + 1:
      worker_config.inter_op_parallelism_threads = num_workers + 1

    for i in range(num_workers):
      tf.distribute.Server(
          cluster_spec,
          job_name="worker",
          task_index=i,
          config=worker_config,
          protocol="grpc")

    for i in range(num_ps):
      tf.distribute.Server(
          cluster_spec, job_name="ps", task_index=i, protocol="grpc")

    return cluster_spec

</source>
</class>

<class classid="65" nclones="3" nlines="10" similarity="90">
<source file="systems/keras-2.7.0/keras/integration_test/parameter_server_keras_preprocessing_test.py" startline="71" endline="82" pcid="1663">
  def setUp(self):
    super(KPLTest, self).setUp()

    cluster_spec = create_in_process_cluster(num_workers=3, num_ps=2)
    cluster_resolver = tf.distribute.cluster_resolver.SimpleClusterResolver(
        cluster_spec, rpc_layer="grpc")
    self.strategy = tf.distribute.experimental.ParameterServerStrategy(
        cluster_resolver)
    self.coordinator = (
        tf.distribute.experimental.coordinator.ClusterCoordinator(
            self.strategy))

</source>
<source file="systems/keras-2.7.0/keras/integration_test/parameter_server_keras_preprocessing_test.py" startline="268" endline="279" pcid="1673">
  def setUp(self):
    super(KPLCreatedInDatasetsFromFunctionTest, self).setUp()

    cluster_spec = create_in_process_cluster(num_workers=3, num_ps=2)
    cluster_resolver = tf.distribute.cluster_resolver.SimpleClusterResolver(
        cluster_spec, rpc_layer="grpc")
    self.strategy = tf.distribute.experimental.ParameterServerStrategy(
        cluster_resolver)
    self.coordinator = (
        tf.distribute.experimental.coordinator.ClusterCoordinator(
            self.strategy))

</source>
<source file="systems/keras-2.7.0/keras/integration_test/parameter_server_custom_training_loop_test.py" startline="63" endline="74" pcid="1739">
  def setUp(self):
    super(ParameterServerCustomTrainingLoopTest, self).setUp()

    cluster_spec = self.create_in_process_cluster(num_workers=3, num_ps=2)
    cluster_resolver = tf.distribute.cluster_resolver.SimpleClusterResolver(
        cluster_spec, rpc_layer="grpc")
    self.strategy = tf.distribute.experimental.ParameterServerStrategy(
        cluster_resolver)
    self.coordinator = (
        tf.distribute.experimental.coordinator.ClusterCoordinator(
            self.strategy))

</source>
</class>

<class classid="66" nclones="2" nlines="21" similarity="90">
<source file="systems/keras-2.7.0/keras/losses_test.py" startline="72" endline="95" pcid="1764">
  def test_categorical_crossentropy_loss(self):
    target = backend.variable(np.random.randint(0, 1, (5, 1)))
    logits = backend.variable(np.random.random((5, 1)))
    softmax_output = backend.softmax(logits)
    output_from_logit = losses.categorical_crossentropy(
        target, logits, from_logits=True)
    output_from_softmax = losses.categorical_crossentropy(
        target, softmax_output)
    np.testing.assert_allclose(
        backend.eval(output_from_logit),
        backend.eval(output_from_softmax),
        atol=1e-5)

    axis = 0
    output_from_logit_axis = losses.categorical_crossentropy(
        target, logits, from_logits=True, axis=axis)
    output_from_softmax_axis = losses.categorical_crossentropy(
        target, softmax_output, axis=axis)

    np.testing.assert_allclose(
        backend.eval(output_from_logit_axis),
        backend.eval(output_from_softmax_axis),
        atol=1e-5)

</source>
<source file="systems/keras-2.7.0/keras/losses_test.py" startline="183" endline="205" pcid="1769">
  def test_binary_crossentropy_loss(self):
    target = backend.variable(np.random.randint(0, 1, (5, 1)))
    logits = backend.variable(np.random.random((5, 1)))
    sigmoid_output = backend.sigmoid(logits)
    output_from_logit = losses.binary_crossentropy(
        target, logits, from_logits=True)
    output_from_sigmoid = losses.binary_crossentropy(target, sigmoid_output)
    np.testing.assert_allclose(
        backend.eval(output_from_logit),
        backend.eval(output_from_sigmoid),
        atol=1e-5)

    axis = 0
    output_from_logit_axis = losses.binary_crossentropy(
        target, logits, from_logits=True, axis=axis)
    output_from_sigmoid_axis = losses.binary_crossentropy(
        target, sigmoid_output, axis=axis)

    np.testing.assert_allclose(
        backend.eval(output_from_logit_axis),
        backend.eval(output_from_sigmoid_axis),
        atol=1e-5)

</source>
</class>

<class classid="67" nclones="2" nlines="18" similarity="88">
<source file="systems/keras-2.7.0/keras/losses_test.py" startline="97" endline="120" pcid="1765">
  def test_categorical_crossentropy_loss_with_unknown_rank_tensor(self):
    t = backend.placeholder()
    p = backend.placeholder()
    o = losses.categorical_crossentropy(t, p)

    t_val = tf.convert_to_tensor([[1., 0., 0.], [0., 1., 0.],
                                                    [0., 0., 1.]])
    p_val = tf.convert_to_tensor([[.9, .05, .05],
                                                    [.05, .89, .06],
                                                    [.05, .01, .94]])
    f = backend.function([t, p], o)

    result = f([t_val, p_val])
    self.assertArrayNear(result, [.105, .116, .062], 1e-3)

    # from logits
    p_val = tf.convert_to_tensor([[8., 1., 1.], [0., 9., 1.],
                                                    [2., 3., 5.]])
    o = losses.categorical_crossentropy(t, p, from_logits=True)
    f = backend.function([t, p], o)

    result = f([t_val, p_val])
    self.assertArrayNear(result, [.002, 0, .17], 1e-3)

</source>
<source file="systems/keras-2.7.0/keras/losses_test.py" startline="136" endline="160" pcid="1767">
  def test_sparse_categorical_crossentropy_loss_with_unknown_rank_tensor(self):
    # This test only runs in graph because the TF op layer is not supported yet
    # for sparse ops.
    t = backend.placeholder()
    p = backend.placeholder()
    o = losses.sparse_categorical_crossentropy(t, p)

    t_val = tf.convert_to_tensor([0, 1, 2])
    p_val = tf.convert_to_tensor([[.9, .05, .05],
                                                    [.05, .89, .06],
                                                    [.05, .01, .94]])
    f = backend.function([t, p], o)

    result = f([t_val, p_val])
    self.assertArrayNear(result, [.105, .116, .062], 1e-3)

    # from logits
    p_val = tf.convert_to_tensor([[8., 1., 1.], [0., 9., 1.],
                                                    [2., 3., 5.]])
    o = losses.sparse_categorical_crossentropy(t, p, from_logits=True)
    f = backend.function([t, p], o)

    result = f([t_val, p_val])
    self.assertArrayNear(result, [.002, 0, .17], 1e-3)

</source>
</class>

<class classid="68" nclones="9" nlines="11" similarity="72">
<source file="systems/keras-2.7.0/keras/losses_test.py" startline="795" endline="809" pcid="1833">
  def test_all_correct_unweighted(self):
    y_true = tf.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]],
                                  dtype=tf.float32)
    bce_obj = losses.BinaryCrossentropy()
    loss = bce_obj(y_true, y_true)
    self.assertAlmostEqual(self.evaluate(loss), 0.0, 3)

    # Test with logits.
    logits = tf.constant([[100.0, -100.0, -100.0],
                                   [-100.0, 100.0, -100.0],
                                   [-100.0, -100.0, 100.0]])
    bce_obj = losses.BinaryCrossentropy(from_logits=True)
    loss = bce_obj(y_true, logits)
    self.assertAlmostEqual(self.evaluate(loss), 0.0, 3)

</source>
<source file="systems/keras-2.7.0/keras/losses_test.py" startline="1010" endline="1024" pcid="1842">
  def test_all_correct_unweighted(self):
    y_true = tf.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]],
                                  dtype=tf.int64)
    y_pred = tf.constant([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]],
                                  dtype=tf.float32)
    cce_obj = losses.CategoricalCrossentropy()
    loss = cce_obj(y_true, y_pred)
    self.assertAlmostEqual(self.evaluate(loss), 0.0, 3)

    # Test with logits.
    logits = tf.constant([[10., 0., 0.], [0., 10., 0.], [0., 0., 10.]])
    cce_obj = losses.CategoricalCrossentropy(from_logits=True)
    loss = cce_obj(y_true, logits)
    self.assertAlmostEqual(self.evaluate(loss), 0.0, 3)

</source>
<source file="systems/keras-2.7.0/keras/losses_test.py" startline="1180" endline="1193" pcid="1853">
  def test_all_correct_unweighted(self):
    y_true = tf.constant([[0], [1], [2]], dtype=tf.int64)
    y_pred = tf.constant([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]],
                                  dtype=tf.float32)
    cce_obj = losses.SparseCategoricalCrossentropy()
    loss = cce_obj(y_true, y_pred)
    self.assertAlmostEqual(self.evaluate(loss), 0.0, 3)

    # Test with logits.
    logits = tf.constant([[10., 0., 0.], [0., 10., 0.], [0., 0., 10.]])
    cce_obj = losses.SparseCategoricalCrossentropy(from_logits=True)
    loss = cce_obj(y_true, logits)
    self.assertAlmostEqual(self.evaluate(loss), 0.0, 3)

</source>
<source file="systems/keras-2.7.0/keras/losses_test.py" startline="1025" endline="1038" pcid="1843">
  def test_unweighted(self):
    cce_obj = losses.CategoricalCrossentropy()
    y_true = tf.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]])
    y_pred = tf.constant(
        [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]], dtype=tf.float32)
    loss = cce_obj(y_true, y_pred)
    self.assertAlmostEqual(self.evaluate(loss), .3239, 3)

    # Test with logits.
    logits = tf.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])
    cce_obj = losses.CategoricalCrossentropy(from_logits=True)
    loss = cce_obj(y_true, logits)
    self.assertAlmostEqual(self.evaluate(loss), .0573, 3)

</source>
<source file="systems/keras-2.7.0/keras/losses_test.py" startline="1194" endline="1207" pcid="1854">
  def test_unweighted(self):
    cce_obj = losses.SparseCategoricalCrossentropy()
    y_true = tf.constant([0, 1, 2])
    y_pred = tf.constant(
        [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]], dtype=tf.float32)
    loss = cce_obj(y_true, y_pred)
    self.assertAlmostEqual(self.evaluate(loss), .3239, 3)

    # Test with logits.
    logits = tf.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])
    cce_obj = losses.SparseCategoricalCrossentropy(from_logits=True)
    loss = cce_obj(y_true, logits)
    self.assertAlmostEqual(self.evaluate(loss), .0573, 3)

</source>
<source file="systems/keras-2.7.0/keras/losses_test.py" startline="1053" endline="1067" pcid="1845">
  def test_sample_weighted(self):
    cce_obj = losses.CategoricalCrossentropy()
    y_true = tf.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]])
    y_pred = tf.constant(
        [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]], dtype=tf.float32)
    sample_weight = tf.constant([[1.2], [3.4], [5.6]], shape=(3, 1))
    loss = cce_obj(y_true, y_pred, sample_weight=sample_weight)
    self.assertAlmostEqual(self.evaluate(loss), 1.0696, 3)

    # Test with logits.
    logits = tf.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])
    cce_obj = losses.CategoricalCrossentropy(from_logits=True)
    loss = cce_obj(y_true, logits, sample_weight=sample_weight)
    self.assertAlmostEqual(self.evaluate(loss), 0.31829, 3)

</source>
<source file="systems/keras-2.7.0/keras/losses_test.py" startline="1039" endline="1052" pcid="1844">
  def test_scalar_weighted(self):
    cce_obj = losses.CategoricalCrossentropy()
    y_true = tf.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]])
    y_pred = tf.constant(
        [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]], dtype=tf.float32)
    loss = cce_obj(y_true, y_pred, sample_weight=2.3)
    self.assertAlmostEqual(self.evaluate(loss), .7449, 3)

    # Test with logits.
    logits = tf.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])
    cce_obj = losses.CategoricalCrossentropy(from_logits=True)
    loss = cce_obj(y_true, logits, sample_weight=2.3)
    self.assertAlmostEqual(self.evaluate(loss), .1317, 3)

</source>
<source file="systems/keras-2.7.0/keras/losses_test.py" startline="1208" endline="1221" pcid="1855">
  def test_scalar_weighted(self):
    cce_obj = losses.SparseCategoricalCrossentropy()
    y_true = tf.constant([[0], [1], [2]])
    y_pred = tf.constant(
        [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]], dtype=tf.float32)
    loss = cce_obj(y_true, y_pred, sample_weight=2.3)
    self.assertAlmostEqual(self.evaluate(loss), .7449, 3)

    # Test with logits.
    logits = tf.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])
    cce_obj = losses.SparseCategoricalCrossentropy(from_logits=True)
    loss = cce_obj(y_true, logits, sample_weight=2.3)
    self.assertAlmostEqual(self.evaluate(loss), .1317, 3)

</source>
<source file="systems/keras-2.7.0/keras/losses_test.py" startline="1222" endline="1236" pcid="1856">
  def test_sample_weighted(self):
    cce_obj = losses.SparseCategoricalCrossentropy()
    y_true = tf.constant([[0], [1], [2]])
    y_pred = tf.constant(
        [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]], dtype=tf.float32)
    sample_weight = tf.constant([[1.2], [3.4], [5.6]], shape=(3, 1))
    loss = cce_obj(y_true, y_pred, sample_weight=sample_weight)
    self.assertAlmostEqual(self.evaluate(loss), 1.0696, 3)

    # Test with logits.
    logits = tf.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])
    cce_obj = losses.SparseCategoricalCrossentropy(from_logits=True)
    loss = cce_obj(y_true, logits, sample_weight=sample_weight)
    self.assertAlmostEqual(self.evaluate(loss), 0.31829, 3)

</source>
</class>

<class classid="69" nclones="2" nlines="12" similarity="75">
<source file="systems/keras-2.7.0/keras/losses_test.py" startline="810" endline="847" pcid="1834">
  def test_unweighted(self):
    y_true = np.asarray([1, 0, 1, 0]).reshape([2, 2])
    y_pred = np.asarray([1, 1, 1, 0], dtype=np.float32).reshape([2, 2])
    bce_obj = losses.BinaryCrossentropy()
    loss = bce_obj(y_true, y_pred)

    # EPSILON = 1e-7, y = y_true, y` = y_pred, Y_MAX = 0.9999999
    # y` = clip_ops.clip_by_value(output, EPSILON, 1. - EPSILON)
    # y` = [Y_MAX, Y_MAX, Y_MAX, EPSILON]

    # Loss = -(y log(y` + EPSILON) + (1 - y) log(1 - y` + EPSILON))
    #      = [-log(Y_MAX + EPSILON), -log(1 - Y_MAX + EPSILON),
    #         -log(Y_MAX + EPSILON), -log(1)]
    #      = [0, 15.33, 0, 0]
    # Reduced loss = 15.33 / 4

    self.assertAlmostEqual(self.evaluate(loss), 3.833, 3)

    # Test with logits.
    y_true = tf.constant([[1, 0, 1], [0, 1, 1]])
    logits = tf.constant([[100.0, -100.0, 100.0],
                                   [100.0, 100.0, -100.0]])
    bce_obj = losses.BinaryCrossentropy(from_logits=True)
    loss = bce_obj(y_true, logits)

    # Loss = max(x, 0) - x * z + log(1 + exp(-abs(x)))
    #            (where x = logits and z = y_true)
    #      = [((100 - 100 * 1 + log(1 + exp(-100))) +
    #          (0 + 100 * 0 + log(1 + exp(-100))) +
    #          (100 - 100 * 1 + log(1 + exp(-100))),
    #         ((100 - 100 * 0 + log(1 + exp(-100))) +
    #          (100 - 100 * 1 + log(1 + exp(-100))) +
    #          (0 + 100 * 1 + log(1 + exp(-100))))]
    #      = [(0 + 0 + 0) / 3, 200 / 3]
    # Reduced loss = (0 + 66.666) / 2

    self.assertAlmostEqual(self.evaluate(loss), 33.333, 3)

</source>
<source file="systems/keras-2.7.0/keras/losses_test.py" startline="848" endline="881" pcid="1835">
  def test_scalar_weighted(self):
    bce_obj = losses.BinaryCrossentropy()
    y_true = np.asarray([1, 0, 1, 0]).reshape([2, 2])
    y_pred = np.asarray([1, 1, 1, 0], dtype=np.float32).reshape([2, 2])
    loss = bce_obj(y_true, y_pred, sample_weight=2.3)

    # EPSILON = 1e-7, y = y_true, y` = y_pred, Y_MAX = 0.9999999
    # y` = clip_ops.clip_by_value(output, EPSILON, 1. - EPSILON)
    # y` = [Y_MAX, Y_MAX, Y_MAX, EPSILON]

    # Loss = -(y log(y` + EPSILON) + (1 - y) log(1 - y` + EPSILON))
    #      = [-log(Y_MAX + EPSILON), -log(1 - Y_MAX + EPSILON),
    #         -log(Y_MAX + EPSILON), -log(1)]
    #      = [0, 15.33, 0, 0]
    # Weighted loss = [0, 15.33 * 2.3, 0, 0]
    # Reduced loss = 15.33 * 2.3 / 4

    self.assertAlmostEqual(self.evaluate(loss), 8.817, 3)

    # Test with logits.
    y_true = tf.constant([[1, 0, 1], [0, 1, 1]])
    logits = tf.constant([[100.0, -100.0, 100.0],
                                   [100.0, 100.0, -100.0]])
    bce_obj = losses.BinaryCrossentropy(from_logits=True)
    loss = bce_obj(y_true, logits, sample_weight=2.3)

    # Loss = max(x, 0) - x * z + log(1 + exp(-abs(x)))
    #            (where x = logits and z = y_true)
    # Loss = [(0 + 0 + 0) / 3, 200 / 3]
    # Weighted loss = [0 * 2.3, 66.666 * 2.3]
    # Reduced loss = (0 + 66.666 * 2.3) / 2

    self.assertAlmostEqual(self.evaluate(loss), 76.667, 3)

</source>
</class>

<class classid="70" nclones="4" nlines="14" similarity="73">
<source file="systems/keras-2.7.0/keras/losses_test.py" startline="1129" endline="1148" pcid="1850">
  def test_ragged_tensors(self):
    cce_obj = losses.CategoricalCrossentropy()
    y_true = tf.ragged.constant([[[1, 0, 0], [0, 1, 0]], [[0, 0, 1]]])
    y_pred = tf.ragged.constant(
        [[[.9, .05, .05], [.5, .89, .6]], [[.05, .01, .94]]],
        dtype=tf.float32)
    # batch losses [[0.1054, 0.8047], [0.0619]]
    sample_weight = tf.constant([[1.2], [3.4]], shape=(2, 1))
    loss = cce_obj(y_true, y_pred, sample_weight=sample_weight)
    # sum([0.1054, 0.8047, 0.0619]) / 3
    self.assertAlmostEqual(self.evaluate(loss), 0.4341, 3)

    # Test with logits.
    logits = tf.ragged.constant([[[8., 1., 1.], [0., 9., 1.]],
                                          [[2., 3., 5.]]])
    cce_obj = losses.CategoricalCrossentropy(from_logits=True)
    # batch losses [[0.0018, 0.0004], [0.1698]]
    loss = cce_obj(y_true, logits, sample_weight=sample_weight)
    self.assertAlmostEqual(self.evaluate(loss), 0.1934, 3)

</source>
<source file="systems/keras-2.7.0/keras/losses_test.py" startline="1273" endline="1293" pcid="1860">
  def test_ragged_tensors_rank_1(self):
    cce_obj = losses.SparseCategoricalCrossentropy()
    y_true = tf.ragged.constant([[0, 1], [2]])
    y_pred = tf.ragged.constant(
        [[[.9, .05, .05], [.5, .89, .6]], [[.05, .01, .94]]],
        ragged_rank=1,
        dtype=tf.float32)
    # batch losses [[0.1054, 0.8047], [0.0619]]
    sample_weight = tf.constant([[1.2], [3.4]], shape=(2, 1))
    loss = cce_obj(y_true, y_pred, sample_weight=sample_weight)
    # sum([0.1054, 0.8047, 0.0619]) / 3
    self.assertAlmostEqual(self.evaluate(loss), 0.4341, 3)

    # Test with logits.
    logits = tf.ragged.constant(
        [[[8., 1., 1.], [0., 9., 1.]], [[2., 3., 5.]]], ragged_rank=1)
    cce_obj = losses.SparseCategoricalCrossentropy(from_logits=True)
    # batch losses [[0.0018, 0.0004], [0.1698]]
    loss = cce_obj(y_true, logits, sample_weight=sample_weight)
    self.assertAlmostEqual(self.evaluate(loss), 0.1934, 3)

</source>
<source file="systems/keras-2.7.0/keras/losses_test.py" startline="1149" endline="1170" pcid="1851">
  def test_ragged_tensors_ragged_sample_weights(self):
    cce_obj = losses.CategoricalCrossentropy()
    y_true = tf.ragged.constant([[[1, 0, 0], [0, 1, 0]], [[0, 0, 1]]])
    y_pred = tf.ragged.constant(
        [[[.9, .05, .05], [.05, .89, .06]], [[.05, .01, .94]]],
        dtype=tf.float32)
    # batch losses [[0.1054, 0.1165], [0.0619]]
    # Use independent weights for each batch element
    sample_weight = tf.ragged.constant([[1.2, 3.4], [5.6]], dtype=tf.float32)
    loss = cce_obj(y_true, y_pred, sample_weight=sample_weight)
    # sum([0.1054*1.2, 0.1165*3.4, 0.0619*5.6])/3
    self.assertAlmostEqual(self.evaluate(loss), 0.2897, 3)

    # Test with logits.
    logits = tf.ragged.constant([[[8., 1., 1.], [0., 9., 1.]], [[2., 3., 5.]]])
    cce_obj = losses.CategoricalCrossentropy(from_logits=True)
    # batch losses [[0.0018, 0.0004], [0.1698]]
    # sum([0.0018*1.2, 0.0004*3.4, 0.1698*5.6]) / 3
    loss = cce_obj(y_true, logits, sample_weight=sample_weight)
    self.assertAlmostEqual(self.evaluate(loss), 0.3181, 3)


</source>
<source file="systems/keras-2.7.0/keras/losses_test.py" startline="1253" endline="1272" pcid="1859">
  def test_ragged_tensors(self):
    cce_obj = losses.SparseCategoricalCrossentropy()
    y_true = tf.ragged.constant([[0, 1], [2]])
    y_pred = tf.ragged.constant(
        [[[.9, .05, .05], [.5, .89, .6]], [[.05, .01, .94]]],
        dtype=tf.float32)
    # batch losses [[0.1054, 0.8047], [0.0619]]
    sample_weight = tf.constant([[1.2], [3.4]], shape=(2, 1))
    loss = cce_obj(y_true, y_pred, sample_weight=sample_weight)
    # sum([0.1054, 0.8047, 0.0619]) / 3
    self.assertAlmostEqual(self.evaluate(loss), 0.4341, 3)

    # Test with logits.
    logits = tf.ragged.constant([[[8., 1., 1.], [0., 9., 1.]],
                                          [[2., 3., 5.]]])
    cce_obj = losses.SparseCategoricalCrossentropy(from_logits=True)
    # batch losses [[0.0018, 0.0004], [0.1698]]
    loss = cce_obj(y_true, logits, sample_weight=sample_weight)
    self.assertAlmostEqual(self.evaluate(loss), 0.1934, 3)

</source>
</class>

<class classid="71" nclones="3" nlines="10" similarity="90">
<source file="systems/keras-2.7.0/keras/losses_test.py" startline="1608" endline="1621" pcid="1883">
  def test_scalar_weighted(self):
    self.setup()
    logcosh_obj = losses.LogCosh()
    sample_weight = 2.3

    loss = logcosh_obj(self.y_true, self.y_pred, sample_weight=sample_weight)
    expected_loss = sample_weight * np.sum(
        self.expected_losses) / self.batch_size
    self.assertAlmostEqual(self.evaluate(loss), expected_loss, 3)

    # Verify we get the same output when the same input is given
    loss_2 = logcosh_obj(self.y_true, self.y_pred, sample_weight=sample_weight)
    self.assertAlmostEqual(self.evaluate(loss), self.evaluate(loss_2), 3)

</source>
<source file="systems/keras-2.7.0/keras/losses_test.py" startline="1770" endline="1783" pcid="1897">
  def test_scalar_weighted(self):
    self.setup()
    k_obj = losses.KLDivergence()
    sample_weight = 2.3

    loss = k_obj(self.y_true, self.y_pred, sample_weight=sample_weight)
    expected_loss = sample_weight * np.sum(
        self.expected_losses) / self.batch_size
    self.assertAlmostEqual(self.evaluate(loss), expected_loss, 3)

    # Verify we get the same output when the same input is given
    loss_2 = k_obj(self.y_true, self.y_pred, sample_weight=sample_weight)
    self.assertAlmostEqual(self.evaluate(loss), self.evaluate(loss_2), 3)

</source>
<source file="systems/keras-2.7.0/keras/losses_test.py" startline="1689" endline="1703" pcid="1890">
  def test_scalar_weighted(self):
    self.setup()
    poisson_obj = losses.Poisson()
    sample_weight = 2.3
    loss = poisson_obj(self.y_true, self.y_pred, sample_weight=sample_weight)

    expected_loss = sample_weight * np.sum(
        self.expected_losses) / self.batch_size
    self.assertAlmostEqual(self.evaluate(loss), expected_loss, 3)
    self.assertAlmostEqual(self.evaluate(loss), expected_loss, 3)

    # Verify we get the same output when the same input is given
    loss_2 = poisson_obj(self.y_true, self.y_pred, sample_weight=sample_weight)
    self.assertAlmostEqual(self.evaluate(loss), self.evaluate(loss_2), 3)

</source>
</class>

<class classid="72" nclones="4" nlines="10" similarity="90">
<source file="systems/keras-2.7.0/keras/losses_test.py" startline="1622" endline="1634" pcid="1884">
  def test_sample_weighted(self):
    self.setup()
    logcosh_obj = losses.LogCosh()

    sample_weight = tf.constant([1.2, 3.4], shape=(2, 1))
    loss = logcosh_obj(self.y_true, self.y_pred, sample_weight=sample_weight)

    expected_loss = np.multiply(
        self.expected_losses,
        np.asarray([1.2, 1.2, 1.2, 3.4, 3.4, 3.4]).reshape((2, 3)))
    expected_loss = np.sum(expected_loss) / self.batch_size
    self.assertAlmostEqual(self.evaluate(loss), expected_loss, 3)

</source>
<source file="systems/keras-2.7.0/keras/losses_test.py" startline="1784" endline="1795" pcid="1898">
  def test_sample_weighted(self):
    self.setup()
    k_obj = losses.KLDivergence()
    sample_weight = tf.constant([1.2, 3.4], shape=(2, 1))
    loss = k_obj(self.y_true, self.y_pred, sample_weight=sample_weight)

    expected_loss = np.multiply(
        self.expected_losses,
        np.asarray([1.2, 1.2, 1.2, 3.4, 3.4, 3.4]).reshape(2, 3))
    expected_loss = np.sum(expected_loss) / self.batch_size
    self.assertAlmostEqual(self.evaluate(loss), expected_loss, 3)

</source>
<source file="systems/keras-2.7.0/keras/losses_test.py" startline="1704" endline="1716" pcid="1891">
  def test_sample_weighted(self):
    self.setup()
    poisson_obj = losses.Poisson()

    sample_weight = tf.constant([1.2, 3.4], shape=(2, 1))
    loss = poisson_obj(self.y_true, self.y_pred, sample_weight=sample_weight)

    expected_loss = np.multiply(
        self.expected_losses,
        np.asarray([1.2, 1.2, 1.2, 3.4, 3.4, 3.4]).reshape((2, 3)))
    expected_loss = np.sum(expected_loss) / self.batch_size
    self.assertAlmostEqual(self.evaluate(loss), expected_loss, 3)

</source>
<source file="systems/keras-2.7.0/keras/losses_test.py" startline="1876" endline="1887" pcid="1907">
  def test_sample_weighted(self):
    self.setup()
    h_obj = losses.Huber()
    sample_weight = tf.constant((1.2, 3.4), shape=(2, 1))

    loss = h_obj(self.y_true, self.y_pred, sample_weight=sample_weight)
    actual_loss = np.multiply(
        self.expected_losses,
        np.asarray([1.2, 1.2, 1.2, 3.4, 3.4, 3.4]).reshape((2, 3)))
    actual_loss = np.sum(actual_loss) / self.batch_size
    self.assertAlmostEqual(self.evaluate(loss), actual_loss, 3)

</source>
</class>

<class classid="73" nclones="2" nlines="16" similarity="75">
<source file="systems/keras-2.7.0/keras/losses_test.py" startline="1635" endline="1652" pcid="1885">
  def test_timestep_weighted(self):
    self.setup()
    logcosh_obj = losses.LogCosh()
    y_true = np.asarray([1, 9, 2, -5, -2, 6]).reshape(2, 3, 1)
    y_pred = np.asarray([4, 8, 12, 8, 1, 3]).reshape(2, 3, 1)
    error = y_pred - y_true
    expected_losses = np.log((np.exp(error) + np.exp(-error)) / 2)
    sample_weight = np.array([3, 6, 5, 0, 4, 2]).reshape((2, 3, 1))

    y_pred = tf.constant(y_pred, dtype=tf.float32)
    y_true = tf.constant(y_true)
    loss = logcosh_obj(
        y_true,
        y_pred,
        sample_weight=tf.constant(sample_weight, shape=(2, 3)))
    expected_loss = np.sum(expected_losses * sample_weight) / self.batch_size
    self.assertAlmostEqual(self.evaluate(loss), expected_loss, 3)

</source>
<source file="systems/keras-2.7.0/keras/losses_test.py" startline="1717" endline="1734" pcid="1892">
  def test_timestep_weighted(self):
    self.setup()
    poisson_obj = losses.Poisson()
    y_true = self.np_y_true.reshape(2, 3, 1)
    y_pred = self.np_y_pred.reshape(2, 3, 1)
    sample_weight = np.asarray([3, 6, 5, 0, 4, 2]).reshape(2, 3, 1)
    expected_losses = y_pred - np.multiply(y_true, np.log(y_pred))

    y_pred = tf.constant(y_pred, dtype=tf.float32)
    y_true = tf.constant(y_true)

    loss = poisson_obj(
        y_true,
        y_pred,
        sample_weight=tf.constant(sample_weight, shape=(2, 3)))
    expected_loss = np.sum(expected_losses * sample_weight) / self.batch_size
    self.assertAlmostEqual(self.evaluate(loss), expected_loss, 3)

</source>
</class>

<class classid="74" nclones="4" nlines="27" similarity="70">
<source file="systems/keras-2.7.0/keras/tests/integration_test.py" startline="54" endline="81" pcid="1917">
  def test_vector_classification(self):
    np.random.seed(1337)
    (x_train, y_train), _ = testing_utils.get_test_data(
        train_samples=100,
        test_samples=0,
        input_shape=(10,),
        num_classes=2)
    y_train = np_utils.to_categorical(y_train)

    model = testing_utils.get_model_from_layers(
        [keras.layers.Dense(16, activation='relu'),
         keras.layers.Dropout(0.1),
         keras.layers.Dense(y_train.shape[-1], activation='softmax')],
        input_shape=x_train.shape[1:])
    model.compile(
        loss='categorical_crossentropy',
        optimizer=keras.optimizer_v2.adam.Adam(0.005),
        metrics=['acc'],
        run_eagerly=testing_utils.should_run_eagerly())
    history = model.fit(x_train, y_train, epochs=10, batch_size=10,
                        validation_data=(x_train, y_train),
                        verbose=2)
    self.assertGreater(history.history['val_acc'][-1], 0.7)
    _, val_acc = model.evaluate(x_train, y_train)
    self.assertAlmostEqual(history.history['val_acc'][-1], val_acc)
    predictions = model.predict(x_train)
    self.assertEqual(predictions.shape, (x_train.shape[0], 2))

</source>
<source file="systems/keras-2.7.0/keras/tests/integration_test.py" startline="243" endline="276" pcid="1922">
  def test_image_classification(self):
    np.random.seed(1337)
    (x_train, y_train), _ = testing_utils.get_test_data(
        train_samples=100,
        test_samples=0,
        input_shape=(10, 10, 3),
        num_classes=2)
    y_train = np_utils.to_categorical(y_train)

    layers = [
        keras.layers.Conv2D(4, 3, padding='same', activation='relu'),
        keras.layers.Conv2D(8, 3, padding='same'),
        keras.layers.BatchNormalization(),
        keras.layers.Conv2D(8, 3, padding='same'),
        keras.layers.Flatten(),
        keras.layers.Dense(y_train.shape[-1], activation='softmax')
    ]
    model = testing_utils.get_model_from_layers(
        layers, input_shape=x_train.shape[1:])
    model.compile(
        loss='categorical_crossentropy',
        optimizer=keras.optimizer_v2.adam.Adam(0.005),
        metrics=['acc'],
        run_eagerly=testing_utils.should_run_eagerly())
    history = model.fit(x_train, y_train, epochs=10, batch_size=10,
                        validation_data=(x_train, y_train),
                        verbose=2)
    self.assertGreater(history.history['val_acc'][-1], 0.7)
    _, val_acc = model.evaluate(x_train, y_train)
    self.assertAlmostEqual(history.history['val_acc'][-1], val_acc)
    predictions = model.predict(x_train)
    self.assertEqual(predictions.shape, (x_train.shape[0], 2))


</source>
<source file="systems/keras-2.7.0/keras/tests/integration_test.py" startline="207" endline="238" pcid="1921">
  def test_timeseries_classification_sequential_tf_rnn(self):
    np.random.seed(1337)
    (x_train, y_train), _ = testing_utils.get_test_data(
        train_samples=100,
        test_samples=0,
        input_shape=(4, 10),
        num_classes=2)
    y_train = np_utils.to_categorical(y_train)

    with base_layer.keras_style_scope():
      model = keras.models.Sequential()
      model.add(keras.layers.RNN(rnn_cell.LSTMCell(5), return_sequences=True,
                                 input_shape=x_train.shape[1:]))
      model.add(keras.layers.RNN(rnn_cell.GRUCell(y_train.shape[-1],
                                                  activation='softmax',
                                                  dtype=tf.float32)))
      model.compile(
          loss='categorical_crossentropy',
          optimizer=keras.optimizer_v2.adam.Adam(0.005),
          metrics=['acc'],
          run_eagerly=testing_utils.should_run_eagerly())

    history = model.fit(x_train, y_train, epochs=15, batch_size=10,
                        validation_data=(x_train, y_train),
                        verbose=2)
    self.assertGreater(history.history['val_acc'][-1], 0.7)
    _, val_acc = model.evaluate(x_train, y_train)
    self.assertAlmostEqual(history.history['val_acc'][-1], val_acc)
    predictions = model.predict(x_train)
    self.assertEqual(predictions.shape, (x_train.shape[0], 2))


</source>
<source file="systems/keras-2.7.0/keras/tests/integration_test.py" startline="178" endline="206" pcid="1920">
  def test_timeseries_classification(self):
    np.random.seed(1337)
    (x_train, y_train), _ = testing_utils.get_test_data(
        train_samples=100,
        test_samples=0,
        input_shape=(4, 10),
        num_classes=2)
    y_train = np_utils.to_categorical(y_train)

    layers = [
        keras.layers.LSTM(5, return_sequences=True),
        keras.layers.GRU(y_train.shape[-1], activation='softmax')
    ]
    model = testing_utils.get_model_from_layers(
        layers, input_shape=x_train.shape[1:])
    model.compile(
        loss='categorical_crossentropy',
        optimizer=keras.optimizer_v2.adam.Adam(0.005),
        metrics=['acc'],
        run_eagerly=testing_utils.should_run_eagerly())
    history = model.fit(x_train, y_train, epochs=15, batch_size=10,
                        validation_data=(x_train, y_train),
                        verbose=2)
    self.assertGreater(history.history['val_acc'][-1], 0.7)
    _, val_acc = model.evaluate(x_train, y_train)
    self.assertAlmostEqual(history.history['val_acc'][-1], val_acc)
    predictions = model.predict(x_train)
    self.assertEqual(predictions.shape, (x_train.shape[0], 2))

</source>
</class>

<class classid="75" nclones="2" nlines="15" similarity="73">
<source file="systems/keras-2.7.0/keras/tests/model_architectures.py" startline="178" endline="199" pcid="1937">
def nested_subclassed_model():
  """A subclass model nested in another subclass model."""

  class NestedSubclassModel(keras.Model):
    """A nested subclass model."""

    def __init__(self):
      super(NestedSubclassModel, self).__init__()
      self.dense1 = keras.layers.Dense(4, activation='relu')
      self.dense2 = keras.layers.Dense(2, activation='relu')
      self.bn = keras.layers.BatchNormalization()
      self.inner_subclass_model = MySubclassModel()

    def call(self, inputs):
      x = self.dense1(inputs)
      x = self.bn(x)
      x = self.inner_subclass_model(x)
      return self.dense2(x)

  return ModelFn(NestedSubclassModel(), (None, 3), (None, 2))


</source>
<source file="systems/keras-2.7.0/keras/tests/model_architectures.py" startline="237" endline="257" pcid="1945">
def shared_layer_subclassed_model():
  """Shared layer in a subclass model."""

  class SharedLayerSubclassModel(keras.Model):
    """A subclass model with shared layers."""

    def __init__(self):
      super(SharedLayerSubclassModel, self).__init__(
          name='shared_layer_subclass_model')
      self.dense = keras.layers.Dense(3, activation='relu')
      self.dp = keras.layers.Dropout(0.5)
      self.bn = keras.layers.BatchNormalization()

    def call(self, inputs):
      x = self.dense(inputs)
      x = self.dp(x)
      x = self.bn(x)
      return self.dense(x)
  return ModelFn(SharedLayerSubclassModel(), (None, 3), (None, 3))


</source>
</class>

<class classid="76" nclones="2" nlines="13" similarity="78">
<source file="systems/keras-2.7.0/keras/tests/temporal_sample_weights_correctness_test.py" startline="252" endline="268" pcid="1997">
  def test_fit_with_sample_weight(self):

    def _train_and_assert(model):
      history = model.fit([self.x, self.x], [self.y1, self.y2],
                          sample_weight={
                              'output_1': self.sample_weight_1,
                              'output_2': self.sample_weight_2,
                          },
                          batch_size=3,
                          epochs=2,
                          shuffle=False)
      for key, value in self.expected_fit_result_with_weights.items():
        self.assertAllClose(history.history[key], value, 1e-3)

    run_with_different_sample_weight_mode_inputs(
        _train_and_assert, partial_sw=False)

</source>
<source file="systems/keras-2.7.0/keras/tests/temporal_sample_weights_correctness_test.py" startline="269" endline="283" pcid="1999">
  def test_fit_with_partial_sample_weight(self):

    def _train_and_assert(model):
      history = model.fit([self.x, self.x], [self.y1, self.y2],
                          sample_weight={
                              'output_2': self.sample_weight_2,
                          },
                          batch_size=3,
                          epochs=2,
                          shuffle=False)
      for key, value in self.expected_fit_result_with_weights_output_2.items():
        self.assertAllClose(history.history[key], value, 1e-3)

    run_with_different_sample_weight_mode_inputs(_train_and_assert)

</source>
</class>

<class classid="77" nclones="7" nlines="14" similarity="70">
<source file="systems/keras-2.7.0/keras/tests/temporal_sample_weights_correctness_test.py" startline="294" endline="313" pcid="2003">
  def test_eval_with_sample_weight(self):

    def _eval_and_assert(model):
      model.train_on_batch([self.x, self.x], [self.y1, self.y2],
                           sample_weight={
                               'output_1': self.sample_weight_1,
                               'output_2': self.sample_weight_2,
                           })
      eval_result = model.evaluate([self.x, self.x], [self.y1, self.y2],
                                   batch_size=3,
                                   sample_weight={
                                       'output_1': self.sample_weight_1,
                                       'output_2': self.sample_weight_2,
                                   })
      self.assertAllClose(eval_result, self.expected_batch_result_with_weights,
                          1e-3)

    run_with_different_sample_weight_mode_inputs(
        _eval_and_assert, partial_sw=False)

</source>
<source file="systems/keras-2.7.0/keras/tests/temporal_sample_weights_correctness_test.py" startline="314" endline="331" pcid="2005">
  def test_eval_with_partial_sample_weight(self):

    def _eval_and_assert(model):
      model.train_on_batch([self.x, self.x], [self.y1, self.y2],
                           sample_weight={
                               'output_2': self.sample_weight_2,
                           })
      eval_result = model.evaluate([self.x, self.x], [self.y1, self.y2],
                                   batch_size=3,
                                   sample_weight={
                                       'output_2': self.sample_weight_2,
                                   })
      self.assertAllClose(eval_result,
                          self.expected_batch_result_with_weights_output_2,
                          1e-3)

    run_with_different_sample_weight_mode_inputs(_eval_and_assert)

</source>
<source file="systems/keras-2.7.0/keras/tests/temporal_sample_weights_correctness_test.py" startline="378" endline="395" pcid="2015">
  def test_test_on_batch_with_sample_weight(self):

    def _test_and_assert(model):
      model.train_on_batch([self.x, self.x], [self.y1, self.y2],
                           sample_weight={
                               'output_1': self.sample_weight_1,
                               'output_2': self.sample_weight_2,
                           })
      result = model.test_on_batch([self.x, self.x], [self.y1, self.y2],
                                   sample_weight={
                                       'output_1': self.sample_weight_1,
                                       'output_2': self.sample_weight_2,
                                   })
      self.assertAllClose(result, self.expected_batch_result_with_weights, 1e-3)

    run_with_different_sample_weight_mode_inputs(
        _test_and_assert, partial_sw=False)

</source>
<source file="systems/keras-2.7.0/keras/tests/temporal_sample_weights_correctness_test.py" startline="396" endline="412" pcid="2017">
  def test_test_on_batch_with_partial_sample_weight(self):

    def _test_and_assert(model):
      model.train_on_batch([self.x, self.x], [self.y1, self.y2],
                           sample_weight={
                               'output_2': self.sample_weight_2,
                           })
      result = model.test_on_batch([self.x, self.x], [self.y1, self.y2],
                                   sample_weight={
                                       'output_2': self.sample_weight_2,
                                   })
      self.assertAllClose(result,
                          self.expected_batch_result_with_weights_output_2,
                          1e-3)

    run_with_different_sample_weight_mode_inputs(_test_and_assert)

</source>
<source file="systems/keras-2.7.0/keras/tests/temporal_sample_weights_correctness_test.py" startline="355" endline="368" pcid="2011">
  def test_train_on_batch_with_partial_sample_weight(self):

    def _train_and_assert(model):
      for _ in range(2):
        result = model.train_on_batch([self.x, self.x], [self.y1, self.y2],
                                      sample_weight={
                                          'output_2': self.sample_weight_2,
                                      })
      self.assertAllClose(result,
                          self.expected_batch_result_with_weights_output_2,
                          1e-3)

    run_with_different_sample_weight_mode_inputs(_train_and_assert)

</source>
<source file="systems/keras-2.7.0/keras/tests/temporal_sample_weights_correctness_test.py" startline="296" endline="310" pcid="2004">
    def _eval_and_assert(model):
      model.train_on_batch([self.x, self.x], [self.y1, self.y2],
                           sample_weight={
                               'output_1': self.sample_weight_1,
                               'output_2': self.sample_weight_2,
                           })
      eval_result = model.evaluate([self.x, self.x], [self.y1, self.y2],
                                   batch_size=3,
                                   sample_weight={
                                       'output_1': self.sample_weight_1,
                                       'output_2': self.sample_weight_2,
                                   })
      self.assertAllClose(eval_result, self.expected_batch_result_with_weights,
                          1e-3)

</source>
<source file="systems/keras-2.7.0/keras/tests/temporal_sample_weights_correctness_test.py" startline="480" endline="496" pcid="2029">
  def test_eval_generator_with_partial_sample_weight(self):

    def _test_and_assert(model):
      model.train_on_batch([self.x, self.x], [self.y1, self.y2],
                           sample_weight={
                               'output_2': self.sample_weight_2,
                           })
      eval_result = model.evaluate_generator(
          self.custom_generator_multi_io_temporal(
              sample_weights={'output_2': self.sample_weight_2}),
          steps=2)
      self.assertAllClose(eval_result,
                          self.expected_batch_result_with_weights_output_2,
                          1e-3)

    run_with_different_sample_weight_mode_inputs(_test_and_assert)

</source>
</class>

<class classid="78" nclones="2" nlines="12" similarity="75">
<source file="systems/keras-2.7.0/keras/tests/temporal_sample_weights_correctness_test.py" startline="380" endline="392" pcid="2016">
    def _test_and_assert(model):
      model.train_on_batch([self.x, self.x], [self.y1, self.y2],
                           sample_weight={
                               'output_1': self.sample_weight_1,
                               'output_2': self.sample_weight_2,
                           })
      result = model.test_on_batch([self.x, self.x], [self.y1, self.y2],
                                   sample_weight={
                                       'output_1': self.sample_weight_1,
                                       'output_2': self.sample_weight_2,
                                   })
      self.assertAllClose(result, self.expected_batch_result_with_weights, 1e-3)

</source>
<source file="systems/keras-2.7.0/keras/tests/temporal_sample_weights_correctness_test.py" startline="398" endline="410" pcid="2018">
    def _test_and_assert(model):
      model.train_on_batch([self.x, self.x], [self.y1, self.y2],
                           sample_weight={
                               'output_2': self.sample_weight_2,
                           })
      result = model.test_on_batch([self.x, self.x], [self.y1, self.y2],
                                   sample_weight={
                                       'output_2': self.sample_weight_2,
                                   })
      self.assertAllClose(result,
                          self.expected_batch_result_with_weights_output_2,
                          1e-3)

</source>
</class>

<class classid="79" nclones="2" nlines="11" similarity="72">
<source file="systems/keras-2.7.0/keras/tests/temporal_sample_weights_correctness_test.py" startline="425" endline="438" pcid="2021">
  def test_fit_generator_with_sample_weight(self):

    def _train_and_assert(model):
      history = model.fit_generator(
          self.custom_generator_multi_io_temporal(
              sample_weights=[self.sample_weight_1, self.sample_weight_2]),
          steps_per_epoch=1,
          epochs=2)
      for key, value in self.expected_fit_result_with_weights.items():
        self.assertAllClose(history.history[key], value, 1e-3)

    run_with_different_sample_weight_mode_inputs(
        _train_and_assert, partial_sw=False)

</source>
<source file="systems/keras-2.7.0/keras/tests/temporal_sample_weights_correctness_test.py" startline="439" endline="451" pcid="2023">
  def test_fit_generator_with_partial_sample_weight(self):

    def _train_and_assert(model):
      history = model.fit_generator(
          self.custom_generator_multi_io_temporal(
              sample_weights={'output_2': self.sample_weight_2}),
          steps_per_epoch=1,
          epochs=2)
      for key, value in self.expected_fit_result_with_weights_output_2.items():
        self.assertAllClose(history.history[key], value, 1e-3)

    run_with_different_sample_weight_mode_inputs(_train_and_assert)

</source>
</class>

<class classid="80" nclones="2" nlines="12" similarity="75">
<source file="systems/keras-2.7.0/keras/tests/temporal_sample_weights_correctness_test.py" startline="464" endline="476" pcid="2028">
    def _test_and_assert(model):
      model.train_on_batch([self.x, self.x], [self.y1, self.y2],
                           sample_weight={
                               'output_1': self.sample_weight_1,
                               'output_2': self.sample_weight_2,
                           })
      eval_result = model.evaluate_generator(
          self.custom_generator_multi_io_temporal(
              sample_weights=[self.sample_weight_1, self.sample_weight_2]),
          steps=2)
      self.assertAllClose(eval_result, self.expected_batch_result_with_weights,
                          1e-3)

</source>
<source file="systems/keras-2.7.0/keras/tests/temporal_sample_weights_correctness_test.py" startline="482" endline="494" pcid="2030">
    def _test_and_assert(model):
      model.train_on_batch([self.x, self.x], [self.y1, self.y2],
                           sample_weight={
                               'output_2': self.sample_weight_2,
                           })
      eval_result = model.evaluate_generator(
          self.custom_generator_multi_io_temporal(
              sample_weights={'output_2': self.sample_weight_2}),
          steps=2)
      self.assertAllClose(eval_result,
                          self.expected_batch_result_with_weights_output_2,
                          1e-3)

</source>
</class>

<class classid="81" nclones="2" nlines="17" similarity="72">
<source file="systems/keras-2.7.0/keras/tests/model_subclassing_compiled_test.py" startline="37" endline="55" pcid="2042">
  def test_single_io_workflow_with_np_arrays(self):
    num_classes = 2
    num_samples = 100
    input_dim = 50

    model = testing_utils.SmallSubclassMLP(
        num_hidden=32, num_classes=num_classes, use_dp=True, use_bn=True)
    model.compile(
        loss='mse',
        optimizer='rmsprop',
        metrics=['acc', keras.metrics.CategoricalAccuracy()],
        run_eagerly=testing_utils.should_run_eagerly())

    x = np.ones((num_samples, input_dim))
    y = np.zeros((num_samples, num_classes))

    model.fit(x, y, epochs=2, batch_size=32, verbose=0)
    _ = model.evaluate(x, y, verbose=0)

</source>
<source file="systems/keras-2.7.0/keras/tests/model_subclassing_compiled_test.py" startline="347" endline="369" pcid="2057">
  def test_subclass_nested_in_graph(self):
    num_classes = 2
    num_samples = 100
    input_dim = 50

    model = model_util.get_nested_model_3(
        input_dim=input_dim, num_classes=num_classes)
    model.compile(
        loss='mse',
        optimizer='rmsprop',
        metrics=['acc'],
        run_eagerly=testing_utils.should_run_eagerly())

    x = np.ones((num_samples, input_dim))
    y = np.zeros((num_samples, num_classes))

    model.fit(x, y, epochs=2, batch_size=32, verbose=0)
    _ = model.evaluate(x, y, verbose=0)

    self.assertEqual(len(model.weights), 16)
    self.assertEqual(len(model.non_trainable_weights), 4)
    self.assertEqual(len(model.trainable_weights), 12)

</source>
</class>

<class classid="82" nclones="2" nlines="22" similarity="78">
<source file="systems/keras-2.7.0/keras/tests/model_subclassing_compiled_test.py" startline="159" endline="188" pcid="2049">
  def test_training_and_inference_behavior(self):
    # test that dropout is applied in training and not inference

    num_samples = 100
    input_dim = 50

    class DPNet(keras.Model):

      def __init__(self):
        super(DPNet, self).__init__()
        self.dp = keras.layers.Dropout(0.5)
        self.dense = keras.layers.Dense(1,
                                        use_bias=False,
                                        kernel_initializer='ones')

      def call(self, inputs):
        x = self.dp(inputs)
        return self.dense(x)

    model = DPNet()
    x = np.ones((num_samples, input_dim))
    y = model.predict(x)
    self.assertEqual(np.sum(y), np.sum(x))
    model.compile(
        loss='mse',
        optimizer='rmsprop',
        run_eagerly=testing_utils.should_run_eagerly())
    loss = model.train_on_batch(x, y)
    self.assertGreater(loss, 0.1)

</source>
<source file="systems/keras-2.7.0/keras/tests/model_subclassing_compiled_test.py" startline="404" endline="436" pcid="2061">
  def test_support_for_manual_training_arg(self):
    # In most cases, the `training` argument is left unspecified, in which
    # case it defaults to value corresponding to the Model method being used
    # (fit -> True, predict -> False, etc).
    # If the user writes their model `call` method to take
    # an explicit `training` argument, we must check that the correct value
    # is being passed to the model for each method call.

    class DPNet(keras.Model):

      def __init__(self):
        super(DPNet, self).__init__()
        self.dp = keras.layers.Dropout(0.5)
        self.dense = keras.layers.Dense(1,
                                        use_bias=False,
                                        kernel_initializer='ones')

      def call(self, inputs, training=False):
        x = self.dp(inputs, training=training)
        return self.dense(x)

    model = DPNet()
    x = np.ones((10, 10))
    y = model.predict(x)
    self.assertEqual(np.sum(y), np.sum(x))
    model.compile(
        loss='mse',
        optimizer='rmsprop',
        run_eagerly=testing_utils.should_run_eagerly())
    loss = model.train_on_batch(x, y)
    self.assertGreater(loss, 0.1)


</source>
</class>

<class classid="83" nclones="2" nlines="19" similarity="100">
<source file="systems/keras-2.7.0/keras/tests/model_subclassing_compiled_test.py" startline="299" endline="322" pcid="2055">
  def test_subclass_nested_in_subclass(self):
    num_classes = 2
    num_samples = 100
    input_dim = 50

    model = model_util.NestedTestModel1(num_classes=num_classes)
    model.compile(
        loss='mse',
        optimizer='rmsprop',
        metrics=['acc'],
        run_eagerly=testing_utils.should_run_eagerly())

    x = np.ones((num_samples, input_dim))
    y = np.zeros((num_samples, num_classes))

    model.fit(x, y, epochs=2, batch_size=32, verbose=0)
    _ = model.evaluate(x, y, verbose=0)

    self.assertEqual(len(model.weights), 8 + len(model.test_net.weights))
    self.assertEqual(len(model.non_trainable_weights),
                     2 + len(model.test_net.non_trainable_weights))
    self.assertEqual(len(model.trainable_weights),
                     6 + len(model.test_net.trainable_weights))

</source>
<source file="systems/keras-2.7.0/keras/tests/model_subclassing_compiled_test.py" startline="323" endline="346" pcid="2056">
  def test_graph_nested_in_subclass(self):
    num_classes = 2
    num_samples = 100
    input_dim = 50

    model = model_util.NestedTestModel2(num_classes=num_classes)
    model.compile(
        loss='mse',
        optimizer='rmsprop',
        metrics=['acc'],
        run_eagerly=testing_utils.should_run_eagerly())

    x = np.ones((num_samples, input_dim))
    y = np.zeros((num_samples, num_classes))

    model.fit(x, y, epochs=2, batch_size=32, verbose=0)
    _ = model.evaluate(x, y, verbose=0)

    self.assertEqual(len(model.weights), 8 + len(model.test_net.weights))
    self.assertEqual(len(model.non_trainable_weights),
                     2 + len(model.test_net.non_trainable_weights))
    self.assertEqual(len(model.trainable_weights),
                     6 + len(model.test_net.trainable_weights))

</source>
</class>

<class classid="84" nclones="2" nlines="14" similarity="92">
<source file="systems/keras-2.7.0/keras/tests/tracking_test.py" startline="109" endline="127" pcid="2103">
  def testSubSequentialTracking(self):

    class _Subclassed(training.Model):

      def __init__(self, wrapped):
        super(_Subclassed, self).__init__()
        self._wrapped = wrapped

      def call(self, x):
        return self._wrapped(x)

    model = sequential.Sequential()
    layer = core.Dense(1)
    model.add(layer)
    model2 = _Subclassed(model)
    model2(tf.ones([1, 2]))
    model2.m = [model]
    self.assertIn(layer.kernel, model2.trainable_weights)

</source>
<source file="systems/keras-2.7.0/keras/tests/tracking_test.py" startline="452" endline="470" pcid="2135">
  def testSubSequentialTracking(self):

    class _Subclassed(training.Model):

      def __init__(self, wrapped):
        super(_Subclassed, self).__init__()
        self._wrapped = wrapped

      def call(self, x):
        return self._wrapped(x)

    model = sequential.Sequential()
    layer = core.Dense(1)
    model.add(layer)
    model2 = _Subclassed(model)
    model2(tf.ones([1, 2]))
    model2.m = (model,)
    self.assertIn(layer.kernel, model2.trainable_weights)

</source>
</class>

<class classid="85" nclones="2" nlines="11" similarity="72">
<source file="systems/keras-2.7.0/keras/tests/tracking_test.py" startline="201" endline="217" pcid="2116">
  def testTensorConversion(self):

    class ListToTensor(training.Model):

      def __init__(self):
        super(ListToTensor, self).__init__()
        self.l = [1., 2., 3.]

    self.assertAllEqual(
        [1., 2., 3.],
        self.evaluate(tf.constant(ListToTensor().l)))

    self.assertAllEqual(
        [1., 2., 3.],
        self.evaluate(tf.raw_ops.Pack(values=ListToTensor().l)))


</source>
<source file="systems/keras-2.7.0/keras/tests/tracking_test.py" startline="514" endline="530" pcid="2142">
  def testTensorConversion(self):

    class TupleToTensor(training.Model):

      def __init__(self):
        super(TupleToTensor, self).__init__()
        self.l = (1., 2., 3.)

    self.assertAllEqual(
        (1., 2., 3.),
        self.evaluate(tf.constant(TupleToTensor().l)))

    self.assertAllEqual(
        (1., 2., 3.),
        self.evaluate(tf.raw_ops.Pack(values=TupleToTensor().l)))


</source>
</class>

<class classid="86" nclones="6" nlines="10" similarity="90">
<source file="systems/keras-2.7.0/keras/initializers/initializers_test.py" startline="115" endline="125" pcid="2208">
  def test_lecun_uniform(self):
    tensor_shape = (5, 6, 4, 2)
    with self.cached_session():
      fan_in, _ = _compute_fans(tensor_shape)
      std = np.sqrt(1. / fan_in)
      self._runner(
          initializers.LecunUniformV2(seed=123),
          tensor_shape,
          target_mean=0.,
          target_std=std)

</source>
<source file="systems/keras-2.7.0/keras/initializers/initializers_test.py" startline="159" endline="169" pcid="2212">
  def test_glorot_normal(self):
    tensor_shape = (5, 6, 4, 2)
    with self.cached_session():
      fan_in, fan_out = _compute_fans(tensor_shape)
      std = np.sqrt(2. / (fan_in + fan_out))
      self._runner(
          initializers.GlorotNormalV2(seed=123),
          tensor_shape,
          target_mean=0.,
          target_std=std)

</source>
<source file="systems/keras-2.7.0/keras/initializers/initializers_test.py" startline="126" endline="136" pcid="2209">
  def test_glorot_uniform(self):
    tensor_shape = (5, 6, 4, 2)
    with self.cached_session():
      fan_in, fan_out = _compute_fans(tensor_shape)
      std = np.sqrt(2. / (fan_in + fan_out))
      self._runner(
          initializers.GlorotUniformV2(seed=123),
          tensor_shape,
          target_mean=0.,
          target_std=std)

</source>
<source file="systems/keras-2.7.0/keras/initializers/initializers_test.py" startline="137" endline="147" pcid="2210">
  def test_he_uniform(self):
    tensor_shape = (5, 6, 4, 2)
    with self.cached_session():
      fan_in, _ = _compute_fans(tensor_shape)
      std = np.sqrt(2. / fan_in)
      self._runner(
          initializers.HeUniformV2(seed=123),
          tensor_shape,
          target_mean=0.,
          target_std=std)

</source>
<source file="systems/keras-2.7.0/keras/initializers/initializers_test.py" startline="148" endline="158" pcid="2211">
  def test_lecun_normal(self):
    tensor_shape = (5, 6, 4, 2)
    with self.cached_session():
      fan_in, _ = _compute_fans(tensor_shape)
      std = np.sqrt(1. / fan_in)
      self._runner(
          initializers.LecunNormalV2(seed=123),
          tensor_shape,
          target_mean=0.,
          target_std=std)

</source>
<source file="systems/keras-2.7.0/keras/initializers/initializers_test.py" startline="170" endline="180" pcid="2213">
  def test_he_normal(self):
    tensor_shape = (5, 6, 4, 2)
    with self.cached_session():
      fan_in, _ = _compute_fans(tensor_shape)
      std = np.sqrt(2. / fan_in)
      self._runner(
          initializers.HeNormalV2(seed=123),
          tensor_shape,
          target_mean=0.,
          target_std=std)

</source>
</class>

<class classid="87" nclones="4" nlines="33" similarity="72">
<source file="systems/keras-2.7.0/keras/saving/metrics_serialization_test.py" startline="156" endline="200" pcid="2244">
  def test_serializing_model_with_metric_with_custom_object_scope(self, value):

    def get_instance(x):
      if isinstance(x, str):
        return x
      if isinstance(x, type) and issubclass(x, metrics.Metric):
        return x()
      return x

    metric_input = tf.nest.map_structure(get_instance, value)
    weighted_metric_input = tf.nest.map_structure(get_instance, value)

    with generic_utils.custom_object_scope({
        'MyMeanAbsoluteError': MyMeanAbsoluteError,
        '_my_mae': _my_mae,
        'Bias': testing_utils.Bias,
    }):
      model = _get_multi_io_model()
      model.compile(
          optimizer_v2.gradient_descent.SGD(0.1),
          'mae',
          metrics=metric_input,
          weighted_metrics=weighted_metric_input,
          run_eagerly=testing_utils.should_run_eagerly())
      history = model.fit([self.x, self.x], [self.y, self.y],
                          batch_size=3,
                          epochs=3,
                          sample_weight=[self.w, self.w])

      # Assert training.
      self.assertAllClose(history.history['loss'], [2., 1.6, 1.2], 1e-3)
      eval_results = model.evaluate([self.x, self.x], [self.y, self.y],
                                    sample_weight=[self.w, self.w])

      if h5py is None:
        return
      model.save(self.model_filename)
      loaded_model = keras.models.load_model(self.model_filename)
      loaded_model.predict([self.x, self.x])
      loaded_eval_results = loaded_model.evaluate(
          [self.x, self.x], [self.y, self.y], sample_weight=[self.w, self.w])

      # Assert all evaluation results are the same.
      self.assertAllClose(eval_results, loaded_eval_results, 1e-9)

</source>
<source file="systems/keras-2.7.0/keras/saving/losses_serialization_test.py" startline="124" endline="155" pcid="2274">
  def test_serializing_model_with_loss_with_custom_object_scope(self, value):
    with generic_utils.custom_object_scope({
        'MyMeanAbsoluteError': MyMeanAbsoluteError,
        'my_mae': my_mae,
        'Bias': testing_utils.Bias,
    }):
      model = _get_multi_io_model()
      model.compile(
          optimizer_v2.gradient_descent.SGD(0.1),
          loss=value,
          run_eagerly=testing_utils.should_run_eagerly())
      history = model.fit([self.x, self.x], [self.y, self.y],
                          batch_size=3,
                          epochs=3,
                          sample_weight=[self.w, self.w])

      # Assert training.
      self.assertAllClose(history.history['loss'], [2., 1.6, 1.2], 1e-3)
      eval_results = model.evaluate([self.x, self.x], [self.y, self.y],
                                    sample_weight=[self.w, self.w])

      if h5py is None:
        return
      model.save(self.model_filename)
      loaded_model = keras.models.load_model(self.model_filename)
      loaded_model.predict([self.x, self.x])
      loaded_eval_results = loaded_model.evaluate(
          [self.x, self.x], [self.y, self.y], sample_weight=[self.w, self.w])

      # Assert all evaluation results are the same.
      self.assertAllClose(eval_results, loaded_eval_results, 1e-9)

</source>
<source file="systems/keras-2.7.0/keras/saving/metrics_serialization_test.py" startline="201" endline="248" pcid="2246">
  def test_serializing_model_with_metric_with_custom_objects(self, value):

    def get_instance(x):
      if isinstance(x, str):
        return x
      if isinstance(x, type) and issubclass(x, metrics.Metric):
        return x()
      return x

    metric_input = tf.nest.map_structure(get_instance, value)
    weighted_metric_input = tf.nest.map_structure(get_instance, value)

    model = _get_multi_io_model()
    model.compile(
        optimizer_v2.gradient_descent.SGD(0.1),
        'mae',
        metrics=metric_input,
        weighted_metrics=weighted_metric_input,
        run_eagerly=testing_utils.should_run_eagerly())
    history = model.fit([self.x, self.x], [self.y, self.y],
                        batch_size=3,
                        epochs=3,
                        sample_weight=[self.w, self.w])

    # Assert training.
    self.assertAllClose(history.history['loss'], [2., 1.6, 1.2], 1e-3)
    eval_results = model.evaluate([self.x, self.x], [self.y, self.y],
                                  sample_weight=[self.w, self.w])

    if h5py is None:
      return
    model.save(self.model_filename)
    loaded_model = keras.models.load_model(
        self.model_filename,
        custom_objects={
            'MyMeanAbsoluteError': MyMeanAbsoluteError,
            '_my_mae': _my_mae,
            'Bias': testing_utils.Bias,
        })
    loaded_model.predict([self.x, self.x])
    loaded_eval_results = loaded_model.evaluate([self.x, self.x],
                                                [self.y, self.y],
                                                sample_weight=[self.w, self.w])

    # Assert all evaluation results are the same.
    self.assertAllClose(eval_results, loaded_eval_results, 1e-9)


</source>
<source file="systems/keras-2.7.0/keras/saving/losses_serialization_test.py" startline="156" endline="190" pcid="2275">
  def test_serializing_model_with_loss_with_custom_objects(self, value):
    model = _get_multi_io_model()
    model.compile(
        optimizer_v2.gradient_descent.SGD(0.1),
        loss=value,
        run_eagerly=testing_utils.should_run_eagerly())
    history = model.fit([self.x, self.x], [self.y, self.y],
                        batch_size=3,
                        epochs=3,
                        sample_weight=[self.w, self.w])

    # Assert training.
    self.assertAllClose(history.history['loss'], [2., 1.6, 1.2], 1e-3)
    eval_results = model.evaluate([self.x, self.x], [self.y, self.y],
                                  sample_weight=[self.w, self.w])

    if h5py is None:
      return
    model.save(self.model_filename)
    loaded_model = keras.models.load_model(
        self.model_filename,
        custom_objects={
            'MyMeanAbsoluteError': MyMeanAbsoluteError,
            'my_mae': my_mae,
            'Bias': testing_utils.Bias,
        })
    loaded_model.predict([self.x, self.x])
    loaded_eval_results = loaded_model.evaluate([self.x, self.x],
                                                [self.y, self.y],
                                                sample_weight=[self.w, self.w])

    # Assert all evaluation results are the same.
    self.assertAllClose(eval_results, loaded_eval_results, 1e-9)


</source>
</class>

<class classid="88" nclones="2" nlines="10" similarity="80">
<source file="systems/keras-2.7.0/keras/engine/base_preprocessing_layer_test.py" startline="122" endline="136" pcid="2411">
  def test_pre_build_adapt_update_dataset(self):
    """Test that preproc layers can adapt() before build() is called."""
    input_dataset = tf.data.Dataset.from_tensor_slices(
        np.array([[1], [2], [3], [4], [5], [0]]))

    layer = AddingPreprocessingLayer()
    layer.adapt(input_dataset)

    input_data = keras.Input(shape=(1,))
    output = layer(input_data)
    model = keras.Model(input_data, output)
    model._run_eagerly = testing_utils.should_run_eagerly()

    self.assertAllEqual([[16], [17], [18]], model.predict([1., 2., 3.]))

</source>
<source file="systems/keras-2.7.0/keras/engine/base_preprocessing_layer_test.py" startline="137" endline="151" pcid="2412">
  def test_post_build_adapt_update_dataset(self):
    """Test that preproc layers can adapt() after build() is called."""
    input_dataset = tf.data.Dataset.from_tensor_slices(
        np.array([[1], [2], [3], [4], [5], [0]]))

    input_data = keras.Input(shape=(1,))
    layer = AddingPreprocessingLayer()
    output = layer(input_data)
    model = keras.Model(input_data, output)
    model._run_eagerly = testing_utils.should_run_eagerly()

    layer.adapt(input_dataset)

    self.assertAllEqual([[16], [17], [18]], model.predict([1., 2., 3.]))

</source>
</class>

<class classid="89" nclones="2" nlines="14" similarity="71">
<source file="systems/keras-2.7.0/keras/engine/training_utils_v1_test.py" startline="269" endline="287" pcid="2454">
  def _run_without_steps(self):
    aggregator = training_utils_v1.OutputsAggregator(
        use_steps=False, num_samples=6)

    batch_start = 0
    for i, batch in enumerate(np.array_split(_TEST_DATA, 4)):
      if i == 0:
        aggregator.create(batch)

      batch_end = batch_start + batch.shape[0]
      aggregator.aggregate(batch, batch_start, batch_end)
      batch_start = batch_end

    assert len(aggregator.results) == 1
    assert isinstance(aggregator.results[0], training_utils_v1.SliceAggregator)

    aggregator.finalize()
    return aggregator.results

</source>
<source file="systems/keras-2.7.0/keras/engine/training_utils_v1_test.py" startline="294" endline="311" pcid="2457">
  def test_nested_aggregation(self):
    aggregator = training_utils_v1.OutputsAggregator(
        use_steps=False, num_samples=6)

    batches = np.array_split(_TEST_DATA, 4)
    batch_start = 0
    for i, batch in enumerate(zip(batches, batches)):
      if i == 0:
        aggregator.create(batch)

      batch_end = batch_start + batch[0].shape[0]
      aggregator.aggregate(batch, batch_start, batch_end)
      batch_start = batch_end

    assert len(aggregator.results) == 2
    aggregator.finalize()
    self.assertAllEqual(aggregator.results, (_TEST_DATA, _TEST_DATA))

</source>
</class>

<class classid="90" nclones="2" nlines="33" similarity="79">
<source file="systems/keras-2.7.0/keras/engine/training_distributed_v1.py" startline="678" endline="718" pcid="2478">
  def evaluate(self,
               model,
               x=None,
               y=None,
               batch_size=None,
               verbose=1,
               sample_weight=None,
               steps=None,
               callbacks=None,
               **kwargs):
    """Evaluate loop for Distribution Strategies."""
    dist_utils.validate_inputs(x, y)
    batch_size, steps = dist_utils.process_batch_and_step_size(
        model._distribution_strategy, x, batch_size, steps, ModeKeys.TEST)
    batch_size = model._validate_or_infer_batch_size(batch_size, steps, x)
    dataset = model._distribution_standardize_user_data(
        x, y,
        sample_weight=sample_weight,
        batch_size=batch_size,
        allow_partial_batch=True)

    if backend.is_tpu_strategy(model._distribution_strategy):
      steps = training_utils_v1.infer_steps_for_dataset(
          model, dataset, steps, steps_name='steps')
      if steps is None:
        raise ValueError('Number of steps could not be inferred from the data, '
                         'please pass the steps argument.')

      if not tf.executing_eagerly():
        # Run TPU evaluation in a custom loop in graph mode.
        return experimental_tpu_test_loop(
            model, dataset, verbose=verbose, steps=steps, callbacks=callbacks)

    return training_arrays_v1.test_loop(
        model,
        inputs=dataset,
        batch_size=batch_size,
        verbose=verbose,
        steps=steps,
        callbacks=callbacks)

</source>
<source file="systems/keras-2.7.0/keras/engine/training_distributed_v1.py" startline="719" endline="753" pcid="2479">
  def predict(self,
              model,
              x,
              batch_size=None,
              verbose=0,
              steps=None,
              callbacks=None,
              **kwargs):
    """Predict loop for Distribution Strategies."""
    dist_utils.validate_inputs(x=x, y=None)
    batch_size, steps = dist_utils.process_batch_and_step_size(
        model._distribution_strategy, x, batch_size, steps, ModeKeys.PREDICT)
    batch_size = model._validate_or_infer_batch_size(batch_size, steps, x)
    dataset = model._distribution_standardize_user_data(
        x,
        batch_size=batch_size,
        allow_partial_batch=True)
    if backend.is_tpu_strategy(model._distribution_strategy):
      steps = training_utils_v1.infer_steps_for_dataset(
          model, dataset, steps, steps_name='steps')
      if steps is None:
        raise ValueError('Number of steps could not be inferred from the data, '
                         'please pass the steps argument.')
      if not tf.executing_eagerly():
        return experimental_tpu_predict_loop(
            model, dataset, verbose=verbose, steps=steps, callbacks=callbacks)
    return training_arrays_v1.predict_loop(
        model,
        dataset,
        batch_size=batch_size,
        verbose=verbose,
        steps=steps,
        callbacks=callbacks)


</source>
</class>

<class classid="91" nclones="2" nlines="14" similarity="92">
<source file="systems/keras-2.7.0/keras/engine/deferred_sequential_test.py" startline="120" endline="134" pcid="2490">
  def test_saving_savedmodel(self):
    model = get_model()
    model(np.random.random((3, 6)))  # Build model

    path = os.path.join(self.get_temp_dir(), 'model_path')
    model.save(path)
    new_model = keras.models.load_model(path)
    model_layers = model._flatten_layers(include_self=True, recursive=False)
    new_model_layers = new_model._flatten_layers(
        include_self=True, recursive=False)
    for layer1, layer2 in zip(model_layers, new_model_layers):
      self.assertEqual(layer1.name, layer2.name)
      for w1, w2 in zip(layer1.weights, layer2.weights):
        self.assertAllClose(w1, w2)

</source>
<source file="systems/keras-2.7.0/keras/engine/deferred_sequential_test.py" startline="137" endline="152" pcid="2491">
  def test_saving_h5(self):
    path = os.path.join(self.get_temp_dir(), 'model_path.h5')
    model = get_model()
    model(np.random.random((3, 6)))  # Build model

    path = os.path.join(self.get_temp_dir(), 'model_path.h5')
    model.save(path)
    new_model = keras.models.load_model(path)
    model_layers = model._flatten_layers(include_self=True, recursive=False)
    new_model_layers = new_model._flatten_layers(
        include_self=True, recursive=False)
    for layer1, layer2 in zip(model_layers, new_model_layers):
      self.assertEqual(layer1.name, layer2.name)
      for w1, w2 in zip(layer1.weights, layer2.weights):
        self.assertAllClose(w1, w2)

</source>
</class>

<class classid="92" nclones="2" nlines="10" similarity="80">
<source file="systems/keras-2.7.0/keras/engine/keras_tensor_test.py" startline="119" endline="132" pcid="2526">
  def test_sparse_instance_property(self, property_name):
    inp = layers.Input(shape=[3], sparse=True)
    out = getattr(inp, property_name)
    model = training.Model(inp, out)

    x = tf.SparseTensor([[0, 0], [0, 1], [1, 1], [1, 2]], [1, 2, 3, 4], [2, 3])
    expected_property = getattr(x, property_name)
    self.assertAllEqual(model(x), expected_property)

    # Test that it works with serialization and deserialization as well
    model_config = model.get_config()
    model2 = training.Model.from_config(model_config)
    self.assertAllEqual(model2(x), expected_property)

</source>
<source file="systems/keras-2.7.0/keras/engine/ragged_keras_tensor_test.py" startline="107" endline="120" pcid="2540">
  def test_instance_property(self, property_name):
    inp = layers.Input(shape=[None], ragged=True)
    out = getattr(inp, property_name)
    model = training.Model(inp, out)

    x = tf.ragged.constant([[3, 4], [1, 2], [3, 5]])
    expected_property = getattr(x, property_name)
    self.assertAllEqual(model(x), expected_property)

    # Test that it works with serialization and deserialization as well
    model_config = model.get_config()
    model2 = training.Model.from_config(model_config)
    self.assertAllEqual(model2(x), expected_property)

</source>
</class>

<class classid="93" nclones="4" nlines="12" similarity="83">
<source file="systems/keras-2.7.0/keras/engine/ragged_keras_tensor_test.py" startline="184" endline="199" pcid="2542">
  def test_from_value_rowids(self):
    inp = layers.Input(shape=[None])
    out = tf.RaggedTensor.from_value_rowids(
        inp, value_rowids=[0, 0, 0, 0, 2, 2, 2, 3], nrows=5)
    model = training.Model(inp, out)

    x = tf.constant([3, 1, 4, 1, 5, 9, 2, 6])
    expected = tf.RaggedTensor.from_value_rowids(
        x, value_rowids=[0, 0, 0, 0, 2, 2, 2, 3], nrows=5)
    self.assertAllEqual(model(x), expected)

    # Test that the model can serialize and deserialize as well
    model_config = model.get_config()
    model2 = training.Model.from_config(model_config)
    self.assertAllEqual(model2(x), expected)

</source>
<source file="systems/keras-2.7.0/keras/engine/ragged_keras_tensor_test.py" startline="216" endline="231" pcid="2544">
  def test_from_row_lengths(self):
    inp = layers.Input(shape=[None])
    out = tf.RaggedTensor.from_row_lengths(
        inp, row_lengths=[4, 0, 3, 1, 0])
    model = training.Model(inp, out)

    x = tf.constant([3, 1, 4, 1, 5, 9, 2, 6])
    expected = tf.RaggedTensor.from_row_lengths(
        x, row_lengths=[4, 0, 3, 1, 0])
    self.assertAllEqual(model(x), expected)

    # Test that the model can serialize and deserialize as well
    model_config = model.get_config()
    model2 = training.Model.from_config(model_config)
    self.assertAllEqual(model2(x), expected)

</source>
<source file="systems/keras-2.7.0/keras/engine/ragged_keras_tensor_test.py" startline="200" endline="215" pcid="2543">
  def test_from_row_splits(self):
    inp = layers.Input(shape=[None])
    out = tf.RaggedTensor.from_row_splits(
        inp, row_splits=[0, 4, 4, 7, 8, 8])
    model = training.Model(inp, out)

    x = tf.constant([3, 1, 4, 1, 5, 9, 2, 6])
    expected = tf.RaggedTensor.from_row_splits(
        x, row_splits=[0, 4, 4, 7, 8, 8])
    self.assertAllEqual(model(x), expected)

    # Test that the model can serialize and deserialize as well
    model_config = model.get_config()
    model2 = training.Model.from_config(model_config)
    self.assertAllEqual(model2(x), expected)

</source>
<source file="systems/keras-2.7.0/keras/engine/ragged_keras_tensor_test.py" startline="232" endline="247" pcid="2545">
  def test_from_row_starts(self):
    inp = layers.Input(shape=[None])
    out = tf.RaggedTensor.from_row_starts(
        inp, row_starts=[0, 4, 4, 7, 8])
    model = training.Model(inp, out)

    x = tf.constant([3, 1, 4, 1, 5, 9, 2, 6])
    expected = tf.RaggedTensor.from_row_starts(
        x, row_starts=[0, 4, 4, 7, 8])
    self.assertAllEqual(model(x), expected)

    # Test that the model can serialize and deserialize as well
    model_config = model.get_config()
    model2 = training.Model.from_config(model_config)
    self.assertAllEqual(model2(x), expected)

</source>
</class>

<class classid="94" nclones="3" nlines="15" similarity="86">
<source file="systems/keras-2.7.0/keras/engine/ragged_keras_tensor_test.py" startline="281" endline="300" pcid="2548">
  def test_from_nested_value_row_ids(self):
    nested_value_rowids = [
        tf.constant([0, 0, 1, 3, 3], tf.int64),
        tf.constant([0, 0, 2, 2, 2, 3, 4], tf.int64)
    ]
    inp = layers.Input(shape=[None], dtype=tf.string)
    out = tf.RaggedTensor.from_nested_value_rowids(
        inp, nested_value_rowids)
    model = training.Model(inp, out)

    x = tf.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])
    expected = tf.RaggedTensor.from_nested_value_rowids(
        x, nested_value_rowids)
    self.assertAllEqual(model(x), expected)

    # Test that the model can serialize and deserialize as well
    model_config = model.get_config()
    model2 = training.Model.from_config(model_config)
    self.assertAllEqual(model2(x), expected)

</source>
<source file="systems/keras-2.7.0/keras/engine/ragged_keras_tensor_test.py" startline="321" endline="340" pcid="2550">
  def test_from_nested_row_lengths(self):
    nested_row_lengths = [
        tf.constant([2, 1, 0, 2], tf.int64),
        tf.constant([2, 0, 3, 1, 1], tf.int64)
    ]
    inp = layers.Input(shape=[None], dtype=tf.string)
    out = tf.RaggedTensor.from_nested_row_lengths(
        inp, nested_row_lengths)
    model = training.Model(inp, out)

    x = tf.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])
    expected = tf.RaggedTensor.from_nested_row_lengths(
        x, nested_row_lengths)
    self.assertAllEqual(model(x), expected)

    # Test that the model can serialize and deserialize as well
    model_config = model.get_config()
    model2 = training.Model.from_config(model_config)
    self.assertAllEqual(model2(x), expected)

</source>
<source file="systems/keras-2.7.0/keras/engine/ragged_keras_tensor_test.py" startline="301" endline="320" pcid="2549">
  def test_from_nested_row_splits(self):
    nested_row_splits = [
        tf.constant([0, 2, 3, 3, 5], tf.int64),
        tf.constant([0, 2, 2, 5, 6, 7], tf.int64)
    ]
    inp = layers.Input(shape=[None], dtype=tf.string)
    out = tf.RaggedTensor.from_nested_row_splits(
        inp, nested_row_splits)
    model = training.Model(inp, out)

    x = tf.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])
    expected = tf.RaggedTensor.from_nested_row_splits(
        x, nested_row_splits)
    self.assertAllEqual(model(x), expected)

    # Test that the model can serialize and deserialize as well
    model_config = model.get_config()
    model2 = training.Model.from_config(model_config)
    self.assertAllEqual(model2(x), expected)

</source>
</class>

<class classid="95" nclones="2" nlines="11" similarity="81">
<source file="systems/keras-2.7.0/keras/engine/data_adapter_test.py" startline="136" endline="146" pcid="2689">
  def test_epochs(self):
    num_epochs = 3
    adapter = self.adapter_cls(
        self.numpy_input, self.numpy_target, batch_size=5, epochs=num_epochs)
    ds_iter = iter(adapter.get_dataset())
    num_batches_per_epoch = self.numpy_input.shape[0] // 5
    for _ in range(num_batches_per_epoch * num_epochs):
      next(ds_iter)
    with self.assertRaises(StopIteration):
      next(ds_iter)

</source>
<source file="systems/keras-2.7.0/keras/engine/data_adapter_test.py" startline="434" endline="445" pcid="2706">
  def test_epochs(self):
    num_epochs = 3
    adapter = self.adapter_cls(
        self.arraylike_input,
        self.numpy_target, batch_size=5, epochs=num_epochs)
    ds_iter = iter(adapter.get_dataset())
    num_batches_per_epoch = self.numpy_input.shape[0] // 5
    for _ in range(num_batches_per_epoch * num_epochs):
      next(ds_iter)
    with self.assertRaises(StopIteration):
      next(ds_iter)

</source>
</class>

<class classid="96" nclones="2" nlines="20" similarity="95">
<source file="systems/keras-2.7.0/keras/engine/data_adapter_test.py" startline="269" endline="300" pcid="2696">
  def test_shuffle_correctness(self):
    num_samples = 100
    batch_size = 32
    x = np.arange(num_samples)
    np.random.seed(99)
    adapter = self.adapter_cls(
        x, y=None, batch_size=batch_size, shuffle=True, epochs=2)

    def _get_epoch(ds_iter):
      ds_data = []
      for _ in range(int(math.ceil(num_samples / batch_size))):
        ds_data.append(next(ds_iter).numpy())
      return np.concatenate(ds_data)

    ds_iter = iter(adapter.get_dataset())

    # First epoch.
    epoch_data = _get_epoch(ds_iter)
    # Check that shuffling occurred.
    self.assertNotAllClose(x, epoch_data)
    # Check that each elements appears, and only once.
    self.assertAllClose(x, np.sort(epoch_data))

    # Second epoch.
    second_epoch_data = _get_epoch(ds_iter)
    # Check that shuffling occurred.
    self.assertNotAllClose(x, second_epoch_data)
    # Check that shuffling is different across epochs.
    self.assertNotAllClose(epoch_data, second_epoch_data)
    # Check that each elements appears, and only once.
    self.assertAllClose(x, np.sort(second_epoch_data))

</source>
<source file="systems/keras-2.7.0/keras/engine/data_adapter_test.py" startline="500" endline="531" pcid="2710">
  def test_shuffle_correctness(self):
    num_samples = 100
    batch_size = 32
    x = DummyArrayLike(np.arange(num_samples))
    np.random.seed(99)
    adapter = self.adapter_cls(
        x, y=None, batch_size=batch_size, shuffle=True, epochs=2)

    def _get_epoch(ds_iter):
      ds_data = []
      for _ in range(int(math.ceil(num_samples / batch_size))):
        ds_data.append(next(ds_iter).numpy())
      return np.concatenate(ds_data)

    ds_iter = iter(adapter.get_dataset())

    # First epoch.
    epoch_data = _get_epoch(ds_iter)
    # Check that shuffling occurred.
    self.assertNotAllClose(x, epoch_data)
    # Check that each elements appears, and only once.
    self.assertAllClose(x, np.sort(epoch_data))

    # Second epoch.
    second_epoch_data = _get_epoch(ds_iter)
    # Check that shuffling occurred.
    self.assertNotAllClose(x, second_epoch_data)
    # Check that shuffling is different across epochs.
    self.assertNotAllClose(epoch_data, second_epoch_data)
    # Check that each elements appears, and only once.
    self.assertAllClose(x, np.sort(second_epoch_data))

</source>
</class>

<class classid="97" nclones="2" nlines="31" similarity="96">
<source file="systems/keras-2.7.0/keras/engine/data_adapter_test.py" startline="302" endline="354" pcid="2698">
  def test_batch_shuffle_correctness(self):
    num_samples = 100
    batch_size = 6
    x = np.arange(num_samples)
    np.random.seed(99)
    adapter = self.adapter_cls(
        x, y=None, batch_size=batch_size, shuffle='batch', epochs=2)

    def _get_epoch_batches(ds_iter):
      ds_data = []
      for _ in range(int(math.ceil(num_samples / batch_size))):
        ds_data.append(next(ds_iter)[0].numpy())
      return ds_data

    ds_iter = iter(adapter.get_dataset())

    # First epoch.
    epoch_batch_data = _get_epoch_batches(ds_iter)
    epoch_data = np.concatenate(epoch_batch_data)

    def _verify_batch(batch):
      # Verify that a batch contains only contiguous data, and that it has
      # been shuffled.
      shuffled_batch = np.sort(batch)
      self.assertNotAllClose(batch, shuffled_batch)
      for i in range(1, len(batch)):
        self.assertEqual(shuffled_batch[i-1] + 1, shuffled_batch[i])

    # Assert that the data within each batch remains contiguous
    for batch in epoch_batch_data:
      _verify_batch(batch)

    # Check that individual batches are unshuffled
    # Check that shuffling occurred.
    self.assertNotAllClose(x, epoch_data)
    # Check that each elements appears, and only once.
    self.assertAllClose(x, np.sort(epoch_data))

    # Second epoch.
    second_epoch_batch_data = _get_epoch_batches(ds_iter)
    second_epoch_data = np.concatenate(second_epoch_batch_data)

    # Assert that the data within each batch remains contiguous
    for batch in second_epoch_batch_data:
      _verify_batch(batch)

    # Check that shuffling occurred.
    self.assertNotAllClose(x, second_epoch_data)
    # Check that shuffling is different across epochs.
    self.assertNotAllClose(epoch_data, second_epoch_data)
    # Check that each elements appears, and only once.
    self.assertAllClose(x, np.sort(second_epoch_data))

</source>
<source file="systems/keras-2.7.0/keras/engine/data_adapter_test.py" startline="533" endline="585" pcid="2712">
  def test_batch_shuffle_correctness(self):
    num_samples = 100
    batch_size = 6
    x = DummyArrayLike(np.arange(num_samples))
    np.random.seed(99)
    adapter = self.adapter_cls(
        x, y=None, batch_size=batch_size, shuffle='batch', epochs=2)

    def _get_epoch_batches(ds_iter):
      ds_data = []
      for _ in range(int(math.ceil(num_samples / batch_size))):
        ds_data.append(next(ds_iter)[0].numpy())
      return ds_data

    ds_iter = iter(adapter.get_dataset())

    # First epoch.
    epoch_batch_data = _get_epoch_batches(ds_iter)
    epoch_data = np.concatenate(epoch_batch_data)

    def _verify_batch(batch):
      # Verify that a batch contains only contiguous data, but that it has
      # been shuffled.
      shuffled_batch = np.sort(batch)
      self.assertNotAllClose(batch, shuffled_batch)
      for i in range(1, len(batch)):
        self.assertEqual(shuffled_batch[i-1] + 1, shuffled_batch[i])

    # Assert that the data within each batch is shuffled contiguous data
    for batch in epoch_batch_data:
      _verify_batch(batch)

    # Check that individual batches are unshuffled
    # Check that shuffling occurred.
    self.assertNotAllClose(x, epoch_data)
    # Check that each elements appears, and only once.
    self.assertAllClose(x, np.sort(epoch_data))

    # Second epoch.
    second_epoch_batch_data = _get_epoch_batches(ds_iter)
    second_epoch_data = np.concatenate(second_epoch_batch_data)

    # Assert that the data within each batch remains contiguous
    for batch in second_epoch_batch_data:
      _verify_batch(batch)

    # Check that shuffling occurred.
    self.assertNotAllClose(x, second_epoch_data)
    # Check that shuffling is different across epochs.
    self.assertNotAllClose(epoch_data, second_epoch_data)
    # Check that each elements appears, and only once.
    self.assertAllClose(x, np.sort(second_epoch_data))

</source>
</class>

<class classid="98" nclones="2" nlines="13" similarity="100">
<source file="systems/keras-2.7.0/keras/engine/data_adapter_test.py" startline="470" endline="483" pcid="2708">
  def test_training_numpy_target(self):
    self.model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd',
                       run_eagerly=testing_utils.should_run_eagerly())
    self.model.fit(self.arraylike_input,
                   self.numpy_target, batch_size=5)
    self.model.fit(self.arraylike_input,
                   self.numpy_target, shuffle=True,
                   batch_size=5)
    self.model.fit(self.arraylike_input,
                   self.numpy_target, shuffle='batch',
                   batch_size=5)
    self.model.evaluate(self.arraylike_input,
                        self.numpy_target, batch_size=5)

</source>
<source file="systems/keras-2.7.0/keras/engine/data_adapter_test.py" startline="485" endline="498" pcid="2709">
  def test_training_tensor_target(self):
    self.model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd',
                       run_eagerly=testing_utils.should_run_eagerly())
    self.model.fit(self.arraylike_input,
                   self.tensor_target, batch_size=5)
    self.model.fit(self.arraylike_input,
                   self.tensor_target, shuffle=True,
                   batch_size=5)
    self.model.fit(self.arraylike_input,
                   self.tensor_target, shuffle='batch',
                   batch_size=5)
    self.model.evaluate(self.arraylike_input,
                        self.tensor_target, batch_size=5)

</source>
</class>

<class classid="99" nclones="4" nlines="12" similarity="72">
<source file="systems/keras-2.7.0/keras/engine/data_adapter_test.py" startline="805" endline="819" pcid="2748">
  def test_finite_dataset_with_steps_per_epoch(self):
    data = tf.data.Dataset.from_tensor_slices([0, 1, 2, 3]).batch(1)
    # User can choose to only partially consume `Dataset`.
    data_handler = data_adapter.DataHandler(
        data, initial_epoch=0, epochs=2, steps_per_epoch=2)
    self.assertEqual(data_handler.inferred_steps, 2)
    self.assertFalse(data_handler._adapter.should_recreate_iterator())
    returned_data = []
    for _, iterator in data_handler.enumerate_epochs():
      epoch_data = []
      for _ in data_handler.steps():
        epoch_data.append(next(iterator).numpy())
      returned_data.append(epoch_data)
    self.assertEqual(returned_data, [[0, 1], [2, 3]])

</source>
<source file="systems/keras-2.7.0/keras/engine/data_adapter_test.py" startline="832" endline="846" pcid="2750">
  def test_finite_dataset_with_steps_per_epoch_exact_size(self):
    data = tf.data.Dataset.from_tensor_slices([0, 1, 2, 3]).batch(1)
    # If user specifies exact size of `Dataset` as `steps_per_epoch`,
    # create a new iterator each epoch.
    data_handler = data_adapter.DataHandler(
        data, initial_epoch=0, epochs=2, steps_per_epoch=4)
    self.assertTrue(data_handler._adapter.should_recreate_iterator())
    returned_data = []
    for _, iterator in data_handler.enumerate_epochs():
      epoch_data = []
      for _ in data_handler.steps():
        epoch_data.append(next(iterator).numpy())
      returned_data.append(epoch_data)
    self.assertEqual(returned_data, [[0, 1, 2, 3], [0, 1, 2, 3]])

</source>
<source file="systems/keras-2.7.0/keras/engine/data_adapter_test.py" startline="847" endline="858" pcid="2751">
  def test_infinite_dataset_with_steps_per_epoch(self):
    data = tf.data.Dataset.from_tensor_slices([0, 1, 2]).batch(1).repeat()
    data_handler = data_adapter.DataHandler(
        data, initial_epoch=0, epochs=2, steps_per_epoch=3)
    returned_data = []
    for _, iterator in data_handler.enumerate_epochs():
      epoch_data = []
      for _ in data_handler.steps():
        epoch_data.append(next(iterator).numpy())
      returned_data.append(epoch_data)
    self.assertEqual(returned_data, [[0, 1, 2], [0, 1, 2]])

</source>
<source file="systems/keras-2.7.0/keras/engine/data_adapter_test.py" startline="820" endline="831" pcid="2749">
  def test_finite_dataset_without_steps_per_epoch(self):
    data = tf.data.Dataset.from_tensor_slices([0, 1, 2]).batch(1)
    data_handler = data_adapter.DataHandler(data, initial_epoch=0, epochs=2)
    self.assertEqual(data_handler.inferred_steps, 3)
    returned_data = []
    for _, iterator in data_handler.enumerate_epochs():
      epoch_data = []
      for _ in data_handler.steps():
        epoch_data.append(next(iterator).numpy())
      returned_data.append(epoch_data)
    self.assertEqual(returned_data, [[0, 1, 2], [0, 1, 2]])

</source>
</class>

<class classid="100" nclones="2" nlines="18" similarity="78">
<source file="systems/keras-2.7.0/keras/engine/data_adapter_test.py" startline="859" endline="878" pcid="2752">
  def test_unknown_cardinality_dataset_with_steps_per_epoch(self):
    ds = tf.data.Dataset.from_tensor_slices([0, 1, 2, 3, 4, 5, 6])
    filtered_ds = ds.filter(lambda x: x < 4)
    self.assertEqual(
        tf.data.experimental.cardinality(filtered_ds).numpy(), tf.data.experimental.UNKNOWN_CARDINALITY)

    # User can choose to only partially consume `Dataset`.
    data_handler = data_adapter.DataHandler(
        filtered_ds, initial_epoch=0, epochs=2, steps_per_epoch=2)
    self.assertFalse(data_handler._adapter.should_recreate_iterator())
    returned_data = []
    for _, iterator in data_handler.enumerate_epochs():
      epoch_data = []
      for _ in data_handler.steps():
        epoch_data.append(next(iterator))
      returned_data.append(epoch_data)
    returned_data = self.evaluate(returned_data)
    self.assertEqual(returned_data, [[0, 1], [2, 3]])
    self.assertEqual(data_handler.inferred_steps, 2)

</source>
<source file="systems/keras-2.7.0/keras/engine/data_adapter_test.py" startline="879" endline="899" pcid="2753">
  def test_unknown_cardinality_dataset_without_steps_per_epoch(self):
    ds = tf.data.Dataset.from_tensor_slices([0, 1, 2, 3, 4, 5, 6])
    filtered_ds = ds.filter(lambda x: x < 4)
    self.assertEqual(
        tf.data.experimental.cardinality(filtered_ds).numpy(), tf.data.experimental.UNKNOWN_CARDINALITY)

    data_handler = data_adapter.DataHandler(
        filtered_ds, initial_epoch=0, epochs=2)
    self.assertEqual(data_handler.inferred_steps, None)
    self.assertTrue(data_handler._adapter.should_recreate_iterator())
    returned_data = []
    for _, iterator in data_handler.enumerate_epochs():
      epoch_data = []
      with data_handler.catch_stop_iteration():
        for _ in data_handler.steps():
          epoch_data.append(next(iterator))
      returned_data.append(epoch_data)
    returned_data = self.evaluate(returned_data)
    self.assertEqual(returned_data, [[0, 1, 2, 3], [0, 1, 2, 3]])
    self.assertEqual(data_handler.inferred_steps, 4)

</source>
</class>

<class classid="101" nclones="2" nlines="17" similarity="70">
<source file="systems/keras-2.7.0/keras/engine/data_adapter_test.py" startline="933" endline="951" pcid="2756">
  def test_generator(self):

    def generator():
      for _ in range(2):
        for step in range(3):
          yield (tf.convert_to_tensor([step]),)

    data_handler = data_adapter.DataHandler(
        generator(), epochs=2, steps_per_epoch=3)
    returned_data = []
    for _, iterator in data_handler.enumerate_epochs():
      epoch_data = []
      for _ in data_handler.steps():
        epoch_data.append(next(iterator))
      returned_data.append(epoch_data)
    returned_data = self.evaluate(returned_data)
    self.assertEqual(returned_data, [[([0],), ([1],),
                                      ([2],)], [([0],), ([1],), ([2],)]])

</source>
<source file="systems/keras-2.7.0/keras/engine/data_adapter_test.py" startline="967" endline="985" pcid="2759">
  def test_iterator(self):
    def generator():
      for _ in range(2):
        for step in range(3):
          yield (tf.convert_to_tensor([step]),)

    it = iter(tf.data.Dataset.from_generator(
        generator, output_types=('float32',)))
    data_handler = data_adapter.DataHandler(it, epochs=2, steps_per_epoch=3)
    returned_data = []
    for _, iterator in data_handler.enumerate_epochs():
      epoch_data = []
      for _ in data_handler.steps():
        epoch_data.append(next(iterator))
      returned_data.append(epoch_data)
    returned_data = self.evaluate(returned_data)
    self.assertEqual(returned_data, [[([0],), ([1],), ([2],)],
                                     [([0],), ([1],), ([2],)]])

</source>
</class>

<class classid="102" nclones="2" nlines="16" similarity="76">
<source file="systems/keras-2.7.0/keras/engine/compile_utils_test.py" startline="112" endline="132" pcid="2778">
  def test_loss_partial_dict_with_output_names(self):
    loss_container = compile_utils.LossesContainer(
        {'out2': 'mae'}, {'out2': 1.}, output_names=['out1', 'out2'])

    y_t = [tf.ones((10, 1)), tf.zeros((10, 1))]
    y_p = [tf.ones((10, 1)), tf.ones((10, 1))]
    sw = tf.convert_to_tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])

    total_loss = loss_container(y_t, y_p, sample_weight=sw)

    self.assertEqual(total_loss.numpy(), 0.5)
    self.assertLen(loss_container.metrics, 2)

    loss_metric = loss_container.metrics[0]
    self.assertEqual(loss_metric.name, 'loss')
    self.assertEqual(loss_metric.result().numpy(), 0.5)

    out2_metric = loss_container.metrics[1]
    self.assertEqual(out2_metric.name, 'out2_loss')
    self.assertEqual(out2_metric.result().numpy(), 0.5)

</source>
<source file="systems/keras-2.7.0/keras/engine/compile_utils_test.py" startline="194" endline="216" pcid="2781">
  def test_broadcast_single_loss(self):
    loss_container = compile_utils.LossesContainer('mse')

    y_t = [tf.ones((10, 1)), tf.zeros((10, 1))]
    y_p = [tf.ones((10, 1)), tf.ones((10, 1))]
    sw = tf.convert_to_tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])

    total_loss = loss_container(y_t, y_p, sample_weight=sw)
    self.assertEqual(total_loss.numpy(), 0.5)
    self.assertLen(loss_container.metrics, 3)

    loss_metric = loss_container.metrics[0]
    self.assertEqual(loss_metric.name, 'loss')
    self.assertEqual(loss_metric.result().numpy(), 0.5)

    output_1_metric = loss_container.metrics[1]
    self.assertEqual(output_1_metric.name, 'output_1_loss')
    self.assertEqual(output_1_metric.result().numpy(), 0.)

    output_2_metric = loss_container.metrics[2]
    self.assertEqual(output_2_metric.name, 'output_2_loss')
    self.assertEqual(output_2_metric.result().numpy(), 0.5)

</source>
</class>

<class classid="103" nclones="2" nlines="28" similarity="75">
<source file="systems/keras-2.7.0/keras/engine/compile_utils_test.py" startline="156" endline="193" pcid="2780">
  def test_nested_structure(self):
    loss_container = compile_utils.LossesContainer(
        {
            'b': ['mse', None],
            'a': 'mae'
        }, loss_weights={
            'b': [0.5, 0],
            'a': 1
        })

    y_t = {
        'b': [tf.ones((10, 1)),
              tf.zeros((10, 1))],
        'a': tf.zeros((10, 1))
    }
    y_p = {
        'b': [tf.zeros((10, 1)),
              tf.zeros((10, 1))],
        'a': tf.ones((10, 1))
    }
    sw = tf.convert_to_tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])

    total_loss = loss_container(y_t, y_p, sample_weight=sw)
    self.assertEqual(total_loss.numpy(), 0.75)
    self.assertLen(loss_container.metrics, 3)

    loss_metric = loss_container.metrics[0]
    self.assertEqual(loss_metric.name, 'loss')
    self.assertEqual(loss_metric.result().numpy(), 0.75)

    a_metric = loss_container.metrics[1]
    self.assertEqual(a_metric.name, 'a_loss')
    self.assertEqual(a_metric.result().numpy(), 0.5)

    b_1_metric = loss_container.metrics[2]
    self.assertEqual(b_1_metric.name, 'b_1_loss')
    self.assertEqual(b_1_metric.result().numpy(), 0.5)

</source>
<source file="systems/keras-2.7.0/keras/engine/compile_utils_test.py" startline="544" endline="581" pcid="2804">
  def test_nested_structure(self):
    metric_container = compile_utils.MetricsContainer(
        metrics={
            'b': ['mse', None],
            'a': 'mae'
        },
        weighted_metrics={
            'b': [None, None],
            'a': 'mse'
        })

    y_t = {
        'b': [2 * tf.ones((10, 1)),
              tf.zeros((10, 1))],
        'a': tf.zeros((10, 1))
    }
    y_p = {
        'b': [tf.zeros((10, 1)),
              tf.zeros((10, 1))],
        'a': tf.ones((10, 1))
    }
    sw = tf.convert_to_tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])

    metric_container.update_state(y_t, y_p, sample_weight=sw)
    self.assertLen(metric_container.metrics, 3)

    a_mae_metric = metric_container.metrics[0]
    self.assertEqual(a_mae_metric.name, 'a_mae')
    self.assertEqual(a_mae_metric.result().numpy(), 1.)

    weighted_a_mae_metric = metric_container.metrics[1]
    self.assertEqual(weighted_a_mae_metric.name, 'a_mse')
    self.assertEqual(weighted_a_mae_metric.result().numpy(), 1.)

    b_1_mse_metric = metric_container.metrics[2]
    self.assertEqual(b_1_mse_metric.name, 'b_1_mse')
    self.assertEqual(b_1_mse_metric.result().numpy(), 4.)

</source>
</class>

<class classid="104" nclones="2" nlines="21" similarity="73">
<source file="systems/keras-2.7.0/keras/engine/compile_utils_test.py" startline="217" endline="250" pcid="2782">
  def test_missing_label_with_no_loss(self):
    # It's ok to exclude a label if that label has no
    # losses or metrics associated with it.
    loss_container = compile_utils.LossesContainer({
        'output1': 'mse',
        'output3': 'mae'
    })

    y_p = {
        'output1': tf.convert_to_tensor([[0], [1], [2]]),
        'output2': tf.convert_to_tensor([[3], [4], [5]]),
        'output3': tf.convert_to_tensor([[6], [7], [8]])
    }
    y_t = {
        'output1': tf.convert_to_tensor([[1], [2], [3]]),
        'output3': tf.convert_to_tensor([[4], [5], [6]])
    }

    total_loss = loss_container(y_t, y_p)
    self.assertEqual(total_loss.numpy(), 3.)
    self.assertLen(loss_container.metrics, 3)

    loss_metric = loss_container.metrics[0]
    self.assertEqual(loss_metric.name, 'loss')
    self.assertEqual(loss_metric.result().numpy(), 3.)

    output_1_metric = loss_container.metrics[1]
    self.assertEqual(output_1_metric.name, 'output1_loss')
    self.assertEqual(output_1_metric.result().numpy(), 1.)

    output_3_metric = loss_container.metrics[2]
    self.assertEqual(output_3_metric.name, 'output3_loss')
    self.assertEqual(output_3_metric.result().numpy(), 2.)

</source>
<source file="systems/keras-2.7.0/keras/engine/compile_utils_test.py" startline="670" endline="698" pcid="2810">
  def test_missing_label_with_no_metrics(self):
    # It's ok to exclude a label if that label has no
    # losses or metrics associated with it.
    metric_container = compile_utils.MetricsContainer(metrics={
        'output1': 'mae',
        'output3': 'mse'
    })

    y_p = {
        'output1': tf.convert_to_tensor([[0], [1], [2]]),
        'output2': tf.convert_to_tensor([[3], [4], [5]]),
        'output3': tf.convert_to_tensor([[6], [7], [8]])
    }
    y_t = {
        'output1': tf.convert_to_tensor([[1], [2], [3]]),
        'output3': tf.convert_to_tensor([[4], [5], [6]])
    }

    metric_container.update_state(y_t, y_p)
    self.assertLen(metric_container.metrics, 2)

    mae_metric = metric_container.metrics[0]
    self.assertEqual(mae_metric.name, 'output1_mae')
    self.assertEqual(mae_metric.result().numpy(), 1.)

    mse_metric = metric_container.metrics[1]
    self.assertEqual(mse_metric.name, 'output3_mse')
    self.assertEqual(mse_metric.result().numpy(), 4.)

</source>
</class>

<class classid="105" nclones="2" nlines="14" similarity="78">
<source file="systems/keras-2.7.0/keras/engine/compile_utils_test.py" startline="251" endline="266" pcid="2783">
  def test_mismatched_dtypes(self):
    y_t = tf.constant([1, 9, 2, -5], shape=(2, 2))
    y_p = tf.constant([4, 8, 12, 8],
                               shape=(2, 2),
                               dtype=tf.float32)

    def my_mae(labels, preds):
      self.assertEqual(labels.dtype, tf.int32)
      self.assertEqual(preds.dtype, tf.float32)
      labels = tf.cast(labels, preds.dtype)
      return backend.mean(tf.abs(preds - labels), axis=-1)

    loss_container = compile_utils.LossesContainer(my_mae)
    total_loss = loss_container(y_t, y_p)
    self.assertEqual(total_loss.dtype, tf.float32)

</source>
<source file="systems/keras-2.7.0/keras/engine/compile_utils_test.py" startline="280" endline="296" pcid="2787">
  def test_float_dtypes(self):
    y_t = tf.constant([1, 9, 2, -5],
                               shape=(2, 2),
                               dtype=tf.float32)
    y_p = tf.constant([4, 8, 12, 8],
                               shape=(2, 2),
                               dtype=tf.float64)

    def my_mae(labels, preds):
      self.assertEqual(labels.dtype, tf.float64)
      self.assertEqual(preds.dtype, tf.float64)
      return backend.mean(tf.abs(preds - labels), axis=-1)

    loss_container = compile_utils.LossesContainer(my_mae)
    total_loss = loss_container(y_t, y_p)
    self.assertEqual(total_loss.dtype, tf.float64)

</source>
</class>

<class classid="106" nclones="3" nlines="12" similarity="75">
<source file="systems/keras-2.7.0/keras/engine/compile_utils_test.py" startline="297" endline="311" pcid="2789">
  def test_loss_masking(self):
    loss_container = compile_utils.LossesContainer('mae')
    y_p = tf.constant([[[1], [1]], [[0], [0]]], dtype=tf.float32)
    y_t = tf.constant([[[1], [1]], [[1], [1]]], dtype=tf.float32)
    y_p._keras_mask = tf.constant([[1, 0], [1, 0]],
                                           dtype=tf.float32)

    total_loss = loss_container(y_t, y_p)
    self.assertAlmostEqual(total_loss.numpy(), .25)  # sum over batch size

    self.assertLen(loss_container.metrics, 1)
    loss_metric = loss_container.metrics[0]
    self.assertEqual(loss_metric.name, 'loss')
    self.assertAlmostEqual(loss_metric.result().numpy(), .25)

</source>
<source file="systems/keras-2.7.0/keras/engine/compile_utils_test.py" startline="312" endline="326" pcid="2790">
  def test_loss_sample_weight(self):
    loss_container = compile_utils.LossesContainer('mae')
    y_p = tf.constant([[[1], [1]], [[0], [0]]], dtype=tf.float32)
    y_t = tf.constant([[[1], [1]], [[1], [1]]], dtype=tf.float32)
    sw = tf.constant([[.2, .3], [.5, 0]], dtype=tf.float32)

    total_loss = loss_container(y_t, y_p, sample_weight=sw)
    # (0 * .2 + 0 * .3 + 1 * .5 + 1 * 0) / 4
    self.assertAlmostEqual(total_loss.numpy(), .125)

    self.assertLen(loss_container.metrics, 1)
    loss_metric = loss_container.metrics[0]
    self.assertEqual(loss_metric.name, 'loss')
    self.assertAlmostEqual(loss_metric.result().numpy(), .125)

</source>
<source file="systems/keras-2.7.0/keras/engine/compile_utils_test.py" startline="327" endline="343" pcid="2791">
  def test_loss_masking_sample_weight(self):
    loss_container = compile_utils.LossesContainer('mae')
    y_p = tf.constant([[[1], [1]], [[0], [0]]], dtype=tf.float32)
    y_t = tf.constant([[[1], [1]], [[1], [1]]], dtype=tf.float32)
    sw = tf.constant([[.2, .3], [.5, 0]], dtype=tf.float32)
    y_p._keras_mask = tf.constant([[1, 0], [1, 0]],
                                           dtype=tf.float32)

    total_loss = loss_container(y_t, y_p, sample_weight=sw)
    # (0 * .2 + 1 * .5) / 4
    self.assertAlmostEqual(total_loss.numpy(), .125)  # sum over batch size

    self.assertLen(loss_container.metrics, 1)
    loss_metric = loss_container.metrics[0]
    self.assertEqual(loss_metric.name, 'loss')
    self.assertAlmostEqual(loss_metric.result().numpy(), .125)

</source>
</class>

<class classid="107" nclones="2" nlines="12" similarity="91">
<source file="systems/keras-2.7.0/keras/engine/compile_utils_test.py" startline="344" endline="361" pcid="2792">
  def test_custom_loss_callables(self):

    def custom_loss_fn(y_true, y_pred):
      return tf.reduce_sum(y_true - y_pred)

    class CustomLossClass:

      def __call__(self, y_true, y_pred):
        return tf.reduce_sum(y_true - y_pred)

    loss_container = compile_utils.LossesContainer(
        [custom_loss_fn, CustomLossClass()])
    y_t, y_p = tf.ones((10, 5)), tf.zeros((10, 5))
    loss_container(y_t, y_p)

    self.assertEqual(loss_container._losses[0].name, 'custom_loss_fn')
    self.assertEqual(loss_container._losses[1].name, 'custom_loss_class')

</source>
<source file="systems/keras-2.7.0/keras/engine/compile_utils_test.py" startline="769" endline="786" pcid="2815">
  def test_custom_metric_callables(self):

    def custom_metric_fn(y_true, y_pred):
      return tf.reduce_sum(y_true - y_pred)

    class CustomMetricClass:

      def __call__(self, y_true, y_pred):
        return tf.reduce_sum(y_true - y_pred)

    metric_container = compile_utils.MetricsContainer(
        [custom_metric_fn, CustomMetricClass()])
    y_t, y_p = tf.ones((10, 5)), tf.zeros((10, 5))
    metric_container.update_state(y_t, y_p)

    self.assertEqual(metric_container.metrics[0].name, 'custom_metric_fn')
    self.assertEqual(metric_container.metrics[1].name, 'custom_metric_class')

</source>
</class>

<class classid="108" nclones="2" nlines="20" similarity="77">
<source file="systems/keras-2.7.0/keras/engine/compile_utils_test.py" startline="582" endline="601" pcid="2805">
  def test_crossentropy(self):
    metric_container = compile_utils.MetricsContainer('crossentropy')
    y_t, y_p = tf.ones((10, 1)), tf.ones((10, 1))
    metric_container.update_state(y_t, y_p)
    self.assertEqual(metric_container.metrics[0]._fn,
                     metrics_mod.binary_crossentropy)

    metric_container = compile_utils.MetricsContainer('crossentropy')
    y_t, y_p = tf.ones((10, 1)), tf.ones((10, 20))
    self.assertEqual(y_p.shape.as_list()[-1], 20)
    metric_container.update_state(y_t, y_p)
    self.assertEqual(metric_container.metrics[0]._fn,
                     metrics_mod.sparse_categorical_crossentropy)

    metric_container = compile_utils.MetricsContainer('crossentropy')
    y_t, y_p = tf.ones((10, 20)), tf.ones((10, 20))
    metric_container.update_state(y_t, y_p)
    self.assertEqual(metric_container.metrics[0]._fn,
                     metrics_mod.categorical_crossentropy)

</source>
<source file="systems/keras-2.7.0/keras/engine/compile_utils_test.py" startline="602" endline="627" pcid="2806">
  def test_accuracy(self):
    metric_container = compile_utils.MetricsContainer('accuracy')
    y_t, y_p = tf.ones((10, 1)), tf.ones((10, 1))
    metric_container.update_state(y_t, y_p)
    self.assertEqual(metric_container.metrics[0]._fn,
                     metrics_mod.binary_accuracy)

    metric_container = compile_utils.MetricsContainer('Accuracy')
    y_t, y_p = tf.ones((10, 1)), tf.ones((10, 1))
    metric_container.update_state(y_t, y_p)
    self.assertEqual(metric_container.metrics[0]._fn,
                     metrics_mod.binary_accuracy)

    metric_container = compile_utils.MetricsContainer('accuracy')
    y_t, y_p = tf.ones((10, 1)), tf.ones((10, 20))
    self.assertEqual(y_p.shape.as_list()[-1], 20)
    metric_container.update_state(y_t, y_p)
    self.assertEqual(metric_container.metrics[0]._fn,
                     metrics_mod.sparse_categorical_accuracy)

    metric_container = compile_utils.MetricsContainer('accuracy')
    y_t, y_p = tf.ones((10, 20)), tf.ones((10, 20))
    metric_container.update_state(y_t, y_p)
    self.assertEqual(metric_container.metrics[0]._fn,
                     metrics_mod.categorical_accuracy)

</source>
</class>

<class classid="109" nclones="4" nlines="14" similarity="75">
<source file="systems/keras-2.7.0/keras/engine/compile_utils_test.py" startline="628" endline="646" pcid="2807">
  def test_metric_weighting(self):
    metric_container = compile_utils.MetricsContainer(
        metrics=['mae'], weighted_metrics=['mae'])

    y_t = tf.convert_to_tensor([[0], [3], [0]])
    y_p = tf.convert_to_tensor([[0], [0], [0]])
    sw = tf.convert_to_tensor([[1], [0], [1]])

    metric_container.update_state(y_t, y_p, sample_weight=sw)
    self.assertLen(metric_container.metrics, 2)

    mae_metric = metric_container.metrics[0]
    self.assertEqual(mae_metric.name, 'mae')
    self.assertEqual(mae_metric.result().numpy(), 1.)

    weighted_mae_metric = metric_container.metrics[1]
    self.assertEqual(weighted_mae_metric.name, 'weighted_mae')
    self.assertEqual(weighted_mae_metric.result().numpy(), 0.)

</source>
<source file="systems/keras-2.7.0/keras/engine/compile_utils_test.py" startline="718" endline="735" pcid="2812">
  def test_metrics_sample_weight(self):
    metrics_container = compile_utils.MetricsContainer(
        metrics=['mae'], weighted_metrics=['mse'])
    y_p = tf.constant([[[1], [1]], [[0], [1]]], dtype=tf.float32)
    y_t = tf.constant([[[1], [1]], [[1], [1]]], dtype=tf.float32)
    sw = tf.constant([[.2, .3], [.5, 0]], dtype=tf.float32)

    metrics_container.update_state(y_t, y_p, sample_weight=sw)
    self.assertLen(metrics_container.metrics, 2)

    mae_metric = metrics_container.metrics[0]
    self.assertEqual(mae_metric.name, 'mae')
    self.assertAlmostEqual(mae_metric.result().numpy(), .25)  # 1 / 4

    weighted_mae_metric = metrics_container.metrics[1]
    self.assertEqual(weighted_mae_metric.name, 'mse')
    self.assertAlmostEqual(weighted_mae_metric.result().numpy(), .5)  # .5 / 1

</source>
<source file="systems/keras-2.7.0/keras/engine/compile_utils_test.py" startline="736" endline="755" pcid="2813">
  def test_metrics_masking_sample_weight(self):
    metrics_container = compile_utils.MetricsContainer(
        metrics=['mae'], weighted_metrics=['mse'])
    y_p = tf.constant([[[1], [1]], [[0], [1]]], dtype=tf.float32)
    y_t = tf.constant([[[1], [1]], [[1], [1]]], dtype=tf.float32)
    sw = tf.constant([[.3, .2], [.2, .3]], dtype=tf.float32)
    y_p._keras_mask = tf.constant([[1, 0], [1, 0]],
                                           dtype=tf.float32)

    metrics_container.update_state(y_t, y_p, sample_weight=sw)
    self.assertLen(metrics_container.metrics, 2)

    mae_metric = metrics_container.metrics[0]
    self.assertEqual(mae_metric.name, 'mae')
    self.assertAlmostEqual(mae_metric.result().numpy(), .5)  # 1 / .5

    weighted_mae_metric = metrics_container.metrics[1]
    self.assertEqual(weighted_mae_metric.name, 'mse')
    self.assertAlmostEqual(weighted_mae_metric.result().numpy(), .2 / .5)

</source>
<source file="systems/keras-2.7.0/keras/engine/compile_utils_test.py" startline="699" endline="717" pcid="2811">
  def test_metrics_masking(self):
    metrics_container = compile_utils.MetricsContainer(
        metrics=['mae'], weighted_metrics=['mse'])
    y_p = tf.constant([[[1], [1]], [[0], [0]]], dtype=tf.float32)
    y_t = tf.constant([[[1], [1]], [[1], [1]]], dtype=tf.float32)
    y_p._keras_mask = tf.constant([[1, 1], [0, 0]],
                                           dtype=tf.float32)

    metrics_container.update_state(y_t, y_p)
    self.assertLen(metrics_container.metrics, 2)

    mae_metric = metrics_container.metrics[0]
    self.assertEqual(mae_metric.name, 'mae')
    self.assertAlmostEqual(mae_metric.result().numpy(), 0)

    weighted_mae_metric = metrics_container.metrics[1]
    self.assertEqual(weighted_mae_metric.name, 'mse')
    self.assertAlmostEqual(weighted_mae_metric.result().numpy(), 0)

</source>
</class>

<class classid="110" nclones="2" nlines="16" similarity="76">
<source file="systems/keras-2.7.0/keras/engine/training_eager_test.py" startline="231" endline="248" pcid="2839">
  def test_loss_correctness(self, optimizer_kwargs):
    # Test that training loss is the same in eager and graph
    # (by comparing it to a reference value in a deterministic case)
    layers = [
        keras.layers.Dense(3, activation='relu',
                           kernel_initializer='ones'),
        keras.layers.Dense(2, activation='softmax', kernel_initializer='ones')]
    model = testing_utils.get_model_from_layers(layers, input_shape=(4,))
    model.compile(
        loss='sparse_categorical_crossentropy',
        optimizer=rmsprop.RMSprop(learning_rate=0.001, **optimizer_kwargs),
        run_eagerly=testing_utils.should_run_eagerly())
    x = np.ones((100, 4))
    np.random.seed(123)
    y = np.random.randint(0, 1, size=(100, 1))
    history = model.fit(x, y, epochs=1, batch_size=10)
    self.assertAlmostEqual(history.history['loss'][-1], 0.5836, 4)

</source>
<source file="systems/keras-2.7.0/keras/engine/training_eager_test.py" startline="251" endline="271" pcid="2840">
  def test_loss_correctness_clipvalue_zero(self):
    # Test that training loss is the same in eager and graph
    # (by comparing it to a reference value in a deterministic case)
    # And confirm that setting clipvalue to zero stops all training
    layers = [
        keras.layers.Dense(3, activation='relu',
                           kernel_initializer='ones'),
        keras.layers.Dense(2, activation='softmax', kernel_initializer='ones')]
    model = testing_utils.get_model_from_layers(layers, input_shape=(4,))
    model.compile(
        loss='sparse_categorical_crossentropy',
        optimizer=rmsprop.RMSprop(learning_rate=0.001, clipvalue=0.0),
        run_eagerly=testing_utils.should_run_eagerly())
    x = np.ones((100, 4))
    np.random.seed(123)
    y = np.random.randint(0, 1, size=(100, 1))
    history = model.fit(x, y, epochs=3, batch_size=10)
    self.assertAlmostEqual(history.history['loss'][-3], 0.6931, 4)
    self.assertAlmostEqual(history.history['loss'][-2], 0.6931, 4)
    self.assertAlmostEqual(history.history['loss'][-1], 0.6931, 4)

</source>
</class>

<class classid="111" nclones="2" nlines="40" similarity="80">
<source file="systems/keras-2.7.0/keras/engine/training_generator_v1.py" startline="547" endline="587" pcid="2854">
  def fit(self,
          model,
          x=None,
          y=None,
          batch_size=None,
          epochs=1,
          verbose=1,
          callbacks=None,
          validation_split=0.,
          validation_data=None,
          shuffle=True,
          class_weight=None,
          sample_weight=None,
          initial_epoch=0,
          steps_per_epoch=None,
          validation_steps=None,
          validation_freq=1,
          max_queue_size=10,
          workers=1,
          use_multiprocessing=False):
    model._validate_or_infer_batch_size(batch_size, steps_per_epoch, x)
    training_utils_v1.check_generator_arguments(
        y, sample_weight, validation_split=validation_split)
    return fit_generator(
        model,
        x,
        steps_per_epoch=steps_per_epoch,
        epochs=epochs,
        verbose=verbose,
        callbacks=callbacks,
        validation_data=validation_data,
        validation_steps=validation_steps,
        validation_freq=validation_freq,
        class_weight=class_weight,
        max_queue_size=max_queue_size,
        workers=workers,
        use_multiprocessing=use_multiprocessing,
        shuffle=shuffle,
        initial_epoch=initial_epoch,
        steps_name='steps_per_epoch')

</source>
<source file="systems/keras-2.7.0/keras/engine/training_generator_v1.py" startline="637" endline="678" pcid="2857">
  def fit(self,
          model,
          x=None,
          y=None,
          batch_size=None,
          epochs=1,
          verbose=1,
          callbacks=None,
          validation_split=0.,
          validation_data=None,
          shuffle=True,
          class_weight=None,
          sample_weight=None,
          initial_epoch=0,
          steps_per_epoch=None,
          validation_steps=None,
          validation_freq=1,
          **kwargs):
    model._validate_or_infer_batch_size(batch_size, steps_per_epoch, x)
    # Make sure that y, sample_weights, validation_split are not passed.
    training_utils_v1.validate_dataset_input(x, y, sample_weight,
                                             validation_split)
    if (isinstance(x, (tf.compat.v1.data.Dataset, tf.data.Dataset)) and
        shuffle):
      training_utils_v1.verify_dataset_shuffled(x)

    return fit_generator(
        model,
        x,
        steps_per_epoch=steps_per_epoch,
        epochs=epochs,
        verbose=verbose,
        callbacks=callbacks,
        validation_data=validation_data,
        validation_steps=validation_steps,
        validation_freq=validation_freq,
        class_weight=class_weight,
        workers=0,
        shuffle=shuffle,
        initial_epoch=initial_epoch,
        steps_name='steps_per_epoch')

</source>
</class>

<class classid="112" nclones="5" nlines="21" similarity="70">
<source file="systems/keras-2.7.0/keras/engine/training_generator_v1.py" startline="588" endline="611" pcid="2855">
  def evaluate(self,
               model,
               x=None,
               y=None,
               batch_size=None,
               verbose=1,
               sample_weight=None,
               steps=None,
               callbacks=None,
               max_queue_size=10,
               workers=1,
               use_multiprocessing=False):
    model._validate_or_infer_batch_size(batch_size, steps, x)
    training_utils_v1.check_generator_arguments(y, sample_weight)
    return evaluate_generator(
        model,
        x,
        steps=steps,
        verbose=verbose,
        callbacks=callbacks,
        max_queue_size=max_queue_size,
        workers=workers,
        use_multiprocessing=use_multiprocessing)

</source>
<source file="systems/keras-2.7.0/keras/engine/training_generator_v1.py" startline="612" endline="633" pcid="2856">
  def predict(self,
              model,
              x,
              batch_size=None,
              verbose=0,
              steps=None,
              callbacks=None,
              max_queue_size=10,
              workers=1,
              use_multiprocessing=False):
    model._validate_or_infer_batch_size(batch_size, steps, x)
    return predict_generator(
        model,
        x,
        steps=steps,
        verbose=verbose,
        callbacks=callbacks,
        max_queue_size=max_queue_size,
        workers=workers,
        use_multiprocessing=use_multiprocessing)


</source>
<source file="systems/keras-2.7.0/keras/preprocessing/image.py" startline="833" endline="897" pcid="3658">
  def flow(self,
           x,
           y=None,
           batch_size=32,
           shuffle=True,
           sample_weight=None,
           seed=None,
           save_to_dir=None,
           save_prefix='',
           save_format='png',
           subset=None):
    """Takes data & label arrays, generates batches of augmented data.

    Args:
        x: Input data. Numpy array of rank 4 or a tuple. If tuple, the first
          element should contain the images and the second element another numpy
          array or a list of numpy arrays that gets passed to the output without
          any modifications. Can be used to feed the model miscellaneous data
          along with the images. In case of grayscale data, the channels axis of
          the image array should have value 1, in case of RGB data, it should
          have value 3, and in case of RGBA data, it should have value 4.
        y: Labels.
        batch_size: Int (default: 32).
        shuffle: Boolean (default: True).
        sample_weight: Sample weights.
        seed: Int (default: None).
        save_to_dir: None or str (default: None). This allows you to optionally
          specify a directory to which to save the augmented pictures being
          generated (useful for visualizing what you are doing).
        save_prefix: Str (default: `''`). Prefix to use for filenames of saved
          pictures (only relevant if `save_to_dir` is set).
        save_format: one of "png", "jpeg", "bmp", "pdf", "ppm", "gif",
            "tif", "jpg"
            (only relevant if `save_to_dir` is set). Default: "png".
        subset: Subset of data (`"training"` or `"validation"`) if
          `validation_split` is set in `ImageDataGenerator`.

    Returns:
        An `Iterator` yielding tuples of `(x, y)`
            where `x` is a numpy array of image data
            (in the case of a single image input) or a list
            of numpy arrays (in the case with
            additional inputs) and `y` is a numpy array
            of corresponding labels. If 'sample_weight' is not None,
            the yielded tuples are of the form `(x, y, sample_weight)`.
            If `y` is None, only the numpy array `x` is returned.
    Raises:
      ValueError: If the Value of the argument, `subset` is other than
            "training" or "validation".

    """
    return NumpyArrayIterator(
        x,
        y,
        self,
        batch_size=batch_size,
        shuffle=shuffle,
        sample_weight=sample_weight,
        seed=seed,
        data_format=self.data_format,
        save_to_dir=save_to_dir,
        save_prefix=save_prefix,
        save_format=save_format,
        subset=subset)

</source>
<source file="systems/keras-2.7.0/keras/engine/training_arrays_v1.py" startline="688" endline="705" pcid="2923">
  def predict(self,
              model,
              x,
              batch_size=None,
              verbose=0,
              steps=None,
              callbacks=None,
              **kwargs):
    batch_size = model._validate_or_infer_batch_size(batch_size, steps, x)
    x, _, _ = model._standardize_user_data(
        x, check_steps=True, steps_name='steps', steps=steps)
    return predict_loop(
        model,
        x,
        batch_size=batch_size,
        verbose=verbose,
        steps=steps,
        callbacks=callbacks)
</source>
<source file="systems/keras-2.7.0/keras/engine/training_generator_v1.py" startline="806" endline="824" pcid="2862">
  def predict(self,
              model,
              x,
              batch_size=None,
              verbose=0,
              steps=None,
              callbacks=None,
              **kwargs):
    batch_size = model._validate_or_infer_batch_size(batch_size, steps, x)
    x, _, _ = model._standardize_user_data(
        x, check_steps=True, steps_name='steps', steps=steps)
    return predict_generator(
        model,
        x,
        steps=steps,
        batch_size=batch_size,
        verbose=verbose,
        workers=0,
        callbacks=callbacks)
</source>
</class>

<class classid="113" nclones="2" nlines="13" similarity="71">
<source file="systems/keras-2.7.0/keras/engine/training_generator_v1.py" startline="679" endline="694" pcid="2858">
  def evaluate(self,
               model,
               x=None,
               y=None,
               batch_size=None,
               verbose=1,
               sample_weight=None,
               steps=None,
               callbacks=None,
               **kwargs):
    model._validate_or_infer_batch_size(batch_size, steps, x)
    # Make sure that y, sample_weights, validation_split are not passed.
    training_utils_v1.validate_dataset_input(x, y, sample_weight)
    return evaluate_generator(
        model, x, steps=steps, verbose=verbose, workers=0, callbacks=callbacks)

</source>
<source file="systems/keras-2.7.0/keras/engine/training_generator_v1.py" startline="695" endline="707" pcid="2859">
  def predict(self,
              model,
              x,
              batch_size=None,
              verbose=0,
              steps=None,
              callbacks=None,
              **kwargs):
    model._validate_or_infer_batch_size(batch_size, steps, x)
    return predict_generator(
        model, x, steps=steps, verbose=verbose, workers=0, callbacks=callbacks)


</source>
</class>

<class classid="114" nclones="2" nlines="59" similarity="81">
<source file="systems/keras-2.7.0/keras/engine/training_generator_v1.py" startline="717" endline="778" pcid="2860">
  def fit(self,
          model,
          x=None,
          y=None,
          batch_size=None,
          epochs=1,
          verbose=1,
          callbacks=None,
          validation_split=0.,
          validation_data=None,
          shuffle=True,
          class_weight=None,
          sample_weight=None,
          initial_epoch=0,
          steps_per_epoch=None,
          validation_steps=None,
          validation_freq=1,
          **kwargs):
    batch_size = model._validate_or_infer_batch_size(batch_size,
                                                     steps_per_epoch, x)
    x, y, sample_weights = model._standardize_user_data(
        x,
        y,
        sample_weight=sample_weight,
        class_weight=class_weight,
        batch_size=batch_size,
        check_steps=True,
        steps_name='steps_per_epoch',
        steps=steps_per_epoch,
        validation_split=validation_split,
        shuffle=shuffle)

    if validation_data:
      validation_data = model._prepare_validation_data(validation_data,
                                                       batch_size,
                                                       validation_steps)
    elif validation_split and 0. < validation_split < 1.:
      (x, y, sample_weights, val_x, val_y,
       val_sample_weights) = (
           training_utils_v1.split_training_and_validation_data(
               x, y, sample_weights, validation_split))
      validation_data = (val_x, val_y, val_sample_weights)
    else:
      if validation_steps:
        raise ValueError('`validation_steps` should not be specified if '
                         '`validation_data` is None.')

    return fit_generator(
        model, (x, y, sample_weights),
        steps_per_epoch=steps_per_epoch,
        batch_size=batch_size,
        epochs=epochs,
        verbose=verbose,
        callbacks=callbacks,
        validation_data=validation_data,
        validation_steps=validation_steps,
        validation_freq=validation_freq,
        workers=0,
        shuffle=shuffle,
        initial_epoch=initial_epoch,
        steps_name='steps_per_epoch')

</source>
<source file="systems/keras-2.7.0/keras/engine/training_arrays_v1.py" startline="594" endline="658" pcid="2921">
  def fit(self,
          model,
          x=None,
          y=None,
          batch_size=None,
          epochs=1,
          verbose=1,
          callbacks=None,
          validation_split=0.,
          validation_data=None,
          shuffle=True,
          class_weight=None,
          sample_weight=None,
          initial_epoch=0,
          steps_per_epoch=None,
          validation_steps=None,
          validation_freq=1,
          **kwargs):
    batch_size = model._validate_or_infer_batch_size(batch_size,
                                                     steps_per_epoch, x)

    x, y, sample_weights = model._standardize_user_data(
        x,
        y,
        sample_weight=sample_weight,
        class_weight=class_weight,
        batch_size=batch_size,
        check_steps=True,
        steps_name='steps_per_epoch',
        steps=steps_per_epoch,
        validation_split=validation_split,
        shuffle=shuffle)

    if validation_data:
      val_x, val_y, val_sample_weights = model._prepare_validation_data(
          validation_data, batch_size, validation_steps)
    elif validation_split and 0. < validation_split < 1.:
      (x, y, sample_weights, val_x, val_y, val_sample_weights
      ) = training_utils_v1.split_training_and_validation_data(
          x, y, sample_weights, validation_split)
    else:
      if validation_steps:
        raise ValueError('`validation_steps` should not be specified if '
                         '`validation_data` is None.')
      val_x, val_y, val_sample_weights = None, None, None

    return fit_loop(
        model,
        inputs=x,
        targets=y,
        sample_weights=sample_weights,
        batch_size=batch_size,
        epochs=epochs,
        verbose=verbose,
        callbacks=callbacks,
        val_inputs=val_x,
        val_targets=val_y,
        val_sample_weights=val_sample_weights,
        shuffle=shuffle,
        initial_epoch=initial_epoch,
        steps_per_epoch=steps_per_epoch,
        validation_steps=validation_steps,
        validation_freq=validation_freq,
        steps_name='steps_per_epoch')

</source>
</class>

<class classid="115" nclones="2" nlines="27" similarity="85">
<source file="systems/keras-2.7.0/keras/engine/training_generator_v1.py" startline="779" endline="805" pcid="2861">
  def evaluate(self,
               model,
               x=None,
               y=None,
               batch_size=None,
               verbose=1,
               sample_weight=None,
               steps=None,
               callbacks=None,
               **kwargs):
    batch_size = model._validate_or_infer_batch_size(batch_size, steps, x)
    x, y, sample_weights = model._standardize_user_data(
        x,
        y,
        sample_weight=sample_weight,
        batch_size=batch_size,
        check_steps=True,
        steps_name='steps',
        steps=steps)
    return evaluate_generator(
        model, (x, y, sample_weights),
        steps=steps,
        batch_size=batch_size,
        verbose=verbose,
        workers=0,
        callbacks=callbacks)

</source>
<source file="systems/keras-2.7.0/keras/engine/training_arrays_v1.py" startline="659" endline="687" pcid="2922">
  def evaluate(self,
               model,
               x=None,
               y=None,
               batch_size=None,
               verbose=1,
               sample_weight=None,
               steps=None,
               callbacks=None,
               **kwargs):
    batch_size = model._validate_or_infer_batch_size(batch_size, steps, x)
    x, y, sample_weights = model._standardize_user_data(
        x,
        y,
        sample_weight=sample_weight,
        batch_size=batch_size,
        check_steps=True,
        steps_name='steps',
        steps=steps)
    return test_loop(
        model,
        inputs=x,
        targets=y,
        sample_weights=sample_weights,
        batch_size=batch_size,
        verbose=verbose,
        steps=steps,
        callbacks=callbacks)

</source>
</class>

<class classid="116" nclones="2" nlines="12" similarity="83">
<source file="systems/keras-2.7.0/keras/engine/input_layer_test.py" startline="162" endline="180" pcid="2877">
  def testInputTensorArgInTFFunction(self):
    # We use a mutable model container instead of a model python variable,
    # because python 2.7 does not have `nonlocal`
    model_container = {}

    @tf.function
    def run_model(inp):
      if not model_container:
        # Create a Keras Input
        x = input_layer_lib.Input(tensor=tf.zeros((10, 16)))
        self.assertAllEqual(x.shape.as_list(), [10, 16])

        # Verify you can construct and use a model w/ this input
        model_container['model'] = functional.Functional(x, x * 3.0)
      return model_container['model'](inp)

    self.assertAllEqual(run_model(tf.ones((10, 16))),
                        tf.ones((10, 16)) * 3.0)

</source>
<source file="systems/keras-2.7.0/keras/engine/input_layer_test.py" startline="269" endline="288" pcid="2884">
  def testTypeSpecArgInTFFunction(self):
    # We use a mutable model container instead of a model python variable,
    # because python 2.7 does not have `nonlocal`
    model_container = {}

    @tf.function
    def run_model(inp):
      if not model_container:
        # Create a Keras Input
        x = input_layer_lib.Input(
            type_spec=tf.TensorSpec((10, 16), tf.float32))
        self.assertAllEqual(x.shape.as_list(), [10, 16])

        # Verify you can construct and use a model w/ this input
        model_container['model'] = functional.Functional(x, x * 3.0)
      return model_container['model'](inp)

    self.assertAllEqual(run_model(tf.ones((10, 16))),
                        tf.ones((10, 16)) * 3.0)

</source>
</class>

<class classid="117" nclones="2" nlines="13" similarity="92">
<source file="systems/keras-2.7.0/keras/engine/input_layer_test.py" startline="197" endline="218" pcid="2880">
  def testCompositeInputTensorArgInTFFunction(self):
    # We use a mutable model container instead of a model python variable,
    # because python 2.7 does not have `nonlocal`
    model_container = {}

    @tf.function
    def run_model(inp):
      if not model_container:
        # Create a Keras Input
        rt = tf.RaggedTensor.from_row_splits(
            values=[3, 1, 4, 1, 5, 9, 2, 6], row_splits=[0, 4, 4, 7, 8, 8])
        x = input_layer_lib.Input(tensor=rt)

        # Verify you can construct and use a model w/ this input
        model_container['model'] = functional.Functional(x, x * 3)
      return model_container['model'](inp)

    # And verify the model works
    rt = tf.RaggedTensor.from_row_splits(
        values=[3, 21, 4, 1, 53, 9, 2, 6], row_splits=[0, 4, 4, 7, 8, 8])
    self.assertAllEqual(run_model(rt), rt * 3)

</source>
<source file="systems/keras-2.7.0/keras/engine/input_layer_test.py" startline="311" endline="332" pcid="2887">
  def testCompositeTypeSpecArgInTFFunction(self):
    # We use a mutable model container instead of a model pysthon variable,
    # because python 2.7 does not have `nonlocal`
    model_container = {}

    @tf.function
    def run_model(inp):
      if not model_container:
        # Create a Keras Input
        rt = tf.RaggedTensor.from_row_splits(
            values=[3, 1, 4, 1, 5, 9, 2, 6], row_splits=[0, 4, 4, 7, 8, 8])
        x = input_layer_lib.Input(type_spec=rt._type_spec)

        # Verify you can construct and use a model w/ this input
        model_container['model'] = functional.Functional(x, x * 3)
      return model_container['model'](inp)

    # And verify the model works
    rt = tf.RaggedTensor.from_row_splits(
        values=[3, 21, 4, 1, 53, 9, 2, 6], row_splits=[0, 4, 4, 7, 8, 8])
    self.assertAllEqual(run_model(rt), rt * 3)

</source>
</class>

<class classid="118" nclones="2" nlines="20" similarity="71">
<source file="systems/keras-2.7.0/keras/engine/feature_columns_integration_test.py" startline="70" endline="93" pcid="2904">
  def test_sequential_model_with_ds_input(self):
    columns = [tf.feature_column.numeric_column('a')]
    model = keras.models.Sequential([
        df.DenseFeatures(columns),
        keras.layers.Dense(64, activation='relu'),
        keras.layers.Dense(20, activation='softmax')
    ])
    model.compile(
        optimizer='rmsprop',
        loss='categorical_crossentropy',
        metrics=['accuracy'],
        run_eagerly=testing_utils.should_run_eagerly())

    y = np.random.randint(20, size=(100, 1))
    y = np_utils.to_categorical(y, num_classes=20)
    x = {'a': np.random.random((100, 1))}
    ds1 = tf.data.Dataset.from_tensor_slices(x)
    ds2 = tf.data.Dataset.from_tensor_slices(y)
    ds = tf.data.Dataset.zip((ds1, ds2)).batch(5)
    model.fit(ds, steps_per_epoch=1)
    model.fit(ds, steps_per_epoch=1)
    model.evaluate(ds, steps=1)
    model.predict(ds, steps=1)

</source>
<source file="systems/keras-2.7.0/keras/engine/feature_columns_integration_test.py" startline="155" endline="178" pcid="2907">
  def test_subclassed_model_with_feature_columns_with_ds_input(self):
    col_a = tf.feature_column.numeric_column('a')
    col_b = tf.feature_column.numeric_column('b')

    dnn_model = TestDNNModel([col_a, col_b], 20)

    dnn_model.compile(
        optimizer='rmsprop',
        loss='categorical_crossentropy',
        metrics=['accuracy'],
        run_eagerly=testing_utils.should_run_eagerly())

    y = np.random.randint(20, size=(100, 1))
    y = np_utils.to_categorical(y, num_classes=20)
    x = {'a': np.random.random((100, 1)), 'b': np.random.random((100, 1))}
    ds1 = tf.data.Dataset.from_tensor_slices(x)
    ds2 = tf.data.Dataset.from_tensor_slices(y)
    ds = tf.data.Dataset.zip((ds1, ds2)).batch(5)
    dnn_model.fit(ds, steps_per_epoch=1)
    dnn_model.fit(ds, steps_per_epoch=1)
    dnn_model.evaluate(ds, steps=1)
    dnn_model.predict(ds, steps=1)

  # TODO(kaftan) seems to throw an error when enabled.
</source>
</class>

<class classid="119" nclones="4" nlines="15" similarity="86">
<source file="systems/keras-2.7.0/keras/feature_column/sequence_feature_column_test.py" startline="144" endline="164" pcid="2928">
  def test_embedding_column_with_non_sequence_categorical(self):
    """Tests that error is raised for non-sequence embedding column."""
    vocabulary_size = 3
    sparse_input = tf.compat.v1.SparseTensorValue(
        # example 0, ids [2]
        # example 1, ids [0, 1]
        indices=((0, 0), (1, 0), (1, 1)),
        values=(2, 0, 1),
        dense_shape=(2, 2))

    categorical_column_a = tf.feature_column.categorical_column_with_identity(
        key='aaa', num_buckets=vocabulary_size)
    embedding_column_a = tf.feature_column.embedding_column(
        categorical_column_a, dimension=2)
    sequence_input_layer = ksfc.SequenceFeatures([embedding_column_a])
    with self.assertRaisesRegex(
        ValueError,
        r'In embedding_column: aaa_embedding\. categorical_column must be of '
        r'type SequenceCategoricalColumn to use SequenceFeatures\.'):
      _, _ = sequence_input_layer({'aaa': sparse_input})

</source>
<source file="systems/keras-2.7.0/keras/feature_column/dense_features_test.py" startline="1121" endline="1143" pcid="3015">
  def test_indicator_column(self):
    """Tests that error is raised for sequence indicator column."""
    vocabulary_size = 3
    sparse_input = tf.compat.v1.SparseTensorValue(
        # example 0, ids [2]
        # example 1, ids [0, 1]
        indices=((0, 0), (1, 0), (1, 1)),
        values=(2, 0, 1),
        dense_shape=(2, 2))

    categorical_column_a = tf.feature_column.sequence_categorical_column_with_identity(
        key='aaa', num_buckets=vocabulary_size)
    indicator_column_a = tf.feature_column.indicator_column(
        categorical_column_a)

    input_layer = df.DenseFeatures([indicator_column_a])
    with self.assertRaisesRegex(
        ValueError,
        r'In indicator_column: aaa_indicator\. categorical_column must not be '
        r'of type SequenceCategoricalColumn\.'):
      _ = input_layer({'aaa': sparse_input})


</source>
<source file="systems/keras-2.7.0/keras/feature_column/sequence_feature_column_test.py" startline="332" endline="352" pcid="2934">
  def test_indicator_column_with_non_sequence_categorical(self):
    """Tests that error is raised for non-sequence categorical column."""
    vocabulary_size = 3
    sparse_input = tf.compat.v1.SparseTensorValue(
        # example 0, ids [2]
        # example 1, ids [0, 1]
        indices=((0, 0), (1, 0), (1, 1)),
        values=(2, 0, 1),
        dense_shape=(2, 2))

    categorical_column_a = tf.feature_column.categorical_column_with_identity(
        key='aaa', num_buckets=vocabulary_size)
    indicator_column_a = tf.feature_column.indicator_column(categorical_column_a)

    sequence_input_layer = ksfc.SequenceFeatures([indicator_column_a])
    with self.assertRaisesRegex(
        ValueError,
        r'In indicator_column: aaa_indicator\. categorical_column must be of '
        r'type SequenceCategoricalColumn to use SequenceFeatures\.'):
      _, _ = sequence_input_layer({'aaa': sparse_input})

</source>
<source file="systems/keras-2.7.0/keras/feature_column/dense_features_test.py" startline="1099" endline="1120" pcid="3014">
  def test_embedding_column(self):
    """Tests that error is raised for sequence embedding column."""
    vocabulary_size = 3
    sparse_input = tf.compat.v1.SparseTensorValue(
        # example 0, ids [2]
        # example 1, ids [0, 1]
        indices=((0, 0), (1, 0), (1, 1)),
        values=(2, 0, 1),
        dense_shape=(2, 2))

    categorical_column_a = tf.feature_column.sequence_categorical_column_with_identity(
        key='aaa', num_buckets=vocabulary_size)
    embedding_column_a = tf.feature_column.embedding_column(
        categorical_column_a, dimension=2)

    input_layer = df.DenseFeatures([embedding_column_a])
    with self.assertRaisesRegex(
        ValueError,
        r'In embedding_column: aaa_embedding\. categorical_column must not be '
        r'of type SequenceCategoricalColumn\.'):
      _ = input_layer({'aaa': sparse_input})

</source>
</class>

<class classid="120" nclones="3" nlines="12" similarity="75">
<source file="systems/keras-2.7.0/keras/feature_column/dense_features_v2.py" startline="61" endline="89" pcid="2945">
  def __init__(self,
               feature_columns,
               trainable=True,
               name=None,
               **kwargs):
    """Creates a DenseFeatures object.

    Args:
      feature_columns: An iterable containing the FeatureColumns to use as
        inputs to your model. All items should be instances of classes derived
        from `DenseColumn` such as `numeric_column`, `embedding_column`,
        `bucketized_column`, `indicator_column`. If you have categorical
        features, you can wrap them with an `embedding_column` or
        `indicator_column`.
      trainable:  Boolean, whether the layer's variables will be updated via
        gradient descent during training.
      name: Name to give to the DenseFeatures.
      **kwargs: Keyword arguments to construct a layer.

    Raises:
      ValueError: if an item in `feature_columns` is not a `DenseColumn`.
    """
    super(DenseFeatures, self).__init__(
        feature_columns=feature_columns,
        trainable=trainable,
        name=name,
        **kwargs)
    self._state_manager = _StateManagerImplV2(self, self.trainable)

</source>
<source file="systems/keras-2.7.0/keras/feature_column/dense_features.py" startline="69" endline="100" pcid="3058">
  def __init__(self,
               feature_columns,
               trainable=True,
               name=None,
               partitioner=None,
               **kwargs):
    """Constructs a DenseFeatures layer.

    Args:
      feature_columns: An iterable containing the FeatureColumns to use as
        inputs to your model. All items should be instances of classes derived
        from `DenseColumn` such as `numeric_column`, `embedding_column`,
        `bucketized_column`, `indicator_column`. If you have categorical
        features, you can wrap them with an `embedding_column` or
        `indicator_column`.
      trainable:  Boolean, whether the layer's variables will be updated via
        gradient descent during training.
      name: Name to give to the DenseFeatures.
      partitioner: Partitioner for input layer. Defaults to None.
      **kwargs: Keyword arguments to construct a layer.

    Raises:
      ValueError: if an item in `feature_columns` is not a `DenseColumn`.
    """
    super(DenseFeatures, self).__init__(
        feature_columns=feature_columns,
        trainable=trainable,
        name=name,
        partitioner=partitioner,
        expected_column_type=tf.__internal__.feature_column.DenseColumn,
        **kwargs)

</source>
<source file="systems/keras-2.7.0/keras/feature_column/sequence_feature_column.py" startline="81" endline="108" pcid="3053">
  def __init__(
      self,
      feature_columns,
      trainable=True,
      name=None,
      **kwargs):
    """"Constructs a SequenceFeatures layer.

    Args:
      feature_columns: An iterable of dense sequence columns. Valid columns are
        - `embedding_column` that wraps a `sequence_categorical_column_with_*`
        - `sequence_numeric_column`.
      trainable: Boolean, whether the layer's variables will be updated via
        gradient descent during training.
      name: Name to give to the SequenceFeatures.
      **kwargs: Keyword arguments to construct a layer.

    Raises:
      ValueError: If any of the `feature_columns` is not a
        `SequenceDenseColumn`.
    """
    super(SequenceFeatures, self).__init__(
        feature_columns=feature_columns,
        trainable=trainable,
        name=name,
        expected_column_type=tf.__internal__.feature_column.SequenceDenseColumn,
        **kwargs)

</source>
</class>

<class classid="121" nclones="4" nlines="30" similarity="74">
<source file="systems/keras-2.7.0/keras/feature_column/dense_features_test.py" startline="49" endline="90" pcid="2968">
  def test_reuses_variables(self):
    sparse_input = tf.SparseTensor(
        indices=((0, 0), (1, 0), (2, 0)), values=(0, 1, 2), dense_shape=(3, 3))

    # Create feature columns (categorical and embedding).
    categorical_column = tf.feature_column.categorical_column_with_identity(
        key='a', num_buckets=3)
    embedding_dimension = 2

    def _embedding_column_initializer(shape, dtype, partition_info=None):
      del shape  # unused
      del dtype  # unused
      del partition_info  # unused
      embedding_values = (
          (1, 0),  # id 0
          (0, 1),  # id 1
          (1, 1))  # id 2
      return embedding_values

    embedding_column = tf.feature_column.embedding_column(
        categorical_column,
        dimension=embedding_dimension,
        initializer=_embedding_column_initializer)

    dense_features = df.DenseFeatures([embedding_column])
    features = {'a': sparse_input}

    inputs = dense_features(features)
    variables = dense_features.variables

    # Sanity check: test that the inputs are correct.
    self.assertAllEqual([[1, 0], [0, 1], [1, 1]], inputs)

    # Check that only one variable was created.
    self.assertEqual(1, len(variables))

    # Check that invoking dense_features on the same features does not create
    # additional variables
    _ = dense_features(features)
    self.assertEqual(1, len(variables))
    self.assertIs(variables[0], dense_features.variables[0])

</source>
<source file="systems/keras-2.7.0/keras/feature_column/dense_features_test.py" startline="143" endline="185" pcid="2972">
  def test_feature_column_dense_features_gradient(self):
    sparse_input = tf.SparseTensor(
        indices=((0, 0), (1, 0), (2, 0)), values=(0, 1, 2), dense_shape=(3, 3))

    # Create feature columns (categorical and embedding).
    categorical_column = tf.feature_column.categorical_column_with_identity(
        key='a', num_buckets=3)
    embedding_dimension = 2

    def _embedding_column_initializer(shape, dtype, partition_info=None):
      del shape  # unused
      del dtype  # unused
      del partition_info  # unused
      embedding_values = (
          (1, 0),  # id 0
          (0, 1),  # id 1
          (1, 1))  # id 2
      return embedding_values

    embedding_column = tf.feature_column.embedding_column(
        categorical_column,
        dimension=embedding_dimension,
        initializer=_embedding_column_initializer)

    dense_features = df.DenseFeatures([embedding_column])
    features = {'a': sparse_input}

    def scale_matrix():
      matrix = dense_features(features)
      return 2 * matrix

    # Sanity check: Verify that scale_matrix returns the correct output.
    self.assertAllEqual([[2, 0], [0, 2], [2, 2]], scale_matrix())

    # Check that the returned gradient is correct.
    grad_function = backprop.implicit_grad(scale_matrix)
    grads_and_vars = grad_function()
    indexed_slice = grads_and_vars[0][0]
    gradient = grads_and_vars[0][0].values

    self.assertAllEqual([0, 1, 2], indexed_slice.indices)
    self.assertAllEqual([[2, 2], [2, 2], [2, 2]], gradient)

</source>
<source file="systems/keras-2.7.0/keras/feature_column/dense_features_v2_test.py" startline="47" endline="90" pcid="3018">
  def test_reuses_variables(self):
    sparse_input = tf.SparseTensor(
        indices=((0, 0), (1, 0), (2, 0)),
        values=(0, 1, 2),
        dense_shape=(3, 3))

    # Create feature columns (categorical and embedding).
    categorical_column = tf.feature_column.categorical_column_with_identity(
        key='a', num_buckets=3)
    embedding_dimension = 2

    def _embedding_column_initializer(shape, dtype, partition_info=None):
      del shape  # unused
      del dtype  # unused
      del partition_info  # unused
      embedding_values = (
          (1, 0),  # id 0
          (0, 1),  # id 1
          (1, 1))  # id 2
      return embedding_values

    embedding_column = tf.feature_column.embedding_column(
        categorical_column,
        dimension=embedding_dimension,
        initializer=_embedding_column_initializer)

    dense_features = df.DenseFeatures([embedding_column])
    features = {'a': sparse_input}

    inputs = dense_features(features)
    variables = dense_features.variables

    # Sanity check: test that the inputs are correct.
    self.assertAllEqual([[1, 0], [0, 1], [1, 1]], inputs)

    # Check that only one variable was created.
    self.assertEqual(1, len(variables))

    # Check that invoking dense_features on the same features does not create
    # additional variables
    _ = dense_features(features)
    self.assertEqual(1, len(variables))
    self.assertIs(variables[0], dense_features.variables[0])

</source>
<source file="systems/keras-2.7.0/keras/feature_column/dense_features_v2_test.py" startline="92" endline="136" pcid="3020">
  def test_feature_column_dense_features_gradient(self):
    sparse_input = tf.SparseTensor(
        indices=((0, 0), (1, 0), (2, 0)),
        values=(0, 1, 2),
        dense_shape=(3, 3))

    # Create feature columns (categorical and embedding).
    categorical_column = tf.feature_column.categorical_column_with_identity(
        key='a', num_buckets=3)
    embedding_dimension = 2

    def _embedding_column_initializer(shape, dtype, partition_info=None):
      del shape  # unused
      del dtype  # unused
      del partition_info  # unused
      embedding_values = (
          (1, 0),  # id 0
          (0, 1),  # id 1
          (1, 1))  # id 2
      return embedding_values

    embedding_column = tf.feature_column.embedding_column(
        categorical_column,
        dimension=embedding_dimension,
        initializer=_embedding_column_initializer)

    dense_features = df.DenseFeatures([embedding_column])
    features = {'a': sparse_input}

    def scale_matrix():
      matrix = dense_features(features)
      return 2 * matrix

    # Sanity check: Verify that scale_matrix returns the correct output.
    self.assertAllEqual([[2, 0], [0, 2], [2, 2]], scale_matrix())

    # Check that the returned gradient is correct.
    grad_function = backprop.implicit_grad(scale_matrix)
    grads_and_vars = grad_function()
    indexed_slice = grads_and_vars[0][0]
    gradient = grads_and_vars[0][0].values

    self.assertAllEqual([0, 1, 2], indexed_slice.indices)
    self.assertAllEqual([[2, 2], [2, 2], [2, 2]], gradient)

</source>
</class>

<class classid="122" nclones="2" nlines="14" similarity="100">
<source file="systems/keras-2.7.0/keras/feature_column/dense_features_test.py" startline="263" endline="280" pcid="2983">
  def test_compute_output_shape(self):
    price1 = tf.feature_column.numeric_column('price1', shape=2)
    price2 = tf.feature_column.numeric_column('price2', shape=4)
    with tf.Graph().as_default():
      features = {
          'price1': [[1., 2.], [5., 6.]],
          'price2': [[3., 4., 5., 6.], [7., 8., 9., 10.]]
      }
      dense_features = df.DenseFeatures([price1, price2])
      self.assertEqual((None, 6), dense_features.compute_output_shape((None,)))
      net = dense_features(features)

      self.evaluate(tf.compat.v1.global_variables_initializer())
      self.evaluate(tf.compat.v1.tables_initializer())

      self.assertAllClose([[1., 2., 3., 4., 5., 6.], [5., 6., 7., 8., 9., 10.]],
                          self.evaluate(net))

</source>
<source file="systems/keras-2.7.0/keras/feature_column/dense_features_v2_test.py" startline="249" endline="266" pcid="3034">
  def test_compute_output_shape(self):
    price1 = tf.feature_column.numeric_column('price1', shape=2)
    price2 = tf.feature_column.numeric_column('price2', shape=4)
    with tf.Graph().as_default():
      features = {
          'price1': [[1., 2.], [5., 6.]],
          'price2': [[3., 4., 5., 6.], [7., 8., 9., 10.]]
      }
      dense_features = df.DenseFeatures([price1, price2])
      self.assertEqual((None, 6), dense_features.compute_output_shape((None,)))
      net = dense_features(features)

      self.evaluate(tf.compat.v1.global_variables_initializer())
      self.evaluate(tf.compat.v1.tables_initializer())

      self.assertAllClose([[1., 2., 3., 4., 5., 6.], [5., 6., 7., 8., 9., 10.]],
                          self.evaluate(net))

</source>
</class>

<class classid="123" nclones="2" nlines="14" similarity="100">
<source file="systems/keras-2.7.0/keras/feature_column/dense_features_test.py" startline="313" endline="329" pcid="2987">
  def test_cols_to_output_tensors(self):
    price1 = tf.feature_column.numeric_column('price1', shape=2)
    price2 = tf.feature_column.numeric_column('price2')
    with tf.Graph().as_default():
      cols_dict = {}
      features = {'price1': [[1., 2.], [5., 6.]], 'price2': [[3.], [4.]]}
      dense_features = df.DenseFeatures([price1, price2])
      net = dense_features(features, cols_dict)

      self.evaluate(tf.compat.v1.global_variables_initializer())
      self.evaluate(tf.compat.v1.tables_initializer())

      self.assertAllClose([[1., 2.], [5., 6.]],
                          self.evaluate(cols_dict[price1]))
      self.assertAllClose([[3.], [4.]], self.evaluate(cols_dict[price2]))
      self.assertAllClose([[1., 2., 3.], [5., 6., 4.]], self.evaluate(net))

</source>
<source file="systems/keras-2.7.0/keras/feature_column/dense_features_v2_test.py" startline="299" endline="315" pcid="3038">
  def test_cols_to_output_tensors(self):
    price1 = tf.feature_column.numeric_column('price1', shape=2)
    price2 = tf.feature_column.numeric_column('price2')
    with tf.Graph().as_default():
      cols_dict = {}
      features = {'price1': [[1., 2.], [5., 6.]], 'price2': [[3.], [4.]]}
      dense_features = df.DenseFeatures([price1, price2])
      net = dense_features(features, cols_dict)

      self.evaluate(tf.compat.v1.global_variables_initializer())
      self.evaluate(tf.compat.v1.tables_initializer())

      self.assertAllClose([[1., 2.], [5., 6.]],
                          self.evaluate(cols_dict[price1]))
      self.assertAllClose([[3.], [4.]], self.evaluate(cols_dict[price2]))
      self.assertAllClose([[1., 2., 3.], [5., 6., 4.]], self.evaluate(net))

</source>
</class>

<class classid="124" nclones="2" nlines="14" similarity="100">
<source file="systems/keras-2.7.0/keras/feature_column/dense_features_test.py" startline="330" endline="346" pcid="2988">
  def test_column_order(self):
    price_a = tf.feature_column.numeric_column('price_a')
    price_b = tf.feature_column.numeric_column('price_b')
    with tf.Graph().as_default():
      features = {
          'price_a': [[1.]],
          'price_b': [[3.]],
      }
      net1 = df.DenseFeatures([price_a, price_b])(features)
      net2 = df.DenseFeatures([price_b, price_a])(features)

      self.evaluate(tf.compat.v1.global_variables_initializer())
      self.evaluate(tf.compat.v1.tables_initializer())

      self.assertAllClose([[1., 3.]], self.evaluate(net1))
      self.assertAllClose([[1., 3.]], self.evaluate(net2))

</source>
<source file="systems/keras-2.7.0/keras/feature_column/dense_features_v2_test.py" startline="316" endline="332" pcid="3039">
  def test_column_order(self):
    price_a = tf.feature_column.numeric_column('price_a')
    price_b = tf.feature_column.numeric_column('price_b')
    with tf.Graph().as_default():
      features = {
          'price_a': [[1.]],
          'price_b': [[3.]],
      }
      net1 = df.DenseFeatures([price_a, price_b])(features)
      net2 = df.DenseFeatures([price_b, price_a])(features)

      self.evaluate(tf.compat.v1.global_variables_initializer())
      self.evaluate(tf.compat.v1.tables_initializer())

      self.assertAllClose([[1., 3.]], self.evaluate(net1))
      self.assertAllClose([[1., 3.]], self.evaluate(net2))

</source>
</class>

<class classid="125" nclones="2" nlines="11" similarity="100">
<source file="systems/keras-2.7.0/keras/feature_column/dense_features_test.py" startline="359" endline="371" pcid="2990">
  def test_static_batch_size_mismatch(self):
    price1 = tf.feature_column.numeric_column('price1')
    price2 = tf.feature_column.numeric_column('price2')
    with tf.Graph().as_default():
      features = {
          'price1': [[1.], [5.], [7.]],  # batchsize = 3
          'price2': [[3.], [4.]]  # batchsize = 2
      }
      with self.assertRaisesRegex(
          ValueError,
          r'Batch size \(first dimension\) of each feature must be same.'):  # pylint: disable=anomalous-backslash-in-string
        df.DenseFeatures([price1, price2])(features)

</source>
<source file="systems/keras-2.7.0/keras/feature_column/dense_features_v2_test.py" startline="344" endline="356" pcid="3041">
  def test_static_batch_size_mismatch(self):
    price1 = tf.feature_column.numeric_column('price1')
    price2 = tf.feature_column.numeric_column('price2')
    with tf.Graph().as_default():
      features = {
          'price1': [[1.], [5.], [7.]],  # batchsize = 3
          'price2': [[3.], [4.]]  # batchsize = 2
      }
      with self.assertRaisesRegex(
          ValueError,
          r'Batch size \(first dimension\) of each feature must be same.'):  # pylint: disable=anomalous-backslash-in-string
        df.DenseFeatures([price1, price2])(features)

</source>
</class>

<class classid="126" nclones="2" nlines="13" similarity="100">
<source file="systems/keras-2.7.0/keras/feature_column/dense_features_test.py" startline="372" endline="386" pcid="2991">
  def test_subset_of_static_batch_size_mismatch(self):
    price1 = tf.feature_column.numeric_column('price1')
    price2 = tf.feature_column.numeric_column('price2')
    price3 = tf.feature_column.numeric_column('price3')
    with tf.Graph().as_default():
      features = {
          'price1': tf.compat.v1.placeholder(dtype=tf.int64),  # batchsize = 3
          'price2': [[3.], [4.]],  # batchsize = 2
          'price3': [[3.], [4.], [5.]]  # batchsize = 3
      }
      with self.assertRaisesRegex(
          ValueError,
          r'Batch size \(first dimension\) of each feature must be same.'):  # pylint: disable=anomalous-backslash-in-string
        df.DenseFeatures([price1, price2, price3])(features)

</source>
<source file="systems/keras-2.7.0/keras/feature_column/dense_features_v2_test.py" startline="357" endline="371" pcid="3042">
  def test_subset_of_static_batch_size_mismatch(self):
    price1 = tf.feature_column.numeric_column('price1')
    price2 = tf.feature_column.numeric_column('price2')
    price3 = tf.feature_column.numeric_column('price3')
    with tf.Graph().as_default():
      features = {
          'price1': tf.compat.v1.placeholder(dtype=tf.int64),  # batchsize = 3
          'price2': [[3.], [4.]],  # batchsize = 2
          'price3': [[3.], [4.], [5.]]  # batchsize = 3
      }
      with self.assertRaisesRegex(
          ValueError,
          r'Batch size \(first dimension\) of each feature must be same.'):  # pylint: disable=anomalous-backslash-in-string
        df.DenseFeatures([price1, price2, price3])(features)

</source>
</class>

<class classid="127" nclones="2" nlines="12" similarity="100">
<source file="systems/keras-2.7.0/keras/feature_column/dense_features_test.py" startline="387" endline="400" pcid="2992">
  def test_runtime_batch_size_mismatch(self):
    price1 = tf.feature_column.numeric_column('price1')
    price2 = tf.feature_column.numeric_column('price2')
    with tf.Graph().as_default():
      features = {
          'price1': tf.compat.v1.placeholder(dtype=tf.int64),  # batchsize = 3
          'price2': [[3.], [4.]]  # batchsize = 2
      }
      net = df.DenseFeatures([price1, price2])(features)
      with _initialized_session() as sess:
        with self.assertRaisesRegex(tf.errors.OpError,
                                    'Dimensions of inputs should match'):
          sess.run(net, feed_dict={features['price1']: [[1.], [5.], [7.]]})

</source>
<source file="systems/keras-2.7.0/keras/feature_column/dense_features_v2_test.py" startline="372" endline="385" pcid="3043">
  def test_runtime_batch_size_mismatch(self):
    price1 = tf.feature_column.numeric_column('price1')
    price2 = tf.feature_column.numeric_column('price2')
    with tf.Graph().as_default():
      features = {
          'price1': tf.compat.v1.placeholder(dtype=tf.int64),  # batchsize = 3
          'price2': [[3.], [4.]]  # batchsize = 2
      }
      net = df.DenseFeatures([price1, price2])(features)
      with _initialized_session() as sess:
        with self.assertRaisesRegex(tf.errors.OpError,
                                    'Dimensions of inputs should match'):
          sess.run(net, feed_dict={features['price1']: [[1.], [5.], [7.]]})

</source>
</class>

<class classid="128" nclones="2" nlines="16" similarity="100">
<source file="systems/keras-2.7.0/keras/feature_column/dense_features_test.py" startline="401" endline="417" pcid="2993">
  def test_runtime_batch_size_matches(self):
    price1 = tf.feature_column.numeric_column('price1')
    price2 = tf.feature_column.numeric_column('price2')
    with tf.Graph().as_default():
      features = {
          'price1': tf.compat.v1.placeholder(dtype=tf.int64),  # batchsize = 2
          'price2': tf.compat.v1.placeholder(dtype=tf.int64),  # batchsize = 2
      }
      net = df.DenseFeatures([price1, price2])(features)
      with _initialized_session() as sess:
        sess.run(
            net,
            feed_dict={
                features['price1']: [[1.], [5.]],
                features['price2']: [[1.], [5.]],
            })

</source>
<source file="systems/keras-2.7.0/keras/feature_column/dense_features_v2_test.py" startline="386" endline="402" pcid="3044">
  def test_runtime_batch_size_matches(self):
    price1 = tf.feature_column.numeric_column('price1')
    price2 = tf.feature_column.numeric_column('price2')
    with tf.Graph().as_default():
      features = {
          'price1': tf.compat.v1.placeholder(dtype=tf.int64),  # batchsize = 2
          'price2': tf.compat.v1.placeholder(dtype=tf.int64),  # batchsize = 2
      }
      net = df.DenseFeatures([price1, price2])(features)
      with _initialized_session() as sess:
        sess.run(
            net,
            feed_dict={
                features['price1']: [[1.], [5.]],
                features['price2']: [[1.], [5.]],
            })

</source>
</class>

<class classid="129" nclones="2" nlines="32" similarity="72">
<source file="systems/keras-2.7.0/keras/feature_column/dense_features_test.py" startline="447" endline="483" pcid="2995">
  def test_multiple_layers_with_same_shared_embedding_column(self):
    categorical_column_a = tf.feature_column.categorical_column_with_identity(
        key='aaa', num_buckets=3)
    categorical_column_b = tf.feature_column.categorical_column_with_identity(
        key='bbb', num_buckets=3)
    embedding_dimension = 2
    embedding_column_b, embedding_column_a = tf.feature_column.shared_embeddings(
        [categorical_column_b, categorical_column_a],
        dimension=embedding_dimension)

    with tf.Graph().as_default():
      features = {
          'aaa':
              tf.SparseTensor(
                  indices=((0, 0), (1, 0), (1, 1)),
                  values=(0, 1, 0),
                  dense_shape=(2, 2)),
          'bbb':
              tf.SparseTensor(
                  indices=((0, 0), (1, 0), (1, 1)),
                  values=(1, 2, 1),
                  dense_shape=(2, 2)),
      }
      all_cols = [embedding_column_a, embedding_column_b]
      df.DenseFeatures(all_cols)(features)
      df.DenseFeatures(all_cols)(features)
      # Make sure that only 1 variable gets created in this case.
      self.assertEqual(
          1,
          len(
              tf.compat.v1.get_collection(
                  tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)))
      self.assertCountEqual(['aaa_bbb_shared_embedding:0'], [
          v.name for v in tf.compat.v1.get_collection(
              tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)
      ])

</source>
<source file="systems/keras-2.7.0/keras/feature_column/dense_features_v2_test.py" startline="427" endline="460" pcid="3046">
  def test_multiple_layers_with_same_shared_embedding_column(self):
    categorical_column_a = tf.feature_column.categorical_column_with_identity(
        key='aaa', num_buckets=3)
    categorical_column_b = tf.feature_column.categorical_column_with_identity(
        key='bbb', num_buckets=3)
    embedding_dimension = 2

    # feature_column.shared_embeddings is not supported in eager.
    with tf.Graph().as_default():
      embedding_column_b, embedding_column_a = tf.feature_column.shared_embeddings(
          [categorical_column_b, categorical_column_a],
          dimension=embedding_dimension)
      features = {
          'aaa':
              tf.SparseTensor(
                  indices=((0, 0), (1, 0), (1, 1)),
                  values=(0, 1, 0),
                  dense_shape=(2, 2)),
          'bbb':
              tf.SparseTensor(
                  indices=((0, 0), (1, 0), (1, 1)),
                  values=(1, 2, 1),
                  dense_shape=(2, 2)),
      }
      all_cols = [embedding_column_a, embedding_column_b]
      df.DenseFeatures(all_cols)(features)
      df.DenseFeatures(all_cols)(features)
      # Make sure that only 1 variable gets created in this case.
      self.assertEqual(1,
                       len(tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)))
      self.assertItemsEqual(
          ['aaa_bbb_shared_embedding:0'],
          [v.name for v in tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)])

</source>
</class>

<class classid="130" nclones="2" nlines="49" similarity="72">
<source file="systems/keras-2.7.0/keras/feature_column/dense_features_test.py" startline="485" endline="542" pcid="2996">
  def test_multiple_layers_with_same_shared_embedding_column_diff_graphs(self):
    categorical_column_a = tf.feature_column.categorical_column_with_identity(
        key='aaa', num_buckets=3)
    categorical_column_b = tf.feature_column.categorical_column_with_identity(
        key='bbb', num_buckets=3)
    embedding_dimension = 2
    embedding_column_b, embedding_column_a = tf.feature_column.shared_embeddings(
        [categorical_column_b, categorical_column_a],
        dimension=embedding_dimension)
    all_cols = [embedding_column_a, embedding_column_b]

    with tf.Graph().as_default():
      features = {
          'aaa':
              tf.SparseTensor(
                  indices=((0, 0), (1, 0), (1, 1)),
                  values=(0, 1, 0),
                  dense_shape=(2, 2)),
          'bbb':
              tf.SparseTensor(
                  indices=((0, 0), (1, 0), (1, 1)),
                  values=(1, 2, 1),
                  dense_shape=(2, 2)),
      }
      df.DenseFeatures(all_cols)(features)
      # Make sure that only 1 variable gets created in this case.
      self.assertEqual(
          1,
          len(
              tf.compat.v1.get_collection(
                  tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)))

    with tf.Graph().as_default():
      features1 = {
          'aaa':
              tf.SparseTensor(
                  indices=((0, 0), (1, 0), (1, 1)),
                  values=(0, 1, 0),
                  dense_shape=(2, 2)),
          'bbb':
              tf.SparseTensor(
                  indices=((0, 0), (1, 0), (1, 1)),
                  values=(1, 2, 1),
                  dense_shape=(2, 2)),
      }

      df.DenseFeatures(all_cols)(features1)
      # Make sure that only 1 variable gets created in this case.
      self.assertEqual(
          1,
          len(
              tf.compat.v1.get_collection(
                  tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)))
      self.assertCountEqual(['aaa_bbb_shared_embedding:0'], [
          v.name for v in tf.compat.v1.get_collection(
              tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)
      ])

</source>
<source file="systems/keras-2.7.0/keras/feature_column/dense_features_v2_test.py" startline="461" endline="512" pcid="3047">
  def test_multiple_layers_with_same_shared_embedding_column_diff_graphs(self):
    categorical_column_a = tf.feature_column.categorical_column_with_identity(
        key='aaa', num_buckets=3)
    categorical_column_b = tf.feature_column.categorical_column_with_identity(
        key='bbb', num_buckets=3)
    embedding_dimension = 2

    # feature_column.shared_embeddings is not supported in eager.
    with tf.Graph().as_default():
      embedding_column_b, embedding_column_a = tf.feature_column.shared_embeddings(
          [categorical_column_b, categorical_column_a],
          dimension=embedding_dimension)
      all_cols = [embedding_column_a, embedding_column_b]
      features = {
          'aaa':
              tf.SparseTensor(
                  indices=((0, 0), (1, 0), (1, 1)),
                  values=(0, 1, 0),
                  dense_shape=(2, 2)),
          'bbb':
              tf.SparseTensor(
                  indices=((0, 0), (1, 0), (1, 1)),
                  values=(1, 2, 1),
                  dense_shape=(2, 2)),
      }
      df.DenseFeatures(all_cols)(features)
      # Make sure that only 1 variable gets created in this case.
      self.assertEqual(1,
                       len(tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)))

    with tf.Graph().as_default():
      features1 = {
          'aaa':
              tf.SparseTensor(
                  indices=((0, 0), (1, 0), (1, 1)),
                  values=(0, 1, 0),
                  dense_shape=(2, 2)),
          'bbb':
              tf.SparseTensor(
                  indices=((0, 0), (1, 0), (1, 1)),
                  values=(1, 2, 1),
                  dense_shape=(2, 2)),
      }

      df.DenseFeatures(all_cols)(features1)
      # Make sure that only 1 variable gets created in this case.
      self.assertEqual(1,
                       len(tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)))
      self.assertItemsEqual(
          ['aaa_bbb_shared_embedding:0'],
          [v.name for v in tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)])

</source>
</class>

<class classid="131" nclones="2" nlines="41" similarity="97">
<source file="systems/keras-2.7.0/keras/feature_column/dense_features_test.py" startline="544" endline="599" pcid="2997">
  def test_with_1d_sparse_tensor(self):
    embedding_values = (
        (1., 2., 3., 4., 5.),  # id 0
        (6., 7., 8., 9., 10.),  # id 1
        (11., 12., 13., 14., 15.)  # id 2
    )

    def _initializer(shape, dtype, partition_info=None):
      del shape, dtype, partition_info
      return embedding_values

    # price has 1 dimension in dense_features
    price = tf.feature_column.numeric_column('price')

    # one_hot_body_style has 3 dims in dense_features.
    body_style = tf.feature_column.categorical_column_with_vocabulary_list(
        'body-style', vocabulary_list=['hardtop', 'wagon', 'sedan'])
    one_hot_body_style = tf.feature_column.indicator_column(body_style)

    # embedded_body_style has 5 dims in dense_features.
    country = tf.feature_column.categorical_column_with_vocabulary_list(
        'country', vocabulary_list=['US', 'JP', 'CA'])
    embedded_country = tf.feature_column.embedding_column(
        country, dimension=5, initializer=_initializer)

    # Provides 1-dim tensor and dense tensor.
    features = {
        'price':
            tf.constant([
                11.,
                12.,
            ]),
        'body-style':
            tf.SparseTensor(
                indices=((0,), (1,)),
                values=('sedan', 'hardtop'),
                dense_shape=(2,)),
        # This is dense tensor for the categorical_column.
        'country':
            tf.constant(['CA', 'US']),
    }
    self.assertEqual(1, features['price'].shape.ndims)
    self.assertEqual(1, features['body-style'].dense_shape.get_shape()[0])
    self.assertEqual(1, features['country'].shape.ndims)

    net = df.DenseFeatures([price, one_hot_body_style, embedded_country])(
        features)
    self.assertEqual(1 + 3 + 5, net.shape[1])
    with _initialized_session() as sess:

      # Each row is formed by concatenating `embedded_body_style`,
      # `one_hot_body_style`, and `price` in order.
      self.assertAllEqual([[0., 0., 1., 11., 12., 13., 14., 15., 11.],
                           [1., 0., 0., 1., 2., 3., 4., 5., 12.]],
                          sess.run(net))

</source>
<source file="systems/keras-2.7.0/keras/feature_column/dense_features_v2_test.py" startline="513" endline="569" pcid="3048">
  def test_with_1d_sparse_tensor(self):
    embedding_values = (
        (1., 2., 3., 4., 5.),  # id 0
        (6., 7., 8., 9., 10.),  # id 1
        (11., 12., 13., 14., 15.)  # id 2
    )

    def _initializer(shape, dtype, partition_info=None):
      del shape, dtype, partition_info
      return embedding_values

    # price has 1 dimension in dense_features
    price = tf.feature_column.numeric_column('price')

    # one_hot_body_style has 3 dims in dense_features.
    body_style = tf.feature_column.categorical_column_with_vocabulary_list(
        'body-style', vocabulary_list=['hardtop', 'wagon', 'sedan'])
    one_hot_body_style = tf.feature_column.indicator_column(body_style)

    # embedded_body_style has 5 dims in dense_features.
    country = tf.feature_column.categorical_column_with_vocabulary_list(
        'country', vocabulary_list=['US', 'JP', 'CA'])
    embedded_country = tf.feature_column.embedding_column(
        country, dimension=5, initializer=_initializer)

    with tf.Graph().as_default():
      # Provides 1-dim tensor and dense tensor.
      features = {
          'price':
              tf.constant([
                  11.,
                  12.,
              ]),
          'body-style':
              tf.SparseTensor(
                  indices=((0,), (1,)),
                  values=('sedan', 'hardtop'),
                  dense_shape=(2,)),
          # This is dense tensor for the categorical_column.
          'country':
              tf.constant(['CA', 'US']),
      }
      self.assertEqual(1, features['price'].shape.ndims)
      self.assertEqual(1, features['body-style'].dense_shape.get_shape()[0])
      self.assertEqual(1, features['country'].shape.ndims)

      net = df.DenseFeatures([price, one_hot_body_style, embedded_country])(
          features)
      self.assertEqual(1 + 3 + 5, net.shape[1])
      with _initialized_session() as sess:

        # Each row is formed by concatenating `embedded_body_style`,
        # `one_hot_body_style`, and `price` in order.
        self.assertAllEqual([[0., 0., 1., 11., 12., 13., 14., 15., 11.],
                             [1., 0., 0., 1., 2., 3., 4., 5., 12.]],
                            sess.run(net))

</source>
</class>

<class classid="132" nclones="2" nlines="41" similarity="97">
<source file="systems/keras-2.7.0/keras/feature_column/dense_features_test.py" startline="601" endline="658" pcid="2999">
  def test_with_1d_unknown_shape_sparse_tensor(self):
    embedding_values = (
        (1., 2.),  # id 0
        (6., 7.),  # id 1
        (11., 12.)  # id 2
    )

    def _initializer(shape, dtype, partition_info=None):
      del shape, dtype, partition_info
      return embedding_values

    # price has 1 dimension in dense_features
    price = tf.feature_column.numeric_column('price')

    # one_hot_body_style has 3 dims in dense_features.
    body_style = tf.feature_column.categorical_column_with_vocabulary_list(
        'body-style', vocabulary_list=['hardtop', 'wagon', 'sedan'])
    one_hot_body_style = tf.feature_column.indicator_column(body_style)

    # embedded_body_style has 5 dims in dense_features.
    country = tf.feature_column.categorical_column_with_vocabulary_list(
        'country', vocabulary_list=['US', 'JP', 'CA'])
    embedded_country = tf.feature_column.embedding_column(
        country, dimension=2, initializer=_initializer)

    # Provides 1-dim tensor and dense tensor.
    features = {
        'price': tf.compat.v1.placeholder(tf.float32),
        'body-style': tf.compat.v1.sparse_placeholder(tf.string),
        # This is dense tensor for the categorical_column.
        'country': tf.compat.v1.placeholder(tf.string),
    }
    self.assertIsNone(features['price'].shape.ndims)
    self.assertIsNone(features['body-style'].get_shape().ndims)
    self.assertIsNone(features['country'].shape.ndims)

    price_data = np.array([11., 12.])
    body_style_data = tf.compat.v1.SparseTensorValue(
        indices=((0,), (1,)), values=('sedan', 'hardtop'), dense_shape=(2,))
    country_data = np.array([['US'], ['CA']])

    net = df.DenseFeatures([price, one_hot_body_style, embedded_country])(
        features)
    self.assertEqual(1 + 3 + 2, net.shape[1])
    with _initialized_session() as sess:

      # Each row is formed by concatenating `embedded_body_style`,
      # `one_hot_body_style`, and `price` in order.
      self.assertAllEqual(
          [[0., 0., 1., 1., 2., 11.], [1., 0., 0., 11., 12., 12.]],
          sess.run(
              net,
              feed_dict={
                  features['price']: price_data,
                  features['body-style']: body_style_data,
                  features['country']: country_data
              }))

</source>
<source file="systems/keras-2.7.0/keras/feature_column/dense_features_v2_test.py" startline="570" endline="628" pcid="3050">
  def test_with_1d_unknown_shape_sparse_tensor(self):
    embedding_values = (
        (1., 2.),  # id 0
        (6., 7.),  # id 1
        (11., 12.)  # id 2
    )

    def _initializer(shape, dtype, partition_info=None):
      del shape, dtype, partition_info
      return embedding_values

    # price has 1 dimension in dense_features
    price = tf.feature_column.numeric_column('price')

    # one_hot_body_style has 3 dims in dense_features.
    body_style = tf.feature_column.categorical_column_with_vocabulary_list(
        'body-style', vocabulary_list=['hardtop', 'wagon', 'sedan'])
    one_hot_body_style = tf.feature_column.indicator_column(body_style)

    # embedded_body_style has 5 dims in dense_features.
    country = tf.feature_column.categorical_column_with_vocabulary_list(
        'country', vocabulary_list=['US', 'JP', 'CA'])
    embedded_country = tf.feature_column.embedding_column(
        country, dimension=2, initializer=_initializer)

    # Provides 1-dim tensor and dense tensor.
    with tf.Graph().as_default():
      features = {
          'price': tf.compat.v1.placeholder(tf.float32),
          'body-style': tf.compat.v1.sparse_placeholder(tf.string),
          # This is dense tensor for the categorical_column.
          'country': tf.compat.v1.placeholder(tf.string),
      }
      self.assertIsNone(features['price'].shape.ndims)
      self.assertIsNone(features['body-style'].get_shape().ndims)
      self.assertIsNone(features['country'].shape.ndims)

      price_data = np.array([11., 12.])
      body_style_data = tf.compat.v1.SparseTensorValue(
          indices=((0,), (1,)), values=('sedan', 'hardtop'), dense_shape=(2,))
      country_data = np.array([['US'], ['CA']])

      net = df.DenseFeatures([price, one_hot_body_style, embedded_country])(
          features)
      self.assertEqual(1 + 3 + 2, net.shape[1])
      with _initialized_session() as sess:

        # Each row is formed by concatenating `embedded_body_style`,
        # `one_hot_body_style`, and `price` in order.
        self.assertAllEqual(
            [[0., 0., 1., 1., 2., 11.], [1., 0., 0., 11., 12., 12.]],
            sess.run(
                net,
                feed_dict={
                    features['price']: price_data,
                    features['body-style']: body_style_data,
                    features['country']: country_data
                }))

</source>
</class>

<class classid="133" nclones="2" nlines="17" similarity="93">
<source file="systems/keras-2.7.0/keras/feature_column/dense_features_test.py" startline="660" endline="682" pcid="3001">
  def test_with_rank_0_feature(self):
    # price has 1 dimension in dense_features
    price = tf.feature_column.numeric_column('price')
    features = {
        'price': tf.constant(0),
    }
    self.assertEqual(0, features['price'].shape.ndims)

    # Static rank 0 should fail
    with self.assertRaisesRegex(ValueError, 'Feature .* cannot have rank 0'):
      df.DenseFeatures([price])(features)

    # Dynamic rank 0 should fail
    features = {
        'price': tf.compat.v1.placeholder(tf.float32),
    }
    net = df.DenseFeatures([price])(features)
    self.assertEqual(1, net.shape[1])
    with _initialized_session() as sess:
      with self.assertRaisesOpError('Feature .* cannot have rank 0'):
        sess.run(net, feed_dict={features['price']: np.array(1)})


</source>
<source file="systems/keras-2.7.0/keras/feature_column/dense_features_v2_test.py" startline="629" endline="652" pcid="3052">
  def test_with_rank_0_feature(self):
    # price has 1 dimension in dense_features
    price = tf.feature_column.numeric_column('price')
    features = {
        'price': tf.constant(0),
    }
    self.assertEqual(0, features['price'].shape.ndims)

    # Static rank 0 should fail
    with self.assertRaisesRegex(ValueError, 'Feature .* cannot have rank 0'):
      df.DenseFeatures([price])(features)

    with tf.Graph().as_default():
      # Dynamic rank 0 should fail
      features = {
          'price': tf.compat.v1.placeholder(tf.float32),
      }
      net = df.DenseFeatures([price])(features)
      self.assertEqual(1, net.shape[1])
      with _initialized_session() as sess:
        with self.assertRaisesOpError('Feature .* cannot have rank 0'):
          sess.run(net, feed_dict={features['price']: np.array(1)})


</source>
</class>

<class classid="134" nclones="11" nlines="30" similarity="70">
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/normalization_test.py" startline="307" endline="349" pcid="3081">
  def test3DInputAxis1(self):
    epsilon = 1e-3
    bn = normalization_layers.BatchNormalization(
        axis=1, epsilon=epsilon, momentum=0.9)
    inputs = tf.Variable(
        np.random.random((5, 4, 3)) + 100, dtype=tf.float32)
    training = tf.compat.v1.placeholder(dtype='bool')
    outputs = bn.apply(inputs, training=training)

    with self.cached_session() as sess:
      # Test training with placeholder learning phase.
      self.evaluate(tf.compat.v1.global_variables_initializer())

      np_gamma, np_beta = self.evaluate([bn.gamma, bn.beta])
      np_gamma = np.reshape(np_gamma, (1, 4, 1))
      np_beta = np.reshape(np_beta, (1, 4, 1))

      for _ in range(100):
        np_output, _, _ = sess.run([outputs] + bn.updates,
                                   feed_dict={training: True})
        # Verify that the axis is normalized during training.
        normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
        self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
        self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

      # Verify that the statistics are updated during training.
      moving_mean, moving_var = self.evaluate(
          [bn.moving_mean, bn.moving_variance])
      np_inputs = self.evaluate(inputs)
      mean = np.mean(np_inputs, axis=(0, 2))
      std = np.std(np_inputs, axis=(0, 2))
      variance = np.square(std)
      self.assertAllClose(mean, moving_mean, atol=1e-2)
      self.assertAllClose(variance, moving_var, atol=1e-2)

      # Test inference with placeholder learning phase.
      np_output = sess.run(outputs, feed_dict={training: False})

      # Verify that the axis is normalized during inference.
      normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
      self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
      self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

</source>
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/normalization_test.py" startline="1301" endline="1341" pcid="3109">
  def test3DInputMultiAxis12(self):
    epsilon = 1e-3
    bn = normalization_layers.BatchNormalization(
        axis=[1, 2], epsilon=epsilon, momentum=0.9)
    inputs = tf.Variable(
        np.random.random((5, 4, 3)) + 100, dtype=tf.float32)
    training = tf.compat.v1.placeholder(dtype='bool')
    outputs = bn.apply(inputs, training=training)

    with self.cached_session() as sess:
      # Test training with placeholder learning phase.
      self.evaluate(tf.compat.v1.global_variables_initializer())

      np_gamma, np_beta = self.evaluate([bn.gamma, bn.beta])

      for _ in range(100):
        np_output, _, _ = sess.run([outputs] + bn.updates,
                                   feed_dict={training: True})
        # Verify that the axis is normalized during training.
        normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
        self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
        self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

      # Verify that the statistics are updated during training.
      moving_mean, moving_var = self.evaluate(
          [bn.moving_mean, bn.moving_variance])
      np_inputs = self.evaluate(inputs)
      mean = np.mean(np_inputs, axis=0, keepdims=True)
      std = np.std(np_inputs, axis=0, keepdims=True)
      variance = np.square(std)
      self.assertAllClose(mean, moving_mean, atol=1e-2)
      self.assertAllClose(variance, moving_var, atol=1e-2)

      # Test inference with placeholder learning phase.
      np_output = sess.run(outputs, feed_dict={training: False})

      # Verify that the axis is normalized during inference.
      normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
      self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
      self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

</source>
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/normalization_test.py" startline="391" endline="432" pcid="3083">
  def test4DInputAxis1(self):
    if tf.test.is_gpu_available(cuda_only=True):
      epsilon = 1e-3
      bn = normalization_layers.BatchNormalization(
          axis=1, epsilon=epsilon, momentum=0.9)
      inputs = tf.Variable(
          np.random.random((5, 4, 3, 6)) + 100, dtype=tf.float32)
      training = tf.compat.v1.placeholder(dtype='bool')
      outputs = bn.apply(inputs, training=training)

      with self.session() as sess:
        # Test training with placeholder learning phase.
        self.evaluate(tf.compat.v1.global_variables_initializer())
        np_gamma, np_beta = self.evaluate([bn.gamma, bn.beta])
        np_gamma = np.reshape(np_gamma, (1, 4, 1, 1))
        np_beta = np.reshape(np_beta, (1, 4, 1, 1))
        for _ in range(100):
          np_output, _, _ = sess.run(
              [outputs] + bn.updates, feed_dict={training: True})
          # Verify that the axis is normalized during training.
          normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
          self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
          self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

        # Verify that the statistics are updated during training.
        moving_mean, moving_var = self.evaluate(
            [bn.moving_mean, bn.moving_variance])
        np_inputs = self.evaluate(inputs)
        mean = np.mean(np_inputs, axis=(0, 2, 3))
        std = np.std(np_inputs, axis=(0, 2, 3))
        variance = np.square(std)
        self.assertAllClose(mean, moving_mean, atol=1e-2)
        self.assertAllClose(variance, moving_var, atol=1e-2)

        # Test inference with placeholder learning phase.
        np_output = sess.run(outputs, feed_dict={training: False})

        # Verify that the axis is normalized during inference.
        normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
        self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
        self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

</source>
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/normalization_test.py" startline="515" endline="555" pcid="3086">
  def test4DInputAxis3Fused(self):
    epsilon = 1e-3
    bn = normalization_layers.BatchNormalization(
        axis=3, epsilon=epsilon, momentum=0.9, fused=True)
    inputs = tf.Variable(
        np.random.random((5, 4, 3, 6)) + 100, dtype=tf.float32)
    training = tf.compat.v1.placeholder(dtype='bool')
    outputs = bn.apply(inputs, training=training)

    with self.cached_session() as sess:
      # Test training with placeholder learning phase.
      self.evaluate(tf.compat.v1.global_variables_initializer())
      np_gamma, np_beta = self.evaluate([bn.gamma, bn.beta])
      np_gamma = np.reshape(np_gamma, (1, 1, 1, 6))
      np_beta = np.reshape(np_beta, (1, 1, 1, 6))
      for _ in range(100):
        np_output, _, _ = sess.run(
            [outputs] + bn.updates, feed_dict={training: True})
        # Verify that the axis is normalized during training.
        normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
        self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
        self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

      # Verify that the statistics are updated during training.
      moving_mean, moving_var = self.evaluate(
          [bn.moving_mean, bn.moving_variance])
      np_inputs = self.evaluate(inputs)
      mean = np.mean(np_inputs, axis=(0, 1, 2))
      std = np.std(np_inputs, axis=(0, 1, 2))
      variance = np.square(std)
      self.assertAllClose(mean, moving_mean, atol=1e-2)
      self.assertAllClose(variance, moving_var, atol=1e-2)

      # Test inference with placeholder learning phase.
      np_output = sess.run(outputs, feed_dict={training: False})

      # Verify that the axis is normalized during inference.
      normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
      self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
      self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

</source>
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/normalization_test.py" startline="474" endline="514" pcid="3085">
  def test4DInputAxis3(self):
    epsilon = 1e-3
    bn = normalization_layers.BatchNormalization(
        axis=3, epsilon=epsilon, momentum=0.9)
    inputs = tf.Variable(
        np.random.random((5, 4, 3, 6)) + 100, dtype=tf.float32)
    training = tf.compat.v1.placeholder(dtype='bool')
    outputs = bn.apply(inputs, training=training)

    with self.cached_session() as sess:
      # Test training with placeholder learning phase.
      self.evaluate(tf.compat.v1.global_variables_initializer())
      np_gamma, np_beta = self.evaluate([bn.gamma, bn.beta])
      np_gamma = np.reshape(np_gamma, (1, 1, 1, 6))
      np_beta = np.reshape(np_beta, (1, 1, 1, 6))
      for _ in range(100):
        np_output, _, _ = sess.run([outputs] + bn.updates,
                                   feed_dict={training: True})
        # Verify that the axis is normalized during training.
        normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
        self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
        self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

      # Verify that the statistics are updated during training.
      moving_mean, moving_var = self.evaluate(
          [bn.moving_mean, bn.moving_variance])
      np_inputs = self.evaluate(inputs)
      mean = np.mean(np_inputs, axis=(0, 1, 2))
      std = np.std(np_inputs, axis=(0, 1, 2))
      variance = np.square(std)
      self.assertAllClose(mean, moving_mean, atol=1e-2)
      self.assertAllClose(variance, moving_var, atol=1e-2)

      # Test inference with placeholder learning phase.
      np_output = sess.run(outputs, feed_dict={training: False})

      # Verify that the axis is normalized during inference.
      normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
      self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
      self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

</source>
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/normalization_test.py" startline="598" endline="639" pcid="3088">
  def testNegativeAxis(self):
    epsilon = 1e-3
    bn = normalization_layers.BatchNormalization(
        axis=-1, epsilon=epsilon, momentum=0.9)
    inputs = tf.Variable(
        np.random.random((5, 4, 3, 6)) + 100, dtype=tf.float32)
    training = tf.compat.v1.placeholder(dtype='bool')
    outputs = bn.apply(inputs, training=training)

    with self.cached_session() as sess:
      # Test training with placeholder learning phase.
      self.evaluate(tf.compat.v1.global_variables_initializer())
      np_gamma, np_beta = self.evaluate([bn.gamma, bn.beta])
      np_gamma = np.reshape(np_gamma, (1, 1, 1, 6))
      np_beta = np.reshape(np_beta, (1, 1, 1, 6))
      for _ in range(100):
        np_output, _, _ = sess.run([outputs] + bn.updates,
                                   feed_dict={training: True})

        # Verify that the axis is normalized during training.
        normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
        self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
        self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

      # Verify that the statistics are updated during training.
      moving_mean, moving_var = self.evaluate(
          [bn.moving_mean, bn.moving_variance])
      np_inputs = self.evaluate(inputs)
      mean = np.mean(np_inputs, axis=(0, 1, 2))
      std = np.std(np_inputs, axis=(0, 1, 2))
      variance = np.square(std)
      self.assertAllClose(mean, moving_mean, atol=1e-2)
      self.assertAllClose(variance, moving_var, atol=1e-2)

      # Test inference with placeholder learning phase.
      np_output = sess.run(outputs, feed_dict={training: False})

      # Verify that the axis is normalized during inference.
      normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
      self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
      self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

</source>
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/normalization_test.py" startline="433" endline="473" pcid="3084">
  def test4DInputAxis2(self):
    epsilon = 1e-3
    bn = normalization_layers.BatchNormalization(
        axis=2, epsilon=epsilon, momentum=0.9)
    inputs = tf.Variable(
        np.random.random((5, 4, 3, 6)) + 100, dtype=tf.float32)
    training = tf.compat.v1.placeholder(dtype='bool')
    outputs = bn.apply(inputs, training=training)

    with self.cached_session() as sess:
      # Test training with placeholder learning phase.
      self.evaluate(tf.compat.v1.global_variables_initializer())
      np_gamma, np_beta = self.evaluate([bn.gamma, bn.beta])
      np_gamma = np.reshape(np_gamma, (1, 1, 3, 1))
      np_beta = np.reshape(np_beta, (1, 1, 3, 1))
      for _ in range(100):
        np_output, _, _ = sess.run([outputs] + bn.updates,
                                   feed_dict={training: True})
        # Verify that the axis is normalized during training.
        normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
        self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
        self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

      # Verify that the statistics are updated during training.
      moving_mean, moving_var = self.evaluate(
          [bn.moving_mean, bn.moving_variance])
      np_inputs = self.evaluate(inputs)
      mean = np.mean(np_inputs, axis=(0, 1, 3))
      std = np.std(np_inputs, axis=(0, 1, 3))
      variance = np.square(std)
      self.assertAllClose(mean, moving_mean, atol=1e-2)
      self.assertAllClose(variance, moving_var, atol=1e-2)

      # Test inference with placeholder learning phase.
      np_output = sess.run(outputs, feed_dict={training: False})

      # Verify that the axis is normalized during inference.
      normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
      self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
      self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

</source>
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/normalization_test.py" startline="556" endline="597" pcid="3087">
  def test4DInputAxis1Fused(self):
    if tf.test.is_gpu_available(cuda_only=True):
      epsilon = 1e-3
      bn = normalization_layers.BatchNormalization(
          axis=1, epsilon=epsilon, momentum=0.9, fused=True)
      inputs = tf.Variable(
          np.random.random((5, 4, 3, 6)) + 100, dtype=tf.float32)
      training = tf.compat.v1.placeholder(dtype='bool')
      outputs = bn.apply(inputs, training=training)

      with self.cached_session() as sess:
        # Test training with placeholder learning phase.
        self.evaluate(tf.compat.v1.global_variables_initializer())
        np_gamma, np_beta = self.evaluate([bn.gamma, bn.beta])
        np_gamma = np.reshape(np_gamma, (1, 4, 1, 1))
        np_beta = np.reshape(np_beta, (1, 4, 1, 1))
        for _ in range(100):
          np_output, _, _ = sess.run(
              [outputs] + bn.updates, feed_dict={training: True})
          # Verify that the axis is normalized during training.
          normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
          self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
          self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

        # Verify that the statistics are updated during training.
        moving_mean, moving_var = self.evaluate(
            [bn.moving_mean, bn.moving_variance])
        np_inputs = self.evaluate(inputs)
        mean = np.mean(np_inputs, axis=(0, 2, 3))
        std = np.std(np_inputs, axis=(0, 2, 3))
        variance = np.square(std)
        self.assertAllClose(mean, moving_mean, atol=1e-2)
        self.assertAllClose(variance, moving_var, atol=1e-2)

        # Test inference with placeholder learning phase.
        np_output = sess.run(outputs, feed_dict={training: False})

        # Verify that the axis is normalized during inference.
        normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
        self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
        self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

</source>
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/normalization_test.py" startline="1342" endline="1382" pcid="3110">
  def test5DInputMultiAxis123(self):
    epsilon = 1e-3
    bn = normalization_layers.BatchNormalization(
        axis=[1, 2, 3], epsilon=epsilon, momentum=0.9)
    inputs = tf.Variable(
        np.random.random((5, 3, 4, 4, 3)) + 100, dtype=tf.float32)
    training = tf.compat.v1.placeholder(dtype='bool')
    outputs = bn.apply(inputs, training=training)

    with self.cached_session() as sess:
      # Test training with placeholder learning phase.
      self.evaluate(tf.compat.v1.global_variables_initializer())

      np_gamma, np_beta = self.evaluate([bn.gamma, bn.beta])

      for _ in range(100):
        np_output, _, _ = sess.run([outputs] + bn.updates,
                                   feed_dict={training: True})
        # Verify that the axis is normalized during training.
        normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
        self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
        self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

      # Verify that the statistics are updated during training.
      moving_mean, moving_var = self.evaluate(
          [bn.moving_mean, bn.moving_variance])
      np_inputs = self.evaluate(inputs)
      mean = np.mean(np_inputs, axis=(0, 4), keepdims=True)
      std = np.std(np_inputs, axis=(0, 4), keepdims=True)
      variance = np.square(std)
      self.assertAllClose(mean, moving_mean, atol=1e-2)
      self.assertAllClose(variance, moving_var, atol=1e-2)

      # Test inference with placeholder learning phase.
      np_output = sess.run(outputs, feed_dict={training: False})

      # Verify that the axis is normalized during inference.
      normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
      self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
      self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

</source>
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/normalization_test.py" startline="350" endline="390" pcid="3082">
  def test3DInputAxis2(self):
    epsilon = 1e-3
    bn = normalization_layers.BatchNormalization(
        axis=2, epsilon=epsilon, momentum=0.9)
    inputs = tf.Variable(
        np.random.random((5, 4, 3)) + 100, dtype=tf.float32)
    training = tf.compat.v1.placeholder(dtype='bool')
    outputs = bn.apply(inputs, training=training)

    with self.cached_session() as sess:
      # Test training with placeholder learning phase.
      self.evaluate(tf.compat.v1.global_variables_initializer())
      np_gamma, np_beta = self.evaluate([bn.gamma, bn.beta])
      np_gamma = np.reshape(np_gamma, (1, 1, 3))
      np_beta = np.reshape(np_beta, (1, 1, 3))
      for _ in range(100):
        np_output, _, _ = sess.run([outputs] + bn.updates,
                                   feed_dict={training: True})
        # Verify that the axis is normalized during training.
        normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
        self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
        self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

      # Verify that the statistics are updated during training.
      moving_mean, moving_var = self.evaluate(
          [bn.moving_mean, bn.moving_variance])
      np_inputs = self.evaluate(inputs)
      mean = np.mean(np_inputs, axis=(0, 1))
      std = np.std(np_inputs, axis=(0, 1))
      variance = np.square(std)
      self.assertAllClose(mean, moving_mean, atol=1e-2)
      self.assertAllClose(variance, moving_var, atol=1e-2)

      # Test inference with placeholder learning phase.
      np_output = sess.run(outputs, feed_dict={training: False})

      # Verify that the axis is normalized during inference.
      normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
      self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
      self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

</source>
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/normalization_test.py" startline="640" endline="679" pcid="3089">
  def testBooleanLearningPhase(self):
    epsilon = 1e-3
    bn = normalization_layers.BatchNormalization(
        axis=-1, epsilon=epsilon, momentum=0.9)
    inputs = tf.Variable(
        np.random.random((5, 4, 3, 6)) + 100, dtype=tf.float32)
    outputs_training = bn.apply(inputs, training=True)
    outputs_infer = bn.apply(inputs, training=False)

    with self.cached_session() as sess:
      # Test training with placeholder learning phase.
      self.evaluate(tf.compat.v1.global_variables_initializer())
      np_gamma, np_beta = self.evaluate([bn.gamma, bn.beta])
      np_gamma = np.reshape(np_gamma, (1, 1, 1, 6))
      np_beta = np.reshape(np_beta, (1, 1, 1, 6))
      for _ in range(100):
        np_output, _, _ = sess.run([outputs_training] + bn.updates)
        # Verify that the axis is normalized during training.
        normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
        self.assertAlmostEqual(np.mean(normed_np_output), 0., places=2)
        self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

      # Verify that the statistics are updated during training.
      moving_mean, moving_var = self.evaluate(
          [bn.moving_mean, bn.moving_variance])
      np_inputs = self.evaluate(inputs)
      mean = np.mean(np_inputs, axis=(0, 1, 2))
      std = np.std(np_inputs, axis=(0, 1, 2))
      variance = np.square(std)
      self.assertAllClose(mean, moving_mean, atol=1e-2)
      self.assertAllClose(variance, moving_var, atol=1e-2)

      # Test inference with placeholder learning phase.
      np_output = self.evaluate(outputs_infer)

      # Verify that the axis is normalized during inference.
      normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
      self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
      self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

</source>
</class>

<class classid="135" nclones="2" nlines="10" similarity="100">
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/normalization_test.py" startline="810" endline="824" pcid="3093">
  def testNoCenter(self):
    bn = normalization_layers.BatchNormalization(axis=1, center=False)
    inputs = tf.random.uniform((5, 4, 3), seed=1)
    training = tf.compat.v1.placeholder(dtype='bool')
    outputs = bn.apply(inputs, training=training)

    # Verify shape.
    self.assertListEqual(outputs.get_shape().as_list(), [5, 4, 3])

    # Verify layer attributes.
    self.assertEqual(len(bn.updates), 2)
    self.assertEqual(len(bn.variables), 3)
    self.assertEqual(len(bn.trainable_variables), 1)
    self.assertEqual(len(bn.non_trainable_variables), 2)

</source>
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/normalization_test.py" startline="825" endline="839" pcid="3094">
  def testNoScale(self):
    bn = normalization_layers.BatchNormalization(axis=1, scale=False)
    inputs = tf.random.uniform((5, 4, 3), seed=1)
    training = tf.compat.v1.placeholder(dtype='bool')
    outputs = bn.apply(inputs, training=training)

    # Verify shape.
    self.assertListEqual(outputs.get_shape().as_list(), [5, 4, 3])

    # Verify layer attributes.
    self.assertEqual(len(bn.updates), 2)
    self.assertEqual(len(bn.variables), 3)
    self.assertEqual(len(bn.trainable_variables), 1)
    self.assertEqual(len(bn.non_trainable_variables), 2)

</source>
</class>

<class classid="136" nclones="3" nlines="50" similarity="79">
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/normalization_test.py" startline="865" endline="918" pcid="3097">
  def testRenorm(self):
    shape = (4, 3)
    xt = tf.compat.v1.placeholder(tf.float32, shape)
    momentum = 0.99
    renorm_momentum = 0.8
    rmax = 1.1
    rmin = 0.9
    dmax = 0.1
    gamma = 2.
    beta = 3.
    epsilon = 0.001
    bn = normalization_layers.BatchNormalization(
        axis=1,
        gamma_initializer=tf.compat.v1.constant_initializer(gamma),
        beta_initializer=tf.compat.v1.constant_initializer(beta),
        epsilon=epsilon,
        momentum=momentum,
        renorm=True,
        renorm_clipping={'rmax': rmax, 'rmin': rmin, 'dmax': dmax},
        renorm_momentum=renorm_momentum)
    training = tf.compat.v1.placeholder(tf.bool)
    yt = bn.apply(xt, training=training)

    moving_mean = 0.
    moving_stddev = 1.
    renorm_mean = 0.
    renorm_stddev = 1.
    with self.session() as sess:
      self.evaluate(tf.compat.v1.global_variables_initializer())
      for _ in range(5):
        x = np.random.random(shape)

        mean = x.mean(0)
        variance = x.var(0)
        stddev = np.sqrt(variance + epsilon)
        r = (stddev / renorm_stddev).clip(rmin, rmax)
        d = ((mean - renorm_mean) / renorm_stddev).clip(-dmax, dmax)
        y_train = ((x - mean) / stddev * r + d) * gamma + beta
        renorm_mean += (mean - renorm_mean) * (1. - renorm_momentum)
        renorm_stddev += (stddev - renorm_stddev) * (1. - renorm_momentum)
        moving_mean += (mean - moving_mean) * (1. - momentum)
        moving_stddev += (stddev - moving_stddev) * (1. - momentum)

        y_test = ((x - moving_mean) /
                  (moving_stddev * moving_stddev)**0.5 * gamma) + beta

        yt_val_train, _, _ = sess.run([yt] + bn.updates,
                                      feed_dict={xt: x, training: True})
        yt_val_test, _, _ = sess.run([yt] + bn.updates,
                                     feed_dict={xt: x, training: False})

        self.assertAllClose(y_train, yt_val_train, atol=1e-5)
        self.assertAllClose(y_test, yt_val_test, atol=1e-5)

</source>
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/normalization_test.py" startline="1019" endline="1076" pcid="3100">
  def testRenormWithAdjustment(self):
    shape = (4, 3)
    xt = tf.compat.v1.placeholder(tf.float32, shape)
    momentum = 0.99
    renorm_momentum = 0.8
    rmax = 1.1
    rmin = 0.9
    dmax = 0.1
    gamma = 2.
    beta = 3.
    epsilon = 0.001
    adjust_scale = tf.random.uniform(shape[-1:], 0.5, 1.5)
    adjust_bias = tf.random.uniform(shape[-1:], -.2, .2)
    bn = normalization_layers.BatchNormalization(
        axis=1,
        gamma_initializer=tf.compat.v1.constant_initializer(gamma),
        beta_initializer=tf.compat.v1.constant_initializer(beta),
        epsilon=epsilon,
        momentum=momentum,
        renorm=True,
        renorm_clipping={'rmax': rmax, 'rmin': rmin, 'dmax': dmax},
        renorm_momentum=renorm_momentum,
        adjustment=lambda _: (adjust_scale, adjust_bias))
    training = tf.compat.v1.placeholder(tf.bool)
    yt = bn.apply(xt, training=training)

    moving_mean = 0.
    moving_stddev = 1.
    renorm_mean = 0.
    renorm_stddev = 1.
    with self.session() as sess:
      self.evaluate(tf.compat.v1.global_variables_initializer())
      for _ in range(5):
        x = np.random.random(shape)
        yt_val_train, adj_scale_val, adj_bias_val = sess.run(
            [yt, adjust_scale, adjust_bias] + bn.updates,
            feed_dict={xt: x, training: True})[:3]
        yt_val_test = sess.run([yt] + bn.updates,
                               feed_dict={xt: x, training: False})[0]

        mean = x.mean(0)
        variance = x.var(0)
        stddev = np.sqrt(variance + epsilon)
        r = (stddev / renorm_stddev).clip(rmin, rmax)
        d = ((mean - renorm_mean) / renorm_stddev).clip(-dmax, dmax)
        y_train = (((x - mean) / stddev * r + d) * adj_scale_val +
                   adj_bias_val) * gamma + beta
        renorm_mean += (mean - renorm_mean) * (1. - renorm_momentum)
        renorm_stddev += (stddev - renorm_stddev) * (1. - renorm_momentum)
        moving_mean += (mean - moving_mean) * (1. - momentum)
        moving_stddev += (stddev - moving_stddev) * (1. - momentum)

        y_test = ((x - moving_mean) /
                  (moving_stddev * moving_stddev)**0.5 * gamma) + beta

        self.assertAllClose(y_train, yt_val_train, atol=1e-5)
        self.assertAllClose(y_test, yt_val_test, atol=1e-5)

</source>
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/normalization_test.py" startline="919" endline="974" pcid="3098">
  def testRenormNoClippingSameMomentumGivesSameTestTrain(self):
    shape = (4, 3)
    xt = tf.compat.v1.placeholder(tf.float32, shape)
    momentum = 0.9
    renorm_momentum = 0.9
    gamma = 2.
    beta = 3.
    epsilon = 0.001
    bn = normalization_layers.BatchNormalization(
        axis=1,
        gamma_initializer=tf.compat.v1.constant_initializer(gamma),
        beta_initializer=tf.compat.v1.constant_initializer(beta),
        epsilon=epsilon,
        momentum=momentum,
        renorm=True,
        renorm_clipping=None,
        renorm_momentum=momentum)
    training = tf.compat.v1.placeholder(tf.bool)
    yt = bn.apply(xt, training=training)
    moving_mean = 0.
    moving_stddev = 1.
    renorm_mean = 0.
    renorm_stddev = 1.
    with self.session() as sess:
      self.evaluate(tf.compat.v1.global_variables_initializer())
      for step in range(6):
        x = np.random.random(shape)

        mean = x.mean(0)
        variance = x.var(0)
        stddev = np.sqrt(variance + epsilon)
        r = (stddev / renorm_stddev)
        d = ((mean - renorm_mean) / renorm_stddev)
        y_test = ((x - moving_mean) /
                  (moving_stddev * moving_stddev)**0.5 * gamma) + beta
        y_train = ((x - mean) / stddev * r + d) * gamma + beta
        renorm_mean += (mean - renorm_mean) * (1. - renorm_momentum)
        renorm_stddev += (stddev - renorm_stddev) * (1. - renorm_momentum)
        moving_mean += (mean - moving_mean) * (1. - momentum)
        moving_stddev += (stddev - moving_stddev) * (1. - momentum)

        # Compute test values first, before the train mode updates the moving
        # averages.
        yt_val_test, _, _ = sess.run([yt] + bn.updates,
                                     feed_dict={xt: x, training: False})
        yt_val_train, _, _ = sess.run([yt] + bn.updates,
                                      feed_dict={xt: x, training: True})

        # Due to initialization inconsistencies, values may not be identical
        # on the first iteration (but shouldn't be different by much more than
        # epsilon). After the first iteration they should be identical.
        atol = epsilon * 1.5 if step == 0 else 1e-5
        self.assertAllClose(y_train, yt_val_train, atol=atol)
        self.assertAllClose(y_test, yt_val_test, atol=atol)
        self.assertAllClose(yt_val_train, yt_val_test, atol=atol)

</source>
</class>

<class classid="137" nclones="4" nlines="43" similarity="79">
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/normalization_test.py" startline="1125" endline="1177" pcid="3105">
  def testGhostBN2Dims(self):
    shape = [6, 2]
    virtual_batch_size = 3
    beta = 2.
    gamma = 3.
    momentum = 0.8
    epsilon = 1e-3
    moving_means = np.zeros([2, 2], dtype=np.float32)
    moving_vars = np.ones([2, 2], dtype=np.float32)

    inp = tf.compat.v1.placeholder(tf.float32, shape)
    is_training = tf.compat.v1.placeholder(tf.bool)
    bn = normalization_layers.BatchNormalization(
        momentum=momentum,
        epsilon=epsilon,
        beta_initializer=tf.compat.v1.constant_initializer(beta),
        gamma_initializer=tf.compat.v1.constant_initializer(gamma),
        virtual_batch_size=virtual_batch_size)
    out = bn.apply(inp, training=is_training)
    ghost_shape = ([virtual_batch_size,
                    shape[0] // virtual_batch_size,
                    shape[1]])

    with self.session() as sess:
      self.evaluate(tf.compat.v1.global_variables_initializer())
      for _ in range(5):
        x = np.random.random(shape)

        sub_batched = np.reshape(x, ghost_shape)
        means = np.mean(sub_batched, axis=0, keepdims=True)
        variances = np.var(sub_batched, axis=0, keepdims=True)

        avg_means = np.mean(means, axis=1, keepdims=True)
        avg_variances = np.mean(variances, axis=1, keepdims=True)

        moving_means = moving_means * momentum + avg_means * (1. - momentum)
        moving_vars = moving_vars * momentum + avg_variances * (1. - momentum)

        y_train = ((sub_batched - means) /
                   (variances + epsilon) ** 0.5 * gamma) + beta
        y_test = ((sub_batched - moving_means) /
                  (moving_vars + epsilon) ** 0.5 * gamma) + beta

        y_train = np.reshape(y_train, shape)
        y_test = np.reshape(y_test, shape)

        y_val_train, _, _ = sess.run([out] + bn.updates,
                                     feed_dict={inp: x, is_training: True})
        y_val_test = sess.run(out, feed_dict={inp: x, is_training: False})

        self.assertAllClose(y_train, y_val_train, atol=1e-5)
        self.assertAllClose(y_test, y_val_test, atol=1e-5)

</source>
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/normalization_test.py" startline="1178" endline="1230" pcid="3106">
  def testGhostBN4DimsAxis3(self):
    shape = [6, 10, 10, 3]
    virtual_batch_size = 2
    beta = 2.
    gamma = 3.
    momentum = 0.8
    epsilon = 1e-3
    moving_means = np.zeros([1, 1, 1, 1, 3], dtype=np.float32)
    moving_vars = np.ones([1, 1, 1, 1, 3], dtype=np.float32)

    inp = tf.compat.v1.placeholder(tf.float32, shape)
    is_training = tf.compat.v1.placeholder(tf.bool)
    bn = normalization_layers.BatchNormalization(
        axis=3,
        momentum=momentum,
        epsilon=epsilon,
        beta_initializer=tf.compat.v1.constant_initializer(beta),
        gamma_initializer=tf.compat.v1.constant_initializer(gamma),
        virtual_batch_size=virtual_batch_size)
    out = bn.apply(inp, training=is_training)
    ghost_shape = ([virtual_batch_size, shape[0] // virtual_batch_size] +
                   shape[1:])

    with self.session() as sess:
      self.evaluate(tf.compat.v1.global_variables_initializer())
      for _ in range(5):
        x = np.random.random(shape)

        sub_batched = np.reshape(x, ghost_shape)
        means = np.mean(sub_batched, axis=(0, 2, 3), keepdims=True)
        variances = np.var(sub_batched, axis=(0, 2, 3), keepdims=True)

        avg_means = np.mean(means, axis=1, keepdims=True)
        avg_variances = np.mean(variances, axis=1, keepdims=True)

        moving_means = moving_means * momentum + avg_means * (1. - momentum)
        moving_vars = moving_vars * momentum + avg_variances * (1. - momentum)

        y_train = ((sub_batched - means) /
                   (variances + epsilon) ** 0.5 * gamma) + beta
        y_test = ((sub_batched - moving_means) /
                  (moving_vars + epsilon) ** 0.5 * gamma) + beta

        y_train = np.reshape(y_train, shape)
        y_test = np.reshape(y_test, shape)

        y_val_train, _, _ = sess.run([out] + bn.updates,
                                     feed_dict={inp: x, is_training: True})
        y_val_test = sess.run(out, feed_dict={inp: x, is_training: False})

        self.assertAllClose(y_train, y_val_train, atol=1e-2)
        self.assertAllClose(y_test, y_val_test, atol=1e-2)

</source>
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/normalization_test.py" startline="1231" endline="1284" pcid="3107">
  def testGhostBN4DimsAxis1(self):
    shape = [6, 3, 10, 10]
    virtual_batch_size = 2
    beta = 2.
    gamma = 3.
    momentum = 0.8
    epsilon = 1e-3
    moving_means = np.zeros([1, 1, 3, 1, 1], dtype=np.float32)
    moving_vars = np.ones([1, 1, 3, 1, 1], dtype=np.float32)

    inp = tf.compat.v1.placeholder(tf.float32, shape)
    is_training = tf.compat.v1.placeholder(tf.bool)
    bn = normalization_layers.BatchNormalization(
        axis=1,
        momentum=momentum,
        epsilon=epsilon,
        beta_initializer=tf.compat.v1.constant_initializer(beta),
        gamma_initializer=tf.compat.v1.constant_initializer(gamma),
        virtual_batch_size=virtual_batch_size,
        fused=False)      # NCHW is unsupported by CPU fused batch norm
    out = bn.apply(inp, training=is_training)
    ghost_shape = ([virtual_batch_size, shape[0] // virtual_batch_size] +
                   shape[1:])

    with self.session() as sess:
      self.evaluate(tf.compat.v1.global_variables_initializer())
      for _ in range(5):
        x = np.random.random(shape)

        sub_batched = np.reshape(x, ghost_shape)
        means = np.mean(sub_batched, axis=(0, 3, 4), keepdims=True)
        variances = np.var(sub_batched, axis=(0, 3, 4), keepdims=True)

        avg_means = np.mean(means, axis=1, keepdims=True)
        avg_variances = np.mean(variances, axis=1, keepdims=True)

        moving_means = moving_means * momentum + avg_means * (1. - momentum)
        moving_vars = moving_vars * momentum + avg_variances * (1. - momentum)

        y_train = ((sub_batched - means) /
                   (variances + epsilon) ** 0.5 * gamma) + beta
        y_test = ((sub_batched - moving_means) /
                  (moving_vars + epsilon) ** 0.5 * gamma) + beta

        y_train = np.reshape(y_train, shape)
        y_test = np.reshape(y_test, shape)

        y_val_train, _, _ = sess.run([out] + bn.updates,
                                     feed_dict={inp: x, is_training: True})
        y_val_test = sess.run(out, feed_dict={inp: x, is_training: False})

        self.assertAllClose(y_train, y_val_train, atol=1e-2)
        self.assertAllClose(y_test, y_val_test, atol=1e-2)

</source>
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/normalization_test.py" startline="1383" endline="1437" pcid="3111">
  def testGhostBN5DimsMultiAxis14(self):
    shape = [6, 3, 10, 10, 4]
    virtual_batch_size = 3
    beta = 2.
    gamma = 3.
    momentum = 0.8
    epsilon = 1e-3
    moving_means = np.zeros([1, 1, 3, 1, 1, 4], dtype=np.float32)
    moving_vars = np.ones([1, 1, 3, 1, 1, 4], dtype=np.float32)

    inp = tf.compat.v1.placeholder(tf.float32, shape)
    is_training = tf.compat.v1.placeholder(tf.bool)
    bn = normalization_layers.BatchNormalization(
        axis=[1, 4],
        momentum=momentum,
        epsilon=epsilon,
        beta_initializer=tf.compat.v1.constant_initializer(beta),
        gamma_initializer=tf.compat.v1.constant_initializer(gamma),
        virtual_batch_size=virtual_batch_size,
        fused=False)
    out = bn.apply(inp, training=is_training)
    ghost_shape = ([virtual_batch_size, shape[0] // virtual_batch_size] +
                   shape[1:])

    with self.session() as sess:
      self.evaluate(tf.compat.v1.global_variables_initializer())
      for _ in range(5):
        x = np.random.random(shape)

        sub_batched = np.reshape(x, ghost_shape)
        means = np.mean(sub_batched, axis=(0, 3, 4), keepdims=True)
        variances = np.var(sub_batched, axis=(0, 3, 4), keepdims=True)

        avg_means = np.mean(means, axis=1, keepdims=True)
        avg_variances = np.mean(variances, axis=1, keepdims=True)

        moving_means = moving_means * momentum + avg_means * (1. - momentum)
        moving_vars = moving_vars * momentum + avg_variances * (1. - momentum)

        y_train = ((sub_batched - means) /
                   (variances + epsilon) ** 0.5 * gamma) + beta
        y_test = ((sub_batched - moving_means) /
                  (moving_vars + epsilon) ** 0.5 * gamma) + beta

        y_train = np.reshape(y_train, shape)
        y_test = np.reshape(y_test, shape)

        y_val_train, _, _ = sess.run([out] + bn.updates,
                                     feed_dict={inp: x, is_training: True})
        y_val_test = sess.run(out, feed_dict={inp: x, is_training: False})

        self.assertAllClose(y_train, y_val_train, atol=1e-2)
        self.assertAllClose(y_test, y_val_test, atol=1e-2)


</source>
</class>

<class classid="138" nclones="30" nlines="47" similarity="70">
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/normalization.py" startline="179" endline="225" pcid="3112">
  def __init__(self,
               axis=-1,
               momentum=0.99,
               epsilon=1e-3,
               center=True,
               scale=True,
               beta_initializer=tf.compat.v1.zeros_initializer(),
               gamma_initializer=tf.compat.v1.ones_initializer(),
               moving_mean_initializer=tf.compat.v1.zeros_initializer(),
               moving_variance_initializer=tf.compat.v1.ones_initializer(),
               beta_regularizer=None,
               gamma_regularizer=None,
               beta_constraint=None,
               gamma_constraint=None,
               renorm=False,
               renorm_clipping=None,
               renorm_momentum=0.99,
               fused=None,
               trainable=True,
               virtual_batch_size=None,
               adjustment=None,
               name=None,
               **kwargs):
    super(BatchNormalization, self).__init__(
        axis=axis,
        momentum=momentum,
        epsilon=epsilon,
        center=center,
        scale=scale,
        beta_initializer=beta_initializer,
        gamma_initializer=gamma_initializer,
        moving_mean_initializer=moving_mean_initializer,
        moving_variance_initializer=moving_variance_initializer,
        beta_regularizer=beta_regularizer,
        gamma_regularizer=gamma_regularizer,
        beta_constraint=beta_constraint,
        gamma_constraint=gamma_constraint,
        renorm=renorm,
        renorm_clipping=renorm_clipping,
        renorm_momentum=renorm_momentum,
        fused=fused,
        trainable=trainable,
        virtual_batch_size=virtual_batch_size,
        adjustment=adjustment,
        name=name,
        **kwargs)

</source>
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/convolutional.py" startline="934" endline="980" pcid="3176">
  def __init__(self, filters,
               kernel_size,
               strides=1,
               padding='valid',
               data_format='channels_last',
               dilation_rate=1,
               depth_multiplier=1,
               activation=None,
               use_bias=True,
               depthwise_initializer=None,
               pointwise_initializer=None,
               bias_initializer=tf.compat.v1.zeros_initializer(),
               depthwise_regularizer=None,
               pointwise_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               depthwise_constraint=None,
               pointwise_constraint=None,
               bias_constraint=None,
               trainable=True,
               name=None,
               **kwargs):
    super(SeparableConv1D, self).__init__(
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        depth_multiplier=depth_multiplier,
        activation=activation,
        use_bias=use_bias,
        depthwise_initializer=depthwise_initializer,
        pointwise_initializer=pointwise_initializer,
        bias_initializer=bias_initializer,
        depthwise_regularizer=depthwise_regularizer,
        pointwise_regularizer=pointwise_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        depthwise_constraint=depthwise_constraint,
        pointwise_constraint=pointwise_constraint,
        bias_constraint=bias_constraint,
        trainable=trainable,
        name=name,
        **kwargs)


</source>
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/normalization.py" startline="232" endline="459" pcid="3114">
def batch_normalization(inputs,
                        axis=-1,
                        momentum=0.99,
                        epsilon=1e-3,
                        center=True,
                        scale=True,
                        beta_initializer=tf.compat.v1.zeros_initializer(),
                        gamma_initializer=tf.compat.v1.ones_initializer(),
                        moving_mean_initializer=tf.compat.v1.zeros_initializer(),
                        moving_variance_initializer=tf.compat.v1.ones_initializer(),
                        beta_regularizer=None,
                        gamma_regularizer=None,
                        beta_constraint=None,
                        gamma_constraint=None,
                        training=False,
                        trainable=True,
                        name=None,
                        reuse=None,
                        renorm=False,
                        renorm_clipping=None,
                        renorm_momentum=0.99,
                        fused=None,
                        virtual_batch_size=None,
                        adjustment=None):
  """Functional interface for the batch normalization layer from_config(Ioffe et al., 2015).

  Note: when training, the moving_mean and moving_variance need to be updated.
  By default the update ops are placed in `tf.GraphKeys.UPDATE_OPS`, so they
  need to be executed alongside the `train_op`. Also, be sure to add any
  batch_normalization ops before getting the update_ops collection. Otherwise,
  update_ops will be empty, and training/inference will not work properly. For
  example:

  ```python
    x_norm = tf.compat.v1.layers.batch_normalization(x, training=training)

    # ...

    update_ops = tf.compat.v1.get_collection(tf.GraphKeys.UPDATE_OPS)
    train_op = optimizer.minimize(loss)
    train_op = tf.group([train_op, update_ops])
  ```

  Args:
    inputs: Tensor input.
    axis: An `int`, the axis that should be normalized (typically the features
      axis). For instance, after a `Convolution2D` layer with
      `data_format="channels_first"`, set `axis=1` in `BatchNormalization`.
    momentum: Momentum for the moving average.
    epsilon: Small float added to variance to avoid dividing by zero.
    center: If True, add offset of `beta` to normalized tensor. If False, `beta`
      is ignored.
    scale: If True, multiply by `gamma`. If False, `gamma` is not used. When the
      next layer is linear (also e.g. `nn.relu`), this can be disabled since the
      scaling can be done by the next layer.
    beta_initializer: Initializer for the beta weight.
    gamma_initializer: Initializer for the gamma weight.
    moving_mean_initializer: Initializer for the moving mean.
    moving_variance_initializer: Initializer for the moving variance.
    beta_regularizer: Optional regularizer for the beta weight.
    gamma_regularizer: Optional regularizer for the gamma weight.
    beta_constraint: An optional projection function to be applied to the `beta`
      weight after being updated by an `Optimizer` (e.g. used to implement norm
      constraints or value constraints for layer weights). The function must
      take as input the unprojected variable and must return the projected
      variable (which must have the same shape). Constraints are not safe to use
      when doing asynchronous distributed training.
    gamma_constraint: An optional projection function to be applied to the
      `gamma` weight after being updated by an `Optimizer`.
    training: Either a Python boolean, or a TensorFlow boolean scalar tensor
      (e.g. a placeholder). Whether to return the output in training mode
      (normalized with statistics of the current batch) or in inference mode
      (normalized with moving statistics). **NOTE**: make sure to set this
        parameter correctly, or else your training/inference will not work
        properly.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable).
    name: String, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer by the same
      name.
    renorm: Whether to use Batch Renormalization (Ioffe, 2017). This adds extra
      variables during training. The inference is the same for either value of
      this parameter.
    renorm_clipping: A dictionary that may map keys 'rmax', 'rmin', 'dmax' to
      scalar `Tensors` used to clip the renorm correction. The correction `(r,
      d)` is used as `corrected_value = normalized_value * r + d`, with `r`
      clipped to [rmin, rmax], and `d` to [-dmax, dmax]. Missing rmax, rmin,
      dmax are set to inf, 0, inf, respectively.
    renorm_momentum: Momentum used to update the moving means and standard
      deviations with renorm. Unlike `momentum`, this affects training and
      should be neither too small (which would add noise) nor too large (which
      would give stale estimates). Note that `momentum` is still applied to get
      the means and variances for inference.
    fused: if `None` or `True`, use a faster, fused implementation if possible.
      If `False`, use the system recommended implementation.
    virtual_batch_size: An `int`. By default, `virtual_batch_size` is `None`,
      which means batch normalization is performed across the whole batch. When
      `virtual_batch_size` is not `None`, instead perform "Ghost Batch
      Normalization", which creates virtual sub-batches which are each
      normalized separately (with shared gamma, beta, and moving statistics).
      Must divide the actual batch size during execution.
    adjustment: A function taking the `Tensor` containing the (dynamic) shape of
      the input tensor and returning a pair (scale, bias) to apply to the
      normalized values (before gamma and beta), only during training. For
      example, if axis==-1,
        `adjustment = lambda shape: (
          tf.random.uniform(shape[-1:], 0.93, 1.07),
          tf.random.uniform(shape[-1:], -0.1, 0.1))` will scale the normalized
            value by up to 7% up or down, then shift the result by up to 0.1
            (with independent scaling and bias for each feature but shared
            across all examples), and finally apply gamma and/or beta. If
            `None`, no adjustment is applied. Cannot be specified if
            virtual_batch_size is specified.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.

  References:
    Batch Normalization - Accelerating Deep Network Training by Reducing
    Internal Covariate Shift:
      [Ioffe et al., 2015](http://proceedings.mlr.press/v37/ioffe15.html)
      ([pdf](http://proceedings.mlr.press/v37/ioffe15.pdf))
    Batch Renormalization - Towards Reducing Minibatch Dependence in
    Batch-Normalized Models:
      [Ioffe,
      2017](http://papers.nips.cc/paper/6790-batch-renormalization-towards-reducing-minibatch-dependence-in-batch-normalized-models)
      ([pdf](http://papers.nips.cc/paper/6790-batch-renormalization-towards-reducing-minibatch-dependence-in-batch-normalized-models.pdf))

  @compatibility(TF2)
  This API is not compatible with eager execution or `tf.function`.

  Please refer to [tf.layers section of the migration guide]
  (https://www.tensorflow.org/guide/migrate#models_based_on_tflayers)
  to migrate a TensorFlow v1 model to Keras. The corresponding TensorFlow v2
  layer is `tf.keras.layers.BatchNormalization`.

  The batch updating pattern with
  `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used in
  native TF2. Consult the `tf.keras.layers.BatchNormalization` documentation
  for further information.

  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   x_norm = tf.compat.v1.layers.batch_normalization(x)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input(shape=(28, 28, 1),)
   y = tf.keras.layers.BatchNormalization()(x)
   model = tf.keras.Model(x, y)
  ```
  #### How to Map Arguments

  TF1 Arg Name              | TF2 Arg Name              | Note
  :------------------------ | :------------------------ | :---------------
  `name`                    | `name`                    | Layer base class
  `trainable`               | `trainable`               | Layer base class
  `axis`                    | `axis`                    | -
  `momentum`                | `momentum`                | -
  `epsilon`                 | `epsilon`                 | -
  `center`                  | `center`                  | -
  `scale`                   | `scale`                   | -
  `beta_initializer`        | `beta_initializer`        | -
  `gamma_initializer`       | `gamma_initializer`       | -
  `moving_mean_initializer` | `moving_mean_initializer` | -
  `beta_regularizer`        | `beta_regularizer'        | -
  `gamma_regularizer`       | `gamma_regularizer'       | -
  `beta_constraint`         | `beta_constraint'         | -
  `gamma_constraint`        | `gamma_constraint'        | -
  `renorm`                  | Not supported             | -
  `renorm_clipping`         | Not supported             | -
  `renorm_momentum`         | Not supported             | -
  `fused`                   | Not supported             | -
  `virtual_batch_size`      | Not supported             | -
  `adjustment`              | Not supported             | -

  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.batch_normalization` is deprecated and '
      'will be removed in a future version. '
      'Please use `tf.keras.layers.BatchNormalization` instead. '
      'In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` '
      'should not be used (consult the `tf.keras.layers.BatchNormalization` '
      'documentation).',
      stacklevel=2)
  layer = BatchNormalization(
      axis=axis,
      momentum=momentum,
      epsilon=epsilon,
      center=center,
      scale=scale,
      beta_initializer=beta_initializer,
      gamma_initializer=gamma_initializer,
      moving_mean_initializer=moving_mean_initializer,
      moving_variance_initializer=moving_variance_initializer,
      beta_regularizer=beta_regularizer,
      gamma_regularizer=gamma_regularizer,
      beta_constraint=beta_constraint,
      gamma_constraint=gamma_constraint,
      renorm=renorm,
      renorm_clipping=renorm_clipping,
      renorm_momentum=renorm_momentum,
      fused=fused,
      trainable=trainable,
      virtual_batch_size=virtual_batch_size,
      adjustment=adjustment,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs, training=training)


# Aliases

</source>
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/convolutional.py" startline="1077" endline="1123" pcid="3177">
  def __init__(self, filters,
               kernel_size,
               strides=(1, 1),
               padding='valid',
               data_format='channels_last',
               dilation_rate=(1, 1),
               depth_multiplier=1,
               activation=None,
               use_bias=True,
               depthwise_initializer=None,
               pointwise_initializer=None,
               bias_initializer=tf.compat.v1.zeros_initializer(),
               depthwise_regularizer=None,
               pointwise_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               depthwise_constraint=None,
               pointwise_constraint=None,
               bias_constraint=None,
               trainable=True,
               name=None,
               **kwargs):
    super(SeparableConv2D, self).__init__(
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        depth_multiplier=depth_multiplier,
        activation=activation,
        use_bias=use_bias,
        depthwise_initializer=depthwise_initializer,
        pointwise_initializer=pointwise_initializer,
        bias_initializer=bias_initializer,
        depthwise_regularizer=depthwise_regularizer,
        pointwise_regularizer=pointwise_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        depthwise_constraint=depthwise_constraint,
        pointwise_constraint=pointwise_constraint,
        bias_constraint=bias_constraint,
        trainable=trainable,
        name=name,
        **kwargs)


</source>
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/convolutional.py" startline="696" endline="842" pcid="3175">
def conv3d(inputs,
           filters,
           kernel_size,
           strides=(1, 1, 1),
           padding='valid',
           data_format='channels_last',
           dilation_rate=(1, 1, 1),
           activation=None,
           use_bias=True,
           kernel_initializer=None,
           bias_initializer=tf.compat.v1.zeros_initializer(),
           kernel_regularizer=None,
           bias_regularizer=None,
           activity_regularizer=None,
           kernel_constraint=None,
           bias_constraint=None,
           trainable=True,
           name=None,
           reuse=None):
  """Functional interface for the 3D convolution layer.

  This layer creates a convolution kernel that is convolved
  (actually cross-correlated) with the layer input to produce a tensor of
  outputs. If `use_bias` is True (and a `bias_initializer` is provided),
  a bias vector is created and added to the outputs. Finally, if
  `activation` is not `None`, it is applied to the outputs as well.

  Args:
    inputs: Tensor input.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: An integer or tuple/list of 3 integers, specifying the
      depth, height and width of the 3D convolution window.
      Can be a single integer to specify the same value for
      all spatial dimensions.
    strides: An integer or tuple/list of 3 integers,
      specifying the strides of the convolution along the depth,
      height and width.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Specifying any stride value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, depth, height, width, channels)` while `channels_first`
      corresponds to inputs with shape
      `(batch, channels, depth, height, width)`.
    dilation_rate: An integer or tuple/list of 3 integers, specifying
      the dilation rate to use for dilated convolution.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is not compatible with eager execution or `tf.function`.

  Please refer to [tf.layers section of the migration guide]
  (https://www.tensorflow.org/guide/migrate#models_based_on_tflayers)
  to migrate a TensorFlow v1 model to Keras. The corresponding TensorFlow v2
  layer is `tf.keras.layers.Conv3D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv3d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv3D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv3d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv3D` instead.',
      stacklevel=2)
  layer = Conv3D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</source>
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/convolutional.py" startline="112" endline="149" pcid="3170">
  def __init__(self, filters,
               kernel_size,
               strides=1,
               padding='valid',
               data_format='channels_last',
               dilation_rate=1,
               activation=None,
               use_bias=True,
               kernel_initializer=None,
               bias_initializer=tf.compat.v1.zeros_initializer(),
               kernel_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               bias_constraint=None,
               trainable=True,
               name=None,
               **kwargs):
    super(Conv1D, self).__init__(
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        activation=activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        bias_initializer=bias_initializer,
        kernel_regularizer=kernel_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        kernel_constraint=kernel_constraint,
        bias_constraint=bias_constraint,
        trainable=trainable,
        name=name, **kwargs)


</source>
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/convolutional.py" startline="152" endline="290" pcid="3171">
def conv1d(inputs,
           filters,
           kernel_size,
           strides=1,
           padding='valid',
           data_format='channels_last',
           dilation_rate=1,
           activation=None,
           use_bias=True,
           kernel_initializer=None,
           bias_initializer=tf.compat.v1.zeros_initializer(),
           kernel_regularizer=None,
           bias_regularizer=None,
           activity_regularizer=None,
           kernel_constraint=None,
           bias_constraint=None,
           trainable=True,
           name=None,
           reuse=None):
  """Functional interface for 1D convolution layer (e.g. temporal convolution).

  This layer creates a convolution kernel that is convolved
  (actually cross-correlated) with the layer input to produce a tensor of
  outputs. If `use_bias` is True (and a `bias_initializer` is provided),
  a bias vector is created and added to the outputs. Finally, if
  `activation` is not `None`, it is applied to the outputs as well.

  Args:
    inputs: Tensor input.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: An integer or tuple/list of a single integer, specifying the
      length of the 1D convolution window.
    strides: An integer or tuple/list of a single integer,
      specifying the stride length of the convolution.
      Specifying any stride value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, length, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, length)`.
    dilation_rate: An integer or tuple/list of a single integer, specifying
      the dilation rate to use for dilated convolution.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any `strides` value != 1.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is not compatible with eager execution or `tf.function`.

  Please refer to [tf.layers section of the migration guide]
  (https://www.tensorflow.org/guide/migrate#models_based_on_tflayers)
  to migrate a TensorFlow v1 model to Keras. The corresponding TensorFlow v2
  layer is `tf.keras.layers.Conv1D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv1d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv1D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv1d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv1D` instead.',
      stacklevel=2)
  layer = Conv1D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</source>
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/convolutional.py" startline="1772" endline="1809" pcid="3182">
  def __init__(self,
               filters,
               kernel_size,
               strides=(1, 1, 1),
               padding='valid',
               data_format='channels_last',
               activation=None,
               use_bias=True,
               kernel_initializer=None,
               bias_initializer=tf.compat.v1.zeros_initializer(),
               kernel_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               bias_constraint=None,
               trainable=True,
               name=None,
               **kwargs):
    super(Conv3DTranspose, self).__init__(
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        activation=activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        bias_initializer=bias_initializer,
        kernel_regularizer=kernel_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        kernel_constraint=kernel_constraint,
        bias_constraint=bias_constraint,
        trainable=trainable,
        name=name,
        **kwargs)


</source>
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/convolutional.py" startline="420" endline="565" pcid="3173">
def conv2d(inputs,
           filters,
           kernel_size,
           strides=(1, 1),
           padding='valid',
           data_format='channels_last',
           dilation_rate=(1, 1),
           activation=None,
           use_bias=True,
           kernel_initializer=None,
           bias_initializer=tf.compat.v1.zeros_initializer(),
           kernel_regularizer=None,
           bias_regularizer=None,
           activity_regularizer=None,
           kernel_constraint=None,
           bias_constraint=None,
           trainable=True,
           name=None,
           reuse=None):
  """Functional interface for the 2D convolution layer.

  This layer creates a convolution kernel that is convolved
  (actually cross-correlated) with the layer input to produce a tensor of
  outputs. If `use_bias` is True (and a `bias_initializer` is provided),
  a bias vector is created and added to the outputs. Finally, if
  `activation` is not `None`, it is applied to the outputs as well.

  Args:
    inputs: Tensor input.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: An integer or tuple/list of 2 integers, specifying the
      height and width of the 2D convolution window.
      Can be a single integer to specify the same value for
      all spatial dimensions.
    strides: An integer or tuple/list of 2 integers,
      specifying the strides of the convolution along the height and width.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Specifying any stride value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, height, width)`.

    dilation_rate: An integer or tuple/list of 2 integers, specifying
      the dilation rate to use for dilated convolution.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is not compatible with eager execution or `tf.function`.

  Please refer to [tf.layers section of the migration guide]
  (https://www.tensorflow.org/guide/migrate#models_based_on_tflayers)
  to migrate a TensorFlow v1 model to Keras. The corresponding TensorFlow v2
  layer is `tf.keras.layers.Conv2D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv2d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv2D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv2d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv2D` instead.',
      stacklevel=2)
  layer = Conv2D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</source>
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/convolutional.py" startline="656" endline="693" pcid="3174">
  def __init__(self, filters,
               kernel_size,
               strides=(1, 1, 1),
               padding='valid',
               data_format='channels_last',
               dilation_rate=(1, 1, 1),
               activation=None,
               use_bias=True,
               kernel_initializer=None,
               bias_initializer=tf.compat.v1.zeros_initializer(),
               kernel_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               bias_constraint=None,
               trainable=True,
               name=None,
               **kwargs):
    super(Conv3D, self).__init__(
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        activation=activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        bias_initializer=bias_initializer,
        kernel_regularizer=kernel_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        kernel_constraint=kernel_constraint,
        bias_constraint=bias_constraint,
        trainable=trainable,
        name=name, **kwargs)


</source>
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/convolutional.py" startline="380" endline="417" pcid="3172">
  def __init__(self, filters,
               kernel_size,
               strides=(1, 1),
               padding='valid',
               data_format='channels_last',
               dilation_rate=(1, 1),
               activation=None,
               use_bias=True,
               kernel_initializer=None,
               bias_initializer=tf.compat.v1.zeros_initializer(),
               kernel_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               bias_constraint=None,
               trainable=True,
               name=None,
               **kwargs):
    super(Conv2D, self).__init__(
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        activation=activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        bias_initializer=bias_initializer,
        kernel_regularizer=kernel_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        kernel_constraint=kernel_constraint,
        bias_constraint=bias_constraint,
        trainable=trainable,
        name=name, **kwargs)


</source>
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/convolutional.py" startline="1524" endline="1560" pcid="3180">
  def __init__(self, filters,
               kernel_size,
               strides=(1, 1),
               padding='valid',
               data_format='channels_last',
               activation=None,
               use_bias=True,
               kernel_initializer=None,
               bias_initializer=tf.compat.v1.zeros_initializer(),
               kernel_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               bias_constraint=None,
               trainable=True,
               name=None,
               **kwargs):
    super(Conv2DTranspose, self).__init__(
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        activation=activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        bias_initializer=bias_initializer,
        kernel_regularizer=kernel_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        kernel_constraint=kernel_constraint,
        bias_constraint=bias_constraint,
        trainable=trainable,
        name=name,
        **kwargs)


</source>
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/convolutional.py" startline="1284" endline="1444" pcid="3179">
def separable_conv2d(inputs,
                     filters,
                     kernel_size,
                     strides=(1, 1),
                     padding='valid',
                     data_format='channels_last',
                     dilation_rate=(1, 1),
                     depth_multiplier=1,
                     activation=None,
                     use_bias=True,
                     depthwise_initializer=None,
                     pointwise_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     depthwise_regularizer=None,
                     pointwise_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     depthwise_constraint=None,
                     pointwise_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for the depthwise separable 2D convolution layer.

  This layer performs a depthwise convolution that acts separately on
  channels, followed by a pointwise convolution that mixes channels.
  If `use_bias` is True and a bias initializer is provided,
  it adds a bias vector to the output.
  It then optionally applies an activation function to produce the final output.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A tuple or list of 2 integers specifying the spatial
      dimensions of the filters. Can be a single integer to specify the same
      value for all spatial dimensions.
    strides: A tuple or list of 2 positive integers specifying the strides
      of the convolution. Can be a single integer to specify the same value for
      all spatial dimensions.
      Specifying any `stride` value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, height, width)`.

    dilation_rate: An integer or tuple/list of 2 integers, specifying
      the dilation rate to use for dilated convolution.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    depth_multiplier: The number of depthwise convolution output channels for
      each input channel. The total number of depthwise convolution output
      channels will be equal to `num_filters_in * depth_multiplier`.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    depthwise_initializer: An initializer for the depthwise convolution kernel.
    pointwise_initializer: An initializer for the pointwise convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    depthwise_regularizer: Optional regularizer for the depthwise
      convolution kernel.
    pointwise_regularizer: Optional regularizer for the pointwise
      convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    depthwise_constraint: Optional projection function to be applied to the
        depthwise kernel after being updated by an `Optimizer` (e.g. used for
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    pointwise_constraint: Optional projection function to be applied to the
        pointwise kernel after being updated by an `Optimizer`.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is not compatible with eager execution or `tf.function`.

  Please refer to [tf.layers section of the migration guide]
  (https://www.tensorflow.org/guide/migrate#models_based_on_tflayers)
  to migrate a TensorFlow v1 model to Keras. The corresponding TensorFlow v2
  layer is `tf.keras.layers.SeparableConv2D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.separable_conv2d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.SeparableConv2D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.separable_conv2d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.SeparableConv2D` instead.',
      stacklevel=2)
  layer = SeparableConv2D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      depth_multiplier=depth_multiplier,
      activation=activation,
      use_bias=use_bias,
      depthwise_initializer=depthwise_initializer,
      pointwise_initializer=pointwise_initializer,
      bias_initializer=bias_initializer,
      depthwise_regularizer=depthwise_regularizer,
      pointwise_regularizer=pointwise_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      depthwise_constraint=depthwise_constraint,
      pointwise_constraint=pointwise_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</source>
<source file="systems/keras-2.7.0/keras/layers/convolutional_recurrent.py" startline="1244" endline="1302" pcid="3924">
  def __init__(self,
               filters,
               kernel_size,
               strides=(1, 1),
               padding='valid',
               data_format=None,
               dilation_rate=(1, 1),
               activation='tanh',
               recurrent_activation='hard_sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               unit_forget_bias=True,
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               return_sequences=False,
               return_state=False,
               go_backwards=False,
               stateful=False,
               dropout=0.0,
               recurrent_dropout=0.0,
               **kwargs):
    super(ConvLSTM2D, self).__init__(
        rank=2,
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        unit_forget_bias=unit_forget_bias,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        return_sequences=return_sequences,
        return_state=return_state,
        go_backwards=go_backwards,
        stateful=stateful,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        **kwargs)


</source>
<source file="systems/keras-2.7.0/keras/layers/convolutional_recurrent.py" startline="1078" endline="1136" pcid="3923">
  def __init__(self,
               filters,
               kernel_size,
               strides=1,
               padding='valid',
               data_format=None,
               dilation_rate=1,
               activation='tanh',
               recurrent_activation='hard_sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               unit_forget_bias=True,
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               return_sequences=False,
               return_state=False,
               go_backwards=False,
               stateful=False,
               dropout=0.0,
               recurrent_dropout=0.0,
               **kwargs):
    super(ConvLSTM1D, self).__init__(
        rank=1,
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        unit_forget_bias=unit_forget_bias,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        return_sequences=return_sequences,
        return_state=return_state,
        go_backwards=go_backwards,
        stateful=stateful,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        **kwargs)


</source>
<source file="systems/keras-2.7.0/keras/layers/convolutional_recurrent.py" startline="1410" endline="1466" pcid="3925">
  def __init__(self,
               filters,
               kernel_size,
               strides=(1, 1, 1),
               padding='valid',
               data_format=None,
               dilation_rate=(1, 1, 1),
               activation='tanh',
               recurrent_activation='hard_sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               unit_forget_bias=True,
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               return_sequences=False,
               return_state=False,
               go_backwards=False,
               stateful=False,
               dropout=0.0,
               recurrent_dropout=0.0,
               **kwargs):
    super(ConvLSTM3D, self).__init__(
        rank=3,
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        unit_forget_bias=unit_forget_bias,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        return_sequences=return_sequences,
        return_state=return_state,
        go_backwards=go_backwards,
        stateful=stateful,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        **kwargs)
</source>
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/convolutional.py" startline="1126" endline="1281" pcid="3178">
def separable_conv1d(inputs,
                     filters,
                     kernel_size,
                     strides=1,
                     padding='valid',
                     data_format='channels_last',
                     dilation_rate=1,
                     depth_multiplier=1,
                     activation=None,
                     use_bias=True,
                     depthwise_initializer=None,
                     pointwise_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     depthwise_regularizer=None,
                     pointwise_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     depthwise_constraint=None,
                     pointwise_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for the depthwise separable 1D convolution layer.

  This layer performs a depthwise convolution that acts separately on
  channels, followed by a pointwise convolution that mixes channels.
  If `use_bias` is True and a bias initializer is provided,
  it adds a bias vector to the output.
  It then optionally applies an activation function to produce the final output.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A single integer specifying the spatial
      dimensions of the filters.
    strides: A single integer specifying the strides
      of the convolution.
      Specifying any `stride` value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, length, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, length)`.
    dilation_rate: A single integer, specifying
      the dilation rate to use for dilated convolution.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    depth_multiplier: The number of depthwise convolution output channels for
      each input channel. The total number of depthwise convolution output
      channels will be equal to `num_filters_in * depth_multiplier`.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    depthwise_initializer: An initializer for the depthwise convolution kernel.
    pointwise_initializer: An initializer for the pointwise convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    depthwise_regularizer: Optional regularizer for the depthwise
      convolution kernel.
    pointwise_regularizer: Optional regularizer for the pointwise
      convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    depthwise_constraint: Optional projection function to be applied to the
        depthwise kernel after being updated by an `Optimizer` (e.g. used for
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    pointwise_constraint: Optional projection function to be applied to the
        pointwise kernel after being updated by an `Optimizer`.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is not compatible with eager execution or `tf.function`.

  Please refer to [tf.layers section of the migration guide]
  (https://www.tensorflow.org/guide/migrate#models_based_on_tflayers)
  to migrate a TensorFlow v1 model to Keras. The corresponding TensorFlow v2
  layer is `tf.keras.layers.SeparableConv1D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.separable_conv1d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.SeparableConv1D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.separable_conv1d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.SeparableConv1D` instead.',
      stacklevel=2)
  layer = SeparableConv1D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      depth_multiplier=depth_multiplier,
      activation=activation,
      use_bias=use_bias,
      depthwise_initializer=depthwise_initializer,
      pointwise_initializer=pointwise_initializer,
      bias_initializer=bias_initializer,
      depthwise_regularizer=depthwise_regularizer,
      pointwise_regularizer=pointwise_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      depthwise_constraint=depthwise_constraint,
      pointwise_constraint=pointwise_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</source>
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/convolutional.py" startline="1563" endline="1696" pcid="3181">
def conv2d_transpose(inputs,
                     filters,
                     kernel_size,
                     strides=(1, 1),
                     padding='valid',
                     data_format='channels_last',
                     activation=None,
                     use_bias=True,
                     kernel_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     kernel_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     kernel_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for transposed 2D convolution layer.

  The need for transposed convolutions generally arises
  from the desire to use a transformation going in the opposite direction
  of a normal convolution, i.e., from something that has the shape of the
  output of some convolution to something that has the shape of its input
  while maintaining a connectivity pattern that is compatible with
  said convolution.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A tuple or list of 2 positive integers specifying the spatial
      dimensions of the filters. Can be a single integer to specify the same
      value for all spatial dimensions.
    strides: A tuple or list of 2 positive integers specifying the strides
      of the convolution. Can be a single integer to specify the same value for
      all spatial dimensions.
    padding: one of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, height, width)`.
    activation: Activation function. Set it to `None` to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If `None`, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is not compatible with eager execution or `tf.function`.

  Please refer to [tf.layers section of the migration guide]
  (https://www.tensorflow.org/guide/migrate#models_based_on_tflayers)
  to migrate a TensorFlow v1 model to Keras. The corresponding TensorFlow v2
  layer is `tf.keras.layers.Conv2DTranspose`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv2d_transpose(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv2DTranspose(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv2d_transpose` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv2DTranspose` instead.',
      stacklevel=2)
  layer = Conv2DTranspose(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</source>
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/convolutional.py" startline="1812" endline="1941" pcid="3183">
def conv3d_transpose(inputs,
                     filters,
                     kernel_size,
                     strides=(1, 1, 1),
                     padding='valid',
                     data_format='channels_last',
                     activation=None,
                     use_bias=True,
                     kernel_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     kernel_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     kernel_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for transposed 3D convolution layer.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A tuple or list of 3 positive integers specifying the spatial
      dimensions of the filters. Can be a single integer to specify the same
      value for all spatial dimensions.
    strides: A tuple or list of 3 positive integers specifying the strides
      of the convolution. Can be a single integer to specify the same value for
      all spatial dimensions.
    padding: one of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, depth, height, width, channels)` while `channels_first`
      corresponds to inputs with shape
      `(batch, channels, depth, height, width)`.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is not compatible with eager execution or `tf.function`.

  Please refer to [tf.layers section of the migration guide]
  (https://www.tensorflow.org/guide/migrate#models_based_on_tflayers)
  to migrate a TensorFlow v1 model to Keras. The corresponding TensorFlow v2
  layer is `tf.keras.layers.Conv3DTranspose`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv3d_transpose(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv3DTranspose(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv3d_transpose` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv3DTranspose` instead.',
      stacklevel=2)
  layer = Conv3DTranspose(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


# Aliases

</source>
<source file="systems/keras-2.7.0/keras/layers/recurrent.py" startline="2583" endline="2626" pcid="4259">
  def __init__(self,
               units,
               activation='tanh',
               recurrent_activation='hard_sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               unit_forget_bias=True,
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               **kwargs):
    warnings.warn(
        '`tf.keras.experimental.PeepholeLSTMCell` is deprecated '
        'and will be removed in a future version. '
        'Please use tensorflow_addons.rnn.PeepholeLSTMCell '
        'instead.',
        stacklevel=2)
    super(PeepholeLSTMCell, self).__init__(
        units=units,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        unit_forget_bias=unit_forget_bias,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        implementation=kwargs.pop('implementation', 1),
        **kwargs)

</source>
<source file="systems/keras-2.7.0/keras/layers/recurrent_v2.py" startline="164" endline="202" pcid="4712">
  def __init__(self,
               units,
               activation='tanh',
               recurrent_activation='sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               reset_after=True,
               **kwargs):
    super(GRUCell, self).__init__(
        units,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        implementation=kwargs.pop('implementation', 2),
        reset_after=reset_after,
        **kwargs)


</source>
<source file="systems/keras-2.7.0/keras/layers/recurrent_v2.py" startline="904" endline="942" pcid="4724">
  def __init__(self,
               units,
               activation='tanh',
               recurrent_activation='sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               unit_forget_bias=True,
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               **kwargs):
    super(LSTMCell, self).__init__(
        units,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        unit_forget_bias=unit_forget_bias,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        implementation=kwargs.pop('implementation', 2),
        **kwargs)


</source>
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/core.py" startline="116" endline="143" pcid="3115">
  def __init__(self, units,
               activation=None,
               use_bias=True,
               kernel_initializer=None,
               bias_initializer=tf.compat.v1.zeros_initializer(),
               kernel_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               bias_constraint=None,
               trainable=True,
               name=None,
               **kwargs):
    super(Dense, self).__init__(units=units,
                                activation=activation,
                                use_bias=use_bias,
                                kernel_initializer=kernel_initializer,
                                bias_initializer=bias_initializer,
                                kernel_regularizer=kernel_regularizer,
                                bias_regularizer=bias_regularizer,
                                activity_regularizer=activity_regularizer,
                                kernel_constraint=kernel_constraint,
                                bias_constraint=bias_constraint,
                                trainable=trainable,
                                name=name,
                                **kwargs)


</source>
<source file="systems/keras-2.7.0/keras/layers/recurrent.py" startline="2759" endline="2824" pcid="4263">
  def __init__(self,
               units,
               activation='tanh',
               recurrent_activation='hard_sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               unit_forget_bias=True,
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               return_sequences=False,
               return_state=False,
               go_backwards=False,
               stateful=False,
               unroll=False,
               **kwargs):
    implementation = kwargs.pop('implementation', 1)
    if implementation == 0:
      logging.warning('`implementation=0` has been deprecated, '
                      'and now defaults to `implementation=1`.'
                      'Please update your layer call.')
    if 'enable_caching_device' in kwargs:
      cell_kwargs = {'enable_caching_device':
                     kwargs.pop('enable_caching_device')}
    else:
      cell_kwargs = {}
    cell = LSTMCell(
        units,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        unit_forget_bias=unit_forget_bias,
        bias_initializer=bias_initializer,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        implementation=implementation,
        dtype=kwargs.get('dtype'),
        trainable=kwargs.get('trainable', True),
        **cell_kwargs)
    super(LSTM, self).__init__(
        cell,
        return_sequences=return_sequences,
        return_state=return_state,
        go_backwards=go_backwards,
        stateful=stateful,
        unroll=unroll,
        **kwargs)
    self.activity_regularizer = regularizers.get(activity_regularizer)
    self.input_spec = [InputSpec(ndim=3)]

</source>
<source file="systems/keras-2.7.0/keras/layers/convolutional_recurrent.py" startline="782" endline="844" pcid="3898">
  def __init__(self,
               rank,
               filters,
               kernel_size,
               strides=1,
               padding='valid',
               data_format=None,
               dilation_rate=1,
               activation='tanh',
               recurrent_activation='hard_sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               unit_forget_bias=True,
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               return_sequences=False,
               return_state=False,
               go_backwards=False,
               stateful=False,
               dropout=0.0,
               recurrent_dropout=0.0,
               **kwargs):
    cell = ConvLSTMCell(
        rank=rank,
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        unit_forget_bias=unit_forget_bias,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        dtype=kwargs.get('dtype'))
    super(ConvLSTM, self).__init__(
        rank,
        cell,
        return_sequences=return_sequences,
        return_state=return_state,
        go_backwards=go_backwards,
        stateful=stateful,
        **kwargs)
    self.activity_regularizer = regularizers.get(activity_regularizer)

</source>
<source file="systems/keras-2.7.0/keras/layers/recurrent_v2.py" startline="332" endline="403" pcid="4713">
  def __init__(self,
               units,
               activation='tanh',
               recurrent_activation='sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               return_sequences=False,
               return_state=False,
               go_backwards=False,
               stateful=False,
               unroll=False,
               time_major=False,
               reset_after=True,
               **kwargs):
    # return_runtime is a flag for testing, which shows the real backend
    # implementation chosen by grappler in graph mode.
    self._return_runtime = kwargs.pop('return_runtime', False)

    super(GRU, self).__init__(
        units,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        implementation=kwargs.pop('implementation', 2),
        return_sequences=return_sequences,
        return_state=return_state,
        go_backwards=go_backwards,
        stateful=stateful,
        unroll=unroll,
        time_major=time_major,
        reset_after=reset_after,
        **kwargs)
    # GPU kernel uses following setting by default and not configurable.
    self._could_use_gpu_kernel = (
        self.activation in (activations.tanh, tf.tanh) and
        self.recurrent_activation in (activations.sigmoid, tf.sigmoid) and
        recurrent_dropout == 0 and not unroll and use_bias and
        reset_after and tf.compat.v1.executing_eagerly_outside_functions())
    if tf.config.list_logical_devices('GPU'):
      # Only show the message when there is GPU available, user will not care
      # about the cuDNN if there isn't any GPU.
      if self._could_use_gpu_kernel:
        logging.debug(_CUDNN_AVAILABLE_MSG % self.name)
      else:
        logging.warning(_CUDNN_NOT_AVAILABLE_MSG % self.name)

    if _use_new_code():
      self._defun_wrapper = _DefunWrapper(time_major, go_backwards, 'gru')

</source>
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/core.py" startline="146" endline="257" pcid="3116">
def dense(
    inputs, units,
    activation=None,
    use_bias=True,
    kernel_initializer=None,
    bias_initializer=tf.compat.v1.zeros_initializer(),
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    trainable=True,
    name=None,
    reuse=None):
  """Functional interface for the densely-connected layer.

  This layer implements the operation:
  `outputs = activation(inputs * kernel + bias)`
  where `activation` is the activation function passed as the `activation`
  argument (if not `None`), `kernel` is a weights matrix created by the layer,
  and `bias` is a bias vector created by the layer
  (only if `use_bias` is `True`).

  Args:
    inputs: Tensor input.
    units: Integer or Long, dimensionality of the output space.
    activation: Activation function (callable). Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: Initializer function for the weight matrix.
      If `None` (default), weights are initialized using the default
      initializer used by `tf.compat.v1.get_variable`.
    bias_initializer: Initializer function for the bias.
    kernel_regularizer: Regularizer function for the weight matrix.
    bias_regularizer: Regularizer function for the bias.
    activity_regularizer: Regularizer function for the output.
    kernel_constraint: An optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: An optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: String, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor the same shape as `inputs` except the last dimension is of
    size `units`.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is not compatible with eager execution or `tf.function`.

  Please refer to [tf.layers section of the migration guide]
  (https://www.tensorflow.org/guide/migrate#models_based_on_tflayers)
  to migrate a TensorFlow v1 model to Keras. The corresponding TensorFlow v2
  layer is `tf.keras.layers.Dense`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.dense(x, units=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28,))
   y = tf.keras.layers.Dense(units=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility

  """
  warnings.warn(
      '`tf.layers.dense` is deprecated and '
      'will be removed in a future version. '
      'Please use `tf.keras.layers.Dense` instead.',
      stacklevel=2)
  layer = Dense(units,
                activation=activation,
                use_bias=use_bias,
                kernel_initializer=kernel_initializer,
                bias_initializer=bias_initializer,
                kernel_regularizer=kernel_regularizer,
                bias_regularizer=bias_regularizer,
                activity_regularizer=activity_regularizer,
                kernel_constraint=kernel_constraint,
                bias_constraint=bias_constraint,
                trainable=trainable,
                name=name,
                _scope=name,
                _reuse=reuse)
  return layer.apply(inputs)


</source>
<source file="systems/keras-2.7.0/keras/layers/recurrent.py" startline="1547" endline="1607" pcid="4207">
  def __init__(self,
               units,
               activation='tanh',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               return_sequences=False,
               return_state=False,
               go_backwards=False,
               stateful=False,
               unroll=False,
               **kwargs):
    if 'implementation' in kwargs:
      kwargs.pop('implementation')
      logging.warning('The `implementation` argument '
                      'in `SimpleRNN` has been deprecated. '
                      'Please remove it from your layer call.')
    if 'enable_caching_device' in kwargs:
      cell_kwargs = {'enable_caching_device':
                     kwargs.pop('enable_caching_device')}
    else:
      cell_kwargs = {}
    cell = SimpleRNNCell(
        units,
        activation=activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        dtype=kwargs.get('dtype'),
        trainable=kwargs.get('trainable', True),
        **cell_kwargs)
    super(SimpleRNN, self).__init__(
        cell,
        return_sequences=return_sequences,
        return_state=return_state,
        go_backwards=go_backwards,
        stateful=stateful,
        unroll=unroll,
        **kwargs)
    self.activity_regularizer = regularizers.get(activity_regularizer)
    self.input_spec = [InputSpec(ndim=3)]

</source>
<source file="systems/keras-2.7.0/keras/layers/recurrent_v2.py" startline="1057" endline="1131" pcid="4725">
  def __init__(self,
               units,
               activation='tanh',
               recurrent_activation='sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               unit_forget_bias=True,
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               return_sequences=False,
               return_state=False,
               go_backwards=False,
               stateful=False,
               time_major=False,
               unroll=False,
               **kwargs):
    # return_runtime is a flag for testing, which shows the real backend
    # implementation chosen by grappler in graph mode.
    self.return_runtime = kwargs.pop('return_runtime', False)

    super(LSTM, self).__init__(
        units,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        unit_forget_bias=unit_forget_bias,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        implementation=kwargs.pop('implementation', 2),
        return_sequences=return_sequences,
        return_state=return_state,
        go_backwards=go_backwards,
        stateful=stateful,
        time_major=time_major,
        unroll=unroll,
        **kwargs)

    self.state_spec = [
        InputSpec(shape=(None, dim)) for dim in (self.units, self.units)
    ]
    self._could_use_gpu_kernel = (
        self.activation in (activations.tanh, tf.tanh) and
        self.recurrent_activation in (activations.sigmoid, tf.sigmoid) and
        recurrent_dropout == 0 and not unroll and use_bias and
        tf.compat.v1.executing_eagerly_outside_functions())
    if tf.config.list_logical_devices('GPU'):
      # Only show the message when there is GPU available, user will not care
      # about the cuDNN if there isn't any GPU.
      if self._could_use_gpu_kernel:
        logging.debug(_CUDNN_AVAILABLE_MSG % self.name)
      else:
        logging.warning(_CUDNN_NOT_AVAILABLE_MSG % self.name)

    if _use_new_code():
      self._defun_wrapper = _DefunWrapper(time_major, go_backwards, 'lstm')

</source>
<source file="systems/keras-2.7.0/keras/layers/recurrent.py" startline="2084" endline="2149" pcid="4230">
  def __init__(self,
               units,
               activation='tanh',
               recurrent_activation='hard_sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               return_sequences=False,
               return_state=False,
               go_backwards=False,
               stateful=False,
               unroll=False,
               reset_after=False,
               **kwargs):
    implementation = kwargs.pop('implementation', 1)
    if implementation == 0:
      logging.warning('`implementation=0` has been deprecated, '
                      'and now defaults to `implementation=1`.'
                      'Please update your layer call.')
    if 'enable_caching_device' in kwargs:
      cell_kwargs = {'enable_caching_device':
                     kwargs.pop('enable_caching_device')}
    else:
      cell_kwargs = {}
    cell = GRUCell(
        units,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        implementation=implementation,
        reset_after=reset_after,
        dtype=kwargs.get('dtype'),
        trainable=kwargs.get('trainable', True),
        **cell_kwargs)
    super(GRU, self).__init__(
        cell,
        return_sequences=return_sequences,
        return_state=return_state,
        go_backwards=go_backwards,
        stateful=stateful,
        unroll=unroll,
        **kwargs)
    self.activity_regularizer = regularizers.get(activity_regularizer)
    self.input_spec = [InputSpec(ndim=3)]

</source>
</class>

<class classid="139" nclones="2" nlines="12" similarity="100">
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/pooling.py" startline="76" endline="89" pcid="3140">
  def __init__(self, pool_size, strides,
               padding='valid', data_format='channels_last',
               name=None, **kwargs):
    if strides is None:
      raise ValueError('Argument `strides` must not be None.')
    super(AveragePooling1D, self).__init__(
        pool_size=pool_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        name=name,
        **kwargs)


</source>
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/pooling.py" startline="210" endline="223" pcid="3142">
  def __init__(self, pool_size, strides,
               padding='valid', data_format='channels_last',
               name=None, **kwargs):
    if strides is None:
      raise ValueError('Argument `strides` must not be None.')
    super(MaxPooling1D, self).__init__(
        pool_size=pool_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        name=name,
        **kwargs)


</source>
</class>

<class classid="140" nclones="2" nlines="12" similarity="100">
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/pooling.py" startline="92" endline="162" pcid="3141">
def average_pooling1d(inputs, pool_size, strides,
                      padding='valid', data_format='channels_last',
                      name=None):
  """Average Pooling layer for 1D inputs.

  Args:
    inputs: The tensor over which to pool. Must have rank 3.
    pool_size: An integer or tuple/list of a single integer,
      representing the size of the pooling window.
    strides: An integer or tuple/list of a single integer, specifying the
      strides of the pooling operation.
    padding: A string. The padding method, either 'valid' or 'same'.
      Case-insensitive.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, length, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, length)`.
    name: A string, the name of the layer.

  Returns:
    The output tensor, of rank 3.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is not compatible with eager execution or `tf.function`.

  Please refer to [tf.layers section of the migration guide]
  (https://www.tensorflow.org/guide/migrate#models_based_on_tflayers)
  to migrate a TensorFlow v1 model to Keras. The corresponding TensorFlow v2
  layer is `tf.keras.layers.AveragePooling1D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.average_pooling1d(x, pool_size=2, strides=2)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.AveragePooling1D(pool_size=2, strides=2)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.average_pooling1d` is deprecated and '
      'will be removed in a future version. '
      'Please use `tf.keras.layers.AveragePooling1D` instead.',
      stacklevel=2)
  layer = AveragePooling1D(pool_size=pool_size,
                           strides=strides,
                           padding=padding,
                           data_format=data_format,
                           name=name)
  return layer.apply(inputs)


</source>
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/pooling.py" startline="226" endline="296" pcid="3143">
def max_pooling1d(inputs, pool_size, strides,
                  padding='valid', data_format='channels_last',
                  name=None):
  """Max Pooling layer for 1D inputs.

  Args:
    inputs: The tensor over which to pool. Must have rank 3.
    pool_size: An integer or tuple/list of a single integer,
      representing the size of the pooling window.
    strides: An integer or tuple/list of a single integer, specifying the
      strides of the pooling operation.
    padding: A string. The padding method, either 'valid' or 'same'.
      Case-insensitive.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, length, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, length)`.
    name: A string, the name of the layer.

  Returns:
    The output tensor, of rank 3.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is not compatible with eager execution or `tf.function`.

  Please refer to [tf.layers section of the migration guide]
  (https://www.tensorflow.org/guide/migrate#models_based_on_tflayers)
  to migrate a TensorFlow v1 model to Keras. The corresponding TensorFlow v2
  layer is `tf.keras.layers.MaxPooling1D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.max_pooling1d(x, pool_size=2, strides=2)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.MaxPooling1D(pool_size=2, strides=2)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.max_pooling1d` is deprecated and '
      'will be removed in a future version. '
      'Please use `tf.keras.layers.MaxPooling1D` instead.',
      stacklevel=2)
  layer = MaxPooling1D(pool_size=pool_size,
                       strides=strides,
                       padding=padding,
                       data_format=data_format,
                       name=name)
  return layer.apply(inputs)


</source>
</class>

<class classid="141" nclones="4" nlines="11" similarity="100">
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/pooling.py" startline="360" endline="433" pcid="3145">
def average_pooling2d(inputs,
                      pool_size, strides,
                      padding='valid', data_format='channels_last',
                      name=None):
  """Average pooling layer for 2D inputs (e.g. images).

  Args:
    inputs: The tensor over which to pool. Must have rank 4.
    pool_size: An integer or tuple/list of 2 integers: (pool_height, pool_width)
      specifying the size of the pooling window.
      Can be a single integer to specify the same value for
      all spatial dimensions.
    strides: An integer or tuple/list of 2 integers,
      specifying the strides of the pooling operation.
      Can be a single integer to specify the same value for
      all spatial dimensions.
    padding: A string. The padding method, either 'valid' or 'same'.
      Case-insensitive.
    data_format: A string. The ordering of the dimensions in the inputs.
      `channels_last` (default) and `channels_first` are supported.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, height, width)`.
    name: A string, the name of the layer.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is not compatible with eager execution or `tf.function`.

  Please refer to [tf.layers section of the migration guide]
  (https://www.tensorflow.org/guide/migrate#models_based_on_tflayers)
  to migrate a TensorFlow v1 model to Keras. The corresponding TensorFlow v2
  layer is `tf.keras.layers.AveragePooling2D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.average_pooling2d(x, pool_size=2, strides=2)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.AveragePooling2D(pool_size=2, strides=2)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.average_pooling2d` is deprecated and '
      'will be removed in a future version. '
      'Please use `tf.keras.layers.AveragePooling2D` instead.',
      stacklevel=2)
  layer = AveragePooling2D(pool_size=pool_size, strides=strides,
                           padding=padding, data_format=data_format,
                           name=name)
  return layer.apply(inputs)


</source>
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/pooling.py" startline="777" endline="851" pcid="3151">
def max_pooling3d(inputs,
                  pool_size, strides,
                  padding='valid', data_format='channels_last',
                  name=None):
  """Max pooling layer for 3D inputs (e.g.

  volumes).

  Args:
    inputs: The tensor over which to pool. Must have rank 5.
    pool_size: An integer or tuple/list of 3 integers: (pool_depth, pool_height,
      pool_width) specifying the size of the pooling window. Can be a single
      integer to specify the same value for all spatial dimensions.
    strides: An integer or tuple/list of 3 integers, specifying the strides of
      the pooling operation. Can be a single integer to specify the same value
      for all spatial dimensions.
    padding: A string. The padding method, either 'valid' or 'same'.
      Case-insensitive.
    data_format: A string. The ordering of the dimensions in the inputs.
      `channels_last` (default) and `channels_first` are supported.
      `channels_last` corresponds to inputs with shape `(batch, depth, height,
      width, channels)` while `channels_first` corresponds to inputs with shape
      `(batch, channels, depth, height, width)`.
    name: A string, the name of the layer.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is not compatible with eager execution or `tf.function`.

  Please refer to [tf.layers section of the migration guide]
  (https://www.tensorflow.org/guide/migrate#models_based_on_tflayers)
  to migrate a TensorFlow v1 model to Keras. The corresponding TensorFlow v2
  layer is `tf.keras.layers.MaxPooling3D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.max_pooling3d(x, pool_size=2, strides=2)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.MaxPooling3D(pool_size=2, strides=2)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.max_pooling3d` is deprecated and '
      'will be removed in a future version. '
      'Please use `tf.keras.layers.MaxPooling3D` instead.',
      stacklevel=2)
  layer = MaxPooling3D(pool_size=pool_size, strides=strides,
                       padding=padding, data_format=data_format,
                       name=name)
  return layer.apply(inputs)

# Aliases

</source>
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/pooling.py" startline="497" endline="570" pcid="3147">
def max_pooling2d(inputs,
                  pool_size, strides,
                  padding='valid', data_format='channels_last',
                  name=None):
  """Max pooling layer for 2D inputs (e.g. images).

  Args:
    inputs: The tensor over which to pool. Must have rank 4.
    pool_size: An integer or tuple/list of 2 integers: (pool_height, pool_width)
      specifying the size of the pooling window.
      Can be a single integer to specify the same value for
      all spatial dimensions.
    strides: An integer or tuple/list of 2 integers,
      specifying the strides of the pooling operation.
      Can be a single integer to specify the same value for
      all spatial dimensions.
    padding: A string. The padding method, either 'valid' or 'same'.
      Case-insensitive.
    data_format: A string. The ordering of the dimensions in the inputs.
      `channels_last` (default) and `channels_first` are supported.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, height, width)`.
    name: A string, the name of the layer.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is not compatible with eager execution or `tf.function`.

  Please refer to [tf.layers section of the migration guide]
  (https://www.tensorflow.org/guide/migrate#models_based_on_tflayers)
  to migrate a TensorFlow v1 model to Keras. The corresponding TensorFlow v2
  layer is `tf.keras.layers.MaxPooling2D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.max_pooling2d(x, pool_size=2, strides=2)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.max_pooling2d` is deprecated and '
      'will be removed in a future version. '
      'Please use `tf.keras.layers.MaxPooling2D` instead.',
      stacklevel=2)
  layer = MaxPooling2D(pool_size=pool_size, strides=strides,
                       padding=padding, data_format=data_format,
                       name=name)
  return layer.apply(inputs)


</source>
<source file="systems/keras-2.7.0/keras/legacy_tf_layers/pooling.py" startline="636" endline="711" pcid="3149">
def average_pooling3d(inputs,
                      pool_size, strides,
                      padding='valid', data_format='channels_last',
                      name=None):
  """Average pooling layer for 3D inputs (e.g. volumes).

  Args:
    inputs: The tensor over which to pool. Must have rank 5.
    pool_size: An integer or tuple/list of 3 integers:
      (pool_depth, pool_height, pool_width)
      specifying the size of the pooling window.
      Can be a single integer to specify the same value for
      all spatial dimensions.
    strides: An integer or tuple/list of 3 integers,
      specifying the strides of the pooling operation.
      Can be a single integer to specify the same value for
      all spatial dimensions.
    padding: A string. The padding method, either 'valid' or 'same'.
      Case-insensitive.
    data_format: A string. The ordering of the dimensions in the inputs.
      `channels_last` (default) and `channels_first` are supported.
      `channels_last` corresponds to inputs with shape
      `(batch, depth, height, width, channels)` while `channels_first`
      corresponds to inputs with shape
      `(batch, channels, depth, height, width)`.
    name: A string, the name of the layer.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is not compatible with eager execution or `tf.function`.

  Please refer to [tf.layers section of the migration guide]
  (https://www.tensorflow.org/guide/migrate#models_based_on_tflayers)
  to migrate a TensorFlow v1 model to Keras. The corresponding TensorFlow v2
  layer is `tf.keras.layers.AveragePooling3D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.average_pooling3d(x, pool_size=2, strides=2)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.AveragePooling3D(pool_size=2, strides=2)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.average_pooling3d` is deprecated and '
      'will be removed in a future version. '
      'Please use `tf.keras.layers.AveragePooling3D` instead.',
      stacklevel=2)
  layer = AveragePooling3D(pool_size=pool_size, strides=strides,
                           padding=padding, data_format=data_format,
                           name=name)
  return layer.apply(inputs)


</source>
</class>

<class classid="142" nclones="2" nlines="19" similarity="80">
<source file="systems/keras-2.7.0/keras/testing_utils.py" startline="651" endline="675" pcid="3216">
  def call(self, inputs, **kwargs):
    if self._shared_input_branch:
      for layer in self._shared_input_branch:
        inputs = layer(inputs)
      a = inputs
      b = inputs
    elif isinstance(inputs, dict):
      a = inputs['input_1']
      b = inputs['input_2']
    else:
      a, b = inputs

    for layer in self._branch_a:
      a = layer(a)
    for layer in self._branch_b:
      b = layer(b)
    outs = [a, b]

    if self._shared_output_branch:
      for layer in self._shared_output_branch:
        outs = layer(outs)

    return outs


</source>
<source file="systems/keras-2.7.0/keras/testing_utils.py" startline="702" endline="723" pcid="3219">
  def call(self, inputs, **kwargs):
    if self._shared_input_branch:
      for layer in self._shared_input_branch:
        inputs = layer(inputs)
      a = inputs
      b = inputs
    else:
      a, b = inputs

    for layer in self._branch_a:
      a = layer(a)
    for layer in self._branch_b:
      b = layer(b)
    outs = a, b

    if self._shared_output_branch:
      for layer in self._shared_output_branch:
        outs = layer(outs)

    return outs


</source>
</class>

<class classid="143" nclones="2" nlines="23" similarity="75">
<source file="systems/keras-2.7.0/keras/optimizers_test.py" startline="160" endline="186" pcid="3251">
  def test_tf_optimizer(self):
    if tf.executing_eagerly():
      self.skipTest(
          'v1 optimizer does not run in eager mode')
    optimizer = optimizer_v1.TFOptimizer(AdamOptimizer(0.01))
    model = keras.models.Sequential()
    model.add(keras.layers.Dense(
        2, input_shape=(3,), kernel_constraint=keras.constraints.MaxNorm(1)))
    # This is possible
    model.compile(
        loss='mean_squared_error',
        optimizer=optimizer,
        run_eagerly=testing_utils.should_run_eagerly())
    keras.backend.track_tf_optimizer(optimizer)
    model.fit(np.random.random((5, 3)),
              np.random.random((5, 2)),
              epochs=1,
              batch_size=5,
              verbose=0)
    # not supported
    with self.assertRaises(NotImplementedError):
      _ = optimizer.weights
    with self.assertRaises(NotImplementedError):
      optimizer.get_config()
    with self.assertRaises(NotImplementedError):
      optimizer.from_config(None)

</source>
<source file="systems/keras-2.7.0/keras/optimizers_test.py" startline="203" endline="225" pcid="3253">
  def test_tf_optimizer_iterations(self):
    if tf.executing_eagerly():
      self.skipTest(
          'v1 optimizer does not run in eager mode')
    with self.cached_session():
      optimizer = optimizer_v1.TFOptimizer(AdamOptimizer(0.01))
      model = keras.models.Sequential()
      model.add(keras.layers.Dense(
          2, input_shape=(3,), kernel_constraint=keras.constraints.MaxNorm(1)))
      model.compile(
          loss='mean_squared_error',
          optimizer=optimizer,
          run_eagerly=testing_utils.should_run_eagerly())
      keras.backend.track_tf_optimizer(optimizer)
      self.assertEqual(keras.backend.get_value(model.optimizer.iterations), 0)

      model.fit(np.random.random((55, 3)),
                np.random.random((55, 2)),
                epochs=1,
                batch_size=5,
                verbose=0)
      self.assertEqual(keras.backend.get_value(model.optimizer.iterations), 11)

</source>
</class>

<class classid="144" nclones="32" nlines="14" similarity="75">
<source file="systems/keras-2.7.0/keras/benchmarks/keras_examples_benchmarks/text_classification_transformer_benchmark_test.py" startline="68" endline="84" pcid="3296">
  def benchmark_text_classification_bs_128(self):
    """Measure performance with batch_size=128."""
    batch_size = 128
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.imdb_x,
        y=self.imdb_y,
        batch_size=batch_size,
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata(
        'transformer', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</source>
<source file="systems/keras-2.7.0/keras/benchmarks/keras_examples_benchmarks/reuters_mlp_benchmark_test.py" startline="114" endline="136" pcid="3352">
  def benchmark_mlp_reuters_bs_512_gpu_2(self):
    """Measure performance with batch_size=512, gpu=2 and

    distribution_strategy='mirrored'
    """
    batch_size = 512
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        num_gpus=2,
        distribution_strategy='mirrored',
        epochs=self.epochs,
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('mlp', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)


</source>
<source file="systems/keras-2.7.0/keras/benchmarks/keras_examples_benchmarks/mnist_conv_benchmark_test.py" startline="114" endline="136" pcid="3358">
  def benchmark_conv_mnist_bs_512_gpu_2(self):
    """Measure performance with batch_size=512, gpu=2 and

    distribution_strategy='mirrored'
    """
    batch_size = 512
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        num_gpus=2,
        distribution_strategy='mirrored',
        epochs=self.epochs,
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('conv', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)


</source>
<source file="systems/keras-2.7.0/keras/benchmarks/keras_examples_benchmarks/bidirectional_lstm_benchmark_test.py" startline="109" endline="131" pcid="3364">
  def benchmark_bidirect_lstm_imdb_bs_512_gpu_2(self):
    """Measure performance with batch_size=512, gpu=2 and

    distribution_strategy=`mirrored`.
    """
    batch_size = 512
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.imdb_x,
        y=self.imdb_y,
        batch_size=batch_size,
        num_gpus=2,
        distribution_strategy='mirrored',
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata(
        'bidirectional_lstm', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)


</source>
<source file="systems/keras-2.7.0/keras/benchmarks/keras_examples_benchmarks/text_classification_transformer_benchmark_test.py" startline="102" endline="118" pcid="3298">
  def benchmark_text_classification_bs_512(self):
    """Measure performance with batch_size=512."""
    batch_size = 512
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.imdb_x,
        y=self.imdb_y,
        batch_size=batch_size,
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata(
        'transformer', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</source>
<source file="systems/keras-2.7.0/keras/benchmarks/keras_examples_benchmarks/text_classification_transformer_benchmark_test.py" startline="85" endline="101" pcid="3297">
  def benchmark_text_classification_bs_256(self):
    """Measure performance with batch_size=256."""
    batch_size = 256
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.imdb_x,
        y=self.imdb_y,
        batch_size=batch_size,
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata(
        'transformer', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</source>
<source file="systems/keras-2.7.0/keras/benchmarks/keras_examples_benchmarks/mnist_hierarchical_rnn_benchmark_test.py" startline="115" endline="137" pcid="3319">
  def benchmark_hrnn_mnist_bs_1024_gpu_2(self):
    """Measure performance with batch_size=1024, gpu=2 and

    distribution_strategy='mirrored'
    """
    batch_size = 1024
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        num_gpus=2,
        distribution_strategy='mirrored',
        optimizer='rmsprop',
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata(
        'hierarchical_rnn', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)


</source>
<source file="systems/keras-2.7.0/keras/benchmarks/keras_examples_benchmarks/antirectifier_benchmark_test.py" startline="107" endline="129" pcid="3325">
  def benchmark_antirectifier_bs_512_gpu_2(self):
    """Measure performance with batch_size=512, gpu=2 and

    distribution_strategy=`mirrored`.
    """
    batch_size = 512
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        num_gpus=2,
        distribution_strategy="mirrored",
        optimizer="rmsprop",
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=["sparse_categorical_accuracy"])

    metadata = benchmark_util.get_keras_examples_metadata(
        "antirectifier", batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)


</source>
<source file="systems/keras-2.7.0/keras/benchmarks/keras_examples_benchmarks/mnist_irnn_benchmark_test.py" startline="96" endline="111" pcid="3312">
  def benchmark_irnn_mnist_bs_1024(self):
    """Measure performance with batch_size=1024."""
    batch_size = 1024
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        optimizer=tf.keras.optimizers.RMSprop(learning_rate=self.learning_rate),
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('irnn', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</source>
<source file="systems/keras-2.7.0/keras/benchmarks/keras_examples_benchmarks/text_classification_transformer_benchmark_test.py" startline="119" endline="141" pcid="3299">
  def benchmark_text_classification_bs_512_gpu_2(self):
    """Measure performance with batch_size=512, gpu=1 and

    distribution_strategy='mirrored'
    """
    batch_size = 512
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.imdb_x,
        y=self.imdb_y,
        batch_size=batch_size,
        num_gpus=2,
        distribution_strategy='mirrored',
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata(
        'transformer', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)


</source>
<source file="systems/keras-2.7.0/keras/benchmarks/keras_examples_benchmarks/mnist_hierarchical_rnn_benchmark_test.py" startline="64" endline="80" pcid="3316">
  def benchmark_hrnn_mnist_bs_256(self):
    """Measure performance with batch_size=256."""
    batch_size = 256
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        optimizer='rmsprop',
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata(
        'hierarchical_rnn', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</source>
<source file="systems/keras-2.7.0/keras/benchmarks/keras_examples_benchmarks/mnist_hierarchical_rnn_benchmark_test.py" startline="81" endline="97" pcid="3317">
  def benchmark_hrnn_mnist_bs_512(self):
    """Measure performance with batch_size=512."""
    batch_size = 512
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        optimizer='rmsprop',
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata(
        'hierarchical_rnn', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</source>
<source file="systems/keras-2.7.0/keras/benchmarks/keras_examples_benchmarks/cifar10_cnn_benchmark_test.py" startline="123" endline="145" pcid="3335">
  def benchmark_cnn_cifar10_bs_1024_gpu_2(self):
    """Measure performance with batch_size=1024, gpu=2 and

    distribution_strategy=`mirrored`.
    """
    batch_size = 1024
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        num_gpus=2,
        distribution_strategy='mirrored',
        epochs=self.epochs,
        optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6),
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('cnn', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)


</source>
<source file="systems/keras-2.7.0/keras/benchmarks/keras_examples_benchmarks/mnist_conv_benchmark_test.py" startline="80" endline="96" pcid="3356">
  def benchmark_conv_mnist_bs_256(self):
    """Measure performance with batch_size=256."""
    batch_size = 256
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        epochs=self.epochs,
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('conv', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</source>
<source file="systems/keras-2.7.0/keras/benchmarks/keras_examples_benchmarks/mnist_hierarchical_rnn_benchmark_test.py" startline="98" endline="114" pcid="3318">
  def benchmark_hrnn_mnist_bs_1024(self):
    """Measure performance with batch_size=1024."""
    batch_size = 1024
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        optimizer='rmsprop',
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata(
        'hierarchical_rnn', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</source>
<source file="systems/keras-2.7.0/keras/benchmarks/keras_examples_benchmarks/antirectifier_benchmark_test.py" startline="56" endline="72" pcid="3322">
  def benchmark_antirectifier_bs_128(self):
    """Measure performance with batch_size=128."""
    batch_size = 128
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        optimizer="rmsprop",
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=["sparse_categorical_accuracy"])

    metadata = benchmark_util.get_keras_examples_metadata(
        "antirectifier", batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</source>
<source file="systems/keras-2.7.0/keras/benchmarks/keras_examples_benchmarks/cifar10_cnn_benchmark_test.py" startline="72" endline="88" pcid="3332">
  def benchmark_cnn_cifar10_bs_256(self):
    """Measure performance with batch_size=256."""
    batch_size = 256
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        epochs=self.epochs,
        optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6),
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('cnn', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</source>
<source file="systems/keras-2.7.0/keras/benchmarks/keras_examples_benchmarks/mnist_conv_benchmark_test.py" startline="63" endline="79" pcid="3355">
  def benchmark_conv_mnist_bs_128(self):
    """Measure performance with batch_size=128."""
    batch_size = 128
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        epochs=self.epochs,
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('conv', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</source>
<source file="systems/keras-2.7.0/keras/benchmarks/keras_examples_benchmarks/cifar10_cnn_benchmark_test.py" startline="89" endline="105" pcid="3333">
  def benchmark_cnn_cifar10_bs_512(self):
    """Measure performance with batch_size=512."""
    batch_size = 512
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        epochs=self.epochs,
        optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6),
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('cnn', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</source>
<source file="systems/keras-2.7.0/keras/benchmarks/keras_examples_benchmarks/cifar10_cnn_benchmark_test.py" startline="106" endline="122" pcid="3334">
  def benchmark_cnn_cifar10_bs_1024(self):
    """Measure performance with batch_size=1024."""
    batch_size = 1024
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        epochs=self.epochs,
        optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6),
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('cnn', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</source>
<source file="systems/keras-2.7.0/keras/benchmarks/keras_examples_benchmarks/mnist_conv_benchmark_test.py" startline="97" endline="113" pcid="3357">
  def benchmark_conv_mnist_bs_512(self):
    """Measure performance with batch_size=512."""
    batch_size = 512
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        epochs=self.epochs,
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('conv', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</source>
<source file="systems/keras-2.7.0/keras/benchmarks/keras_examples_benchmarks/bidirectional_lstm_benchmark_test.py" startline="58" endline="74" pcid="3361">
  def benchmark_bidirect_lstm_imdb_bs_128(self):
    """Measure performance with batch_size=128."""
    batch_size = 128
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.imdb_x,
        y=self.imdb_y,
        batch_size=batch_size,
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata(
        'bidirectional_lstm', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</source>
<source file="systems/keras-2.7.0/keras/benchmarks/keras_examples_benchmarks/antirectifier_benchmark_test.py" startline="90" endline="106" pcid="3324">
  def benchmark_antirectifier_bs_512(self):
    """Measure performance with batch_size=512."""
    batch_size = 512
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        optimizer="rmsprop",
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=["sparse_categorical_accuracy"])

    metadata = benchmark_util.get_keras_examples_metadata(
        "antirectifier", batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</source>
<source file="systems/keras-2.7.0/keras/benchmarks/keras_examples_benchmarks/reuters_mlp_benchmark_test.py" startline="63" endline="79" pcid="3349">
  def benchmark_mlp_reuters_bs_128(self):
    """Measure performance with batch_size=128."""
    batch_size = 128
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        epochs=self.epochs,
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('mlp', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</source>
<source file="systems/keras-2.7.0/keras/benchmarks/keras_examples_benchmarks/reuters_mlp_benchmark_test.py" startline="80" endline="96" pcid="3350">
  def benchmark_mlp_reuters_bs_256(self):
    """Measure performance with batch_size=256."""
    batch_size = 256
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        epochs=self.epochs,
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('mlp', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</source>
<source file="systems/keras-2.7.0/keras/benchmarks/keras_examples_benchmarks/mnist_irnn_benchmark_test.py" startline="80" endline="95" pcid="3311">
  def benchmark_irnn_mnist_bs_512(self):
    """Measure performance with batch_size=512."""
    batch_size = 512
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        optimizer=tf.keras.optimizers.RMSprop(learning_rate=self.learning_rate),
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('irnn', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</source>
<source file="systems/keras-2.7.0/keras/benchmarks/keras_examples_benchmarks/reuters_mlp_benchmark_test.py" startline="97" endline="113" pcid="3351">
  def benchmark_mlp_reuters_bs_512(self):
    """Measure performance with batch_size=512."""
    batch_size = 512
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        epochs=self.epochs,
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('mlp', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</source>
<source file="systems/keras-2.7.0/keras/benchmarks/keras_examples_benchmarks/mnist_irnn_benchmark_test.py" startline="112" endline="133" pcid="3313">
  def benchmark_irnn_mnist_bs_1024_gpu_2(self):
    """Measure performance with batch_size=1024, gpu=2 and

    distribution_strategy='mirrored'
    """
    batch_size = 1024
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        num_gpus=2,
        distribution_strategy='mirrored',
        optimizer=tf.keras.optimizers.RMSprop(learning_rate=self.learning_rate),
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('irnn', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)


</source>
<source file="systems/keras-2.7.0/keras/benchmarks/keras_examples_benchmarks/mnist_irnn_benchmark_test.py" startline="64" endline="79" pcid="3310">
  def benchmark_irnn_mnist_bs_256(self):
    """Measure performance with batch_size=256."""
    batch_size = 256
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        optimizer=tf.keras.optimizers.RMSprop(learning_rate=self.learning_rate),
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('irnn', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</source>
<source file="systems/keras-2.7.0/keras/benchmarks/keras_examples_benchmarks/antirectifier_benchmark_test.py" startline="73" endline="89" pcid="3323">
  def benchmark_antirectifier_bs_256(self):
    """Measure performance with batch_size=256."""
    batch_size = 256
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        optimizer="rmsprop",
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=["sparse_categorical_accuracy"])

    metadata = benchmark_util.get_keras_examples_metadata(
        "antirectifier", batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</source>
<source file="systems/keras-2.7.0/keras/benchmarks/keras_examples_benchmarks/bidirectional_lstm_benchmark_test.py" startline="75" endline="91" pcid="3362">
  def benchmark_bidirect_lstm_imdb_bs_256(self):
    """Measure performance with batch_size=256."""
    batch_size = 256
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.imdb_x,
        y=self.imdb_y,
        batch_size=batch_size,
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata(
        'bidirectional_lstm', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</source>
<source file="systems/keras-2.7.0/keras/benchmarks/keras_examples_benchmarks/bidirectional_lstm_benchmark_test.py" startline="92" endline="108" pcid="3363">
  def benchmark_bidirect_lstm_imdb_bs_512(self):
    """Measure performance with batch_size=512."""
    batch_size = 512
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.imdb_x,
        y=self.imdb_y,
        batch_size=batch_size,
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata(
        'bidirectional_lstm', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</source>
</class>

<class classid="145" nclones="2" nlines="12" similarity="100">
<source file="systems/keras-2.7.0/keras/benchmarks/keras_examples_benchmarks/mnist_conv_custom_training_benchmark_test.py" startline="45" endline="59" pcid="3337">
  def _build_model(self):
    """Model from https://keras.io/examples/vision/mnist_convnet/."""
    model = tf.keras.Sequential([
        tf.keras.Input(shape=self.input_shape),
        tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(self.num_classes, activation='softmax'),
    ])

    return model

</source>
<source file="systems/keras-2.7.0/keras/benchmarks/keras_examples_benchmarks/mnist_conv_benchmark_test.py" startline="40" endline="62" pcid="3354">
  def _build_model(self):
    """Model from https://keras.io/examples/vision/mnist_convnet/."""
    model = tf.keras.Sequential([
        tf.keras.Input(shape=self.input_shape),
        tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(self.num_classes, activation='softmax'),
    ])
    return model

  # In each benchmark test, the required arguments for the
  # method `measure_performance` include:
  #   x: Input data, it could be Numpy or loaded from tfds.
  #   y: Target data. If `x` is a dataset or generator instance,
  #      `y` should not be specified.
  #   loss: Loss function for model.
  #   optimizer: Optimizer for model.
  #   Check more details in `measure_performance()` method of
  #   benchmark_util.
</source>
</class>

<class classid="146" nclones="3" nlines="16" similarity="100">
<source file="systems/keras-2.7.0/keras/benchmarks/keras_examples_benchmarks/mnist_conv_custom_training_benchmark_test.py" startline="266" endline="287" pcid="3343">
    return metrics, wall_time

  def benchmark_custom_training_mnist_bs_128(self):
    """Measure performance with batch_size=128 and run_iters=5."""
    batch_size = 128
    run_iters = 5
    train_dataset = self.train_dataset.shuffle(
        buffer_size=1024).batch(batch_size)

    # Instantiate a loss function.
    loss_fn = tf.keras.losses.CategoricalCrossentropy(
        reduction=tf.keras.losses.Reduction.NONE)
    # Instantiate an optimizer to train the model.
    optimizer = tf.keras.optimizers.Adam()
    model = self._build_model()

    metrics, wall_time = self.measure_performance(model, train_dataset, loss_fn,
                                                  optimizer, batch_size,
                                                  run_iters, self.epochs)
    extras = benchmark_util.get_keras_examples_metadata('conv', batch_size,
                                                        '.keras.ctl_graph')
    self.report_benchmark(
</source>
<source file="systems/keras-2.7.0/keras/benchmarks/keras_examples_benchmarks/mnist_conv_custom_training_benchmark_test.py" startline="288" endline="309" pcid="3344">
        iters=run_iters, wall_time=wall_time, metrics=metrics, extras=extras)

  def benchmark_custom_training_mnist_bs_256(self):
    """Measure performance with batch_size=256 and run_iters=5."""
    batch_size = 256
    run_iters = 5
    train_dataset = self.train_dataset.shuffle(
        buffer_size=1024).batch(batch_size)

    # Instantiate a loss function.
    loss_fn = tf.keras.losses.CategoricalCrossentropy(
        reduction=tf.keras.losses.Reduction.NONE)
    # Instantiate an optimizer to train the model.
    optimizer = tf.keras.optimizers.Adam()
    model = self._build_model()

    metrics, wall_time = self.measure_performance(model, train_dataset, loss_fn,
                                                  optimizer, batch_size,
                                                  run_iters, self.epochs)
    extras = benchmark_util.get_keras_examples_metadata('conv', batch_size,
                                                        '.keras.ctl_graph')
    self.report_benchmark(
</source>
<source file="systems/keras-2.7.0/keras/benchmarks/keras_examples_benchmarks/mnist_conv_custom_training_benchmark_test.py" startline="310" endline="331" pcid="3345">
        iters=run_iters, wall_time=wall_time, metrics=metrics, extras=extras)

  def benchmark_custom_training_mnist_bs_512(self):
    """Measure performance with batch_size=512 and run_iters=10."""
    batch_size = 512
    run_iters = 5
    train_dataset = self.train_dataset.shuffle(
        buffer_size=1024).batch(batch_size)

    # Instantiate a loss function.
    loss_fn = tf.keras.losses.CategoricalCrossentropy(
        reduction=tf.keras.losses.Reduction.NONE)
    # Instantiate an optimizer to train the model.
    optimizer = tf.keras.optimizers.Adam()
    model = self._build_model()

    metrics, wall_time = self.measure_performance(model, train_dataset, loss_fn,
                                                  optimizer, batch_size,
                                                  run_iters, self.epochs)
    extras = benchmark_util.get_keras_examples_metadata('conv', batch_size,
                                                        '.keras.ctl_graph')
    self.report_benchmark(
</source>
</class>

<class classid="147" nclones="8" nlines="12" similarity="100">
<source file="systems/keras-2.7.0/keras/benchmarks/saved_model_benchmarks/densenet_benchmark_test.py" startline="27" endline="41" pcid="3385">
  def benchmark_save_and_load_densenet_201(self):
    app = tf.keras.applications.DenseNet201
    save_result, load_result = (
        saved_model_benchmark_util.save_and_load_benchmark(app))

    self.report_benchmark(
        iters=save_result['iters'],
        wall_time=save_result['wall_time'],
        name=save_result['name'])

    self.report_benchmark(
        iters=load_result['iters'],
        wall_time=load_result['wall_time'],
        name=load_result['name'])

</source>
<source file="systems/keras-2.7.0/keras/benchmarks/saved_model_benchmarks/mobilenet_benchmark_test.py" startline="27" endline="41" pcid="3391">
  def benchmark_save_and_load_mobilenet_v2(self):
    app = tf.keras.applications.MobileNetV2
    save_result, load_result = (
        saved_model_benchmark_util.save_and_load_benchmark(app))

    self.report_benchmark(
        iters=save_result['iters'],
        wall_time=save_result['wall_time'],
        name=save_result['name'])

    self.report_benchmark(
        iters=load_result['iters'],
        wall_time=load_result['wall_time'],
        name=load_result['name'])

</source>
<source file="systems/keras-2.7.0/keras/benchmarks/saved_model_benchmarks/efficientnet_benchmark_test.py" startline="27" endline="41" pcid="3387">
  def benchmark_save_and_load_efficient_net_b7(self):
    app = tf.keras.applications.EfficientNetB7
    save_result, load_result = (
        saved_model_benchmark_util.save_and_load_benchmark(app))

    self.report_benchmark(
        iters=save_result['iters'],
        wall_time=save_result['wall_time'],
        name=save_result['name'])

    self.report_benchmark(
        iters=load_result['iters'],
        wall_time=load_result['wall_time'],
        name=load_result['name'])

</source>
<source file="systems/keras-2.7.0/keras/benchmarks/saved_model_benchmarks/resnet152_v2_benchmark_test.py" startline="27" endline="42" pcid="3389">
  def benchmark_save_and_load_resnet152_v2(self):
    app = tf.keras.applications.ResNet152V2
    save_result, load_result = (
        saved_model_benchmark_util.save_and_load_benchmark(app))

    self.report_benchmark(
        iters=save_result['iters'],
        wall_time=save_result['wall_time'],
        name=save_result['name'])

    self.report_benchmark(
        iters=load_result['iters'],
        wall_time=load_result['wall_time'],
        name=load_result['name'])


</source>
<source file="systems/keras-2.7.0/keras/benchmarks/saved_model_benchmarks/xception_benchmark_test.py" startline="27" endline="42" pcid="3386">
  def benchmark_save_and_load_xception(self):
    app = tf.keras.applications.Xception
    save_result, load_result = (
        saved_model_benchmark_util.save_and_load_benchmark(app))

    self.report_benchmark(
        iters=save_result['iters'],
        wall_time=save_result['wall_time'],
        name=save_result['name'])

    self.report_benchmark(
        iters=load_result['iters'],
        wall_time=load_result['wall_time'],
        name=load_result['name'])


</source>
<source file="systems/keras-2.7.0/keras/benchmarks/saved_model_benchmarks/vgg_benchmark_test.py" startline="27" endline="42" pcid="3392">
  def benchmark_save_and_load_vgg19(self):
    app = tf.keras.applications.VGG19
    save_result, load_result = (
        saved_model_benchmark_util.save_and_load_benchmark(app))

    self.report_benchmark(
        iters=save_result['iters'],
        wall_time=save_result['wall_time'],
        name=save_result['name'])

    self.report_benchmark(
        iters=load_result['iters'],
        wall_time=load_result['wall_time'],
        name=load_result['name'])


</source>
<source file="systems/keras-2.7.0/keras/benchmarks/saved_model_benchmarks/inception_resnet_v2_benchmark_test.py" startline="27" endline="42" pcid="3393">
  def benchmark_save_and_load_inception_resnet_v2(self):
    app = tf.keras.applications.InceptionResNetV2
    save_result, load_result = (
        saved_model_benchmark_util.save_and_load_benchmark(app))

    self.report_benchmark(
        iters=save_result['iters'],
        wall_time=save_result['wall_time'],
        name=save_result['name'])

    self.report_benchmark(
        iters=load_result['iters'],
        wall_time=load_result['wall_time'],
        name=load_result['name'])


</source>
<source file="systems/keras-2.7.0/keras/benchmarks/saved_model_benchmarks/nasnet_large_benchmark_test.py" startline="27" endline="41" pcid="3388">
  def benchmark_save_and_load_nasnet_large(self):
    app = tf.keras.applications.NASNetLarge
    save_result, load_result = (
        saved_model_benchmark_util.save_and_load_benchmark(app))

    self.report_benchmark(
        iters=save_result['iters'],
        wall_time=save_result['wall_time'],
        name=save_result['name'])

    self.report_benchmark(
        iters=load_result['iters'],
        wall_time=load_result['wall_time'],
        name=load_result['name'])

</source>
</class>

<class classid="148" nclones="2" nlines="10" similarity="80">
<source file="systems/keras-2.7.0/keras/benchmarks/model_components_benchmarks_test.py" startline="149" endline="161" pcid="3405">
  def _benchmark_keras_model_fit(self, model, run_eagerly=False):
    data = tf.random.uniform((10, 10), minval=-1, maxval=1)
    labels = tf.random.uniform((10, 10), minval=-1, maxval=1)
    dataset = tf.data.Dataset.from_tensors((data, labels)).repeat()
    model.compile(
        "sgd",
        loss="mse", run_eagerly=run_eagerly)
    func = lambda: model.fit(dataset, epochs=1, steps_per_epoch=1000, verbose=0)
    # First call is more expensive (creates variables etc.), discount that.
    model.fit(dataset, epochs=1, steps_per_epoch=1, verbose=0)

    self._run(func, 1)

</source>
<source file="systems/keras-2.7.0/keras/benchmarks/model_components_benchmarks_test.py" startline="162" endline="174" pcid="3406">
  def _benchmark_keras_model_evaluate(self, model, run_eagerly=False):
    data = tf.random.uniform((10, 10), minval=-1, maxval=1)
    labels = tf.random.uniform((10, 10), minval=-1, maxval=1)
    dataset = tf.data.Dataset.from_tensors((data, labels)).repeat()
    model.compile(
        "sgd",
        loss="mse", run_eagerly=run_eagerly)
    func = lambda: model.evaluate(dataset, steps=1000, verbose=0)
    # First call is more expensive (creates variables etc.), discount that.
    model.evaluate(dataset, steps=1, verbose=0)

    self._run(func, 1)

</source>
</class>

<class classid="149" nclones="4" nlines="11" similarity="100">
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="46" endline="59" pcid="3432">
  def test_unweighted(self):
    fp_obj = metrics.FalsePositives()
    self.evaluate(tf.compat.v1.variables_initializer(fp_obj.variables))

    y_true = tf.constant(((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),
                                   (1, 1, 1, 1, 0), (0, 0, 0, 0, 1)))
    y_pred = tf.constant(((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),
                                   (0, 1, 0, 1, 0), (1, 1, 1, 1, 1)))

    update_op = fp_obj.update_state(y_true, y_pred)
    self.evaluate(update_op)
    result = fp_obj.result()
    self.assertAllClose(7., result)

</source>
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="262" endline="275" pcid="3448">
  def test_unweighted(self):
    tp_obj = metrics.TruePositives()
    self.evaluate(tf.compat.v1.variables_initializer(tp_obj.variables))

    y_true = tf.constant(((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),
                                   (1, 1, 1, 1, 0), (0, 0, 0, 0, 1)))
    y_pred = tf.constant(((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),
                                   (0, 1, 0, 1, 0), (1, 1, 1, 1, 1)))

    update_op = tp_obj.update_state(y_true, y_pred)
    self.evaluate(update_op)
    result = tp_obj.result()
    self.assertAllClose(7., result)

</source>
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="194" endline="207" pcid="3443">
  def test_unweighted(self):
    tn_obj = metrics.TrueNegatives()
    self.evaluate(tf.compat.v1.variables_initializer(tn_obj.variables))

    y_true = tf.constant(((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),
                                   (1, 1, 1, 1, 0), (0, 0, 0, 0, 1)))
    y_pred = tf.constant(((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),
                                   (0, 1, 0, 1, 0), (1, 1, 1, 1, 1)))

    update_op = tn_obj.update_state(y_true, y_pred)
    self.evaluate(update_op)
    result = tn_obj.result()
    self.assertAllClose(3., result)

</source>
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="126" endline="139" pcid="3438">
  def test_unweighted(self):
    fn_obj = metrics.FalseNegatives()
    self.evaluate(tf.compat.v1.variables_initializer(fn_obj.variables))

    y_true = tf.constant(((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),
                                   (1, 1, 1, 1, 0), (0, 0, 0, 0, 1)))
    y_pred = tf.constant(((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),
                                   (0, 1, 0, 1, 0), (1, 1, 1, 1, 1)))

    update_op = fn_obj.update_state(y_true, y_pred)
    self.evaluate(update_op)
    result = fn_obj.result()
    self.assertAllClose(3., result)

</source>
</class>

<class classid="150" nclones="4" nlines="10" similarity="100">
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="60" endline="70" pcid="3433">
  def test_weighted(self):
    fp_obj = metrics.FalsePositives()
    self.evaluate(tf.compat.v1.variables_initializer(fp_obj.variables))
    y_true = tf.constant(((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),
                                   (1, 1, 1, 1, 0), (0, 0, 0, 0, 1)))
    y_pred = tf.constant(((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),
                                   (0, 1, 0, 1, 0), (1, 1, 1, 1, 1)))
    sample_weight = tf.constant((1., 1.5, 2., 2.5))
    result = fp_obj(y_true, y_pred, sample_weight=sample_weight)
    self.assertAllClose(14., self.evaluate(result))

</source>
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="276" endline="286" pcid="3449">
  def test_weighted(self):
    tp_obj = metrics.TruePositives()
    self.evaluate(tf.compat.v1.variables_initializer(tp_obj.variables))
    y_true = tf.constant(((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),
                                   (1, 1, 1, 1, 0), (0, 0, 0, 0, 1)))
    y_pred = tf.constant(((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),
                                   (0, 1, 0, 1, 0), (1, 1, 1, 1, 1)))
    sample_weight = tf.constant((1., 1.5, 2., 2.5))
    result = tp_obj(y_true, y_pred, sample_weight=sample_weight)
    self.assertAllClose(12., self.evaluate(result))

</source>
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="208" endline="218" pcid="3444">
  def test_weighted(self):
    tn_obj = metrics.TrueNegatives()
    self.evaluate(tf.compat.v1.variables_initializer(tn_obj.variables))
    y_true = tf.constant(((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),
                                   (1, 1, 1, 1, 0), (0, 0, 0, 0, 1)))
    y_pred = tf.constant(((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),
                                   (0, 1, 0, 1, 0), (1, 1, 1, 1, 1)))
    sample_weight = tf.constant((1., 1.5, 2., 2.5))
    result = tn_obj(y_true, y_pred, sample_weight=sample_weight)
    self.assertAllClose(4., self.evaluate(result))

</source>
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="140" endline="150" pcid="3439">
  def test_weighted(self):
    fn_obj = metrics.FalseNegatives()
    self.evaluate(tf.compat.v1.variables_initializer(fn_obj.variables))
    y_true = tf.constant(((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),
                                   (1, 1, 1, 1, 0), (0, 0, 0, 0, 1)))
    y_pred = tf.constant(((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),
                                   (0, 1, 0, 1, 0), (1, 1, 1, 1, 1)))
    sample_weight = tf.constant((1., 1.5, 2., 2.5))
    result = fn_obj(y_true, y_pred, sample_weight=sample_weight)
    self.assertAllClose(5., self.evaluate(result))

</source>
</class>

<class classid="151" nclones="4" nlines="11" similarity="100">
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="71" endline="84" pcid="3434">
  def test_unweighted_with_thresholds(self):
    fp_obj = metrics.FalsePositives(thresholds=[0.15, 0.5, 0.85])
    self.evaluate(tf.compat.v1.variables_initializer(fp_obj.variables))

    y_pred = tf.constant(((0.9, 0.2, 0.8, 0.1), (0.2, 0.9, 0.7, 0.6),
                                   (0.1, 0.2, 0.4, 0.3), (0, 1, 0.7, 0.3)))
    y_true = tf.constant(((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0),
                                   (1, 1, 1, 1)))

    update_op = fp_obj.update_state(y_true, y_pred)
    self.evaluate(update_op)
    result = fp_obj.result()
    self.assertAllClose([7., 4., 2.], result)

</source>
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="287" endline="300" pcid="3450">
  def test_unweighted_with_thresholds(self):
    tp_obj = metrics.TruePositives(thresholds=[0.15, 0.5, 0.85])
    self.evaluate(tf.compat.v1.variables_initializer(tp_obj.variables))

    y_pred = tf.constant(((0.9, 0.2, 0.8, 0.1), (0.2, 0.9, 0.7, 0.6),
                                   (0.1, 0.2, 0.4, 0.3), (0, 1, 0.7, 0.3)))
    y_true = tf.constant(((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0),
                                   (1, 1, 1, 1)))

    update_op = tp_obj.update_state(y_true, y_pred)
    self.evaluate(update_op)
    result = tp_obj.result()
    self.assertAllClose([6., 3., 1.], result)

</source>
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="219" endline="232" pcid="3445">
  def test_unweighted_with_thresholds(self):
    tn_obj = metrics.TrueNegatives(thresholds=[0.15, 0.5, 0.85])
    self.evaluate(tf.compat.v1.variables_initializer(tn_obj.variables))

    y_pred = tf.constant(((0.9, 0.2, 0.8, 0.1), (0.2, 0.9, 0.7, 0.6),
                                   (0.1, 0.2, 0.4, 0.3), (0, 1, 0.7, 0.3)))
    y_true = tf.constant(((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0),
                                   (1, 1, 1, 1)))

    update_op = tn_obj.update_state(y_true, y_pred)
    self.evaluate(update_op)
    result = tn_obj.result()
    self.assertAllClose([2., 5., 7.], result)

</source>
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="151" endline="164" pcid="3440">
  def test_unweighted_with_thresholds(self):
    fn_obj = metrics.FalseNegatives(thresholds=[0.15, 0.5, 0.85])
    self.evaluate(tf.compat.v1.variables_initializer(fn_obj.variables))

    y_pred = tf.constant(((0.9, 0.2, 0.8, 0.1), (0.2, 0.9, 0.7, 0.6),
                                   (0.1, 0.2, 0.4, 0.3), (0, 1, 0.7, 0.3)))
    y_true = tf.constant(((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0),
                                   (1, 1, 1, 1)))

    update_op = fn_obj.update_state(y_true, y_pred)
    self.evaluate(update_op)
    result = fn_obj.result()
    self.assertAllClose([1., 4., 6.], result)

</source>
</class>

<class classid="152" nclones="3" nlines="11" similarity="81">
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="85" endline="98" pcid="3435">
  def test_weighted_with_thresholds(self):
    fp_obj = metrics.FalsePositives(thresholds=[0.15, 0.5, 0.85])
    self.evaluate(tf.compat.v1.variables_initializer(fp_obj.variables))

    y_pred = tf.constant(((0.9, 0.2, 0.8, 0.1), (0.2, 0.9, 0.7, 0.6),
                                   (0.1, 0.2, 0.4, 0.3), (0, 1, 0.7, 0.3)))
    y_true = tf.constant(((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0),
                                   (1, 1, 1, 1)))
    sample_weight = ((1.0, 2.0, 3.0, 5.0), (7.0, 11.0, 13.0, 17.0),
                     (19.0, 23.0, 29.0, 31.0), (5.0, 15.0, 10.0, 0))

    result = fp_obj(y_true, y_pred, sample_weight=sample_weight)
    self.assertAllClose([125., 42., 12.], self.evaluate(result))

</source>
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="165" endline="178" pcid="3441">
  def test_weighted_with_thresholds(self):
    fn_obj = metrics.FalseNegatives(thresholds=[0.15, 0.5, 0.85])
    self.evaluate(tf.compat.v1.variables_initializer(fn_obj.variables))

    y_pred = tf.constant(((0.9, 0.2, 0.8, 0.1), (0.2, 0.9, 0.7, 0.6),
                                   (0.1, 0.2, 0.4, 0.3), (0, 1, 0.7, 0.3)))
    y_true = tf.constant(((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0),
                                   (1, 1, 1, 1)))
    sample_weight = ((3.0,), (5.0,), (7.0,), (4.0,))

    result = fn_obj(y_true, y_pred, sample_weight=sample_weight)
    self.assertAllClose([4., 16., 23.], self.evaluate(result))


</source>
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="233" endline="246" pcid="3446">
  def test_weighted_with_thresholds(self):
    tn_obj = metrics.TrueNegatives(thresholds=[0.15, 0.5, 0.85])
    self.evaluate(tf.compat.v1.variables_initializer(tn_obj.variables))

    y_pred = tf.constant(((0.9, 0.2, 0.8, 0.1), (0.2, 0.9, 0.7, 0.6),
                                   (0.1, 0.2, 0.4, 0.3), (0, 1, 0.7, 0.3)))
    y_true = tf.constant(((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0),
                                   (1, 1, 1, 1)))
    sample_weight = ((0.0, 2.0, 3.0, 5.0),)

    result = tn_obj(y_true, y_pred, sample_weight=sample_weight)
    self.assertAllClose([5., 15., 23.], self.evaluate(result))


</source>
</class>

<class classid="153" nclones="2" nlines="16" similarity="100">
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="317" endline="335" pcid="3452">
  def test_config(self):
    p_obj = metrics.Precision(
        name='my_precision', thresholds=[0.4, 0.9], top_k=15, class_id=12)
    self.assertEqual(p_obj.name, 'my_precision')
    self.assertLen(p_obj.variables, 2)
    self.assertEqual([v.name for v in p_obj.variables],
                     ['true_positives:0', 'false_positives:0'])
    self.assertEqual(p_obj.thresholds, [0.4, 0.9])
    self.assertEqual(p_obj.top_k, 15)
    self.assertEqual(p_obj.class_id, 12)

    # Check save and restore config
    p_obj2 = metrics.Precision.from_config(p_obj.get_config())
    self.assertEqual(p_obj2.name, 'my_precision')
    self.assertLen(p_obj2.variables, 2)
    self.assertEqual(p_obj2.thresholds, [0.4, 0.9])
    self.assertEqual(p_obj2.top_k, 15)
    self.assertEqual(p_obj2.class_id, 12)

</source>
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="522" endline="540" pcid="3466">
  def test_config(self):
    r_obj = metrics.Recall(
        name='my_recall', thresholds=[0.4, 0.9], top_k=15, class_id=12)
    self.assertEqual(r_obj.name, 'my_recall')
    self.assertLen(r_obj.variables, 2)
    self.assertEqual([v.name for v in r_obj.variables],
                     ['true_positives:0', 'false_negatives:0'])
    self.assertEqual(r_obj.thresholds, [0.4, 0.9])
    self.assertEqual(r_obj.top_k, 15)
    self.assertEqual(r_obj.class_id, 12)

    # Check save and restore config
    r_obj2 = metrics.Recall.from_config(r_obj.get_config())
    self.assertEqual(r_obj2.name, 'my_recall')
    self.assertLen(r_obj2.variables, 2)
    self.assertEqual(r_obj2.thresholds, [0.4, 0.9])
    self.assertEqual(r_obj2.top_k, 15)
    self.assertEqual(r_obj2.class_id, 12)

</source>
</class>

<class classid="154" nclones="2" nlines="12" similarity="83">
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="336" endline="352" pcid="3453">
  def test_value_is_idempotent(self):
    p_obj = metrics.Precision(thresholds=[0.3, 0.72])
    y_pred = tf.random.uniform(shape=(10, 3))
    y_true = tf.random.uniform(shape=(10, 3))
    update_op = p_obj.update_state(y_true, y_pred)
    self.evaluate(tf.compat.v1.variables_initializer(p_obj.variables))

    # Run several updates.
    for _ in range(10):
      self.evaluate(update_op)

    # Then verify idempotency.
    initial_precision = self.evaluate(p_obj.result())
    for _ in range(10):
      self.assertArrayNear(initial_precision, self.evaluate(p_obj.result()),
                           1e-3)

</source>
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="541" endline="556" pcid="3467">
  def test_value_is_idempotent(self):
    r_obj = metrics.Recall(thresholds=[0.3, 0.72])
    y_pred = tf.random.uniform(shape=(10, 3))
    y_true = tf.random.uniform(shape=(10, 3))
    update_op = r_obj.update_state(y_true, y_pred)
    self.evaluate(tf.compat.v1.variables_initializer(r_obj.variables))

    # Run several updates.
    for _ in range(10):
      self.evaluate(update_op)

    # Then verify idempotency.
    initial_recall = self.evaluate(r_obj.result())
    for _ in range(10):
      self.assertArrayNear(initial_recall, self.evaluate(r_obj.result()), 1e-3)

</source>
</class>

<class classid="155" nclones="2" nlines="13" similarity="100">
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="370" endline="383" pcid="3456">
  def test_weighted(self):
    p_obj = metrics.Precision()
    y_pred = tf.constant([[1, 0, 1, 0], [1, 0, 1, 0]])
    y_true = tf.constant([[0, 1, 1, 0], [1, 0, 0, 1]])
    self.evaluate(tf.compat.v1.variables_initializer(p_obj.variables))
    result = p_obj(
        y_true,
        y_pred,
        sample_weight=tf.constant([[1, 2, 3, 4], [4, 3, 2, 1]]))
    weighted_tp = 3.0 + 4.0
    weighted_positives = (1.0 + 3.0) + (4.0 + 2.0)
    expected_precision = weighted_tp / weighted_positives
    self.assertAlmostEqual(expected_precision, self.evaluate(result))

</source>
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="574" endline="587" pcid="3470">
  def test_weighted(self):
    r_obj = metrics.Recall()
    y_pred = tf.constant([[1, 0, 1, 0], [0, 1, 0, 1]])
    y_true = tf.constant([[0, 1, 1, 0], [1, 0, 0, 1]])
    self.evaluate(tf.compat.v1.variables_initializer(r_obj.variables))
    result = r_obj(
        y_true,
        y_pred,
        sample_weight=tf.constant([[1, 2, 3, 4], [4, 3, 2, 1]]))
    weighted_tp = 3.0 + 1.0
    weighted_t = (2.0 + 3.0) + (4.0 + 1.0)
    expected_recall = weighted_tp / weighted_t
    self.assertAlmostEqual(expected_recall, self.evaluate(result))

</source>
</class>

<class classid="156" nclones="2" nlines="15" similarity="100">
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="400" endline="415" pcid="3459">
  def test_weighted_with_threshold(self):
    p_obj = metrics.Precision(thresholds=[0.5, 1.])
    y_true = tf.constant([[0, 1], [1, 0]], shape=(2, 2))
    y_pred = tf.constant([[1, 0], [0.6, 0]],
                                  shape=(2, 2),
                                  dtype=tf.float32)
    weights = tf.constant([[4, 0], [3, 1]],
                                   shape=(2, 2),
                                   dtype=tf.float32)
    self.evaluate(tf.compat.v1.variables_initializer(p_obj.variables))
    result = p_obj(y_true, y_pred, sample_weight=weights)
    weighted_tp = 0 + 3.
    weighted_positives = (0 + 3.) + (4. + 0.)
    expected_precision = weighted_tp / weighted_positives
    self.assertArrayNear([expected_precision, 0], self.evaluate(result), 1e-3)

</source>
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="604" endline="619" pcid="3473">
  def test_weighted_with_threshold(self):
    r_obj = metrics.Recall(thresholds=[0.5, 1.])
    y_true = tf.constant([[0, 1], [1, 0]], shape=(2, 2))
    y_pred = tf.constant([[1, 0], [0.6, 0]],
                                  shape=(2, 2),
                                  dtype=tf.float32)
    weights = tf.constant([[1, 4], [3, 2]],
                                   shape=(2, 2),
                                   dtype=tf.float32)
    self.evaluate(tf.compat.v1.variables_initializer(r_obj.variables))
    result = r_obj(y_true, y_pred, sample_weight=weights)
    weighted_tp = 0 + 3.
    weighted_positives = (0 + 3.) + (4. + 0.)
    expected_recall = weighted_tp / weighted_positives
    self.assertArrayNear([expected_recall, 0], self.evaluate(result), 1e-3)

</source>
</class>

<class classid="157" nclones="2" nlines="18" similarity="100">
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="416" endline="435" pcid="3460">
  def test_multiple_updates(self):
    p_obj = metrics.Precision(thresholds=[0.5, 1.])
    y_true = tf.constant([[0, 1], [1, 0]], shape=(2, 2))
    y_pred = tf.constant([[1, 0], [0.6, 0]],
                                  shape=(2, 2),
                                  dtype=tf.float32)
    weights = tf.constant([[4, 0], [3, 1]],
                                   shape=(2, 2),
                                   dtype=tf.float32)
    self.evaluate(tf.compat.v1.variables_initializer(p_obj.variables))
    update_op = p_obj.update_state(y_true, y_pred, sample_weight=weights)
    for _ in range(2):
      self.evaluate(update_op)

    weighted_tp = (0 + 3.) + (0 + 3.)
    weighted_positives = ((0 + 3.) + (4. + 0.)) + ((0 + 3.) + (4. + 0.))
    expected_precision = weighted_tp / weighted_positives
    self.assertArrayNear([expected_precision, 0], self.evaluate(p_obj.result()),
                         1e-3)

</source>
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="620" endline="639" pcid="3474">
  def test_multiple_updates(self):
    r_obj = metrics.Recall(thresholds=[0.5, 1.])
    y_true = tf.constant([[0, 1], [1, 0]], shape=(2, 2))
    y_pred = tf.constant([[1, 0], [0.6, 0]],
                                  shape=(2, 2),
                                  dtype=tf.float32)
    weights = tf.constant([[1, 4], [3, 2]],
                                   shape=(2, 2),
                                   dtype=tf.float32)
    self.evaluate(tf.compat.v1.variables_initializer(r_obj.variables))
    update_op = r_obj.update_state(y_true, y_pred, sample_weight=weights)
    for _ in range(2):
      self.evaluate(update_op)

    weighted_tp = (0 + 3.) + (0 + 3.)
    weighted_positives = ((0 + 3.) + (4. + 0.)) + ((0 + 3.) + (4. + 0.))
    expected_recall = weighted_tp / weighted_positives
    self.assertArrayNear([expected_recall, 0], self.evaluate(r_obj.result()),
                         1e-3)

</source>
</class>

<class classid="158" nclones="2" nlines="17" similarity="94">
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="444" endline="463" pcid="3462">
  def test_weighted_top_k(self):
    p_obj = metrics.Precision(top_k=3)
    y_pred1 = tf.constant([0.2, 0.1, 0.4, 0, 0.2], shape=(1, 5))
    y_true1 = tf.constant([0, 1, 1, 0, 1], shape=(1, 5))
    self.evaluate(tf.compat.v1.variables_initializer(p_obj.variables))
    self.evaluate(
        p_obj(
            y_true1,
            y_pred1,
            sample_weight=tf.constant([[1, 4, 2, 3, 5]])))

    y_pred2 = tf.constant([0.2, 0.6, 0.4, 0.2, 0.2], shape=(1, 5))
    y_true2 = tf.constant([1, 0, 1, 1, 1], shape=(1, 5))
    result = p_obj(y_true2, y_pred2, sample_weight=tf.constant(3))

    tp = (2 + 5) + (3 + 3)
    predicted_positives = (1 + 2 + 5) + (3 + 3 + 3)
    expected_precision = tp / predicted_positives
    self.assertAlmostEqual(expected_precision, self.evaluate(result))

</source>
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="648" endline="667" pcid="3476">
  def test_weighted_top_k(self):
    r_obj = metrics.Recall(top_k=3)
    y_pred1 = tf.constant([0.2, 0.1, 0.4, 0, 0.2], shape=(1, 5))
    y_true1 = tf.constant([0, 1, 1, 0, 1], shape=(1, 5))
    self.evaluate(tf.compat.v1.variables_initializer(r_obj.variables))
    self.evaluate(
        r_obj(
            y_true1,
            y_pred1,
            sample_weight=tf.constant([[1, 4, 2, 3, 5]])))

    y_pred2 = tf.constant([0.2, 0.6, 0.4, 0.2, 0.2], shape=(1, 5))
    y_true2 = tf.constant([1, 0, 1, 1, 1], shape=(1, 5))
    result = r_obj(y_true2, y_pred2, sample_weight=tf.constant(3))

    tp = (2 + 5) + (3 + 3)
    positives = (4 + 2 + 5) + (3 + 3 + 3 + 3)
    expected_recall = tp / positives
    self.assertAlmostEqual(expected_recall, self.evaluate(result))

</source>
</class>

<class classid="159" nclones="2" nlines="21" similarity="95">
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="464" endline="488" pcid="3463">
  def test_unweighted_class_id(self):
    p_obj = metrics.Precision(class_id=2)
    self.evaluate(tf.compat.v1.variables_initializer(p_obj.variables))

    y_pred = tf.constant([0.2, 0.1, 0.6, 0, 0.2], shape=(1, 5))
    y_true = tf.constant([0, 1, 1, 0, 0], shape=(1, 5))
    result = p_obj(y_true, y_pred)
    self.assertAlmostEqual(1, self.evaluate(result))
    self.assertAlmostEqual(1, self.evaluate(p_obj.true_positives))
    self.assertAlmostEqual(0, self.evaluate(p_obj.false_positives))

    y_pred = tf.constant([0.2, 0.1, 0, 0, 0.2], shape=(1, 5))
    y_true = tf.constant([0, 1, 1, 0, 0], shape=(1, 5))
    result = p_obj(y_true, y_pred)
    self.assertAlmostEqual(1, self.evaluate(result))
    self.assertAlmostEqual(1, self.evaluate(p_obj.true_positives))
    self.assertAlmostEqual(0, self.evaluate(p_obj.false_positives))

    y_pred = tf.constant([0.2, 0.1, 0.6, 0, 0.2], shape=(1, 5))
    y_true = tf.constant([0, 1, 0, 0, 0], shape=(1, 5))
    result = p_obj(y_true, y_pred)
    self.assertAlmostEqual(0.5, self.evaluate(result))
    self.assertAlmostEqual(1, self.evaluate(p_obj.true_positives))
    self.assertAlmostEqual(1, self.evaluate(p_obj.false_positives))

</source>
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="668" endline="692" pcid="3477">
  def test_unweighted_class_id(self):
    r_obj = metrics.Recall(class_id=2)
    self.evaluate(tf.compat.v1.variables_initializer(r_obj.variables))

    y_pred = tf.constant([0.2, 0.1, 0.6, 0, 0.2], shape=(1, 5))
    y_true = tf.constant([0, 1, 1, 0, 0], shape=(1, 5))
    result = r_obj(y_true, y_pred)
    self.assertAlmostEqual(1, self.evaluate(result))
    self.assertAlmostEqual(1, self.evaluate(r_obj.true_positives))
    self.assertAlmostEqual(0, self.evaluate(r_obj.false_negatives))

    y_pred = tf.constant([0.2, 0.1, 0, 0, 0.2], shape=(1, 5))
    y_true = tf.constant([0, 1, 1, 0, 0], shape=(1, 5))
    result = r_obj(y_true, y_pred)
    self.assertAlmostEqual(0.5, self.evaluate(result))
    self.assertAlmostEqual(1, self.evaluate(r_obj.true_positives))
    self.assertAlmostEqual(1, self.evaluate(r_obj.false_negatives))

    y_pred = tf.constant([0.2, 0.1, 0.6, 0, 0.2], shape=(1, 5))
    y_true = tf.constant([0, 1, 0, 0, 0], shape=(1, 5))
    result = r_obj(y_true, y_pred)
    self.assertAlmostEqual(0.5, self.evaluate(result))
    self.assertAlmostEqual(1, self.evaluate(r_obj.true_positives))
    self.assertAlmostEqual(1, self.evaluate(r_obj.false_negatives))

</source>
</class>

<class classid="160" nclones="2" nlines="15" similarity="93">
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="489" endline="506" pcid="3464">
  def test_unweighted_top_k_and_class_id(self):
    p_obj = metrics.Precision(class_id=2, top_k=2)
    self.evaluate(tf.compat.v1.variables_initializer(p_obj.variables))

    y_pred = tf.constant([0.2, 0.6, 0.3, 0, 0.2], shape=(1, 5))
    y_true = tf.constant([0, 1, 1, 0, 0], shape=(1, 5))
    result = p_obj(y_true, y_pred)
    self.assertAlmostEqual(1, self.evaluate(result))
    self.assertAlmostEqual(1, self.evaluate(p_obj.true_positives))
    self.assertAlmostEqual(0, self.evaluate(p_obj.false_positives))

    y_pred = tf.constant([1, 1, 0.9, 1, 1], shape=(1, 5))
    y_true = tf.constant([0, 1, 1, 0, 0], shape=(1, 5))
    result = p_obj(y_true, y_pred)
    self.assertAlmostEqual(1, self.evaluate(result))
    self.assertAlmostEqual(1, self.evaluate(p_obj.true_positives))
    self.assertAlmostEqual(0, self.evaluate(p_obj.false_positives))

</source>
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="693" endline="710" pcid="3478">
  def test_unweighted_top_k_and_class_id(self):
    r_obj = metrics.Recall(class_id=2, top_k=2)
    self.evaluate(tf.compat.v1.variables_initializer(r_obj.variables))

    y_pred = tf.constant([0.2, 0.6, 0.3, 0, 0.2], shape=(1, 5))
    y_true = tf.constant([0, 1, 1, 0, 0], shape=(1, 5))
    result = r_obj(y_true, y_pred)
    self.assertAlmostEqual(1, self.evaluate(result))
    self.assertAlmostEqual(1, self.evaluate(r_obj.true_positives))
    self.assertAlmostEqual(0, self.evaluate(r_obj.false_negatives))

    y_pred = tf.constant([1, 1, 0.9, 1, 1], shape=(1, 5))
    y_true = tf.constant([0, 1, 1, 0, 0], shape=(1, 5))
    result = r_obj(y_true, y_pred)
    self.assertAlmostEqual(0.5, self.evaluate(result))
    self.assertAlmostEqual(1, self.evaluate(r_obj.true_positives))
    self.assertAlmostEqual(1, self.evaluate(r_obj.false_negatives))

</source>
</class>

<class classid="161" nclones="4" nlines="16" similarity="76">
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="726" endline="745" pcid="3480">
  def test_config(self):
    s_obj = metrics.SensitivityAtSpecificity(
        0.4,
        num_thresholds=100,
        class_id=12,
        name='sensitivity_at_specificity_1')
    self.assertEqual(s_obj.name, 'sensitivity_at_specificity_1')
    self.assertLen(s_obj.variables, 4)
    self.assertEqual(s_obj.specificity, 0.4)
    self.assertEqual(s_obj.num_thresholds, 100)
    self.assertEqual(s_obj.class_id, 12)

    # Check save and restore config
    s_obj2 = metrics.SensitivityAtSpecificity.from_config(s_obj.get_config())
    self.assertEqual(s_obj2.name, 'sensitivity_at_specificity_1')
    self.assertLen(s_obj2.variables, 4)
    self.assertEqual(s_obj2.specificity, 0.4)
    self.assertEqual(s_obj2.num_thresholds, 100)
    self.assertEqual(s_obj.class_id, 12)

</source>
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="1067" endline="1083" pcid="3507">
  def test_config(self):
    s_obj = metrics.RecallAtPrecision(
        0.4, num_thresholds=100, class_id=12, name='recall_at_precision_1')
    self.assertEqual(s_obj.name, 'recall_at_precision_1')
    self.assertLen(s_obj.variables, 4)
    self.assertEqual(s_obj.precision, 0.4)
    self.assertEqual(s_obj.num_thresholds, 100)
    self.assertEqual(s_obj.class_id, 12)

    # Check save and restore config
    s_obj2 = metrics.RecallAtPrecision.from_config(s_obj.get_config())
    self.assertEqual(s_obj2.name, 'recall_at_precision_1')
    self.assertLen(s_obj2.variables, 4)
    self.assertEqual(s_obj2.precision, 0.4)
    self.assertEqual(s_obj2.num_thresholds, 100)
    self.assertEqual(s_obj.class_id, 12)

</source>
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="953" endline="969" pcid="3498">
  def test_config(self):
    s_obj = metrics.PrecisionAtRecall(
        0.4, num_thresholds=100, class_id=12, name='precision_at_recall_1')
    self.assertEqual(s_obj.name, 'precision_at_recall_1')
    self.assertLen(s_obj.variables, 4)
    self.assertEqual(s_obj.recall, 0.4)
    self.assertEqual(s_obj.num_thresholds, 100)
    self.assertEqual(s_obj.class_id, 12)

    # Check save and restore config
    s_obj2 = metrics.PrecisionAtRecall.from_config(s_obj.get_config())
    self.assertEqual(s_obj2.name, 'precision_at_recall_1')
    self.assertLen(s_obj2.variables, 4)
    self.assertEqual(s_obj2.recall, 0.4)
    self.assertEqual(s_obj2.num_thresholds, 100)
    self.assertEqual(s_obj.class_id, 12)

</source>
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="840" endline="859" pcid="3489">
  def test_config(self):
    s_obj = metrics.SpecificityAtSensitivity(
        0.4,
        num_thresholds=100,
        class_id=12,
        name='specificity_at_sensitivity_1')
    self.assertEqual(s_obj.name, 'specificity_at_sensitivity_1')
    self.assertLen(s_obj.variables, 4)
    self.assertEqual(s_obj.sensitivity, 0.4)
    self.assertEqual(s_obj.num_thresholds, 100)
    self.assertEqual(s_obj.class_id, 12)

    # Check save and restore config
    s_obj2 = metrics.SpecificityAtSensitivity.from_config(s_obj.get_config())
    self.assertEqual(s_obj2.name, 'specificity_at_sensitivity_1')
    self.assertLen(s_obj2.variables, 4)
    self.assertEqual(s_obj2.sensitivity, 0.4)
    self.assertEqual(s_obj2.num_thresholds, 100)
    self.assertEqual(s_obj.class_id, 12)

</source>
</class>

<class classid="162" nclones="4" nlines="18" similarity="100">
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="746" endline="768" pcid="3481">
  def test_value_is_idempotent(self):
    s_obj = metrics.SensitivityAtSpecificity(0.7)
    y_pred = tf.random.uniform((10, 3),
                                       maxval=1,
                                       dtype=tf.float32,
                                       seed=1)
    y_true = tf.random.uniform((10, 3),
                                       maxval=2,
                                       dtype=tf.int64,
                                       seed=1)
    update_op = s_obj.update_state(y_true, y_pred)
    self.evaluate(tf.compat.v1.variables_initializer(s_obj.variables))

    # Run several updates.
    for _ in range(10):
      self.evaluate(update_op)

    # Then verify idempotency.
    initial_sensitivity = self.evaluate(s_obj.result())
    for _ in range(10):
      self.assertAlmostEqual(initial_sensitivity, self.evaluate(s_obj.result()),
                             1e-3)

</source>
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="860" endline="882" pcid="3490">
  def test_value_is_idempotent(self):
    s_obj = metrics.SpecificityAtSensitivity(0.7)
    y_pred = tf.random.uniform((10, 3),
                                       maxval=1,
                                       dtype=tf.float32,
                                       seed=1)
    y_true = tf.random.uniform((10, 3),
                                       maxval=2,
                                       dtype=tf.int64,
                                       seed=1)
    update_op = s_obj.update_state(y_true, y_pred)
    self.evaluate(tf.compat.v1.variables_initializer(s_obj.variables))

    # Run several updates.
    for _ in range(10):
      self.evaluate(update_op)

    # Then verify idempotency.
    initial_specificity = self.evaluate(s_obj.result())
    for _ in range(10):
      self.assertAlmostEqual(initial_specificity, self.evaluate(s_obj.result()),
                             1e-3)

</source>
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="970" endline="992" pcid="3499">
  def test_value_is_idempotent(self):
    s_obj = metrics.PrecisionAtRecall(0.7)
    y_pred = tf.random.uniform((10, 3),
                                       maxval=1,
                                       dtype=tf.float32,
                                       seed=1)
    y_true = tf.random.uniform((10, 3),
                                       maxval=2,
                                       dtype=tf.int64,
                                       seed=1)
    update_op = s_obj.update_state(y_true, y_pred)
    self.evaluate(tf.compat.v1.variables_initializer(s_obj.variables))

    # Run several updates.
    for _ in range(10):
      self.evaluate(update_op)

    # Then verify idempotency.
    initial_precision = self.evaluate(s_obj.result())
    for _ in range(10):
      self.assertAlmostEqual(initial_precision, self.evaluate(s_obj.result()),
                             1e-3)

</source>
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="1084" endline="1106" pcid="3508">
  def test_value_is_idempotent(self):
    s_obj = metrics.RecallAtPrecision(0.7)
    y_pred = tf.random.uniform((10, 3),
                                       maxval=1,
                                       dtype=tf.float32,
                                       seed=1)
    y_true = tf.random.uniform((10, 3),
                                       maxval=2,
                                       dtype=tf.int64,
                                       seed=1)
    update_op = s_obj.update_state(y_true, y_pred)
    self.evaluate(tf.compat.v1.variables_initializer(s_obj.variables))

    # Run several updates.
    for _ in range(10):
      self.evaluate(update_op)

    # Then verify idempotency.
    initial_recall = self.evaluate(s_obj.result())
    for _ in range(10):
      self.assertAlmostEqual(initial_recall, self.evaluate(s_obj.result()),
                             1e-3)

</source>
</class>

<class classid="163" nclones="4" nlines="11" similarity="72">
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="813" endline="825" pcid="3486">
  def test_weighted(self, label_dtype):
    s_obj = metrics.SensitivityAtSpecificity(0.4)
    pred_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.01, 0.02, 0.25, 0.26, 0.26]
    label_values = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]
    weight_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

    y_pred = tf.constant(pred_values, dtype=tf.float32)
    y_true = tf.cast(label_values, dtype=label_dtype)
    weights = tf.constant(weight_values)
    self.evaluate(tf.compat.v1.variables_initializer(s_obj.variables))
    result = s_obj(y_true, y_pred, sample_weight=weights)
    self.assertAlmostEqual(0.675, self.evaluate(result))

</source>
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="1039" endline="1052" pcid="3504">
  def test_weighted(self, label_dtype):
    s_obj = metrics.PrecisionAtRecall(7.0/8)
    pred_values = [0.0, 0.1, 0.2, 0.5, 0.6, 0.2, 0.5, 0.6, 0.8, 0.9]
    label_values = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]
    weight_values = [2, 1, 2, 1, 2, 1, 2, 2, 1, 2]

    y_pred = tf.constant(pred_values, dtype=tf.float32)
    y_true = tf.cast(label_values, dtype=label_dtype)
    weights = tf.constant(weight_values)
    self.evaluate(tf.compat.v1.variables_initializer(s_obj.variables))
    result = s_obj(y_true, y_pred, sample_weight=weights)
    # For 0.0 < decision threshold < 0.2.
    self.assertAlmostEqual(0.7, self.evaluate(result))

</source>
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="926" endline="938" pcid="3495">
  def test_weighted(self, label_dtype):
    s_obj = metrics.SpecificityAtSensitivity(0.4)
    pred_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.01, 0.02, 0.25, 0.26, 0.26]
    label_values = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]
    weight_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

    y_pred = tf.constant(pred_values, dtype=tf.float32)
    y_true = tf.cast(label_values, dtype=label_dtype)
    weights = tf.constant(weight_values)
    self.evaluate(tf.compat.v1.variables_initializer(s_obj.variables))
    result = s_obj(y_true, y_pred, sample_weight=weights)
    self.assertAlmostEqual(0.4, self.evaluate(result))

</source>
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="1162" endline="1173" pcid="3513">
  def test_weighted(self, label_dtype):
    s_obj = metrics.RecallAtPrecision(0.75)
    pred_values = [0.1, 0.2, 0.3, 0.5, 0.6, 0.9, 0.9]
    label_values = [0, 1, 0, 0, 0, 1, 1]
    weight_values = [1, 2, 1, 2, 1, 2, 1]
    y_pred = tf.constant(pred_values, dtype=tf.float32)
    y_true = tf.cast(label_values, dtype=label_dtype)
    weights = tf.constant(weight_values)
    self.evaluate(tf.compat.v1.variables_initializer(s_obj.variables))
    result = s_obj(y_true, y_pred, sample_weight=weights)
    self.assertAlmostEqual(0.6, self.evaluate(result))

</source>
</class>

<class classid="164" nclones="3" nlines="10" similarity="70">
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="1116" endline="1130" pcid="3510">
  def test_unweighted_high_precision(self):
    s_obj = metrics.RecallAtPrecision(0.75)
    pred_values = [
        0.05, 0.1, 0.2, 0.3, 0.3, 0.35, 0.4, 0.45, 0.5, 0.6, 0.9, 0.95
    ]
    label_values = [0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1]
    # precisions: [1/2, 6/11, 1/2, 5/9, 5/8, 5/7, 2/3, 3/5, 3/5, 2/3, 1/2, 1].
    # recalls:    [1,   1,    5/6, 5/6, 5/6, 5/6, 2/3, 1/2, 1/2, 1/3, 1/6, 1/6].
    y_pred = tf.constant(pred_values, dtype=tf.float32)
    y_true = tf.constant(label_values)
    self.evaluate(tf.compat.v1.variables_initializer(s_obj.variables))
    result = s_obj(y_true, y_pred)
    # The precision 0.75 can be reached at thresholds 0.4<=t<0.45.
    self.assertAlmostEqual(0.5, self.evaluate(result))

</source>
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="1131" endline="1145" pcid="3511">
  def test_unweighted_low_precision(self):
    s_obj = metrics.RecallAtPrecision(2.0 / 3)
    pred_values = [
        0.05, 0.1, 0.2, 0.3, 0.3, 0.35, 0.4, 0.45, 0.5, 0.6, 0.9, 0.95
    ]
    label_values = [0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1]
    # precisions: [1/2, 6/11, 1/2, 5/9, 5/8, 5/7, 2/3, 3/5, 3/5, 2/3, 1/2, 1].
    # recalls:    [1,   1,    5/6, 5/6, 5/6, 5/6, 2/3, 1/2, 1/2, 1/3, 1/6, 1/6].
    y_pred = tf.constant(pred_values, dtype=tf.float32)
    y_true = tf.constant(label_values)
    self.evaluate(tf.compat.v1.variables_initializer(s_obj.variables))
    result = s_obj(y_true, y_pred)
    # The precision 5/7 can be reached at thresholds 00.3<=t<0.35.
    self.assertAlmostEqual(5. / 6, self.evaluate(result))

</source>
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="1146" endline="1160" pcid="3512">
  def test_unweighted_class_id(self):
    s_obj = metrics.RecallAtPrecision(2.0 / 3, class_id=2)
    pred_values = [
        0.05, 0.1, 0.2, 0.3, 0.3, 0.35, 0.4, 0.45, 0.5, 0.6, 0.9, 0.95
    ]
    label_values = [0, 2, 0, 0, 0, 2, 2, 0, 2, 2, 0, 2]
    # precisions: [1/2, 6/11, 1/2, 5/9, 5/8, 5/7, 2/3, 3/5, 3/5, 2/3, 1/2, 1].
    # recalls:    [1,   1,    5/6, 5/6, 5/6, 5/6, 2/3, 1/2, 1/2, 1/3, 1/6, 1/6].
    y_pred = tf.transpose([pred_values] * 3)
    y_true = tf.one_hot(label_values, depth=3)
    self.evaluate(tf.compat.v1.variables_initializer(s_obj.variables))
    result = s_obj(y_true, y_pred)
    # The precision 5/7 can be reached at thresholds 00.3<=t<0.35.
    self.assertAlmostEqual(5. / 6, self.evaluate(result))

</source>
</class>

<class classid="165" nclones="2" nlines="29" similarity="86">
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="1229" endline="1260" pcid="3518">
  def test_config(self):
    self.setup()
    auc_obj = metrics.AUC(
        num_thresholds=100,
        curve='PR',
        summation_method='majoring',
        name='auc_1')
    auc_obj.update_state(self.y_true, self.y_pred)
    self.assertEqual(auc_obj.name, 'auc_1')
    self.assertLen(auc_obj.variables, 4)
    self.assertEqual(auc_obj.num_thresholds, 100)
    self.assertEqual(auc_obj.curve, metrics_utils.AUCCurve.PR)
    self.assertEqual(auc_obj.summation_method,
                     metrics_utils.AUCSummationMethod.MAJORING)
    old_config = auc_obj.get_config()
    self.assertNotIn('thresholds', old_config)
    self.assertDictEqual(old_config, json.loads(json.dumps(old_config)))

    # Check save and restore config.
    auc_obj2 = metrics.AUC.from_config(auc_obj.get_config())
    auc_obj2.update_state(self.y_true, self.y_pred)
    self.assertEqual(auc_obj2.name, 'auc_1')
    self.assertLen(auc_obj2.variables, 4)
    self.assertEqual(auc_obj2.num_thresholds, 100)
    self.assertEqual(auc_obj2.curve, metrics_utils.AUCCurve.PR)
    self.assertEqual(auc_obj2.summation_method,
                     metrics_utils.AUCSummationMethod.MAJORING)
    new_config = auc_obj2.get_config()
    self.assertNotIn('thresholds', new_config)
    self.assertDictEqual(old_config, new_config)
    self.assertAllClose(auc_obj.thresholds, auc_obj2.thresholds)

</source>
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="1261" endline="1292" pcid="3519">
  def test_config_manual_thresholds(self):
    self.setup()
    auc_obj = metrics.AUC(
        num_thresholds=None,
        curve='PR',
        summation_method='majoring',
        name='auc_1',
        thresholds=[0.3, 0.5])
    auc_obj.update_state(self.y_true, self.y_pred)
    self.assertEqual(auc_obj.name, 'auc_1')
    self.assertLen(auc_obj.variables, 4)
    self.assertEqual(auc_obj.num_thresholds, 4)
    self.assertAllClose(auc_obj.thresholds, [0.0, 0.3, 0.5, 1.0])
    self.assertEqual(auc_obj.curve, metrics_utils.AUCCurve.PR)
    self.assertEqual(auc_obj.summation_method,
                     metrics_utils.AUCSummationMethod.MAJORING)
    old_config = auc_obj.get_config()
    self.assertDictEqual(old_config, json.loads(json.dumps(old_config)))

    # Check save and restore config.
    auc_obj2 = metrics.AUC.from_config(auc_obj.get_config())
    auc_obj2.update_state(self.y_true, self.y_pred)
    self.assertEqual(auc_obj2.name, 'auc_1')
    self.assertLen(auc_obj2.variables, 4)
    self.assertEqual(auc_obj2.num_thresholds, 4)
    self.assertEqual(auc_obj2.curve, metrics_utils.AUCCurve.PR)
    self.assertEqual(auc_obj2.summation_method,
                     metrics_utils.AUCSummationMethod.MAJORING)
    new_config = auc_obj2.get_config()
    self.assertDictEqual(old_config, new_config)
    self.assertAllClose(auc_obj.thresholds, auc_obj2.thresholds)

</source>
</class>

<class classid="166" nclones="2" nlines="11" similarity="81">
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="1293" endline="1307" pcid="3520">
  def test_value_is_idempotent(self):
    self.setup()
    auc_obj = metrics.AUC(num_thresholds=3)
    self.evaluate(tf.compat.v1.variables_initializer(auc_obj.variables))

    # Run several updates.
    update_op = auc_obj.update_state(self.y_true, self.y_pred)
    for _ in range(10):
      self.evaluate(update_op)

    # Then verify idempotency.
    initial_auc = self.evaluate(auc_obj.result())
    for _ in range(10):
      self.assertAllClose(initial_auc, self.evaluate(auc_obj.result()), 1e-3)

</source>
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="1548" endline="1563" pcid="3536">
  def test_value_is_idempotent(self):
    with self.test_session():
      self.setup()
      auc_obj = metrics.AUC(num_thresholds=5, multi_label=True)
      self.evaluate(tf.compat.v1.variables_initializer(auc_obj.variables))

      # Run several updates.
      update_op = auc_obj.update_state(self.y_true_good, self.y_pred)
      for _ in range(10):
        self.evaluate(update_op)

      # Then verify idempotency.
      initial_auc = self.evaluate(auc_obj.result())
      for _ in range(10):
        self.assertAllClose(initial_auc, self.evaluate(auc_obj.result()), 1e-3)

</source>
</class>

<class classid="167" nclones="2" nlines="10" similarity="90">
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="1404" endline="1420" pcid="3528">
  def test_weighted_pr_majoring(self):
    self.setup()
    auc_obj = metrics.AUC(
        num_thresholds=self.num_thresholds,
        curve='PR',
        summation_method='majoring')
    self.evaluate(tf.compat.v1.variables_initializer(auc_obj.variables))
    result = auc_obj(self.y_true, self.y_pred, sample_weight=self.sample_weight)

    # tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]
    # precision = [7/(7+3), 4/4, 0] = [0.7, 1, 0]
    # recall = [7/7, 4/(4+3), 0] = [1, 0.571, 0]
    # heights = [max(0.7, 1), max(1, 0)] = [1, 1]
    # widths = [(1 - 0.571), (0.571 - 0)] = [0.429, 0.571]
    expected_result = (1 * 0.429 + 1 * 0.571)
    self.assertAllClose(self.evaluate(result), expected_result, 1e-3)

</source>
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="1421" endline="1437" pcid="3529">
  def test_weighted_pr_minoring(self):
    self.setup()
    auc_obj = metrics.AUC(
        num_thresholds=self.num_thresholds,
        curve='PR',
        summation_method='minoring')
    self.evaluate(tf.compat.v1.variables_initializer(auc_obj.variables))
    result = auc_obj(self.y_true, self.y_pred, sample_weight=self.sample_weight)

    # tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]
    # precision = [7/(7+3), 4/4, 0] = [0.7, 1, 0]
    # recall = [7/7, 4/(4+3), 0] = [1, 0.571, 0]
    # heights = [min(0.7, 1), min(1, 0)] = [0.7, 0]
    # widths = [(1 - 0.571), (0.571 - 0)] = [0.429, 0.571]
    expected_result = (0.7 * 0.429 + 0 * 0.571)
    self.assertAllClose(self.evaluate(result), expected_result, 1e-3)

</source>
</class>

<class classid="168" nclones="4" nlines="11" similarity="72">
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="1592" endline="1606" pcid="3540">
  def test_unweighted_from_logits(self):
    with self.test_session():
      self.setup()
      auc_obj = metrics.AUC(
          num_thresholds=self.num_thresholds,
          multi_label=True,
          from_logits=True)
      self.evaluate(tf.compat.v1.variables_initializer(auc_obj.variables))
      result = auc_obj(self.y_true_good, self.y_pred_logits)

      # tpr = [[1, 1, 0.5, 0.5, 0], [1, 1, 0, 0, 0]]
      # fpr = [[1, 0.5, 0, 0, 0], [1, 0, 0, 0, 0]]
      expected_result = (0.875 + 1.0) / 2.0
      self.assertAllClose(self.evaluate(result), expected_result, 1e-3)

</source>
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="1646" endline="1659" pcid="3544">
  def test_label_weights_flat(self):
    self.setup()
    auc_obj = metrics.AUC(
        num_thresholds=self.num_thresholds,
        multi_label=False,
        label_weights=[0.75, 0.25])
    self.evaluate(tf.compat.v1.variables_initializer(auc_obj.variables))
    result = auc_obj(self.y_true_good, self.y_pred)

    # tpr = [1, 1, 0.375, 0.375, 0]
    # fpr = [1, 0.375, 0, 0, 0]
    expected_result = 1.0 - ((1.0 - 0.375) * 0.375 / 2.0)
    self.assertAllClose(self.evaluate(result), expected_result, 1e-2)

</source>
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="1693" endline="1717" pcid="3547">
  def test_manual_thresholds(self):
    with self.test_session():
      self.setup()
      # Verify that when specified, thresholds are used instead of
      # num_thresholds.
      auc_obj = metrics.AUC(num_thresholds=2, thresholds=[0.5],
                            multi_label=True)
      self.assertEqual(auc_obj.num_thresholds, 3)
      self.assertAllClose(auc_obj.thresholds, [0.0, 0.5, 1.0])
      self.evaluate(tf.compat.v1.variables_initializer(auc_obj.variables))
      result = auc_obj(self.y_true_good, self.y_pred)

      # tp = [[2, 1, 0], [2, 0, 0]]
      # fp = [2, 0, 0], [2, 0, 0]]
      # fn = [[0, 1, 2], [0, 2, 2]]
      # tn = [[0, 2, 2], [0, 2, 2]]

      # tpr = [[1, 0.5, 0], [1, 0, 0]]
      # fpr = [[1, 0, 0], [1, 0, 0]]

      # auc by slice = [0.75, 0.5]
      expected_result = (0.75 + 0.5) / 2.0

      self.assertAllClose(self.evaluate(result), expected_result, 1e-3)

</source>
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="1631" endline="1645" pcid="3543">
  def test_label_weights(self):
    with self.test_session():
      self.setup()
      auc_obj = metrics.AUC(
          num_thresholds=self.num_thresholds,
          multi_label=True,
          label_weights=[0.75, 0.25])
      self.evaluate(tf.compat.v1.variables_initializer(auc_obj.variables))
      result = auc_obj(self.y_true_good, self.y_pred)

      # tpr = [[1, 1, 0.5, 0.5, 0], [1, 1, 0, 0, 0]]
      # fpr = [[1, 0.5, 0, 0, 0], [1, 0, 0, 0, 0]]
      expected_result = (0.875 * 0.75 + 1.0 * 0.25) / (0.75 + 0.25)
      self.assertAllClose(self.evaluate(result), expected_result, 1e-3)

</source>
</class>

<class classid="169" nclones="2" nlines="16" similarity="70">
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="1845" endline="1869" pcid="3556">
  def test_even_thresholds_correctness(self, metric_cls):
    with tf.compat.forward_compatibility_horizon(2021, 6, 9):
      # make sure the old approach and new approach produce same result
      # for evenly distributed thresholds
      y_true = np.random.randint(2, size=(10,))
      y_pred = np.random.rand(10)

      even_thresholds = [0.0, 0.25, 0.5, 0.75, 1.0]
      if metric_cls == metrics.AUC:
        even_thresholds = even_thresholds[1:-1]
      metric_obj = metric_cls(thresholds=even_thresholds)
      metric_obj.update_state(y_true, y_pred)
      result1 = metric_obj.result()

      metric_obj2 = metric_cls(thresholds=even_thresholds)
      # Force to use the old approach
      metric_obj2._thresholds_distributed_evenly = False
      metric_obj2.update_state(y_true, y_pred)
      result2 = metric_obj2.result()

      self.assertAllClose(result1, result2)
      # Check all the variables are the same, eg tp, tn, fp, fn
      for v1, v2 in zip(metric_obj.variables, metric_obj2.variables):
        self.assertAllClose(v1, v2)

</source>
<source file="systems/keras-2.7.0/keras/metrics_confusion_matrix_test.py" startline="1875" endline="1895" pcid="3557">
  def test_even_thresholds_correctness_2(self, metric_cls):
    with tf.compat.forward_compatibility_horizon(2021, 6, 9):
      y_true = np.random.randint(2, size=(10,))
      y_pred = np.random.rand(10)

      metric_obj = metric_cls(0.5)
      metric_obj.update_state(y_true, y_pred)
      result1 = metric_obj.result()

      metric_obj2 = metric_cls(0.5)
      # Force to use the old approach
      metric_obj2._thresholds_distributed_evenly = False
      metric_obj2.update_state(y_true, y_pred)
      result2 = metric_obj2.result()

      self.assertAllClose(result1, result2)
      # Check all the variables are the same, eg tp, tn, fp, fn
      for v1, v2 in zip(metric_obj.variables, metric_obj2.variables):
        self.assertAllClose(v1, v2)


</source>
</class>

<class classid="170" nclones="2" nlines="32" similarity="76">
<source file="systems/keras-2.7.0/keras/preprocessing/image_test.py" startline="99" endline="139" pcid="3574">
  def test_image_data_generator(self):
    if PIL is None:
      return  # Skip test if PIL is not available.

    for test_images in _generate_test_images():
      img_list = []
      for im in test_images:
        img_list.append(preprocessing_image.img_to_array(im)[None, ...])

      images = np.vstack(img_list)
      generator = preprocessing_image.ImageDataGenerator(
          featurewise_center=True,
          samplewise_center=True,
          featurewise_std_normalization=True,
          samplewise_std_normalization=True,
          zca_whitening=True,
          rotation_range=90.,
          width_shift_range=0.1,
          height_shift_range=0.1,
          shear_range=0.5,
          zoom_range=0.2,
          channel_shift_range=0.,
          brightness_range=(1, 5),
          fill_mode='nearest',
          cval=0.5,
          horizontal_flip=True,
          vertical_flip=True)
      # Basic test before fit
      x = np.random.random((32, 10, 10, 3))
      generator.flow(x)

      # Fit
      generator.fit(images, augment=True)

      for x, _ in generator.flow(
          images,
          np.arange(images.shape[0]),
          shuffle=True):
        self.assertEqual(x.shape[1:], images.shape[1:])
        break

</source>
<source file="systems/keras-2.7.0/keras/preprocessing/image_test.py" startline="397" endline="431" pcid="3585">
  def test_batch_standardize(self):
    if PIL is None:
      return  # Skip test if PIL is not available.

    # ImageDataGenerator.standardize should work on batches
    for test_images in _generate_test_images():
      img_list = []
      for im in test_images:
        img_list.append(preprocessing_image.img_to_array(im)[None, ...])

      images = np.vstack(img_list)
      generator = preprocessing_image.ImageDataGenerator(
          featurewise_center=True,
          samplewise_center=True,
          featurewise_std_normalization=True,
          samplewise_std_normalization=True,
          zca_whitening=True,
          rotation_range=90.,
          width_shift_range=0.1,
          height_shift_range=0.1,
          shear_range=0.5,
          zoom_range=0.2,
          channel_shift_range=0.,
          brightness_range=(1, 5),
          fill_mode='nearest',
          cval=0.5,
          horizontal_flip=True,
          vertical_flip=True)
      generator.fit(images, augment=True)

      transformed = np.copy(images)
      for i, im in enumerate(transformed):
        transformed[i] = generator.random_transform(im)
      transformed = generator.standardize(transformed)

</source>
</class>

<class classid="171" nclones="2" nlines="28" similarity="71">
<source file="systems/keras-2.7.0/keras/preprocessing/text_dataset_test.py" startline="90" endline="119" pcid="3597">
  def test_text_dataset_from_directory_binary(self):
    directory = self._prepare_directory(num_classes=2)
    dataset = text_dataset.text_dataset_from_directory(
        directory, batch_size=8, label_mode='int', max_length=10)
    batch = next(iter(dataset))
    self.assertLen(batch, 2)
    self.assertEqual(batch[0].shape, (8,))
    self.assertEqual(batch[0].dtype.name, 'string')
    self.assertEqual(len(batch[0].numpy()[0]), 10)  # Test max_length
    self.assertEqual(batch[1].shape, (8,))
    self.assertEqual(batch[1].dtype.name, 'int32')

    dataset = text_dataset.text_dataset_from_directory(
        directory, batch_size=8, label_mode='binary')
    batch = next(iter(dataset))
    self.assertLen(batch, 2)
    self.assertEqual(batch[0].shape, (8,))
    self.assertEqual(batch[0].dtype.name, 'string')
    self.assertEqual(batch[1].shape, (8, 1))
    self.assertEqual(batch[1].dtype.name, 'float32')

    dataset = text_dataset.text_dataset_from_directory(
        directory, batch_size=8, label_mode='categorical')
    batch = next(iter(dataset))
    self.assertLen(batch, 2)
    self.assertEqual(batch[0].shape, (8,))
    self.assertEqual(batch[0].dtype.name, 'string')
    self.assertEqual(batch[1].shape, (8, 2))
    self.assertEqual(batch[1].dtype.name, 'float32')

</source>
<source file="systems/keras-2.7.0/keras/preprocessing/image_dataset_test.py" startline="115" endline="146" pcid="3619">
  def test_image_dataset_from_directory_binary(self):
    if PIL is None:
      return  # Skip test if PIL is not available.

    directory = self._prepare_directory(num_classes=2)
    dataset = image_dataset.image_dataset_from_directory(
        directory, batch_size=8, image_size=(18, 18), label_mode='int')
    batch = next(iter(dataset))
    self.assertLen(batch, 2)
    self.assertEqual(batch[0].shape, (8, 18, 18, 3))
    self.assertEqual(batch[0].dtype.name, 'float32')
    self.assertEqual(batch[1].shape, (8,))
    self.assertEqual(batch[1].dtype.name, 'int32')

    dataset = image_dataset.image_dataset_from_directory(
        directory, batch_size=8, image_size=(18, 18), label_mode='binary')
    batch = next(iter(dataset))
    self.assertLen(batch, 2)
    self.assertEqual(batch[0].shape, (8, 18, 18, 3))
    self.assertEqual(batch[0].dtype.name, 'float32')
    self.assertEqual(batch[1].shape, (8, 1))
    self.assertEqual(batch[1].dtype.name, 'float32')

    dataset = image_dataset.image_dataset_from_directory(
        directory, batch_size=8, image_size=(18, 18), label_mode='categorical')
    batch = next(iter(dataset))
    self.assertLen(batch, 2)
    self.assertEqual(batch[0].shape, (8, 18, 18, 3))
    self.assertEqual(batch[0].dtype.name, 'float32')
    self.assertEqual(batch[1].shape, (8, 2))
    self.assertEqual(batch[1].dtype.name, 'float32')

</source>
</class>

<class classid="172" nclones="2" nlines="30" similarity="70">
<source file="systems/keras-2.7.0/keras/preprocessing/text_dataset_test.py" startline="129" endline="162" pcid="3599">
  def test_text_dataset_from_directory_multiclass(self):
    directory = self._prepare_directory(num_classes=4, count=15)

    dataset = text_dataset.text_dataset_from_directory(
        directory, batch_size=8, label_mode=None)
    batch = next(iter(dataset))
    self.assertEqual(batch.shape, (8,))

    dataset = text_dataset.text_dataset_from_directory(
        directory, batch_size=8, label_mode=None)
    sample_count = 0
    iterator = iter(dataset)
    for batch in dataset:
      sample_count += next(iterator).shape[0]
    self.assertEqual(sample_count, 15)

    dataset = text_dataset.text_dataset_from_directory(
        directory, batch_size=8, label_mode='int')
    batch = next(iter(dataset))
    self.assertLen(batch, 2)
    self.assertEqual(batch[0].shape, (8,))
    self.assertEqual(batch[0].dtype.name, 'string')
    self.assertEqual(batch[1].shape, (8,))
    self.assertEqual(batch[1].dtype.name, 'int32')

    dataset = text_dataset.text_dataset_from_directory(
        directory, batch_size=8, label_mode='categorical')
    batch = next(iter(dataset))
    self.assertLen(batch, 2)
    self.assertEqual(batch[0].shape, (8,))
    self.assertEqual(batch[0].dtype.name, 'string')
    self.assertEqual(batch[1].shape, (8, 4))
    self.assertEqual(batch[1].dtype.name, 'float32')

</source>
<source file="systems/keras-2.7.0/keras/preprocessing/image_dataset_test.py" startline="175" endline="211" pcid="3623">
  def test_image_dataset_from_directory_multiclass(self):
    if PIL is None:
      return  # Skip test if PIL is not available.

    directory = self._prepare_directory(num_classes=4, count=15)

    dataset = image_dataset.image_dataset_from_directory(
        directory, batch_size=8, image_size=(18, 18), label_mode=None)
    batch = next(iter(dataset))
    self.assertEqual(batch.shape, (8, 18, 18, 3))

    dataset = image_dataset.image_dataset_from_directory(
        directory, batch_size=8, image_size=(18, 18), label_mode=None)
    sample_count = 0
    iterator = iter(dataset)
    for batch in dataset:
      sample_count += next(iterator).shape[0]
    self.assertEqual(sample_count, 15)

    dataset = image_dataset.image_dataset_from_directory(
        directory, batch_size=8, image_size=(18, 18), label_mode='int')
    batch = next(iter(dataset))
    self.assertLen(batch, 2)
    self.assertEqual(batch[0].shape, (8, 18, 18, 3))
    self.assertEqual(batch[0].dtype.name, 'float32')
    self.assertEqual(batch[1].shape, (8,))
    self.assertEqual(batch[1].dtype.name, 'int32')

    dataset = image_dataset.image_dataset_from_directory(
        directory, batch_size=8, image_size=(18, 18), label_mode='categorical')
    batch = next(iter(dataset))
    self.assertLen(batch, 2)
    self.assertEqual(batch[0].shape, (8, 18, 18, 3))
    self.assertEqual(batch[0].dtype.name, 'float32')
    self.assertEqual(batch[1].shape, (8, 4))
    self.assertEqual(batch[1].dtype.name, 'float32')

</source>
</class>

<class classid="173" nclones="2" nlines="42" similarity="88">
<source file="systems/keras-2.7.0/keras/preprocessing/text_dataset_test.py" startline="201" endline="251" pcid="3604">
  def test_text_dataset_from_directory_errors(self):
    directory = self._prepare_directory(num_classes=3, count=5)

    with self.assertRaisesRegex(ValueError, '`labels` argument should be'):
      _ = text_dataset.text_dataset_from_directory(
          directory, labels='other')

    with self.assertRaisesRegex(ValueError, '`label_mode` argument must be'):
      _ = text_dataset.text_dataset_from_directory(
          directory, label_mode='other')

    with self.assertRaisesRegex(
        ValueError, 'only pass `class_names` if `labels="inferred"`'):
      _ = text_dataset.text_dataset_from_directory(
          directory, labels=[0, 0, 1, 1, 1],
          class_names=['class_0', 'class_1', 'class_2'])

    with self.assertRaisesRegex(
        ValueError,
        'Expected the lengths of `labels` to match the number of files'):
      _ = text_dataset.text_dataset_from_directory(
          directory, labels=[0, 0, 1, 1])

    with self.assertRaisesRegex(
        ValueError, '`class_names` passed did not match'):
      _ = text_dataset.text_dataset_from_directory(
          directory, class_names=['class_0', 'class_2'])

    with self.assertRaisesRegex(ValueError, 'there must be exactly 2'):
      _ = text_dataset.text_dataset_from_directory(
          directory, label_mode='binary')

    with self.assertRaisesRegex(ValueError,
                                '`validation_split` must be between 0 and 1'):
      _ = text_dataset.text_dataset_from_directory(
          directory, validation_split=2)

    with self.assertRaisesRegex(ValueError,
                                '`subset` must be either "training" or'):
      _ = text_dataset.text_dataset_from_directory(
          directory, validation_split=0.2, subset='other')

    with self.assertRaisesRegex(ValueError, '`validation_split` must be set'):
      _ = text_dataset.text_dataset_from_directory(
          directory, validation_split=0, subset='training')

    with self.assertRaisesRegex(ValueError, 'must provide a `seed`'):
      _ = text_dataset.text_dataset_from_directory(
          directory, validation_split=0.2, subset='training')


</source>
<source file="systems/keras-2.7.0/keras/preprocessing/image_dataset_test.py" startline="292" endline="349" pcid="3630">
  def test_image_dataset_from_directory_errors(self):
    if PIL is None:
      return  # Skip test if PIL is not available.

    directory = self._prepare_directory(num_classes=3, count=5)

    with self.assertRaisesRegex(ValueError, '`labels` argument should be'):
      _ = image_dataset.image_dataset_from_directory(
          directory, labels='other')

    with self.assertRaisesRegex(ValueError, '`label_mode` argument must be'):
      _ = image_dataset.image_dataset_from_directory(
          directory, label_mode='other')

    with self.assertRaisesRegex(ValueError, '`color_mode` must be one of'):
      _ = image_dataset.image_dataset_from_directory(
          directory, color_mode='other')

    with self.assertRaisesRegex(
        ValueError, 'only pass `class_names` if `labels="inferred"`'):
      _ = image_dataset.image_dataset_from_directory(
          directory, labels=[0, 0, 1, 1, 1],
          class_names=['class_0', 'class_1', 'class_2'])

    with self.assertRaisesRegex(
        ValueError,
        'Expected the lengths of `labels` to match the number of files'):
      _ = image_dataset.image_dataset_from_directory(
          directory, labels=[0, 0, 1, 1])

    with self.assertRaisesRegex(
        ValueError, '`class_names` passed did not match'):
      _ = image_dataset.image_dataset_from_directory(
          directory, class_names=['class_0', 'class_2'])

    with self.assertRaisesRegex(ValueError, 'there must be exactly 2'):
      _ = image_dataset.image_dataset_from_directory(
          directory, label_mode='binary')

    with self.assertRaisesRegex(ValueError,
                                '`validation_split` must be between 0 and 1'):
      _ = image_dataset.image_dataset_from_directory(
          directory, validation_split=2)

    with self.assertRaisesRegex(ValueError,
                                '`subset` must be either "training" or'):
      _ = image_dataset.image_dataset_from_directory(
          directory, validation_split=0.2, subset='other')

    with self.assertRaisesRegex(ValueError, '`validation_split` must be set'):
      _ = image_dataset.image_dataset_from_directory(
          directory, validation_split=0, subset='training')

    with self.assertRaisesRegex(ValueError, 'must provide a `seed`'):
      _ = image_dataset.image_dataset_from_directory(
          directory, validation_split=0.2, subset='training')


</source>
</class>

<class classid="174" nclones="3" nlines="16" similarity="70">
<source file="systems/keras-2.7.0/keras/preprocessing/timeseries_test.py" startline="25" endline="45" pcid="3641">
  def test_basics(self):
    # Test ordering, targets, sequence length, batch size
    data = np.arange(100)
    targets = data * 2
    dataset = timeseries.timeseries_dataset_from_array(
        data, targets, sequence_length=9, batch_size=5)
    # Expect 19 batches
    for i, batch in enumerate(dataset):
      self.assertLen(batch, 2)
      inputs, targets = batch
      if i < 18:
        self.assertEqual(inputs.shape, (5, 9))
      if i == 18:
        # Last batch: size 2
        self.assertEqual(inputs.shape, (2, 9))
      # Check target values
      self.assertAllClose(targets, inputs[:, 0] * 2)
      for j in range(min(5, len(inputs))):
        # Check each sample in the batch
        self.assertAllClose(inputs[j], np.arange(i * 5 + j, i * 5 + j + 9))

</source>
<source file="systems/keras-2.7.0/keras/preprocessing/timeseries_test.py" startline="100" endline="120" pcid="3645">
  def test_sampling_rate(self):
    data = np.arange(100)
    targets = data * 2
    dataset = timeseries.timeseries_dataset_from_array(
        data, targets, sequence_length=9, batch_size=5, sampling_rate=2)
    for i, batch in enumerate(dataset):
      self.assertLen(batch, 2)
      inputs, targets = batch
      if i < 16:
        self.assertEqual(inputs.shape, (5, 9))
      if i == 16:
        # Last batch: size 3
        self.assertEqual(inputs.shape, (3, 9))
      # Check target values
      self.assertAllClose(inputs[:, 0] * 2, targets)
      for j in range(min(5, len(inputs))):
        # Check each sample in the batch
        start_index = i * 5 + j
        end_index = start_index + 9 * 2
        self.assertAllClose(inputs[j], np.arange(start_index, end_index, 2))

</source>
<source file="systems/keras-2.7.0/keras/preprocessing/timeseries_test.py" startline="121" endline="142" pcid="3646">
  def test_sequence_stride(self):
    data = np.arange(100)
    targets = data * 2
    dataset = timeseries.timeseries_dataset_from_array(
        data, targets, sequence_length=9, batch_size=5, sequence_stride=3)
    for i, batch in enumerate(dataset):
      self.assertLen(batch, 2)
      inputs, targets = batch
      if i < 6:
        self.assertEqual(inputs.shape, (5, 9))
      if i == 6:
        # Last batch: size 1
        self.assertEqual(inputs.shape, (1, 9))
      # Check target values
      self.assertAllClose(inputs[:, 0] * 2, targets)
      for j in range(min(5, len(inputs))):
        # Check each sample in the batch
        start_index = i * 5 * 3 + j * 3
        end_index = start_index + 9
        self.assertAllClose(inputs[j],
                            np.arange(start_index, end_index))

</source>
</class>

<class classid="175" nclones="3" nlines="24" similarity="75">
<source file="systems/keras-2.7.0/keras/premade/linear_test.py" startline="142" endline="164" pcid="3668">
  def test_linear_model_with_feature_column(self):
    vocab_list = ['alpha', 'beta', 'gamma']
    vocab_val = [0.4, 0.6, 0.9]
    data = np.random.choice(vocab_list, size=256)
    y = np.zeros_like(data, dtype=np.float32)
    for vocab, val in zip(vocab_list, vocab_val):
      indices = np.where(data == vocab)
      y[indices] = val + np.random.uniform(
          low=-0.01, high=0.01, size=indices[0].shape)
    cat_column = tf.feature_column.categorical_column_with_vocabulary_list(
        key='symbol', vocabulary_list=vocab_list)
    ind_column = tf.feature_column.indicator_column(cat_column)
    dense_feature_layer = dense_features_v2.DenseFeatures([ind_column])
    linear_model = linear.LinearModel(
        use_bias=False, kernel_initializer='zeros')
    combined = sequential.Sequential([dense_feature_layer, linear_model])
    opt = gradient_descent.SGD(learning_rate=0.1)
    combined.compile(opt, 'mse', [])
    combined.fit(x={'symbol': data}, y=y, batch_size=32, epochs=10)
    self.assertAllClose([[0.4], [0.6], [0.9]],
                        combined.layers[1].dense_layers[0].kernel.numpy(),
                        atol=0.01)

</source>
<source file="systems/keras-2.7.0/keras/premade/wide_deep_test.py" startline="185" endline="213" pcid="3682">
  def test_wide_deep_model_with_single_feature_column(self):
    vocab_list = ['alpha', 'beta', 'gamma']
    vocab_val = [0.4, 0.6, 0.9]
    data = np.random.choice(vocab_list, size=256)
    y = np.zeros_like(data, dtype=np.float32)
    for vocab, val in zip(vocab_list, vocab_val):
      indices = np.where(data == vocab)
      y[indices] = val + np.random.uniform(
          low=-0.01, high=0.01, size=indices[0].shape)
    cat_column = tf.feature_column.categorical_column_with_vocabulary_list(
        key='symbol', vocabulary_list=vocab_list)
    ind_column = tf.feature_column.indicator_column(cat_column)
    dense_feature_layer = dense_features_v2.DenseFeatures([ind_column])
    linear_model = linear.LinearModel(
        use_bias=False, kernel_initializer='zeros')
    dnn_model = sequential.Sequential([core.Dense(units=1)])
    wide_deep_model = wide_deep.WideDeepModel(linear_model, dnn_model)
    combined = sequential.Sequential([dense_feature_layer, wide_deep_model])
    opt = gradient_descent.SGD(learning_rate=0.1)
    combined.compile(
        opt,
        'mse', [],
        run_eagerly=testing_utils.should_run_eagerly())
    combined.fit(x={'symbol': data}, y=y, batch_size=32, epochs=10)

  # This test is an example for cases where linear and dnn model accepts
  # same raw input but different transformed inputs, i.e,. the raw input is
  # categorical, and linear model accepts one hot encoding, while dnn model
  # accepts embedding encoding.
</source>
<source file="systems/keras-2.7.0/keras/premade/wide_deep_test.py" startline="214" endline="242" pcid="3683">
  def test_wide_deep_model_with_two_feature_columns(self):
    vocab_list = ['alpha', 'beta', 'gamma']
    vocab_val = [0.4, 0.6, 0.9]
    data = np.random.choice(vocab_list, size=256)
    y = np.zeros_like(data, dtype=np.float32)
    for vocab, val in zip(vocab_list, vocab_val):
      indices = np.where(data == vocab)
      y[indices] = val + np.random.uniform(
          low=-0.01, high=0.01, size=indices[0].shape)
    cat_column = tf.feature_column.categorical_column_with_vocabulary_list(
        key='symbol', vocabulary_list=vocab_list)
    ind_column = tf.feature_column.indicator_column(cat_column)
    emb_column = tf.feature_column.embedding_column(cat_column, dimension=5)
    linear_feature_layer = dense_features_v2.DenseFeatures([ind_column])
    linear_model = linear.LinearModel(
        use_bias=False, kernel_initializer='zeros')
    combined_linear = sequential.Sequential(
        [linear_feature_layer, linear_model])
    dnn_model = sequential.Sequential([core.Dense(units=1)])
    dnn_feature_layer = dense_features_v2.DenseFeatures([emb_column])
    combined_dnn = sequential.Sequential([dnn_feature_layer, dnn_model])
    wide_deep_model = wide_deep.WideDeepModel(combined_linear, combined_dnn)
    opt = gradient_descent.SGD(learning_rate=0.1)
    wide_deep_model.compile(
        opt,
        'mse', [],
        run_eagerly=testing_utils.should_run_eagerly())
    wide_deep_model.fit(x={'symbol': data}, y=y, batch_size=32, epochs=10)

</source>
</class>

<class classid="176" nclones="3" nlines="14" similarity="73">
<source file="systems/keras-2.7.0/keras/premade/wide_deep_test.py" startline="35" endline="50" pcid="3675">
  def test_wide_deep_model(self):
    linear_model = linear.LinearModel(units=1)
    dnn_model = sequential.Sequential([core.Dense(units=1, input_dim=3)])
    wide_deep_model = wide_deep.WideDeepModel(linear_model, dnn_model)
    linear_inp = np.random.uniform(low=-5., high=5., size=(64, 2))
    dnn_inp = np.random.uniform(low=-5., high=5., size=(64, 3))
    inputs = [linear_inp, dnn_inp]
    output = .3 * linear_inp[:, 0] + .2 * dnn_inp[:, 1]
    wide_deep_model.compile(
        optimizer=['sgd', 'adam'],
        loss='mse',
        metrics=[],
        run_eagerly=testing_utils.should_run_eagerly())
    wide_deep_model.fit(inputs, output, epochs=5)
    self.assertTrue(wide_deep_model.built)

</source>
<source file="systems/keras-2.7.0/keras/premade/wide_deep_test.py" startline="77" endline="89" pcid="3677">
  def test_wide_deep_model_with_single_input(self):
    linear_model = linear.LinearModel(units=1)
    dnn_model = sequential.Sequential([core.Dense(units=1, input_dim=3)])
    wide_deep_model = wide_deep.WideDeepModel(linear_model, dnn_model)
    inputs = np.random.uniform(low=-5., high=5., size=(64, 3))
    output = .3 * inputs[:, 0]
    wide_deep_model.compile(
        optimizer=['sgd', 'adam'],
        loss='mse',
        metrics=[],
        run_eagerly=testing_utils.should_run_eagerly())
    wide_deep_model.fit(inputs, output, epochs=5)

</source>
<source file="systems/keras-2.7.0/keras/premade/wide_deep_test.py" startline="114" endline="129" pcid="3679">
  def test_wide_deep_model_with_single_optimizer(self):
    linear_model = linear.LinearModel(units=1)
    dnn_model = sequential.Sequential([core.Dense(units=1, input_dim=3)])
    wide_deep_model = wide_deep.WideDeepModel(linear_model, dnn_model)
    linear_inp = np.random.uniform(low=-5., high=5., size=(64, 2))
    dnn_inp = np.random.uniform(low=-5., high=5., size=(64, 3))
    inputs = [linear_inp, dnn_inp]
    output = .3 * linear_inp[:, 0] + .2 * dnn_inp[:, 1]
    wide_deep_model.compile(
        optimizer='sgd',
        loss='mse',
        metrics=[],
        run_eagerly=testing_utils.should_run_eagerly())
    wide_deep_model.fit(inputs, output, epochs=5)
    self.assertTrue(wide_deep_model.built)

</source>
</class>

<class classid="177" nclones="2" nlines="12" similarity="75">
<source file="systems/keras-2.7.0/keras/layers/lstm_v2_test.py" startline="74" endline="88" pcid="3689">
  def test_static_shape_inference_LSTM(self):
    # Github issue: 15165
    timesteps = 3
    embedding_dim = 4
    units = 2

    model = keras.models.Sequential()
    inputs = keras.layers.Dense(
        embedding_dim, input_shape=(timesteps, embedding_dim))
    model.add(inputs)
    layer = rnn.LSTM(units, return_sequences=True)
    model.add(layer)
    outputs = model.layers[-1].output
    self.assertEqual(outputs.shape.as_list(), [None, timesteps, units])

</source>
<source file="systems/keras-2.7.0/keras/layers/lstm_test.py" startline="60" endline="74" pcid="3953">
  def test_static_shape_inference_LSTM(self):
    # Github issue: 15165
    timesteps = 3
    embedding_dim = 4
    units = 2

    model = keras.models.Sequential()
    inputs = keras.layers.Dense(embedding_dim,
                                input_shape=(timesteps, embedding_dim))
    model.add(inputs)
    layer = keras.layers.LSTM(units, return_sequences=True)
    model.add(layer)
    outputs = model.layers[-1].output
    self.assertEqual(outputs.shape.as_list(), [None, timesteps, units])

</source>
</class>

<class classid="178" nclones="5" nlines="12" similarity="73">
<source file="systems/keras-2.7.0/keras/layers/lstm_v2_test.py" startline="89" endline="101" pcid="3690">
  def test_dynamic_behavior_LSTM(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    layer = rnn.LSTM(units, input_shape=(None, embedding_dim))
    model = keras.models.Sequential()
    model.add(layer)
    model.compile(tf.compat.v1.train.GradientDescentOptimizer(0.001), 'mse')
    x = np.random.random((num_samples, timesteps, embedding_dim))
    y = np.random.random((num_samples, units))
    model.train_on_batch(x, y)

</source>
<source file="systems/keras-2.7.0/keras/layers/simplernn_test.py" startline="57" endline="69" pcid="3974">
  def test_dynamic_behavior_SimpleRNN(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    layer = keras.layers.SimpleRNN(units, input_shape=(None, embedding_dim))
    model = keras.models.Sequential()
    model.add(layer)
    model.compile('rmsprop', 'mse')
    x = np.random.random((num_samples, timesteps, embedding_dim))
    y = np.random.random((num_samples, units))
    model.train_on_batch(x, y)

</source>
<source file="systems/keras-2.7.0/keras/layers/gru_v2_test.py" startline="103" endline="115" pcid="4297">
  def test_dynamic_behavior_GRU(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    layer = rnn.GRU(units, input_shape=(None, embedding_dim))
    model = keras.models.Sequential()
    model.add(layer)
    model.compile(tf.compat.v1.train.GradientDescentOptimizer(0.001), 'mse')
    x = np.random.random((num_samples, timesteps, embedding_dim))
    y = np.random.random((num_samples, units))
    model.train_on_batch(x, y)

</source>
<source file="systems/keras-2.7.0/keras/layers/lstm_test.py" startline="75" endline="91" pcid="3954">
  def test_dynamic_behavior_LSTM(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    layer = keras.layers.LSTM(units, input_shape=(None, embedding_dim))
    model = keras.models.Sequential()
    model.add(layer)
    model.compile(
        'rmsprop',
        'mse',
        run_eagerly=testing_utils.should_run_eagerly())

    x = np.random.random((num_samples, timesteps, embedding_dim))
    y = np.random.random((num_samples, units))
    model.train_on_batch(x, y)

</source>
<source file="systems/keras-2.7.0/keras/layers/gru_test.py" startline="62" endline="77" pcid="4508">
  def test_dynamic_behavior_GRU(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    layer = keras.layers.GRU(units, input_shape=(None, embedding_dim))
    model = keras.models.Sequential()
    model.add(layer)
    model.compile(
        'rmsprop',
        'mse',
        run_eagerly=testing_utils.should_run_eagerly())
    x = np.random.random((num_samples, timesteps, embedding_dim))
    y = np.random.random((num_samples, units))
    model.train_on_batch(x, y)

</source>
</class>

<class classid="179" nclones="4" nlines="11" similarity="91">
<source file="systems/keras-2.7.0/keras/layers/lstm_v2_test.py" startline="102" endline="113" pcid="3691">
  def test_stacking_LSTM(self):
    inputs = np.random.random((2, 3, 4))
    targets = np.abs(np.random.random((2, 3, 5)))
    targets /= targets.sum(axis=-1, keepdims=True)
    model = keras.models.Sequential()
    model.add(rnn.LSTM(10, return_sequences=True, unroll=False))
    model.add(rnn.LSTM(5, return_sequences=True, unroll=False))
    model.compile(
        loss='categorical_crossentropy',
        optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01))
    model.fit(inputs, targets, epochs=1, batch_size=2, verbose=1)

</source>
<source file="systems/keras-2.7.0/keras/layers/gru_v2_test.py" startline="116" endline="127" pcid="4298">
  def test_stacking_GRU(self):
    inputs = np.random.random((2, 3, 4))
    targets = np.abs(np.random.random((2, 3, 5)))
    targets /= targets.sum(axis=-1, keepdims=True)
    model = keras.models.Sequential()
    model.add(rnn.GRU(10, return_sequences=True, unroll=False))
    model.add(rnn.GRU(5, return_sequences=True, unroll=False))
    model.compile(
        loss='categorical_crossentropy',
        optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01))
    model.fit(inputs, targets, epochs=1, batch_size=2, verbose=1)

</source>
<source file="systems/keras-2.7.0/keras/layers/gru_v2_test.py" startline="325" endline="337" pcid="4307">
  def test_masking_with_stacking_GRU(self):
    inputs = np.random.random((2, 3, 4))
    targets = np.abs(np.random.random((2, 3, 5)))
    targets /= targets.sum(axis=-1, keepdims=True)
    model = keras.models.Sequential()
    model.add(keras.layers.Masking(input_shape=(3, 4)))
    model.add(rnn.GRU(10, return_sequences=True, unroll=False))
    model.add(rnn.GRU(5, return_sequences=True, unroll=False))
    model.compile(
        loss='categorical_crossentropy',
        optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01))
    model.fit(inputs, targets, epochs=1, batch_size=2, verbose=1)

</source>
<source file="systems/keras-2.7.0/keras/layers/lstm_v2_test.py" startline="405" endline="417" pcid="3702">
  def test_masking_with_stacking_LSTM(self):
    inputs = np.random.random((2, 3, 4))
    targets = np.abs(np.random.random((2, 3, 5)))
    targets /= targets.sum(axis=-1, keepdims=True)
    model = keras.models.Sequential()
    model.add(keras.layers.Masking(input_shape=(3, 4)))
    model.add(rnn.LSTM(10, return_sequences=True, unroll=False))
    model.add(rnn.LSTM(5, return_sequences=True, unroll=False))
    model.compile(
        loss='categorical_crossentropy',
        optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01))
    model.fit(inputs, targets, epochs=1, batch_size=2, verbose=1)

</source>
</class>

<class classid="180" nclones="7" nlines="24" similarity="71">
<source file="systems/keras-2.7.0/keras/layers/lstm_v2_test.py" startline="121" endline="151" pcid="3693">
  def test_specify_initial_state_keras_tensor(self):
    num_states = 2
    timesteps = 3
    embedding_dim = 4
    units = 3
    num_samples = 2

    # Test with Keras tensor
    inputs = keras.Input((timesteps, embedding_dim))
    initial_state = [keras.Input((units,)) for _ in range(num_states)]
    layer = rnn.LSTM(units)
    if len(initial_state) == 1:
      output = layer(inputs, initial_state=initial_state[0])
    else:
      output = layer(inputs, initial_state=initial_state)
    self.assertTrue(
        any(initial_state[0] is t
            for t in layer._inbound_nodes[0].input_tensors))

    model = keras.models.Model([inputs] + initial_state, output)
    model.compile(
        loss='categorical_crossentropy',
        optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01))

    inputs = np.random.random((num_samples, timesteps, embedding_dim))
    initial_state = [
        np.random.random((num_samples, units)) for _ in range(num_states)
    ]
    targets = np.random.random((num_samples, units))
    model.train_on_batch([inputs] + initial_state, targets)

</source>
<source file="systems/keras-2.7.0/keras/layers/lstm_test.py" startline="187" endline="217" pcid="3963">
  def test_specify_initial_state_keras_tensor(self):
    num_states = 2
    timesteps = 3
    embedding_dim = 4
    units = 3
    num_samples = 2

    # Test with Keras tensor
    inputs = keras.Input((timesteps, embedding_dim))
    initial_state = [keras.Input((units,)) for _ in range(num_states)]
    layer = keras.layers.LSTM(units)
    if len(initial_state) == 1:
      output = layer(inputs, initial_state=initial_state[0])
    else:
      output = layer(inputs, initial_state=initial_state)
    self.assertTrue(
        any(initial_state[0] is t
            for t in layer._inbound_nodes[0].input_tensors))

    model = keras.models.Model([inputs] + initial_state, output)
    model.compile(
        loss='categorical_crossentropy',
        optimizer=tf.compat.v1.train.AdamOptimizer(),
        run_eagerly=testing_utils.should_run_eagerly())

    inputs = np.random.random((num_samples, timesteps, embedding_dim))
    initial_state = [np.random.random((num_samples, units))
                     for _ in range(num_states)]
    targets = np.random.random((num_samples, units))
    model.train_on_batch([inputs] + initial_state, targets)

</source>
<source file="systems/keras-2.7.0/keras/layers/lstm_v2_test.py" startline="214" endline="238" pcid="3696">
  def test_specify_state_with_masking(self):
    num_states = 2
    timesteps = 3
    embedding_dim = 4
    units = 3
    num_samples = 2

    inputs = keras.Input((timesteps, embedding_dim))
    _ = keras.layers.Masking()(inputs)
    initial_state = [keras.Input((units,)) for _ in range(num_states)]
    output = rnn.LSTM(units)(
        inputs, initial_state=initial_state)

    model = keras.models.Model([inputs] + initial_state, output)
    model.compile(
        loss='categorical_crossentropy',
        optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01))

    inputs = np.random.random((num_samples, timesteps, embedding_dim))
    initial_state = [
        np.random.random((num_samples, units)) for _ in range(num_states)
    ]
    targets = np.random.random((num_samples, units))
    model.train_on_batch([inputs] + initial_state, targets)

</source>
<source file="systems/keras-2.7.0/keras/layers/lstm_v2_test.py" startline="278" endline="308" pcid="3699">
  def test_initial_states_as_other_inputs(self):
    timesteps = 3
    embedding_dim = 4
    units = 3
    num_samples = 2
    num_states = 2
    layer_class = rnn.LSTM

    # Test with Keras tensor
    main_inputs = keras.Input((timesteps, embedding_dim))
    initial_state = [keras.Input((units,)) for _ in range(num_states)]
    inputs = [main_inputs] + initial_state

    layer = layer_class(units)
    output = layer(inputs)
    self.assertTrue(
        any(initial_state[0] is t
            for t in layer._inbound_nodes[0].input_tensors))

    model = keras.models.Model(inputs, output)
    model.compile(
        loss='categorical_crossentropy',
        optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01))

    main_inputs = np.random.random((num_samples, timesteps, embedding_dim))
    initial_state = [
        np.random.random((num_samples, units)) for _ in range(num_states)
    ]
    targets = np.random.random((num_samples, units))
    model.train_on_batch([main_inputs] + initial_state, targets)

</source>
<source file="systems/keras-2.7.0/keras/layers/cudnn_recurrent_test.py" startline="124" endline="155" pcid="4524">
  def test_specify_initial_state_keras_tensor(self, layer_class):
    input_size = 10
    timesteps = 6
    units = 2
    num_samples = 32
    num_states = 2 if layer_class is keras.layers.CuDNNLSTM else 1

    inputs = keras.Input((timesteps, input_size))
    initial_state = [keras.Input((units,)) for _ in range(num_states)]
    layer = layer_class(units)
    if len(initial_state) == 1:
      output = layer(inputs, initial_state=initial_state[0])
    else:
      output = layer(inputs, initial_state=initial_state)
    self.assertTrue(
        any(initial_state[0] is t
            for t in layer._inbound_nodes[0].input_tensors))

    model = keras.models.Model([inputs] + initial_state, output)
    model.compile(
        loss='categorical_crossentropy',
        optimizer=RMSprop(learning_rate=0.001),
        run_eagerly=testing_utils.should_run_eagerly())

    inputs = np.random.random((num_samples, timesteps, input_size))
    initial_state = [
        np.random.random((num_samples, units)) for _ in range(num_states)
    ]
    targets = np.random.random((num_samples, units))
    model.fit([inputs] + initial_state, targets)


</source>
<source file="systems/keras-2.7.0/keras/layers/lstm_test.py" startline="331" endline="361" pcid="3969">
  def test_initial_states_as_other_inputs(self):
    timesteps = 3
    embedding_dim = 4
    units = 3
    num_samples = 2
    num_states = 2
    layer_class = keras.layers.LSTM

    # Test with Keras tensor
    main_inputs = keras.Input((timesteps, embedding_dim))
    initial_state = [keras.Input((units,)) for _ in range(num_states)]
    inputs = [main_inputs] + initial_state

    layer = layer_class(units)
    output = layer(inputs)
    self.assertTrue(
        any(initial_state[0] is t
            for t in layer._inbound_nodes[0].input_tensors))

    model = keras.models.Model(inputs, output)
    model.compile(
        loss='categorical_crossentropy',
        optimizer=tf.compat.v1.train.AdamOptimizer(),
        run_eagerly=testing_utils.should_run_eagerly())

    main_inputs = np.random.random((num_samples, timesteps, embedding_dim))
    initial_state = [np.random.random((num_samples, units))
                     for _ in range(num_states)]
    targets = np.random.random((num_samples, units))
    model.train_on_batch([main_inputs] + initial_state, targets)

</source>
<source file="systems/keras-2.7.0/keras/layers/lstm_test.py" startline="273" endline="296" pcid="3966">
  def test_specify_state_with_masking(self):
    num_states = 2
    timesteps = 3
    embedding_dim = 4
    units = 3
    num_samples = 2

    inputs = keras.Input((timesteps, embedding_dim))
    _ = keras.layers.Masking()(inputs)
    initial_state = [keras.Input((units,)) for _ in range(num_states)]
    output = keras.layers.LSTM(units)(inputs, initial_state=initial_state)

    model = keras.models.Model([inputs] + initial_state, output)
    model.compile(
        loss='categorical_crossentropy',
        optimizer='rmsprop',
        run_eagerly=testing_utils.should_run_eagerly())

    inputs = np.random.random((num_samples, timesteps, embedding_dim))
    initial_state = [np.random.random((num_samples, units))
                     for _ in range(num_states)]
    targets = np.random.random((num_samples, units))
    model.train_on_batch([inputs] + initial_state, targets)

</source>
</class>

<class classid="181" nclones="2" nlines="19" similarity="73">
<source file="systems/keras-2.7.0/keras/layers/lstm_v2_test.py" startline="152" endline="176" pcid="3694">
  def test_specify_initial_state_non_keras_tensor(self):
    num_states = 2
    timesteps = 3
    embedding_dim = 4
    units = 3
    num_samples = 2

    # Test with non-Keras tensor
    inputs = keras.Input((timesteps, embedding_dim))
    initial_state = [
        keras.backend.random_normal_variable((num_samples, units), 0, 1)
        for _ in range(num_states)
    ]
    layer = rnn.LSTM(units)
    output = layer(inputs, initial_state=initial_state)

    model = keras.models.Model(inputs, output)
    model.compile(
        loss='categorical_crossentropy',
        optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01))

    inputs = np.random.random((num_samples, timesteps, embedding_dim))
    targets = np.random.random((num_samples, units))
    model.train_on_batch(inputs, targets)

</source>
<source file="systems/keras-2.7.0/keras/layers/lstm_test.py" startline="218" endline="242" pcid="3964">
  def test_specify_initial_state_non_keras_tensor(self):
    num_states = 2
    timesteps = 3
    embedding_dim = 4
    units = 3
    num_samples = 2

    # Test with non-Keras tensor
    inputs = keras.Input((timesteps, embedding_dim))
    initial_state = [keras.backend.random_normal_variable(
        (num_samples, units), 0, 1)
                     for _ in range(num_states)]
    layer = keras.layers.LSTM(units)
    output = layer(inputs, initial_state=initial_state)

    model = keras.models.Model(inputs, output)
    model.compile(
        loss='categorical_crossentropy',
        optimizer=tf.compat.v1.train.AdamOptimizer(),
        run_eagerly=testing_utils.should_run_eagerly())

    inputs = np.random.random((num_samples, timesteps, embedding_dim))
    targets = np.random.random((num_samples, units))
    model.train_on_batch(inputs, targets)

</source>
</class>

<class classid="182" nclones="2" nlines="29" similarity="80">
<source file="systems/keras-2.7.0/keras/layers/lstm_v2_test.py" startline="177" endline="213" pcid="3695">
  def test_reset_states_with_values(self):
    num_states = 2
    timesteps = 3
    embedding_dim = 4
    units = 3
    num_samples = 2

    layer = rnn.LSTM(units, stateful=True)
    layer.build((num_samples, timesteps, embedding_dim))
    initial_weight_count = len(layer.weights)
    layer.reset_states()
    assert len(layer.states) == num_states
    assert layer.states[0] is not None
    self.assertAllClose(
        keras.backend.eval(layer.states[0]),
        np.zeros(keras.backend.int_shape(layer.states[0])),
        atol=1e-4)
    state_shapes = [keras.backend.int_shape(state) for state in layer.states]
    values = [np.ones(shape) for shape in state_shapes]
    if len(values) == 1:
      values = values[0]
    layer.reset_states(values)
    self.assertAllClose(
        keras.backend.eval(layer.states[0]),
        np.ones(keras.backend.int_shape(layer.states[0])),
        atol=1e-4)

    # Test with invalid data
    with self.assertRaises(ValueError):
      layer.reset_states([1] * (len(layer.states) + 1))

    self.assertEqual(initial_weight_count, len(layer.weights))
    # Variables in "states" shouldn't show up in .weights
    layer.states = tf.nest.map_structure(tf.Variable, values)
    layer.reset_states()
    self.assertEqual(initial_weight_count, len(layer.weights))

</source>
<source file="systems/keras-2.7.0/keras/layers/lstm_test.py" startline="243" endline="272" pcid="3965">
  def test_reset_states_with_values(self):
    num_states = 2
    timesteps = 3
    embedding_dim = 4
    units = 3
    num_samples = 2

    layer = keras.layers.LSTM(units, stateful=True)
    layer.build((num_samples, timesteps, embedding_dim))
    layer.reset_states()
    assert len(layer.states) == num_states
    assert layer.states[0] is not None
    self.assertAllClose(
        keras.backend.eval(layer.states[0]),
        np.zeros(keras.backend.int_shape(layer.states[0])),
        atol=1e-4)
    state_shapes = [keras.backend.int_shape(state) for state in layer.states]
    values = [np.ones(shape) for shape in state_shapes]
    if len(values) == 1:
      values = values[0]
    layer.reset_states(values)
    self.assertAllClose(
        keras.backend.eval(layer.states[0]),
        np.ones(keras.backend.int_shape(layer.states[0])),
        atol=1e-4)

    # Test with invalid data
    with self.assertRaises(ValueError):
      layer.reset_states([1] * (len(layer.states) + 1))

</source>
</class>

<class classid="183" nclones="2" nlines="16" similarity="87">
<source file="systems/keras-2.7.0/keras/layers/lstm_v2_test.py" startline="242" endline="260" pcid="3697">
  def test_return_state(self):
    num_states = 2
    timesteps = 3
    embedding_dim = 4
    units = 3
    num_samples = 2

    inputs = keras.Input(batch_shape=(num_samples, timesteps, embedding_dim))
    masked = keras.layers.Masking()(inputs)
    layer = rnn.LSTM(units, return_state=True, stateful=True)
    outputs = layer(masked)
    state = outputs[1:]
    assert len(state) == num_states
    model = keras.models.Model(inputs, state[0])

    inputs = np.random.random((num_samples, timesteps, embedding_dim))
    state = model.predict(inputs)
    self.assertAllClose(keras.backend.eval(layer.states[0]), state, atol=1e-4)

</source>
<source file="systems/keras-2.7.0/keras/layers/lstm_test.py" startline="297" endline="314" pcid="3967">
  def test_return_state(self):
    num_states = 2
    timesteps = 3
    embedding_dim = 4
    units = 3
    num_samples = 2

    inputs = keras.Input(batch_shape=(num_samples, timesteps, embedding_dim))
    layer = keras.layers.LSTM(units, return_state=True, stateful=True)
    outputs = layer(inputs)
    state = outputs[1:]
    assert len(state) == num_states
    model = keras.models.Model(inputs, state[0])

    inputs = np.random.random((num_samples, timesteps, embedding_dim))
    state = model.predict(inputs)
    self.assertAllClose(keras.backend.eval(layer.states[0]), state, atol=1e-4)

</source>
</class>

<class classid="184" nclones="2" nlines="14" similarity="71">
<source file="systems/keras-2.7.0/keras/layers/lstm_v2_test.py" startline="261" endline="277" pcid="3698">
  def test_state_reuse(self):
    timesteps = 3
    embedding_dim = 4
    units = 3
    num_samples = 2

    inputs = keras.Input(batch_shape=(num_samples, timesteps, embedding_dim))
    layer = rnn.LSTM(
        units, return_state=True, return_sequences=True)
    outputs = layer(inputs)
    output, state = outputs[0], outputs[1:]
    output = rnn.LSTM(units)(output, initial_state=state)
    model = keras.models.Model(inputs, output)

    inputs = np.random.random((num_samples, timesteps, embedding_dim))
    model.predict(inputs)

</source>
<source file="systems/keras-2.7.0/keras/layers/lstm_test.py" startline="315" endline="330" pcid="3968">
  def test_state_reuse(self):
    timesteps = 3
    embedding_dim = 4
    units = 3
    num_samples = 2

    inputs = keras.Input(batch_shape=(num_samples, timesteps, embedding_dim))
    layer = keras.layers.LSTM(units, return_state=True, return_sequences=True)
    outputs = layer(inputs)
    output, state = outputs[0], outputs[1:]
    output = keras.layers.LSTM(units)(output, initial_state=state)
    model = keras.models.Model(inputs, output)

    inputs = np.random.random((num_samples, timesteps, embedding_dim))
    outputs = model.predict(inputs)

</source>
</class>

<class classid="185" nclones="2" nlines="38" similarity="87">
<source file="systems/keras-2.7.0/keras/layers/lstm_v2_test.py" startline="313" endline="355" pcid="3700">
  def test_lstm_v2_feature_parity_with_canonical_lstm(self):
    input_shape = 10
    rnn_state_size = 8
    timestep = 4
    batch = 20

    (x_train, y_train), _ = testing_utils.get_test_data(
        train_samples=batch,
        test_samples=0,
        input_shape=(timestep, input_shape),
        num_classes=rnn_state_size,
        random_seed=87654321)
    y_train = np_utils.to_categorical(y_train, rnn_state_size)
    # For the last batch item of the test data, we filter out the last
    # timestep to simulate the variable length sequence and masking test.
    x_train[-2:, -1, :] = 0.0
    y_train[-2:] = 0

    inputs = keras.layers.Input(
        shape=[timestep, input_shape], dtype=tf.float32)
    masked_input = keras.layers.Masking()(inputs)
    lstm_layer = rnn_v1.LSTM(rnn_state_size,
                             recurrent_activation='sigmoid')
    output = lstm_layer(masked_input)
    lstm_model = keras.models.Model(inputs, output)
    weights = lstm_model.get_weights()
    y_1 = lstm_model.predict(x_train)
    lstm_model.compile('rmsprop', 'mse')
    lstm_model.fit(x_train, y_train)
    y_2 = lstm_model.predict(x_train)

    with testing_utils.device(should_use_gpu=True):
      cudnn_layer = rnn.LSTM(rnn_state_size)
      cudnn_model = keras.models.Model(inputs, cudnn_layer(masked_input))
    cudnn_model.set_weights(weights)
    y_3 = cudnn_model.predict(x_train)
    cudnn_model.compile('rmsprop', 'mse')
    cudnn_model.fit(x_train, y_train)
    y_4 = cudnn_model.predict(x_train)

    self.assertAllClose(y_1, y_3, rtol=1e-5, atol=2e-5)
    self.assertAllClose(y_2, y_4, rtol=1e-5, atol=2e-5)

</source>
<source file="systems/keras-2.7.0/keras/layers/gru_v2_test.py" startline="139" endline="184" pcid="4300">
  def test_gru_v2_feature_parity_with_canonical_gru(self):
    input_shape = 10
    rnn_state_size = 8
    timestep = 4
    batch = 20

    (x_train, y_train), _ = testing_utils.get_test_data(
        train_samples=batch,
        test_samples=0,
        input_shape=(timestep, input_shape),
        num_classes=rnn_state_size,
        random_seed=87654321)
    y_train = np_utils.to_categorical(y_train, rnn_state_size)
    # For the last batch item of the test data, we filter out the last
    # timestep to simulate the variable length sequence and masking test.
    x_train[-2:, -1, :] = 0.0
    y_train[-2:] = 0

    inputs = keras.layers.Input(
        shape=[timestep, input_shape], dtype=tf.float32)
    masked_input = keras.layers.Masking()(inputs)
    gru_layer = rnn_v1.GRU(rnn_state_size,
                           recurrent_activation='sigmoid',
                           reset_after=True)
    output = gru_layer(masked_input)
    gru_model = keras.models.Model(inputs, output)
    weights = gru_model.get_weights()
    y_1 = gru_model.predict(x_train)
    gru_model.compile('rmsprop', 'mse')
    gru_model.fit(x_train, y_train)
    y_2 = gru_model.predict(x_train)

    with testing_utils.device(should_use_gpu=True):
      cudnn_layer = rnn.GRU(rnn_state_size,
                            recurrent_activation='sigmoid',
                            reset_after=True)
      cudnn_model = keras.models.Model(inputs, cudnn_layer(masked_input))
    cudnn_model.set_weights(weights)
    y_3 = cudnn_model.predict(x_train)
    cudnn_model.compile('rmsprop', 'mse')
    cudnn_model.fit(x_train, y_train)
    y_4 = cudnn_model.predict(x_train)

    self.assertAllClose(y_1, y_3, rtol=2e-5, atol=2e-5)
    self.assertAllClose(y_2, y_4, rtol=2e-5, atol=2e-5)

</source>
</class>

<class classid="186" nclones="2" nlines="18" similarity="94">
<source file="systems/keras-2.7.0/keras/layers/lstm_v2_test.py" startline="433" endline="450" pcid="3704">
    def build_model(layer_cls):
      inputs = keras.layers.Input(
          shape=[timestep, input_shape], dtype=tf.float32)
      layer = layer_cls(rnn_state_size,
                        recurrent_activation='sigmoid',
                        time_major=time_major,
                        return_sequences=True,
                        go_backwards=go_backwards)
      if time_major:
        converted_input = keras.layers.Lambda(
            lambda t: tf.transpose(t, [1, 0, 2]))(inputs)
        outputs = layer(converted_input)
        outputs = keras.layers.Lambda(
            lambda t: tf.transpose(t, [1, 0, 2]))(outputs)
      else:
        outputs = layer(inputs)
      return keras.models.Model(inputs, outputs)

</source>
<source file="systems/keras-2.7.0/keras/layers/gru_v2_test.py" startline="278" endline="296" pcid="4305">
    def build_model(layer_cls):
      inputs = keras.layers.Input(
          shape=[timestep, input_shape], dtype=tf.float32)
      layer = layer_cls(rnn_state_size,
                        recurrent_activation='sigmoid',
                        time_major=time_major,
                        return_sequences=True,
                        go_backwards=go_backwards,
                        reset_after=True)
      if time_major:
        converted_input = keras.layers.Lambda(
            lambda t: tf.transpose(t, [1, 0, 2]))(inputs)
        outputs = layer(converted_input)
        outputs = keras.layers.Lambda(
            lambda t: tf.transpose(t, [1, 0, 2]))(outputs)
      else:
        outputs = layer(inputs)
      return keras.models.Model(inputs, outputs)

</source>
</class>

<class classid="187" nclones="2" nlines="26" similarity="100">
<source file="systems/keras-2.7.0/keras/layers/lstm_v2_test.py" startline="493" endline="525" pcid="3705">
  def test_lstm_model_save_load(self, use_bias, bias_initializer):
    temp_dir = self.get_temp_dir()
    self.addCleanup(shutil.rmtree, temp_dir)
    h5_path = os.path.join(temp_dir, 'test.h5')

    batch = 10
    timestep = 3
    input_dim = 5
    units = 2

    x = np.random.random((batch, timestep, input_dim))

    def build_model():
      inputs = keras.layers.Input(
          shape=[timestep, input_dim], dtype=tf.float32)
      layer = rnn.LSTM(
          units,
          use_bias=use_bias,
          bias_initializer=bias_initializer)
      output = layer(inputs)
      return keras.models.Model(inputs, output), layer

    model, layer = build_model()
    y_ref = model.predict(x)
    model.save_weights(h5_path)

    cloned_model, new_layer = build_model()
    cloned_model.load_weights(h5_path)
    y = cloned_model.predict(x)

    self.assertAllClose(y, y_ref)
    self.assertAllClose(layer.get_weights(), new_layer.get_weights())

</source>
<source file="systems/keras-2.7.0/keras/layers/gru_v2_test.py" startline="191" endline="223" pcid="4301">
  def test_gru_v2_model_save_load(self, use_bias, bias_initializer):
    temp_dir = self.get_temp_dir()
    self.addCleanup(shutil.rmtree, temp_dir)
    h5_path = os.path.join(temp_dir, 'test.h5')

    batch = 10
    timestep = 3
    input_dim = 5
    units = 2

    x = np.random.random((batch, timestep, input_dim))

    def build_model():
      inputs = keras.layers.Input(
          shape=[timestep, input_dim], dtype=tf.float32)
      layer = rnn.GRU(
          units,
          use_bias=use_bias,
          bias_initializer=bias_initializer)
      output = layer(inputs)
      return keras.models.Model(inputs, output), layer

    model, layer = build_model()
    y_ref = model.predict(x)
    model.save_weights(h5_path)

    cloned_model, new_layer = build_model()
    cloned_model.load_weights(h5_path)
    y = cloned_model.predict(x)

    self.assertAllClose(y, y_ref)
    self.assertAllClose(layer.get_weights(), new_layer.get_weights())

</source>
</class>

<class classid="188" nclones="2" nlines="29" similarity="80">
<source file="systems/keras-2.7.0/keras/layers/lstm_v2_test.py" startline="526" endline="563" pcid="3707">
  def test_lstm_output_on_multiple_kernel(self):
    input_shape = 10
    rnn_state_size = 8
    timestep = 4
    batch = 100

    x_train = np.random.random((batch, timestep, input_shape))

    inputs = keras.layers.Input(
        shape=[timestep, input_shape], dtype=tf.float32)
    with testing_utils.device(should_use_gpu=False):
      layer = rnn.LSTM(rnn_state_size)
      output = layer(inputs)
      cpu_model = keras.models.Model(inputs, output)
      weights = cpu_model.get_weights()
    y_1 = cpu_model.predict(x_train)

    with testing_utils.device(should_use_gpu=True):
      layer = rnn.LSTM(rnn_state_size)
      output = layer(inputs)
      gpu_model = keras.models.Model(inputs, output)
      gpu_model.set_weights(weights)
    y_2 = gpu_model.predict(x_train)

    # Note that cuDNN uses 'sigmoid' as activation, so the LSTM V2 uses
    # 'sigmoid' as default. Construct the canonical LSTM with sigmoid to achieve
    # the same output.
    with testing_utils.device(should_use_gpu=True):
      layer = rnn_v1.LSTM(rnn_state_size, recurrent_activation='sigmoid')
      output = layer(inputs)
      canonical_model = keras.models.Model(inputs, output)
      # Remove the extra cudnn bias since canonical lstm will not use it.
      canonical_model.set_weights(weights[:3])
    y_3 = canonical_model.predict(x_train)

    self.assertAllClose(y_1, y_2)
    self.assertAllClose(y_2, y_3)

</source>
<source file="systems/keras-2.7.0/keras/layers/gru_v2_test.py" startline="224" endline="262" pcid="4303">
  def test_gru_v2_output_on_multiple_kernel(self):
    input_shape = 10
    rnn_state_size = 8
    timestep = 4
    batch = 100

    x_train = np.random.random((batch, timestep, input_shape))

    inputs = keras.layers.Input(
        shape=[timestep, input_shape], dtype=tf.float32)
    with testing_utils.device(should_use_gpu=False):
      layer = rnn.GRU(rnn_state_size)
      output = layer(inputs)
      cpu_model = keras.models.Model(inputs, output)
      weights = cpu_model.get_weights()
      y_1 = cpu_model.predict(x_train)

    with testing_utils.device(should_use_gpu=True):
      layer = rnn.GRU(rnn_state_size)
      output = layer(inputs)
      gpu_model = keras.models.Model(inputs, output)
      gpu_model.set_weights(weights)
      y_2 = gpu_model.predict(x_train)

    # Note that cuDNN uses 'sigmoid' as activation, so the GRU V2 uses
    # 'sigmoid' as default. Construct the canonical GRU with sigmoid to achieve
    # the same output.
    with testing_utils.device(should_use_gpu=True):
      layer = rnn_v1.GRU(rnn_state_size,
                         recurrent_activation='sigmoid',
                         reset_after=True)
      output = layer(inputs)
      canonical_model = keras.models.Model(inputs, output)
      canonical_model.set_weights(weights)
      y_3 = canonical_model.predict(x_train)

    self.assertAllClose(y_1, y_2, rtol=1e-5, atol=1e-5)
    self.assertAllClose(y_2, y_3, rtol=1e-5, atol=1e-5)

</source>
</class>

<class classid="189" nclones="25" nlines="10" similarity="70">
<source file="systems/keras-2.7.0/keras/layers/lstm_v2_test.py" startline="564" endline="576" pcid="3708">
  def test_return_sequences_LSTM(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        rnn.LSTM,
        kwargs={
            'units': units,
            'return_sequences': True
        },
        input_shape=(num_samples, timesteps, embedding_dim))

</source>
<source file="systems/keras-2.7.0/keras/layers/lstm_test.py" startline="32" endline="42" pcid="3951">
  def test_return_sequences_LSTM(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        keras.layers.LSTM,
        kwargs={'units': units,
                'return_sequences': True},
        input_shape=(num_samples, timesteps, embedding_dim))

</source>
<source file="systems/keras-2.7.0/keras/layers/gru_v2_test.py" startline="416" endline="426" pcid="4313">
  def test_implementation_mode_GRU(self, implementation_mode):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        rnn.GRU,
        kwargs={'units': units,
                'implementation': implementation_mode},
        input_shape=(num_samples, timesteps, embedding_dim))

</source>
<source file="systems/keras-2.7.0/keras/layers/simplernn_test.py" startline="32" endline="42" pcid="3972">
  def test_return_sequences_SimpleRNN(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        keras.layers.SimpleRNN,
        kwargs={'units': units,
                'return_sequences': True},
        input_shape=(num_samples, timesteps, embedding_dim))

</source>
<source file="systems/keras-2.7.0/keras/layers/simplernn_test.py" startline="82" endline="93" pcid="3976">
  def test_implementation_mode_SimpleRNN(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    for mode in [0, 1, 2]:
      testing_utils.layer_test(
          keras.layers.SimpleRNN,
          kwargs={'units': units,
                  'implementation': mode},
          input_shape=(num_samples, timesteps, embedding_dim))

</source>
<source file="systems/keras-2.7.0/keras/layers/lstm_v2_test.py" startline="715" endline="728" pcid="3713">
  def test_dropout_LSTM(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        rnn.LSTM,
        kwargs={
            'units': units,
            'dropout': 0.1,
            'recurrent_dropout': 0.1
        },
        input_shape=(num_samples, timesteps, embedding_dim))

</source>
<source file="systems/keras-2.7.0/keras/layers/gru_v2_test.py" startline="338" endline="348" pcid="4308">
  def test_return_sequences_GRU(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        rnn.GRU,
        kwargs={'units': units,
                'return_sequences': True},
        input_shape=(num_samples, timesteps, embedding_dim))

</source>
<source file="systems/keras-2.7.0/keras/layers/gru_test.py" startline="34" endline="44" pcid="4506">
  def test_return_sequences_GRU(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        keras.layers.GRU,
        kwargs={'units': units,
                'return_sequences': True},
        input_shape=(num_samples, timesteps, embedding_dim))

</source>
<source file="systems/keras-2.7.0/keras/layers/gru_v2_test.py" startline="384" endline="395" pcid="4311">
  def test_dropout_GRU(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        rnn.GRU,
        kwargs={'units': units,
                'dropout': 0.1,
                'recurrent_dropout': 0.1},
        input_shape=(num_samples, timesteps, embedding_dim))

</source>
<source file="systems/keras-2.7.0/keras/layers/gru_test.py" startline="78" endline="89" pcid="4509">
  def test_dropout_GRU(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        keras.layers.GRU,
        kwargs={'units': units,
                'dropout': 0.1,
                'recurrent_dropout': 0.1},
        input_shape=(num_samples, timesteps, embedding_dim))

</source>
<source file="systems/keras-2.7.0/keras/layers/lstm_test.py" startline="110" endline="120" pcid="3957">
  def test_implementation_mode_LSTM(self, implementation_mode):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        keras.layers.LSTM,
        kwargs={'units': units,
                'implementation': implementation_mode},
        input_shape=(num_samples, timesteps, embedding_dim))

</source>
<source file="systems/keras-2.7.0/keras/layers/cudnn_recurrent_test.py" startline="41" endline="51" pcid="4520">
  def test_cudnn_rnn_return_sequence(self, layer_class, return_sequences):
    input_size = 10
    timesteps = 6
    units = 2
    num_samples = 32
    testing_utils.layer_test(
        layer_class,
        kwargs={'units': units,
                'return_sequences': return_sequences},
        input_shape=(num_samples, timesteps, input_size))

</source>
<source file="systems/keras-2.7.0/keras/layers/gru_test.py" startline="96" endline="106" pcid="4511">
  def test_implementation_mode_GRU(self, implementation_mode):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        keras.layers.GRU,
        kwargs={'units': units,
                'implementation': implementation_mode},
        input_shape=(num_samples, timesteps, embedding_dim))

</source>
<source file="systems/keras-2.7.0/keras/layers/cudnn_recurrent_test.py" startline="57" endline="67" pcid="4521">
  def test_cudnn_rnn_go_backward(self, layer_class, go_backwards):
    input_size = 10
    timesteps = 6
    units = 2
    num_samples = 32
    testing_utils.layer_test(
        layer_class,
        kwargs={'units': units,
                'go_backwards': go_backwards},
        input_shape=(num_samples, timesteps, input_size))

</source>
<source file="systems/keras-2.7.0/keras/layers/lstm_test.py" startline="92" endline="103" pcid="3955">
  def test_dropout_LSTM(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        keras.layers.LSTM,
        kwargs={'units': units,
                'dropout': 0.1,
                'recurrent_dropout': 0.1},
        input_shape=(num_samples, timesteps, embedding_dim))

</source>
<source file="systems/keras-2.7.0/keras/layers/simplernn_test.py" startline="70" endline="81" pcid="3975">
  def test_dropout_SimpleRNN(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        keras.layers.SimpleRNN,
        kwargs={'units': units,
                'dropout': 0.1,
                'recurrent_dropout': 0.1},
        input_shape=(num_samples, timesteps, embedding_dim))

</source>
<source file="systems/keras-2.7.0/keras/layers/separable_convolutional_test.py" startline="98" endline="109" pcid="4693">
  def _run_test(self, kwargs):
    num_samples = 2
    stack_size = 3
    num_row = 7
    num_col = 6

    with self.cached_session():
      testing_utils.layer_test(
          keras.layers.SeparableConv2D,
          kwargs=kwargs,
          input_shape=(num_samples, num_row, num_col, stack_size))

</source>
<source file="systems/keras-2.7.0/keras/layers/convolutional_transpose_test.py" startline="30" endline="41" pcid="4123">
  def _run_test(self, kwargs):
    num_samples = 2
    stack_size = 3
    num_row = 7
    num_col = 6

    with self.cached_session():
      testing_utils.layer_test(
          keras.layers.Conv2DTranspose,
          kwargs=kwargs,
          input_shape=(num_samples, num_row, num_col, stack_size))

</source>
<source file="systems/keras-2.7.0/keras/layers/convolutional_test.py" startline="1169" endline="1180" pcid="3775">
  def _run_test(self, kwargs, expected_output_shape=None):
    num_samples = 2
    stack_size = 3
    num_row = 7

    with self.cached_session():
      testing_utils.layer_test(
          keras.layers.DepthwiseConv1D,
          kwargs=kwargs,
          input_shape=(num_samples, num_row, stack_size),
          expected_output_shape=expected_output_shape)

</source>
<source file="systems/keras-2.7.0/keras/layers/convolutional_test.py" startline="31" endline="42" pcid="3730">
  def _run_test(self, kwargs, expected_output_shape):
    num_samples = 2
    stack_size = 3
    length = 7

    with self.cached_session():
      testing_utils.layer_test(
          keras.layers.Conv1D,
          kwargs=kwargs,
          input_shape=(num_samples, length, stack_size),
          expected_output_shape=expected_output_shape)

</source>
<source file="systems/keras-2.7.0/keras/layers/convolutional_test.py" startline="1232" endline="1244" pcid="3778">
  def _run_test(self, kwargs, expected_output_shape=None):
    num_samples = 2
    stack_size = 3
    num_row = 7
    num_col = 6

    with self.cached_session():
      testing_utils.layer_test(
          keras.layers.DepthwiseConv2D,
          kwargs=kwargs,
          input_shape=(num_samples, num_row, num_col, stack_size),
          expected_output_shape=expected_output_shape)

</source>
<source file="systems/keras-2.7.0/keras/layers/convolutional_transpose_test.py" startline="123" endline="135" pcid="4128">
  def _run_test(self, kwargs):
    num_samples = 2
    stack_size = 3
    num_row = 7
    num_col = 6
    depth = 5

    with self.cached_session():
      testing_utils.layer_test(
          keras.layers.Conv3DTranspose,
          kwargs=kwargs,
          input_shape=(num_samples, depth, num_row, num_col, stack_size))

</source>
<source file="systems/keras-2.7.0/keras/layers/convolutional_test.py" startline="508" endline="519" pcid="3756">
  def _run_test(self, kwargs, expected_output_shape):
    num_samples = 2
    stack_size = 3
    num_col = 6

    with testing_utils.use_gpu():
      testing_utils.layer_test(
          keras.layers.Conv1DTranspose,
          kwargs=kwargs,
          input_shape=(num_samples, num_col, stack_size),
          expected_output_shape=expected_output_shape)

</source>
<source file="systems/keras-2.7.0/keras/layers/convolutional_test.py" startline="541" endline="554" pcid="3758">
  def _run_test(self, kwargs, expected_output_shape):
    num_samples = 2
    stack_size = 3
    num_row = 7
    num_col = 6
    depth = 5

    with testing_utils.use_gpu():
      testing_utils.layer_test(
          keras.layers.Conv3DTranspose,
          kwargs=kwargs,
          input_shape=(num_samples, depth, num_row, num_col, stack_size),
          expected_output_shape=expected_output_shape)

</source>
<source file="systems/keras-2.7.0/keras/layers/convolutional_test.py" startline="314" endline="328" pcid="3746">
  def _run_test(self, kwargs, expected_output_shape, validate_training=True):
    num_samples = 2
    stack_size = 3
    num_row = 7
    num_col = 6
    depth = 5

    with self.cached_session():
      testing_utils.layer_test(
          keras.layers.Conv3D,
          kwargs=kwargs,
          input_shape=(num_samples, depth, num_row, num_col, stack_size),
          expected_output_shape=expected_output_shape,
          validate_training=validate_training)

</source>
</class>

<class classid="190" nclones="5" nlines="13" similarity="76">
<source file="systems/keras-2.7.0/keras/layers/lstm_v2_test.py" startline="581" endline="595" pcid="3709">
  def test_float64_LSTM(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        rnn.LSTM,
        kwargs={
            'units': units,
            'return_sequences': True,
            'dtype': 'float64'
        },
        input_shape=(num_samples, timesteps, embedding_dim),
        input_dtype='float64')

</source>
<source file="systems/keras-2.7.0/keras/layers/gru_test.py" startline="49" endline="61" pcid="4507">
  def test_float64_GRU(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        keras.layers.GRU,
        kwargs={'units': units,
                'return_sequences': True,
                'dtype': 'float64'},
        input_shape=(num_samples, timesteps, embedding_dim),
        input_dtype='float64')

</source>
<source file="systems/keras-2.7.0/keras/layers/simplernn_test.py" startline="44" endline="56" pcid="3973">
  def test_float64_SimpleRNN(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        keras.layers.SimpleRNN,
        kwargs={'units': units,
                'return_sequences': True,
                'dtype': 'float64'},
        input_shape=(num_samples, timesteps, embedding_dim),
        input_dtype='float64')

</source>
<source file="systems/keras-2.7.0/keras/layers/gru_v2_test.py" startline="353" endline="365" pcid="4309">
  def test_float64_GRU(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        rnn.GRU,
        kwargs={'units': units,
                'return_sequences': True,
                'dtype': 'float64'},
        input_shape=(num_samples, timesteps, embedding_dim),
        input_dtype='float64')

</source>
<source file="systems/keras-2.7.0/keras/layers/lstm_test.py" startline="47" endline="59" pcid="3952">
  def test_float64_LSTM(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        keras.layers.LSTM,
        kwargs={'units': units,
                'return_sequences': True,
                'dtype': 'float64'},
        input_shape=(num_samples, timesteps, embedding_dim),
        input_dtype='float64')

</source>
</class>

<class classid="191" nclones="5" nlines="20" similarity="80">
<source file="systems/keras-2.7.0/keras/layers/lstm_v2_test.py" startline="596" endline="616" pcid="3710">
  def test_regularizers_LSTM(self):
    embedding_dim = 4
    layer_class = rnn.LSTM
    layer = layer_class(
        5,
        return_sequences=False,
        weights=None,
        input_shape=(None, embedding_dim),
        kernel_regularizer=keras.regularizers.l1(0.01),
        recurrent_regularizer=keras.regularizers.l1(0.01),
        bias_regularizer='l2',
        activity_regularizer='l1')
    layer.build((None, None, 2))
    self.assertEqual(len(layer.losses), 3)
    x = keras.backend.variable(np.ones((2, 3, 2)))
    layer(x)
    if tf.executing_eagerly():
      self.assertEqual(len(layer.losses), 4)
    else:
      self.assertEqual(len(layer.get_losses_for(x)), 1)

</source>
<source file="systems/keras-2.7.0/keras/layers/gru_test.py" startline="261" endline="283" pcid="4519">
  def test_regularizers_GRU(self):
    embedding_dim = 4
    layer_class = keras.layers.GRU
    layer = layer_class(
        5,
        return_sequences=False,
        weights=None,
        input_shape=(None, embedding_dim),
        kernel_regularizer=keras.regularizers.l1(0.01),
        recurrent_regularizer=keras.regularizers.l1(0.01),
        bias_regularizer='l2',
        activity_regularizer='l1')
    layer.build((None, None, 2))
    self.assertEqual(len(layer.losses), 3)

    x = keras.backend.variable(np.ones((2, 3, 2)))
    layer(x)
    if tf.executing_eagerly():
      self.assertEqual(len(layer.losses), 4)
    else:
      self.assertEqual(len(layer.get_losses_for(x)), 1)


</source>
<source file="systems/keras-2.7.0/keras/layers/lstm_test.py" startline="362" endline="382" pcid="3970">
  def test_regularizers_LSTM(self):
    embedding_dim = 4
    layer_class = keras.layers.LSTM
    layer = layer_class(
        5,
        return_sequences=False,
        weights=None,
        input_shape=(None, embedding_dim),
        kernel_regularizer=keras.regularizers.l1(0.01),
        recurrent_regularizer=keras.regularizers.l1(0.01),
        bias_regularizer='l2',
        activity_regularizer='l1')
    layer.build((None, None, 2))
    self.assertEqual(len(layer.losses), 3)
    x = keras.backend.variable(np.ones((2, 3, 2)))
    layer(x)
    if tf.executing_eagerly():
      self.assertEqual(len(layer.losses), 4)
    else:
      self.assertEqual(len(layer.get_losses_for(x)), 1)

</source>
<source file="systems/keras-2.7.0/keras/layers/simplernn_test.py" startline="137" endline="158" pcid="3981">
  def test_regularizers_SimpleRNN(self):
    embedding_dim = 4
    layer_class = keras.layers.SimpleRNN
    layer = layer_class(
        5,
        return_sequences=False,
        weights=None,
        input_shape=(None, embedding_dim),
        kernel_regularizer=keras.regularizers.l1(0.01),
        recurrent_regularizer=keras.regularizers.l1(0.01),
        bias_regularizer='l2',
        activity_regularizer='l1')
    layer.build((None, None, 2))
    self.assertLen(layer.losses, 3)

    x = keras.backend.variable(np.ones((2, 3, 2)))
    layer(x)
    if tf.executing_eagerly():
      self.assertLen(layer.losses, 4)
    else:
      self.assertLen(layer.get_losses_for(x), 1)

</source>
<source file="systems/keras-2.7.0/keras/layers/gru_v2_test.py" startline="427" endline="448" pcid="4314">
  def test_regularizers_GRU(self):
    embedding_dim = 4
    layer_class = rnn.GRU
    layer = layer_class(
        5,
        return_sequences=False,
        weights=None,
        input_shape=(None, embedding_dim),
        kernel_regularizer=keras.regularizers.l1(0.01),
        recurrent_regularizer=keras.regularizers.l1(0.01),
        bias_regularizer='l2',
        activity_regularizer='l1')
    layer.build((None, None, 2))
    self.assertEqual(len(layer.losses), 3)

    x = keras.backend.variable(np.ones((2, 3, 2)))
    layer(x)
    if tf.executing_eagerly():
      self.assertEqual(len(layer.losses), 4)
    else:
      self.assertEqual(len(layer.get_losses_for(x)), 1)

</source>
</class>

<class classid="192" nclones="5" nlines="50" similarity="79">
<source file="systems/keras-2.7.0/keras/layers/lstm_v2_test.py" startline="620" endline="692" pcid="3711">
  def test_statefulness_LSTM(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    layer_class = rnn.LSTM
    model = keras.models.Sequential()
    model.add(
        keras.layers.Embedding(
            4,
            embedding_dim,
            mask_zero=True,
            input_length=timesteps,
            batch_input_shape=(num_samples, timesteps)))
    layer = layer_class(
        units, return_sequences=False, stateful=True, weights=None)
    model.add(layer)
    model.compile(
        optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01),
        loss='mse',
        run_eagerly=testing_utils.should_run_eagerly())
    out1 = model.predict(np.ones((num_samples, timesteps)))
    self.assertEqual(out1.shape, (num_samples, units))

    # train once so that the states change
    model.train_on_batch(
        np.ones((num_samples, timesteps)), np.ones((num_samples, units)))
    out2 = model.predict(np.ones((num_samples, timesteps)))

    # if the state is not reset, output should be different
    self.assertNotEqual(out1.max(), out2.max())

    # check that output changes after states are reset
    # (even though the model itself didn't change)
    layer.reset_states()
    out3 = model.predict(np.ones((num_samples, timesteps)))
    self.assertNotEqual(out2.max(), out3.max())

    # check that container-level reset_states() works
    model.reset_states()
    out4 = model.predict(np.ones((num_samples, timesteps)))
    self.assertAllClose(out3, out4, atol=1e-5)

    # check that the call to `predict` updated the states
    out5 = model.predict(np.ones((num_samples, timesteps)))
    self.assertNotEqual(out4.max(), out5.max())

    # Check masking
    layer.reset_states()

    left_padded_input = np.ones((num_samples, timesteps))
    left_padded_input[0, :1] = 0
    left_padded_input[1, :2] = 0
    out6 = model.predict(left_padded_input)

    layer.reset_states()

    right_padded_input = np.ones((num_samples, timesteps))
    right_padded_input[0, -1:] = 0
    right_padded_input[1, -2:] = 0
    out7 = model.predict(right_padded_input)

    layer.reset_states()

    mix_padded_input = np.ones((num_samples, timesteps))
    mix_padded_input[0, 1] = 0
    mix_padded_input[1, 0] = 0
    mix_padded_input[1, 2] = 0
    out8 = model.predict(mix_padded_input)

    self.assertAllClose(out7, out6, atol=1e-5)
    self.assertAllClose(out8, out7, atol=1e-5)

</source>
<source file="systems/keras-2.7.0/keras/layers/simplernn_test.py" startline="159" endline="222" pcid="3982">
  def test_statefulness_SimpleRNN(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    layer_class = keras.layers.SimpleRNN
    model = keras.models.Sequential()
    model.add(
        keras.layers.Embedding(
            4,
            embedding_dim,
            mask_zero=True,
            input_length=timesteps,
            batch_input_shape=(num_samples, timesteps)))
    layer = layer_class(
        units, return_sequences=False, stateful=True, weights=None)
    model.add(layer)
    model.compile(
        optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01),
        loss='mse',
        run_eagerly=testing_utils.should_run_eagerly())
    out1 = model.predict(np.ones((num_samples, timesteps)))
    self.assertEqual(out1.shape, (num_samples, units))

    # train once so that the states change
    model.train_on_batch(
        np.ones((num_samples, timesteps)), np.ones((num_samples, units)))
    out2 = model.predict(np.ones((num_samples, timesteps)))

    # if the state is not reset, output should be different
    self.assertNotEqual(out1.max(), out2.max())

    # check that output changes after states are reset
    # (even though the model itself didn't change)
    layer.reset_states()
    out3 = model.predict(np.ones((num_samples, timesteps)))
    self.assertNotEqual(out2.max(), out3.max())

    # check that container-level reset_states() works
    model.reset_states()
    out4 = model.predict(np.ones((num_samples, timesteps)))
    np.testing.assert_allclose(out3, out4, atol=1e-5)

    # check that the call to `predict` updated the states
    out5 = model.predict(np.ones((num_samples, timesteps)))
    self.assertNotEqual(out4.max(), out5.max())

    # Check masking
    layer.reset_states()

    left_padded_input = np.ones((num_samples, timesteps))
    left_padded_input[0, :1] = 0
    left_padded_input[1, :2] = 0
    out6 = model.predict(left_padded_input)

    layer.reset_states()

    right_padded_input = np.ones((num_samples, timesteps))
    right_padded_input[0, -1:] = 0
    right_padded_input[1, -2:] = 0
    out7 = model.predict(right_padded_input)

    np.testing.assert_allclose(out7, out6, atol=1e-5)

</source>
<source file="systems/keras-2.7.0/keras/layers/gru_test.py" startline="152" endline="216" pcid="4514">
  def test_statefulness_GRU(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    layer_class = keras.layers.GRU

    model = keras.models.Sequential()
    model.add(
        keras.layers.Embedding(
            4,
            embedding_dim,
            mask_zero=True,
            input_length=timesteps,
            batch_input_shape=(num_samples, timesteps)))
    layer = layer_class(
        units, return_sequences=False, stateful=True, weights=None)
    model.add(layer)
    model.compile(
        optimizer='sgd',
        loss='mse',
        run_eagerly=testing_utils.should_run_eagerly())
    out1 = model.predict(np.ones((num_samples, timesteps)))
    self.assertEqual(out1.shape, (num_samples, units))

    # train once so that the states change
    model.train_on_batch(
        np.ones((num_samples, timesteps)), np.ones((num_samples, units)))
    out2 = model.predict(np.ones((num_samples, timesteps)))

    # if the state is not reset, output should be different
    self.assertNotEqual(out1.max(), out2.max())

    # check that output changes after states are reset
    # (even though the model itself didn't change)
    layer.reset_states()
    out3 = model.predict(np.ones((num_samples, timesteps)))
    self.assertNotEqual(out2.max(), out3.max())

    # check that container-level reset_states() works
    model.reset_states()
    out4 = model.predict(np.ones((num_samples, timesteps)))
    np.testing.assert_allclose(out3, out4, atol=1e-5)

    # check that the call to `predict` updated the states
    out5 = model.predict(np.ones((num_samples, timesteps)))
    self.assertNotEqual(out4.max(), out5.max())

    # Check masking
    layer.reset_states()

    left_padded_input = np.ones((num_samples, timesteps))
    left_padded_input[0, :1] = 0
    left_padded_input[1, :2] = 0
    out6 = model.predict(left_padded_input)

    layer.reset_states()

    right_padded_input = np.ones((num_samples, timesteps))
    right_padded_input[0, -1:] = 0
    right_padded_input[1, -2:] = 0
    out7 = model.predict(right_padded_input)

    np.testing.assert_allclose(out7, out6, atol=1e-5)

</source>
<source file="systems/keras-2.7.0/keras/layers/lstm_test.py" startline="386" endline="450" pcid="3971">
  def test_statefulness_LSTM(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    layer_class = keras.layers.LSTM
    model = keras.models.Sequential()
    model.add(
        keras.layers.Embedding(
            4,
            embedding_dim,
            mask_zero=True,
            input_length=timesteps,
            batch_input_shape=(num_samples, timesteps)))
    layer = layer_class(
        units, return_sequences=False, stateful=True, weights=None)
    model.add(layer)
    model.compile(
        optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01),
        loss='mse',
        run_eagerly=testing_utils.should_run_eagerly())
    out1 = model.predict(np.ones((num_samples, timesteps)))
    self.assertEqual(out1.shape, (num_samples, units))

    # train once so that the states change
    model.train_on_batch(
        np.ones((num_samples, timesteps)), np.ones((num_samples, units)))
    out2 = model.predict(np.ones((num_samples, timesteps)))

    # if the state is not reset, output should be different
    self.assertNotEqual(out1.max(), out2.max())

    # check that output changes after states are reset
    # (even though the model itself didn't change)
    layer.reset_states()
    out3 = model.predict(np.ones((num_samples, timesteps)))
    self.assertNotEqual(out2.max(), out3.max())

    # check that container-level reset_states() works
    model.reset_states()
    out4 = model.predict(np.ones((num_samples, timesteps)))
    self.assertAllClose(out3, out4, atol=1e-5)

    # check that the call to `predict` updated the states
    out5 = model.predict(np.ones((num_samples, timesteps)))
    self.assertNotEqual(out4.max(), out5.max())

    # Check masking
    layer.reset_states()

    left_padded_input = np.ones((num_samples, timesteps))
    left_padded_input[0, :1] = 0
    left_padded_input[1, :2] = 0
    out6 = model.predict(left_padded_input)

    layer.reset_states()

    right_padded_input = np.ones((num_samples, timesteps))
    right_padded_input[0, -1:] = 0
    right_padded_input[1, -2:] = 0
    out7 = model.predict(right_padded_input)

    self.assertAllClose(out7, out6, atol=1e-5)


</source>
<source file="systems/keras-2.7.0/keras/layers/gru_v2_test.py" startline="452" endline="524" pcid="4315">
  def test_statefulness_GRU(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    layer_class = rnn.GRU
    model = keras.models.Sequential()
    model.add(
        keras.layers.Embedding(
            4,
            embedding_dim,
            mask_zero=True,
            input_length=timesteps,
            batch_input_shape=(num_samples, timesteps)))
    layer = layer_class(
        units, return_sequences=False, stateful=True, weights=None)
    model.add(layer)
    model.compile(
        optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01),
        loss='mse',
        run_eagerly=testing_utils.should_run_eagerly())
    out1 = model.predict(np.ones((num_samples, timesteps)))
    self.assertEqual(out1.shape, (num_samples, units))

    # train once so that the states change
    model.train_on_batch(
        np.ones((num_samples, timesteps)), np.ones((num_samples, units)))
    out2 = model.predict(np.ones((num_samples, timesteps)))

    # if the state is not reset, output should be different
    self.assertNotEqual(out1.max(), out2.max())

    # check that output changes after states are reset
    # (even though the model itself didn't change)
    layer.reset_states()
    out3 = model.predict(np.ones((num_samples, timesteps)))
    self.assertNotEqual(out2.max(), out3.max())

    # check that container-level reset_states() works
    model.reset_states()
    out4 = model.predict(np.ones((num_samples, timesteps)))
    np.testing.assert_allclose(out3, out4, atol=1e-5)

    # check that the call to `predict` updated the states
    out5 = model.predict(np.ones((num_samples, timesteps)))
    self.assertNotEqual(out4.max(), out5.max())

    # Check masking
    layer.reset_states()

    left_padded_input = np.ones((num_samples, timesteps))
    left_padded_input[0, :1] = 0
    left_padded_input[1, :2] = 0
    out6 = model.predict(left_padded_input)

    layer.reset_states()

    right_padded_input = np.ones((num_samples, timesteps))
    right_padded_input[0, -1:] = 0
    right_padded_input[1, -2:] = 0
    out7 = model.predict(right_padded_input)

    layer.reset_states()

    mix_padded_input = np.ones((num_samples, timesteps))
    mix_padded_input[0, 1] = 0
    mix_padded_input[1, 0] = 0
    mix_padded_input[1, 2] = 0
    out8 = model.predict(mix_padded_input)

    self.assertAllClose(out7, out6, atol=1e-5)
    self.assertAllClose(out8, out7, atol=1e-5)

</source>
</class>

<class classid="193" nclones="3" nlines="18" similarity="76">
<source file="systems/keras-2.7.0/keras/layers/lstm_v2_test.py" startline="693" endline="714" pcid="3712">
  def test_stateful_LSTM_training(self):
    # See b/123587692 for more context.
    vocab_size = 20
    embedding_dim = 10
    batch_size = 8
    timestep = 12
    units = 5
    x = np.random.randint(0, vocab_size, size=(batch_size, timestep))
    y = np.random.randint(0, vocab_size, size=(batch_size, timestep))

    model = keras.Sequential([
        keras.layers.Embedding(vocab_size, embedding_dim,
                               batch_input_shape=[batch_size, timestep]),
        rnn.LSTM(units, return_sequences=True, stateful=True),
        keras.layers.Dense(vocab_size)
    ])
    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        run_eagerly=testing_utils.should_run_eagerly())
    model.fit(x, y, epochs=1, shuffle=False)

</source>
<source file="systems/keras-2.7.0/keras/layers/gru_v2_test.py" startline="525" endline="546" pcid="4316">
  def test_stateful_GRU_training(self):
    # See b/123587692 for more context.
    vocab_size = 20
    embedding_dim = 10
    batch_size = 8
    timestep = 12
    units = 5
    x = np.random.randint(0, vocab_size, size=(batch_size, timestep))
    y = np.random.randint(0, vocab_size, size=(batch_size, timestep))

    model = keras.Sequential([
        keras.layers.Embedding(vocab_size, embedding_dim,
                               batch_input_shape=[batch_size, timestep]),
        rnn.GRU(units, return_sequences=True, stateful=True),
        keras.layers.Dense(vocab_size)
    ])
    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        run_eagerly=testing_utils.should_run_eagerly())
    model.fit(x, y, epochs=1, shuffle=False)

</source>
<source file="systems/keras-2.7.0/keras/layers/recurrent_v2_test.py" startline="37" endline="62" pcid="4389">
  def test_device_placement(self, layer):
    if not tf.test.is_gpu_available():
      self.skipTest('Need GPU for testing.')
    vocab_size = 20
    embedding_dim = 10
    batch_size = 8
    timestep = 12
    units = 5
    x = np.random.randint(0, vocab_size, size=(batch_size, timestep))
    y = np.random.randint(0, vocab_size, size=(batch_size, timestep))

    # Test when GPU is available but not used, the graph should be properly
    # created with CPU ops.
    with testing_utils.device(should_use_gpu=False):
      model = keras.Sequential([
          keras.layers.Embedding(vocab_size, embedding_dim,
                                 batch_input_shape=[batch_size, timestep]),
          layer(units, return_sequences=True, stateful=True),
          keras.layers.Dense(vocab_size)
      ])
      model.compile(
          optimizer='adam',
          loss='sparse_categorical_crossentropy',
          run_eagerly=testing_utils.should_run_eagerly())
      model.fit(x, y, epochs=1, shuffle=False)

</source>
</class>

<class classid="194" nclones="2" nlines="18" similarity="100">
<source file="systems/keras-2.7.0/keras/layers/lstm_v2_test.py" startline="756" endline="779" pcid="3715">
  def test_explicit_device_with_go_backward_and_mask(self):
    batch_size = 8
    timestep = 7
    masksteps = 5
    units = 4

    inputs = np.random.randn(batch_size, timestep, units).astype(np.float32)
    mask = np.ones((batch_size, timestep)).astype(np.bool)
    mask[:, masksteps:] = 0

    # Test for V1 behavior.
    lstm_v1 = rnn_v1.LSTM(units, return_sequences=True, go_backwards=True)
    with testing_utils.device(should_use_gpu=True):
      outputs_masked_v1 = lstm_v1(inputs, mask=tf.constant(mask))
      outputs_trimmed_v1 = lstm_v1(inputs[:, :masksteps])
    self.assertAllClose(outputs_masked_v1[:, -masksteps:], outputs_trimmed_v1)

    # Test for V2 behavior.
    lstm = rnn.LSTM(units, return_sequences=True, go_backwards=True)
    with testing_utils.device(should_use_gpu=True):
      outputs_masked = lstm(inputs, mask=tf.constant(mask))
      outputs_trimmed = lstm(inputs[:, :masksteps])
    self.assertAllClose(outputs_masked[:, -masksteps:], outputs_trimmed)

</source>
<source file="systems/keras-2.7.0/keras/layers/gru_v2_test.py" startline="551" endline="574" pcid="4317">
  def test_explicit_device_with_go_backward_and_mask(self):
    batch_size = 8
    timestep = 7
    masksteps = 5
    units = 4

    inputs = np.random.randn(batch_size, timestep, units).astype(np.float32)
    mask = np.ones((batch_size, timestep)).astype(np.bool)
    mask[:, masksteps:] = 0

    # Test for V1 behavior.
    lstm_v1 = rnn_v1.GRU(units, return_sequences=True, go_backwards=True)
    with testing_utils.device(should_use_gpu=True):
      outputs_masked_v1 = lstm_v1(inputs, mask=tf.constant(mask))
      outputs_trimmed_v1 = lstm_v1(inputs[:, :masksteps])
    self.assertAllClose(outputs_masked_v1[:, -masksteps:], outputs_trimmed_v1)

    # Test for V2 behavior.
    lstm = rnn.GRU(units, return_sequences=True, go_backwards=True)
    with testing_utils.device(should_use_gpu=True):
      outputs_masked = lstm(inputs, mask=tf.constant(mask))
      outputs_trimmed = lstm(inputs[:, :masksteps])
    self.assertAllClose(outputs_masked[:, -masksteps:], outputs_trimmed)

</source>
</class>

<class classid="195" nclones="2" nlines="12" similarity="100">
<source file="systems/keras-2.7.0/keras/layers/lstm_v2_test.py" startline="781" endline="797" pcid="3716">
  def test_v1_session_behavior(self):
    with tf.compat.v1.get_default_graph().as_default():
      # See b/139132348 for more details.
      x = np.random.uniform(size=(100, 4, 8))
      y = np.random.uniform(size=(100, 1))
      dataset = tf.data.Dataset.from_tensor_slices(
          (x, y)).shuffle(100).batch(32)

      inp = keras.layers.Input(shape=(4, 8))
      layer = rnn.LSTM(1)(inp)
      layer = keras.layers.Dense(1)(layer)

      model = keras.models.Model(inp, layer)

      model.compile(loss='mse', optimizer='sgd')
      model.fit(dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/gru_v2_test.py" startline="576" endline="592" pcid="4318">
  def test_v1_session_behavior(self):
    with tf.compat.v1.get_default_graph().as_default():
      # See b/139132348 for more details.
      x = np.random.uniform(size=(100, 4, 8))
      y = np.random.uniform(size=(100, 1))
      dataset = tf.data.Dataset.from_tensor_slices(
          (x, y)).shuffle(100).batch(32)

      inp = keras.layers.Input(shape=(4, 8))
      layer = rnn.GRU(1)(inp)
      layer = keras.layers.Dense(1)(layer)

      model = keras.models.Model(inp, layer)

      model.compile(loss='mse', optimizer='sgd')
      model.fit(dataset)

</source>
</class>

<class classid="196" nclones="2" nlines="23" similarity="100">
<source file="systems/keras-2.7.0/keras/layers/lstm_v2_test.py" startline="798" endline="826" pcid="3717">
  def test_with_fully_masked_inputs(self):
    num_samples = 8
    timestep = 5
    embedding_dim = 4
    vocab_size = 20
    units = 2

    inputs = np.random.randint(0, vocab_size, size=(num_samples, timestep))
    # Set the first inputs to be fully zero.
    inputs[0, :] = 0.0

    model = keras.models.Sequential()
    model.add(
        keras.layers.Embedding(
            vocab_size,
            embedding_dim,
            mask_zero=True,
            input_length=timestep,
            batch_input_shape=(num_samples, timestep)))
    layer = rnn.LSTM(units)
    model.add(layer)
    model.compile(
        optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01),
        loss='mse',
        run_eagerly=testing_utils.should_run_eagerly())
    # Make sure it doesn't crash with cudnn kernel.
    model.predict(inputs)

  # TODO (b/169895267): test with xla_gpu is disabled.
</source>
<source file="systems/keras-2.7.0/keras/layers/gru_v2_test.py" startline="593" endline="621" pcid="4319">
  def test_with_fully_masked_inputs(self):
    num_samples = 8
    timestep = 5
    embedding_dim = 4
    vocab_size = 20
    units = 2

    inputs = np.random.randint(0, vocab_size, size=(num_samples, timestep))
    # Set the first inputs to be fully zero.
    inputs[0, :] = 0.0

    model = keras.models.Sequential()
    model.add(
        keras.layers.Embedding(
            vocab_size,
            embedding_dim,
            mask_zero=True,
            input_length=timestep,
            batch_input_shape=(num_samples, timestep)))
    layer = rnn.GRU(units)
    model.add(layer)
    model.compile(
        optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01),
        loss='mse',
        run_eagerly=testing_utils.should_run_eagerly())
    # Make sure it doesn't crash with cudnn kernel.
    model.predict(inputs)

  # TODO (b/169895267): test with xla_gpu is disabled.
</source>
</class>

<class classid="197" nclones="2" nlines="19" similarity="100">
<source file="systems/keras-2.7.0/keras/layers/lstm_v2_test.py" startline="827" endline="851" pcid="3718">
  def test_deepcopy(self):
    if not tf.executing_eagerly():
      self.skipTest('v2-only test')
    original_layer = rnn.LSTM(5)
    copied_layer = copy.deepcopy(original_layer)
    self.assertEqual(copied_layer.units, 5)
    self.assertEqual(original_layer.get_config(), original_layer.get_config())

    # Copy layer before layer call on inputs without weight initialization.
    inputs = np.random.normal(size=[32, 10, 8]).astype(np.float32)
    original_layer = rnn.LSTM(4)
    copied_layer = copy.deepcopy(original_layer)
    outputs = original_layer(inputs)
    copied_outputs = copied_layer(inputs)
    self.assertNotAllClose(
        self.evaluate(outputs), self.evaluate(copied_outputs))

    # Copy layer after layer call on inputs with weight initialization.
    original_layer = rnn.LSTM(4)
    outputs = original_layer(inputs)
    copied_layer = copy.deepcopy(original_layer)
    copied_outputs = copied_layer(inputs)
    self.assertAllClose(self.evaluate(outputs), self.evaluate(copied_outputs))


</source>
<source file="systems/keras-2.7.0/keras/layers/gru_v2_test.py" startline="622" endline="646" pcid="4320">
  def test_deepcopy(self):
    if not tf.executing_eagerly():
      self.skipTest('v2-only test')
    original_layer = rnn.GRU(5)
    copied_layer = copy.deepcopy(original_layer)
    self.assertEqual(copied_layer.units, 5)
    self.assertEqual(original_layer.get_config(), original_layer.get_config())

    # Copy layer before layer call on inputs without weight initialization.
    inputs = np.random.normal(size=[32, 10, 8]).astype(np.float32)
    original_layer = rnn.GRU(4)
    copied_layer = copy.deepcopy(original_layer)
    outputs = original_layer(inputs)
    copied_outputs = copied_layer(inputs)
    self.assertNotAllClose(
        self.evaluate(outputs), self.evaluate(copied_outputs))

    # Copy layer after layer call on inputs with weight initialization.
    original_layer = rnn.GRU(4)
    outputs = original_layer(inputs)
    copied_layer = copy.deepcopy(original_layer)
    copied_outputs = copied_layer(inputs)
    self.assertAllClose(self.evaluate(outputs), self.evaluate(copied_outputs))


</source>
</class>

<class classid="198" nclones="2" nlines="22" similarity="90">
<source file="systems/keras-2.7.0/keras/layers/lstm_v2_test.py" startline="862" endline="889" pcid="3719">
  def _test_runtime_with_model(self, model):

    (x_train, y_train), _ = testing_utils.get_test_data(
        train_samples=self.batch,
        test_samples=0,
        input_shape=(self.timestep, self.input_shape),
        num_classes=self.output_shape)
    y_train = np_utils.to_categorical(y_train, self.output_shape)

    model.compile(
        optimizer='sgd',
        loss=['categorical_crossentropy', None],
        run_eagerly=testing_utils.should_run_eagerly())

    existing_loss = 0
    for _ in range(self.epoch):
      history = model.fit(x_train, y_train)
      loss_value = history.history['loss'][0]

      self.assertNotEqual(existing_loss, loss_value)
      existing_loss = loss_value

    _, runtime_value = model.predict(x_train)
    if tf.test.is_gpu_available():
      self.assertEqual(runtime_value[0], rnn._RUNTIME_GPU)
    else:
      self.assertEqual(runtime_value[0], rnn._RUNTIME_CPU)

</source>
<source file="systems/keras-2.7.0/keras/layers/gru_v2_test.py" startline="686" endline="711" pcid="4322">
  def _test_runtime_with_model(self, model):
    (x_train, y_train), _ = testing_utils.get_test_data(
        train_samples=self.batch,
        test_samples=0,
        input_shape=(self.timestep, self.input_shape),
        num_classes=self.output_shape)
    y_train = np_utils.to_categorical(y_train, self.output_shape)

    model.compile(
        optimizer='sgd',
        loss=['categorical_crossentropy', None])

    existing_loss = 0
    for _ in range(self.epoch):
      history = model.fit(x_train, y_train)
      loss_value = history.history['loss'][0]

      self.assertNotEqual(existing_loss, loss_value)
      existing_loss = loss_value

    _, runtime_value = model.predict(x_train)
    if tf.test.is_gpu_available():
      self.assertEqual(runtime_value[0], rnn._RUNTIME_GPU)
    else:
      self.assertEqual(runtime_value[0], rnn._RUNTIME_CPU)

</source>
</class>

<class classid="199" nclones="2" nlines="36" similarity="100">
<source file="systems/keras-2.7.0/keras/layers/lstm_v2_test.py" startline="910" endline="963" pcid="3721">
  def test_LSTM_runtime_with_mask(self):
    # Masking will affect which backend is selected based on whether the mask
    # is strictly right padded.
    layer = rnn.LSTM(self.rnn_state_size, return_runtime=True)

    inputs = keras.layers.Input(
        shape=[self.timestep, self.input_shape], dtype=tf.float32)
    masked_inputs = keras.layers.Masking()(inputs)

    outputs, runtime = layer(masked_inputs)
    # Expand the runtime so that it is a 1D tensor instead of scalar.
    # TF model does not work with scalar model output, specially during
    # aggregation.
    runtime = keras.layers.Lambda(
        lambda x: tf.expand_dims(x, axis=-1))(runtime)
    model = keras.models.Model(inputs=inputs, outputs=[outputs, runtime])

    (x_train, y_train), _ = testing_utils.get_test_data(
        train_samples=self.batch,
        test_samples=0,
        input_shape=(self.timestep, self.input_shape),
        num_classes=self.output_shape)
    y_train = np_utils.to_categorical(y_train, self.output_shape)

    model.compile(
        optimizer='sgd',
        loss=['categorical_crossentropy', None],
        run_eagerly=testing_utils.should_run_eagerly())

    model.fit(x_train, y_train)

    # Verify unpadded data.
    _, runtime_value = model.predict(x_train)
    if tf.test.is_gpu_available():
      self.assertEqual(runtime_value[0], rnn._RUNTIME_GPU)
    else:
      self.assertEqual(runtime_value[0], rnn._RUNTIME_CPU)

    # Update x/y to be right padded by setting the last timestep to 0
    x_train[:, -1, :] = 0
    y_train[:, -1] = 0
    _, runtime_value = model.predict(x_train)
    if tf.test.is_gpu_available():
      self.assertEqual(runtime_value[0], rnn._RUNTIME_GPU)
    else:
      self.assertEqual(runtime_value[0], rnn._RUNTIME_CPU)

    # Further update x/y to be mix padded (masks in the middle), and verify
    # only cpu kernel can be selected.
    x_train[:, -3, :] = 0
    y_train[:, -3] = 0
    _, runtime_value = model.predict(x_train)
    self.assertEqual(runtime_value[0], rnn._RUNTIME_CPU)

</source>
<source file="systems/keras-2.7.0/keras/layers/gru_v2_test.py" startline="732" endline="785" pcid="4324">
  def test_GRU_runtime_with_mask(self):
    # Masking will affect which backend is selected based on whether the mask
    # is strictly right padded.
    layer = rnn.GRU(self.rnn_state_size, return_runtime=True)

    inputs = keras.layers.Input(
        shape=[self.timestep, self.input_shape], dtype=tf.float32)
    masked_inputs = keras.layers.Masking()(inputs)

    outputs, runtime = layer(masked_inputs)
    # Expand the runtime so that it is a 1D tensor instead of scalar.
    # TF model does not work with scalar model output, specially during
    # aggregation.
    runtime = keras.layers.Lambda(
        lambda x: tf.expand_dims(x, axis=-1))(runtime)
    model = keras.models.Model(inputs=inputs, outputs=[outputs, runtime])

    (x_train, y_train), _ = testing_utils.get_test_data(
        train_samples=self.batch,
        test_samples=0,
        input_shape=(self.timestep, self.input_shape),
        num_classes=self.output_shape)
    y_train = np_utils.to_categorical(y_train, self.output_shape)

    model.compile(
        optimizer='sgd',
        loss=['categorical_crossentropy', None],
        run_eagerly=testing_utils.should_run_eagerly())

    model.fit(x_train, y_train)

    # Verify unpadded data.
    _, runtime_value = model.predict(x_train)
    if tf.test.is_gpu_available():
      self.assertEqual(runtime_value[0], rnn._RUNTIME_GPU)
    else:
      self.assertEqual(runtime_value[0], rnn._RUNTIME_CPU)

    # Update x/y to be right padded by setting the last timestep to 0
    x_train[:, -1, :] = 0
    y_train[:, -1] = 0
    _, runtime_value = model.predict(x_train)
    if tf.test.is_gpu_available():
      self.assertEqual(runtime_value[0], rnn._RUNTIME_GPU)
    else:
      self.assertEqual(runtime_value[0], rnn._RUNTIME_CPU)

    # Further update x/y to be mix padded (masks in the middle), and verify
    # only cpu kernel can be selected.
    x_train[:, -3, :] = 0
    y_train[:, -3] = 0
    _, runtime_value = model.predict(x_train)
    self.assertEqual(runtime_value[0], rnn._RUNTIME_CPU)

</source>
</class>

<class classid="200" nclones="2" nlines="16" similarity="100">
<source file="systems/keras-2.7.0/keras/layers/lstm_v2_test.py" startline="965" endline="992" pcid="3722">
  def test_LSTM_runtime_with_cond(self):
    # This test is to demonstrate the graph rewrite of grappler plugin under
    # the condition that the function returns different number of internal
    # states.
    layer = rnn.LSTM(self.rnn_state_size, return_runtime=True)

    inputs = keras.layers.Input(
        shape=[self.timestep, self.input_shape], dtype=tf.float32)

    zeros = tf.zeros([self.batch, self.output_shape])
    dummy_runtime = rnn._runtime(rnn._RUNTIME_UNKNOWN)
    a = tf.constant(0)
    b = tf.constant(1)
    # Will always run the lstm layer.
    outputs, runtime = tf.cond(
        tf.less(a, b),
        lambda: layer(inputs),
        lambda: (zeros, dummy_runtime))

    # Expand the runtime so that it is a 1D tensor instead of scalar.
    # TF model does not work with scalar model output, specially during
    # aggregation.
    runtime = keras.layers.Lambda(
        lambda x: tf.expand_dims(x, axis=-1))(runtime)
    model = keras.models.Model(inputs=inputs, outputs=[outputs, runtime])
    self._test_runtime_with_model(model)


</source>
<source file="systems/keras-2.7.0/keras/layers/gru_v2_test.py" startline="787" endline="814" pcid="4325">
  def test_GRU_runtime_with_cond(self):
    # This test is to demonstrate the graph rewrite of grappler plugin under
    # the condition that the function returns different number of internal
    # states.
    layer = rnn.GRU(self.rnn_state_size, return_runtime=True)

    inputs = keras.layers.Input(
        shape=[self.timestep, self.input_shape], dtype=tf.float32)

    zeros = tf.zeros([self.batch, self.output_shape])
    dummy_runtime = rnn._runtime(rnn._RUNTIME_UNKNOWN)
    a = tf.constant(0)
    b = tf.constant(1)
    # Will always run the GRU layer.
    outputs, runtime = tf.cond(
        tf.less(a, b),
        lambda: layer(inputs),
        lambda: (zeros, dummy_runtime))

    # Expand the runtime so that it is a 1D tensor instead of scalar.
    # TF model does not work with scalar model output, specially during
    # aggregation.
    runtime = keras.layers.Lambda(
        lambda x: tf.expand_dims(x, axis=-1))(runtime)
    model = keras.models.Model(inputs=inputs, outputs=[outputs, runtime])
    self._test_runtime_with_model(model)


</source>
</class>

<class classid="201" nclones="3" nlines="16" similarity="81">
<source file="systems/keras-2.7.0/keras/layers/lstm_v2_test.py" startline="1007" endline="1026" pcid="3724">
  def _time_performance_run_cudnn_lstm(self, test_config, x_train, y_train):
    # Get the performance number for standard Cudnn LSTM
    input_shape = test_config['input_shape']
    rnn_state_size = test_config['rnn_state_size']
    timestep = test_config['timestep']

    cudnn_lstm_layer = keras.layers.CuDNNLSTM(rnn_state_size)
    inputs = keras.layers.Input(
        shape=[timestep, input_shape], dtype=tf.float32)

    outputs = cudnn_lstm_layer(inputs)
    model = keras.models.Model(inputs, outputs)
    model.compile('sgd', 'mse')

    sec_per_epoch = self._measure_performance(
        test_config, model, x_train, y_train)
    logging.info('Average performance for %s per epoch is: %s',
                 'CuDNN LSTM', sec_per_epoch)
    return sec_per_epoch

</source>
<source file="systems/keras-2.7.0/keras/layers/lstm_v2_test.py" startline="1048" endline="1068" pcid="3726">
  def _time_performance_run_normal_lstm(
      self, test_config, x_train, y_train):
    # Get performance number for standard LSTM on GPU.
    input_shape = test_config['input_shape']
    rnn_state_size = test_config['rnn_state_size']
    timestep = test_config['timestep']

    layer = rnn_v1.LSTM(rnn_state_size)
    inputs = keras.layers.Input(
        shape=[timestep, input_shape], dtype=tf.float32)

    outputs = layer(inputs)
    model = keras.models.Model(inputs, outputs)
    model.compile('sgd', 'mse')

    sec_per_epoch = self._measure_performance(
        test_config, model, x_train, y_train)
    logging.info('Average performance for %s per epoch is: %s',
                 'Normal LSTM', sec_per_epoch)
    return sec_per_epoch

</source>
<source file="systems/keras-2.7.0/keras/layers/lstm_v2_test.py" startline="1027" endline="1047" pcid="3725">
  def _time_performance_run_unifed_lstm_gpu(
      self, test_config, x_train, y_train):
    # Get performance number for lstm_v2 with grappler swap the impl
    input_shape = test_config['input_shape']
    rnn_state_size = test_config['rnn_state_size']
    timestep = test_config['timestep']

    layer = rnn.LSTM(rnn_state_size)
    inputs = keras.layers.Input(
        shape=[timestep, input_shape], dtype=tf.float32)

    outputs = layer(inputs)
    model = keras.models.Model(inputs, outputs)
    model.compile('sgd', 'mse')

    sec_per_epoch = self._measure_performance(
        test_config, model, x_train, y_train)
    logging.info('Average performance for %s per epoch is: %s',
                 'LSTM V2', sec_per_epoch)
    return sec_per_epoch

</source>
</class>

<class classid="202" nclones="7" nlines="15" similarity="81">
<source file="systems/keras-2.7.0/keras/layers/convolutional_test.py" startline="96" endline="112" pcid="3733">
  def test_conv1d_regularizers(self):
    kwargs = {
        'filters': 3,
        'kernel_size': 3,
        'padding': 'valid',
        'kernel_regularizer': 'l2',
        'bias_regularizer': 'l2',
        'activity_regularizer': 'l2',
        'strides': 1
    }
    with self.cached_session():
      layer = keras.layers.Conv1D(**kwargs)
      layer.build((None, 5, 2))
      self.assertEqual(len(layer.losses), 2)
      layer(keras.backend.variable(np.ones((1, 5, 2))))
      self.assertEqual(len(layer.losses), 3)

</source>
<source file="systems/keras-2.7.0/keras/layers/convolutional_transpose_test.py" startline="152" endline="168" pcid="4130">
  def test_conv3d_transpose_regularizers(self):
    kwargs = {
        'filters': 3,
        'kernel_size': 3,
        'padding': 'valid',
        'kernel_regularizer': 'l2',
        'bias_regularizer': 'l2',
        'activity_regularizer': 'l2',
        'strides': 1
    }
    with self.cached_session():
      layer = keras.layers.Conv3DTranspose(**kwargs)
      layer.build((None, 5, 5, 5, 2))
      self.assertEqual(len(layer.losses), 2)
      layer(keras.backend.variable(np.ones((1, 5, 5, 5, 2))))
      self.assertEqual(len(layer.losses), 3)

</source>
<source file="systems/keras-2.7.0/keras/layers/convolutional_transpose_test.py" startline="57" endline="73" pcid="4125">
  def test_conv2d_transpose_regularizers(self):
    kwargs = {
        'filters': 3,
        'kernel_size': 3,
        'padding': 'valid',
        'kernel_regularizer': 'l2',
        'bias_regularizer': 'l2',
        'activity_regularizer': 'l2',
        'strides': 1
    }
    with self.cached_session():
      layer = keras.layers.Conv2DTranspose(**kwargs)
      layer.build((None, 5, 5, 2))
      self.assertEqual(len(layer.losses), 2)
      layer(keras.backend.variable(np.ones((1, 5, 5, 2))))
      self.assertEqual(len(layer.losses), 3)

</source>
<source file="systems/keras-2.7.0/keras/layers/separable_convolutional_test.py" startline="55" endline="72" pcid="4691">
  def test_separable_conv1d_regularizers(self):
    kwargs = {
        'filters': 3,
        'kernel_size': 3,
        'padding': 'valid',
        'depthwise_regularizer': 'l2',
        'pointwise_regularizer': 'l2',
        'bias_regularizer': 'l2',
        'activity_regularizer': 'l2',
        'strides': 1
    }
    with self.cached_session():
      layer = keras.layers.SeparableConv1D(**kwargs)
      layer.build((None, 5, 2))
      self.assertEqual(len(layer.losses), 3)
      layer(keras.backend.variable(np.ones((1, 5, 2))))
      self.assertEqual(len(layer.losses), 4)

</source>
<source file="systems/keras-2.7.0/keras/layers/convolutional_test.py" startline="263" endline="279" pcid="3742">
  def test_conv2d_regularizers(self):
    kwargs = {
        'filters': 3,
        'kernel_size': 3,
        'padding': 'valid',
        'kernel_regularizer': 'l2',
        'bias_regularizer': 'l2',
        'activity_regularizer': 'l2',
        'strides': 1
    }
    with self.cached_session():
      layer = keras.layers.Conv2D(**kwargs)
      layer.build((None, 5, 5, 2))
      self.assertEqual(len(layer.losses), 2)
      layer(keras.backend.variable(np.ones((1, 5, 5, 2))))
      self.assertEqual(len(layer.losses), 3)

</source>
<source file="systems/keras-2.7.0/keras/layers/separable_convolutional_test.py" startline="127" endline="144" pcid="4695">
  def test_separable_conv2d_regularizers(self):
    kwargs = {
        'filters': 3,
        'kernel_size': 3,
        'padding': 'valid',
        'depthwise_regularizer': 'l2',
        'pointwise_regularizer': 'l2',
        'bias_regularizer': 'l2',
        'activity_regularizer': 'l2',
        'strides': 1
    }
    with self.cached_session():
      layer = keras.layers.SeparableConv2D(**kwargs)
      layer.build((None, 5, 5, 2))
      self.assertEqual(len(layer.losses), 3)
      layer(keras.backend.variable(np.ones((1, 5, 5, 2))))
      self.assertEqual(len(layer.losses), 4)

</source>
<source file="systems/keras-2.7.0/keras/layers/convolutional_test.py" startline="385" endline="402" pcid="3749">
  def test_conv3d_regularizers(self):
    kwargs = {
        'filters': 3,
        'kernel_size': 3,
        'padding': 'valid',
        'kernel_regularizer': 'l2',
        'bias_regularizer': 'l2',
        'activity_regularizer': 'l2',
        'strides': 1
    }
    with self.cached_session():
      layer = keras.layers.Conv3D(**kwargs)
      layer.build((None, 5, 5, 5, 2))
      self.assertEqual(len(layer.losses), 2)
      self.assertEqual(len(layer.losses), 2)
      layer(keras.backend.variable(np.ones((1, 5, 5, 5, 2))))
      self.assertEqual(len(layer.losses), 3)

</source>
</class>

<class classid="203" nclones="7" nlines="15" similarity="77">
<source file="systems/keras-2.7.0/keras/layers/convolutional_test.py" startline="113" endline="130" pcid="3734">
  def test_conv1d_constraints(self):
    k_constraint = lambda x: x
    b_constraint = lambda x: x

    kwargs = {
        'filters': 3,
        'kernel_size': 3,
        'padding': 'valid',
        'kernel_constraint': k_constraint,
        'bias_constraint': b_constraint,
        'strides': 1
    }
    with self.cached_session():
      layer = keras.layers.Conv1D(**kwargs)
      layer.build((None, 5, 2))
      self.assertEqual(layer.kernel.constraint, k_constraint)
      self.assertEqual(layer.bias.constraint, b_constraint)

</source>
<source file="systems/keras-2.7.0/keras/layers/convolutional_test.py" startline="403" endline="420" pcid="3750">
  def test_conv3d_constraints(self):
    k_constraint = lambda x: x
    b_constraint = lambda x: x

    kwargs = {
        'filters': 3,
        'kernel_size': 3,
        'padding': 'valid',
        'kernel_constraint': k_constraint,
        'bias_constraint': b_constraint,
        'strides': 1
    }
    with self.cached_session():
      layer = keras.layers.Conv3D(**kwargs)
      layer.build((None, 5, 5, 5, 2))
      self.assertEqual(layer.kernel.constraint, k_constraint)
      self.assertEqual(layer.bias.constraint, b_constraint)

</source>
<source file="systems/keras-2.7.0/keras/layers/convolutional_test.py" startline="280" endline="297" pcid="3743">
  def test_conv2d_constraints(self):
    k_constraint = lambda x: x
    b_constraint = lambda x: x

    kwargs = {
        'filters': 3,
        'kernel_size': 3,
        'padding': 'valid',
        'kernel_constraint': k_constraint,
        'bias_constraint': b_constraint,
        'strides': 1
    }
    with self.cached_session():
      layer = keras.layers.Conv2D(**kwargs)
      layer.build((None, 5, 5, 2))
      self.assertEqual(layer.kernel.constraint, k_constraint)
      self.assertEqual(layer.bias.constraint, b_constraint)

</source>
<source file="systems/keras-2.7.0/keras/layers/convolutional_transpose_test.py" startline="74" endline="91" pcid="4126">
  def test_conv2d_transpose_constraints(self):
    k_constraint = lambda x: x
    b_constraint = lambda x: x

    kwargs = {
        'filters': 3,
        'kernel_size': 3,
        'padding': 'valid',
        'kernel_constraint': k_constraint,
        'bias_constraint': b_constraint,
        'strides': 1
    }
    with self.cached_session():
      layer = keras.layers.Conv2DTranspose(**kwargs)
      layer.build((None, 5, 5, 2))
      self.assertEqual(layer.kernel.constraint, k_constraint)
      self.assertEqual(layer.bias.constraint, b_constraint)

</source>
<source file="systems/keras-2.7.0/keras/layers/convolutional_transpose_test.py" startline="169" endline="186" pcid="4131">
  def test_conv3d_transpose_constraints(self):
    k_constraint = lambda x: x
    b_constraint = lambda x: x

    kwargs = {
        'filters': 3,
        'kernel_size': 3,
        'padding': 'valid',
        'kernel_constraint': k_constraint,
        'bias_constraint': b_constraint,
        'strides': 1
    }
    with self.cached_session():
      layer = keras.layers.Conv3DTranspose(**kwargs)
      layer.build((None, 5, 5, 5, 2))
      self.assertEqual(layer.kernel.constraint, k_constraint)
      self.assertEqual(layer.bias.constraint, b_constraint)

</source>
<source file="systems/keras-2.7.0/keras/layers/separable_convolutional_test.py" startline="73" endline="94" pcid="4692">
  def test_separable_conv1d_constraints(self):
    d_constraint = lambda x: x
    p_constraint = lambda x: x
    b_constraint = lambda x: x

    kwargs = {
        'filters': 3,
        'kernel_size': 3,
        'padding': 'valid',
        'pointwise_constraint': p_constraint,
        'depthwise_constraint': d_constraint,
        'bias_constraint': b_constraint,
        'strides': 1
    }
    with self.cached_session():
      layer = keras.layers.SeparableConv1D(**kwargs)
      layer.build((None, 5, 2))
      self.assertEqual(layer.depthwise_kernel.constraint, d_constraint)
      self.assertEqual(layer.pointwise_kernel.constraint, p_constraint)
      self.assertEqual(layer.bias.constraint, b_constraint)


</source>
<source file="systems/keras-2.7.0/keras/layers/separable_convolutional_test.py" startline="145" endline="164" pcid="4696">
  def test_separable_conv2d_constraints(self):
    d_constraint = lambda x: x
    p_constraint = lambda x: x
    b_constraint = lambda x: x

    kwargs = {
        'filters': 3,
        'kernel_size': 3,
        'padding': 'valid',
        'pointwise_constraint': p_constraint,
        'depthwise_constraint': d_constraint,
        'bias_constraint': b_constraint,
        'strides': 1
    }
    with self.cached_session():
      layer = keras.layers.SeparableConv2D(**kwargs)
      layer.build((None, 5, 5, 2))
      self.assertEqual(layer.depthwise_kernel.constraint, d_constraint)
      self.assertEqual(layer.pointwise_kernel.constraint, p_constraint)
      self.assertEqual(layer.bias.constraint, b_constraint)
</source>
</class>

<class classid="204" nclones="2" nlines="20" similarity="100">
<source file="systems/keras-2.7.0/keras/layers/convolutional_test.py" startline="421" endline="444" pcid="3751">
  def test_conv3d_dynamic_shape(self):
    input_data = np.random.random((1, 3, 3, 3, 3)).astype(np.float32)
    with self.cached_session():
      # Won't raise error here.
      testing_utils.layer_test(
          keras.layers.Conv3D,
          kwargs={
              'data_format': 'channels_last',
              'filters': 3,
              'kernel_size': 3
          },
          input_shape=(None, None, None, None, 3),
          input_data=input_data)
      if tf.test.is_gpu_available(cuda_only=True):
        testing_utils.layer_test(
            keras.layers.Conv3D,
            kwargs={
                'data_format': 'channels_first',
                'filters': 3,
                'kernel_size': 3
            },
            input_shape=(None, 3, None, None, None),
            input_data=input_data)

</source>
<source file="systems/keras-2.7.0/keras/layers/convolutional_transpose_test.py" startline="187" endline="210" pcid="4132">
  def test_conv3d_transpose_dynamic_shape(self):
    input_data = np.random.random((1, 3, 3, 3, 3)).astype(np.float32)
    with self.cached_session():
      # Won't raise error here.
      testing_utils.layer_test(
          keras.layers.Conv3DTranspose,
          kwargs={
              'data_format': 'channels_last',
              'filters': 3,
              'kernel_size': 3
          },
          input_shape=(None, None, None, None, 3),
          input_data=input_data)
      if tf.test.is_gpu_available(cuda_only=True):
        testing_utils.layer_test(
            keras.layers.Conv3DTranspose,
            kwargs={
                'data_format': 'channels_first',
                'filters': 3,
                'kernel_size': 3
            },
            input_shape=(None, 3, None, None, None),
            input_data=input_data)

</source>
</class>

<class classid="205" nclones="2" nlines="45" similarity="75">
<source file="systems/keras-2.7.0/keras/layers/convolutional_test.py" startline="875" endline="923" pcid="3769">
  def test_upsampling_2d(self):
    num_samples = 2
    stack_size = 2
    input_num_row = 11
    input_num_col = 12

    for data_format in ['channels_first', 'channels_last']:
      if data_format == 'channels_first':
        inputs = np.random.rand(num_samples, stack_size, input_num_row,
                                input_num_col)
      else:
        inputs = np.random.rand(num_samples, input_num_row, input_num_col,
                                stack_size)

      # basic test
      with self.cached_session():
        testing_utils.layer_test(
            keras.layers.UpSampling2D,
            kwargs={'size': (2, 2),
                    'data_format': data_format},
            input_shape=inputs.shape)

        for length_row in [2]:
          for length_col in [2, 3]:
            layer = keras.layers.UpSampling2D(
                size=(length_row, length_col), data_format=data_format)
            layer.build(inputs.shape)
            output = layer(keras.backend.variable(inputs))
            if tf.executing_eagerly():
              np_output = output.numpy()
            else:
              np_output = keras.backend.eval(output)
            if data_format == 'channels_first':
              assert np_output.shape[2] == length_row * input_num_row
              assert np_output.shape[3] == length_col * input_num_col
            else:  # tf
              assert np_output.shape[1] == length_row * input_num_row
              assert np_output.shape[2] == length_col * input_num_col

            # compare with numpy
            if data_format == 'channels_first':
              expected_out = np.repeat(inputs, length_row, axis=2)
              expected_out = np.repeat(expected_out, length_col, axis=3)
            else:  # tf
              expected_out = np.repeat(inputs, length_row, axis=1)
              expected_out = np.repeat(expected_out, length_col, axis=2)

            np.testing.assert_allclose(np_output, expected_out)

</source>
<source file="systems/keras-2.7.0/keras/layers/convolutional_test.py" startline="959" endline="1015" pcid="3771">
  def test_upsampling_3d(self):
    num_samples = 2
    stack_size = 2
    input_len_dim1 = 10
    input_len_dim2 = 11
    input_len_dim3 = 12

    for data_format in ['channels_first', 'channels_last']:
      if data_format == 'channels_first':
        inputs = np.random.rand(num_samples, stack_size, input_len_dim1,
                                input_len_dim2, input_len_dim3)
      else:
        inputs = np.random.rand(num_samples, input_len_dim1, input_len_dim2,
                                input_len_dim3, stack_size)

      # basic test
      with self.cached_session():
        testing_utils.layer_test(
            keras.layers.UpSampling3D,
            kwargs={'size': (2, 2, 2),
                    'data_format': data_format},
            input_shape=inputs.shape)

        for length_dim1 in [2, 3]:
          for length_dim2 in [2]:
            for length_dim3 in [3]:
              layer = keras.layers.UpSampling3D(
                  size=(length_dim1, length_dim2, length_dim3),
                  data_format=data_format)
              layer.build(inputs.shape)
              output = layer(keras.backend.variable(inputs))
              if tf.executing_eagerly():
                np_output = output.numpy()
              else:
                np_output = keras.backend.eval(output)
              if data_format == 'channels_first':
                assert np_output.shape[2] == length_dim1 * input_len_dim1
                assert np_output.shape[3] == length_dim2 * input_len_dim2
                assert np_output.shape[4] == length_dim3 * input_len_dim3
              else:  # tf
                assert np_output.shape[1] == length_dim1 * input_len_dim1
                assert np_output.shape[2] == length_dim2 * input_len_dim2
                assert np_output.shape[3] == length_dim3 * input_len_dim3

              # compare with numpy
              if data_format == 'channels_first':
                expected_out = np.repeat(inputs, length_dim1, axis=2)
                expected_out = np.repeat(expected_out, length_dim2, axis=3)
                expected_out = np.repeat(expected_out, length_dim3, axis=4)
              else:  # tf
                expected_out = np.repeat(inputs, length_dim1, axis=1)
                expected_out = np.repeat(expected_out, length_dim2, axis=2)
                expected_out = np.repeat(expected_out, length_dim3, axis=3)

              np.testing.assert_allclose(np_output, expected_out)


</source>
</class>

<class classid="206" nclones="2" nlines="16" similarity="86">
<source file="systems/keras-2.7.0/keras/layers/convolutional_test.py" startline="1211" endline="1228" pcid="3777">
  def test_depthwise_conv1d_full(self):
    kwargs = {
        'kernel_size': 3,
        'padding': 'valid',
        'data_format': 'channels_last',
        'dilation_rate': 1,
        'activation': None,
        'depthwise_regularizer': 'l2',
        'bias_regularizer': 'l2',
        'activity_regularizer': 'l2',
        'depthwise_constraint': 'unit_norm',
        'use_bias': True,
        'strides': 2,
        'depth_multiplier': 1,
    }
    self._run_test(kwargs)


</source>
<source file="systems/keras-2.7.0/keras/layers/convolutional_test.py" startline="1261" endline="1277" pcid="3780">
  def test_depthwise_conv2d_full(self):
    kwargs = {
        'kernel_size': 3,
        'padding': 'valid',
        'data_format': 'channels_last',
        'dilation_rate': (1, 1),
        'activation': None,
        'depthwise_regularizer': 'l2',
        'bias_regularizer': 'l2',
        'activity_regularizer': 'l2',
        'depthwise_constraint': 'unit_norm',
        'use_bias': True,
        'strides': (2, 2),
        'depth_multiplier': 1,
    }
    self._run_test(kwargs)

</source>
</class>

<class classid="207" nclones="2" nlines="12" similarity="76">
<source file="systems/keras-2.7.0/keras/layers/dense_attention_test.py" startline="185" endline="206" pcid="3822">
  def test_calculate_scores_multi_dim(self):
    # Query tensor of shape [1, 2, 4]
    q = np.array([[[1., 1.1, 1.2, 1.3], [2., 2.1, 2.2, 2.3]]], dtype=np.float32)
    # Key tensor of shape [1, 3, 4]
    k = np.array(
        [[[1.5, 1.6, 1.7, 1.8], [2.5, 2.6, 2.7, 2.8], [3.5, 3.6, 3.7, 3.8]]],
        dtype=np.float32)
    attention_layer = dense_attention.Attention()
    attention_layer.build(input_shape=([1, 2, 4], [1, 3, 4]))
    actual = attention_layer._calculate_scores(query=q, key=k)

    # Expected tensor of shape [1, 2, 3].
    # expected000 = 1.*1.5+1.1*1.6+1.2*1.7+1.3*1.8 = 7.64
    # expected001 = 1.*2.5+1.1*2.6+1.2*2.7+1.3*2.8 = 12.24
    # expected002 = 1.*3.5+1.1*3.6+1.2*3.7+1.3*3.8 = 16.84
    # expected010 = 2.*1.5+2.1*1.6+2.2*1.7+2.3*1.8 = 14.24
    # expected011 = 2.*2.5+2.1*2.6+2.2*2.7+2.3*2.8 = 22.84
    # expected012 = 2.*3.5+2.1*3.6+2.2*3.7+2.3*3.8 = 31.44
    expected = np.array([[[7.64, 12.24, 16.84], [14.24, 22.84, 31.44]]],
                        dtype=np.float32)
    self.assertAllClose(expected, actual)

</source>
<source file="systems/keras-2.7.0/keras/layers/dense_attention_test.py" startline="536" endline="561" pcid="3844">
  def test_calculate_scores_multi_dim(self):
    # Query tensor of shape [1, 2, 4]
    q = np.array([[[1., 1.1, 1.2, 1.3], [2., 2.1, 2.2, 2.3]]], dtype=np.float32)
    # Key tensor of shape [1, 3, 4]
    k = np.array(
        [[[1.5, 1.6, 1.7, 1.8], [2.5, 2.6, 2.7, 2.8], [3.5, 3.6, 3.7, 3.8]]],
        dtype=np.float32)
    attention_layer = dense_attention.AdditiveAttention()
    attention_layer.build(input_shape=([1, 2, 4], [1, 3, 4]))
    # Scale tensor of shape [4]
    attention_layer.scale = np.array([[[0.5, 0.6, 0.7, 0.8]]], dtype=np.float32)
    actual = attention_layer._calculate_scores(query=q, key=k)

    # pylint:disable=line-too-long
    # expected000 = 0.5*tanh(1.+1.5) + 0.6*tanh(1.1+1.6) + 0.7*tanh(1.2+1.7) + 0.8*tanh(1.3+1.8) = 2.58044532581
    # expected001 = 0.5*tanh(1.+2.5) + 0.6*tanh(1.1+2.6) + 0.7*tanh(1.2+2.7) + 0.8*tanh(1.3+2.8) = 2.59734317449
    # expected002 = 0.5*tanh(1.+3.5) + 0.6*tanh(1.1+3.6) + 0.7*tanh(1.2+3.7) + 0.8*tanh(1.3+3.8) = 2.59964024652
    # expected010 = 0.5*tanh(2.+1.5) + 0.6*tanh(2.1+1.6) + 0.7*tanh(2.2+1.7) + 0.8*tanh(2.3+1.8) = 2.59734317449
    # expected011 = 0.5*tanh(2.+2.5) + 0.6*tanh(2.1+2.6) + 0.7*tanh(2.2+2.7) + 0.8*tanh(2.3+2.8) = 2.59964024652
    # expected012 = 0.5*tanh(2.+3.5) + 0.6*tanh(2.1+3.6) + 0.7*tanh(2.2+3.7) + 0.8*tanh(2.3+3.8) = 2.59995130916
    # pylint:enable=line-too-long
    expected = np.array([[[2.58044532581, 2.59734317449, 2.59964024652],
                          [2.59734317449, 2.59964024652, 2.59995130916]]],
                        dtype=np.float32)
    self.assertAllClose(expected, actual)

</source>
</class>

<class classid="208" nclones="3" nlines="10" similarity="90">
<source file="systems/keras-2.7.0/keras/layers/dense_attention_test.py" startline="238" endline="252" pcid="3825">
  def test_shape(self):
    # Query tensor of shape [1, 2, 4]
    q = np.array([[[1., 1.1, 1.2, 1.3], [2., 2.1, 2.2, 2.3]]], dtype=np.float32)
    # Value tensor of shape [1, 3, 4]
    v = np.array(
        [[[1.5, 1.6, 1.7, 1.8], [2.5, 2.6, 2.7, 2.8], [3.5, 3.6, 3.7, 3.8]]],
        dtype=np.float32)
    # Value mask tensor of shape [1, 3]
    v_mask = np.array([[True, True, False]], dtype=np.bool_)
    attention_layer = dense_attention.Attention()
    actual = attention_layer([q, v], mask=[None, v_mask])

    expected_shape = [1, 2, 4]
    self.assertAllEqual(expected_shape, tf.shape(actual))

</source>
<source file="systems/keras-2.7.0/keras/layers/dense_attention_test.py" startline="595" endline="609" pcid="3847">
  def test_shape_no_scale(self):
    # Query tensor of shape [1, 2, 4]
    q = np.array([[[1., 1.1, 1.2, 1.3], [2., 2.1, 2.2, 2.3]]], dtype=np.float32)
    # Value tensor of shape [1, 3, 4]
    v = np.array(
        [[[1.5, 1.6, 1.7, 1.8], [2.5, 2.6, 2.7, 2.8], [3.5, 3.6, 3.7, 3.8]]],
        dtype=np.float32)
    # Value mask tensor of shape [1, 3]
    v_mask = np.array([[True, True, False]], dtype=np.bool_)
    attention_layer = dense_attention.AdditiveAttention(use_scale=False)
    actual = attention_layer([q, v], mask=[None, v_mask])

    expected_shape = [1, 2, 4]
    self.assertAllEqual(expected_shape, tf.shape(actual))

</source>
<source file="systems/keras-2.7.0/keras/layers/dense_attention_test.py" startline="580" endline="594" pcid="3846">
  def test_shape(self):
    # Query tensor of shape [1, 2, 4]
    q = np.array([[[1., 1.1, 1.2, 1.3], [2., 2.1, 2.2, 2.3]]], dtype=np.float32)
    # Value tensor of shape [1, 3, 4]
    v = np.array(
        [[[1.5, 1.6, 1.7, 1.8], [2.5, 2.6, 2.7, 2.8], [3.5, 3.6, 3.7, 3.8]]],
        dtype=np.float32)
    # Value mask tensor of shape [1, 3]
    v_mask = np.array([[True, True, False]], dtype=np.bool_)
    attention_layer = dense_attention.AdditiveAttention()
    actual = attention_layer([q, v], mask=[None, v_mask])

    expected_shape = [1, 2, 4]
    self.assertAllEqual(expected_shape, tf.shape(actual))

</source>
</class>

<class classid="209" nclones="2" nlines="13" similarity="100">
<source file="systems/keras-2.7.0/keras/layers/dense_attention_test.py" startline="253" endline="271" pcid="3826">
  def test_shape_with_key(self):
    # Query tensor of shape [1, 2, 4]
    q = np.array([[[1., 1.1, 1.2, 1.3], [2., 2.1, 2.2, 2.3]]], dtype=np.float32)
    # Value tensor of shape [1, 3, 4]
    v = np.array(
        [[[1.5, 1.6, 1.7, 1.8], [2.5, 2.6, 2.7, 2.8], [3.5, 3.6, 3.7, 3.8]]],
        dtype=np.float32)
    # Key tensor of shape [1, 3, 4]
    k = np.array(
        [[[1.5, 1.6, 1.7, 1.8], [2.5, 2.6, 2.7, 2.8], [3.5, 3.6, 3.7, 3.8]]],
        dtype=np.float32)
    # Value mask tensor of shape [1, 3]
    v_mask = np.array([[True, True, False]], dtype=np.bool_)
    attention_layer = dense_attention.Attention()
    actual = attention_layer([q, v, k], mask=[None, v_mask])

    expected_shape = [1, 2, 4]
    self.assertAllEqual(expected_shape, tf.shape(actual))

</source>
<source file="systems/keras-2.7.0/keras/layers/dense_attention_test.py" startline="610" endline="628" pcid="3848">
  def test_shape_with_key(self):
    # Query tensor of shape [1, 2, 4]
    q = np.array([[[1., 1.1, 1.2, 1.3], [2., 2.1, 2.2, 2.3]]], dtype=np.float32)
    # Value tensor of shape [1, 3, 4]
    v = np.array(
        [[[1.5, 1.6, 1.7, 1.8], [2.5, 2.6, 2.7, 2.8], [3.5, 3.6, 3.7, 3.8]]],
        dtype=np.float32)
    # Key tensor of shape [1, 3, 4]
    k = np.array(
        [[[1.5, 1.6, 1.7, 1.8], [2.5, 2.6, 2.7, 2.8], [3.5, 3.6, 3.7, 3.8]]],
        dtype=np.float32)
    # Value mask tensor of shape [1, 3]
    v_mask = np.array([[True, True, False]], dtype=np.bool_)
    attention_layer = dense_attention.AdditiveAttention()
    actual = attention_layer([q, v, k], mask=[None, v_mask])

    expected_shape = [1, 2, 4]
    self.assertAllEqual(expected_shape, tf.shape(actual))

</source>
</class>

<class classid="210" nclones="3" nlines="11" similarity="72">
<source file="systems/keras-2.7.0/keras/layers/dense_attention_test.py" startline="629" endline="662" pcid="3849">
  def test_multi_dim(self):
    # Query tensor of shape [1, 1, 1]
    q = np.array([[[1.1]]], dtype=np.float32)
    # Value tensor of shape [1, 3, 1]
    v = np.array([[[1.6], [0.7], [-0.8]]], dtype=np.float32)
    # Value mask tensor of shape [1, 3]
    v_mask = np.array([[True, True, False]], dtype=np.bool_)
    attention_layer = dense_attention.AdditiveAttention()
    attention_layer.build(input_shape=([1, 1, 1], [1, 3, 1]))
    # Scale tensor of shape [1]
    attention_layer.scale = np.array([[[0.5]]], dtype=np.float32)
    actual = attention_layer([q, v], mask=[None, v_mask])

    # pylint:disable=line-too-long
    # Expected scores of shape [1, 1, 3]
    # scores = [[[0.5 * tanh(1.1 + 1.6), 0.5 * tanh(1.1 + 0.7), 0.5 * tanh(1.1 - 0.8)]]]
    #        = [[[0.49550372683, 0.47340300642, 0.14565630622]]]
    # Expected attention distribution = softmax(scores) with zeros in
    # positions where v_mask == False.
    # => attention_distribution000
    #      = exp(0.49550372683)/(exp(0.49550372683) + exp(0.47340300642))
    #      = 0.50552495521
    #    attention_distribution001
    #      = exp(0.47340300642)/(exp(0.49550372683) + exp(0.47340300642))
    #      = 0.49447504478
    #    attention_distribution002 = 0
    #
    # Expected tensor of shape [1, 1, 1].
    # expected000 = 0.50552495521 * 1.6 + 0.49447504478 * 0.7 - 0 * 0.8
    #             = 1.15497245968
    # pylint:enable=line-too-long
    expected = np.array([[[1.15497245968]]], dtype=np.float32)
    self.assertAllClose(expected, actual)

</source>
<source file="systems/keras-2.7.0/keras/layers/dense_attention_test.py" startline="663" endline="698" pcid="3850">
  def test_multi_dim_with_key(self):
    # Query tensor of shape [1, 1, 1]
    q = np.array([[[1.1]]], dtype=np.float32)
    # Value tensor of shape [1, 3, 1]
    v = np.array([[[0.5], [0.8], [-0.3]]], dtype=np.float32)
    # Key tensor of shape [1, 3, 1]
    k = np.array([[[1.6], [0.7], [-0.8]]], dtype=np.float32)
    # Value mask tensor of shape [1, 3]
    v_mask = np.array([[True, True, False]], dtype=np.bool_)
    attention_layer = dense_attention.AdditiveAttention()
    attention_layer.build(input_shape=([1, 1, 1], [1, 3, 1]))
    # Scale tensor of shape [1]
    attention_layer.scale = np.array([[[0.5]]], dtype=np.float32)
    actual = attention_layer([q, v, k], mask=[None, v_mask])

    # pylint:disable=line-too-long
    # Expected scores of shape [1, 1, 3]
    # scores = [[[0.5 * tanh(1.1 + 1.6), 0.5 * tanh(1.1 + 0.7), 0.5 * tanh(1.1 - 0.8)]]]
    #        = [[[0.49550372683, 0.47340300642, 0.14565630622]]]
    # Expected attention distribution = softmax(scores) with zeros in
    # positions where v_mask == False.
    # => attention_distribution000
    #        = exp(0.49550372683)/(exp(0.49550372683) + exp(0.47340300642))
    #        = 0.50552495521
    #    attention_distribution001
    #        = exp(0.47340300642)/(exp(0.49550372683) + exp(0.47340300642))
    #        = 0.49447504478
    #    attention_distribution002 = 0
    #
    # Expected tensor of shape [1, 1, 1].
    # expected000 = 0.50552495521 * 0.5 + 0.49447504478 * 0.8 - 0 * 0.3
    #             = 0.64834251342
    # pylint:enable=line-too-long
    expected = np.array([[[0.64834251342]]], dtype=np.float32)
    self.assertAllClose(expected, actual)

</source>
<source file="systems/keras-2.7.0/keras/layers/dense_attention_test.py" startline="699" endline="744" pcid="3851">
  def test_multi_dim_with_query_mask(self):
    # Query tensor of shape [1, 2, 1]
    q = np.array([[[1.1], [-0.5]]], dtype=np.float32)
    # Value tensor of shape [1, 3, 1]
    v = np.array([[[1.6], [0.7], [-0.8]]], dtype=np.float32)
    # Query mask tensor of shape [1, 2]
    q_mask = np.array([[True, False]], dtype=np.bool_)
    # Value mask tensor of shape [1, 3]
    v_mask = np.array([[True, True, False]], dtype=np.bool_)
    attention_layer = dense_attention.AdditiveAttention()
    attention_layer.build(input_shape=([1, 1, 1], [1, 3, 1]))
    # Scale tensor of shape [1]
    attention_layer.scale = np.array([[[0.5]]], dtype=np.float32)
    actual = attention_layer([q, v], mask=[q_mask, v_mask])

    # pylint:disable=line-too-long
    # Expected scores of shape [1, 2, 3]
    # scores = [[[0.5 * tanh(1.1 + 1.6), 0.5 * tanh(1.1 + 0.7), 0.5 * tanh(1.1 - 0.8)],
    #            [0.5 * tanh(-0.5 + 1.6), 0.5 * tanh(-0.5 + 0.7), 0.5 * tanh(-0.5 - 0.8)]]]
    #        = [[[0.49550372683, 0.47340300642, 0.14565630622],
    #            [0.40024951088, 0.09868766011, -0.43086157965]]]
    # Expected attention distribution = softmax(scores) with zeros in
    # positions where v_mask == False.
    # => attention_distribution000
    #        = exp(0.49550372683)/(exp(0.49550372683) + exp(0.47340300642))
    #        = 0.50552495521
    #    attention_distribution001
    #        = exp(0.47340300642)/(exp(0.49550372683) + exp(0.47340300642))
    #        = 0.49447504478
    #    attention_distribution002 = 0
    # => attention_distribution010
    #        = exp(0.40024951088)/(exp(0.40024951088) + exp(0.09868766011))
    #        = 0.57482427975
    #    attention_distribution011
    #        = exp(0.09868766011)/(exp(0.40024951088) + exp(0.09868766011))
    #        = 0.42517572025
    #    attention_distribution012 = 0
    #
    # Expected tensor of shape [1, 2, 1] with zeros where  q_mask == False.
    # expected000 = 0.50552495521 * 1.6 + 0.49447504478 * 0.7 - 0 * 0.8
    #             = 1.15497245968
    # expected000 = 0
    # pylint:enable=line-too-long
    expected = np.array([[[1.15497245968], [0.]]], dtype=np.float32)
    self.assertAllClose(expected, actual)

</source>
</class>

<class classid="211" nclones="2" nlines="38" similarity="97">
<source file="systems/keras-2.7.0/keras/layers/local.py" startline="116" endline="155" pcid="3865">
  def __init__(self,
               filters,
               kernel_size,
               strides=1,
               padding='valid',
               data_format=None,
               activation=None,
               use_bias=True,
               kernel_initializer='glorot_uniform',
               bias_initializer='zeros',
               kernel_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               bias_constraint=None,
               implementation=1,
               **kwargs):
    super(LocallyConnected1D, self).__init__(**kwargs)
    self.filters = filters
    self.kernel_size = conv_utils.normalize_tuple(kernel_size, 1, 'kernel_size')
    self.strides = conv_utils.normalize_tuple(
        strides, 1, 'strides', allow_zero=True)
    self.padding = conv_utils.normalize_padding(padding)
    if self.padding != 'valid' and implementation == 1:
      raise ValueError('Invalid border mode for LocallyConnected1D '
                       '(only "valid" is supported if implementation is 1): ' +
                       padding)
    self.data_format = conv_utils.normalize_data_format(data_format)
    self.activation = activations.get(activation)
    self.use_bias = use_bias
    self.kernel_initializer = initializers.get(kernel_initializer)
    self.bias_initializer = initializers.get(bias_initializer)
    self.kernel_regularizer = regularizers.get(kernel_regularizer)
    self.bias_regularizer = regularizers.get(bias_regularizer)
    self.activity_regularizer = regularizers.get(activity_regularizer)
    self.kernel_constraint = constraints.get(kernel_constraint)
    self.bias_constraint = constraints.get(bias_constraint)
    self.implementation = implementation
    self.input_spec = InputSpec(ndim=3)

</source>
<source file="systems/keras-2.7.0/keras/layers/local.py" startline="433" endline="472" pcid="3871">
  def __init__(self,
               filters,
               kernel_size,
               strides=(1, 1),
               padding='valid',
               data_format=None,
               activation=None,
               use_bias=True,
               kernel_initializer='glorot_uniform',
               bias_initializer='zeros',
               kernel_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               bias_constraint=None,
               implementation=1,
               **kwargs):
    super(LocallyConnected2D, self).__init__(**kwargs)
    self.filters = filters
    self.kernel_size = conv_utils.normalize_tuple(kernel_size, 2, 'kernel_size')
    self.strides = conv_utils.normalize_tuple(
        strides, 2, 'strides', allow_zero=True)
    self.padding = conv_utils.normalize_padding(padding)
    if self.padding != 'valid' and implementation == 1:
      raise ValueError('Invalid border mode for LocallyConnected2D '
                       '(only "valid" is supported if implementation is 1): ' +
                       padding)
    self.data_format = conv_utils.normalize_data_format(data_format)
    self.activation = activations.get(activation)
    self.use_bias = use_bias
    self.kernel_initializer = initializers.get(kernel_initializer)
    self.bias_initializer = initializers.get(bias_initializer)
    self.kernel_regularizer = regularizers.get(kernel_regularizer)
    self.bias_regularizer = regularizers.get(bias_regularizer)
    self.activity_regularizer = regularizers.get(activity_regularizer)
    self.kernel_constraint = constraints.get(kernel_constraint)
    self.bias_constraint = constraints.get(bias_constraint)
    self.implementation = implementation
    self.input_spec = InputSpec(ndim=4)

</source>
</class>

<class classid="212" nclones="2" nlines="81" similarity="70">
<source file="systems/keras-2.7.0/keras/layers/local.py" startline="161" endline="259" pcid="3867">
  def build(self, input_shape):
    if self.data_format == 'channels_first':
      input_dim, input_length = input_shape[1], input_shape[2]
    else:
      input_dim, input_length = input_shape[2], input_shape[1]

    if input_dim is None:
      raise ValueError(
          'Axis 2 of input should be fully-defined. '
          'Found shape:', input_shape)
    self.output_length = conv_utils.conv_output_length(input_length,
                                                       self.kernel_size[0],
                                                       self.padding,
                                                       self.strides[0])

    if self.output_length <= 0:
      raise ValueError(
          f'One of the dimensions in the output is <= 0 '
          f'due to downsampling in {self.name}. Consider '
          f'increasing the input size. '
          f'Received input shape {input_shape} which would produce '
          f'output shape with a zero or negative value in a '
          f'dimension.')

    if self.implementation == 1:
      self.kernel_shape = (self.output_length, self.kernel_size[0] * input_dim,
                           self.filters)

      self.kernel = self.add_weight(
          shape=self.kernel_shape,
          initializer=self.kernel_initializer,
          name='kernel',
          regularizer=self.kernel_regularizer,
          constraint=self.kernel_constraint)

    elif self.implementation == 2:
      if self.data_format == 'channels_first':
        self.kernel_shape = (input_dim, input_length, self.filters,
                             self.output_length)
      else:
        self.kernel_shape = (input_length, input_dim, self.output_length,
                             self.filters)

      self.kernel = self.add_weight(
          shape=self.kernel_shape,
          initializer=self.kernel_initializer,
          name='kernel',
          regularizer=self.kernel_regularizer,
          constraint=self.kernel_constraint)

      self.kernel_mask = get_locallyconnected_mask(
          input_shape=(input_length,),
          kernel_shape=self.kernel_size,
          strides=self.strides,
          padding=self.padding,
          data_format=self.data_format,
      )

    elif self.implementation == 3:
      self.kernel_shape = (self.output_length * self.filters,
                           input_length * input_dim)

      self.kernel_idxs = sorted(
          conv_utils.conv_kernel_idxs(
              input_shape=(input_length,),
              kernel_shape=self.kernel_size,
              strides=self.strides,
              padding=self.padding,
              filters_in=input_dim,
              filters_out=self.filters,
              data_format=self.data_format))

      self.kernel = self.add_weight(
          shape=(len(self.kernel_idxs),),
          initializer=self.kernel_initializer,
          name='kernel',
          regularizer=self.kernel_regularizer,
          constraint=self.kernel_constraint)

    else:
      raise ValueError('Unrecognized implementation mode: %d.' %
                       self.implementation)

    if self.use_bias:
      self.bias = self.add_weight(
          shape=(self.output_length, self.filters),
          initializer=self.bias_initializer,
          name='bias',
          regularizer=self.bias_regularizer,
          constraint=self.bias_constraint)
    else:
      self.bias = None

    if self.data_format == 'channels_first':
      self.input_spec = InputSpec(ndim=3, axes={1: input_dim})
    else:
      self.input_spec = InputSpec(ndim=3, axes={-1: input_dim})
    self.built = True

</source>
<source file="systems/keras-2.7.0/keras/layers/local.py" startline="478" endline="579" pcid="3873">
  def build(self, input_shape):
    if self.data_format == 'channels_last':
      input_row, input_col = input_shape[1:-1]
      input_filter = input_shape[3]
    else:
      input_row, input_col = input_shape[2:]
      input_filter = input_shape[1]
    if input_row is None or input_col is None:
      raise ValueError('The spatial dimensions of the inputs to '
                       ' a LocallyConnected2D layer '
                       'should be fully-defined, but layer received '
                       'the inputs shape ' + str(input_shape))
    output_row = conv_utils.conv_output_length(input_row, self.kernel_size[0],
                                               self.padding, self.strides[0])
    output_col = conv_utils.conv_output_length(input_col, self.kernel_size[1],
                                               self.padding, self.strides[1])
    self.output_row = output_row
    self.output_col = output_col

    if self.output_row <= 0 or self.output_col <= 0:
      raise ValueError(
          f'One of the dimensions in the output is <= 0 '
          f'due to downsampling in {self.name}. Consider '
          f'increasing the input size. '
          f'Received input shape {input_shape} which would produce '
          f'output shape with a zero or negative value in a '
          f'dimension.')

    if self.implementation == 1:
      self.kernel_shape = (output_row * output_col, self.kernel_size[0] *
                           self.kernel_size[1] * input_filter, self.filters)

      self.kernel = self.add_weight(
          shape=self.kernel_shape,
          initializer=self.kernel_initializer,
          name='kernel',
          regularizer=self.kernel_regularizer,
          constraint=self.kernel_constraint)

    elif self.implementation == 2:
      if self.data_format == 'channels_first':
        self.kernel_shape = (input_filter, input_row, input_col, self.filters,
                             self.output_row, self.output_col)
      else:
        self.kernel_shape = (input_row, input_col, input_filter,
                             self.output_row, self.output_col, self.filters)

      self.kernel = self.add_weight(
          shape=self.kernel_shape,
          initializer=self.kernel_initializer,
          name='kernel',
          regularizer=self.kernel_regularizer,
          constraint=self.kernel_constraint)

      self.kernel_mask = get_locallyconnected_mask(
          input_shape=(input_row, input_col),
          kernel_shape=self.kernel_size,
          strides=self.strides,
          padding=self.padding,
          data_format=self.data_format,
      )

    elif self.implementation == 3:
      self.kernel_shape = (self.output_row * self.output_col * self.filters,
                           input_row * input_col * input_filter)

      self.kernel_idxs = sorted(
          conv_utils.conv_kernel_idxs(
              input_shape=(input_row, input_col),
              kernel_shape=self.kernel_size,
              strides=self.strides,
              padding=self.padding,
              filters_in=input_filter,
              filters_out=self.filters,
              data_format=self.data_format))

      self.kernel = self.add_weight(
          shape=(len(self.kernel_idxs),),
          initializer=self.kernel_initializer,
          name='kernel',
          regularizer=self.kernel_regularizer,
          constraint=self.kernel_constraint)

    else:
      raise ValueError('Unrecognized implementation mode: %d.' %
                       self.implementation)

    if self.use_bias:
      self.bias = self.add_weight(
          shape=(output_row, output_col, self.filters),
          initializer=self.bias_initializer,
          name='bias',
          regularizer=self.bias_regularizer,
          constraint=self.bias_constraint)
    else:
      self.bias = None
    if self.data_format == 'channels_first':
      self.input_spec = InputSpec(ndim=4, axes={1: input_filter})
    else:
      self.input_spec = InputSpec(ndim=4, axes={-1: input_filter})
    self.built = True

</source>
</class>

<class classid="213" nclones="2" nlines="20" similarity="90">
<source file="systems/keras-2.7.0/keras/layers/local.py" startline="275" endline="299" pcid="3869">
  def call(self, inputs):
    if self.implementation == 1:
      output = backend.local_conv(
          inputs, self.kernel, self.kernel_size, self.strides,
          (self.output_length,), self.data_format)

    elif self.implementation == 2:
      output = local_conv_matmul(inputs, self.kernel, self.kernel_mask,
                                 self.compute_output_shape(inputs.shape))

    elif self.implementation == 3:
      output = local_conv_sparse_matmul(inputs, self.kernel, self.kernel_idxs,
                                        self.kernel_shape,
                                        self.compute_output_shape(inputs.shape))

    else:
      raise ValueError('Unrecognized implementation mode: %d.' %
                       self.implementation)

    if self.use_bias:
      output = backend.bias_add(output, self.bias, data_format=self.data_format)

    output = self.activation(output)
    return output

</source>
<source file="systems/keras-2.7.0/keras/layers/local.py" startline="599" endline="624" pcid="3875">
  def call(self, inputs):
    if self.implementation == 1:
      output = backend.local_conv(
          inputs, self.kernel, self.kernel_size, self.strides,
          (self.output_row, self.output_col),
          self.data_format)

    elif self.implementation == 2:
      output = local_conv_matmul(inputs, self.kernel, self.kernel_mask,
                                 self.compute_output_shape(inputs.shape))

    elif self.implementation == 3:
      output = local_conv_sparse_matmul(inputs, self.kernel, self.kernel_idxs,
                                        self.kernel_shape,
                                        self.compute_output_shape(inputs.shape))

    else:
      raise ValueError('Unrecognized implementation mode: %d.' %
                       self.implementation)

    if self.use_bias:
      output = backend.bias_add(output, self.bias, data_format=self.data_format)

    output = self.activation(output)
    return output

</source>
</class>

<class classid="214" nclones="8" nlines="36" similarity="71">
<source file="systems/keras-2.7.0/keras/layers/local.py" startline="300" endline="336" pcid="3870">
  def get_config(self):
    config = {
        'filters':
            self.filters,
        'kernel_size':
            self.kernel_size,
        'strides':
            self.strides,
        'padding':
            self.padding,
        'data_format':
            self.data_format,
        'activation':
            activations.serialize(self.activation),
        'use_bias':
            self.use_bias,
        'kernel_initializer':
            initializers.serialize(self.kernel_initializer),
        'bias_initializer':
            initializers.serialize(self.bias_initializer),
        'kernel_regularizer':
            regularizers.serialize(self.kernel_regularizer),
        'bias_regularizer':
            regularizers.serialize(self.bias_regularizer),
        'activity_regularizer':
            regularizers.serialize(self.activity_regularizer),
        'kernel_constraint':
            constraints.serialize(self.kernel_constraint),
        'bias_constraint':
            constraints.serialize(self.bias_constraint),
        'implementation':
            self.implementation
    }
    base_config = super(LocallyConnected1D, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))


</source>
<source file="systems/keras-2.7.0/keras/layers/recurrent.py" startline="1423" endline="1458" pcid="4206">
  def get_config(self):
    config = {
        'units':
            self.units,
        'activation':
            activations.serialize(self.activation),
        'use_bias':
            self.use_bias,
        'kernel_initializer':
            initializers.serialize(self.kernel_initializer),
        'recurrent_initializer':
            initializers.serialize(self.recurrent_initializer),
        'bias_initializer':
            initializers.serialize(self.bias_initializer),
        'kernel_regularizer':
            regularizers.serialize(self.kernel_regularizer),
        'recurrent_regularizer':
            regularizers.serialize(self.recurrent_regularizer),
        'bias_regularizer':
            regularizers.serialize(self.bias_regularizer),
        'kernel_constraint':
            constraints.serialize(self.kernel_constraint),
        'recurrent_constraint':
            constraints.serialize(self.recurrent_constraint),
        'bias_constraint':
            constraints.serialize(self.bias_constraint),
        'dropout':
            self.dropout,
        'recurrent_dropout':
            self.recurrent_dropout
    }
    config.update(_config_for_enable_caching_device(self))
    base_config = super(SimpleRNNCell, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))


</source>
<source file="systems/keras-2.7.0/keras/layers/recurrent.py" startline="2897" endline="2940" pcid="4282">
  def get_config(self):
    config = {
        'units':
            self.units,
        'activation':
            activations.serialize(self.activation),
        'recurrent_activation':
            activations.serialize(self.recurrent_activation),
        'use_bias':
            self.use_bias,
        'kernel_initializer':
            initializers.serialize(self.kernel_initializer),
        'recurrent_initializer':
            initializers.serialize(self.recurrent_initializer),
        'bias_initializer':
            initializers.serialize(self.bias_initializer),
        'unit_forget_bias':
            self.unit_forget_bias,
        'kernel_regularizer':
            regularizers.serialize(self.kernel_regularizer),
        'recurrent_regularizer':
            regularizers.serialize(self.recurrent_regularizer),
        'bias_regularizer':
            regularizers.serialize(self.bias_regularizer),
        'activity_regularizer':
            regularizers.serialize(self.activity_regularizer),
        'kernel_constraint':
            constraints.serialize(self.kernel_constraint),
        'recurrent_constraint':
            constraints.serialize(self.recurrent_constraint),
        'bias_constraint':
            constraints.serialize(self.bias_constraint),
        'dropout':
            self.dropout,
        'recurrent_dropout':
            self.recurrent_dropout,
        'implementation':
            self.implementation
    }
    config.update(_config_for_enable_caching_device(self.cell))
    base_config = super(LSTM, self).get_config()
    del base_config['cell']
    return dict(list(base_config.items()) + list(config.items()))

</source>
<source file="systems/keras-2.7.0/keras/layers/convolutional_recurrent.py" startline="654" endline="702" pcid="3897">
  def get_config(self):
    config = {
        'filters':
            self.filters,
        'kernel_size':
            self.kernel_size,
        'strides':
            self.strides,
        'padding':
            self.padding,
        'data_format':
            self.data_format,
        'dilation_rate':
            self.dilation_rate,
        'activation':
            activations.serialize(self.activation),
        'recurrent_activation':
            activations.serialize(self.recurrent_activation),
        'use_bias':
            self.use_bias,
        'kernel_initializer':
            initializers.serialize(self.kernel_initializer),
        'recurrent_initializer':
            initializers.serialize(self.recurrent_initializer),
        'bias_initializer':
            initializers.serialize(self.bias_initializer),
        'unit_forget_bias':
            self.unit_forget_bias,
        'kernel_regularizer':
            regularizers.serialize(self.kernel_regularizer),
        'recurrent_regularizer':
            regularizers.serialize(self.recurrent_regularizer),
        'bias_regularizer':
            regularizers.serialize(self.bias_regularizer),
        'kernel_constraint':
            constraints.serialize(self.kernel_constraint),
        'recurrent_constraint':
            constraints.serialize(self.recurrent_constraint),
        'bias_constraint':
            constraints.serialize(self.bias_constraint),
        'dropout':
            self.dropout,
        'recurrent_dropout':
            self.recurrent_dropout,
    }
    base_config = super(ConvLSTMCell, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))


</source>
<source file="systems/keras-2.7.0/keras/layers/local.py" startline="625" endline="661" pcid="3876">
  def get_config(self):
    config = {
        'filters':
            self.filters,
        'kernel_size':
            self.kernel_size,
        'strides':
            self.strides,
        'padding':
            self.padding,
        'data_format':
            self.data_format,
        'activation':
            activations.serialize(self.activation),
        'use_bias':
            self.use_bias,
        'kernel_initializer':
            initializers.serialize(self.kernel_initializer),
        'bias_initializer':
            initializers.serialize(self.bias_initializer),
        'kernel_regularizer':
            regularizers.serialize(self.kernel_regularizer),
        'bias_regularizer':
            regularizers.serialize(self.bias_regularizer),
        'activity_regularizer':
            regularizers.serialize(self.activity_regularizer),
        'kernel_constraint':
            constraints.serialize(self.kernel_constraint),
        'bias_constraint':
            constraints.serialize(self.bias_constraint),
        'implementation':
            self.implementation
    }
    base_config = super(LocallyConnected2D, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))


</source>
<source file="systems/keras-2.7.0/keras/layers/recurrent.py" startline="1668" endline="1705" pcid="4223">
  def get_config(self):
    config = {
        'units':
            self.units,
        'activation':
            activations.serialize(self.activation),
        'use_bias':
            self.use_bias,
        'kernel_initializer':
            initializers.serialize(self.kernel_initializer),
        'recurrent_initializer':
            initializers.serialize(self.recurrent_initializer),
        'bias_initializer':
            initializers.serialize(self.bias_initializer),
        'kernel_regularizer':
            regularizers.serialize(self.kernel_regularizer),
        'recurrent_regularizer':
            regularizers.serialize(self.recurrent_regularizer),
        'bias_regularizer':
            regularizers.serialize(self.bias_regularizer),
        'activity_regularizer':
            regularizers.serialize(self.activity_regularizer),
        'kernel_constraint':
            constraints.serialize(self.kernel_constraint),
        'recurrent_constraint':
            constraints.serialize(self.recurrent_constraint),
        'bias_constraint':
            constraints.serialize(self.bias_constraint),
        'dropout':
            self.dropout,
        'recurrent_dropout':
            self.recurrent_dropout
    }
    base_config = super(SimpleRNN, self).get_config()
    config.update(_config_for_enable_caching_device(self.cell))
    del base_config['cell']
    return dict(list(base_config.items()) + list(config.items()))

</source>
<source file="systems/keras-2.7.0/keras/layers/recurrent.py" startline="2222" endline="2265" pcid="4249">
  def get_config(self):
    config = {
        'units':
            self.units,
        'activation':
            activations.serialize(self.activation),
        'recurrent_activation':
            activations.serialize(self.recurrent_activation),
        'use_bias':
            self.use_bias,
        'kernel_initializer':
            initializers.serialize(self.kernel_initializer),
        'recurrent_initializer':
            initializers.serialize(self.recurrent_initializer),
        'bias_initializer':
            initializers.serialize(self.bias_initializer),
        'kernel_regularizer':
            regularizers.serialize(self.kernel_regularizer),
        'recurrent_regularizer':
            regularizers.serialize(self.recurrent_regularizer),
        'bias_regularizer':
            regularizers.serialize(self.bias_regularizer),
        'activity_regularizer':
            regularizers.serialize(self.activity_regularizer),
        'kernel_constraint':
            constraints.serialize(self.kernel_constraint),
        'recurrent_constraint':
            constraints.serialize(self.recurrent_constraint),
        'bias_constraint':
            constraints.serialize(self.bias_constraint),
        'dropout':
            self.dropout,
        'recurrent_dropout':
            self.recurrent_dropout,
        'implementation':
            self.implementation,
        'reset_after':
            self.reset_after
    }
    config.update(_config_for_enable_caching_device(self.cell))
    base_config = super(GRU, self).get_config()
    del base_config['cell']
    return dict(list(base_config.items()) + list(config.items()))

</source>
<source file="systems/keras-2.7.0/keras/layers/recurrent.py" startline="2505" endline="2545" pcid="4257">
  def get_config(self):
    config = {
        'units':
            self.units,
        'activation':
            activations.serialize(self.activation),
        'recurrent_activation':
            activations.serialize(self.recurrent_activation),
        'use_bias':
            self.use_bias,
        'kernel_initializer':
            initializers.serialize(self.kernel_initializer),
        'recurrent_initializer':
            initializers.serialize(self.recurrent_initializer),
        'bias_initializer':
            initializers.serialize(self.bias_initializer),
        'unit_forget_bias':
            self.unit_forget_bias,
        'kernel_regularizer':
            regularizers.serialize(self.kernel_regularizer),
        'recurrent_regularizer':
            regularizers.serialize(self.recurrent_regularizer),
        'bias_regularizer':
            regularizers.serialize(self.bias_regularizer),
        'kernel_constraint':
            constraints.serialize(self.kernel_constraint),
        'recurrent_constraint':
            constraints.serialize(self.recurrent_constraint),
        'bias_constraint':
            constraints.serialize(self.bias_constraint),
        'dropout':
            self.dropout,
        'recurrent_dropout':
            self.recurrent_dropout,
        'implementation':
            self.implementation
    }
    config.update(_config_for_enable_caching_device(self))
    base_config = super(LSTMCell, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))

</source>
</class>

<class classid="215" nclones="4" nlines="18" similarity="94">
<source file="systems/keras-2.7.0/keras/layers/lstm_test.py" startline="121" endline="139" pcid="3958">
  def test_constraints_LSTM(self):
    embedding_dim = 4
    layer_class = keras.layers.LSTM
    k_constraint = keras.constraints.max_norm(0.01)
    r_constraint = keras.constraints.max_norm(0.01)
    b_constraint = keras.constraints.max_norm(0.01)
    layer = layer_class(
        5,
        return_sequences=False,
        weights=None,
        input_shape=(None, embedding_dim),
        kernel_constraint=k_constraint,
        recurrent_constraint=r_constraint,
        bias_constraint=b_constraint)
    layer.build((None, None, embedding_dim))
    self.assertEqual(layer.cell.kernel.constraint, k_constraint)
    self.assertEqual(layer.cell.recurrent_kernel.constraint, r_constraint)
    self.assertEqual(layer.cell.bias.constraint, b_constraint)

</source>
<source file="systems/keras-2.7.0/keras/layers/simplernn_test.py" startline="94" endline="112" pcid="3977">
  def test_constraints_SimpleRNN(self):
    embedding_dim = 4
    layer_class = keras.layers.SimpleRNN
    k_constraint = keras.constraints.max_norm(0.01)
    r_constraint = keras.constraints.max_norm(0.01)
    b_constraint = keras.constraints.max_norm(0.01)
    layer = layer_class(
        5,
        return_sequences=False,
        weights=None,
        input_shape=(None, embedding_dim),
        kernel_constraint=k_constraint,
        recurrent_constraint=r_constraint,
        bias_constraint=b_constraint)
    layer.build((None, None, embedding_dim))
    self.assertEqual(layer.cell.kernel.constraint, k_constraint)
    self.assertEqual(layer.cell.recurrent_kernel.constraint, r_constraint)
    self.assertEqual(layer.cell.bias.constraint, b_constraint)

</source>
<source file="systems/keras-2.7.0/keras/layers/gru_v2_test.py" startline="396" endline="414" pcid="4312">
  def test_constraints_GRU(self):
    embedding_dim = 4
    layer_class = rnn.GRU
    k_constraint = keras.constraints.max_norm(0.01)
    r_constraint = keras.constraints.max_norm(0.01)
    b_constraint = keras.constraints.max_norm(0.01)
    layer = layer_class(
        5,
        return_sequences=False,
        weights=None,
        input_shape=(None, embedding_dim),
        kernel_constraint=k_constraint,
        recurrent_constraint=r_constraint,
        bias_constraint=b_constraint)
    layer.build((None, None, embedding_dim))
    self.assertEqual(layer.cell.kernel.constraint, k_constraint)
    self.assertEqual(layer.cell.recurrent_kernel.constraint, r_constraint)
    self.assertEqual(layer.cell.bias.constraint, b_constraint)

</source>
<source file="systems/keras-2.7.0/keras/layers/gru_test.py" startline="229" endline="247" pcid="4516">
  def test_constraints_GRU(self):
    embedding_dim = 4
    layer_class = keras.layers.GRU
    k_constraint = keras.constraints.max_norm(0.01)
    r_constraint = keras.constraints.max_norm(0.01)
    b_constraint = keras.constraints.max_norm(0.01)
    layer = layer_class(
        5,
        return_sequences=False,
        weights=None,
        input_shape=(None, embedding_dim),
        kernel_constraint=k_constraint,
        recurrent_constraint=r_constraint,
        bias_constraint=b_constraint)
    layer.build((None, None, embedding_dim))
    self.assertEqual(layer.cell.kernel.constraint, k_constraint)
    self.assertEqual(layer.cell.recurrent_kernel.constraint, r_constraint)
    self.assertEqual(layer.cell.bias.constraint, b_constraint)

</source>
</class>

<class classid="216" nclones="3" nlines="13" similarity="78">
<source file="systems/keras-2.7.0/keras/layers/lstm_test.py" startline="144" endline="157" pcid="3959">
  def test_with_masking_layer_LSTM(self, unroll):
    layer_class = keras.layers.LSTM
    inputs = np.random.random((2, 3, 4))
    targets = np.abs(np.random.random((2, 3, 5)))
    targets /= targets.sum(axis=-1, keepdims=True)
    model = keras.models.Sequential()
    model.add(keras.layers.Masking(input_shape=(3, 4)))
    model.add(layer_class(units=5, return_sequences=True, unroll=unroll))
    model.compile(
        loss='categorical_crossentropy',
        optimizer='rmsprop',
        run_eagerly=testing_utils.should_run_eagerly())
    model.fit(inputs, targets, epochs=1, batch_size=2, verbose=1)

</source>
<source file="systems/keras-2.7.0/keras/layers/gru_test.py" startline="135" endline="148" pcid="4513">
  def test_with_masking_layer_GRU(self):
    layer_class = keras.layers.GRU
    inputs = np.random.random((2, 3, 4))
    targets = np.abs(np.random.random((2, 3, 5)))
    targets /= targets.sum(axis=-1, keepdims=True)
    model = keras.models.Sequential()
    model.add(keras.layers.Masking(input_shape=(3, 4)))
    model.add(layer_class(units=5, return_sequences=True, unroll=False))
    model.compile(
        loss='categorical_crossentropy',
        optimizer='rmsprop',
        run_eagerly=testing_utils.should_run_eagerly())
    model.fit(inputs, targets, epochs=1, batch_size=2, verbose=1)

</source>
<source file="systems/keras-2.7.0/keras/layers/lstm_test.py" startline="159" endline="173" pcid="3960">
  def test_masking_with_stacking_LSTM(self, unroll):
    inputs = np.random.random((2, 3, 4))
    targets = np.abs(np.random.random((2, 3, 5)))
    targets /= targets.sum(axis=-1, keepdims=True)
    model = keras.models.Sequential()
    model.add(keras.layers.Masking(input_shape=(3, 4)))
    lstm_cells = [keras.layers.LSTMCell(10), keras.layers.LSTMCell(5)]
    model.add(keras.layers.RNN(
        lstm_cells, return_sequences=True, unroll=unroll))
    model.compile(
        loss='categorical_crossentropy',
        optimizer='rmsprop',
        run_eagerly=testing_utils.should_run_eagerly())
    model.fit(inputs, targets, epochs=1, batch_size=2, verbose=1)

</source>
</class>

<class classid="217" nclones="2" nlines="11" similarity="72">
<source file="systems/keras-2.7.0/keras/layers/simplernn_test.py" startline="113" endline="123" pcid="3978">
  def test_with_masking_layer_SimpleRNN(self):
    layer_class = keras.layers.SimpleRNN
    inputs = np.random.random((2, 3, 4))
    targets = np.abs(np.random.random((2, 3, 5)))
    targets /= targets.sum(axis=-1, keepdims=True)
    model = keras.models.Sequential()
    model.add(keras.layers.Masking(input_shape=(3, 4)))
    model.add(layer_class(units=5, return_sequences=True, unroll=False))
    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')
    model.fit(inputs, targets, epochs=1, batch_size=2, verbose=1)

</source>
<source file="systems/keras-2.7.0/keras/layers/gru_v2_test.py" startline="310" endline="321" pcid="4306">
  def test_with_masking_layer_GRU(self):
    layer_class = rnn.GRU
    inputs = np.random.random((2, 3, 4))
    targets = np.abs(np.random.random((2, 3, 5)))
    targets /= targets.sum(axis=-1, keepdims=True)
    model = keras.models.Sequential()
    model.add(keras.layers.Masking(input_shape=(3, 4)))
    model.add(layer_class(units=5, return_sequences=True, unroll=False))
    model.compile(loss='categorical_crossentropy',
                  optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.001))
    model.fit(inputs, targets, epochs=1, batch_size=2, verbose=1)

</source>
</class>

<class classid="218" nclones="2" nlines="15" similarity="73">
<source file="systems/keras-2.7.0/keras/layers/pooling_test.py" startline="64" endline="82" pcid="4041">
  def test_globalpooling_1d_with_ragged(self):
    ragged_data = tf.ragged.constant(
        [[[1.0, 1.0], [2.0, 2.0], [3.0, 3.0]], [[1.0, 1.0], [2.0, 2.0]]],
        ragged_rank=1)
    dense_data = ragged_data.to_tensor()

    inputs = keras.Input(shape=(None, 2), dtype='float32', ragged=True)
    out = keras.layers.GlobalAveragePooling1D()(inputs)
    model = keras.models.Model(inputs=inputs, outputs=out)
    output_ragged = model.predict(ragged_data, steps=1)

    inputs = keras.Input(shape=(None, 2), dtype='float32')
    masking = keras.layers.Masking(mask_value=0., input_shape=(3, 2))(inputs)
    out = keras.layers.GlobalAveragePooling1D()(masking)
    model = keras.models.Model(inputs=inputs, outputs=out)
    output_dense = model.predict(dense_data, steps=1)

    self.assertAllEqual(output_ragged, output_dense)

</source>
<source file="systems/keras-2.7.0/keras/layers/pooling_test.py" startline="83" endline="101" pcid="4042">
  def test_globalpooling_2d_with_ragged(self):
    ragged_data = tf.ragged.constant(
        [[[[1.0], [1.0]], [[2.0], [2.0]], [[3.0], [3.0]]],
         [[[1.0], [1.0]], [[2.0], [2.0]]]],
        ragged_rank=1)
    dense_data = ragged_data.to_tensor()

    inputs = keras.Input(shape=(None, 2, 1), dtype='float32', ragged=True)
    out = keras.layers.GlobalMaxPooling2D()(inputs)
    model = keras.models.Model(inputs=inputs, outputs=out)
    output_ragged = model.predict(ragged_data, steps=1)

    inputs = keras.Input(shape=(None, 2, 1), dtype='float32')
    out = keras.layers.GlobalMaxPooling2D()(inputs)
    model = keras.models.Model(inputs=inputs, outputs=out)
    output_dense = model.predict(dense_data, steps=1)

    self.assertAllEqual(output_ragged, output_dense)

</source>
</class>

<class classid="219" nclones="2" nlines="17" similarity="76">
<source file="systems/keras-2.7.0/keras/layers/pooling_test.py" startline="117" endline="134" pcid="4044">
  def test_globalpooling_2d(self):
    testing_utils.layer_test(
        keras.layers.pooling.GlobalMaxPooling2D,
        kwargs={'data_format': 'channels_first'},
        input_shape=(3, 4, 5, 6))
    testing_utils.layer_test(
        keras.layers.pooling.GlobalMaxPooling2D,
        kwargs={'data_format': 'channels_last'},
        input_shape=(3, 5, 6, 4))
    testing_utils.layer_test(
        keras.layers.pooling.GlobalAveragePooling2D,
        kwargs={'data_format': 'channels_first'},
        input_shape=(3, 4, 5, 6))
    testing_utils.layer_test(
        keras.layers.pooling.GlobalAveragePooling2D,
        kwargs={'data_format': 'channels_last'},
        input_shape=(3, 5, 6, 4))

</source>
<source file="systems/keras-2.7.0/keras/layers/pooling_test.py" startline="135" endline="152" pcid="4045">
  def test_globalpooling_3d(self):
    testing_utils.layer_test(
        keras.layers.pooling.GlobalMaxPooling3D,
        kwargs={'data_format': 'channels_first'},
        input_shape=(3, 4, 3, 4, 3))
    testing_utils.layer_test(
        keras.layers.pooling.GlobalMaxPooling3D,
        kwargs={'data_format': 'channels_last'},
        input_shape=(3, 4, 3, 4, 3))
    testing_utils.layer_test(
        keras.layers.pooling.GlobalAveragePooling3D,
        kwargs={'data_format': 'channels_first'},
        input_shape=(3, 4, 3, 4, 3))
    testing_utils.layer_test(
        keras.layers.pooling.GlobalAveragePooling3D,
        kwargs={'data_format': 'channels_last'},
        input_shape=(3, 4, 3, 4, 3))

</source>
</class>

<class classid="220" nclones="2" nlines="17" similarity="100">
<source file="systems/keras-2.7.0/keras/layers/pooling_test.py" startline="285" endline="304" pcid="4052">
  def test_maxpooling_3d(self):
    pool_size = (3, 3, 3)
    testing_utils.layer_test(
        keras.layers.MaxPooling3D,
        kwargs={
            'strides': 2,
            'padding': 'valid',
            'pool_size': pool_size
        },
        input_shape=(3, 11, 12, 10, 4))
    testing_utils.layer_test(
        keras.layers.MaxPooling3D,
        kwargs={
            'strides': 3,
            'padding': 'valid',
            'data_format': 'channels_first',
            'pool_size': pool_size
        },
        input_shape=(3, 4, 11, 12, 10))

</source>
<source file="systems/keras-2.7.0/keras/layers/pooling_test.py" startline="305" endline="325" pcid="4053">
  def test_averagepooling_3d(self):
    pool_size = (3, 3, 3)
    testing_utils.layer_test(
        keras.layers.AveragePooling3D,
        kwargs={
            'strides': 2,
            'padding': 'valid',
            'pool_size': pool_size
        },
        input_shape=(3, 11, 12, 10, 4))
    testing_utils.layer_test(
        keras.layers.AveragePooling3D,
        kwargs={
            'strides': 3,
            'padding': 'valid',
            'data_format': 'channels_first',
            'pool_size': pool_size
        },
        input_shape=(3, 4, 11, 12, 10))


</source>
</class>

<class classid="221" nclones="2" nlines="13" similarity="100">
<source file="systems/keras-2.7.0/keras/layers/pooling_test.py" startline="329" endline="343" pcid="4054">
  def test_maxpooling_1d(self):
    for padding in ['valid', 'same']:
      for stride in [1, 2]:
        testing_utils.layer_test(
            keras.layers.MaxPooling1D,
            kwargs={
                'strides': stride,
                'padding': padding
            },
            input_shape=(3, 5, 4))
    testing_utils.layer_test(
        keras.layers.MaxPooling1D,
        kwargs={'data_format': 'channels_first'},
        input_shape=(3, 2, 6))

</source>
<source file="systems/keras-2.7.0/keras/layers/pooling_test.py" startline="344" endline="360" pcid="4055">
  def test_averagepooling_1d(self):
    for padding in ['valid', 'same']:
      for stride in [1, 2]:
        testing_utils.layer_test(
            keras.layers.AveragePooling1D,
            kwargs={
                'strides': stride,
                'padding': padding
            },
            input_shape=(3, 5, 4))

    testing_utils.layer_test(
        keras.layers.AveragePooling1D,
        kwargs={'data_format': 'channels_first'},
        input_shape=(3, 2, 6))


</source>
</class>

<class classid="222" nclones="2" nlines="14" similarity="73">
<source file="systems/keras-2.7.0/keras/layers/tensorflow_op_layer_test.py" startline="123" endline="143" pcid="4066">
def _int32_manipulation_too_big_for_shape():
  # This test verifies that the Keras Functional API
  # won't crash when manipulating int32 tensors that are too large
  # to represent shapes.
  inputs = keras.Input(batch_size=2, shape=(10,))
  batch_size = tf.shape(inputs)[0]
  num_features = 3 * 1024 * 16
  x = tf.range(batch_size * num_features, dtype='int32')
  assert x.shape.as_list() == [inputs.shape[0] * num_features]
  x = tf.reshape(x, (batch_size, num_features))
  x = tf.cast(x, dtype='float32')
  outputs = keras.layers.Dense(10)(x)
  if tf.executing_eagerly():
    return keras.Model(inputs, outputs)
  else:
    # In V1 the op layer fails for some reason,
    # but we don't have access to the test case to call
    # self.skip_test in this util method
    return keras.Model(inputs, inputs)


</source>
<source file="systems/keras-2.7.0/keras/layers/tensorflow_op_layer_test.py" startline="144" endline="171" pcid="4067">
def _int32_manipulation_at_max_shape_dims_limit():
  # This test verifies that the Keras Functional API
  # won't crash when manipulating int32 tensors that are at the limit
  # of the max tensor size Keras can try inferring values for.
  inputs = keras.Input(batch_size=2, shape=(10,))
  batch_size = tf.shape(inputs)[0]
  num_features = int(keras_tensor._MAX_TENSOR_RANK / int(inputs.shape[0]))
  x = tf.range(batch_size * num_features, dtype='int32')
  assert x.shape.as_list() == [keras_tensor._MAX_TENSOR_RANK]

  # Verify that a value was actually inferred for a tensor that *might*
  # represent the shape, bying checking that a value in
  # the range appears in the printed inferred value
  if tf.compat.v1.executing_eagerly_outside_functions():
    assert str(keras_tensor._MAX_TENSOR_RANK - 1) in str(x)

  x = tf.reshape(x, (batch_size, num_features))
  x = tf.cast(x, dtype='float32')
  outputs = keras.layers.Dense(10)(x)
  if tf.executing_eagerly():
    return keras.Model(inputs, outputs)
  else:
    # In V1 the op layer fails for some reason,
    # but we don't have access to the test case to call
    # self.skip_test in this util method
    return keras.Model(inputs, inputs)


</source>
</class>

<class classid="223" nclones="5" nlines="29" similarity="70">
<source file="systems/keras-2.7.0/keras/layers/tensorflow_op_layer_test.py" startline="391" endline="427" pcid="4083">
  def test_getitem_slice_with_step_only(self):
    if not tf.executing_eagerly():
      self.skipTest('Complex slicing like this fails in v1')
    inp = keras.Input(shape=(8,))
    slice_step = keras.Input(shape=(), dtype='int32')

    out = inp[..., ::slice_step[0]]
    model = keras.Model(
        inputs=[inp, slice_step],
        outputs=out)
    model.compile(
        adam.Adam(0.001),
        'mse',
        run_eagerly=testing_utils.should_run_eagerly())
    batch_size = 7
    step = 3
    x = tf.stack([
        tf.range(8) for _ in range(batch_size)])
    args = [x, tf.constant(step, shape=(batch_size,))]
    expected = tf.stack([
        tf.range(8)[::step] for _ in range(batch_size)])

    if tf.compat.v1.executing_eagerly_outside_functions():
      self.assertIn('tf.__operators__.getitem', (
          x.name for x in model.layers))
      self.assertNotIn('tf.strided_slice', (
          x.name for x in model.layers))
    self.assertAllEqual(model(args), expected)
    self.assertAllEqual(model.predict(args, batch_size=batch_size), expected)

    # Make sure it can be successfully saved and loaded
    config = model.get_config()
    model = keras.Model.from_config(config)

    self.assertAllEqual(model(args), expected)
    self.assertAllEqual(model.predict(args, batch_size=batch_size), expected)

</source>
<source file="systems/keras-2.7.0/keras/layers/tensorflow_op_layer_test.py" startline="497" endline="532" pcid="4086">
  def test_getitem_slice_with_stop_only(self):
    if not tf.executing_eagerly():
      self.skipTest('Complex slicing like this fails in v1')
    inp = keras.Input(shape=(8,))
    slice_stop = keras.Input(shape=(), dtype='int32')

    out = inp[:slice_stop[0]]
    model = keras.Model(
        inputs=[inp, slice_stop],
        outputs=out)
    model.compile(
        adam.Adam(0.001),
        'mse',
        run_eagerly=testing_utils.should_run_eagerly())
    batch_size = 7
    stop = 6
    x = tf.stack([
        tf.range(8) for _ in range(batch_size)])
    args = [x, tf.constant(stop, shape=(batch_size,))]
    expected = x[:stop]

    if tf.compat.v1.executing_eagerly_outside_functions():
      self.assertIn('tf.__operators__.getitem', (
          x.name for x in model.layers))
      self.assertNotIn('tf.strided_slice', (
          x.name for x in model.layers))
    self.assertAllEqual(model(args), expected)
    self.assertAllEqual(model.predict(args, batch_size=batch_size), expected)

    # Make sure it can be successfully saved and loaded
    config = model.get_config()
    model = keras.Model.from_config(config)

    self.assertAllEqual(model(args), expected)
    self.assertAllEqual(model.predict(args, batch_size=batch_size), expected)

</source>
<source file="systems/keras-2.7.0/keras/layers/tensorflow_op_layer_test.py" startline="533" endline="569" pcid="4087">
  def test_getitem_slice_with_stop_and_ellipsis_only(self):
    if not tf.executing_eagerly():
      self.skipTest('Complex slicing like this fails in v1')
    inp = keras.Input(shape=(8,))
    slice_stop = keras.Input(shape=(), dtype='int32')

    out = inp[..., :slice_stop[0]]
    model = keras.Model(
        inputs=[inp, slice_stop],
        outputs=out)
    model.compile(
        adam.Adam(0.001),
        'mse',
        run_eagerly=testing_utils.should_run_eagerly())
    batch_size = 7
    stop = 6
    x = tf.stack([
        tf.range(8) for _ in range(batch_size)])
    args = [x, tf.constant(stop, shape=(batch_size,))]
    expected = tf.stack([
        tf.range(8)[:stop] for _ in range(batch_size)])

    if tf.compat.v1.executing_eagerly_outside_functions():
      self.assertIn('tf.__operators__.getitem', (
          x.name for x in model.layers))
      self.assertNotIn('tf.strided_slice', (
          x.name for x in model.layers))
    self.assertAllEqual(model(args), expected)
    self.assertAllEqual(model.predict(args, batch_size=batch_size), expected)

    # Make sure it can be successfully saved and loaded
    config = model.get_config()
    model = keras.Model.from_config(config)

    self.assertAllEqual(model(args), expected)
    self.assertAllEqual(model.predict(args, batch_size=batch_size), expected)

</source>
<source file="systems/keras-2.7.0/keras/layers/tensorflow_op_layer_test.py" startline="462" endline="496" pcid="4085">
  def test_getitem_index_real_tensor(self):
    if not tf.executing_eagerly():
      self.skipTest('Complex slicing like this fails in v1')
    x = tf.range(10.0)
    slice_stop = keras.Input(shape=(), dtype='int32')

    out = x[slice_stop[0]]
    model = keras.Model(
        inputs=slice_stop,
        outputs=out)
    model.compile(
        adam.Adam(0.001),
        'mse',
        run_eagerly=testing_utils.should_run_eagerly())
    batch_size = 7
    index = 6
    args = tf.constant(index, shape=(batch_size,))
    expected = x[index]

    if tf.compat.v1.executing_eagerly_outside_functions():
      self.assertIn('tf.__operators__.getitem', (
          x.name for x in model.layers))
      # TODO(b/161925288): Fix the bug then uncomment:
      # self.assertNotIn('tf.strided_slice', (
      #     x.name for x in model.layers))
    self.assertAllEqual(model(args), expected)
    self.assertAllEqual(model.predict(args, batch_size=batch_size), expected)

    # Make sure it can be successfully saved and loaded
    config = model.get_config()
    model = keras.Model.from_config(config)

    self.assertAllEqual(model(args), expected)
    self.assertAllEqual(model.predict(args, batch_size=batch_size), expected)

</source>
<source file="systems/keras-2.7.0/keras/layers/tensorflow_op_layer_test.py" startline="428" endline="461" pcid="4084">
  def test_getitem_slice_real_tensor(self):
    if not tf.executing_eagerly():
      self.skipTest('Complex slicing like this fails in v1')
    x = tf.range(10.0)
    slice_stop = keras.Input(shape=(), dtype='int32')

    out = x[:slice_stop[0]]
    model = keras.Model(
        inputs=slice_stop,
        outputs=out)
    model.compile(
        adam.Adam(0.001),
        'mse',
        run_eagerly=testing_utils.should_run_eagerly())
    batch_size = 7
    stop = 6
    args = tf.constant(stop, shape=(batch_size,))
    expected = x[:stop]

    if tf.compat.v1.executing_eagerly_outside_functions():
      self.assertIn('tf.__operators__.getitem', (
          x.name for x in model.layers))
      # TODO(b/161925288): Fix the dispatch triggering then uncomment:
      # self.assertNotIn('tf.strided_slice', (
      #     x.name for x in model.layers))
    self.assertAllEqual(model(args), expected)
    self.assertAllEqual(model.predict(args, batch_size=batch_size), expected)

    config = model.get_config()
    model = keras.Model.from_config(config)

    self.assertAllEqual(model(args), expected)
    self.assertAllEqual(model.predict(args, batch_size=batch_size), expected)

</source>
</class>

<class classid="224" nclones="2" nlines="17" similarity="94">
<source file="systems/keras-2.7.0/keras/layers/serialization_test.py" startline="99" endline="116" pcid="4108">
  def test_serialize_deserialize_batchnorm(self, batchnorm_layer):
    layer = batchnorm_layer(
        momentum=0.9, beta_initializer='zeros', gamma_regularizer='l2')
    config = keras.layers.serialize(layer)
    self.assertEqual(config['class_name'], 'BatchNormalization')
    new_layer = keras.layers.deserialize(config)
    self.assertEqual(new_layer.momentum, 0.9)
    if tf.__internal__.tf2.enabled():
      self.assertIsInstance(new_layer, batchnorm_v2.BatchNormalization)
      self.assertEqual(new_layer.beta_initializer.__class__,
                       keras.initializers.ZerosV2)
    else:
      self.assertIsInstance(new_layer, batchnorm_v1.BatchNormalization)
      self.assertEqual(new_layer.beta_initializer.__class__,
                       keras.initializers.Zeros)
    self.assertEqual(new_layer.gamma_regularizer.__class__,
                     keras.regularizers.L2)

</source>
<source file="systems/keras-2.7.0/keras/layers/serialization_test.py" startline="119" endline="135" pcid="4109">
  def test_deserialize_batchnorm_backwards_compatibility(self, batchnorm_layer):
    layer = batchnorm_layer(
        momentum=0.9, beta_initializer='zeros', gamma_regularizer='l2')
    config = keras.layers.serialize(layer)
    new_layer = keras.layers.deserialize(config)
    self.assertEqual(new_layer.momentum, 0.9)
    if tf.__internal__.tf2.enabled():
      self.assertIsInstance(new_layer, batchnorm_v2.BatchNormalization)
      self.assertEqual(new_layer.beta_initializer.__class__,
                       keras.initializers.ZerosV2)
    else:
      self.assertIsInstance(new_layer, batchnorm_v1.BatchNormalization)
      self.assertEqual(new_layer.beta_initializer.__class__,
                       keras.initializers.Zeros)
    self.assertEqual(new_layer.gamma_regularizer.__class__,
                     keras.regularizers.L2)

</source>
</class>

<class classid="225" nclones="2" nlines="12" similarity="100">
<source file="systems/keras-2.7.0/keras/layers/serialization_test.py" startline="137" endline="149" pcid="4110">
  def test_serialize_deserialize_lstm(self, layer):
    lstm = layer(5, return_sequences=True)
    config = keras.layers.serialize(lstm)
    self.assertEqual(config['class_name'], 'LSTM')
    new_layer = keras.layers.deserialize(config)
    self.assertEqual(new_layer.units, 5)
    self.assertEqual(new_layer.return_sequences, True)
    if tf.__internal__.tf2.enabled():
      self.assertIsInstance(new_layer, rnn_v2.LSTM)
    else:
      self.assertIsInstance(new_layer, rnn_v1.LSTM)
      self.assertNotIsInstance(new_layer, rnn_v2.LSTM)

</source>
<source file="systems/keras-2.7.0/keras/layers/serialization_test.py" startline="151" endline="164" pcid="4111">
  def test_serialize_deserialize_gru(self, layer):
    gru = layer(5, return_sequences=True)
    config = keras.layers.serialize(gru)
    self.assertEqual(config['class_name'], 'GRU')
    new_layer = keras.layers.deserialize(config)
    self.assertEqual(new_layer.units, 5)
    self.assertEqual(new_layer.return_sequences, True)
    if tf.__internal__.tf2.enabled():
      self.assertIsInstance(new_layer, rnn_v2.GRU)
    else:
      self.assertIsInstance(new_layer, rnn_v1.GRU)
      self.assertNotIsInstance(new_layer, rnn_v2.GRU)


</source>
</class>

<class classid="226" nclones="2" nlines="26" similarity="77">
<source file="systems/keras-2.7.0/keras/layers/local_test.py" startline="86" endline="114" pcid="4137">
  def test_locallyconnected_1d(self, data_format, padding, implementation):
    with self.cached_session():
      num_samples = 2
      num_steps = 8
      input_dim = 5
      filter_length = 3
      filters = 4

      for strides in [1]:
        if padding == 'same' and strides != 1:
          continue
        kwargs = {
            'filters': filters,
            'kernel_size': filter_length,
            'padding': padding,
            'strides': strides,
            'data_format': data_format,
            'implementation': implementation
        }

        if padding == 'same' and implementation == 1:
          self.assertRaises(ValueError, keras.layers.LocallyConnected1D,
                            **kwargs)
        else:
          testing_utils.layer_test(
              keras.layers.LocallyConnected1D,
              kwargs=kwargs,
              input_shape=(num_samples, num_steps, input_dim))

</source>
<source file="systems/keras-2.7.0/keras/layers/local_test.py" startline="172" endline="203" pcid="4140">
  def test_locallyconnected_2d(self, data_format, padding, implementation):
    with self.cached_session():
      num_samples = 8
      filters = 3
      stack_size = 4
      num_row = 6
      num_col = 10

      for strides in [(1, 1), (2, 2)]:
        if padding == 'same' and strides != (1, 1):
          continue

        kwargs = {
            'filters': filters,
            'kernel_size': 3,
            'padding': padding,
            'kernel_regularizer': 'l2',
            'bias_regularizer': 'l2',
            'strides': strides,
            'data_format': data_format,
            'implementation': implementation
        }

        if padding == 'same' and implementation == 1:
          self.assertRaises(ValueError, keras.layers.LocallyConnected2D,
                            **kwargs)
        else:
          testing_utils.layer_test(
              keras.layers.LocallyConnected2D,
              kwargs=kwargs,
              input_shape=(num_samples, num_row, num_col, stack_size))

</source>
</class>

<class classid="227" nclones="2" nlines="40" similarity="87">
<source file="systems/keras-2.7.0/keras/layers/local_test.py" startline="116" endline="159" pcid="4138">
  def test_locallyconnected_1d_regularization(self, data_format, padding,
                                              implementation):
    num_samples = 2
    num_steps = 8
    input_dim = 5
    filter_length = 3
    filters = 4
    kwargs = {
        'filters': filters,
        'kernel_size': filter_length,
        'kernel_regularizer': 'l2',
        'bias_regularizer': 'l2',
        'activity_regularizer': 'l2',
        'data_format': data_format,
        'implementation': implementation,
        'padding': padding
    }

    if padding == 'same' and implementation == 1:
      self.assertRaises(ValueError, keras.layers.LocallyConnected1D, **kwargs)
    else:
      with self.cached_session():
        layer = keras.layers.LocallyConnected1D(**kwargs)
        layer.build((num_samples, num_steps, input_dim))
        self.assertEqual(len(layer.losses), 2)
        layer(
            keras.backend.variable(
                np.ones((num_samples, num_steps, input_dim))))
        self.assertEqual(len(layer.losses), 3)

      k_constraint = keras.constraints.max_norm(0.01)
      b_constraint = keras.constraints.max_norm(0.01)
      kwargs = {
          'filters': filters,
          'kernel_size': filter_length,
          'kernel_constraint': k_constraint,
          'bias_constraint': b_constraint,
      }
      with self.cached_session():
        layer = keras.layers.LocallyConnected1D(**kwargs)
        layer.build((num_samples, num_steps, input_dim))
        self.assertEqual(layer.kernel.constraint, k_constraint)
        self.assertEqual(layer.bias.constraint, b_constraint)

</source>
<source file="systems/keras-2.7.0/keras/layers/local_test.py" startline="230" endline="273" pcid="4142">
  def test_locallyconnected_2d_regularization(self, data_format, padding,
                                              implementation):
    num_samples = 2
    filters = 3
    stack_size = 4
    num_row = 6
    num_col = 7
    kwargs = {
        'filters': filters,
        'kernel_size': 3,
        'kernel_regularizer': 'l2',
        'bias_regularizer': 'l2',
        'activity_regularizer': 'l2',
        'implementation': implementation,
        'padding': padding,
        'data_format': data_format
    }

    if padding == 'same' and implementation == 1:
      self.assertRaises(ValueError, keras.layers.LocallyConnected2D, **kwargs)
    else:
      with self.cached_session():
        layer = keras.layers.LocallyConnected2D(**kwargs)
        layer.build((num_samples, num_row, num_col, stack_size))
        self.assertEqual(len(layer.losses), 2)
        layer(
            keras.backend.variable(
                np.ones((num_samples, num_row, num_col, stack_size))))
        self.assertEqual(len(layer.losses), 3)

      k_constraint = keras.constraints.max_norm(0.01)
      b_constraint = keras.constraints.max_norm(0.01)
      kwargs = {
          'filters': filters,
          'kernel_size': 3,
          'kernel_constraint': k_constraint,
          'bias_constraint': b_constraint,
      }
      with self.cached_session():
        layer = keras.layers.LocallyConnected2D(**kwargs)
        layer.build((num_samples, num_row, num_col, stack_size))
        self.assertEqual(layer.kernel.constraint, k_constraint)
        self.assertEqual(layer.bias.constraint, b_constraint)

</source>
</class>

<class classid="228" nclones="2" nlines="67" similarity="71">
<source file="systems/keras-2.7.0/keras/layers/local_test.py" startline="292" endline="388" pcid="4144">
  def test_locallyconnected_implementation(self, width, data_format):
    with self.cached_session():
      num_samples = 4
      num_classes = 3
      num_epochs = 2

      np.random.seed(1)
      tf_test_util.random_seed.set_seed(1)
      # Following code generates sparse targets and converts them
      # to one-hot encoded vectors
      # Create sparse targets eg. [0,1,2]
      sparse_targets = np.random.randint(0, num_classes, (num_samples,))

      # Convert to one-hot encoding
      # Final targets:
      # [[ 1. 0. 0. ]
      #  [ 0. 1. 0. ]
      #  [ 0. 0. 1. ]]

      targets = np.zeros((sparse_targets.size, num_classes))
      targets[np.arange(sparse_targets.size), sparse_targets] = 1
      height = 7
      filters = 2
      inputs = get_inputs(data_format, filters, height, num_samples, width)

      kernel_x = (3,)
      kernel_y = () if width == 1 else (2,)
      stride_x = (1,)
      stride_y = () if width == 1 else (3,)
      layers = 2

      kwargs = {
          'layers': layers,
          'filters': filters,
          'kernel_size': kernel_x + kernel_y,
          'strides': stride_x + stride_y,
          'data_format': data_format,
          'num_classes': num_classes
      }

      model_1 = get_model(implementation=1, **kwargs)
      model_2 = get_model(implementation=2, **kwargs)
      model_3 = get_model(implementation=3, **kwargs)

      # Build models.
      model_1.train_on_batch(inputs, targets)
      model_2.train_on_batch(inputs, targets)
      model_3.train_on_batch(inputs, targets)

      # Copy weights.
      copy_model_weights(model_from=model_2, model_to=model_1)
      copy_model_weights(model_from=model_2, model_to=model_3)

      # Compare outputs at initialization.
      out_1 = model_1(inputs)
      out_2 = model_2(inputs)
      out_3 = model_3(inputs)

      self.assertAllCloseAccordingToType(
          out_2, out_1, rtol=1e-5, atol=1e-5)
      self.assertAllCloseAccordingToType(
          out_2, out_3, rtol=1e-5, atol=1e-5)
      self.assertAllCloseAccordingToType(
          out_1, out_3, rtol=1e-5, atol=1e-5)

      # Train.
      model_1.fit(
          x=inputs,
          y=targets,
          epochs=num_epochs,
          batch_size=num_samples,
          shuffle=False)
      model_2.fit(
          x=inputs,
          y=targets,
          epochs=num_epochs,
          batch_size=num_samples,
          shuffle=False)
      model_3.fit(
          x=inputs,
          y=targets,
          epochs=num_epochs,
          batch_size=num_samples,
          shuffle=False)

      # Compare outputs after a few training steps.
      out_1 = model_1(inputs)
      out_2 = model_2(inputs)
      out_3 = model_3(inputs)

      self.assertAllCloseAccordingToType(
          out_2, out_1, atol=2e-4)
      self.assertAllCloseAccordingToType(
          out_2, out_3, atol=2e-4)
      self.assertAllCloseAccordingToType(
          out_1, out_3, atol=2e-4)

</source>
<source file="systems/keras-2.7.0/keras/layers/local_test.py" startline="407" endline="493" pcid="4145">
  def test_locallyconnected_save(self, width, data_format):
    with self.cached_session():
      num_samples = 4
      num_classes = 3
      num_epochs = 2

      np.random.seed(1)
      tf_test_util.random_seed.set_seed(1)
      # Following code generates sparse targets and converts them
      # to one-hot encoded vectors
      # Create sparse targets eg. [0,1,2]
      sparse_targets = np.random.randint(0, num_classes, (num_samples,))

      # Convert to one-hot encoding
      # Final targets:
      # [[ 1. 0. 0. ]
      #  [ 0. 1. 0. ]
      #  [ 0. 0. 1. ]]

      targets = np.zeros((sparse_targets.size, num_classes))
      targets[np.arange(sparse_targets.size), sparse_targets] = 1

      height = 7
      filters = 2
      inputs = get_inputs(data_format, filters, height, num_samples, width)

      kernel_x = (3,)
      kernel_y = () if width == 1 else (2,)
      stride_x = (1,)
      stride_y = () if width == 1 else (3,)
      layers = 2

      kwargs = {
          'layers': layers,
          'filters': filters,
          'kernel_size': kernel_x + kernel_y,
          'strides': stride_x + stride_y,
          'data_format': data_format,
          'num_classes': num_classes
      }

      model_1 = get_model_saveable(implementation=1, **kwargs)
      model_2 = get_model_saveable(implementation=2, **kwargs)
      model_3 = get_model_saveable(implementation=3, **kwargs)

      # Train.
      model_1.fit(
          x=inputs,
          y=targets,
          epochs=num_epochs,
          batch_size=num_samples,
          shuffle=False)
      model_2.fit(
          x=inputs,
          y=targets,
          epochs=num_epochs,
          batch_size=num_samples,
          shuffle=False)
      model_3.fit(
          x=inputs,
          y=targets,
          epochs=num_epochs,
          batch_size=num_samples,
          shuffle=False)

      out_1_before = model_1(inputs)
      out_2_before = model_2(inputs)
      out_3_before = model_3(inputs)

      path_1 = os.path.join(self.get_temp_dir(), 'model_1_path')
      model_1.save(path_1)
      model_1 = keras.models.load_model(path_1, custom_objects={'xent': xent})
      path_2 = os.path.join(self.get_temp_dir(), 'model_2_path')
      model_2.save(path_2)
      model_2 = keras.models.load_model(path_2, custom_objects={'xent': xent})
      path_3 = os.path.join(self.get_temp_dir(), 'model_3_path')
      model_3.save(path_3)
      model_3 = keras.models.load_model(path_3, custom_objects={'xent': xent})

      out_1_after = model_1(inputs)
      out_2_after = model_2(inputs)
      out_3_after = model_3(inputs)

      self.assertAllCloseAccordingToType(out_1_before, out_1_after, atol=2e-4)
      self.assertAllCloseAccordingToType(out_2_before, out_2_after, atol=2e-4)
      self.assertAllCloseAccordingToType(out_3_before, out_3_after, atol=2e-4)

</source>
</class>

<class classid="229" nclones="2" nlines="30" similarity="71">
<source file="systems/keras-2.7.0/keras/layers/local_test.py" startline="562" endline="598" pcid="4149">
def get_model(implementation,
              filters,
              kernel_size,
              strides,
              layers,
              num_classes,
              data_format):
  model = keras.Sequential()

  if len(kernel_size) == 1:
    lc_layer = keras.layers.LocallyConnected1D
  elif len(kernel_size) == 2:
    lc_layer = keras.layers.LocallyConnected2D
  else:
    raise NotImplementedError(kernel_size)

  for _ in range(layers):
    model.add(lc_layer(
        padding='valid',
        kernel_initializer=keras.initializers.random_normal(),
        bias_initializer=keras.initializers.random_normal(),
        filters=filters,
        strides=strides,
        kernel_size=kernel_size,
        activation=keras.activations.relu,
        data_format=data_format,
        implementation=implementation))

  model.add(keras.layers.Flatten())
  model.add(keras.layers.Dense(num_classes))
  model.compile(
      optimizer=RMSPropOptimizer(0.01),
      metrics=[keras.metrics.categorical_accuracy],
      loss=keras.losses.CategoricalCrossentropy(from_logits=True))
  return model


</source>
<source file="systems/keras-2.7.0/keras/layers/local_test.py" startline="599" endline="631" pcid="4150">
def get_model_saveable(implementation, filters, kernel_size, strides, layers,
                       num_classes, data_format):
  model = keras.Sequential()

  if len(kernel_size) == 1:
    lc_layer = keras.layers.LocallyConnected1D
  elif len(kernel_size) == 2:
    lc_layer = keras.layers.LocallyConnected2D
  else:
    raise NotImplementedError(kernel_size)

  for _ in range(layers):
    model.add(
        lc_layer(
            padding='valid',
            kernel_initializer=keras.initializers.random_normal(),
            bias_initializer=keras.initializers.random_normal(),
            filters=filters,
            strides=strides,
            kernel_size=kernel_size,
            activation=keras.activations.relu,
            data_format=data_format,
            implementation=implementation))

  model.add(keras.layers.Flatten())
  model.add(keras.layers.Dense(num_classes))
  model.compile(
      optimizer=rmsprop.RMSProp(learning_rate=0.01),
      metrics=[keras.metrics.categorical_accuracy],
      loss=keras.losses.CategoricalCrossentropy(from_logits=True))
  return model


</source>
</class>

<class classid="230" nclones="3" nlines="44" similarity="77">
<source file="systems/keras-2.7.0/keras/layers/recurrent.py" startline="1323" endline="1368" pcid="4202">
  def __init__(self,
               units,
               activation='tanh',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               **kwargs):
    if units < 0:
      raise ValueError(f'Received an invalid value for argument `units`, '
                       f'expected a positive integer, got {units}.')
    # By default use cached variable under v2 mode, see b/143699808.
    if tf.compat.v1.executing_eagerly_outside_functions():
      self._enable_caching_device = kwargs.pop('enable_caching_device', True)
    else:
      self._enable_caching_device = kwargs.pop('enable_caching_device', False)
    super(SimpleRNNCell, self).__init__(**kwargs)
    self.units = units
    self.activation = activations.get(activation)
    self.use_bias = use_bias

    self.kernel_initializer = initializers.get(kernel_initializer)
    self.recurrent_initializer = initializers.get(recurrent_initializer)
    self.bias_initializer = initializers.get(bias_initializer)

    self.kernel_regularizer = regularizers.get(kernel_regularizer)
    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)
    self.bias_regularizer = regularizers.get(bias_regularizer)

    self.kernel_constraint = constraints.get(kernel_constraint)
    self.recurrent_constraint = constraints.get(recurrent_constraint)
    self.bias_constraint = constraints.get(bias_constraint)

    self.dropout = min(1., max(0., dropout))
    self.recurrent_dropout = min(1., max(0., recurrent_dropout))
    self.state_size = self.units
    self.output_size = self.units

</source>
<source file="systems/keras-2.7.0/keras/layers/recurrent.py" startline="2325" endline="2380" pcid="4251">
  def __init__(self,
               units,
               activation='tanh',
               recurrent_activation='hard_sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               unit_forget_bias=True,
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               **kwargs):
    if units < 0:
      raise ValueError(f'Received an invalid value for argument `units`, '
                       f'expected a positive integer, got {units}.')
    # By default use cached variable under v2 mode, see b/143699808.
    if tf.compat.v1.executing_eagerly_outside_functions():
      self._enable_caching_device = kwargs.pop('enable_caching_device', True)
    else:
      self._enable_caching_device = kwargs.pop('enable_caching_device', False)
    super(LSTMCell, self).__init__(**kwargs)
    self.units = units
    self.activation = activations.get(activation)
    self.recurrent_activation = activations.get(recurrent_activation)
    self.use_bias = use_bias

    self.kernel_initializer = initializers.get(kernel_initializer)
    self.recurrent_initializer = initializers.get(recurrent_initializer)
    self.bias_initializer = initializers.get(bias_initializer)
    self.unit_forget_bias = unit_forget_bias

    self.kernel_regularizer = regularizers.get(kernel_regularizer)
    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)
    self.bias_regularizer = regularizers.get(bias_regularizer)

    self.kernel_constraint = constraints.get(kernel_constraint)
    self.recurrent_constraint = constraints.get(recurrent_constraint)
    self.bias_constraint = constraints.get(bias_constraint)

    self.dropout = min(1., max(0., dropout))
    self.recurrent_dropout = min(1., max(0., recurrent_dropout))
    implementation = kwargs.pop('implementation', 1)
    if self.recurrent_dropout != 0 and implementation != 1:
      logging.debug(RECURRENT_DROPOUT_WARNING_MSG)
      self.implementation = 1
    else:
      self.implementation = implementation
    self.state_size = [self.units, self.units]
    self.output_size = self.units

</source>
<source file="systems/keras-2.7.0/keras/layers/recurrent.py" startline="1762" endline="1818" pcid="4225">
  def __init__(self,
               units,
               activation='tanh',
               recurrent_activation='hard_sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               reset_after=False,
               **kwargs):
    if units < 0:
      raise ValueError(f'Received an invalid value for argument `units`, '
                       f'expected a positive integer, got {units}.')
    # By default use cached variable under v2 mode, see b/143699808.
    if tf.compat.v1.executing_eagerly_outside_functions():
      self._enable_caching_device = kwargs.pop('enable_caching_device', True)
    else:
      self._enable_caching_device = kwargs.pop('enable_caching_device', False)
    super(GRUCell, self).__init__(**kwargs)
    self.units = units
    self.activation = activations.get(activation)
    self.recurrent_activation = activations.get(recurrent_activation)
    self.use_bias = use_bias

    self.kernel_initializer = initializers.get(kernel_initializer)
    self.recurrent_initializer = initializers.get(recurrent_initializer)
    self.bias_initializer = initializers.get(bias_initializer)

    self.kernel_regularizer = regularizers.get(kernel_regularizer)
    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)
    self.bias_regularizer = regularizers.get(bias_regularizer)

    self.kernel_constraint = constraints.get(kernel_constraint)
    self.recurrent_constraint = constraints.get(recurrent_constraint)
    self.bias_constraint = constraints.get(bias_constraint)

    self.dropout = min(1., max(0., dropout))
    self.recurrent_dropout = min(1., max(0., recurrent_dropout))

    implementation = kwargs.pop('implementation', 1)
    if self.recurrent_dropout != 0 and implementation != 1:
      logging.debug(RECURRENT_DROPOUT_WARNING_MSG)
      self.implementation = 1
    else:
      self.implementation = implementation
    self.reset_after = reset_after
    self.state_size = self.units
    self.output_size = self.units

</source>
</class>

<class classid="231" nclones="2" nlines="29" similarity="74">
<source file="systems/keras-2.7.0/keras/layers/recurrent.py" startline="1370" endline="1397" pcid="4203">
  def build(self, input_shape):
    default_caching_device = _caching_device(self)
    self.kernel = self.add_weight(
        shape=(input_shape[-1], self.units),
        name='kernel',
        initializer=self.kernel_initializer,
        regularizer=self.kernel_regularizer,
        constraint=self.kernel_constraint,
        caching_device=default_caching_device)
    self.recurrent_kernel = self.add_weight(
        shape=(self.units, self.units),
        name='recurrent_kernel',
        initializer=self.recurrent_initializer,
        regularizer=self.recurrent_regularizer,
        constraint=self.recurrent_constraint,
        caching_device=default_caching_device)
    if self.use_bias:
      self.bias = self.add_weight(
          shape=(self.units,),
          name='bias',
          initializer=self.bias_initializer,
          regularizer=self.bias_regularizer,
          constraint=self.bias_constraint,
          caching_device=default_caching_device)
    else:
      self.bias = None
    self.built = True

</source>
<source file="systems/keras-2.7.0/keras/layers/recurrent.py" startline="1820" endline="1856" pcid="4226">
  def build(self, input_shape):
    input_dim = input_shape[-1]
    default_caching_device = _caching_device(self)
    self.kernel = self.add_weight(
        shape=(input_dim, self.units * 3),
        name='kernel',
        initializer=self.kernel_initializer,
        regularizer=self.kernel_regularizer,
        constraint=self.kernel_constraint,
        caching_device=default_caching_device)
    self.recurrent_kernel = self.add_weight(
        shape=(self.units, self.units * 3),
        name='recurrent_kernel',
        initializer=self.recurrent_initializer,
        regularizer=self.recurrent_regularizer,
        constraint=self.recurrent_constraint,
        caching_device=default_caching_device)

    if self.use_bias:
      if not self.reset_after:
        bias_shape = (3 * self.units,)
      else:
        # separate biases for input and recurrent kernels
        # Note: the shape is intentionally different from CuDNNGRU biases
        # `(2 * 3 * self.units,)`, so that we can distinguish the classes
        # when loading and converting saved weights.
        bias_shape = (2, 3 * self.units)
      self.bias = self.add_weight(shape=bias_shape,
                                  name='bias',
                                  initializer=self.bias_initializer,
                                  regularizer=self.bias_regularizer,
                                  constraint=self.bias_constraint,
                                  caching_device=default_caching_device)
    else:
      self.bias = None
    self.built = True

</source>
</class>

<class classid="232" nclones="3" nlines="15" similarity="93">
<source file="systems/keras-2.7.0/keras/layers/pooling.py" startline="50" endline="65" pcid="4326">
  def __init__(self, pool_function, pool_size, strides,
               padding='valid', data_format='channels_last',
               name=None, **kwargs):
    super(Pooling1D, self).__init__(name=name, **kwargs)
    if data_format is None:
      data_format = backend.image_data_format()
    if strides is None:
      strides = pool_size
    self.pool_function = pool_function
    self.pool_size = conv_utils.normalize_tuple(pool_size, 1, 'pool_size')
    self.strides = conv_utils.normalize_tuple(
        strides, 1, 'strides', allow_zero=True)
    self.padding = conv_utils.normalize_padding(padding)
    self.data_format = conv_utils.normalize_data_format(data_format)
    self.input_spec = InputSpec(ndim=3)

</source>
<source file="systems/keras-2.7.0/keras/layers/pooling.py" startline="334" endline="349" pcid="4332">
  def __init__(self, pool_function, pool_size, strides,
               padding='valid', data_format=None,
               name=None, **kwargs):
    super(Pooling2D, self).__init__(name=name, **kwargs)
    if data_format is None:
      data_format = backend.image_data_format()
    if strides is None:
      strides = pool_size
    self.pool_function = pool_function
    self.pool_size = conv_utils.normalize_tuple(pool_size, 2, 'pool_size')
    self.strides = conv_utils.normalize_tuple(
        strides, 2, 'strides', allow_zero=True)
    self.padding = conv_utils.normalize_padding(padding)
    self.data_format = conv_utils.normalize_data_format(data_format)
    self.input_spec = InputSpec(ndim=4)

</source>
<source file="systems/keras-2.7.0/keras/layers/pooling.py" startline="673" endline="688" pcid="4338">
  def __init__(self, pool_function, pool_size, strides,
               padding='valid', data_format='channels_last',
               name=None, **kwargs):
    super(Pooling3D, self).__init__(name=name, **kwargs)
    if data_format is None:
      data_format = backend.image_data_format()
    if strides is None:
      strides = pool_size
    self.pool_function = pool_function
    self.pool_size = conv_utils.normalize_tuple(pool_size, 3, 'pool_size')
    self.strides = conv_utils.normalize_tuple(
        strides, 3, 'strides', allow_zero=True)
    self.padding = conv_utils.normalize_padding(padding)
    self.data_format = conv_utils.normalize_data_format(data_format)
    self.input_spec = InputSpec(ndim=5)

</source>
</class>

<class classid="233" nclones="4" nlines="10" similarity="80">
<source file="systems/keras-2.7.0/keras/layers/pooling.py" startline="520" endline="531" pcid="4336">
  def __init__(self,
               pool_size=(2, 2),
               strides=None,
               padding='valid',
               data_format=None,
               **kwargs):
    super(MaxPooling2D, self).__init__(
        tf.compat.v1.nn.max_pool,
        pool_size=pool_size, strides=strides,
        padding=padding, data_format=data_format, **kwargs)


</source>
<source file="systems/keras-2.7.0/keras/layers/pooling.py" startline="634" endline="645" pcid="4337">
  def __init__(self,
               pool_size=(2, 2),
               strides=None,
               padding='valid',
               data_format=None,
               **kwargs):
    super(AveragePooling2D, self).__init__(
        tf.nn.avg_pool,
        pool_size=pool_size, strides=strides,
        padding=padding, data_format=data_format, **kwargs)


</source>
<source file="systems/keras-2.7.0/keras/layers/pooling.py" startline="873" endline="884" pcid="4343">
  def __init__(self,
               pool_size=(2, 2, 2),
               strides=None,
               padding='valid',
               data_format=None,
               **kwargs):
    super(AveragePooling3D, self).__init__(
        tf.nn.avg_pool3d,
        pool_size=pool_size, strides=strides,
        padding=padding, data_format=data_format, **kwargs)


</source>
<source file="systems/keras-2.7.0/keras/layers/pooling.py" startline="802" endline="813" pcid="4342">
  def __init__(self,
               pool_size=(2, 2, 2),
               strides=None,
               padding='valid',
               data_format=None,
               **kwargs):
    super(MaxPooling3D, self).__init__(
        tf.nn.max_pool3d,
        pool_size=pool_size, strides=strides,
        padding=padding, data_format=data_format, **kwargs)


</source>
</class>

<class classid="234" nclones="3" nlines="12" similarity="71">
<source file="systems/keras-2.7.0/keras/layers/pooling.py" startline="894" endline="906" pcid="4345">
  def compute_output_shape(self, input_shape):
    input_shape = tf.TensorShape(input_shape).as_list()
    if self.data_format == 'channels_first':
      if self.keepdims:
        return tf.TensorShape([input_shape[0], input_shape[1], 1])
      else:
        return tf.TensorShape([input_shape[0], input_shape[1]])
    else:
      if self.keepdims:
        return tf.TensorShape([input_shape[0], 1, input_shape[2]])
      else:
        return tf.TensorShape([input_shape[0], input_shape[2]])

</source>
<source file="systems/keras-2.7.0/keras/layers/pooling.py" startline="1061" endline="1073" pcid="4353">
  def compute_output_shape(self, input_shape):
    input_shape = tf.TensorShape(input_shape).as_list()
    if self.data_format == 'channels_last':
      if self.keepdims:
        return tf.TensorShape([input_shape[0], 1, 1, input_shape[3]])
      else:
        return tf.TensorShape([input_shape[0], input_shape[3]])
    else:
      if self.keepdims:
        return tf.TensorShape([input_shape[0], input_shape[1], 1, 1])
      else:
        return tf.TensorShape([input_shape[0], input_shape[1]])

</source>
<source file="systems/keras-2.7.0/keras/layers/pooling.py" startline="1199" endline="1213" pcid="4359">
  def compute_output_shape(self, input_shape):
    input_shape = tf.TensorShape(input_shape).as_list()
    if self.data_format == 'channels_last':
      if self.keepdims:
        return tf.TensorShape(
            [input_shape[0], 1, 1, 1, input_shape[4]])
      else:
        return tf.TensorShape([input_shape[0], input_shape[4]])
    else:
      if self.keepdims:
        return tf.TensorShape(
            [input_shape[0], input_shape[1], 1, 1, 1])
      else:
        return tf.TensorShape([input_shape[0], input_shape[1]])

</source>
</class>

<class classid="235" nclones="2" nlines="17" similarity="72">
<source file="systems/keras-2.7.0/keras/layers/subclassed_layers_test.py" startline="29" endline="49" pcid="4383">
  def test_simple_build_with_constant(self):

    class BuildConstantLayer(keras.layers.Layer):

      def build(self, input_shape):
        self.b = tf.convert_to_tensor(2.0)

      def call(self, inputs):
        return self.b * inputs

    layer = BuildConstantLayer()
    model = testing_utils.get_model_from_layers(
        [layer, keras.layers.Dense(1)], input_shape=(1,))

    x = tf.convert_to_tensor([[3.0]])
    self.assertEqual(
        tf_utils.is_symbolic_tensor(model(x)), not tf.executing_eagerly())
    self.assertEqual(
        tf_utils.is_symbolic_tensor(layer(x)), not tf.executing_eagerly())
    self.assertAllClose(keras.backend.get_value(layer(x)), [[6.0]])

</source>
<source file="systems/keras-2.7.0/keras/layers/subclassed_layers_test.py" startline="50" endline="74" pcid="4386">
  def test_build_with_derived_constant(self):

    class BuildDerivedConstantLayer(keras.layers.Layer):

      def build(self, input_shape):
        a = tf.convert_to_tensor(1.0)
        b = 2.0 * a
        self.variable = tf.Variable(b)
        self.constant = tf.convert_to_tensor(self.variable)

      def call(self, inputs):
        return self.variable * self.constant * inputs

    layer = BuildDerivedConstantLayer()
    model = testing_utils.get_model_from_layers(
        [layer, keras.layers.Dense(1)], input_shape=(1,))

    x = tf.convert_to_tensor([[3.0]])
    self.assertEqual(
        tf_utils.is_symbolic_tensor(model(x)), not tf.executing_eagerly())
    self.assertEqual(
        tf_utils.is_symbolic_tensor(layer(x)), not tf.executing_eagerly())
    self.assertAllClose(keras.backend.get_value(layer(x)), [[12.0]])


</source>
</class>

<class classid="236" nclones="2" nlines="10" similarity="70">
<source file="systems/keras-2.7.0/keras/layers/legacy_rnn/rnn_cell_wrapper_impl.py" startline="305" endline="316" pcid="4411">
  def from_config(cls, config, custom_objects=None):
    if "dropout_fn" in config:
      config = config.copy()
      dropout_state_filter = _parse_config_to_function(
          config, custom_objects, "dropout_fn", "dropout_fn_type",
          "dropout_fn_module")
      config.pop("dropout_fn")
      config["dropout_state_filter_visitor"] = dropout_state_filter
    return super(DropoutWrapperBase, cls).from_config(
        config, custom_objects=custom_objects)


</source>
<source file="systems/keras-2.7.0/keras/layers/legacy_rnn/rnn_cell_wrapper_impl.py" startline="393" endline="404" pcid="4420">
  def from_config(cls, config, custom_objects=None):
    if "residual_fn" in config:
      config = config.copy()
      residual_function = _parse_config_to_function(config, custom_objects,
                                                    "residual_fn",
                                                    "residual_fn_type",
                                                    "residual_fn_module")
      config["residual_fn"] = residual_function
    return super(ResidualWrapperBase, cls).from_config(
        config, custom_objects=custom_objects)


</source>
</class>

<class classid="237" nclones="3" nlines="24" similarity="75">
<source file="systems/keras-2.7.0/keras/layers/legacy_rnn/rnn_cell_impl.py" startline="409" endline="439" pcid="4448">
  def __init__(self,
               num_units,
               activation=None,
               reuse=None,
               name=None,
               dtype=None,
               **kwargs):
    warnings.warn(
        "`tf.nn.rnn_cell.BasicRNNCell` is deprecated and will be "
        "removed in a future version. This class "
        "is equivalent as `tf.keras.layers.SimpleRNNCell`, "
        "and will be replaced by that in Tensorflow 2.0.",
        stacklevel=2)
    super(BasicRNNCell, self).__init__(
        _reuse=reuse, name=name, dtype=dtype, **kwargs)
    _check_supported_dtypes(self.dtype)
    if tf.executing_eagerly() and tf.config.list_logical_devices("GPU"):
      logging.warning(
          "%s: Note that this cell is not optimized for performance. "
          "Please use tf.contrib.cudnn_rnn.CudnnRNNTanh for better "
          "performance on GPU.", self)

    # Inputs must be 2-dimensional.
    self.input_spec = input_spec.InputSpec(ndim=2)

    self._num_units = num_units
    if activation:
      self._activation = activations.get(activation)
    else:
      self._activation = tf.tanh

</source>
<source file="systems/keras-2.7.0/keras/layers/legacy_rnn/rnn_cell_impl.py" startline="519" endline="553" pcid="4454">
  def __init__(self,
               num_units,
               activation=None,
               reuse=None,
               kernel_initializer=None,
               bias_initializer=None,
               name=None,
               dtype=None,
               **kwargs):
    warnings.warn(
        "`tf.nn.rnn_cell.GRUCell` is deprecated and will be removed "
        "in a future version. This class "
        "is equivalent as `tf.keras.layers.GRUCell`, "
        "and will be replaced by that in Tensorflow 2.0.",
        stacklevel=2)
    super(GRUCell, self).__init__(
        _reuse=reuse, name=name, dtype=dtype, **kwargs)
    _check_supported_dtypes(self.dtype)

    if tf.executing_eagerly() and tf.config.list_logical_devices("GPU"):
      logging.warning(
          "%s: Note that this cell is not optimized for performance. "
          "Please use tf.contrib.cudnn_rnn.CudnnGRU for better "
          "performance on GPU.", self)
    # Inputs must be 2-dimensional.
    self.input_spec = input_spec.InputSpec(ndim=2)

    self._num_units = num_units
    if activation:
      self._activation = activations.get(activation)
    else:
      self._activation = tf.tanh
    self._kernel_initializer = initializers.get(kernel_initializer)
    self._bias_initializer = initializers.get(bias_initializer)

</source>
<source file="systems/keras-2.7.0/keras/layers/legacy_rnn/rnn_cell_impl.py" startline="674" endline="735" pcid="4461">
  def __init__(self,
               num_units,
               forget_bias=1.0,
               state_is_tuple=True,
               activation=None,
               reuse=None,
               name=None,
               dtype=None,
               **kwargs):
    """Initialize the basic LSTM cell.

    Args:
      num_units: int, The number of units in the LSTM cell.
      forget_bias: float, The bias added to forget gates (see above). Must set
        to `0.0` manually when restoring from CudnnLSTM-trained checkpoints.
      state_is_tuple: If True, accepted and returned states are 2-tuples of the
        `c_state` and `m_state`.  If False, they are concatenated along the
        column axis.  The latter behavior will soon be deprecated.
      activation: Activation function of the inner states.  Default: `tanh`. It
        could also be string that is within Keras activation function names.
      reuse: (optional) Python boolean describing whether to reuse variables in
        an existing scope.  If not `True`, and the existing scope already has
        the given variables, an error is raised.
      name: String, the name of the layer. Layers with the same name will share
        weights, but to avoid mistakes we require reuse=True in such cases.
      dtype: Default dtype of the layer (default of `None` means use the type of
        the first input). Required when `build` is called before `call`.
      **kwargs: Dict, keyword named properties for common layer attributes, like
        `trainable` etc when constructing the cell from configs of get_config().
        When restoring from CudnnLSTM-trained checkpoints, must use
        `CudnnCompatibleLSTMCell` instead.
    """
    warnings.warn(
        "`tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be "
        "removed in a future version. This class "
        "is equivalent as `tf.keras.layers.LSTMCell`, "
        "and will be replaced by that in Tensorflow 2.0.",
        stacklevel=2)
    super(BasicLSTMCell, self).__init__(
        _reuse=reuse, name=name, dtype=dtype, **kwargs)
    _check_supported_dtypes(self.dtype)
    if not state_is_tuple:
      logging.warning(
          "%s: Using a concatenated state is slower and will soon be "
          "deprecated.  Use state_is_tuple=True.", self)
    if tf.executing_eagerly() and tf.config.list_logical_devices("GPU"):
      logging.warning(
          "%s: Note that this cell is not optimized for performance. "
          "Please use tf.contrib.cudnn_rnn.CudnnLSTM for better "
          "performance on GPU.", self)

    # Inputs must be 2-dimensional.
    self.input_spec = input_spec.InputSpec(ndim=2)

    self._num_units = num_units
    self._forget_bias = forget_bias
    self._state_is_tuple = state_is_tuple
    if activation:
      self._activation = activations.get(activation)
    else:
      self._activation = tf.tanh

</source>
</class>

<class classid="238" nclones="2" nlines="15" similarity="80">
<source file="systems/keras-2.7.0/keras/layers/legacy_rnn/rnn_cell_impl.py" startline="449" endline="466" pcid="4451">
  def build(self, inputs_shape):
    if inputs_shape[-1] is None:
      raise ValueError(
          "Expected inputs.shape[-1] to be known, "
          f"received shape: {inputs_shape}")
    _check_supported_dtypes(self.dtype)

    input_depth = inputs_shape[-1]
    self._kernel = self.add_variable(
        _WEIGHTS_VARIABLE_NAME,
        shape=[input_depth + self._num_units, self._num_units])
    self._bias = self.add_variable(
        _BIAS_VARIABLE_NAME,
        shape=[self._num_units],
        initializer=tf.compat.v1.zeros_initializer(dtype=self.dtype))

    self.built = True

</source>
<source file="systems/keras-2.7.0/keras/layers/legacy_rnn/rnn_cell_impl.py" startline="746" endline="763" pcid="4464">
  def build(self, inputs_shape):
    if inputs_shape[-1] is None:
      raise ValueError(
          "Expected inputs.shape[-1] to be known, "
          f"received shape: {inputs_shape}")
    _check_supported_dtypes(self.dtype)
    input_depth = inputs_shape[-1]
    h_depth = self._num_units
    self._kernel = self.add_variable(
        _WEIGHTS_VARIABLE_NAME,
        shape=[input_depth + h_depth, 4 * self._num_units])
    self._bias = self.add_variable(
        _BIAS_VARIABLE_NAME,
        shape=[4 * self._num_units],
        initializer=tf.compat.v1.zeros_initializer(dtype=self.dtype))

    self.built = True

</source>
</class>

<class classid="239" nclones="3" nlines="10" similarity="70">
<source file="systems/keras-2.7.0/keras/layers/legacy_rnn/rnn_cell_impl.py" startline="614" endline="625" pcid="4459">
  def get_config(self):
    config = {
        "num_units": self._num_units,
        "kernel_initializer": initializers.serialize(self._kernel_initializer),
        "bias_initializer": initializers.serialize(self._bias_initializer),
        "activation": activations.serialize(self._activation),
        "reuse": self._reuse,
    }
    base_config = super(GRUCell, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))


</source>
<source file="systems/keras-2.7.0/keras/layers/legacy_rnn/rnn_cell_impl.py" startline="812" endline="823" pcid="4466">
  def get_config(self):
    config = {
        "num_units": self._num_units,
        "forget_bias": self._forget_bias,
        "state_is_tuple": self._state_is_tuple,
        "activation": activations.serialize(self._activation),
        "reuse": self._reuse,
    }
    base_config = super(BasicLSTMCell, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))


</source>
<source file="systems/keras-2.7.0/keras/layers/cudnn_recurrent.py" startline="117" endline="128" pcid="4567">
  def get_config(self):
    config = {
        'return_sequences': self.return_sequences,
        'return_state': self.return_state,
        'go_backwards': self.go_backwards,
        'stateful': self.stateful,
        'time_major': self.time_major,
    }
    base_config = super(  # pylint: disable=bad-super-call
        RNN, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))

</source>
</class>

<class classid="240" nclones="2" nlines="13" similarity="71">
<source file="systems/keras-2.7.0/keras/layers/dense_attention.py" startline="321" endline="333" pcid="4548">
  def build(self, input_shape):
    """Creates scale variable if use_scale==True."""
    if self.use_scale:
      self.scale = self.add_weight(
          name='scale',
          shape=(),
          initializer='ones',
          dtype=self.dtype,
          trainable=True)
    else:
      self.scale = None
    super(Attention, self).build(input_shape)

</source>
<source file="systems/keras-2.7.0/keras/layers/dense_attention.py" startline="462" endline="476" pcid="4552">
  def build(self, input_shape):
    v_shape = tf.TensorShape(input_shape[1])
    dim = v_shape[-1]
    dim = tf.compat.dimension_value(dim)
    if self.use_scale:
      self.scale = self.add_weight(
          name='scale',
          shape=[dim],
          initializer='glorot_uniform',
          dtype=self.dtype,
          trainable=True)
    else:
      self.scale = None
    super(AdditiveAttention, self).build(input_shape)

</source>
</class>

<class classid="241" nclones="3" nlines="41" similarity="70">
<source file="systems/keras-2.7.0/keras/layers/convolutional_recurrent_test.py" startline="34" endline="82" pcid="4557">
  def test_conv_lstm(self, data_format, return_sequences):
    num_row = 3
    filters = 3
    num_samples = 1
    input_channel = 2
    input_num_row = 5
    sequence_len = 2
    if data_format == 'channels_first':
      inputs = np.random.rand(num_samples, sequence_len, input_channel,
                              input_num_row)
    else:
      inputs = np.random.rand(num_samples, sequence_len, input_num_row,
                              input_channel)

    # test for return state:
    x = keras.Input(batch_shape=inputs.shape)
    kwargs = {
        'data_format': data_format,
        'return_sequences': return_sequences,
        'return_state': True,
        'stateful': True,
        'filters': filters,
        'kernel_size': num_row,
        'padding': 'valid',
    }
    layer = keras.layers.ConvLSTM1D(**kwargs)
    layer.build(inputs.shape)
    outputs = layer(x)
    _, states = outputs[0], outputs[1:]
    self.assertEqual(len(states), 2)
    model = keras.models.Model(x, states[0])

    state = model.predict(inputs)

    self.assertAllClose(keras.backend.eval(layer.states[0]), state, atol=1e-4)

    # test for output shape:
    testing_utils.layer_test(
        keras.layers.ConvLSTM1D,
        kwargs={
            'data_format': data_format,
            'return_sequences': return_sequences,
            'filters': filters,
            'kernel_size': num_row,
            'padding': 'valid'
        },
        input_shape=inputs.shape)


</source>
<source file="systems/keras-2.7.0/keras/layers/convolutional_recurrent_test.py" startline="90" endline="136" pcid="4558">
  def test_conv_lstm(self, data_format, return_sequences):
    num_row = 3
    num_col = 3
    filters = 2
    num_samples = 1
    input_channel = 2
    input_num_row = 5
    input_num_col = 5
    sequence_len = 2
    if data_format == 'channels_first':
      inputs = np.random.rand(num_samples, sequence_len,
                              input_channel,
                              input_num_row, input_num_col)
    else:
      inputs = np.random.rand(num_samples, sequence_len,
                              input_num_row, input_num_col,
                              input_channel)

    # test for return state:
    x = keras.Input(batch_shape=inputs.shape)
    kwargs = {'data_format': data_format,
              'return_sequences': return_sequences,
              'return_state': True,
              'stateful': True,
              'filters': filters,
              'kernel_size': (num_row, num_col),
              'padding': 'valid'}
    layer = keras.layers.ConvLSTM2D(**kwargs)
    layer.build(inputs.shape)
    outputs = layer(x)
    _, states = outputs[0], outputs[1:]
    self.assertEqual(len(states), 2)
    model = keras.models.Model(x, states[0])
    state = model.predict(inputs)

    self.assertAllClose(keras.backend.eval(layer.states[0]), state, atol=1e-4)

    # test for output shape:
    testing_utils.layer_test(
        keras.layers.ConvLSTM2D,
        kwargs={'data_format': data_format,
                'return_sequences': return_sequences,
                'filters': filters,
                'kernel_size': (num_row, num_col),
                'padding': 'valid'},
        input_shape=inputs.shape)

</source>
<source file="systems/keras-2.7.0/keras/layers/convolutional_recurrent_test.py" startline="295" endline="347" pcid="4564">
  def test_conv_lstm(self, data_format, return_sequences):
    num_height = 3
    num_width = 3
    num_depth = 3
    filters = 3
    num_samples = 1
    input_channel = 2
    input_height = 5
    input_width = 5
    input_depth = 5
    sequence_len = 2
    if data_format == 'channels_first':
      inputs = np.random.rand(num_samples, sequence_len, input_channel,
                              input_height, input_width, input_depth)
    else:
      inputs = np.random.rand(num_samples, sequence_len, input_height,
                              input_width, input_depth, input_channel)

    # test for return state:
    x = keras.Input(batch_shape=inputs.shape)
    kwargs = {
        'data_format': data_format,
        'return_sequences': return_sequences,
        'return_state': True,
        'stateful': True,
        'filters': filters,
        'kernel_size': (num_height, num_width, num_depth),
        'padding': 'same'
    }
    layer = keras.layers.ConvLSTM3D(**kwargs)
    layer.build(inputs.shape)
    outputs = layer(x)
    _, states = outputs[0], outputs[1:]
    self.assertEqual(len(states), 2)
    model = keras.models.Model(x, states[0])

    state = model.predict(inputs)

    self.assertAllClose(keras.backend.eval(layer.states[0]), state, atol=1e-4)

    # test for output shape:
    testing_utils.layer_test(
        keras.layers.ConvLSTM3D,
        kwargs={
            'data_format': data_format,
            'return_sequences': return_sequences,
            'filters': filters,
            'kernel_size': (num_height, num_width, num_depth),
            'padding': 'valid'
        },
        input_shape=inputs.shape)


</source>
</class>

<class classid="242" nclones="2" nlines="37" similarity="92">
<source file="systems/keras-2.7.0/keras/layers/cudnn_recurrent.py" startline="192" endline="231" pcid="4573">
  def __init__(self,
               units,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               return_sequences=False,
               return_state=False,
               go_backwards=False,
               stateful=False,
               **kwargs):
    self.units = units
    cell_spec = collections.namedtuple('cell', 'state_size')
    self._cell = cell_spec(state_size=self.units)
    super(CuDNNGRU, self).__init__(
        return_sequences=return_sequences,
        return_state=return_state,
        go_backwards=go_backwards,
        stateful=stateful,
        **kwargs)

    self.kernel_initializer = initializers.get(kernel_initializer)
    self.recurrent_initializer = initializers.get(recurrent_initializer)
    self.bias_initializer = initializers.get(bias_initializer)

    self.kernel_regularizer = regularizers.get(kernel_regularizer)
    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)
    self.bias_regularizer = regularizers.get(bias_regularizer)
    self.activity_regularizer = regularizers.get(activity_regularizer)

    self.kernel_constraint = constraints.get(kernel_constraint)
    self.recurrent_constraint = constraints.get(recurrent_constraint)
    self.bias_constraint = constraints.get(bias_constraint)

</source>
<source file="systems/keras-2.7.0/keras/layers/cudnn_recurrent.py" startline="376" endline="417" pcid="4578">
  def __init__(self,
               units,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               unit_forget_bias=True,
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               return_sequences=False,
               return_state=False,
               go_backwards=False,
               stateful=False,
               **kwargs):
    self.units = units
    cell_spec = collections.namedtuple('cell', 'state_size')
    self._cell = cell_spec(state_size=(self.units, self.units))
    super(CuDNNLSTM, self).__init__(
        return_sequences=return_sequences,
        return_state=return_state,
        go_backwards=go_backwards,
        stateful=stateful,
        **kwargs)

    self.kernel_initializer = initializers.get(kernel_initializer)
    self.recurrent_initializer = initializers.get(recurrent_initializer)
    self.bias_initializer = initializers.get(bias_initializer)
    self.unit_forget_bias = unit_forget_bias

    self.kernel_regularizer = regularizers.get(kernel_regularizer)
    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)
    self.bias_regularizer = regularizers.get(bias_regularizer)
    self.activity_regularizer = regularizers.get(activity_regularizer)

    self.kernel_constraint = constraints.get(kernel_constraint)
    self.recurrent_constraint = constraints.get(recurrent_constraint)
    self.bias_constraint = constraints.get(bias_constraint)

</source>
</class>

<class classid="243" nclones="2" nlines="45" similarity="74">
<source file="systems/keras-2.7.0/keras/layers/cudnn_recurrent.py" startline="265" endline="311" pcid="4576">
  def _process_batch(self, inputs, initial_state):
    if not self.time_major:
      inputs = tf.transpose(inputs, perm=(1, 0, 2))
    input_h = initial_state[0]
    input_h = tf.expand_dims(input_h, axis=0)

    params = recurrent_v2._canonical_to_params(    # pylint: disable=protected-access
        weights=[
            self.kernel[:, self.units:self.units * 2],
            self.kernel[:, :self.units],
            self.kernel[:, self.units * 2:],
            self.recurrent_kernel[:, self.units:self.units * 2],
            self.recurrent_kernel[:, :self.units],
            self.recurrent_kernel[:, self.units * 2:],
        ],
        biases=[
            self.bias[self.units:self.units * 2],
            self.bias[:self.units],
            self.bias[self.units * 2:self.units * 3],
            self.bias[self.units * 4:self.units * 5],
            self.bias[self.units * 3:self.units * 4],
            self.bias[self.units * 5:],
        ],
        shape=self._vector_shape)

    args = {
        'input': inputs,
        'input_h': input_h,
        'input_c': 0,
        'params': params,
        'is_training': True,
        'rnn_mode': 'gru',
    }

    outputs, h, _, _, _ = tf.raw_ops.CudnnRNNV2(**args)

    if self.stateful or self.return_state:
      h = h[0]
    if self.return_sequences:
      if self.time_major:
        output = outputs
      else:
        output = tf.transpose(outputs, perm=(1, 0, 2))
    else:
      output = outputs[-1]
    return output, [h]

</source>
<source file="systems/keras-2.7.0/keras/layers/cudnn_recurrent.py" startline="461" endline="513" pcid="4582">
  def _process_batch(self, inputs, initial_state):
    if not self.time_major:
      inputs = tf.transpose(inputs, perm=(1, 0, 2))
    input_h = initial_state[0]
    input_c = initial_state[1]
    input_h = tf.expand_dims(input_h, axis=0)
    input_c = tf.expand_dims(input_c, axis=0)

    params = recurrent_v2._canonical_to_params(    # pylint: disable=protected-access
        weights=[
            self.kernel[:, :self.units],
            self.kernel[:, self.units:self.units * 2],
            self.kernel[:, self.units * 2:self.units * 3],
            self.kernel[:, self.units * 3:],
            self.recurrent_kernel[:, :self.units],
            self.recurrent_kernel[:, self.units:self.units * 2],
            self.recurrent_kernel[:, self.units * 2:self.units * 3],
            self.recurrent_kernel[:, self.units * 3:],
        ],
        biases=[
            self.bias[:self.units],
            self.bias[self.units:self.units * 2],
            self.bias[self.units * 2:self.units * 3],
            self.bias[self.units * 3:self.units * 4],
            self.bias[self.units * 4:self.units * 5],
            self.bias[self.units * 5:self.units * 6],
            self.bias[self.units * 6:self.units * 7],
            self.bias[self.units * 7:],
        ],
        shape=self._vector_shape)

    args = {
        'input': inputs,
        'input_h': input_h,
        'input_c': input_c,
        'params': params,
        'is_training': True,
    }

    outputs, h, c, _, _ = tf.raw_ops.CudnnRNNV2(**args)

    if self.stateful or self.return_state:
      h = h[0]
      c = c[0]
    if self.return_sequences:
      if self.time_major:
        output = outputs
      else:
        output = tf.transpose(outputs, perm=(1, 0, 2))
    else:
      output = outputs[-1]
    return output, [h, c]

</source>
</class>

<class classid="244" nclones="2" nlines="20" similarity="95">
<source file="systems/keras-2.7.0/keras/layers/cudnn_recurrent.py" startline="312" endline="333" pcid="4577">
  def get_config(self):
    config = {
        'units': self.units,
        'kernel_initializer': initializers.serialize(self.kernel_initializer),
        'recurrent_initializer':
            initializers.serialize(self.recurrent_initializer),
        'bias_initializer': initializers.serialize(self.bias_initializer),
        'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),
        'recurrent_regularizer':
            regularizers.serialize(self.recurrent_regularizer),
        'bias_regularizer': regularizers.serialize(self.bias_regularizer),
        'activity_regularizer':
            regularizers.serialize(self.activity_regularizer),
        'kernel_constraint': constraints.serialize(self.kernel_constraint),
        'recurrent_constraint':
            constraints.serialize(self.recurrent_constraint),
        'bias_constraint': constraints.serialize(self.bias_constraint)
    }
    base_config = super(CuDNNGRU, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))


</source>
<source file="systems/keras-2.7.0/keras/layers/cudnn_recurrent.py" startline="514" endline="534" pcid="4583">
  def get_config(self):
    config = {
        'units': self.units,
        'kernel_initializer': initializers.serialize(self.kernel_initializer),
        'recurrent_initializer':
            initializers.serialize(self.recurrent_initializer),
        'bias_initializer': initializers.serialize(self.bias_initializer),
        'unit_forget_bias': self.unit_forget_bias,
        'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),
        'recurrent_regularizer':
            regularizers.serialize(self.recurrent_regularizer),
        'bias_regularizer': regularizers.serialize(self.bias_regularizer),
        'activity_regularizer':
            regularizers.serialize(self.activity_regularizer),
        'kernel_constraint': constraints.serialize(self.kernel_constraint),
        'recurrent_constraint':
            constraints.serialize(self.recurrent_constraint),
        'bias_constraint': constraints.serialize(self.bias_constraint)
    }
    base_config = super(CuDNNLSTM, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))
</source>
</class>

<class classid="245" nclones="2" nlines="11" similarity="81">
<source file="systems/keras-2.7.0/keras/layers/core/core_test.py" startline="54" endline="67" pcid="4614">
  def test_spatial_dropout_2d(self):
    testing_utils.layer_test(
        keras.layers.SpatialDropout2D,
        kwargs={'rate': 0.5},
        input_shape=(2, 3, 4, 5))

    testing_utils.layer_test(
        keras.layers.SpatialDropout2D,
        kwargs={
            'rate': 0.5,
            'data_format': 'channels_first'
        },
        input_shape=(2, 3, 4, 5))

</source>
<source file="systems/keras-2.7.0/keras/layers/core/core_test.py" startline="68" endline="81" pcid="4615">
  def test_spatial_dropout_3d(self):
    testing_utils.layer_test(
        keras.layers.SpatialDropout3D,
        kwargs={'rate': 0.5},
        input_shape=(2, 3, 4, 4, 5))

    testing_utils.layer_test(
        keras.layers.SpatialDropout3D,
        kwargs={
            'rate': 0.5,
            'data_format': 'channels_first'
        },
        input_shape=(2, 3, 4, 4, 5))

</source>
</class>

<class classid="246" nclones="2" nlines="54" similarity="79">
<source file="systems/keras-2.7.0/keras/layers/recurrent_v2.py" startline="612" endline="697" pcid="4719">
def gpu_gru(inputs, init_h, kernel, recurrent_kernel, bias, mask, time_major,
            go_backwards, sequence_lengths):
  """GRU with cuDNN implementation which is only available for GPU."""
  if not time_major and mask is None:
    inputs = tf.transpose(inputs, perm=(1, 0, 2))
    seq_axis, batch_axis = (0, 1)
  else:
    seq_axis, batch_axis = (0, 1) if time_major else (1, 0)
  # For init_h, cuDNN expects one more dim of num_layers before or after batch
  # dim for time major or batch major inputs respectively
  init_h = tf.expand_dims(init_h, axis=seq_axis)

  weights = tf.split(kernel, 3, axis=1)
  weights += tf.split(recurrent_kernel, 3, axis=1)
  # Note that the bias was initialized as shape (2, 3 * units), flat it into
  # (6 * units)
  bias = tf.split(backend.flatten(bias), 6)

  if tf.sysconfig.get_build_info()['is_cuda_build']:
    # Note that the gate order for cuDNN is different from the canonical format.
    # canonical format is [z, r, h], whereas cuDNN is [r, z, h]. The swap need
    # to be done for kernel, recurrent_kernel, input_bias, recurrent_bias.
    # z is update gate weights.
    # r is reset gate weights.
    # h is output gate weights.
    weights[0], weights[1] = weights[1], weights[0]
    weights[3], weights[4] = weights[4], weights[3]
    bias[0], bias[1] = bias[1], bias[0]
    bias[3], bias[4] = bias[4], bias[3]

  params = _canonical_to_params(
      weights=weights,
      biases=bias,
      shape=tf.constant([-1]),
      transpose_weights=True)

  if mask is not None:
    sequence_lengths = calculate_sequence_by_mask(mask, time_major)

  if sequence_lengths is not None:
    if go_backwards:
      # Three reversals are required. E.g.,
      # normal input = [1, 2, 3, 0, 0]  # where 0 need to be masked
      # reversed_input_to_cudnn = [3, 2, 1, 0, 0]
      # output_from_cudnn = [6, 5, 4, 0, 0]
      # expected_output = [0, 0, 6, 5 ,4]
      inputs = tf.reverse_sequence(
          inputs, sequence_lengths, seq_axis=seq_axis, batch_axis=batch_axis)
    outputs, h, _, _, _ = tf.raw_ops.CudnnRNNV3(
        input=inputs,
        input_h=init_h,
        input_c=0,
        params=params,
        is_training=True,
        rnn_mode='gru',
        sequence_lengths=sequence_lengths,
        time_major=time_major)
    if go_backwards:
      outputs = tf.reverse_sequence(
          outputs, sequence_lengths, seq_axis=seq_axis, batch_axis=batch_axis)
      outputs = tf.reverse(outputs, axis=[seq_axis])
  else:
    if go_backwards:
      # Reverse axis 0 since the input is already convert to time major.
      inputs = tf.reverse(inputs, axis=[0])
    outputs, h, _, _ = tf.raw_ops.CudnnRNN(
        input=inputs, input_h=init_h, input_c=0, params=params,
        is_training=True, rnn_mode='gru')

  last_output = outputs[-1]
  if not time_major and mask is None:
    outputs = tf.transpose(outputs, perm=[1, 0, 2])
  h = tf.squeeze(h, axis=seq_axis)

  # In the case of variable length input, the cudnn kernel will fill zeros for
  # the output, whereas the default keras behavior is to bring over the previous
  # output for t-1, so that in the return_sequence=False case, user can quickly
  # get the final effect output instead just 0s at the last timestep.
  # In order to mimic the default keras behavior, we copy the final h state as
  # the last_output, since it is numerically same as the output.
  if mask is not None:
    last_output = h

  return last_output, outputs, h, _runtime(_RUNTIME_GPU)


</source>
<source file="systems/keras-2.7.0/keras/layers/recurrent_v2.py" startline="1395" endline="1519" pcid="4732">
def gpu_lstm(inputs, init_h, init_c, kernel, recurrent_kernel, bias, mask,
             time_major, go_backwards, sequence_lengths):
  """LSTM with either cuDNN or ROCm implementation which is only available for GPU.

  Note that currently only right padded data is supported, or the result will be
  polluted by the unmasked data which should be filtered.

  Args:
    inputs: Input tensor of LSTM layer.
    init_h: Initial state tensor for the cell output.
    init_c: Initial state tensor for the cell hidden state.
    kernel: Weights for cell kernel.
    recurrent_kernel: Weights for cell recurrent kernel.
    bias: Weights for cell kernel bias and recurrent bias. Only recurrent bias
      is used in this case.
    mask: Boolean tensor for mask out the steps within sequence. An individual
      `True` entry indicates that the corresponding timestep should be utilized,
      while a `False` entry indicates that the corresponding timestep should be
      ignored.
    time_major: Boolean, whether the inputs are in the format of [time, batch,
      feature] or [batch, time, feature].
    go_backwards: Boolean (default False). If True, process the input sequence
      backwards and return the reversed sequence.
    sequence_lengths: The lengths of all sequences coming from a variable length
      input, such as ragged tensors. If the input has a fixed timestep size,
      this should be None.

  Returns:
    last_output: Output tensor for the last timestep, which has shape
      [batch, units].
    outputs: Output tensor for all timesteps, which has shape
      [batch, time, units].
    state_0: The cell output, which has same shape as init_h.
    state_1: The cell hidden state, which has same shape as init_c.
    runtime: Constant string tensor which indicate real runtime hardware. This
      value is for testing purpose and should not be used by user.
  """
  if not time_major and mask is None:
    inputs = tf.transpose(inputs, perm=(1, 0, 2))
    seq_axis, batch_axis = (0, 1)
  else:
    seq_axis, batch_axis = (0, 1) if time_major else (1, 0)
  # For init_h and init_c, cuDNN expects one more dim of num_layers before or
  # after batch dim for time major or batch major inputs respectively
  init_h = tf.expand_dims(init_h, axis=seq_axis)
  init_c = tf.expand_dims(init_c, axis=seq_axis)

  weights = tf.split(kernel, 4, axis=1)
  weights += tf.split(recurrent_kernel, 4, axis=1)
  # cuDNN has an extra set of bias for inputs, we disable them (setting to 0),
  # so that mathematically it is same as the canonical LSTM implementation.
  full_bias = tf.concat((tf.zeros_like(bias), bias), 0)

  if tf.sysconfig.get_build_info()['is_rocm_build']:
    # ROCm MIOpen's weight sequence for LSTM is different from both canonical
    # and Cudnn format
    # MIOpen: [i, f, o, c] Cudnn/Canonical: [i, f, c, o]
    # i is input gate weights.
    # f is forget gate weights.
    # o is output gate weights.
    # c is cell gate weights.
    weights = [weights[x] for x in (0, 1, 3, 2, 4, 5, 7, 6)]
    # full_bias is a tensor of shape (8*n,)
    full_bias = tf.split(full_bias, 8, axis=0)
    full_bias = [full_bias[x] for x in (0, 1, 3, 2, 4, 5, 7, 6)]

  params = _canonical_to_params(
      weights=weights,
      biases=tf.split(full_bias, 8),
      shape=tf.constant([-1]),
      transpose_weights=True)

  if mask is not None:
    sequence_lengths = calculate_sequence_by_mask(mask, time_major)

  if sequence_lengths is not None:
    if go_backwards:
      # Three reversals are required. E.g.,
      # normal input = [1, 2, 3, 0, 0]  # where 0 need to be masked
      # reversed_input_to_cudnn = [3, 2, 1, 0, 0]
      # output_from_cudnn = [6, 5, 4, 0, 0]
      # expected_output = [0, 0, 6, 5 ,4]
      inputs = tf.reverse_sequence(
          inputs, sequence_lengths, seq_axis=seq_axis, batch_axis=batch_axis)
    outputs, h, c, _, _ = tf.raw_ops.CudnnRNNV3(
        input=inputs,
        input_h=init_h,
        input_c=init_c,
        params=params,
        is_training=True,
        rnn_mode='lstm',
        sequence_lengths=sequence_lengths,
        time_major=time_major)
    if go_backwards:
      outputs = tf.reverse_sequence(
          outputs, sequence_lengths, seq_axis=seq_axis, batch_axis=batch_axis)
      outputs = tf.reverse(outputs, axis=[seq_axis])
  else:
    # # Fill the array with shape [batch] with value of max timesteps.
    # sequence_length = array_ops.fill([array_ops.shape(inputs)[1]],
    #                                  array_ops.shape(inputs)[0])
    if go_backwards:
      # Reverse axis 0 since the input is already convert to time major.
      inputs = tf.reverse(inputs, axis=[0])
    outputs, h, c, _ = tf.raw_ops.CudnnRNN(
        input=inputs, input_h=init_h, input_c=init_c, params=params,
        is_training=True, rnn_mode='lstm')

  last_output = outputs[-1]
  if not time_major and mask is None:
    outputs = tf.transpose(outputs, perm=[1, 0, 2])
  h = tf.squeeze(h, axis=seq_axis)
  c = tf.squeeze(c, axis=seq_axis)

  # In the case of variable length input, the cudnn kernel will fill zeros for
  # the output, whereas the default keras behavior is to bring over the previous
  # output for t-1, so that in the return_sequence=False case, user can quickly
  # get the final effect output instead just 0s at the last timestep.
  # In order to mimic the default keras behavior, we copy the final h state as
  # the last_output, since it is numerically same as the output.
  if mask is not None:
    last_output = h
  return last_output, outputs, h, c, _runtime(_RUNTIME_GPU)


</source>
</class>

<class classid="247" nclones="2" nlines="80" similarity="87">
<source file="systems/keras-2.7.0/keras/layers/recurrent_v2.py" startline="698" endline="826" pcid="4720">
def gru_with_backend_selection(inputs, init_h, kernel, recurrent_kernel, bias,
                               mask, time_major, go_backwards, sequence_lengths,
                               zero_output_for_mask):
  """Call the GRU with optimized backend kernel selection.

  Under the hood, this function will create two TF function, one with the most
  generic kernel and can run on all device condition, and the second one with
  cuDNN specific kernel, which can only run on GPU.

  The first function will be called with normal_lstm_params, while the second
  function is not called, but only registered in the graph. The Grappler will
  do the proper graph rewrite and swap the optimized TF function based on the
  device placement.

  Args:
    inputs: Input tensor of GRU layer.
    init_h: Initial state tensor for the cell output.
    kernel: Weights for cell kernel.
    recurrent_kernel: Weights for cell recurrent kernel.
    bias: Weights for cell kernel bias and recurrent bias. Only recurrent bias
      is used in this case.
    mask: Boolean tensor for mask out the steps within sequence.
      An individual `True` entry indicates that the corresponding timestep
      should be utilized, while a `False` entry indicates that the corresponding
      timestep should be ignored.
    time_major: Boolean, whether the inputs are in the format of
      [time, batch, feature] or [batch, time, feature].
    go_backwards: Boolean (default False). If True, process the input sequence
      backwards and return the reversed sequence.
    sequence_lengths: The lengths of all sequences coming from a variable length
      input, such as ragged tensors. If the input has a fixed timestep size,
      this should be None.
    zero_output_for_mask: Boolean, whether to output zero for masked timestep.

  Returns:
    List of output tensors, same as standard_gru.
  """
  params = {
      'inputs': inputs,
      'init_h': init_h,
      'kernel': kernel,
      'recurrent_kernel': recurrent_kernel,
      'bias': bias,
      'mask': mask,
      'time_major': time_major,
      'go_backwards': go_backwards,
      'sequence_lengths': sequence_lengths,
      'zero_output_for_mask': zero_output_for_mask,
  }

  def gpu_gru_with_fallback(inputs, init_h, kernel, recurrent_kernel, bias,
                            mask, time_major, go_backwards, sequence_lengths,
                            zero_output_for_mask):
    """Use cuDNN kernel when mask is none or strictly right padded."""
    if mask is None:
      return gpu_gru(
          inputs=inputs,
          init_h=init_h,
          kernel=kernel,
          recurrent_kernel=recurrent_kernel,
          bias=bias,
          mask=mask,
          time_major=time_major,
          go_backwards=go_backwards,
          sequence_lengths=sequence_lengths)

    def cudnn_gru_fn():
      return gpu_gru(
          inputs=inputs,
          init_h=init_h,
          kernel=kernel,
          recurrent_kernel=recurrent_kernel,
          bias=bias,
          mask=mask,
          time_major=time_major,
          go_backwards=go_backwards,
          sequence_lengths=sequence_lengths)

    def standard_gru_fn():
      return standard_gru(
          inputs=inputs,
          init_h=init_h,
          kernel=kernel,
          recurrent_kernel=recurrent_kernel,
          bias=bias,
          mask=mask,
          time_major=time_major,
          go_backwards=go_backwards,
          sequence_lengths=sequence_lengths,
          zero_output_for_mask=zero_output_for_mask)

    return tf.cond(
        is_cudnn_supported_inputs(mask, time_major),
        true_fn=cudnn_gru_fn,
        false_fn=standard_gru_fn)

  if _use_new_code():
    # Chooses the implementation dynamically based on the running device.
    (last_output, outputs, new_h,
     runtime) = tf.__internal__.execute_fn_for_device(
         {
             _CPU_DEVICE_NAME: lambda: standard_gru(**params),
             _GPU_DEVICE_NAME: lambda: gpu_gru_with_fallback(**params)
         }, lambda: standard_gru(**params))
  else:
    # Each time a `tf.function` is called, we will give it a unique
    # identifiable API name, so that Grappler won't get confused when it
    # sees multiple GRU layers added into same graph, and it will be able
    # to pair up the different implementations across them.
    api_name = 'gru_' + str(uuid.uuid4())
    supportive_attribute = {
        'time_major': time_major,
        'go_backwards': go_backwards,
    }
    defun_standard_gru = _generate_defun_backend(api_name, _CPU_DEVICE_NAME,
                                                 standard_gru,
                                                 supportive_attribute)
    defun_gpu_gru = _generate_defun_backend(api_name, _GPU_DEVICE_NAME,
                                            gpu_gru_with_fallback,
                                            supportive_attribute)

    # Call the normal GRU impl and register the cuDNN impl function. The
    # grappler will kick in during session execution to optimize the graph.
    last_output, outputs, new_h, runtime = defun_standard_gru(**params)
    _function_register(defun_gpu_gru, **params)

  return last_output, outputs, new_h, runtime


</source>
<source file="systems/keras-2.7.0/keras/layers/recurrent_v2.py" startline="1520" endline="1654" pcid="4733">
def lstm_with_backend_selection(inputs, init_h, init_c, kernel,
                                recurrent_kernel, bias, mask, time_major,
                                go_backwards, sequence_lengths,
                                zero_output_for_mask):
  """Call the LSTM with optimized backend kernel selection.

  Under the hood, this function will create two TF function, one with the most
  generic kernel and can run on all device condition, and the second one with
  cuDNN specific kernel, which can only run on GPU.

  The first function will be called with normal_lstm_params, while the second
  function is not called, but only registered in the graph. The Grappler will
  do the proper graph rewrite and swap the optimized TF function based on the
  device placement.

  Args:
    inputs: Input tensor of LSTM layer.
    init_h: Initial state tensor for the cell output.
    init_c: Initial state tensor for the cell hidden state.
    kernel: Weights for cell kernel.
    recurrent_kernel: Weights for cell recurrent kernel.
    bias: Weights for cell kernel bias and recurrent bias. Only recurrent bias
      is used in this case.
    mask: Boolean tensor for mask out the steps within sequence.
      An individual `True` entry indicates that the corresponding timestep
      should be utilized, while a `False` entry indicates that the corresponding
      timestep should be ignored.
    time_major: Boolean, whether the inputs are in the format of
      [time, batch, feature] or [batch, time, feature].
    go_backwards: Boolean (default False). If True, process the input sequence
      backwards and return the reversed sequence.
    sequence_lengths: The lengths of all sequences coming from a variable length
      input, such as ragged tensors. If the input has a fixed timestep size,
      this should be None.
    zero_output_for_mask: Boolean, whether to output zero for masked timestep.

  Returns:
    List of output tensors, same as standard_lstm.
  """
  params = {
      'inputs': inputs,
      'init_h': init_h,
      'init_c': init_c,
      'kernel': kernel,
      'recurrent_kernel': recurrent_kernel,
      'bias': bias,
      'mask': mask,
      'time_major': time_major,
      'go_backwards': go_backwards,
      'sequence_lengths': sequence_lengths,
      'zero_output_for_mask': zero_output_for_mask,
  }

  def gpu_lstm_with_fallback(inputs, init_h, init_c, kernel, recurrent_kernel,
                             bias, mask, time_major, go_backwards,
                             sequence_lengths, zero_output_for_mask):
    """Use cuDNN kernel when mask is none or strictly right padded."""
    if mask is None:
      return gpu_lstm(
          inputs=inputs,
          init_h=init_h,
          init_c=init_c,
          kernel=kernel,
          recurrent_kernel=recurrent_kernel,
          bias=bias,
          mask=mask,
          time_major=time_major,
          go_backwards=go_backwards,
          sequence_lengths=sequence_lengths)

    def cudnn_lstm_fn():
      return gpu_lstm(
          inputs=inputs,
          init_h=init_h,
          init_c=init_c,
          kernel=kernel,
          recurrent_kernel=recurrent_kernel,
          bias=bias,
          mask=mask,
          time_major=time_major,
          go_backwards=go_backwards,
          sequence_lengths=sequence_lengths)

    def stardard_lstm_fn():
      return standard_lstm(
          inputs=inputs,
          init_h=init_h,
          init_c=init_c,
          kernel=kernel,
          recurrent_kernel=recurrent_kernel,
          bias=bias,
          mask=mask,
          time_major=time_major,
          go_backwards=go_backwards,
          sequence_lengths=sequence_lengths,
          zero_output_for_mask=zero_output_for_mask)

    return tf.cond(
        is_cudnn_supported_inputs(mask, time_major),
        true_fn=cudnn_lstm_fn,
        false_fn=stardard_lstm_fn)

  if _use_new_code():
    # Chooses the implementation dynamically based on the running device.
    (last_output, outputs, new_h, new_c,
     runtime) = tf.__internal__.execute_fn_for_device(
         {
             _CPU_DEVICE_NAME: lambda: standard_lstm(**params),
             _GPU_DEVICE_NAME: lambda: gpu_lstm_with_fallback(**params)
         }, lambda: standard_lstm(**params))
  else:
    # Each time a `tf.function` is called, we will give it a unique
    # identifiable API name, so that Grappler won't get confused when it
    # sees multiple LSTM layers added into same graph, and it will be able
    # to pair up the different implementations across them.
    api_name = 'lstm_' + str(uuid.uuid4())
    supportive_attribute = {
        'time_major': time_major,
        'go_backwards': go_backwards,
    }
    defun_standard_lstm = _generate_defun_backend(api_name, _CPU_DEVICE_NAME,
                                                  standard_lstm,
                                                  supportive_attribute)
    defun_gpu_lstm = _generate_defun_backend(api_name, _GPU_DEVICE_NAME,
                                             gpu_lstm_with_fallback,
                                             supportive_attribute)

    # Call the normal LSTM impl and register the cuDNN impl function. The
    # grappler will kick in during session execution to optimize the graph.
    last_output, outputs, new_h, new_c, runtime = defun_standard_lstm(**params)
    _function_register(defun_gpu_lstm, **params)

  return last_output, outputs, new_h, new_c, runtime


</source>
</class>

<class classid="248" nclones="2" nlines="12" similarity="91">
<source file="systems/keras-2.7.0/keras/layers/recurrent_v2.py" startline="764" endline="775" pcid="4722">
    def cudnn_gru_fn():
      return gpu_gru(
          inputs=inputs,
          init_h=init_h,
          kernel=kernel,
          recurrent_kernel=recurrent_kernel,
          bias=bias,
          mask=mask,
          time_major=time_major,
          go_backwards=go_backwards,
          sequence_lengths=sequence_lengths)

</source>
<source file="systems/keras-2.7.0/keras/layers/recurrent_v2.py" startline="776" endline="788" pcid="4723">
    def standard_gru_fn():
      return standard_gru(
          inputs=inputs,
          init_h=init_h,
          kernel=kernel,
          recurrent_kernel=recurrent_kernel,
          bias=bias,
          mask=mask,
          time_major=time_major,
          go_backwards=go_backwards,
          sequence_lengths=sequence_lengths,
          zero_output_for_mask=zero_output_for_mask)

</source>
</class>

<class classid="249" nclones="2" nlines="13" similarity="92">
<source file="systems/keras-2.7.0/keras/layers/recurrent_v2.py" startline="1590" endline="1602" pcid="4735">
    def cudnn_lstm_fn():
      return gpu_lstm(
          inputs=inputs,
          init_h=init_h,
          init_c=init_c,
          kernel=kernel,
          recurrent_kernel=recurrent_kernel,
          bias=bias,
          mask=mask,
          time_major=time_major,
          go_backwards=go_backwards,
          sequence_lengths=sequence_lengths)

</source>
<source file="systems/keras-2.7.0/keras/layers/recurrent_v2.py" startline="1603" endline="1616" pcid="4736">
    def stardard_lstm_fn():
      return standard_lstm(
          inputs=inputs,
          init_h=init_h,
          init_c=init_c,
          kernel=kernel,
          recurrent_kernel=recurrent_kernel,
          bias=bias,
          mask=mask,
          time_major=time_major,
          go_backwards=go_backwards,
          sequence_lengths=sequence_lengths,
          zero_output_for_mask=zero_output_for_mask)

</source>
</class>

<class classid="250" nclones="3" nlines="15" similarity="100">
<source file="systems/keras-2.7.0/keras/layers/advanced_activations_test.py" startline="91" endline="109" pcid="4760">
  def test_relu_with_invalid_negative_slope(self):
    with self.assertRaisesRegex(
        ValueError, 'negative_slope of a ReLU layer cannot be a negative '
        'value. Received: None'):
      testing_utils.layer_test(
          keras.layers.ReLU,
          kwargs={'negative_slope': None},
          input_shape=(2, 3, 4),
          supports_masking=True)

    with self.assertRaisesRegex(
        ValueError, 'negative_slope of a ReLU layer cannot be a negative '
        'value. Received: -10'):
      testing_utils.layer_test(
          keras.layers.ReLU,
          kwargs={'negative_slope': -10},
          input_shape=(2, 3, 4),
          supports_masking=True)

</source>
<source file="systems/keras-2.7.0/keras/layers/advanced_activations_test.py" startline="110" endline="128" pcid="4761">
  def test_relu_with_invalid_threshold(self):
    with self.assertRaisesRegex(
        ValueError, 'threshold of a ReLU layer cannot be a negative '
        'value. Received: None'):
      testing_utils.layer_test(
          keras.layers.ReLU,
          kwargs={'threshold': None},
          input_shape=(2, 3, 4),
          supports_masking=True)

    with self.assertRaisesRegex(
        ValueError, 'threshold of a ReLU layer cannot be a negative '
        'value. Received: -10'):
      testing_utils.layer_test(
          keras.layers.ReLU,
          kwargs={'threshold': -10},
          input_shape=(2, 3, 4),
          supports_masking=True)

</source>
<source file="systems/keras-2.7.0/keras/layers/advanced_activations_test.py" startline="161" endline="180" pcid="4765">
  def test_threshold_relu_with_invalid_theta(self):
    with self.assertRaisesRegex(
        ValueError, 'Theta of a Thresholded ReLU layer cannot '
        'be None, expecting a float. Received: None'):
      testing_utils.layer_test(
          keras.layers.ThresholdedReLU,
          kwargs={'theta': None},
          input_shape=(2, 3, 4),
          supports_masking=True)

    with self.assertRaisesRegex(
        ValueError, 'The theta value of a Thresholded ReLU '
        'layer should be >=0. Received: -10'):
      testing_utils.layer_test(
          keras.layers.ThresholdedReLU,
          kwargs={'theta': -10},
          input_shape=(2, 3, 4),
          supports_masking=True)


</source>
</class>

<class classid="251" nclones="2" nlines="30" similarity="76">
<source file="systems/keras-2.7.0/keras/layers/merge_test.py" startline="65" endline="99" pcid="4767">
  def test_merge_subtract(self):
    i1 = keras.layers.Input(shape=(4, 5))
    i2 = keras.layers.Input(shape=(4, 5))
    i3 = keras.layers.Input(shape=(4, 5))

    subtract_layer = keras.layers.Subtract()
    o = subtract_layer([i1, i2])
    self.assertListEqual(o.shape.as_list(), [None, 4, 5])
    model = keras.models.Model([i1, i2], o)
    model.run_eagerly = testing_utils.should_run_eagerly()

    x1 = np.random.random((2, 4, 5))
    x2 = np.random.random((2, 4, 5))
    out = model.predict([x1, x2])
    self.assertEqual(out.shape, (2, 4, 5))
    self.assertAllClose(out, x1 - x2, atol=1e-4)

    self.assertEqual(subtract_layer.compute_mask([i1, i2], [None, None]), None)
    self.assertTrue(
        np.all(
            backend.eval(
                subtract_layer.compute_mask(
                    [i1, i2], [backend.variable(x1), backend.variable(x2)]))))

    with self.assertRaisesRegex(ValueError, '`mask` should be a list.'):
      subtract_layer.compute_mask([i1, i2], x1)
    with self.assertRaisesRegex(ValueError, '`inputs` should be a list.'):
      subtract_layer.compute_mask(i1, [None, None])
    with self.assertRaisesRegex(ValueError,
                                'layer should be called on exactly 2 inputs'):
      subtract_layer([i1, i2, i3])
    with self.assertRaisesRegex(ValueError,
                                'layer should be called on exactly 2 inputs'):
      subtract_layer([i1])

</source>
<source file="systems/keras-2.7.0/keras/layers/merge_test.py" startline="158" endline="193" pcid="4772">
  def test_merge_concatenate(self):
    i1 = keras.layers.Input(shape=(4, 5))
    i2 = keras.layers.Input(shape=(4, 5))
    concat_layer = keras.layers.Concatenate(axis=1)
    o = concat_layer([i1, i2])
    self.assertListEqual(o.shape.as_list(), [None, 8, 5])
    model = keras.models.Model([i1, i2], o)
    model.run_eagerly = testing_utils.should_run_eagerly()

    x1 = np.random.random((2, 4, 5))
    x2 = np.random.random((2, 4, 5))
    out = model.predict([x1, x2])
    self.assertEqual(out.shape, (2, 8, 5))
    self.assertAllClose(out, np.concatenate([x1, x2], axis=1), atol=1e-4)

    self.assertEqual(concat_layer.compute_mask([i1, i2], [None, None]), None)
    self.assertTrue(
        np.all(
            backend.eval(
                concat_layer.compute_mask(
                    [i1, i2], [backend.variable(x1), backend.variable(x2)]))))

    # Should work with unit-length input.
    unit_length_o = concat_layer([i1])
    self.assertListEqual(unit_length_o.shape.as_list(), i1.shape.as_list())

    with self.assertRaisesRegex(ValueError, '`mask` should be a list.'):
      concat_layer.compute_mask([i1, i2], x1)
    with self.assertRaisesRegex(ValueError, '`inputs` should be a list.'):
      concat_layer.compute_mask(i1, [None, None])
    with self.assertRaisesRegex(ValueError, 'should have the same length'):
      concat_layer.compute_mask([i1, i2], [None])
    with self.assertRaisesRegex(ValueError,
                                'layer should be called on a list of inputs'):
      concat_layer(i1)

</source>
</class>

<class classid="252" nclones="3" nlines="12" similarity="91">
<source file="systems/keras-2.7.0/keras/layers/merge_test.py" startline="116" endline="129" pcid="4769">
  def test_merge_average(self):
    i1 = keras.layers.Input(shape=(4, 5))
    i2 = keras.layers.Input(shape=(4, 5))
    o = keras.layers.average([i1, i2])
    self.assertListEqual(o.shape.as_list(), [None, 4, 5])
    model = keras.models.Model([i1, i2], o)
    model.run_eagerly = testing_utils.should_run_eagerly()

    x1 = np.random.random((2, 4, 5))
    x2 = np.random.random((2, 4, 5))
    out = model.predict([x1, x2])
    self.assertEqual(out.shape, (2, 4, 5))
    self.assertAllClose(out, 0.5 * (x1 + x2), atol=1e-4)

</source>
<source file="systems/keras-2.7.0/keras/layers/merge_test.py" startline="144" endline="157" pcid="4771">
  def test_merge_minimum(self):
    i1 = keras.layers.Input(shape=(4, 5))
    i2 = keras.layers.Input(shape=(4, 5))
    o = keras.layers.minimum([i1, i2])
    self.assertListEqual(o.shape.as_list(), [None, 4, 5])
    model = keras.models.Model([i1, i2], o)
    model.run_eagerly = testing_utils.should_run_eagerly()

    x1 = np.random.random((2, 4, 5))
    x2 = np.random.random((2, 4, 5))
    out = model.predict([x1, x2])
    self.assertEqual(out.shape, (2, 4, 5))
    self.assertAllClose(out, np.minimum(x1, x2), atol=1e-4)

</source>
<source file="systems/keras-2.7.0/keras/layers/merge_test.py" startline="130" endline="143" pcid="4770">
  def test_merge_maximum(self):
    i1 = keras.layers.Input(shape=(4, 5))
    i2 = keras.layers.Input(shape=(4, 5))
    o = keras.layers.maximum([i1, i2])
    self.assertListEqual(o.shape.as_list(), [None, 4, 5])
    model = keras.models.Model([i1, i2], o)
    model.run_eagerly = testing_utils.should_run_eagerly()

    x1 = np.random.random((2, 4, 5))
    x2 = np.random.random((2, 4, 5))
    out = model.predict([x1, x2])
    self.assertEqual(out.shape, (2, 4, 5))
    self.assertAllClose(out, np.maximum(x1, x2), atol=1e-4)

</source>
</class>

<class classid="253" nclones="3" nlines="62" similarity="79">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/normalization_test.py" startline="32" endline="112" pcid="4808">
def _get_layer_computation_test_cases():
  test_cases = ({
      "adapt_data": np.array([[1.], [2.], [3.], [4.], [5.]], dtype=np.float32),
      "axis": -1,
      "test_data": np.array([[1.], [2.], [3.]], np.float32),
      "expected": np.array([[-1.414214], [-.707107], [0]], np.float32),
      "testcase_name": "2d_single_element"
  }, {
      "adapt_data": np.array([[1], [2], [3], [4], [5]], dtype=np.int32),
      "axis": -1,
      "test_data": np.array([[1], [2], [3]], np.int32),
      "expected": np.array([[-1.414214], [-.707107], [0]], np.float32),
      "testcase_name": "2d_int_data"
  }, {
      "adapt_data": np.array([[1.], [2.], [3.], [4.], [5.]], dtype=np.float32),
      "axis": None,
      "test_data": np.array([[1.], [2.], [3.]], np.float32),
      "expected": np.array([[-1.414214], [-.707107], [0]], np.float32),
      "testcase_name": "2d_single_element_none_axis"
  }, {
      "adapt_data": np.array([[1., 2., 3., 4., 5.]], dtype=np.float32),
      "axis": None,
      "test_data": np.array([[1.], [2.], [3.]], np.float32),
      "expected": np.array([[-1.414214], [-.707107], [0]], np.float32),
      "testcase_name": "2d_single_element_none_axis_flat_data"
  }, {
      "adapt_data":
          np.array([[[1., 2., 3.], [2., 3., 4.]], [[3., 4., 5.], [4., 5., 6.]]],
                   np.float32),
      "axis":
          1,
      "test_data":
          np.array([[[1., 2., 3.], [2., 3., 4.]], [[3., 4., 5.], [4., 5., 6.]]],
                   np.float32),
      "expected":
          np.array([[[-1.549193, -0.774597, 0.], [-1.549193, -0.774597, 0.]],
                    [[0., 0.774597, 1.549193], [0., 0.774597, 1.549193]]],
                   np.float32),
      "testcase_name":
          "3d_internal_axis"
  }, {
      "adapt_data":
          np.array(
              [[[1., 0., 3.], [2., 3., 4.]], [[3., -1., 5.], [4., 5., 8.]]],
              np.float32),
      "axis": (1, 2),
      "test_data":
          np.array(
              [[[3., 1., -1.], [2., 5., 4.]], [[3., 0., 5.], [2., 5., 8.]]],
              np.float32),
      "expected":
          np.array(
              [[[1., 3., -5.], [-1., 1., -1.]], [[1., 1., 1.], [-1., 1., 1.]]],
              np.float32),
      "testcase_name":
          "3d_multiple_axis"
  }, {
      "adapt_data":
          np.zeros((3, 4)),
      "axis": -1,
      "test_data":
          np.zeros((3, 4)),
      "expected":
          np.zeros((3, 4)),
      "testcase_name":
          "zero_variance"
  })

  crossed_test_cases = []
  # Cross above test cases with use_dataset in (True, False)
  for use_dataset in (True, False):
    for case in test_cases:
      case = case.copy()
      if use_dataset:
        case["testcase_name"] = case["testcase_name"] + "_with_dataset"
      case["use_dataset"] = use_dataset
      crossed_test_cases.append(case)

  return crossed_test_cases


</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/normalization_distribution_test.py" startline="28" endline="92" pcid="5232">
def _get_layer_computation_test_cases():
  test_cases = ({
      "adapt_data": np.array([[1.], [2.], [3.], [4.], [5.]], dtype=np.float32),
      "axis": -1,
      "test_data": np.array([[1.], [2.], [3.]], np.float32),
      "expected": np.array([[-1.414214], [-.707107], [0]], np.float32),
      "testcase_name": "2d_single_element"
  }, {
      "adapt_data": np.array([[1.], [2.], [3.], [4.], [5.]], dtype=np.float32),
      "axis": None,
      "test_data": np.array([[1.], [2.], [3.]], np.float32),
      "expected": np.array([[-1.414214], [-.707107], [0]], np.float32),
      "testcase_name": "2d_single_element_none_axis"
  }, {
      "adapt_data": np.array([[1., 2., 3., 4., 5.]], dtype=np.float32),
      "axis": None,
      "test_data": np.array([[1.], [2.], [3.]], np.float32),
      "expected": np.array([[-1.414214], [-.707107], [0]], np.float32),
      "testcase_name": "2d_single_element_none_axis_flat_data"
  }, {
      "adapt_data":
          np.array([[[1., 2., 3.], [2., 3., 4.]], [[3., 4., 5.], [4., 5., 6.]]],
                   np.float32),
      "axis":
          1,
      "test_data":
          np.array([[[1., 2., 3.], [2., 3., 4.]], [[3., 4., 5.], [4., 5., 6.]]],
                   np.float32),
      "expected":
          np.array([[[-1.549193, -0.774597, 0.], [-1.549193, -0.774597, 0.]],
                    [[0., 0.774597, 1.549193], [0., 0.774597, 1.549193]]],
                   np.float32),
      "testcase_name":
          "3d_internal_axis"
  }, {
      "adapt_data":
          np.array(
              [[[1., 0., 3.], [2., 3., 4.]], [[3., -1., 5.], [4., 5., 8.]]],
              np.float32),
      "axis": (1, 2),
      "test_data":
          np.array(
              [[[3., 1., -1.], [2., 5., 4.]], [[3., 0., 5.], [2., 5., 8.]]],
              np.float32),
      "expected":
          np.array(
              [[[1., 3., -5.], [-1., 1., -1.]], [[1., 1., 1.], [-1., 1., 1.]]],
              np.float32),
      "testcase_name":
          "3d_multiple_axis"
  })

  crossed_test_cases = []
  # Cross above test cases with use_dataset in (True, False)
  for use_dataset in (True, False):
    for case in test_cases:
      case = case.copy()
      if use_dataset:
        case["testcase_name"] = case["testcase_name"] + "_with_dataset"
      case["use_dataset"] = use_dataset
      crossed_test_cases.append(case)

  return crossed_test_cases


</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/normalization_tpu_test.py" startline="30" endline="94" pcid="5041">
def _get_layer_computation_test_cases():
  test_cases = ({
      "adapt_data": np.array([[1.], [2.], [3.], [4.], [5.]], dtype=np.float32),
      "axis": -1,
      "test_data": np.array([[1.], [2.], [3.]], np.float32),
      "expected": np.array([[-1.414214], [-.707107], [0]], np.float32),
      "testcase_name": "2d_single_element"
  }, {
      "adapt_data": np.array([[1.], [2.], [3.], [4.], [5.]], dtype=np.float32),
      "axis": None,
      "test_data": np.array([[1.], [2.], [3.]], np.float32),
      "expected": np.array([[-1.414214], [-.707107], [0]], np.float32),
      "testcase_name": "2d_single_element_none_axis"
  }, {
      "adapt_data": np.array([[1., 2., 3., 4., 5.]], dtype=np.float32),
      "axis": None,
      "test_data": np.array([[1.], [2.], [3.]], np.float32),
      "expected": np.array([[-1.414214], [-.707107], [0]], np.float32),
      "testcase_name": "2d_single_element_none_axis_flat_data"
  }, {
      "adapt_data":
          np.array([[[1., 2., 3.], [2., 3., 4.]], [[3., 4., 5.], [4., 5., 6.]]],
                   np.float32),
      "axis":
          1,
      "test_data":
          np.array([[[1., 2., 3.], [2., 3., 4.]], [[3., 4., 5.], [4., 5., 6.]]],
                   np.float32),
      "expected":
          np.array([[[-1.549193, -0.774597, 0.], [-1.549193, -0.774597, 0.]],
                    [[0., 0.774597, 1.549193], [0., 0.774597, 1.549193]]],
                   np.float32),
      "testcase_name":
          "3d_internal_axis"
  }, {
      "adapt_data":
          np.array(
              [[[1., 0., 3.], [2., 3., 4.]], [[3., -1., 5.], [4., 5., 8.]]],
              np.float32),
      "axis": (1, 2),
      "test_data":
          np.array(
              [[[3., 1., -1.], [2., 5., 4.]], [[3., 0., 5.], [2., 5., 8.]]],
              np.float32),
      "expected":
          np.array(
              [[[1., 3., -5.], [-1., 1., -1.]], [[1., 1., 1.], [-1., 1., 1.]]],
              np.float32),
      "testcase_name":
          "3d_multiple_axis"
  })

  crossed_test_cases = []
  # Cross above test cases with use_dataset in (True, False)
  for use_dataset in (True, False):
    for case in test_cases:
      case = case.copy()
      if use_dataset:
        case["testcase_name"] = case["testcase_name"] + "_with_dataset"
      case["use_dataset"] = use_dataset
      crossed_test_cases.append(case)

  return crossed_test_cases


</source>
</class>

<class classid="254" nclones="4" nlines="16" similarity="70">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/normalization_test.py" startline="205" endline="224" pcid="4819">
  def test_layer_computation(self, adapt_data, axis, test_data, use_dataset,
                             expected):
    input_shape = tuple([test_data.shape[i] for i in range(1, test_data.ndim)])
    if use_dataset:
      # Keras APIs expect batched datasets
      adapt_data = tf.data.Dataset.from_tensor_slices(adapt_data).batch(
          test_data.shape[0] // 2)
      test_data = tf.data.Dataset.from_tensor_slices(test_data).batch(
          test_data.shape[0] // 2)

    layer = normalization.Normalization(axis=axis)
    layer.adapt(adapt_data)

    input_data = keras.Input(shape=input_shape)
    output = layer(input_data)
    model = keras.Model(input_data, output)
    model._run_eagerly = testing_utils.should_run_eagerly()
    output_data = model.predict(test_data)
    self.assertAllClose(expected, output_data)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/normalization_distribution_test.py" startline="102" endline="121" pcid="5233">
  def test_layer_computation(self, strategy, adapt_data, axis, test_data,
                             use_dataset, expected):
    input_shape = tuple([None for _ in range(test_data.ndim - 1)])
    if use_dataset:
      # Keras APIs expect batched datasets
      adapt_data = tf.data.Dataset.from_tensor_slices(adapt_data).batch(
          test_data.shape[0] // 2)
      test_data = tf.data.Dataset.from_tensor_slices(test_data).batch(
          test_data.shape[0] // 2)

    with strategy.scope():
      input_data = keras.Input(shape=input_shape)
      layer = normalization.Normalization(axis=axis)
      layer.adapt(adapt_data)
      output = layer(input_data)
      model = keras.Model(input_data, output)
      output_data = model.predict(test_data)
    self.assertAllClose(expected, output_data)


</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/normalization_tpu_test.py" startline="101" endline="122" pcid="5042">
  def test_layer_computation(self, adapt_data, axis, test_data, use_dataset,
                             expected):
    input_shape = tuple([None for _ in range(test_data.ndim - 1)])
    if use_dataset:
      # Keras APIs expect batched datasets
      adapt_data = tf.data.Dataset.from_tensor_slices(adapt_data).batch(
          test_data.shape[0] // 2)
      test_data = tf.data.Dataset.from_tensor_slices(test_data).batch(
          test_data.shape[0] // 2)

    strategy = tpu_strategy_test_utils.get_tpu_strategy()

    with strategy.scope():
      input_data = keras.Input(shape=input_shape)
      layer = normalization.Normalization(axis=axis)
      layer.adapt(adapt_data)
      output = layer(input_data)
      model = keras.Model(input_data, output)
      output_data = model.predict(test_data)
    self.assertAllClose(expected, output_data)


</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/discretization_test.py" startline="205" endline="226" pcid="4982">
  def test_layer_computation(self, adapt_data, test_data, use_dataset,
                             expected, num_bins=5, epsilon=0.01):

    input_shape = tuple(list(test_data.shape)[1:])
    np.random.shuffle(adapt_data)
    if use_dataset:
      # Keras APIs expect batched datasets
      adapt_data = tf.data.Dataset.from_tensor_slices(adapt_data).batch(
          test_data.shape[0] // 2)
      test_data = tf.data.Dataset.from_tensor_slices(test_data).batch(
          test_data.shape[0] // 2)

    layer = discretization.Discretization(epsilon=epsilon, num_bins=num_bins)
    layer.adapt(adapt_data)

    input_data = keras.Input(shape=input_shape)
    output = layer(input_data)
    model = keras.Model(input_data, output)
    model._run_eagerly = testing_utils.should_run_eagerly()
    output_data = model.predict(test_data)
    self.assertAllClose(expected, output_data)

</source>
</class>

<class classid="255" nclones="2" nlines="17" similarity="76">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/normalization_test.py" startline="274" endline="297" pcid="4824">
  def test_multiple_adapts(self):
    first_adapt = [[0], [2], [0], [2]]
    second_adapt = [[2], [4], [2], [4]]
    predict_input = [[2], [2]]
    expected_first_output = [[1], [1]]
    expected_second_output = [[-1], [-1]]

    inputs = keras.Input(shape=(1,), dtype=tf.int32)
    layer = normalization.Normalization(axis=-1)
    layer.adapt(first_adapt)
    outputs = layer(inputs)
    model = keras.Model(inputs=inputs, outputs=outputs)

    actual_output = model.predict(predict_input)
    self.assertAllClose(actual_output, expected_first_output)

    # Re-adapt the layer on new inputs.
    layer.adapt(second_adapt)
    # Re-compile the model.
    model.compile()
    # `predict` should now use the new model state.
    actual_output = model.predict(predict_input)
    self.assertAllClose(actual_output, expected_second_output)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/discretization_test.py" startline="227" endline="250" pcid="4983">
  def test_multiple_adapts(self):
    first_adapt = [[1], [2], [3]]
    second_adapt = [[4], [5], [6]]
    predict_input = [[2], [2]]
    expected_first_output = [[2], [2]]
    expected_second_output = [[0], [0]]

    inputs = keras.Input(shape=(1,), dtype=tf.int32)
    layer = discretization.Discretization(num_bins=3)
    layer.adapt(first_adapt)
    outputs = layer(inputs)
    model = keras.Model(inputs=inputs, outputs=outputs)

    actual_output = model.predict(predict_input)
    self.assertAllClose(actual_output, expected_first_output)

    # Re-adapt the layer on new inputs.
    layer.adapt(second_adapt)
    # Re-compile the model.
    model.compile()
    # `predict` should now use the new model state.
    actual_output = model.predict(predict_input)
    self.assertAllClose(actual_output, expected_second_output)

</source>
</class>

<class classid="256" nclones="2" nlines="21" similarity="90">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/normalization_test.py" startline="335" endline="364" pcid="4826">
  def test_saved_model_keras(self, adapted):
    input_data = [[0.], [2.], [0.], [2.]]
    expected_output = [[-1.], [1.], [-1.], [1.]]

    cls = normalization.Normalization
    inputs = keras.Input(shape=(1,), dtype=tf.float32)
    if adapted:
      layer = cls(axis=-1)
      layer.adapt(input_data)
    else:
      layer = cls(mean=1., variance=1.)
    outputs = layer(inputs)
    model = keras.Model(inputs=inputs, outputs=outputs)

    output_data = model.predict(input_data)
    self.assertAllClose(output_data, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model")
    model.save(output_path, save_format="tf")
    loaded_model = keras.models.load_model(
        output_path, custom_objects={"Normalization": cls})

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_data = loaded_model.predict(input_data)
    self.assertAllClose(new_output_data, expected_output)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/normalization_test.py" startline="369" endline="397" pcid="4827">
  def test_saved_weights_keras(self, adapted):
    input_data = [[0.], [2.], [0.], [2.]]
    expected_output = [[-1.], [1.], [-1.], [1.]]

    cls = normalization.Normalization
    inputs = keras.Input(shape=(1,), dtype=tf.float32)
    if adapted:
      layer = cls(axis=-1)
      layer.adapt(input_data)
    else:
      layer = cls(mean=1., variance=1.)
    outputs = layer(inputs)
    model = keras.Model(inputs=inputs, outputs=outputs)

    output_data = model.predict(input_data)
    self.assertAllClose(output_data, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_weights")
    model.save_weights(output_path, save_format="tf")
    new_model = keras.Model.from_config(
        model.get_config(), custom_objects={"Normalization": cls})
    new_model.load_weights(output_path)

    # Validate correctness of the new model.
    new_output_data = new_model.predict(input_data)
    self.assertAllClose(new_output_data, expected_output)


</source>
</class>

<class classid="257" nclones="4" nlines="22" similarity="70">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/integer_lookup_test.py" startline="76" endline="111" pcid="4841">
  def test_layer_end_to_end_with_adapt(self, vocab_data, input_data, kwargs,
                                       use_dataset, expected_output,
                                       input_dtype):
    cls = integer_lookup.IntegerLookup
    expected_output_dtype = tf.int64
    input_shape = input_data.shape

    if use_dataset:
      # Keras APIs expect batched datasets.
      # TODO(rachelim): `model.predict` predicts the result on each
      # dataset batch separately, then tries to concatenate the results
      # together. When the results have different shapes on the non-concat
      # axis (which can happen in the output_mode = INT case for
      # IntegerLookup), the concatenation fails. In real use cases, this may
      # not be an issue because users are likely to pipe the preprocessing layer
      # into other keras layers instead of predicting it directly. A workaround
      # for these unit tests is to have the dataset only contain one batch, so
      # no concatenation needs to happen with the result. For consistency with
      # numpy input, we should make `predict` join differently shaped results
      # together sensibly, with 0 padding.
      input_data = tf.data.Dataset.from_tensor_slices(input_data).batch(
          input_shape[0])
      vocab_data = tf.data.Dataset.from_tensor_slices(vocab_data).batch(
          input_shape[0])

    output_data = testing_utils.layer_test(
        cls,
        kwargs=kwargs,
        input_shape=input_shape,
        input_data=input_data,
        input_dtype=input_dtype,
        expected_output_dtype=expected_output_dtype,
        validate_training=False,
        adapt_data=vocab_data)
    self.assertAllClose(expected_output, output_data)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/string_lookup_test.py" startline="70" endline="106" pcid="5044">
  def test_layer_end_to_end_with_adapt(self, vocab_data, input_data, kwargs,
                                       use_dataset, expected_output,
                                       input_dtype):
    cls = string_lookup.StringLookup
    expected_output_dtype = tf.int64
    input_shape = input_data.shape

    if use_dataset:
      # Keras APIs expect batched datasets.
      # TODO(rachelim): `model.predict` predicts the result on each
      # dataset batch separately, then tries to concatenate the results
      # together. When the results have different shapes on the non-concat
      # axis (which can happen in the output_mode = INT case for
      # StringLookup), the concatenation fails. In real use cases, this may
      # not be an issue because users are likely to pipe the preprocessing layer
      # into other keras layers instead of predicting it directly. A workaround
      # for these unit tests is to have the dataset only contain one batch, so
      # no concatenation needs to happen with the result. For consistency with
      # numpy input, we should make `predict` join differently shaped results
      # together sensibly, with 0 padding.
      input_data = tf.data.Dataset.from_tensor_slices(input_data).batch(
          input_shape[0])
      vocab_data = tf.data.Dataset.from_tensor_slices(vocab_data).batch(
          input_shape[0])

    output_data = testing_utils.layer_test(
        cls,
        kwargs=kwargs,
        input_shape=input_shape,
        input_data=input_data,
        input_dtype=input_dtype,
        expected_output_dtype=expected_output_dtype,
        validate_training=False,
        adapt_data=vocab_data)
    self.assertAllClose(expected_output, output_data)


</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="303" endline="349" pcid="5370">
  def test_layer_end_to_end_with_adapt(self, vocab_data, input_data, kwargs,
                                       use_dataset, expected_output,
                                       input_dtype):
    cls = index_lookup.IndexLookup
    if "invert" in kwargs and kwargs["invert"]:
      expected_output_dtype = kwargs["dtype"]
    elif "output_mode" in kwargs and kwargs["output_mode"] != index_lookup.INT:
      expected_output_dtype = tf.float32
    else:
      expected_output_dtype = tf.int64

    input_shape = input_data.shape

    if use_dataset:
      # Keras APIs expect batched datasets.
      # TODO(rachelim): `model.predict` predicts the result on each
      # dataset batch separately, then tries to concatenate the results
      # together. When the results have different shapes on the non-concat
      # axis (which can happen in the output_mode = INT case for
      # IndexLookup), the concatenation fails. In real use cases, this may
      # not be an issue because users are likely to pipe the preprocessing layer
      # into other keras layers instead of predicting it directly. A workaround
      # for these unit tests is to have the dataset only contain one batch, so
      # no concatenation needs to happen with the result. For consistency with
      # numpy input, we should make `predict` join differently shaped results
      # together sensibly, with 0 padding.
      input_data = tf.data.Dataset.from_tensor_slices(input_data).batch(
          input_shape[0])
      vocab_data = tf.data.Dataset.from_tensor_slices(vocab_data).batch(
          input_shape[0])

    with CustomObjectScope({"IndexLookup": cls}):
      output_data = testing_utils.layer_test(
          cls,
          kwargs=kwargs,
          input_shape=input_shape,
          input_data=input_data,
          input_dtype=input_dtype,
          expected_output_dtype=expected_output_dtype,
          validate_training=False,
          adapt_data=vocab_data)
    if "invert" in kwargs and kwargs["invert"]:
      self.assertAllEqual(expected_output, output_data)
    else:
      self.assertAllClose(expected_output, output_data)


</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="275" endline="312" pcid="4888">
  def test_layer_end_to_end_with_adapt(self, vocab_data, input_data, kwargs,
                                       use_dataset, expected_output):
    cls = text_vectorization.TextVectorization
    if kwargs.get("output_mode") == text_vectorization.INT:
      expected_output_dtype = tf.int64
    else:
      expected_output_dtype = tf.float32
    input_shape = input_data.shape

    if use_dataset:
      # Keras APIs expect batched datasets.
      # TODO(rachelim): `model.predict` predicts the result on each
      # dataset batch separately, then tries to concatenate the results
      # together. When the results have different shapes on the non-concat
      # axis (which can happen in the output_mode = INT case for
      # TextVectorization), the concatenation fails. In real use cases, this may
      # not be an issue because users are likely to pipe the preprocessing layer
      # into other keras layers instead of predicting it directly. A workaround
      # for these unit tests is to have the dataset only contain one batch, so
      # no concatenation needs to happen with the result. For consistency with
      # numpy input, we should make `predict` join differently shaped results
      # together sensibly, with 0 padding.
      input_data = tf.data.Dataset.from_tensor_slices(input_data).batch(
          input_shape[0])
      vocab_data = tf.data.Dataset.from_tensor_slices(vocab_data).batch(
          input_shape[0])

    output_data = testing_utils.layer_test(
        cls,
        kwargs=kwargs,
        input_shape=input_shape,
        input_data=input_data,
        input_dtype=tf.string,
        expected_output_dtype=expected_output_dtype,
        validate_training=False,
        adapt_data=vocab_data)
    self.assertAllClose(expected_output, output_data)

</source>
</class>

<class classid="258" nclones="7" nlines="21" similarity="73">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/integer_lookup_test.py" startline="126" endline="146" pcid="4843">
  def test_sparse_int_input(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.SparseTensor(
        indices=[[0, 0], [1, 2]],
        values=np.array([13, 32], dtype=np.int64),
        dense_shape=[3, 4])

    expected_indices = [[0, 0], [1, 2]]
    expected_values = [4, 0]
    expected_dense_shape = [3, 4]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, sparse=True)
    layer = integer_lookup.IntegerLookup(max_tokens=None)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array, steps=1)
    self.assertAllEqual(expected_indices, output_data.indices)
    self.assertAllEqual(expected_values, output_data.values)
    self.assertAllEqual(expected_dense_shape, output_data.dense_shape)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/integer_lookup_test.py" startline="167" endline="192" pcid="4845">
  def test_sparse_int_input_multi_bucket(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.SparseTensor(
        indices=[[0, 0], [1, 2]],
        values=np.array([13, 133], dtype=np.int64),
        dense_shape=[3, 4])

    expected_indices = [[0, 0], [1, 2]]
    expected_values = [6, 2]
    expected_dense_shape = [3, 4]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, sparse=True)
    layer = integer_lookup.IntegerLookup(
        max_tokens=None,
        dtype=tf.int64,
        num_oov_indices=2,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array, steps=1)
    self.assertAllEqual(expected_indices, output_data.indices)
    self.assertAllEqual(expected_values, output_data.values)
    self.assertAllEqual(expected_dense_shape, output_data.dense_shape)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="494" endline="519" pcid="5377">
  def test_sparse_int_input_multi_bucket(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.SparseTensor(
        indices=[[0, 0], [1, 2]],
        values=np.array([13, 133], dtype=np.int64),
        dense_shape=[3, 4])

    expected_indices = [[0, 0], [1, 2]]
    expected_values = [6, 2]
    expected_dense_shape = [3, 4]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, sparse=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        dtype=tf.int64,
        num_oov_indices=2,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array, steps=1)
    self.assertAllEqual(expected_indices, output_data.indices)
    self.assertAllEqual(expected_values, output_data.values)
    self.assertAllEqual(expected_dense_shape, output_data.dense_shape)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="381" endline="406" pcid="5372">
  def test_sparse_int_input(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.SparseTensor(
        indices=[[0, 0], [1, 2]],
        values=np.array([13, 32], dtype=np.int64),
        dense_shape=[3, 4])

    expected_indices = [[0, 0], [1, 2]]
    expected_values = [5, 1]
    expected_dense_shape = [3, 4]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, sparse=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        dtype=tf.int64,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array, steps=1)
    self.assertAllEqual(expected_indices, output_data.indices)
    self.assertAllEqual(expected_values, output_data.values)
    self.assertAllEqual(expected_dense_shape, output_data.dense_shape)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="597" endline="622" pcid="5382">
  def test_sparse_int_input(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.SparseTensor(
        indices=[[0, 0], [1, 2]],
        values=np.array([13, 32], dtype=np.int64),
        dense_shape=[3, 4])

    expected_indices = [[0, 0], [1, 2]]
    expected_values = [5, 1]
    expected_dense_shape = [3, 4]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, sparse=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        dtype=tf.int64,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array, steps=1)
    self.assertAllEqual(expected_indices, output_data.indices)
    self.assertAllEqual(expected_values, output_data.values)
    self.assertAllEqual(expected_dense_shape, output_data.dense_shape)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="355" endline="380" pcid="5371">
  def test_sparse_string_input(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = tf.SparseTensor(
        indices=[[0, 0], [1, 2]],
        values=["fire", "michigan"],
        dense_shape=[3, 4])

    expected_indices = [[0, 0], [1, 2]]
    expected_values = [5, 1]
    expected_dense_shape = [3, 4]

    input_data = keras.Input(shape=(None,), dtype=tf.string, sparse=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array, steps=1)
    self.assertAllEqual(expected_indices, output_data.indices)
    self.assertAllEqual(expected_values, output_data.values)
    self.assertAllEqual(expected_dense_shape, output_data.dense_shape)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="470" endline="493" pcid="5376">
  def test_sparse_string_input_multi_bucket(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = tf.SparseTensor(
        indices=[[0, 0], [1, 2]], values=["fire", "ohio"], dense_shape=[3, 4])

    expected_indices = [[0, 0], [1, 2]]
    expected_values = [6, 2]
    expected_dense_shape = [3, 4]

    input_data = keras.Input(shape=(None,), dtype=tf.string, sparse=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=2,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array, steps=1)
    self.assertAllEqual(expected_indices, output_data.indices)
    self.assertAllEqual(expected_values, output_data.values)
    self.assertAllEqual(expected_dense_shape, output_data.dense_shape)

</source>
</class>

<class classid="259" nclones="2" nlines="12" similarity="91">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/integer_lookup_test.py" startline="147" endline="161" pcid="4844">
  def test_ragged_int_input(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.ragged.constant([[10, 11, 13], [13, 12, 10, 42]],
                                     dtype=np.int64)
    expected_output = [[1, 2, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, ragged=True)
    layer = integer_lookup.IntegerLookup(max_tokens=None)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)


</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/integer_lookup_test.py" startline="193" endline="207" pcid="4846">
  def test_ragged_int_input_multi_bucket(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.ragged.constant([[10, 11, 13], [13, 12, 10, 133]],
                                     dtype=np.int64)
    expected_output = [[2, 3, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, ragged=True)
    layer = integer_lookup.IntegerLookup(max_tokens=None, num_oov_indices=2)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)


</source>
</class>

<class classid="260" nclones="21" nlines="11" similarity="71">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/integer_lookup_test.py" startline="253" endline="265" pcid="4851">
  def test_int_output(self):
    vocab_data = [42, 1138, 725, 1729]
    input_array = np.array([[42, 1138, 725, 1729], [1729, 725, 42, 203]])
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = integer_lookup.IntegerLookup()
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/integer_lookup_test.py" startline="486" endline="499" pcid="4869">
  def test_int_output_inverted_vocab_from_file(self):
    vocab_list = [42, 1138, 725, 1729]
    vocab_path = self._write_to_temp_file("vocab_file", vocab_list)

    input_array = np.array([[1, 2, 3, 4], [4, 3, 1, 0]])
    expected_output = [[42, 1138, 725, 1729], [1729, 725, 42, -1]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = integer_lookup.IntegerLookup(vocabulary=vocab_path, invert=True)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/integer_lookup_test.py" startline="361" endline="377" pcid="4859">
  def test_forward_backward_adapted_vocab(self):
    adapt_data = [42, 1138, 725, 1729]
    input_array = np.array([[42, 1138, 725, 1729], [1729, 725, 42, 203]])
    expected_output = np.array([[42, 1138, 725, 1729], [1729, 725, 42, -1]])

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = integer_lookup.IntegerLookup()
    layer.adapt(adapt_data)
    inverse_layer = integer_lookup.IntegerLookup(
        vocabulary=layer.get_vocabulary(), invert=True)
    int_data = layer(input_data)
    inverse_data = inverse_layer(int_data)
    model = keras.Model(inputs=input_data, outputs=inverse_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)


</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/integer_lookup_test.py" startline="472" endline="485" pcid="4868">
  def test_int_output_explicit_vocab_from_file(self):
    vocab_list = [42, 1138, 725, 1729]
    vocab_path = self._write_to_temp_file("vocab_file", vocab_list)

    input_array = np.array([[42, 1138, 725, 1729], [1729, 725, 42, 203]])
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = integer_lookup.IntegerLookup(vocabulary=vocab_path)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/integer_lookup_test.py" startline="272" endline="284" pcid="4853">
  def test_int_output_with_mask(self):
    vocab_data = [42, 1138, 725, 1729]
    input_array = np.array([[42, 1138, 725, 1729], [1729, 725, 42, 203]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = integer_lookup.IntegerLookup(max_tokens=None, mask_token=0)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/integer_lookup_test.py" startline="333" endline="345" pcid="4857">
  def test_inverse_output(self):
    vocab_data = [-1, 42, 1138, 725, 1729]
    input_array = np.array([[1, 2, 3, 4], [4, 3, 1, 0]])
    expected_output = np.array([[42, 1138, 725, 1729], [1729, 725, 42, -1]])

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = integer_lookup.IntegerLookup(invert=True)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/integer_lookup_test.py" startline="392" endline="403" pcid="4861">
  def test_int_output_explicit_vocab(self):
    vocab_data = [42, 1138, 725, 1729]
    input_array = np.array([[42, 1138, 725, 1729], [1729, 725, 42, 203]])
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = integer_lookup.IntegerLookup(vocabulary=vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/integer_lookup_test.py" startline="429" endline="441" pcid="4864">
  def test_multi_hot_output(self):
    vocab_data = [2, 3, 4, 5]
    input_array = np.array([[2, 2, 3, 4], [0, 1, 5, 2]])
    expected_output = [[0, 1, 1, 1, 0], [1, 1, 0, 0, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = integer_lookup.IntegerLookup(
        vocabulary=vocab_data, output_mode="multi_hot")
    res = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=res)
    output_data = model.predict(input_array)
    self.assertAllEqual(expected_output, output_data)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/integer_lookup_test.py" startline="442" endline="454" pcid="4865">
  def test_count_output(self):
    vocab_data = [2, 3, 4, 5]
    input_array = np.array([[2, 2, 3, 4], [0, 1, 5, 6]])
    expected_output = [[0, 2, 1, 1, 0], [3, 0, 0, 0, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = integer_lookup.IntegerLookup(
        vocabulary=vocab_data, output_mode="count")
    res = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=res)
    output_data = model.predict(input_array)
    self.assertAllEqual(expected_output, output_data)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/integer_lookup_test.py" startline="515" endline="529" pcid="4871">
  def test_int_output_explicit_vocab_from_file_via_setter(self):
    vocab_list = [42, 1138, 725, 1729]
    vocab_path = self._write_to_temp_file("vocab_file", vocab_list)

    input_array = np.array([[42, 1138, 725, 1729], [1729, 725, 42, 203]])
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = integer_lookup.IntegerLookup()
    layer.set_vocabulary(vocab_path)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/integer_lookup_test.py" startline="500" endline="514" pcid="4870">
  def test_int_output_inverted_vocab_from_file_with_mask(self):
    vocab_list = [42, 1138, 725, 1729]
    vocab_path = self._write_to_temp_file("vocab_file", vocab_list)

    input_array = np.array([[2, 3, 4, 5], [5, 4, 2, 0]])
    expected_output = [[42, 1138, 725, 1729], [1729, 725, 42, -10]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = integer_lookup.IntegerLookup(
        vocabulary=vocab_path, invert=True, mask_value=-10)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/integer_lookup_test.py" startline="346" endline="360" pcid="4858">
  def test_forward_backward_explicit_vocab(self):
    vocab_data = [42, 1138, 725, 1729]
    input_array = np.array([[42, 1138, 725, 1729], [1729, 725, 42, 203]])
    expected_output = np.array([[42, 1138, 725, 1729], [1729, 725, 42, -1]])

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = integer_lookup.IntegerLookup(vocabulary=vocab_data)
    inverse_layer = integer_lookup.IntegerLookup(
        vocabulary=vocab_data, invert=True)
    int_data = layer(input_data)
    inverse_data = inverse_layer(int_data)
    model = keras.Model(inputs=input_data, outputs=inverse_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/string_lookup_test.py" startline="242" endline="256" pcid="5055">
  def test_int_output_explicit_vocab_from_file(self):
    vocab_list = ["earth", "wind", "and", "fire"]
    vocab_path = self._write_to_temp_file("vocab_file", vocab_list)

    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = string_lookup.StringLookup(vocabulary=vocab_path)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array)
    self.assertAllEqual(expected_output, output_data)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/string_lookup_test.py" startline="121" endline="133" pcid="5046">
  def test_int_output_explicit_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = string_lookup.StringLookup(vocabulary=vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array)
    self.assertAllEqual(expected_output, output_data)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/string_lookup_test.py" startline="205" endline="218" pcid="5052">
  def test_count_output(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "earth", "fire", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[0, 2, 0, 0, 2], [1, 1, 0, 1, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = string_lookup.StringLookup(
        vocabulary=vocab_data, output_mode="count")
    res = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=res)
    output_data = model.predict(input_array)
    self.assertAllEqual(expected_output, output_data)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/string_lookup_test.py" startline="191" endline="204" pcid="5051">
  def test_multi_hot_output(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[0, 1, 1, 1, 1], [1, 1, 0, 1, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = string_lookup.StringLookup(
        vocabulary=vocab_data, output_mode="multi_hot")
    res = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=res)
    output_data = model.predict(input_array)
    self.assertAllEqual(expected_output, output_data)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/string_lookup_test.py" startline="257" endline="272" pcid="5056">
  def test_int_output_explicit_vocab_from_file_via_setter(self):
    vocab_list = ["earth", "wind", "and", "fire"]
    vocab_path = self._write_to_temp_file("vocab_file", vocab_list)

    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = string_lookup.StringLookup()
    layer.set_vocabulary(vocab_path)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array)
    self.assertAllEqual(expected_output, output_data)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/string_lookup_test.py" startline="300" endline="313" pcid="5060">
  def test_inverse_layer_from_file(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([[1, 2, 3, 4], [4, 3, 1, 0]])
    expected_output = np.array([["earth", "wind", "and", "fire"],
                                ["fire", "and", "earth", "[UNK]"]])
    vocab_path = self._write_to_temp_file("vocab_file", vocab_data)

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = string_lookup.StringLookup(vocabulary=vocab_path, invert=True)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array)
    self.assertAllEqual(expected_output, output_data)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/string_lookup_test.py" startline="286" endline="299" pcid="5059">
  def test_inverse_layer(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([[2, 3, 4, 5], [5, 4, 2, 0]])
    expected_output = np.array([["earth", "wind", "and", "fire"],
                                ["fire", "and", "earth", ""]])

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = string_lookup.StringLookup(
        vocabulary=vocab_data, invert=True, mask_token="")
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array)
    self.assertAllEqual(expected_output, output_data)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/string_lookup_test.py" startline="134" endline="146" pcid="5047">
  def test_int_output_explicit_vocab_with_special_tokens(self):
    vocab_data = ["", "[UNK]", "earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = string_lookup.StringLookup(vocabulary=vocab_data, mask_token="")
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array)
    self.assertAllEqual(expected_output, output_data)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/string_lookup_test.py" startline="314" endline="328" pcid="5061">
  def test_inverse_layer_from_file_with_mask(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([[2, 3, 4, 5], [5, 4, 2, 0]])
    expected_output = np.array([["earth", "wind", "and", "fire"],
                                ["fire", "and", "earth", "[M]"]])
    vocab_path = self._write_to_temp_file("vocab_file", vocab_data)

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = string_lookup.StringLookup(
        vocabulary=vocab_path, invert=True, mask_token="[M]")
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array)
    self.assertAllEqual(expected_output, output_data)

</source>
</class>

<class classid="261" nclones="49" nlines="19" similarity="70">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/integer_lookup_test.py" startline="285" endline="299" pcid="4854">
  def test_int_output_explicit_vocab(self):
    vocab_data = [42, 1138, 725, 1729]
    input_array = np.array([[42, 1138, 725, 1729], [1729, 725, 42, 203]])
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = integer_lookup.IntegerLookup(
        vocabulary=vocab_data,
        max_tokens=None,
    )
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/integer_lookup_test.py" startline="300" endline="315" pcid="4855">
  def test_int_output_explicit_vocab_with_special_tokens(self):
    vocab_data = [0, -1, 42, 1138, 725, 1729]
    input_array = np.array([[42, 1138, 725, 1729], [1729, 725, 42, 203]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = integer_lookup.IntegerLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        mask_token=0,
    )
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1301" endline="1319" pcid="5413">
  def test_int_output_int_file_vocab(self):
    vocab_data = ["10", "20", "30", "40"]
    input_array = np.array([[10, 20, 30, 40], [40, 0, 10, 42]])
    expected_output = [[2, 3, 4, 5], [5, 0, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)
    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1,
        dtype=tf.int64)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1088" endline="1108" pcid="5404">
  def test_int_output_file_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 0, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="837" endline="854" pcid="4919">
  def test_int_output(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=None,
        output_mode=text_vectorization.INT)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="855" endline="873" pcid="5396">
  def test_int_output_explicit_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="744" endline="763" pcid="4915">
  def test_vocab_setting_via_init_file(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    vocab_path = self._write_to_temp_file("vocab_file", vocab_data)
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=None,
        output_mode=text_vectorization.INT,
        vocabulary=vocab_path)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)

    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="784" endline="804" pcid="4917">
  def test_vocab_setting_with_oov_via_setter(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    vocab_path = self._write_to_temp_file("vocab_file", vocab_data)
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=None,
        output_mode=text_vectorization.INT)
    layer.set_vocabulary(vocab_path)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)

    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)


</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="759" endline="777" pcid="5392">
  def test_int_output_no_reserved_zero(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token=None,
        oov_token="[OOV]",
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1356" endline="1374" pcid="5417">
  def test_int_output_explicit_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1375" endline="1393" pcid="5418">
  def test_int_output_explicit_vocab_with_special_tokens(self):
    vocab_data = ["", "[OOV]", "earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="764" endline="783" pcid="4916">
  def test_vocab_setting_via_setter(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    vocab_path = self._write_to_temp_file("vocab_file", vocab_data)
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=None,
        output_mode=text_vectorization.INT)
    layer.set_vocabulary(vocab_path)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)

    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1227" endline="1247" pcid="5410">
  def test_int_output_file_vocab_no_mask(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "", "earth", "michigan"]])
    expected_output = [[1, 2, 3, 4], [4, 0, 1, 0]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        mask_token=None,
        num_oov_indices=1,
        oov_token="[OOV]",
        dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1248" endline="1268" pcid="5411">
  def test_int_output_file_vocab_no_oov_or_mask(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "wind", "earth", "and"]])
    expected_output = [[0, 1, 2, 3], [3, 1, 0, 2]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        mask_token=None,
        num_oov_indices=0,
        oov_token=None,
        dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1714" endline="1733" pcid="5445">
  def test_int_output_explicit_vocab(self):
    vocab_data = ["", "[OOV]", "earth", "wind", "and", "fire"]
    input_array = np.array([[2, 3, 4, 5], [5, 4, 2, 1]])
    expected_output = np.array([["earth", "wind", "and", "fire"],
                                ["fire", "and", "earth", "[OOV]"]])

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string,
        invert=True)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="725" endline="743" pcid="4914">
  def test_vocab_setting_via_init(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=None,
        output_mode=text_vectorization.INT,
        vocabulary=vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)

    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="810" endline="831" pcid="4918">
  def test_distribution_strategy_output(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    strategy = tf.distribute.OneDeviceStrategy("/cpu:0")
    with strategy.scope():
      input_data = keras.Input(shape=(None,), dtype=tf.string)
      layer = text_vectorization.TextVectorization(
          max_tokens=None,
          standardize=None,
          split=None,
          output_mode=text_vectorization.INT)
      layer.set_vocabulary(vocab_data)
      int_data = layer(input_data)
      model = keras.Model(inputs=input_data, outputs=int_data)

    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)


</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="778" endline="801" pcid="5393">
  def test_int_output_no_oov(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    valid_input = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", ""]])
    invalid_input = np.array([["earth", "wind", "and", "michigan"],
                              ["fire", "and", "earth", "michigan"]])
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=0,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(valid_input)
    self.assertAllEqual(expected_output, output_data)
    with self.assertRaisesRegex(tf.errors.InvalidArgumentError,
                                "found OOV values.*michigan"):
      _ = model.predict(invalid_input)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="490" endline="507" pcid="4901">
  def test_normalization(self):
    input_array = np.array([["Earth", "wInD", "aNd", "firE"],
                            ["fire|", "an<>d", "{earth}", "michigan@%$"]])
    expected_output = np.array([[b"earth", b"wind", b"and", b"fire"],
                                [b"fire", b"and", b"earth", b"michigan"]])

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=text_vectorization.LOWER_AND_STRIP_PUNCTUATION,
        split=None,
        ngrams=None,
        output_mode=None)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="623" endline="641" pcid="5383">
  def test_ragged_string_input(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = tf.ragged.constant(
        [["earth", "wind", "fire"], ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="407" endline="425" pcid="5373">
  def test_ragged_string_input(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = tf.ragged.constant(
        [["earth", "wind", "fire"], ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="1072" endline="1098" pcid="4927">
  def test_multi_hot_output_hard_maximum_set_vocabulary_after_build(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "earth"],
                            ["ohio", "and", "earth", "michigan"]])

    # pyformat: disable
    expected_output = [[0, 1, 1, 1, 0],
                       [1, 1, 0, 1, 0]]
    # pyformat: enable
    max_tokens = 5
    expected_output_shape = [None, max_tokens]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=max_tokens,
        standardize=None,
        split=None,
        output_mode=text_vectorization.MULTI_HOT,
        pad_to_max_tokens=True)
    int_data = layer(input_data)
    layer.set_vocabulary(vocab_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="526" endline="545" pcid="4903">
  def test_custom_normalization(self):
    input_array = np.array([["Earth", "wInD", "aNd", "firE"],
                            ["fire|", "an<>d", "{earth}", "michigan@%$"]])
    expected_output = np.array(
        [[b"earth", b"wind", b"and", b"fire"],
         [b"fire|", b"an<>d", b"{earth}", b"michigan@%$"]])

    custom_standardization = tf.strings.lower
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=custom_standardization,
        split=None,
        ngrams=None,
        output_mode=None)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1018" endline="1047" pcid="5402">
  def test_multi_hot_output_no_oov(self):
    """Check multi hot output when num_oov_indices=0."""
    vocab_data = ["earth", "wind", "and", "fire"]
    valid_input = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", ""]])
    invalid_input = np.array([["earth", "wind", "and", "michigan"],
                              ["fire", "and", "earth", "michigan"]])
    expected_output = [
        [1, 1, 1, 1, 0],
        [1, 0, 1, 1, 0],
    ]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=5,
        num_oov_indices=0,
        mask_token="",
        oov_token="[OOV]",
        output_mode=index_lookup.MULTI_HOT,
        pad_to_max_tokens=True,
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    binary_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=binary_data)
    output_data = model.predict(valid_input)
    self.assertAllEqual(expected_output, output_data)
    with self.assertRaisesRegex(tf.errors.InvalidArgumentError,
                                "found OOV values.*michigan"):
      _ = model.predict(invalid_input)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/string_lookup_test.py" startline="147" endline="165" pcid="5048">
  def test_int_output_no_oov(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    valid_input = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", ""]])
    invalid_input = np.array([["earth", "wind", "and", "michigan"],
                              ["fire", "and", "earth", "michigan"]])
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = string_lookup.StringLookup(
        vocabulary=vocab_data, mask_token="", num_oov_indices=0)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(valid_input)
    self.assertAllEqual(expected_output, output_data)
    with self.assertRaisesRegex(tf.errors.InvalidArgumentError,
                                "found OOV values.*michigan"):
      _ = model.predict(invalid_input)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1872" endline="1909" pcid="5457">
  def test_vocabulary_persistence_across_saving(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    # Build and validate a golden model.
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(output_dataset, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model")
    model.save(output_path, save_format="tf")

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()

    loaded_model = keras.models.load_model(
        output_path, custom_objects={"IndexLookup": index_lookup.IndexLookup})

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_dataset = loaded_model.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="828" endline="854" pcid="5395">
  def test_int_output_no_oov_sparse(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    valid_input = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", ""]])
    invalid_input = np.array([["earth", "wind", "and", "michigan"],
                              ["fire", "and", "earth", "michigan"]])
    valid_input = tf.sparse.from_dense(valid_input)
    invalid_input = tf.sparse.from_dense(invalid_input)
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=0,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(valid_input)
    self.assertAllEqual(expected_output,
                        tf.sparse.to_dense(output_data))
    with self.assertRaisesRegex(tf.errors.InvalidArgumentError,
                                "found OOV values.*michigan"):
      _ = model.predict(invalid_input)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="802" endline="827" pcid="5394">
  def test_int_output_no_oov_ragged(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    valid_input = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", ""]])
    invalid_input = np.array([["earth", "wind", "and", "michigan"],
                              ["fire", "and", "earth", "michigan"]])
    valid_input = tf.RaggedTensor.from_tensor(valid_input)
    invalid_input = tf.RaggedTensor.from_tensor(invalid_input)
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=0,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(valid_input)
    self.assertAllEqual(expected_output, output_data)
    with self.assertRaisesRegex(tf.errors.InvalidArgumentError,
                                "found OOV values.*michigan"):
      _ = model.predict(invalid_input)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="520" endline="539" pcid="5378">
  def test_ragged_string_input_multi_bucket(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = tf.ragged.constant([["earth", "wind", "fire"],
                                               ["fire", "and", "earth",
                                                "ohio"]])
    expected_output = [[3, 4, 6], [6, 5, 3, 2]]

    input_data = keras.Input(shape=(None,), dtype=tf.string, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=2,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="1240" endline="1266" pcid="4933">
  def test_count_output_soft_maximum(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "earth"],
                            ["ohio", "and", "earth", "michigan"]])

    # pyformat: disable
    expected_output = [[0, 2, 1, 1, 0],
                       [2, 1, 0, 1, 0]]
    # pyformat: enable
    max_tokens = 5
    expected_output_shape = [None, max_tokens]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=10,
        standardize=None,
        split=None,
        output_mode=text_vectorization.COUNT,
        pad_to_max_tokens=False)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="1821" endline="1850" pcid="4968">
  def test_keras_vocab_trimming_example(self):
    vocab_data = np.array([
        "earth", "earth", "earth", "earth", "wind", "wind", "wind", "and",
        "and", "fire"
    ])
    input_array = np.array([["earth", "wind", "and", "earth"],
                            ["ohio", "and", "earth", "michigan"]])

    # pyformat: disable
    expected_output = [[1, 2, 1],
                       [3, 1, 0]]
    # pyformat: enable
    max_tokens = 3
    expected_output_shape = [None, max_tokens]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=max_tokens,
        standardize=None,
        split=None,
        output_mode=text_vectorization.COUNT,
        pad_to_max_tokens=True)
    int_data = layer(input_data)
    layer.adapt(vocab_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())
    model = keras.Model(input_data, int_data)
    output = model.predict(input_array)
    self.assertAllEqual(expected_output, output)


</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="1099" endline="1128" pcid="4928">
  def test_multi_hot_output_hard_maximum_adapt_after_build(self):
    vocab_data = np.array([
        "earth", "earth", "earth", "earth", "wind", "wind", "wind", "and",
        "and", "fire"
    ])
    input_array = np.array([["earth", "wind", "and", "earth"],
                            ["ohio", "and", "earth", "michigan"]])

    # pyformat: disable
    expected_output = [[0, 1, 1, 1, 0],
                       [1, 1, 0, 1, 0]]
    # pyformat: enable
    max_tokens = 5
    expected_output_shape = [None, max_tokens]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=max_tokens,
        standardize=None,
        split=None,
        output_mode=text_vectorization.MULTI_HOT,
        pad_to_max_tokens=True)
    int_data = layer(input_data)
    layer.adapt(vocab_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="1213" endline="1239" pcid="4932">
  def test_count_output_hard_maximum(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "earth"],
                            ["ohio", "and", "earth", "michigan"]])

    # pyformat: disable
    expected_output = [[0, 2, 1, 1, 0, 0],
                       [2, 1, 0, 1, 0, 0]]
    # pyformat: enable
    max_tokens = 6
    expected_output_shape = [None, max_tokens]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=6,
        standardize=None,
        split=None,
        output_mode=text_vectorization.COUNT,
        pad_to_max_tokens=True)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="1167" endline="1194" pcid="4930">
  def test_multi_hot_output_soft_maximum_set_state_after_build(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "earth"],
                            ["ohio", "and", "earth", "michigan"]])

    # pyformat: disable
    expected_output = [[0, 1, 1, 1, 0],
                       [1, 1, 0, 1, 0]]
    # pyformat: enable
    max_tokens = 5
    expected_output_shape = [None, max_tokens]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=10,
        standardize=None,
        split=None,
        output_mode=text_vectorization.MULTI_HOT,
        pad_to_max_tokens=False)
    layer.build(input_data.shape)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/string_lookup_test.py" startline="346" endline="363" pcid="5063">
  def test_forward_backward_adapted_vocab(self):
    adapt_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = np.array([["earth", "wind", "and", "fire"],
                                ["fire", "and", "earth", "[UNK]"]])

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = string_lookup.StringLookup()
    layer.adapt(adapt_data)
    invert_layer = string_lookup.StringLookup(
        vocabulary=layer.get_vocabulary(), invert=True)
    int_data = layer(input_data)
    out_data = invert_layer(int_data)
    model = keras.Model(inputs=input_data, outputs=out_data)
    output_data = model.predict(input_array)
    self.assertAllEqual(expected_output, output_data)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/string_lookup_test.py" startline="329" endline="345" pcid="5062">
  def test_forward_backward_explicit_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = np.array([["earth", "wind", "and", "fire"],
                                ["fire", "and", "earth", "[UNK]"]])

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = string_lookup.StringLookup(vocabulary=vocab_data)
    invert_layer = string_lookup.StringLookup(
        vocabulary=vocab_data, invert=True)
    int_data = layer(input_data)
    out_data = invert_layer(int_data)
    model = keras.Model(inputs=input_data, outputs=out_data)
    output_data = model.predict(input_array)
    self.assertAllEqual(expected_output, output_data)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1982" endline="2021" pcid="5460">
  def test_vocabulary_persistence_file_vocab_keras_save_tf_load(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    # Build and validate a golden model.
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string,
        vocabulary=vocab_file)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(output_dataset, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model")
    model.save(output_path, save_format="tf")

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()

    loaded_model = tf.saved_model.load(output_path)
    f = loaded_model.signatures["serving_default"]

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_dataset = f(tf.constant(input_array))["index_lookup"]
    self.assertAllEqual(new_output_dataset, expected_output)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1942" endline="1981" pcid="5459">
  def test_persistence_file_vocabs_tf_save_tf_load(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    # Build and validate a golden model.
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string,
        vocabulary=vocab_file)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(output_dataset, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model")
    tf.saved_model.save(obj=model, export_dir=output_path)

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()

    loaded_model = tf.saved_model.load(output_path)
    f = loaded_model.signatures["serving_default"]

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_dataset = f(tf.constant(input_array))["index_lookup"]
    self.assertAllEqual(new_output_dataset, expected_output)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1910" endline="1941" pcid="5458">
  def test_vocabulary_persistence_file_across_cloning(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]
    vocab_file = self._write_to_temp_file("temp", vocab_data)

    # Build and validate a golden model.
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string,
        vocabulary=vocab_file)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(output_dataset, expected_output)

    # Clone the model and set weights.
    new_model = keras.models.clone_model(model)
    new_model.set_weights(model.get_weights())

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, new_model)

    # Validate correctness of the new model.
    new_output_dataset = new_model.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="908" endline="935" pcid="4922">
  def test_int_output_densifies_with_zeros_and_pads(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    # Create an input array that has 5 elements in the first example and 4 in
    # the second. This should output a 2x6 tensor with a padding value in the
    # second example, since output_sequence_length is set to 6.
    input_array = np.array([["earth wind and also fire"],
                            ["fire and earth michigan"]])
    expected_output = [[2, 3, 4, 1, 5, 0], [5, 4, 2, 1, 0, 0]]

    output_sequence_length = 6
    expected_output_shape = [None, output_sequence_length]

    # The input shape here is explicitly 1 because we're tokenizing.
    input_data = keras.Input(shape=(1,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=text_vectorization.SPLIT_ON_WHITESPACE,
        output_mode=text_vectorization.INT,
        output_sequence_length=output_sequence_length)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="1039" endline="1071" pcid="4926">
  def test_multi_hot_output_soft_maximum(self, sparse):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "earth"],
                            ["ohio", "and", "earth", "michigan"]])

    # pyformat: disable
    expected_output = [[0, 1, 1, 1, 0],
                       [1, 1, 0, 1, 0]]
    # pyformat: enable
    max_tokens = 5
    expected_output_shape = [None, max_tokens]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=10,
        standardize=None,
        split=None,
        output_mode=text_vectorization.MULTI_HOT,
        pad_to_max_tokens=False,
        sparse=sparse)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    if sparse:
      expected_output = tf.sparse.from_dense(tf.constant(expected_output))
      self.assertAllEqual(expected_output.indices, output_dataset.indices)
      self.assertAllEqual(expected_output.values, output_dataset.values)
    else:
      self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="936" endline="962" pcid="4923">
  def test_int_output_densifies_with_zeros_and_strips(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    # Create an input array that has 5 elements in the first example and 4 in
    # the second. This should output a 2x3 tensor with a padding value in the
    # second example, since output_sequence_length is set to 3.
    input_array = np.array([["earth wind and also fire"],
                            ["fire and earth michigan"]])
    expected_output = [[2, 3, 4], [5, 4, 2]]
    output_sequence_length = 3
    expected_output_shape = [None, output_sequence_length]

    # The input shape here is explicitly 1 because we're tokenizing.
    input_data = keras.Input(shape=(1,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=text_vectorization.SPLIT_ON_WHITESPACE,
        output_mode=text_vectorization.INT,
        output_sequence_length=output_sequence_length)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="1002" endline="1034" pcid="4925">
  def test_multi_hot_output_hard_maximum(self, sparse):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "earth"],
                            ["ohio", "and", "earth", "michigan"]])

    # pyformat: disable
    expected_output = [[0, 1, 1, 1, 0, 0],
                       [1, 1, 0, 1, 0, 0]]
    # pyformat: enable
    max_tokens = 6
    expected_output_shape = [None, max_tokens]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=max_tokens,
        standardize=None,
        split=None,
        output_mode=text_vectorization.MULTI_HOT,
        pad_to_max_tokens=True,
        sparse=sparse)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    if sparse:
      expected_output = tf.sparse.from_dense(tf.constant(expected_output))
      self.assertAllEqual(expected_output.indices, output_dataset.indices)
      self.assertAllEqual(expected_output.values, output_dataset.values)
    else:
      self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="855" endline="882" pcid="4920">
  def test_int_output_densifies_with_zeros(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    # Create an input array that has 5 elements in the first example and 4 in
    # the second. This should output a 2x5 tensor with a padding value in the
    # second example.
    input_array = np.array([["earth wind and also fire"],
                            ["fire and earth michigan"]])
    expected_output = [[2, 3, 4, 1, 5], [5, 4, 2, 1, 0]]

    # This test doesn't explicitly set an output shape, so the 2nd dimension
    # should stay 'None'.
    expected_output_shape = [None, None]

    # The input shape here is explicitly 1 because we're tokenizing.
    input_data = keras.Input(shape=(1,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=text_vectorization.SPLIT_ON_WHITESPACE,
        output_mode=text_vectorization.INT)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="883" endline="907" pcid="4921">
  def test_int_output_ragged(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    # Create an input array that has 5 elements in the first example and 4 in
    # the second.
    input_array = np.array([["earth wind and also fire"],
                            ["fire and earth michigan"]])
    expected_output = tf.ragged.constant([[2, 3, 4, 1, 5], [5, 4, 2, 1]])
    expected_output_shape = [None, None]

    # The input shape here is explicitly 1 because we're tokenizing.
    input_data = keras.Input(shape=(1,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=text_vectorization.SPLIT_ON_WHITESPACE,
        output_mode=text_vectorization.INT,
        ragged=True)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="963" endline="997" pcid="4924">
  def test_int_output_dynamically_strips_and_pads(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    # Create an input array that has 5 elements in the first example and 4 in
    # the second. This should output a 2x3 tensor with a padding value in the
    # second example, since output_sequence_length is set to 3.
    input_array = np.array([["earth wind and also fire"],
                            ["fire and earth michigan"]])
    expected_output = [[2, 3, 4], [5, 4, 2]]
    output_sequence_length = 3
    expected_output_shape = [None, output_sequence_length]

    # The input shape here is explicitly 1 because we're tokenizing.
    input_data = keras.Input(shape=(1,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=text_vectorization.SPLIT_ON_WHITESPACE,
        output_mode=text_vectorization.INT,
        output_sequence_length=output_sequence_length)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

    # Create an input array that has 1 element in the first example and 2 in
    # the second. This should output a 2x3 tensor with a padding value in the
    # second example, since output_sequence_length is set to 3.
    input_array_2 = np.array([["wind"], ["fire and"]])
    expected_output_2 = [[3, 0, 0], [5, 4, 0]]
    output_dataset = model.predict(input_array_2)
    self.assertAllEqual(expected_output_2, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="1271" endline="1305" pcid="4934">
  def test_tfidf_output_hard_maximum(self, sparse):
    vocab_data = ["earth", "wind", "and", "fire"]
    # OOV idf weight (bucket 0) should 0.5, the average of passed weights.
    idf_weights = [.4, .25, .75, .6]
    input_array = np.array([["earth", "wind", "and", "earth"],
                            ["ohio", "fire", "earth", "michigan"]])

    # pyformat: disable
    # pylint: disable=bad-whitespace
    expected_output = [[ 0, .8, .25, .75,  0, 0],
                       [ 1, .4,   0,   0, .6, 0]]
    # pylint: enable=bad-whitespace
    # pyformat: enable
    max_tokens = 6
    expected_output_shape = [None, max_tokens]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=6,
        standardize=None,
        split=None,
        output_mode=text_vectorization.TF_IDF,
        pad_to_max_tokens=True,
        sparse=sparse,
        vocabulary=vocab_data,
        idf_weights=idf_weights)
    int_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    if sparse:
      output_dataset = tf.sparse.to_dense(output_dataset)
    self.assertAllClose(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="1349" endline="1382" pcid="4936">
  def test_tfidf_output_set_oov_weight(self, sparse):
    vocab_data = ["[UNK]", "earth", "wind", "and", "fire"]
    idf_weights = [.1, .4, .25, .75, .6]
    input_array = np.array([["earth", "wind", "and", "earth"],
                            ["ohio", "fire", "earth", "michigan"]])

    # pyformat: disable
    # pylint: disable=bad-whitespace
    expected_output = [[  0, .8, .25, .75,  0],
                       [ .2, .4,   0,   0, .6]]
    # pylint: enable=bad-whitespace
    # pyformat: enable
    max_tokens = 5
    expected_output_shape = [None, max_tokens]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=10,
        standardize=None,
        split=None,
        output_mode=text_vectorization.TF_IDF,
        pad_to_max_tokens=False,
        sparse=sparse,
        vocabulary=vocab_data,
        idf_weights=idf_weights)
    int_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    if sparse:
      output_dataset = tf.sparse.to_dense(output_dataset)
    self.assertAllClose(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="1310" endline="1344" pcid="4935">
  def test_tfidf_output_soft_maximum(self, sparse):
    vocab_data = ["earth", "wind", "and", "fire"]
    # OOV idf weight (bucket 0) should 0.5, the average of passed weights.
    idf_weights = [.4, .25, .75, .6]
    input_array = np.array([["earth", "wind", "and", "earth"],
                            ["ohio", "fire", "earth", "michigan"]])

    # pyformat: disable
    # pylint: disable=bad-whitespace
    expected_output = [[ 0, .8, .25, .75,  0],
                       [ 1, .4,   0,   0, .6]]
    # pylint: enable=bad-whitespace
    # pyformat: enable
    max_tokens = 5
    expected_output_shape = [None, max_tokens]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=10,
        standardize=None,
        split=None,
        output_mode=text_vectorization.TF_IDF,
        pad_to_max_tokens=False,
        sparse=sparse,
        vocabulary=vocab_data,
        idf_weights=idf_weights)
    int_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    if sparse:
      output_dataset = tf.sparse.to_dense(output_dataset)
    self.assertAllClose(expected_output, output_dataset)

</source>
</class>

<class classid="262" nclones="2" nlines="17" similarity="88">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/integer_lookup_test.py" startline="410" endline="428" pcid="4863">
  def test_one_hot_output(self):
    vocab_data = [2, 3, 4, 5]
    input_array = np.array([2, 3, 4, 5, 6])
    expected_output = [
        [0, 1, 0, 0, 0],
        [0, 0, 1, 0, 0],
        [0, 0, 0, 1, 0],
        [0, 0, 0, 0, 1],
        [1, 0, 0, 0, 0],
    ]

    input_data = keras.Input(shape=(1,), dtype=tf.int64)
    layer = integer_lookup.IntegerLookup(
        vocabulary=vocab_data, output_mode="one_hot")
    res = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=res)
    output_data = model.predict(input_array)
    self.assertAllEqual(expected_output, output_data)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/string_lookup_test.py" startline="172" endline="190" pcid="5050">
  def test_one_hot_output(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array(["earth", "wind", "and", "fire", "michigan"])
    expected_output = [
        [0, 1, 0, 0, 0],
        [0, 0, 1, 0, 0],
        [0, 0, 0, 1, 0],
        [0, 0, 0, 0, 1],
        [1, 0, 0, 0, 0],
    ]

    input_data = keras.Input(shape=(1,), dtype=tf.string)
    layer = string_lookup.StringLookup(
        vocabulary=vocab_data, output_mode="one_hot")
    res = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=res)
    output_data = model.predict(input_array)
    self.assertAllEqual(expected_output, output_data)

</source>
</class>

<class classid="263" nclones="2" nlines="10" similarity="80">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/integer_lookup_test.py" startline="543" endline="554" pcid="4874">
  def test_tensor_vocab(self):
    vocab_data = [-1, 42, 1138, 725, 1729]
    vocab_tensor = tf.constant(vocab_data, tf.int64)
    layer = integer_lookup.IntegerLookup(vocabulary=vocab_tensor)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(vocab_data, returned_vocab)
    self.assertAllEqual(layer.vocabulary_size(), 5)
    fn = tf.function(lambda: layer.set_vocabulary(vocab_tensor))
    with self.assertRaisesRegex(RuntimeError, "Cannot set a tensor vocabulary"):
      fn()


</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/string_lookup_test.py" startline="379" endline="389" pcid="5065">
  def test_tensor_vocab(self):
    vocab_data = ["[UNK]", "wind", "and", "fire"]
    vocab_tensor = tf.constant(vocab_data)
    layer = string_lookup.StringLookup(vocabulary=vocab_tensor)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(vocab_data, returned_vocab)
    self.assertAllEqual(layer.vocabulary_size(), 4)
    fn = tf.function(lambda: layer.set_vocabulary(vocab_tensor))
    with self.assertRaisesRegex(RuntimeError, "Cannot set a tensor vocabulary"):
      fn()

</source>
</class>

<class classid="264" nclones="5" nlines="24" similarity="76">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_distribution_test.py" startline="38" endline="66" pcid="4880">
  def test_distribution_strategy_output(self, strategy):
    # TODO(b/180614455): remove this check when MLIR bridge is always enabled.
    if backend.is_tpu_strategy(strategy):
      self.skipTest("This test needs MLIR bridge on TPU.")

    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    input_dataset = tf.data.Dataset.from_tensor_slices(input_array).batch(
        2, drop_remainder=True)

    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    tf.config.set_soft_device_placement(True)

    with strategy.scope():
      input_data = keras.Input(shape=(None,), dtype=tf.string)
      layer = text_vectorization.TextVectorization(
          max_tokens=None,
          standardize=None,
          split=None,
          output_mode=text_vectorization.INT,
          vocabulary=vocab_data)
      int_data = layer(input_data)
      model = keras.Model(inputs=input_data, outputs=int_data)

    output_dataset = model.predict(input_dataset)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_distribution_test.py" startline="67" endline="99" pcid="4881">
  def test_distribution_strategy_output_with_adapt(self, strategy):
    # TODO(b/180614455): remove this check when MLIR bridge is always enabled.
    if backend.is_tpu_strategy(strategy):
      self.skipTest("This test needs MLIR bridge on TPU.")

    vocab_data = [[
        "earth", "earth", "earth", "earth", "wind", "wind", "wind", "and",
        "and", "fire"
    ]]
    vocab_dataset = tf.data.Dataset.from_tensors(vocab_data)
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    input_dataset = tf.data.Dataset.from_tensor_slices(input_array).batch(
        2, drop_remainder=True)

    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    tf.config.set_soft_device_placement(True)

    with strategy.scope():
      input_data = keras.Input(shape=(None,), dtype=tf.string)
      layer = text_vectorization.TextVectorization(
          max_tokens=None,
          standardize=None,
          split=None,
          output_mode=text_vectorization.INT)
      layer.adapt(vocab_dataset)
      int_data = layer(input_data)
      model = keras.Model(inputs=input_data, outputs=int_data)

    output_dataset = model.predict(input_dataset)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_distribution_test.py" startline="81" endline="111" pcid="4885">
  def test_strategy_with_file(self, strategy):
    # TODO(b/180614455): remove this check when MLIR bridge is always enabled.
    if backend.is_tpu_strategy(strategy):
      self.skipTest("This test needs MLIR bridge on TPU.")

    vocab_data = ["earth", "wind", "and", "fire"]
    vocab_file = self._write_to_temp_file("temp", vocab_data)

    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    input_dataset = tf.data.Dataset.from_tensor_slices(input_array).batch(
        2, drop_remainder=True)
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    tf.config.set_soft_device_placement(True)

    with strategy.scope():
      input_data = keras.Input(shape=(None,), dtype=tf.string)
      layer = index_lookup.IndexLookup(
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          dtype=tf.string,
          vocabulary=vocab_file)
      int_data = layer(input_data)
      model = keras.Model(inputs=input_data, outputs=int_data)
    model.compile(loss="mse")
    output_dataset = model.predict(input_dataset)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_distribution_test.py" startline="48" endline="80" pcid="4884">
  def test_strategy(self, strategy):
    # TODO(b/180614455): remove this check when MLIR bridge is always enabled.
    if backend.is_tpu_strategy(strategy):
      self.skipTest("This test needs MLIR bridge on TPU.")

    vocab_data = [[
        "earth", "earth", "earth", "earth", "wind", "wind", "wind", "and",
        "and", "fire"
    ]]
    vocab_dataset = tf.data.Dataset.from_tensors(vocab_data)
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    input_dataset = tf.data.Dataset.from_tensor_slices(input_array).batch(
        2, drop_remainder=True)
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    tf.config.set_soft_device_placement(True)

    with strategy.scope():
      input_data = keras.Input(shape=(None,), dtype=tf.string)
      layer = index_lookup.IndexLookup(
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          dtype=tf.string)
      layer.adapt(vocab_dataset)
      int_data = layer(input_data)
      model = keras.Model(inputs=input_data, outputs=int_data)
    model.compile(loss="mse")
    output_dataset = model.predict(input_dataset)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_distribution_test.py" startline="112" endline="145" pcid="4886">
  def test_tpu_with_multiple_oov(self, strategy):
    # TODO(b/180614455): remove this check when MLIR bridge is always enabled.
    if backend.is_tpu_strategy(strategy):
      self.skipTest("This test needs MLIR bridge on TPU.")

    vocab_data = [[
        "earth", "earth", "earth", "earth", "wind", "wind", "wind", "and",
        "and", "fire"
    ]]
    vocab_dataset = tf.data.Dataset.from_tensors(vocab_data)
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    input_dataset = tf.data.Dataset.from_tensor_slices(input_array).batch(
        2, drop_remainder=True)
    expected_output = [[3, 4, 5, 6], [6, 5, 3, 1]]

    tf.config.set_soft_device_placement(True)

    with strategy.scope():
      input_data = keras.Input(shape=(None,), dtype=tf.string)
      layer = index_lookup.IndexLookup(
          max_tokens=None,
          num_oov_indices=2,
          mask_token="",
          oov_token="[OOV]",
          dtype=tf.string)
      layer.adapt(vocab_dataset)
      int_data = layer(input_data)
      model = keras.Model(inputs=input_data, outputs=int_data)
    model.compile(loss="mse")
    output_dataset = model.predict(input_dataset)
    self.assertAllEqual(expected_output, output_dataset)


</source>
</class>

<class classid="265" nclones="3" nlines="11" similarity="72">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="313" endline="325" pcid="4889">
  def test_scalar_input_int_mode_no_len_limit(self):
    vocab_data = [
        "fire earth earth", "earth earth", "wind wind", "and wind and"
    ]
    input_data = "earth wind and fire fire and earth michigan"
    layer = text_vectorization.TextVectorization()
    layer.adapt(vocab_data)
    out = layer(input_data)
    self.assertAllClose(out.numpy(), [2, 3, 4, 5, 5, 4, 2, 1])
    layer.set_vocabulary(["earth", "wind", "and", "fire"])
    out = layer(input_data)
    self.assertAllClose(out.numpy(), [2, 3, 4, 5, 5, 4, 2, 1])

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="339" endline="351" pcid="4891">
  def test_scalar_input_int_pad_to_len_limit(self):
    vocab_data = [
        "fire earth earth", "earth earth", "wind wind", "and wind and"
    ]
    input_data = "earth wind and fire fire and earth michigan"
    layer = text_vectorization.TextVectorization(output_sequence_length=10)
    layer.adapt(vocab_data)
    out = layer(input_data)
    self.assertAllClose(out.numpy(), [2, 3, 4, 5, 5, 4, 2, 1, 0, 0])
    layer.set_vocabulary(["earth", "wind", "and", "fire"])
    out = layer(input_data)
    self.assertAllClose(out.numpy(), [2, 3, 4, 5, 5, 4, 2, 1, 0, 0])

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="326" endline="338" pcid="4890">
  def test_scalar_input_int_mode_trim_to_len_limit(self):
    vocab_data = [
        "fire earth earth", "earth earth", "wind wind", "and wind and"
    ]
    input_data = "earth wind and fire fire and earth michigan"
    layer = text_vectorization.TextVectorization(output_sequence_length=3)
    layer.adapt(vocab_data)
    out = layer(input_data)
    self.assertAllClose(out.numpy(), [2, 3, 4])
    layer.set_vocabulary(["earth", "wind", "and", "fire"])
    out = layer(input_data)
    self.assertAllClose(out.numpy(), [2, 3, 4])

</source>
</class>

<class classid="266" nclones="3" nlines="11" similarity="72">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="352" endline="362" pcid="4892">
  def test_list_inputs_1d(self):
    vocab_data = ["two two two", "two three three", "three four four five"]
    input_data = ["two three", "four five"]
    layer = text_vectorization.TextVectorization()
    layer.adapt(vocab_data)
    out = layer(input_data)
    self.assertAllClose(out.numpy(), [[2, 3], [4, 5]])
    layer.set_vocabulary(["two", "three", "four", "five"])
    out = layer(input_data)
    self.assertAllClose(out.numpy(), [[2, 3], [4, 5]])

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="375" endline="386" pcid="4894">
  def test_list_inputs_2d(self):
    vocab_data = [
        ["two two two"], ["two three three"], ["three four four five"]]
    input_data = [["two three"], ["four five"]]
    layer = text_vectorization.TextVectorization()
    layer.adapt(vocab_data)
    out = layer(input_data)
    self.assertAllClose(out.numpy(), [[2, 3], [4, 5]])
    layer.set_vocabulary(["two", "three", "four", "five"])
    out = layer(input_data)
    self.assertAllClose(out.numpy(), [[2, 3], [4, 5]])

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="363" endline="374" pcid="4893">
  def test_tensor_inputs(self):
    vocab_data = tf.constant(
        ["two two two", "two three three", "three four four five"])
    input_data = tf.constant(["two three", "four five"])
    layer = text_vectorization.TextVectorization()
    layer.adapt(vocab_data)
    out = layer(input_data)
    self.assertAllClose(out.numpy(), [[2, 3], [4, 5]])
    layer.set_vocabulary(["two", "three", "four", "five"])
    out = layer(input_data)
    self.assertAllClose(out.numpy(), [[2, 3], [4, 5]])

</source>
</class>

<class classid="267" nclones="3" nlines="17" similarity="76">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="546" endline="563" pcid="4904">
  def test_string_splitting(self):
    input_array = np.array([["earth wind and fire"],
                            ["\tfire\tand\nearth    michigan  "]])
    expected_output = [[b"earth", b"wind", b"and", b"fire"],
                       [b"fire", b"and", b"earth", b"michigan"]]

    input_data = keras.Input(shape=(1,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=text_vectorization.SPLIT_ON_WHITESPACE,
        ngrams=None,
        output_mode=None)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="564" endline="582" pcid="4905">
  def test_custom_string_splitting(self):
    input_array = np.array([["earth>wind>and fire"],
                            ["\tfire>and\nearth>michigan"]])
    expected_output = [[b"earth", b"wind", b"and fire"],
                       [b"\tfire", b"and\nearth", b"michigan"]]

    custom_split = lambda x: tf.strings.split(x, sep=">")
    input_data = keras.Input(shape=(1,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=custom_split,
        ngrams=None,
        output_mode=None)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="1793" endline="1816" pcid="4967">
  def test_serialization_with_custom_callables(self):
    input_array = np.array([["earth>wind>and Fire"],
                            ["\tfire>And\nearth>michigan"]])
    expected_output = [[b"earth", b"wind", b"and fire"],
                       [b"\tfire", b"and\nearth", b"michigan"]]

    input_data = keras.Input(shape=(1,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=custom_standardize_fn,
        split=custom_split_fn,
        ngrams=None,
        output_mode=None)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

    serialized_model_data = model.get_config()
    new_model = keras.Model.from_config(serialized_model_data)
    new_output_dataset = new_model.predict(input_array)
    self.assertAllEqual(expected_output, new_output_dataset)


</source>
</class>

<class classid="268" nclones="3" nlines="19" similarity="70">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="583" endline="606" pcid="4906">
  def test_single_ngram_value_ragged_inputs(self):
    input_array = tf.ragged.constant([["earth", "wind", "and", "fire"],
                                               ["fire", "and", "earth"]])
    # pyformat: disable
    expected_output = [[b"earth", b"wind", b"and", b"fire",
                        b"earth wind", b"wind and", b"and fire",
                        b"earth wind and", b"wind and fire"],
                       [b"fire", b"and", b"earth",
                        b"fire and", b"and earth",
                        b"fire and earth"]]
    # pyformat: enable

    input_data = keras.Input(shape=(None,), ragged=True, dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=None,
        ngrams=3,
        output_mode=None)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="607" endline="630" pcid="4907">
  def test_single_ngram_value(self):
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    # pyformat: disable
    expected_output = [[b"earth", b"wind", b"and", b"fire",
                        b"earth wind", b"wind and", b"and fire",
                        b"earth wind and", b"wind and fire"],
                       [b"fire", b"and", b"earth", b"michigan",
                        b"fire and", b"and earth", b"earth michigan",
                        b"fire and earth", b"and earth michigan"]]
    # pyformat: enable

    input_data = keras.Input(shape=(4,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=None,
        ngrams=3,
        output_mode=None)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="631" endline="652" pcid="4908">
  def test_multiple_ngram_values(self):
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    # pyformat: disable
    expected_output = [[b"earth wind", b"wind and", b"and fire",
                        b"earth wind and", b"wind and fire"],
                       [b"fire and", b"and earth", b"earth michigan",
                        b"fire and earth", b"and earth michigan"]]
    # pyformat: enable

    input_data = keras.Input(shape=(4,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=None,
        ngrams=(2, 3),
        output_mode=None)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
</class>

<class classid="269" nclones="2" nlines="11" similarity="81">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="687" endline="697" pcid="4910">
  def test_string_splitting_with_non_1d_array_fails(self):
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=text_vectorization.SPLIT_ON_WHITESPACE,
        output_mode=None)
    with self.assertRaisesRegex(RuntimeError,
                                ".*tokenize strings, the innermost dime.*"):
      _ = layer(input_data)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="698" endline="709" pcid="4911">
  def test_string_splitting_with_non_1d_raggedarray_fails(self):
    input_data = keras.Input(shape=(None,), ragged=True, dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        vocabulary=["a"],
        max_tokens=None,
        standardize=None,
        split=text_vectorization.SPLIT_ON_WHITESPACE,
        output_mode=None)
    with self.assertRaisesRegex(RuntimeError,
                                ".*tokenize strings, the innermost dime.*"):
      _ = layer(input_data)

</source>
</class>

<class classid="270" nclones="2" nlines="32" similarity="84">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="1129" endline="1166" pcid="4929">
  def test_multi_hot_output_hard_maximum_multiple_adapts(self):
    input_array = np.array([["earth", "wind", "and", "earth"],
                            ["ohio", "and", "earth", "michigan"]])
    adapt_data = ["earth", "earth", "earth", "earth", "wind", "wind", "wind"]
    first_expected_output = [
        [1, 1, 1, 0, 0],
        [1, 1, 0, 0, 0],
    ]
    second_adapt_data = [
        "earth", "earth", "earth", "earth", "wind", "wind", "wind", "and",
        "and", "fire"
    ]
    second_expected_output = [
        [0, 1, 1, 1, 0],
        [1, 1, 0, 1, 0],
    ]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=5,
        standardize=None,
        split=None,
        output_mode=text_vectorization.MULTI_HOT,
        pad_to_max_tokens=True)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)

    # Test the first adapt
    layer.adapt(adapt_data)
    first_output = model.predict(input_array)
    # Test the second adapt
    layer.adapt(second_adapt_data)
    # We need to recompile the model to retrace our call graph.
    model.compile()
    second_output = model.predict(input_array)
    self.assertAllEqual(first_expected_output, first_output)
    self.assertAllEqual(second_expected_output, second_output)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1048" endline="1087" pcid="5403">
  def test_multi_hot_output_hard_maximum_multiple_adapts(self):
    input_array = np.array([["earth", "wind", "and", "earth"],
                            ["ohio", "and", "earth", "michigan"]])
    adapt_data = ["earth", "earth", "earth", "earth", "wind", "wind", "wind"]
    first_expected_output = [
        [1, 1, 1, 0, 0],
        [1, 1, 0, 0, 0],
    ]
    second_adapt_data = [
        "earth", "earth", "earth", "earth", "wind", "wind", "wind", "and",
        "and", "fire"
    ]
    second_expected_output = [
        [0, 1, 1, 1, 0],
        [1, 1, 0, 1, 0],
    ]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=5,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        output_mode=index_lookup.MULTI_HOT,
        pad_to_max_tokens=True,
        dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)

    # Test the first adapt
    layer.adapt(adapt_data)
    first_output = model.predict(input_array)
    # Test the second adapt
    layer.adapt(second_adapt_data)
    # We need to recompile the model to retrace our call graph.
    model.compile()
    second_output = model.predict(input_array)
    self.assertAllEqual(first_expected_output, first_output)
    self.assertAllEqual(second_expected_output, second_output)

</source>
</class>

<class classid="271" nclones="2" nlines="11" similarity="72">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="1511" endline="1522" pcid="4943">
  def test_too_long_vocab_fails_in_single_setting(self):
    vocab_data = ["earth", "wind", "and", "fire"]

    layer = text_vectorization.TextVectorization(
        max_tokens=4,
        standardize=None,
        split=None,
        output_mode=text_vectorization.INT)
    with self.assertRaisesRegex(ValueError,
                                "vocabulary larger than the maximum vocab.*"):
      layer.set_vocabulary(vocab_data)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1836" endline="1848" pcid="5454">
  def test_too_long_vocab_fails_in_single_setting(self):
    vocab_data = ["earth", "wind", "and", "fire"]

    layer = index_lookup.IndexLookup(
        max_tokens=4,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    with self.assertRaisesRegex(ValueError,
                                "vocabulary larger than the maximum vocab.*"):
      layer.set_vocabulary(vocab_data)

</source>
</class>

<class classid="272" nclones="3" nlines="11" similarity="75">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="1523" endline="1534" pcid="4944">
  def test_setting_vocab_without_idf_weights_fails_in_tfidf_mode(self):
    vocab_data = ["earth", "wind", "and", "fire"]

    with self.assertRaisesRegex(
        ValueError, "`idf_weights` must be set if output_mode is TF_IDF"):
      text_vectorization.TextVectorization(
          max_tokens=5,
          standardize=None,
          split=None,
          output_mode=text_vectorization.TF_IDF,
          vocabulary=vocab_data)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="1535" endline="1547" pcid="4945">
  def test_idf_weights_length_mismatch_fails(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    idf_weights = [1, 2, 3]
    with self.assertRaisesRegex(
        ValueError, "`idf_weights` must be the same length as vocab"):
      text_vectorization.TextVectorization(
          max_tokens=5,
          standardize=None,
          split=None,
          output_mode=text_vectorization.TF_IDF,
          vocabulary=vocab_data,
          idf_weights=idf_weights)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="1548" endline="1560" pcid="4946">
  def test_set_tfidf_in_non_tfidf_fails(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    idf_weights = [1, 2, 3, 4]
    with self.assertRaisesRegex(ValueError,
                                "`idf_weights` should only be set if"):
      text_vectorization.TextVectorization(
          max_tokens=5,
          standardize=None,
          split=None,
          output_mode=text_vectorization.MULTI_HOT,
          vocabulary=vocab_data,
          idf_weights=idf_weights)

</source>
</class>

<class classid="273" nclones="3" nlines="22" similarity="72">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="1649" endline="1680" pcid="4963">
  def test_saving(self, init_vocab):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    # Build and validate a golden model.
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    vocabulary = vocab_data if init_vocab else None
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=None,
        output_mode=text_vectorization.INT,
        vocabulary=vocabulary)
    if not init_vocab:
      layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model")

    model.save(output_path, save_format="tf")

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()

    loaded_model = keras.models.load_model(output_path)
    self.assertAllEqual(loaded_model.predict(input_array), expected_output)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="1720" endline="1751" pcid="4965">
  def test_saving_when_adapted(self):
    adapt_data = [
        "earth", "earth", "earth", "earth", "wind", "wind", "wind", "and",
        "and", "fire"
    ]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    # Build and validate a golden model.
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=None,
        output_mode=text_vectorization.INT)
    layer.adapt(adapt_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model")

    model.save(output_path, save_format="tf")

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()

    loaded_model = keras.models.load_model(output_path)
    self.assertAllEqual(loaded_model.predict(input_array), expected_output)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/text_vectorization_test.py" startline="1685" endline="1719" pcid="4964">
  def test_saving_when_nested(self, init_vocab):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    # Build and validate a golden model.
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    vocabulary = vocab_data if init_vocab else None
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=None,
        output_mode=text_vectorization.INT,
        vocabulary=vocabulary)
    if not init_vocab:
      layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)

    outer_input = keras.Input(shape=(None,), dtype=tf.string)
    outer_output = model(outer_input)
    outer_model = keras.Model(inputs=outer_input, outputs=outer_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model")
    outer_model.save(output_path, save_format="tf")

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()

    loaded_model = keras.models.load_model(output_path)
    self.assertAllEqual(loaded_model.predict(input_array), expected_output)

</source>
</class>

<class classid="274" nclones="3" nlines="11" similarity="72">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/discretization_test.py" startline="36" endline="50" pcid="4972">
  def test_bucketize_with_explicit_buckets_integer(self):
    input_array = np.array([[-1.5, 1.0, 3.4, .5], [0.0, 3.0, 1.3, 0.0]])

    expected_output = [[0, 2, 3, 1], [1, 3, 2, 1]]
    expected_output_shape = [None, 4]

    input_data = keras.Input(shape=(4,))
    layer = discretization.Discretization(bin_boundaries=[0., 1., 2.])
    bucket_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, bucket_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=bucket_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/discretization_test.py" startline="51" endline="65" pcid="4973">
  def test_bucketize_with_explicit_buckets_int_input(self):
    input_array = np.array([[-1, 1, 3, 0], [0, 3, 1, 0]], dtype=np.int64)

    expected_output = [[0, 2, 3, 1], [1, 3, 2, 1]]
    expected_output_shape = [None, 4]

    input_data = keras.Input(shape=(4,), dtype=tf.int64)
    layer = discretization.Discretization(bin_boundaries=[-.5, 0.5, 1.5])
    bucket_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, bucket_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=bucket_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/discretization_distribution_test.py" startline="37" endline="55" pcid="5239">
  def test_distribution(self, strategy):
    input_array = np.array([[-1.5, 1.0, 3.4, .5], [0.0, 3.0, 1.3, 0.0]])

    expected_output = [[0, 2, 3, 1], [1, 3, 2, 1]]
    expected_output_shape = [None, 4]

    tf.config.set_soft_device_placement(True)

    with strategy.scope():
      input_data = keras.Input(shape=(4,))
      layer = discretization.Discretization(bin_boundaries=[0., 1., 2.])
      bucket_data = layer(input_data)
      self.assertAllEqual(expected_output_shape, bucket_data.shape.as_list())

      model = keras.Model(inputs=input_data, outputs=bucket_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)


</source>
</class>

<class classid="275" nclones="2" nlines="12" similarity="91">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/discretization_test.py" startline="66" endline="79" pcid="4974">
  def test_bucketize_with_explicit_buckets_sparse_float_input(self):
    indices = [[0, 1], [0, 2], [1, 1]]
    input_array = tf.SparseTensor(
        indices=indices, values=[-1.5, 1.0, 3.4], dense_shape=[2, 3])
    expected_output = [0, 2, 3]
    input_data = keras.Input(shape=(3,), dtype=tf.float32, sparse=True)
    layer = discretization.Discretization(bin_boundaries=[-.5, 0.5, 1.5])
    bucket_data = layer(input_data)

    model = keras.Model(inputs=input_data, outputs=bucket_data)
    output_dataset = model.predict(input_array, steps=1)
    self.assertAllEqual(indices, output_dataset.indices)
    self.assertAllEqual(expected_output, output_dataset.values)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/discretization_test.py" startline="111" endline="124" pcid="4977">
  def test_bucketize_with_explicit_buckets_sparse_int_input(self):
    indices = [[0, 1], [0, 2], [1, 1]]
    input_array = tf.SparseTensor(
        indices=indices, values=[-1, 1, 3], dense_shape=[2, 3])
    expected_output = [0, 2, 3]
    input_data = keras.Input(shape=(3,), dtype=tf.int32, sparse=True)
    layer = discretization.Discretization(bin_boundaries=[-.5, 0.5, 1.5])
    bucket_data = layer(input_data)

    model = keras.Model(inputs=input_data, outputs=bucket_data)
    output_dataset = model.predict(input_array, steps=1)
    self.assertAllEqual(indices, output_dataset.indices)
    self.assertAllEqual(expected_output, output_dataset.values)

</source>
</class>

<class classid="276" nclones="2" nlines="19" similarity="89">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/discretization_test.py" startline="278" endline="305" pcid="4985">
  def test_saved_model_keras(self):
    input_data = [[1], [2], [3]]
    predict_data = [[0.5], [1.5], [2.5]]
    expected_output = [[0], [1], [2]]

    cls = discretization.Discretization
    inputs = keras.Input(shape=(1,), dtype=tf.float32)
    layer = cls(num_bins=3)
    layer.adapt(input_data)
    outputs = layer(inputs)
    model = keras.Model(inputs=inputs, outputs=outputs)

    output_data = model.predict(predict_data)
    self.assertAllClose(output_data, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model")
    model.save(output_path, save_format="tf")
    loaded_model = keras.models.load_model(
        output_path, custom_objects={"Discretization": cls})

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_data = loaded_model.predict(predict_data)
    self.assertAllClose(new_output_data, expected_output)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/discretization_test.py" startline="306" endline="332" pcid="4986">
  def test_saved_weights_keras(self):
    input_data = [[1], [2], [3]]
    predict_data = [[0.5], [1.5], [2.5]]
    expected_output = [[0], [1], [2]]

    cls = discretization.Discretization
    inputs = keras.Input(shape=(1,), dtype=tf.float32)
    layer = cls(num_bins=3)
    layer.adapt(input_data)
    outputs = layer(inputs)
    model = keras.Model(inputs=inputs, outputs=outputs)

    output_data = model.predict(predict_data)
    self.assertAllClose(output_data, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_weights")
    model.save_weights(output_path, save_format="tf")
    new_model = keras.Model.from_config(
        model.get_config(), custom_objects={"Discretization": cls})
    new_model.load_weights(output_path)

    # Validate correctness of the new model.
    new_output_data = new_model.predict(predict_data)
    self.assertAllClose(new_output_data, expected_output)


</source>
</class>

<class classid="277" nclones="3" nlines="24" similarity="73">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/category_encoding_test.py" startline="35" endline="66" pcid="5005">
  def test_dense_input_sparse_output(self):
    input_array = tf.constant([[1, 2, 3], [3, 3, 0]])

    # The expected output should be (X for missing value):
    # [[X, 1, 1, 1, X, X]
    #  [1, X, X, 2, X, X]]
    expected_indices = [[0, 1], [0, 2], [0, 3], [1, 0], [1, 3]]
    expected_values = [1, 1, 1, 1, 2]
    num_tokens = 6

    input_data = keras.Input(shape=(None,), dtype=tf.int32)
    layer = category_encoding.CategoryEncoding(
        num_tokens=num_tokens, output_mode=category_encoding.COUNT, sparse=True)
    int_data = layer(input_data)

    model = keras.Model(inputs=input_data, outputs=int_data)
    sp_output_dataset = model.predict(input_array, steps=1)
    self.assertAllEqual(expected_values, sp_output_dataset.values)
    self.assertAllEqual(expected_indices, sp_output_dataset.indices)

    # Assert sparse output is same as dense output.
    layer = category_encoding.CategoryEncoding(
        num_tokens=num_tokens,
        output_mode=category_encoding.COUNT,
        sparse=False)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array, steps=1)
    self.assertAllEqual(
        tf.sparse.to_dense(sp_output_dataset, default_value=0),
        output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/category_encoding_test.py" startline="115" endline="151" pcid="5008">
  def test_sparse_input_sparse_output(self):
    sp_inp = tf.SparseTensor(
        indices=[[0, 0], [1, 1], [2, 0], [2, 1], [3, 1]],
        values=[0, 2, 1, 1, 0],
        dense_shape=[4, 2])
    input_data = keras.Input(shape=(None,), dtype=tf.int64, sparse=True)

    # The expected output should be (X for missing value):
    # [[1, X, X, X]
    #  [X, X, 1, X]
    #  [X, 2, X, X]
    #  [1, X, X, X]]
    expected_indices = [[0, 0], [1, 2], [2, 1], [3, 0]]
    expected_values = [1, 1, 2, 1]
    num_tokens = 6

    layer = category_encoding.CategoryEncoding(
        num_tokens=num_tokens, output_mode=category_encoding.COUNT, sparse=True)
    int_data = layer(input_data)

    model = keras.Model(inputs=input_data, outputs=int_data)
    sp_output_dataset = model.predict(sp_inp, steps=1)
    self.assertAllEqual(expected_values, sp_output_dataset.values)
    self.assertAllEqual(expected_indices, sp_output_dataset.indices)

    # Assert sparse output is same as dense output.
    layer = category_encoding.CategoryEncoding(
        num_tokens=num_tokens,
        output_mode=category_encoding.COUNT,
        sparse=False)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(sp_inp, steps=1)
    self.assertAllEqual(
        tf.sparse.to_dense(sp_output_dataset, default_value=0),
        output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/category_encoding_test.py" startline="201" endline="232" pcid="5011">
  def test_ragged_input_sparse_output(self):
    input_array = tf.ragged.constant([[1, 2, 3], [3, 3]])

    # The expected output should be (X for missing value):
    # [[X, 1, 1, 1]
    #  [X, X, X, 2]]
    expected_indices = [[0, 1], [0, 2], [0, 3], [1, 3]]
    expected_values = [1, 1, 1, 2]
    num_tokens = 6

    input_data = keras.Input(shape=(None,), dtype=tf.int32, ragged=True)
    layer = category_encoding.CategoryEncoding(
        num_tokens=num_tokens, output_mode=category_encoding.COUNT, sparse=True)
    int_data = layer(input_data)

    model = keras.Model(inputs=input_data, outputs=int_data)
    sp_output_dataset = model.predict(input_array, steps=1)
    self.assertAllEqual(expected_values, sp_output_dataset.values)
    self.assertAllEqual(expected_indices, sp_output_dataset.indices)

    # Assert sparse output is same as dense output.
    layer = category_encoding.CategoryEncoding(
        num_tokens=num_tokens,
        output_mode=category_encoding.COUNT,
        sparse=False)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array, steps=1)
    self.assertAllEqual(
        tf.sparse.to_dense(sp_output_dataset, default_value=0),
        output_dataset)

</source>
</class>

<class classid="278" nclones="4" nlines="14" similarity="71">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/category_encoding_test.py" startline="67" endline="88" pcid="5006">
  def test_sparse_input(self):
    input_array = np.array([[1, 2, 3, 0], [0, 3, 1, 0]], dtype=np.int64)
    sparse_tensor_data = tf.sparse.from_dense(input_array)

    # pyformat: disable
    expected_output = [[0, 1, 1, 1, 0, 0],
                       [0, 1, 0, 1, 0, 0]]
    # pyformat: enable
    num_tokens = 6
    expected_output_shape = [None, num_tokens]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, sparse=True)

    layer = category_encoding.CategoryEncoding(
        num_tokens=num_tokens, output_mode=category_encoding.MULTI_HOT)
    int_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(sparse_tensor_data, steps=1)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/category_encoding_test.py" startline="179" endline="200" pcid="5010">
  def test_ragged_input(self):
    input_array = tf.ragged.constant([[1, 2, 3], [3, 1]])

    # pyformat: disable
    expected_output = [[0, 1, 1, 1, 0, 0],
                       [0, 1, 0, 1, 0, 0]]
    # pyformat: enable
    num_tokens = 6
    expected_output_shape = [None, num_tokens]

    input_data = keras.Input(shape=(None,), dtype=tf.int32, ragged=True)

    layer = category_encoding.CategoryEncoding(
        num_tokens=num_tokens, output_mode=category_encoding.MULTI_HOT)
    int_data = layer(input_data)

    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array, steps=1)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/category_encoding_test.py" startline="444" endline="464" pcid="5024">
  def test_count_output(self):
    input_array = np.array([[1, 2, 3, 1], [0, 3, 1, 0]])

    # pyformat: disable
    expected_output = [[0, 2, 1, 1, 0, 0],
                       [2, 1, 0, 1, 0, 0]]
    # pyformat: enable
    num_tokens = 6
    expected_output_shape = [None, num_tokens]

    input_data = keras.Input(shape=(None,), dtype=tf.int32)
    layer = category_encoding.CategoryEncoding(
        num_tokens=6, output_mode=category_encoding.COUNT)
    int_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)


</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/category_encoding_test.py" startline="282" endline="298" pcid="5015">
  def test_legacy_max_tokens_arg(self):
    input_array = np.array([[1, 2, 3, 1]])
    expected_output = [[0, 1, 1, 1, 0, 0]]
    num_tokens = 6
    expected_output_shape = [None, num_tokens]

    input_data = keras.Input(shape=(None,), dtype=tf.int32)
    layer = category_encoding.CategoryEncoding(
        max_tokens=num_tokens, output_mode=category_encoding.MULTI_HOT)
    int_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)


</source>
</class>

<class classid="279" nclones="2" nlines="15" similarity="93">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/category_encoding_test.py" startline="248" endline="264" pcid="5013">
  def test_dense_oov_input(self):
    valid_array = tf.constant([[0, 1, 2], [0, 1, 2]])
    invalid_array = tf.constant([[0, 1, 2], [2, 3, 1]])
    num_tokens = 3
    expected_output_shape = [None, num_tokens]
    encoder_layer = category_encoding.CategoryEncoding(num_tokens)
    input_data = keras.Input(shape=(3,), dtype=tf.int32)
    int_data = encoder_layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())
    model = keras.Model(inputs=input_data, outputs=int_data)
    # Call predict once on valid input to compile a graph and test control flow.
    _ = model.predict(valid_array, steps=1)
    with self.assertRaisesRegex(
        tf.errors.InvalidArgumentError,
        ".*must be in the range 0 <= values < num_tokens.*"):
      _ = model.predict(invalid_array, steps=1)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/category_encoding_test.py" startline="265" endline="281" pcid="5014">
  def test_dense_negative(self):
    valid_array = tf.constant([[0, 1, 2], [0, 1, 2]])
    invalid_array = tf.constant([[1, 2, 0], [2, 2, -1]])
    num_tokens = 3
    expected_output_shape = [None, num_tokens]
    encoder_layer = category_encoding.CategoryEncoding(num_tokens)
    input_data = keras.Input(shape=(3,), dtype=tf.int32)
    int_data = encoder_layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())
    model = keras.Model(inputs=input_data, outputs=int_data)
    # Call predict once on valid input to compile a graph and test control flow.
    _ = model.predict(valid_array, steps=1)
    with self.assertRaisesRegex(
        tf.errors.InvalidArgumentError,
        ".*must be in the range 0 <= values < num_tokens.*"):
      _ = model.predict(invalid_array, steps=1)

</source>
</class>

<class classid="280" nclones="5" nlines="17" similarity="70">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/category_encoding_test.py" startline="304" endline="323" pcid="5016">
  def test_one_hot_output(self):
    input_data = np.array([[3], [2], [0], [1]])
    expected_output = [
        [0, 0, 0, 1],
        [0, 0, 1, 0],
        [1, 0, 0, 0],
        [0, 1, 0, 0],
    ]
    num_tokens = 4
    expected_output_shape = [None, num_tokens]

    layer = category_encoding.CategoryEncoding(
        num_tokens=num_tokens, output_mode=category_encoding.ONE_HOT)
    inputs = keras.Input(shape=(1,), dtype=tf.int32)
    outputs = layer(inputs)
    model = keras.Model(inputs=inputs, outputs=outputs)
    output_dataset = model(input_data)
    self.assertAllEqual(expected_output_shape, outputs.shape.as_list())
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/category_encoding_test.py" startline="324" endline="348" pcid="5017">
  def test_one_hot_output_rank_one_input(self):
    input_data = np.array([3, 2, 0, 1])
    expected_output = [
        [0, 0, 0, 1],
        [0, 0, 1, 0],
        [1, 0, 0, 0],
        [0, 1, 0, 0],
    ]
    num_tokens = 4
    expected_output_shape = [None, num_tokens]

    # Test call on layer directly.
    layer = category_encoding.CategoryEncoding(
        num_tokens=num_tokens, output_mode=category_encoding.ONE_HOT)
    output_data = layer(input_data)
    self.assertAllEqual(expected_output, output_data)

    # Test call on model.
    inputs = keras.Input(shape=(1,), dtype=tf.int32)
    outputs = layer(inputs)
    model = keras.Model(inputs=inputs, outputs=outputs)
    output_data = model(input_data)
    self.assertAllEqual(expected_output_shape, outputs.shape.as_list())
    self.assertAllEqual(expected_output, output_data)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/category_encoding_test.py" startline="396" endline="415" pcid="5021">
  def test_multi_hot_output_rank_one_input(self):
    input_data = np.array([3, 2, 0, 1])
    expected_output = [1, 1, 1, 1, 0, 0]
    num_tokens = 6
    expected_output_shape = [None, num_tokens]

    # Test call on layer directly.
    layer = category_encoding.CategoryEncoding(
        num_tokens=num_tokens, output_mode=category_encoding.MULTI_HOT)
    output_data = layer(input_data)
    self.assertAllEqual(expected_output, output_data)

    # Test call on model.
    inputs = keras.Input(shape=(4,), dtype=tf.int32)
    outputs = layer(inputs)
    model = keras.Model(inputs=inputs, outputs=outputs)
    output_data = model(input_data)
    self.assertAllEqual(expected_output_shape, outputs.shape.as_list())
    self.assertAllEqual(expected_output, output_data)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/category_encoding_test.py" startline="349" endline="369" pcid="5018">
  def test_one_hot_output_rank_zero_input(self):
    input_data = np.array(3)
    expected_output = [0, 0, 0, 1]
    num_tokens = 4
    expected_output_shape = [None, num_tokens]

    # Test call on layer directly.
    layer = category_encoding.CategoryEncoding(
        num_tokens=num_tokens, output_mode=category_encoding.ONE_HOT)
    output_data = layer(input_data)
    self.assertAllEqual(expected_output, output_data)

    # Test call on model.
    inputs = keras.Input(shape=(1,), dtype=tf.int32)
    outputs = layer(inputs)
    model = keras.Model(inputs=inputs, outputs=outputs)
    output_data = model(input_data)

    self.assertAllEqual(expected_output_shape, outputs.shape.as_list())
    self.assertAllEqual(expected_output, output_data)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/category_encoding_test.py" startline="416" endline="435" pcid="5022">
  def test_multi_hot_output_rank_zero_input(self):
    input_data = np.array(3)
    expected_output = [0, 0, 0, 1, 0, 0]
    num_tokens = 6
    expected_output_shape = [None, num_tokens]

    # Test call on layer directly.
    layer = category_encoding.CategoryEncoding(
        num_tokens=num_tokens, output_mode=category_encoding.MULTI_HOT)
    output_data = layer(input_data)
    self.assertAllEqual(expected_output, output_data)

    # Test call on model.
    inputs = keras.Input(shape=(4,), dtype=tf.int32)
    outputs = layer(inputs)
    model = keras.Model(inputs=inputs, outputs=outputs)
    output_data = model(input_data)
    self.assertAllEqual(expected_output_shape, outputs.shape.as_list())
    self.assertAllEqual(expected_output, output_data)

</source>
</class>

<class classid="281" nclones="3" nlines="11" similarity="90">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/category_crossing_test.py" startline="30" endline="41" pcid="5066">
  def test_crossing_sparse_inputs(self):
    layer = category_crossing.CategoryCrossing()
    inputs_0 = tf.SparseTensor(
        indices=[[0, 0], [1, 0], [1, 1]],
        values=['a', 'b', 'c'],
        dense_shape=[2, 2])
    inputs_1 = tf.SparseTensor(
        indices=[[0, 1], [1, 2]], values=['d', 'e'], dense_shape=[2, 3])
    output = layer([inputs_0, inputs_1])
    self.assertAllClose(np.asarray([[0, 0], [1, 0], [1, 1]]), output.indices)
    self.assertAllEqual([b'a_X_d', b'b_X_e', b'c_X_e'], output.values)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/category_crossing_test.py" startline="42" endline="53" pcid="5067">
  def test_crossing_sparse_inputs_custom_sep(self):
    layer = category_crossing.CategoryCrossing(separator='_Y_')
    inputs_0 = tf.SparseTensor(
        indices=[[0, 0], [1, 0], [1, 1]],
        values=['a', 'b', 'c'],
        dense_shape=[2, 2])
    inputs_1 = tf.SparseTensor(
        indices=[[0, 1], [1, 2]], values=['d', 'e'], dense_shape=[2, 3])
    output = layer([inputs_0, inputs_1])
    self.assertAllClose(np.asarray([[0, 0], [1, 0], [1, 1]]), output.indices)
    self.assertAllEqual([b'a_Y_d', b'b_Y_e', b'c_Y_e'], output.values)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/category_crossing_test.py" startline="54" endline="65" pcid="5068">
  def test_crossing_sparse_inputs_empty_sep(self):
    layer = category_crossing.CategoryCrossing(separator='')
    inputs_0 = tf.SparseTensor(
        indices=[[0, 0], [1, 0], [1, 1]],
        values=['a', 'b', 'c'],
        dense_shape=[2, 2])
    inputs_1 = tf.SparseTensor(
        indices=[[0, 1], [1, 2]], values=['d', 'e'], dense_shape=[2, 3])
    output = layer([inputs_0, inputs_1])
    self.assertAllClose(np.asarray([[0, 0], [1, 0], [1, 1]]), output.indices)
    self.assertAllEqual([b'ad', b'be', b'ce'], output.values)

</source>
</class>

<class classid="282" nclones="2" nlines="21" similarity="80">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/benchmarks/embedding_varlen_benchmark.py" startline="31" endline="65" pcid="5125">
def embedding_varlen(batch_size, max_length):
  """Benchmark a variable-length embedding."""
  # Data and constants.
  embedding_size = 32768
  data = fc_bm.create_data(
      max_length, batch_size * NUM_REPEATS, embedding_size - 1, dtype=int)

  # Keras implementation
  model = keras.Sequential()
  model.add(
      keras.Input(shape=(None,), ragged=True, name="data", dtype=tf.int64))
  model.add(keras.layers.Embedding(embedding_size, 256))
  model.add(keras.layers.Lambda(lambda x: tf.reduce_mean(x, axis=-1)))

  # FC implementation
  fc = tf.feature_column.embedding_column(
      tf.feature_column.categorical_column_with_identity(
          "data", num_buckets=embedding_size - 1),
      dimension=256)

  # Wrap the FC implementation in a tf.function for a fair comparison
  @tf_function()
  def fc_fn(tensors):
    fc.transform_feature(tf.__internal__.feature_column.FeatureTransformationCache(tensors), None)

  # Benchmark runs
  keras_data = {"data": data}
  k_avg_time = fc_bm.run_keras(keras_data, model, batch_size, NUM_REPEATS)

  fc_data = {"data": data.to_sparse()}
  fc_avg_time = fc_bm.run_fc(fc_data, fc_fn, batch_size, NUM_REPEATS)

  return k_avg_time, fc_avg_time


</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/benchmarks/embedding_dense_benchmark.py" startline="31" endline="64" pcid="5156">
def embedding_varlen(batch_size, max_length):
  """Benchmark a variable-length embedding."""
  # Data and constants.
  embedding_size = 32768
  data = fc_bm.create_data(
      max_length, batch_size * NUM_REPEATS, embedding_size - 1, dtype=int)

  # Keras implementation
  model = keras.Sequential()
  model.add(keras.Input(shape=(None,), name="data", dtype=tf.int64))
  model.add(keras.layers.Embedding(embedding_size, 256))
  model.add(keras.layers.Lambda(lambda x: tf.reduce_mean(x, axis=-1)))

  # FC implementation
  fc = tf.feature_column.embedding_column(
      tf.feature_column.categorical_column_with_identity(
          "data", num_buckets=embedding_size - 1),
      dimension=256)

  # Wrap the FC implementation in a tf.function for a fair comparison
  @tf_function()
  def fc_fn(tensors):
    fc.transform_feature(tf.__internal__.feature_column.FeatureTransformationCache(tensors), None)

  # Benchmark runs
  keras_data = {"data": data.to_tensor(default_value=0)}
  k_avg_time = fc_bm.run_keras(keras_data, model, batch_size, NUM_REPEATS)

  fc_data = {"data": data.to_tensor(default_value=0)}
  fc_avg_time = fc_bm.run_fc(fc_data, fc_fn, batch_size, NUM_REPEATS)

  return k_avg_time, fc_avg_time


</source>
</class>

<class classid="283" nclones="8" nlines="20" similarity="70">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/benchmarks/category_hash_dense_benchmark.py" startline="31" endline="66" pcid="5134">
def embedding_varlen(batch_size, max_length):
  """Benchmark a variable-length embedding."""
  # Data and constants.

  num_buckets = 10000
  vocab = fc_bm.create_vocabulary(32768)
  data = fc_bm.create_string_data(
      max_length, batch_size * NUM_REPEATS, vocab, pct_oov=0.0)

  # Keras implementation
  model = keras.Sequential()
  model.add(keras.Input(shape=(max_length,), name="data", dtype=tf.string))
  model.add(hashing.Hashing(num_buckets))

  # FC implementation
  fc = tf.feature_column.sequence_categorical_column_with_hash_bucket("data", num_buckets)

  # Wrap the FC implementation in a tf.function for a fair comparison
  @tf_function()
  def fc_fn(tensors):
    fc.transform_feature(tf.__internal__.feature_column.FeatureTransformationCache(tensors), None)

  # Benchmark runs
  keras_data = {
      "data": data.to_tensor(default_value="", shape=(batch_size, max_length))
  }
  k_avg_time = fc_bm.run_keras(keras_data, model, batch_size, NUM_REPEATS)

  fc_data = {
      "data": data.to_tensor(default_value="", shape=(batch_size, max_length))
  }
  fc_avg_time = fc_bm.run_fc(fc_data, fc_fn, batch_size, NUM_REPEATS)

  return k_avg_time, fc_avg_time


</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/benchmarks/category_hash_varlen_benchmark.py" startline="31" endline="64" pcid="5188">
def embedding_varlen(batch_size, max_length):
  """Benchmark a variable-length embedding."""
  # Data and constants.

  num_buckets = 10000
  vocab = fc_bm.create_vocabulary(32768)
  data = fc_bm.create_string_data(
      max_length, batch_size * NUM_REPEATS, vocab, pct_oov=0.0)

  # Keras implementation
  model = keras.Sequential()
  model.add(
      keras.Input(
          shape=(max_length,), name="data", ragged=True, dtype=tf.string))
  model.add(hashing.Hashing(num_buckets))

  # FC implementation
  fc = tf.feature_column.categorical_column_with_hash_bucket("data", num_buckets)

  # Wrap the FC implementation in a tf.function for a fair comparison
  @tf_function()
  def fc_fn(tensors):
    fc.transform_feature(tf.__internal__.feature_column.FeatureTransformationCache(tensors), None)

  # Benchmark runs
  keras_data = {"data": data}
  k_avg_time = fc_bm.run_keras(keras_data, model, batch_size, NUM_REPEATS)

  fc_data = {"data": data.to_sparse()}
  fc_avg_time = fc_bm.run_fc(fc_data, fc_fn, batch_size, NUM_REPEATS)

  return k_avg_time, fc_avg_time


</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/benchmarks/category_vocab_list_dense_benchmark.py" startline="31" endline="65" pcid="5149">
def embedding_varlen(batch_size, max_length):
  """Benchmark a variable-length embedding."""
  # Data and constants.
  vocab = fc_bm.create_vocabulary(32768)
  data = fc_bm.create_string_data(
      max_length, batch_size * NUM_REPEATS, vocab, pct_oov=0.15)

  # Keras implementation
  model = keras.Sequential()
  model.add(keras.Input(shape=(max_length,), name="data", dtype=tf.string))
  model.add(string_lookup.StringLookup(vocabulary=vocab, mask_token=None))

  # FC implementation
  fc = tf.feature_column.categorical_column_with_vocabulary_list(
      key="data", vocabulary_list=vocab, num_oov_buckets=1)

  # Wrap the FC implementation in a tf.function for a fair comparison
  @tf_function()
  def fc_fn(tensors):
    fc.transform_feature(tf.__internal__.feature_column.FeatureTransformationCache(tensors), None)

  # Benchmark runs
  keras_data = {
      "data": data.to_tensor(default_value="", shape=(batch_size, max_length))
  }
  k_avg_time = fc_bm.run_keras(keras_data, model, batch_size, NUM_REPEATS)

  fc_data = {
      "data": data.to_tensor(default_value="", shape=(batch_size, max_length))
  }
  fc_avg_time = fc_bm.run_fc(fc_data, fc_fn, batch_size, NUM_REPEATS)

  return k_avg_time, fc_avg_time


</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/benchmarks/category_vocab_file_varlen_benchmark.py" startline="45" endline="78" pcid="5173">
  def embedding_varlen(self, batch_size, max_length):
    """Benchmark a variable-length embedding."""
    # Data and constants.
    vocab = fc_bm.create_vocabulary(32768)
    path = self._write_to_temp_file("tmp", vocab)

    data = fc_bm.create_string_data(
        max_length, batch_size * NUM_REPEATS, vocab, pct_oov=0.15)

    # Keras implementation
    model = keras.Sequential()
    model.add(
        keras.Input(
            shape=(max_length,), name="data", ragged=True, dtype=tf.string))
    model.add(string_lookup.StringLookup(vocabulary=path, mask_token=None))

    # FC implementation
    fc = tf.feature_column.sequence_categorical_column_with_vocabulary_list(
        key="data", vocabulary_list=vocab, num_oov_buckets=1)

    # Wrap the FC implementation in a tf.function for a fair comparison
    @tf_function()
    def fc_fn(tensors):
      fc.transform_feature(tf.__internal__.feature_column.FeatureTransformationCache(tensors), None)

    # Benchmark runs
    keras_data = {"data": data}
    k_avg_time = fc_bm.run_keras(keras_data, model, batch_size, NUM_REPEATS)

    fc_data = {"data": data.to_sparse()}
    fc_avg_time = fc_bm.run_fc(fc_data, fc_fn, batch_size, NUM_REPEATS)

    return k_avg_time, fc_avg_time

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/benchmarks/category_vocab_list_varlen_benchmark.py" startline="31" endline="63" pcid="5200">
def embedding_varlen(batch_size, max_length):
  """Benchmark a variable-length embedding."""
  # Data and constants.
  vocab = fc_bm.create_vocabulary(32768)
  data = fc_bm.create_string_data(
      max_length, batch_size * NUM_REPEATS, vocab, pct_oov=0.15)

  # Keras implementation
  model = keras.Sequential()
  model.add(
      keras.Input(
          shape=(max_length,), name="data", ragged=True, dtype=tf.string))
  model.add(string_lookup.StringLookup(vocabulary=vocab, mask_token=None))

  # FC implementation
  fc = tf.feature_column.sequence_categorical_column_with_vocabulary_list(
      key="data", vocabulary_list=vocab, num_oov_buckets=1)

  # Wrap the FC implementation in a tf.function for a fair comparison
  @tf_function()
  def fc_fn(tensors):
    fc.transform_feature(tf.__internal__.feature_column.FeatureTransformationCache(tensors), None)

  # Benchmark runs
  keras_data = {"data": data}
  k_avg_time = fc_bm.run_keras(keras_data, model, batch_size, NUM_REPEATS)

  fc_data = {"data": data.to_sparse()}
  fc_avg_time = fc_bm.run_fc(fc_data, fc_fn, batch_size, NUM_REPEATS)

  return k_avg_time, fc_avg_time


</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/benchmarks/category_vocab_list_indicator_dense_benchmark.py" startline="32" endline="71" pcid="5193">
def embedding_varlen(batch_size, max_length):
  """Benchmark a variable-length embedding."""
  # Data and constants.
  vocab_size = 32768
  vocab = fc_bm.create_vocabulary(vocab_size)
  data = fc_bm.create_string_data(
      max_length, batch_size * NUM_REPEATS, vocab, pct_oov=0.15)

  # Keras implementation
  model = keras.Sequential()
  model.add(keras.Input(shape=(max_length,), name="data", dtype=tf.string))
  model.add(string_lookup.StringLookup(vocabulary=vocab, mask_token=None))
  model.add(
      category_encoding.CategoryEncoding(
          num_tokens=vocab_size + 1, output_mode="count"))

  # FC implementation
  fc = tf.feature_column.indicator_column(
      tf.feature_column.categorical_column_with_vocabulary_list(
          key="data", vocabulary_list=vocab, num_oov_buckets=1))

  # Wrap the FC implementation in a tf.function for a fair comparison
  @tf_function()
  def fc_fn(tensors):
    fc.transform_feature(tf.__internal__.feature_column.FeatureTransformationCache(tensors), None)

  # Benchmark runs
  keras_data = {
      "data": data.to_tensor(default_value="", shape=(batch_size, max_length))
  }
  k_avg_time = fc_bm.run_keras(keras_data, model, batch_size, NUM_REPEATS)

  fc_data = {
      "data": data.to_tensor(default_value="", shape=(batch_size, max_length))
  }
  fc_avg_time = fc_bm.run_fc(fc_data, fc_fn, batch_size, NUM_REPEATS)

  return k_avg_time, fc_avg_time


</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/benchmarks/category_vocab_file_dense_benchmark.py" startline="45" endline="83" pcid="5204">
  def embedding_varlen(self, batch_size, max_length):
    """Benchmark a variable-length embedding."""
    # Data and constants.
    vocab = fc_bm.create_vocabulary(32768)

    path = self._write_to_temp_file("tmp", vocab)

    data = fc_bm.create_string_data(
        max_length, batch_size * NUM_REPEATS, vocab, pct_oov=0.15)

    # Keras implementation
    model = keras.Sequential()
    model.add(keras.Input(shape=(max_length,), name="data", dtype=tf.string))
    model.add(string_lookup.StringLookup(vocabulary=path, mask_token=None))

    # FC implementation
    fc = tf.feature_column.categorical_column_with_vocabulary_list(
        key="data", vocabulary_list=vocab, num_oov_buckets=1)

    # Wrap the FC implementation in a tf.function for a fair comparison
    @tf_function()
    def fc_fn(tensors):
      fc.transform_feature(tf.__internal__.feature_column.FeatureTransformationCache(tensors), None)

    # Benchmark runs
    keras_data = {
        "data": data.to_tensor(
            default_value="", shape=(batch_size, max_length))
    }
    k_avg_time = fc_bm.run_keras(keras_data, model, batch_size, NUM_REPEATS)

    fc_data = {
        "data": data.to_tensor(
            default_value="", shape=(batch_size, max_length))
    }
    fc_avg_time = fc_bm.run_fc(fc_data, fc_fn, batch_size, NUM_REPEATS)

    return k_avg_time, fc_avg_time

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/benchmarks/category_vocab_list_indicator_varlen_benchmark.py" startline="32" endline="69" pcid="5140">
def embedding_varlen(batch_size, max_length):
  """Benchmark a variable-length embedding."""
  # Data and constants.
  vocab_size = 32768
  vocab = fc_bm.create_vocabulary(vocab_size)
  data = fc_bm.create_string_data(
      max_length, batch_size * NUM_REPEATS, vocab, pct_oov=0.15)

  # Keras implementation
  model = keras.Sequential()
  model.add(
      keras.Input(
          shape=(max_length,), name="data", ragged=True, dtype=tf.string))
  model.add(string_lookup.StringLookup(vocabulary=vocab, mask_token=None))
  model.add(
      category_encoding.CategoryEncoding(
          num_tokens=vocab_size + 1, output_mode="count"))

  # FC implementation
  fc = tf.feature_column.indicator_column(
      tf.feature_column.sequence_categorical_column_with_vocabulary_list(
          key="data", vocabulary_list=vocab, num_oov_buckets=1))

  # Wrap the FC implementation in a tf.function for a fair comparison
  @tf_function()
  def fc_fn(tensors):
    fc.transform_feature(tf.__internal__.feature_column.FeatureTransformationCache(tensors), None)

  # Benchmark runs
  keras_data = {"data": data}
  k_avg_time = fc_bm.run_keras(keras_data, model, batch_size, NUM_REPEATS)

  fc_data = {"data": data.to_sparse()}
  fc_avg_time = fc_bm.run_fc(fc_data, fc_fn, batch_size, NUM_REPEATS)

  return k_avg_time, fc_avg_time


</source>
</class>

<class classid="284" nclones="2" nlines="20" similarity="76">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/benchmarks/normalization_adapt_benchmark.py" startline="54" endline="80" pcid="5153">
  def run_dataset_implementation(self, num_elements, batch_size):
    input_t = keras.Input(shape=(1,))
    layer = normalization.Normalization()
    _ = layer(input_t)

    num_repeats = 5
    starts = []
    ends = []
    for _ in range(num_repeats):
      ds = tf.data.Dataset.range(num_elements)
      ds = ds.map(
          lambda x: tf.expand_dims(tf.cast(x, tf.float32), -1))
      ds = ds.batch(batch_size)

      starts.append(time.time())
      # Benchmarked code begins here.
      k, n, ex, ex2 = ds.reduce((0.0, 0, 0.0, 0.0), reduce_fn)
      mean = k.numpy() + ex.numpy() / n.numpy()
      var = (ex2.numpy() - (ex.numpy() * ex.numpy()) / n.numpy()) / (
          n.numpy() - 1)
      layer.set_weights([mean, var])
      # Benchmarked code ends here.
      ends.append(time.time())

    avg_time = np.mean(np.array(ends) - np.array(starts))
    return avg_time

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/benchmarks/discretization_adapt_benchmark.py" startline="44" endline="69" pcid="5197">
  def run_dataset_implementation(self, num_elements, batch_size):
    input_t = keras.Input(shape=(1,))
    layer = discretization.Discretization()
    _ = layer(input_t)

    num_repeats = 5
    starts = []
    ends = []
    for _ in range(num_repeats):
      ds = tf.data.Dataset.range(num_elements)
      ds = ds.map(
          lambda x: tf.expand_dims(tf.cast(x, tf.float32), -1))
      ds = ds.batch(batch_size)

      starts.append(time.time())
      # Benchmarked code begins here.
      state = ds.reduce((np.zeros((1, 2)),), reduce_fn)

      bins = discretization.get_bucket_boundaries(state, 100)
      layer.set_weights([bins])
      # Benchmarked code ends here.
      ends.append(time.time())

    avg_time = np.mean(np.array(ends) - np.array(starts))
    return avg_time

</source>
</class>

<class classid="285" nclones="2" nlines="25" similarity="100">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/benchmarks/normalization_adapt_benchmark.py" startline="81" endline="113" pcid="5154">
  def bm_adapt_implementation(self, num_elements, batch_size):
    """Test the KPL adapt implementation."""
    input_t = keras.Input(shape=(1,), dtype=tf.float32)
    layer = normalization.Normalization()
    _ = layer(input_t)

    num_repeats = 5
    starts = []
    ends = []
    for _ in range(num_repeats):
      ds = tf.data.Dataset.range(num_elements)
      ds = ds.map(
          lambda x: tf.expand_dims(tf.cast(x, tf.float32), -1))
      ds = ds.batch(batch_size)

      starts.append(time.time())
      # Benchmarked code begins here.
      layer.adapt(ds)
      # Benchmarked code ends here.
      ends.append(time.time())

    avg_time = np.mean(np.array(ends) - np.array(starts))
    name = "normalization_adapt|%s_elements|batch_%s" % (num_elements,
                                                         batch_size)
    baseline = self.run_dataset_implementation(num_elements, batch_size)
    extras = {
        "tf.data implementation baseline": baseline,
        "delta seconds": (baseline - avg_time),
        "delta percent": ((baseline - avg_time) / baseline) * 100
    }
    self.report_benchmark(
        iters=num_repeats, wall_time=avg_time, extras=extras, name=name)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/benchmarks/discretization_adapt_benchmark.py" startline="70" endline="102" pcid="5198">
  def bm_adapt_implementation(self, num_elements, batch_size):
    """Test the KPL adapt implementation."""
    input_t = keras.Input(shape=(1,), dtype=tf.float32)
    layer = discretization.Discretization()
    _ = layer(input_t)

    num_repeats = 5
    starts = []
    ends = []
    for _ in range(num_repeats):
      ds = tf.data.Dataset.range(num_elements)
      ds = ds.map(
          lambda x: tf.expand_dims(tf.cast(x, tf.float32), -1))
      ds = ds.batch(batch_size)

      starts.append(time.time())
      # Benchmarked code begins here.
      layer.adapt(ds)
      # Benchmarked code ends here.
      ends.append(time.time())

    avg_time = np.mean(np.array(ends) - np.array(starts))
    name = "discretization_adapt|%s_elements|batch_%s" % (num_elements,
                                                          batch_size)
    baseline = self.run_dataset_implementation(num_elements, batch_size)
    extras = {
        "tf.data implementation baseline": baseline,
        "delta seconds": (baseline - avg_time),
        "delta percent": ((baseline - avg_time) / baseline) * 100
    }
    self.report_benchmark(
        iters=num_repeats, wall_time=avg_time, extras=extras, name=name)

</source>
</class>

<class classid="286" nclones="2" nlines="19" similarity="78">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/benchmarks/category_crossing_benchmark.py" startline="40" endline="62" pcid="5169">
  def run_dataset_implementation(self, batch_size):
    num_repeats = 5
    starts = []
    ends = []
    for _ in range(num_repeats):
      ds = tf.data.Dataset.from_generator(
          int_gen, (tf.int64, tf.int64),
          (tf.TensorShape([1]), tf.TensorShape([1])))
      ds = ds.shuffle(batch_size * 100)
      ds = ds.batch(batch_size)
      num_batches = 5
      ds = ds.take(num_batches)
      ds = ds.prefetch(num_batches)
      starts.append(time.time())
      # Benchmarked code begins here.
      for i in ds:
        _ = tf.sparse.cross([i[0], i[1]])
      # Benchmarked code ends here.
      ends.append(time.time())

    avg_time = np.mean(np.array(ends) - np.array(starts)) / num_batches
    return avg_time

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/benchmarks/hashing_benchmark.py" startline="42" endline="63" pcid="5177">
  def run_dataset_implementation(self, batch_size):
    num_repeats = 5
    starts = []
    ends = []
    for _ in range(num_repeats):
      ds = tf.data.Dataset.from_generator(word_gen, tf.string,
                                              tf.TensorShape([]))
      ds = ds.shuffle(batch_size * 100)
      ds = ds.batch(batch_size)
      num_batches = 5
      ds = ds.take(num_batches)
      ds = ds.prefetch(num_batches)
      starts.append(time.time())
      # Benchmarked code begins here.
      for i in ds:
        _ = tf.strings.to_hash_bucket(i, num_buckets=2)
      # Benchmarked code ends here.
      ends.append(time.time())

    avg_time = np.mean(np.array(ends) - np.array(starts)) / num_batches
    return avg_time

</source>
</class>

<class classid="287" nclones="2" nlines="29" similarity="73">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/benchmarks/category_crossing_benchmark.py" startline="63" endline="98" pcid="5170">
  def bm_layer_implementation(self, batch_size):
    input_1 = keras.Input(shape=(1,), dtype=tf.int64, name="word")
    input_2 = keras.Input(shape=(1,), dtype=tf.int64, name="int")
    layer = category_crossing.CategoryCrossing()
    _ = layer([input_1, input_2])

    num_repeats = 5
    starts = []
    ends = []
    for _ in range(num_repeats):
      ds = tf.data.Dataset.from_generator(
          int_gen, (tf.int64, tf.int64),
          (tf.TensorShape([1]), tf.TensorShape([1])))
      ds = ds.shuffle(batch_size * 100)
      ds = ds.batch(batch_size)
      num_batches = 5
      ds = ds.take(num_batches)
      ds = ds.prefetch(num_batches)
      starts.append(time.time())
      # Benchmarked code begins here.
      for i in ds:
        _ = layer([i[0], i[1]])
      # Benchmarked code ends here.
      ends.append(time.time())

    avg_time = np.mean(np.array(ends) - np.array(starts)) / num_batches
    name = "category_crossing|batch_%s" % batch_size
    baseline = self.run_dataset_implementation(batch_size)
    extras = {
        "dataset implementation baseline": baseline,
        "delta seconds": (baseline - avg_time),
        "delta percent": ((baseline - avg_time) / baseline) * 100
    }
    self.report_benchmark(
        iters=num_repeats, wall_time=avg_time, extras=extras, name=name)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/benchmarks/hashing_benchmark.py" startline="64" endline="97" pcid="5178">
  def bm_layer_implementation(self, batch_size):
    input_1 = keras.Input(shape=(None,), dtype=tf.string, name="word")
    layer = hashing.Hashing(num_bins=2)
    _ = layer(input_1)

    num_repeats = 5
    starts = []
    ends = []
    for _ in range(num_repeats):
      ds = tf.data.Dataset.from_generator(word_gen, tf.string,
                                              tf.TensorShape([]))
      ds = ds.shuffle(batch_size * 100)
      ds = ds.batch(batch_size)
      num_batches = 5
      ds = ds.take(num_batches)
      ds = ds.prefetch(num_batches)
      starts.append(time.time())
      # Benchmarked code begins here.
      for i in ds:
        _ = layer(i)
      # Benchmarked code ends here.
      ends.append(time.time())

    avg_time = np.mean(np.array(ends) - np.array(starts)) / num_batches
    name = "hashing|batch_%s" % batch_size
    baseline = self.run_dataset_implementation(batch_size)
    extras = {
        "dataset implementation baseline": baseline,
        "delta seconds": (baseline - avg_time),
        "delta percent": ((baseline - avg_time) / baseline) * 100
    }
    self.report_benchmark(
        iters=num_repeats, wall_time=avg_time, extras=extras, name=name)

</source>
</class>

<class classid="288" nclones="3" nlines="10" similarity="75">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/hashing_test.py" startline="164" endline="178" pcid="5222">
  def test_hash_ragged_string_input_farmhash(self):
    layer = hashing.Hashing(num_bins=2)
    inp_data = tf.ragged.constant(
        [['omar', 'stringer', 'marlo', 'wire'], ['marlo', 'skywalker', 'wire']],
        dtype=tf.string)
    out_data = layer(inp_data)
    # Same hashed output as test_hash_sparse_input_farmhash
    expected_output = [[0, 0, 1, 0], [1, 0, 0]]
    self.assertAllEqual(expected_output, out_data)

    inp_t = input_layer.Input(shape=(None,), ragged=True, dtype=tf.string)
    out_t = layer(inp_t)
    model = training.Model(inputs=inp_t, outputs=out_t)
    self.assertAllClose(out_data, model.predict(inp_data))

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/hashing_test.py" startline="195" endline="207" pcid="5224">
  def test_hash_ragged_int_input_farmhash(self):
    layer = hashing.Hashing(num_bins=3)
    inp_data = tf.ragged.constant([[0, 1, 3, 4], [2, 1, 0]], dtype=tf.int64)
    out_data = layer(inp_data)
    # Same hashed output as test_hash_sparse_input_farmhash
    expected_output = [[1, 0, 0, 2], [1, 0, 1]]
    self.assertAllEqual(expected_output, out_data)

    inp_t = input_layer.Input(shape=(None,), ragged=True, dtype=tf.int64)
    out_t = layer(inp_t)
    model = training.Model(inputs=inp_t, outputs=out_t)
    self.assertAllClose(out_data, model.predict(inp_data))

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/hashing_test.py" startline="232" endline="244" pcid="5226">
  def test_hash_ragged_int_input_siphash(self):
    layer = hashing.Hashing(num_bins=3, salt=[133, 137])
    inp_data = tf.ragged.constant([[0, 1, 3, 4], [2, 1, 0]], dtype=tf.int64)
    out_data = layer(inp_data)
    # Same hashed output as test_hash_sparse_input_farmhash
    expected_output = [[1, 1, 0, 1], [2, 1, 1]]
    self.assertAllEqual(expected_output, out_data)

    inp_t = input_layer.Input(shape=(None,), ragged=True, dtype=tf.int64)
    out_t = layer(inp_t)
    model = training.Model(inputs=inp_t, outputs=out_t)
    self.assertAllClose(out_data, model.predict(inp_data))

</source>
</class>

<class classid="289" nclones="6" nlines="14" similarity="70">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/image_preprocessing_test.py" startline="33" endline="47" pcid="5261">
  def _run_test(self, kwargs, expected_height, expected_width):
    np.random.seed(1337)
    num_samples = 2
    orig_height = 5
    orig_width = 8
    channels = 3
    kwargs.update({'height': expected_height, 'width': expected_width})
    with testing_utils.use_gpu():
      testing_utils.layer_test(
          image_preprocessing.Resizing,
          kwargs=kwargs,
          input_shape=(num_samples, orig_height, orig_width, channels),
          expected_output_shape=(None, expected_height, expected_width,
                                 channels))

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/image_preprocessing_test.py" startline="620" endline="633" pcid="5313">
  def _run_test(self, height_factor, width_factor):
    np.random.seed(1337)
    num_samples = 2
    orig_height = 5
    orig_width = 8
    channels = 3
    kwargs = {'height_factor': height_factor, 'width_factor': width_factor}
    with testing_utils.use_gpu():
      testing_utils.layer_test(
          image_preprocessing.RandomTranslation,
          kwargs=kwargs,
          input_shape=(num_samples, orig_height, orig_width, channels),
          expected_output_shape=(None, orig_height, orig_width, channels))

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/image_preprocessing_test.py" startline="1171" endline="1184" pcid="5332">
  def _run_test(self, factor):
    np.random.seed(1337)
    num_samples = 2
    orig_height = 5
    orig_width = 8
    channels = 3
    kwargs = {'factor': factor}
    with testing_utils.use_gpu():
      testing_utils.layer_test(
          image_preprocessing.RandomRotation,
          kwargs=kwargs,
          input_shape=(num_samples, orig_height, orig_width, channels),
          expected_output_shape=(None, orig_height, orig_width, channels))

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/image_preprocessing_test.py" startline="1237" endline="1250" pcid="5338">
  def _run_test(self, height_factor, width_factor):
    np.random.seed(1337)
    num_samples = 2
    orig_height = 5
    orig_width = 8
    channels = 3
    kwargs = {'height_factor': height_factor, 'width_factor': width_factor}
    with testing_utils.use_gpu():
      testing_utils.layer_test(
          image_preprocessing.RandomZoom,
          kwargs=kwargs,
          input_shape=(num_samples, orig_height, orig_width, channels),
          expected_output_shape=(None, orig_height, orig_width, channels))

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/image_preprocessing_test.py" startline="280" endline="294" pcid="5281">
  def _run_test(self, expected_height, expected_width):
    np.random.seed(1337)
    num_samples = 2
    orig_height = 5
    orig_width = 8
    channels = 3
    kwargs = {'height': expected_height, 'width': expected_width}
    with testing_utils.use_gpu():
      testing_utils.layer_test(
          image_preprocessing.RandomCrop,
          kwargs=kwargs,
          input_shape=(num_samples, orig_height, orig_width, channels),
          expected_output_shape=(None, expected_height, expected_width,
                                 channels))

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/image_preprocessing_test.py" startline="209" endline="229" pcid="5274">
  def _run_test(self, expected_height, expected_width):
    np.random.seed(1337)
    num_samples = 2
    orig_height = 5
    orig_width = 8
    channels = 3
    kwargs = {'height': expected_height, 'width': expected_width}
    input_images = np.random.random(
        (num_samples, orig_height, orig_width, channels)).astype(np.float32)
    expected_output = get_numpy_center_crop(input_images, expected_height,
                                            expected_width)
    with testing_utils.use_gpu():
      testing_utils.layer_test(
          image_preprocessing.CenterCrop,
          kwargs=kwargs,
          input_shape=(num_samples, orig_height, orig_width, channels),
          input_data=input_images,
          expected_output=expected_output,
          expected_output_shape=(None, expected_height, expected_width,
                                 channels))

</source>
</class>

<class classid="290" nclones="4" nlines="12" similarity="71">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/image_preprocessing_test.py" startline="86" endline="101" pcid="5264">
  def test_down_sampling_numeric(self):
    for dtype in (np.int64, np.float32):
      with testing_utils.use_gpu():
        input_image = np.reshape(np.arange(0, 16), (1, 4, 4, 1)).astype(dtype)
        layer = image_preprocessing.Resizing(
            height=2, width=2, interpolation='nearest')
        output_image = layer(input_image)
        # pyformat: disable
        expected_output = np.asarray([
            [5, 7],
            [13, 15]
        ]).astype(dtype)
        # pyformat: enable
        expected_output = np.reshape(expected_output, (1, 2, 2, 1))
        self.assertAllEqual(expected_output, output_image)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/image_preprocessing_test.py" startline="102" endline="119" pcid="5265">
  def test_up_sampling_numeric(self):
    for dtype in (np.int64, np.float32):
      with testing_utils.use_gpu():
        input_image = np.reshape(np.arange(0, 4), (1, 2, 2, 1)).astype(dtype)
        layer = image_preprocessing.Resizing(
            height=4, width=4, interpolation='nearest')
        output_image = layer(input_image)
        # pyformat: disable
        expected_output = np.asarray([
            [0, 0, 1, 1],
            [0, 0, 1, 1],
            [2, 2, 3, 3],
            [2, 2, 3, 3]
        ]).astype(dtype)
        # pyformat: enable
        expected_output = np.reshape(expected_output, (1, 4, 4, 1))
        self.assertAllEqual(expected_output, output_image)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/image_preprocessing_test.py" startline="1502" endline="1517" pcid="5360">
  def test_random_width_shorter_numeric(self):
    for dtype in (np.int64, np.float32):
      with testing_utils.use_gpu():
        input_image = np.reshape(np.arange(0, 8), (2, 4, 1)).astype(dtype)
        layer = image_preprocessing.RandomWidth(
            factor=(-.5, -.5), interpolation='nearest')
        output_image = layer(np.expand_dims(input_image, axis=0))
        # pyformat: disable
        expected_output = np.asarray([
            [1, 3],
            [5, 7]
        ]).astype(dtype)
        # pyformat: enable
        expected_output = np.reshape(expected_output, (1, 2, 2, 1))
        self.assertAllEqual(expected_output, output_image)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/image_preprocessing_test.py" startline="1402" endline="1417" pcid="5351">
  def test_random_height_shorter_numeric(self):
    for dtype in (np.int64, np.float32):
      with testing_utils.use_gpu():
        input_image = np.reshape(np.arange(0, 8), (4, 2, 1)).astype(dtype)
        layer = image_preprocessing.RandomHeight(
            factor=(-.5, -.5), interpolation='nearest')
        output_image = layer(np.expand_dims(input_image, axis=0))
        # pyformat: disable
        expected_output = np.asarray([
            [2, 3],
            [6, 7]
        ]).astype(dtype)
        # pyformat: enable
        expected_output = np.reshape(expected_output, (1, 2, 2, 1))
        self.assertAllEqual(expected_output, output_image)

</source>
</class>

<class classid="291" nclones="2" nlines="11" similarity="90">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/image_preprocessing_test.py" startline="150" endline="161" pcid="5270">
  def test_unbatched_image(self):
    with testing_utils.use_gpu():
      input_image = np.reshape(np.arange(0, 16), (4, 4, 1)).astype('float32')
      layer = image_preprocessing.Resizing(2, 2, interpolation='nearest')
      output_image = layer(input_image)
      expected_output = np.asarray([
          [5, 7],
          [13, 15],
      ]).astype('float32')
      expected_output = np.reshape(expected_output, (2, 2, 1))
      self.assertAllEqual(expected_output, output_image)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/image_preprocessing_test.py" startline="264" endline="276" pcid="5280">
  def test_unbatched_image(self):
    with testing_utils.use_gpu():
      input_image = np.reshape(np.arange(0, 16), (4, 4, 1)).astype('float32')
      layer = image_preprocessing.CenterCrop(2, 2)
      output_image = layer(input_image)
      expected_output = np.asarray([
          [5, 6],
          [9, 10],
      ]).astype('float32')
      expected_output = np.reshape(expected_output, (2, 2, 1))
      self.assertAllEqual(expected_output, output_image)


</source>
</class>

<class classid="292" nclones="2" nlines="11" similarity="100">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/image_preprocessing_test.py" startline="245" endline="257" pcid="5278">
  def test_input_smaller_than_crop_box(self):
    np.random.seed(1337)
    height, width = 10, 8
    inp = np.random.random((12, 3, 3, 3))
    with testing_utils.use_gpu():
      layer = image_preprocessing.CenterCrop(height, width)
      actual_output = layer(inp)
      # In this case, output should equal resizing with crop_to_aspect ratio.
      resize_layer = image_preprocessing.Resizing(
          height, width, crop_to_aspect_ratio=True)
      expected_output = resize_layer(inp)
      self.assertAllEqual(expected_output, actual_output)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/image_preprocessing_test.py" startline="295" endline="307" pcid="5282">
  def test_input_smaller_than_crop_box(self):
    np.random.seed(1337)
    height, width = 10, 8
    inp = np.random.random((12, 3, 3, 3))
    with testing_utils.use_gpu():
      layer = image_preprocessing.RandomCrop(height, width)
      actual_output = layer(inp)
      # In this case, output should equal resizing with crop_to_aspect ratio.
      resize_layer = image_preprocessing.Resizing(
          height, width, crop_to_aspect_ratio=True)
      expected_output = resize_layer(inp)
      self.assertAllEqual(expected_output, actual_output)

</source>
</class>

<class classid="293" nclones="2" nlines="10" similarity="90">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/image_preprocessing_test.py" startline="344" endline="354" pcid="5288">
  def test_predicting_with_mock_longer_height(self):
    np.random.seed(1337)
    height, width = 3, 3
    inp = np.random.random((12, 10, 6, 3))
    with testing_utils.use_gpu():
      layer = image_preprocessing.RandomCrop(height, width)
      actual_output = layer(inp, training=False)
      resized_inp = tf.image.resize(inp, size=[5, 3])
      expected_output = resized_inp[:, 1:4, :, :]
      self.assertAllClose(expected_output, actual_output)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/image_preprocessing_test.py" startline="355" endline="365" pcid="5289">
  def test_predicting_with_mock_longer_width(self):
    np.random.seed(1337)
    height, width = 4, 6
    inp = np.random.random((12, 8, 16, 3))
    with testing_utils.use_gpu():
      layer = image_preprocessing.RandomCrop(height, width)
      actual_output = layer(inp, training=False)
      resized_inp = tf.image.resize(inp, size=[4, 8])
      expected_output = resized_inp[:, :, 1:7, :]
      self.assertAllClose(expected_output, actual_output)

</source>
</class>

<class classid="294" nclones="12" nlines="16" similarity="70">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/image_preprocessing_test.py" startline="640" endline="657" pcid="5315">
  def test_random_translation_up_numeric_reflect(self):
    for dtype in (np.int64, np.float32):
      with testing_utils.use_gpu():
        input_image = np.reshape(np.arange(0, 25), (1, 5, 5, 1)).astype(dtype)
        # Shifting by -.2 * 5 = 1 pixel.
        layer = image_preprocessing.RandomTranslation(
            height_factor=(-.2, -.2), width_factor=0.)
        output_image = layer(input_image)
        expected_output = np.asarray([
            [5, 6, 7, 8, 9],
            [10, 11, 12, 13, 14],
            [15, 16, 17, 18, 19],
            [20, 21, 22, 23, 24],
            [20, 21, 22, 23, 24],
        ]).astype(dtype)
        expected_output = np.reshape(expected_output, (1, 5, 5, 1))
        self.assertAllEqual(expected_output, output_image)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/image_preprocessing_test.py" startline="753" endline="770" pcid="5321">
  def test_random_translation_left_numeric_constant(self):
    for dtype in (np.int64, np.float32):
      with testing_utils.use_gpu():
        input_image = np.reshape(np.arange(0, 25), (1, 5, 5, 1)).astype(dtype)
        # Shifting by -.2 * 5 = 1 pixel.
        layer = image_preprocessing.RandomTranslation(
            height_factor=0., width_factor=(-.2, -.2), fill_mode='constant')
        output_image = layer(input_image)
        expected_output = np.asarray([
            [1, 2, 3, 4, 0],
            [6, 7, 8, 9, 0],
            [11, 12, 13, 14, 0],
            [16, 17, 18, 19, 0],
            [21, 22, 23, 24, 0],
        ]).astype(dtype)
        expected_output = np.reshape(expected_output, (1, 5, 5, 1))
        self.assertAllEqual(expected_output, output_image)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/image_preprocessing_test.py" startline="717" endline="734" pcid="5319">
  def test_random_translation_down_numeric_constant(self):
    for dtype in (np.int64, np.float32):
      with testing_utils.use_gpu():
        input_image = np.reshape(np.arange(0, 25), (1, 5, 5, 1)).astype(dtype)
        # Shifting by -.2 * 5 = 1 pixel.
        layer = image_preprocessing.RandomTranslation(
            height_factor=(.2, .2), width_factor=0., fill_mode='constant')
        output_image = layer(input_image)
        expected_output = np.asarray([
            [0, 0, 0, 0, 0],
            [0, 1, 2, 3, 4],
            [5, 6, 7, 8, 9],
            [10, 11, 12, 13, 14],
            [15, 16, 17, 18, 19],
        ]).astype(dtype)
        expected_output = np.reshape(expected_output, (1, 5, 5, 1))
        self.assertAllEqual(expected_output, output_image)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/image_preprocessing_test.py" startline="658" endline="675" pcid="5316">
  def test_random_translation_up_numeric_constant(self):
    for dtype in (np.int64, np.float32):
      with testing_utils.use_gpu():
        input_image = np.reshape(np.arange(0, 25), (1, 5, 5, 1)).astype(dtype)
        # Shifting by -.2 * 5 = 1 pixel.
        layer = image_preprocessing.RandomTranslation(
            height_factor=(-.2, -.2), width_factor=0., fill_mode='constant')
        output_image = layer(input_image)
        expected_output = np.asarray([
            [5, 6, 7, 8, 9],
            [10, 11, 12, 13, 14],
            [15, 16, 17, 18, 19],
            [20, 21, 22, 23, 24],
            [0, 0, 0, 0, 0],
        ]).astype(dtype)
        expected_output = np.reshape(expected_output, (1, 5, 5, 1))
        self.assertAllEqual(expected_output, output_image)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/image_preprocessing_test.py" startline="1280" endline="1297" pcid="5342">
  def test_random_zoom_out_numeric(self):
    for dtype in (np.int64, np.float32):
      with testing_utils.use_gpu():
        input_image = np.reshape(np.arange(0, 25), (5, 5, 1)).astype(dtype)
        layer = image_preprocessing.RandomZoom((.5, .5), (.8, .8),
                                               fill_mode='constant',
                                               interpolation='nearest')
        output_image = layer(np.expand_dims(input_image, axis=0))
        expected_output = np.asarray([
            [0, 0, 0, 0, 0],
            [0, 5, 7, 9, 0],
            [0, 10, 12, 14, 0],
            [0, 20, 22, 24, 0],
            [0, 0, 0, 0, 0],
        ]).astype(dtype)
        expected_output = np.reshape(expected_output, (1, 5, 5, 1))
        self.assertAllEqual(expected_output, output_image)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/image_preprocessing_test.py" startline="676" endline="693" pcid="5317">
  def test_random_translation_down_numeric_reflect(self):
    for dtype in (np.int64, np.float32):
      with testing_utils.use_gpu():
        input_image = np.reshape(np.arange(0, 25), (1, 5, 5, 1)).astype(dtype)
        # Shifting by .2 * 5 = 1 pixel.
        layer = image_preprocessing.RandomTranslation(
            height_factor=(.2, .2), width_factor=0.)
        output_image = layer(input_image)
        expected_output = np.asarray([
            [0, 1, 2, 3, 4],
            [0, 1, 2, 3, 4],
            [5, 6, 7, 8, 9],
            [10, 11, 12, 13, 14],
            [15, 16, 17, 18, 19],
        ]).astype(dtype)
        expected_output = np.reshape(expected_output, (1, 5, 5, 1))
        self.assertAllEqual(expected_output, output_image)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/image_preprocessing_test.py" startline="786" endline="803" pcid="5324">
  def test_unbatched_image(self):
    with testing_utils.use_gpu():
      input_image = np.reshape(np.arange(0, 25), (5, 5, 1)).astype(np.int64)
      # Shifting by -.2 * 5 = 1 pixel.
      layer = image_preprocessing.RandomTranslation(
          height_factor=(-.2, -.2), width_factor=0.)
      output_image = layer(input_image)
      expected_output = np.asarray([
          [5, 6, 7, 8, 9],
          [10, 11, 12, 13, 14],
          [15, 16, 17, 18, 19],
          [20, 21, 22, 23, 24],
          [20, 21, 22, 23, 24],
      ]).astype(np.int64)
      expected_output = np.reshape(expected_output, (5, 5, 1))
      self.assertAllEqual(expected_output, output_image)


</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/image_preprocessing_test.py" startline="1263" endline="1279" pcid="5341">
  def test_random_zoom_in_numeric(self):
    for dtype in (np.int64, np.float32):
      with testing_utils.use_gpu():
        input_image = np.reshape(np.arange(0, 25), (5, 5, 1)).astype(dtype)
        layer = image_preprocessing.RandomZoom((-.5, -.5), (-.5, -.5),
                                               interpolation='nearest')
        output_image = layer(np.expand_dims(input_image, axis=0))
        expected_output = np.asarray([
            [6, 7, 7, 8, 8],
            [11, 12, 12, 13, 13],
            [11, 12, 12, 13, 13],
            [16, 17, 17, 18, 18],
            [16, 17, 17, 18, 18],
        ]).astype(dtype)
        expected_output = np.reshape(expected_output, (1, 5, 5, 1))
        self.assertAllEqual(expected_output, output_image)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/image_preprocessing_test.py" startline="1298" endline="1315" pcid="5343">
  def test_random_zoom_out_numeric_preserve_aspect_ratio(self):
    for dtype in (np.int64, np.float32):
      with testing_utils.use_gpu():
        input_image = np.reshape(np.arange(0, 25), (5, 5, 1)).astype(dtype)
        layer = image_preprocessing.RandomZoom((.5, .5),
                                               fill_mode='constant',
                                               interpolation='nearest')
        output_image = layer(np.expand_dims(input_image, axis=0))
        expected_output = np.asarray([
            [0, 0, 0, 0, 0],
            [0, 6, 7, 9, 0],
            [0, 11, 12, 14, 0],
            [0, 21, 22, 24, 0],
            [0, 0, 0, 0, 0],
        ]).astype(dtype)
        expected_output = np.reshape(expected_output, (1, 5, 5, 1))
        self.assertAllEqual(expected_output, output_image)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/image_preprocessing_test.py" startline="735" endline="752" pcid="5320">
  def test_random_translation_left_numeric_reflect(self):
    for dtype in (np.int64, np.float32):
      with testing_utils.use_gpu():
        input_image = np.reshape(np.arange(0, 25), (1, 5, 5, 1)).astype(dtype)
        # Shifting by .2 * 5 = 1 pixel.
        layer = image_preprocessing.RandomTranslation(
            height_factor=0., width_factor=(-.2, -.2))
        output_image = layer(input_image)
        expected_output = np.asarray([
            [1, 2, 3, 4, 4],
            [6, 7, 8, 9, 9],
            [11, 12, 13, 14, 14],
            [16, 17, 18, 19, 19],
            [21, 22, 23, 24, 24],
        ]).astype(dtype)
        expected_output = np.reshape(expected_output, (1, 5, 5, 1))
        self.assertAllEqual(expected_output, output_image)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/image_preprocessing_test.py" startline="1217" endline="1233" pcid="5337">
  def test_unbatched_image(self):
    with testing_utils.use_gpu():
      input_image = np.reshape(np.arange(0, 25), (5, 5, 1)).astype(np.float32)
      # 180 rotation.
      layer = image_preprocessing.RandomRotation(factor=(0.5, 0.5))
      output_image = layer(input_image)
      expected_output = np.asarray([
          [24, 23, 22, 21, 20],
          [19, 18, 17, 16, 15],
          [14, 13, 12, 11, 10],
          [9, 8, 7, 6, 5],
          [4, 3, 2, 1, 0],
      ]).astype(np.float32)
      expected_output = np.reshape(expected_output, (5, 5, 1))
      self.assertAllClose(expected_output, output_image)


</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/image_preprocessing_test.py" startline="1331" endline="1347" pcid="5346">
  def test_unbatched_image(self):
    with testing_utils.use_gpu():
      input_image = np.reshape(np.arange(0, 25), (5, 5, 1)).astype(np.int64)
      layer = image_preprocessing.RandomZoom((-.5, -.5), (-.5, -.5),
                                             interpolation='nearest')
      output_image = layer(input_image)
      expected_output = np.asarray([
          [6, 7, 7, 8, 8],
          [11, 12, 12, 13, 13],
          [11, 12, 12, 13, 13],
          [16, 17, 17, 18, 18],
          [16, 17, 17, 18, 18],
      ]).astype(np.int64)
      expected_output = np.reshape(expected_output, (5, 5, 1))
      self.assertAllEqual(expected_output, output_image)


</source>
</class>

<class classid="295" nclones="5" nlines="37" similarity="76">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/image_preprocessing_test.py" startline="823" endline="878" pcid="5326">
  def test_random_translation_reflect(self):
    # reflected output is (dcba|abcd|dcba)

    # Test down shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[0., 1., 2.],
         [0., 1., 2.],
         [3., 4., 5.],
         [6., 7., 8],
         [9., 10., 11]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., 0., 0., 1., -1., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'reflect')

    # Test up shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[3., 4., 5.],
         [6., 7., 8],
         [9., 10., 11.],
         [12., 13., 14.],
         [12., 13., 14.]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., 0., 0., 1., 1., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'reflect')

    # Test left shift by 1.
    # reflected output is (dcba|abcd|dcba)
    # pyformat: disable
    expected_output = np.asarray(
        [[1., 2., 2.],
         [4., 5., 5.],
         [7., 8., 8.],
         [10., 11., 11.],
         [13., 14., 14.]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., 1., 0., 1., 0., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'reflect')

    # Test right shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[0., 0., 1.],
         [3., 3., 4],
         [6., 6., 7.],
         [9., 9., 10.],
         [12., 12., 13.]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., -1., 0., 1., 0., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'reflect')

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/image_preprocessing_test.py" startline="989" endline="1043" pcid="5329">
  def test_random_translation_constant_0(self):
    # constant output is (0000|abcd|0000)

    # Test down shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[0., 0., 0.],
         [0., 1., 2.],
         [3., 4., 5.],
         [6., 7., 8],
         [9., 10., 11]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., 0., 0., 1., -1., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'constant')

    # Test up shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[3., 4., 5.],
         [6., 7., 8],
         [9., 10., 11.],
         [12., 13., 14.],
         [0., 0., 0.]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., 0., 0., 1., 1., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'constant')

    # Test left shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[1., 2., 0.],
         [4., 5., 0.],
         [7., 8., 0.],
         [10., 11., 0.],
         [13., 14., 0.]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., 1., 0., 1., 0., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'constant')

    # Test right shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[0., 0., 1.],
         [0., 3., 4],
         [0., 6., 7.],
         [0., 9., 10.],
         [0., 12., 13.]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., -1., 0., 1., 0., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'constant')

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/image_preprocessing_test.py" startline="879" endline="933" pcid="5327">
  def test_random_translation_wrap(self):
    # warpped output is (abcd|abcd|abcd)

    # Test down shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[12., 13., 14.],
         [0., 1., 2.],
         [3., 4., 5.],
         [6., 7., 8],
         [9., 10., 11]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., 0., 0., 1., -1., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'wrap')

    # Test up shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[3., 4., 5.],
         [6., 7., 8],
         [9., 10., 11.],
         [12., 13., 14.],
         [0., 1., 2.]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., 0., 0., 1., 1., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'wrap')

    # Test left shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[1., 2., 0.],
         [4., 5., 3.],
         [7., 8., 6.],
         [10., 11., 9.],
         [13., 14., 12.]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., 1., 0., 1., 0., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'wrap')

    # Test right shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[2., 0., 1.],
         [5., 3., 4],
         [8., 6., 7.],
         [11., 9., 10.],
         [14., 12., 13.]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., -1., 0., 1., 0., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'wrap')

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/image_preprocessing_test.py" startline="1044" endline="1099" pcid="5330">
  def test_random_translation_constant_1(self):
    with tf.compat.forward_compatibility_horizon(2020, 8, 6):
      # constant output is (1111|abcd|1111)

      # Test down shift by 1.
      # pyformat: disable
      expected_output = np.asarray(
          [[1., 1., 1.],
           [0., 1., 2.],
           [3., 4., 5.],
           [6., 7., 8],
           [9., 10., 11]]).reshape((1, 5, 3, 1)).astype(np.float32)
      # pyformat: enable
      transform_matrix = np.asarray([[1., 0., 0., 0., 1., -1., 0., 0.]])
      self._run_random_transform_with_mock(
          transform_matrix, expected_output, 'constant', fill_value=1.0)

      # Test up shift by 1.
      # pyformat: disable
      expected_output = np.asarray(
          [[3., 4., 5.],
           [6., 7., 8],
           [9., 10., 11.],
           [12., 13., 14.],
           [1., 1., 1.]]).reshape((1, 5, 3, 1)).astype(np.float32)
      # pyformat: enable
      transform_matrix = np.asarray([[1., 0., 0., 0., 1., 1., 0., 0.]])
      self._run_random_transform_with_mock(
          transform_matrix, expected_output, 'constant', fill_value=1.0)

      # Test left shift by 1.
      # pyformat: disable
      expected_output = np.asarray(
          [[1., 2., 1.],
           [4., 5., 1.],
           [7., 8., 1.],
           [10., 11., 1.],
           [13., 14., 1.]]).reshape((1, 5, 3, 1)).astype(np.float32)
      # pyformat: enable
      transform_matrix = np.asarray([[1., 0., 1., 0., 1., 0., 0., 0.]])
      self._run_random_transform_with_mock(
          transform_matrix, expected_output, 'constant', fill_value=1.0)

      # Test right shift by 1.
      # pyformat: disable
      expected_output = np.asarray(
          [[1., 0., 1.],
           [1., 3., 4],
           [1., 6., 7.],
           [1., 9., 10.],
           [1., 12., 13.]]).reshape((1, 5, 3, 1)).astype(np.float32)
      # pyformat: enable
      transform_matrix = np.asarray([[1., 0., -1., 0., 1., 0., 0., 0.]])
      self._run_random_transform_with_mock(
          transform_matrix, expected_output, 'constant', fill_value=1.0)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/image_preprocessing_test.py" startline="934" endline="988" pcid="5328">
  def test_random_translation_nearest(self):
    # nearest output is (aaaa|abcd|dddd)

    # Test down shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[0., 1., 2.],
         [0., 1., 2.],
         [3., 4., 5.],
         [6., 7., 8],
         [9., 10., 11]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., 0., 0., 1., -1., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'nearest')

    # Test up shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[3., 4., 5.],
         [6., 7., 8],
         [9., 10., 11.],
         [12., 13., 14.],
         [12., 13., 14.]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., 0., 0., 1., 1., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'nearest')

    # Test left shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[1., 2., 2.],
         [4., 5., 5.],
         [7., 8., 8.],
         [10., 11., 11.],
         [13., 14., 14.]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., 1., 0., 1., 0., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'nearest')

    # Test right shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[0., 0., 1.],
         [3., 3., 4],
         [6., 6., 7.],
         [9., 9., 10.],
         [12., 12., 13.]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., -1., 0., 1., 0., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'nearest')

</source>
</class>

<class classid="296" nclones="2" nlines="13" similarity="100">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/image_preprocessing_test.py" startline="1351" endline="1364" pcid="5347">
  def _run_test(self, factor):
    np.random.seed(1337)
    num_samples = 2
    orig_height = 5
    orig_width = 8
    channels = 3
    with testing_utils.use_gpu():
      img = np.random.random((num_samples, orig_height, orig_width, channels))
      layer = image_preprocessing.RandomHeight(factor)
      img_out = layer(img, training=True)
      self.assertEqual(img_out.shape[0], 2)
      self.assertEqual(img_out.shape[2], 8)
      self.assertEqual(img_out.shape[3], 3)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/image_preprocessing_test.py" startline="1452" endline="1465" pcid="5356">
  def _run_test(self, factor):
    np.random.seed(1337)
    num_samples = 2
    orig_height = 5
    orig_width = 8
    channels = 3
    with testing_utils.use_gpu():
      img = np.random.random((num_samples, orig_height, orig_width, channels))
      layer = image_preprocessing.RandomWidth(factor)
      img_out = layer(img, training=True)
      self.assertEqual(img_out.shape[0], 2)
      self.assertEqual(img_out.shape[1], 5)
      self.assertEqual(img_out.shape[3], 3)

</source>
</class>

<class classid="297" nclones="2" nlines="14" similarity="71">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/image_preprocessing_test.py" startline="1382" endline="1401" pcid="5350">
  def test_random_height_longer_numeric(self):
    for dtype in (np.int64, np.float32):
      with testing_utils.use_gpu():
        input_image = np.reshape(np.arange(0, 6), (2, 3, 1)).astype(dtype)
        layer = image_preprocessing.RandomHeight(factor=(1., 1.))
        # Return type of RandomHeight() is float32 if `interpolation` is not
        # set to `ResizeMethod.NEAREST_NEIGHBOR`; cast `layer` to desired dtype.
        output_image = tf.cast(
            layer(np.expand_dims(input_image, axis=0)), dtype=dtype)
        # pyformat: disable
        expected_output = np.asarray([
            [0, 1, 2],
            [0.75, 1.75, 2.75],
            [2.25, 3.25, 4.25],
            [3, 4, 5]
        ]).astype(dtype)
        # pyformat: enable
        expected_output = np.reshape(expected_output, (1, 4, 3, 1))
        self.assertAllEqual(expected_output, output_image)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/image_preprocessing_test.py" startline="1483" endline="1501" pcid="5359">
  def test_random_width_longer_numeric(self):
    for dtype in (np.int64, np.float32):
      with testing_utils.use_gpu():
        input_image = np.reshape(np.arange(0, 6), (3, 2, 1)).astype(dtype)
        layer = image_preprocessing.RandomWidth(factor=(1., 1.))
        # Return type of RandomWidth() is float32 if `interpolation` is not
        # set to `ResizeMethod.NEAREST_NEIGHBOR`; cast `layer` to desired dtype.
        output_image = tf.cast(
            layer(np.expand_dims(input_image, axis=0)), dtype=dtype)
        # pyformat: disable
        expected_output = np.asarray([
            [0, 0.25, 0.75, 1],
            [2, 2.25, 2.75, 3],
            [4, 4.25, 4.75, 5]
        ]).astype(dtype)
        # pyformat: enable
        expected_output = np.reshape(expected_output, (1, 3, 4, 1))
        self.assertAllEqual(expected_output, output_image)

</source>
</class>

<class classid="298" nclones="2" nlines="11" similarity="83">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/image_preprocessing_test.py" startline="1552" endline="1564" pcid="5365">
  def test_plain_call(self):
    layer = image_preprocessing.RandomWidth(.5, seed=123)
    shape = (12, 12, 3)
    img = np.random.random((12,) + shape)
    out = layer(img)  # Default to training=True
    self.assertNotEqual(tuple(int(i) for i in out.shape[1:]), shape)

    out = layer(img, training=True)
    self.assertNotEqual(tuple(int(i) for i in out.shape[1:]), shape)

    out = layer(img, training=False)
    self.assertEqual(tuple(int(i) for i in out.shape[1:]), shape)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/image_preprocessing_test.py" startline="1565" endline="1581" pcid="5366">
  def test_call_in_container(self):
    layer1 = image_preprocessing.RandomWidth(.5, seed=123)
    layer2 = image_preprocessing.RandomHeight(.5, seed=123)
    seq = sequential.Sequential([layer1, layer2])

    shape = (12, 12, 3)
    img = np.random.random((12,) + shape)
    out = seq(img)  # Default to training=True
    self.assertNotEqual(tuple(int(i) for i in out.shape[1:]), shape)

    out = seq(img, training=True)
    self.assertNotEqual(tuple(int(i) for i in out.shape[1:]), shape)

    out = seq(img, training=False)
    self.assertEqual(tuple(int(i) for i in out.shape[1:]), shape)


</source>
</class>

<class classid="299" nclones="4" nlines="17" similarity="100">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="426" endline="444" pcid="5374">
  def test_ragged_int_input(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.ragged.constant([[10, 11, 13], [13, 12, 10, 42]],
                                              dtype=np.int64)
    expected_output = [[2, 3, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        dtype=tf.int64,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="642" endline="660" pcid="5384">
  def test_ragged_int_input(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.ragged.constant([[10, 11, 13], [13, 12, 10, 42]],
                                              dtype=np.int64)
    expected_output = [[2, 3, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        dtype=tf.int64,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="445" endline="464" pcid="5375">
  def test_int32_input_with_int64_keys(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.ragged.constant([[10, 11, 13], [13, 12, 10, 42]],
                                              dtype=np.int32)
    expected_output = [[2, 3, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.int32, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        dtype=tf.int64,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)


</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="540" endline="559" pcid="5379">
  def test_ragged_int_input_multi_bucket(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.ragged.constant([[10, 11, 13], [13, 12, 10, 133]],
                                              dtype=np.int64)
    expected_output = [[3, 4, 6], [6, 5, 3, 2]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        dtype=tf.int64,
        num_oov_indices=2,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)


</source>
</class>

<class classid="300" nclones="2" nlines="14" similarity="73">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="565" endline="581" pcid="5380">
  def test_sparse_adapt(self):
    vocab_data = tf.SparseTensor(
        indices=[[0, 0], [0, 1], [1, 2]],
        values=["michigan", "fire", "michigan"],
        dense_shape=[3, 4])
    vocab_dataset = tf.data.Dataset.from_tensors(vocab_data)

    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    layer.adapt(vocab_dataset)
    expected_vocabulary = ["", "[OOV]", "michigan", "fire"]
    self.assertAllEqual(expected_vocabulary, layer.get_vocabulary())

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="582" endline="596" pcid="5381">
  def test_ragged_adapt(self):
    vocab_data = tf.ragged.constant([["michigan"],
                                              ["fire", "michigan"]])
    vocab_dataset = tf.data.Dataset.from_tensors(vocab_data)

    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    layer.adapt(vocab_dataset)
    expected_vocabulary = ["", "[OOV]", "michigan", "fire"]
    self.assertAllEqual(expected_vocabulary, layer.get_vocabulary())

</source>
</class>

<class classid="301" nclones="2" nlines="11" similarity="83">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="748" endline="758" pcid="5391">
  def test_int_output_shape(self):
    input_data = keras.Input(batch_size=16, shape=(4,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=2,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    int_data = layer(input_data)
    self.assertAllEqual(int_data.shape.as_list(), [16, 4])

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="946" endline="958" pcid="5400">
  def test_one_hot_output_shape(self):
    inputs = keras.Input(batch_size=16, shape=(1,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=["earth"],
        max_tokens=2,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        output_mode=index_lookup.ONE_HOT,
        dtype=tf.string)
    outputs = layer(inputs)
    self.assertAllEqual(outputs.shape.as_list(), [16, 2])

</source>
</class>

<class classid="302" nclones="30" nlines="10" similarity="70">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1123" endline="1133" pcid="5406">
    def compute(data):
      layer = index_lookup.IndexLookup(
          vocabulary=vocab_file,
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          output_mode=index_lookup.MULTI_HOT,
          dtype=tf.string)
      return layer(data)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1760" endline="1771" pcid="5448">
  def test_non_unique_vocab_fails(self):
    vocab_data = ["earth", "wind", "and", "fire", "fire"]
    with self.assertRaisesRegex(ValueError, ".*repeated term.*fire.*"):
      _ = index_lookup.IndexLookup(
          vocabulary=vocab_data,
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          dtype=tf.string,
          invert=True)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1548" endline="1558" pcid="5432">
  def test_vocab_with_reserved_mask_element_fails(self):
    vocab_data = ["earth", "mask_token", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="mask_token",
        oov_token="[OOV]",
        dtype=tf.string)
    with self.assertRaisesRegex(ValueError, ".*Reserved mask.*"):
      layer.set_vocabulary(vocab_data)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1537" endline="1547" pcid="5431">
  def test_vocab_with_reserved_oov_element_fails(self):
    vocab_data = ["earth", "test", "[OOV]", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    with self.assertRaisesRegex(ValueError, ".*Reserved OOV.*"):
      layer.set_vocabulary(vocab_data)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1772" endline="1782" pcid="5449">
  def test_non_int_output_fails(self):
    with self.assertRaisesRegex(ValueError, "`output_mode` must be `'int'`"):
      _ = index_lookup.IndexLookup(
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          dtype=tf.string,
          output_mode=index_lookup.COUNT,
          invert=True)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1482" endline="1492" pcid="5426">
  def test_non_unique_vocab_fails(self):
    vocab_data = ["earth", "wind", "and", "fire", "fire"]
    with self.assertRaisesRegex(ValueError, ".*repeated term.*fire.*"):
      _ = index_lookup.IndexLookup(
          vocabulary=vocab_data,
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          dtype=tf.string)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1493" endline="1503" pcid="5427">
  def test_vocab_with_oov_and_wrong_mask_fails(self):
    vocab_data = ["custom_mask", "[OOV]", "earth", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    with self.assertRaisesRegex(ValueError, ".*does not have the mask token.*"):
      layer.set_vocabulary(vocab_data)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1526" endline="1536" pcid="5430">
  def test_vocab_with_repeated_element_fails(self):
    vocab_data = ["earth", "earth", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    with self.assertRaisesRegex(ValueError, ".*repeated term.*earth.*"):
      layer.set_vocabulary(vocab_data)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1504" endline="1514" pcid="5428">
  def test_vocab_with_oov_and_no_mask_fails(self):
    vocab_data = ["[OOV]", "earth", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    with self.assertRaisesRegex(ValueError, ".*Reserved OOV.*"):
      layer.set_vocabulary(vocab_data)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1515" endline="1525" pcid="5429">
  def test_vocab_with_mask_but_no_oov_fails(self):
    vocab_data = ["", "earth", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    with self.assertRaisesRegex(ValueError, ".*does not have the OOV token.*"):
      layer.set_vocabulary(vocab_data)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1795" endline="1806" pcid="5451">
  def test_vocab_with_reserved_mask_element_fails(self):
    vocab_data = ["earth", "mask_token", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="mask_token",
        oov_token="[OOV]",
        dtype=tf.string,
        invert=True)
    with self.assertRaisesRegex(ValueError, ".*Reserved mask.*"):
      layer.set_vocabulary(vocab_data)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1783" endline="1794" pcid="5450">
  def test_vocab_with_repeated_element_fails(self):
    vocab_data = ["earth", "earth", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string,
        invert=True)
    with self.assertRaisesRegex(ValueError, ".*repeated term.*earth.*"):
      layer.set_vocabulary(vocab_data)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1807" endline="1818" pcid="5452">
  def test_non_unique_int_vocab_fails(self):
    vocab_data = [12, 13, 14, 15, 15]
    with self.assertRaisesRegex(ValueError, ".*repeated term.*15.*"):
      _ = index_lookup.IndexLookup(
          vocabulary=vocab_data,
          max_tokens=None,
          num_oov_indices=1,
          mask_token=0,
          oov_token=-1,
          dtype=tf.int64,
          invert=True)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1632" endline="1642" pcid="5438">
  def test_int_vocab_with_oov_and_wrong_mask_fails(self):
    vocab_data = [1234, -1, 11, 21, 13, 14]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1,
        dtype=tf.int64)
    with self.assertRaisesRegex(ValueError, "does not have the mask token `0`"):
      layer.set_vocabulary(vocab_data)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1665" endline="1675" pcid="5441">
  def test_int_vocab_with_repeated_element_fails(self):
    vocab_data = [11, 11, 34, 23, 124]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1,
        dtype=tf.int64)
    with self.assertRaisesRegex(ValueError, "repeated term.*11"):
      layer.set_vocabulary(vocab_data)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1643" endline="1653" pcid="5439">
  def test_int_vocab_with_oov_and_no_mask_fails(self):
    vocab_data = [-1, 11, 12, 13, 14]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1,
        dtype=tf.int64)
    with self.assertRaisesRegex(ValueError, "Reserved OOV"):
      layer.set_vocabulary(vocab_data)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1676" endline="1686" pcid="5442">
  def test_int_vocab_with_reserved_oov_element_fails(self):
    vocab_data = [14, 38, -1, 34, 3, 84]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1,
        dtype=tf.int64)
    with self.assertRaisesRegex(ValueError, "Reserved OOV"):
      layer.set_vocabulary(vocab_data)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1687" endline="1697" pcid="5443">
  def test_int_vocab_with_reserved_mask_element_fails(self):
    vocab_data = [125, 0, 3, 4, 94]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1,
        dtype=tf.int64)
    with self.assertRaisesRegex(ValueError, "Reserved mask"):
      layer.set_vocabulary(vocab_data)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1458" endline="1469" pcid="5424">
  def test_vocab_with_multiple_oov_indices(self):
    vocab_data = ["", "[OOV]", "[OOV]", "[OOV]", "wind"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=3,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(vocab_data, returned_vocab)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1407" endline="1418" pcid="5420">
  def test_vocab_multi_oov(self):
    vocab_data = ["", "[OOV]", "[OOV]", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=2,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(returned_vocab, vocab_data)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1654" endline="1664" pcid="5440">
  def test_int_vocab_with_mask_but_no_oov_fails(self):
    vocab_data = [0, 11, 12, 13, 14]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1,
        dtype=tf.int64)
    with self.assertRaisesRegex(ValueError, "does not have the OOV token `-1`"):
      layer.set_vocabulary(vocab_data)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1621" endline="1631" pcid="5437">
  def test_non_unique_int_vocab_fails(self):
    vocab_data = [12, 13, 14, 15, 15]
    with self.assertRaisesRegex(ValueError, "repeated term.*15"):
      _ = index_lookup.IndexLookup(
          vocabulary=vocab_data,
          max_tokens=None,
          num_oov_indices=1,
          mask_token=0,
          oov_token=-1,
          dtype=tf.int64)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1734" endline="1746" pcid="5446">
  def test_vocab_with_max_cap(self):
    vocab_data = ["", "[OOV]", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=5,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string,
        invert=True)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(vocab_data, returned_vocab)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1819" endline="1831" pcid="5453">
  def test_int_vocab_with_repeated_element_fails(self):
    vocab_data = [11, 11, 34, 23, 124]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1,
        dtype=tf.int64,
        invert=True)
    with self.assertRaisesRegex(ValueError, ".*repeated term.*11.*"):
      layer.set_vocabulary(vocab_data)


</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1470" endline="1481" pcid="5425">
  def test_int_vocab_with_multiple_oov_indices(self):
    vocab_data = [0, -1, -1, -1, 42]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=3,
        mask_token=0,
        oov_token=-1,
        dtype=tf.int64)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(vocab_data, returned_vocab)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1432" endline="1444" pcid="5422">
  def test_vocab_with_max_cap(self):
    vocab_data = ["", "[OOV]", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=5,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(vocab_data, returned_vocab)
    self.assertAllEqual(layer.vocabulary_size(), 5)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1419" endline="1431" pcid="5421">
  def test_vocab_multi_oov_not_present(self):
    vocab_data = ["wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=10,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(returned_vocab,
                        [""] + ["[OOV]"] * 10 + ["wind", "and", "fire"])

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1747" endline="1759" pcid="5447">
  def test_int_vocab_with_max_cap(self):
    vocab_data = [0, -1, 42, 1276, 1138]
    layer = index_lookup.IndexLookup(
        max_tokens=5,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1,
        dtype=tf.int64,
        invert=True)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(vocab_data, returned_vocab)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1445" endline="1457" pcid="5423">
  def test_int_vocab_with_max_cap(self):
    vocab_data = [0, -1, 42, 1276, 1138]
    layer = index_lookup.IndexLookup(
        max_tokens=5,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1,
        dtype=tf.int64)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(vocab_data, returned_vocab)
    self.assertAllEqual(layer.vocabulary_size(), 5)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1394" endline="1406" pcid="5419">
  def test_get_vocabulary_no_special_tokens(self):
    vocab_data = ["", "[OOV]", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=5,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary(include_special_tokens=False)
    self.assertAllEqual(returned_vocab, ["wind", "and", "fire"])
    self.assertAllEqual(layer.vocabulary_size(), 5)

</source>
</class>

<class classid="303" nclones="3" nlines="25" similarity="92">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1137" endline="1166" pcid="5407">
  def test_file_vocab_and_list_vocab_identical_attrs(self):
    vocab_data = ["earth", "wind", "and", "fire"]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    file_layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)

    list_layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)

    expected_vocab = ["", "[OOV]", "earth", "wind", "and", "fire"]
    self.assertAllEqual(expected_vocab, list_layer.get_vocabulary())
    expected_vocab_size = 6
    self.assertAllEqual(expected_vocab_size, list_layer.vocabulary_size())
    self.assertAllEqual(list_layer.get_vocabulary(),
                        file_layer.get_vocabulary())
    self.assertAllEqual(list_layer.vocabulary_size(),
                        file_layer.vocabulary_size())

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1197" endline="1226" pcid="5409">
  def test_file_vocab_and_list_vocab_identical_attrs_no_mask(self):
    vocab_data = ["earth", "wind", "and", "fire"]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    file_layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        num_oov_indices=2,
        mask_token=None,
        oov_token="[OOV]",
        dtype=tf.string)

    list_layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=2,
        mask_token=None,
        oov_token="[OOV]",
        dtype=tf.string)

    expected_vocab = ["[OOV]", "[OOV]", "earth", "wind", "and", "fire"]
    self.assertAllEqual(expected_vocab, list_layer.get_vocabulary())
    expected_vocab_size = 6
    self.assertAllEqual(expected_vocab_size, list_layer.vocabulary_size())
    self.assertAllEqual(list_layer.get_vocabulary(),
                        file_layer.get_vocabulary())
    self.assertAllEqual(list_layer.vocabulary_size(),
                        file_layer.vocabulary_size())

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1167" endline="1196" pcid="5408">
  def test_file_vocab_and_list_vocab_identical_attrs_multi_oov(self):
    vocab_data = ["earth", "wind", "and", "fire"]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    file_layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        num_oov_indices=2,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)

    list_layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=2,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)

    expected_vocab = ["", "[OOV]", "[OOV]", "earth", "wind", "and", "fire"]
    self.assertAllEqual(expected_vocab, list_layer.get_vocabulary())
    expected_vocab_size = 7
    self.assertAllEqual(expected_vocab_size, list_layer.vocabulary_size())
    self.assertAllEqual(list_layer.get_vocabulary(),
                        file_layer.get_vocabulary())
    self.assertAllEqual(list_layer.vocabulary_size(),
                        file_layer.vocabulary_size())

</source>
</class>

<class classid="304" nclones="2" nlines="13" similarity="85">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1320" endline="1332" pcid="5414">
  def test_dataset_map_output(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=0,
        mask_token=None,
        oov_token="[OOV]",
        vocabulary=vocab_data,
        dtype=tf.string)
    ds = tf.data.Dataset.from_tensor_slices([["earth"], ["wind"], ["and"]])
    ds = ds.map(layer)
    self.assertAllEqual(list(ds.as_numpy_iterator()), [[0], [1], [2]])

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1333" endline="1350" pcid="5415">
  def test_dataset_map_output_layer_created_in_function(self):
    vocab_data = ["earth", "wind", "and", "fire"]

    def apply_lookup(data):
      layer = index_lookup.IndexLookup(
          max_tokens=None,
          num_oov_indices=0,
          mask_token=None,
          oov_token="[OOV]",
          vocabulary=vocab_data,
          dtype=tf.string)
      return layer(data)

    ds = tf.data.Dataset.from_tensor_slices([["earth"], ["wind"], ["and"]])
    ds = ds.map(apply_lookup)
    self.assertAllEqual(list(ds.as_numpy_iterator()), [[0], [1], [2]])


</source>
</class>

<class classid="305" nclones="3" nlines="13" similarity="71">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1578" endline="1592" pcid="5434">
  def test_vocab_with_idf_weights_non_tfidf_output_fails(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    weight_data = [1, 1, 1, 1, 1]
    with self.assertRaisesRegex(ValueError,
                                "`idf_weights` should only be set if"):
      index_lookup.IndexLookup(
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          output_mode=index_lookup.MULTI_HOT,
          dtype=tf.string,
          vocabulary=vocab_data,
          idf_weights=weight_data)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1593" endline="1607" pcid="5435">
  def test_vocab_with_idf_weights_length_mismatch_fails(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    weight_data = [1, 1, 1, 1, 1]  # too long
    with self.assertRaisesRegex(
        ValueError, "`idf_weights` must be the same length as vocab"):
      index_lookup.IndexLookup(
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          output_mode=index_lookup.TF_IDF,
          dtype=tf.string,
          vocabulary=vocab_data,
          idf_weights=weight_data)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="1608" endline="1620" pcid="5436">
  def test_vocab_without_idf_weights_tfidf_output_fails(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    with self.assertRaisesRegex(
        ValueError, "`idf_weights` must be set if output_mode is TF_IDF"):
      index_lookup.IndexLookup(
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          output_mode=index_lookup.TF_IDF,
          dtype=tf.string,
          vocabulary=vocab_data)

</source>
</class>

<class classid="306" nclones="3" nlines="40" similarity="90">
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="2022" endline="2088" pcid="5461">
  def test_persistence_file_vocab_keras_save_keras_load(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    # Build and validate a golden model.
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string,
        vocabulary=vocab_file)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(output_dataset, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model")
    model.save(output_path, save_format="tf")

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()
    tf.io.gfile.remove(vocab_file)

    loaded_model = keras.models.load_model(
        output_path, custom_objects={"IndexLookup": index_lookup.IndexLookup})

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_dataset = loaded_model.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

    # Try re-saving the layer. This simulates saving a layer contained at
    # a hub Module.
    input_data_2 = keras.Input(shape=(None,), dtype=tf.string)
    output_2 = loaded_model(input_data_2)
    model_2 = keras.Model(inputs=input_data_2, outputs=output_2)
    new_output_dataset = model_2.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model_2")
    model_2.save(output_path, save_format="tf")

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()

    loaded_model = keras.models.load_model(
        output_path, custom_objects={"IndexLookup": index_lookup.IndexLookup})

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_dataset = loaded_model.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="2089" endline="2155" pcid="5462">
  def test_persistence_file_vocab_keras_save_keras_load_tf_save_tf_load(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    # Build and validate a golden model.
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string,
        vocabulary=vocab_file)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(output_dataset, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model")
    model.save(output_path, save_format="tf")

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()
    tf.io.gfile.remove(vocab_file)

    loaded_model = keras.models.load_model(
        output_path, custom_objects={"IndexLookup": index_lookup.IndexLookup})

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_dataset = loaded_model.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

    # Try re-saving the layer. This simulates saving a layer contained at
    # a hub Module.
    input_data_2 = keras.Input(shape=(None,), dtype=tf.string)
    output_2 = loaded_model(input_data_2)
    model_2 = keras.Model(inputs=input_data_2, outputs=output_2)
    new_output_dataset = model_2.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model_2")
    tf.saved_model.save(model_2, output_path)

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()

    loaded_model = tf.saved_model.load(output_path)
    f = loaded_model.signatures["serving_default"]

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_dataset = f(tf.constant(input_array))["model"]
    self.assertAllEqual(new_output_dataset, expected_output)

</source>
<source file="systems/keras-2.7.0/keras/layers/preprocessing/index_lookup_test.py" startline="2156" endline="2223" pcid="5463">
  def test_persistence_file_vocab_keras_save_keras_load_keras_save_keras_load(
      self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    # Build and validate a golden model.
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string,
        vocabulary=vocab_file)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(output_dataset, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model")
    model.save(output_path, save_format="tf")

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()
    tf.io.gfile.remove(vocab_file)

    loaded_model = keras.models.load_model(
        output_path, custom_objects={"IndexLookup": index_lookup.IndexLookup})

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_dataset = loaded_model.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

    # Try re-saving the layer. This simulates saving a layer contained at
    # a hub Module.
    input_data_2 = keras.Input(shape=(None,), dtype=tf.string)
    output_2 = loaded_model(input_data_2)
    model_2 = keras.Model(inputs=input_data_2, outputs=output_2)
    new_output_dataset = model_2.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model_2")
    model_2.save(output_path, save_format="tf")

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()

    loaded_model = keras.models.load_model(
        output_path, custom_objects={"IndexLookup": index_lookup.IndexLookup})

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_dataset = model_2.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

</source>
</class>

<class classid="307" nclones="2" nlines="12" similarity="84">
<source file="systems/keras-2.7.0/keras/regularizers_test.py" startline="130" endline="141" pcid="5483">
  def test_regularization_shared_layer(self, regularizer):
    dense_layer = keras.layers.Dense(
        NUM_CLASSES,
        kernel_regularizer=regularizer,
        activity_regularizer=regularizer)
    model = self.create_multi_input_model_from(dense_layer, dense_layer)
    model.compile(
        loss='categorical_crossentropy',
        optimizer='sgd',
        run_eagerly=testing_utils.should_run_eagerly())
    self.assertLen(model.losses, 5)

</source>
<source file="systems/keras-2.7.0/keras/regularizers_test.py" startline="148" endline="163" pcid="5484">
  def test_regularization_shared_model(self, regularizer):
    dense_layer = keras.layers.Dense(
        NUM_CLASSES,
        kernel_regularizer=regularizer,
        activity_regularizer=regularizer)

    input_tensor = keras.layers.Input(shape=(DATA_DIM,))
    dummy_model = keras.models.Model(input_tensor, dense_layer(input_tensor))

    model = self.create_multi_input_model_from(dummy_model, dummy_model)
    model.compile(
        loss='categorical_crossentropy',
        optimizer='sgd',
        run_eagerly=testing_utils.should_run_eagerly())
    self.assertLen(model.losses, 6)

</source>
</class>

</clones>

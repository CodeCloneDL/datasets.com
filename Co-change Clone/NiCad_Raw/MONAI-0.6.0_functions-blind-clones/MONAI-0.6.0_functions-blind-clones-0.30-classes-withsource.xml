<clones>
<systeminfo processor="nicad6" system="MONAI-0.6.0" granularity="functions-blind" threshold="30%" minlines="10" maxlines="2500"/>
<cloneinfo npcs="1584" npairs="197"/>
<runinfo ncompares="64135" cputime="67390"/>
<classinfo nclasses="79"/>

<class classid="1" nclones="2" nlines="11" similarity="80">
<source file="systems/MONAI-0.6.0/tests/test_concat_itemsd.py" startline="21" endline="32" pcid="6">
    def test_tensor_values(self):
        device = torch.device("cuda:0") if torch.cuda.is_available() else torch.device("cpu:0")
        input_data = {
            "img1": torch.tensor([[0, 1], [1, 2]], device=device),
            "img2": torch.tensor([[0, 1], [1, 2]], device=device),
        }
        result = ConcatItemsd(keys=["img1", "img2"], name="cat_img")(input_data)
        self.assertTrue("cat_img" in result)
        result["cat_img"] += 1
        torch.testing.assert_allclose(result["img1"], torch.tensor([[0, 1], [1, 2]], device=device))
        torch.testing.assert_allclose(result["cat_img"], torch.tensor([[1, 2], [2, 3], [1, 2], [2, 3]], device=device))

</source>
<source file="systems/MONAI-0.6.0/tests/test_copy_itemsd.py" startline="42" endline="53" pcid="958">
    def test_tensor_values(self):
        device = torch.device("cuda:0") if torch.cuda.is_available() else torch.device("cpu:0")
        input_data = {
            "img": torch.tensor([[0, 1], [1, 2]], device=device),
            "seg": torch.tensor([[0, 1], [1, 2]], device=device),
        }
        result = CopyItemsd(keys="img", times=1, names="img_1")(input_data)
        self.assertTrue("img_1" in result)
        result["img_1"] += 1
        torch.testing.assert_allclose(result["img"], torch.tensor([[0, 1], [1, 2]], device=device))
        torch.testing.assert_allclose(result["img_1"], torch.tensor([[1, 2], [2, 3]], device=device))

</source>
</class>

<class classid="2" nclones="2" nlines="17" similarity="94">
<source file="systems/MONAI-0.6.0/tests/hvd_evenly_divisible_all_gather.py" startline="26" endline="46" pcid="21">
    def _run(self):
        if hvd.rank() == 0:
            data1 = torch.tensor([[1, 2], [3, 4]])
            data2 = torch.tensor([[1.0, 2.0]])
            data3 = torch.tensor(7)

        if hvd.rank() == 1:
            data1 = torch.tensor([[5, 6]])
            data2 = torch.tensor([[3.0, 4.0], [5.0, 6.0]])
            data3 = torch.tensor(8)

        result1 = evenly_divisible_all_gather(data=data1, concat=True)
        torch.testing.assert_allclose(result1, torch.tensor([[1, 2], [3, 4], [5, 6]]))
        result2 = evenly_divisible_all_gather(data=data2, concat=False)
        for r, e in zip(result2, [torch.tensor([[1.0, 2.0]]), torch.tensor([[3.0, 4.0], [5.0, 6.0]])]):
            torch.testing.assert_allclose(r, e)
        result3 = evenly_divisible_all_gather(data=data3, concat=False)
        for r in result3:
            torch.testing.assert_allclose(r.ndimension(), 0)


</source>
<source file="systems/MONAI-0.6.0/tests/test_evenly_divisible_all_gather_dist.py" startline="26" endline="46" pcid="799">
    def _run(self):
        if dist.get_rank() == 0:
            data1 = torch.tensor([[1, 2], [3, 4]])
            data2 = torch.tensor([[1.0, 2.0]])
            data3 = torch.tensor(7)

        if dist.get_rank() == 1:
            data1 = torch.tensor([[5, 6]])
            data2 = torch.tensor([[3.0, 4.0], [5.0, 6.0]])
            data3 = torch.tensor(8)

        result1 = evenly_divisible_all_gather(data=data1, concat=True)
        torch.testing.assert_allclose(result1, torch.tensor([[1, 2], [3, 4], [5, 6]]))
        result2 = evenly_divisible_all_gather(data=data2, concat=False)
        for r, e in zip(result2, [torch.tensor([[1.0, 2.0]]), torch.tensor([[3.0, 4.0], [5.0, 6.0]])]):
            torch.testing.assert_allclose(r, e)
        result3 = evenly_divisible_all_gather(data=data3, concat=False)
        for r in result3:
            self.assertEqual(r.ndimension(), 0)


</source>
</class>

<class classid="3" nclones="2" nlines="12" similarity="100">
<source file="systems/MONAI-0.6.0/tests/test_rand_elasticd_3d.py" startline="111" endline="124" pcid="32">
    def test_rand_3d_elasticd(self, input_param, input_data, expected_val):
        g = Rand3DElasticd(**input_param)
        g.set_random_state(123)
        res = g(input_data)
        for key in res:
            result = res[key]
            expected = expected_val[key] if isinstance(expected_val, dict) else expected_val
            self.assertEqual(isinstance(result, torch.Tensor), isinstance(expected, torch.Tensor))
            if isinstance(result, torch.Tensor):
                np.testing.assert_allclose(result.cpu().numpy(), expected.cpu().numpy(), rtol=1e-4, atol=1e-4)
            else:
                np.testing.assert_allclose(result, expected, rtol=1e-4, atol=1e-4)


</source>
<source file="systems/MONAI-0.6.0/tests/test_rand_elasticd_2d.py" startline="140" endline="153" pcid="985">
    def test_rand_2d_elasticd(self, input_param, input_data, expected_val):
        g = Rand2DElasticd(**input_param)
        g.set_random_state(123)
        res = g(input_data)
        for key in res:
            result = res[key]
            expected = expected_val[key] if isinstance(expected_val, dict) else expected_val
            self.assertEqual(isinstance(result, torch.Tensor), isinstance(expected, torch.Tensor))
            if isinstance(result, torch.Tensor):
                np.testing.assert_allclose(result.cpu().numpy(), expected.cpu().numpy(), rtol=1e-4, atol=1e-4)
            else:
                np.testing.assert_allclose(result, expected, rtol=1e-4, atol=1e-4)


</source>
</class>

<class classid="4" nclones="2" nlines="18" similarity="78">
<source file="systems/MONAI-0.6.0/tests/test_handler_segmentation_saver.py" startline="31" endline="55" pcid="39">
    def test_saved_content(self, output_ext):
        with tempfile.TemporaryDirectory() as tempdir:

            # set up engine
            def _train_func(engine, batch):
                engine.state.batch = decollate_batch(batch)
                return [torch.randint(0, 255, (1, 2, 2)).float() for _ in range(8)]

            engine = Engine(_train_func)

            # set up testing handler
            saver = SegmentationSaver(output_dir=tempdir, output_postfix="seg", output_ext=output_ext, scale=255)
            saver.attach(engine)

            data = [
                {
                    "filename_or_obj": ["testfile" + str(i) + ".nii.gz" for i in range(8)],
                    "patch_index": torch.tensor(list(range(8))),
                }
            ]
            engine.run(data, max_epochs=1)
            for i in range(8):
                filepath = os.path.join("testfile" + str(i), "testfile" + str(i) + "_seg" + f"_{i}" + output_ext)
                self.assertTrue(os.path.exists(os.path.join(tempdir, filepath)))

</source>
<source file="systems/MONAI-0.6.0/tests/test_handler_segmentation_saver.py" startline="57" endline="84" pcid="41">
    def test_save_resized_content(self, output_ext):
        with tempfile.TemporaryDirectory() as tempdir:

            # set up engine
            def _train_func(engine, batch):
                engine.state.batch = decollate_batch(batch)
                return [torch.randint(0, 255, (1, 2, 2)).float() for _ in range(8)]

            engine = Engine(_train_func)

            # set up testing handler
            saver = SegmentationSaver(output_dir=tempdir, output_postfix="seg", output_ext=output_ext, scale=255)
            saver.attach(engine)

            data = [
                {
                    "filename_or_obj": ["testfile" + str(i) + ".nii.gz" for i in range(8)],
                    "spatial_shape": torch.tensor([[28, 28] for _ in range(8)]),
                    "affine": torch.tensor([np.diag(np.ones(4)) * 5 for _ in range(8)]),
                    "original_affine": torch.tensor([np.diag(np.ones(4)) * 1.0 for _ in range(8)]),
                }
            ]
            engine.run(data, max_epochs=1)
            for i in range(8):
                filepath = os.path.join("testfile" + str(i), "testfile" + str(i) + "_seg" + output_ext)
                self.assertTrue(os.path.exists(os.path.join(tempdir, filepath)))


</source>
</class>

<class classid="5" nclones="4" nlines="13" similarity="70">
<source file="systems/MONAI-0.6.0/tests/test_scale_intensity_range_percentiles.py" startline="21" endline="34" pcid="44">
    def test_scaling(self):
        img = self.imt
        lower = 10
        upper = 99
        b_min = 0
        b_max = 255

        a_min = np.percentile(img, lower)
        a_max = np.percentile(img, upper)
        expected = (img - a_min) / (a_max - a_min)
        expected = (expected * (b_max - b_min)) + b_min
        scaler = ScaleIntensityRangePercentiles(lower=lower, upper=upper, b_min=b_min, b_max=b_max)
        self.assertTrue(np.allclose(expected, scaler(img)))

</source>
<source file="systems/MONAI-0.6.0/tests/test_scale_intensity_range_percentiles.py" startline="35" endline="51" pcid="45">
    def test_relative_scaling(self):
        img = self.imt
        lower = 10
        upper = 99
        b_min = 100
        b_max = 300
        scaler = ScaleIntensityRangePercentiles(lower=lower, upper=upper, b_min=b_min, b_max=b_max, relative=True)

        expected_a_min = np.percentile(img, lower)
        expected_a_max = np.percentile(img, upper)
        expected_b_min = ((b_max - b_min) * (lower / 100.0)) + b_min
        expected_b_max = ((b_max - b_min) * (upper / 100.0)) + b_min
        expected_img = (img - expected_a_min) / (expected_a_max - expected_a_min)
        expected_img = (expected_img * (expected_b_max - expected_b_min)) + expected_b_min

        self.assertTrue(np.allclose(expected_img, scaler(img)))

</source>
<source file="systems/MONAI-0.6.0/tests/test_scale_intensity_range_percentilesd.py" startline="21" endline="38" pcid="1186">
    def test_scaling(self):
        img = self.imt
        data = {}
        data["img"] = img
        lower = 10
        upper = 99
        b_min = 0
        b_max = 255

        a_min = np.percentile(img, lower)
        a_max = np.percentile(img, upper)
        expected = (img - a_min) / (a_max - a_min)
        expected = (expected * (b_max - b_min)) + b_min

        scaler = ScaleIntensityRangePercentilesd(keys=data.keys(), lower=lower, upper=upper, b_min=b_min, b_max=b_max)

        self.assertTrue(np.allclose(expected, scaler(data)["img"]))

</source>
<source file="systems/MONAI-0.6.0/tests/test_scale_intensity_range_percentilesd.py" startline="39" endline="59" pcid="1187">
    def test_relative_scaling(self):
        img = self.imt
        data = {}
        data["img"] = img
        lower = 10
        upper = 99
        b_min = 100
        b_max = 300
        scaler = ScaleIntensityRangePercentilesd(
            keys=data.keys(), lower=lower, upper=upper, b_min=b_min, b_max=b_max, relative=True
        )

        expected_a_min = np.percentile(img, lower)
        expected_a_max = np.percentile(img, upper)
        expected_b_min = ((b_max - b_min) * (lower / 100.0)) + b_min
        expected_b_max = ((b_max - b_min) * (upper / 100.0)) + b_min
        expected_img = (img - expected_a_min) / (expected_a_max - expected_a_min)
        expected_img = (expected_img * (expected_b_max - expected_b_min)) + expected_b_min

        self.assertTrue(np.allclose(expected_img, scaler(data)["img"]))

</source>
</class>

<class classid="6" nclones="2" nlines="24" similarity="86">
<source file="systems/MONAI-0.6.0/tests/test_data_stats.py" startline="142" endline="166" pcid="86">
    def test_file(self, input_data, expected_print):
        with tempfile.TemporaryDirectory() as tempdir:
            filename = os.path.join(tempdir, "test_data_stats.log")
            handler = logging.FileHandler(filename, mode="w")
            handler.setLevel(logging.INFO)
            input_param = {
                "prefix": "test data",
                "data_type": True,
                "data_shape": True,
                "value_range": True,
                "data_value": True,
                "additional_info": np.mean,
                "logger_handler": handler,
            }
            transform = DataStats(**input_param)
            _ = transform(input_data)
            _logger = logging.getLogger(transform._logger_name)
            for h in _logger.handlers[:]:
                h.close()
                _logger.removeHandler(h)
            with open(filename, "r") as f:
                content = f.read()
            self.assertEqual(content, expected_print)


</source>
<source file="systems/MONAI-0.6.0/tests/test_data_statsd.py" startline="174" endline="199" pcid="477">
    def test_file(self, input_data, expected_print):
        with tempfile.TemporaryDirectory() as tempdir:
            filename = os.path.join(tempdir, "test_stats.log")
            handler = logging.FileHandler(filename, mode="w")
            handler.setLevel(logging.INFO)
            input_param = {
                "keys": "img",
                "prefix": "test data",
                "data_shape": True,
                "value_range": True,
                "data_value": True,
                "additional_info": np.mean,
                "logger_handler": handler,
            }
            transform = DataStatsd(**input_param)
            _ = transform(input_data)
            _logger = logging.getLogger(transform.printer._logger_name)
            for h in _logger.handlers[:]:
                h.close()
                _logger.removeHandler(h)
            del handler
            with open(filename, "r") as f:
                content = f.read()
            self.assertEqual(content, expected_print)


</source>
</class>

<class classid="7" nclones="2" nlines="14" similarity="78">
<source file="systems/MONAI-0.6.0/tests/test_rand_zoom.py" startline="64" endline="78" pcid="97">
    def test_auto_expand_3d(self):
        random_zoom = RandZoom(
            prob=1.0,
            min_zoom=[0.8, 0.7],
            max_zoom=[1.2, 1.3],
            mode="nearest",
            keep_size=False,
        )
        random_zoom.set_random_state(1234)
        test_data = np.random.randint(0, 2, size=[2, 2, 3, 4])
        zoomed = random_zoom(test_data)
        np.testing.assert_allclose(random_zoom._zoom, (1.048844, 1.048844, 0.962637), atol=1e-2)
        np.testing.assert_allclose(zoomed.shape, (2, 2, 3, 3))


</source>
<source file="systems/MONAI-0.6.0/tests/test_rand_zoomd.py" startline="61" endline="76" pcid="742">
    def test_auto_expand_3d(self):
        random_zoom = RandZoomd(
            keys="img",
            prob=1.0,
            min_zoom=[0.8, 0.7],
            max_zoom=[1.2, 1.3],
            mode="nearest",
            keep_size=False,
        )
        random_zoom.set_random_state(1234)
        test_data = {"img": np.random.randint(0, 2, size=[2, 2, 3, 4])}
        zoomed = random_zoom(test_data)
        np.testing.assert_allclose(random_zoom._zoom, (1.048844, 1.048844, 0.962637), atol=1e-2)
        np.testing.assert_allclose(zoomed["img"].shape, (2, 2, 3, 3))


</source>
</class>

<class classid="8" nclones="2" nlines="15" similarity="87">
<source file="systems/MONAI-0.6.0/tests/test_handler_tb_stats.py" startline="23" endline="45" pcid="103">
    def test_metrics_print(self):
        with tempfile.TemporaryDirectory() as tempdir:

            # set up engine
            def _train_func(engine, batch):
                return [batch + 1.0]

            engine = Engine(_train_func)

            # set up dummy metric
            @engine.on(Events.EPOCH_COMPLETED)
            def _update_metric(engine):
                current_metric = engine.state.metrics.get("acc", 0.1)
                engine.state.metrics["acc"] = current_metric + 0.1

            # set up testing handler
            stats_handler = TensorBoardStatsHandler(log_dir=tempdir)
            stats_handler.attach(engine)
            engine.run(range(3), max_epochs=2)
            stats_handler.close()
            # check logging output
            self.assertTrue(len(glob.glob(tempdir)) > 0)

</source>
<source file="systems/MONAI-0.6.0/tests/test_handler_tb_stats.py" startline="46" endline="72" pcid="106">
    def test_metrics_writer(self):
        with tempfile.TemporaryDirectory() as tempdir:

            # set up engine
            def _train_func(engine, batch):
                return [batch + 1.0]

            engine = Engine(_train_func)

            # set up dummy metric
            @engine.on(Events.EPOCH_COMPLETED)
            def _update_metric(engine):
                current_metric = engine.state.metrics.get("acc", 0.1)
                engine.state.metrics["acc"] = current_metric + 0.1

            # set up testing handler
            writer = SummaryWriter(log_dir=tempdir)
            stats_handler = TensorBoardStatsHandler(
                writer, output_transform=lambda x: {"loss": x[0] * 2.0}, global_epoch_transform=lambda x: x * 3.0
            )
            stats_handler.attach(engine)
            engine.run(range(3), max_epochs=2)
            writer.close()
            # check logging output
            self.assertTrue(len(glob.glob(tempdir)) > 0)


</source>
</class>

<class classid="9" nclones="2" nlines="20" similarity="80">
<source file="systems/MONAI-0.6.0/tests/test_affine_transform.py" startline="316" endline="339" pcid="122">
    def test_forward_2d(self):
        x = torch.rand(2, 1, 4, 4)
        theta = torch.Tensor([[[0, -1, 0], [1, 0, 0]]]).repeat(2, 1, 1)
        grid = torch.nn.functional.affine_grid(theta, x.size(), align_corners=False)
        expected = torch.nn.functional.grid_sample(x, grid, align_corners=False)
        expected = expected.detach().cpu().numpy()

        actual = AffineTransform(normalized=True, reverse_indexing=False)(x, theta)
        actual = actual.detach().cpu().numpy()
        np.testing.assert_allclose(actual, expected)
        np.testing.assert_allclose(list(theta.shape), [2, 2, 3])

        theta = torch.Tensor([[0, -1, 0], [1, 0, 0]])
        actual = AffineTransform(normalized=True, reverse_indexing=False)(x, theta)
        actual = actual.detach().cpu().numpy()
        np.testing.assert_allclose(actual, expected)
        np.testing.assert_allclose(list(theta.shape), [2, 3])

        theta = torch.Tensor([[[0, -1, 0], [1, 0, 0]]])
        actual = AffineTransform(normalized=True, reverse_indexing=False)(x, theta)
        actual = actual.detach().cpu().numpy()
        np.testing.assert_allclose(actual, expected)
        np.testing.assert_allclose(list(theta.shape), [1, 2, 3])

</source>
<source file="systems/MONAI-0.6.0/tests/test_affine_transform.py" startline="340" endline="364" pcid="123">
    def test_forward_3d(self):
        x = torch.rand(2, 1, 4, 4, 4)
        theta = torch.Tensor([[[0, 0, -1, 0], [1, 0, 0, 0], [0, 0, 1, 0]]]).repeat(2, 1, 1)
        grid = torch.nn.functional.affine_grid(theta, x.size(), align_corners=False)
        expected = torch.nn.functional.grid_sample(x, grid, align_corners=False)
        expected = expected.detach().cpu().numpy()

        actual = AffineTransform(normalized=True, reverse_indexing=False)(x, theta)
        actual = actual.detach().cpu().numpy()
        np.testing.assert_allclose(actual, expected)
        np.testing.assert_allclose(list(theta.shape), [2, 3, 4])

        theta = torch.Tensor([[0, 0, -1, 0], [1, 0, 0, 0], [0, 0, 1, 0]])
        actual = AffineTransform(normalized=True, reverse_indexing=False)(x, theta)
        actual = actual.detach().cpu().numpy()
        np.testing.assert_allclose(actual, expected)
        np.testing.assert_allclose(list(theta.shape), [3, 4])

        theta = torch.Tensor([[[0, 0, -1, 0], [1, 0, 0, 0], [0, 0, 1, 0]]])
        actual = AffineTransform(normalized=True, reverse_indexing=False)(x, theta)
        actual = actual.detach().cpu().numpy()
        np.testing.assert_allclose(actual, expected)
        np.testing.assert_allclose(list(theta.shape), [1, 3, 4])


</source>
</class>

<class classid="10" nclones="2" nlines="22" similarity="90">
<source file="systems/MONAI-0.6.0/tests/test_spacingd.py" startline="44" endline="66" pcid="138">
    def test_interp_all(self):
        data = {
            "image": np.arange(20).reshape((2, 1, 10)),
            "seg": np.ones((2, 1, 10)),
            "image_meta_dict": {"affine": np.eye(4)},
            "seg_meta_dict": {"affine": np.eye(4)},
        }
        spacing = Spacingd(
            keys=("image", "seg"),
            mode="nearest",
            pixdim=(
                1,
                0.2,
            ),
        )
        res = spacing(data)
        self.assertEqual(
            ("image", "image_meta_dict", "image_transforms", "seg", "seg_meta_dict", "seg_transforms"),
            tuple(sorted(res)),
        )
        np.testing.assert_allclose(res["image"].shape, (2, 1, 46))
        np.testing.assert_allclose(res["image_meta_dict"]["affine"], np.diag((1, 0.2, 1, 1)))

</source>
<source file="systems/MONAI-0.6.0/tests/test_spacingd.py" startline="67" endline="90" pcid="139">
    def test_interp_sep(self):
        data = {
            "image": np.ones((2, 1, 10)),
            "seg": np.ones((2, 1, 10)),
            "image_meta_dict": {"affine": np.eye(4)},
            "seg_meta_dict": {"affine": np.eye(4)},
        }
        spacing = Spacingd(
            keys=("image", "seg"),
            mode=("bilinear", "nearest"),
            pixdim=(
                1,
                0.2,
            ),
        )
        res = spacing(data)
        self.assertEqual(
            ("image", "image_meta_dict", "image_transforms", "seg", "seg_meta_dict", "seg_transforms"),
            tuple(sorted(res)),
        )
        np.testing.assert_allclose(res["image"].shape, (2, 1, 46))
        np.testing.assert_allclose(res["image_meta_dict"]["affine"], np.diag((1, 0.2, 1, 1)))


</source>
</class>

<class classid="11" nclones="2" nlines="25" similarity="88">
<source file="systems/MONAI-0.6.0/tests/test_rand_rotate.py" startline="31" endline="58" pcid="143">
    def test_correct_results(self, degrees, keep_size, mode, padding_mode, align_corners):
        rotate_fn = RandRotate(
            range_x=degrees,
            prob=1.0,
            keep_size=keep_size,
            mode=mode,
            padding_mode=padding_mode,
            align_corners=align_corners,
        )
        rotate_fn.set_random_state(243)
        rotated = rotate_fn(self.imt[0])

        _order = 0 if mode == "nearest" else 1
        if mode == "border":
            _mode = "nearest"
        elif mode == "reflection":
            _mode = "reflect"
        else:
            _mode = "constant"
        angle = rotate_fn.x
        expected = scipy.ndimage.rotate(
            self.imt[0, 0], -np.rad2deg(angle), (0, 1), not keep_size, order=_order, mode=_mode, prefilter=False
        )
        expected = np.stack(expected).astype(np.float32)
        good = np.sum(np.isclose(expected, rotated[0], atol=1e-3))
        self.assertLessEqual(np.abs(good - expected.size), 5, "diff at most 5 pixels")


</source>
<source file="systems/MONAI-0.6.0/tests/test_rand_rotated.py" startline="32" endline="60" pcid="665">
    def test_correct_results(self, degrees, keep_size, mode, padding_mode, align_corners):
        rotate_fn = RandRotated(
            "img",
            range_x=degrees,
            prob=1.0,
            keep_size=keep_size,
            mode=mode,
            padding_mode=padding_mode,
            align_corners=align_corners,
        )
        rotate_fn.set_random_state(243)
        rotated = rotate_fn({"img": self.imt[0], "seg": self.segn[0]})

        _order = 0 if mode == "nearest" else 1
        if padding_mode == "border":
            _mode = "nearest"
        elif padding_mode == "reflection":
            _mode = "reflect"
        else:
            _mode = "constant"
        angle = rotate_fn.x
        expected = scipy.ndimage.rotate(
            self.imt[0, 0], -np.rad2deg(angle), (0, 1), not keep_size, order=_order, mode=_mode, prefilter=False
        )
        expected = np.stack(expected).astype(np.float32)
        good = np.sum(np.isclose(expected, rotated["img"][0], atol=1e-3))
        self.assertLessEqual(np.abs(good - expected.size), 5, "diff at most 5 pixels")


</source>
</class>

<class classid="12" nclones="2" nlines="15" similarity="80">
<source file="systems/MONAI-0.6.0/tests/test_rand_rotate.py" startline="86" endline="101" pcid="144">
    def test_correct_results(self, x, y, z, keep_size, mode, padding_mode, align_corners, expected):
        rotate_fn = RandRotate(
            range_x=x,
            range_y=y,
            range_z=z,
            prob=1.0,
            keep_size=keep_size,
            mode=mode,
            padding_mode=padding_mode,
            align_corners=align_corners,
        )
        rotate_fn.set_random_state(243)
        rotated = rotate_fn(self.imt[0])
        np.testing.assert_allclose(rotated.shape, expected)


</source>
<source file="systems/MONAI-0.6.0/tests/test_rand_rotated.py" startline="119" endline="135" pcid="666">
    def test_correct_shapes(self, x, y, z, keep_size, mode, padding_mode, align_corners, expected):
        rotate_fn = RandRotated(
            "img",
            range_x=x,
            range_y=y,
            range_z=z,
            prob=1.0,
            keep_size=keep_size,
            mode=mode,
            padding_mode=padding_mode,
            align_corners=align_corners,
        )
        rotate_fn.set_random_state(243)
        rotated = rotate_fn({"img": self.imt[0], "seg": self.segn[0]})
        np.testing.assert_allclose(rotated["img"].shape, expected)


</source>
</class>

<class classid="13" nclones="2" nlines="15" similarity="86">
<source file="systems/MONAI-0.6.0/tests/test_resized.py" startline="33" endline="51" pcid="151">
    def test_correct_results(self, spatial_size, mode):
        resize = Resized("img", spatial_size, mode)
        _order = 0
        if mode.endswith("linear"):
            _order = 1
        if spatial_size == (32, -1):
            spatial_size = (32, 64)
        expected = []
        for channel in self.imt[0]:
            expected.append(
                skimage.transform.resize(
                    channel, spatial_size, order=_order, clip=False, preserve_range=False, anti_aliasing=False
                )
            )
        expected = np.stack(expected).astype(np.float32)
        out = resize({"img": self.imt[0]})["img"]
        np.testing.assert_allclose(out, expected, atol=0.9)


</source>
<source file="systems/MONAI-0.6.0/tests/test_resize.py" startline="35" endline="53" pcid="927">
    def test_correct_results(self, spatial_size, mode):
        resize = Resize(spatial_size, mode=mode)
        _order = 0
        if mode.endswith("linear"):
            _order = 1
        if spatial_size == (32, -1):
            spatial_size = (32, 64)
        expected = []
        for channel in self.imt[0]:
            expected.append(
                skimage.transform.resize(
                    channel, spatial_size, order=_order, clip=False, preserve_range=False, anti_aliasing=False
                )
            )
        expected = np.stack(expected).astype(np.float32)
        out = resize(self.imt[0])
        np.testing.assert_allclose(out, expected, atol=0.9)


</source>
</class>

<class classid="14" nclones="4" nlines="12" similarity="78">
<source file="systems/MONAI-0.6.0/tests/test_handler_parameter_scheduler.py" startline="66" endline="78" pcid="160">
    def test_exponential_scheduler(self):
        net = ToyNet(value=-1)
        engine = Engine(lambda e, b: None)
        ParamSchedulerHandler(
            parameter_setter=net.set_value,
            value_calculator="exponential",
            vc_kwargs={"initial_value": 10, "gamma": 0.99},
            epoch_level=True,
            event=Events.EPOCH_COMPLETED,
        ).attach(engine)
        engine.run([0] * 8, max_epochs=2)
        torch.testing.assert_allclose(net.get_value(), 10 * 0.99 * 0.99)

</source>
<source file="systems/MONAI-0.6.0/tests/test_handler_parameter_scheduler.py" startline="105" endline="121" pcid="163">
    def test_custom_scheduler(self):
        def custom_logic(initial_value, gamma, current_step):
            return initial_value * gamma ** (current_step % 9)

        net = ToyNet(value=-1)
        engine = Engine(lambda e, b: None)
        ParamSchedulerHandler(
            parameter_setter=net.set_value,
            value_calculator=custom_logic,
            vc_kwargs={"initial_value": 10, "gamma": 0.99},
            epoch_level=True,
            event=Events.EPOCH_COMPLETED,
        ).attach(engine)
        engine.run([0] * 8, max_epochs=2)
        torch.testing.assert_allclose(net.get_value(), 10 * 0.99 * 0.99)


</source>
<source file="systems/MONAI-0.6.0/tests/test_handler_parameter_scheduler.py" startline="79" endline="91" pcid="161">
    def test_step_scheduler(self):
        net = ToyNet(value=-1)
        engine = Engine(lambda e, b: None)
        ParamSchedulerHandler(
            parameter_setter=net.set_value,
            value_calculator="step",
            vc_kwargs={"initial_value": 10, "gamma": 0.99, "step_size": 5},
            epoch_level=True,
            event=Events.EPOCH_COMPLETED,
        ).attach(engine)
        engine.run([0] * 8, max_epochs=10)
        torch.testing.assert_allclose(net.get_value(), 10 * 0.99 * 0.99)

</source>
<source file="systems/MONAI-0.6.0/tests/test_handler_parameter_scheduler.py" startline="92" endline="104" pcid="162">
    def test_multistep_scheduler(self):
        net = ToyNet(value=-1)
        engine = Engine(lambda e, b: None)
        ParamSchedulerHandler(
            parameter_setter=net.set_value,
            value_calculator="multistep",
            vc_kwargs={"initial_value": 10, "gamma": 0.99, "milestones": [3, 6]},
            epoch_level=True,
            event=Events.EPOCH_COMPLETED,
        ).attach(engine)
        engine.run([0] * 8, max_epochs=10)
        torch.testing.assert_allclose(net.get_value(), 10 * 0.99 * 0.99)

</source>
</class>

<class classid="15" nclones="2" nlines="10" similarity="100">
<source file="systems/MONAI-0.6.0/tests/test_as_channel_lastd.py" startline="28" endline="39" pcid="180">
    def test_shape(self, input_param, expected_shape):
        test_data = {
            "image": np.random.randint(0, 2, size=[1, 2, 3, 4]),
            "label": np.random.randint(0, 2, size=[1, 2, 3, 4]),
            "extra": np.random.randint(0, 2, size=[1, 2, 3, 4]),
        }
        result = AsChannelLastd(**input_param)(test_data)
        self.assertTupleEqual(result["image"].shape, expected_shape)
        self.assertTupleEqual(result["label"].shape, expected_shape)
        self.assertTupleEqual(result["extra"].shape, expected_shape)


</source>
<source file="systems/MONAI-0.6.0/tests/test_as_channel_firstd.py" startline="28" endline="39" pcid="834">
    def test_shape(self, input_param, expected_shape):
        test_data = {
            "image": np.random.randint(0, 2, size=[1, 2, 3, 4]),
            "label": np.random.randint(0, 2, size=[1, 2, 3, 4]),
            "extra": np.random.randint(0, 2, size=[1, 2, 3, 4]),
        }
        result = AsChannelFirstd(**input_param)(test_data)
        self.assertTupleEqual(result["image"].shape, expected_shape)
        self.assertTupleEqual(result["label"].shape, expected_shape)
        self.assertTupleEqual(result["extra"].shape, expected_shape)


</source>
</class>

<class classid="16" nclones="3" nlines="11" similarity="75">
<source file="systems/MONAI-0.6.0/tests/test_integration_workflows_gan.py" startline="131" endline="143" pcid="184">
    def setUp(self):
        set_determinism(seed=0)

        self.data_dir = tempfile.mkdtemp()
        for i in range(40):
            im, _ = create_test_image_2d(64, 64, num_objs=3, rad_max=14, num_seg_classes=1, channel_dim=-1)
            n = nib.Nifti1Image(im, np.eye(4))
            nib.save(n, os.path.join(self.data_dir, f"img{i:d}.nii.gz"))

        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu:0")
        monai.config.print_config()
        logging.basicConfig(stream=sys.stdout, level=logging.INFO)

</source>
<source file="systems/MONAI-0.6.0/tests/test_integration_workflows.py" startline="286" endline="300" pcid="600">
    def setUp(self):
        set_determinism(seed=0)

        self.data_dir = tempfile.mkdtemp()
        for i in range(40):
            im, seg = create_test_image_3d(128, 128, 128, num_seg_classes=1, channel_dim=-1)
            n = nib.Nifti1Image(im, np.eye(4))
            nib.save(n, os.path.join(self.data_dir, f"img{i:d}.nii.gz"))
            n = nib.Nifti1Image(seg, np.eye(4))
            nib.save(n, os.path.join(self.data_dir, f"seg{i:d}.nii.gz"))

        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu:0")
        monai.config.print_config()
        logging.basicConfig(stream=sys.stdout, level=logging.INFO)

</source>
<source file="systems/MONAI-0.6.0/tests/test_integration_segmentation_3d.py" startline="232" endline="244" pcid="930">
    def setUp(self):
        set_determinism(seed=0)

        self.data_dir = tempfile.mkdtemp()
        for i in range(40):
            im, seg = create_test_image_3d(128, 128, 128, num_seg_classes=1, channel_dim=-1)
            n = nib.Nifti1Image(im, np.eye(4))
            nib.save(n, os.path.join(self.data_dir, f"img{i:d}.nii.gz"))
            n = nib.Nifti1Image(seg, np.eye(4))
            nib.save(n, os.path.join(self.data_dir, f"seg{i:d}.nii.gz"))

        self.device = "cuda:0" if torch.cuda.is_available() else "cpu:0"

</source>
</class>

<class classid="17" nclones="3" nlines="18" similarity="73">
<source file="systems/MONAI-0.6.0/tests/test_compute_regression_metrics.py" startline="88" endline="118" pcid="203">
    def test_compare_numpy(self):
        set_determinism(seed=123)
        device = "cuda" if torch.cuda.is_available() else "cpu"

        # regression metrics to check + truth metric function in numpy
        metrics = [MSEMetric, MAEMetric, RMSEMetric, partial(PSNRMetric, max_val=1.0)]
        metrics_np = [msemetric_np, maemetric_np, rmsemetric_np, partial(psnrmetric_np, max_val=1.0)]

        # define variations in batch/base_dims/spatial_dims
        batch_dims = [1, 2, 4, 16]
        base_dims = [16, 32, 64]
        spatial_dims = [2, 3, 4]

        # iterate over all variations and check shapes for different reduction functions
        for batch in batch_dims:
            for spatial in spatial_dims:
                for base in base_dims:

                    # create random tensors
                    in_tensor_a = torch.rand((batch,) + (base,) * (spatial - 1)).to(device)
                    in_tensor_b = torch.rand((batch,) + (base,) * (spatial - 1)).to(device)

                    # check metrics
                    for mt_fn, mt_fn_np in zip(metrics, metrics_np):
                        mt = mt_fn(reduction="mean")
                        mt(y_pred=in_tensor_a, y=in_tensor_b)
                        out_tensor = mt.aggregate()
                        out_np = mt_fn_np(y_pred=in_tensor_a.cpu().numpy(), y=in_tensor_b.cpu().numpy())

                        np.testing.assert_allclose(out_tensor.cpu().numpy(), out_np, atol=1e-4)

</source>
<source file="systems/MONAI-0.6.0/tests/test_compute_regression_metrics.py" startline="166" endline="193" pcid="206">
    def test_diff_input(self):
        set_determinism(seed=123)
        device = "cuda" if torch.cuda.is_available() else "cpu"
        metrics = [MSEMetric, MAEMetric, RMSEMetric, partial(PSNRMetric, max_val=1.0)]
        results = [1.0, 1.0, 1.0, 0.0]

        # define variations in batch/base_dims/spatial_dims
        batch_dims = [1, 2, 4, 16]
        base_dims = [16, 32, 64]
        spatial_dims = [2, 3, 4]

        # iterate over all variations and check shapes for different reduction functions
        for batch in batch_dims:
            for spatial in spatial_dims:
                for base in base_dims:

                    # create random tensors
                    in_tensor_a = torch.zeros((batch,) + (base,) * (spatial - 1)).to(device)
                    in_tensor_b = torch.ones((batch,) + (base,) * (spatial - 1)).to(device)

                    # check metrics
                    for mt_fn, rs in zip(metrics, results):
                        mt = mt_fn(reduction="mean")
                        mt(in_tensor_a, in_tensor_b)
                        out_tensor = mt.aggregate()
                        np.testing.assert_allclose(out_tensor.cpu(), rs, atol=1e-4)


</source>
<source file="systems/MONAI-0.6.0/tests/test_compute_regression_metrics.py" startline="140" endline="165" pcid="205">
    def test_same_input(self):
        set_determinism(seed=123)
        device = "cuda" if torch.cuda.is_available() else "cpu"
        metrics = [MSEMetric, MAEMetric, RMSEMetric, partial(PSNRMetric, max_val=1.0)]
        results = [0.0, 0.0, 0.0, float("inf")]

        # define variations in batch/base_dims/spatial_dims
        batch_dims = [1, 2, 4, 16]
        base_dims = [16, 32, 64]
        spatial_dims = [2, 3, 4]

        # iterate over all variations and check shapes for different reduction functions
        for batch in batch_dims:
            for spatial in spatial_dims:
                for base in base_dims:

                    # create random tensors
                    in_tensor = torch.rand((batch,) + (base,) * (spatial - 1)).to(device)

                    # check metrics
                    for mt_fn, rs in zip(metrics, results):
                        mt = mt_fn(reduction="mean")
                        mt(in_tensor, in_tensor)
                        out_tensor = mt.aggregate()
                        np.testing.assert_allclose(out_tensor.cpu(), rs, atol=1e-4)

</source>
</class>

<class classid="18" nclones="2" nlines="66" similarity="90">
<source file="systems/MONAI-0.6.0/tests/test_persistentdataset.py" startline="72" endline="146" pcid="211">
    def test_shape(self, transform, expected_shape):
        test_image = nib.Nifti1Image(np.random.randint(0, 2, size=[128, 128, 128]), np.eye(4))
        with tempfile.TemporaryDirectory() as tempdir:
            nib.save(test_image, os.path.join(tempdir, "test_image1.nii.gz"))
            nib.save(test_image, os.path.join(tempdir, "test_label1.nii.gz"))
            nib.save(test_image, os.path.join(tempdir, "test_extra1.nii.gz"))
            nib.save(test_image, os.path.join(tempdir, "test_image2.nii.gz"))
            nib.save(test_image, os.path.join(tempdir, "test_label2.nii.gz"))
            nib.save(test_image, os.path.join(tempdir, "test_extra2.nii.gz"))
            test_data = [
                {
                    "image": os.path.join(tempdir, "test_image1.nii.gz"),
                    "label": os.path.join(tempdir, "test_label1.nii.gz"),
                    "extra": os.path.join(tempdir, "test_extra1.nii.gz"),
                },
                {
                    "image": os.path.join(tempdir, "test_image2.nii.gz"),
                    "label": os.path.join(tempdir, "test_label2.nii.gz"),
                    "extra": os.path.join(tempdir, "test_extra2.nii.gz"),
                },
            ]

            cache_dir = os.path.join(os.path.join(tempdir, "cache"), "data")
            dataset_precached = PersistentDataset(data=test_data, transform=transform, cache_dir=cache_dir)
            data1_precached = dataset_precached[0]
            data2_precached = dataset_precached[1]

            dataset_postcached = PersistentDataset(data=test_data, transform=transform, cache_dir=cache_dir)
            data1_postcached = dataset_postcached[0]
            data2_postcached = dataset_postcached[1]
            data3_postcached = dataset_postcached[0:2]

            if transform is None:
                self.assertEqual(data1_precached["image"], os.path.join(tempdir, "test_image1.nii.gz"))
                self.assertEqual(data2_precached["label"], os.path.join(tempdir, "test_label2.nii.gz"))
                self.assertEqual(data1_postcached["image"], os.path.join(tempdir, "test_image1.nii.gz"))
                self.assertEqual(data2_postcached["extra"], os.path.join(tempdir, "test_extra2.nii.gz"))
            else:
                self.assertTupleEqual(data1_precached["image"].shape, expected_shape)
                self.assertTupleEqual(data1_precached["label"].shape, expected_shape)
                self.assertTupleEqual(data1_precached["extra"].shape, expected_shape)
                self.assertTupleEqual(data2_precached["image"].shape, expected_shape)
                self.assertTupleEqual(data2_precached["label"].shape, expected_shape)
                self.assertTupleEqual(data2_precached["extra"].shape, expected_shape)

                self.assertTupleEqual(data1_postcached["image"].shape, expected_shape)
                self.assertTupleEqual(data1_postcached["label"].shape, expected_shape)
                self.assertTupleEqual(data1_postcached["extra"].shape, expected_shape)
                self.assertTupleEqual(data2_postcached["image"].shape, expected_shape)
                self.assertTupleEqual(data2_postcached["label"].shape, expected_shape)
                self.assertTupleEqual(data2_postcached["extra"].shape, expected_shape)
                for d in data3_postcached:
                    self.assertTupleEqual(d["image"].shape, expected_shape)

            # update the data to cache
            test_data_new = [
                {
                    "image": os.path.join(tempdir, "test_image1_new.nii.gz"),
                    "label": os.path.join(tempdir, "test_label1_new.nii.gz"),
                    "extra": os.path.join(tempdir, "test_extra1_new.nii.gz"),
                },
                {
                    "image": os.path.join(tempdir, "test_image2_new.nii.gz"),
                    "label": os.path.join(tempdir, "test_label2_new.nii.gz"),
                    "extra": os.path.join(tempdir, "test_extra2_new.nii.gz"),
                },
            ]
            dataset_postcached.set_data(data=test_data_new)
            # test new exchanged cache content
            if transform is None:
                self.assertEqual(dataset_postcached[0]["image"], os.path.join(tempdir, "test_image1_new.nii.gz"))
                self.assertEqual(dataset_postcached[0]["label"], os.path.join(tempdir, "test_label1_new.nii.gz"))
                self.assertEqual(dataset_postcached[1]["extra"], os.path.join(tempdir, "test_extra2_new.nii.gz"))


</source>
<source file="systems/MONAI-0.6.0/tests/test_lmdbdataset.py" startline="125" endline="201" pcid="522">
    def test_shape(self, transform, expected_shape, kwargs=None):
        kwargs = kwargs or {}
        test_image = nib.Nifti1Image(np.random.randint(0, 2, size=[128, 128, 128]), np.eye(4))
        with tempfile.TemporaryDirectory() as tempdir:
            nib.save(test_image, os.path.join(tempdir, "test_image1.nii.gz"))
            nib.save(test_image, os.path.join(tempdir, "test_label1.nii.gz"))
            nib.save(test_image, os.path.join(tempdir, "test_extra1.nii.gz"))
            nib.save(test_image, os.path.join(tempdir, "test_image2.nii.gz"))
            nib.save(test_image, os.path.join(tempdir, "test_label2.nii.gz"))
            nib.save(test_image, os.path.join(tempdir, "test_extra2.nii.gz"))
            test_data = [
                {
                    "image": os.path.join(tempdir, "test_image1.nii.gz"),
                    "label": os.path.join(tempdir, "test_label1.nii.gz"),
                    "extra": os.path.join(tempdir, "test_extra1.nii.gz"),
                },
                {
                    "image": os.path.join(tempdir, "test_image2.nii.gz"),
                    "label": os.path.join(tempdir, "test_label2.nii.gz"),
                    "extra": os.path.join(tempdir, "test_extra2.nii.gz"),
                },
            ]

            cache_dir = os.path.join(os.path.join(tempdir, "cache"), "data")
            dataset_precached = LMDBDataset(
                data=test_data, transform=transform, progress=False, cache_dir=cache_dir, **kwargs
            )
            data1_precached = dataset_precached[0]
            data2_precached = dataset_precached[1]

            dataset_postcached = LMDBDataset(
                data=test_data, transform=transform, progress=False, cache_dir=cache_dir, **kwargs
            )
            data1_postcached = dataset_postcached[0]
            data2_postcached = dataset_postcached[1]

            if transform is None:
                self.assertEqual(data1_precached["image"], os.path.join(tempdir, "test_image1.nii.gz"))
                self.assertEqual(data2_precached["label"], os.path.join(tempdir, "test_label2.nii.gz"))
                self.assertEqual(data1_postcached["image"], os.path.join(tempdir, "test_image1.nii.gz"))
                self.assertEqual(data2_postcached["extra"], os.path.join(tempdir, "test_extra2.nii.gz"))
            else:
                self.assertTupleEqual(data1_precached["image"].shape, expected_shape)
                self.assertTupleEqual(data1_precached["label"].shape, expected_shape)
                self.assertTupleEqual(data1_precached["extra"].shape, expected_shape)
                self.assertTupleEqual(data2_precached["image"].shape, expected_shape)
                self.assertTupleEqual(data2_precached["label"].shape, expected_shape)
                self.assertTupleEqual(data2_precached["extra"].shape, expected_shape)

                self.assertTupleEqual(data1_postcached["image"].shape, expected_shape)
                self.assertTupleEqual(data1_postcached["label"].shape, expected_shape)
                self.assertTupleEqual(data1_postcached["extra"].shape, expected_shape)
                self.assertTupleEqual(data2_postcached["image"].shape, expected_shape)
                self.assertTupleEqual(data2_postcached["label"].shape, expected_shape)
                self.assertTupleEqual(data2_postcached["extra"].shape, expected_shape)

            # update the data to cache
            test_data_new = [
                {
                    "image": os.path.join(tempdir, "test_image1_new.nii.gz"),
                    "label": os.path.join(tempdir, "test_label1_new.nii.gz"),
                    "extra": os.path.join(tempdir, "test_extra1_new.nii.gz"),
                },
                {
                    "image": os.path.join(tempdir, "test_image2_new.nii.gz"),
                    "label": os.path.join(tempdir, "test_label2_new.nii.gz"),
                    "extra": os.path.join(tempdir, "test_extra2_new.nii.gz"),
                },
            ]
            dataset_postcached.set_data(data=test_data_new)
            # test new exchanged cache content
            if transform is None:
                self.assertEqual(dataset_postcached[0]["image"], os.path.join(tempdir, "test_image1_new.nii.gz"))
                self.assertEqual(dataset_postcached[0]["label"], os.path.join(tempdir, "test_label1_new.nii.gz"))
                self.assertEqual(dataset_postcached[1]["extra"], os.path.join(tempdir, "test_extra2_new.nii.gz"))


</source>
</class>

<class classid="19" nclones="2" nlines="10" similarity="70">
<source file="systems/MONAI-0.6.0/tests/test_grid_dataset.py" startline="35" endline="45" pcid="215">
    def test_shape(self):
        test_dataset = ["vwxyz", "helloworld", "worldfoobar"]
        result = GridPatchDataset(dataset=test_dataset, patch_iter=identity_generator, with_coordinates=False)
        output = []
        n_workers = 0 if sys.platform == "win32" else 2
        for item in DataLoader(result, batch_size=3, num_workers=n_workers):
            output.append("".join(item))
        expected = ["vwx", "wor", "yzh", "ldf", "ell", "oob", "owo", "ar", "rld"]
        self.assertEqual(sorted(output), sorted(expected))
        self.assertEqual(len("".join(expected)), len("".join(test_dataset)))

</source>
<source file="systems/MONAI-0.6.0/tests/test_patch_dataset.py" startline="28" endline="40" pcid="1158">
    def test_shape(self):
        test_dataset = ["vwxyz", "hello", "world"]
        n_per_image = len(test_dataset[0])

        result = PatchDataset(dataset=test_dataset, patch_func=identity, samples_per_image=n_per_image)

        output = []
        n_workers = 0 if sys.platform == "win32" else 2
        for item in DataLoader(result, batch_size=3, num_workers=n_workers):
            output.append("".join(item))
        expected = ["vwx", "yzh", "ell", "owo", "rld"]
        self.assertEqual(output, expected)

</source>
</class>

<class classid="20" nclones="4" nlines="12" similarity="100">
<source file="systems/MONAI-0.6.0/tests/test_handler_hausdorff_distance.py" startline="22" endline="50" pcid="234">
def create_spherical_seg_3d(
    radius: float = 20.0,
    centre: Tuple[int, int, int] = (49, 49, 49),
    im_shape: Tuple[int, int, int] = (99, 99, 99),
) -> np.ndarray:
    """
    Return a 3D image with a sphere inside. Voxel values will be
    1 inside the sphere, and 0 elsewhere.

    Args:
        radius: radius of sphere (in terms of number of voxels, can be partial)
        centre: location of sphere centre.
        im_shape: shape of image to create

    See also:
        :py:meth:`~create_test_image_3d`
    """
    # Create image
    image = np.zeros(im_shape, dtype=np.int32)
    spy, spx, spz = np.ogrid[
        -centre[0] : im_shape[0] - centre[0], -centre[1] : im_shape[1] - centre[1], -centre[2] : im_shape[2] - centre[2]
    ]
    circle = (spx * spx + spy * spy + spz * spz) <= radius * radius

    image[circle] = 1
    image[~circle] = 0
    return image


</source>
<source file="systems/MONAI-0.6.0/tests/test_hausdorff_distance.py" startline="22" endline="50" pcid="1038">
def create_spherical_seg_3d(
    radius: float = 20.0,
    centre: Tuple[int, int, int] = (49, 49, 49),
    im_shape: Tuple[int, int, int] = (99, 99, 99),
) -> np.ndarray:
    """
    Return a 3D image with a sphere inside. Voxel values will be
    1 inside the sphere, and 0 elsewhere.

    Args:
        radius: radius of sphere (in terms of number of voxels, can be partial)
        centre: location of sphere centre.
        im_shape: shape of image to create

    See also:
        :py:meth:`~create_test_image_3d`
    """
    # Create image
    image = np.zeros(im_shape, dtype=np.int32)
    spy, spx, spz = np.ogrid[
        -centre[0] : im_shape[0] - centre[0], -centre[1] : im_shape[1] - centre[1], -centre[2] : im_shape[2] - centre[2]
    ]
    circle = (spx * spx + spy * spy + spz * spz) <= radius * radius

    image[circle] = 1
    image[~circle] = 0
    return image


</source>
<source file="systems/MONAI-0.6.0/tests/test_handler_surface_distance.py" startline="22" endline="50" pcid="1190">
def create_spherical_seg_3d(
    radius: float = 20.0,
    centre: Tuple[int, int, int] = (49, 49, 49),
    im_shape: Tuple[int, int, int] = (99, 99, 99),
) -> np.ndarray:
    """
    Return a 3D image with a sphere inside. Voxel values will be
    1 inside the sphere, and 0 elsewhere.

    Args:
        radius: radius of sphere (in terms of number of voxels, can be partial)
        centre: location of sphere centre.
        im_shape: shape of image to create

    See also:
        :py:meth:`~create_test_image_3d`
    """
    # Create image
    image = np.zeros(im_shape, dtype=np.int32)
    spy, spx, spz = np.ogrid[
        -centre[0] : im_shape[0] - centre[0], -centre[1] : im_shape[1] - centre[1], -centre[2] : im_shape[2] - centre[2]
    ]
    circle = (spx * spx + spy * spy + spz * spz) <= radius * radius

    image[circle] = 1
    image[~circle] = 0
    return image


</source>
<source file="systems/MONAI-0.6.0/tests/test_surface_distance.py" startline="22" endline="50" pcid="238">
def create_spherical_seg_3d(
    radius: float = 20.0,
    centre: Tuple[int, int, int] = (49, 49, 49),
    im_shape: Tuple[int, int, int] = (99, 99, 99),
) -> np.ndarray:
    """
    Return a 3D image with a sphere inside. Voxel values will be
    1 inside the sphere, and 0 elsewhere.

    Args:
        radius: radius of sphere (in terms of number of voxels, can be partial)
        centre: location of sphere centre.
        im_shape: shape of image to create

    See also:
        :py:meth:`~create_test_image_3d`
    """
    # Create image
    image = np.zeros(im_shape, dtype=np.int32)
    spy, spx, spz = np.ogrid[
        -centre[0] : im_shape[0] - centre[0], -centre[1] : im_shape[1] - centre[1], -centre[2] : im_shape[2] - centre[2]
    ]
    circle = (spx * spx + spy * spy + spz * spz) <= radius * radius

    image[circle] = 1
    image[~circle] = 0
    return image


</source>
</class>

<class classid="21" nclones="2" nlines="18" similarity="88">
<source file="systems/MONAI-0.6.0/tests/test_handler_hausdorff_distance.py" startline="65" endline="86" pcid="235">
    def test_compute(self):
        hd_metric = HausdorffDistance(include_background=True)

        def _val_func(engine, batch):
            pass

        engine = Engine(_val_func)
        hd_metric.attach(engine, "hausdorff_distance")

        y_pred, y = TEST_SAMPLE_1
        hd_metric.update([y_pred, y])
        self.assertEqual(hd_metric.compute(), 10)
        y_pred, y = TEST_SAMPLE_2
        hd_metric.update([y_pred, y])
        self.assertEqual(hd_metric.compute(), 5)
        y_pred, y = TEST_SAMPLE_3
        hd_metric.update([y_pred, y])
        self.assertEqual(hd_metric.compute(), float("inf"))
        y_pred, y = TEST_SAMPLE_4
        hd_metric.update([y_pred, y])
        self.assertEqual(hd_metric.compute(), float("inf"))

</source>
<source file="systems/MONAI-0.6.0/tests/test_handler_surface_distance.py" startline="65" endline="86" pcid="1191">
    def test_compute(self):
        sur_metric = SurfaceDistance(include_background=True)

        def _val_func(engine, batch):
            pass

        engine = Engine(_val_func)
        sur_metric.attach(engine, "surface_distance")

        y_pred, y = TEST_SAMPLE_1
        sur_metric.update([y_pred, y])
        self.assertAlmostEqual(sur_metric.compute(), 4.17133, places=4)
        y_pred, y = TEST_SAMPLE_2
        sur_metric.update([y_pred, y])
        self.assertAlmostEqual(sur_metric.compute(), 2.08566, places=4)
        y_pred, y = TEST_SAMPLE_3
        sur_metric.update([y_pred, y])
        self.assertAlmostEqual(sur_metric.compute(), float("inf"))
        y_pred, y = TEST_SAMPLE_4
        sur_metric.update([y_pred, y])
        self.assertAlmostEqual(sur_metric.compute(), float("inf"))

</source>
</class>

<class classid="22" nclones="2" nlines="20" similarity="80">
<source file="systems/MONAI-0.6.0/tests/test_surface_distance.py" startline="124" endline="144" pcid="239">
    def test_value(self, input_data, expected_value):
        if len(input_data) == 3:
            [seg_1, seg_2, metric] = input_data
        else:
            [seg_1, seg_2] = input_data
            metric = "euclidean"
        ct = 0
        seg_1 = torch.tensor(seg_1)
        seg_2 = torch.tensor(seg_2)
        for symmetric in [True, False]:
            sur_metric = SurfaceDistanceMetric(include_background=False, symmetric=symmetric, distance_metric=metric)
            # shape of seg_1, seg_2 are: HWD, converts to BNHWD
            batch, n_class = 2, 3
            batch_seg_1 = seg_1.unsqueeze(0).unsqueeze(0).repeat([batch, n_class, 1, 1, 1])
            batch_seg_2 = seg_2.unsqueeze(0).unsqueeze(0).repeat([batch, n_class, 1, 1, 1])
            sur_metric(batch_seg_1, batch_seg_2)
            result = sur_metric.aggregate()
            expected_value_curr = expected_value[ct]
            np.testing.assert_allclose(expected_value_curr, result, rtol=1e-7)
            ct += 1

</source>
<source file="systems/MONAI-0.6.0/tests/test_hausdorff_distance.py" startline="116" endline="139" pcid="1039">
    def test_value(self, input_data, expected_value):
        percentile = None
        if len(input_data) == 3:
            [seg_1, seg_2, percentile] = input_data
        else:
            [seg_1, seg_2] = input_data
        ct = 0
        seg_1 = torch.tensor(seg_1)
        seg_2 = torch.tensor(seg_2)
        for metric in ["euclidean", "chessboard", "taxicab"]:
            for directed in [True, False]:
                hd_metric = HausdorffDistanceMetric(
                    include_background=False, distance_metric=metric, percentile=percentile, directed=directed
                )
                # shape of seg_1, seg_2 are: HWD, converts to BNHWD
                batch, n_class = 2, 3
                batch_seg_1 = seg_1.unsqueeze(0).unsqueeze(0).repeat([batch, n_class, 1, 1, 1])
                batch_seg_2 = seg_2.unsqueeze(0).unsqueeze(0).repeat([batch, n_class, 1, 1, 1])
                hd_metric(batch_seg_1, batch_seg_2)
                result = hd_metric.aggregate()
                expected_value_curr = expected_value[ct]
                np.testing.assert_allclose(expected_value_curr, result, rtol=1e-7)
                ct += 1

</source>
</class>

<class classid="23" nclones="2" nlines="11" similarity="81">
<source file="systems/MONAI-0.6.0/tests/test_surface_distance.py" startline="146" endline="159" pcid="240">
    def test_nans(self, input_data):
        [seg_1, seg_2] = input_data
        seg_1 = torch.tensor(seg_1)
        seg_2 = torch.tensor(seg_2)
        sur_metric = SurfaceDistanceMetric(include_background=False, get_not_nans=True)
        # test list of channel-first Tensor
        batch_seg_1 = [seg_1.unsqueeze(0)]
        batch_seg_2 = [seg_2.unsqueeze(0)]
        sur_metric(batch_seg_1, batch_seg_2)
        result, not_nans = sur_metric.aggregate()
        np.testing.assert_allclose(0, result, rtol=1e-7)
        np.testing.assert_allclose(0, not_nans, rtol=1e-7)


</source>
<source file="systems/MONAI-0.6.0/tests/test_hausdorff_distance.py" startline="141" endline="153" pcid="1040">
    def test_nans(self, input_data):
        [seg_1, seg_2] = input_data
        seg_1 = torch.tensor(seg_1)
        seg_2 = torch.tensor(seg_2)
        hd_metric = HausdorffDistanceMetric(include_background=False, get_not_nans=True)
        batch_seg_1 = seg_1.unsqueeze(0).unsqueeze(0)
        batch_seg_2 = seg_2.unsqueeze(0).unsqueeze(0)
        hd_metric(batch_seg_1, batch_seg_2)
        result, not_nans = hd_metric.aggregate()
        np.testing.assert_allclose(0, result, rtol=1e-7)
        np.testing.assert_allclose(0, not_nans, rtol=1e-7)


</source>
</class>

<class classid="24" nclones="2" nlines="15" similarity="86">
<source file="systems/MONAI-0.6.0/tests/test_load_image.py" startline="78" endline="93" pcid="268">
    def test_nibabel_reader(self, input_param, filenames, expected_shape):
        test_image = np.random.rand(128, 128, 128)
        with tempfile.TemporaryDirectory() as tempdir:
            for i, name in enumerate(filenames):
                filenames[i] = os.path.join(tempdir, name)
                nib.save(nib.Nifti1Image(test_image, np.eye(4)), filenames[i])
            result = LoadImage(**input_param)(filenames)

            if isinstance(result, tuple):
                result, header = result
                self.assertTrue("affine" in header)
                self.assertEqual(header["filename_or_obj"], os.path.join(tempdir, "test_image.nii.gz"))
                np.testing.assert_allclose(header["affine"], np.eye(4))
                np.testing.assert_allclose(header["original_affine"], np.eye(4))
            self.assertTupleEqual(result.shape, expected_shape)

</source>
<source file="systems/MONAI-0.6.0/tests/test_load_image.py" startline="95" endline="111" pcid="269">
    def test_itk_reader(self, input_param, filenames, expected_shape):
        test_image = np.random.rand(128, 128, 128)
        with tempfile.TemporaryDirectory() as tempdir:
            for i, name in enumerate(filenames):
                filenames[i] = os.path.join(tempdir, name)
                itk_np_view = itk.image_view_from_array(test_image)
                itk.imwrite(itk_np_view, filenames[i])
            result = LoadImage(**input_param)(filenames)

            if isinstance(result, tuple):
                result, header = result
                self.assertTrue("affine" in header)
                self.assertEqual(header["filename_or_obj"], os.path.join(tempdir, "test_image.nii.gz"))
                np.testing.assert_allclose(header["affine"], np.eye(4))
                np.testing.assert_allclose(header["original_affine"], np.eye(4))
            self.assertTupleEqual(result.shape, expected_shape)

</source>
</class>

<class classid="25" nclones="16" nlines="13" similarity="71">
<source file="systems/MONAI-0.6.0/tests/test_rand_weighted_cropd.py" startline="21" endline="35" pcid="275">
    def test_rand_weighted_crop_small_roi(self):
        img = self.seg1[0]
        n_samples = 3
        crop = RandWeightedCropd("img", "w", (10, 12), n_samples)
        weight = np.zeros_like(img)
        weight[0, 30, 17] = 1.1
        weight[0, 40, 31] = 1
        weight[0, 80, 21] = 1
        crop.set_random_state(10)
        d = {"img": img, "w": weight}
        result = crop(d)
        self.assertTrue(len(result) == n_samples)
        np.testing.assert_allclose(result[0]["img"].shape, (1, 10, 12))
        np.testing.assert_allclose(np.asarray(crop.centers), [[80, 21], [30, 17], [40, 31]])

</source>
<source file="systems/MONAI-0.6.0/tests/test_rand_weighted_cropd.py" startline="52" endline="67" pcid="277">
    def test_rand_weighted_crop_large_roi(self):
        img = self.segn[0]
        n_samples = 3
        crop = RandWeightedCropd(("img", "seg"), "weight", (10000, 400), n_samples, "location")
        weight = np.zeros_like(img)
        weight[0, 30, 17] = 1.1
        weight[0, 10, 1] = 1
        crop.set_random_state(10)
        data = {"img": img, "seg": self.imt[0], "weight": weight}
        result = crop(data)
        self.assertTrue(len(result) == n_samples)
        np.testing.assert_allclose(result[0]["img"].shape, (1, 128, 64))
        np.testing.assert_allclose(result[0]["seg"].shape, (1, 128, 64))
        np.testing.assert_allclose(np.asarray(crop.centers), [[64, 32], [64, 32], [64, 32]])
        np.testing.assert_allclose(result[1]["location"], [64, 32])

</source>
<source file="systems/MONAI-0.6.0/tests/test_rand_weighted_cropd.py" startline="36" endline="51" pcid="276">
    def test_rand_weighted_crop_default_roi(self):
        img = self.imt[0]
        n_samples = 3
        crop = RandWeightedCropd("im", "weight", (10, -1), n_samples, "coords")
        weight = np.zeros_like(img)
        weight[0, 30, 17] = 1.1
        weight[0, 40, 31] = 1
        weight[0, 80, 21] = 1
        crop.set_random_state(10)
        data = {"im": img, "weight": weight, "others": np.nan}
        result = crop(data)
        self.assertTrue(len(result) == n_samples)
        np.testing.assert_allclose(result[0]["im"].shape, (1, 10, 64))
        np.testing.assert_allclose(np.asarray(crop.centers), [[14, 32], [105, 32], [20, 32]])
        np.testing.assert_allclose(result[1]["coords"], [105, 32])

</source>
<source file="systems/MONAI-0.6.0/tests/test_rand_weighted_crop.py" startline="35" endline="48" pcid="1161">
    def test_rand_weighted_crop_default_roi(self):
        img = self.imt[0]
        n_samples = 3
        crop = RandWeightedCrop((10, -1), n_samples)
        weight = np.zeros_like(img)
        weight[0, 30, 17] = 1.1
        weight[0, 40, 31] = 1
        weight[0, 80, 21] = 1
        crop.set_random_state(10)
        result = crop(img, weight)
        self.assertTrue(len(result) == n_samples)
        np.testing.assert_allclose(result[0].shape, (1, 10, 64))
        np.testing.assert_allclose(np.asarray(crop.centers), [[14, 32], [105, 32], [20, 32]])

</source>
<source file="systems/MONAI-0.6.0/tests/test_rand_weighted_crop.py" startline="21" endline="34" pcid="1160">
    def test_rand_weighted_crop_small_roi(self):
        img = self.seg1[0]
        n_samples = 3
        crop = RandWeightedCrop((10, 12), n_samples)
        weight = np.zeros_like(img)
        weight[0, 30, 17] = 1.1
        weight[0, 40, 31] = 1
        weight[0, 80, 21] = 1
        crop.set_random_state(10)
        result = crop(img, weight)
        self.assertTrue(len(result) == n_samples)
        np.testing.assert_allclose(result[0].shape, (1, 10, 12))
        np.testing.assert_allclose(np.asarray(crop.centers), [[80, 21], [30, 17], [40, 31]])

</source>
<source file="systems/MONAI-0.6.0/tests/test_rand_weighted_crop.py" startline="49" endline="63" pcid="1162">
    def test_rand_weighted_crop_large_roi(self):
        img = self.segn[0]
        n_samples = 3
        crop = RandWeightedCrop((10000, 400), n_samples)
        weight = np.zeros_like(img)
        weight[0, 30, 17] = 1.1
        weight[0, 10, 1] = 1
        crop.set_random_state(10)
        result = crop(img, weight)
        self.assertTrue(len(result) == n_samples)
        np.testing.assert_allclose(result[0].shape, (1, 128, 64))
        np.testing.assert_allclose(np.asarray(crop.centers), [[64, 32], [64, 32], [64, 32]])
        for res in result:
            np.testing.assert_allclose(res, self.segn[0])

</source>
<source file="systems/MONAI-0.6.0/tests/test_rand_weighted_crop.py" startline="64" endline="78" pcid="1163">
    def test_rand_weighted_crop_bad_w(self):
        img = self.imt[0]
        n_samples = 3
        crop = RandWeightedCrop((20, 40), n_samples)
        weight = np.zeros_like(img)
        weight[0, 30, 17] = np.inf
        weight[0, 10, 1] = -np.inf
        weight[0, 10, 20] = -np.nan
        crop.set_random_state(10)
        result = crop(img, weight)
        self.assertTrue(len(result) == n_samples)
        np.testing.assert_allclose(result[0].shape, (1, 20, 40))
        np.testing.assert_allclose(np.asarray(crop.centers), [[63, 37], [31, 43], [66, 20]])


</source>
<source file="systems/MONAI-0.6.0/tests/test_rand_weighted_crop.py" startline="123" endline="137" pcid="1167">
    def test_rand_weighted_crop_bad_w(self):
        img = self.imt[0]
        n_samples = 3
        crop = RandWeightedCrop((48, 64, 80), n_samples)
        weight = np.zeros_like(img)
        weight[0, 30, 17] = np.inf
        weight[0, 10, 1] = -np.inf
        weight[0, 10, 20] = -np.nan
        crop.set_random_state(10)
        result = crop(img, weight)
        self.assertTrue(len(result) == n_samples)
        np.testing.assert_allclose(result[0].shape, (1, 48, 64, 80))
        np.testing.assert_allclose(np.asarray(crop.centers), [[24, 32, 40], [24, 32, 40], [24, 32, 40]])


</source>
<source file="systems/MONAI-0.6.0/tests/test_rand_weighted_cropd.py" startline="68" endline="83" pcid="278">
    def test_rand_weighted_crop_bad_w(self):
        img = self.imt[0]
        n_samples = 3
        crop = RandWeightedCropd(("img", "seg"), "w", (20, 40), n_samples)
        weight = np.zeros_like(img)
        weight[0, 30, 17] = np.inf
        weight[0, 10, 1] = -np.inf
        weight[0, 10, 20] = -np.nan
        crop.set_random_state(10)
        result = crop({"img": img, "seg": self.segn[0], "w": weight})
        self.assertTrue(len(result) == n_samples)
        np.testing.assert_allclose(result[0]["img"].shape, (1, 20, 40))
        np.testing.assert_allclose(result[0]["seg"].shape, (1, 20, 40))
        np.testing.assert_allclose(np.asarray(crop.centers), [[63, 37], [31, 43], [66, 20]])


</source>
<source file="systems/MONAI-0.6.0/tests/test_rand_weighted_crop.py" startline="80" endline="93" pcid="1164">
    def test_rand_weighted_crop_small_roi(self):
        img = self.seg1[0]
        n_samples = 3
        crop = RandWeightedCrop((8, 10, 12), n_samples)
        weight = np.zeros_like(img)
        weight[0, 5, 30, 17] = 1.1
        weight[0, 8, 40, 31] = 1
        weight[0, 11, 23, 21] = 1
        crop.set_random_state(10)
        result = crop(img, weight)
        self.assertTrue(len(result) == n_samples)
        np.testing.assert_allclose(result[0].shape, (1, 8, 10, 12))
        np.testing.assert_allclose(np.asarray(crop.centers), [[11, 23, 21], [5, 30, 17], [8, 40, 31]])

</source>
<source file="systems/MONAI-0.6.0/tests/test_rand_weighted_cropd.py" startline="127" endline="141" pcid="282">
    def test_rand_weighted_crop_bad_w(self):
        img = self.imt[0]
        n_samples = 3
        crop = RandWeightedCropd(("img", "seg"), "w", (48, 64, 80), n_samples)
        weight = np.zeros_like(img)
        weight[0, 30, 17] = np.inf
        weight[0, 10, 1] = -np.inf
        weight[0, 10, 20] = -np.nan
        crop.set_random_state(10)
        result = crop({"img": img, "seg": self.segn[0], "w": weight})
        self.assertTrue(len(result) == n_samples)
        np.testing.assert_allclose(result[0]["img"].shape, (1, 48, 64, 80))
        np.testing.assert_allclose(result[0]["seg"].shape, (1, 48, 64, 80))
        np.testing.assert_allclose(np.asarray(crop.centers), [[24, 32, 40], [24, 32, 40], [24, 32, 40]])

</source>
<source file="systems/MONAI-0.6.0/tests/test_rand_weighted_crop.py" startline="108" endline="122" pcid="1166">
    def test_rand_weighted_crop_large_roi(self):
        img = self.segn[0]
        n_samples = 3
        crop = RandWeightedCrop((10000, 400, 80), n_samples)
        weight = np.zeros_like(img)
        weight[0, 30, 17, 20] = 1.1
        weight[0, 10, 1, 17] = 1
        crop.set_random_state(10)
        result = crop(img, weight)
        self.assertTrue(len(result) == n_samples)
        np.testing.assert_allclose(result[0].shape, (1, 48, 64, 80))
        np.testing.assert_allclose(np.asarray(crop.centers), [[24, 32, 40], [24, 32, 40], [24, 32, 40]])
        for res in result:
            np.testing.assert_allclose(res, self.segn[0])

</source>
<source file="systems/MONAI-0.6.0/tests/test_rand_weighted_cropd.py" startline="85" endline="98" pcid="279">
    def test_rand_weighted_crop_small_roi(self):
        img = self.seg1[0]
        n_samples = 3
        crop = RandWeightedCropd("img", "w", (8, 10, 12), n_samples)
        weight = np.zeros_like(img)
        weight[0, 5, 30, 17] = 1.1
        weight[0, 8, 40, 31] = 1
        weight[0, 11, 23, 21] = 1
        crop.set_random_state(10)
        result = crop({"img": img, "w": weight})
        self.assertTrue(len(result) == n_samples)
        np.testing.assert_allclose(result[0]["img"].shape, (1, 8, 10, 12))
        np.testing.assert_allclose(np.asarray(crop.centers), [[11, 23, 21], [5, 30, 17], [8, 40, 31]])

</source>
<source file="systems/MONAI-0.6.0/tests/test_rand_weighted_cropd.py" startline="99" endline="113" pcid="280">
    def test_rand_weighted_crop_default_roi(self):
        img = self.imt[0]
        n_samples = 3
        crop = RandWeightedCropd(("img", "seg"), "w", (10, -1, -1), n_samples)
        weight = np.zeros_like(img)
        weight[0, 7, 17] = 1.1
        weight[0, 13, 31] = 1.1
        weight[0, 24, 21] = 1
        crop.set_random_state(10)
        result = crop({"img": img, "seg": self.segn[0], "w": weight})
        self.assertTrue(len(result) == n_samples)
        np.testing.assert_allclose(result[0]["img"].shape, (1, 10, 64, 80))
        np.testing.assert_allclose(result[0]["seg"].shape, (1, 10, 64, 80))
        np.testing.assert_allclose(np.asarray(crop.centers), [[14, 32, 40], [41, 32, 40], [20, 32, 40]])

</source>
<source file="systems/MONAI-0.6.0/tests/test_rand_weighted_cropd.py" startline="114" endline="126" pcid="281">
    def test_rand_weighted_crop_large_roi(self):
        img = self.segn[0]
        n_samples = 3
        crop = RandWeightedCropd("img", "w", (10000, 400, 80), n_samples)
        weight = np.zeros_like(img)
        weight[0, 30, 17, 20] = 1.1
        weight[0, 10, 1, 17] = 1
        crop.set_random_state(10)
        result = crop({"img": img, "w": weight})
        self.assertTrue(len(result) == n_samples)
        np.testing.assert_allclose(result[0]["img"].shape, (1, 48, 64, 80))
        np.testing.assert_allclose(np.asarray(crop.centers), [[24, 32, 40], [24, 32, 40], [24, 32, 40]])

</source>
<source file="systems/MONAI-0.6.0/tests/test_rand_weighted_crop.py" startline="94" endline="107" pcid="1165">
    def test_rand_weighted_crop_default_roi(self):
        img = self.imt[0]
        n_samples = 3
        crop = RandWeightedCrop((10, -1, -1), n_samples)
        weight = np.zeros_like(img)
        weight[0, 7, 17] = 1.1
        weight[0, 13, 31] = 1.1
        weight[0, 24, 21] = 1
        crop.set_random_state(10)
        result = crop(img, weight)
        self.assertTrue(len(result) == n_samples)
        np.testing.assert_allclose(result[0].shape, (1, 10, 64, 80))
        np.testing.assert_allclose(np.asarray(crop.centers), [[14, 32, 40], [41, 32, 40], [20, 32, 40]])

</source>
</class>

<class classid="26" nclones="3" nlines="21" similarity="81">
<source file="systems/MONAI-0.6.0/tests/test_vis_gradcam.py" startline="66" endline="91" pcid="298">
    def test_shape(self, input_data, expected_shape):
        if input_data["model"] == "densenet2d":
            model = DenseNet121(spatial_dims=2, in_channels=1, out_channels=3)
        if input_data["model"] == "densenet3d":
            model = DenseNet(
                spatial_dims=3, in_channels=1, out_channels=3, init_features=2, growth_rate=2, block_config=(6,)
            )
        if input_data["model"] == "senet2d":
            model = SEResNet50(spatial_dims=2, in_channels=3, num_classes=4)
        if input_data["model"] == "senet3d":
            model = SEResNet50(spatial_dims=3, in_channels=3, num_classes=4)
        device = "cuda:0" if torch.cuda.is_available() else "cpu"
        model.to(device)
        model.eval()
        cam = GradCAM(nn_module=model, target_layers=input_data["target_layers"])
        image = torch.rand(input_data["shape"], device=device)
        result = cam(x=image, layer_idx=-1)
        np.testing.assert_array_equal(cam.nn_module.class_idx.cpu(), model(image).max(1)[-1].cpu())
        fea_shape = cam.feature_map_size(input_data["shape"], device=device)
        self.assertTupleEqual(fea_shape, input_data["feature_shape"])
        self.assertTupleEqual(result.shape, expected_shape)
        # check result is same whether class_idx=None is used or not
        result2 = cam(x=image, layer_idx=-1, class_idx=model(image).max(1)[-1].cpu())
        torch.testing.assert_allclose(result, result2)


</source>
<source file="systems/MONAI-0.6.0/tests/test_vis_gradcampp.py" startline="65" endline="86" pcid="835">
    def test_shape(self, input_data, expected_shape):
        if input_data["model"] == "densenet2d":
            model = DenseNet121(spatial_dims=2, in_channels=1, out_channels=3)
        if input_data["model"] == "densenet3d":
            model = DenseNet(
                spatial_dims=3, in_channels=1, out_channels=3, init_features=2, growth_rate=2, block_config=(6,)
            )
        if input_data["model"] == "senet2d":
            model = SEResNet50(spatial_dims=2, in_channels=3, num_classes=4)
        if input_data["model"] == "senet3d":
            model = SEResNet50(spatial_dims=3, in_channels=3, num_classes=4)
        device = "cuda:0" if torch.cuda.is_available() else "cpu"
        model.to(device)
        model.eval()
        cam = GradCAMpp(nn_module=model, target_layers=input_data["target_layers"])
        image = torch.rand(input_data["shape"], device=device)
        result = cam(x=image, layer_idx=-1)
        fea_shape = cam.feature_map_size(input_data["shape"], device=device)
        self.assertTupleEqual(fea_shape, input_data["feature_shape"])
        self.assertTupleEqual(result.shape, expected_shape)


</source>
<source file="systems/MONAI-0.6.0/tests/test_vis_cam.py" startline="69" endline="90" pcid="466">
    def test_shape(self, input_data, expected_shape):
        if input_data["model"] == "densenet2d":
            model = DenseNet121(spatial_dims=2, in_channels=1, out_channels=3)
        if input_data["model"] == "densenet3d":
            model = DenseNet(
                spatial_dims=3, in_channels=1, out_channels=3, init_features=2, growth_rate=2, block_config=(6,)
            )
        if input_data["model"] == "senet2d":
            model = SEResNet50(spatial_dims=2, in_channels=3, num_classes=4)
        if input_data["model"] == "senet3d":
            model = SEResNet50(spatial_dims=3, in_channels=3, num_classes=4)
        device = "cuda:0" if torch.cuda.is_available() else "cpu"
        model.to(device)
        model.eval()
        cam = CAM(nn_module=model, target_layers=input_data["target_layers"], fc_layers=input_data["fc_layers"])
        image = torch.rand(input_data["shape"], device=device)
        result = cam(x=image, layer_idx=-1)
        fea_shape = cam.feature_map_size(input_data["shape"], device=device)
        self.assertTupleEqual(fea_shape, input_data["feature_shape"])
        self.assertTupleEqual(result.shape, expected_shape)


</source>
</class>

<class classid="27" nclones="4" nlines="19" similarity="75">
<source file="systems/MONAI-0.6.0/tests/test_focal_loss.py" startline="24" endline="46" pcid="321">
    def test_consistency_with_cross_entropy_2d(self):
        """For gamma=0 the focal loss reduces to the cross entropy loss"""
        focal_loss = FocalLoss(to_onehot_y=False, gamma=0.0, reduction="mean", weight=1.0)
        ce = nn.BCEWithLogitsLoss(reduction="mean")
        max_error = 0
        class_num = 10
        batch_size = 128
        for _ in range(100):
            # Create a random tensor of shape (batch_size, class_num, 8, 4)
            x = torch.rand(batch_size, class_num, 8, 4, requires_grad=True)
            # Create a random batch of classes
            l = torch.randint(low=0, high=2, size=(batch_size, class_num, 8, 4)).float()
            if torch.cuda.is_available():
                x = x.cuda()
                l = l.cuda()
            output0 = focal_loss(x, l)
            output1 = ce(x, l)
            a = float(output0.cpu().detach())
            b = float(output1.cpu().detach())
            if abs(a - b) > max_error:
                max_error = abs(a - b)
        self.assertAlmostEqual(max_error, 0.0, places=3)

</source>
<source file="systems/MONAI-0.6.0/tests/test_focal_loss.py" startline="70" endline="93" pcid="323">
    def test_consistency_with_cross_entropy_classification(self):
        """for gamma=0 the focal loss reduces to the cross entropy loss"""
        focal_loss = FocalLoss(to_onehot_y=True, gamma=0.0, reduction="mean")
        ce = nn.BCEWithLogitsLoss(reduction="mean")
        max_error = 0
        class_num = 10
        batch_size = 128
        for _ in range(100):
            # Create a random scores tensor of shape (batch_size, class_num)
            x = torch.rand(batch_size, class_num, requires_grad=True)
            # Create a random batch of classes
            l = torch.randint(low=0, high=class_num, size=(batch_size, 1))
            l = l.long()
            if torch.cuda.is_available():
                x = x.cuda()
                l = l.cuda()
            output0 = focal_loss(x, l)
            output1 = ce(x, one_hot(l, num_classes=class_num))
            a = float(output0.cpu().detach())
            b = float(output1.cpu().detach())
            if abs(a - b) > max_error:
                max_error = abs(a - b)
        self.assertAlmostEqual(max_error, 0.0, places=3)

</source>
<source file="systems/MONAI-0.6.0/tests/test_focal_loss.py" startline="94" endline="117" pcid="324">
    def test_consistency_with_cross_entropy_classification_01(self):
        # for gamma=0.1 the focal loss differs from the cross entropy loss
        focal_loss = FocalLoss(to_onehot_y=True, gamma=0.1, reduction="mean")
        ce = nn.BCEWithLogitsLoss(reduction="mean")
        max_error = 0
        class_num = 10
        batch_size = 128
        for _ in range(100):
            # Create a random scores tensor of shape (batch_size, class_num)
            x = torch.rand(batch_size, class_num, requires_grad=True)
            # Create a random batch of classes
            l = torch.randint(low=0, high=class_num, size=(batch_size, 1))
            l = l.long()
            if torch.cuda.is_available():
                x = x.cuda()
                l = l.cuda()
            output0 = focal_loss(x, l)
            output1 = ce(x, one_hot(l, num_classes=class_num))
            a = float(output0.cpu().detach())
            b = float(output1.cpu().detach())
            if abs(a - b) > max_error:
                max_error = abs(a - b)
        self.assertNotAlmostEqual(max_error, 0.0, places=3)

</source>
<source file="systems/MONAI-0.6.0/tests/test_focal_loss.py" startline="47" endline="69" pcid="322">
    def test_consistency_with_cross_entropy_2d_onehot_label(self):
        """For gamma=0 the focal loss reduces to the cross entropy loss"""
        focal_loss = FocalLoss(to_onehot_y=True, gamma=0.0, reduction="mean")
        ce = nn.BCEWithLogitsLoss(reduction="mean")
        max_error = 0
        class_num = 10
        batch_size = 128
        for _ in range(100):
            # Create a random tensor of shape (batch_size, class_num, 8, 4)
            x = torch.rand(batch_size, class_num, 8, 4, requires_grad=True)
            # Create a random batch of classes
            l = torch.randint(low=0, high=class_num, size=(batch_size, 1, 8, 4))
            if torch.cuda.is_available():
                x = x.cuda()
                l = l.cuda()
            output0 = focal_loss(x, l)
            output1 = ce(x, one_hot(l, num_classes=class_num))
            a = float(output0.cpu().detach())
            b = float(output1.cpu().detach())
            if abs(a - b) > max_error:
                max_error = abs(a - b)
        self.assertAlmostEqual(max_error, 0.0, places=3)

</source>
</class>

<class classid="28" nclones="2" nlines="10" similarity="100">
<source file="systems/MONAI-0.6.0/tests/test_numpy_reader.py" startline="22" endline="33" pcid="376">
    def test_npy(self):
        test_data = np.random.randint(0, 256, size=[3, 4, 4])
        with tempfile.TemporaryDirectory() as tempdir:
            filepath = os.path.join(tempdir, "test_data.npy")
            np.save(filepath, test_data)

            reader = NumpyReader()
            result = reader.get_data(reader.read(filepath))
        self.assertTupleEqual(result[1]["spatial_shape"], test_data.shape)
        self.assertTupleEqual(result[0].shape, test_data.shape)
        np.testing.assert_allclose(result[0], test_data)

</source>
<source file="systems/MONAI-0.6.0/tests/test_numpy_reader.py" startline="34" endline="45" pcid="377">
    def test_npz1(self):
        test_data1 = np.random.randint(0, 256, size=[3, 4, 4])
        with tempfile.TemporaryDirectory() as tempdir:
            filepath = os.path.join(tempdir, "test_data.npy")
            np.save(filepath, test_data1)

            reader = NumpyReader()
            result = reader.get_data(reader.read(filepath))
        self.assertTupleEqual(result[1]["spatial_shape"], test_data1.shape)
        self.assertTupleEqual(result[0].shape, test_data1.shape)
        np.testing.assert_allclose(result[0], test_data1)

</source>
</class>

<class classid="29" nclones="2" nlines="11" similarity="81">
<source file="systems/MONAI-0.6.0/tests/test_numpy_reader.py" startline="46" endline="58" pcid="378">
    def test_npz2(self):
        test_data1 = np.random.randint(0, 256, size=[3, 4, 4])
        test_data2 = np.random.randint(0, 256, size=[3, 4, 4])
        with tempfile.TemporaryDirectory() as tempdir:
            filepath = os.path.join(tempdir, "test_data.npz")
            np.savez(filepath, test_data1, test_data2)

            reader = NumpyReader()
            result = reader.get_data(reader.read(filepath))
        self.assertTupleEqual(result[1]["spatial_shape"], test_data1.shape)
        self.assertTupleEqual(result[0].shape, (2, 3, 4, 4))
        np.testing.assert_allclose(result[0], np.stack([test_data1, test_data2]))

</source>
<source file="systems/MONAI-0.6.0/tests/test_numpy_reader.py" startline="59" endline="71" pcid="379">
    def test_npz3(self):
        test_data1 = np.random.randint(0, 256, size=[3, 4, 4])
        test_data2 = np.random.randint(0, 256, size=[3, 4, 4])
        with tempfile.TemporaryDirectory() as tempdir:
            filepath = os.path.join(tempdir, "test_data.npz")
            np.savez(filepath, test1=test_data1, test2=test_data2)

            reader = NumpyReader(npz_keys=["test1", "test2"])
            result = reader.get_data(reader.read(filepath))
        self.assertTupleEqual(result[1]["spatial_shape"], test_data1.shape)
        self.assertTupleEqual(result[0].shape, (2, 3, 4, 4))
        np.testing.assert_allclose(result[0], np.stack([test_data1, test_data2]))

</source>
</class>

<class classid="30" nclones="4" nlines="27" similarity="85">
<source file="systems/MONAI-0.6.0/tests/test_handler_regression_metrics_dist.py" startline="63" endline="102" pcid="391">
    def _compute(self):
        device = f"cuda:{dist.get_rank()}" if torch.cuda.is_available() else "cpu"
        metric = MeanSquaredError()

        def _val_func(engine, batch):
            pass

        engine = Engine(_val_func)
        metric.attach(engine, "MSE")

        # get testing data
        batch = BATCH_SIZE
        base = BASE_DIM_SIZE
        spatial = SPATIAL_DIM
        in_tensor_a1 = torch.rand((batch,) + (base,) * (spatial - 1))
        in_tensor_b1 = torch.rand((batch,) + (base,) * (spatial - 1))

        in_tensor_a2 = torch.rand((batch,) + (base,) * (spatial - 1))
        in_tensor_b2 = torch.rand((batch,) + (base,) * (spatial - 1))

        if dist.get_rank() == 0:
            y_pred = in_tensor_a1.to(device)
            y = in_tensor_b1.to(device)
            metric.update([y_pred, y])

        if dist.get_rank() == 1:
            y_pred = in_tensor_a2.to(device)
            y = in_tensor_b2.to(device)
            metric.update([y_pred, y])

        out_tensor = metric.compute()

        # do numpy functions to get ground truth referece
        out_tensor_np1 = msemetric_np(y_pred=in_tensor_a1.cpu().numpy(), y=in_tensor_b1.cpu().numpy())
        out_tensor_np2 = msemetric_np(y_pred=in_tensor_a2.cpu().numpy(), y=in_tensor_b2.cpu().numpy())
        out_tensor_np = (out_tensor_np1 + out_tensor_np2) / 2.0

        np.testing.assert_allclose(out_tensor, out_tensor_np, rtol=1e-04, atol=1e-04)


</source>
<source file="systems/MONAI-0.6.0/tests/test_handler_regression_metrics_dist.py" startline="109" endline="148" pcid="394">
    def _compute(self):
        device = f"cuda:{dist.get_rank()}" if torch.cuda.is_available() else "cpu"
        metric = MeanAbsoluteError()

        def _val_func(engine, batch):
            pass

        engine = Engine(_val_func)
        metric.attach(engine, "MAE")

        # get testing data
        batch = BATCH_SIZE
        base = BASE_DIM_SIZE
        spatial = SPATIAL_DIM
        in_tensor_a1 = torch.rand((batch,) + (base,) * (spatial - 1))
        in_tensor_b1 = torch.rand((batch,) + (base,) * (spatial - 1))

        in_tensor_a2 = torch.rand((batch,) + (base,) * (spatial - 1))
        in_tensor_b2 = torch.rand((batch,) + (base,) * (spatial - 1))

        if dist.get_rank() == 0:
            y_pred = in_tensor_a1.to(device)
            y = in_tensor_b1.to(device)
            metric.update([y_pred, y])

        if dist.get_rank() == 1:
            y_pred = in_tensor_a2.to(device)
            y = in_tensor_b2.to(device)
            metric.update([y_pred, y])

        out_tensor = metric.compute()

        # do numpy functions to get ground truth referece
        out_tensor_np1 = maemetric_np(y_pred=in_tensor_a1.cpu().numpy(), y=in_tensor_b1.cpu().numpy())
        out_tensor_np2 = maemetric_np(y_pred=in_tensor_a2.cpu().numpy(), y=in_tensor_b2.cpu().numpy())
        out_tensor_np = (out_tensor_np1 + out_tensor_np2) / 2.0

        np.testing.assert_allclose(out_tensor, out_tensor_np, rtol=1e-04, atol=1e-04)


</source>
<source file="systems/MONAI-0.6.0/tests/test_handler_regression_metrics_dist.py" startline="155" endline="194" pcid="397">
    def _compute(self):
        device = f"cuda:{dist.get_rank()}" if torch.cuda.is_available() else "cpu"
        metric = RootMeanSquaredError()

        def _val_func(engine, batch):
            pass

        engine = Engine(_val_func)
        metric.attach(engine, "RMSE")

        # get testing data
        batch = BATCH_SIZE
        base = BASE_DIM_SIZE
        spatial = SPATIAL_DIM
        in_tensor_a1 = torch.rand((batch,) + (base,) * (spatial - 1))
        in_tensor_b1 = torch.rand((batch,) + (base,) * (spatial - 1))

        in_tensor_a2 = torch.rand((batch,) + (base,) * (spatial - 1))
        in_tensor_b2 = torch.rand((batch,) + (base,) * (spatial - 1))

        if dist.get_rank() == 0:
            y_pred = in_tensor_a1.to(device)
            y = in_tensor_b1.to(device)
            metric.update([y_pred, y])

        if dist.get_rank() == 1:
            y_pred = in_tensor_a2.to(device)
            y = in_tensor_b2.to(device)
            metric.update([y_pred, y])

        out_tensor = metric.compute()

        # do numpy functions to get ground truth referece
        out_tensor_np1 = rmsemetric_np(y_pred=in_tensor_a1.cpu().numpy(), y=in_tensor_b1.cpu().numpy())
        out_tensor_np2 = rmsemetric_np(y_pred=in_tensor_a2.cpu().numpy(), y=in_tensor_b2.cpu().numpy())
        out_tensor_np = (out_tensor_np1 + out_tensor_np2) / 2.0

        np.testing.assert_allclose(out_tensor, out_tensor_np, rtol=1e-04, atol=1e-04)


</source>
<source file="systems/MONAI-0.6.0/tests/test_handler_regression_metrics_dist.py" startline="201" endline="241" pcid="400">
    def _compute(self):
        device = f"cuda:{dist.get_rank()}" if torch.cuda.is_available() else "cpu"
        max_val = 1.0
        metric = PeakSignalToNoiseRatio(max_val=max_val)

        def _val_func(engine, batch):
            pass

        engine = Engine(_val_func)
        metric.attach(engine, "PSNR")

        # get testing data
        batch = BATCH_SIZE
        base = BASE_DIM_SIZE
        spatial = SPATIAL_DIM
        in_tensor_a1 = torch.rand((batch,) + (base,) * (spatial - 1))
        in_tensor_b1 = torch.rand((batch,) + (base,) * (spatial - 1))

        in_tensor_a2 = torch.rand((batch,) + (base,) * (spatial - 1))
        in_tensor_b2 = torch.rand((batch,) + (base,) * (spatial - 1))

        if dist.get_rank() == 0:
            y_pred = in_tensor_a1.to(device)
            y = in_tensor_b1.to(device)
            metric.update([y_pred, y])

        if dist.get_rank() == 1:
            y_pred = in_tensor_a2.to(device)
            y = in_tensor_b2.to(device)
            metric.update([y_pred, y])

        out_tensor = metric.compute()

        # do numpy functions to get ground truth referece
        out_tensor_np1 = psnrmetric_np(max_val=max_val, y_pred=in_tensor_a1.cpu().numpy(), y=in_tensor_b1.cpu().numpy())
        out_tensor_np2 = psnrmetric_np(max_val=max_val, y_pred=in_tensor_a2.cpu().numpy(), y=in_tensor_b2.cpu().numpy())
        out_tensor_np = (out_tensor_np1 + out_tensor_np2) / 2.0

        np.testing.assert_allclose(out_tensor, out_tensor_np, rtol=1e-04, atol=1e-04)


</source>
</class>

<class classid="31" nclones="2" nlines="12" similarity="75">
<source file="systems/MONAI-0.6.0/tests/test_ensure_channel_first.py" startline="54" endline="67" pcid="403">
    def test_load_nifti(self, input_param, filenames, original_channel_dim):
        if original_channel_dim is None:
            test_image = np.random.rand(128, 128, 128)
        elif original_channel_dim == -1:
            test_image = np.random.rand(128, 128, 128, 1)

        with tempfile.TemporaryDirectory() as tempdir:
            for i, name in enumerate(filenames):
                filenames[i] = os.path.join(tempdir, name)
                nib.save(nib.Nifti1Image(test_image, np.eye(4)), filenames[i])
            result, header = LoadImage(**input_param)(filenames)
            result = EnsureChannelFirst()(result, header)
            self.assertEqual(result.shape[0], len(filenames))

</source>
<source file="systems/MONAI-0.6.0/tests/test_ensure_channel_firstd.py" startline="36" endline="49" pcid="986">
    def test_load_nifti(self, input_param, filenames, original_channel_dim):
        if original_channel_dim is None:
            test_image = np.random.rand(128, 128, 128)
        elif original_channel_dim == -1:
            test_image = np.random.rand(128, 128, 128, 1)

        with tempfile.TemporaryDirectory() as tempdir:
            for i, name in enumerate(filenames):
                filenames[i] = os.path.join(tempdir, name)
                nib.save(nib.Nifti1Image(test_image, np.eye(4)), filenames[i])
            result = LoadImaged(**input_param)({"img": filenames})
            result = EnsureChannelFirstd(**input_param)(result)
            self.assertEqual(result["img"].shape[0], len(filenames))

</source>
</class>

<class classid="32" nclones="2" nlines="32" similarity="79">
<source file="systems/MONAI-0.6.0/tests/test_handler_regression_metrics.py" startline="47" endline="88" pcid="429">
    def test_compute(self):
        set_determinism(seed=123)
        device = "cuda" if torch.cuda.is_available() else "cpu"

        # regression metrics to check + truth metric function in numpy
        metrics = [
            MeanSquaredError,
            MeanAbsoluteError,
            RootMeanSquaredError,
            partial(PeakSignalToNoiseRatio, max_val=1.0),
        ]
        metrics_np = [msemetric_np, maemetric_np, rmsemetric_np, partial(psnrmetric_np, max_val=1.0)]

        # define variations in batch/base_dims/spatial_dims
        batch_dims = [1, 2, 4, 16]
        base_dims = [16, 32, 64]
        spatial_dims = [2, 3, 4]

        # iterate over all variations and check shapes for different reduction functions
        for mt_fn, mt_fn_np in zip(metrics, metrics_np):

            for batch in batch_dims:
                for spatial in spatial_dims:
                    for base in base_dims:
                        mt_fn_obj = mt_fn(**{"save_details": False})

                        # create random tensor
                        in_tensor_a1 = torch.rand((batch,) + (base,) * (spatial - 1)).to(device)
                        in_tensor_b1 = torch.rand((batch,) + (base,) * (spatial - 1)).to(device)
                        mt_fn_obj.update([in_tensor_a1, in_tensor_b1])
                        out_tensor_np1 = mt_fn_np(y_pred=in_tensor_a1.cpu().numpy(), y=in_tensor_b1.cpu().numpy())

                        in_tensor_a2 = torch.rand((batch,) + (base,) * (spatial - 1)).to(device)
                        in_tensor_b2 = torch.rand((batch,) + (base,) * (spatial - 1)).to(device)
                        mt_fn_obj.update([in_tensor_a2, in_tensor_b2])
                        out_tensor_np2 = mt_fn_np(y_pred=in_tensor_a2.cpu().numpy(), y=in_tensor_b2.cpu().numpy())

                        out_tensor = mt_fn_obj.compute()
                        out_tensor_np = (out_tensor_np1 + out_tensor_np2) / 2.0

                        np.testing.assert_allclose(out_tensor, out_tensor_np, atol=1e-4)

</source>
<source file="systems/MONAI-0.6.0/tests/test_handler_regression_metrics.py" startline="89" endline="135" pcid="430">
    def test_compute_engine(self):
        set_determinism(seed=123)
        device = "cuda" if torch.cuda.is_available() else "cpu"

        # regression metrics to check + truth metric function in numpy
        metrics_names = ["MSE", "MAE", "RMSE", "PSNR"]
        metrics = [
            MeanSquaredError,
            MeanAbsoluteError,
            RootMeanSquaredError,
            partial(PeakSignalToNoiseRatio, max_val=1.0),
        ]
        metrics_np = [msemetric_np, maemetric_np, rmsemetric_np, partial(psnrmetric_np, max_val=1.0)]

        def _val_func(engine, batch):
            pass

        # define variations in batch/base_dims/spatial_dims
        batch_dims = [1, 2, 4, 16]
        base_dims = [16, 32, 64]
        spatial_dims = [2, 3, 4]

        # iterate over all variations and check shapes for different reduction functions
        for mt_fn_name, mt_fn, mt_fn_np in zip(metrics_names, metrics, metrics_np):
            for batch in batch_dims:
                for spatial in spatial_dims:
                    for base in base_dims:
                        mt_fn_obj = mt_fn()  # 'save_details' == True
                        engine = Engine(_val_func)
                        mt_fn_obj.attach(engine, mt_fn_name)

                        # create random tensor
                        in_tensor_a1 = torch.rand((batch,) + (base,) * (spatial - 1)).to(device)
                        in_tensor_b1 = torch.rand((batch,) + (base,) * (spatial - 1)).to(device)
                        mt_fn_obj.update([in_tensor_a1, in_tensor_b1])
                        out_tensor_np1 = mt_fn_np(y_pred=in_tensor_a1.cpu().numpy(), y=in_tensor_b1.cpu().numpy())

                        in_tensor_a2 = torch.rand((batch,) + (base,) * (spatial - 1)).to(device)
                        in_tensor_b2 = torch.rand((batch,) + (base,) * (spatial - 1)).to(device)
                        mt_fn_obj.update([in_tensor_a2, in_tensor_b2])
                        out_tensor_np2 = mt_fn_np(y_pred=in_tensor_a2.cpu().numpy(), y=in_tensor_b2.cpu().numpy())

                        out_tensor = mt_fn_obj.compute()
                        out_tensor_np = (out_tensor_np1 + out_tensor_np2) / 2.0

                        np.testing.assert_allclose(out_tensor, out_tensor_np, atol=1e-4)

</source>
</class>

<class classid="33" nclones="2" nlines="11" similarity="72">
<source file="systems/MONAI-0.6.0/tests/test_adjust_contrast.py" startline="29" endline="41" pcid="442">
    def test_correct_results(self, gamma):
        adjuster = AdjustContrast(gamma=gamma)
        result = adjuster(self.imt)
        if gamma == 1.0:
            expected = self.imt
        else:
            epsilon = 1e-7
            img_min = self.imt.min()
            img_range = self.imt.max() - img_min
            expected = np.power(((self.imt - img_min) / float(img_range + epsilon)), gamma) * img_range + img_min
        np.testing.assert_allclose(expected, result, rtol=1e-05)


</source>
<source file="systems/MONAI-0.6.0/tests/test_adjust_contrastd.py" startline="29" endline="41" pcid="499">
    def test_correct_results(self, gamma):
        adjuster = AdjustContrastd("img", gamma=gamma)
        result = adjuster({"img": self.imt})
        if gamma == 1.0:
            expected = self.imt
        else:
            epsilon = 1e-7
            img_min = self.imt.min()
            img_range = self.imt.max() - img_min
            expected = np.power(((self.imt - img_min) / float(img_range + epsilon)), gamma) * img_range + img_min
        np.testing.assert_allclose(expected, result["img"], rtol=1e-05)


</source>
</class>

<class classid="34" nclones="2" nlines="10" similarity="80">
<source file="systems/MONAI-0.6.0/tests/test_npzdictitemdataset.py" startline="22" endline="36" pcid="492">
    def test_load_stream(self):
        dat0 = np.random.rand(10, 1, 4, 4)
        dat1 = np.random.rand(10, 1, 4, 4)

        npzfile = BytesIO()
        np.savez_compressed(npzfile, dat0=dat0, dat1=dat1)
        npzfile.seek(0)

        npzds = NPZDictItemDataset(npzfile, {"dat0": "images", "dat1": "seg"})

        item = npzds[0]

        np.testing.assert_allclose(item["images"].shape, (1, 4, 4))
        np.testing.assert_allclose(item["seg"].shape, (1, 4, 4))

</source>
<source file="systems/MONAI-0.6.0/tests/test_npzdictitemdataset.py" startline="37" endline="53" pcid="493">
    def test_load_file(self):
        dat0 = np.random.rand(10, 1, 4, 4)
        dat1 = np.random.rand(10, 1, 4, 4)

        with tempfile.TemporaryDirectory() as tempdir:
            npzfile = f"{tempdir}/test.npz"

            np.savez_compressed(npzfile, dat0=dat0, dat1=dat1)

            npzds = NPZDictItemDataset(npzfile, {"dat0": "images", "dat1": "seg"})

            item = npzds[0]

            np.testing.assert_allclose(item["images"].shape, (1, 4, 4))
            np.testing.assert_allclose(item["seg"].shape, (1, 4, 4))


</source>
</class>

<class classid="35" nclones="2" nlines="10" similarity="90">
<source file="systems/MONAI-0.6.0/tests/test_ensure_type.py" startline="29" endline="39" pcid="512">
    def test_single_input(self):
        for test_data in (5, 5.0, False, np.asarray(5), torch.tensor(5)):
            for dtype in ("tensor", "numpy"):
                result = EnsureType(data_type=dtype)(test_data)
                self.assertTrue(isinstance(result, torch.Tensor if dtype == "tensor" else np.ndarray))
                if isinstance(test_data, bool):
                    self.assertFalse(result)
                else:
                    torch.testing.assert_allclose(result, test_data)
                self.assertEqual(result.ndim, 0)

</source>
<source file="systems/MONAI-0.6.0/tests/test_ensure_typed.py" startline="29" endline="39" pcid="1045">
    def test_single_input(self):
        for test_data in (5, 5.0, False, np.asarray(5), torch.tensor(5)):
            for dtype in ("tensor", "numpy"):
                result = EnsureTyped(keys="data", data_type=dtype)({"data": test_data})["data"]
                self.assertTrue(isinstance(result, torch.Tensor if dtype == "tensor" else np.ndarray))
                if isinstance(test_data, bool):
                    self.assertFalse(result)
                else:
                    torch.testing.assert_allclose(result, test_data)
                self.assertEqual(result.ndim, 0)

</source>
</class>

<class classid="36" nclones="2" nlines="10" similarity="80">
<source file="systems/MONAI-0.6.0/tests/test_ensure_type.py" startline="51" endline="62" pcid="514">
    def test_list_tuple(self):
        for dtype in ("tensor", "numpy"):
            result = EnsureType(data_type=dtype)([[1, 2], [3, 4]])
            self.assertTrue(isinstance(result, list))
            self.assertTrue(isinstance(result[0][1], torch.Tensor if dtype == "tensor" else np.ndarray))
            torch.testing.assert_allclose(result[1][0], torch.as_tensor(3))
            # tuple of numpy arrays
            result = EnsureType(data_type=dtype)((np.array([1, 2]), np.array([3, 4])))
            self.assertTrue(isinstance(result, tuple))
            self.assertTrue(isinstance(result[0], torch.Tensor if dtype == "tensor" else np.ndarray))
            torch.testing.assert_allclose(result[1], torch.as_tensor([3, 4]))

</source>
<source file="systems/MONAI-0.6.0/tests/test_ensure_typed.py" startline="51" endline="62" pcid="1047">
    def test_list_tuple(self):
        for dtype in ("tensor", "numpy"):
            result = EnsureTyped(keys="data", data_type=dtype)({"data": [[1, 2], [3, 4]]})["data"]
            self.assertTrue(isinstance(result, list))
            self.assertTrue(isinstance(result[0][1], torch.Tensor if dtype == "tensor" else np.ndarray))
            torch.testing.assert_allclose(result[1][0], torch.as_tensor(3))
            # tuple of numpy arrays
            result = EnsureTyped(keys="data", data_type=dtype)({"data": (np.array([1, 2]), np.array([3, 4]))})["data"]
            self.assertTrue(isinstance(result, tuple))
            self.assertTrue(isinstance(result[0], torch.Tensor if dtype == "tensor" else np.ndarray))
            torch.testing.assert_allclose(result[1], torch.as_tensor([3, 4]))

</source>
</class>

<class classid="37" nclones="2" nlines="15" similarity="92">
<source file="systems/MONAI-0.6.0/tests/test_ensure_type.py" startline="63" endline="80" pcid="515">
    def test_dict(self):
        # simulate complicated input data
        test_data = {
            "img": np.array([1.0, 2.0], dtype=np.float32),
            "meta": {"dims": 3, "size": np.array([1, 2, 3]), "path": "temp/test"},
            "extra": None,
        }
        for dtype in ("tensor", "numpy"):
            result = EnsureType(data_type=dtype)(test_data)
            self.assertTrue(isinstance(result, dict))
            self.assertTrue(isinstance(result["img"], torch.Tensor if dtype == "tensor" else np.ndarray))
            torch.testing.assert_allclose(result["img"], torch.as_tensor([1.0, 2.0]))
            self.assertTrue(isinstance(result["meta"]["size"], torch.Tensor if dtype == "tensor" else np.ndarray))
            torch.testing.assert_allclose(result["meta"]["size"], torch.as_tensor([1, 2, 3]))
            self.assertEqual(result["meta"]["path"], "temp/test")
            self.assertEqual(result["extra"], None)


</source>
<source file="systems/MONAI-0.6.0/tests/test_ensure_typed.py" startline="63" endline="80" pcid="1048">
    def test_dict(self):
        # simulate complicated input data
        test_data = {
            "img": np.array([1.0, 2.0], dtype=np.float32),
            "meta": {"dims": 3, "size": np.array([1, 2, 3]), "path": "temp/test"},
            "extra": None,
        }
        for dtype in ("tensor", "numpy"):
            result = EnsureTyped(keys="data", data_type=dtype)({"data": test_data})["data"]
            self.assertTrue(isinstance(result, dict))
            self.assertTrue(isinstance(result["img"], torch.Tensor if dtype == "tensor" else np.ndarray))
            torch.testing.assert_allclose(result["img"], torch.as_tensor([1.0, 2.0]))
            self.assertTrue(isinstance(result["meta"]["size"], torch.Tensor if dtype == "tensor" else np.ndarray))
            torch.testing.assert_allclose(result["meta"]["size"], torch.as_tensor([1, 2, 3]))
            self.assertEqual(result["meta"]["path"], "temp/test")
            self.assertEqual(result["extra"], None)


</source>
</class>

<class classid="38" nclones="2" nlines="26" similarity="80">
<source file="systems/MONAI-0.6.0/tests/test_lmdbdataset.py" startline="93" endline="123" pcid="521">
    def test_cache(self):
        """testing no inplace change to the hashed item"""
        items = [[list(range(i))] for i in range(5)]

        with tempfile.TemporaryDirectory() as tempdir:
            ds = LMDBDataset(items, transform=_InplaceXform(), cache_dir=tempdir, lmdb_kwargs={"map_size": 10 * 1024})
            self.assertEqual(items, [[[]], [[0]], [[0, 1]], [[0, 1, 2]], [[0, 1, 2, 3]]])
            ds1 = LMDBDataset(items, transform=_InplaceXform(), cache_dir=tempdir, lmdb_kwargs={"map_size": 10 * 1024})
            self.assertEqual(list(ds1), list(ds))
            self.assertEqual(items, [[[]], [[0]], [[0, 1]], [[0, 1, 2]], [[0, 1, 2, 3]]])

            ds = LMDBDataset(
                items,
                transform=_InplaceXform(),
                cache_dir=tempdir,
                lmdb_kwargs={"map_size": 10 * 1024},
                hash_func=json_hashing,
            )
            self.assertEqual(items, [[[]], [[0]], [[0, 1]], [[0, 1, 2]], [[0, 1, 2, 3]]])
            ds1 = LMDBDataset(
                items,
                transform=_InplaceXform(),
                cache_dir=tempdir,
                lmdb_kwargs={"map_size": 10 * 1024},
                hash_func=json_hashing,
            )
            self.assertEqual(list(ds1), list(ds))
            self.assertEqual(items, [[[]], [[0]], [[0, 1]], [[0, 1, 2]], [[0, 1, 2, 3]]])

        self.assertTrue(isinstance(ds1.info(), dict))

</source>
<source file="systems/MONAI-0.6.0/tests/test_lmdbdataset.py" startline="211" endline="240" pcid="525">
    def test_mp_cache(self):
        items = [[list(range(i))] for i in range(5)]

        ds = LMDBDataset(items, transform=_InplaceXform(), cache_dir=self.tempdir, lmdb_kwargs={"map_size": 10 * 1024})
        self.assertEqual(items, [[[]], [[0]], [[0, 1]], [[0, 1, 2]], [[0, 1, 2, 3]]])
        ds1 = LMDBDataset(items, transform=_InplaceXform(), cache_dir=self.tempdir, lmdb_kwargs={"map_size": 10 * 1024})
        self.assertEqual(list(ds1), list(ds))
        self.assertEqual(items, [[[]], [[0]], [[0, 1]], [[0, 1, 2]], [[0, 1, 2, 3]]])

        ds = LMDBDataset(
            items,
            transform=_InplaceXform(),
            cache_dir=self.tempdir,
            lmdb_kwargs={"map_size": 10 * 1024},
            hash_func=json_hashing,
        )
        self.assertEqual(items, [[[]], [[0]], [[0, 1]], [[0, 1, 2]], [[0, 1, 2, 3]]])
        ds1 = LMDBDataset(
            items,
            transform=_InplaceXform(),
            cache_dir=self.tempdir,
            lmdb_kwargs={"map_size": 10 * 1024},
            hash_func=json_hashing,
        )
        self.assertEqual(list(ds1), list(ds))
        self.assertEqual(items, [[[]], [[0]], [[0, 1]], [[0, 1, 2]], [[0, 1, 2, 3]]])

        self.assertTrue(isinstance(ds1.info(), dict))


</source>
</class>

<class classid="39" nclones="4" nlines="21" similarity="71">
<source file="systems/MONAI-0.6.0/tests/test_handler_checkpoint_loader.py" startline="60" endline="81" pcid="528">
    def test_two_save_one_load(self):
        logging.basicConfig(stream=sys.stdout, level=logging.INFO)
        net1 = torch.nn.PReLU()
        optimizer = optim.SGD(net1.parameters(), lr=0.02)
        data1 = net1.state_dict()
        data1["weight"] = torch.tensor([0.1])
        net1.load_state_dict(data1)
        net2 = torch.nn.PReLU()
        data2 = net2.state_dict()
        data2["weight"] = torch.tensor([0.2])
        net2.load_state_dict(data2)
        with tempfile.TemporaryDirectory() as tempdir:
            engine = Engine(lambda e, b: None)
            save_dict = {"net": net1, "opt": optimizer}
            CheckpointSaver(save_dir=tempdir, save_dict=save_dict, save_final=True).attach(engine)
            engine.run([0] * 8, max_epochs=5)
            path = tempdir + "/checkpoint_final_iteration=40.pt"
            engine = Engine(lambda e, b: None)
            CheckpointLoader(load_path=path, load_dict={"net": net2}, strict=True).attach(engine)
            engine.run([0] * 8, max_epochs=1)
            torch.testing.assert_allclose(net2.state_dict()["weight"], torch.tensor([0.1]))

</source>
<source file="systems/MONAI-0.6.0/tests/test_handler_checkpoint_loader.py" startline="82" endline="102" pcid="529">
    def test_save_single_device_load_multi_devices(self):
        logging.basicConfig(stream=sys.stdout, level=logging.INFO)
        net1 = torch.nn.PReLU()
        data1 = net1.state_dict()
        data1["weight"] = torch.tensor([0.1])
        net1.load_state_dict(data1)
        net2 = torch.nn.PReLU()
        data2 = net2.state_dict()
        data2["weight"] = torch.tensor([0.2])
        net2.load_state_dict(data2)
        net2 = torch.nn.DataParallel(net2)
        with tempfile.TemporaryDirectory() as tempdir:
            engine = Engine(lambda e, b: None)
            CheckpointSaver(save_dir=tempdir, save_dict={"net": net1}, save_final=True).attach(engine)
            engine.run([0] * 8, max_epochs=5)
            path = tempdir + "/net_final_iteration=40.pt"
            engine = Engine(lambda e, b: None)
            CheckpointLoader(load_path=path, load_dict={"net": net2}, strict=True).attach(engine)
            engine.run([0] * 8, max_epochs=1)
            torch.testing.assert_allclose(net2.state_dict()["module.weight"].cpu(), torch.tensor([0.1]))

</source>
<source file="systems/MONAI-0.6.0/tests/test_handler_checkpoint_loader.py" startline="103" endline="125" pcid="530">
    def test_partial_under_load(self):
        logging.basicConfig(stream=sys.stdout, level=logging.INFO)
        net1 = torch.nn.Sequential(*[torch.nn.PReLU(), torch.nn.PReLU()])
        data1 = net1.state_dict()
        data1["0.weight"] = torch.tensor([0.1])
        data1["1.weight"] = torch.tensor([0.2])
        net1.load_state_dict(data1)

        net2 = torch.nn.Sequential(*[torch.nn.PReLU()])
        data2 = net2.state_dict()
        data2["0.weight"] = torch.tensor([0.3])
        net2.load_state_dict(data2)

        with tempfile.TemporaryDirectory() as tempdir:
            engine = Engine(lambda e, b: None)
            CheckpointSaver(save_dir=tempdir, save_dict={"net": net1}, save_final=True).attach(engine)
            engine.run([0] * 8, max_epochs=5)
            path = tempdir + "/net_final_iteration=40.pt"
            engine = Engine(lambda e, b: None)
            CheckpointLoader(load_path=path, load_dict={"net": net2}, strict=False).attach(engine)
            engine.run([0] * 8, max_epochs=1)
            torch.testing.assert_allclose(net2.state_dict()["0.weight"].cpu(), torch.tensor([0.1]))

</source>
<source file="systems/MONAI-0.6.0/tests/test_handler_checkpoint_loader.py" startline="126" endline="148" pcid="531">
    def test_partial_over_load(self):
        logging.basicConfig(stream=sys.stdout, level=logging.INFO)
        net1 = torch.nn.Sequential(*[torch.nn.PReLU()])
        data1 = net1.state_dict()
        data1["0.weight"] = torch.tensor([0.1])
        net1.load_state_dict(data1)

        net2 = torch.nn.Sequential(*[torch.nn.PReLU(), torch.nn.PReLU()])
        data2 = net2.state_dict()
        data2["0.weight"] = torch.tensor([0.2])
        data2["1.weight"] = torch.tensor([0.3])
        net2.load_state_dict(data2)

        with tempfile.TemporaryDirectory() as tempdir:
            engine = Engine(lambda e, b: None)
            CheckpointSaver(save_dir=tempdir, save_dict={"net": net1}, save_final=True).attach(engine)
            engine.run([0] * 8, max_epochs=5)
            path = tempdir + "/net_final_iteration=40.pt"
            engine = Engine(lambda e, b: None)
            CheckpointLoader(load_path=path, load_dict={"net": net2}, strict=False).attach(engine)
            engine.run([0] * 8, max_epochs=1)
            torch.testing.assert_allclose(net2.state_dict()["0.weight"].cpu(), torch.tensor([0.1]))

</source>
</class>

<class classid="40" nclones="2" nlines="10" similarity="70">
<source file="systems/MONAI-0.6.0/tests/test_rand_k_space_spike_noise.py" startline="64" endline="74" pcid="583">
    def test_same_result(self, im_shape, as_tensor_output, as_tensor_input, channel_wise):
        im = self.get_data(im_shape, as_tensor_input)
        intensity_range = [14, 15]
        t = RandKSpaceSpikeNoise(0.0, intensity_range, channel_wise, as_tensor_output)
        t.set_random_state(42)
        out1 = t(deepcopy(im))
        t.set_random_state(42)
        out2 = t(deepcopy(im))
        np.testing.assert_allclose(out1, out2)
        self.assertIsInstance(out1, torch.Tensor if as_tensor_output else np.ndarray)

</source>
<source file="systems/MONAI-0.6.0/tests/test_rand_gibbs_noise.py" startline="53" endline="63" pcid="673">
    def test_same_result(self, im_shape, as_tensor_output, as_tensor_input):
        im = self.get_data(im_shape, as_tensor_input)
        alpha = [0.5, 0.8]
        t = RandGibbsNoise(1.0, alpha, as_tensor_output)
        t.set_random_state(42)
        out1 = t(deepcopy(im))
        t.set_random_state(42)
        out2 = t(deepcopy(im))
        np.testing.assert_allclose(out1, out2)
        self.assertIsInstance(out1, torch.Tensor if as_tensor_output else np.ndarray)

</source>
</class>

<class classid="41" nclones="4" nlines="10" similarity="80">
<source file="systems/MONAI-0.6.0/tests/test_rand_rotate90d.py" startline="21" endline="31" pcid="586">
    def test_default(self):
        key = None
        rotate = RandRotate90d(keys=key)
        rotate.set_random_state(123)
        rotated = rotate({key: self.imt[0]})
        expected = []
        for channel in self.imt[0]:
            expected.append(np.rot90(channel, 0, (0, 1)))
        expected = np.stack(expected)
        self.assertTrue(np.allclose(rotated[key], expected))

</source>
<source file="systems/MONAI-0.6.0/tests/test_rand_rotate90d.py" startline="43" endline="53" pcid="588">
    def test_spatial_axes(self):
        key = "test"
        rotate = RandRotate90d(keys=key, spatial_axes=(0, 1))
        rotate.set_random_state(234)
        rotated = rotate({key: self.imt[0]})
        expected = []
        for channel in self.imt[0]:
            expected.append(np.rot90(channel, 0, (0, 1)))
        expected = np.stack(expected)
        self.assertTrue(np.allclose(rotated[key], expected))

</source>
<source file="systems/MONAI-0.6.0/tests/test_rand_rotate90d.py" startline="54" endline="64" pcid="589">
    def test_prob_k_spatial_axes(self):
        key = "test"
        rotate = RandRotate90d(keys=key, prob=1.0, max_k=2, spatial_axes=(0, 1))
        rotate.set_random_state(234)
        rotated = rotate({key: self.imt[0]})
        expected = []
        for channel in self.imt[0]:
            expected.append(np.rot90(channel, 1, (0, 1)))
        expected = np.stack(expected)
        self.assertTrue(np.allclose(rotated[key], expected))

</source>
<source file="systems/MONAI-0.6.0/tests/test_rand_rotate90d.py" startline="32" endline="42" pcid="587">
    def test_k(self):
        key = "test"
        rotate = RandRotate90d(keys=key, max_k=2)
        rotate.set_random_state(234)
        rotated = rotate({key: self.imt[0]})
        expected = []
        for channel in self.imt[0]:
            expected.append(np.rot90(channel, 0, (0, 1)))
        expected = np.stack(expected)
        self.assertTrue(np.allclose(rotated[key], expected))

</source>
</class>

<class classid="42" nclones="4" nlines="15" similarity="71">
<source file="systems/MONAI-0.6.0/tests/test_orientationd.py" startline="29" endline="44" pcid="620">
    def test_orntd_3d(self):
        data = {
            "seg": np.ones((2, 1, 2, 3)),
            "img": np.ones((2, 1, 2, 3)),
            "seg_meta_dict": {"affine": np.eye(4)},
            "img_meta_dict": {"affine": np.eye(4)},
        }
        ornt = Orientationd(keys=("img", "seg"), axcodes="PLI")
        res = ornt(data)
        np.testing.assert_allclose(res["img"].shape, (2, 2, 1, 3))
        np.testing.assert_allclose(res["seg"].shape, (2, 2, 1, 3))
        code = nib.aff2axcodes(res["seg_meta_dict"]["affine"], ornt.ornt_transform.labels)
        self.assertEqual(code, ("P", "L", "I"))
        code = nib.aff2axcodes(res["img_meta_dict"]["affine"], ornt.ornt_transform.labels)
        self.assertEqual(code, ("P", "L", "I"))

</source>
<source file="systems/MONAI-0.6.0/tests/test_orientationd.py" startline="45" endline="59" pcid="621">
    def test_orntd_2d(self):
        data = {
            "seg": np.ones((2, 1, 3)),
            "img": np.ones((2, 1, 3)),
            "seg_meta_dict": {"affine": np.eye(4)},
            "img_meta_dict": {"affine": np.eye(4)},
        }
        ornt = Orientationd(keys=("img", "seg"), axcodes="PLI")
        res = ornt(data)
        np.testing.assert_allclose(res["img"].shape, (2, 3, 1))
        code = nib.aff2axcodes(res["seg_meta_dict"]["affine"], ornt.ornt_transform.labels)
        self.assertEqual(code, ("P", "L", "S"))
        code = nib.aff2axcodes(res["img_meta_dict"]["affine"], ornt.ornt_transform.labels)
        self.assertEqual(code, ("P", "L", "S"))

</source>
<source file="systems/MONAI-0.6.0/tests/test_orientationd.py" startline="60" endline="74" pcid="622">
    def test_orntd_1d(self):
        data = {
            "seg": np.ones((2, 3)),
            "img": np.ones((2, 3)),
            "seg_meta_dict": {"affine": np.eye(4)},
            "img_meta_dict": {"affine": np.eye(4)},
        }
        ornt = Orientationd(keys=("img", "seg"), axcodes="L")
        res = ornt(data)
        np.testing.assert_allclose(res["img"].shape, (2, 3))
        code = nib.aff2axcodes(res["seg_meta_dict"]["affine"], ornt.ornt_transform.labels)
        self.assertEqual(code, ("L", "A", "S"))
        code = nib.aff2axcodes(res["img_meta_dict"]["affine"], ornt.ornt_transform.labels)
        self.assertEqual(code, ("L", "A", "S"))

</source>
<source file="systems/MONAI-0.6.0/tests/test_orientationd.py" startline="75" endline="90" pcid="623">
    def test_orntd_canonical(self):
        data = {
            "seg": np.ones((2, 1, 2, 3)),
            "img": np.ones((2, 1, 2, 3)),
            "seg_meta_dict": {"affine": np.eye(4)},
            "img_meta_dict": {"affine": np.eye(4)},
        }
        ornt = Orientationd(keys=("img", "seg"), as_closest_canonical=True)
        res = ornt(data)
        np.testing.assert_allclose(res["img"].shape, (2, 1, 2, 3))
        np.testing.assert_allclose(res["seg"].shape, (2, 1, 2, 3))
        code = nib.aff2axcodes(res["seg_meta_dict"]["affine"], ornt.ornt_transform.labels)
        self.assertEqual(code, ("R", "A", "S"))
        code = nib.aff2axcodes(res["img_meta_dict"]["affine"], ornt.ornt_transform.labels)
        self.assertEqual(code, ("R", "A", "S"))

</source>
</class>

<class classid="43" nclones="2" nlines="11" similarity="81">
<source file="systems/MONAI-0.6.0/tests/test_handler_classification_saver.py" startline="45" endline="56" pcid="684">
            def _test_file(filename):
                filepath = os.path.join(tempdir, filename)
                self.assertTrue(os.path.exists(filepath))
                with open(filepath, "r") as f:
                    reader = csv.reader(f)
                    i = 0
                    for row in reader:
                        self.assertEqual(row[0], "testfile" + str(i))
                        self.assertEqual(np.array(row[1:]).astype(np.float32), 0.0)
                        i += 1
                    self.assertEqual(i, 8)

</source>
<source file="systems/MONAI-0.6.0/tests/test_save_classificationd.py" startline="83" endline="94" pcid="718">
            def _test_file(filename, count):
                filepath = os.path.join(tempdir, filename)
                self.assertTrue(os.path.exists(filepath))
                with open(filepath, "r") as f:
                    reader = csv.reader(f)
                    i = 0
                    for row in reader:
                        self.assertEqual(row[0], "testfile" + str(i))
                        self.assertEqual(np.array(row[1:]).astype(np.float32), 0.0)
                        i += 1
                    self.assertEqual(i, count)

</source>
</class>

<class classid="44" nclones="2" nlines="24" similarity="100">
<source file="systems/MONAI-0.6.0/tests/test_label_to_contour.py" startline="118" endline="143" pcid="693">
def gen_fixed_img():
    img = torch.tensor(
        [
            [0, 0, 0, 1, 1, 1, 1],
            [0, 0, 0, 1, 1, 1, 1],
            [0, 0, 1, 1, 1, 1, 1],
            [0, 1, 1, 1, 1, 1, 1],
            [1, 1, 1, 1, 1, 1, 1],
        ],
        dtype=torch.float32,
    )
    batch_size, channels = 10, 6
    img = img.repeat(batch_size, channels, 1, 1)
    expected_output_for_img = torch.tensor(
        [
            [0, 0, 0, 1, 1, 1, 1],
            [0, 0, 0, 1, 0, 0, 1],
            [0, 0, 1, 1, 0, 0, 1],
            [0, 1, 1, 0, 0, 0, 1],
            [1, 1, 1, 1, 1, 1, 1],
        ],
        dtype=torch.float32,
    )
    return img, expected_output_for_img


</source>
<source file="systems/MONAI-0.6.0/tests/test_label_to_contourd.py" startline="118" endline="143" pcid="975">
def gen_fixed_img():
    img = torch.tensor(
        [
            [0, 0, 0, 1, 1, 1, 1],
            [0, 0, 0, 1, 1, 1, 1],
            [0, 0, 1, 1, 1, 1, 1],
            [0, 1, 1, 1, 1, 1, 1],
            [1, 1, 1, 1, 1, 1, 1],
        ],
        dtype=torch.float32,
    )
    batch_size, channels = 10, 6
    img = img.repeat(batch_size, channels, 1, 1)
    expected_output_for_img = torch.tensor(
        [
            [0, 0, 0, 1, 1, 1, 1],
            [0, 0, 0, 1, 0, 0, 1],
            [0, 0, 1, 1, 0, 0, 1],
            [0, 1, 1, 0, 0, 0, 1],
            [1, 1, 1, 1, 1, 1, 1],
        ],
        dtype=torch.float32,
    )
    return img, expected_output_for_img


</source>
</class>

<class classid="45" nclones="2" nlines="27" similarity="92">
<source file="systems/MONAI-0.6.0/tests/test_rotate.py" startline="46" endline="76" pcid="706">
    def test_correct_results(self, angle, keep_size, mode, padding_mode, align_corners):
        rotate_fn = Rotate(angle, keep_size, mode, padding_mode, align_corners)
        rotated = rotate_fn(self.imt[0])
        if keep_size:
            np.testing.assert_allclose(self.imt[0].shape, rotated.shape)
        _order = 0 if mode == "nearest" else 1
        if padding_mode == "border":
            _mode = "nearest"
        elif padding_mode == "reflection":
            _mode = "reflect"
        else:
            _mode = "constant"

        expected = []
        for channel in self.imt[0]:
            expected.append(
                scipy.ndimage.rotate(
                    channel,
                    -np.rad2deg(angle),
                    (0, 1),
                    not keep_size,
                    order=_order,
                    mode=_mode,
                    prefilter=False,
                )
            )
        expected = np.stack(expected).astype(np.float32)
        good = np.sum(np.isclose(expected, rotated, atol=1e-3))
        self.assertLessEqual(np.abs(good - expected.size), 5, "diff at most 5 pixels")


</source>
<source file="systems/MONAI-0.6.0/tests/test_rotate.py" startline="79" endline="108" pcid="707">
    def test_correct_results(self, angle, keep_size, mode, padding_mode, align_corners):
        rotate_fn = Rotate([angle, 0, 0], keep_size, mode, padding_mode, align_corners)
        rotated = rotate_fn(self.imt[0])
        if keep_size:
            np.testing.assert_allclose(self.imt[0].shape, rotated.shape)
        _order = 0 if mode == "nearest" else 1
        if padding_mode == "border":
            _mode = "nearest"
        elif padding_mode == "reflection":
            _mode = "reflect"
        else:
            _mode = "constant"

        expected = []
        for channel in self.imt[0]:
            expected.append(
                scipy.ndimage.rotate(
                    channel,
                    -np.rad2deg(angle),
                    (1, 2),
                    not keep_size,
                    order=_order,
                    mode=_mode,
                    prefilter=False,
                )
            )
        expected = np.stack(expected).astype(np.float32)
        n_good = np.sum(np.isclose(expected, rotated, atol=1e-3))
        self.assertLessEqual(expected.size - n_good, 5, "diff at most 5 pixels")

</source>
</class>

<class classid="46" nclones="4" nlines="12" similarity="100">
<source file="systems/MONAI-0.6.0/tests/test_masked_dice_loss.py" startline="143" endline="156" pcid="713">
    def test_input_warnings(self):
        chn_input = torch.ones((1, 1, 3))
        chn_target = torch.ones((1, 1, 3))
        with self.assertWarns(Warning):
            loss = MaskedDiceLoss(include_background=False)
            loss.forward(chn_input, chn_target)
        with self.assertWarns(Warning):
            loss = MaskedDiceLoss(softmax=True)
            loss.forward(chn_input, chn_target)
        with self.assertWarns(Warning):
            loss = MaskedDiceLoss(to_onehot_y=True)
            loss.forward(chn_input, chn_target)


</source>
<source file="systems/MONAI-0.6.0/tests/test_tversky_loss.py" startline="174" endline="186" pcid="791">
    def test_input_warnings(self):
        chn_input = torch.ones((1, 1, 3))
        chn_target = torch.ones((1, 1, 3))
        with self.assertWarns(Warning):
            loss = TverskyLoss(include_background=False)
            loss.forward(chn_input, chn_target)
        with self.assertWarns(Warning):
            loss = TverskyLoss(softmax=True)
            loss.forward(chn_input, chn_target)
        with self.assertWarns(Warning):
            loss = TverskyLoss(to_onehot_y=True)
            loss.forward(chn_input, chn_target)

</source>
<source file="systems/MONAI-0.6.0/tests/test_generalized_dice_loss.py" startline="169" endline="181" pcid="983">
    def test_input_warnings(self):
        chn_input = torch.ones((1, 1, 3))
        chn_target = torch.ones((1, 1, 3))
        with self.assertWarns(Warning):
            loss = GeneralizedDiceLoss(include_background=False)
            loss.forward(chn_input, chn_target)
        with self.assertWarns(Warning):
            loss = GeneralizedDiceLoss(softmax=True)
            loss.forward(chn_input, chn_target)
        with self.assertWarns(Warning):
            loss = GeneralizedDiceLoss(to_onehot_y=True)
            loss.forward(chn_input, chn_target)

</source>
<source file="systems/MONAI-0.6.0/tests/test_dice_loss.py" startline="186" endline="198" pcid="842">
    def test_input_warnings(self):
        chn_input = torch.ones((1, 1, 3))
        chn_target = torch.ones((1, 1, 3))
        with self.assertWarns(Warning):
            loss = DiceLoss(include_background=False)
            loss.forward(chn_input, chn_target)
        with self.assertWarns(Warning):
            loss = DiceLoss(softmax=True)
            loss.forward(chn_input, chn_target)
        with self.assertWarns(Warning):
            loss = DiceLoss(to_onehot_y=True)
            loss.forward(chn_input, chn_target)

</source>
</class>

<class classid="47" nclones="2" nlines="18" similarity="88">
<source file="systems/MONAI-0.6.0/tests/test_nifti_saver.py" startline="35" endline="54" pcid="754">
    def test_saved_resize_content(self):
        with tempfile.TemporaryDirectory() as tempdir:

            saver = NiftiSaver(
                output_dir=tempdir,
                output_postfix="seg",
                output_ext=".nii.gz",
                dtype=np.float32,
            )

            meta_data = {
                "filename_or_obj": ["testfile" + str(i) + ".nii" for i in range(8)],
                "affine": [np.diag(np.ones(4)) * 5] * 8,
                "original_affine": [np.diag(np.ones(4)) * 1.0] * 8,
            }
            saver.save_batch(torch.randint(0, 255, (8, 8, 2, 2)), meta_data)
            for i in range(8):
                filepath = os.path.join("testfile" + str(i), "testfile" + str(i) + "_seg.nii.gz")
                self.assertTrue(os.path.exists(os.path.join(tempdir, filepath)))

</source>
<source file="systems/MONAI-0.6.0/tests/test_nifti_saver.py" startline="55" endline="75" pcid="755">
    def test_saved_3d_resize_content(self):
        with tempfile.TemporaryDirectory() as tempdir:

            saver = NiftiSaver(
                output_dir=tempdir,
                output_postfix="seg",
                output_ext=".nii.gz",
                dtype=np.float32,
            )

            meta_data = {
                "filename_or_obj": ["testfile" + str(i) + ".nii.gz" for i in range(8)],
                "spatial_shape": [(10, 10, 2)] * 8,
                "affine": [np.diag(np.ones(4)) * 5] * 8,
                "original_affine": [np.diag(np.ones(4)) * 1.0] * 8,
            }
            saver.save_batch(torch.randint(0, 255, (8, 8, 1, 2, 2)), meta_data)
            for i in range(8):
                filepath = os.path.join("testfile" + str(i), "testfile" + str(i) + "_seg.nii.gz")
                self.assertTrue(os.path.exists(os.path.join(tempdir, filepath)))

</source>
</class>

<class classid="48" nclones="4" nlines="18" similarity="72">
<source file="systems/MONAI-0.6.0/tests/test_load_decathlon_datalist.py" startline="21" endline="40" pcid="765">
    def test_seg_values(self):
        with tempfile.TemporaryDirectory() as tempdir:
            test_data = {
                "name": "Spleen",
                "description": "Spleen Segmentation",
                "labels": {"0": "background", "1": "spleen"},
                "training": [
                    {"image": "spleen_19.nii.gz", "label": "spleen_19.nii.gz"},
                    {"image": "spleen_31.nii.gz", "label": "spleen_31.nii.gz"},
                ],
                "test": ["spleen_15.nii.gz", "spleen_23.nii.gz"],
            }
            json_str = json.dumps(test_data)
            file_path = os.path.join(tempdir, "test_data.json")
            with open(file_path, "w") as json_file:
                json_file.write(json_str)
            result = load_decathlon_datalist(file_path, True, "training", tempdir)
            self.assertEqual(result[0]["image"], os.path.join(tempdir, "spleen_19.nii.gz"))
            self.assertEqual(result[0]["label"], os.path.join(tempdir, "spleen_19.nii.gz"))

</source>
<source file="systems/MONAI-0.6.0/tests/test_load_decathlon_datalist.py" startline="84" endline="98" pcid="768">
    def test_seg_no_labels(self):
        with tempfile.TemporaryDirectory() as tempdir:
            test_data = {
                "name": "Spleen",
                "description": "Spleen Segmentation",
                "labels": {"0": "background", "1": "spleen"},
                "test": ["spleen_15.nii.gz", "spleen_23.nii.gz"],
            }
            json_str = json.dumps(test_data)
            file_path = os.path.join(tempdir, "test_data.json")
            with open(file_path, "w") as json_file:
                json_file.write(json_str)
            result = load_decathlon_datalist(file_path, True, "test", tempdir)
            self.assertEqual(result[0]["image"], os.path.join(tempdir, "spleen_15.nii.gz"))

</source>
<source file="systems/MONAI-0.6.0/tests/test_load_decathlon_datalist.py" startline="99" endline="124" pcid="769">
    def test_additional_items(self):
        with tempfile.TemporaryDirectory() as tempdir:
            with open(os.path.join(tempdir, "mask31.txt"), "w") as f:
                f.write("spleen31 mask")

            test_data = {
                "name": "Spleen",
                "description": "Spleen Segmentation",
                "labels": {"0": "background", "1": "spleen"},
                "training": [
                    {"image": "spleen_19.nii.gz", "label": "spleen_19.nii.gz", "mask": "spleen mask"},
                    {"image": "spleen_31.nii.gz", "label": "spleen_31.nii.gz", "mask": "mask31.txt"},
                ],
                "test": ["spleen_15.nii.gz", "spleen_23.nii.gz"],
            }
            json_str = json.dumps(test_data)
            file_path = os.path.join(tempdir, "test_data.json")
            with open(file_path, "w") as json_file:
                json_file.write(json_str)
            result = load_decathlon_datalist(file_path, True, "training", tempdir)
            self.assertEqual(result[0]["image"], os.path.join(tempdir, "spleen_19.nii.gz"))
            self.assertEqual(result[0]["label"], os.path.join(tempdir, "spleen_19.nii.gz"))
            self.assertEqual(result[1]["mask"], os.path.join(tempdir, "mask31.txt"))
            self.assertEqual(result[0]["mask"], "spleen mask")


</source>
<source file="systems/MONAI-0.6.0/tests/test_load_decathlon_datalist.py" startline="41" endline="57" pcid="766">
    def test_cls_values(self):
        with tempfile.TemporaryDirectory() as tempdir:
            test_data = {
                "name": "ChestXRay",
                "description": "Chest X-ray classification",
                "labels": {"0": "background", "1": "chest"},
                "training": [{"image": "chest_19.nii.gz", "label": 0}, {"image": "chest_31.nii.gz", "label": 1}],
                "test": ["chest_15.nii.gz", "chest_23.nii.gz"],
            }
            json_str = json.dumps(test_data)
            file_path = os.path.join(tempdir, "test_data.json")
            with open(file_path, "w") as json_file:
                json_file.write(json_str)
            result = load_decathlon_datalist(file_path, False, "training", tempdir)
            self.assertEqual(result[0]["image"], os.path.join(tempdir, "chest_19.nii.gz"))
            self.assertEqual(result[0]["label"], 0)

</source>
</class>

<class classid="49" nclones="2" nlines="19" similarity="77">
<source file="systems/MONAI-0.6.0/tests/test_dice_focal_loss.py" startline="22" endline="42" pcid="852">
    def test_result_onehot_target_include_bg(self):
        size = [3, 3, 5, 5]
        label = torch.randint(low=0, high=2, size=size)
        pred = torch.randn(size)
        for reduction in ["sum", "mean", "none"]:
            common_params = {
                "include_background": True,
                "to_onehot_y": False,
                "reduction": reduction,
            }
            for focal_weight in [None, torch.tensor([1.0, 1.0, 2.0]), (3, 2.0, 1)]:
                for lambda_focal in [0.5, 1.0, 1.5]:
                    dice_focal = DiceFocalLoss(
                        focal_weight=focal_weight, gamma=1.0, lambda_focal=lambda_focal, **common_params
                    )
                    dice = DiceLoss(**common_params)
                    focal = FocalLoss(weight=focal_weight, gamma=1.0, **common_params)
                    result = dice_focal(pred, label)
                    expected_val = dice(pred, label) + lambda_focal * focal(pred, label)
                    np.testing.assert_allclose(result, expected_val)

</source>
<source file="systems/MONAI-0.6.0/tests/test_dice_focal_loss.py" startline="43" endline="62" pcid="853">
    def test_result_no_onehot_no_bg(self):
        size = [3, 3, 5, 5]
        label = torch.randint(low=0, high=2, size=size)
        label = torch.argmax(label, dim=1, keepdim=True)
        pred = torch.randn(size)
        for reduction in ["sum", "mean", "none"]:
            common_params = {
                "include_background": False,
                "to_onehot_y": True,
                "reduction": reduction,
            }
            for focal_weight in [2.0, torch.tensor([1.0, 2.0]), (2.0, 1)]:
                for lambda_focal in [0.5, 1.0, 1.5]:
                    dice_focal = DiceFocalLoss(focal_weight=focal_weight, lambda_focal=lambda_focal, **common_params)
                    dice = DiceLoss(**common_params)
                    focal = FocalLoss(weight=focal_weight, **common_params)
                    result = dice_focal(pred, label)
                    expected_val = dice(pred, label) + lambda_focal * focal(pred, label)
                    np.testing.assert_allclose(result, expected_val)

</source>
</class>

<class classid="50" nclones="2" nlines="12" similarity="100">
<source file="systems/MONAI-0.6.0/tests/test_rand_rician_noise.py" startline="23" endline="37" pcid="924">
    def test_correct_results(self, _, mean, std):
        seed = 0
        rician_fn = RandRicianNoise(prob=1.0, mean=mean, std=std)
        rician_fn.set_random_state(seed)
        noised = rician_fn(self.imt)
        np.random.seed(seed)
        np.random.random()
        _std = np.random.uniform(0, std)
        expected = np.sqrt(
            (self.imt + np.random.normal(mean, _std, size=self.imt.shape)) ** 2
            + np.random.normal(mean, _std, size=self.imt.shape) ** 2
        )
        np.testing.assert_allclose(expected, noised, atol=1e-5)


</source>
<source file="systems/MONAI-0.6.0/tests/test_rand_rician_noise.py" startline="40" endline="54" pcid="925">
    def test_correct_results(self, _, mean, std):
        seed = 0
        rician_fn = RandRicianNoise(prob=1.0, mean=mean, std=std)
        rician_fn.set_random_state(seed)
        noised = rician_fn(self.imt)
        np.random.seed(seed)
        np.random.random()
        _std = np.random.uniform(0, std)
        expected = np.sqrt(
            (self.imt + np.random.normal(mean, _std, size=self.imt.shape)) ** 2
            + np.random.normal(mean, _std, size=self.imt.shape) ** 2
        )
        np.testing.assert_allclose(expected, noised, atol=1e-5)


</source>
</class>

<class classid="51" nclones="4" nlines="20" similarity="72">
<source file="systems/MONAI-0.6.0/tests/test_copy_model_state.py" startline="60" endline="81" pcid="969">
    def test_set_state(self, device_0, device_1):
        set_determinism(0)
        model_one = _TestModelOne(10, 20, 3)
        model_two = _TestModelTwo(10, 20, 10, 4)
        model_one.to(device_0)
        model_two.to(device_1)
        model_dict, ch, unch = copy_model_state(model_one, model_two)
        x = np.random.randn(4, 10)
        x = torch.tensor(x, device=device_0, dtype=torch.float32)
        output = model_one(x).detach().cpu().numpy()
        expected = np.array(
            [
                [-0.36076584, -0.03177825, -0.7702266],
                [-0.0526831, -0.15855855, -0.01149344],
                [-0.3760508, -0.22485238, -0.0634037],
                [0.5977675, -0.67991066, 0.1919502],
            ]
        )
        np.testing.assert_allclose(output, expected, atol=1e-3)
        self.assertEqual(len(ch), 2)
        self.assertEqual(len(unch), 2)

</source>
<source file="systems/MONAI-0.6.0/tests/test_copy_model_state.py" startline="103" endline="125" pcid="971">
    def test_set_exclude_vars(self, device_0, device_1):
        set_determinism(0)
        model_one = _TestModelOne(10, 20, 3)
        model_two = _TestModelTwo(10, 20, 10, 4)
        model_one.to(device_0)
        model_two.to(device_1)
        # test skip layer.bias
        model_dict, ch, unch = copy_model_state(model_one, model_two, exclude_vars="layer.bias")
        x = np.random.randn(4, 10)
        x = torch.tensor(x, device=device_0, dtype=torch.float32)
        output = model_one(x).detach().cpu().numpy()
        expected = np.array(
            [
                [-0.34172416, 0.0375042, -0.98340976],
                [-0.03364138, -0.08927619, -0.2246768],
                [-0.35700908, -0.15556987, -0.27658707],
                [0.61680925, -0.6106281, -0.02123314],
            ]
        )
        np.testing.assert_allclose(output, expected, atol=1e-3)
        self.assertEqual(len(ch), 1)
        self.assertEqual(len(unch), 3)

</source>
<source file="systems/MONAI-0.6.0/tests/test_copy_model_state.py" startline="127" endline="152" pcid="972">
    def test_set_map_across(self, device_0, device_1):
        set_determinism(0)
        model_one = _TestModelOne(10, 10, 3)
        model_two = _TestModelTwo(10, 10, 10, 4)
        model_one.to(device_0)
        model_two.to(device_1)
        # test weight map
        model_dict, ch, unch = copy_model_state(
            model_one, model_two, mapping={"layer_1.weight": "layer.weight", "layer_1.bias": "layer_1.weight"}
        )
        model_one.load_state_dict(model_dict)
        x = np.random.randn(4, 10)
        x = torch.tensor(x, device=device_0, dtype=torch.float32)
        output = model_one(x).detach().cpu().numpy()
        expected = np.array(
            [
                [0.8244487, -0.19650555, 0.65723234],
                [0.71239626, 0.25617486, 0.5247122],
                [0.24168758, 1.0301148, 0.39089814],
                [0.25791705, 0.8653245, 0.14833644],
            ]
        )
        np.testing.assert_allclose(output, expected, atol=1e-3)
        self.assertEqual(len(ch), 2)
        self.assertEqual(len(unch), 2)

</source>
<source file="systems/MONAI-0.6.0/tests/test_copy_model_state.py" startline="154" endline="180" pcid="973">
    def test_set_prefix(self, device_0, device_1):
        set_determinism(0)
        model_one = torch.nn.Sequential(_TestModelOne(10, 20, 3))
        model_two = _TestModelTwo(10, 20, 10, 4)
        model_one.to(device_0)
        model_two.to(device_1)
        # test skip layer.bias
        model_dict, ch, unch = copy_model_state(
            model_one, model_two, dst_prefix="0.", exclude_vars="layer.bias", inplace=False
        )
        model_one.load_state_dict(model_dict)
        x = np.random.randn(4, 10)
        x = torch.tensor(x, device=device_0, dtype=torch.float32)
        output = model_one(x).detach().cpu().numpy()
        expected = np.array(
            [
                [-0.360766, -0.031778, -0.770227],
                [-0.052683, -0.158559, -0.011493],
                [-0.376051, -0.224852, -0.063404],
                [0.597767, -0.679911, 0.19195],
            ]
        )
        np.testing.assert_allclose(output, expected, atol=1e-3)
        self.assertEqual(len(ch), 2)
        self.assertEqual(len(unch), 2)


</source>
</class>

<class classid="52" nclones="2" nlines="18" similarity="100">
<source file="systems/MONAI-0.6.0/tests/test_create_grid_and_affine.py" startline="218" endline="236" pcid="996">
    def test_create_scale(self):
        test_assert(create_scale, (2, 2), np.array([[2.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]))
        test_assert(create_scale, (2, [2, 2, 2]), np.array([[2.0, 0.0, 0.0], [0.0, 2.0, 0.0], [0.0, 0.0, 1.0]]))
        test_assert(
            create_scale,
            (3, [1.5, 2.4]),
            np.array([[1.5, 0.0, 0.0, 0.0], [0.0, 2.4, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]]),
        )
        test_assert(
            create_scale,
            (3, 1.5),
            np.array([[1.5, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]]),
        )
        test_assert(
            create_scale,
            (3, [1, 2, 3, 4, 5]),
            np.array([[1.0, 0.0, 0.0, 0.0], [0.0, 2.0, 0.0, 0.0], [0.0, 0.0, 3.0, 0.0], [0.0, 0.0, 0.0, 1.0]]),
        )

</source>
<source file="systems/MONAI-0.6.0/tests/test_create_grid_and_affine.py" startline="237" endline="256" pcid="997">
    def test_create_translate(self):
        test_assert(create_translate, (2, 2), np.array([[1.0, 0.0, 2.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]))
        test_assert(create_translate, (2, [2, 2, 2]), np.array([[1.0, 0.0, 2.0], [0.0, 1.0, 2.0], [0.0, 0.0, 1.0]]))
        test_assert(
            create_translate,
            (3, [1.5, 2.4]),
            np.array([[1.0, 0.0, 0.0, 1.5], [0.0, 1.0, 0.0, 2.4], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]]),
        )
        test_assert(
            create_translate,
            (3, 1.5),
            np.array([[1.0, 0.0, 0.0, 1.5], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]]),
        )
        test_assert(
            create_translate,
            (3, [1, 2, 3, 4, 5]),
            np.array([[1.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 2.0], [0.0, 0.0, 1.0, 3.0], [0.0, 0.0, 0.0, 1.0]]),
        )


</source>
</class>

<class classid="53" nclones="2" nlines="15" similarity="80">
<source file="systems/MONAI-0.6.0/tests/test_distributed_weighted_random_sampler.py" startline="24" endline="40" pcid="1015">
    def test_sampling(self):
        data = [1, 2, 3, 4, 5]
        weights = [1, 2, 3, 4, 5]
        sampler = DistributedWeightedRandomSampler(
            weights=weights,
            dataset=data,
            shuffle=False,
            generator=torch.Generator().manual_seed(0),
        )
        samples = np.array([data[i] for i in list(sampler)])

        if dist.get_rank() == 0:
            np.testing.assert_allclose(samples, np.array([5, 5, 5]))

        if dist.get_rank() == 1:
            np.testing.assert_allclose(samples, np.array([1, 4, 4]))

</source>
<source file="systems/MONAI-0.6.0/tests/test_distributed_weighted_random_sampler.py" startline="42" endline="60" pcid="1016">
    def test_num_samples(self):
        data = [1, 2, 3, 4, 5]
        weights = [1, 2, 3, 4, 5]
        sampler = DistributedWeightedRandomSampler(
            weights=weights,
            num_samples_per_rank=5,
            dataset=data,
            shuffle=False,
            generator=torch.Generator().manual_seed(123),
        )
        samples = np.array([data[i] for i in list(sampler)])

        if dist.get_rank() == 0:
            np.testing.assert_allclose(samples, np.array([3, 1, 5, 1, 5]))

        if dist.get_rank() == 1:
            np.testing.assert_allclose(samples, np.array([4, 2, 4, 2, 4]))


</source>
</class>

<class classid="54" nclones="3" nlines="20" similarity="80">
<source file="systems/MONAI-0.6.0/tests/test_rotated.py" startline="40" endline="64" pcid="1041">
    def test_correct_results(self, angle, keep_size, mode, padding_mode, align_corners):
        rotate_fn = Rotated(("img", "seg"), angle, keep_size, (mode, "nearest"), padding_mode, align_corners)
        rotated = rotate_fn({"img": self.imt[0], "seg": self.segn[0]})
        if keep_size:
            np.testing.assert_allclose(self.imt[0].shape, rotated["img"].shape)
        _order = 0 if mode == "nearest" else 1
        if padding_mode == "border":
            _mode = "nearest"
        elif padding_mode == "reflection":
            _mode = "reflect"
        else:
            _mode = "constant"
        expected = scipy.ndimage.rotate(
            self.imt[0, 0], -np.rad2deg(angle), (0, 1), not keep_size, order=_order, mode=_mode, prefilter=False
        )
        good = np.sum(np.isclose(expected, rotated["img"][0], atol=1e-3))
        self.assertLessEqual(np.abs(good - expected.size), 5, "diff at most 5 pixels")

        expected = scipy.ndimage.rotate(
            self.segn[0, 0], -np.rad2deg(angle), (0, 1), not keep_size, order=0, mode=_mode, prefilter=False
        )
        expected = np.stack(expected).astype(int)
        self.assertLessEqual(np.count_nonzero(expected != rotated["seg"][0]), 30)


</source>
<source file="systems/MONAI-0.6.0/tests/test_rotated.py" startline="67" endline="91" pcid="1042">
    def test_correct_results(self, angle, keep_size, mode, padding_mode, align_corners):
        rotate_fn = Rotated(("img", "seg"), [0, angle, 0], keep_size, (mode, "nearest"), padding_mode, align_corners)
        rotated = rotate_fn({"img": self.imt[0], "seg": self.segn[0]})
        if keep_size:
            np.testing.assert_allclose(self.imt[0].shape, rotated["img"].shape)
        _order = 0 if mode == "nearest" else 1
        if padding_mode == "border":
            _mode = "nearest"
        elif padding_mode == "reflection":
            _mode = "reflect"
        else:
            _mode = "constant"
        expected = scipy.ndimage.rotate(
            self.imt[0, 0], np.rad2deg(angle), (0, 2), not keep_size, order=_order, mode=_mode, prefilter=False
        )
        good = np.sum(np.isclose(expected.astype(np.float32), rotated["img"][0], atol=1e-3))
        self.assertLessEqual(np.abs(good - expected.size), 5, "diff at most 5 voxels.")

        expected = scipy.ndimage.rotate(
            self.segn[0, 0], np.rad2deg(angle), (0, 2), not keep_size, order=0, mode=_mode, prefilter=False
        )
        expected = np.stack(expected).astype(int)
        self.assertLessEqual(np.count_nonzero(expected != rotated["seg"][0]), 130)


</source>
<source file="systems/MONAI-0.6.0/tests/test_rotated.py" startline="94" endline="118" pcid="1043">
    def test_correct_results(self, angle, keep_size, mode, padding_mode, align_corners):
        rotate_fn = Rotated(("img", "seg"), [0, 0, angle], keep_size, (mode, "nearest"), padding_mode, align_corners)
        rotated = rotate_fn({"img": self.imt[0], "seg": self.segn[0]})
        if keep_size:
            np.testing.assert_allclose(self.imt[0].shape, rotated["img"].shape)
        _order = 0 if mode == "nearest" else 1
        if padding_mode == "border":
            _mode = "nearest"
        elif padding_mode == "reflection":
            _mode = "reflect"
        else:
            _mode = "constant"
        expected = scipy.ndimage.rotate(
            self.imt[0, 0], -np.rad2deg(angle), (0, 1), not keep_size, order=_order, mode=_mode, prefilter=False
        )
        good = np.sum(np.isclose(expected, rotated["img"][0], atol=1e-3))
        self.assertLessEqual(np.abs(good - expected.size), 5, "diff at most 5 voxels")

        expected = scipy.ndimage.rotate(
            self.segn[0, 0], -np.rad2deg(angle), (0, 1), not keep_size, order=0, mode=_mode, prefilter=False
        )
        expected = np.stack(expected).astype(int)
        self.assertLessEqual(np.count_nonzero(expected != rotated["seg"][0]), 130)


</source>
</class>

<class classid="55" nclones="2" nlines="19" similarity="84">
<source file="systems/MONAI-0.6.0/tests/test_compose.py" startline="60" endline="81" pcid="1132">
    def test_list_dict_compose(self):
        def a(d):  # transform to handle dict data
            d = dict(d)
            d["a"] += 1
            return d

        def b(d):  # transform to generate a batch list of data
            d = dict(d)
            d["b"] += 1
            d = [d] * 5
            return d

        def c(d):  # transform to handle dict data
            d = dict(d)
            d["c"] += 1
            return d

        transforms = Compose([a, a, b, c, c])
        value = transforms({"a": 0, "b": 0, "c": 0})
        for item in value:
            self.assertDictEqual(item, {"a": 2, "b": 1, "c": 2})

</source>
<source file="systems/MONAI-0.6.0/tests/test_compose.py" startline="102" endline="124" pcid="1142">
    def test_list_dict_compose_no_map(self):
        def a(d):  # transform to handle dict data
            d = dict(d)
            d["a"] += 1
            return d

        def b(d):  # transform to generate a batch list of data
            d = dict(d)
            d["b"] += 1
            d = [d] * 5
            return d

        def c(d):  # transform to handle dict data
            d = [dict(di) for di in d]
            for di in d:
                di["c"] += 1
            return d

        transforms = Compose([a, a, b, c, c], map_items=False)
        value = transforms({"a": 0, "b": 0, "c": 0})
        for item in value:
            self.assertDictEqual(item, {"a": 2, "b": 1, "c": 2})

</source>
</class>

<class classid="56" nclones="2" nlines="18" similarity="83">
<source file="systems/MONAI-0.6.0/tests/test_compose.py" startline="161" endline="183" pcid="1153">
    def test_data_loader(self):
        xform_1 = Compose([_RandXform()])
        train_ds = Dataset([1], transform=xform_1)

        xform_1.set_random_state(123)
        out_1 = train_ds[0]
        self.assertAlmostEqual(out_1, 0.2045649)

        set_determinism(seed=123)
        train_loader = DataLoader(train_ds, num_workers=0)
        out_1 = next(iter(train_loader))
        self.assertAlmostEqual(out_1.cpu().item(), 0.84291356)

        if sys.platform != "win32":  # skip multi-worker tests on win32
            train_loader = DataLoader(train_ds, num_workers=1)
            out_1 = next(iter(train_loader))
            self.assertAlmostEqual(out_1.cpu().item(), 0.180814653)

            train_loader = DataLoader(train_ds, num_workers=2)
            out_1 = next(iter(train_loader))
            self.assertAlmostEqual(out_1.cpu().item(), 0.04293707)
        set_determinism(None)

</source>
<source file="systems/MONAI-0.6.0/tests/test_compose.py" startline="184" endline="205" pcid="1154">
    def test_data_loader_2(self):
        set_determinism(seed=123)
        xform_2 = Compose([_RandXform(), _RandXform()])
        train_ds = Dataset([1], transform=xform_2)

        out_2 = train_ds[0]
        self.assertAlmostEqual(out_2, 0.4092510)

        train_loader = DataLoader(train_ds, num_workers=0)
        out_2 = next(iter(train_loader))
        self.assertAlmostEqual(out_2.cpu().item(), 0.7858843729)

        if sys.platform != "win32":  # skip multi-worker tests on win32
            train_loader = DataLoader(train_ds, num_workers=1)
            out_2 = next(iter(train_loader))
            self.assertAlmostEqual(out_2.cpu().item(), 0.305763411)

            train_loader = DataLoader(train_ds, num_workers=2)
            out_1 = next(iter(train_loader))
            self.assertAlmostEqual(out_1.cpu().item(), 0.131966779)
        set_determinism(None)

</source>
</class>

<class classid="57" nclones="3" nlines="22" similarity="75">
<source file="systems/MONAI-0.6.0/tests/test_handler_stats.py" startline="26" endline="60" pcid="1195">
    def test_metrics_print(self):
        log_stream = StringIO()
        log_handler = logging.StreamHandler(log_stream)
        log_handler.setLevel(logging.INFO)
        key_to_handler = "test_logging"
        key_to_print = "testing_metric"

        # set up engine
        def _train_func(engine, batch):
            return [torch.tensor(0.0)]

        engine = Engine(_train_func)

        # set up dummy metric
        @engine.on(Events.EPOCH_COMPLETED)
        def _update_metric(engine):
            current_metric = engine.state.metrics.get(key_to_print, 0.1)
            engine.state.metrics[key_to_print] = current_metric + 0.1

        # set up testing handler
        stats_handler = StatsHandler(name=key_to_handler, logger_handler=log_handler)
        stats_handler.attach(engine)

        engine.run(range(3), max_epochs=2)

        # check logging output
        output_str = log_stream.getvalue()
        log_handler.close()
        grep = re.compile(f".*{key_to_handler}.*")
        has_key_word = re.compile(f".*{key_to_print}.*")
        for idx, line in enumerate(output_str.split("\n")):
            if grep.match(line):
                if idx in [5, 10]:
                    self.assertTrue(has_key_word.match(line))

</source>
<source file="systems/MONAI-0.6.0/tests/test_handler_stats.py" startline="61" endline="89" pcid="1198">
    def test_loss_print(self):
        log_stream = StringIO()
        log_handler = logging.StreamHandler(log_stream)
        log_handler.setLevel(logging.INFO)
        key_to_handler = "test_logging"
        key_to_print = "myLoss"

        # set up engine
        def _train_func(engine, batch):
            return [torch.tensor(0.0)]

        engine = Engine(_train_func)

        # set up testing handler
        stats_handler = StatsHandler(name=key_to_handler, tag_name=key_to_print, logger_handler=log_handler)
        stats_handler.attach(engine)

        engine.run(range(3), max_epochs=2)

        # check logging output
        output_str = log_stream.getvalue()
        log_handler.close()
        grep = re.compile(f".*{key_to_handler}.*")
        has_key_word = re.compile(f".*{key_to_print}.*")
        for idx, line in enumerate(output_str.split("\n")):
            if grep.match(line):
                if idx in [1, 2, 3, 6, 7, 8]:
                    self.assertTrue(has_key_word.match(line))

</source>
<source file="systems/MONAI-0.6.0/tests/test_handler_stats.py" startline="90" endline="120" pcid="1200">
    def test_loss_dict(self):
        log_stream = StringIO()
        log_handler = logging.StreamHandler(log_stream)
        log_handler.setLevel(logging.INFO)
        key_to_handler = "test_logging"
        key_to_print = "myLoss1"

        # set up engine
        def _train_func(engine, batch):
            return [torch.tensor(0.0)]

        engine = Engine(_train_func)

        # set up testing handler
        stats_handler = StatsHandler(
            name=key_to_handler, output_transform=lambda x: {key_to_print: x}, logger_handler=log_handler
        )
        stats_handler.attach(engine)

        engine.run(range(3), max_epochs=2)

        # check logging output
        output_str = log_stream.getvalue()
        log_handler.close()
        grep = re.compile(f".*{key_to_handler}.*")
        has_key_word = re.compile(f".*{key_to_print}.*")
        for idx, line in enumerate(output_str.split("\n")):
            if grep.match(line):
                if idx in [1, 2, 3, 6, 7, 8]:
                    self.assertTrue(has_key_word.match(line))

</source>
</class>

<class classid="58" nclones="2" nlines="55" similarity="85">
<source file="systems/MONAI-0.6.0/monai/apps/deepgrow/dataset.py" startline="135" endline="212" pcid="1226">
def _save_data_2d(vol_idx, vol_image, vol_label, dataset_dir, relative_path):
    data_list = []

    if len(vol_image.shape) == 4:
        logging.info(
            "4D-Image, pick only first series; Image: {}; Label: {}".format(
                vol_image.shape, vol_label.shape if vol_label is not None else None
            )
        )
        vol_image = vol_image[0]
        vol_image = np.moveaxis(vol_image, -1, 0)

    image_count = 0
    label_count = 0
    unique_labels_count = 0
    for sid in range(vol_image.shape[0]):
        image = vol_image[sid, ...]
        label = vol_label[sid, ...] if vol_label is not None else None

        if vol_label is not None and np.sum(label) == 0:
            continue

        image_file_prefix = "vol_idx_{:0>4d}_slice_{:0>3d}".format(vol_idx, sid)
        image_file = os.path.join(dataset_dir, "images", image_file_prefix)
        image_file += ".npy"

        os.makedirs(os.path.join(dataset_dir, "images"), exist_ok=True)
        np.save(image_file, image)
        image_count += 1

        # Test Data
        if vol_label is None:
            data_list.append(
                {
                    "image": image_file.replace(dataset_dir + os.pathsep, "") if relative_path else image_file,
                }
            )
            continue

        # For all Labels
        unique_labels = np.unique(label.flatten())
        unique_labels = unique_labels[unique_labels != 0]
        unique_labels_count = max(unique_labels_count, len(unique_labels))

        for idx in unique_labels:
            label_file_prefix = "{}_region_{:0>2d}".format(image_file_prefix, int(idx))
            label_file = os.path.join(dataset_dir, "labels", label_file_prefix)
            label_file += ".npy"

            os.makedirs(os.path.join(dataset_dir, "labels"), exist_ok=True)
            curr_label = (label == idx).astype(np.float32)
            np.save(label_file, curr_label)

            label_count += 1
            data_list.append(
                {
                    "image": image_file.replace(dataset_dir + os.pathsep, "") if relative_path else image_file,
                    "label": label_file.replace(dataset_dir + os.pathsep, "") if relative_path else label_file,
                    "region": int(idx),
                }
            )

    if unique_labels_count >= 20:
        logging.warning(f"Unique labels {unique_labels_count} exceeds 20. Please check if this is correct.")

    logging.info(
        "{} => Image Shape: {} => {}; Label Shape: {} => {}; Unique Labels: {}".format(
            vol_idx,
            vol_image.shape,
            image_count,
            vol_label.shape if vol_label is not None else None,
            label_count,
            unique_labels_count,
        )
    )
    return data_list


</source>
<source file="systems/MONAI-0.6.0/monai/apps/deepgrow/dataset.py" startline="213" endline="281" pcid="1227">
def _save_data_3d(vol_idx, vol_image, vol_label, dataset_dir, relative_path):
    data_list = []

    if len(vol_image.shape) == 4:
        logging.info(
            "4D-Image, pick only first series; Image: {}; Label: {}".format(
                vol_image.shape, vol_label.shape if vol_label is not None else None
            )
        )
        vol_image = vol_image[0]
        vol_image = np.moveaxis(vol_image, -1, 0)

    image_count = 0
    label_count = 0
    unique_labels_count = 0

    image_file_prefix = "vol_idx_{:0>4d}".format(vol_idx)
    image_file = os.path.join(dataset_dir, "images", image_file_prefix)
    image_file += ".npy"

    os.makedirs(os.path.join(dataset_dir, "images"), exist_ok=True)
    np.save(image_file, vol_image)
    image_count += 1

    # Test Data
    if vol_label is None:
        data_list.append(
            {
                "image": image_file.replace(dataset_dir + os.pathsep, "") if relative_path else image_file,
            }
        )
    else:
        # For all Labels
        unique_labels = np.unique(vol_label.flatten())
        unique_labels = unique_labels[unique_labels != 0]
        unique_labels_count = max(unique_labels_count, len(unique_labels))

        for idx in unique_labels:
            label_file_prefix = "{}_region_{:0>2d}".format(image_file_prefix, int(idx))
            label_file = os.path.join(dataset_dir, "labels", label_file_prefix)
            label_file += ".npy"

            curr_label = (vol_label == idx).astype(np.float32)
            os.makedirs(os.path.join(dataset_dir, "labels"), exist_ok=True)
            np.save(label_file, curr_label)

            label_count += 1
            data_list.append(
                {
                    "image": image_file.replace(dataset_dir + os.pathsep, "") if relative_path else image_file,
                    "label": label_file.replace(dataset_dir + os.pathsep, "") if relative_path else label_file,
                    "region": int(idx),
                }
            )

    if unique_labels_count >= 20:
        logging.warning(f"Unique labels {unique_labels_count} exceeds 20. Please check if this is correct.")

    logging.info(
        "{} => Image Shape: {} => {}; Label Shape: {} => {}; Unique Labels: {}".format(
            vol_idx,
            vol_image.shape,
            image_count,
            vol_label.shape if vol_label is not None else None,
            label_count,
            unique_labels_count,
        )
    )
    return data_list
</source>
</class>

<class classid="59" nclones="2" nlines="16" similarity="76">
<source file="systems/MONAI-0.6.0/monai/engines/multi_gpu_supervised_trainer.py" startline="53" endline="101" pcid="1257">
def create_multigpu_supervised_trainer(
    net: torch.nn.Module,
    optimizer: Optimizer,
    loss_fn: Callable,
    devices: Optional[Sequence[torch.device]] = None,
    non_blocking: bool = False,
    prepare_batch: Callable = _prepare_batch,
    output_transform: Callable = _default_transform,
    distributed: bool = False,
) -> Engine:
    """
    Derived from `create_supervised_trainer` in Ignite.

    Factory function for creating a trainer for supervised models.

    Args:
        net: the network to train.
        optimizer: the optimizer to use.
        loss_fn: the loss function to use.
        devices: device(s) type specification (default: None).
            Applies to both model and batches. None is all devices used, empty list is CPU only.
        non_blocking: if True and this copy is between CPU and GPU, the copy may occur asynchronously
            with respect to the host. For other cases, this argument has no effect.
        prepare_batch: function that receives `batch`, `device`, `non_blocking` and outputs
            tuple of tensors `(batch_x, batch_y)`.
        output_transform: function that receives 'x', 'y', 'y_pred', 'loss' and returns value
            to be assigned to engine's state.output after each iteration. Default is returning `loss.item()`.
        distributed: whether convert model to `DistributedDataParallel`, if have multiple devices, use
            the first device as output device.

    Returns:
        Engine: a trainer engine with supervised update function.

    Note:
        `engine.state.output` for this engine is defined by `output_transform` parameter and is the loss
        of the processed batch by default.
    """

    devices_ = get_devices_spec(devices)
    if distributed:
        net = DistributedDataParallel(net, device_ids=devices_)
    elif len(devices_) > 1:
        net = DataParallel(net)

    return create_supervised_trainer(
        net, optimizer, loss_fn, devices_[0], non_blocking, prepare_batch, output_transform
    )


</source>
<source file="systems/MONAI-0.6.0/monai/engines/multi_gpu_supervised_trainer.py" startline="102" endline="146" pcid="1258">
def create_multigpu_supervised_evaluator(
    net: torch.nn.Module,
    metrics: Optional[Dict[str, Metric]] = None,
    devices: Optional[Sequence[torch.device]] = None,
    non_blocking: bool = False,
    prepare_batch: Callable = _prepare_batch,
    output_transform: Callable = _default_eval_transform,
    distributed: bool = False,
) -> Engine:
    """
    Derived from `create_supervised_evaluator` in Ignite.

    Factory function for creating an evaluator for supervised models.

    Args:
        net: the model to train.
        metrics: a map of metric names to Metrics.
        devices: device(s) type specification (default: None).
            Applies to both model and batches. None is all devices used, empty list is CPU only.
        non_blocking: if True and this copy is between CPU and GPU, the copy may occur asynchronously
            with respect to the host. For other cases, this argument has no effect.
        prepare_batch: function that receives `batch`, `device`, `non_blocking` and outputs
            tuple of tensors `(batch_x, batch_y)`.
        output_transform: function that receives 'x', 'y', 'y_pred' and returns value
            to be assigned to engine's state.output after each iteration. Default is returning `(y_pred, y,)`
            which fits output expected by metrics. If you change it you should use `output_transform` in metrics.
        distributed: whether convert model to `DistributedDataParallel`, if have multiple devices, use
            the first device as output device.

    Note:
        `engine.state.output` for this engine is defined by `output_transform` parameter and is
        a tuple of `(batch_pred, batch_y)` by default.

    Returns:
        Engine: an evaluator engine with supervised inference function.
    """

    devices_ = get_devices_spec(devices)

    if distributed:
        net = DistributedDataParallel(net, device_ids=devices_)
    elif len(devices_) > 1:
        net = DataParallel(net)

    return create_supervised_evaluator(net, metrics, devices_[0], non_blocking, prepare_batch, output_transform)
</source>
</class>

<class classid="60" nclones="5" nlines="15" similarity="70">
<source file="systems/MONAI-0.6.0/monai/handlers/earlystop_handler.py" startline="52" endline="70" pcid="1261">
    def __init__(
        self,
        patience: int,
        score_function: Callable,
        trainer: Optional[Engine] = None,
        min_delta: float = 0.0,
        cumulative_delta: bool = False,
        epoch_level: bool = True,
    ) -> None:
        self.patience = patience
        self.score_function = score_function
        self.min_delta = min_delta
        self.cumulative_delta = cumulative_delta
        self.epoch_level = epoch_level
        self._handler = None

        if trainer is not None:
            self.set_trainer(trainer=trainer)

</source>
<source file="systems/MONAI-0.6.0/monai/metrics/hausdorff_distance.py" startline="53" endline="69" pcid="1429">
    def __init__(
        self,
        include_background: bool = False,
        distance_metric: str = "euclidean",
        percentile: Optional[float] = None,
        directed: bool = False,
        reduction: Union[MetricReduction, str] = MetricReduction.MEAN,
        get_not_nans: bool = False,
    ) -> None:
        super().__init__()
        self.include_background = include_background
        self.distance_metric = distance_metric
        self.percentile = percentile
        self.directed = directed
        self.reduction = reduction
        self.get_not_nans = get_not_nans

</source>
<source file="systems/MONAI-0.6.0/monai/metrics/surface_distance.py" startline="48" endline="62" pcid="1442">
    def __init__(
        self,
        include_background: bool = False,
        symmetric: bool = False,
        distance_metric: str = "euclidean",
        reduction: Union[MetricReduction, str] = MetricReduction.MEAN,
        get_not_nans: bool = False,
    ) -> None:
        super().__init__()
        self.include_background = include_background
        self.distance_metric = distance_metric
        self.symmetric = symmetric
        self.reduction = reduction
        self.get_not_nans = get_not_nans

</source>
<source file="systems/MONAI-0.6.0/monai/transforms/post/array.py" startline="128" endline="141" pcid="1401">
    def __init__(
        self,
        argmax: bool = False,
        to_onehot: bool = False,
        n_classes: Optional[int] = None,
        threshold_values: bool = False,
        logit_thresh: float = 0.5,
    ) -> None:
        self.argmax = argmax
        self.to_onehot = to_onehot
        self.n_classes = n_classes
        self.threshold_values = threshold_values
        self.logit_thresh = logit_thresh

</source>
<source file="systems/MONAI-0.6.0/monai/metrics/meandice.py" startline="46" endline="56" pcid="1446">
    def __init__(
        self,
        include_background: bool = True,
        reduction: Union[MetricReduction, str] = MetricReduction.MEAN,
        get_not_nans: bool = False,
    ) -> None:
        super().__init__()
        self.include_background = include_background
        self.reduction = reduction
        self.get_not_nans = get_not_nans

</source>
</class>

<class classid="61" nclones="6" nlines="11" similarity="81">
<source file="systems/MONAI-0.6.0/monai/handlers/regression_metrics.py" startline="24" endline="50" pcid="1279">
    def __init__(
        self,
        output_transform: Callable = lambda x: x,
        save_details: bool = True,
    ) -> None:
        """

        Args:
            output_transform: callable to extract `y_pred` and `y` from `ignite.engine.state.output` then
                construct `(y_pred, y)` pair, where `y_pred` and `y` can be `batch-first` Tensors or
                lists of `channel-first` Tensors. the form of `(y_pred, y)` is required by the `update()`.
                for example: if `ignite.engine.state.output` is `{"pred": xxx, "label": xxx, "other": xxx}`,
                output_transform can be `lambda x: (x["pred"], x["label"])`.
            save_details: whether to save metric computation details per image, for example: mean squared error of every image.
                default to True, will save to `engine.state.metric_details` dict with the metric name as key.

        See also:
            :py:class:`monai.metrics.MSEMetric`
        """
        metric_fn = MSEMetric(reduction=MetricReduction.MEAN)
        super().__init__(
            metric_fn=metric_fn,
            output_transform=output_transform,
            save_details=save_details,
        )


</source>
<source file="systems/MONAI-0.6.0/monai/handlers/roc_auc.py" startline="48" endline="58" pcid="1283">
    def __init__(
        self,
        average: Union[Average, str] = Average.MACRO,
        output_transform: Callable = lambda x: x,
    ) -> None:
        metric_fn = ROCAUCMetric(average=Average(average))
        super().__init__(
            metric_fn=metric_fn,
            output_transform=output_transform,
            save_details=False,
        )
</source>
<source file="systems/MONAI-0.6.0/monai/handlers/regression_metrics.py" startline="56" endline="78" pcid="1280">
    def __init__(
        self,
        output_transform: Callable = lambda x: x,
        save_details: bool = True,
    ) -> None:
        """

        Args:
            output_transform: transform the ignite.engine.state.output into [y_pred, y] pair.
            save_details: whether to save metric computation details per image, for example: mean absolute error of every image.
                default to True, will save to `engine.state.metric_details` dict with the metric name as key.

        See also:
            :py:class:`monai.metrics.MAEMetric`
        """
        metric_fn = MAEMetric(reduction=MetricReduction.MEAN)
        super().__init__(
            metric_fn=metric_fn,
            output_transform=output_transform,
            save_details=save_details,
        )


</source>
<source file="systems/MONAI-0.6.0/monai/handlers/regression_metrics.py" startline="84" endline="106" pcid="1281">
    def __init__(
        self,
        output_transform: Callable = lambda x: x,
        save_details: bool = True,
    ) -> None:
        """

        Args:
            output_transform: transform the ignite.engine.state.output into [y_pred, y] pair.
            save_details: whether to save metric computation details per image, for example: root mean squared error of every image.
                default to True, will save to `engine.state.metric_details` dict with the metric name as key.

        See also:
            :py:class:`monai.metrics.RMSEMetric`
        """
        metric_fn = RMSEMetric(reduction=MetricReduction.MEAN)
        super().__init__(
            metric_fn=metric_fn,
            output_transform=output_transform,
            save_details=save_details,
        )


</source>
<source file="systems/MONAI-0.6.0/monai/handlers/mean_dice.py" startline="24" endline="51" pcid="1338">
    def __init__(
        self,
        include_background: bool = True,
        output_transform: Callable = lambda x: x,
        save_details: bool = True,
    ) -> None:
        """

        Args:
            include_background: whether to include dice computation on the first channel of the predicted output.
                Defaults to True.
            output_transform: callable to extract `y_pred` and `y` from `ignite.engine.state.output` then
                construct `(y_pred, y)` pair, where `y_pred` and `y` can be `batch-first` Tensors or
                lists of `channel-first` Tensors. the form of `(y_pred, y)` is required by the `update()`.
                for example: if `ignite.engine.state.output` is `{"pred": xxx, "label": xxx, "other": xxx}`,
                output_transform can be `lambda x: (x["pred"], x["label"])`.
            save_details: whether to save metric computation details per image, for example: mean dice of every image.
                default to True, will save to `engine.state.metric_details` dict with the metric name as key.

        See also:
            :py:meth:`monai.metrics.meandice.compute_meandice`
        """
        metric_fn = DiceMetric(include_background=include_background, reduction=MetricReduction.MEAN)
        super().__init__(
            metric_fn=metric_fn,
            output_transform=output_transform,
            save_details=save_details,
        )
</source>
<source file="systems/MONAI-0.6.0/monai/handlers/regression_metrics.py" startline="112" endline="136" pcid="1282">
    def __init__(
        self,
        max_val: Union[int, float],
        output_transform: Callable = lambda x: x,
        save_details: bool = True,
    ) -> None:
        """

        Args:
            max_val: The dynamic range of the images/volumes (i.e., the difference between the
                maximum and the minimum allowed values e.g. 255 for a uint8 image).
            output_transform: transform the ignite.engine.state.output into [y_pred, y] pair.
            save_details: whether to save metric computation details per image, for example: PSNR of every image.
                default to True, will save to `engine.state.metric_details` dict with the metric name as key.
            reduction: {``"none"``, ``"mean"``, ``"sum"``, ``"mean_batch"``, ``"sum_batch"``,

        See also:
            :py:class:`monai.metrics.PSNRMetric`
        """
        metric_fn = PSNRMetric(max_val=max_val, reduction=MetricReduction.MEAN)
        super().__init__(
            metric_fn=metric_fn,
            output_transform=output_transform,
            save_details=save_details,
        )
</source>
</class>

<class classid="62" nclones="3" nlines="20" similarity="85">
<source file="systems/MONAI-0.6.0/monai/handlers/hausdorff_distance.py" startline="24" endline="64" pcid="1284">
    def __init__(
        self,
        include_background: bool = False,
        distance_metric: str = "euclidean",
        percentile: Optional[float] = None,
        directed: bool = False,
        output_transform: Callable = lambda x: x,
        save_details: bool = True,
    ) -> None:
        """

        Args:
            include_background: whether to include distance computation on the first channel of the predicted output.
                Defaults to ``False``.
            distance_metric: : [``"euclidean"``, ``"chessboard"``, ``"taxicab"``]
                the metric used to compute surface distance. Defaults to ``"euclidean"``.
            percentile: an optional float number between 0 and 100. If specified, the corresponding
                percentile of the Hausdorff Distance rather than the maximum result will be achieved.
                Defaults to ``None``.
            directed: whether to calculate directed Hausdorff distance. Defaults to ``False``.
            output_transform: callable to extract `y_pred` and `y` from `ignite.engine.state.output` then
                construct `(y_pred, y)` pair, where `y_pred` and `y` can be `batch-first` Tensors or
                lists of `channel-first` Tensors. the form of `(y_pred, y)` is required by the `update()`.
                for example: if `ignite.engine.state.output` is `{"pred": xxx, "label": xxx, "other": xxx}`,
                output_transform can be `lambda x: (x["pred"], x["label"])`.
            save_details: whether to save metric computation details per image, for example: hausdorff distance
                of every image. default to True, will save to `engine.state.metric_details` dict with the metric name as key.

        """
        metric_fn = HausdorffDistanceMetric(
            include_background=include_background,
            distance_metric=distance_metric,
            percentile=percentile,
            directed=directed,
            reduction=MetricReduction.MEAN,
        )
        super().__init__(
            metric_fn=metric_fn,
            output_transform=output_transform,
            save_details=save_details,
        )
</source>
<source file="systems/MONAI-0.6.0/monai/handlers/confusion_matrix.py" startline="24" endline="65" pcid="1337">
    def __init__(
        self,
        include_background: bool = True,
        metric_name: str = "hit_rate",
        output_transform: Callable = lambda x: x,
        save_details: bool = True,
    ) -> None:
        """

        Args:
            include_background: whether to skip metric computation on the first channel of
                the predicted output. Defaults to True.
            metric_name: [``"sensitivity"``, ``"specificity"``, ``"precision"``, ``"negative predictive value"``,
                ``"miss rate"``, ``"fall out"``, ``"false discovery rate"``, ``"false omission rate"``,
                ``"prevalence threshold"``, ``"threat score"``, ``"accuracy"``, ``"balanced accuracy"``,
                ``"f1 score"``, ``"matthews correlation coefficient"``, ``"fowlkes mallows index"``,
                ``"informedness"``, ``"markedness"``]
                Some of the metrics have multiple aliases (as shown in the wikipedia page aforementioned),
                and you can also input those names instead.
            output_transform: callable to extract `y_pred` and `y` from `ignite.engine.state.output` then
                construct `(y_pred, y)` pair, where `y_pred` and `y` can be `batch-first` Tensors or
                lists of `channel-first` Tensors. the form of `(y_pred, y)` is required by the `update()`.
                for example: if `ignite.engine.state.output` is `{"pred": xxx, "label": xxx, "other": xxx}`,
                output_transform can be `lambda x: (x["pred"], x["label"])`.
            save_details: whether to save metric computation details per image, for example: TP/TN/FP/FN of every image.
                default to True, will save to `engine.state.metric_details` dict with the metric name as key.

        See also:
            :py:meth:`monai.metrics.confusion_matrix`
        """
        metric_fn = ConfusionMatrixMetric(
            include_background=include_background,
            metric_name=metric_name,
            compute_sample=False,
            reduction=MetricReduction.MEAN,
        )
        self.metric_name = metric_name
        super().__init__(
            metric_fn=metric_fn,
            output_transform=output_transform,
            save_details=save_details,
        )
</source>
<source file="systems/MONAI-0.6.0/monai/handlers/surface_distance.py" startline="24" endline="60" pcid="1324">
    def __init__(
        self,
        include_background: bool = False,
        symmetric: bool = False,
        distance_metric: str = "euclidean",
        output_transform: Callable = lambda x: x,
        save_details: bool = True,
    ) -> None:
        """

        Args:
            include_background: whether to include distance computation on the first channel of the predicted output.
                Defaults to ``False``.
            symmetric: whether to calculate the symmetric average surface distance between
                `seg_pred` and `seg_gt`. Defaults to ``False``.
            distance_metric: : [``"euclidean"``, ``"chessboard"``, ``"taxicab"``]
                the metric used to compute surface distance. Defaults to ``"euclidean"``.
            output_transform: callable to extract `y_pred` and `y` from `ignite.engine.state.output` then
                construct `(y_pred, y)` pair, where `y_pred` and `y` can be `batch-first` Tensors or
                lists of `channel-first` Tensors. the form of `(y_pred, y)` is required by the `update()`.
                for example: if `ignite.engine.state.output` is `{"pred": xxx, "label": xxx, "other": xxx}`,
                output_transform can be `lambda x: (x["pred"], x["label"])`.
            save_details: whether to save metric computation details per image, for example: surface dice
                of every image. default to True, will save to `engine.state.metric_details` dict with the metric name as key.

        """
        metric_fn = SurfaceDistanceMetric(
            include_background=include_background,
            symmetric=symmetric,
            distance_metric=distance_metric,
            reduction=MetricReduction.MEAN,
        )
        super().__init__(
            metric_fn=metric_fn,
            output_transform=output_transform,
            save_details=save_details,
        )
</source>
</class>

<class classid="63" nclones="3" nlines="11" similarity="75">
<source file="systems/MONAI-0.6.0/monai/handlers/checkpoint_saver.py" startline="245" endline="262" pcid="1305">
    def completed(self, engine: Engine) -> None:
        """Callback for train or validation/evaluation completed Event.
        Save final checkpoint if configure save_final is True.

        Args:
            engine: Ignite Engine, it can be a trainer, validator or evaluator.
        """
        if not callable(self._final_checkpoint):
            raise AssertionError("Error: _final_checkpoint function not specified.")
        # delete previous saved final checkpoint if existing
        self._delete_previous_final_ckpt()
        self._final_checkpoint(engine)
        if self.logger is None:
            raise AssertionError
        if not hasattr(self.logger, "info"):
            raise AssertionError("Error, provided logger has not info attribute.")
        self.logger.info(f"Train completed, saved final checkpoint: {self._final_checkpoint.last_checkpoint}")

</source>
<source file="systems/MONAI-0.6.0/monai/handlers/checkpoint_saver.py" startline="263" endline="283" pcid="1306">
    def exception_raised(self, engine: Engine, e: Exception) -> None:
        """Callback for train or validation/evaluation exception raised Event.
        Save current data as final checkpoint if configure save_final is True. This callback may be skipped
        because the logic with Ignite can only trigger the first attached handler for `EXCEPTION_RAISED` event.

        Args:
            engine: Ignite Engine, it can be a trainer, validator or evaluator.
            e: the exception caught in Ignite during engine.run().
        """
        if not callable(self._final_checkpoint):
            raise AssertionError("Error: _final_checkpoint function not specified.")
        # delete previous saved final checkpoint if existing
        self._delete_previous_final_ckpt()
        self._final_checkpoint(engine)
        if self.logger is None:
            raise AssertionError
        if not hasattr(self.logger, "info"):
            raise AssertionError("Error, provided logger has not info attribute.")
        self.logger.info(f"Exception raised, saved the last checkpoint: {self._final_checkpoint.last_checkpoint}")
        raise e

</source>
<source file="systems/MONAI-0.6.0/monai/handlers/checkpoint_saver.py" startline="294" endline="311" pcid="1308">
    def interval_completed(self, engine: Engine) -> None:
        """Callback for train epoch/iteration completed Event.
        Save checkpoint if configure save_interval = N

        Args:
            engine: Ignite Engine, it can be a trainer, validator or evaluator.
        """
        if not callable(self._interval_checkpoint):
            raise AssertionError("Error: _interval_checkpoint function not specified.")
        self._interval_checkpoint(engine)
        if self.logger is None:
            raise AssertionError
        if not hasattr(self.logger, "info"):
            raise AssertionError("Error, provided logger has not info attribute.")
        if self.epoch_level:
            self.logger.info(f"Saved checkpoint at epoch: {engine.state.epoch}")
        else:
            self.logger.info(f"Saved checkpoint at iteration: {engine.state.iteration}")
</source>
</class>

<class classid="64" nclones="2" nlines="43" similarity="71">
<source file="systems/MONAI-0.6.0/monai/utils/deprecated.py" startline="36" endline="109" pcid="1346">
def deprecated(
    since: Optional[str] = None, removed: Optional[str] = None, msg_suffix: str = "", version_val: str = __version__
):
    """
    Marks a function or class as deprecated. If `since` is given this should be a version at or earlier than the
    current version and states at what version of the definition was marked as deprecated. If `removed` is given
    this can be any version and marks when the definition was removed.

    When the decorated definition is called, that is when the function is called or the class instantiated,
    a `DeprecationWarning` is issued if `since` is given and the current version is at or later than that given.
    a `DeprecatedError` exception is instead raised if `removed` is given and the current version is at or later
    than that, or if neither `since` nor `removed` is provided.

    Args:
        since: version at which the definition was marked deprecated but not removed.
        removed: version at which the definition was removed and no longer usable.
        msg_suffix: message appended to warning/exception detailing reasons for deprecation and what to use instead.
        version_val: (used for testing) version to compare since and removed against, default is MONAI version.

    Returns:
        Decorated definition which warns or raises exception when used
    """

    if since is not None and removed is not None and not version_leq(since, removed):
        raise ValueError(f"since must be less or equal to removed, got since={since}, removed={removed}.")
    is_not_yet_deprecated = since is not None and version_val != since and version_leq(version_val, since)
    if is_not_yet_deprecated:
        # smaller than `since`, do nothing
        return lambda obj: obj

    if since is None and removed is None:
        # raise a DeprecatedError directly
        is_removed = True
        is_deprecated = True
    else:
        # compare the numbers
        is_deprecated = since is not None and version_leq(since, version_val)
        is_removed = removed is not None and version_leq(removed, version_val)

    def _decorator(obj):
        is_func = isinstance(obj, FunctionType)
        call_obj = obj if is_func else obj.__init__

        msg_prefix = f"{'Function' if is_func else 'Class'} `{obj.__name__}`"

        if is_removed:
            msg_infix = f"was removed in version {removed}."
        elif is_deprecated:
            msg_infix = f"has been deprecated since version {since}."
            if removed is not None:
                msg_infix += f" It will be removed in version {removed}."
        else:
            msg_infix = "has been deprecated."

        msg = f"{msg_prefix} {msg_infix} {msg_suffix}".strip()

        @wraps(call_obj)
        def _wrapper(*args, **kwargs):
            if is_removed:
                raise DeprecatedError(msg)
            if is_deprecated:
                warn_deprecated(obj, msg)

            return call_obj(*args, **kwargs)

        if is_func:
            return _wrapper
        else:
            obj.__init__ = _wrapper
            return obj

    return _decorator


</source>
<source file="systems/MONAI-0.6.0/monai/utils/deprecated.py" startline="110" endline="190" pcid="1349">
def deprecated_arg(
    name,
    since: Optional[str] = None,
    removed: Optional[str] = None,
    msg_suffix: str = "",
    version_val: str = __version__,
):
    """
    Marks a particular named argument of a callable as deprecated. The same conditions for `since` and `removed` as
    described in the `deprecated` decorator.

    When the decorated definition is called, that is when the function is called or the class instantiated with args,
    a `DeprecationWarning` is issued if `since` is given and the current version is at or later than that given.
    a `DeprecatedError` exception is instead raised if `removed` is given and the current version is at or later
    than that, or if neither `since` nor `removed` is provided.

    Args:
        name: name of position or keyword argument to mark as deprecated.
        since: version at which the argument was marked deprecated but not removed.
        removed: version at which the argument was removed and no longer usable.
        msg_suffix: message appended to warning/exception detailing reasons for deprecation and what to use instead.
        version_val: (used for testing) version to compare since and removed against, default is MONAI version.

    Returns:
        Decorated callable which warns or raises exception when deprecated argument used
    """
    if since is not None and removed is not None and not version_leq(since, removed):
        raise ValueError(f"since must be less or equal to removed, got since={since}, removed={removed}.")
    is_not_yet_deprecated = since is not None and version_val != since and version_leq(version_val, since)
    if is_not_yet_deprecated:
        # smaller than `since`, do nothing
        return lambda obj: obj

    if since is None and removed is None:
        # raise a DeprecatedError directly
        is_removed = True
        is_deprecated = True
    else:
        # compare the numbers
        is_deprecated = since is not None and version_leq(since, version_val)
        is_removed = removed is not None and version_leq(removed, version_val)

    if is_not_yet_deprecated:
        return lambda obj: obj

    def _decorator(func):
        argname = f"{func.__name__}_{name}"

        msg_prefix = f"Argument `{name}`"

        if is_removed:
            msg_infix = f"was removed in version {removed}."
        elif is_deprecated:
            msg_infix = f"has been deprecated since version {since}."
            if removed is not None:
                msg_infix += f" It will be removed in version {removed}."
        else:
            msg_infix = "has been deprecated."

        msg = f"{msg_prefix} {msg_infix} {msg_suffix}".strip()

        sig = inspect.signature(func)

        @wraps(func)
        def _wrapper(*args, **kwargs):
            binding = sig.bind(*args, **kwargs).arguments

            positional_found = name in binding
            kw_found = "kwargs" in binding and name in binding["kwargs"]

            if positional_found or kw_found:
                if is_removed:
                    raise DeprecatedError(msg)
                if is_deprecated:
                    warn_deprecated(argname, msg)

            return func(*args, **kwargs)

        return _wrapper

    return _decorator
</source>
</class>

<class classid="65" nclones="2" nlines="29" similarity="83">
<source file="systems/MONAI-0.6.0/monai/_version.py" startline="70" endline="104" pcid="1376">
def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False,
                env=None):
    """Call the given command(s)."""
    assert isinstance(commands, list)
    p = None
    for c in commands:
        try:
            dispcmd = str([c] + args)
            # remember shell=False, so use git.cmd on windows, not just git
            p = subprocess.Popen([c] + args, cwd=cwd, env=env,
                                 stdout=subprocess.PIPE,
                                 stderr=(subprocess.PIPE if hide_stderr
                                         else None))
            break
        except EnvironmentError:
            e = sys.exc_info()[1]
            if e.errno == errno.ENOENT:
                continue
            if verbose:
                print("unable to run %s" % dispcmd)
                print(e)
            return None, None
    else:
        if verbose:
            print("unable to find command, tried %s" % (commands,))
        return None, None
    stdout = p.communicate()[0].strip().decode()
    if p.returncode != 0:
        if verbose:
            print("unable to run %s (error)" % dispcmd)
            print("stdout was %s" % stdout)
        return None, p.returncode
    return stdout, p.returncode


</source>
<source file="systems/MONAI-0.6.0/versioneer.py" startline="380" endline="412" pcid="1555">
def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):
    """Call the given command(s)."""
    assert isinstance(commands, list)
    p = None
    for c in commands:
        try:
            dispcmd = str([c] + args)
            # remember shell=False, so use git.cmd on windows, not just git
            p = subprocess.Popen(
                [c] + args, cwd=cwd, env=env, stdout=subprocess.PIPE, stderr=(subprocess.PIPE if hide_stderr else None)
            )
            break
        except EnvironmentError:
            e = sys.exc_info()[1]
            if e.errno == errno.ENOENT:
                continue
            if verbose:
                print("unable to run %s" % dispcmd)
                print(e)
            return None, None
    else:
        if verbose:
            print("unable to find command, tried %s" % (commands,))
        return None, None
    stdout = p.communicate()[0].strip().decode()
    if p.returncode != 0:
        if verbose:
            print("unable to run %s (error)" % dispcmd)
            print("stdout was %s" % stdout)
        return None, p.returncode
    return stdout, p.returncode


</source>
</class>

<class classid="66" nclones="2" nlines="21" similarity="100">
<source file="systems/MONAI-0.6.0/monai/_version.py" startline="131" endline="158" pcid="1378">
def git_get_keywords(versionfile_abs):
    """Extract version information from the given file."""
    # the code embedded in _version.py can just fetch the value of these
    # keywords. When used from setup.py, we don't want to import _version.py,
    # so we do it with a regexp instead. This function is not used from
    # _version.py.
    keywords = {}
    try:
        f = open(versionfile_abs, "r")
        for line in f.readlines():
            if line.strip().startswith("git_refnames ="):
                mo = re.search(r'=\s*"(.*)"', line)
                if mo:
                    keywords["refnames"] = mo.group(1)
            if line.strip().startswith("git_full ="):
                mo = re.search(r'=\s*"(.*)"', line)
                if mo:
                    keywords["full"] = mo.group(1)
            if line.strip().startswith("git_date ="):
                mo = re.search(r'=\s*"(.*)"', line)
                if mo:
                    keywords["date"] = mo.group(1)
        f.close()
    except EnvironmentError:
        pass
    return keywords


</source>
<source file="systems/MONAI-0.6.0/versioneer.py" startline="944" endline="971" pcid="1556">
def git_get_keywords(versionfile_abs):
    """Extract version information from the given file."""
    # the code embedded in _version.py can just fetch the value of these
    # keywords. When used from setup.py, we don't want to import _version.py,
    # so we do it with a regexp instead. This function is not used from
    # _version.py.
    keywords = {}
    try:
        f = open(versionfile_abs, "r")
        for line in f.readlines():
            if line.strip().startswith("git_refnames ="):
                mo = re.search(r'=\s*"(.*)"', line)
                if mo:
                    keywords["refnames"] = mo.group(1)
            if line.strip().startswith("git_full ="):
                mo = re.search(r'=\s*"(.*)"', line)
                if mo:
                    keywords["full"] = mo.group(1)
            if line.strip().startswith("git_date ="):
                mo = re.search(r'=\s*"(.*)"', line)
                if mo:
                    keywords["date"] = mo.group(1)
        f.close()
    except EnvironmentError:
        pass
    return keywords


</source>
</class>

<class classid="67" nclones="2" nlines="39" similarity="75">
<source file="systems/MONAI-0.6.0/monai/_version.py" startline="160" endline="217" pcid="1379">
def git_versions_from_keywords(keywords, tag_prefix, verbose):
    """Get version information from git keywords."""
    if not keywords:
        raise NotThisMethod("no keywords at all, weird")
    date = keywords.get("date")
    if date is not None:
        # Use only the last line.  Previous lines may contain GPG signature
        # information.
        date = date.splitlines()[-1]

        # git-2.2.0 added "%cI", which expands to an ISO-8601 -compliant
        # datestamp. However we prefer "%ci" (which expands to an "ISO-8601
        # -like" string, which we must then edit to make compliant), because
        # it's been around since git-1.5.3, and it's too difficult to
        # discover which version we're using, or to work around using an
        # older one.
        date = date.strip().replace(" ", "T", 1).replace(" ", "", 1)
    refnames = keywords["refnames"].strip()
    if refnames.startswith("$Format"):
        if verbose:
            print("keywords are unexpanded, not using")
        raise NotThisMethod("unexpanded keywords, not a git-archive tarball")
    refs = set([r.strip() for r in refnames.strip("()").split(",")])
    # starting in git-1.8.3, tags are listed as "tag: foo-1.0" instead of
    # just "foo-1.0". If we see a "tag: " prefix, prefer those.
    TAG = "tag: "
    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])
    if not tags:
        # Either we're using git < 1.8.3, or there really are no tags. We use
        # a heuristic: assume all version tags have a digit. The old git %d
        # expansion behaves like git log --decorate=short and strips out the
        # refs/heads/ and refs/tags/ prefixes that would let us distinguish
        # between branches and tags. By ignoring refnames without digits, we
        # filter out many common branch names like "release" and
        # "stabilization", as well as "HEAD" and "master".
        tags = set([r for r in refs if re.search(r'\d', r)])
        if verbose:
            print("discarding '%s', no digits" % ",".join(refs - tags))
    if verbose:
        print("likely tags: %s" % ",".join(sorted(tags)))
    for ref in sorted(tags):
        # sorting will prefer e.g. "2.0" over "2.0rc1"
        if ref.startswith(tag_prefix):
            r = ref[len(tag_prefix):]
            if verbose:
                print("picking %s" % r)
            return {"version": r,
                    "full-revisionid": keywords["full"].strip(),
                    "dirty": False, "error": None,
                    "date": date}
    # no suitable tags, so version is "0+unknown", but full hex is still there
    if verbose:
        print("no suitable tags, using unknown + full revision id")
    return {"version": "0+unknown",
            "full-revisionid": keywords["full"].strip(),
            "dirty": False, "error": "no suitable tags", "date": None}


</source>
<source file="systems/MONAI-0.6.0/versioneer.py" startline="973" endline="1037" pcid="1557">
def git_versions_from_keywords(keywords, tag_prefix, verbose):
    """Get version information from git keywords."""
    if not keywords:
        raise NotThisMethod("no keywords at all, weird")
    date = keywords.get("date")
    if date is not None:
        # Use only the last line.  Previous lines may contain GPG signature
        # information.
        date = date.splitlines()[-1]

        # git-2.2.0 added "%cI", which expands to an ISO-8601 -compliant
        # datestamp. However we prefer "%ci" (which expands to an "ISO-8601
        # -like" string, which we must then edit to make compliant), because
        # it's been around since git-1.5.3, and it's too difficult to
        # discover which version we're using, or to work around using an
        # older one.
        date = date.strip().replace(" ", "T", 1).replace(" ", "", 1)
    refnames = keywords["refnames"].strip()
    if refnames.startswith("$Format"):
        if verbose:
            print("keywords are unexpanded, not using")
        raise NotThisMethod("unexpanded keywords, not a git-archive tarball")
    refs = set([r.strip() for r in refnames.strip("()").split(",")])
    # starting in git-1.8.3, tags are listed as "tag: foo-1.0" instead of
    # just "foo-1.0". If we see a "tag: " prefix, prefer those.
    TAG = "tag: "
    tags = set([r[len(TAG) :] for r in refs if r.startswith(TAG)])
    if not tags:
        # Either we're using git < 1.8.3, or there really are no tags. We use
        # a heuristic: assume all version tags have a digit. The old git %d
        # expansion behaves like git log --decorate=short and strips out the
        # refs/heads/ and refs/tags/ prefixes that would let us distinguish
        # between branches and tags. By ignoring refnames without digits, we
        # filter out many common branch names like "release" and
        # "stabilization", as well as "HEAD" and "master".
        tags = set([r for r in refs if re.search(r"\d", r)])
        if verbose:
            print("discarding '%s', no digits" % ",".join(refs - tags))
    if verbose:
        print("likely tags: %s" % ",".join(sorted(tags)))
    for ref in sorted(tags):
        # sorting will prefer e.g. "2.0" over "2.0rc1"
        if ref.startswith(tag_prefix):
            r = ref[len(tag_prefix) :]
            if verbose:
                print("picking %s" % r)
            return {
                "version": r,
                "full-revisionid": keywords["full"].strip(),
                "dirty": False,
                "error": None,
                "date": date,
            }
    # no suitable tags, so version is "0+unknown", but full hex is still there
    if verbose:
        print("no suitable tags, using unknown + full revision id")
    return {
        "version": "0+unknown",
        "full-revisionid": keywords["full"].strip(),
        "dirty": False,
        "error": "no suitable tags",
        "date": None,
    }


</source>
</class>

<class classid="68" nclones="2" nlines="54" similarity="75">
<source file="systems/MONAI-0.6.0/monai/_version.py" startline="219" endline="312" pcid="1380">
def git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):
    """Get version from 'git describe' in the root of the source tree.

    This only gets called if the git-archive 'subst' keywords were *not*
    expanded, and _version.py hasn't already been rewritten with a short
    version string, meaning we're inside a checked out source tree.
    """
    GITS = ["git"]
    if sys.platform == "win32":
        GITS = ["git.cmd", "git.exe"]

    out, rc = run_command(GITS, ["rev-parse", "--git-dir"], cwd=root,
                          hide_stderr=True)
    if rc != 0:
        if verbose:
            print("Directory %s not under git control" % root)
        raise NotThisMethod("'git rev-parse --git-dir' returned error")

    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]
    # if there isn't one, this yields HEX[-dirty] (no NUM)
    describe_out, rc = run_command(GITS, ["describe", "--tags", "--dirty",
                                          "--always", "--long",
                                          "--match", "%s*" % tag_prefix],
                                   cwd=root)
    # --long was added in git-1.5.5
    if describe_out is None:
        raise NotThisMethod("'git describe' failed")
    describe_out = describe_out.strip()
    full_out, rc = run_command(GITS, ["rev-parse", "HEAD"], cwd=root)
    if full_out is None:
        raise NotThisMethod("'git rev-parse' failed")
    full_out = full_out.strip()

    pieces = {}
    pieces["long"] = full_out
    pieces["short"] = full_out[:7]  # maybe improved later
    pieces["error"] = None

    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]
    # TAG might have hyphens.
    git_describe = describe_out

    # look for -dirty suffix
    dirty = git_describe.endswith("-dirty")
    pieces["dirty"] = dirty
    if dirty:
        git_describe = git_describe[:git_describe.rindex("-dirty")]

    # now we have TAG-NUM-gHEX or HEX

    if "-" in git_describe:
        # TAG-NUM-gHEX
        mo = re.search(r'^(.+)-(\d+)-g([0-9a-f]+)$', git_describe)
        if not mo:
            # unparseable. Maybe git-describe is misbehaving?
            pieces["error"] = ("unable to parse git-describe output: '%s'"
                               % describe_out)
            return pieces

        # tag
        full_tag = mo.group(1)
        if not full_tag.startswith(tag_prefix):
            if verbose:
                fmt = "tag '%s' doesn't start with prefix '%s'"
                print(fmt % (full_tag, tag_prefix))
            pieces["error"] = ("tag '%s' doesn't start with prefix '%s'"
                               % (full_tag, tag_prefix))
            return pieces
        pieces["closest-tag"] = full_tag[len(tag_prefix):]

        # distance: number of commits since tag
        pieces["distance"] = int(mo.group(2))

        # commit: short hex revision ID
        pieces["short"] = mo.group(3)

    else:
        # HEX: no tags
        pieces["closest-tag"] = None
        count_out, rc = run_command(GITS, ["rev-list", "HEAD", "--count"],
                                    cwd=root)
        pieces["distance"] = int(count_out)  # total number of commits

    # commit date: see ISO-8601 comment in git_versions_from_keywords()
    date = run_command(GITS, ["show", "-s", "--format=%ci", "HEAD"],
                       cwd=root)[0].strip()
    # Use only the last line.  Previous lines may contain GPG signature
    # information.
    date = date.splitlines()[-1]
    pieces["date"] = date.strip().replace(" ", "T", 1).replace(" ", "", 1)

    return pieces


</source>
<source file="systems/MONAI-0.6.0/versioneer.py" startline="1039" endline="1126" pcid="1558">
def git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):
    """Get version from 'git describe' in the root of the source tree.

    This only gets called if the git-archive 'subst' keywords were *not*
    expanded, and _version.py hasn't already been rewritten with a short
    version string, meaning we're inside a checked out source tree.
    """
    GITS = ["git"]
    if sys.platform == "win32":
        GITS = ["git.cmd", "git.exe"]

    out, rc = run_command(GITS, ["rev-parse", "--git-dir"], cwd=root, hide_stderr=True)
    if rc != 0:
        if verbose:
            print("Directory %s not under git control" % root)
        raise NotThisMethod("'git rev-parse --git-dir' returned error")

    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]
    # if there isn't one, this yields HEX[-dirty] (no NUM)
    describe_out, rc = run_command(
        GITS, ["describe", "--tags", "--dirty", "--always", "--long", "--match", "%s*" % tag_prefix], cwd=root
    )
    # --long was added in git-1.5.5
    if describe_out is None:
        raise NotThisMethod("'git describe' failed")
    describe_out = describe_out.strip()
    full_out, rc = run_command(GITS, ["rev-parse", "HEAD"], cwd=root)
    if full_out is None:
        raise NotThisMethod("'git rev-parse' failed")
    full_out = full_out.strip()

    pieces = {}
    pieces["long"] = full_out
    pieces["short"] = full_out[:7]  # maybe improved later
    pieces["error"] = None

    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]
    # TAG might have hyphens.
    git_describe = describe_out

    # look for -dirty suffix
    dirty = git_describe.endswith("-dirty")
    pieces["dirty"] = dirty
    if dirty:
        git_describe = git_describe[: git_describe.rindex("-dirty")]

    # now we have TAG-NUM-gHEX or HEX

    if "-" in git_describe:
        # TAG-NUM-gHEX
        mo = re.search(r"^(.+)-(\d+)-g([0-9a-f]+)$", git_describe)
        if not mo:
            # unparseable. Maybe git-describe is misbehaving?
            pieces["error"] = "unable to parse git-describe output: '%s'" % describe_out
            return pieces

        # tag
        full_tag = mo.group(1)
        if not full_tag.startswith(tag_prefix):
            if verbose:
                fmt = "tag '%s' doesn't start with prefix '%s'"
                print(fmt % (full_tag, tag_prefix))
            pieces["error"] = "tag '%s' doesn't start with prefix '%s'" % (full_tag, tag_prefix)
            return pieces
        pieces["closest-tag"] = full_tag[len(tag_prefix) :]

        # distance: number of commits since tag
        pieces["distance"] = int(mo.group(2))

        # commit: short hex revision ID
        pieces["short"] = mo.group(3)

    else:
        # HEX: no tags
        pieces["closest-tag"] = None
        count_out, rc = run_command(GITS, ["rev-list", "HEAD", "--count"], cwd=root)
        pieces["distance"] = int(count_out)  # total number of commits

    # commit date: see ISO-8601 comment in git_versions_from_keywords()
    date = run_command(GITS, ["show", "-s", "--format=%ci", "HEAD"], cwd=root)[0].strip()
    # Use only the last line.  Previous lines may contain GPG signature
    # information.
    date = date.splitlines()[-1]
    pieces["date"] = date.strip().replace(" ", "T", 1).replace(" ", "", 1)

    return pieces


</source>
</class>

<class classid="69" nclones="6" nlines="13" similarity="71">
<source file="systems/MONAI-0.6.0/monai/_version.py" startline="320" endline="344" pcid="1382">
def render_pep440(pieces):
    """Build up version string, with post-release "local version identifier".

    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you
    get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty

    Exceptions:
    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += plus_or_dot(pieces)
            rendered += "%d.g%s" % (pieces["distance"], pieces["short"])
            if pieces["dirty"]:
                rendered += ".dirty"
    else:
        # exception #1
        rendered = "0+untagged.%d.g%s" % (pieces["distance"],
                                          pieces["short"])
        if pieces["dirty"]:
            rendered += ".dirty"
    return rendered


</source>
<source file="systems/MONAI-0.6.0/versioneer.py" startline="1310" endline="1331" pcid="1567">
def render_pep440_old(pieces):
    """TAG[.postDISTANCE[.dev0]] .

    The ".dev0" means dirty.

    Exceptions:
    1: no tags. 0.postDISTANCE[.dev0]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += ".post%d" % pieces["distance"]
            if pieces["dirty"]:
                rendered += ".dev0"
    else:
        # exception #1
        rendered = "0.post%d" % pieces["distance"]
        if pieces["dirty"]:
            rendered += ".dev0"
    return rendered


</source>
<source file="systems/MONAI-0.6.0/monai/_version.py" startline="388" endline="409" pcid="1385">
def render_pep440_old(pieces):
    """TAG[.postDISTANCE[.dev0]] .

    The ".dev0" means dirty.

    Exceptions:
    1: no tags. 0.postDISTANCE[.dev0]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += ".post%d" % pieces["distance"]
            if pieces["dirty"]:
                rendered += ".dev0"
    else:
        # exception #1
        rendered = "0.post%d" % pieces["distance"]
        if pieces["dirty"]:
            rendered += ".dev0"
    return rendered


</source>
<source file="systems/MONAI-0.6.0/versioneer.py" startline="1243" endline="1266" pcid="1564">
def render_pep440(pieces):
    """Build up version string, with post-release "local version identifier".

    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you
    get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty

    Exceptions:
    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += plus_or_dot(pieces)
            rendered += "%d.g%s" % (pieces["distance"], pieces["short"])
            if pieces["dirty"]:
                rendered += ".dirty"
    else:
        # exception #1
        rendered = "0+untagged.%d.g%s" % (pieces["distance"], pieces["short"])
        if pieces["dirty"]:
            rendered += ".dirty"
    return rendered


</source>
<source file="systems/MONAI-0.6.0/versioneer.py" startline="1283" endline="1309" pcid="1566">
def render_pep440_post(pieces):
    """TAG[.postDISTANCE[.dev0]+gHEX] .

    The ".dev0" means dirty. Note that .dev0 sorts backwards
    (a dirty tree will appear "older" than the corresponding clean one),
    but you shouldn't be releasing software with -dirty anyways.

    Exceptions:
    1: no tags. 0.postDISTANCE[.dev0]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += ".post%d" % pieces["distance"]
            if pieces["dirty"]:
                rendered += ".dev0"
            rendered += plus_or_dot(pieces)
            rendered += "g%s" % pieces["short"]
    else:
        # exception #1
        rendered = "0.post%d" % pieces["distance"]
        if pieces["dirty"]:
            rendered += ".dev0"
        rendered += "+g%s" % pieces["short"]
    return rendered


</source>
<source file="systems/MONAI-0.6.0/monai/_version.py" startline="361" endline="387" pcid="1384">
def render_pep440_post(pieces):
    """TAG[.postDISTANCE[.dev0]+gHEX] .

    The ".dev0" means dirty. Note that .dev0 sorts backwards
    (a dirty tree will appear "older" than the corresponding clean one),
    but you shouldn't be releasing software with -dirty anyways.

    Exceptions:
    1: no tags. 0.postDISTANCE[.dev0]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += ".post%d" % pieces["distance"]
            if pieces["dirty"]:
                rendered += ".dev0"
            rendered += plus_or_dot(pieces)
            rendered += "g%s" % pieces["short"]
    else:
        # exception #1
        rendered = "0.post%d" % pieces["distance"]
        if pieces["dirty"]:
            rendered += ".dev0"
        rendered += "+g%s" % pieces["short"]
    return rendered


</source>
</class>

<class classid="70" nclones="2" nlines="10" similarity="100">
<source file="systems/MONAI-0.6.0/monai/_version.py" startline="410" endline="429" pcid="1386">
def render_git_describe(pieces):
    """TAG[-DISTANCE-gHEX][-dirty].

    Like 'git describe --tags --dirty --always'.

    Exceptions:
    1: no tags. HEX[-dirty]  (note: no 'g' prefix)
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"]:
            rendered += "-%d-g%s" % (pieces["distance"], pieces["short"])
    else:
        # exception #1
        rendered = pieces["short"]
    if pieces["dirty"]:
        rendered += "-dirty"
    return rendered


</source>
<source file="systems/MONAI-0.6.0/versioneer.py" startline="1332" endline="1351" pcid="1568">
def render_git_describe(pieces):
    """TAG[-DISTANCE-gHEX][-dirty].

    Like 'git describe --tags --dirty --always'.

    Exceptions:
    1: no tags. HEX[-dirty]  (note: no 'g' prefix)
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"]:
            rendered += "-%d-g%s" % (pieces["distance"], pieces["short"])
    else:
        # exception #1
        rendered = pieces["short"]
    if pieces["dirty"]:
        rendered += "-dirty"
    return rendered


</source>
</class>

<class classid="71" nclones="2" nlines="29" similarity="70">
<source file="systems/MONAI-0.6.0/monai/_version.py" startline="450" endline="481" pcid="1388">
def render(pieces, style):
    """Render the given version pieces into the requested style."""
    if pieces["error"]:
        return {"version": "unknown",
                "full-revisionid": pieces.get("long"),
                "dirty": None,
                "error": pieces["error"],
                "date": None}

    if not style or style == "default":
        style = "pep440"  # the default

    if style == "pep440":
        rendered = render_pep440(pieces)
    elif style == "pep440-pre":
        rendered = render_pep440_pre(pieces)
    elif style == "pep440-post":
        rendered = render_pep440_post(pieces)
    elif style == "pep440-old":
        rendered = render_pep440_old(pieces)
    elif style == "git-describe":
        rendered = render_git_describe(pieces)
    elif style == "git-describe-long":
        rendered = render_git_describe_long(pieces)
    else:
        raise ValueError("unknown style '%s'" % style)

    return {"version": rendered, "full-revisionid": pieces["long"],
            "dirty": pieces["dirty"], "error": None,
            "date": pieces.get("date")}


</source>
<source file="systems/MONAI-0.6.0/versioneer.py" startline="1372" endline="1409" pcid="1570">
def render(pieces, style):
    """Render the given version pieces into the requested style."""
    if pieces["error"]:
        return {
            "version": "unknown",
            "full-revisionid": pieces.get("long"),
            "dirty": None,
            "error": pieces["error"],
            "date": None,
        }

    if not style or style == "default":
        style = "pep440"  # the default

    if style == "pep440":
        rendered = render_pep440(pieces)
    elif style == "pep440-pre":
        rendered = render_pep440_pre(pieces)
    elif style == "pep440-post":
        rendered = render_pep440_post(pieces)
    elif style == "pep440-old":
        rendered = render_pep440_old(pieces)
    elif style == "git-describe":
        rendered = render_git_describe(pieces)
    elif style == "git-describe-long":
        rendered = render_git_describe_long(pieces)
    else:
        raise ValueError("unknown style '%s'" % style)

    return {
        "version": rendered,
        "full-revisionid": pieces["long"],
        "dirty": pieces["dirty"],
        "error": None,
        "date": pieces.get("date"),
    }


</source>
</class>

<class classid="72" nclones="3" nlines="17" similarity="83">
<source file="systems/MONAI-0.6.0/monai/metrics/hausdorff_distance.py" startline="70" endline="101" pcid="1430">
    def _compute_tensor(self, y_pred: torch.Tensor, y: torch.Tensor):  # type: ignore
        """
        Args:
            y_pred: input data to compute, typical segmentation model output.
                It must be one-hot format and first dim is batch, example shape: [16, 3, 32, 32]. The values
                should be binarized.
            y: ground truth to compute the distance. It must be one-hot format and first dim is batch.
                The values should be binarized.

        Raises:
            ValueError: when `y` is not a binarized tensor.
            ValueError: when `y_pred` has less than three dimensions.
        """
        if not isinstance(y_pred, torch.Tensor) or not isinstance(y, torch.Tensor):
            raise ValueError("y_pred and y must be PyTorch Tensor.")
        if not torch.all(y_pred.byte() == y_pred):
            warnings.warn("y_pred should be a binarized tensor.")
        if not torch.all(y.byte() == y):
            raise ValueError("y should be a binarized tensor.")
        dims = y_pred.ndimension()
        if dims < 3:
            raise ValueError("y_pred should have at least three dimensions.")
        # compute (BxC) for each channel for each batch
        return compute_hausdorff_distance(
            y_pred=y_pred,
            y=y,
            include_background=self.include_background,
            distance_metric=self.distance_metric,
            percentile=self.percentile,
            directed=self.directed,
        )

</source>
<source file="systems/MONAI-0.6.0/monai/metrics/surface_distance.py" startline="63" endline="93" pcid="1443">
    def _compute_tensor(self, y_pred: torch.Tensor, y: torch.Tensor):  # type: ignore
        """
        Args:
            y_pred: input data to compute, typical segmentation model output.
                It must be one-hot format and first dim is batch, example shape: [16, 3, 32, 32]. The values
                should be binarized.
            y: ground truth to compute the distance. It must be one-hot format and first dim is batch.
                The values should be binarized.

        Raises:
            ValueError: when `y` is not a binarized tensor.
            ValueError: when `y_pred` has less than three dimensions.
        """
        if not isinstance(y_pred, torch.Tensor) or not isinstance(y, torch.Tensor):
            raise ValueError("y_pred and y must be PyTorch Tensor.")
        if not torch.all(y_pred.byte() == y_pred):
            warnings.warn("y_pred should be a binarized tensor.")
        if not torch.all(y.byte() == y):
            raise ValueError("y should be a binarized tensor.")
        dims = y_pred.ndimension()
        if dims < 3:
            raise ValueError("y_pred should have at least three dimensions.")
        # compute (BxC) for each channel for each batch
        return compute_average_surface_distance(
            y_pred=y_pred,
            y=y,
            include_background=self.include_background,
            symmetric=self.symmetric,
            distance_metric=self.distance_metric,
        )

</source>
<source file="systems/MONAI-0.6.0/monai/metrics/meandice.py" startline="57" endline="85" pcid="1447">
    def _compute_tensor(self, y_pred: torch.Tensor, y: torch.Tensor):  # type: ignore
        """
        Args:
            y_pred: input data to compute, typical segmentation model output.
                It must be one-hot format and first dim is batch, example shape: [16, 3, 32, 32]. The values
                should be binarized.
            y: ground truth to compute mean dice metric. It must be one-hot format and first dim is batch.
                The values should be binarized.

        Raises:
            ValueError: when `y` is not a binarized tensor.
            ValueError: when `y_pred` has less than three dimensions.
        """
        if not isinstance(y_pred, torch.Tensor) or not isinstance(y, torch.Tensor):
            raise ValueError("y_pred and y must be PyTorch Tensor.")
        if not torch.all(y_pred.byte() == y_pred):
            warnings.warn("y_pred should be a binarized tensor.")
        if not torch.all(y.byte() == y):
            raise ValueError("y should be a binarized tensor.")
        dims = y_pred.ndimension()
        if dims < 3:
            raise ValueError("y_pred should have at least three dimensions.")
        # compute dice (BxC) for each channel for each batch
        return compute_meandice(
            y_pred=y_pred,
            y=y,
            include_background=self.include_background,
        )

</source>
</class>

<class classid="73" nclones="2" nlines="14" similarity="85">
<source file="systems/MONAI-0.6.0/monai/networks/nets/basic_unet.py" startline="27" endline="52" pcid="1466">
    def __init__(
        self,
        dim: int,
        in_chns: int,
        out_chns: int,
        act: Union[str, tuple],
        norm: Union[str, tuple],
        dropout: Union[float, tuple] = 0.0,
    ):
        """
        Args:
            dim: number of spatial dimensions.
            in_chns: number of input channels.
            out_chns: number of output channels.
            act: activation type and arguments.
            norm: feature normalization type and arguments.
            dropout: dropout ratio. Defaults to no dropout.
        """
        super().__init__()

        conv_0 = Convolution(dim, in_chns, out_chns, act=act, norm=norm, dropout=dropout, padding=1)
        conv_1 = Convolution(dim, out_chns, out_chns, act=act, norm=norm, dropout=dropout, padding=1)
        self.add_module("conv_0", conv_0)
        self.add_module("conv_1", conv_1)


</source>
<source file="systems/MONAI-0.6.0/monai/networks/nets/basic_unet.py" startline="56" endline="81" pcid="1467">
    def __init__(
        self,
        dim: int,
        in_chns: int,
        out_chns: int,
        act: Union[str, tuple],
        norm: Union[str, tuple],
        dropout: Union[float, tuple] = 0.0,
    ):
        """
        Args:
            dim: number of spatial dimensions.
            in_chns: number of input channels.
            out_chns: number of output channels.
            act: activation type and arguments.
            norm: feature normalization type and arguments.
            dropout: dropout ratio. Defaults to no dropout.
        """
        super().__init__()

        max_pooling = Pool["MAX", dim](kernel_size=2)
        convs = TwoConv(dim, in_chns, out_chns, act, norm, dropout)
        self.add_module("max_pooling", max_pooling)
        self.add_module("convs", convs)


</source>
</class>

<class classid="74" nclones="2" nlines="14" similarity="92">
<source file="systems/MONAI-0.6.0/monai/networks/nets/classifier.py" startline="73" endline="101" pcid="1473">
    def __init__(
        self,
        in_shape: Sequence[int],
        channels: Sequence[int],
        strides: Sequence[int],
        kernel_size: Union[Sequence[int], int] = 3,
        num_res_units: int = 2,
        act=Act.PRELU,
        norm=Norm.INSTANCE,
        dropout: Optional[float] = 0.25,
        bias: bool = True,
        last_act=Act.SIGMOID,
    ) -> None:
        """
        Args:
            in_shape: tuple of integers stating the dimension of the input tensor (minus batch dimension)
            channels: tuple of integers stating the output channels of each convolutional layer
            strides: tuple of integers stating the stride (downscale factor) of each convolutional layer
            kernel_size: integer or tuple of integers stating size of convolutional kernels
            num_res_units: integer stating number of convolutions in residual units, 0 means no residual units
            act: name or type defining activation layers
            norm: name or type defining normalization layers
            dropout: optional float value in range [0, 1] stating dropout probability for layers, None for no dropout
            bias: boolean stating if convolution layers should have a bias component
            last_act: name defining the last activation layer
        """
        super().__init__(in_shape, 1, channels, strides, kernel_size, num_res_units, act, norm, dropout, bias, last_act)


</source>
<source file="systems/MONAI-0.6.0/monai/networks/nets/classifier.py" startline="109" endline="134" pcid="1474">
    def __init__(
        self,
        in_shape: Sequence[int],
        channels: Sequence[int],
        strides: Sequence[int],
        kernel_size: Union[Sequence[int], int] = 3,
        num_res_units: int = 2,
        act=Act.PRELU,
        norm=Norm.INSTANCE,
        dropout: Optional[float] = 0.25,
        bias: bool = True,
    ) -> None:
        """
        Args:
            in_shape: tuple of integers stating the dimension of the input tensor (minus batch dimension)
            channels: tuple of integers stating the output channels of each convolutional layer
            strides: tuple of integers stating the stride (downscale factor) of each convolutional layer
            kernel_size: integer or tuple of integers stating size of convolutional kernels
            num_res_units: integer stating number of convolutions in residual units, 0 means no residual units
            act: name or type defining activation layers
            norm: name or type defining normalization layers
            dropout: optional float value in range [0, 1] stating dropout probability for layers, None for no dropout
            bias: boolean stating if convolution layers should have a bias component
        """
        super().__init__(in_shape, 1, channels, strides, kernel_size, num_res_units, act, norm, dropout, bias, None)

</source>
</class>

<class classid="75" nclones="2" nlines="18" similarity="70">
<source file="systems/MONAI-0.6.0/monai/networks/nets/segresnet.py" startline="156" endline="177" pcid="1483">
    def forward(self, x):
        x = self.convInit(x)
        if self.dropout_prob is not None:
            x = self.dropout(x)

        down_x = []

        for down in self.down_layers:
            x = down(x)
            down_x.append(x)

        down_x.reverse()

        for i, (up, upl) in enumerate(zip(self.up_samples, self.up_layers)):
            x = up(x) + down_x[i + 1]
            x = upl(x)

        if self.use_conv_final:
            x = self.conv_final(x)
        return x


</source>
<source file="systems/MONAI-0.6.0/monai/networks/nets/segresnet.py" startline="316" endline="342" pcid="1487">
    def forward(self, x):
        net_input = x
        x = self.convInit(x)
        if self.dropout_prob is not None:
            x = self.dropout(x)

        down_x = []
        for down in self.down_layers:
            x = down(x)
            down_x.append(x)

        down_x.reverse()

        vae_input = x

        for i, (up, upl) in enumerate(zip(self.up_samples, self.up_layers)):
            x = up(x) + down_x[i + 1]
            x = upl(x)

        if self.use_conv_final:
            x = self.conv_final(x)

        if self.training:
            vae_loss = self._get_vae_loss(net_input, vae_input)
            return x, vae_loss

        return x, None
</source>
</class>

<class classid="76" nclones="2" nlines="12" similarity="76">
<source file="systems/MONAI-0.6.0/monai/networks/blocks/segresnet_block.py" startline="85" endline="99" pcid="1496">
    def forward(self, x):

        identity = x

        x = self.norm1(x)
        x = self.relu(x)
        x = self.conv1(x)

        x = self.norm2(x)
        x = self.relu(x)
        x = self.conv2(x)

        x += identity

        return x
</source>
<source file="systems/MONAI-0.6.0/monai/networks/blocks/dynunet_block.py" startline="82" endline="96" pcid="1502">
    def forward(self, inp):
        residual = inp
        out = self.conv1(inp)
        out = self.norm1(out)
        out = self.lrelu(out)
        out = self.conv2(out)
        out = self.norm2(out)
        if self.downsample:
            residual = self.conv3(residual)
            residual = self.norm3(residual)
        out += residual
        out = self.lrelu(out)
        return out


</source>
</class>

<class classid="77" nclones="7" nlines="35" similarity="70">
<source file="systems/MONAI-0.6.0/monai/networks/blocks/dynunet_block_v1.py" startline="27" endline="70" pcid="1497">
    def __init__(
        self,
        spatial_dims: int,
        in_channels: int,
        out_channels: int,
        kernel_size: Union[Sequence[int], int],
        stride: Union[Sequence[int], int],
        norm_name: str,
    ):
        nn.Module.__init__(self)
        self.conv1 = get_conv_layer(
            spatial_dims,
            in_channels,
            out_channels,
            kernel_size=kernel_size,
            stride=stride,
            conv_only=True,
        )
        self.conv2 = get_conv_layer(
            spatial_dims,
            out_channels,
            out_channels,
            kernel_size=kernel_size,
            stride=1,
            conv_only=True,
        )
        self.conv3 = get_conv_layer(
            spatial_dims,
            in_channels,
            out_channels,
            kernel_size=1,
            stride=stride,
            conv_only=True,
        )
        self.lrelu = get_act_layer(("leakyrelu", {"inplace": True, "negative_slope": 0.01}))
        self.norm1 = _get_norm_layer(spatial_dims, out_channels, norm_name)
        self.norm2 = _get_norm_layer(spatial_dims, out_channels, norm_name)
        self.norm3 = _get_norm_layer(spatial_dims, out_channels, norm_name)
        self.downsample = in_channels != out_channels
        stride_np = np.atleast_1d(stride)
        if not np.all(stride_np == 1):
            self.downsample = True


</source>
<source file="systems/MONAI-0.6.0/monai/networks/blocks/dynunet_block.py" startline="39" endline="81" pcid="1501">
    def __init__(
        self,
        spatial_dims: int,
        in_channels: int,
        out_channels: int,
        kernel_size: Union[Sequence[int], int],
        stride: Union[Sequence[int], int],
        norm_name: Union[Tuple, str],
    ):
        super(UnetResBlock, self).__init__()
        self.conv1 = get_conv_layer(
            spatial_dims,
            in_channels,
            out_channels,
            kernel_size=kernel_size,
            stride=stride,
            conv_only=True,
        )
        self.conv2 = get_conv_layer(
            spatial_dims,
            out_channels,
            out_channels,
            kernel_size=kernel_size,
            stride=1,
            conv_only=True,
        )
        self.conv3 = get_conv_layer(
            spatial_dims,
            in_channels,
            out_channels,
            kernel_size=1,
            stride=stride,
            conv_only=True,
        )
        self.lrelu = get_act_layer(("leakyrelu", {"inplace": True, "negative_slope": 0.01}))
        self.norm1 = get_norm_layer(name=norm_name, spatial_dims=spatial_dims, channels=out_channels)
        self.norm2 = get_norm_layer(name=norm_name, spatial_dims=spatial_dims, channels=out_channels)
        self.norm3 = get_norm_layer(name=norm_name, spatial_dims=spatial_dims, channels=out_channels)
        self.downsample = in_channels != out_channels
        stride_np = np.atleast_1d(stride)
        if not np.all(stride_np == 1):
            self.downsample = True

</source>
<source file="systems/MONAI-0.6.0/monai/networks/blocks/unetr_block.py" startline="27" endline="81" pcid="1528">
    def __init__(
        self,
        spatial_dims: int,
        in_channels: int,
        out_channels: int,  # type: ignore
        kernel_size: Union[Sequence[int], int],
        stride: Union[Sequence[int], int],
        upsample_kernel_size: Union[Sequence[int], int],
        norm_name: Union[Tuple, str],
        res_block: bool = False,
    ) -> None:
        """
        Args:
            spatial_dims: number of spatial dimensions.
            in_channels: number of input channels.
            out_channels: number of output channels.
            kernel_size: convolution kernel size.
            stride: convolution stride.
            upsample_kernel_size: convolution kernel size for transposed convolution layers.
            norm_name: feature normalization type and arguments.
            res_block: bool argument to determine if residual block is used.

        """

        super(UnetrUpBlock, self).__init__()
        upsample_stride = upsample_kernel_size
        self.transp_conv = get_conv_layer(
            spatial_dims,
            in_channels,
            out_channels,
            kernel_size=upsample_kernel_size,
            stride=upsample_stride,
            conv_only=True,
            is_transposed=True,
        )

        if res_block:
            self.conv_block = UnetResBlock(
                spatial_dims,
                out_channels + out_channels,
                out_channels,
                kernel_size=kernel_size,
                stride=1,
                norm_name=norm_name,
            )
        else:
            self.conv_block = UnetBasicBlock(  # type: ignore
                spatial_dims,
                out_channels + out_channels,
                out_channels,
                kernel_size=kernel_size,
                stride=1,
                norm_name=norm_name,
            )

</source>
<source file="systems/MONAI-0.6.0/monai/networks/blocks/dynunet_block.py" startline="170" endline="199" pcid="1505">
    def __init__(
        self,
        spatial_dims: int,
        in_channels: int,
        out_channels: int,
        kernel_size: Union[Sequence[int], int],
        stride: Union[Sequence[int], int],
        upsample_kernel_size: Union[Sequence[int], int],
        norm_name: Union[Tuple, str],
    ):
        super(UnetUpBlock, self).__init__()
        upsample_stride = upsample_kernel_size
        self.transp_conv = get_conv_layer(
            spatial_dims,
            in_channels,
            out_channels,
            kernel_size=upsample_kernel_size,
            stride=upsample_stride,
            conv_only=True,
            is_transposed=True,
        )
        self.conv_block = UnetBasicBlock(
            spatial_dims,
            out_channels + out_channels,
            out_channels,
            kernel_size=kernel_size,
            stride=1,
            norm_name=norm_name,
        )

</source>
<source file="systems/MONAI-0.6.0/monai/networks/blocks/dynunet_block_v1.py" startline="76" endline="106" pcid="1498">
    def __init__(
        self,
        spatial_dims: int,
        in_channels: int,
        out_channels: int,
        kernel_size: Union[Sequence[int], int],
        stride: Union[Sequence[int], int],
        norm_name: str,
    ):
        nn.Module.__init__(self)
        self.conv1 = get_conv_layer(
            spatial_dims,
            in_channels,
            out_channels,
            kernel_size=kernel_size,
            stride=stride,
            conv_only=True,
        )
        self.conv2 = get_conv_layer(
            spatial_dims,
            out_channels,
            out_channels,
            kernel_size=kernel_size,
            stride=1,
            conv_only=True,
        )
        self.lrelu = get_act_layer(("leakyrelu", {"inplace": True, "negative_slope": 0.01}))
        self.norm1 = _get_norm_layer(spatial_dims, out_channels, norm_name)
        self.norm2 = _get_norm_layer(spatial_dims, out_channels, norm_name)


</source>
<source file="systems/MONAI-0.6.0/monai/networks/blocks/dynunet_block.py" startline="113" endline="142" pcid="1503">
    def __init__(
        self,
        spatial_dims: int,
        in_channels: int,
        out_channels: int,
        kernel_size: Union[Sequence[int], int],
        stride: Union[Sequence[int], int],
        norm_name: Union[Tuple, str],
    ):
        super(UnetBasicBlock, self).__init__()
        self.conv1 = get_conv_layer(
            spatial_dims,
            in_channels,
            out_channels,
            kernel_size=kernel_size,
            stride=stride,
            conv_only=True,
        )
        self.conv2 = get_conv_layer(
            spatial_dims,
            out_channels,
            out_channels,
            kernel_size=kernel_size,
            stride=1,
            conv_only=True,
        )
        self.lrelu = get_act_layer(("leakyrelu", {"inplace": True, "negative_slope": 0.01}))
        self.norm1 = get_norm_layer(name=norm_name, spatial_dims=spatial_dims, channels=out_channels)
        self.norm2 = get_norm_layer(name=norm_name, spatial_dims=spatial_dims, channels=out_channels)

</source>
<source file="systems/MONAI-0.6.0/monai/networks/blocks/dynunet_block_v1.py" startline="112" endline="142" pcid="1499">
    def __init__(
        self,
        spatial_dims: int,
        in_channels: int,
        out_channels: int,
        kernel_size: Union[Sequence[int], int],
        stride: Union[Sequence[int], int],
        upsample_kernel_size: Union[Sequence[int], int],
        norm_name: str,
    ):
        nn.Module.__init__(self)
        upsample_stride = upsample_kernel_size
        self.transp_conv = get_conv_layer(
            spatial_dims,
            in_channels,
            out_channels,
            kernel_size=upsample_kernel_size,
            stride=upsample_stride,
            conv_only=True,
            is_transposed=True,
        )
        self.conv_block = _UnetBasicBlockV1(
            spatial_dims,
            out_channels + out_channels,
            out_channels,
            kernel_size=kernel_size,
            stride=1,
            norm_name=norm_name,
        )


</source>
</class>

<class classid="78" nclones="2" nlines="12" similarity="76">
<source file="systems/MONAI-0.6.0/monai/networks/blocks/dynunet_block.py" startline="252" endline="266" pcid="1510">
def get_padding(
    kernel_size: Union[Sequence[int], int],
    stride: Union[Sequence[int], int],
) -> Union[Tuple[int, ...], int]:

    kernel_size_np = np.atleast_1d(kernel_size)
    stride_np = np.atleast_1d(stride)
    padding_np = (kernel_size_np - stride_np + 1) / 2
    if np.min(padding_np) < 0:
        raise AssertionError("padding value should not be negative, please change the kernel size and/or stride.")
    padding = tuple(int(p) for p in padding_np)

    return padding if len(padding) > 1 else padding[0]


</source>
<source file="systems/MONAI-0.6.0/monai/networks/blocks/dynunet_block.py" startline="267" endline="281" pcid="1511">
def get_output_padding(
    kernel_size: Union[Sequence[int], int],
    stride: Union[Sequence[int], int],
    padding: Union[Sequence[int], int],
) -> Union[Tuple[int, ...], int]:
    kernel_size_np = np.atleast_1d(kernel_size)
    stride_np = np.atleast_1d(stride)
    padding_np = np.atleast_1d(padding)

    out_padding_np = 2 * padding_np + stride_np - kernel_size_np
    if np.min(out_padding_np) < 0:
        raise AssertionError("out_padding value should not be negative, please change the kernel size and/or stride.")
    out_padding = tuple(int(p) for p in out_padding_np)

    return out_padding if len(out_padding) > 1 else out_padding[0]
</source>
</class>

<class classid="79" nclones="2" nlines="20" similarity="100">
<source file="systems/MONAI-0.6.0/versioneer.py" startline="1620" endline="1642" pcid="1579">
            def run(self):
                root = get_root()
                cfg = get_config_from_root(root)
                versions = get_versions()
                target_versionfile = cfg.versionfile_source
                print("UPDATING %s" % target_versionfile)
                write_to_version_file(target_versionfile, versions)

                _build_exe.run(self)
                os.unlink(target_versionfile)
                with open(cfg.versionfile_source, "w") as f:
                    LONG = LONG_VERSION_PY[cfg.VCS]
                    f.write(
                        LONG
                        % {
                            "DOLLAR": "$",
                            "STYLE": cfg.style,
                            "TAG_PREFIX": cfg.tag_prefix,
                            "PARENTDIR_PREFIX": cfg.parentdir_prefix,
                            "VERSIONFILE_SOURCE": cfg.versionfile_source,
                        }
                    )

</source>
<source file="systems/MONAI-0.6.0/versioneer.py" startline="1650" endline="1672" pcid="1580">
            def run(self):
                root = get_root()
                cfg = get_config_from_root(root)
                versions = get_versions()
                target_versionfile = cfg.versionfile_source
                print("UPDATING %s" % target_versionfile)
                write_to_version_file(target_versionfile, versions)

                _py2exe.run(self)
                os.unlink(target_versionfile)
                with open(cfg.versionfile_source, "w") as f:
                    LONG = LONG_VERSION_PY[cfg.VCS]
                    f.write(
                        LONG
                        % {
                            "DOLLAR": "$",
                            "STYLE": cfg.style,
                            "TAG_PREFIX": cfg.tag_prefix,
                            "PARENTDIR_PREFIX": cfg.parentdir_prefix,
                            "VERSIONFILE_SOURCE": cfg.versionfile_source,
                        }
                    )

</source>
</class>

</clones>

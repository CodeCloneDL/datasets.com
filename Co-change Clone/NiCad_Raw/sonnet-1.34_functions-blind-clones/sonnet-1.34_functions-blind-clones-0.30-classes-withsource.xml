<clones>
<systeminfo processor="nicad6" system="sonnet-1.34" granularity="functions-blind" threshold="30%" minlines="10" maxlines="2500"/>
<cloneinfo npcs="926" npairs="87"/>
<runinfo ncompares="33512" cputime="50347"/>
<classinfo nclasses="35"/>

<class classid="1" nclones="2" nlines="10" similarity="100">
<source file="systems/sonnet-1.34/sonnet/examples/rmc_learn_to_execute.py" startline="58" endline="68" pcid="113">
  def __init__(
      self,
      core,
      target_size,
      final_mlp,
      name="sequence_model"):
    super(SequenceModel, self).__init__(name=name)
    self._core = core
    self._target_size = target_size
    self._final_mlp = final_mlp

</source>
<source file="systems/sonnet-1.34/sonnet/examples/rmc_nth_farthest.py" startline="59" endline="69" pcid="118">
  def __init__(
      self,
      core,
      target_size,
      final_mlp,
      name="sequence_model"):
    super(SequenceModel, self).__init__(name=name)
    self._core = core
    self._target_size = target_size
    self._final_mlp = final_mlp

</source>
</class>

<class classid="2" nclones="2" nlines="23" similarity="86">
<source file="systems/sonnet-1.34/sonnet/python/ops/initializers_test.py" startline="129" endline="155" pcid="149">
  def testMoreMultipleRestore(self):
    restore_initializers = {
        'w': initializers.restore_initializer(_checkpoint(), 'w'),
        'b': initializers.restore_initializer(_checkpoint(), 'b')
    }

    with tf.variable_scope('agent'):
      c = convnet.ConvNet2D(
          output_channels=(16, 32),
          kernel_shapes=(8, 4),
          strides=(4, 2),
          paddings=[conv.VALID],
          activation=tf.nn.relu,
          activate_final=True,
          initializers=restore_initializers)

    inputs = tf.constant(1 / 255.0, shape=[1, 86, 86, 3])
    outputs = c(inputs)
    init = tf.global_variables_initializer()
    tf.get_default_graph().finalize()
    with self.test_session() as session:
      session.run(init)
      o = session.run(outputs)

    self.assertAllClose(
        np.linalg.norm(o), _TWO_CONV_LAYERS_RELU, atol=_TOLERANCE)

</source>
<source file="systems/sonnet-1.34/sonnet/python/ops/initializers_test.py" startline="156" endline="183" pcid="150">
  def testFromDifferentScope(self):
    sub = functools.partial(re.sub, r'^[^/]+/', 'agent/')
    restore_initializers = {
        'w': initializers.restore_initializer(_checkpoint(), 'w', sub),
        'b': initializers.restore_initializer(_checkpoint(), 'b', sub)
    }

    with tf.variable_scope('some_random_scope'):
      c = convnet.ConvNet2D(
          output_channels=(16, 32),
          kernel_shapes=(8, 4),
          strides=(4, 2),
          paddings=[conv.VALID],
          activation=tf.nn.relu,
          activate_final=True,
          initializers=restore_initializers)

    inputs = tf.constant(1 / 255.0, shape=[1, 86, 86, 3])
    outputs = c(inputs)
    init = tf.global_variables_initializer()
    tf.get_default_graph().finalize()
    with self.test_session() as session:
      session.run(init)
      o = session.run(outputs)

    self.assertAllClose(
        np.linalg.norm(o), _TWO_CONV_LAYERS_RELU, atol=_TOLERANCE)

</source>
</class>

<class classid="3" nclones="2" nlines="14" similarity="73">
<source file="systems/sonnet-1.34/sonnet/python/custom_getters/context_test.py" startline="36" endline="50" pcid="175">
  def testContextCallsCustomGetterOnlyWhenInScope(self):
    custom_getter = snt.custom_getters.Context(_suffix_getter, verbose=True)
    with tf.variable_scope('', custom_getter=custom_getter):
      lin = snt.Linear(10, name='linear')

    inputs = tf.placeholder(tf.float32, [10, 10])

    _ = lin(inputs)
    self.assertEqual('linear/w:0', lin.w.name)
    with custom_getter:
      _ = lin(inputs)
      self.assertEqual('linear/w_custom:0', lin.w.name)
    _ = lin(inputs)
    self.assertEqual('linear/w:0', lin.w.name)

</source>
<source file="systems/sonnet-1.34/sonnet/python/custom_getters/context_test.py" startline="51" endline="67" pcid="176">
  def testNestedContextCallsCustomGetterOnlyWhenInScope(self):
    custom_getter = snt.custom_getters.Context(_suffix_getter)
    with tf.variable_scope('', custom_getter=custom_getter):
      lin = snt.Linear(10, name='linear')

    inputs = tf.placeholder(tf.float32, [10, 10])
    with custom_getter:
      _ = lin(inputs)
      self.assertEqual('linear/w_custom:0', lin.w.name)
      with custom_getter:
        _ = lin(inputs)
        self.assertEqual('linear/w_custom:0', lin.w.name)
      _ = lin(inputs)
      self.assertEqual('linear/w_custom:0', lin.w.name)
    _ = lin(inputs)
    self.assertEqual('linear/w:0', lin.w.name)

</source>
</class>

<class classid="4" nclones="3" nlines="18" similarity="72">
<source file="systems/sonnet-1.34/sonnet/python/custom_getters/restore_initializer_test.py" startline="54" endline="76" pcid="183">
  def testSimpleUsage(self):
    checkpoint_path, expected_values = self._save_test_checkpoint()
    checkpoint_path = tf.train.latest_checkpoint(checkpoint_path)

    g = tf.Graph()
    with g.as_default():
      custom_getter = snt.custom_getters.restore_initializer(
          filename=checkpoint_path)

      with tf.variable_scope("", custom_getter=custom_getter):
        inputs = tf.placeholder(tf.float32, [10, 10])
        lin1 = snt.Linear(10, name="linear1")
        lin1(inputs)

      init = tf.global_variables_initializer()

    with self.test_session(graph=g) as sess:
      sess.run(init)
      w_value, b_value = sess.run([lin1.w, lin1.b])

    self.assertAllClose(expected_values["w"], w_value)
    self.assertAllClose(expected_values["b"], b_value)

</source>
<source file="systems/sonnet-1.34/sonnet/python/custom_getters/restore_initializer_test.py" startline="104" endline="130" pcid="186">
  def testCollections(self):
    checkpoint_path, expected_values = self._save_test_checkpoint()
    checkpoint_path = tf.train.latest_checkpoint(checkpoint_path)

    g = tf.Graph()
    with g.as_default():
      custom_getter = snt.custom_getters.restore_initializer(
          filename=checkpoint_path,
          collection="blah")

      with tf.variable_scope("", custom_getter=custom_getter):
        inputs = tf.placeholder(tf.float32, [10, 10])
        lin1 = snt.Linear(10, name="linear1")
        lin1(inputs)

        tf.add_to_collection("blah", lin1.w)

      init = tf.global_variables_initializer()

    with self.test_session(graph=g) as sess:
      sess.run(init)
      w_value = sess.run(lin1.w)

    self.assertFalse(np.allclose(expected_values["w"], w_value))
    # b is initialized to zero always.


</source>
<source file="systems/sonnet-1.34/sonnet/python/custom_getters/restore_initializer_test.py" startline="77" endline="103" pcid="184">
  def testNameFn(self):
    checkpoint_path, expected_values = self._save_test_checkpoint()
    checkpoint_path = tf.train.latest_checkpoint(checkpoint_path)

    def name_fn(name):
      return name.replace("linear2", "linear1")

    g = tf.Graph()
    with g.as_default():
      custom_getter = snt.custom_getters.restore_initializer(
          filename=checkpoint_path,
          name_fn=name_fn)

      with tf.variable_scope("", custom_getter=custom_getter):
        inputs = tf.placeholder(tf.float32, [10, 10])
        lin1 = snt.Linear(10, name="linear2")
        lin1(inputs)

      init = tf.global_variables_initializer()

    with self.test_session(graph=g) as sess:
      sess.run(init)
      w_value, b_value = sess.run([lin1.w, lin1.b])

    self.assertAllClose(expected_values["w"], w_value)
    self.assertAllClose(expected_values["b"], b_value)

</source>
</class>

<class classid="5" nclones="2" nlines="37" similarity="86">
<source file="systems/sonnet-1.34/sonnet/python/custom_getters/bayes_by_backprop_test.py" startline="332" endline="385" pcid="213">
  def testRecurrentNetSamplesWeightsOnce(self):
    """Test that sampling of the weights is done only once for a sequence.

    Test strategy: Provide an input sequence x whose value is the same at each
    time step. If the outputs from f_theta() are the same at each time step,
    this is evidence (but not proof) that theta is the same at each time step.
    """
    seq_length = 10
    batch_size = 1
    input_dim = 5
    output_dim = 5

    bbb_getter = bbb.bayes_by_backprop_getter(
        posterior_builder=bbb.diagonal_gaussian_posterior_builder,
        prior_builder=bbb.fixed_gaussian_prior_builder,
        kl_builder=bbb.stochastic_kl_builder,
        sampling_mode_tensor=tf.constant(bbb.EstimatorModes.sample))

    class NoStateLSTM(snt.LSTM):
      """An LSTM which ignores hidden state."""

      def _build(self, inputs, state):
        outputs, _ = super(NoStateLSTM, self)._build(inputs, state)
        return outputs, state

    with tf.variable_scope("model", custom_getter=bbb_getter):
      core = NoStateLSTM(output_dim)

    input_seq = tf.ones(shape=(seq_length, batch_size, input_dim))
    output_seq, _ = tf.nn.dynamic_rnn(
        core,
        inputs=input_seq,
        initial_state=core.initial_state(batch_size=batch_size),
        time_major=True)

    init_op = tf.global_variables_initializer()
    with self.test_session() as sess:
      sess.run(init_op)
      output_res_one = sess.run(output_seq)
      output_res_two = sess.run(output_seq)

    # Ensure that the sequence is the same at every time step, a necessary
    # but not sufficient condition for the weights to be the same.
    output_zero = output_res_one[0]
    for time_step_output in output_res_one[1:]:
      self.assertAllClose(output_zero, time_step_output)

    # Ensure that the noise is different in the second run by checking that
    # the output sequence is different now.
    for first_run_elem, second_run_elem in zip(output_res_one, output_res_two):
      distance = np.linalg.norm(
          first_run_elem.flatten() - second_run_elem.flatten())
      self.assertGreater(distance, 0.001)

</source>
<source file="systems/sonnet-1.34/sonnet/python/custom_getters/bayes_by_backprop_test.py" startline="431" endline="489" pcid="217">
  def testWeightsResampledWithKeepControlDeps(self):
    """Test that weights are resampled with `keep_control_dependencies=True`.

    Test strategy: We test the inverse of `testRecurrentNetSamplesWeightsOnce`.
    Provide an input sequence x whose value is the same at each time step. If
    the outputs from f_theta() are the different at each time step, then theta
    is different at each time step. In principle, it is possible that different
    thetas give the same outputs, but this is very unlikely.
    """
    seq_length = 10
    batch_size = 1
    input_dim = 5
    output_dim = 5

    bbb_getter = bbb.bayes_by_backprop_getter(
        posterior_builder=bbb.diagonal_gaussian_posterior_builder,
        prior_builder=bbb.fixed_gaussian_prior_builder,
        kl_builder=bbb.stochastic_kl_builder,
        sampling_mode_tensor=tf.constant(bbb.EstimatorModes.sample),
        keep_control_dependencies=True)

    class NoStateLSTM(snt.LSTM):
      """An LSTM which ignores hidden state."""

      def _build(self, inputs, state):
        outputs, _ = super(NoStateLSTM, self)._build(inputs, state)
        return outputs, state

    with tf.variable_scope("model", custom_getter=bbb_getter):
      core = NoStateLSTM(output_dim)

    input_seq = tf.ones(shape=(seq_length, batch_size, input_dim))
    output_seq, _ = tf.nn.dynamic_rnn(
        core,
        inputs=input_seq,
        initial_state=core.initial_state(batch_size=batch_size),
        time_major=True)

    init_op = tf.global_variables_initializer()
    with self.test_session() as sess:
      sess.run(init_op)
      output_res_one = sess.run(output_seq)
      output_res_two = sess.run(output_seq)

    # Ensure that the sequence is different at every time step
    output_zero = output_res_one[0]
    for time_step_output in output_res_one[1:]:
      distance = np.linalg.norm(
          time_step_output.flatten() - output_zero.flatten())
      self.assertGreater(distance, 0.001)

    # Ensure that the noise is different in the second run by checking that
    # the output sequence is different now.
    for first_run_elem, second_run_elem in zip(output_res_one, output_res_two):
      distance = np.linalg.norm(
          first_run_elem.flatten() - second_run_elem.flatten())
      self.assertGreater(distance, 0.001)


</source>
</class>

<class classid="6" nclones="2" nlines="10" similarity="100">
<source file="systems/sonnet-1.34/sonnet/python/custom_getters/override_args_test.py" startline="61" endline="77" pcid="222">
  def testNestedWithin(self, custom_getter_fn):
    # Create a module with an 'override args' custom getter, within the scope
    # of another custom getter.
    local_custom_getter = custom_getter_fn(
        collections=[tf.GraphKeys.LOCAL_VARIABLES])
    with tf.variable_scope("", custom_getter=_suffix_custom_getter):
      local_linear = snt.Linear(10, custom_getter=local_custom_getter)

    # Connect the module to the graph, creating its variables.
    inputs = tf.placeholder(dtype=tf.float32, shape=(7, 11))
    local_linear(inputs)

    # Both custom getters should be effective.
    self.assertIn(local_linear.w, tf.local_variables())
    self.assertNotIn(local_linear.w, tf.global_variables())
    self.assertEqual("linear/w_test", local_linear.w.op.name)

</source>
<source file="systems/sonnet-1.34/sonnet/python/custom_getters/override_args_test.py" startline="82" endline="98" pcid="223">
  def testWithNested(self, custom_getter_fn):
    # Create a module with a custom getter, within the scope of an
    # 'override args' custom getter.
    local_custom_getter = custom_getter_fn(
        collections=[tf.GraphKeys.LOCAL_VARIABLES])
    with tf.variable_scope("", custom_getter=local_custom_getter):
      local_linear = snt.Linear(10, custom_getter=_suffix_custom_getter)

    # Connect the module to the graph, creating its variables.
    inputs = tf.placeholder(dtype=tf.float32, shape=(7, 11))
    local_linear(inputs)

    # Both custom getters should be effective.
    self.assertIn(local_linear.w, tf.local_variables())
    self.assertNotIn(local_linear.w, tf.global_variables())
    self.assertEqual("linear/w_test", local_linear.w.op.name)

</source>
</class>

<class classid="7" nclones="3" nlines="19" similarity="78">
<source file="systems/sonnet-1.34/sonnet/python/modules/relational_memory_test.py" startline="138" endline="160" pcid="293">
  def testMemoryUpdating(self):
    """Checks if memory is updating correctly."""
    mem_slots = 2
    head_size = 32
    num_heads = 4
    batch_size = 5
    input_shape = (batch_size, 3, 3)
    mem = relational_memory.RelationalMemory(mem_slots, head_size, num_heads,
                                             gate_style=None)
    inputs = tf.placeholder(tf.float32, input_shape)

    memory_0 = mem.initial_state(batch_size)
    _, memory_1 = mem(inputs, memory_0)

    with self.test_session() as session:
      tf.global_variables_initializer().run()
      results = session.run(
          {"memory_1": memory_1, "memory_0": memory_0},
          feed_dict={inputs: np.zeros(input_shape)})

    self.assertTrue(np.any(np.not_equal(results["memory_0"],
                                        results["memory_1"])))

</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/relational_memory_test.py" startline="190" endline="216" pcid="295">
  def testDifferingKeyHeadSizes(self, gate_style):
    """Checks if arbitrary key sizes are still supported."""
    mem_slots = 2
    head_size = 32
    num_heads = 2
    key_size = 128
    batch_size = 5

    input_shape = (batch_size, 3, 3)
    mem = relational_memory.RelationalMemory(mem_slots, head_size, num_heads,
                                             gate_style=gate_style,
                                             key_size=key_size)
    self.assertNotEqual(key_size, mem._head_size)
    inputs = tf.placeholder(tf.float32, input_shape)

    memory_0 = mem.initial_state(batch_size)
    _, memory_1 = mem(inputs, memory_0)

    with self.test_session() as session:
      tf.global_variables_initializer().run()
      results = session.run(
          {"memory_1": memory_1, "memory_0": memory_0},
          feed_dict={inputs: np.ones(input_shape)})

    self.assertTrue(np.any(np.not_equal(results["memory_0"],
                                        results["memory_1"])))

</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/relational_memory_test.py" startline="164" endline="186" pcid="294">
  def testInputErasureWorking(self, gate_style):
    """Checks if gating is working by ignoring the input."""
    mem_slots = 2
    head_size = 32
    num_heads = 2
    batch_size = 5
    input_shape = (batch_size, 3, 3)
    mem = relational_memory.RelationalMemory(mem_slots, head_size, num_heads,
                                             forget_bias=float("+inf"),
                                             input_bias=float("-inf"),
                                             gate_style=gate_style)
    inputs = tf.placeholder(tf.float32, input_shape)

    memory_0 = mem.initial_state(batch_size)
    _, memory_1 = mem(inputs, memory_0)

    with self.test_session() as session:
      tf.global_variables_initializer().run()
      results = session.run(
          {"memory_1": memory_1, "memory_0": memory_0},
          feed_dict={inputs: np.ones(input_shape)})
    self.assertAllEqual(results["memory_0"], results["memory_1"])

</source>
</class>

<class classid="8" nclones="2" nlines="13" similarity="84">
<source file="systems/sonnet-1.34/sonnet/python/modules/optimization_constraints_test.py" startline="55" endline="68" pcid="338">
  def testRateDefaults(self, mocked_parametrized):
    mocked_parametrized.side_effect = (
        lambda x, rate: scale_gradient.scale_gradient(x, -rate))
    rate = 0.1
    cons = optimization_constraints.OptimizationConstraints(rate=rate)
    lhs = tf.zeros_like(1.0)
    rhs = tf.ones_like(1.0)
    x = cons.add(lhs < rhs)()
    v = tf.all_variables()[0]
    dxdl = tf.gradients(x, v)
    with tf.train.MonitoredSession() as sess:
      grads = sess.run(dxdl)
    self.assertAllClose(grads[0], rate)

</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/optimization_constraints_test.py" startline="70" endline="83" pcid="339">
  def testRateOverrides(self, mocked_parametrized):
    mocked_parametrized.side_effect = (
        lambda x, rate: scale_gradient.scale_gradient(x, -rate))
    rate = 7.3
    cons = optimization_constraints.OptimizationConstraints()
    lhs = tf.zeros_like(1.0)
    rhs = tf.ones_like(1.0)
    x = cons.add(lhs < rhs, rate=rate)()
    v = tf.all_variables()[0]
    dxdl = tf.gradients(x, v)
    with tf.train.MonitoredSession() as sess:
      grads = sess.run(dxdl)
    self.assertAllClose(grads[0], rate)

</source>
</class>

<class classid="9" nclones="4" nlines="15" similarity="73">
<source file="systems/sonnet-1.34/sonnet/python/modules/base_info_test.py" startline="167" endline="182" pcid="370">
  def testModuleInfo_tensor(self):
    # pylint: disable=not-callable
    tf.reset_default_graph()
    dumb = DumbModule(name="dumb_a")
    ph_0 = tf.placeholder(dtype=tf.float32, shape=(1, 10,))
    dumb(ph_0)
    def check():
      sonnet_collection = tf.get_default_graph().get_collection(
          base_info.SONNET_COLLECTION_NAME)
      connected_subgraph = sonnet_collection[0].connected_subgraphs[0]
      self.assertIsInstance(connected_subgraph.inputs["inputs"], tf.Tensor)
      self.assertIsInstance(connected_subgraph.outputs, tf.Tensor)
    check()
    _copy_default_graph()
    check()

</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/base_info_test.py" startline="203" endline="219" pcid="374">
  def testModuleInfo_tuple(self):
    # pylint: disable=not-callable
    tf.reset_default_graph()
    dumb = DumbModule(name="dumb_a")
    ph_0 = tf.placeholder(dtype=tf.float32, shape=(1, 10,))
    ph_1 = tf.placeholder(dtype=tf.float32, shape=(1, 10,))
    dumb((ph_0, ph_1))
    def check():
      sonnet_collection = tf.get_default_graph().get_collection(
          base_info.SONNET_COLLECTION_NAME)
      connected_subgraph = sonnet_collection[0].connected_subgraphs[0]
      self.assertIsInstance(connected_subgraph.inputs["inputs"], tuple)
      self.assertIsInstance(connected_subgraph.outputs, tuple)
    check()
    _copy_default_graph()
    check()

</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/base_info_test.py" startline="238" endline="254" pcid="378">
  def testModuleInfo_dict(self):
    # pylint: disable=not-callable
    tf.reset_default_graph()
    dumb = DumbModule(name="dumb_a")
    ph_0 = tf.placeholder(dtype=tf.float32, shape=(1, 10,))
    ph_1 = tf.placeholder(dtype=tf.float32, shape=(1, 10,))
    dumb({"ph_0": ph_0, "ph_1": ph_1})
    def check():
      sonnet_collection = tf.get_default_graph().get_collection(
          base_info.SONNET_COLLECTION_NAME)
      connected_subgraph = sonnet_collection[0].connected_subgraphs[0]
      self.assertIsInstance(connected_subgraph.inputs["inputs"], dict)
      self.assertIsInstance(connected_subgraph.outputs, dict)
    check()
    _copy_default_graph()
    check()

</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/base_info_test.py" startline="220" endline="237" pcid="376">
  def testModuleInfo_namedtuple(self):
    # pylint: disable=not-callable
    tf.reset_default_graph()
    dumb = DumbModule(name="dumb_a")
    ph_0 = tf.placeholder(dtype=tf.float32, shape=(1, 10,))
    ph_1 = tf.placeholder(dtype=tf.float32, shape=(1, 10,))
    dumb(DumbNamedTuple(ph_0, ph_1))
    def check():
      sonnet_collection = tf.get_default_graph().get_collection(
          base_info.SONNET_COLLECTION_NAME)
      connected_subgraph = sonnet_collection[0].connected_subgraphs[0]
      self.assertTrue(
          base_info._is_namedtuple(connected_subgraph.inputs["inputs"]))
      self.assertTrue(base_info._is_namedtuple(connected_subgraph.outputs))
    check()
    _copy_default_graph()
    check()

</source>
</class>

<class classid="10" nclones="2" nlines="29" similarity="96">
<source file="systems/sonnet-1.34/sonnet/python/modules/batch_norm_test.py" startline="33" endline="70" pcid="384">
  def testConstruct(self):
    inputs = tf.placeholder(tf.float32, shape=[None, 64, 64, 3])

    batch_norm1 = snt.BatchNorm(offset=False, scale=False)
    batch_norm1(inputs, is_training=True)

    err = "Batch normalization doesn't have an offset, so no beta"
    with self.assertRaisesRegexp(snt.Error, err):
      _ = batch_norm1.beta

    err = "Batch normalization doesn't have a scale, so no gamma"
    with self.assertRaisesRegexp(snt.Error, err):
      _ = batch_norm1.gamma

    batch_norm2 = snt.BatchNorm(offset=True, scale=False)
    batch_norm2(inputs, is_training=True)
    _ = batch_norm2.beta

    batch_norm3 = snt.BatchNorm(offset=False, scale=True)
    batch_norm3(inputs, is_training=True)
    _ = batch_norm3.gamma

    batch_norm4 = snt.BatchNorm(offset=True, scale=True)
    batch_norm4(inputs, is_training=True)
    _ = batch_norm4.beta
    _ = batch_norm4.gamma

    batch_norm4(inputs, is_training=True, test_local_stats=True)
    batch_norm4(inputs,
                is_training=tf.constant(True),
                test_local_stats=tf.constant(True))

    is_training_ph = tf.placeholder(tf.bool)
    test_local_stats_ph = tf.placeholder(tf.bool)
    batch_norm4(inputs,
                is_training=is_training_ph,
                test_local_stats=test_local_stats_ph)

</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/batch_norm_v2_test.py" startline="41" endline="78" pcid="425">
  def testConstruct(self):
    inputs = tf.placeholder(tf.float32, shape=[None, 64, 64, 3])

    batch_norm1 = snt.BatchNormV2(offset=False, scale=False, fused=False)
    batch_norm1(inputs, is_training=True)

    err = "Batch normalization doesn't have an offset, so no beta"
    with self.assertRaisesRegexp(snt.Error, err):
      _ = batch_norm1.beta

    err = "Batch normalization doesn't have a scale, so no gamma"
    with self.assertRaisesRegexp(snt.Error, err):
      _ = batch_norm1.gamma

    batch_norm2 = snt.BatchNormV2(offset=True, scale=False)
    batch_norm2(inputs, is_training=True)
    _ = batch_norm2.beta

    batch_norm3 = snt.BatchNormV2(offset=False, scale=True)
    batch_norm3(inputs, is_training=True)
    _ = batch_norm3.gamma

    batch_norm4 = snt.BatchNormV2(offset=True, scale=True)
    batch_norm4(inputs, is_training=True)
    _ = batch_norm4.beta
    _ = batch_norm4.gamma

    batch_norm4(inputs, is_training=True, test_local_stats=True)
    batch_norm4(inputs,
                is_training=tf.constant(True),
                test_local_stats=tf.constant(True))

    is_training_ph = tf.placeholder(tf.bool)
    test_local_stats_ph = tf.placeholder(tf.bool)
    batch_norm4(inputs,
                is_training=is_training_ph,
                test_local_stats=test_local_stats_ph)

</source>
</class>

<class classid="11" nclones="2" nlines="28" similarity="76">
<source file="systems/sonnet-1.34/sonnet/python/modules/batch_norm_test.py" startline="175" endline="236" pcid="390">
  def testCheckStatsDouble(self, dtype):
    """The correct statistics are being computed for double connection.

    Connected in parallel, it's ill-defined what order the updates will happen
    in. A double update could happen, or two sequential updates. E.g. If
    decay_rate is 0.9, the start value is 1.0, and the target value is 0.0, the
    value could progress as

      1.00 -> 0.90 -> 0.81,

    if the second update uses the fresh second value. Or as

      1.00 -> 0.90 -> 0.80

    if the second update uses the stale first value.

    We fix this here by running them in sequential run calls to ensure that this
    test is deterministic.

    The two situations are minimally different, especially if decay_rate is
    close to one (e.g. the default of 0.999).

    Args:
      dtype: TensorFlow datatype of input test batch.
    """

    v, _, inputs = self._get_inputs(dtype)
    bn = snt.BatchNorm(offset=False, scale=False, decay_rate=0.9)

    with tf.name_scope("net1"):
      bn(inputs, is_training=True)

    with tf.name_scope("net2"):
      bn(inputs, is_training=True)

    update_ops_1 = tuple(tf.get_collection(tf.GraphKeys.UPDATE_OPS, "net1"))
    self.assertEqual(len(update_ops_1), 2)
    update_ops_2 = tuple(tf.get_collection(tf.GraphKeys.UPDATE_OPS, "net2"))
    self.assertEqual(len(update_ops_2), 2)

    with self.test_session() as sess:
      sess.run(tf.global_variables_initializer())

      mm, mv = sess.run([bn.moving_mean, bn.moving_variance])

      self.assertAllClose(np.zeros([1, 6]), mm)
      self.assertAllClose(np.ones([1, 6]), mv)

      sess.run(update_ops_1)
      sess.run(update_ops_2)

      mm, mv = sess.run([bn.moving_mean,
                         bn.moving_variance])

      correct_mm = (1.0 - bn._decay_rate) * v
      correct_mm = (1.0 - bn._decay_rate) * v + bn._decay_rate * correct_mm
      correct_mv = np.ones([1, 6]) * bn._decay_rate**2

      atol = 1.e-2 if dtype == tf.float16 else 1.e-6
      self.assertAllClose(np.reshape(correct_mm, [1, 6]), mm, atol=atol)
      self.assertAllClose(np.reshape(correct_mv, [1, 6]), mv, atol=atol)

</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/batch_norm_v2_test.py" startline="187" endline="253" pcid="431">
  def testCheckStatsDouble(self, dtype):
    """The correct statistics are being computed for double connection.

    Connected in parallel, it's ill-defined what order the updates will happen
    in. A double update could happen, or two sequential updates. E.g. If
    decay_rate is 0.9, the start value is 1.0, and the target value is 0.0, the
    value could progress as

      1.00 -> 0.90 -> 0.81,

    if the second update uses the fresh second value. Or as

      1.00 -> 0.90 -> 0.80

    if the second update uses the stale first value.

    We fix this here by running them in sequential run calls to ensure that this
    test is deterministic.

    The two situations are minimally different, especially if decay_rate is
    close to one (e.g. the default of 0.999).

    Args:
      dtype: TensorFlow datatype of input test batch.
    """

    v, _, inputs = self._get_inputs(dtype)
    bn = snt.BatchNormV2(
        offset=False,
        scale=False,
        decay_rate=0.9,
        update_ops_collection=tf.GraphKeys.UPDATE_OPS)

    with tf.name_scope("net1"):
      bn(inputs, is_training=True)

    with tf.name_scope("net2"):
      bn(inputs, is_training=True)

    update_ops_1 = tuple(tf.get_collection(tf.GraphKeys.UPDATE_OPS, "net1"))
    self.assertLen(update_ops_1, 2)
    update_ops_2 = tuple(tf.get_collection(tf.GraphKeys.UPDATE_OPS, "net2"))
    self.assertLen(update_ops_2, 2)

    with self.test_session() as sess:
      sess.run(tf.global_variables_initializer())

      mm, mv = sess.run([bn.moving_mean, bn.moving_variance])

      self.assertAllClose(np.zeros([1, 6]), mm)
      self.assertAllClose(np.ones([1, 6]), mv)

      sess.run(update_ops_1)
      sess.run(update_ops_2)

      mm, mv = sess.run([bn.moving_mean,
                         bn.moving_variance])

      correct_mm = (1.0 - bn._decay_rate) * v
      correct_mm = (1.0 - bn._decay_rate) * v + bn._decay_rate * correct_mm
      correct_mv = np.ones([1, 6]) * bn._decay_rate**2

      atol = 1.e-2 if dtype == tf.float16 else 1.e-6

      self.assertAllClose(np.reshape(correct_mm, [1, 6]), mm, atol=atol)
      self.assertAllClose(np.reshape(correct_mv, [1, 6]), mv, atol=atol)

</source>
</class>

<class classid="12" nclones="2" nlines="26" similarity="75">
<source file="systems/sonnet-1.34/sonnet/python/modules/batch_norm_test.py" startline="237" endline="277" pcid="391">
  def testCheckStatsPython(self):
    """The correct normalization is being used for different Python flags."""

    v, input_v, inputs = self._get_inputs()

    bn = snt.BatchNorm(offset=False, scale=False, decay_rate=0.5)
    out1 = bn(inputs, is_training=True, test_local_stats=True)
    out2 = bn(inputs, is_training=False, test_local_stats=True)
    out3 = bn(inputs, is_training=False, test_local_stats=False)

    update_ops = tuple(tf.get_collection(tf.GraphKeys.UPDATE_OPS))
    self.assertEqual(len(update_ops), 2)

    with tf.control_dependencies(update_ops):
      out1 = tf.identity(out1)

    with self.test_session() as sess:
      sess.run(tf.global_variables_initializer())

      out_v = sess.run(out1)
      mm, mv = sess.run([bn.moving_mean, bn.moving_variance])

      # Single moving average steps should have happened.
      correct_mm = (1.0 - bn._decay_rate) * v
      correct_mv = np.ones([1, 6]) * bn._decay_rate

      self.assertAllClose(np.reshape(correct_mm, [1, 6]), mm)
      self.assertAllClose(np.reshape(correct_mv, [1, 6]), mv)
      self.assertAllClose(np.zeros([7, 6]), out_v, rtol=1e-6, atol=1e-5)

      out2_, out3_ = sess.run([out2, out3])

      # Out2: Tested using local batch stats.
      # Better numerical precision due to using shifted estimators.
      self.assertAllClose(np.zeros([7, 6]), out2_)

      # Out3: Tested using moving average stats.
      self.assertAllClose(
          (input_v - mm) / np.sqrt(mv + bn._eps),
          out3_)

</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/batch_norm_v2_test.py" startline="254" endline="299" pcid="432">
  def testCheckStatsPython(self):
    """The correct normalization is being used for different Python flags."""

    v, input_v, inputs = self._get_inputs()

    bn = snt.BatchNormV2(
        offset=False,
        scale=False,
        decay_rate=0.5,
        update_ops_collection=tf.GraphKeys.UPDATE_OPS
    )
    out1 = bn(inputs, is_training=True, test_local_stats=True)
    out2 = bn(inputs, is_training=False, test_local_stats=True)
    out3 = bn(inputs, is_training=False, test_local_stats=False)

    update_ops = tuple(tf.get_collection(tf.GraphKeys.UPDATE_OPS))
    self.assertLen(update_ops, 2)

    with tf.control_dependencies(update_ops):
      out1 = tf.identity(out1)

    with self.test_session() as sess:
      sess.run(tf.global_variables_initializer())

      out_v = sess.run(out1)
      mm, mv = sess.run([bn.moving_mean, bn.moving_variance])

      # Single moving average steps should have happened.
      correct_mm = (1.0 - bn._decay_rate) * v
      correct_mv = np.ones([1, 6]) * bn._decay_rate

      self.assertAllClose(np.reshape(correct_mm, [1, 6]), mm)
      self.assertAllClose(np.reshape(correct_mv, [1, 6]), mv)
      self.assertAllClose(np.zeros([7, 6]), out_v, rtol=1e-6, atol=1e-5)

      out2_, out3_ = sess.run([out2, out3])

      # Out2: Tested using local batch stats.
      # Better numerical precision due to using shifted estimators.
      self.assertAllClose(np.zeros([7, 6]), out2_, rtol=1e-6, atol=1e-5)

      # Out3: Tested using moving average stats.
      self.assertAllClose(
          (input_v - mm) / np.sqrt(mv + bn._eps),
          out3_)

</source>
</class>

<class classid="13" nclones="2" nlines="42" similarity="90">
<source file="systems/sonnet-1.34/sonnet/python/modules/batch_norm_test.py" startline="283" endline="353" pcid="392">
  def testCheckStatsInGraph(self, update_ops_collection):
    """The correct normalization is being used for different TF flags."""

    v, input_v, inputs = self._get_inputs()

    bn = snt.BatchNorm(offset=False,
                       scale=False,
                       decay_rate=0.5,
                       update_ops_collection=update_ops_collection)

    is_training = tf.placeholder(tf.bool)
    test_local_stats = tf.placeholder(tf.bool)

    out = bn(inputs,
             is_training=is_training,
             test_local_stats=test_local_stats)

    if update_ops_collection is not None:
      update_ops = tuple(tf.get_collection(update_ops_collection))
      self.assertEqual(len(update_ops), 2)

      with tf.control_dependencies(update_ops):
        out = tf.identity(out)

    with self.test_session() as sess:
      sess.run(tf.global_variables_initializer())

      # Run with `is_training=True`, `test_local_stats=True`.
      out_v = sess.run(out, feed_dict={is_training: True,
                                       test_local_stats: True})

      # Moving averages not updated until after calculation so shifted
      # stats are poor.
      self.assertAllClose(np.zeros([7, 6]), out_v, rtol=1e-6, atol=1e-5)

      ops = (bn.moving_mean, bn.moving_variance)
      mm1, mv1 = sess.run(ops)

      # Single moving average step should have happened.
      correct_mm = (1.0 - bn._decay_rate) * v
      correct_mv = np.ones([1, 6]) * bn._decay_rate

      self.assertAllClose(np.reshape(correct_mm, [1, 6]), mm1)
      self.assertAllClose(np.reshape(correct_mv, [1, 6]), mv1)

      # Run with `is_training=False`, `test_local_stats=True`.
      # Should have used local batch stats.
      out_v = sess.run(out, feed_dict={is_training: False,
                                       test_local_stats: True})

      # Moving averages should not have changed.
      mm2, mv2 = sess.run(ops)
      self.assertAllClose(mm1, mm2)
      self.assertAllClose(mv1, mv2)

      self.assertAllClose(np.zeros([7, 6]), out_v)

      # Run with `is_training=False`, `test_local_stats=False`.
      # Should have used moving average stats.
      out_v = sess.run(out, feed_dict={is_training: False,
                                       test_local_stats: False})

      # Moving averages should not have changed.
      mm3, mv3 = sess.run(ops)
      self.assertAllClose(mm1, mm3)
      self.assertAllClose(mv1, mv3)

      self.assertAllClose(
          (input_v - mm3) / np.sqrt(mv3 + bn._eps),
          out_v)

</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/batch_norm_v2_test.py" startline="305" endline="376" pcid="433">
  def testCheckStatsInGraph(self, update_ops_collection):
    """The correct normalization is being used for different TF flags."""

    v, input_v, inputs = self._get_inputs()

    bn = snt.BatchNormV2(
        offset=False,
        scale=False,
        decay_rate=0.5,
        update_ops_collection=update_ops_collection)

    is_training = tf.placeholder(tf.bool)
    test_local_stats = tf.placeholder(tf.bool)

    out = bn(inputs,
             is_training=is_training,
             test_local_stats=test_local_stats)

    if update_ops_collection is not None:
      update_ops = tuple(tf.get_collection(update_ops_collection))
      self.assertLen(update_ops, 2)

      with tf.control_dependencies(update_ops):
        out = tf.identity(out)

    with self.test_session() as sess:
      sess.run(tf.global_variables_initializer())

      # Run with `is_training=True`, `test_local_stats=True`.
      out_v = sess.run(out, feed_dict={is_training: True,
                                       test_local_stats: True})

      # Moving averages not updated until after calculation so shifted
      # stats are poor.
      self.assertAllClose(np.zeros([7, 6]), out_v, rtol=1e-6, atol=1e-5)

      ops = (bn.moving_mean, bn.moving_variance)
      mm1, mv1 = sess.run(ops)

      # Single moving average step should have happened.
      correct_mm = (1.0 - bn._decay_rate) * v
      correct_mv = np.ones([1, 6]) * bn._decay_rate

      self.assertAllClose(np.reshape(correct_mm, [1, 6]), mm1)
      self.assertAllClose(np.reshape(correct_mv, [1, 6]), mv1)

      # Run with `is_training=False`, `test_local_stats=True`.
      # Should have used local batch stats.
      out_v = sess.run(out, feed_dict={is_training: False,
                                       test_local_stats: True})

      # Moving averages should not have changed.
      mm2, mv2 = sess.run(ops)
      self.assertAllClose(mm1, mm2)
      self.assertAllClose(mv1, mv2)

      self.assertAllClose(np.zeros([7, 6]), out_v, rtol=1e-6, atol=1e-5)

      # Run with `is_training=False`, `test_local_stats=False`.
      # Should have used moving average stats.
      out_v = sess.run(out, feed_dict={is_training: False,
                                       test_local_stats: False})

      # Moving averages should not have changed.
      mm3, mv3 = sess.run(ops)
      self.assertAllClose(mm1, mm3)
      self.assertAllClose(mv1, mv3)

      self.assertAllClose(
          (input_v - mm3) / np.sqrt(mv3 + bn._eps),
          out_v)

</source>
</class>

<class classid="14" nclones="2" nlines="21" similarity="78">
<source file="systems/sonnet-1.34/sonnet/python/modules/batch_norm_test.py" startline="371" endline="401" pcid="394">
  def testUpdatesInsideCond(self):
    """Demonstrate that updates inside a cond fail.

    """

    _, input_v, inputs = self._get_inputs()
    bn = snt.BatchNorm(offset=False, scale=False, decay_rate=0.5)
    condition = tf.placeholder(tf.bool)
    cond = tf.cond(condition,
                   lambda: bn(inputs, is_training=True),
                   lambda: inputs)

    init = tf.global_variables_initializer()

    with self.test_session() as sess:
      sess.run(init)
      out_v = sess.run(cond, feed_dict={condition: False})
      self.assertAllClose(input_v, out_v)

      out_v = sess.run(cond, feed_dict={condition: True})
      self.assertAllClose(np.zeros([7, 6]), out_v, rtol=1e-4, atol=1e-4)

      # Variables are accessible outside the tf.cond()
      mm, mv = sess.run([bn.moving_mean, bn.moving_variance])
      self.assertAllClose(np.zeros([1, 6]), mm)
      self.assertAllClose(np.ones([1, 6]), mv)

      # Tensors are not accessible outside the tf.cond()
      with self.assertRaisesRegexp(ValueError, "Operation"):
        sess.run(tuple(tf.get_collection(tf.GraphKeys.UPDATE_OPS)))

</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/batch_norm_v2_test.py" startline="397" endline="429" pcid="435">
  def testUpdatesInsideCond(self):
    """Demonstrate that updates inside a cond fail."""

    _, input_v, inputs = self._get_inputs()
    bn = snt.BatchNormV2(
        offset=False,
        scale=False,
        decay_rate=0.5,
        update_ops_collection=tf.GraphKeys.UPDATE_OPS)
    condition = tf.placeholder(tf.bool)
    cond = tf.cond(condition,
                   lambda: bn(inputs, is_training=True),
                   lambda: inputs)

    init = tf.global_variables_initializer()

    with self.test_session() as sess:
      sess.run(init)
      out_v = sess.run(cond, feed_dict={condition: False})
      self.assertAllClose(input_v, out_v)

      out_v = sess.run(cond, feed_dict={condition: True})
      self.assertAllClose(np.zeros([7, 6]), out_v, rtol=1e-4, atol=1e-4)

      # Variables are accessible outside the tf.cond()
      mm, mv = sess.run([bn.moving_mean, bn.moving_variance])
      self.assertAllClose(np.zeros([1, 6]), mm)
      self.assertAllClose(np.ones([1, 6]), mv)

      # Tensors are not accessible outside the tf.cond()
      with self.assertRaisesRegexp(ValueError, "Operation"):
        sess.run(tuple(tf.get_collection(tf.GraphKeys.UPDATE_OPS)))

</source>
</class>

<class classid="15" nclones="2" nlines="17" similarity="88">
<source file="systems/sonnet-1.34/sonnet/python/modules/batch_norm_test.py" startline="402" endline="426" pcid="395">
  def testVariableBatchSize(self):
    """Check the inputs batch_size can change."""

    inputs_shape = [10, 10]
    inputs = tf.placeholder(tf.float32, shape=[None] + inputs_shape)
    bn = snt.BatchNorm(offset=False, scale=False)

    # Outputs should be equal to inputs.
    out = bn(inputs,
             is_training=False,
             test_local_stats=False)

    init = tf.global_variables_initializer()
    update_ops = tuple(tf.get_collection(tf.GraphKeys.UPDATE_OPS))

    with self.test_session() as sess:
      sess.run(init)

      for batch_size in [1, 3, 10]:
        input_data = np.random.rand(batch_size, *inputs_shape)
        out_v = sess.run(out, feed_dict={inputs: input_data})
        self.assertAllClose(input_data / np.sqrt(1.0 + bn._eps), out_v)

        sess.run(update_ops, feed_dict={inputs: input_data})

</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/batch_norm_v2_test.py" startline="430" endline="455" pcid="436">
  def testVariableBatchSize(self):
    """Check the inputs batch_size can change."""

    inputs_shape = [10, 10]
    inputs = tf.placeholder(tf.float32, shape=[None] + inputs_shape)
    bn = snt.BatchNormV2(
        offset=False, scale=False)

    # Outputs should be equal to inputs.
    out = bn(inputs,
             is_training=False,
             test_local_stats=False)

    init = tf.global_variables_initializer()
    update_ops = tuple(tf.get_collection(tf.GraphKeys.UPDATE_OPS))

    with self.test_session() as sess:
      sess.run(init)

      for batch_size in [1, 3, 10]:
        input_data = np.random.rand(batch_size, *inputs_shape)
        out_v = sess.run(out, feed_dict={inputs: input_data})
        self.assertAllClose(input_data / np.sqrt(1.0 + bn._eps), out_v)

        sess.run(update_ops, feed_dict={inputs: input_data})

</source>
</class>

<class classid="16" nclones="2" nlines="26" similarity="84">
<source file="systems/sonnet-1.34/sonnet/python/modules/batch_norm_test.py" startline="460" endline="489" pcid="399">
  def testInitializers(self, offset, scale):
    initializers = {
        "moving_mean": tf.constant_initializer(2.0),
        "moving_variance": tf.constant_initializer(3.0),
    }

    if scale:
      initializers["gamma"] = tf.constant_initializer(4.0)
    if offset:
      initializers["beta"] = tf.constant_initializer(5.0)

    inputs_shape = [10, 10]
    inputs = tf.placeholder(tf.float32, shape=[None] + inputs_shape)
    bn = snt.BatchNorm(offset=offset, scale=scale, initializers=initializers)
    self.assertEqual(bn.initializers, initializers)
    bn(inputs, is_training=True)

    init = tf.global_variables_initializer()
    with self.test_session() as sess:
      sess.run(init)

      ones_v = np.ones([1, 1, inputs_shape[-1]])
      self.assertAllClose(bn.moving_mean.eval(), ones_v * 2.0)
      self.assertAllClose(bn.moving_variance.eval(), ones_v * 3.0)

      if scale:
        self.assertAllClose(bn.gamma.eval(), ones_v * 4.0)
      if offset:
        self.assertAllClose(bn.beta.eval(), ones_v * 5.0)

</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/batch_norm_v2_test.py" startline="489" endline="521" pcid="440">
  def testInitializers(self, offset, scale):
    initializers = {
        "moving_mean": tf.constant_initializer(2.0),
        "moving_variance": tf.constant_initializer(3.0),
    }

    if scale:
      initializers["gamma"] = tf.constant_initializer(4.0)
    if offset:
      initializers["beta"] = tf.constant_initializer(5.0)

    inputs_shape = [10, 10]
    inputs = tf.placeholder(tf.float32, shape=[None] + inputs_shape)
    bn = snt.BatchNormV2(
        offset=offset,
        scale=scale,
        initializers=initializers)
    self.assertEqual(bn.initializers, initializers)
    bn(inputs, is_training=True)

    init = tf.global_variables_initializer()
    with self.test_session() as sess:
      sess.run(init)

      ones_v = np.ones([1, 1, inputs_shape[-1]])
      self.assertAllClose(bn.moving_mean.eval(), ones_v * 2.0)
      self.assertAllClose(bn.moving_variance.eval(), ones_v * 3.0)

      if scale:
        self.assertAllClose(bn.gamma.eval(), ones_v * 4.0)
      if offset:
        self.assertAllClose(bn.beta.eval(), ones_v * 5.0)

</source>
</class>

<class classid="17" nclones="2" nlines="23" similarity="83">
<source file="systems/sonnet-1.34/sonnet/python/modules/batch_norm_test.py" startline="496" endline="519" pcid="400">
  def testRegularizersInRegularizationLosses(self, offset, scale):
    regularizers = {}
    if offset:
      regularizers["beta"] = tf.contrib.layers.l1_regularizer(scale=0.5)
    if scale:
      regularizers["gamma"] = tf.contrib.layers.l2_regularizer(scale=0.5)

    inputs_shape = [10, 10]
    inputs = tf.placeholder(tf.float32, shape=[None] + inputs_shape)
    bn = snt.BatchNorm(offset=offset, scale=scale, regularizers=regularizers)
    self.assertEqual(bn.regularizers, regularizers)
    bn(inputs, is_training=True)

    graph_regularizers = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)
    if not offset and not scale:
      self.assertFalse(graph_regularizers)
    if offset and not scale:
      self.assertRegexpMatches(graph_regularizers[0].name, ".*l1_regularizer.*")
    if scale and not offset:
      self.assertRegexpMatches(graph_regularizers[0].name, ".*l2_regularizer.*")
    if scale and offset:
      self.assertRegexpMatches(graph_regularizers[0].name, ".*l1_regularizer.*")
      self.assertRegexpMatches(graph_regularizers[1].name, ".*l2_regularizer.*")

</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/batch_norm_v2_test.py" startline="528" endline="554" pcid="441">
  def testRegularizersInRegularizationLosses(self, offset, scale):
    regularizers = {}
    if offset:
      regularizers["beta"] = tf.contrib.layers.l1_regularizer(scale=0.5)
    if scale:
      regularizers["gamma"] = tf.contrib.layers.l2_regularizer(scale=0.5)

    inputs_shape = [10, 10]
    inputs = tf.placeholder(tf.float32, shape=[None] + inputs_shape)
    bn = snt.BatchNormV2(
        offset=offset,
        scale=scale,
        regularizers=regularizers)
    self.assertEqual(bn.regularizers, regularizers)
    bn(inputs, is_training=True)

    graph_regularizers = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)
    if not offset and not scale:
      self.assertFalse(graph_regularizers)
    if offset and not scale:
      self.assertRegexpMatches(graph_regularizers[0].name, ".*l1_regularizer.*")
    if scale and not offset:
      self.assertRegexpMatches(graph_regularizers[0].name, ".*l2_regularizer.*")
    if scale and offset:
      self.assertRegexpMatches(graph_regularizers[0].name, ".*l1_regularizer.*")
      self.assertRegexpMatches(graph_regularizers[1].name, ".*l2_regularizer.*")

</source>
</class>

<class classid="18" nclones="2" nlines="31" similarity="79">
<source file="systems/sonnet-1.34/sonnet/python/modules/batch_norm_test.py" startline="558" endline="601" pcid="402">
  def testFusedBatchNorm(self, is_training, test_local_stats, scale,
                         is_training_python_bool):
    input_shape = (32, 9, 9, 8)
    iterations = 5
    x = tf.placeholder(tf.float32, shape=input_shape)
    bn1 = snt.BatchNorm(scale=scale, update_ops_collection=None)

    with self.assertRaises(NotImplementedError):
      # Input does not have 4 dimensions but fused is True.
      xlinear = tf.placeholder(tf.float32, shape=(2, 3))
      snt.BatchNorm(fused=True, scale=scale)(xlinear, is_training=True)

    with self.assertRaises(ValueError):
      # The axis is incorrect
      snt.BatchNorm(axis=(1, 2, 3), fused=True, scale=scale)(
          x, is_training=True)

    bn2 = snt.BatchNorm(scale=scale, fused=True, update_ops_collection=None)

    xx = np.random.random(input_shape)
    feed_dict = {x: xx}
    if not is_training_python_bool:
      is_training_node = tf.placeholder(tf.bool, shape=())
      feed_dict.update({is_training_node: is_training})
      is_training = is_training_node
      test_local_stats_node = tf.placeholder(tf.bool, shape=())
      feed_dict.update({test_local_stats_node: test_local_stats})
      test_local_stats = test_local_stats_node

    o1 = bn1(x, is_training=is_training, test_local_stats=test_local_stats)
    o2 = bn2(x, is_training=is_training, test_local_stats=test_local_stats)

    with self.test_session() as sess:
      sess.run(tf.global_variables_initializer())
      params = [
          o1, o2, bn1._moving_mean, bn1._moving_variance, bn2._moving_mean,
          bn2._moving_variance
      ]
      for _ in range(iterations):
        y1, y2, mean1, var1, mean2, var2 = sess.run(params, feed_dict=feed_dict)
        self.assertAllClose(y1, y2, atol=1e-4)
        self.assertAllClose(mean1, mean2, atol=1e-4)
        self.assertAllClose(var1, var2, atol=1e-4)

</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/batch_norm_v2_test.py" startline="596" endline="628" pcid="443">
  def testFusedBatchNormV2(self, is_training, test_local_stats, scale,
                           is_training_python_bool):
    input_shape = (32, 9, 9, 8)
    iterations = 5
    x = tf.placeholder(tf.float32, shape=input_shape)
    bn1 = snt.BatchNormV2(scale=scale)
    bn2 = snt.BatchNormV2(fused=False, scale=scale)

    xx = np.random.random(input_shape)
    feed_dict = {x: xx}
    if not is_training_python_bool:
      is_training_node = tf.placeholder(tf.bool, shape=())
      feed_dict.update({is_training_node: is_training})
      is_training = is_training_node
      test_local_stats_node = tf.placeholder(tf.bool, shape=())
      feed_dict.update({test_local_stats_node: test_local_stats})
      test_local_stats = test_local_stats_node

    o1 = bn1(x, is_training=is_training, test_local_stats=test_local_stats)
    o2 = bn2(x, is_training=is_training, test_local_stats=test_local_stats)

    with self.test_session() as sess:
      sess.run(tf.global_variables_initializer())
      params = [
          o1, o2, bn1._moving_mean, bn1._moving_variance, bn2._moving_mean,
          bn2._moving_variance
      ]
      for _ in range(iterations):
        y1, y2, mean1, var1, mean2, var2 = sess.run(params, feed_dict=feed_dict)
        self.assertAllClose(y1, y2, atol=1e-4)
        self.assertAllClose(mean1, mean2, atol=1e-4)
        self.assertAllClose(var1, var2, atol=1e-4)

</source>
</class>

<class classid="19" nclones="2" nlines="19" similarity="94">
<source file="systems/sonnet-1.34/sonnet/python/modules/batch_norm_test.py" startline="606" endline="630" pcid="403">
  def testFusedBatchNormFloat16(self, is_training, test_local_stats):
    input_shape = (31, 7, 7, 5)
    iterations = 3
    x = tf.placeholder(tf.float16, shape=input_shape)
    bn1 = snt.BatchNorm(update_ops_collection=None)
    bn2 = snt.BatchNorm(fused=True, update_ops_collection=None)

    feed_dict = {x: np.random.random(input_shape)}

    o1 = bn1(x, is_training=is_training, test_local_stats=test_local_stats)
    o2 = bn2(x, is_training=is_training, test_local_stats=test_local_stats)

    with self.test_session() as sess:
      sess.run(tf.global_variables_initializer())
      params = [
          o1, o2, bn1._moving_mean, bn1._moving_variance, bn2._moving_mean,
          bn2._moving_variance
      ]
      for _ in range(iterations):
        y1, y2, mean1, var1, mean2, var2 = sess.run(params, feed_dict=feed_dict)
        self.assertAllClose(y1, y2, atol=1e-2)
        self.assertAllClose(mean1, mean2, atol=1e-2)
        self.assertAllClose(var1, var2, atol=1e-2)


</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/batch_norm_v2_test.py" startline="633" endline="656" pcid="444">
  def testFusedBatchNormFloat16(self, is_training, test_local_stats):
    input_shape = (31, 7, 7, 5)
    iterations = 3
    x = tf.placeholder(tf.float16, shape=input_shape)
    bn1 = snt.BatchNormV2(fused=False)
    bn2 = snt.BatchNormV2()

    feed_dict = {x: np.random.random(input_shape)}

    o1 = bn1(x, is_training=is_training, test_local_stats=test_local_stats)
    o2 = bn2(x, is_training=is_training, test_local_stats=test_local_stats)

    with self.test_session() as sess:
      sess.run(tf.global_variables_initializer())
      params = [
          o1, o2, bn1._moving_mean, bn1._moving_variance, bn2._moving_mean,
          bn2._moving_variance
      ]
      for _ in range(iterations):
        y1, y2, mean1, var1, mean2, var2 = sess.run(params, feed_dict=feed_dict)
        self.assertAllClose(y1, y2, atol=1e-2)
        self.assertAllClose(mean1, mean2, atol=1e-2)
        self.assertAllClose(var1, var2, atol=1e-2)

</source>
</class>

<class classid="20" nclones="2" nlines="11" similarity="90">
<source file="systems/sonnet-1.34/sonnet/python/modules/clip_gradient.py" startline="62" endline="94" pcid="423">
def clip_gradient(net, clip_value_min, clip_value_max, name=None):
  """Clips respective gradients of a given tensor.

  Acts as identity for the forward pass, but clips gradient tensor element-wise
  by value during the backward pass. Any gradient values less than
  `clip_value_min` or greater than `clip_values_max` are set to the respective
  limit values.

  Args:
    net: A `tf.Tensor`.
    clip_value_min: A 0-D Tensor or scalar. The minimum value to clip by.
    clip_value_max: A 0-D Tensor or scalar. The maximum value to clip by.
    name: A name for the operation (optional, default 'clip_gradient').

  Returns:
    A `tf.Tensor` with the same type as the input tensor.

  Raises:
    ValueError: If `net` dtype is non-float.
  """
  if not net.dtype.is_floating:
    raise ValueError("clip_gradient does not support non-float `net` inputs.")

  with tf.name_scope(name, "clip_gradient", values=[net]):
    dtype = net.dtype.base_dtype  # Convert ref dtypes to regular dtypes.
    min_tensor = tf.convert_to_tensor(clip_value_min, dtype=dtype)
    max_tensor = tf.convert_to_tensor(clip_value_max, dtype=dtype)

    clip_gradient_op = _clip_gradient_op(dtype)
    output = clip_gradient_op(net, min_tensor, max_tensor)
    output.set_shape(net.get_shape())

  return output
</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/optimization_constraints.py" startline="365" endline="381" pcid="824">
def _constrain_to_range(x, min_value, max_value, name='constrain_to_range'):
  """Clips the inputs to a given range, whilst forwarding gradients."""
  if not x.dtype.is_floating:
    raise ValueError('_clip_by_value does not support non-float `x` inputs.')

  with tf.name_scope(name, 'constrain_to_range', values=[x]):
    dtype = x.dtype.base_dtype  # Convert ref dtypes to regular dtypes.
    min_tensor = tf.convert_to_tensor(min_value, dtype=dtype)
    max_tensor = tf.convert_to_tensor(max_value, dtype=dtype)

    constrain_to_range_op = _get_constrain_to_range_op(dtype)
    output = constrain_to_range_op(x, min_tensor, max_tensor)
    output.set_shape(x.get_shape())

  return output


</source>
</class>

<class classid="21" nclones="2" nlines="11" similarity="72">
<source file="systems/sonnet-1.34/sonnet/python/modules/basic_test.py" startline="471" endline="484" pcid="523">
  def testPartitioners(self):
    if tf.executing_eagerly():
      self.skipTest("Partitioned variables are not supported in eager mode.")
    inputs = tf.zeros([1, 100])
    partitioners = {
        "w": tf.variable_axis_size_partitioner(10000),
        "b": tf.variable_axis_size_partitioner(100),
    }
    linear = snt.Linear(100, partitioners=partitioners)
    linear(inputs)

    self.assertEqual(type(linear.w), variables.PartitionedVariable)
    self.assertEqual(type(linear.b), variables.PartitionedVariable)

</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/basic_test.py" startline="730" endline="743" pcid="538">
  def testPartitioners(self):
    if tf.executing_eagerly():
      self.skipTest("Partitioned variables are not supported in eager mode.")
    inputs = tf.zeros([1, 100])
    partitioners = {
        "b": tf.variable_axis_size_partitioner(10000),
    }
    bias = snt.AddBias(partitioners=partitioners)
    bias(inputs)

    self.assertEqual(type(bias.b), variables.PartitionedVariable)


# @tf.contrib.eager.run_all_tests_in_graph_and_eager_modes
</source>
</class>

<class classid="22" nclones="3" nlines="10" similarity="70">
<source file="systems/sonnet-1.34/sonnet/python/modules/basic_test.py" startline="671" endline="683" pcid="534">
  def testInvalidInitializationParameters(self, bias_dims, unused_bias_shape):
    err = "Invalid initializer keys.*"
    with self.assertRaisesRegexp(KeyError, err):
      snt.AddBias(
          bias_dims=bias_dims,
          initializers={"not_b": tf.truncated_normal_initializer(stddev=1.0)})

    err = "Initializer for 'b' is not a callable function"
    with self.assertRaisesRegexp(TypeError, err):
      snt.AddBias(
          bias_dims=bias_dims,
          initializers={"b": tf.zeros([1, 2, 3])})

</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/basic_test.py" startline="685" endline="696" pcid="535">
  def testInvalidPartitionerParameters(self, bias_dims, unused_bias_shape):
    with self.assertRaisesRegexp(KeyError, "Invalid partitioner keys.*"):
      snt.AddBias(
          bias_dims=bias_dims,
          partitioners={"not_b": tf.fixed_size_partitioner(num_shards=2)})

    err = "Partitioner for 'b' is not a callable function"
    with self.assertRaisesRegexp(TypeError, err):
      snt.AddBias(
          bias_dims=bias_dims,
          partitioners={"b": tf.zeros([1, 2, 3])})

</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/basic_test.py" startline="824" endline="835" pcid="544">
  def testInvalidPartitionerParameters(self):
    with self.assertRaisesRegexp(KeyError, "Invalid partitioner keys.*"):
      snt.TrainableVariable(
          shape=[1],
          partitioners={"not_w": tf.fixed_size_partitioner(num_shards=2)})

    err = "Partitioner for 'w' is not a callable function"
    with self.assertRaisesRegexp(TypeError, err):
      snt.TrainableVariable(
          shape=[1],
          partitioners={"w": tf.zeros([1, 2, 3])})

</source>
</class>

<class classid="23" nclones="2" nlines="10" similarity="70">
<source file="systems/sonnet-1.34/sonnet/python/modules/basic_test.py" startline="1583" endline="1595" pcid="603">
  def testComputation(self):
    inputs = tf.constant(dtype=tf.int32, value=[[1, 2, 3], [1, 2, 3]])

    dims = [0, 1]
    begin = [0, 1]
    size = [1, 2]
    mod = snt.SliceByDim(dims=dims, begin=begin, size=size)
    output = mod(inputs)

    actual = self.evaluate(output)
    expected = [[2, 3]]
    self.assertAllEqual(actual, expected)

</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/basic_test.py" startline="1596" endline="1608" pcid="604">
  def testNegativeDim(self):
    inputs = tf.constant(dtype=tf.int32, value=[[1, 2, 3], [4, 5, 6]])

    dims = [0, -1]
    begin = [0, 1]
    size = [-1, 2]
    mod = snt.SliceByDim(dims=dims, begin=begin, size=size)
    output = mod(inputs)

    actual = self.evaluate(output)
    expected = [[2, 3], [5, 6]]
    self.assertAllEqual(actual, expected)

</source>
</class>

<class classid="24" nclones="2" nlines="11" similarity="72">
<source file="systems/sonnet-1.34/sonnet/python/modules/nets/convnet_test.py" startline="220" endline="233" pcid="655">
  def testBatchNormBuildFlag(self, module):
    model = module(output_channels=self.output_channels,
                   kernel_shapes=self.kernel_shapes,
                   strides=self.strides,
                   paddings=self.paddings,
                   use_batch_norm=True)
    self.assertTrue(model.use_batch_norm)
    input_to_net = tf.random_normal(dtype=tf.float32, shape=(1, 100, 100, 3))

    # Check that an error is raised if we don't specify the is_training flag
    err = "is_training flag must be explicitly specified"
    with self.assertRaisesRegexp(ValueError, err):
      model(input_to_net)

</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/nets/convnet_test.py" startline="298" endline="311" pcid="658">
  def testNoBias(self, module):
    model = module(output_channels=self.output_channels,
                   kernel_shapes=self.kernel_shapes,
                   strides=self.strides,
                   paddings=self.paddings,
                   use_bias=False)
    self.assertEqual(model.use_bias, (False,) * len(self.output_channels))
    input_to_net = tf.random_normal(dtype=tf.float32, shape=(1, 100, 100, 3))
    model(input_to_net)

    model_variables = model.get_variables()

    self.assertLen(model_variables, len(self.output_channels))

</source>
</class>

<class classid="25" nclones="2" nlines="23" similarity="82">
<source file="systems/sonnet-1.34/sonnet/python/modules/nets/convnet_test.py" startline="668" endline="694" pcid="670">
  def testCustomGetterTranspose(self):
    """Tests passing a custom getter to the transpose method."""
    conv2d = snt.nets.ConvNet2D(output_channels=self.output_channels,
                                kernel_shapes=self.kernel_shapes,
                                strides=self.strides,
                                paddings=self.paddings)
    input_shape = [10, 100, 100, 3]
    output_of_conv2d = conv2d(tf.zeros(dtype=tf.float32, shape=input_shape))
    # We'll be able to check if the custom_getter was used by checking for
    # gradients.
    conv2d_transpose = conv2d.transpose(
        custom_getter=snt.custom_getters.stop_gradient)
    if tf.executing_eagerly():
      with tf.GradientTape() as tape:
        output_of_transpose = conv2d_transpose(output_of_conv2d)
      conv2d_transpose_vars = conv2d_transpose.get_variables()
      self.assertTrue(len(conv2d_transpose_vars))
      for tensor in tape.gradient(output_of_transpose, conv2d_transpose_vars):
        self.assertIsNone(tensor)

    else:
      output_of_transpose = conv2d_transpose(output_of_conv2d)
      conv2d_transpose_vars = conv2d_transpose.get_variables()
      self.assertTrue(len(conv2d_transpose_vars))
      for tensor in tf.gradients(output_of_transpose, conv2d_transpose_vars):
        self.assertIsNone(tensor)

</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/nets/convnet_test.py" startline="1058" endline="1085" pcid="683">
  def testCustomGetterTranspose(self):
    """Tests passing a custom getter to the transpose method."""
    conv2d_t = snt.nets.ConvNet2DTranspose(
        output_shapes=self.output_shapes,
        output_channels=self.output_channels,
        kernel_shapes=self.kernel_shapes,
        strides=self.strides,
        paddings=self.paddings)
    input_shape = [10, 100, 100, 3]
    output_of_conv2d_t = conv2d_t(tf.zeros(dtype=tf.float32, shape=input_shape))
    # We'll be able to check if the custom_getter was used by checking for
    # gradients.
    conv2d = conv2d_t.transpose(custom_getter=snt.custom_getters.stop_gradient)
    if tf.executing_eagerly():
      with tf.GradientTape() as tape:
        output_of_conv = conv2d(output_of_conv2d_t)
      conv2d_vars = conv2d.get_variables()
      self.assertTrue(len(conv2d_vars))
      for tensor in tape.gradient(output_of_conv, conv2d_vars):
        self.assertIsNone(tensor)

    else:
      output_of_conv = conv2d(output_of_conv2d_t)
      conv2d_vars = conv2d.get_variables()
      self.assertTrue(len(conv2d_vars))
      for tensor in tf.gradients(output_of_conv, conv2d_vars):
        self.assertIsNone(tensor)

</source>
</class>

<class classid="26" nclones="4" nlines="32" similarity="70">
<source file="systems/sonnet-1.34/sonnet/python/modules/nets/convnet_test.py" startline="695" endline="736" pcid="671">
  def testNoCustomGetterTranspose(self):
    """Tests not passing a custom getter to the transpose method."""
    conv2d = snt.nets.ConvNet2D(output_channels=self.output_channels,
                                kernel_shapes=self.kernel_shapes,
                                strides=self.strides,
                                paddings=self.paddings,
                                custom_getter=snt.custom_getters.stop_gradient)
    input_shape = [10, 100, 100, 3]
    input_to_conv2d = tf.zeros(dtype=tf.float32, shape=input_shape)
    if tf.executing_eagerly():
      with tf.GradientTape() as tape0:
        output_of_conv2d = conv2d(input_to_conv2d)
      # Create a transpose without a custom getter
      conv2d_transpose = conv2d.transpose()
      with tf.GradientTape() as tape1:
        output_of_transpose = conv2d_transpose(output_of_conv2d)
      conv2d_vars = conv2d.get_variables()
      conv2d_grads = tape0.gradient(output_of_conv2d, conv2d_vars)
      conv2d_transpose_vars = conv2d_transpose.get_variables()
      conv2d_transpose_grads = tape1.gradient(output_of_transpose,
                                              conv2d_transpose_vars)
    else:
      output_of_conv2d = conv2d(input_to_conv2d)
      conv2d_vars = conv2d.get_variables()
      conv2d_grads = tf.gradients(output_of_conv2d, conv2d_vars)
      # Create a transpose without a custom getter
      conv2d_transpose = conv2d.transpose()
      output_of_transpose = conv2d_transpose(output_of_conv2d)
      conv2d_transpose_vars = conv2d_transpose.get_variables()
      conv2d_transpose_grads = tf.gradients(output_of_transpose,
                                            conv2d_transpose_vars)

    # Sanity check that the custom getter was indeed used for the conv net.
    self.assertTrue(len(conv2d_vars))
    for tensor in conv2d_grads:
      self.assertIsNone(tensor)
    # Check the transpose did not use the custom getter that was passed to the
    # original conv net.
    self.assertTrue(len(conv2d_transpose_vars))
    for tensor in conv2d_transpose_grads:
      self.assertIsNotNone(tensor)

</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/nets/convnet_test.py" startline="1086" endline="1129" pcid="684">
  def testNoCustomGetterTranspose(self):
    """Tests not passing a custom getter to the transpose method."""
    conv2d_t = snt.nets.ConvNet2DTranspose(
        output_shapes=self.output_shapes,
        output_channels=self.output_channels,
        kernel_shapes=self.kernel_shapes,
        strides=self.strides,
        paddings=self.paddings,
        custom_getter=snt.custom_getters.stop_gradient)
    input_shape = [10, 100, 100, 3]
    input_to_conv2d_t = tf.zeros(dtype=tf.float32, shape=input_shape)
    if tf.executing_eagerly():
      with tf.GradientTape() as tape0:
        output_of_conv2d_t = conv2d_t(input_to_conv2d_t)
      # Create a transpose without a custom getter
      conv2d = conv2d_t.transpose()
      with tf.GradientTape() as tape1:
        output_of_conv = conv2d(output_of_conv2d_t)
      conv2d_t_vars = conv2d_t.get_variables()
      conv2d_t_grads = tape0.gradient(output_of_conv2d_t, conv2d_t_vars)
      conv2d_vars = conv2d.get_variables()
      conv2d_grads = tape1.gradient(output_of_conv, conv2d_vars)
    else:
      output_of_conv2d_t = conv2d_t(input_to_conv2d_t)
      conv2d_t_vars = conv2d_t.get_variables()
      conv2d_t_grads = tf.gradients(output_of_conv2d_t, conv2d_t_vars)
      # Create a transpose without a custom getter
      conv2d = conv2d_t.transpose()
      output_of_conv = conv2d(output_of_conv2d_t)
      conv2d_vars = conv2d.get_variables()
      conv2d_grads = tf.gradients(output_of_conv, conv2d_vars)

    # Sanity check that the custom getter was indeed used for the conv net.
    self.assertTrue(len(conv2d_t_vars))
    for tensor in conv2d_t_grads:
      self.assertIsNone(tensor)
    # Check the transpose did not use the custom getter that was passed to the
    # original conv net.
    self.assertTrue(len(conv2d_vars))
    for tensor in conv2d_grads:
      self.assertIsNotNone(tensor)


# @tf.contrib.eager.run_all_tests_in_graph_and_eager_modes
</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/nets/convnet_test.py" startline="1023" endline="1057" pcid="682">
  def testCustomGetter(self):
    custom_getter = snt.custom_getters.Context(snt.custom_getters.stop_gradient)
    module = snt.nets.ConvNet2DTranspose(
        output_shapes=self.output_shapes,
        output_channels=self.output_channels,
        kernel_shapes=self.kernel_shapes,
        strides=self.strides,
        paddings=self.paddings,
        custom_getter=custom_getter)

    input_shape = [10, 100, 100, 3]
    input_to_net = tf.random_normal(dtype=tf.float32, shape=input_shape)

    if tf.executing_eagerly():
      with tf.GradientTape() as tape0:
        out0 = module(input_to_net)
      with tf.GradientTape() as tape1:
        with custom_getter:
          out1 = module(input_to_net)
      all_vars = tf.trainable_variables()
      out0_grads = tape0.gradient(out0, all_vars)
      out1_grads = tape1.gradient(out1, all_vars)

    else:
      out0 = module(input_to_net)
      with custom_getter:
        out1 = module(input_to_net)
      all_vars = tf.trainable_variables()
      out0_grads = tf.gradients(out0, all_vars)
      out1_grads = tf.gradients(out1, all_vars)

    for grad in out0_grads:
      self.assertIsNotNone(grad)
    self.assertEqual([None] * len(out1_grads), out1_grads)

</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/nets/convnet_test.py" startline="801" endline="834" pcid="674">
  def testCustomGetter(self):
    custom_getter = snt.custom_getters.Context(snt.custom_getters.stop_gradient)
    module = snt.nets.ConvNet2D(output_channels=self.output_channels,
                                kernel_shapes=self.kernel_shapes,
                                rates=self.rates,
                                strides=self.strides,
                                paddings=self.paddings,
                                custom_getter=custom_getter)

    input_shape = [10, 100, 100, 3]
    input_to_net = tf.random_normal(dtype=tf.float32, shape=input_shape)

    if tf.executing_eagerly():
      with tf.GradientTape() as tape0:
        out0 = module(input_to_net)
      with tf.GradientTape() as tape1:
        with custom_getter:
          out1 = module(input_to_net)
      all_vars = tf.trainable_variables()
      out0_grads = tape0.gradient(out0, all_vars)
      out1_grads = tape1.gradient(out1, all_vars)

    else:
      out0 = module(input_to_net)
      with custom_getter:
        out1 = module(input_to_net)
      all_vars = tf.trainable_variables()
      out0_grads = tf.gradients(out0, all_vars)
      out1_grads = tf.gradients(out1, all_vars)

    for grad in out0_grads:
      self.assertNotEqual(None, grad)
    self.assertEqual([None] * len(out1_grads), out1_grads)

</source>
</class>

<class classid="27" nclones="2" nlines="19" similarity="100">
<source file="systems/sonnet-1.34/sonnet/python/modules/nets/convnet_test.py" startline="776" endline="800" pcid="673">
  def testPartitioners(self):
    if tf.executing_eagerly():
      self.skipTest("Eager does not support partitioned variables.")

    partitioners = {
        "w": tf.variable_axis_size_partitioner(10),
        "b": tf.variable_axis_size_partitioner(8),
    }

    module = snt.nets.ConvNet2D(output_channels=self.output_channels,
                                kernel_shapes=self.kernel_shapes,
                                rates=self.rates,
                                strides=self.strides,
                                paddings=self.paddings,
                                partitioners=partitioners)

    input_shape = [10, 100, 100, 3]
    input_to_net = tf.placeholder(tf.float32, shape=input_shape)

    _ = module(input_to_net)

    for layer in module._layers:
      self.assertEqual(type(layer.w), variables.PartitionedVariable)
      self.assertEqual(type(layer.b), variables.PartitionedVariable)

</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/nets/convnet_test.py" startline="974" endline="998" pcid="680">
  def testPartitioners(self):
    if tf.executing_eagerly():
      self.skipTest("Eager does not support partitioned variables.")

    partitioners = {
        "w": tf.variable_axis_size_partitioner(10),
        "b": tf.variable_axis_size_partitioner(8),
    }

    module = snt.nets.ConvNet2DTranspose(output_channels=self.output_channels,
                                         output_shapes=self.output_shapes,
                                         kernel_shapes=self.kernel_shapes,
                                         strides=self.strides,
                                         paddings=self.paddings,
                                         partitioners=partitioners)

    input_shape = [10, 100, 100, 3]
    input_to_net = tf.placeholder(tf.float32, shape=input_shape)

    _ = module(input_to_net)

    for layer in module._layers:
      self.assertEqual(type(layer.w), variables.PartitionedVariable)
      self.assertEqual(type(layer.b), variables.PartitionedVariable)

</source>
</class>

<class classid="28" nclones="3" nlines="11" similarity="90">
<source file="systems/sonnet-1.34/sonnet/python/modules/nets/mlp_test.py" startline="360" endline="372" pcid="745">
  def testDropoutOff(self):
    """Make sure dropout layers aren't added to the computation graph."""
    if tf.executing_eagerly():
      self.skipTest("Test not supported when executing eagerly")
    mlp_name = "test_dropout_on_mlp"
    mlp = snt.nets.MLP([1], use_dropout=False, use_bias=False,
                       activate_final=True, name=mlp_name)
    _ = mlp(tf.ones([1, 1]), is_training=True,
            dropout_keep_prob=0.5)
    op_names = [op.name for op in tf.get_default_graph().get_operations()]
    op_to_look_for = "{}_1/dropout/Shape".format(mlp_name)
    self.assertNotIn(op_to_look_for, op_names)

</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/nets/mlp_test.py" startline="385" endline="397" pcid="747">
  def testDropoutTensor(self):
    """Checks support for tf.Bool Tensors."""
    if tf.executing_eagerly():
      self.skipTest("Test not supported when executing eagerly")
    mlp_name = "test_dropout_on_mlp"
    mlp = snt.nets.MLP([1], use_dropout=True, use_bias=False,
                       activate_final=True, name=mlp_name)
    _ = mlp(tf.ones([1, 1]), is_training=tf.convert_to_tensor(True, tf.bool),
            dropout_keep_prob=0.5)
    op_names = [op.name for op in tf.get_default_graph().get_operations()]
    op_to_look_for = "{}_1/dropout/Shape".format(mlp_name)
    self.assertIn(op_to_look_for, op_names)

</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/nets/mlp_test.py" startline="373" endline="384" pcid="746">
  def testDropout(self):
    if tf.executing_eagerly():
      self.skipTest("Test not supported when executing eagerly")
    mlp_name = "test_dropout_on_mlp"
    mlp = snt.nets.MLP([1], use_dropout=True, use_bias=False,
                       activate_final=True, name=mlp_name)
    _ = mlp(tf.ones([1, 1]), is_training=True,
            dropout_keep_prob=0.5)
    op_names = [op.name for op in tf.get_default_graph().get_operations()]
    op_to_look_for = "{}_1/dropout/Shape".format(mlp_name)
    self.assertIn(op_to_look_for, op_names)

</source>
</class>

<class classid="29" nclones="5" nlines="16" similarity="82">
<source file="systems/sonnet-1.34/sonnet/python/modules/block_matrix_test.py" startline="38" endline="60" pcid="797">
  def test_lower(self):
    """Tests block lower-triangular matrix."""

    btm = block_matrix.BlockTriangularMatrix(
        block_shape=(2, 3), block_rows=3, upper=False)
    self.assertEqual(btm.num_blocks, 6)
    self.assertEqual(btm.block_size, 6)
    self.assertEqual(btm.input_size, 36)

    output = btm(create_input(btm.input_size))
    with self.test_session() as sess:
      result = sess.run(output)

    self._check_output_size(btm, result)

    expected = np.array([[[0, 1, 2, 0, 0, 0, 0, 0, 0],
                          [3, 4, 5, 0, 0, 0, 0, 0, 0],
                          [6, 7, 8, 9, 10, 11, 0, 0, 0],
                          [12, 13, 14, 15, 16, 17, 0, 0, 0],
                          [18, 19, 20, 21, 22, 23, 24, 25, 26],
                          [27, 28, 29, 30, 31, 32, 33, 34, 35]]])
    self.assertAllEqual(result, expected)

</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/block_matrix_test.py" startline="61" endline="83" pcid="798">
  def test_lower_no_diagonal(self):
    """Tests block lower-triangular matrix without diagonal."""

    btm = block_matrix.BlockTriangularMatrix(
        block_shape=(2, 3), block_rows=3, include_diagonal=False)
    self.assertEqual(btm.num_blocks, 3)
    self.assertEqual(btm.block_size, 6)
    self.assertEqual(btm.input_size, 18)

    output = btm(create_input(btm.input_size))
    with self.test_session() as sess:
      result = sess.run(output)

    self._check_output_size(btm, result)

    expected = np.array([[[0, 0, 0, 0, 0, 0, 0, 0, 0],
                          [0, 0, 0, 0, 0, 0, 0, 0, 0],
                          [0, 1, 2, 0, 0, 0, 0, 0, 0],
                          [3, 4, 5, 0, 0, 0, 0, 0, 0],
                          [6, 7, 8, 9, 10, 11, 0, 0, 0],
                          [12, 13, 14, 15, 16, 17, 0, 0, 0]]])
    self.assertAllEqual(result, expected)

</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/block_matrix_test.py" startline="155" endline="174" pcid="802">
  def test_default(self):
    """Tests BlockDiagonalMatrix."""

    bdm = block_matrix.BlockDiagonalMatrix(block_shape=(2, 3), block_rows=3)
    self.assertEqual(bdm.num_blocks, 3)
    self.assertEqual(bdm.block_size, 6)
    self.assertEqual(bdm.input_size, 18)

    output = bdm(create_input(bdm.input_size))
    with self.test_session() as sess:
      result = sess.run(output)

    expected = np.array([[[0, 1, 2, 0, 0, 0, 0, 0, 0],
                          [3, 4, 5, 0, 0, 0, 0, 0, 0],
                          [0, 0, 0, 6, 7, 8, 0, 0, 0],
                          [0, 0, 0, 9, 10, 11, 0, 0, 0],
                          [0, 0, 0, 0, 0, 0, 12, 13, 14],
                          [0, 0, 0, 0, 0, 0, 15, 16, 17]]])
    self.assertAllEqual(result, expected)

</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/block_matrix_test.py" startline="84" endline="106" pcid="799">
  def test_upper(self):
    """Tests block upper-triangular matrix."""

    btm = block_matrix.BlockTriangularMatrix(
        block_shape=(2, 3), block_rows=3, upper=True)
    self.assertEqual(btm.num_blocks, 6)
    self.assertEqual(btm.block_size, 6)
    self.assertEqual(btm.input_size, 36)

    output = btm(create_input(btm.input_size))
    with self.test_session() as sess:
      result = sess.run(output)

    self._check_output_size(btm, result)

    expected = np.array([[[0, 1, 2, 3, 4, 5, 6, 7, 8],
                          [9, 10, 11, 12, 13, 14, 15, 16, 17],
                          [0, 0, 0, 18, 19, 20, 21, 22, 23],
                          [0, 0, 0, 24, 25, 26, 27, 28, 29],
                          [0, 0, 0, 0, 0, 0, 30, 31, 32],
                          [0, 0, 0, 0, 0, 0, 33, 34, 35]]])
    self.assertAllEqual(result, expected)

</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/block_matrix_test.py" startline="107" endline="129" pcid="800">
  def test_upper_no_diagonal(self):
    """Tests block upper-triangular matrix without diagonal."""

    btm = block_matrix.BlockTriangularMatrix(
        block_shape=(2, 3), block_rows=3, upper=True, include_diagonal=False)
    self.assertEqual(btm.num_blocks, 3)
    self.assertEqual(btm.block_size, 6)
    self.assertEqual(btm.input_size, 18)

    output = btm(create_input(btm.input_size))
    with self.test_session() as sess:
      result = sess.run(output)

    self._check_output_size(btm, result)

    expected = np.array([[[0, 0, 0, 0, 1, 2, 3, 4, 5],
                          [0, 0, 0, 6, 7, 8, 9, 10, 11],
                          [0, 0, 0, 0, 0, 0, 12, 13, 14],
                          [0, 0, 0, 0, 0, 0, 15, 16, 17],
                          [0, 0, 0, 0, 0, 0, 0, 0, 0],
                          [0, 0, 0, 0, 0, 0, 0, 0, 0]]])
    self.assertAllEqual(result, expected)

</source>
</class>

<class classid="30" nclones="2" nlines="12" similarity="83">
<source file="systems/sonnet-1.34/sonnet/python/modules/residual_test.py" startline="98" endline="110" pcid="838">
  def testShape(self):
    inputs = tf.placeholder(tf.float32, shape=[self.batch_size, self.in_size])
    prev_state = tf.placeholder(
        tf.float32, shape=[self.batch_size, self.in_size])
    vanilla_rnn = snt.VanillaRNN(self.in_size)
    residual_wrapper = snt.ResidualCore(vanilla_rnn, name="residual")
    output, next_state = residual_wrapper(inputs, prev_state)
    shape = np.ndarray((self.batch_size, self.in_size))

    self.assertEqual(self.in_size, residual_wrapper.output_size)
    self.assertShapeEqual(shape, output)
    self.assertShapeEqual(shape, next_state)

</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/residual_test.py" startline="189" endline="202" pcid="843">
  def testShape(self):
    inputs = tf.placeholder(tf.float32, shape=[self.batch_size, self.in_size])
    prev_state = tf.placeholder(
        tf.float32, shape=[self.batch_size, self.hidden_size])
    vanilla_rnn = snt.VanillaRNN(self.hidden_size)
    skip_wrapper = snt.SkipConnectionCore(vanilla_rnn, name="skip")
    output, next_state = skip_wrapper(inputs, prev_state)
    output_shape = np.ndarray((self.batch_size,
                               self.in_size + self.hidden_size))
    state_shape = np.ndarray((self.batch_size, self.hidden_size))

    self.assertShapeEqual(output_shape, output)
    self.assertShapeEqual(state_shape, next_state)

</source>
</class>

<class classid="31" nclones="2" nlines="25" similarity="96">
<source file="systems/sonnet-1.34/sonnet/python/modules/residual_test.py" startline="111" endline="142" pcid="839">
  def testComputation(self):
    inputs = tf.placeholder(tf.float32, shape=[self.batch_size, self.in_size])
    prev_state = tf.placeholder(tf.float32,
                                shape=[self.batch_size, self.in_size])

    vanilla_rnn = snt.VanillaRNN(name="rnn", hidden_size=self.in_size)
    residual = snt.ResidualCore(vanilla_rnn, name="residual")

    output, new_state = residual(inputs, prev_state)
    in_to_hid = vanilla_rnn.in_to_hidden_variables
    hid_to_hid = vanilla_rnn.hidden_to_hidden_variables
    with self.test_session() as sess:
      # With random data, check the TF calculation matches the Numpy version.
      input_data = np.random.randn(self.batch_size, self.in_size)
      prev_state_data = np.random.randn(self.batch_size, self.in_size)
      tf.global_variables_initializer().run()

      fetches = [output, new_state, in_to_hid[0], in_to_hid[1],
                 hid_to_hid[0], hid_to_hid[1]]
      output = sess.run(fetches,
                        {inputs: input_data, prev_state: prev_state_data})
    output_v, new_state_v, in_to_hid_w, in_to_hid_b = output[:4]
    hid_to_hid_w, hid_to_hid_b = output[4:]

    real_in_to_hid = np.dot(input_data, in_to_hid_w) + in_to_hid_b
    real_hid_to_hid = np.dot(prev_state_data, hid_to_hid_w) + hid_to_hid_b
    vanilla_output = np.tanh(real_in_to_hid + real_hid_to_hid)
    residual_output = vanilla_output + input_data

    self.assertAllClose(residual_output, output_v)
    self.assertAllClose(vanilla_output, new_state_v)

</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/residual_test.py" startline="203" endline="234" pcid="844">
  def testComputation(self):
    inputs = tf.placeholder(tf.float32, shape=[self.batch_size, self.in_size])
    prev_state = tf.placeholder(tf.float32,
                                shape=[self.batch_size, self.in_size])

    vanilla_rnn = snt.VanillaRNN(name="rnn", hidden_size=self.in_size)
    residual = snt.SkipConnectionCore(vanilla_rnn, name="skip")

    output, new_state = residual(inputs, prev_state)
    in_to_hid = vanilla_rnn.in_to_hidden_variables
    hid_to_hid = vanilla_rnn.hidden_to_hidden_variables
    with self.test_session() as sess:
      # With random data, check the TF calculation matches the Numpy version.
      input_data = np.random.randn(self.batch_size, self.in_size)
      prev_state_data = np.random.randn(self.batch_size, self.in_size)
      tf.global_variables_initializer().run()

      fetches = [output, new_state, in_to_hid[0], in_to_hid[1],
                 hid_to_hid[0], hid_to_hid[1]]
      output = sess.run(fetches,
                        {inputs: input_data, prev_state: prev_state_data})
    output_v, new_state_v, in_to_hid_w, in_to_hid_b = output[:4]
    hid_to_hid_w, hid_to_hid_b = output[4:]

    real_in_to_hid = np.dot(input_data, in_to_hid_w) + in_to_hid_b
    real_hid_to_hid = np.dot(prev_state_data, hid_to_hid_w) + hid_to_hid_b
    vanilla_output = np.tanh(real_in_to_hid + real_hid_to_hid)
    skip_output = np.concatenate((input_data, vanilla_output), -1)

    self.assertAllClose(skip_output, output_v)
    self.assertAllClose(vanilla_output, new_state_v)

</source>
</class>

<class classid="32" nclones="2" nlines="13" similarity="100">
<source file="systems/sonnet-1.34/sonnet/python/modules/residual_test.py" startline="143" endline="161" pcid="840">
  def testHeterogeneousState(self):
    """Checks that the shape and type of the initial state are preserved."""

    core = HeterogeneousStateCore(name="rnn", hidden_size=self.in_size)
    residual = snt.ResidualCore(core, name="residual")

    core_state = core.initial_state(self.batch_size)
    residual_state = residual.initial_state(self.batch_size)

    self.assertEqual(core_state[0].shape.as_list(),
                     residual_state[0].shape.as_list())
    self.assertEqual(core_state[1].shape.as_list(),
                     residual_state[1].shape.as_list())
    self.assertEqual(core_state[0].dtype,
                     residual_state[0].dtype)
    self.assertEqual(core_state[1].dtype,
                     residual_state[1].dtype)


</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/residual_test.py" startline="235" endline="253" pcid="845">
  def testHeterogeneousState(self):
    """Checks that the shape and type of the initial state are preserved."""

    core = HeterogeneousStateCore(name="rnn", hidden_size=self.hidden_size)
    skip_wrapper = snt.SkipConnectionCore(core, name="skip")

    core_state = core.initial_state(self.batch_size)
    skip_state = skip_wrapper.initial_state(self.batch_size)

    self.assertEqual(core_state[0].shape.as_list(),
                     skip_state[0].shape.as_list())
    self.assertEqual(core_state[1].shape.as_list(),
                     skip_state[1].shape.as_list())
    self.assertEqual(core_state[0].dtype,
                     skip_state[0].dtype)
    self.assertEqual(core_state[1].dtype,
                     skip_state[1].dtype)


</source>
</class>

<class classid="33" nclones="2" nlines="43" similarity="82">
<source file="systems/sonnet-1.34/sonnet/python/modules/spatial_transformer_test.py" startline="109" endline="154" pcid="867">
  def testSameAsNumPyReference(self, output_shape, source_shape, constraints):
    def chain(x):
      return itertools.chain(*x)

    def predict(output_shape, source_shape, inputs):
      ranges = [np.linspace(-1, 1, x, dtype=np.float32)
                for x in reversed(output_shape)]
      n = len(source_shape)
      grid = np.meshgrid(*ranges, indexing="xy")
      for _ in range(len(output_shape), len(source_shape)):
        grid.append(np.zeros_like(grid[0]))
      grid.append(np.ones_like(grid[0]))
      grid = np.array([x.reshape(1, -1) for x in grid]).squeeze()
      predicted_output = []
      for i in range(0, batch_size):
        x = np.dot(inputs[i, :].reshape(n, n+1), grid)
        for k, s in enumerate(reversed(source_shape)):
          s = (s - 1) * 0.5
          x[k, :] = x[k, :] * s + s
        x = np.concatenate([v.reshape(v.shape + (1,)) for v in x], -1)
        predicted_output.append(x.reshape(tuple(output_shape) + (n,)))
      return predicted_output

    batch_size = 20
    agw = snt.AffineGridWarper(source_shape=source_shape,
                               output_shape=output_shape,
                               constraints=constraints)
    inputs = tf.placeholder(tf.float32, [None, constraints.num_free_params])
    warped_grid = agw(inputs)
    full_size = constraints.num_dim * (constraints.num_dim + 1)
    full_input_np = np.random.rand(batch_size, full_size)

    con_i = [i for i, x in enumerate(chain(constraints.mask)) if not x]
    con_val = [x for x in chain(constraints.constraints) if x is not None]
    for i, v in zip(con_i, con_val):
      full_input_np[:, i] = v
    uncon_i = [i for i, x in enumerate(chain(constraints.mask)) if x]
    with self.test_session() as sess:
      output = sess.run(warped_grid,
                        feed_dict={inputs: full_input_np[:, uncon_i]})

    self.assertAllClose(output,
                        predict(output_shape, source_shape, full_input_np),
                        rtol=1e-05,
                        atol=1e-05)

</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/spatial_transformer_test.py" startline="176" endline="228" pcid="871">
  def testInvSameAsNumPyRef(self, output_shape, source_shape, constraints):
    def chain(x):
      return itertools.chain(*x)

    def predict(output_shape, source_shape, inputs):
      ranges = [np.linspace(-1, 1, x, dtype=np.float32)
                for x in reversed(source_shape)]
      n = len(output_shape)
      grid = np.meshgrid(*ranges, indexing="xy")
      for _ in range(len(source_shape), len(output_shape)):
        grid.append(np.zeros_like(grid[0]))
      grid.append(np.ones_like(grid[0]))
      grid = np.array([x.reshape(1, -1) for x in grid]).squeeze()
      predicted_output = []
      for i in range(0, batch_size):
        affine_matrix = inputs[i, :].reshape(n, n+1)
        inv_matrix = np.linalg.inv(affine_matrix[:2, :2])
        inv_transform = np.concatenate(
            [inv_matrix, -np.dot(inv_matrix,
                                 affine_matrix[:, 2].reshape(2, 1))], 1)
        x = np.dot(inv_transform, grid)
        for k, s in enumerate(reversed(output_shape)):
          s = (s - 1) * 0.5
          x[k, :] = x[k, :] * s + s
        x = np.concatenate([v.reshape(v.shape + (1,)) for v in x], -1)
        predicted_output.append(x.reshape(tuple(source_shape) + (n,)))
      return predicted_output

    batch_size = 20
    agw = snt.AffineGridWarper(source_shape=source_shape,
                               output_shape=output_shape,
                               constraints=constraints).inverse()
    inputs = tf.placeholder(tf.float32, [None, constraints.num_free_params])
    warped_grid = agw(inputs)
    full_size = constraints.num_dim * (constraints.num_dim + 1)
    # Adding a bit of mass to the matrix to avoid singular matrices
    full_input_np = np.random.rand(batch_size, full_size) + 0.1

    con_i = [i for i, x in enumerate(chain(constraints.mask)) if not x]
    con_val = [x for x in chain(constraints.constraints) if x is not None]
    for i, v in zip(con_i, con_val):
      full_input_np[:, i] = v
    uncon_i = [i for i, x in enumerate(chain(constraints.mask)) if x]
    with self.test_session() as sess:
      output = sess.run(warped_grid,
                        feed_dict={inputs: full_input_np[:, uncon_i]})

    self.assertAllClose(output,
                        predict(output_shape, source_shape, full_input_np),
                        rtol=1e-05,
                        atol=1e-05)


</source>
</class>

<class classid="34" nclones="7" nlines="19" similarity="70">
<source file="systems/sonnet-1.34/sonnet/python/modules/conv_gpu_test.py" startline="79" endline="103" pcid="882">
  def testConv1DDataFormats(self, use_bias, stride):
    """Check the module produces the same result for supported data formats."""
    func = functools.partial(
        snt.Conv1D,
        output_channels=self.OUT_CHANNELS,
        kernel_shape=self.KERNEL_SHAPE,
        use_bias=use_bias,
        stride=stride,
        initializers=create_initializers(use_bias))

    conv_nwc = func(name="NWC", data_format="NWC")
    x = tf.constant(np.random.random(self.INPUT_SHAPE).astype(np.float32))
    result_nwc = conv_nwc(x)

    # We will force both modules to share the same weights by creating
    # a custom getter that returns the weights from the first conv module when
    # tf.get_variable is called.
    custom_getter = {"w": create_custom_field_getter(conv_nwc, "w"),
                     "b": create_custom_field_getter(conv_nwc, "b")}
    conv_nwc = func(name="NCW", data_format="NCW", custom_getter=custom_getter)
    x_transpose = tf.transpose(x, perm=(0, 2, 1))
    result_ncw = tf.transpose(conv_nwc(x_transpose), perm=(0, 2, 1))

    self.checkEquality(result_nwc, result_ncw)

</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/conv_gpu_test.py" startline="315" endline="340" pcid="897">
  def testConv3DDataFormats(self, use_bias, stride):
    """Check the module produces the same result for supported data formats."""
    func = functools.partial(
        snt.Conv3D,
        output_channels=self.OUT_CHANNELS,
        kernel_shape=self.KERNEL_SHAPE,
        use_bias=use_bias,
        stride=stride,
        initializers=create_initializers(use_bias))

    conv_ndhwc = func(name="NDHWC", data_format="NDHWC")
    x = tf.constant(np.random.random(self.INPUT_SHAPE).astype(np.float32))
    result_ndhwc = conv_ndhwc(x)

    # We will force both modules to share the same weights by creating
    # a custom getter that returns the weights from the first conv module when
    # tf.get_variable is called.
    custom_getter = {"w": create_custom_field_getter(conv_ndhwc, "w"),
                     "b": create_custom_field_getter(conv_ndhwc, "b")}
    conv_ncdhw = func(name="NCDHW", data_format="NCDHW",
                      custom_getter=custom_getter)
    x_transpose = tf.transpose(x, perm=(0, 4, 1, 2, 3))
    result_ncdhw = tf.transpose(conv_ncdhw(x_transpose), perm=(0, 2, 3, 4, 1))

    self.checkEquality(result_ndhwc, result_ncdhw)

</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/conv_gpu_test.py" startline="157" endline="181" pcid="887">
  def testCausalConv1DDataFormats(self, use_bias, stride):
    """Check the module produces the same result for supported data formats."""
    func = functools.partial(
        snt.CausalConv1D,
        output_channels=self.OUT_CHANNELS,
        kernel_shape=self.KERNEL_SHAPE,
        use_bias=use_bias,
        stride=stride,
        initializers=create_initializers(use_bias))

    conv_nwc = func(name="NWC", data_format="NWC")
    x = tf.constant(np.random.random(self.INPUT_SHAPE).astype(np.float32))
    result_nwc = conv_nwc(x)

    # We will force both modules to share the same weights by creating
    # a custom getter that returns the weights from the first conv module when
    # tf.get_variable is called.
    custom_getter = {"w": create_custom_field_getter(conv_nwc, "w"),
                     "b": create_custom_field_getter(conv_nwc, "b")}
    conv_ncw = func(name="NCW", data_format="NCW", custom_getter=custom_getter)
    x_transpose = tf.transpose(x, perm=(0, 2, 1))
    result_ncw = tf.transpose(conv_ncw(x_transpose), perm=(0, 2, 1))

    self.checkEquality(result_nwc, result_ncw)

</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/conv_gpu_test.py" startline="395" endline="424" pcid="902">
  def testConv1DTransposeDataFormats(self, use_bias, stride):
    """Check the module produces the same result for supported data formats."""
    input_shape = (self.INPUT_SHAPE.input_batch,
                   int(np.ceil(self.INPUT_SHAPE.input_width / stride)),
                   self.INPUT_SHAPE.input_channels)

    func = functools.partial(
        snt.Conv1DTranspose,
        output_channels=self.OUT_CHANNELS,
        kernel_shape=self.KERNEL_SHAPE,
        output_shape=(self.INPUT_SHAPE.input_width,),
        use_bias=use_bias,
        stride=stride,
        initializers=create_initializers(use_bias))

    conv_nwc = func(name="NWC", data_format="NWC")
    x = tf.constant(np.random.random(input_shape).astype(np.float32))
    result_nwc = conv_nwc(x)

    # We will force both modules to share the same weights by creating
    # a custom getter that returns the weights from the first conv module when
    # tf.get_variable is called.
    custom_getter = {"w": create_custom_field_getter(conv_nwc, "w"),
                     "b": create_custom_field_getter(conv_nwc, "b")}
    conv_ncw = func(name="NCW", data_format="NCW", custom_getter=custom_getter)
    x_transpose = tf.transpose(x, perm=(0, 2, 1))
    result_ncw = tf.transpose(conv_ncw(x_transpose), perm=(0, 2, 1))

    self.checkEquality(result_nwc, result_ncw)

</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/conv_gpu_test.py" startline="235" endline="260" pcid="892">
  def testConv2DDataFormats(self, use_bias, stride):
    """Check the module produces the same result for supported data formats."""
    func = functools.partial(
        snt.Conv2D,
        output_channels=self.OUT_CHANNELS,
        kernel_shape=self.KERNEL_SHAPE,
        use_bias=use_bias,
        stride=stride,
        initializers=create_initializers(use_bias))

    conv_nhwc = func(name="NHWC", data_format="NHWC")
    x = tf.constant(np.random.random(self.INPUT_SHAPE).astype(np.float32))
    result_nhwc = conv_nhwc(x)

    # We will force both modules to share the same weights by creating
    # a custom getter that returns the weights from the first conv module when
    # tf.get_variable is called.
    custom_getter = {"w": create_custom_field_getter(conv_nhwc, "w"),
                     "b": create_custom_field_getter(conv_nhwc, "b")}
    conv_nchw = func(name="NCHW", data_format="NCHW",
                     custom_getter=custom_getter)
    x_transpose = tf.transpose(x, perm=(0, 3, 1, 2))
    result_nchw = tf.transpose(conv_nchw(x_transpose), perm=(0, 2, 3, 1))

    self.checkEquality(result_nhwc, result_nchw)

</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/conv_gpu_test.py" startline="479" endline="511" pcid="907">
  def testConv2DTransposeDataFormats(self, use_bias, stride):
    """Check the module produces the same result for supported data formats."""
    input_shape = (self.INPUT_SHAPE.input_batch,
                   int(np.ceil(self.INPUT_SHAPE.input_height / stride)),
                   int(np.ceil(self.INPUT_SHAPE.input_width / stride)),
                   self.INPUT_SHAPE.input_channels)

    func = functools.partial(
        snt.Conv2DTranspose,
        output_channels=self.OUT_CHANNELS,
        kernel_shape=self.KERNEL_SHAPE,
        output_shape=(self.INPUT_SHAPE.input_height,
                      self.INPUT_SHAPE.input_width),
        use_bias=use_bias,
        stride=stride,
        initializers=create_initializers(use_bias))

    conv_nhwc = func(name="NHWC", data_format="NHWC")
    x = tf.constant(np.random.random(input_shape).astype(np.float32))
    result_nhwc = conv_nhwc(x)

    # We will force both modules to share the same weights by creating
    # a custom getter that returns the weights from the first conv module when
    # tf.get_variable is called.
    custom_getter = {"w": create_custom_field_getter(conv_nhwc, "w"),
                     "b": create_custom_field_getter(conv_nhwc, "b")}
    conv_nchw = func(name="NCHW", data_format="NCHW",
                     custom_getter=custom_getter)
    x_transpose = tf.transpose(x, perm=(0, 3, 1, 2))
    result_nchw = tf.transpose(conv_nchw(x_transpose), perm=(0, 2, 3, 1))

    self.checkEquality(result_nhwc, result_nchw)

</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/conv_gpu_test.py" startline="568" endline="602" pcid="912">
  def testConv3DTransposeDataFormats(self, use_bias, stride):
    """Check the module produces the same result for supported data formats."""
    input_shape = (self.INPUT_SHAPE.input_batch,
                   int(np.ceil(self.INPUT_SHAPE.input_depth / stride)),
                   int(np.ceil(self.INPUT_SHAPE.input_height / stride)),
                   int(np.ceil(self.INPUT_SHAPE.input_width / stride)),
                   self.INPUT_SHAPE.input_channels)

    func = functools.partial(
        snt.Conv3DTranspose,
        output_channels=self.OUT_CHANNELS,
        kernel_shape=self.KERNEL_SHAPE,
        output_shape=(self.INPUT_SHAPE.input_depth,
                      self.INPUT_SHAPE.input_height,
                      self.INPUT_SHAPE.input_width),
        use_bias=use_bias,
        stride=stride,
        initializers=create_initializers(use_bias))

    conv_ndhwc = func(name="NDHWC", data_format="NDHWC")
    x = tf.constant(np.random.random(input_shape).astype(np.float32))
    result_ndhwc = conv_ndhwc(x)

    # We will force both modules to share the same weights by creating
    # a custom getter that returns the weights from the first conv module when
    # tf.get_variable is called.
    custom_getter = {"w": create_custom_field_getter(conv_ndhwc, "w"),
                     "b": create_custom_field_getter(conv_ndhwc, "b")}
    conv_ncdhw = func(name="NCDHW", data_format="NCDHW",
                      custom_getter=custom_getter)
    x_transpose = tf.transpose(x, perm=(0, 4, 1, 2, 3))
    result_ncdhw = tf.transpose(conv_ncdhw(x_transpose), perm=(0, 2, 3, 4, 1))

    self.checkEquality(result_ndhwc, result_ncdhw)

</source>
</class>

<class classid="35" nclones="7" nlines="27" similarity="73">
<source file="systems/sonnet-1.34/sonnet/python/modules/conv_gpu_test.py" startline="105" endline="137" pcid="883">
  def testConv1DDataFormatsBatchNorm(self, use_bias):
    """Similar to `testConv1DDataFormats`, but this checks BatchNorm support."""

    def func(name, data_format, custom_getter=None):
      conv = snt.Conv1D(
          name=name,
          output_channels=self.OUT_CHANNELS,
          kernel_shape=self.KERNEL_SHAPE,
          use_bias=use_bias,
          initializers=create_initializers(use_bias),
          data_format=data_format,
          custom_getter=custom_getter)
      if data_format == "NWC":
        batch_norm = snt.BatchNorm(scale=True, update_ops_collection=None)
      else:  # data_format = "NCW"
        batch_norm = snt.BatchNorm(scale=True, update_ops_collection=None,
                                   axis=(0, 2))
      return snt.Sequential([conv,
                             functools.partial(batch_norm, is_training=True)])

    seq_nwc = func(name="NWC", data_format="NWC")
    x = tf.constant(np.random.random(self.INPUT_SHAPE).astype(np.float32))
    result_nwc = seq_nwc(x)

    custom_getter = {"w": create_custom_field_getter(seq_nwc.layers[0], "w"),
                     "b": create_custom_field_getter(seq_nwc.layers[0], "b")}
    seq_ncw = func(name="NCW", data_format="NCW", custom_getter=custom_getter)
    x_transpose = tf.transpose(x, perm=(0, 2, 1))
    result_ncw = tf.transpose(seq_ncw(x_transpose), perm=(0, 2, 1))

    self.checkEquality(result_nwc, result_ncw)


</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/conv_gpu_test.py" startline="426" endline="459" pcid="903">
  def testConv1DTransposeDataFormatsBatchNorm(self, use_bias):
    """Like `testConv1DTransposeDataFormats` but checks BatchNorm support."""

    def func(name, data_format, custom_getter=None):
      conv = snt.Conv1DTranspose(
          name=name,
          output_channels=self.OUT_CHANNELS,
          kernel_shape=self.KERNEL_SHAPE,
          output_shape=(self.INPUT_SHAPE.input_width,),
          use_bias=use_bias,
          initializers=create_initializers(use_bias),
          data_format=data_format,
          custom_getter=custom_getter)
      if data_format == "NWC":
        batch_norm = snt.BatchNorm(scale=True, update_ops_collection=None)
      else:  # data_format == "NCW"
        batch_norm = snt.BatchNorm(scale=True, update_ops_collection=None,
                                   axis=(0, 2))
      return snt.Sequential([conv,
                             functools.partial(batch_norm, is_training=True)])

    seq_nwc = func(name="NWC", data_format="NWC")
    x = tf.constant(np.random.random(self.INPUT_SHAPE).astype(np.float32))
    result_nwc = seq_nwc(x)

    custom_getter = {"w": create_custom_field_getter(seq_nwc.layers[0], "w"),
                     "b": create_custom_field_getter(seq_nwc.layers[0], "b")}
    seq_ncw = func(name="NCW", data_format="NCW", custom_getter=custom_getter)
    x_transpose = tf.transpose(x, perm=(0, 2, 1))
    result_ncw = tf.transpose(seq_ncw(x_transpose), perm=(0, 2, 1))

    self.checkEquality(result_nwc, result_ncw)


</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/conv_gpu_test.py" startline="342" endline="375" pcid="898">
  def testConv3DDataFormatsBatchNorm(self, use_bias):
    """Similar to `testConv3DDataFormats`, but this checks BatchNorm support."""

    def func(name, data_format, custom_getter=None):
      conv = snt.Conv3D(
          name=name,
          output_channels=self.OUT_CHANNELS,
          kernel_shape=self.KERNEL_SHAPE,
          use_bias=use_bias,
          initializers=create_initializers(use_bias),
          data_format=data_format,
          custom_getter=custom_getter)
      if data_format == "NDHWC":
        batch_norm = snt.BatchNorm(scale=True, update_ops_collection=None)
      else:  # data_format = "NCDHW"
        batch_norm = snt.BatchNorm(scale=True, update_ops_collection=None,
                                   axis=(0, 2, 3, 4))
      return snt.Sequential([conv,
                             functools.partial(batch_norm, is_training=True)])

    seq_ndhwc = func(name="NDHWC", data_format="NDHWC")
    x = tf.constant(np.random.random(self.INPUT_SHAPE).astype(np.float32))
    result_ndhwc = seq_ndhwc(x)

    custom_getter = {"w": create_custom_field_getter(seq_ndhwc.layers[0], "w"),
                     "b": create_custom_field_getter(seq_ndhwc.layers[0], "b")}
    seq_ncdhw = func(name="NCDHW", data_format="NCDHW",
                     custom_getter=custom_getter)
    x_transpose = tf.transpose(x, perm=(0, 4, 1, 2, 3))
    result_ncdhw = tf.transpose(seq_ncdhw(x_transpose), perm=(0, 2, 3, 4, 1))

    self.checkEquality(result_ndhwc, result_ncdhw)


</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/conv_gpu_test.py" startline="183" endline="215" pcid="888">
  def testCausalConv1DDataFormatsBatchNorm(self, use_bias):
    """Similar to `testCausalConv1DDataFormats`. Checks BatchNorm support."""

    def func(name, data_format, custom_getter=None):
      conv = snt.CausalConv1D(
          name=name,
          output_channels=self.OUT_CHANNELS,
          kernel_shape=self.KERNEL_SHAPE,
          use_bias=use_bias,
          initializers=create_initializers(use_bias),
          data_format=data_format,
          custom_getter=custom_getter)
      if data_format == "NWC":
        batch_norm = snt.BatchNorm(scale=True, update_ops_collection=None)
      else:  # data_format == "NCW"
        batch_norm = snt.BatchNorm(scale=True, update_ops_collection=None,
                                   axis=(0, 2))
      return snt.Sequential([conv,
                             functools.partial(batch_norm, is_training=True)])

    seq_nwc = func(name="NWC", data_format="NWC")
    x = tf.constant(np.random.random(self.INPUT_SHAPE).astype(np.float32))
    result_nwc = seq_nwc(x)

    custom_getter = {"w": create_custom_field_getter(seq_nwc.layers[0], "w"),
                     "b": create_custom_field_getter(seq_nwc.layers[0], "b")}
    seq_ncw = func(name="NCW", data_format="NCW", custom_getter=custom_getter)
    x_transpose = tf.transpose(x, perm=(0, 2, 1))
    result_ncw = tf.transpose(seq_ncw(x_transpose), perm=(0, 2, 1))

    self.checkEquality(result_nwc, result_ncw)


</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/conv_gpu_test.py" startline="262" endline="295" pcid="893">
  def testConv2DDataFormatsBatchNorm(self, use_bias):
    """Similar to `testConv2DDataFormats`, but this checks BatchNorm support."""

    def func(name, data_format, custom_getter=None):
      conv = snt.Conv2D(
          name=name,
          output_channels=self.OUT_CHANNELS,
          kernel_shape=self.KERNEL_SHAPE,
          use_bias=use_bias,
          initializers=create_initializers(use_bias),
          data_format=data_format,
          custom_getter=custom_getter)
      if data_format == "NHWC":
        batch_norm = snt.BatchNorm(scale=True, update_ops_collection=None)
      else:  # data_format = "NCHW"
        batch_norm = snt.BatchNorm(scale=True, update_ops_collection=None,
                                   fused=True, axis=(0, 2, 3))
      return snt.Sequential([conv,
                             functools.partial(batch_norm, is_training=True)])

    seq_nhwc = func(name="NHWC", data_format="NHWC")
    x = tf.constant(np.random.random(self.INPUT_SHAPE).astype(np.float32))
    result_nhwc = seq_nhwc(x)

    custom_getter = {"w": create_custom_field_getter(seq_nhwc.layers[0], "w"),
                     "b": create_custom_field_getter(seq_nhwc.layers[0], "b")}
    seq_nchw = func(name="NCHW", data_format="NCHW",
                    custom_getter=custom_getter)
    x_transpose = tf.transpose(x, perm=(0, 3, 1, 2))
    result_nchw = tf.transpose(seq_nchw(x_transpose), perm=(0, 2, 3, 1))

    self.checkEquality(result_nhwc, result_nchw)


</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/conv_gpu_test.py" startline="604" endline="639" pcid="913">
  def testConv3DTransposeDataFormatsBatchNorm(self, use_bias):
    """Like `testConv3DTransposeDataFormats` but checks BatchNorm support."""

    def func(name, data_format, custom_getter=None):
      conv = snt.Conv3DTranspose(
          name=name,
          output_channels=self.OUT_CHANNELS,
          kernel_shape=self.KERNEL_SHAPE,
          output_shape=(self.INPUT_SHAPE.input_depth,
                        self.INPUT_SHAPE.input_height,
                        self.INPUT_SHAPE.input_width),
          use_bias=use_bias,
          initializers=create_initializers(use_bias),
          data_format=data_format,
          custom_getter=custom_getter)
      if data_format == "NDHWC":
        batch_norm = snt.BatchNorm(scale=True, update_ops_collection=None)
      else:  # data_format == "NCDHW"
        batch_norm = snt.BatchNorm(scale=True, update_ops_collection=None,
                                   axis=(0, 2, 3, 4))
      return snt.Sequential([conv,
                             functools.partial(batch_norm, is_training=True)])

    seq_ndhwc = func(name="NDHWC", data_format="NDHWC")
    x = tf.constant(np.random.random(self.INPUT_SHAPE).astype(np.float32))
    result_ndhwc = seq_ndhwc(x)

    custom_getter = {"w": create_custom_field_getter(seq_ndhwc.layers[0], "w"),
                     "b": create_custom_field_getter(seq_ndhwc.layers[0], "b")}
    seq_ncdhw = func(name="NCDHW", data_format="NCDHW",
                     custom_getter=custom_getter)
    x_transpose = tf.transpose(x, perm=(0, 4, 1, 2, 3))
    result_ncdhw = tf.transpose(seq_ncdhw(x_transpose), perm=(0, 2, 3, 4, 1))

    self.checkEquality(result_ndhwc, result_ncdhw)

</source>
<source file="systems/sonnet-1.34/sonnet/python/modules/conv_gpu_test.py" startline="513" endline="548" pcid="908">
  def testConv2DTransposeDataFormatsBatchNorm(self, use_bias):
    """Like `testConv2DTransposeDataFormats` but checks BatchNorm support."""

    def func(name, data_format, custom_getter=None):
      conv = snt.Conv2DTranspose(
          name=name,
          output_channels=self.OUT_CHANNELS,
          kernel_shape=self.KERNEL_SHAPE,
          output_shape=(self.INPUT_SHAPE.input_height,
                        self.INPUT_SHAPE.input_width),
          use_bias=use_bias,
          initializers=create_initializers(use_bias),
          data_format=data_format,
          custom_getter=custom_getter)
      if data_format == "NHWC":
        batch_norm = snt.BatchNorm(scale=True, update_ops_collection=None)
      else:  # data_format == "NCHW"
        batch_norm = snt.BatchNorm(scale=True, update_ops_collection=None,
                                   fused=True, axis=(0, 2, 3))
      return snt.Sequential([conv,
                             functools.partial(batch_norm, is_training=True)])

    seq_nhwc = func(name="NHWC", data_format="NHWC")
    x = tf.constant(np.random.random(self.INPUT_SHAPE).astype(np.float32))
    result_nhwc = seq_nhwc(x)

    custom_getter = {"w": create_custom_field_getter(seq_nhwc.layers[0], "w"),
                     "b": create_custom_field_getter(seq_nhwc.layers[0], "b")}
    seq_nchw = func(name="NCHW", data_format="NCHW",
                    custom_getter=custom_getter)
    x_transpose = tf.transpose(x, perm=(0, 3, 1, 2))
    result_nchw = tf.transpose(seq_nchw(x_transpose), perm=(0, 2, 3, 1))

    self.checkEquality(result_nhwc, result_nchw)


</source>
</class>

</clones>

<clones>
<systeminfo processor="nicad6" system="deepchem-2.3.0" granularity="functions-blind" threshold="30%" minlines="10" maxlines="2500"/>
<cloneinfo npcs="2180" npairs="580"/>
<runinfo ncompares="236494" cputime="149522"/>
<classinfo nclasses="125"/>

<class classid="1" nclones="14" nlines="15" similarity="72">
<source file="systems/deepchem-2.3.0/contrib/DiabeticRetinopathy/model.py" startline="24" endline="60" pcid="26">
class DRModel(TensorGraph):

  def __init__(self,
               n_tasks=1,
               image_size=512,
               n_downsample=6,
               n_init_kernel=16,
               n_fully_connected=[1024],
               n_classes=5,
               augment=False,
               **kwargs):
    """
    Parameters
    ----------
    n_tasks: int
      Number of tasks
    image_size: int
      Resolution of the input images(square)
    n_downsample: int
      Downsample ratio in power of 2
    n_init_kernel: int
      Kernel size for the first convolutional layer
    n_fully_connected: list of int
      Shape of FC layers after convolutions
    n_classes: int
      Number of classes to predict (only used in classification mode)
    augment: bool
      If to use data augmentation
    """
    self.n_tasks = n_tasks
    self.image_size = image_size
    self.n_downsample = n_downsample
    self.n_init_kernel = n_init_kernel
    self.n_fully_connected = n_fully_connected
    self.n_classes = n_classes
    self.augment = augment
    super(DRModel, self).__init__(**kwargs)
</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/graph_layers.py" startline="216" endline="245" pcid="1016">
  def __init__(self,
               n_embedding=30,
               n_outputs=100,
               layer_sizes=[100],
               output_activation=True,
               init='glorot_uniform',
               activation='tanh',
               **kwargs):
    """
        Parameters
        ----------
        n_embedding: int, optional
          Number of features for each atom
        n_outputs: int, optional
          Number of features for each molecule(output)
        layer_sizes: list of int, optional(default=[1000])
          Structure of hidden layer(s)
        init: str, optional
          Weight initialization for filters.
        activation: str, optional
          Activation function applied
        """
    self.n_embedding = n_embedding
    self.n_outputs = n_outputs
    self.layer_sizes = layer_sizes
    self.output_activation = output_activation
    self.init = init
    self.activation = activation
    super(DTNNGather, self).__init__(**kwargs)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/graph_layers.py" startline="334" endline="371" pcid="1023">
  def __init__(self,
               n_graph_feat=30,
               n_outputs=30,
               max_atoms=50,
               layer_sizes=[100],
               init='glorot_uniform',
               activation='relu',
               dropout=None,
               **kwargs):
    """
        Parameters
        ----------
        n_graph_feat: int, optional
          Number of features for each atom.
        n_outputs: int, optional
          Number of features for each molecule.
        max_atoms: int, optional
          Maximum number of atoms in molecules.
        layer_sizes: list of int, optional
          List of hidden layer size(s):
          length of this list represents the number of hidden layers,
          and each element is the width of corresponding hidden layer.
        init: str, optional
          Weight initialization for filters.
        activation: str, optional
          Activation function applied.
        dropout: float, optional
          Dropout probability in the hidden layer(s).
        """
    super(DAGGather, self).__init__(**kwargs)
    self.init = init
    self.activation = activation
    self.layer_sizes = layer_sizes
    self.dropout = dropout
    self.max_atoms = max_atoms
    self.n_graph_feat = n_graph_feat
    self.n_outputs = n_outputs

</source>
<source file="systems/deepchem-2.3.0/contrib/DeepMHC/deepmhc.py" startline="24" endline="40" pcid="355">
  def __init__(self,
               batch_size=64,
               pad_length=13,
               dropout_p=0.5,
               num_amino_acids=20,
               mode="regression",
               **kwargs):

    assert mode in ["regression", "classification"]
    self.mode = mode
    self.batch_size = batch_size
    self.dropout_p = dropout_p
    self.pad_length = pad_length
    self.num_amino_acids = num_amino_acids
    super(DeepMHC, self).__init__(**kwargs)
    self._build_graph()

</source>
<source file="systems/deepchem-2.3.0/contrib/hagcn/hagcn_model.py" startline="24" endline="63" pcid="303">
  def __init__(self,
               max_nodes,
               num_node_features,
               n_tasks=1,
               k_max=1,
               task_mode='graph',
               combine_method='linear',
               **kwargs):
    """
      Parameters
      ----------
      max_nodes: int
        Maximum number of nodes (atoms) graphs in dataset can have
      num_node_features: int
        Number of features per node
      atoms: list
        List of atoms available across train, valid, test
      k_max: int, optional
        Largest k-hop neighborhood per atom
      batch_size: int, optional
        Batch size used
      task_mode: str, optional
        Whether the model is used for node based tasks or edge based tasks or graph tasks
      combine_method: str, optional
        Combining the inputs for the AdaptiveFilterLayer
    """

    if task_mode not in ['graph', 'node', 'edge']:
      raise ValueError('task_mode must be one of graph, node, edge')

    self.k_max = k_max
    self.n_tasks = n_tasks
    self.max_nodes = max_nodes
    self.num_node_features = num_node_features
    self.task_mode = task_mode
    self.combine_method = combine_method
    super(HAGCN, self).__init__(**kwargs)

    self._build()

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/graph_layers.py" startline="178" endline="205" pcid="1014">
  def __init__(self,
               n_embedding=30,
               n_distance=100,
               n_hidden=60,
               init='glorot_uniform',
               activation='tanh',
               **kwargs):
    """
        Parameters
        ----------
        n_embedding: int, optional
          Number of features for each atom
        n_distance: int, optional
          granularity of distance matrix
        n_hidden: int, optional
          Number of nodes in hidden layer
        init: str, optional
          Weight initialization for filters.
        activation: str, optional
          Activation function applied
        """
    self.n_embedding = n_embedding
    self.n_distance = n_distance
    self.n_hidden = n_hidden
    self.init = init
    self.activation = activation
    super(DTNNStep, self).__init__(**kwargs)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/graph_layers.py" startline="105" endline="138" pcid="1010">
  def __init__(self,
               batch_size,
               n_input=128,
               gaussian_expand=False,
               init='glorot_uniform',
               activation='tanh',
               epsilon=1e-3,
               momentum=0.99,
               **kwargs):
    """
    Parameters
    ----------
    batch_size: int
      number of molecules in a batch
    n_input: int, optional
      number of features for each input molecule
    gaussian_expand: boolean. optional
      Whether to expand each dimension of atomic features by gaussian histogram
    init: str, optional
      Weight initialization for filters.
    activation: str, optional
      Activation function applied

    """
    self.n_input = n_input
    self.batch_size = batch_size
    self.gaussian_expand = gaussian_expand
    self.init = init
    self.activation = activation
    self.epsilon = epsilon
    self.momentum = momentum
    self.W, self.b = None, None
    super(WeaveGather, self).__init__(**kwargs)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/graph_layers.py" startline="272" endline="317" pcid="1020">
  def __init__(self,
               n_graph_feat=30,
               n_atom_feat=75,
               max_atoms=50,
               layer_sizes=[100],
               init='glorot_uniform',
               activation='relu',
               dropout=None,
               batch_size=64,
               **kwargs):
    """
        Parameters
        ----------
        n_graph_feat: int, optional
          Number of features for each node(and the whole grah).
        n_atom_feat: int, optional
          Number of features listed per atom.
        max_atoms: int, optional
          Maximum number of atoms in molecules.
        layer_sizes: list of int, optional(default=[100])
          List of hidden layer size(s):
          length of this list represents the number of hidden layers,
          and each element is the width of corresponding hidden layer.
        init: str, optional
          Weight initialization for filters.
        activation: str, optional
          Activation function applied.
        dropout: float, optional
          Dropout probability in hidden layer(s).
        batch_size: int, optional
          number of molecules in a batch.
        """
    super(DAGLayer, self).__init__(**kwargs)

    self.init = init
    self.activation = activation
    self.layer_sizes = layer_sizes
    self.dropout = dropout
    self.max_atoms = max_atoms
    self.batch_size = batch_size
    self.n_inputs = n_atom_feat + (self.max_atoms - 1) * n_graph_feat
    # number of inputs each step
    self.n_graph_feat = n_graph_feat
    self.n_outputs = n_graph_feat
    self.n_atom_feat = n_atom_feat

</source>
<source file="systems/deepchem-2.3.0/contrib/hagcn/hagcn_layers.py" startline="22" endline="53" pcid="293">
  def __init__(self,
               num_nodes,
               num_node_features,
               batch_size=64,
               init='glorot_uniform',
               combine_method='linear',
               **kwargs):
    """
      Parameters
      ----------
      num_nodes: int
        Number of nodes in the graph
      num_node_features: int
        Number of features per node in the graph
      batch_size: int, optional
        Batch size used for training
      init: str, optional
        Initialization method for the weights
      combine_method: str, optional
        How to combine adjacency matrix and node features

    """

    if combine_method not in ['linear', 'prod']:
      raise ValueError('Combine method needs to be one of linear or product')
    self.num_nodes = num_nodes
    self.num_node_features = num_node_features
    self.batch_size = batch_size
    self.init = initializations.get(init)
    self.combine_method = combine_method
    super(AdaptiveFilter, self).__init__(**kwargs)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/symmetry_functions.py" startline="416" endline="430" pcid="1048">
    flags = in_layers[1].out_tensor

    out_tensor = tf.reduce_sum(out_tensor * flags[:, :, 0:1], axis=1)
    self.out_tensor = out_tensor

    return out_tensor


class AtomicDifferentiatedDense(Layer):
  """ Separate Dense module for different atoms """

  def __init__(self,
               max_atoms,
               out_channels,
               atom_number_cases=[1, 6, 7, 8],
</source>
<source file="systems/deepchem-2.3.0/contrib/hagcn/hagcn_layers.py" startline="101" endline="129" pcid="298">
  def __init__(self,
               num_nodes,
               num_node_features,
               batch_size=64,
               init='glorot_uniform',
               **kwargs):
    """
      Parameters
      ----------
      num_nodes: int
        Number of nodes in the graph
      num_node_features: int
        Number of features per node in the graph
      batch_size: int, optional
        Batch size used for training
      init: str, optional
        Initialization method for the weights
      combine_method: str, optional
        How to combine adjacency matrix and node features

    """

    self.num_nodes = num_nodes
    self.num_node_features = num_node_features
    self.batch_size = batch_size
    self.init = initializations.get(init)

    super(KOrderGraphConv, self).__init__(**kwargs)

</source>
<source file="systems/deepchem-2.3.0/contrib/DiabeticRetinopathy/model.py" startline="190" endline="217" pcid="33">
class DRAugment(Layer):

  def __init__(self,
               augment,
               batch_size,
               distort_color=True,
               central_crop=True,
               size=(512, 512),
               **kwargs):
    """
    Parameters
    ----------
    augment: bool
      If to use data augmentation
    batch_size: int
      Number of images in the batch
    distort_color: bool
      If to apply random distortion on the color
    central_crop: bool
      If to randomly crop the sample around the center
    size: int
      Resolution of the input images(square)
    """
    self.augment = augment
    self.batch_size = batch_size
    self.distort_color = distort_color
    self.central_crop = central_crop
    self.size = size
</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/graph_layers.py" startline="388" endline="412" pcid="1026">
  def __init__(self,
               T,
               message_fn='enn',
               update_fn='gru',
               n_hidden=100,
               **kwargs):
    """
    Parameters
    ----------
    T: int
      Number of message passing steps
    message_fn: str, optional
      message function in the model
    update_fn: str, optional
      update function in the model
    n_hidden: int, optional
      number of hidden units in the passing phase
    """

    self.T = T
    self.message_fn = message_fn
    self.update_fn = update_fn
    self.n_hidden = n_hidden
    super(MessagePassing, self).__init__(**kwargs)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/models/symmetry_function_regression.py" startline="27" endline="51" pcid="594">
class BPSymmetryFunctionRegression(TensorGraph):

  def __init__(self,
               n_tasks,
               max_atoms,
               n_feat=96,
               layer_structures=[128, 64],
               **kwargs):
    """
    Parameters
    ----------
    n_tasks: int
      Number of tasks
    max_atoms: int
      Maximum number of atoms in the dataset
    n_hidden: int, optional
      Number of hidden units in the readout function
    """
    self.n_tasks = n_tasks
    self.max_atoms = max_atoms
    self.n_feat = n_feat
    self.layer_structures = layer_structures

    super(BPSymmetryFunctionRegression, self).__init__(**kwargs)

</source>
</class>

<class classid="2" nclones="2" nlines="16" similarity="87">
<source file="systems/deepchem-2.3.0/contrib/torch/pytorch_graphconv.py" startline="152" endline="176" pcid="47">
    def __init__(self, net, lr, weight_decay):
        """
        net: an instance of class GraphConvolution
        lr: float, learning rate 
        weight_decay: float
        """
        self.net = net
        self.criterion = nn.CrossEntropyLoss()
        self.input_x = torch.FloatTensor(-1, self.net.max_n_atoms, self.net.n_atom_types)
        self.input_g = torch.FloatTensor(-1, self.net.max_n_atoms, self.net.max_n_atoms)
        self.label = torch.FloatTensor(-1)
        
        self.net.cuda()
        self.criterion = nn.CrossEntropyLoss()
        self.criterion.cuda()
        
        self.input_x, self.input_g, self.label = self.input_x.cuda(), self.input_g.cuda(), self.label.cuda()

        self.lr = lr
        self.weight_decay = weight_decay
        # setup optimizer
        self.optimizer = optim.Adam(self.net.parameters(),
                               lr=self.lr,
                               weight_decay=self.weight_decay)

</source>
<source file="systems/deepchem-2.3.0/contrib/torch/pytorch_graphconv.py" startline="307" endline="335" pcid="50">
    def __init__(self, net, lr, weight_decay, n_tasks):
        """
        net: an instance of class GraphConvolution
        lr: float, learning rate 
        weight_decay: float
        n_tasks: int, number of tasks
        """
        self.net = net
        self.criterion = nn.CrossEntropyLoss()
        self.input_x = torch.FloatTensor(-1, self.net.max_n_atoms, self.net.n_atom_types)
        self.input_g = torch.FloatTensor(-1, self.net.max_n_atoms, self.net.max_n_atoms)
        self.label = torch.FloatTensor(-1)
        
        self.net.cuda()

        self.criterion = nn.CrossEntropyLoss()
        self.criterion.cuda()
        
        self.input_x, self.input_g, self.label = self.input_x.cuda(), self.input_g.cuda(), self.label.cuda()

        self.lr = lr
        self.weight_decay = weight_decay
        # setup optimizer
        self.optimizer = optim.Adam(self.net.parameters(),
                               lr=self.lr,
                               weight_decay=self.weight_decay)

        self.n_tasks = n_tasks

</source>
</class>

<class classid="3" nclones="2" nlines="24" similarity="100">
<source file="systems/deepchem-2.3.0/contrib/torch/pytorch_graphconv.py" startline="177" endline="216" pcid="48">
    def train_epoch(self, train_features, y_train, batch_size=32,
                    shuffle_train_inds=True):
        """
        train_features: list of dictionaries. each dictionary represents one sample feature. 
            key "x" maps to max_n_atoms x p feature matrix. key "g" maps to square adjacency matrix 
        y_train: numpy array of labels 
        """

        train_inds = range(0, len(train_features))
        if shuffle_train_inds:
            random.shuffle(train_inds)

        for b in range(0, len(train_inds)/batch_size):
            batch_inds = [train_inds[idx] for idx in range(b*batch_size, (b+1)*batch_size)]
            
            train_x_batch = np.concatenate([np.expand_dims(train_features[idx]["x"], 0) for idx in batch_inds], axis=0)
            train_g_batch = np.concatenate([np.expand_dims(train_features[idx]["g"], 0) for idx in batch_inds], axis=0)

            xb = torch.from_numpy(train_x_batch.astype(np.float32)).cuda()
            gb = torch.from_numpy(train_g_batch.astype(np.float32)).cuda()
            yb = torch.from_numpy(y_train[batch_inds].astype(np.float32)).cuda()

            self.net.train()
            self.net.zero_grad()
            
            self.input_x.resize_as_(xb).copy_(xb)
            self.input_g.resize_as_(gb).copy_(gb)
            self.label.resize_as_(yb).copy_(yb)
            
            input_xv = Variable(self.input_x)
            input_gv = Variable(self.input_g)
            label_v = Variable(self.label)

            output = self.net(input_gv, input_xv)
            
            err = self.criterion(output, label_v)
            err.backward()
            
            self.optimizer.step()

</source>
<source file="systems/deepchem-2.3.0/contrib/torch/pytorch_graphconv.py" startline="359" endline="392" pcid="52">
    def train_epoch(self, train_features, y_train, batch_size=32,
                    shuffle_train_inds=True):
        train_inds = range(0, len(train_features))
        if shuffle_train_inds:
            random.shuffle(train_inds)

        for b in range(0, len(train_inds)/batch_size):
            batch_inds = [train_inds[idx] for idx in range(b*batch_size, (b+1)*batch_size)]
            
            train_x_batch = np.concatenate([np.expand_dims(train_features[idx]["x"], 0) for idx in batch_inds], axis=0)
            train_g_batch = np.concatenate([np.expand_dims(train_features[idx]["g"], 0) for idx in batch_inds], axis=0)

            xb = torch.from_numpy(train_x_batch.astype(np.float32)).cuda()
            gb = torch.from_numpy(train_g_batch.astype(np.float32)).cuda()
            yb = torch.from_numpy(y_train[batch_inds].astype(np.float32)).cuda()

            self.net.train()
            self.net.zero_grad()
            
            self.input_x.resize_as_(xb).copy_(xb)
            self.input_g.resize_as_(gb).copy_(gb)
            self.label.resize_as_(yb).copy_(yb)
            
            input_xv = Variable(self.input_x)
            input_gv = Variable(self.input_g)
            label_v = Variable(self.label)

            output = self.net(input_gv, input_xv)
            
            err = self.multitask_loss(output, label_v)
            err.backward()
            
            self.optimizer.step()

</source>
</class>

<class classid="4" nclones="2" nlines="61" similarity="100">
<source file="systems/deepchem-2.3.0/contrib/torch/pytorch_graphconv.py" startline="217" endline="300" pcid="49">
    def evaluate(self, train_features,
                       test_features,
                       y_train,
                       y_test, 
                       transformer,
                       batch_size=32):
        
        self.net.eval()
        print("TRAIN:")
        
        o = []
        l = []

        train_inds = range(0, len(train_features))

        for b in range(0, len(train_features)/batch_size):
            batch_inds = [train_inds[idx] for idx in range(b*batch_size, (b+1)*batch_size)]
            
            train_x_batch = np.concatenate([np.expand_dims(train_features[idx]["x"], 0) for idx in batch_inds], axis=0)
            train_g_batch = np.concatenate([np.expand_dims(train_features[idx]["g"], 0) for idx in batch_inds], axis=0)

            xb = torch.from_numpy(train_x_batch.astype(np.float32)).cuda()
            gb = torch.from_numpy(train_g_batch.astype(np.float32)).cuda()
            
            self.input_x.resize_as_(xb).copy_(xb)
            self.input_g.resize_as_(gb).copy_(gb)
            
            input_xv = Variable(self.input_x)
            input_gv = Variable(self.input_g)

            output = self.net(input_gv, input_xv)
            
            if transformer is not None:
                o.append(transformer.inverse_transform(output.data.cpu().numpy().reshape((-1,1))).flatten())
                l.append(transformer.inverse_transform(y_train[batch_inds].reshape((-1,1))).flatten())
            else:
                o.append(output.data.cpu().numpy().reshape((-1,1)).flatten())
                l.append(y_train[batch_inds].reshape((-1,1)).flatten())

        o = np.concatenate(o)
        l = np.concatenate(l)
        print("RMSE:")
        print(np.sqrt(np.mean(np.square(l-o))))
        print("ROC AUC:")
        print(roc_auc_score(l, o))
        
        o = []
        l = []

        print("TEST:")
        test_inds = range(0, len(test_features))

        for b in range(0, len(test_features)/batch_size):
            batch_inds = [test_inds[idx] for idx in range(b*batch_size, (b+1)*batch_size)]
            
            test_x_batch = np.concatenate([np.expand_dims(test_features[idx]["x"], 0) for idx in batch_inds], axis=0)
            test_g_batch = np.concatenate([np.expand_dims(test_features[idx]["g"], 0) for idx in batch_inds], axis=0)

            xb = torch.from_numpy(test_x_batch.astype(np.float32)).cuda()
            gb = torch.from_numpy(test_g_batch.astype(np.float32)).cuda()
            
            self.input_x.resize_as_(xb).copy_(xb)
            self.input_g.resize_as_(gb).copy_(gb)
            
            input_xv = Variable(self.input_x)
            input_gv = Variable(self.input_g)

            output = self.net(input_gv, input_xv)
            
            if transformer is not None:
                o.append(transformer.inverse_transform(output.data.cpu().numpy().reshape((-1,1))).flatten())
                l.append(transformer.inverse_transform(y_test[batch_inds].reshape((-1,1))).flatten())
            else:
                o.append(output.data.cpu().numpy().reshape((-1,1)).flatten())
                l.append(y_test[batch_inds].reshape((-1,1)).flatten())

        o = np.concatenate(o)
        l = np.concatenate(l)
        print("RMSE:")
        print(np.sqrt(np.mean(np.square(l-o))))
        print("ROC AUC:")
        print(roc_auc_score(l, o))


</source>
<source file="systems/deepchem-2.3.0/contrib/torch/pytorch_graphconv.py" startline="393" endline="474" pcid="53">
    def evaluate(self, train_features,
                       test_features,
                       y_train,
                       y_test, 
                       transformer,
                       batch_size=32):
        
        self.net.eval()
        print("TRAIN:")
        
        o = []
        l = []

        train_inds = range(0, len(train_features))

        for b in range(0, len(train_features)/batch_size):
            batch_inds = [train_inds[idx] for idx in range(b*batch_size, (b+1)*batch_size)]
            
            train_x_batch = np.concatenate([np.expand_dims(train_features[idx]["x"], 0) for idx in batch_inds], axis=0)
            train_g_batch = np.concatenate([np.expand_dims(train_features[idx]["g"], 0) for idx in batch_inds], axis=0)

            xb = torch.from_numpy(train_x_batch.astype(np.float32)).cuda()
            gb = torch.from_numpy(train_g_batch.astype(np.float32)).cuda()
            
            self.input_x.resize_as_(xb).copy_(xb)
            self.input_g.resize_as_(gb).copy_(gb)
            
            input_xv = Variable(self.input_x)
            input_gv = Variable(self.input_g)

            output = self.net(input_gv, input_xv)
            
            if transformer is not None:
                o.append(transformer.inverse_transform(output.data.cpu().numpy().reshape((-1,1))).flatten())
                l.append(transformer.inverse_transform(y_train[batch_inds].reshape((-1,1))).flatten())
            else:
                o.append(output.data.cpu().numpy().reshape((-1,1)).flatten())
                l.append(y_train[batch_inds].reshape((-1,1)).flatten())

        o = np.concatenate(o)
        l = np.concatenate(l)
        print("RMSE:")
        print(np.sqrt(np.mean(np.square(l-o))))
        print("ROC AUC:")
        print(roc_auc_score(l, o))
        
        o = []
        l = []

        print("TEST:")
        test_inds = range(0, len(test_features))

        for b in range(0, len(test_features)/batch_size):
            batch_inds = [test_inds[idx] for idx in range(b*batch_size, (b+1)*batch_size)]
            
            test_x_batch = np.concatenate([np.expand_dims(test_features[idx]["x"], 0) for idx in batch_inds], axis=0)
            test_g_batch = np.concatenate([np.expand_dims(test_features[idx]["g"], 0) for idx in batch_inds], axis=0)

            xb = torch.from_numpy(test_x_batch.astype(np.float32)).cuda()
            gb = torch.from_numpy(test_g_batch.astype(np.float32)).cuda()
            
            self.input_x.resize_as_(xb).copy_(xb)
            self.input_g.resize_as_(gb).copy_(gb)
            
            input_xv = Variable(self.input_x)
            input_gv = Variable(self.input_g)

            output = self.net(input_gv, input_xv)
            
            if transformer is not None:
                o.append(transformer.inverse_transform(output.data.cpu().numpy().reshape((-1,1))).flatten())
                l.append(transformer.inverse_transform(y_test[batch_inds].reshape((-1,1))).flatten())
            else:
                o.append(output.data.cpu().numpy().reshape((-1,1)).flatten())
                l.append(y_test[batch_inds].reshape((-1,1)).flatten())

        o = np.concatenate(o)
        l = np.concatenate(l)
        print("RMSE:")
        print(np.sqrt(np.mean(np.square(l-o))))
        print("ROC AUC:")
        print(roc_auc_score(l, o))
</source>
</class>

<class classid="5" nclones="2" nlines="17" similarity="77">
<source file="systems/deepchem-2.3.0/contrib/torch/torch_model.py" startline="177" endline="203" pcid="58">
  def predict(self, dataset, transformers=[]):
    """
    Uses self to make predictions on provided Dataset object.

    Returns:
      y_pred: numpy ndarray of shape (n_samples,)
    """
    y_preds = []
    n_tasks = self.n_tasks
    for (X_batch, _, _, ids_batch) in dataset.iterbatches(
        self.batch_size, deterministic=True):
      n_samples = len(X_batch)
      y_pred_batch = self.predict_on_batch(X_batch)
      assert y_pred_batch.shape == (n_samples, n_tasks)
      y_pred_batch = undo_transforms(y_pred_batch, transformers)
      y_preds.append(y_pred_batch)
    y_pred = np.vstack(y_preds)

    # The iterbatches does padding with zero-weight examples on the last batch.
    # Remove padded examples.
    n_samples = len(dataset)
    y_pred = np.reshape(y_pred, (n_samples, n_tasks))
    # Special case to handle singletasks.
    if n_tasks == 1:
      y_pred = np.reshape(y_pred, (n_samples,))
    return y_pred

</source>
<source file="systems/deepchem-2.3.0/contrib/tensorflow_models/__init__.py" startline="520" endline="550" pcid="280">
  def predict(self, dataset, transformers=[]):
    """
    Uses self to make predictions on provided Dataset object.

    Returns:
      y_pred: numpy ndarray of shape (n_samples,)
    """
    y_preds = []
    n_tasks = self.get_num_tasks()
    ind = 0

    for (X_batch, _, _, ids_batch) in dataset.iterbatches(
        self.batch_size, deterministic=True):
      n_samples = len(X_batch)
      y_pred_batch = self.predict_on_batch(X_batch)
      # Discard any padded predictions
      y_pred_batch = y_pred_batch[:n_samples]
      y_pred_batch = np.reshape(y_pred_batch, (n_samples, n_tasks))
      y_pred_batch = undo_transforms(y_pred_batch, transformers)
      y_preds.append(y_pred_batch)
    y_pred = np.vstack(y_preds)

    # The iterbatches does padding with zero-weight examples on the last batch.
    # Remove padded examples.
    n_samples = len(dataset)
    y_pred = np.reshape(y_pred, (n_samples, n_tasks))
    # Special case to handle singletasks.
    if n_tasks == 1:
      y_pred = np.reshape(y_pred, (n_samples,))
    return y_pred

</source>
</class>

<class classid="6" nclones="2" nlines="16" similarity="81">
<source file="systems/deepchem-2.3.0/contrib/torch/torch_model.py" startline="204" endline="221" pcid="59">
  def predict_proba(self, dataset, transformers=[], n_classes=2):
    y_preds = []
    n_tasks = self.n_tasks
    for (X_batch, y_batch, w_batch, ids_batch) in dataset.iterbatches(
        self.batch_size, deterministic=True):
      n_samples = len(X_batch)
      y_pred_batch = self.predict_proba_on_batch(X_batch)
      assert y_pred_batch.shape == (n_samples, n_tasks, n_classes)
      y_pred_batch = undo_transforms(y_pred_batch, transformers)
      y_preds.append(y_pred_batch)
    y_pred = np.vstack(y_preds)
    # The iterbatches does padding with zero-weight examples on the last batch.
    # Remove padded examples.
    n_samples = len(dataset)
    y_pred = y_pred[:n_samples]
    y_pred = np.reshape(y_pred, (n_samples, n_tasks, n_classes))
    return y_pred

</source>
<source file="systems/deepchem-2.3.0/contrib/tensorflow_models/__init__.py" startline="551" endline="599" pcid="281">
  def predict_proba(self, dataset, transformers=[], n_classes=2):
    """
    TODO: Do transformers even make sense here?

    Returns:
      y_pred: numpy ndarray of shape (n_samples, n_classes*n_tasks)
    """
    y_preds = []
    n_tasks = self.get_num_tasks()

    for (X_batch, y_batch, w_batch, ids_batch) in dataset.iterbatches(
        self.batch_size, deterministic=True):
      n_samples = len(X_batch)
      y_pred_batch = self.predict_proba_on_batch(X_batch)
      y_pred_batch = y_pred_batch[:n_samples]
      y_pred_batch = np.reshape(y_pred_batch, (n_samples, n_tasks, n_classes))
      y_pred_batch = undo_transforms(y_pred_batch, transformers)
      y_preds.append(y_pred_batch)
    y_pred = np.vstack(y_preds)
    # The iterbatches does padding with zero-weight examples on the last batch.
    # Remove padded examples.
    n_samples = len(dataset)
    y_pred = y_pred[:n_samples]
    y_pred = np.reshape(y_pred, (n_samples, n_tasks, n_classes))
    return y_pred

  # TODO(rbharath): Verify this can be safely removed.
  #def evaluate(self, dataset, metrics, transformers=[]):
  #  """
  #  Evaluates the performance of this model on specified dataset.
  #
  #  Parameters
  #  ----------
  #  dataset: dc.data.Dataset
  #    Dataset object.
  #  metric: deepchem.metrics.Metric
  #    Evaluation metric
  #  transformers: list
  #    List of deepchem.transformers.Transformer

  #  Returns
  #  -------
  #  dict
  #    Maps tasks to scores under metric.
  #  """
  #  evaluator = Evaluator(self, dataset, transformers)
  #  scores = evaluator.compute_model_performance(metrics)
  #  return scores

</source>
</class>

<class classid="7" nclones="4" nlines="15" similarity="75">
<source file="systems/deepchem-2.3.0/contrib/vina_model/test_vina_model.py" startline="82" endline="102" pcid="69">
  def test_put_atoms_in_cells(self):
    """Test that atoms can be partitioned into spatial cells."""
    N = 10
    start = 0
    stop = 4
    nbr_cutoff = 1
    ndim = 3
    k = 5
    # The number of cells which we should theoretically have
    n_cells = ((stop - start) / nbr_cutoff)**ndim

    with self.session() as sess:
      cells = get_cells(start, stop, nbr_cutoff, ndim=ndim)
      coords = np.random.rand(N, ndim)
      _, atoms_in_cells = put_atoms_in_cells(coords, cells, N, n_cells, ndim, k)
      atoms_in_cells = atoms_in_cells.eval()
      assert len(atoms_in_cells) == n_cells
      # Each atom neighbors tensor should be (k, ndim) shaped.
      for atoms in atoms_in_cells:
        assert atoms.shape == (k, ndim)

</source>
<source file="systems/deepchem-2.3.0/contrib/vina_model/test_vina_model.py" startline="103" endline="124" pcid="70">
  def test_compute_neighbor_cells(self):
    """Test that indices of neighboring cells can be computed."""
    N = 10
    start = 0
    stop = 4
    nbr_cutoff = 1
    ndim = 3
    # The number of cells which we should theoretically have
    n_cells = ((stop - start) / nbr_cutoff)**ndim

    # TODO(rbharath): The test below only checks that shapes work out.
    # Need to do a correctness implementation vs. a simple CPU impl.

    with self.session() as sess:
      cells = get_cells(start, stop, nbr_cutoff, ndim=ndim)
      nbr_cells = compute_neighbor_cells(cells, ndim, n_cells)
      nbr_cells = nbr_cells.eval()
      assert len(nbr_cells) == n_cells
      nbr_cells = [nbr_cell for nbr_cell in nbr_cells]
      for nbr_cell in nbr_cells:
        assert nbr_cell.shape == (26,)

</source>
<source file="systems/deepchem-2.3.0/contrib/vina_model/test_vina_model.py" startline="147" endline="167" pcid="72">
  def test_get_cells_for_atoms(self):
    """Test that atoms are placed in the correct cells."""
    N = 10
    start = 0
    stop = 4
    nbr_cutoff = 1
    ndim = 3
    k = 5
    # The number of cells which we should theoretically have
    n_cells = ((stop - start) / nbr_cutoff)**ndim

    # TODO(rbharath): The test below only checks that shapes work out.
    # Need to do a correctness implementation vs. a simple CPU impl.

    with self.session() as sess:
      cells = get_cells(start, stop, nbr_cutoff, ndim=ndim)
      coords = np.random.rand(N, ndim)
      cells_for_atoms = get_cells_for_atoms(coords, cells, N, n_cells, ndim)
      cells_for_atoms = cells_for_atoms.eval()
      assert cells_for_atoms.shape == (N, 1)

</source>
<source file="systems/deepchem-2.3.0/contrib/vina_model/test_vina_model.py" startline="125" endline="146" pcid="71">
  def test_compute_closest_neighbors(self):
    """Test that closest neighbors can be computed properly"""
    N = 10
    start = 0
    stop = 4
    nbr_cutoff = 1
    ndim = 3
    k = 5
    # The number of cells which we should theoretically have
    n_cells = ((stop - start) / nbr_cutoff)**ndim

    # TODO(rbharath): The test below only checks that shapes work out.
    # Need to do a correctness implementation vs. a simple CPU impl.

    with self.session() as sess:
      cells = get_cells(start, stop, nbr_cutoff, ndim=ndim)
      nbr_cells = compute_neighbor_cells(cells, ndim, n_cells)
      coords = np.random.rand(N, ndim)
      _, atoms_in_cells = put_atoms_in_cells(coords, cells, N, n_cells, ndim, k)
      nbrs = compute_closest_neighbors(coords, cells, atoms_in_cells, nbr_cells,
                                       N, n_cells)

</source>
</class>

<class classid="8" nclones="3" nlines="49" similarity="76">
<source file="systems/deepchem-2.3.0/contrib/rl/test_mcts.py" startline="15" endline="89" pcid="112">
  def test_roulette(self):
    """Test training a policy for the roulette environment."""

    # This is modeled after the Roulette-v0 environment from OpenAI Gym.
    # The player can bet on any number from 0 to 36, or walk away (which ends the
    # game).  The average reward for any bet is slightly negative, so the best
    # strategy is to walk away.

    class RouletteEnvironment(dc.rl.Environment):

      def __init__(self):
        super(RouletteEnvironment, self).__init__([(1,)], 38)
        self._state = [np.array([0])]

      def step(self, action):
        if action == 37:
          self._terminated = True  # Walk away.
          return 0.0
        wheel = np.random.randint(37)
        if wheel == 0:
          if action == 0:
            return 35.0
          return -1.0
        if action != 0 and wheel % 2 == action % 2:
          return 1.0
        return -1.0

      def reset(self):
        self._terminated = False

    env = RouletteEnvironment()

    # This policy just learns a constant probability for each action, and a constant for the value.

    class TestPolicy(dc.rl.Policy):

      def create_layers(self, state, **kwargs):
        action = Variable(np.ones(env.n_actions))
        output = SoftMax(
            in_layers=[Reshape(in_layers=[action], shape=(-1, env.n_actions))])
        value = Variable([0.0])
        return {'action_prob': output, 'value': value}

    # Optimize it.

    mcts = dc.rl.MCTS(
        env,
        TestPolicy(),
        max_search_depth=5,
        n_search_episodes=200,
        optimizer=Adam(learning_rate=0.005))
    mcts.fit(10, steps_per_iteration=50, epochs_per_iteration=50)

    # It should have learned that the expected value is very close to zero, and that the best
    # action is to walk away.

    action_prob, value = mcts.predict([[0]])
    assert -0.5 < value[0] < 0.5
    assert action_prob.argmax() == 37
    assert mcts.select_action([[0]], deterministic=True) == 37

    # Verify that we can create a new MCTS object, reload the parameters from the first one, and
    # get the same result.

    new_mcts = dc.rl.MCTS(env, TestPolicy(), model_dir=mcts._graph.model_dir)
    new_mcts.restore()
    action_prob2, value2 = new_mcts.predict([[0]])
    assert value2 == value

    # Do the same thing, only using the "restore" argument to fit().

    new_mcts = dc.rl.MCTS(env, TestPolicy(), model_dir=mcts._graph.model_dir)
    new_mcts.fit(0, restore=True)
    action_prob2, value2 = new_mcts.predict([[0]])
    assert value2 == value
</source>
<source file="systems/deepchem-2.3.0/deepchem/rl/tests/test_ppo.py" startline="15" endline="100" pcid="1539">
  def test_roulette(self):
    """Test training a policy for the roulette environment."""

    # This is modeled after the Roulette-v0 environment from OpenAI Gym.
    # The player can bet on any number from 0 to 36, or walk away (which ends the
    # game).  The average reward for any bet is slightly negative, so the best
    # strategy is to walk away.

    class RouletteEnvironment(dc.rl.Environment):

      def __init__(self):
        super(RouletteEnvironment, self).__init__([(1,)], 38)
        self._state = [np.array([0])]

      def step(self, action):
        if action == 37:
          self._terminated = True  # Walk away.
          return 0.0
        wheel = np.random.randint(37)
        if wheel == 0:
          if action == 0:
            return 35.0
          return -1.0
        if action != 0 and wheel % 2 == action % 2:
          return 1.0
        return -1.0

      def reset(self):
        self._terminated = False

    env = RouletteEnvironment()

    # This policy just learns a constant probability for each action, and a constant for the value.

    class TestPolicy(dc.rl.Policy):

      def __init__(self):
        super(TestPolicy, self).__init__(['action_prob', 'value'])

      def create_model(self, **kwargs):

        class TestModel(tf.keras.Model):

          def __init__(self):
            super(TestModel, self).__init__(**kwargs)
            self.action = tf.Variable(np.ones(env.n_actions, np.float32))
            self.value = tf.Variable([0.0], tf.float32)

          def call(self, inputs, **kwargs):
            prob = tf.nn.softmax(tf.reshape(self.action, (-1, env.n_actions)))
            return (prob, self.value)

        return TestModel()

    # Optimize it.

    ppo = dc.rl.PPO(
        env,
        TestPolicy(),
        max_rollout_length=20,
        optimizer=Adam(learning_rate=0.003))
    ppo.fit(80000)

    # It should have learned that the expected value is very close to zero, and that the best
    # action is to walk away.

    action_prob, value = ppo.predict([[0]])
    assert -0.8 < value[0] < 0.5
    assert action_prob.argmax() == 37
    assert ppo.select_action([[0]], deterministic=True) == 37

    # Verify that we can create a new PPO object, reload the parameters from the first one, and
    # get the same result.

    new_ppo = dc.rl.PPO(env, TestPolicy(), model_dir=ppo._model.model_dir)
    new_ppo.restore()
    action_prob2, value2 = new_ppo.predict([[0]])
    assert value2 == value

    # Do the same thing, only using the "restore" argument to fit().

    new_ppo = dc.rl.PPO(env, TestPolicy(), model_dir=ppo._model.model_dir)
    new_ppo.fit(0, restore=True)
    action_prob2, value2 = new_ppo.predict([[0]])
    assert value2 == value

</source>
<source file="systems/deepchem-2.3.0/deepchem/rl/tests/test_a3c.py" startline="15" endline="100" pcid="1560">
  def test_roulette(self):
    """Test training a policy for the roulette environment."""

    # This is modeled after the Roulette-v0 environment from OpenAI Gym.
    # The player can bet on any number from 0 to 36, or walk away (which ends the
    # game).  The average reward for any bet is slightly negative, so the best
    # strategy is to walk away.

    class RouletteEnvironment(dc.rl.Environment):

      def __init__(self):
        super(RouletteEnvironment, self).__init__([(1,)], 38)
        self._state = [np.array([0])]

      def step(self, action):
        if action == 37:
          self._terminated = True  # Walk away.
          return 0.0
        wheel = np.random.randint(37)
        if wheel == 0:
          if action == 0:
            return 35.0
          return -1.0
        if action != 0 and wheel % 2 == action % 2:
          return 1.0
        return -1.0

      def reset(self):
        self._terminated = False

    env = RouletteEnvironment()

    # This policy just learns a constant probability for each action, and a constant for the value.

    class TestPolicy(dc.rl.Policy):

      def __init__(self):
        super(TestPolicy, self).__init__(['action_prob', 'value'])

      def create_model(self, **kwargs):

        class TestModel(tf.keras.Model):

          def __init__(self):
            super(TestModel, self).__init__(**kwargs)
            self.action = tf.Variable(np.ones(env.n_actions, np.float32))
            self.value = tf.Variable([0.0], tf.float32)

          def call(self, inputs, **kwargs):
            prob = tf.nn.softmax(tf.reshape(self.action, (-1, env.n_actions)))
            return (prob, self.value)

        return TestModel()

    # Optimize it.

    a3c = dc.rl.A3C(
        env,
        TestPolicy(),
        max_rollout_length=20,
        optimizer=Adam(learning_rate=0.001))
    a3c.fit(100000)

    # It should have learned that the expected value is very close to zero, and that the best
    # action is to walk away.

    action_prob, value = a3c.predict([[0]])
    assert -0.5 < value[0] < 0.5
    assert action_prob.argmax() == 37
    assert a3c.select_action([[0]], deterministic=True) == 37

    # Verify that we can create a new A3C object, reload the parameters from the first one, and
    # get the same result.

    new_a3c = dc.rl.A3C(env, TestPolicy(), model_dir=a3c._model.model_dir)
    new_a3c.restore()
    action_prob2, value2 = new_a3c.predict([[0]])
    assert value2 == value

    # Do the same thing, only using the "restore" argument to fit().

    new_a3c = dc.rl.A3C(env, TestPolicy(), model_dir=a3c._model.model_dir)
    new_a3c.fit(0, restore=True)
    action_prob2, value2 = new_a3c.predict([[0]])
    assert value2 == value

</source>
</class>

<class classid="9" nclones="2" nlines="10" similarity="80">
<source file="systems/deepchem-2.3.0/contrib/dragonn/simulations.py" startline="95" endline="134" pcid="129">
def simulate_single_motif_detection(motif_name, seq_length,
                                    num_pos, num_neg, GC_fraction):
    """
    Simulates two classes of seqeuences:
        - Positive class sequence with a motif
          embedded anywhere in the sequence
        - Negative class sequence without the motif

    Parameters
    ----------
    motif_name : str
        encode motif name
    seq_length : int
        length of sequence
    num_pos : int
        number of positive class sequences
    num_neg : int
        number of negative class sequences
    GC_fraction : float
        GC fraction in background sequence

    Returns
    -------
    sequence_arr : 1darray
        Array with sequence strings.
    y : 1darray
        Array with positive/negative class labels.
    embedding_arr: 1darray
        Array of embedding objects.
    """
    motif_sequence_arr, positive_embedding_arr = simple_motif_embedding(
        motif_name, seq_length, num_pos, GC_fraction)
    random_sequence_arr, negative_embedding_arr = simple_motif_embedding(
        None, seq_length, num_neg, GC_fraction)
    sequence_arr = np.concatenate((motif_sequence_arr, random_sequence_arr))
    y = np.array([[True]] * num_pos + [[False]] * num_neg)
    embedding_arr = positive_embedding_arr + negative_embedding_arr
    return sequence_arr, y, embedding_arr


</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/dnasim.py" startline="108" endline="147" pcid="2003">
def simulate_single_motif_detection(motif_name, seq_length, num_pos, num_neg,
                                    GC_fraction):
  """
    Simulates two classes of seqeuences:
        - Positive class sequence with a motif
          embedded anywhere in the sequence
        - Negative class sequence without the motif

    Parameters
    ----------
    motif_name : str
        encode motif name
    seq_length : int
        length of sequence
    num_pos : int
        number of positive class sequences
    num_neg : int
        number of negative class sequences
    GC_fraction : float
        GC fraction in background sequence

    Returns
    -------
    sequence_arr : 1darray
        Array with sequence strings.
    y : 1darray
        Array with positive/negative class labels.
    embedding_arr: 1darray
        Array of embedding objects.
    """
  motif_sequence_arr, positive_embedding_arr = simple_motif_embedding(
      motif_name, seq_length, num_pos, GC_fraction)
  random_sequence_arr, negative_embedding_arr = simple_motif_embedding(
      None, seq_length, num_neg, GC_fraction)
  sequence_arr = np.concatenate((motif_sequence_arr, random_sequence_arr))
  y = np.array([[True]] * num_pos + [[False]] * num_neg)
  embedding_arr = positive_embedding_arr + negative_embedding_arr
  return sequence_arr, y, embedding_arr


</source>
</class>

<class classid="10" nclones="2" nlines="32" similarity="79">
<source file="systems/deepchem-2.3.0/contrib/dragonn/utils.py" startline="9" endline="58" pcid="136">
def get_motif_scores(encoded_sequences,
                     motif_names,
                     max_scores=None,
                     return_positions=False,
                     GC_fraction=0.4):
  """
  Computes pwm log odds.

  Parameters
  ----------
  encoded_sequences : 4darray
  motif_names : list of strings
  max_scores : int, optional
  return_positions : boolean, optional
  GC_fraction : float, optional

  Returns
  -------
  (num_samples, num_motifs, seq_length) complete score array by default.
  If max_scores, (num_samples, num_motifs*max_scores) max score array.
  If max_scores and return_positions, (num_samples, 2*num_motifs*max_scores)
  array with max scores and their positions.
  """
  num_samples, _, _, seq_length = encoded_sequences.shape
  scores = np.ones((num_samples, len(motif_names), seq_length))
  for j, motif_name in enumerate(motif_names):
    pwm = loaded_motifs.getPwm(motif_name).getRows().T
    log_pwm = np.log(pwm)
    gc_pwm = 0.5 * np.array(
        [[1 - GC_fraction, GC_fraction, GC_fraction, 1 - GC_fraction]] * len(
            pwm[0])).T
    gc_log_pwm = np.log(gc_pwm)
    scores[:, j, :] = get_pssm_scores(encoded_sequences,
                                      log_pwm) - get_pssm_scores(
                                          encoded_sequences, gc_log_pwm)
  if max_scores is not None:
    sorted_scores = np.sort(scores)[:, :, ::-1][:, :, :max_scores]
    if return_positions:
      sorted_positions = scores.argsort()[:, :, ::-1][:, :, :max_scores]
      return np.concatenate(
          (sorted_scores.reshape((num_samples, len(motif_names) * max_scores)),
           sorted_positions.reshape(
               (num_samples, len(motif_names) * max_scores))),
          axis=1)
    else:
      return sorted_scores.reshape((num_samples, len(motif_names) * max_scores))
  else:
    return scores


</source>
<source file="systems/deepchem-2.3.0/deepchem/metrics/genomic_metrics.py" startline="8" endline="61" pcid="2048">
def get_motif_scores(encoded_sequences,
                     motif_names,
                     max_scores=None,
                     return_positions=False,
                     GC_fraction=0.4):
  """Computes pwm log odds.

  Parameters
  ----------
  encoded_sequences : 4darray
       (N_sequences, N_letters, sequence_length, 1) array
  motif_names : list of strings
  max_scores : int, optional
  return_positions : boolean, optional
  GC_fraction : float, optional

  Returns
  -------
  (N_sequences, num_motifs, seq_length) complete score array by default.
  If max_scores, (N_sequences, num_motifs*max_scores) max score array.
  If max_scores and return_positions, (N_sequences, 2*num_motifs*max_scores)
  array with max scores and their positions.
  """
  import simdna
  from simdna import synthetic
  loaded_motifs = synthetic.LoadedEncodeMotifs(
      simdna.ENCODE_MOTIFS_PATH, pseudocountProb=0.001)
  num_samples, _, seq_length, _ = encoded_sequences.shape
  scores = np.ones((num_samples, len(motif_names), seq_length))
  for j, motif_name in enumerate(motif_names):
    pwm = loaded_motifs.getPwm(motif_name).getRows().T
    log_pwm = np.log(pwm)
    gc_pwm = 0.5 * np.array(
        [[1 - GC_fraction, GC_fraction, GC_fraction, 1 - GC_fraction]] * len(
            pwm[0])).T
    gc_log_pwm = np.log(gc_pwm)
    log_scores = get_pssm_scores(encoded_sequences, log_pwm)
    gc_log_scores = get_pssm_scores(encoded_sequences, gc_log_pwm)
    scores[:, j, :] = log_scores - gc_log_scores
  if max_scores is not None:
    sorted_scores = np.sort(scores)[:, :, ::-1][:, :, :max_scores]
    if return_positions:
      sorted_positions = scores.argsort()[:, :, ::-1][:, :, :max_scores]
      return np.concatenate(
          (sorted_scores.reshape((num_samples, len(motif_names) * max_scores)),
           sorted_positions.reshape(
               (num_samples, len(motif_names) * max_scores))),
          axis=1)
    else:
      return sorted_scores.reshape((num_samples, len(motif_names) * max_scores))
  else:
    return scores


</source>
</class>

<class classid="11" nclones="2" nlines="15" similarity="100">
<source file="systems/deepchem-2.3.0/contrib/dragonn/utils.py" startline="59" endline="96" pcid="137">
def get_pssm_scores(encoded_sequences, pssm):
  """
  Convolves pssm and its reverse complement with encoded sequences
  and returns the maximum score at each position of each sequence.

  Parameters
  ----------
  encoded_sequences: 3darray
        (num_examples, 1, 4, seq_length) array
  pssm: 2darray
      (4, pssm_length) array

  Returns
  -------
  scores: 2darray
      (num_examples, seq_length) array
  """
  encoded_sequences = encoded_sequences.squeeze(axis=1)
  # initialize fwd and reverse scores to -infinity
  fwd_scores = np.full_like(encoded_sequences, -np.inf, float)
  rc_scores = np.full_like(encoded_sequences, -np.inf, float)
  # cross-correlate separately for each base,
  # for both the PSSM and its reverse complement
  for base_indx in range(encoded_sequences.shape[1]):
    base_pssm = pssm[base_indx][None]
    base_pssm_rc = base_pssm[:, ::-1]
    fwd_scores[:, base_indx, :] = correlate2d(
        encoded_sequences[:, base_indx, :], base_pssm, mode='same')
    rc_scores[:, base_indx, :] = correlate2d(
        encoded_sequences[:, -(base_indx + 1), :], base_pssm_rc, mode='same')
  # sum over the bases
  fwd_scores = fwd_scores.sum(axis=1)
  rc_scores = rc_scores.sum(axis=1)
  # take max of fwd and reverse scores at each position
  scores = np.maximum(fwd_scores, rc_scores)
  return scores


</source>
<source file="systems/deepchem-2.3.0/deepchem/metrics/genomic_metrics.py" startline="62" endline="99" pcid="2049">
def get_pssm_scores(encoded_sequences, pssm):
  """
  Convolves pssm and its reverse complement with encoded sequences
  and returns the maximum score at each position of each sequence.

  Parameters
  ----------
  encoded_sequences: 3darray
       (N_sequences, N_letters, sequence_length, 1) array
  pssm: 2darray
      (4, pssm_length) array

  Returns
  -------
  scores: 2darray
      (N_sequences, sequence_length)
  """
  encoded_sequences = encoded_sequences.squeeze(axis=3)
  # initialize fwd and reverse scores to -infinity
  fwd_scores = np.full_like(encoded_sequences, -np.inf, float)
  rc_scores = np.full_like(encoded_sequences, -np.inf, float)
  # cross-correlate separately for each base,
  # for both the PSSM and its reverse complement
  for base_indx in range(encoded_sequences.shape[1]):
    base_pssm = pssm[base_indx][None]
    base_pssm_rc = base_pssm[:, ::-1]
    fwd_scores[:, base_indx, :] = correlate2d(
        encoded_sequences[:, base_indx, :], base_pssm, mode='same')
    rc_scores[:, base_indx, :] = correlate2d(
        encoded_sequences[:, -(base_indx + 1), :], base_pssm_rc, mode='same')
  # sum over the bases
  fwd_scores = fwd_scores.sum(axis=1)
  rc_scores = rc_scores.sum(axis=1)
  # take max of fwd and reverse scores at each position
  scores = np.maximum(fwd_scores, rc_scores)
  return scores


</source>
</class>

<class classid="12" nclones="2" nlines="40" similarity="97">
<source file="systems/deepchem-2.3.0/contrib/one_shot_models/multitask_classifier.py" startline="73" endline="129" pcid="157">
  def __init__(self,
               model,
               n_tasks,
               n_feat,
               logdir=None,
               batch_size=50,
               final_loss='cross_entropy',
               learning_rate=.001,
               optimizer_type="adam",
               learning_rate_decay_time=1000,
               beta1=.9,
               beta2=.999,
               pad_batches=True,
               verbose=True):

    warnings.warn(
        "MultitaskGraphClassifier is deprecated. "
        "Will be removed in DeepChem 1.4.", DeprecationWarning)
    super(MultitaskGraphClassifier, self).__init__(
        model_dir=logdir, verbose=verbose)
    self.n_tasks = n_tasks
    self.final_loss = final_loss
    self.model = model
    self.sess = tf.Session(graph=self.model.graph)

    with self.model.graph.as_default():
      # Extract model info
      self.batch_size = batch_size
      self.pad_batches = pad_batches
      # Get graph topology for x
      self.graph_topology = self.model.get_graph_topology()
      self.feat_dim = n_feat

      # Raw logit outputs
      self.logits = self.build()
      self.loss_op = self.add_training_loss(self.final_loss, self.logits)
      self.outputs = self.add_softmax(self.logits)

      self.learning_rate = learning_rate
      self.T = learning_rate_decay_time
      self.optimizer_type = optimizer_type

      self.optimizer_beta1 = beta1
      self.optimizer_beta2 = beta2

      # Set epsilon
      self.epsilon = 1e-7
      self.add_optimizer()

      # Initialize
      self.init_fn = tf.global_variables_initializer()
      self.sess.run(self.init_fn)

      # Path to save checkpoint files, which matches the
      # replicated supervisor's default path.
      self._save_path = os.path.join(self.model_dir, 'model.ckpt')

</source>
<source file="systems/deepchem-2.3.0/contrib/one_shot_models/multitask_regressor.py" startline="32" endline="88" pcid="200">
  def __init__(self,
               model,
               n_tasks,
               n_feat,
               logdir=None,
               batch_size=50,
               final_loss='weighted_L2',
               learning_rate=.001,
               optimizer_type="adam",
               learning_rate_decay_time=1000,
               beta1=.9,
               beta2=.999,
               pad_batches=True,
               verbose=True):

    warnings.warn(
        "MultitaskGraphRegressor is deprecated. "
        "Will be removed in DeepChem 1.4.", DeprecationWarning)

    super(MultitaskGraphRegressor, self).__init__(
        model_dir=logdir, verbose=verbose)
    self.n_tasks = n_tasks
    self.final_loss = final_loss
    self.model = model
    self.sess = tf.Session(graph=self.model.graph)

    with self.model.graph.as_default():
      # Extract model info
      self.batch_size = batch_size
      self.pad_batches = pad_batches
      # Get graph topology for x
      self.graph_topology = self.model.get_graph_topology()
      self.feat_dim = n_feat

      # Building outputs
      self.outputs = self.build()
      self.loss_op = self.add_training_loss(self.final_loss, self.outputs)

      self.learning_rate = learning_rate
      self.T = learning_rate_decay_time
      self.optimizer_type = optimizer_type

      self.optimizer_beta1 = beta1
      self.optimizer_beta2 = beta2

      # Set epsilon
      self.epsilon = 1e-7
      self.add_optimizer()

      # Initialize
      self.init_fn = tf.global_variables_initializer()
      self.sess.run(self.init_fn)

      # Path to save checkpoint files, which matches the
      # replicated supervisor's default path.
      self._save_path = os.path.join(self.model_dir, 'model.ckpt')

</source>
</class>

<class classid="13" nclones="2" nlines="10" similarity="100">
<source file="systems/deepchem-2.3.0/contrib/one_shot_models/multitask_classifier.py" startline="146" endline="158" pcid="159">
  def add_optimizer(self):
    if self.optimizer_type == "adam":
      self.optimizer = tf.train.AdamOptimizer(
          self.learning_rate,
          beta1=self.optimizer_beta1,
          beta2=self.optimizer_beta2,
          epsilon=self.epsilon)
    else:
      raise ValueError("Optimizer type not recognized.")

    # Get train function
    self.train_op = self.optimizer.minimize(self.loss_op)

</source>
<source file="systems/deepchem-2.3.0/contrib/one_shot_models/multitask_regressor.py" startline="110" endline="122" pcid="202">
  def add_optimizer(self):
    if self.optimizer_type == "adam":
      self.optimizer = tf.train.AdamOptimizer(
          self.learning_rate,
          beta1=self.optimizer_beta1,
          beta2=self.optimizer_beta2,
          epsilon=self.epsilon)
    else:
      raise ValueError("Optimizer type not recognized.")

    # Get train function
    self.train_op = self.optimizer.minimize(self.loss_op)

</source>
</class>

<class classid="14" nclones="2" nlines="10" similarity="100">
<source file="systems/deepchem-2.3.0/contrib/one_shot_models/multitask_classifier.py" startline="159" endline="180" pcid="160">
  def construct_feed_dict(self, X_b, y_b=None, w_b=None, training=True):
    """Get initial information about task normalization"""
    # TODO(rbharath): I believe this is total amount of data
    n_samples = len(X_b)
    if y_b is None:
      y_b = np.zeros((n_samples, self.n_tasks))
    if w_b is None:
      w_b = np.zeros((n_samples, self.n_tasks))
    targets_dict = {self.label_placeholder: y_b, self.weight_placeholder: w_b}

    # Get graph information
    atoms_dict = self.graph_topology.batch_to_feed_dict(X_b)

    # TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being
    # the number of labeled data points in target_i. This is to normalize each task
    # num_dat_dict = {self.num_datapoints_placeholder : self.}

    # Get other optimizer information
    # TODO(rbharath): Figure out how to handle phase appropriately
    feed_dict = merge_dicts([targets_dict, atoms_dict])
    return feed_dict

</source>
<source file="systems/deepchem-2.3.0/contrib/one_shot_models/multitask_regressor.py" startline="123" endline="144" pcid="203">
  def construct_feed_dict(self, X_b, y_b=None, w_b=None, training=True):
    """Get initial information about task normalization"""
    # TODO(rbharath): I believe this is total amount of data
    n_samples = len(X_b)
    if y_b is None:
      y_b = np.zeros((n_samples, self.n_tasks))
    if w_b is None:
      w_b = np.zeros((n_samples, self.n_tasks))
    targets_dict = {self.label_placeholder: y_b, self.weight_placeholder: w_b}

    # Get graph information
    atoms_dict = self.graph_topology.batch_to_feed_dict(X_b)

    # TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being
    # the number of labeled data points in target_i. This is to normalize each task
    # num_dat_dict = {self.num_datapoints_placeholder : self.}

    # Get other optimizer information
    # TODO(rbharath): Figure out how to handle phase appropriately
    feed_dict = merge_dicts([targets_dict, atoms_dict])
    return feed_dict

</source>
</class>

<class classid="15" nclones="2" nlines="17" similarity="77">
<source file="systems/deepchem-2.3.0/contrib/one_shot_models/multitask_classifier.py" startline="181" endline="207" pcid="161">
  def add_training_loss(self, final_loss, logits):
    """Computes loss using logits."""
    loss_fn = get_loss_fn(final_loss)  # Get loss function
    task_losses = []
    # label_placeholder of shape (batch_size, n_tasks). Split into n_tasks
    # tensors of shape (batch_size,)
    task_labels = tf.split(
        axis=1, num_or_size_splits=self.n_tasks, value=self.label_placeholder)
    task_weights = tf.split(
        axis=1, num_or_size_splits=self.n_tasks, value=self.weight_placeholder)
    for task in range(self.n_tasks):
      task_label_vector = task_labels[task]
      task_weight_vector = task_weights[task]
      # Convert the labels into one-hot vector encodings.
      one_hot_labels = tf.cast(
          tf.one_hot(tf.cast(tf.squeeze(task_label_vector), tf.int32), 2),
          tf.float32)
      # Since we use tf.nn.softmax_cross_entropy_with_logits note that we pass in
      # un-softmaxed logits rather than softmax outputs.
      task_loss = loss_fn(logits[task], one_hot_labels, task_weight_vector)
      task_losses.append(task_loss)
    # It's ok to divide by just the batch_size rather than the number of nonzero
    # examples (effect averages out)
    total_loss = tf.add_n(task_losses)
    total_loss = tf.math.divide(total_loss, self.batch_size)
    return total_loss

</source>
<source file="systems/deepchem-2.3.0/contrib/one_shot_models/multitask_regressor.py" startline="145" endline="166" pcid="204">
  def add_training_loss(self, final_loss, outputs):
    """Computes loss using logits."""
    loss_fn = get_loss_fn(final_loss)  # Get loss function
    task_losses = []
    # label_placeholder of shape (batch_size, n_tasks). Split into n_tasks
    # tensors of shape (batch_size,)
    task_labels = tf.split(
        axis=1, num_or_size_splits=self.n_tasks, value=self.label_placeholder)
    task_weights = tf.split(
        axis=1, num_or_size_splits=self.n_tasks, value=self.weight_placeholder)
    for task in range(self.n_tasks):
      task_label_vector = task_labels[task]
      task_weight_vector = task_weights[task]
      task_loss = loss_fn(outputs[task], tf.squeeze(task_label_vector),
                          tf.squeeze(task_weight_vector))
      task_losses.append(task_loss)
    # It's ok to divide by just the batch_size rather than the number of nonzero
    # examples (effect averages out)
    total_loss = tf.add_n(task_losses)
    total_loss = tf.math.divide(total_loss, self.batch_size)
    return total_loss

</source>
</class>

<class classid="16" nclones="2" nlines="16" similarity="100">
<source file="systems/deepchem-2.3.0/contrib/one_shot_models/multitask_classifier.py" startline="216" endline="235" pcid="163">
  def fit(self,
          dataset,
          nb_epoch=10,
          max_checkpoints_to_keep=5,
          log_every_N_batches=50,
          checkpoint_interval=10,
          **kwargs):
    # Perform the optimization
    log("Training for %d epochs" % nb_epoch, self.verbose)

    # TODO(rbharath): Disabling saving for now to try to debug.
    for epoch in range(nb_epoch):
      log("Starting epoch %d" % epoch, self.verbose)
      for batch_num, (X_b, y_b, w_b, ids_b) in enumerate(
          dataset.iterbatches(self.batch_size, pad_batches=self.pad_batches)):
        if batch_num % log_every_N_batches == 0:
          log("On batch %d" % batch_num, self.verbose)
        self.sess.run(
            self.train_op, feed_dict=self.construct_feed_dict(X_b, y_b, w_b))

</source>
<source file="systems/deepchem-2.3.0/contrib/one_shot_models/multitask_regressor.py" startline="167" endline="186" pcid="205">
  def fit(self,
          dataset,
          nb_epoch=10,
          max_checkpoints_to_keep=5,
          log_every_N_batches=50,
          checkpoint_interval=10,
          **kwargs):
    # Perform the optimization
    log("Training for %d epochs" % nb_epoch, self.verbose)

    # TODO(rbharath): Disabling saving for now to try to debug.
    for epoch in range(nb_epoch):
      log("Starting epoch %d" % epoch, self.verbose)
      for batch_num, (X_b, y_b, w_b, ids_b) in enumerate(
          dataset.iterbatches(self.batch_size, pad_batches=self.pad_batches)):
        if batch_num % log_every_N_batches == 0:
          log("On batch %d" % batch_num, self.verbose)
        self.sess.run(
            self.train_op, feed_dict=self.construct_feed_dict(X_b, y_b, w_b))

</source>
</class>

<class classid="17" nclones="3" nlines="12" similarity="75">
<source file="systems/deepchem-2.3.0/contrib/one_shot_models/multitask_classifier.py" startline="252" endline="269" pcid="167">
  def predict_on_batch(self, X):
    """Return model output for the provided input.
    """
    if self.pad_batches:
      X = pad_features(self.batch_size, X)
    # run eval data through the model
    n_tasks = self.n_tasks
    with self.sess.as_default():
      feed_dict = self.construct_feed_dict(X)
      # Shape (n_samples, n_tasks)
      batch_outputs = self.sess.run(self.outputs, feed_dict=feed_dict)

    n_samples = len(X)
    outputs = np.zeros((n_samples, self.n_tasks))
    for task, output in enumerate(batch_outputs):
      outputs[:, task] = np.argmax(output, axis=1)
    return outputs

</source>
<source file="systems/deepchem-2.3.0/contrib/one_shot_models/multitask_classifier.py" startline="270" endline="285" pcid="168">
  def predict_proba_on_batch(self, X, n_classes=2):
    """Returns class probabilities on batch"""
    # run eval data through the model
    if self.pad_batches:
      X = pad_features(self.batch_size, X)
    n_tasks = self.n_tasks
    with self.sess.as_default():
      feed_dict = self.construct_feed_dict(X)
      batch_outputs = self.sess.run(self.outputs, feed_dict=feed_dict)

    n_samples = len(X)
    outputs = np.zeros((n_samples, self.n_tasks, n_classes))
    for task, output in enumerate(batch_outputs):
      outputs[:, task, :] = output
    return outputs

</source>
<source file="systems/deepchem-2.3.0/contrib/one_shot_models/multitask_regressor.py" startline="198" endline="215" pcid="208">
  def predict_on_batch(self, X):
    """Return model output for the provided input.
    """
    if self.pad_batches:
      X = pad_features(self.batch_size, X)
    # run eval data through the model
    n_tasks = self.n_tasks
    with self.sess.as_default():
      feed_dict = self.construct_feed_dict(X)
      # Shape (n_samples, n_tasks)
      batch_outputs = self.sess.run(self.outputs, feed_dict=feed_dict)

    n_samples = len(X)
    outputs = np.zeros((n_samples, self.n_tasks))
    for task, output in enumerate(batch_outputs):
      outputs[:, task] = output
    return outputs

</source>
</class>

<class classid="18" nclones="2" nlines="31" similarity="70">
<source file="systems/deepchem-2.3.0/contrib/one_shot_models/graph_topology.py" startline="389" endline="435" pcid="188">
  def __init__(self,
               max_atoms=50,
               n_atom_feat=75,
               n_pair_feat=14,
               name='Weave_topology'):
    """
    Parameters
    ----------
    max_atoms: int, optional
      maximum number of atoms in a molecule
    n_atom_feat: int, optional
      number of basic features of each atom
    n_pair_feat: int, optional
      number of basic features of each pair
    """
    warnings.warn("WeaveGraphTopology is deprecated. "
                  "Will be removed in DeepChem 1.4.", DeprecationWarning)

    #self.n_atoms = n_atoms
    self.name = name
    self.max_atoms = max_atoms
    self.n_atom_feat = n_atom_feat
    self.n_pair_feat = n_pair_feat

    self.atom_features_placeholder = tf.placeholder(
        dtype='float32',
        shape=(None, self.max_atoms, self.n_atom_feat),
        name=self.name + '_atom_features')
    self.atom_mask_placeholder = tf.placeholder(
        dtype='float32',
        shape=(None, self.max_atoms),
        name=self.name + '_atom_mask')
    self.pair_features_placeholder = tf.placeholder(
        dtype='float32',
        shape=(None, self.max_atoms, self.max_atoms, self.n_pair_feat),
        name=self.name + '_pair_features')
    self.pair_mask_placeholder = tf.placeholder(
        dtype='float32',
        shape=(None, self.max_atoms, self.max_atoms),
        name=self.name + '_pair_mask')
    self.membership_placeholder = tf.placeholder(
        dtype='int32', shape=(None,), name=self.name + '_membership')
    # Define the list of tensors to be used as topology
    self.topology = [self.atom_mask_placeholder, self.pair_mask_placeholder]
    self.inputs = [self.atom_features_placeholder]
    self.inputs += self.topology

</source>
<source file="systems/deepchem-2.3.0/contrib/one_shot_models/graph_topology.py" startline="493" endline="543" pcid="191">

  def __init__(self,
               batch_size,
               max_atoms=50,
               n_atom_feat=75,
               n_pair_feat=14,
               name='Weave_topology'):
    """
    Parameters
    ----------
    batch_size: int
      number of molecules in a batch
    max_atoms: int, optional
      maximum number of atoms in a molecule
    n_atom_feat: int, optional
      number of basic features of each atom
    n_pair_feat: int, optional
      number of basic features of each pair
    """
    warnings.warn("AlternateWeaveGraphTopology is deprecated. "
                  "Will be removed in DeepChem 1.4.", DeprecationWarning)

    #self.n_atoms = n_atoms
    self.name = name
    self.batch_size = batch_size
    self.max_atoms = max_atoms * batch_size
    self.n_atom_feat = n_atom_feat
    self.n_pair_feat = n_pair_feat

    self.atom_features_placeholder = tf.placeholder(
        dtype='float32',
        shape=(None, self.n_atom_feat),
        name=self.name + '_atom_features')
    self.pair_features_placeholder = tf.placeholder(
        dtype='float32',
        shape=(None, self.n_pair_feat),
        name=self.name + '_pair_features')
    self.pair_split_placeholder = tf.placeholder(
        dtype='int32', shape=(None,), name=self.name + '_pair_split')
    self.atom_split_placeholder = tf.placeholder(
        dtype='int32', shape=(None,), name=self.name + '_atom_split')
    self.atom_to_pair_placeholder = tf.placeholder(
        dtype='int32', shape=(None, 2), name=self.name + '_atom_to_pair')

    # Define the list of tensors to be used as topology
    self.topology = [
        self.pair_split_placeholder, self.atom_split_placeholder,
        self.atom_to_pair_placeholder
    ]
    self.inputs = [self.atom_features_placeholder]
    self.inputs += self.topology
</source>
</class>

<class classid="19" nclones="2" nlines="17" similarity="100">
<source file="systems/deepchem-2.3.0/contrib/one_shot_models/tests/test_graph_models.py" startline="57" endline="84" pcid="198">
  def test_sample_attn_lstm_architecture(self):
    """Tests that an attention architecture can be created without crash."""
    max_depth = 5
    n_test = 5
    n_support = 11
    n_feat = 10
    batch_size = 3

    support_model = SequentialSupportGraph(n_feat)

    # Add layers
    support_model.add(dc.nn.GraphConv(64, n_feat, activation='relu'))
    # Need to add batch-norm separately to test/support due to differing
    # shapes.
    support_model.add_test(dc.nn.BatchNormalization(epsilon=1e-5, mode=1))
    support_model.add_support(dc.nn.BatchNormalization(epsilon=1e-5, mode=1))
    support_model.add(dc.nn.GraphPool())

    # Apply an attention lstm layer
    support_model.join(
        dc.nn.AttnLSTMEmbedding(n_test, n_support, 64, max_depth))

    # Gather Projection
    support_model.add(dc.nn.Dense(128, 64))
    support_model.add_test(dc.nn.BatchNormalization(epsilon=1e-5, mode=1))
    support_model.add_support(dc.nn.BatchNormalization(epsilon=1e-5, mode=1))
    support_model.add(dc.nn.GraphGather(batch_size, activation="tanh"))

</source>
<source file="systems/deepchem-2.3.0/contrib/one_shot_models/tests/test_graph_models.py" startline="85" endline="111" pcid="199">
  def test_sample_resi_lstm_architecture(self):
    """Tests that an attention architecture can be created without crash."""
    max_depth = 5
    n_test = 5
    n_support = 11
    n_feat = 10
    batch_size = 3

    support_model = SequentialSupportGraph(n_feat)

    # Add layers
    support_model.add(dc.nn.GraphConv(64, n_feat, activation='relu'))
    # Need to add batch-norm separately to test/support due to differing
    # shapes.
    support_model.add_test(dc.nn.BatchNormalization(epsilon=1e-5, mode=1))
    support_model.add_support(dc.nn.BatchNormalization(epsilon=1e-5, mode=1))
    support_model.add(dc.nn.GraphPool())

    # Apply an attention lstm layer
    support_model.join(
        dc.nn.ResiLSTMEmbedding(n_test, n_support, 64, max_depth))

    # Gather Projection
    support_model.add(dc.nn.Dense(128, 64))
    support_model.add_test(dc.nn.BatchNormalization(epsilon=1e-5, mode=1))
    support_model.add_support(dc.nn.BatchNormalization(epsilon=1e-5, mode=1))
    support_model.add(dc.nn.GraphGather(batch_size, activation="tanh"))
</source>
</class>

<class classid="20" nclones="4" nlines="11" similarity="78">
<source file="systems/deepchem-2.3.0/contrib/one_shot_models/graph_models.py" startline="103" endline="116" pcid="219">

  def add(self, layer):
    """Adds a new layer to model."""
    with self.graph.as_default():
      if type(layer).__name__ in ['DTNNStep']:
        self.output = layer([self.output] +
                            self.graph_topology.get_topology_placeholders())
      elif type(layer).__name__ in ['DTNNGather']:
        self.output = layer(
            [self.output, self.graph_topology.atom_membership_placeholder])
      else:
        self.output = layer(self.output)
      self.layers.append(layer)

</source>
<source file="systems/deepchem-2.3.0/contrib/one_shot_models/graph_models.py" startline="230" endline="244" pcid="225">

  def add(self, layer):
    """Adds a new layer to model."""
    with self.graph.as_default():
      if type(layer).__name__ in ['AlternateWeaveLayer']:
        self.output, self.output_P = layer([
            self.output, self.output_P
        ] + self.graph_topology.get_topology_placeholders())
      elif type(layer).__name__ in ['AlternateWeaveGather']:
        self.output = layer(
            [self.output, self.graph_topology.atom_split_placeholder])
      else:
        self.output = layer(self.output)
      self.layers.append(layer)

</source>
<source file="systems/deepchem-2.3.0/contrib/one_shot_models/graph_models.py" startline="139" endline="152" pcid="221">

  def add(self, layer):
    """Adds a new layer to model."""
    with self.graph.as_default():
      if type(layer).__name__ in ['DAGLayer']:
        self.output = layer([self.output] +
                            self.graph_topology.get_topology_placeholders())
      elif type(layer).__name__ in ['DAGGather']:
        self.output = layer(
            [self.output, self.graph_topology.membership_placeholder])
      else:
        self.output = layer(self.output)
      self.layers.append(layer)

</source>
<source file="systems/deepchem-2.3.0/contrib/one_shot_models/graph_models.py" startline="181" endline="198" pcid="223">

  def add(self, layer):
    """Adds a new layer to model."""
    with self.graph.as_default():
      if type(layer).__name__ in ['WeaveLayer']:
        self.output, self.output_P = layer([
            self.output, self.output_P
        ] + self.graph_topology.get_topology_placeholders())
      elif type(layer).__name__ in ['WeaveConcat']:
        self.output = layer(
            [self.output, self.graph_topology.atom_mask_placeholder])
      elif type(layer).__name__ in ['WeaveGather']:
        self.output = layer(
            [self.output, self.graph_topology.membership_placeholder])
      else:
        self.output = layer(self.output)
      self.layers.append(layer)

</source>
</class>

<class classid="21" nclones="2" nlines="15" similarity="80">
<source file="systems/deepchem-2.3.0/contrib/tensorflow_models/test_utils.py" startline="87" endline="102" pcid="252">
  def testMeanWithMask(self):
    self.Check(
        utils.Mean, features=[[9999], [1], [2]], expected=1.5, mask=[0, 1, 1])
    self.Check(
        utils.Mean,
        features=[[0, 1], [9999, 9999]],
        expected=[0, 1],
        axis=0,
        mask=[1, 0])
    self.Check(
        utils.Mean,
        features=[[[0, 1], [9999, 9999]], [[9999, 9999], [6, 7]]],
        expected=[0.5, 6.5],
        axis=[0, 2],
        mask=[[1, 0], [0, 1]])

</source>
<source file="systems/deepchem-2.3.0/contrib/tensorflow_models/test_utils.py" startline="113" endline="128" pcid="254">
  def testVarianceWithMask(self):
    self.Check(
        utils.Variance, features=[[0], [1], [2]], expected=0.25, mask=[0, 1, 1])
    self.Check(
        utils.Variance,
        features=[[0, 2], [9999, 9999], [4, 4]],
        expected=[4, 1],
        axis=0,
        mask=[1, 0, 1])
    self.Check(
        utils.Variance,
        features=[[[0, 1], [9999, 9999]], [[9999, 9999], [6, 8]]],
        expected=[0.25, 1],
        axis=[0, 2],
        mask=[[1, 0], [0, 1]])

</source>
</class>

<class classid="22" nclones="2" nlines="14" similarity="100">
<source file="systems/deepchem-2.3.0/contrib/tensorflow_models/test_utils.py" startline="170" endline="184" pcid="256">
  def testSkewness(self):
    with self.session() as sess:
      features = np.random.random((3, 4, 5))
      features_t = tf.constant(features, dtype=tf.float32)
      self.assertAllClose(
          sess.run(utils.Skewness(features_t)),
          scipy.stats.skew(features, axis=None),
          rtol=1e-5,
          atol=1e-5)
      self.assertAllClose(
          sess.run(utils.Skewness(features_t, 1)),
          scipy.stats.skew(features, axis=1),
          rtol=1e-5,
          atol=1e-5)

</source>
<source file="systems/deepchem-2.3.0/contrib/tensorflow_models/test_utils.py" startline="185" endline="200" pcid="257">
  def testKurtosis(self):
    with self.session() as sess:
      features = np.random.random((3, 4, 5))
      features_t = tf.constant(features, dtype=tf.float32)
      self.assertAllClose(
          sess.run(utils.Kurtosis(features_t)),
          scipy.stats.kurtosis(features, axis=None),
          rtol=1e-5,
          atol=1e-5)
      self.assertAllClose(
          sess.run(utils.Kurtosis(features_t, 1)),
          scipy.stats.kurtosis(features, axis=1),
          rtol=1e-5,
          atol=1e-5)


</source>
</class>

<class classid="23" nclones="3" nlines="13" similarity="75">
<source file="systems/deepchem-2.3.0/contrib/tensorflow_models/__init__.py" startline="439" endline="458" pcid="275">
  def add_example_weight_placeholders(self, graph, name_scopes):
    """Add Placeholders for example weights for each task.

    This method creates the following Placeholders for each task:
      weights_%d: Label tensor with shape batch_size.

    Placeholders are wrapped in identity ops to avoid the error caused by
    feeding and fetching the same tensor.
    """
    weights = []
    placeholder_scope = TensorflowGraph.get_placeholder_scope(
        graph, name_scopes)
    with placeholder_scope:
      for task in range(self.n_tasks):
        weights.append(
            tf.identity(
                tf.placeholder(
                    tf.float32, shape=[None], name='weights_%d' % task)))
    return weights

</source>
<source file="systems/deepchem-2.3.0/contrib/tensorflow_models/__init__.py" startline="798" endline="819" pcid="291">
  def add_label_placeholders(self, graph, name_scopes):
    """Add Placeholders for labels for each task.

    This method creates the following Placeholders for each task:
      labels_%d: Label tensor with shape batch_size.

    Placeholders are wrapped in identity ops to avoid the error caused by
    feeding and fetching the same tensor.
    """
    placeholder_scope = TensorflowGraph.get_placeholder_scope(
        graph, name_scopes)
    with graph.as_default():
      batch_size = self.batch_size
      labels = []
      with placeholder_scope:
        for task in range(self.n_tasks):
          labels.append(
              tf.identity(
                  tf.placeholder(
                      tf.float32, shape=[None], name='labels_%d' % task)))
    return labels

</source>
<source file="systems/deepchem-2.3.0/contrib/tensorflow_models/__init__.py" startline="645" endline="669" pcid="285">
  def add_label_placeholders(self, graph, name_scopes):
    """Add Placeholders for labels for each task.

    This method creates the following Placeholders for each task:
      labels_%d: Label tensor with shape batch_size x n_classes.

    Placeholders are wrapped in identity ops to avoid the error caused by
    feeding and fetching the same tensor.
    """
    placeholder_scope = TensorflowGraph.get_placeholder_scope(
        graph, name_scopes)
    with graph.as_default():
      batch_size = self.batch_size
      n_classes = self.n_classes
      labels = []
      with placeholder_scope:
        for task in range(self.n_tasks):
          labels.append(
              tf.identity(
                  tf.placeholder(
                      tf.float32,
                      shape=[None, n_classes],
                      name='labels_%d' % task)))
      return labels

</source>
</class>

<class classid="24" nclones="2" nlines="16" similarity="75">
<source file="systems/deepchem-2.3.0/contrib/hagcn/hagcn_layers.py" startline="138" endline="158" pcid="300">
  def create_tensor(self, in_layers=None, set_tensors=True, **kwargs):
    if in_layers is None:
      in_layers = self.in_layers
    in_layers = convert_to_layers(in_layers)
    self._build()

    A_tilda_k = in_layers[0].out_tensor
    X = in_layers[1].out_tensor
    adp_fn_val = in_layers[2].out_tensor

    attn_weights = tf.multiply(adp_fn_val, self.W)
    wt_adjacency = attn_weights * A_tilda_k
    out = tf.matmul(wt_adjacency, X) + tf.expand_dims(self.b, axis=1)

    out_tensor = out
    if set_tensors:
      self.variables = self.trainable_weights
      self.out_tensor = out_tensor

    return out_tensor

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/symmetry_functions.py" startline="72" endline="92" pcid="1034">
  def create_tensor(self, in_layers=None, set_tensors=True, **kwargs):
    """ Generate distance matrix for BPSymmetryFunction with trainable cutoff """
    if in_layers is None:
      in_layers = self.in_layers
    in_layers = convert_to_layers(in_layers)

    self.build()
    d = in_layers[0].out_tensor
    d_flag = in_layers[1].out_tensor
    # Cutoff with threshold Rc
    d_flag = d_flag * tf.nn.relu(tf.sign(self.Rc - d))
    d = 0.5 * (tf.cos(np.pi * d / self.Rc) + 1)
    out_tensor = d * d_flag
    out_tensor = out_tensor * tf.expand_dims((1 - tf.eye(self.max_atoms)), 0)
    out_tensor = out_tensor

    if set_tensors:
      self.out_tensor = out_tensor
    return out_tensor


</source>
</class>

<class classid="25" nclones="2" nlines="16" similarity="100">
<source file="systems/deepchem-2.3.0/contrib/autoencoder_models/autoencoder.py" startline="21" endline="54" pcid="317">
  def __init__(self,
               model_dir=None,
               weights_file="model.h5",
               verbose=True,
               charset_length=len(zinc_charset),
               latent_rep_size=292):
    """

    Parameters
    ----------
    model_dir: str
      Folder to store cached weights
    weights_file: str
      File to store cached weights in model_dir
    verbose: bool
      True for more logging
    charset_length: int
      Length of one_hot_encoded vectors
    latent_rep_size: int
      How large a 1D Vector for latent representation
    """
    warnings.warn("TensorflowMoleculeEncoder Deprecated. "
                  "Will be removed in DeepChem 1.4.", DeprecationWarning)
    super(TensorflowMoleculeEncoder, self).__init__(
        model_dir=model_dir, verbose=verbose)
    weights_file = os.path.join(model_dir, weights_file)
    if os.path.isfile(weights_file):
      m = MoleculeVAE()
      m.load(charset_length, weights_file, latent_rep_size=latent_rep_size)
      self.model = m
    else:
      # TODO (LESWING) Lazy Load
      raise ValueError("Model file %s doesn't exist" % weights_file)

</source>
<source file="systems/deepchem-2.3.0/contrib/autoencoder_models/autoencoder.py" startline="90" endline="123" pcid="321">
  def __init__(self,
               model_dir=None,
               weights_file="model.h5",
               verbose=True,
               charset_length=len(zinc_charset),
               latent_rep_size=292):
    """

    Parameters
    ----------
    model_dir: str
      Folder to store cached weights
    weights_file: str
      File to store cached weights in model_dir
    verbose: bool
      True for more logging
    charset_length: int
      Length of one_hot_encoded vectors
    latent_rep_size: int
      How large a 1D Vector for latent representation
    """
    warnings.warn("TensorflowMoleculeDecoder Deprecated. "
                  "Will be removed in DeepChem 1.4.", DeprecationWarning)
    super(TensorflowMoleculeDecoder, self).__init__(
        model_dir=model_dir, verbose=verbose)
    weights_file = os.path.join(model_dir, weights_file)
    if os.path.isfile(weights_file):
      m = MoleculeVAE()
      m.load(charset_length, weights_file, latent_rep_size=latent_rep_size)
      self.model = m
    else:
      # TODO (LESWING) Lazy Load
      raise ValueError("Model file %s doesn't exist" % weights_file)

</source>
</class>

<class classid="26" nclones="2" nlines="11" similarity="100">
<source file="systems/deepchem-2.3.0/contrib/autoencoder_models/autoencoder.py" startline="56" endline="74" pcid="318">
  def zinc_encoder():
    """
    Returns
    -------
    obj
      An Encoder with weights that were trained on the zinc dataset
    """
    current_dir = os.path.dirname(os.path.realpath(__file__))
    weights_filename = "zinc_model.h5"
    weights_file = os.path.join(current_dir, weights_filename)

    if not os.path.exists(weights_file):
      download_url("http://karlleswing.com/misc/keras-molecule/model.h5",
                   current_dir)
      mv_cmd = "mv model.h5 %s" % weights_file
      call(mv_cmd.split())
    return TensorflowMoleculeEncoder(
        model_dir=current_dir, weights_file=weights_filename)

</source>
<source file="systems/deepchem-2.3.0/contrib/autoencoder_models/autoencoder.py" startline="128" endline="146" pcid="323">
  def zinc_decoder():
    """
    Returns
    -------
    obj
      A Decoder with weights that were trained on the zinc dataset
    """
    current_dir = os.path.dirname(os.path.realpath(__file__))
    weights_filename = "zinc_model.h5"
    weights_file = os.path.join(current_dir, weights_filename)

    if not os.path.exists(weights_file):
      download_url("http://karlleswing.com/misc/keras-molecule/model.h5",
                   current_dir)
      mv_cmd = "mv model.h5 %s" % weights_file
      call(mv_cmd.split())
    return TensorflowMoleculeDecoder(
        model_dir=current_dir, weights_file=weights_filename)

</source>
</class>

<class classid="27" nclones="3" nlines="18" similarity="73">
<source file="systems/deepchem-2.3.0/contrib/atomicconv/splits/pdbbind_temporal_split.py" startline="8" endline="42" pcid="326">
def load_pdbbind_labels(labels_file):
  """Loads pdbbind labels as dataframe

  Parameters
  ----------
  labels_file: str
    Location of PDBbind datafile.

  Returns
  -------
  contents_df: pd.DataFrame
    Dataframe containing contents of PDBbind datafile.

  """

  contents = []
  with open(labels_file) as f:
    for line in f:
      if line.startswith("#"):
        continue
      else:
        splitline = line.split()
        if len(splitline) == 8:
          contents.append(splitline)
        else:
          print("Incorrect data format")
          print(splitline)

  contents_df = pd.DataFrame(
      contents,
      columns=("PDB code", "resolution", "release year", "-logKd/Ki", "Kd/Ki",
               "ignore-this-field", "reference", "ligand name"))
  return contents_df


</source>
<source file="systems/deepchem-2.3.0/examples/binding_pockets/binding_pocket_datasets.py" startline="63" endline="87" pcid="2143">
def load_pdbbind_labels(labels_file):
  """Loads pdbbind labels as dataframe"""
  # Some complexes have labels but no PDB files. Filter these manually
  missing_pdbs = ["1d2v", "1jou", "1s8j", "1cam", "4mlt", "4o7d"]
  contents = []
  with open(labels_file) as f:
    for line in f:
      if line.startswith("#"):
        continue
      else:
        # Some of the ligand-names are of form (FMN ox). Use regex
        # to merge into form (FMN-ox)
        p = re.compile('\(([^\)\s]*) ([^\)\s]*)\)')
        line = p.sub('(\\1-\\2)', line)
        elts = line.split()
        # Filter if missing PDB files
        if elts[0] in missing_pdbs:
          continue
        contents.append(elts)
  contents_df = pd.DataFrame(
      contents,
      columns=("PDB code", "resolution", "release year", "-logKd/Ki", "Kd/Ki",
               "ignore-this-field", "reference", "ligand name"))
  return contents_df

</source>
<source file="systems/deepchem-2.3.0/contrib/atomicconv/feat/atomicnet_pdbbind_datasets.py" startline="15" endline="49" pcid="338">
def load_pdbbind_labels(labels_file):
  """Loads pdbbind labels as dataframe

  Parameters
  ----------
  labels_file: str
    Location of PDBbind datafile.

  Returns
  -------
  contents_df: pd.DataFrame
    Dataframe containing contents of PDBbind datafile.

  """

  contents = []
  with open(labels_file) as f:
    for line in f:
      if line.startswith("#"):
        continue
      else:
        splitline = line.split()
        if len(splitline) == 8:
          contents.append(splitline)
        else:
          print("Incorrect data format")
          print(splitline)

  contents_df = pd.DataFrame(
      contents,
      columns=("PDB code", "resolution", "release year", "-logKd/Ki", "Kd/Ki",
               "ignore-this-field", "reference", "ligand name"))
  return contents_df


</source>
</class>

<class classid="28" nclones="2" nlines="13" similarity="100">
<source file="systems/deepchem-2.3.0/contrib/atomicconv/models/atomicnet_ops.py" startline="15" endline="54" pcid="328">
def InitializeWeightsBiases(prev_layer_size,
                            size,
                            weights=None,
                            biases=None,
                            name=None):
  """Initializes weights and biases to be used in a fully-connected layer.

  Parameters
  ----------
  prev_layer_size: int
    Number of features in previous layer.
  size: int 
    Number of nodes in this layer.
  weights: tf.Tensor, optional (Default None)
    Weight tensor.
  biases: tf.Tensor, optional (Default None)
    Bias tensor.
  name: str 
    Name for this op, optional (Defaults to 'fully_connected' if None)

  Returns
  -------
  weights: tf.Variable
    Initialized weights.
  biases: tf.Variable
    Initialized biases.

  """

  if weights is None:
    weights = tf.truncated_normal([prev_layer_size, size], stddev=0.01)
  if biases is None:
    biases = tf.zeros([size])

  with tf.name_scope(name, 'fully_connected', [weights, biases]):
    w = tf.Variable(weights, name='w')
    b = tf.Variable(biases, name='b')
  return w, b


</source>
<source file="systems/deepchem-2.3.0/deepchem/models/atomic_conv.py" startline="20" endline="59" pcid="565">
def initializeWeightsBiases(prev_layer_size,
                            size,
                            weights=None,
                            biases=None,
                            name=None):
  """Initializes weights and biases to be used in a fully-connected layer.

  Parameters
  ----------
  prev_layer_size: int
    Number of features in previous layer.
  size: int
    Number of nodes in this layer.
  weights: tf.Tensor, optional (Default None)
    Weight tensor.
  biases: tf.Tensor, optional (Default None)
    Bias tensor.
  name: str
    Name for this op, optional (Defaults to 'fully_connected' if None)

  Returns
  -------
  weights: tf.Variable
    Initialized weights.
  biases: tf.Variable
    Initialized biases.

  """

  if weights is None:
    weights = tf.truncated_normal([prev_layer_size, size], stddev=0.01)
  if biases is None:
    biases = tf.zeros([size])

  with tf.name_scope(name, 'fully_connected', [weights, biases]):
    w = tf.Variable(weights, name='w')
    b = tf.Variable(biases, name='b')
  return w, b


</source>
</class>

<class classid="29" nclones="3" nlines="14" similarity="92">
<source file="systems/deepchem-2.3.0/contrib/DeepMHC/deepmhc.py" startline="93" endline="107" pcid="359">
  def create_estimator_inputs(self, feature_columns, weight_column, features,
                              labels, mode):
    tensors = dict()
    for layer, column in zip(self.features, feature_columns):
      feature_column = tf.feature_column.input_layer(features, [column])
      if feature_column.dtype != column.dtype:
        feature_column = tf.cast(feature_column, column.dtype)
      tensors[layer] = feature_column
    if weight_column is not None:
      tensors[self.task_weights[0]] = tf.feature_column.input_layer(
          features, [weight_column])
    if labels is not None:
      tensors[self.labels[[0]]] = labels

    return tensors
</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/models/symmetry_function_regression.py" startline="414" endline="429" pcid="609">
        feed_dict[self.atom_feats] = atom_feats
        feed_dict[self.atom_numbers] = atom_numbers
        feed_dict[self.atom_flags] = atom_flags
        yield feed_dict

  def create_estimator_inputs(self, feature_columns, weight_column, features,
                              labels, mode):
    tensors = dict()
    for layer, column in zip(self.features, feature_columns):
      feature_col = tf.feature_column.input_layer(features, [column])
      if feature_col.dtype != column.dtype:
        feature_col = tf.cast(feature_col, column.dtype)
      tensors[layer] = feature_col

      if weight_column is not None:
        tensors[self.task_weights[0]] = tf.feature_column.input_layer(
</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/models/symmetry_function_regression.py" startline="121" endline="138" pcid="598">

        yield feed_dict

  def create_estimator_inputs(self, feature_columns, weight_column, features,
                              labels, mode):
    tensors = dict()
    for layer, column in zip(self.features, feature_columns):
      feature_col = tf.feature_column.input_layer(features, [column])
      if feature_col.dtype != column.dtype:
        feature_col = tf.cast(feature_col, column.dtype)
      tensors[layer] = feature_col

      if weight_column is not None:
        tensors[self.task_weights[0]] = tf.feature_column.input_layer(
            features, [weight_column])
      if labels is not None:
        tensors[self.labels[0]] = labels

</source>
</class>

<class classid="30" nclones="2" nlines="18" similarity="94">
<source file="systems/deepchem-2.3.0/scripts/dock_dude.py" startline="162" endline="184" pcid="381">
def write_conf(receptor_filename,
               ligand_filename,
               centroid,
               box_dims,
               conf_filename,
               exhaustiveness=None):
  with open(conf_filename, "w") as f:
    f.write("receptor = %s\n" % receptor_filename)
    f.write("ligand = %s\n\n" % ligand_filename)

    f.write("center_x = %f\n" % centroid[0])
    f.write("center_y = %f\n" % centroid[1])
    f.write("center_z = %f\n\n" % centroid[2])

    f.write("size_x = %f\n" % box_dims[0])
    f.write("size_y = %f\n" % box_dims[1])
    f.write("size_z = %f\n\n" % box_dims[2])

    if exhaustiveness is not None:
      f.write("exhaustiveness = %d\n" % exhaustiveness)

  return

</source>
<source file="systems/deepchem-2.3.0/deepchem/dock/pose_generation.py" startline="33" endline="55" pcid="486">
def write_conf(receptor_filename,
               ligand_filename,
               centroid,
               box_dims,
               conf_filename,
               exhaustiveness=None):
  """Writes Vina configuration file to disk."""
  with open(conf_filename, "w") as f:
    f.write("receptor = %s\n" % receptor_filename)
    f.write("ligand = %s\n\n" % ligand_filename)

    f.write("center_x = %f\n" % centroid[0])
    f.write("center_y = %f\n" % centroid[1])
    f.write("center_z = %f\n\n" % centroid[2])

    f.write("size_x = %f\n" % box_dims[0])
    f.write("size_y = %f\n" % box_dims[1])
    f.write("size_z = %f\n\n" % box_dims[2])

    if exhaustiveness is not None:
      f.write("exhaustiveness = %d\n" % exhaustiveness)


</source>
</class>

<class classid="31" nclones="2" nlines="11" similarity="81">
<source file="systems/deepchem-2.3.0/devtools/jenkins/compare_results.py" startline="33" endline="46" pcid="387">
def parse_desired_results(desired_results):
  retval = []
  for line in desired_results:
    vars = line.split(',')
    retval.append({
        "split": vars[0],
        "data_set": vars[1],
        "model": vars[2],
        "train_score": float(vars[3]),
        "test_score": float(vars[4])
    })
  return retval


</source>
<source file="systems/deepchem-2.3.0/devtools/jenkins/compare_results.py" startline="48" endline="61" pcid="388">
def parse_test_results(test_results):
  retval = []
  for line in test_results:
    vars = line.split(',')
    retval.append({
        "split": BENCHMARK_TO_DESIRED_KEY_MAP[vars[1]],
        "data_set": vars[0],
        "model": BENCHMARK_TO_DESIRED_KEY_MAP[vars[3]],
        "train_score": float(vars[6]),
        "test_score": float(vars[8])
    })
  return retval


</source>
</class>

<class classid="32" nclones="2" nlines="24" similarity="79">
<source file="systems/deepchem-2.3.0/devtools/jenkins/table_to_csv.py" startline="7" endline="33" pcid="394">
def classification_table_to_csv(lines):
  output = []
  headers = [
      "split", "dataset", "model", "Train score/ROC-AUC", "Valid score/ROC-AUC"
  ]
  output.append(",".join(headers))
  for line in lines:
    vars = [x.strip() for x in line.split('|')]
    if len(vars) == 0:
      continue
    if len(vars) == 1 and vars[0] == "":
      continue
    if len(vars) == 1:
      split = vars[0]
      continue
    if vars[1] == "Dataset":
      continue
    if vars[1].startswith("-----"):
      continue
    my_dataset, model, train, test = vars[1:-1]
    if my_dataset != "":
      dataset = my_dataset
    output.append(",".join([split, dataset, model, train, test]))
  for l in output:
    print(l)


</source>
<source file="systems/deepchem-2.3.0/devtools/jenkins/table_to_csv.py" startline="34" endline="59" pcid="395">
def regression_table_to_csv(lines):
  output = []

  for line in lines:
    vars = [x.strip() for x in line.split('|')]
    if len(vars) == 0:
      continue
    if len(vars) == 1 and vars[0] == "":
      continue
    if len(vars) == 1:
      continue
    if vars[1] == "Dataset":
      continue
    if vars[1].startswith("-----"):
      continue
    my_dataset, model, split, train, test = vars[1:-1]
    if my_dataset != "":
      dataset = my_dataset
    if model == "MT-NN regression":
      model = "NN regression"
    split = "%s splitting" % split
    output.append(",".join([split, dataset, model, train, test]))
  for l in output:
    print(l)


</source>
</class>

<class classid="33" nclones="2" nlines="25" similarity="85">
<source file="systems/deepchem-2.3.0/devtools/conda-recipe/deepchem/run_test.py" startline="37" endline="68" pcid="407">
  def get_dataset(self,
                  mode='classification',
                  featurizer='GraphConv',
                  num_tasks=2):
    from deepchem.molnet import load_bace_classification, load_delaney
    import numpy as np
    import deepchem as dc
    from deepchem.data import NumpyDataset
    data_points = 10
    if mode == 'classification':
      tasks, all_dataset, transformers = load_bace_classification(featurizer)
    else:
      tasks, all_dataset, transformers = load_delaney(featurizer)

    train, valid, test = all_dataset
    for i in range(1, num_tasks):
      tasks.append("random_task")
    w = np.ones(shape=(data_points, len(tasks)))

    if mode == 'classification':
      y = np.random.randint(0, 2, size=(data_points, len(tasks)))
      metric = dc.metrics.Metric(
          dc.metrics.roc_auc_score, np.mean, mode="classification")
    else:
      y = np.random.normal(size=(data_points, len(tasks)))
      metric = dc.metrics.Metric(
          dc.metrics.mean_absolute_error, mode="regression")

    ds = NumpyDataset(train.X[:data_points], y, w, train.ids[:data_points])

    return tasks, ds, transformers, metric

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_graph_models.py" startline="19" endline="46" pcid="1178">
  def get_dataset(self,
                  mode='classification',
                  featurizer='GraphConv',
                  num_tasks=2):
    data_points = 10
    if mode == 'classification':
      tasks, all_dataset, transformers = load_bace_classification(featurizer)
    else:
      tasks, all_dataset, transformers = load_delaney(featurizer)

    train, valid, test = all_dataset
    for i in range(1, num_tasks):
      tasks.append("random_task")
    w = np.ones(shape=(data_points, len(tasks)))

    if mode == 'classification':
      y = np.random.randint(0, 2, size=(data_points, len(tasks)))
      metric = dc.metrics.Metric(
          dc.metrics.roc_auc_score, np.mean, mode="classification")
    else:
      y = np.random.normal(size=(data_points, len(tasks)))
      metric = dc.metrics.Metric(
          dc.metrics.mean_absolute_error, mode="regression")

    ds = NumpyDataset(train.X[:data_points], y, w, train.ids[:data_points])

    return tasks, ds, transformers, metric

</source>
</class>

<class classid="34" nclones="4" nlines="14" similarity="71">
<source file="systems/deepchem-2.3.0/deepchem/splits/tests/test_task_splitter.py" startline="22" endline="41" pcid="413">
  def test_multitask_train_valid_test_split(self):
    """
    Test TaskSplitter train/valid/test split on multitask dataset.
    """
    n_samples = 100
    n_features = 10
    n_tasks = 10
    X = np.random.rand(n_samples, n_features)
    p = .05  # proportion actives
    y = np.random.binomial(1, p, size=(n_samples, n_tasks))
    dataset = dc.data.NumpyDataset(X, y)

    task_splitter = dc.splits.TaskSplitter()
    train, valid, test = task_splitter.train_valid_test_split(
        dataset, frac_train=.4, frac_valid=.3, frac_test=.3)

    assert len(train.get_task_names()) == 4
    assert len(valid.get_task_names()) == 3
    assert len(test.get_task_names()) == 3

</source>
<source file="systems/deepchem-2.3.0/deepchem/splits/tests/test_task_splitter.py" startline="81" endline="101" pcid="416">
  def test_uneven_train_valid_test_split(self):
    """
    Test train/valid/test split works when proportions don't divide n_tasks.
    """
    n_samples = 100
    n_features = 10
    n_tasks = 11
    X = np.random.rand(n_samples, n_features)
    p = .05  # proportion actives
    y = np.random.binomial(1, p, size=(n_samples, n_tasks))
    dataset = dc.data.NumpyDataset(X, y)

    task_splitter = dc.splits.TaskSplitter()
    train, valid, test = task_splitter.train_valid_test_split(
        dataset, frac_train=.4, frac_valid=.3, frac_test=.3)

    assert len(train.get_task_names()) == 4
    assert len(valid.get_task_names()) == 3
    # Note that the extra task goes to test
    assert len(test.get_task_names()) == 4

</source>
<source file="systems/deepchem-2.3.0/deepchem/splits/tests/test_task_splitter.py" startline="42" endline="60" pcid="414">
  def test_multitask_K_fold_split(self):
    """
    Test TaskSplitter K-fold split on multitask dataset.
    """
    n_samples = 100
    n_features = 10
    n_tasks = 10
    X = np.random.rand(n_samples, n_features)
    p = .05  # proportion actives
    y = np.random.binomial(1, p, size=(n_samples, n_tasks))
    dataset = dc.data.NumpyDataset(X, y)
    K = 5

    task_splitter = dc.splits.TaskSplitter()
    fold_datasets = task_splitter.k_fold_split(dataset, K)

    for fold_dataset in fold_datasets:
      assert len(fold_dataset.get_task_names()) == 2

</source>
<source file="systems/deepchem-2.3.0/deepchem/splits/tests/test_task_splitter.py" startline="61" endline="80" pcid="415">
  def test_uneven_k_fold_split(self):
    """
    Test k-fold-split works when K does not divide n_tasks.
    """
    n_samples = 100
    n_features = 10
    n_tasks = 17
    X = np.random.rand(n_samples, n_features)
    p = .05  # proportion actives
    y = np.random.binomial(1, p, size=(n_samples, n_tasks))
    dataset = dc.data.NumpyDataset(X, y)
    K = 4
    task_splitter = dc.splits.TaskSplitter()
    fold_datasets = task_splitter.k_fold_split(dataset, K)

    for fold in range(K - 1):
      fold_dataset = fold_datasets[fold]
      assert len(fold_dataset.get_task_names()) == 4
    assert len(fold_datasets[-1].get_task_names()) == 5

</source>
</class>

<class classid="35" nclones="3" nlines="11" similarity="90">
<source file="systems/deepchem-2.3.0/deepchem/splits/tests/test_splitter.py" startline="50" endline="65" pcid="419">
  def test_singletask_random_split(self):
    """
    Test singletask RandomSplitter class.
    """
    solubility_dataset = dc.data.tests.load_solubility_data()
    random_splitter = dc.splits.RandomSplitter()
    train_data, valid_data, test_data = \
      random_splitter.train_valid_test_split(
        solubility_dataset, frac_train=0.8, frac_valid=0.1, frac_test=0.1)
    assert len(train_data) == 8
    assert len(valid_data) == 1
    assert len(test_data) == 1

    merged_dataset = dc.data.DiskDataset.merge(
        [train_data, valid_data, test_data])
    assert sorted(merged_dataset.ids) == (sorted(solubility_dataset.ids))
</source>
<source file="systems/deepchem-2.3.0/deepchem/splits/tests/test_splitter.py" startline="114" endline="129" pcid="423">
    s1 = set(train_data.ids)
    assert valid_data.ids[0] not in s1
    assert test_data.ids[0] not in s1

  def test_singletask_stratified_split(self):
    """
    Test singletask SingletaskStratifiedSplitter class.
    """
    solubility_dataset = dc.data.tests.load_solubility_data()
    stratified_splitter = dc.splits.ScaffoldSplitter()
    train_data, valid_data, test_data = \
      stratified_splitter.train_valid_test_split(
        solubility_dataset, frac_train=0.8, frac_valid=0.1, frac_test=0.1)
    assert len(train_data) == 8
    assert len(valid_data) == 1
    assert len(test_data) == 1
</source>
<source file="systems/deepchem-2.3.0/deepchem/splits/tests/test_splitter.py" startline="66" endline="82" pcid="420">

  def test_singletask_index_split(self):
    """
    Test singletask IndexSplitter class.
    """
    solubility_dataset = dc.data.tests.load_solubility_data()
    random_splitter = dc.splits.IndexSplitter()
    train_data, valid_data, test_data = \
      random_splitter.train_valid_test_split(
        solubility_dataset)
    assert len(train_data) == 8
    assert len(valid_data) == 1
    assert len(test_data) == 1

    merged_dataset = dc.data.DiskDataset.merge(
        [train_data, valid_data, test_data])
    assert sorted(merged_dataset.ids) == (sorted(solubility_dataset.ids))
</source>
</class>

<class classid="36" nclones="3" nlines="19" similarity="85">
<source file="systems/deepchem-2.3.0/deepchem/splits/tests/test_splitter.py" startline="172" endline="196" pcid="427">
      train, cv = fold_datasets[fold][0], fold_datasets[fold][1]
      self.assertTrue(cv.X[0] == fold)
      train_data = set(list(train.X))
      self.assertFalse(fold in train_data)
      self.assertEqual(K - 1, len(train))
      self.assertEqual(1, len(cv))

  def test_singletask_random_k_fold_split(self):
    """
    Test singletask RandomSplitter class.
    """
    solubility_dataset = dc.data.tests.load_solubility_data()
    random_splitter = dc.splits.RandomSplitter()
    ids_set = set(solubility_dataset.ids)

    K = 5
    fold_datasets = random_splitter.k_fold_split(solubility_dataset, K)
    for fold in range(K):
      fold_dataset = fold_datasets[fold][1]
      # Verify lengths is 10/k == 2
      assert len(fold_dataset) == 2
      # Verify that compounds in this fold are subset of original compounds
      fold_ids_set = set(fold_dataset.ids)
      assert fold_ids_set.issubset(ids_set)
      # Verify that no two folds have overlapping compounds.
</source>
<source file="systems/deepchem-2.3.0/deepchem/splits/tests/test_splitter.py" startline="227" endline="256" pcid="429">
        other_fold_ids_set = set(other_fold_dataset.ids)
        assert fold_ids_set.isdisjoint(other_fold_ids_set)

    merged_dataset = dc.data.DiskDataset.merge([x[1] for x in fold_datasets])
    assert len(merged_dataset) == len(solubility_dataset)
    assert sorted(merged_dataset.ids) == (sorted(solubility_dataset.ids))

  def test_singletask_scaffold_k_fold_split(self):
    """
    Test singletask ScaffoldSplitter class.
    """
    solubility_dataset = dc.data.tests.load_solubility_data()
    scaffold_splitter = dc.splits.ScaffoldSplitter()
    ids_set = set(solubility_dataset.ids)

    K = 5
    fold_datasets = scaffold_splitter.k_fold_split(solubility_dataset, K)

    for fold in range(K):
      fold_dataset = fold_datasets[fold][1]
      # Verify lengths is 10/k == 2
      assert len(fold_dataset) == 2
      # Verify that compounds in this fold are subset of original compounds
      fold_ids_set = set(fold_dataset.ids)
      assert fold_ids_set.issubset(ids_set)
      # Verify that no two folds have overlapping compounds.
      for other_fold in range(K):
        if fold == other_fold:
          continue
        other_fold_dataset = fold_datasets[other_fold][1]
</source>
<source file="systems/deepchem-2.3.0/deepchem/splits/tests/test_splitter.py" startline="197" endline="226" pcid="428">
      for other_fold in range(K):
        if fold == other_fold:
          continue
        other_fold_dataset = fold_datasets[other_fold][1]
        other_fold_ids_set = set(other_fold_dataset.ids)
        assert fold_ids_set.isdisjoint(other_fold_ids_set)

  def test_singletask_index_k_fold_split(self):
    """
    Test singletask IndexSplitter class.
    """
    solubility_dataset = dc.data.tests.load_solubility_data()
    index_splitter = dc.splits.IndexSplitter()
    ids_set = set(solubility_dataset.ids)

    K = 5
    fold_datasets = index_splitter.k_fold_split(solubility_dataset, K)

    for fold in range(K):
      fold_dataset = fold_datasets[fold][1]
      # Verify lengths is 10/k == 2
      assert len(fold_dataset) == 2
      # Verify that compounds in this fold are subset of original compounds
      fold_ids_set = set(fold_dataset.ids)
      assert fold_ids_set.issubset(ids_set)
      # Verify that no two folds have overlapping compounds.
      for other_fold in range(K):
        if fold == other_fold:
          continue
        other_fold_dataset = fold_datasets[other_fold][1]
</source>
</class>

<class classid="37" nclones="2" nlines="17" similarity="83">
<source file="systems/deepchem-2.3.0/deepchem/splits/tests/test_splitter.py" startline="257" endline="279" pcid="430">
        other_fold_ids_set = set(other_fold_dataset.ids)
        assert fold_ids_set.isdisjoint(other_fold_ids_set)

    merged_dataset = dc.data.DiskDataset.merge([x[1] for x in fold_datasets])
    assert len(merged_dataset) == len(solubility_dataset)
    assert sorted(merged_dataset.ids) == (sorted(solubility_dataset.ids))

  def test_singletask_stratified_column_indices(self):
    """
    Test RandomStratifiedSplitter's split method on simple singletas.
    """
    # Test singletask case.
    n_samples = 100
    n_positives = 20
    n_features = 10
    n_tasks = 1

    X = np.random.rand(n_samples, n_features)
    y = np.zeros((n_samples, n_tasks))
    y[:n_positives] = 1
    w = np.ones((n_samples, n_tasks))
    ids = np.arange(n_samples)
    stratified_splitter = dc.splits.RandomStratifiedSplitter()
</source>
<source file="systems/deepchem-2.3.0/deepchem/splits/tests/test_splitter.py" startline="280" endline="309" pcid="431">
    column_indices = stratified_splitter.get_task_split_indices(
        y, w, frac_split=.5)

    split_index = column_indices[0]
    # The split index should partition dataset in half.
    assert np.count_nonzero(y[:split_index]) == 10

  def test_singletask_stratified_column_indices_mask(self):
    """
    Test RandomStratifiedSplitter's split method on dataset with mask.
    """
    # Test singletask case.
    n_samples = 100
    n_positives = 20
    n_features = 10
    n_tasks = 1

    # Test case where some weights are zero (i.e. masked)
    X = np.random.rand(n_samples, n_features)
    y = np.zeros((n_samples, n_tasks))
    y[:n_positives] = 1
    w = np.ones((n_samples, n_tasks))
    # Set half the positives to have zero weight
    w[:n_positives // 2] = 0
    ids = np.arange(n_samples)

    stratified_splitter = dc.splits.RandomStratifiedSplitter()
    column_indices = stratified_splitter.get_task_split_indices(
        y, w, frac_split=.5)

</source>
</class>

<class classid="38" nclones="2" nlines="17" similarity="73">
<source file="systems/deepchem-2.3.0/deepchem/splits/tests/test_splitter.py" startline="310" endline="331" pcid="432">
    split_index = column_indices[0]
    # There are 10 nonzero actives.
    # The split index should partition this into half, so expect 5
    w_present = (w != 0)
    y_present = y * w_present
    assert np.count_nonzero(y_present[:split_index]) == 5

  def test_multitask_stratified_column_indices(self):
    """
    Test RandomStratifiedSplitter split on multitask dataset.
    """
    n_samples = 100
    n_features = 10
    n_tasks = 10
    X = np.random.rand(n_samples, n_features)
    p = .05  # proportion actives
    y = np.random.binomial(1, p, size=(n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))

    stratified_splitter = dc.splits.RandomStratifiedSplitter()
    split_indices = stratified_splitter.get_task_split_indices(
        y, w, frac_split=.5)
</source>
<source file="systems/deepchem-2.3.0/deepchem/splits/tests/test_splitter.py" startline="332" endline="358" pcid="433">

    for task in range(n_tasks):
      split_index = split_indices[task]
      task_actives = np.count_nonzero(y[:, task])
      # The split index should partition dataset in half.
      assert np.count_nonzero(y[:split_index, task]) == int(task_actives / 2)

  def test_multitask_stratified_column_indices_masked(self):
    """
    Test RandomStratifiedSplitter split on multitask dataset.
    """
    n_samples = 200
    n_features = 10
    n_tasks = 10
    X = np.random.rand(n_samples, n_features)
    p = .05  # proportion actives
    y = np.random.binomial(1, p, size=(n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))
    # Mask half the examples
    w[:n_samples // 2] = 0

    stratified_splitter = dc.splits.RandomStratifiedSplitter()
    split_indices = stratified_splitter.get_task_split_indices(
        y, w, frac_split=.5)

    w_present = (w != 0)
    y_present = y * w_present
</source>
</class>

<class classid="39" nclones="2" nlines="22" similarity="82">
<source file="systems/deepchem-2.3.0/deepchem/dock/tests/test_binding_pocket.py" startline="125" endline="152" pcid="464">
  def test_convex_find_pockets(self):
    """Test that some pockets are filtered out."""
    current_dir = os.path.dirname(os.path.realpath(__file__))
    protein_file = os.path.join(current_dir, "1jld_protein.pdb")
    ligand_file = os.path.join(current_dir, "1jld_ligand.sdf")

    import mdtraj as md
    protein = md.load(protein_file)

    finder = dc.dock.ConvexHullPocketFinder()
    all_pockets = finder.find_all_pockets(protein_file)
    pockets, pocket_atoms_map, pocket_coords = finder.find_pockets(
        protein_file, ligand_file)
    # Test that every atom in pocket maps exists
    n_protein_atoms = protein.xyz.shape[1]
    logger.info("protein.xyz.shape")
    logger.info(protein.xyz.shape)
    logger.info("n_protein_atoms")
    logger.info(n_protein_atoms)
    for pocket in pockets:
      pocket_atoms = pocket_atoms_map[pocket]
      for atom in pocket_atoms:
        # Check that the atoms is actually in protein
        assert atom >= 0
        assert atom < n_protein_atoms

    assert len(pockets) < len(all_pockets)

</source>
<source file="systems/deepchem-2.3.0/deepchem/dock/tests/test_binding_pocket.py" startline="154" endline="183" pcid="465">
  def test_rf_convex_find_pockets(self):
    """Test that filter with pre-trained RF models works."""
    if sys.version_info >= (3, 0):
      return

    current_dir = os.path.dirname(os.path.realpath(__file__))
    protein_file = os.path.join(current_dir, "1jld_protein.pdb")
    ligand_file = os.path.join(current_dir, "1jld_ligand.sdf")

    import mdtraj as md
    protein = md.load(protein_file)

    finder = dc.dock.RFConvexHullPocketFinder()
    pockets, pocket_atoms_map, pocket_coords = finder.find_pockets(
        protein_file, ligand_file)
    # Test that every atom in pocket maps exists
    n_protein_atoms = protein.xyz.shape[1]
    logger.info("protein.xyz.shape")
    logger.info(protein.xyz.shape)
    logger.info("n_protein_atoms")
    logger.info(n_protein_atoms)
    logger.info("len(pockets)")
    logger.info(len(pockets))
    for pocket in pockets:
      pocket_atoms = pocket_atoms_map[pocket]
      for atom in pocket_atoms:
        # Check that the atoms is actually in protein
        assert atom >= 0
        assert atom < n_protein_atoms

</source>
</class>

<class classid="40" nclones="2" nlines="12" similarity="75">
<source file="systems/deepchem-2.3.0/deepchem/dock/tests/test_docking.py" startline="52" endline="69" pcid="469">
  def test_vina_grid_rf_docker_dock(self):
    """Test that VinaGridRFDocker can dock."""
    if sys.version_info >= (3, 0):
      return

    current_dir = os.path.dirname(os.path.realpath(__file__))
    protein_file = os.path.join(current_dir, "1jld_protein.pdb")
    ligand_file = os.path.join(current_dir, "1jld_ligand.sdf")

    docker = dc.dock.VinaGridRFDocker(exhaustiveness=1, detect_pockets=False)
    (score, (protein_docked, ligand_docked)) = docker.dock(
        protein_file, ligand_file)

    # Check returned files exist
    assert score.shape == (1,)
    assert os.path.exists(protein_docked)
    assert os.path.exists(ligand_docked)

</source>
<source file="systems/deepchem-2.3.0/deepchem/dock/tests/test_docking.py" startline="92" endline="110" pcid="471">
  def test_pocket_vina_grid_rf_docker_dock(self):
    """Test that VinaGridRFDocker can dock."""
    if sys.version_info >= (3, 0):
      return

    current_dir = os.path.dirname(os.path.realpath(__file__))
    protein_file = os.path.join(current_dir, "1jld_protein.pdb")
    ligand_file = os.path.join(current_dir, "1jld_ligand.sdf")

    docker = dc.dock.VinaGridRFDocker(exhaustiveness=1, detect_pockets=True)
    (score, (protein_docked, ligand_docked)) = docker.dock(
        protein_file, ligand_file, dry_run=True)

    # Check returned files exist
    if sys.version_info >= (3, 0):
      return

    assert score.shape == (1,)

</source>
</class>

<class classid="41" nclones="2" nlines="10" similarity="100">
<source file="systems/deepchem-2.3.0/deepchem/models/optimizers.py" startline="71" endline="82" pcid="503">
  def _create_optimizer(self, global_step):
    if isinstance(self.learning_rate, LearningRateSchedule):
      learning_rate = self.learning_rate._create_tensor(global_step)
    else:
      learning_rate = self.learning_rate
    return tf.train.AdamOptimizer(
        learning_rate=learning_rate,
        beta1=self.beta1,
        beta2=self.beta2,
        epsilon=self.epsilon)


</source>
<source file="systems/deepchem-2.3.0/deepchem/models/optimizers.py" startline="109" endline="120" pcid="505">
  def _create_optimizer(self, global_step):
    if isinstance(self.learning_rate, LearningRateSchedule):
      learning_rate = self.learning_rate._create_tensor(global_step)
    else:
      learning_rate = self.learning_rate
    return tf.train.RMSPropOptimizer(
        learning_rate=learning_rate,
        momentum=self.momentum,
        decay=self.decay,
        epsilon=self.epsilon)


</source>
</class>

<class classid="42" nclones="7" nlines="15" similarity="73">
<source file="systems/deepchem-2.3.0/deepchem/models/text_cnn.py" startline="216" endline="234" pcid="520">
  def default_generator(self,
                        dataset,
                        epochs=1,
                        mode='fit',
                        deterministic=True,
                        pad_batches=True):
    """Transfer smiles strings to fixed length integer vectors"""
    for epoch in range(epochs):
      for (X_b, y_b, w_b, ids_b) in dataset.iterbatches(
          batch_size=self.batch_size,
          deterministic=deterministic,
          pad_batches=pad_batches):
        if y_b is not None:
          if self.mode == 'classification':
            y_b = to_one_hot(y_b.flatten(), 2).reshape(-1, self.n_tasks, 2)
        # Transform SMILES sequence to integers
        X_b = self.smiles_to_seq_batch(ids_b)
        yield ([X_b], [y_b], [w_b])

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/chemnet_models.py" startline="178" endline="194" pcid="1226">
  def default_generator(self,
                        dataset,
                        epochs=1,
                        mode='fit',
                        deterministic=True,
                        pad_batches=True):
    for epoch in range(epochs):
      for (X_b, y_b, w_b, ids_b) in dataset.iterbatches(
          batch_size=self.batch_size,
          deterministic=deterministic,
          pad_batches=pad_batches):
        if self.mode == 'classification':
          y_b = to_one_hot(y_b.flatten(), self.n_classes).reshape(
              -1, self.n_tasks, self.n_classes)
        yield ([X_b], [y_b], [w_b])


</source>
<source file="systems/deepchem-2.3.0/deepchem/models/keras_model.py" startline="897" endline="935" pcid="1213">
  def default_generator(self,
                        dataset,
                        epochs=1,
                        mode='fit',
                        deterministic=True,
                        pad_batches=True):
    """Create a generator that iterates batches for a dataset.

    Subclasses may override this method to customize how model inputs are
    generated from the data.

    Parameters
    ----------
    dataset: Dataset
      the data to iterate
    epochs: int
      the number of times to iterate over the full dataset
    mode: str
      allowed values are 'fit' (called during training), 'predict' (called
      during prediction), and 'uncertainty' (called during uncertainty
      prediction)
    deterministic: bool
      whether to iterate over the dataset in order, or randomly shuffle the
      data for each epoch
    pad_batches: bool
      whether to pad each batch up to this model's preferred batch size

    Returns
    -------
    a generator that iterates batches, each represented as a tuple of lists:
    ([inputs], [outputs], [weights])
    """
    for epoch in range(epochs):
      for (X_b, y_b, w_b, ids_b) in dataset.iterbatches(
          batch_size=self.batch_size,
          deterministic=deterministic,
          pad_batches=pad_batches):
        yield ([X_b], [y_b], [w_b])

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/robust_multitask.py" startline="167" endline="182" pcid="1322">
  def default_generator(self,
                        dataset,
                        epochs=1,
                        mode='fit',
                        deterministic=True,
                        pad_batches=True):
    for epoch in range(epochs):
      for (X_b, y_b, w_b, ids_b) in dataset.iterbatches(
          batch_size=self.batch_size,
          deterministic=deterministic,
          pad_batches=pad_batches):
        if y_b is not None:
          y_b = to_one_hot(y_b.flatten(), self.n_classes).reshape(
              -1, self.n_tasks, self.n_classes)
        yield ([X_b], [y_b], [w_b])

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/fcnet.py" startline="128" endline="144" pcid="524">
  def default_generator(self,
                        dataset,
                        epochs=1,
                        mode='fit',
                        deterministic=True,
                        pad_batches=True):
    for epoch in range(epochs):
      for (X_b, y_b, w_b, ids_b) in dataset.iterbatches(
          batch_size=self.batch_size,
          deterministic=deterministic,
          pad_batches=pad_batches):
        if y_b is not None:
          y_b = to_one_hot(y_b.flatten(), self.n_classes).reshape(
              -1, self.n_tasks, self.n_classes)
        yield ([X_b], [y_b], [w_b])


</source>
<source file="systems/deepchem-2.3.0/deepchem/models/scscore.py" startline="81" endline="93" pcid="546">
  def default_generator(self,
                        dataset,
                        epochs=1,
                        mode='fit',
                        deterministic=True,
                        pad_batches=True):
    for epoch in range(epochs):
      for (X_b, y_b, w_b, ids_b) in dataset.iterbatches(
          batch_size=self.batch_size,
          deterministic=deterministic,
          pad_batches=pad_batches):
        yield ([X_b[:, 0], X_b[:, 1]], [y_b], [w_b])

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/graph_models.py" startline="310" endline="323" pcid="1290">
  def default_generator(self,
                        dataset,
                        epochs=1,
                        mode='fit',
                        deterministic=True,
                        pad_batches=True):
    for epoch in range(epochs):
      for (X_b, y_b, w_b, ids_b) in dataset.iterbatches(
          batch_size=self.batch_size,
          deterministic=deterministic,
          pad_batches=pad_batches):
        yield (self.compute_features_on_batch(X_b), [y_b], [w_b])


</source>
</class>

<class classid="43" nclones="3" nlines="18" similarity="72">
<source file="systems/deepchem-2.3.0/deepchem/models/fcnet.py" startline="268" endline="285" pcid="527">
  def default_generator(self,
                        dataset,
                        epochs=1,
                        mode='fit',
                        deterministic=True,
                        pad_batches=True):
    for epoch in range(epochs):
      for (X_b, y_b, w_b, ids_b) in dataset.iterbatches(
          batch_size=self.batch_size,
          deterministic=deterministic,
          pad_batches=pad_batches):
        if mode == 'predict':
          dropout = np.array(0.0)
        else:
          dropout = np.array(1.0)
        yield ([X_b, dropout], [y_b], [w_b])


</source>
<source file="systems/deepchem-2.3.0/deepchem/models/fcnet.py" startline="344" endline="366" pcid="529">
  def default_generator(self,
                        dataset,
                        epochs=1,
                        mode='fit',
                        deterministic=True,
                        pad_batches=True):
    for epoch in range(epochs):
      for (X_b, y_b, w_b, ids_b) in dataset.iterbatches(
          batch_size=self.batch_size,
          deterministic=deterministic,
          pad_batches=pad_batches):
        if y_b is not None:
          y_b = y_b.reshape(-1, self.n_tasks, 1)
        if X_b is not None:
          if mode == 'fit':
            for transformer in self.fit_transformers:
              X_b = transformer.X_transform(X_b)
        if mode == 'predict':
          dropout = np.array(0.0)
        else:
          dropout = np.array(1.0)
        yield ([X_b, dropout], [y_b], [w_b])

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_kerasmodel.py" startline="174" endline="190" pcid="1096">
      def default_generator(self,
                            dataset,
                            epochs=1,
                            mode='fit',
                            deterministic=True,
                            pad_batches=True):
        for epoch in range(epochs):
          for (X_b, y_b, w_b, ids_b) in dataset.iterbatches(
              batch_size=self.batch_size,
              deterministic=deterministic,
              pad_batches=pad_batches):
            if mode == 'predict':
              dropout = np.array(0.0)
            else:
              dropout = np.array(1.0)
            yield ([X_b, dropout], [y_b], [w_b])

</source>
</class>

<class classid="44" nclones="2" nlines="17" similarity="77">
<source file="systems/deepchem-2.3.0/deepchem/models/xgboost_models/__init__.py" startline="20" endline="47" pcid="534">
  def __init__(self,
               model_instance=None,
               model_dir=None,
               verbose=False,
               **kwargs):
    """Abstract class for XGBoost models.
    Parameters:
    -----------
    model_instance: object
      Scikit-learn wrapper interface of xgboost
    model_dir: str
      Path to directory where model will be stored.
    """
    if model_dir is not None:
      if not os.path.exists(model_dir):
        os.makedirs(model_dir)
    else:
      model_dir = tempfile.mkdtemp()
    self.model_dir = model_dir
    self.model_instance = model_instance
    self.model_class = model_instance.__class__

    self.verbose = verbose
    if 'early_stopping_rounds' in kwargs:
      self.early_stopping_rounds = kwargs['early_stopping_rounds']
    else:
      self.early_stopping_rounds = 50

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/models.py" startline="34" endline="59" pcid="1231">
  def __init__(self,
               model_instance=None,
               model_dir=None,
               verbose=True,
               **kwargs):
    """Abstract class for all models.
    Parameters:
    -----------
    model_instance: object
      Wrapper around ScikitLearn/Keras/Tensorflow model object.
    model_dir: str
      Path to directory where model will be stored.
    """
    self.model_dir_is_temp = False
    if model_dir is not None:
      if not os.path.exists(model_dir):
        os.makedirs(model_dir)
    else:
      model_dir = tempfile.mkdtemp()
      self.model_dir_is_temp = True
    self.model_dir = model_dir
    self.model_instance = model_instance
    self.model_class = model_instance.__class__

    self.verbose = verbose

</source>
</class>

<class classid="45" nclones="3" nlines="14" similarity="73">
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/models/ontology.py" startline="211" endline="227" pcid="586">
  def create_estimator_inputs(self, feature_columns, weight_column, features,
                              labels, mode):
    tensors = {}
    for layer, column in zip(self.features, feature_columns):
      tensors[layer] = tf.feature_column.input_layer(features, [column])
    if weight_column is not None:
      tensors[self.task_weights[0]] = tf.feature_column.input_layer(
          features, [weight_column])
    if labels is not None:
      if self.mode == 'regression':
        tensors[self.labels[0]] = tf.cast(labels, self.labels[0].dtype)
      else:
        tensors[self.labels[0]] = tf.one_hot(
            tf.cast(labels, tf.int32), self.n_classes)
    return tensors


</source>
<source file="systems/deepchem-2.3.0/deepchem/models/robust_multitask.py" startline="183" endline="196" pcid="1323">
  def create_estimator_inputs(self, feature_columns, weight_column, features,
                              labels, mode):
    tensors = {}
    for layer, column in zip(self.features, feature_columns):
      tensors[layer] = tf.feature_column.input_layer(features, [column])
    if weight_column is not None:
      tensors[self.task_weights[0]] = tf.feature_column.input_layer(
          features, [weight_column])
    if labels is not None:
      tensors[self.labels[0]] = tf.one_hot(
          tf.cast(labels, tf.int32), self.n_classes)
    return tensors


</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tensor_graph.py" startline="1319" endline="1346" pcid="660">
  def create_estimator_inputs(self, feature_columns, weight_column, features,
                              labels, mode):
    """This is called by make_estimator() to create tensors for the inputs.

    feature_columns and weight_column are the arguments passed to
    make_estimator().  features, labels, and mode are the arguments passed to
    the estimator's model function.  This method creates and returns a dict with
    one entry for every Feature, Label, or Weights layer in the graph.  The keys
    are the layers, and the values are the tensors that correspond to them.

    Any subclass that overrides default_generator() must also override this
    method.
    """
    if self.__class__.default_generator != TensorGraph.default_generator:
      raise ValueError(
          "Class overrides default_generator() but not create_estimator_inputs()"
      )
    tensors = {}
    for layer, column in zip(self.features, feature_columns):
      tensors[layer] = tf.feature_column.input_layer(features, [column])
    if weight_column is not None:
      tensors[self.task_weights[0]] = tf.feature_column.input_layer(
          features, [weight_column])
    if labels is not None:
      tensors[self.labels[0]] = tf.cast(labels, self.labels[0].dtype)
    return tensors


</source>
</class>

<class classid="46" nclones="2" nlines="33" similarity="72">
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/models/resnet50.py" startline="32" endline="65" pcid="591">
  def identity_block(self, input, kernel_size, filters):
    filters1, filters2, filters3 = filters

    output = Conv2D(
        num_outputs=filters1,
        kernel_size=1,
        activation='linear',
        padding='same',
        in_layers=[input])
    output = BatchNorm(in_layers=[output])
    output = ReLU(output)

    output = Conv2D(
        num_outputs=filters2,
        kernel_size=kernel_size,
        activation='linear',
        padding='same',
        in_layers=[input])
    output = BatchNorm(in_layers=[output])
    output = ReLU(output)

    output = Conv2D(
        num_outputs=filters3,
        kernel_size=1,
        activation='linear',
        padding='same',
        in_layers=[input])
    output = BatchNorm(in_layers=[output])

    output = Add(in_layers=[output, input])
    output = ReLU(output)

    return output

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/models/resnet50.py" startline="66" endline="108" pcid="592">
  def conv_block(self, input, kernel_size, filters, strides=2):
    filters1, filters2, filters3 = filters

    output = Conv2D(
        num_outputs=filters1,
        kernel_size=1,
        stride=strides,
        activation='linear',
        padding='same',
        in_layers=[input])
    output = BatchNorm(in_layers=[output])
    output = ReLU(output)

    output = Conv2D(
        num_outputs=filters2,
        kernel_size=kernel_size,
        activation='linear',
        padding='same',
        in_layers=[output])
    output = BatchNorm(in_layers=[output])
    output = ReLU(output)

    output = Conv2D(
        num_outputs=filters3,
        kernel_size=1,
        activation='linear',
        padding='same',
        in_layers=[output])
    output = BatchNorm(in_layers=[output])

    shortcut = Conv2D(
        num_outputs=filters3,
        kernel_size=1,
        stride=strides,
        activation='linear',
        padding='same',
        in_layers=[input])
    shortcut = BatchNorm(in_layers=[shortcut])
    output = Add(in_layers=[shortcut, output])
    output = ReLU(output)

    return output

</source>
</class>

<class classid="47" nclones="2" nlines="25" similarity="84">
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/models/symmetry_function_regression.py" startline="93" endline="120" pcid="597">
    atom_feats = np.array(X_b[:, :, 1:], dtype=np.float32)
    return [atom_feats, atom_flags]

  def default_generator(self,
                        dataset,
                        epochs=1,
                        predict=False,
                        deterministic=True,
                        pad_batches=True):
    for epoch in range(epochs):
      if not predict:
        print('Starting epoch %i' % epoch)
      for (X_b, y_b, w_b, ids_b) in dataset.iterbatches(
          batch_size=self.batch_size,
          deterministic=deterministic,
          pad_batches=pad_batches):

        feed_dict = dict()
        if y_b is not None and not predict:
          feed_dict[self.labels[0]] = y_b
        if w_b is not None and not predict:
          feed_dict[self.task_weights[0]] = w_b

        atom_feats, atom_flags = self.compute_features_on_batch(X_b)
        atom_feats = atom_feats.reshape(-1, self.max_atoms * self.n_feat)
        atom_flags = atom_flags.reshape(-1, self.max_atoms * self.max_atoms)
        feed_dict[self.atom_feats] = atom_feats
        feed_dict[self.atom_flags] = atom_flags
</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/models/symmetry_function_regression.py" startline="385" endline="413" pcid="608">
    atom_numbers = np.array(X_b[:, :, 0], dtype=np.int32)
    atom_feats = np.array(X_b[:, :, :], dtype=np.float32)

    return [atom_feats, atom_numbers, atom_flags]

  def default_generator(self,
                        dataset,
                        epochs=1,
                        predict=False,
                        deterministic=True,
                        pad_batches=True):
    for epoch in range(epochs):
      if not predict:
        print('Starting epoch %i' % epoch)
      for (X_b, y_b, w_b, ids_b) in dataset.iterbatches(
          batch_size=self.batch_size,
          deterministic=deterministic,
          pad_batches=pad_batches):

        feed_dict = dict()
        if y_b is not None and not predict:
          feed_dict[self.labels[0]] = y_b
        if w_b is not None and not predict:
          feed_dict[self.task_weights[0]] = w_b

        atom_feats, atom_numbers, atom_flags = self.compute_features_on_batch(
            X_b)
        atom_feats = atom_feats.reshape(-1, self.max_atoms * 4)
        atom_flags = atom_flags.reshape(-1, self.max_atoms * self.max_atoms)
</source>
</class>

<class classid="48" nclones="2" nlines="28" similarity="92">
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tensor_graph.py" startline="539" endline="590" pcid="624">
  def predict_uncertainty(self, dataset, masks=50):
    """
    Predict the model's outputs, along with the uncertainty in each one.

    The uncertainty is computed as described in https://arxiv.org/abs/1703.04977.
    It involves repeating the prediction many times with different dropout masks.
    The prediction is computed as the average over all the predictions.  The
    uncertainty includes both the variation among the predicted values (epistemic
    uncertainty) and the model's own estimates for how well it fits the data
    (aleatoric uncertainty).  Not all models support uncertainty prediction.

    Parameters
    ----------
    dataset: dc.data.Dataset
      Dataset to make prediction on
    masks: int
      the number of dropout masks to average over

    Returns
    -------
    for each output, a tuple (y_pred, y_std) where y_pred is the predicted
    value of the output, and each element of y_std estimates the standard
    deviation of the corresponding element of y_pred
    """
    sum_pred = []
    sum_sq_pred = []
    sum_var = []
    for i in range(masks):
      generator = self.default_generator(
          dataset, predict=True, pad_batches=False)
      results = self._predict(generator, [], self.default_outputs, True)
      if len(sum_pred) == 0:
        for p, v in results:
          sum_pred.append(p)
          sum_sq_pred.append(p * p)
          sum_var.append(v)
      else:
        for j, (p, v) in enumerate(results):
          sum_pred[j] += p
          sum_sq_pred[j] += p * p
          sum_var[j] += v
    output = []
    std = []
    for i in range(len(sum_pred)):
      p = sum_pred[i] / masks
      output.append(p)
      std.append(np.sqrt(sum_sq_pred[i] / masks - p * p + sum_var[i] / masks))
    if len(output) == 1:
      return (output[0], std[0])
    else:
      return zip(output, std)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/keras_model.py" startline="713" endline="764" pcid="1208">
  def predict_uncertainty(self, dataset, masks=50):
    """
    Predict the model's outputs, along with the uncertainty in each one.

    The uncertainty is computed as described in https://arxiv.org/abs/1703.04977.
    It involves repeating the prediction many times with different dropout masks.
    The prediction is computed as the average over all the predictions.  The
    uncertainty includes both the variation among the predicted values (epistemic
    uncertainty) and the model's own estimates for how well it fits the data
    (aleatoric uncertainty).  Not all models support uncertainty prediction.

    Parameters
    ----------
    dataset: dc.data.Dataset
      Dataset to make prediction on
    masks: int
      the number of dropout masks to average over

    Returns
    -------
    for each output, a tuple (y_pred, y_std) where y_pred is the predicted
    value of the output, and each element of y_std estimates the standard
    deviation of the corresponding element of y_pred
    """
    sum_pred = []
    sum_sq_pred = []
    sum_var = []
    for i in range(masks):
      generator = self.default_generator(
          dataset, mode='uncertainty', pad_batches=False)
      results = self._predict(generator, [], None, True)
      if len(sum_pred) == 0:
        for p, v in results:
          sum_pred.append(p)
          sum_sq_pred.append(p * p)
          sum_var.append(v)
      else:
        for j, (p, v) in enumerate(results):
          sum_pred[j] += p
          sum_sq_pred[j] += p * p
          sum_var[j] += v
    output = []
    std = []
    for i in range(len(sum_pred)):
      p = sum_pred[i] / masks
      output.append(p)
      std.append(np.sqrt(sum_sq_pred[i] / masks - p * p + sum_var[i] / masks))
    if len(output) == 1:
      return (output[0], std[0])
    else:
      return zip(output, std)

</source>
</class>

<class classid="49" nclones="5" nlines="46" similarity="70">
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/chemnet_layers.py" startline="79" endline="146" pcid="675">
  def _build_layer_components(self):
    """Builds the layers components and set _layers attribute."""
    self.conv_block1 = [
        Conv2D(
            self.num_filters,
            kernel_size=(1, 1),
            strides=1,
            padding="same",
            activation=tf.nn.relu)
    ]

    self.conv_block2 = [
        Conv2D(
            filters=self.num_filters,
            kernel_size=(1, 1),
            strides=1,
            activation=tf.nn.relu,
            padding="same")
    ]

    self.conv_block2.append(
        Conv2D(
            filters=self.num_filters,
            kernel_size=(3, 3),
            strides=1,
            activation=tf.nn.relu,
            padding="same"))

    self.conv_block3 = [
        Conv2D(
            filters=self.num_filters,
            kernel_size=1,
            strides=1,
            activation=tf.nn.relu,
            padding="same")
    ]

    self.conv_block3.append(
        Conv2D(
            filters=int(self.num_filters * 1.5),
            kernel_size=(3, 3),
            strides=1,
            activation=tf.nn.relu,
            padding="same"))
    self.conv_block3.append(
        Conv2D(
            filters=self.num_filters * 2,
            kernel_size=(3, 3),
            strides=1,
            activation=tf.nn.relu,
            padding="same"))

    self.conv_block4 = [
        Conv2D(
            filters=self.input_dim,
            kernel_size=(1, 1),
            strides=1,
            padding="same")
    ]

    self.concat_layer = Concatenate()
    self.add_layer = Add()
    self.activation_layer = ReLU()

    self._layers = self.conv_block1 + self.conv_block2 + self.conv_block3 + self.conv_block4
    self._layers.extend(
        [self.concat_layer, self.add_layer, self.activation_layer])

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/chemnet_layers.py" startline="290" endline="339" pcid="681">
  def _build_layer_components(self):
    """Builds the layers components and set _layers attribute."""
    self.conv_block1 = [
        Conv2D(
            self.num_filters,
            kernel_size=(1, 1),
            strides=1,
            padding="same",
            activation=tf.nn.relu)
    ]

    self.conv_block2 = [
        Conv2D(
            filters=self.num_filters,
            kernel_size=1,
            strides=1,
            activation=tf.nn.relu,
            padding="same")
    ]
    self.conv_block2.append(
        Conv2D(
            filters=int(self.num_filters * 1.16),
            kernel_size=(1, 3),
            strides=1,
            activation=tf.nn.relu,
            padding="same"))
    self.conv_block2.append(
        Conv2D(
            filters=int(self.num_filters * 1.33),
            kernel_size=(3, 1),
            strides=1,
            activation=tf.nn.relu,
            padding="same"))

    self.conv_block3 = [
        Conv2D(
            filters=self.input_dim,
            kernel_size=(1, 1),
            strides=1,
            padding="same")
    ]

    self.concat_layer = Concatenate()
    self.add_layer = Add()
    self.activation_layer = ReLU()

    self._layers = self.conv_block1 + self.conv_block2 + self.conv_block3
    self._layers.extend(
        [self.concat_layer, self.add_layer, self.activation_layer])

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/chemnet_layers.py" startline="458" endline="524" pcid="687">
  def _build_layer_components(self):
    """Builds the layers components and set _layers attribute."""
    self.max_pool1 = MaxPool2D(pool_size=(3, 3), strides=2, padding="valid")

    self.conv_block1 = [
        Conv2D(
            self.num_filters,
            kernel_size=1,
            strides=1,
            padding="same",
            activation=tf.nn.relu)
    ]
    self.conv_block1.append(
        Conv2D(
            int(self.num_filters * 1.5),
            kernel_size=3,
            strides=2,
            padding="valid",
            activation=tf.nn.relu))

    self.conv_block2 = [
        Conv2D(
            filters=self.num_filters,
            kernel_size=1,
            strides=1,
            activation=tf.nn.relu,
            padding="same")
    ]
    self.conv_block2.append(
        Conv2D(
            filters=int(self.num_filters * 1.125),
            kernel_size=3,
            strides=2,
            activation=tf.nn.relu,
            padding="valid"))

    self.conv_block3 = [
        Conv2D(
            filters=self.num_filters,
            kernel_size=1,
            strides=1,
            activation=tf.nn.relu,
            padding="same")
    ]
    self.conv_block3.append(
        Conv2D(
            filters=int(self.num_filters * 1.125),
            kernel_size=(3, 1),
            strides=1,
            activation=tf.nn.relu,
            padding="same"))

    self.conv_block3.append(
        Conv2D(
            filters=int(self.num_filters * 1.25),
            kernel_size=(3, 3),
            strides=2,
            activation=tf.nn.relu,
            padding="valid"))

    self.concat_layer = Concatenate()
    self.activation_layer = ReLU()

    self._layers = self.conv_block1 + self.conv_block2 + self.conv_block3
    self._layers.extend(
        [self.max_pool1, self.concat_layer, self.activation_layer])

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/chemnet_layers.py" startline="196" endline="242" pcid="678">
  def _build_layer_components(self):
    """Builds the layers components and set _layers attribute."""
    self.conv_block1 = [
        Conv2D(
            self.num_filters,
            kernel_size=1,
            strides=1,
            padding="same",
            activation=tf.nn.relu)
    ]

    self.conv_block2 = [
        Conv2D(
            filters=self.num_filters,
            kernel_size=(1, 1),
            strides=1,
            activation=tf.nn.relu,
            padding="same")
    ]
    self.conv_block2.append(
        Conv2D(
            filters=int(self.num_filters * 1.25),
            kernel_size=(1, 7),
            strides=1,
            activation=tf.nn.relu,
            padding="same"))
    self.conv_block2.append(
        Conv2D(
            filters=int(self.num_filters * 1.5),
            kernel_size=(7, 1),
            strides=1,
            activation=tf.nn.relu,
            padding="same"))

    self.conv_block3 = [
        Conv2D(
            filters=self.input_dim, kernel_size=1, strides=1, padding="same")
    ]

    self.concat_layer = Concatenate()
    self.add_layer = Add()
    self.activation_layer = ReLU()

    self._layers = self.conv_block1 + self.conv_block2 + self.conv_block3
    self._layers.extend(
        [self.concat_layer, self.add_layer, self.activation_layer])

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/chemnet_layers.py" startline="380" endline="422" pcid="684">
  def _build_layer_components(self):
    """Builds the layers components and set _layers attribute."""
    self.max_pool1 = MaxPool2D(pool_size=(3, 3), strides=2, padding="valid")

    self.conv_block1 = [
        Conv2D(
            int(self.num_filters * 1.5),
            kernel_size=(3, 3),
            strides=2,
            padding="valid",
            activation=tf.nn.relu)
    ]

    self.conv_block2 = [
        Conv2D(
            filters=self.num_filters,
            kernel_size=1,
            strides=1,
            activation=tf.nn.relu,
            padding="same")
    ]
    self.conv_block2.append(
        Conv2D(
            filters=self.num_filters,
            kernel_size=3,
            strides=1,
            activation=tf.nn.relu,
            padding="same"))
    self.conv_block2.append(
        Conv2D(
            filters=int(self.num_filters * 1.5),
            kernel_size=3,
            strides=2,
            activation=tf.nn.relu,
            padding="valid"))

    self.concat_layer = Concatenate()
    self.activation_layer = ReLU()

    self._layers = self.conv_block1 + self.conv_block2
    self._layers.extend(
        [self.max_pool1, self.concat_layer, self.activation_layer])

</source>
</class>

<class classid="50" nclones="4" nlines="13" similarity="71">
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/chemnet_layers.py" startline="243" endline="263" pcid="679">
  def call(self, inputs):
    """Invoked when __call__ method of the layer is used."""
    conv1 = inputs
    for layer in self.conv_block1:
      conv1 = layer(conv1)

    conv2 = inputs
    for layer in self.conv_block2:
      conv2 = layer(conv2)

    concat_conv = self.concat_layer([conv1, conv2])

    conv3 = concat_conv
    for layer in self.conv_block3:
      conv3 = layer(conv3)

    output = self.add_layer([conv3, inputs])
    output = self.activation_layer(output)
    return output


</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/chemnet_layers.py" startline="340" endline="360" pcid="682">
  def call(self, inputs):
    """Invoked when __call__ method of the layer is used."""
    conv1 = inputs
    for layer in self.conv_block1:
      conv1 = layer(conv1)

    conv2 = inputs
    for layer in self.conv_block2:
      conv2 = layer(conv2)

    concat_conv = self.concat_layer([conv1, conv2])

    conv3 = concat_conv
    for layer in self.conv_block3:
      conv3 = layer(conv3)

    output = self.add_layer([conv3, inputs])
    output = self.activation_layer(output)
    return output


</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/chemnet_layers.py" startline="525" endline="542" pcid="688">
  def call(self, inputs):
    """Invoked when __call__ method of the layer is used."""
    maxpool1 = self.max_pool1(inputs)
    conv1 = inputs
    for layer in self.conv_block1:
      conv1 = layer(conv1)

    conv2 = inputs
    for layer in self.conv_block2:
      conv2 = layer(conv2)

    conv3 = inputs
    for layer in self.conv_block3:
      conv3 = layer(conv3)

    concat = self.concat_layer([maxpool1, conv1, conv2, conv3])
    output = self.activation_layer(concat)
    return output
</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/chemnet_layers.py" startline="423" endline="438" pcid="685">
  def call(self, inputs):
    """Invoked when __call__ method of the layer is used."""
    maxpool1 = self.max_pool1(inputs)
    conv1 = inputs
    for layer in self.conv_block1:
      conv1 = layer(conv1)

    conv2 = inputs
    for layer in self.conv_block2:
      conv2 = layer(conv2)

    output = self.concat_layer([maxpool1, conv1, conv2])
    output = self.activation_layer(output)
    return output


</source>
</class>

<class classid="51" nclones="2" nlines="12" similarity="100">
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/model_ops.py" startline="234" endline="264" pcid="715">
def random_uniform_variable(shape,
                            low,
                            high,
                            dtype=tf.float32,
                            name=None,
                            seed=None):
  """Instantiates an variable filled with
  samples drawn from a uniform distribution and returns it.

  Parameters
  ----------
  shape: Tuple of integers, shape of returned variable.
  low: Float, lower boundary of the output inteval.
  high: Float, upper boundary of the output interval.
  dtype: Tensorflow dtype
  name: String, name of returned variable.
  seed: Integer, random seed.

  Returns
  -------
  A tf.Variable, filled with drawn samples.
  """
  shape = tuple(map(int, shape))
  if seed is None:
    # ensure that randomness is conditioned by the Numpy RNG
    seed = np.random.randint(10e8)
  value = tf.random_uniform_initializer(
      low, high, dtype=dtype, seed=seed)(shape)
  return tf.Variable(value, dtype=dtype, name=name)


</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/model_ops.py" startline="265" endline="295" pcid="716">
def random_normal_variable(shape,
                           mean,
                           scale,
                           dtype=tf.float32,
                           name=None,
                           seed=None):
  """Instantiates an Tensorflow variable filled with
  samples drawn from a normal distribution and returns it.

  Parameters
  ----------
  shape: Tuple of integers, shape of returned Tensorflow variable.
  mean: Float, mean of the normal distribution.
  scale: Float, standard deviation of the normal distribution.
  dtype: Tensorflow dtype
  name: String, name of returned Tensorflow variable.
  seed: Integer, random seed.

  Returns
  -------
  A tf.Variable, filled with drawn samples.
  """
  shape = tuple(map(int, shape))
  if seed is None:
    # ensure that randomness is conditioned by the Numpy RNG
    seed = np.random.randint(10e8)
  value = tf.random_normal_initializer(
      mean, scale, dtype=dtype, seed=seed)(shape)
  return tf.Variable(value, dtype=dtype, name=name)


</source>
</class>

<class classid="52" nclones="3" nlines="16" similarity="70">
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers_pickle.py" startline="385" endline="404" pcid="778">
  layer = MaxPool3D(in_layers=feature)
  tg.add_output(layer)
  tg.set_loss(layer)
  tg.build()
  tg.save()


def test_GraphConv_pickle():
  tg = TensorGraph()
  atom_features = Feature(shape=(None, 75))
  degree_slice = Feature(shape=(None, 2), dtype=tf.int32)
  membership = Feature(shape=(None,), dtype=tf.int32)

  deg_adjs = []
  for i in range(0, 10 + 1):
    deg_adj = Feature(shape=(None, i + 1), dtype=tf.int32)
    deg_adjs.append(deg_adj)
  layer = GraphConv(
      64,
      activation_fn=tf.nn.relu,
</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers_pickle.py" startline="421" endline="438" pcid="780">
  layer = GraphPool(in_layers=[atom_features, degree_slice, membership] +
                    deg_adjs)
  tg.set_loss(layer)
  tg.build()
  tg.save()


def test_GraphGather_Pickle():
  tg = TensorGraph()
  atom_features = Feature(shape=(None, 75))
  degree_slice = Feature(shape=(None, 2), dtype=tf.int32)
  membership = Feature(shape=(None,), dtype=tf.int32)
  deg_adjs = []
  for i in range(0, 10 + 1):
    deg_adj = Feature(shape=(None, i + 1), dtype=tf.int32)
    deg_adjs.append(deg_adj)
  layer = GraphGather(
      batch_size=tg.batch_size,
</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers_pickle.py" startline="405" endline="420" pcid="779">
      in_layers=[atom_features, degree_slice, membership] + deg_adjs)
  tg.add_output(layer)
  tg.set_loss(layer)
  tg.build()
  tg.save()


def test_GraphPool_Pickle():
  tg = TensorGraph()
  atom_features = Feature(shape=(None, 75))
  degree_slice = Feature(shape=(None, 2), dtype=tf.int32)
  membership = Feature(shape=(None,), dtype=tf.int32)
  deg_adjs = []
  for i in range(0, 10 + 1):
    deg_adj = Feature(shape=(None, i + 1), dtype=tf.int32)
    deg_adjs.append(deg_adj)
</source>
</class>

<class classid="53" nclones="4" nlines="11" similarity="75">
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers_pickle.py" startline="459" endline="472" pcid="783">
  layer = WeightedError(in_layers=[feature, feature])
  tg.add_output(layer)
  tg.set_loss(layer)
  tg.build()
  tg.save()


def test_Weave_pickle():
  tg = TensorGraph()
  atom_feature = Feature(shape=(None, 75))
  pair_feature = Feature(shape=(None, 14))
  pair_split = Feature(shape=(None,), dtype=tf.int32)
  atom_to_pair = Feature(shape=(None, 2), dtype=tf.int32)
  weave = WeaveLayer(
</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers_pickle.py" startline="495" endline="509" pcid="786">
  Embedding = DTNNEmbedding(in_layers=[atom_numbers])
  tg.add_output(Embedding)
  tg.set_loss(Embedding)
  tg.build()
  tg.save()


def test_DTNNStep_pickle():
  tg = TensorGraph()
  atom_features = Feature(shape=(None, 30))
  distance = Feature(shape=(None, 100))
  distance_membership_i = Feature(shape=(None,), dtype=tf.int32)
  distance_membership_j = Feature(shape=(None,), dtype=tf.int32)
  DTNN = DTNNStep(in_layers=[
      atom_features, distance, distance_membership_i, distance_membership_j
</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers_pickle.py" startline="473" endline="484" pcid="784">
      in_layers=[atom_feature, pair_feature, pair_split, atom_to_pair])
  tg.add_output(weave)
  tg.set_loss(weave)
  tg.build()
  tg.save()


def test_WeaveGather_pickle():
  tg = TensorGraph()
  atom_feature = Feature(shape=(None, 75))
  atom_split = Feature(shape=(None,), dtype=tf.int32)
  weave_gather = WeaveGather(
</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers_pickle.py" startline="558" endline="569" pcid="791">
  Gather = DAGGather(in_layers=[atom_features, membership])
  tg.add_output(Gather)
  tg.set_loss(Gather)
  tg.build()
  tg.save()


def test_MP_pickle():
  tg = TensorGraph()
  atom_feature = Feature(shape=(None, 75))
  pair_feature = Feature(shape=(None, 14))
  atom_to_pair = Feature(shape=(None, 2), dtype=tf.int32)
</source>
</class>

<class classid="54" nclones="3" nlines="13" similarity="71">
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers_pickle.py" startline="570" endline="587" pcid="792">
  MP = MessagePassing(5, in_layers=[atom_feature, pair_feature, atom_to_pair])
  tg.add_output(MP)
  tg.set_loss(MP)
  tg.build()
  tg.save()


def test_AttnLSTM_pickle():
  """Tests that AttnLSTM can be pickled."""
  max_depth = 5
  n_test = 5
  n_support = 5
  n_feat = 10

  tg = TensorGraph(batch_size=n_test)
  test = Feature(shape=(None, n_feat))
  support = Feature(shape=(None, n_feat))
  out = AttnLSTMEmbedding(
</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers_pickle.py" startline="602" endline="618" pcid="794">
  lstm = LSTMStep(n_feat, 2 * n_feat, in_layers=[y, state_zero, state_one])
  tg.add_output(lstm)
  tg.set_loss(lstm)
  tg.build()
  tg.save()


def test_IterRefLSTM_pickle():
  """Tests that IterRefLSTM can be pickled."""
  n_feat = 10
  max_depth = 5
  n_test = 5
  n_support = 5
  tg = TensorGraph()
  test = Feature(shape=(None, n_feat))
  support = Feature(shape=(None, n_feat))
  lstm = IterRefLSTMEmbedding(
</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers_pickle.py" startline="630" endline="643" pcid="796">
  Gather = SetGather(5, 16, in_layers=[atom_feature, atom_split])
  tg.add_output(Gather)
  tg.set_loss(Gather)
  tg.build()
  tg.save()


def test_AtomicDifferentialDense_pickle():
  max_atoms = 23
  atom_features = 100
  tg = TensorGraph()
  atom_feature = Feature(shape=(None, max_atoms, atom_features))
  atom_numbers = Feature(shape=(None, max_atoms))
  atomic_differential_dense = AtomicDifferentiatedDense(
</source>
</class>

<class classid="55" nclones="3" nlines="12" similarity="83">
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_sequential.py" startline="14" endline="26" pcid="805">
  def test_single_task_classifier(self):
    n_data_points = 20
    n_features = 2
    X = np.random.rand(n_data_points, n_features)
    y = [[0, 1] for x in range(n_data_points)]
    dataset = dc.data.NumpyDataset(X, y)
    model = dc.models.Sequential(loss="binary_crossentropy", learning_rate=0.01)
    model.add(Dense(out_channels=2))
    model.add(SoftMax())
    model.fit(dataset, nb_epoch=1000)
    prediction = np.squeeze(model.predict_on_batch(X))
    assert_true(np.all(np.isclose(prediction, y, atol=0.4)))

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_sequential.py" startline="27" endline="39" pcid="806">
  def test_fit_twice(self):
    n_data_points = 20
    n_features = 2
    X = np.random.rand(n_data_points, n_features)
    y = [[0, 1] for x in range(n_data_points)]
    dataset = dc.data.NumpyDataset(X, y)
    model = dc.models.Sequential(loss="binary_crossentropy", learning_rate=0.01)
    model.add(Dense(out_channels=2))
    model.add(SoftMax())
    # Should be able to call fit twice without failure.
    model.fit(dataset, nb_epoch=1000)
    model.fit(dataset, nb_epoch=1000)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_sequential.py" startline="40" endline="50" pcid="807">
  def test_single_task_regressor(self):
    n_data_points = 20
    n_features = 2
    X = np.random.rand(n_data_points, n_features)
    y = [[0.5] for x in range(n_data_points)]
    dataset = dc.data.NumpyDataset(X, y)
    model = dc.models.Sequential(loss="mse", learning_rate=0.01)
    model.add(Dense(out_channels=1))
    model.fit(dataset, nb_epoch=1000)
    prediction = np.squeeze(model.predict_on_batch(X))
    assert_true(np.all(np.isclose(prediction, y, atol=3.0)))
</source>
</class>

<class classid="56" nclones="4" nlines="16" similarity="72">
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers_eager.py" startline="14" endline="40" pcid="808">
  def test_conv_1d(self):
    """Test invoking Conv1D in eager mode."""
    with context.eager_mode():
      width = 5
      in_channels = 2
      filters = 3
      kernel_size = 2
      batch_size = 10
      input = np.random.rand(batch_size, width, in_channels).astype(np.float32)
      layer = layers.Conv1D(filters, kernel_size)
      result = layer(input)
      self.assertEqual(result.shape[0], batch_size)
      self.assertEqual(result.shape[2], filters)
      assert len(layer.trainable_variables) == 2

      # Creating a second layer should produce different results, since it has
      # different random weights.

      layer2 = layers.Conv1D(filters, kernel_size)
      result2 = layer2(input)
      assert not np.allclose(result, result2)

      # But evaluating the first layer again should produce the same result as before.

      result3 = layer(input)
      assert np.allclose(result, result3)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers_eager.py" startline="215" endline="239" pcid="821">
  def test_time_series_dense(self):
    """Test invoking TimeSeriesDense in eager mode."""
    with context.eager_mode():
      in_dim = 2
      out_dim = 3
      n_steps = 6
      batch_size = 10
      input = np.random.rand(batch_size, n_steps, in_dim).astype(np.float32)
      layer = layers.TimeSeriesDense(out_dim)
      result = layer(input)
      assert result.shape == (batch_size, n_steps, out_dim)
      assert len(layer.trainable_variables) == 2

      # Creating a second layer should produce different results, since it has
      # different random weights.

      layer2 = layers.TimeSeriesDense(out_dim)
      result2 = layer2(input)
      assert not np.allclose(result, result2)

      # But evaluating the first layer again should produce the same result as before.

      result3 = layer(input)
      assert np.allclose(result, result3)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers_eager.py" startline="41" endline="64" pcid="809">
  def test_dense(self):
    """Test invoking Dense in eager mode."""
    with context.eager_mode():
      in_dim = 2
      out_dim = 3
      batch_size = 10
      input = np.random.rand(batch_size, in_dim).astype(np.float32)
      layer = layers.Dense(out_dim)
      result = layer(input)
      assert result.shape == (batch_size, out_dim)
      assert len(layer.trainable_variables) == 2

      # Creating a second layer should produce different results, since it has
      # different random weights.

      layer2 = layers.Dense(out_dim)
      result2 = layer2(input)
      assert not np.allclose(result, result2)

      # But evaluating the first layer again should produce the same result as before.

      result3 = layer(input)
      assert np.allclose(result, result3)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers_eager.py" startline="65" endline="87" pcid="810">
  def test_highway(self):
    """Test invoking Highway in eager mode."""
    with context.eager_mode():
      width = 5
      batch_size = 10
      input = np.random.rand(batch_size, width).astype(np.float32)
      layer = layers.Highway()
      result = layer(input)
      assert result.shape == (batch_size, width)
      assert len(layer.trainable_variables) == 4

      # Creating a second layer should produce different results, since it has
      # different random weights.

      layer2 = layers.Highway()
      result2 = layer2(input)
      assert not np.allclose(result, result2)

      # But evaluating the first layer again should produce the same result as before.

      result3 = layer(input)
      assert np.allclose(result, result3)

</source>
</class>

<class classid="57" nclones="2" nlines="19" similarity="100">
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers_eager.py" startline="151" endline="182" pcid="819">
  def test_gru(self):
    """Test invoking GRU in eager mode."""
    with context.eager_mode():
      batch_size = 10
      n_hidden = 7
      in_channels = 4
      n_steps = 6
      input = np.random.rand(batch_size, n_steps,
                             in_channels).astype(np.float32)
      layer = layers.GRU(n_hidden, batch_size)
      result, state = layer(input)
      assert result.shape == (batch_size, n_steps, n_hidden)
      assert len(layer.trainable_variables) == 3

      # Creating a second layer should produce different results, since it has
      # different random weights.

      layer2 = layers.GRU(n_hidden, batch_size)
      result2, state2 = layer2(input)
      assert not np.allclose(result, result2)

      # But evaluating the first layer again should produce the same result as before.

      result3, state3 = layer(input)
      assert np.allclose(result, result3)

      # But if we specify a different starting state, that should produce a
      # different result.

      result4, state4 = layer(input, initial_state=state3)
      assert not np.allclose(result, result4)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers_eager.py" startline="183" endline="214" pcid="820">
  def test_lstm(self):
    """Test invoking LSTM in eager mode."""
    with context.eager_mode():
      batch_size = 10
      n_hidden = 7
      in_channels = 4
      n_steps = 6
      input = np.random.rand(batch_size, n_steps,
                             in_channels).astype(np.float32)
      layer = layers.LSTM(n_hidden, batch_size)
      result, state = layer(input)
      assert result.shape == (batch_size, n_steps, n_hidden)
      assert len(layer.trainable_variables) == 3

      # Creating a second layer should produce different results, since it has
      # different random weights.

      layer2 = layers.LSTM(n_hidden, batch_size)
      result2, state2 = layer2(input)
      assert not np.allclose(result, result2)

      # But evaluating the first layer again should produce the same result as before.

      result3, state3 = layer(input)
      assert np.allclose(result, result3)

      # But if we specify a different starting state, that should produce a
      # different result.

      result4, state4 = layer(input, initial_state=state3)
      assert not np.allclose(result, result4)

</source>
</class>

<class classid="58" nclones="3" nlines="10" similarity="81">
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers_eager.py" startline="364" endline="375" pcid="837">
  def test_sparse_softmax_cross_entropy(self):
    """Test invoking SparseSoftMaxCrossEntropy in eager mode."""
    with context.eager_mode():
      batch_size = 10
      n_features = 5
      logits = np.random.rand(batch_size, n_features).astype(np.float32)
      labels = np.random.rand(batch_size).astype(np.int32)
      result = layers.SparseSoftMaxCrossEntropy()(labels, logits)
      expected = tf.nn.sparse_softmax_cross_entropy_with_logits(
          labels=labels, logits=logits)
      assert np.allclose(result, expected)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers_eager.py" startline="376" endline="387" pcid="838">
  def test_softmax_cross_entropy(self):
    """Test invoking SoftMaxCrossEntropy in eager mode."""
    with context.eager_mode():
      batch_size = 10
      n_features = 5
      logits = np.random.rand(batch_size, n_features).astype(np.float32)
      labels = np.random.rand(batch_size, n_features).astype(np.float32)
      result = layers.SoftMaxCrossEntropy()(labels, logits)
      expected = tf.nn.softmax_cross_entropy_with_logits_v2(
          labels=labels, logits=logits)
      assert np.allclose(result, expected)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers_eager.py" startline="388" endline="400" pcid="839">
  def test_sigmoid_cross_entropy(self):
    """Test invoking SigmoidCrossEntropy in eager mode."""
    with context.eager_mode():
      batch_size = 10
      n_features = 5
      logits = np.random.rand(batch_size, n_features).astype(np.float32)
      labels = np.random.randint(0, 2,
                                 (batch_size, n_features)).astype(np.float32)
      result = layers.SigmoidCrossEntropy()(labels, logits)
      expected = tf.nn.sigmoid_cross_entropy_with_logits(
          labels=labels, logits=logits)
      assert np.allclose(result, expected)

</source>
</class>

<class classid="59" nclones="4" nlines="21" similarity="70">
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers_eager.py" startline="434" endline="461" pcid="844">
  def test_conv_2d(self):
    """Test invoking Conv2D in eager mode."""
    with context.eager_mode():
      length = 4
      width = 5
      in_channels = 2
      filters = 3
      kernel_size = 2
      batch_size = 10
      input = np.random.rand(batch_size, length, width,
                             in_channels).astype(np.float32)
      layer = layers.Conv2D(filters, kernel_size=kernel_size)
      result = layer(input)
      assert result.shape == (batch_size, length, width, filters)
      assert len(layer.trainable_variables) == 2

      # Creating a second layer should produce different results, since it has
      # different random weights.

      layer2 = layers.Conv2D(filters, kernel_size=kernel_size)
      result2 = layer2(input)
      assert not np.allclose(result, result2)

      # But evaluating the first layer again should produce the same result as before.

      result3 = layer(input)
      assert np.allclose(result, result3)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers_eager.py" startline="462" endline="490" pcid="845">
  def test_conv_3d(self):
    """Test invoking Conv3D in eager mode."""
    with context.eager_mode():
      length = 4
      width = 5
      depth = 6
      in_channels = 2
      filters = 3
      kernel_size = 2
      batch_size = 10
      input = np.random.rand(batch_size, length, width, depth,
                             in_channels).astype(np.float32)
      layer = layers.Conv3D(filters, kernel_size=kernel_size)
      result = layer(input)
      assert result.shape == (batch_size, length, width, depth, filters)
      assert len(layer.trainable_variables) == 2

      # Creating a second layer should produce different results, since it has
      # different random weights.

      layer2 = layers.Conv3D(filters, kernel_size=kernel_size)
      result2 = layer2(input)
      assert not np.allclose(result, result2)

      # But evaluating the first layer again should produce the same result as before.

      result3 = layer(input)
      assert np.allclose(result, result3)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers_eager.py" startline="523" endline="555" pcid="847">
  def test_conv_3d_transpose(self):
    """Test invoking Conv3DTranspose in eager mode."""
    with context.eager_mode():
      length = 4
      width = 5
      depth = 6
      in_channels = 2
      filters = 3
      kernel_size = 2
      stride = 2
      batch_size = 10
      input = np.random.rand(batch_size, length, width, depth,
                             in_channels).astype(np.float32)
      layer = layers.Conv3DTranspose(
          filters, kernel_size=kernel_size, stride=stride)
      result = layer(input)
      assert result.shape == (batch_size, length * stride, width * stride,
                              depth * stride, filters)
      assert len(layer.trainable_variables) == 2

      # Creating a second layer should produce different results, since it has
      # different random weights.

      layer2 = layers.Conv3DTranspose(
          filters, kernel_size=kernel_size, stride=stride)
      result2 = layer2(input)
      assert not np.allclose(result, result2)

      # But evaluating the first layer again should produce the same result as before.

      result3 = layer(input)
      assert np.allclose(result, result3)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers_eager.py" startline="491" endline="522" pcid="846">
  def test_conv_2d_transpose(self):
    """Test invoking Conv2DTranspose in eager mode."""
    with context.eager_mode():
      length = 4
      width = 5
      in_channels = 2
      filters = 3
      kernel_size = 2
      stride = 2
      batch_size = 10
      input = np.random.rand(batch_size, length, width,
                             in_channels).astype(np.float32)
      layer = layers.Conv2DTranspose(
          filters, kernel_size=kernel_size, stride=stride)
      result = layer(input)
      assert result.shape == (batch_size, length * stride, width * stride,
                              filters)
      assert len(layer.trainable_variables) == 2

      # Creating a second layer should produce different results, since it has
      # different random weights.

      layer2 = layers.Conv2DTranspose(
          filters, kernel_size=kernel_size, stride=stride)
      result2 = layer2(input)
      assert not np.allclose(result, result2)

      # But evaluating the first layer again should produce the same result as before.

      result3 = layer(input)
      assert np.allclose(result, result3)

</source>
</class>

<class classid="60" nclones="3" nlines="18" similarity="70">
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers_eager.py" startline="577" endline="598" pcid="851">
  def test_graph_conv(self):
    """Test invoking GraphConv in eager mode."""
    with context.eager_mode():
      out_channels = 2
      n_atoms = 4  # In CCC and C, there are 4 atoms
      raw_smiles = ['CCC', 'C']
      import rdkit
      mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
      featurizer = dc.feat.graph_features.ConvMolFeaturizer()
      mols = featurizer.featurize(mols)
      multi_mol = dc.feat.mol_graphs.ConvMol.agglomerate_mols(mols)
      atom_features = multi_mol.get_atom_features().astype(np.float32)
      degree_slice = multi_mol.deg_slice
      membership = multi_mol.membership
      deg_adjs = multi_mol.get_deg_adjacency_lists()[1:]
      args = [atom_features, degree_slice, membership] + deg_adjs
      layer = layers.GraphConv(out_channels)
      result = layer(*args)
      assert result.shape == (n_atoms, out_channels)
      num_deg = 2 * layer.max_degree + (1 - layer.min_degree)
      assert len(layer.trainable_variables) == 2 * num_deg

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers_eager.py" startline="599" endline="617" pcid="852">
  def test_graph_pool(self):
    """Test invoking GraphPool in eager mode."""
    with context.eager_mode():
      n_atoms = 4  # In CCC and C, there are 4 atoms
      raw_smiles = ['CCC', 'C']
      import rdkit
      mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
      featurizer = dc.feat.graph_features.ConvMolFeaturizer()
      mols = featurizer.featurize(mols)
      multi_mol = dc.feat.mol_graphs.ConvMol.agglomerate_mols(mols)
      atom_features = multi_mol.get_atom_features().astype(np.float32)
      degree_slice = multi_mol.deg_slice
      membership = multi_mol.membership
      deg_adjs = multi_mol.get_deg_adjacency_lists()[1:]
      args = [atom_features, degree_slice, membership] + deg_adjs
      result = layers.GraphPool()(*args)
      assert result.shape[0] == n_atoms
      # TODO What should shape[1] be?  It's not documented.

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers_eager.py" startline="618" endline="638" pcid="853">
  def test_graph_gather(self):
    """Test invoking GraphGather in eager mode."""
    with context.eager_mode():
      batch_size = 2
      n_features = 75
      n_atoms = 4  # In CCC and C, there are 4 atoms
      raw_smiles = ['CCC', 'C']
      import rdkit
      mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
      featurizer = dc.feat.graph_features.ConvMolFeaturizer()
      mols = featurizer.featurize(mols)
      multi_mol = dc.feat.mol_graphs.ConvMol.agglomerate_mols(mols)
      atom_features = multi_mol.get_atom_features().astype(np.float32)
      degree_slice = multi_mol.deg_slice
      membership = multi_mol.membership
      deg_adjs = multi_mol.get_deg_adjacency_lists()[1:]
      args = [atom_features, degree_slice, membership] + deg_adjs
      result = layers.GraphGather(batch_size)(*args)
      # TODO(rbharath): Why is it 2*n_features instead of n_features?
      assert result.shape == (batch_size, 2 * n_features)

</source>
</class>

<class classid="61" nclones="2" nlines="13" similarity="100">
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers_eager.py" startline="656" endline="670" pcid="855">
  def test_attn_lstm_embedding(self):
    """Test invoking AttnLSTMEmbedding in eager mode."""
    with context.eager_mode():
      max_depth = 5
      n_test = 5
      n_support = 11
      n_feat = 10
      test = np.random.rand(n_test, n_feat).astype(np.float32)
      support = np.random.rand(n_support, n_feat).astype(np.float32)
      layer = layers.AttnLSTMEmbedding(n_test, n_support, n_feat, max_depth)
      test_out, support_out = layer(test, support)
      assert test_out.shape == (n_test, n_feat)
      assert support_out.shape == (n_support, n_feat)
      assert len(layer.trainable_variables) == 6

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers_eager.py" startline="671" endline="685" pcid="856">
  def test_iter_ref_lstm_embedding(self):
    """Test invoking IterRefLSTMEmbedding in eager mode."""
    with context.eager_mode():
      max_depth = 5
      n_test = 5
      n_support = 11
      n_feat = 10
      test = np.random.rand(n_test, n_feat).astype(np.float32)
      support = np.random.rand(n_support, n_feat).astype(np.float32)
      layer = layers.IterRefLSTMEmbedding(n_test, n_support, n_feat, max_depth)
      test_out, support_out = layer(test, support)
      assert test_out.shape == (n_test, n_feat)
      assert support_out.shape == (n_support, n_feat)
      assert len(layer.trainable_variables) == 12

</source>
</class>

<class classid="62" nclones="2" nlines="14" similarity="71">
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers_eager.py" startline="747" endline="762" pcid="861">
  def test_neighbor_list(self):
    """Test invoking NeighborList in eager mode."""
    with context.eager_mode():
      N_atoms = 5
      start = 0
      stop = 12
      nbr_cutoff = 3
      ndim = 3
      M_nbrs = 2
      coords = start + np.random.rand(N_atoms, ndim) * (stop - start)
      coords = tf.cast(tf.stack(coords), tf.float32)
      layer = layers.NeighborList(N_atoms, M_nbrs, ndim, nbr_cutoff, start,
                                  stop)
      result = layer(coords)
      assert result.shape == (N_atoms, M_nbrs)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_nbr_list.py" startline="79" endline="95" pcid="882">
  def test_neighbor_list_shape(self):
    """Test that NeighborList works."""
    N_atoms = 5
    start = 0
    stop = 12
    nbr_cutoff = 3
    ndim = 3
    M_nbrs = 2

    with self.session() as sess:
      coords = start + np.random.rand(N_atoms, ndim) * (stop - start)
      coords = tf.cast(tf.stack(coords), tf.float32)
      nbr_list = NeighborList(N_atoms, M_nbrs, ndim, nbr_cutoff, start,
                              stop)(coords)
      nbr_list = nbr_list.eval()
      assert nbr_list.shape == (N_atoms, M_nbrs)

</source>
</class>

<class classid="63" nclones="9" nlines="17" similarity="70">
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_nbr_list.py" startline="96" endline="116" pcid="883">
  def test_get_cells_1D(self):
    """Test neighbor-list method get_cells() in 1D"""
    N_atoms = 4
    start = 0
    stop = 10
    nbr_cutoff = 1
    ndim = 1
    M_nbrs = 1
    # 1 and 2 are nbrs. 8 and 9 are nbrs
    coords = np.array([1.0, 2.0, 8.0, 9.0])
    coords = np.reshape(coords, (N_atoms, ndim))

    with self.session() as sess:
      coords = tf.convert_to_tensor(coords)
      nbr_list_layer = NeighborList(N_atoms, M_nbrs, ndim, nbr_cutoff, start,
                                    stop)
      cells = nbr_list_layer.get_cells()
      cells_eval = cells.eval()
      true_cells = np.reshape(np.arange(10), (10, 1))
      np.testing.assert_array_almost_equal(cells_eval, true_cells)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_nbr_list.py" startline="188" endline="212" pcid="887">
  def test_get_atoms_in_nbrs_1D(self):
    """Test get_atoms_in_brs in 1D"""
    N_atoms = 4
    start = 0
    stop = 10
    nbr_cutoff = 1
    ndim = 1
    M_nbrs = 1
    # 1 and 2 are nbrs. 8 and 9 are nbrs
    coords = np.array([1.0, 2.0, 8.0, 9.0])
    coords = np.reshape(coords, (N_atoms, ndim))

    with self.session() as sess:
      coords = tf.convert_to_tensor(coords, dtype=tf.float32)
      nbr_list_layer = NeighborList(N_atoms, M_nbrs, ndim, nbr_cutoff, start,
                                    stop)
      cells = nbr_list_layer.get_cells()
      uniques = nbr_list_layer.get_atoms_in_nbrs(coords, cells)

      uniques_eval = [unique.eval() for unique in uniques]
      uniques_eval = np.array(uniques_eval)

      true_uniques = np.array([[1], [0], [3], [2]])
      np.testing.assert_array_almost_equal(uniques_eval, true_uniques)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_nbr_list.py" startline="213" endline="232" pcid="888">
  def test_neighbor_list_1D(self):
    """Test neighbor list on 1D example."""
    N_atoms = 4
    start = 0
    stop = 10
    nbr_cutoff = 1
    ndim = 1
    M_nbrs = 1
    # 1 and 2 are nbrs. 8 and 9 are nbrs
    coords = np.array([1.0, 2.0, 8.0, 9.0])
    coords = np.reshape(coords, (N_atoms, ndim))

    with self.session() as sess:
      coords = tf.convert_to_tensor(coords, dtype=tf.float32)
      nbr_list_layer = NeighborList(N_atoms, M_nbrs, ndim, nbr_cutoff, start,
                                    stop)
      nbr_list = nbr_list_layer.compute_nbr_list(coords)
      nbr_list = np.squeeze(nbr_list.eval())
      np.testing.assert_array_almost_equal(nbr_list, np.array([1, 0, 3, 2]))

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_nbr_list.py" startline="165" endline="187" pcid="886">
  def test_get_cells_for_atoms_1D(self):
    """Test that get_cells_for_atoms works in 1D"""
    N_atoms = 4
    start = 0
    stop = 10
    nbr_cutoff = 1
    ndim = 1
    M_nbrs = 1
    # 1 and 2 are nbrs. 8 and 9 are nbrs
    coords = np.array([1.0, 2.0, 8.0, 9.0])
    coords = np.reshape(coords, (N_atoms, ndim))

    with self.session() as sess:
      coords = tf.convert_to_tensor(coords, dtype=tf.float32)
      nbr_list_layer = NeighborList(N_atoms, M_nbrs, ndim, nbr_cutoff, start,
                                    stop)
      cells = nbr_list_layer.get_cells()
      cells_for_atoms = nbr_list_layer.get_cells_for_atoms(coords, cells)
      cells_for_atoms_eval = cells_for_atoms.eval()
      true_cells_for_atoms = np.array([[1], [2], [8], [9]])
      np.testing.assert_array_almost_equal(cells_for_atoms_eval,
                                           true_cells_for_atoms)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_nbr_list.py" startline="141" endline="164" pcid="885">
  def test_get_neighbor_cells_1D(self):
    """Test that get_neighbor_cells works in 1D"""
    N_atoms = 4
    start = 0
    stop = 10
    nbr_cutoff = 1
    ndim = 1
    M_nbrs = 1
    # 1 and 2 are nbrs. 8 and 9 are nbrs
    coords = np.array([1.0, 2.0, 8.0, 9.0])
    coords = np.reshape(coords, (N_atoms, ndim))

    with self.session() as sess:
      coords = tf.convert_to_tensor(coords)
      nbr_list_layer = NeighborList(N_atoms, M_nbrs, ndim, nbr_cutoff, start,
                                    stop)
      cells = nbr_list_layer.get_cells()
      nbr_cells = nbr_list_layer.get_neighbor_cells(cells)
      nbr_cells_eval = nbr_cells.eval()
      true_nbr_cells = np.array([[0, 1, 2], [1, 0, 2], [2, 1, 3], [3, 2, 4],
                                 [4, 3, 5], [5, 4, 6], [6, 5, 7], [7, 6, 8],
                                 [8, 7, 9], [9, 8, 7]])
      np.testing.assert_array_almost_equal(nbr_cells_eval, true_nbr_cells)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_nbr_list.py" startline="117" endline="140" pcid="884">
  def test_get_closest_atoms_1D(self):
    """Test get_closest_atoms works correctly in 1D"""
    N_atoms = 4
    start = 0
    stop = 10
    n_cells = 10
    nbr_cutoff = 1
    ndim = 1
    M_nbrs = 1
    # 1 and 2 are nbrs. 8 and 9 are nbrs
    coords = np.array([1.0, 2.0, 8.0, 9.0])
    coords = np.reshape(coords, (N_atoms, ndim))
    with self.session() as sess:
      coords = tf.convert_to_tensor(coords, dtype=tf.float32)
      nbr_list_layer = NeighborList(N_atoms, M_nbrs, ndim, nbr_cutoff, start,
                                    stop)
      cells = nbr_list_layer.get_cells()
      closest_atoms = nbr_list_layer.get_closest_atoms(coords, cells)
      atoms_in_cells_eval = closest_atoms.eval()
      true_atoms_in_cells = np.reshape(
          np.array([0, 0, 1, 1, 1, 1, 2, 2, 2, 3]), (n_cells, M_nbrs))
      np.testing.assert_array_almost_equal(atoms_in_cells_eval,
                                           true_atoms_in_cells)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_nbr_list.py" startline="253" endline="273" pcid="890">
  def test_neighbor_list_3D(self):
    """Test neighbor list on 3D example."""
    N_atoms = 4
    start = 0
    stop = 10
    nbr_cutoff = 1
    ndim = 3
    M_nbrs = 1
    # 1 and 2 are nbrs. 8 and 9 are nbrs
    coords = np.array([[1.0, 0.0, 1.0], [2.0, 2.0, 2.0], [8.0, 8.0, 8.0],
                       [9.0, 9.0, 9.0]])
    coords = np.reshape(coords, (N_atoms, ndim))

    with self.session() as sess:
      coords = tf.convert_to_tensor(coords, dtype=tf.float32)
      nbr_list_layer = NeighborList(N_atoms, M_nbrs, ndim, nbr_cutoff, start,
                                    stop)
      nbr_list = nbr_list_layer.compute_nbr_list(coords)
      nbr_list = np.squeeze(nbr_list.eval())
      np.testing.assert_array_almost_equal(nbr_list, np.array([1, 0, 3, 2]))

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_nbr_list.py" startline="233" endline="252" pcid="889">
  def test_neighbor_list_2D(self):
    """Test neighbor list on 2D example."""
    N_atoms = 4
    start = 0
    stop = 10
    nbr_cutoff = 1
    ndim = 2
    M_nbrs = 1
    # 1 and 2 are nbrs. 8 and 9 are nbrs
    coords = np.array([[1.0, 1.0], [2.0, 2.0], [8.0, 8.0], [9.0, 9.0]])
    coords = np.reshape(coords, (N_atoms, ndim))

    with self.session() as sess:
      coords = tf.convert_to_tensor(coords, dtype=tf.float32)
      nbr_list_layer = NeighborList(N_atoms, M_nbrs, ndim, nbr_cutoff, start,
                                    stop)
      nbr_list = nbr_list_layer.compute_nbr_list(coords)
      nbr_list = np.squeeze(nbr_list.eval())
      np.testing.assert_array_almost_equal(nbr_list, np.array([1, 0, 3, 2]))

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_nbr_list.py" startline="274" endline="298" pcid="891">
  def test_neighbor_list_3D_empty_cells(self):
    """Test neighbor list on 3D example where cells are empty.

    Stresses the failure mode where the neighboring cells are empty
    so top_k will throw a failure.
    """
    N_atoms = 4
    start = 0
    stop = 10
    nbr_cutoff = 1
    ndim = 3
    M_nbrs = 1
    # 1 and 2 are nbrs. 8 and 9 are nbrs
    coords = np.array([[1.0, 0.0, 1.0], [2.0, 5.0, 2.0], [8.0, 8.0, 8.0],
                       [9.0, 9.0, 9.0]])
    coords = np.reshape(coords, (N_atoms, ndim))

    with self.session() as sess:
      coords = tf.convert_to_tensor(coords, dtype=tf.float32)
      nbr_list_layer = NeighborList(N_atoms, M_nbrs, ndim, nbr_cutoff, start,
                                    stop)
      nbr_list = nbr_list_layer.compute_nbr_list(coords)
      nbr_list = np.squeeze(nbr_list.eval())
      np.testing.assert_array_almost_equal(nbr_list, np.array([-1, -1, 3, 2]))

</source>
</class>

<class classid="64" nclones="28" nlines="12" similarity="71">
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers.py" startline="74" endline="90" pcid="893">
  def test_conv_1D(self):
    """Test that Conv1D can be invoked."""
    width = 5
    in_channels = 2
    filters = 3
    kernel_size = 2
    batch_size = 10
    in_tensor = np.random.rand(batch_size, width, in_channels)
    with self.session() as sess:
      in_tensor = tf.convert_to_tensor(in_tensor, dtype=tf.float32)
      out_tensor = Conv1D(filters, kernel_size)(in_tensor)
      sess.run(tf.global_variables_initializer())
      out_tensor = out_tensor.eval()

      self.assertEqual(out_tensor.shape[0], batch_size)
      self.assertEqual(out_tensor.shape[2], filters)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers.py" startline="191" endline="205" pcid="902">
  def test_lstm(self):
    """Test that LSTM can be invoked."""
    batch_size = 10
    n_hidden = 7
    in_channels = 4
    n_repeat = 2
    n_steps = 6
    in_tensor = np.random.rand(batch_size, n_steps, in_channels)
    with self.session() as sess:
      in_tensor = tf.convert_to_tensor(in_tensor, dtype=tf.float32)
      out_tensor = LSTM(n_hidden, batch_size)(in_tensor)
      sess.run(tf.global_variables_initializer())
      out_tensor = out_tensor.eval()
      assert out_tensor.shape == (batch_size, n_steps, n_hidden)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers.py" startline="176" endline="190" pcid="901">
  def test_gru(self):
    """Test that GRU can be invoked."""
    batch_size = 10
    n_hidden = 7
    in_channels = 4
    n_repeat = 2
    n_steps = 6
    in_tensor = np.random.rand(batch_size, n_steps, in_channels)
    with self.session() as sess:
      in_tensor = tf.convert_to_tensor(in_tensor, dtype=tf.float32)
      out_tensor = GRU(n_hidden, batch_size)(in_tensor)
      sess.run(tf.global_variables_initializer())
      out_tensor = out_tensor.eval()
      assert out_tensor.shape == (batch_size, n_steps, n_hidden)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers.py" startline="529" endline="543" pcid="930">
  def test_max_pool_3D(self):
    """Test that MaxPool3D can be invoked."""
    length = 2
    width = 2
    depth = 2
    in_channels = 2
    batch_size = 20
    in_tensor = np.random.rand(batch_size, length, width, depth, in_channels)
    with self.session() as sess:
      in_tensor = tf.convert_to_tensor(in_tensor, dtype=tf.float32)
      out_tensor = MaxPool3D()(in_tensor)
      sess.run(tf.global_variables_initializer())
      out_tensor = out_tensor.eval()
      assert out_tensor.shape == (batch_size, 1, 1, 1, in_channels)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers.py" startline="448" endline="462" pcid="925">
  def test_conv_2D(self):
    """Test that Conv2D can be invoked."""
    length = 4
    width = 5
    in_channels = 2
    out_channels = 3
    batch_size = 20
    in_tensor = np.random.rand(batch_size, length, width, in_channels)
    with self.session() as sess:
      in_tensor = tf.convert_to_tensor(in_tensor, dtype=tf.float32)
      out_tensor = Conv2D(out_channels, kernel_size=1)(in_tensor)
      sess.run(tf.global_variables_initializer())
      out_tensor = out_tensor.eval()
      assert out_tensor.shape == (batch_size, length, width, out_channels)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers.py" startline="206" endline="219" pcid="903">
  def test_time_series_dense(self):
    """Test that TimeSeriesDense can be invoked."""
    batch_size = 10
    n_hidden = 7
    in_channels = 4
    out_channels = 5
    n_repeat = 2
    n_steps = 6
    in_tensor = np.random.rand(batch_size, n_steps, in_channels)
    with self.session() as sess:
      in_tensor = tf.convert_to_tensor(in_tensor, dtype=tf.float32)
      out_tensor = TimeSeriesDense(out_channels)(in_tensor)
      assert out_tensor.shape == (batch_size, n_steps, out_channels)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers.py" startline="463" endline="479" pcid="926">
  def test_conv_3D(self):
    """Test that Conv3D can be invoked."""
    length = 4
    width = 5
    depth = 6
    in_channels = 2
    out_channels = 3
    batch_size = 20
    in_tensor = np.random.rand(batch_size, length, width, depth, in_channels)
    with self.session() as sess:
      in_tensor = tf.convert_to_tensor(in_tensor, dtype=tf.float32)
      out_tensor = Conv3D(out_channels, kernel_size=1)(in_tensor)
      sess.run(tf.global_variables_initializer())
      out_tensor = out_tensor.eval()
      assert out_tensor.shape == (batch_size, length, width, depth,
                                  out_channels)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers.py" startline="515" endline="528" pcid="929">
  def test_maxpool2D(self):
    """Test that MaxPool2D can be invoked."""
    length = 2
    width = 2
    in_channels = 2
    batch_size = 20
    in_tensor = np.random.rand(batch_size, length, width, in_channels)
    with self.session() as sess:
      in_tensor = tf.convert_to_tensor(in_tensor, dtype=tf.float32)
      out_tensor = MaxPool2D()(in_tensor)
      sess.run(tf.global_variables_initializer())
      out_tensor = out_tensor.eval()
      assert out_tensor.shape == (batch_size, 1, 1, in_channels)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers.py" startline="544" endline="557" pcid="931">
  def test_avgpool2D(self):
    """Test that AvgPool2D can be invoked."""
    length = 4
    width = 4
    in_channels = 5
    batch_size = 10
    in_tensor = np.random.rand(batch_size, length, width, in_channels)
    with self.session() as sess:
      in_tensor = tf.convert_to_tensor(in_tensor, dtype=tf.float32)
      out_tensor = AvgPool2D()(in_tensor)
      sess.run(tf.global_variables_initializer())
      out_tensor = out_tensor.eval()
      assert out_tensor.shape == (batch_size, 2, 2, in_channels)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers.py" startline="480" endline="496" pcid="927">
  def test_conv_2D_transpose(self):
    """Test that Conv2DTranspose can be invoked."""
    length = 4
    width = 5
    in_channels = 2
    out_channels = 3
    batch_size = 20
    in_tensor = np.random.rand(batch_size, length, width, in_channels)
    with self.session() as sess:
      in_tensor = tf.convert_to_tensor(in_tensor, dtype=tf.float32)
      out_tensor = Conv2DTranspose(
          out_channels, kernel_size=1, stride=2)(in_tensor)
      sess.run(tf.global_variables_initializer())
      out_tensor = out_tensor.eval()
      assert out_tensor.shape == (batch_size, 2 * length, 2 * width,
                                  out_channels)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers.py" startline="497" endline="514" pcid="928">
  def test_conv_3D_transpose(self):
    """Test that Conv3DTranspose can be invoked."""
    length = 4
    width = 5
    depth = 6
    in_channels = 2
    out_channels = 3
    batch_size = 20
    in_tensor = np.random.rand(batch_size, length, width, depth, in_channels)
    with self.session() as sess:
      in_tensor = tf.convert_to_tensor(in_tensor, dtype=tf.float32)
      out_tensor = Conv3DTranspose(
          out_channels, kernel_size=1, stride=(2, 3, 1))(in_tensor)
      sess.run(tf.global_variables_initializer())
      out_tensor = out_tensor.eval()
      assert out_tensor.shape == (batch_size, 2 * length, 3 * width, depth,
                                  out_channels)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers.py" startline="104" endline="116" pcid="895">
  def test_flatten(self):
    """Test that Flatten can be invoked."""
    in_dim_1 = 2
    in_dim_2 = 2
    out_dim = 4
    batch_size = 10
    in_tensor = np.random.rand(batch_size, in_dim_1, in_dim_2)
    with self.session() as sess:
      in_tensor = tf.convert_to_tensor(in_tensor, dtype=tf.float32)
      out_tensor = Flatten()(in_tensor)
      out_tensor = out_tensor.eval()
      assert out_tensor.shape == (batch_size, out_dim)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers.py" startline="130" endline="141" pcid="897">
  def test_transpose(self):
    """Test that Transpose can be invoked."""
    in_dim_1 = 2
    in_dim_2 = 7
    batch_size = 10
    in_tensor = np.random.rand(batch_size, in_dim_1, in_dim_2)
    with self.session() as sess:
      in_tensor = tf.convert_to_tensor(in_tensor, dtype=tf.float32)
      out_tensor = Transpose((0, 2, 1))(in_tensor)
      out_tensor = out_tensor.eval()
      assert out_tensor.shape == (batch_size, in_dim_2, in_dim_1)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers.py" startline="718" endline="729" pcid="938">
  def test_batch_norm(self):
    """Test that BatchNorm can be invoked."""
    batch_size = 10
    n_features = 5
    in_tensor = np.random.rand(batch_size, n_features)
    with self.session() as sess:
      in_tensor = tf.convert_to_tensor(in_tensor, dtype=tf.float32)
      out_tensor = BatchNorm()(in_tensor)
      sess.run(tf.global_variables_initializer())
      out_tensor = out_tensor.eval()
      assert out_tensor.shape == (batch_size, n_features)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers.py" startline="117" endline="129" pcid="896">
  def test_reshape(self):
    """Test that Reshape can be invoked."""
    in_dim_1 = 2
    in_dim_2 = 2
    out_dim = 4
    batch_size = 10
    in_tensor = np.random.rand(batch_size, in_dim_1, in_dim_2)
    with self.session() as sess:
      in_tensor = tf.convert_to_tensor(in_tensor, dtype=tf.float32)
      out_tensor = Reshape((batch_size, out_dim))(in_tensor)
      out_tensor = out_tensor.eval()
      assert out_tensor.shape == (batch_size, out_dim)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers.py" startline="91" endline="103" pcid="894">
  def test_dense(self):
    """Test that Dense can be invoked."""
    in_dim = 2
    out_dim = 3
    batch_size = 10
    in_tensor = np.random.rand(batch_size, in_dim)
    with self.session() as sess:
      in_tensor = tf.convert_to_tensor(in_tensor, dtype=tf.float32)
      out_tensor = Dense(out_dim)(in_tensor)
      sess.run(tf.global_variables_initializer())
      out_tensor = out_tensor.eval()
      assert out_tensor.shape == (batch_size, out_dim)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers.py" startline="155" endline="166" pcid="899">
  def test_repeat(self):
    """Test that Repeat can be invoked."""
    in_dim = 4
    batch_size = 10
    n_repeat = 2
    in_tensor = np.random.rand(batch_size, in_dim)
    with self.session() as sess:
      in_tensor = tf.convert_to_tensor(in_tensor, dtype=tf.float32)
      out_tensor = Repeat(n_repeat)(in_tensor)
      out_tensor = out_tensor.eval()
      assert out_tensor.shape == (batch_size, n_repeat, in_dim)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers.py" startline="364" endline="376" pcid="918">
  def test_sigmoid_cross_entropy(self):
    """Test that SigmoidCrossEntropy can be invoked."""
    batch_size = 10
    n_features = 5
    logit_tensor = np.random.rand(batch_size, n_features)
    label_tensor = np.random.randint(0, 2, (batch_size, n_features))
    with self.session() as sess:
      logit_tensor = tf.convert_to_tensor(logit_tensor, dtype=tf.float32)
      label_tensor = tf.convert_to_tensor(label_tensor, dtype=tf.float32)
      out_tensor = SigmoidCrossEntropy()(label_tensor, logit_tensor)
      out_tensor = out_tensor.eval()
      assert out_tensor.shape == (batch_size, n_features)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers.py" startline="762" endline="775" pcid="941">
  def test_weighted_linear_combo(self):
    """Test that WeightedLinearCombo can be invoked."""
    batch_size = 10
    n_features = 5
    in_tensor_1 = np.random.rand(batch_size, n_features)
    in_tensor_2 = np.random.rand(batch_size, n_features)
    with self.session() as sess:
      in_tensor_1 = tf.convert_to_tensor(in_tensor_1, dtype=tf.float32)
      in_tensor_2 = tf.convert_to_tensor(in_tensor_2, dtype=tf.float32)
      out_tensor = WeightedLinearCombo()(in_tensor_1, in_tensor_2)
      sess.run(tf.global_variables_initializer())
      out_tensor = out_tensor.eval()
      assert out_tensor.shape == (batch_size, n_features)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers.py" startline="390" endline="401" pcid="920">
  def test_sparse_softmax_cross_entropy(self):
    batch_size = 10
    n_features = 5
    logit_tensor = np.random.rand(batch_size, n_features)
    label_tensor = np.random.rand(batch_size)
    with self.session() as sess:
      logit_tensor = tf.convert_to_tensor(logit_tensor, dtype=tf.float32)
      label_tensor = tf.convert_to_tensor(label_tensor, dtype=tf.int32)
      out_tensor = SparseSoftMaxCrossEntropy()(label_tensor, logit_tensor)
      out_tensor = out_tensor.eval()
      assert out_tensor.shape == (batch_size,)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers.py" startline="272" endline="284" pcid="909">
  def test_concat(self):
    """Test that Concat can be invoked."""
    batch_size = 10
    n_features = 5
    in_tensor_1 = np.random.rand(batch_size, n_features)
    in_tensor_2 = np.random.rand(batch_size, n_features)
    with self.session() as sess:
      in_tensor_1 = tf.convert_to_tensor(in_tensor_1, dtype=tf.float32)
      in_tensor_2 = tf.convert_to_tensor(in_tensor_2, dtype=tf.float32)
      out_tensor = Concat(axis=1)(in_tensor_1, in_tensor_2)
      out_tensor = out_tensor.eval()
      assert out_tensor.shape == (batch_size, 2 * n_features)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers.py" startline="435" endline="447" pcid="924">
  def test_reduce_square_difference(self):
    """Test that ReduceSquareDifference can be invoked."""
    batch_size = 10
    n_features = 5
    in_tensor_1 = np.random.rand(batch_size, n_features)
    in_tensor_2 = np.random.rand(batch_size, n_features)
    with self.session() as sess:
      in_tensor_1 = tf.convert_to_tensor(in_tensor_1, dtype=tf.float32)
      in_tensor_2 = tf.convert_to_tensor(in_tensor_2, dtype=tf.float32)
      out_tensor = ReduceSquareDifference()(in_tensor_1, in_tensor_2)
      out_tensor = out_tensor.eval()
      assert isinstance(out_tensor, np.float32)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers.py" startline="377" endline="389" pcid="919">
  def test_softmax_cross_entropy(self):
    """Test that SoftMaxCrossEntropy can be invoked."""
    batch_size = 10
    n_features = 5
    logit_tensor = np.random.rand(batch_size, n_features)
    label_tensor = np.random.rand(batch_size, n_features)
    with self.session() as sess:
      logit_tensor = tf.convert_to_tensor(logit_tensor, dtype=tf.float32)
      label_tensor = tf.convert_to_tensor(label_tensor, dtype=tf.float32)
      out_tensor = SoftMaxCrossEntropy()(logit_tensor, label_tensor)
      out_tensor = out_tensor.eval()
      assert out_tensor.shape == (batch_size,)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers.py" startline="226" endline="238" pcid="905">
  def test_l2_loss(self):
    """Test that L2Loss can be invoked."""
    batch_size = 10
    n_features = 5
    guess_tensor = np.random.rand(batch_size, n_features)
    label_tensor = np.random.rand(batch_size, n_features)
    with self.session() as sess:
      guess_tensor = tf.convert_to_tensor(guess_tensor, dtype=tf.float32)
      label_tensor = tf.convert_to_tensor(label_tensor, dtype=tf.float32)
      out_tensor = L2Loss()(guess_tensor, label_tensor)
      out_tensor = out_tensor.eval()
      assert out_tensor.shape == (batch_size,)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers.py" startline="142" endline="154" pcid="898">
  def test_combine_mean_std(self):
    """Test that Transpose can be invoked."""
    dim = 2
    batch_size = 10
    mean_tensor = np.random.rand(dim)
    std_tensor = np.random.rand(1,)
    with self.session() as sess:
      mean_tensor = tf.convert_to_tensor(mean_tensor, dtype=tf.float32)
      std_tensor = tf.convert_to_tensor(std_tensor, dtype=tf.float32)
      out_tensor = CombineMeanStd()(mean_tensor, std_tensor)
      out_tensor = out_tensor.eval()
      assert out_tensor.shape == (dim,)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers.py" startline="730" endline="742" pcid="939">
  def test_weighted_error(self):
    """Test that WeightedError can be invoked."""
    batch_size = 10
    n_features = 5
    guess_tensor = np.random.rand(batch_size, n_features)
    label_tensor = np.random.rand(batch_size, n_features)
    with self.session() as sess:
      guess_tensor = tf.convert_to_tensor(guess_tensor, dtype=tf.float32)
      label_tensor = tf.convert_to_tensor(label_tensor, dtype=tf.float32)
      out_tensor = WeightedError()(guess_tensor, label_tensor)
      out_tensor = out_tensor.eval()
      assert isinstance(out_tensor, np.float32)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers.py" startline="815" endline="832" pcid="946">
  def test_alpha_share_layer(self):
    """Test that alpha share works correctly"""
    batch_size = 50
    length = 10
    test_1 = np.random.rand(batch_size, length)
    test_2 = np.random.rand(batch_size, length)

    with self.session() as sess:
      test_1 = tf.convert_to_tensor(test_1, dtype=tf.float32)
      test_2 = tf.convert_to_tensor(test_2, dtype=tf.float32)

      out_tensor = AlphaShareLayer()(test_1, test_2)
      sess.run(tf.global_variables_initializer())
      test_1_out_tensor = out_tensor[0].eval()
      test_2_out_tensor = out_tensor[1].eval()
      assert test_1.shape == test_1_out_tensor.shape
      assert test_2.shape == test_2_out_tensor.shape

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers.py" startline="833" endline="849" pcid="947">
  def test_beta_share(self):
    """Test that beta share works correctly"""
    batch_size = 50
    length = 10
    test_1 = np.random.rand(batch_size, length)
    test_2 = np.random.rand(batch_size, length)

    with self.session() as sess:
      test_1 = tf.convert_to_tensor(test_1, dtype=tf.float32)
      test_2 = tf.convert_to_tensor(test_2, dtype=tf.float32)

      out_tensor = BetaShare()(test_1, test_2)
      sess.run(tf.global_variables_initializer())
      out_tensor.eval()
      assert test_1.shape == out_tensor.shape
      assert test_2.shape == out_tensor.shape

</source>
</class>

<class classid="65" nclones="2" nlines="25" similarity="92">
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers.py" startline="569" endline="595" pcid="933">
  def test_graph_conv(self):
    """Test that GraphConv can be invoked."""
    out_channels = 2
    n_atoms = 4  # In CCC and C, there are 4 atoms
    raw_smiles = ['CCC', 'C']
    mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
    featurizer = ConvMolFeaturizer()
    mols = featurizer.featurize(mols)
    multi_mol = ConvMol.agglomerate_mols(mols)
    atom_features = multi_mol.get_atom_features()
    degree_slice = multi_mol.deg_slice
    membership = multi_mol.membership
    deg_adjs = multi_mol.get_deg_adjacency_lists()[1:]

    with self.session() as sess:
      atom_features = tf.convert_to_tensor(atom_features, dtype=tf.float32)
      degree_slice = tf.convert_to_tensor(degree_slice, dtype=tf.int32)
      membership = tf.convert_to_tensor(membership, dtype=tf.int32)
      deg_adjs_tf = []
      for deg_adj in deg_adjs:
        deg_adjs_tf.append(tf.convert_to_tensor(deg_adj, dtype=tf.int32))
      args = [atom_features, degree_slice, membership] + deg_adjs_tf
      out_tensor = GraphConv(out_channels)(*args)
      sess.run(tf.global_variables_initializer())
      out_tensor = out_tensor.eval()
      assert out_tensor.shape == (n_atoms, out_channels)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers.py" startline="689" endline="717" pcid="937">
  def test_graph_gather(self):
    """Test that GraphGather can be invoked."""
    batch_size = 2
    n_features = 75
    n_atoms = 4  # In CCC and C, there are 4 atoms
    raw_smiles = ['CCC', 'C']
    mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
    featurizer = ConvMolFeaturizer()
    mols = featurizer.featurize(mols)
    multi_mol = ConvMol.agglomerate_mols(mols)
    atom_features = multi_mol.get_atom_features()
    degree_slice = multi_mol.deg_slice
    membership = multi_mol.membership
    deg_adjs = multi_mol.get_deg_adjacency_lists()[1:]

    with self.session() as sess:
      atom_features = tf.convert_to_tensor(atom_features, dtype=tf.float32)
      degree_slice = tf.convert_to_tensor(degree_slice, dtype=tf.int32)
      membership = tf.convert_to_tensor(membership, dtype=tf.int32)
      deg_adjs_tf = []
      for deg_adj in deg_adjs:
        deg_adjs_tf.append(tf.convert_to_tensor(deg_adj, dtype=tf.int32))
      args = [atom_features, degree_slice, membership] + deg_adjs_tf
      out_tensor = GraphGather(batch_size)(*args)
      sess.run(tf.global_variables_initializer())
      out_tensor = out_tensor.eval()
      # TODO(rbharath): Why is it 2*n_features instead of n_features?
      assert out_tensor.shape == (batch_size, 2 * n_features)

</source>
</class>

<class classid="66" nclones="2" nlines="17" similarity="100">
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers.py" startline="619" endline="639" pcid="935">
  def test_attn_lstm_embedding(self):
    """Test that attention LSTM computation works properly."""
    max_depth = 5
    n_test = 5
    n_support = 11
    n_feat = 10

    test = np.random.rand(n_test, n_feat)
    support = np.random.rand(n_support, n_feat)
    with self.session() as sess:
      test = tf.convert_to_tensor(test, dtype=tf.float32)
      support = tf.convert_to_tensor(support, dtype=tf.float32)

      attn_embedding_layer = AttnLSTMEmbedding(n_test, n_support, n_feat,
                                               max_depth)
      out_tensor = attn_embedding_layer(test, support)
      sess.run(tf.global_variables_initializer())
      test_out, support_out = out_tensor[0].eval(), out_tensor[1].eval()
      assert test_out.shape == (n_test, n_feat)
      assert support_out.shape == (n_support, n_feat)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers.py" startline="640" endline="688" pcid="936">
  def test_iter_ref_lstm_embedding(self):
    """Test that IterRef LSTM computation works properly."""
    max_depth = 5
    n_test = 5
    n_support = 11
    n_feat = 10

    test = np.random.rand(n_test, n_feat)
    support = np.random.rand(n_support, n_feat)
    with self.session() as sess:
      test = tf.convert_to_tensor(test, dtype=tf.float32)
      support = tf.convert_to_tensor(support, dtype=tf.float32)

      iter_ref_embedding_layer = IterRefLSTMEmbedding(n_test, n_support, n_feat,
                                                      max_depth)
      out_tensor = iter_ref_embedding_layer(test, support)
      sess.run(tf.global_variables_initializer())
      test_out, support_out = out_tensor[0].eval(), out_tensor[1].eval()
      assert test_out.shape == (n_test, n_feat)
      assert support_out.shape == (n_support, n_feat)

  # TODO(rbharath): This test should pass. Fix it!
  # def test_graph_pool(self):
  #  """Test that GraphPool can be invoked."""
  #  out_channels = 2
  #  n_atoms = 4 # In CCC and C, there are 4 atoms
  #  raw_smiles = ['CCC', 'C']
  #  mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
  #  featurizer = ConvMolFeaturizer()
  #  mols = featurizer.featurize(mols)
  #  multi_mol = ConvMol.agglomerate_mols(mols)
  #  atom_features = multi_mol.get_atom_features()
  #  degree_slice = multi_mol.deg_slice
  #  membership = multi_mol.membership
  #  deg_adjs = multi_mol.get_deg_adjacency_lists()[1:]

  #  with self.session() as sess:
  #    atom_features = tf.convert_to_tensor(atom_features, dtype=tf.float32)
  #    degree_slice = tf.convert_to_tensor(degree_slice, dtype=tf.int32)
  #    membership = tf.convert_to_tensor(membership, dtype=tf.int32)
  #    deg_adjs_tf = []
  #    for deg_adj in deg_adjs:
  #      deg_adjs_tf.append(tf.convert_to_tensor(deg_adj, dtype=tf.int32))
  #    args = [atom_features, degree_slice, membership] + deg_adjs_tf
  #    out_tensor = GraphPool(out_channels)(*args)
  #    sess.run(tf.global_variables_initializer())
  #    out_tensor = out_tensor.eval()
  #    assert out_tensor.shape == (n_atoms, out_channels)

</source>
</class>

<class classid="67" nclones="2" nlines="26" similarity="80">
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers.py" startline="1027" endline="1057" pcid="958">
  def test_dtnn_gather(self):
    """Test that DTNNGather can be invoked."""
    n_embedding = 2
    n_outputs = 2
    layer_sizes = [2]
    output_activation = False
    init_method = 'one'
    activation = 'sigmoid'

    inputs_np = np.array([[1, -1], [1, -1], [1, -1]])
    atom_membership_np = np.array([0, 0, 1])
    expected_output = np.array([[2, 2], [1, 1]])

    with self.session() as sess:
      inputs_tf = tf.convert_to_tensor(inputs_np, dtype=tf.float32)
      atom_membership_tf = tf.convert_to_tensor(
          atom_membership_np, dtype=tf.int32)
      dtnn_gather = DTNNGather(
          n_embedding=n_embedding,
          n_outputs=n_outputs,
          layer_sizes=layer_sizes,
          output_activation=output_activation,
          init=init_method,
          activation=activation)
      dtnn_gather.create_tensor(in_layers=[inputs_tf, atom_membership_tf])

      sess.run(tf.global_variables_initializer())
      output = dtnn_gather.out_tensor.eval()
      self.assertAllClose(expected_output, output)
      self.assertEqual(expected_output.shape, output.shape)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_layers.py" startline="1058" endline="1088" pcid="959">
  def test_dag_gather(self):
    """Test that DAGGather can be invoked."""
    n_graph_feat = 2
    n_outputs = 2
    layer_sizes = [2]
    init_method = 'one'
    activation = 'sigmoid'

    def sigmoid(x):
      return 1 / (1 + np.exp(-x))

    atom_features_np = np.array([[1, -1], [1, -1], [1, -1]])
    membership_np = np.array([0, 0, 1])
    expected_output = sigmoid(np.array([[1, 1], [1, 1]]))

    with self.session() as sess:
      atom_features_tf = tf.convert_to_tensor(
          atom_features_np, dtype=tf.float32)
      membership_tf = tf.convert_to_tensor(membership_np, dtype=tf.int32)
      dag_gather = DAGGather(
          n_graph_feat=n_graph_feat,
          n_outputs=n_outputs,
          activation=activation,
          init=init_method,
          layer_sizes=layer_sizes)
      dag_gather.create_tensor(in_layers=[atom_features_tf, membership_tf, 0])

      sess.run(tf.global_variables_initializer())
      output = dag_gather.out_tensor.eval()
      self.assertAllClose(output, expected_output)
      self.assertEqual(output.shape, expected_output.shape)
</source>
</class>

<class classid="68" nclones="4" nlines="18" similarity="72">
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_tensor_graph.py" startline="27" endline="45" pcid="964">
  def test_single_task_classifier(self):
    n_data_points = 20
    n_features = 2
    X = np.random.rand(n_data_points, n_features)
    y = [[0, 1] for x in range(n_data_points)]
    dataset = NumpyDataset(X, y)
    features = Feature(shape=(None, n_features))
    dense = Dense(out_channels=2, in_layers=[features])
    output = SoftMax(in_layers=[dense])
    label = Label(shape=(None, 2))
    smce = SoftMaxCrossEntropy(in_layers=[label, dense])
    loss = ReduceMean(in_layers=[smce])
    tg = dc.models.TensorGraph(learning_rate=0.01)
    tg.add_output(output)
    tg.set_loss(loss)
    tg.fit(dataset, nb_epoch=1000)
    prediction = np.squeeze(tg.predict_on_batch(X))
    assert_true(np.all(np.isclose(prediction, y, atol=0.4)))

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_tensor_graph.py" startline="99" endline="115" pcid="968">
  def test_single_task_regressor(self):
    n_data_points = 20
    n_features = 2
    X = np.random.rand(n_data_points, n_features)
    y = [[0.5] for x in range(n_data_points)]
    dataset = NumpyDataset(X, y)
    features = Feature(shape=(None, n_features))
    dense = Dense(out_channels=1, in_layers=[features])
    label = Label(shape=(None, 1))
    loss = ReduceSquareDifference(in_layers=[dense, label])
    tg = dc.models.TensorGraph(learning_rate=0.01)
    tg.add_output(dense)
    tg.set_loss(loss)
    tg.fit(dataset, nb_epoch=1000)
    prediction = np.squeeze(tg.predict_on_batch(X))
    assert_true(np.all(np.isclose(prediction, y, atol=3.0)))

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_tensor_graph.py" startline="167" endline="185" pcid="972">
  def test_no_queue(self):
    n_data_points = 20
    n_features = 2
    X = np.random.rand(n_data_points, n_features)
    y = [[0, 1] for x in range(n_data_points)]
    dataset = NumpyDataset(X, y)
    features = Feature(shape=(None, n_features))
    dense = Dense(out_channels=2, in_layers=[features])
    output = SoftMax(in_layers=[dense])
    label = Label(shape=(None, 2))
    smce = SoftMaxCrossEntropy(in_layers=[label, dense])
    loss = ReduceMean(in_layers=[smce])
    tg = dc.models.TensorGraph(learning_rate=0.01, use_queue=False)
    tg.add_output(output)
    tg.set_loss(loss)
    tg.fit(dataset, nb_epoch=1000)
    prediction = np.squeeze(tg.predict_on_batch(X))
    assert_true(np.all(np.isclose(prediction, y, atol=0.4)))

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_tensor_graph.py" startline="187" endline="213" pcid="973">
  def test_set_optimizer(self):
    n_data_points = 20
    n_features = 2
    X = np.random.rand(n_data_points, n_features)
    y = [[0, 1] for x in range(n_data_points)]
    dataset = NumpyDataset(X, y)
    features = Feature(shape=(None, n_features))
    dense = Dense(out_channels=2, in_layers=[features])
    output = SoftMax(in_layers=[dense])
    label = Label(shape=(None, 2))
    smce = SoftMaxCrossEntropy(in_layers=[label, dense])
    loss = ReduceMean(in_layers=[smce])
    tg = dc.models.TensorGraph(learning_rate=0.01, use_queue=False)
    tg.add_output(output)
    tg.set_loss(loss)
    global_step = tg.get_global_step()
    learning_rate = ExponentialDecay(
        initial_rate=0.1, decay_rate=0.96, decay_steps=100000)
    tg.set_optimizer(GradientDescent(learning_rate=learning_rate))
    tg.fit(dataset, nb_epoch=1000)
    prediction = np.squeeze(tg.predict_on_batch(X))
    tg.save()

    tg1 = TensorGraph.load_from_dir(tg.model_dir)
    prediction2 = np.squeeze(tg1.predict_on_batch(X))
    assert_true(np.all(np.isclose(prediction, prediction2, atol=0.01)))

</source>
</class>

<class classid="69" nclones="3" nlines="32" similarity="70">
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_tensor_graph.py" startline="51" endline="93" pcid="966">
  def test_multi_task_classifier(self):
    n_data_points = 20
    n_features = 2

    X = np.random.rand(n_data_points, n_features)
    y1 = np.array([[0, 1] for x in range(n_data_points)])
    y2 = np.array([[1, 0] for x in range(n_data_points)])
    X = NumpyDataset(X)
    ys = [NumpyDataset(y1), NumpyDataset(y2)]

    databag = Databag()

    features = Feature(shape=(None, n_features))
    databag.add_dataset(features, X)

    outputs = []
    entropies = []
    for i in range(2):
      label = Label(shape=(None, 2))
      dense = Dense(out_channels=2, in_layers=[features])
      output = SoftMax(in_layers=[dense])
      smce = SoftMaxCrossEntropy(in_layers=[label, dense])

      entropies.append(smce)
      outputs.append(output)
      databag.add_dataset(label, ys[i])

    total_loss = ReduceMean(in_layers=entropies)

    tg = dc.models.TensorGraph(learning_rate=0.01)
    for output in outputs:
      tg.add_output(output)
    tg.set_loss(total_loss)

    tg.fit_generator(
        databag.iterbatches(
            epochs=1000, batch_size=tg.batch_size, pad_batches=True))
    predictions = tg.predict_on_generator(databag.iterbatches())
    for i in range(2):
      y_real = ys[i].X
      y_pred = predictions[i]
      assert_true(np.all(np.isclose(y_pred, y_real, atol=0.6)))

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_tensor_graph.py" startline="286" endline="325" pcid="979">
  def test_shared_layer(self):
    n_data_points = 20
    n_features = 2

    X = np.random.rand(n_data_points, n_features)
    y1 = np.array([[0, 1] for x in range(n_data_points)])
    X = NumpyDataset(X)
    ys = [NumpyDataset(y1)]

    databag = Databag()

    features = Feature(shape=(None, n_features))
    databag.add_dataset(features, X)

    outputs = []

    label = Label(shape=(None, 2))
    dense1 = Dense(out_channels=2, in_layers=[features])
    dense2 = dense1.shared(in_layers=[features])
    output1 = SoftMax(in_layers=[dense1])
    output2 = SoftMax(in_layers=[dense2])
    smce = SoftMaxCrossEntropy(in_layers=[label, dense1])

    outputs.append(output1)
    outputs.append(output2)
    databag.add_dataset(label, ys[0])

    total_loss = ReduceMean(in_layers=[smce])

    tg = dc.models.TensorGraph(learning_rate=0.01)
    for output in outputs:
      tg.add_output(output)
    tg.set_loss(total_loss)

    tg.fit_generator(
        databag.iterbatches(
            epochs=1, batch_size=tg.batch_size, pad_batches=True))
    prediction = tg.predict_on_generator(databag.iterbatches())
    assert_true(np.all(np.isclose(prediction[0], prediction[1], atol=0.01)))

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_tensor_graph.py" startline="120" endline="161" pcid="970">
  def test_multi_task_regressor(self):
    n_data_points = 20
    n_features = 2

    X = np.random.rand(n_data_points, n_features)
    y1 = np.expand_dims(np.array([0.5 for x in range(n_data_points)]), axis=-1)
    y2 = np.expand_dims(np.array([-0.5 for x in range(n_data_points)]), axis=-1)
    X = NumpyDataset(X)
    ys = [NumpyDataset(y1), NumpyDataset(y2)]

    databag = Databag()

    features = Feature(shape=(None, n_features))
    databag.add_dataset(features, X)

    outputs = []
    losses = []
    for i in range(2):
      label = Label(shape=(None, 1))
      dense = Dense(out_channels=1, in_layers=[features])
      loss = ReduceSquareDifference(in_layers=[dense, label])

      outputs.append(dense)
      losses.append(loss)
      databag.add_dataset(label, ys[i])

    total_loss = ReduceMean(in_layers=losses)

    tg = dc.models.TensorGraph(learning_rate=0.01)
    for output in outputs:
      tg.add_output(output)
    tg.set_loss(total_loss)

    tg.fit_generator(
        databag.iterbatches(
            epochs=1000, batch_size=tg.batch_size, pad_batches=True))
    predictions = tg.predict_on_generator(databag.iterbatches())
    for i in range(2):
      y_real = ys[i].X
      y_pred = predictions[i]
      assert_true(np.all(np.isclose(y_pred, y_real, atol=1.5)))

</source>
</class>

<class classid="70" nclones="2" nlines="15" similarity="93">
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_tensor_graph.py" startline="570" endline="588" pcid="992">
  def test_get_layer_variable_values(self):
    """Test to get the variable values associated with a layer"""
    # Test for correct value return (normal mode)
    tg = dc.models.TensorGraph()
    var = Variable([10.0, 12.0])
    tg.add_output(var)
    expected = [10.0, 12.0]
    obtained = tg.get_layer_variable_values(var)[0]
    np.testing.assert_array_equal(expected, obtained)

    # Test for shapes (normal mode)
    tg = dc.models.TensorGraph()
    input_tensor = Input(shape=(10, 100))
    output = Dense(out_channels=20, in_layers=[input_tensor])
    tg.add_output(output)
    expected_shape = (100, 20)
    obtained_shape = tg.get_layer_variable_values(output)[0].shape
    assert expected_shape == obtained_shape

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/tests/test_tensor_graph.py" startline="589" endline="608" pcid="993">
  def test_get_layer_variable_values_eager(self):
    """Tests to get variable values associated with a layer in eager mode"""

    with context.eager_mode():
      # Test for correct value return (eager mode)
      tg = dc.models.TensorGraph()
      var = Variable([10.0, 12.0])
      tg.add_output(var)
      expected = [10.0, 12.0]
      obtained = tg.get_layer_variable_values(var)[0]
      np.testing.assert_array_equal(expected, obtained)

      # Test for shape (eager mode)
      tg = dc.models.TensorGraph()
      input_tensor = Input(shape=(10, 100))
      output = Dense(out_channels=20, in_layers=[input_tensor])
      tg.add_output(output)
      expected_shape = (100, 20)
      obtained_shape = tg.get_layer_variable_values(output)[0].shape
      assert expected_shape == obtained_shape
</source>
</class>

<class classid="71" nclones="2" nlines="20" similarity="75">
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/symmetry_functions.py" startline="96" endline="117" pcid="1035">
  def __init__(self,
               max_atoms,
               Rs_init=None,
               ita_init=None,
               atomic_number_differentiated=False,
               atom_numbers=[1, 6, 7, 8],
               **kwargs):
    self.max_atoms = max_atoms
    self.atomic_number_differentiated = atomic_number_differentiated
    self.atom_number_cases = atom_numbers
    if Rs_init is None:
      self.Rs_init = np.array([0.5, 1.17, 1.83, 2.5, 3.17, 3.83, 4.5])
      self.Rs_init = self.Rs_init / 0.52917721092
    else:
      self.Rs_init = np.array(Rs_init)
    if ita_init is None:
      self.ita_init = np.array([1.12])
    else:
      self.ita_init = np.array(ita_init)

    super(RadialSymmetry, self).__init__(**kwargs)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tensorgraph/symmetry_functions.py" startline="166" endline="189" pcid="1038">
  def __init__(self,
               max_atoms,
               lambd_init=None,
               ita_init=None,
               zeta_init=None,
               **kwargs):
    self.max_atoms = max_atoms
    if lambd_init is None:
      self.lambd_init = np.array([1., -1.])
    else:
      self.lambd_init = np.array(lambd_init)

    if ita_init is None:
      self.ita_init = np.array([4.])
    else:
      self.ita_init = np.array(ita_init)

    if zeta_init is None:
      self.zeta_init = np.array([2., 4., 8.])
    else:
      self.zeta_init = np.array(zeta_init)

    super(AngularSymmetry, self).__init__(**kwargs)

</source>
</class>

<class classid="72" nclones="2" nlines="13" similarity="71">
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_gan.py" startline="56" endline="73" pcid="1078">
  def test_cgan(self):
    """Test fitting a conditional GAN."""

    gan = ExampleGAN(learning_rate=0.01)
    gan.fit_gan(
        generate_data(gan, 500, 100),
        generator_steps=0.5,
        checkpoint_interval=0)

    # See if it has done a plausible job of learning the distribution.

    means = 10 * np.random.random([1000, 1])
    values = gan.predict_gan_generator(conditional_inputs=[means])
    deltas = values - means
    assert abs(np.mean(deltas)) < 1.0
    assert np.std(deltas) > 1.0
    assert gan.get_global_step() == 500

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_gan.py" startline="75" endline="94" pcid="1079">
  def test_mix_gan(self):
    """Test a GAN with multiple generators and discriminators."""

    gan = ExampleGAN(n_generators=2, n_discriminators=2, learning_rate=0.01)
    gan.fit_gan(
        generate_data(gan, 1000, 100),
        generator_steps=0.5,
        checkpoint_interval=0)

    # See if it has done a plausible job of learning the distribution.

    means = 10 * np.random.random([1000, 1])
    for i in range(2):
      values = gan.predict_gan_generator(
          conditional_inputs=[means], generator_index=i)
      deltas = values - means
      assert abs(np.mean(deltas)) < 1.0
      assert np.std(deltas) > 1.0
    assert gan.get_global_step() == 1000

</source>
</class>

<class classid="73" nclones="2" nlines="27" similarity="75">
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_kerasmodel.py" startline="284" endline="318" pcid="1103">
  def test_fit_variables(self):
    """Test training a subset of the variables in a model."""

    class VarModel(tf.keras.Model):

      def __init__(self, **kwargs):
        super(VarModel, self).__init__(**kwargs)
        self.var1 = tf.Variable([0.5])
        self.var2 = tf.Variable([0.5])

      def call(self, inputs, training=False):
        return [self.var1, self.var2]

    def loss(outputs, labels, weights):
      return (outputs[0] * outputs[1] - labels[0])**2

    keras_model = VarModel()
    model = dc.models.KerasModel(keras_model, loss, learning_rate=0.01)
    x = np.ones((1, 1))
    vars = model.predict_on_batch(x)
    assert np.allclose(vars[0], 0.5)
    assert np.allclose(vars[1], 0.5)
    model.fit_generator([(x, x, x)] * 300)
    vars = model.predict_on_batch(x)
    assert np.allclose(vars[0], 1.0)
    assert np.allclose(vars[1], 1.0)
    model.fit_generator([(x, 2 * x, x)] * 300, variables=[keras_model.var1])
    vars = model.predict_on_batch(x)
    assert np.allclose(vars[0], 2.0)
    assert np.allclose(vars[1], 1.0)
    model.fit_generator([(x, x, x)] * 300, variables=[keras_model.var2])
    vars = model.predict_on_batch(x)
    assert np.allclose(vars[0], 2.0)
    assert np.allclose(vars[1], 0.5)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_kerasmodel.py" startline="324" endline="356" pcid="1108">
  def test_fit_loss(self):
    """Test specifying a different loss function when calling fit()."""

    class VarModel(tf.keras.Model):

      def __init__(self, **kwargs):
        super(VarModel, self).__init__(**kwargs)
        self.var1 = tf.Variable([0.5])
        self.var2 = tf.Variable([0.5])

      def call(self, inputs, training=False):
        return [self.var1, self.var2]

    def loss1(outputs, labels, weights):
      return (outputs[0] * outputs[1] - labels[0])**2

    def loss2(outputs, labels, weights):
      return (outputs[0] + outputs[1] - labels[0])**2

    keras_model = VarModel()
    model = dc.models.KerasModel(keras_model, loss1, learning_rate=0.01)
    x = np.ones((1, 1))
    vars = model.predict_on_batch(x)
    assert np.allclose(vars[0], 0.5)
    assert np.allclose(vars[1], 0.5)
    model.fit_generator([(x, x, x)] * 300)
    vars = model.predict_on_batch(x)
    assert np.allclose(vars[0], 1.0)
    assert np.allclose(vars[1], 1.0)
    model.fit_generator([(x, 3 * x, x)] * 300, loss=loss2)
    vars = model.predict_on_batch(x)
    assert np.allclose(vars[0] + vars[1], 3.0)

</source>
</class>

<class classid="74" nclones="6" nlines="19" similarity="71">
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_overfit.py" startline="38" endline="63" pcid="1117">
  def test_sklearn_regression_overfit(self):
    """Test that sklearn models can overfit simple regression datasets."""
    n_samples = 10
    n_features = 3
    n_tasks = 1

    # Generate dummy dataset
    np.random.seed(123)
    ids = np.arange(n_samples)
    X = np.random.rand(n_samples, n_features)
    y = np.random.rand(n_samples, n_tasks)
    w = np.ones((n_samples, n_tasks))
    dataset = dc.data.NumpyDataset(X, y, w, ids)

    regression_metric = dc.metrics.Metric(dc.metrics.r2_score)
    sklearn_model = RandomForestRegressor()
    model = dc.models.SklearnModel(sklearn_model)

    # Fit trained model
    model.fit(dataset)
    model.save()

    # Eval model on train
    scores = model.evaluate(dataset, [regression_metric])
    assert scores[regression_metric.name] > .7

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_reload.py" startline="21" endline="51" pcid="1165">
  def test_sklearn_reload(self):
    """Test that trained model can be reloaded correctly."""
    n_samples = 10
    n_features = 3
    n_tasks = 1

    # Generate dummy dataset
    np.random.seed(123)
    ids = np.arange(n_samples)
    X = np.random.rand(n_samples, n_features)
    y = np.random.randint(2, size=(n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))

    dataset = dc.data.NumpyDataset(X, y, w, ids)
    classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)

    sklearn_model = RandomForestClassifier()
    model_dir = tempfile.mkdtemp()
    model = dc.models.SklearnModel(sklearn_model, model_dir)

    # Fit trained model
    model.fit(dataset)
    model.save()

    # Load trained model
    reloaded_model = dc.models.SklearnModel(None, model_dir)
    reloaded_model.reload()

    # Eval model on train
    scores = reloaded_model.evaluate(dataset, [classification_metric])
    assert scores[classification_metric.name] > .9
</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_overfit.py" startline="64" endline="89" pcid="1118">
  def test_sklearn_classification_overfit(self):
    """Test that sklearn models can overfit simple classification datasets."""
    n_samples = 10
    n_features = 3
    n_tasks = 1

    # Generate dummy dataset
    np.random.seed(123)
    ids = np.arange(n_samples)
    X = np.random.rand(n_samples, n_features)
    y = np.random.randint(2, size=(n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))
    dataset = dc.data.NumpyDataset(X, y, w, ids)

    classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
    sklearn_model = RandomForestClassifier()
    model = dc.models.SklearnModel(sklearn_model)

    # Fit trained model
    model.fit(dataset)
    model.save()

    # Eval model on train
    scores = model.evaluate(dataset, [classification_metric])
    assert scores[classification_metric.name] > .9

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_overfit.py" startline="90" endline="117" pcid="1119">
  def test_sklearn_skewed_classification_overfit(self):
    """Test sklearn models can overfit 0/1 datasets with few actives."""
    n_samples = 100
    n_features = 3
    n_tasks = 1

    # Generate dummy dataset
    np.random.seed(123)
    p = .05
    ids = np.arange(n_samples)
    X = np.random.rand(n_samples, n_features)
    y = np.random.binomial(1, p, size=(n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))

    dataset = dc.data.NumpyDataset(X, y, w, ids)

    classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
    sklearn_model = RandomForestClassifier()
    model = dc.models.SklearnModel(sklearn_model)

    # Fit trained model
    model.fit(dataset)
    model.save()

    # Eval model on train
    scores = model.evaluate(dataset, [classification_metric])
    assert scores[classification_metric.name] > .9

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_overfit.py" startline="290" endline="321" pcid="1125">
  def test_sklearn_multitask_classification_overfit(self):
    """Test SKLearn singletask-to-multitask overfits tiny data."""
    n_tasks = 10
    tasks = ["task%d" % task for task in range(n_tasks)]
    n_samples = 10
    n_features = 3

    # Generate dummy dataset
    np.random.seed(123)
    ids = np.arange(n_samples)
    X = np.random.rand(n_samples, n_features)
    y = np.random.randint(2, size=(n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))
    dataset = dc.data.DiskDataset.from_numpy(X, y, w, ids)

    classification_metric = dc.metrics.Metric(
        dc.metrics.roc_auc_score, task_averager=np.mean)

    def model_builder(model_dir):
      sklearn_model = RandomForestClassifier()
      return dc.models.SklearnModel(sklearn_model, model_dir)

    model = dc.models.SingletaskToMultitask(tasks, model_builder)

    # Fit trained model
    model.fit(dataset)
    model.save()

    # Eval model on train
    scores = model.evaluate(dataset, [classification_metric])
    assert scores[classification_metric.name] > .9

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_overfit.py" startline="417" endline="449" pcid="1130">
  def test_sklearn_multitask_regression_overfit(self):
    """Test SKLearn singletask-to-multitask overfits tiny regression data."""
    n_tasks = 2
    tasks = ["task%d" % task for task in range(n_tasks)]
    n_samples = 10
    n_features = 3

    # Generate dummy dataset
    np.random.seed(123)
    ids = np.arange(n_samples)
    X = np.random.rand(n_samples, n_features)
    y = np.random.rand(n_samples, n_tasks)
    w = np.ones((n_samples, n_tasks))

    dataset = dc.data.DiskDataset.from_numpy(X, y, w, ids)

    regression_metric = dc.metrics.Metric(
        dc.metrics.r2_score, task_averager=np.mean)

    def model_builder(model_dir):
      sklearn_model = RandomForestRegressor()
      return dc.models.SklearnModel(sklearn_model, model_dir)

    model = dc.models.SingletaskToMultitask(tasks, model_builder)

    # Fit trained model
    model.fit(dataset)
    model.save()

    # Eval model on train
    scores = model.evaluate(dataset, [regression_metric])
    assert scores[regression_metric.name] > .7

</source>
</class>

<class classid="75" nclones="11" nlines="23" similarity="73">
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_overfit.py" startline="118" endline="148" pcid="1120">
  def test_regression_overfit(self):
    """Test that TensorGraph models can overfit simple regression datasets."""
    n_samples = 10
    n_features = 3
    n_tasks = 1

    # Generate dummy dataset
    np.random.seed(123)
    ids = np.arange(n_samples)
    X = np.random.rand(n_samples, n_features)
    y = np.zeros((n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))
    dataset = dc.data.NumpyDataset(X, y, w, ids)

    regression_metric = dc.metrics.Metric(dc.metrics.mean_squared_error)
    # TODO(rbharath): This breaks with optimizer="momentum". Why?
    model = dc.models.MultitaskRegressor(
        n_tasks,
        n_features,
        dropouts=[0.],
        weight_init_stddevs=[np.sqrt(6) / np.sqrt(1000)],
        batch_size=n_samples,
        learning_rate=0.003)

    # Fit trained model
    model.fit(dataset, nb_epoch=100)

    # Eval model on train
    scores = model.evaluate(dataset, [regression_metric])
    assert scores[regression_metric.name] < .1

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_overfit.py" startline="149" endline="179" pcid="1121">
  def test_classification_overfit(self):
    """Test that TensorGraph models can overfit simple classification datasets."""
    n_samples = 10
    n_features = 3
    n_tasks = 1
    n_classes = 2

    # Generate dummy dataset
    np.random.seed(123)
    ids = np.arange(n_samples)
    X = np.random.rand(n_samples, n_features)
    y = np.zeros((n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))
    dataset = dc.data.NumpyDataset(X, y, w, ids)

    classification_metric = dc.metrics.Metric(dc.metrics.accuracy_score)
    model = dc.models.MultitaskClassifier(
        n_tasks,
        n_features,
        dropouts=[0.],
        weight_init_stddevs=[.1],
        batch_size=n_samples,
        optimizer=Adam(learning_rate=0.0003, beta1=0.9, beta2=0.999))

    # Fit trained model
    model.fit(dataset, nb_epoch=100)

    # Eval model on train
    scores = model.evaluate(dataset, [classification_metric])
    assert scores[classification_metric.name] > .9

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_overfit.py" startline="450" endline="482" pcid="1132">
  def test_multitask_regression_overfit(self):
    """Test TensorGraph multitask overfits tiny data."""
    n_tasks = 10
    n_samples = 10
    n_features = 3
    n_classes = 2

    # Generate dummy dataset
    np.random.seed(123)
    ids = np.arange(n_samples)
    X = np.random.rand(n_samples, n_features)
    y = np.zeros((n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))

    dataset = dc.data.NumpyDataset(X, y, w, ids)

    regression_metric = dc.metrics.Metric(
        dc.metrics.mean_squared_error, task_averager=np.mean, mode="regression")
    model = dc.models.MultitaskRegressor(
        n_tasks,
        n_features,
        dropouts=[0.],
        weight_init_stddevs=[.1],
        batch_size=n_samples,
        optimizer=Adam(learning_rate=0.0003, beta1=0.9, beta2=0.999))

    # Fit trained model
    model.fit(dataset, nb_epoch=50)

    # Eval model on train
    scores = model.evaluate(dataset, [regression_metric])
    assert scores[regression_metric.name] < .1

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_overfit.py" startline="212" endline="245" pcid="1123">
  def test_skewed_classification_overfit(self):
    """Test TensorGraph models can overfit 0/1 datasets with few actives."""
    #n_samples = 100
    n_samples = 100
    n_features = 3
    n_tasks = 1
    n_classes = 2

    # Generate dummy dataset
    np.random.seed(123)
    p = .05
    ids = np.arange(n_samples)
    X = np.random.rand(n_samples, n_features)
    y = np.random.binomial(1, p, size=(n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))

    dataset = dc.data.NumpyDataset(X, y, w, ids)

    classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
    model = dc.models.MultitaskClassifier(
        n_tasks,
        n_features,
        dropouts=[0.],
        weight_init_stddevs=[.1],
        batch_size=n_samples,
        learning_rate=0.003)

    # Fit trained model
    model.fit(dataset, nb_epoch=100)

    # Eval model on train
    scores = model.evaluate(dataset, [classification_metric])
    assert scores[classification_metric.name] > .75

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_overfit.py" startline="180" endline="211" pcid="1122">
  def test_fittransform_regression_overfit(self):
    """Test that TensorGraph FitTransform models can overfit simple regression datasets."""
    n_samples = 10
    n_features = 3
    n_tasks = 1

    # Generate dummy dataset
    np.random.seed(123)
    ids = np.arange(n_samples)
    X = np.random.rand(n_samples, n_features, n_features)
    y = np.zeros((n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))
    dataset = dc.data.NumpyDataset(X, y, w, ids)

    fit_transformers = [dc.trans.CoulombFitTransformer(dataset)]
    regression_metric = dc.metrics.Metric(dc.metrics.mean_squared_error)
    model = dc.models.MultitaskFitTransformRegressor(
        n_tasks, [n_features, n_features],
        dropouts=[0.01],
        weight_init_stddevs=[np.sqrt(6) / np.sqrt(1000)],
        batch_size=n_samples,
        fit_transformers=fit_transformers,
        n_evals=1,
        optimizer=Adam(learning_rate=0.003, beta1=0.9, beta2=0.999))

    # Fit trained model
    model.fit(dataset, nb_epoch=100)

    # Eval model on train
    scores = model.evaluate(dataset, [regression_metric])
    assert scores[regression_metric.name] < .1

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_overfit.py" startline="355" endline="388" pcid="1128">
  def test_tf_robust_multitask_classification_overfit(self):
    """Test tf robust multitask overfits tiny data."""
    n_tasks = 10
    n_samples = 10
    n_features = 3
    n_classes = 2

    # Generate dummy dataset
    np.random.seed(123)
    ids = np.arange(n_samples)
    X = np.random.rand(n_samples, n_features)
    y = np.zeros((n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))
    dataset = dc.data.NumpyDataset(X, y, w, ids)

    classification_metric = dc.metrics.Metric(
        dc.metrics.accuracy_score, task_averager=np.mean)
    model = dc.models.RobustMultitaskClassifier(
        n_tasks,
        n_features,
        layer_sizes=[50],
        bypass_layer_sizes=[10],
        dropouts=[0.],
        learning_rate=0.003,
        weight_init_stddevs=[.1],
        batch_size=n_samples)

    # Fit trained model
    model.fit(dataset, nb_epoch=25)

    # Eval model on train
    scores = model.evaluate(dataset, [classification_metric])
    assert scores[classification_metric.name] > .9

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_overfit.py" startline="323" endline="354" pcid="1127">
  def test_multitask_classification_overfit(self):
    """Test TensorGraph multitask overfits tiny data."""
    n_tasks = 10
    n_samples = 10
    n_features = 3
    n_classes = 2

    # Generate dummy dataset
    np.random.seed(123)
    ids = np.arange(n_samples)
    X = np.random.rand(n_samples, n_features)
    y = np.zeros((n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))
    dataset = dc.data.NumpyDataset(X, y, w, ids)

    classification_metric = dc.metrics.Metric(
        dc.metrics.accuracy_score, task_averager=np.mean)
    model = dc.models.MultitaskClassifier(
        n_tasks,
        n_features,
        dropouts=[0.],
        weight_init_stddevs=[.1],
        batch_size=n_samples,
        optimizer=Adam(learning_rate=0.0003, beta1=0.9, beta2=0.999))

    # Fit trained model
    model.fit(dataset)

    # Eval model on train
    scores = model.evaluate(dataset, [classification_metric])
    assert scores[classification_metric.name] > .9

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_overfit.py" startline="483" endline="519" pcid="1133">
  def test_tf_robust_multitask_regression_overfit(self):
    """Test tf robust multitask overfits tiny data."""
    np.random.seed(123)
    tf.set_random_seed(123)
    n_tasks = 10
    n_samples = 10
    n_features = 3
    n_classes = 2

    # Generate dummy dataset
    np.random.seed(123)
    ids = np.arange(n_samples)
    X = np.random.rand(n_samples, n_features)
    y = np.zeros((n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))

    dataset = dc.data.NumpyDataset(X, y, w, ids)

    regression_metric = dc.metrics.Metric(
        dc.metrics.mean_squared_error, task_averager=np.mean, mode="regression")
    model = dc.models.RobustMultitaskRegressor(
        n_tasks,
        n_features,
        layer_sizes=[50],
        bypass_layer_sizes=[10],
        dropouts=[0.],
        learning_rate=0.003,
        weight_init_stddevs=[.1],
        batch_size=n_samples)

    # Fit trained model
    model.fit(dataset, nb_epoch=25)

    # Eval model on train
    scores = model.evaluate(dataset, [regression_metric])
    assert scores[regression_metric.name] < .2

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_overfit.py" startline="246" endline="289" pcid="1124">
  def test_skewed_missing_classification_overfit(self):
    """TG, skewed data, few actives

    Test TensorGraph models overfit 0/1 datasets with missing data and few
    actives. This is intended to be as close to singletask MUV datasets as
    possible.
    """
    n_samples = 5120
    n_features = 6
    n_tasks = 1
    n_classes = 2

    # Generate dummy dataset
    np.random.seed(123)
    p = .002
    ids = np.arange(n_samples)
    X = np.random.rand(n_samples, n_features)
    y = np.random.binomial(1, p, size=(n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))
    y_flat, w_flat = np.squeeze(y), np.squeeze(w)
    y_nonzero = y_flat[w_flat != 0]
    num_nonzero = np.count_nonzero(y_nonzero)
    weight_nonzero = len(y_nonzero) / num_nonzero
    w_flat[y_flat != 0] = weight_nonzero
    w = np.reshape(w_flat, (n_samples, n_tasks))

    dataset = dc.data.DiskDataset.from_numpy(X, y, w, ids)

    classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
    model = dc.models.MultitaskClassifier(
        n_tasks,
        n_features,
        dropouts=[0.],
        weight_init_stddevs=[1.],
        batch_size=n_samples,
        learning_rate=0.003)

    # Fit trained model
    model.fit(dataset, nb_epoch=100)

    # Eval model on train
    scores = model.evaluate(dataset, [classification_metric])
    assert scores[classification_metric.name] > .7

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_overfit.py" startline="790" endline="824" pcid="1141">
  def test_progressive_classification_overfit(self):
    """Test progressive multitask overfits tiny data."""
    np.random.seed(123)
    n_tasks = 5
    n_samples = 10
    n_features = 6

    # Generate dummy dataset
    np.random.seed(123)
    ids = np.arange(n_samples)
    X = np.random.rand(n_samples, n_features)
    y = np.random.randint(2, size=(n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))

    dataset = dc.data.NumpyDataset(X, y, w, ids)

    metric = dc.metrics.Metric(dc.metrics.accuracy_score, task_averager=np.mean)
    model = dc.models.ProgressiveMultitaskClassifier(
        n_tasks,
        n_features,
        layer_sizes=[50],
        bypass_layer_sizes=[10],
        dropouts=[0.],
        learning_rate=0.002,
        weight_init_stddevs=[.1],
        alpha_init_stddevs=[.02],
        batch_size=n_samples)

    # Fit trained model
    model.fit(dataset, nb_epoch=200)

    # Eval model on train
    scores = model.evaluate(dataset, [metric])
    assert scores[metric.name] > .9

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_overfit.py" startline="825" endline="859" pcid="1142">
  def test_progressive_regression_overfit(self):
    """Test progressive multitask overfits tiny data."""
    np.random.seed(123)
    n_tasks = 5
    n_samples = 10
    n_features = 6

    # Generate dummy dataset
    np.random.seed(123)
    ids = np.arange(n_samples)
    X = np.random.rand(n_samples, n_features)
    y = np.random.rand(n_samples, n_tasks)
    w = np.ones((n_samples, n_tasks))

    dataset = dc.data.NumpyDataset(X, y, w, ids)

    metric = dc.metrics.Metric(dc.metrics.rms_score, task_averager=np.mean)
    model = dc.models.ProgressiveMultitaskRegressor(
        n_tasks,
        n_features,
        layer_sizes=[50],
        bypass_layer_sizes=[10],
        dropouts=[0.],
        learning_rate=0.002,
        weight_init_stddevs=[.1],
        alpha_init_stddevs=[.02],
        batch_size=n_samples)

    # Fit trained model
    model.fit(dataset, nb_epoch=200)

    # Eval model on train
    scores = model.evaluate(dataset, [metric])
    assert scores[metric.name] < .2

</source>
</class>

<class classid="76" nclones="6" nlines="27" similarity="77">
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_overfit.py" startline="560" endline="597" pcid="1135">
  def test_tensorgraph_DAG_singletask_regression_overfit(self):
    """Test DAG regressor multitask overfits tiny data."""
    np.random.seed(123)
    tf.set_random_seed(123)
    n_tasks = 1

    # Load mini log-solubility dataset.
    featurizer = dc.feat.ConvMolFeaturizer()
    tasks = ["outcome"]
    input_file = os.path.join(self.current_dir, "example_regression.csv")
    loader = dc.data.CSVLoader(
        tasks=tasks, smiles_field="smiles", featurizer=featurizer)
    dataset = loader.featurize(input_file)

    regression_metric = dc.metrics.Metric(
        dc.metrics.pearson_r2_score, task_averager=np.mean)

    n_feat = 75
    batch_size = 10
    transformer = dc.trans.DAGTransformer(max_atoms=50)
    dataset = transformer.transform(dataset)

    model = dc.models.DAGModel(
        n_tasks,
        max_atoms=50,
        n_atom_feat=n_feat,
        batch_size=batch_size,
        learning_rate=0.001,
        use_queue=False,
        mode="regression")

    # Fit trained model
    model.fit(dataset, nb_epoch=50)
    # Eval model on train
    scores = model.evaluate(dataset, [regression_metric])

    assert scores[regression_metric.name] > .8

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_overfit.py" startline="598" endline="636" pcid="1136">
  def test_tensorgraph_weave_singletask_classification_overfit(self):
    """Test weave model overfits tiny data."""
    np.random.seed(123)
    tf.set_random_seed(123)
    n_tasks = 1

    # Load mini log-solubility dataset.
    featurizer = dc.feat.WeaveFeaturizer()
    tasks = ["outcome"]
    input_file = os.path.join(self.current_dir, "example_classification.csv")
    loader = dc.data.CSVLoader(
        tasks=tasks, smiles_field="smiles", featurizer=featurizer)
    dataset = loader.featurize(input_file)

    classification_metric = dc.metrics.Metric(dc.metrics.accuracy_score)

    n_atom_feat = 75
    n_pair_feat = 14
    n_feat = 128
    batch_size = 10

    model = dc.models.WeaveModel(
        n_tasks,
        n_atom_feat=n_atom_feat,
        n_pair_feat=n_pair_feat,
        n_graph_feat=n_feat,
        batch_size=batch_size,
        learning_rate=0.001,
        use_queue=False,
        mode="classification")

    # Fit trained model
    model.fit(dataset, nb_epoch=20)

    # Eval model on train
    scores = model.evaluate(dataset, [classification_metric])

    assert scores[classification_metric.name] > .65

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_overfit.py" startline="753" endline="789" pcid="1140">
  def test_textCNN_singletask_regression_overfit(self):
    """Test textCNN model overfits tiny data."""
    np.random.seed(123)
    tf.set_random_seed(123)
    n_tasks = 1

    # Load mini log-solubility dataset.
    featurizer = dc.feat.RawFeaturizer()
    tasks = ["outcome"]
    input_file = os.path.join(self.current_dir, "example_regression.csv")
    loader = dc.data.CSVLoader(
        tasks=tasks, smiles_field="smiles", featurizer=featurizer)
    dataset = loader.featurize(input_file)

    regression_metric = dc.metrics.Metric(
        dc.metrics.pearson_r2_score, task_averager=np.mean)

    char_dict, length = dc.models.TextCNNModel.build_char_dict(dataset)
    batch_size = 10

    model = dc.models.TextCNNModel(
        n_tasks,
        char_dict,
        seq_length=length,
        batch_size=batch_size,
        learning_rate=0.001,
        use_queue=False,
        mode="regression")

    # Fit trained model
    model.fit(dataset, nb_epoch=200)

    # Eval model on train
    scores = model.evaluate(dataset, [regression_metric])

    assert scores[regression_metric.name] > .9

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_overfit.py" startline="637" endline="676" pcid="1137">
  def test_tensorgraph_weave_singletask_regression_overfit(self):
    """Test weave model overfits tiny data."""
    np.random.seed(123)
    tf.set_random_seed(123)
    n_tasks = 1

    # Load mini log-solubility dataset.
    featurizer = dc.feat.WeaveFeaturizer()
    tasks = ["outcome"]
    input_file = os.path.join(self.current_dir, "example_regression.csv")
    loader = dc.data.CSVLoader(
        tasks=tasks, smiles_field="smiles", featurizer=featurizer)
    dataset = loader.featurize(input_file)

    regression_metric = dc.metrics.Metric(
        dc.metrics.pearson_r2_score, task_averager=np.mean)

    n_atom_feat = 75
    n_pair_feat = 14
    n_feat = 128
    batch_size = 10

    model = dc.models.WeaveModel(
        n_tasks,
        n_atom_feat=n_atom_feat,
        n_pair_feat=n_pair_feat,
        n_graph_feat=n_feat,
        batch_size=batch_size,
        learning_rate=0.001,
        use_queue=False,
        mode="regression")

    # Fit trained model
    model.fit(dataset, nb_epoch=120)

    # Eval model on train
    scores = model.evaluate(dataset, [regression_metric])

    assert scores[regression_metric.name] > .8

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_overfit.py" startline="678" endline="716" pcid="1138">
  def test_MPNN_singletask_regression_overfit(self):
    """Test MPNN overfits tiny data."""
    np.random.seed(123)
    tf.set_random_seed(123)
    n_tasks = 1

    # Load mini log-solubility dataset.
    featurizer = dc.feat.WeaveFeaturizer()
    tasks = ["outcome"]
    input_file = os.path.join(self.current_dir, "example_regression.csv")
    loader = dc.data.CSVLoader(
        tasks=tasks, smiles_field="smiles", featurizer=featurizer)
    dataset = loader.featurize(input_file)

    regression_metric = dc.metrics.Metric(
        dc.metrics.pearson_r2_score, task_averager=np.mean)

    n_atom_feat = 75
    n_pair_feat = 14
    batch_size = 10
    model = dc.models.MPNNModel(
        n_tasks,
        n_atom_feat=n_atom_feat,
        n_pair_feat=n_pair_feat,
        T=2,
        M=3,
        batch_size=batch_size,
        learning_rate=0.001,
        use_queue=False,
        mode="regression")

    # Fit trained model
    model.fit(dataset, nb_epoch=50)

    # Eval model on train
    scores = model.evaluate(dataset, [regression_metric])

    assert scores[regression_metric.name] > .8

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_overfit.py" startline="717" endline="751" pcid="1139">
  def test_textCNN_singletask_classification_overfit(self):
    """Test textCNN model overfits tiny data."""
    np.random.seed(123)
    tf.set_random_seed(123)
    n_tasks = 1

    featurizer = dc.feat.RawFeaturizer()
    tasks = ["outcome"]
    input_file = os.path.join(self.current_dir, "example_classification.csv")
    loader = dc.data.CSVLoader(
        tasks=tasks, smiles_field="smiles", featurizer=featurizer)
    dataset = loader.featurize(input_file)

    classification_metric = dc.metrics.Metric(dc.metrics.accuracy_score)

    char_dict, length = dc.models.TextCNNModel.build_char_dict(dataset)
    batch_size = 10

    model = dc.models.TextCNNModel(
        n_tasks,
        char_dict,
        seq_length=length,
        batch_size=batch_size,
        learning_rate=0.001,
        use_queue=False,
        mode="classification")

    # Fit trained model
    model.fit(dataset, nb_epoch=200)

    # Eval model on train
    scores = model.evaluate(dataset, [classification_metric])

    assert scores[classification_metric.name] > .8

</source>
</class>

<class classid="77" nclones="5" nlines="12" similarity="76">
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_chemnet_models.py" startline="70" endline="83" pcid="1153">
  def test_smiles_to_vec_regression(self):
    dataset, metric = self.get_dataset(
        mode="regression", featurizer="smiles2seq")
    model = Smiles2Vec(
        char_to_idx=self.char_to_idx,
        max_seq_len=self.max_seq_len,
        use_conv=True,
        n_tasks=self.n_tasks,
        model_dir=None,
        mode="regression")
    model.fit(dataset, nb_epoch=500)
    scores = model.evaluate(dataset, [metric], [])
    assert all(s < 0.1 for s in scores['mean_absolute_error'])

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_chemnet_models.py" startline="126" endline="137" pcid="1157">
  def test_chemception_fit_with_augmentation(self):
    dataset, metric = self.get_dataset(
        mode="classification", featurizer="smiles2img")
    model = ChemCeption(
        n_tasks=self.n_tasks,
        img_spec="engd",
        model_dir=None,
        augment=True,
        mode="classification")
    model.fit(dataset, nb_epoch=300)
    scores = model.evaluate(dataset, [metric], [])
    assert scores['mean-roc_auc_score'] >= 0.9
</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_chemnet_models.py" startline="85" endline="98" pcid="1154">
  def test_smiles_to_vec_classification(self):
    dataset, metric = self.get_dataset(
        mode="classification", featurizer="smiles2seq")
    model = Smiles2Vec(
        char_to_idx=self.char_to_idx,
        max_seq_len=self.max_seq_len,
        use_conv=True,
        n_tasks=self.n_tasks,
        model_dir=None,
        mode="classification")
    model.fit(dataset, nb_epoch=500)
    scores = model.evaluate(dataset, [metric], [])
    assert scores['mean-roc_auc_score'] >= 0.9

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_chemnet_models.py" startline="100" endline="111" pcid="1155">
  def test_chemception_regression(self):
    dataset, metric = self.get_dataset(
        mode="regression", featurizer="smiles2img")
    model = ChemCeption(
        n_tasks=self.n_tasks,
        img_spec="engd",
        model_dir=None,
        mode="regression")
    model.fit(dataset, nb_epoch=300)
    scores = model.evaluate(dataset, [metric], [])
    assert all(s < 0.1 for s in scores['mean_absolute_error'])

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_chemnet_models.py" startline="113" endline="124" pcid="1156">
  def test_chemception_classification(self):
    dataset, metric = self.get_dataset(
        mode="classification", featurizer="smiles2img")
    model = ChemCeption(
        n_tasks=self.n_tasks,
        img_spec="engd",
        model_dir=None,
        mode="classification")
    model.fit(dataset, nb_epoch=300)
    scores = model.evaluate(dataset, [metric], [])
    assert scores['mean-roc_auc_score'] >= 0.9

</source>
</class>

<class classid="78" nclones="4" nlines="27" similarity="73">
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_api.py" startline="25" endline="59" pcid="1166">
  def test_singletask_sklearn_rf_ECFP_regression_API(self):
    """Test of singletask RF ECFP regression API."""
    splittype = "scaffold"
    featurizer = dc.feat.CircularFingerprint(size=1024)
    tasks = ["log-solubility"]
    current_dir = os.path.dirname(os.path.abspath(__file__))
    input_file = os.path.join(current_dir, "example.csv")
    loader = dc.data.CSVLoader(
        tasks=tasks, smiles_field="smiles", featurizer=featurizer)
    dataset = loader.featurize(input_file)

    splitter = dc.splits.ScaffoldSplitter()
    train_dataset, test_dataset = splitter.train_test_split(dataset)

    transformers = [
        dc.trans.NormalizationTransformer(
            transform_y=True, dataset=train_dataset)
    ]
    regression_metrics = [
        dc.metrics.Metric(dc.metrics.r2_score),
        dc.metrics.Metric(dc.metrics.mean_squared_error),
        dc.metrics.Metric(dc.metrics.mean_absolute_error)
    ]

    sklearn_model = RandomForestRegressor()
    model = dc.models.SklearnModel(sklearn_model)

    # Fit trained model
    model.fit(train_dataset)
    model.save()

    # Eval model on train
    _ = model.evaluate(train_dataset, regression_metrics, transformers)
    _ = model.evaluate(test_dataset, regression_metrics, transformers)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_api.py" startline="100" endline="142" pcid="1168">
  def test_singletask_sklearn_rf_RDKIT_descriptor_regression_API(self):
    """Test of singletask RF RDKIT-descriptor regression API."""
    splittype = "scaffold"
    featurizer = dc.feat.RDKitDescriptors()
    tasks = ["log-solubility"]

    current_dir = os.path.dirname(os.path.abspath(__file__))
    input_file = os.path.join(current_dir, "example.csv")
    loader = dc.data.CSVLoader(
        tasks=tasks, smiles_field="smiles", featurizer=featurizer)
    dataset = loader.featurize(input_file)

    splitter = dc.splits.ScaffoldSplitter()
    train_dataset, test_dataset = splitter.train_test_split(dataset)

    transformers = [
        dc.trans.NormalizationTransformer(
            transform_X=True, dataset=train_dataset),
        dc.trans.ClippingTransformer(transform_X=True, dataset=train_dataset),
        dc.trans.NormalizationTransformer(
            transform_y=True, dataset=train_dataset)
    ]
    for dataset in [train_dataset, test_dataset]:
      for transformer in transformers:
        dataset = transformer.transform(dataset)

    regression_metrics = [
        dc.metrics.Metric(dc.metrics.r2_score),
        dc.metrics.Metric(dc.metrics.mean_squared_error),
        dc.metrics.Metric(dc.metrics.mean_absolute_error)
    ]

    sklearn_model = RandomForestRegressor()
    model = dc.models.SklearnModel(sklearn_model)

    # Fit trained model
    model.fit(train_dataset)
    model.save()

    # Eval model on train/test
    _ = model.evaluate(train_dataset, regression_metrics, transformers)
    _ = model.evaluate(test_dataset, regression_metrics, transformers)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_api.py" startline="60" endline="99" pcid="1167">
  def test_singletask_sklearn_rf_user_specified_regression_API(self):
    """Test of singletask RF USF regression API."""
    splittype = "specified"
    featurizer = dc.feat.UserDefinedFeaturizer(
        ["user-specified1", "user-specified2"])
    tasks = ["log-solubility"]
    current_dir = os.path.dirname(os.path.abspath(__file__))
    input_file = os.path.join(current_dir, "user_specified_example.csv")
    loader = dc.data.UserCSVLoader(
        tasks=tasks, smiles_field="smiles", featurizer=featurizer)
    dataset = loader.featurize(input_file)

    splitter = dc.splits.SpecifiedSplitter(input_file, "split")
    train_dataset, test_dataset = splitter.train_test_split(dataset)

    transformers = [
        dc.trans.NormalizationTransformer(
            transform_y=True, dataset=train_dataset)
    ]
    for dataset in [train_dataset, test_dataset]:
      for transformer in transformers:
        dataset = transformer.transform(dataset)

    regression_metrics = [
        dc.metrics.Metric(dc.metrics.r2_score),
        dc.metrics.Metric(dc.metrics.mean_squared_error),
        dc.metrics.Metric(dc.metrics.mean_absolute_error)
    ]

    sklearn_model = RandomForestRegressor()
    model = dc.models.SklearnModel(sklearn_model)

    # Fit trained model
    model.fit(train_dataset)
    model.save()

    # Eval model on train/test
    _ = model.evaluate(train_dataset, regression_metrics, transformers)
    _ = model.evaluate(test_dataset, regression_metrics, transformers)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_api.py" startline="143" endline="188" pcid="1169">
  def test_singletask_tg_mlp_ECFP_classification_API(self):
    """Test of TensorGraph singletask deepchem classification API."""
    n_features = 1024
    featurizer = dc.feat.CircularFingerprint(size=n_features)

    tasks = ["outcome"]
    current_dir = os.path.dirname(os.path.abspath(__file__))
    input_file = os.path.join(current_dir, "example_classification.csv")

    loader = dc.data.CSVLoader(
        tasks=tasks, smiles_field="smiles", featurizer=featurizer)
    dataset = loader.featurize(input_file)

    splitter = dc.splits.ScaffoldSplitter()
    train_dataset, test_dataset = splitter.train_test_split(dataset)

    transformers = [
        dc.trans.NormalizationTransformer(
            transform_y=True, dataset=train_dataset)
    ]

    for dataset in [train_dataset, test_dataset]:
      for transformer in transformers:
        dataset = transformer.transform(dataset)

    classification_metrics = [
        dc.metrics.Metric(dc.metrics.roc_auc_score),
        dc.metrics.Metric(dc.metrics.matthews_corrcoef),
        dc.metrics.Metric(dc.metrics.recall_score),
        dc.metrics.Metric(dc.metrics.accuracy_score)
    ]

    model = dc.models.MultitaskClassifier(len(tasks), n_features)

    # Test Parameter getting and setting
    param, value = 'weight_decay_penalty_type', 'l2'
    assert model.get_params()[param] is None
    model.set_params(**{param: value})
    assert model.get_params()[param] == value

    # Fit trained model
    model.fit(train_dataset)

    # Eval model on train/test
    _ = model.evaluate(train_dataset, classification_metrics, transformers)
    _ = model.evaluate(test_dataset, classification_metrics, transformers)
</source>
</class>

<class classid="79" nclones="3" nlines="20" similarity="75">
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_generalize.py" startline="30" endline="57" pcid="1170">
  def test_sklearn_regression(self):
    """Test that sklearn models can learn on simple regression datasets."""
    np.random.seed(123)

    dataset = sklearn.datasets.load_diabetes()
    X, y = dataset.data, dataset.target
    y = np.expand_dims(y, 1)
    frac_train = .7
    n_samples = len(X)
    n_train = int(frac_train * n_samples)
    X_train, y_train = X[:n_train], y[:n_train]
    X_test, y_test = X[n_train:], y[n_train:]
    train_dataset = dc.data.NumpyDataset(X_train, y_train)
    test_dataset = dc.data.NumpyDataset(X_test, y_test)

    regression_metric = dc.metrics.Metric(dc.metrics.r2_score)

    sklearn_model = LinearRegression()
    model = dc.models.SklearnModel(sklearn_model)

    # Fit trained model
    model.fit(train_dataset)
    model.save()

    # Eval model on test
    scores = model.evaluate(test_dataset, [regression_metric])
    assert scores[regression_metric.name] > .5

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_generalize.py" startline="196" endline="231" pcid="1174">
  def test_xgboost_regression(self):
    """
    This test is not actually slow -- but cannot currently run
    on Ubuntu 14.04 with Tensorflow 1.4.0

    See Discussion Here
    https://github.com/deepchem/deepchem/issues/960
    """
    import xgboost
    np.random.seed(123)

    dataset = sklearn.datasets.load_diabetes()
    X, y = dataset.data, dataset.target
    frac_train = .7
    n_samples = len(X)
    n_train = int(frac_train * n_samples)
    X_train, y_train = X[:n_train], y[:n_train]
    X_test, y_test = X[n_train:], y[n_train:]
    train_dataset = dc.data.NumpyDataset(X_train, y_train)
    test_dataset = dc.data.NumpyDataset(X_test, y_test)

    regression_metric = dc.metrics.Metric(dc.metrics.mae_score)
    # Set early stopping round = n_estimators so that esr won't work
    esr = {'early_stopping_rounds': 50}

    xgb_model = xgboost.XGBRegressor(n_estimators=50, seed=123)
    model = dc.models.XGBoostModel(xgb_model, verbose=False, **esr)

    # Fit trained model
    model.fit(train_dataset)
    model.save()

    # Eval model on test
    scores = model.evaluate(test_dataset, [regression_metric])
    assert scores[regression_metric.name] < 50

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_generalize.py" startline="278" endline="304" pcid="1177">
  def test_xgboost_classification(self):
    """Test that sklearn models can learn on simple classification datasets."""
    import xgboost
    np.random.seed(123)
    dataset = sklearn.datasets.load_digits(n_class=2)
    X, y = dataset.data, dataset.target

    frac_train = .7
    n_samples = len(X)
    n_train = int(frac_train * n_samples)
    X_train, y_train = X[:n_train], y[:n_train]
    X_test, y_test = X[n_train:], y[n_train:]
    train_dataset = dc.data.NumpyDataset(X_train, y_train)
    test_dataset = dc.data.NumpyDataset(X_test, y_test)

    classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
    esr = {'early_stopping_rounds': 50}
    xgb_model = xgboost.XGBClassifier(n_estimators=50, seed=123)
    model = dc.models.XGBoostModel(xgb_model, verbose=False, **esr)

    # Fit trained model
    model.fit(train_dataset)
    model.save()

    # Eval model on test
    scores = model.evaluate(test_dataset, [classification_metric])
    assert scores[classification_metric.name] > .9
</source>
</class>

<class classid="80" nclones="2" nlines="26" similarity="81">
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_generalize.py" startline="102" endline="194" pcid="1172">
  def test_sklearn_multitask_regression(self):
    """Test that sklearn models can learn on simple multitask regression."""
    np.random.seed(123)
    n_tasks = 4
    tasks = range(n_tasks)
    dataset = sklearn.datasets.load_diabetes()
    X, y = dataset.data, dataset.target
    y = np.reshape(y, (len(y), 1))
    y = np.hstack([y] * n_tasks)

    frac_train = .7
    n_samples = len(X)
    n_train = int(frac_train * n_samples)
    X_train, y_train = X[:n_train], y[:n_train]
    X_test, y_test = X[n_train:], y[n_train:]
    train_dataset = dc.data.DiskDataset.from_numpy(X_train, y_train)
    test_dataset = dc.data.DiskDataset.from_numpy(X_test, y_test)

    regression_metric = dc.metrics.Metric(dc.metrics.r2_score)

    def model_builder(model_dir):
      sklearn_model = LinearRegression()
      return dc.models.SklearnModel(sklearn_model, model_dir)

    model = dc.models.SingletaskToMultitask(tasks, model_builder)

    # Fit trained model
    model.fit(train_dataset)
    model.save()

    # Eval model on test
    scores = model.evaluate(test_dataset, [regression_metric])
    for score in scores[regression_metric.name]:
      assert score > .5

  #def test_sklearn_classification(self):
  #  """Test that sklearn models can learn on simple classification datasets."""
  #  np.random.seed(123)
  #  dataset = sklearn.datasets.load_digits(n_class=2)
  #  X, y = dataset.data, dataset.target

  #  frac_train = .7
  #  n_samples = len(X)
  #  n_train = int(frac_train*n_samples)
  #  X_train, y_train = X[:n_train], y[:n_train]
  #  X_test, y_test = X[n_train:], y[n_train:]
  #  train_dataset = dc.data.NumpyDataset(X_train, y_train)
  #  test_dataset = dc.data.NumpyDataset(X_test, y_test)

  #  classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
  #  sklearn_model = LogisticRegression()
  #  model = dc.models.SklearnModel(sklearn_model)

  #  # Fit trained model
  #  model.fit(train_dataset)
  #  model.save()

  #  # Eval model on test
  #  scores = model.evaluate(test_dataset, [classification_metric])
  #  assert scores[classification_metric.name] > .5

  #def test_sklearn_multitask_classification(self):
  #  """Test that sklearn models can learn on simple multitask classification."""
  #  np.random.seed(123)
  #  n_tasks = 4
  #  tasks = range(n_tasks)
  #  dataset = sklearn.datasets.load_digits(n_class=2)
  #  X, y = dataset.data, dataset.target
  #  y = np.reshape(y, (len(y), 1))
  #  y = np.hstack([y] * n_tasks)
  #
  #  frac_train = .7
  #  n_samples = len(X)
  #  n_train = int(frac_train*n_samples)
  #  X_train, y_train = X[:n_train], y[:n_train]
  #  X_test, y_test = X[n_train:], y[n_train:]
  #  train_dataset = dc.data.DiskDataset.from_numpy(X_train, y_train)
  #  test_dataset = dc.data.DiskDataset.from_numpy(X_test, y_test)

  #  classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
  #  def model_builder(model_dir):
  #    sklearn_model = LogisticRegression()
  #    return dc.models.SklearnModel(sklearn_model, model_dir)
  #  model = dc.models.SingletaskToMultitask(tasks, model_builder)

  #  # Fit trained model
  #  model.fit(train_dataset)
  #  model.save()
  #  # Eval model on test
  #  scores = model.evaluate(test_dataset, [classification_metric])
  #  for score in scores[classification_metric.name]:
  #    assert score > .5

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_generalize.py" startline="233" endline="276" pcid="1175">
  def test_xgboost_multitask_regression(self):
    """
    Test that xgboost models can learn on simple multitask regression.
    This test is not actually slow -- but cannot currently run
    on Ubuntu 14.04 with Tensorflow 1.4.0

    See Discussion Here
    https://github.com/deepchem/deepchem/issues/960
    """
    import xgboost
    np.random.seed(123)
    n_tasks = 4
    tasks = range(n_tasks)
    dataset = sklearn.datasets.load_diabetes()
    X, y = dataset.data, dataset.target
    y = np.reshape(y, (len(y), 1))
    y = np.hstack([y] * n_tasks)

    frac_train = .7
    n_samples = len(X)
    n_train = int(frac_train * n_samples)
    X_train, y_train = X[:n_train], y[:n_train]
    X_test, y_test = X[n_train:], y[n_train:]
    train_dataset = dc.data.DiskDataset.from_numpy(X_train, y_train)
    test_dataset = dc.data.DiskDataset.from_numpy(X_test, y_test)

    regression_metric = dc.metrics.Metric(dc.metrics.mae_score)
    esr = {'early_stopping_rounds': 50}

    def model_builder(model_dir):
      xgb_model = xgboost.XGBRegressor(n_estimators=50, seed=123)
      return dc.models.XGBoostModel(xgb_model, model_dir, verbose=False, **esr)

    model = dc.models.SingletaskToMultitask(tasks, model_builder)

    # Fit trained model
    model.fit(train_dataset)
    model.save()

    # Eval model on test
    scores = model.evaluate(test_dataset, [regression_metric])
    for score in scores[regression_metric.name]:
      assert score < 50

</source>
</class>

<class classid="81" nclones="3" nlines="20" similarity="76">
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_graph_models.py" startline="89" endline="111" pcid="1182">
  def test_graph_conv_regression_uncertainty(self):
    tasks, dataset, transformers, metric = self.get_dataset(
        'regression', 'GraphConv')

    batch_size = 50
    model = GraphConvModel(
        len(tasks),
        batch_size=batch_size,
        mode='regression',
        dropout=0.1,
        uncertainty=True)

    model.fit(dataset, nb_epoch=100)

    # Predict the output and uncertainty.
    pred, std = model.predict_uncertainty(dataset)
    mean_error = np.mean(np.abs(dataset.y - pred))
    mean_value = np.mean(np.abs(dataset.y))
    mean_std = np.mean(std)
    assert mean_error < 0.5 * mean_value
    assert mean_std > 0.5 * mean_error
    assert mean_std < mean_value

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_graph_models.py" startline="260" endline="285" pcid="1191">
  def test_mpnn_regression_uncertainty(self):
    tasks, dataset, transformers, metric = self.get_dataset(
        'regression', 'Weave')

    model = MPNNModel(
        len(tasks),
        mode='regression',
        n_hidden=75,
        n_atom_feat=75,
        n_pair_feat=14,
        T=1,
        M=1,
        dropout=0.1,
        uncertainty=True)

    model.fit(dataset, nb_epoch=40)

    # Predict the output and uncertainty.
    pred, std = model.predict_uncertainty(dataset)
    mean_error = np.mean(np.abs(dataset.y - pred))
    mean_value = np.mean(np.abs(dataset.y))
    mean_std = np.mean(std)
    assert mean_error < 0.5 * mean_value
    assert mean_std > 0.5 * mean_error
    assert mean_std < mean_value

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_graph_models.py" startline="195" endline="222" pcid="1188">
  def test_dag_regression_uncertainty(self):
    tasks, dataset, transformers, metric = self.get_dataset(
        'regression', 'GraphConv')

    max_atoms = max([mol.get_num_atoms() for mol in dataset.X])
    transformer = dc.trans.DAGTransformer(max_atoms=max_atoms)
    dataset = transformer.transform(dataset)

    model = DAGModel(
        len(tasks),
        max_atoms=max_atoms,
        mode='regression',
        learning_rate=0.002,
        use_queue=False,
        dropout=0.1,
        uncertainty=True)

    model.fit(dataset, nb_epoch=100)

    # Predict the output and uncertainty.
    pred, std = model.predict_uncertainty(dataset)
    mean_error = np.mean(np.abs(dataset.y - pred))
    mean_value = np.mean(np.abs(dataset.y))
    mean_std = np.mean(std)
    assert mean_error < 0.5 * mean_value
    assert mean_std > 0.5 * mean_error
    assert mean_std < mean_value

</source>
</class>

<class classid="82" nclones="2" nlines="14" similarity="92">
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_graph_models.py" startline="224" endline="240" pcid="1189">
  def test_mpnn_model(self):
    tasks, dataset, transformers, metric = self.get_dataset(
        'classification', 'Weave')

    model = MPNNModel(
        len(tasks),
        mode='classification',
        n_hidden=75,
        n_atom_feat=75,
        n_pair_feat=14,
        T=1,
        M=1)

    model.fit(dataset, nb_epoch=20)
    scores = model.evaluate(dataset, [metric], transformers)
    assert scores['mean-roc_auc_score'] >= 0.9

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/tests/test_graph_models.py" startline="242" endline="258" pcid="1190">
  def test_mpnn_regression_model(self):
    tasks, dataset, transformers, metric = self.get_dataset(
        'regression', 'Weave')

    model = MPNNModel(
        len(tasks),
        mode='regression',
        n_hidden=75,
        n_atom_feat=75,
        n_pair_feat=14,
        T=1,
        M=1)

    model.fit(dataset, nb_epoch=50)
    scores = model.evaluate(dataset, [metric], transformers)
    assert all(s < 0.1 for s in scores['mean_absolute_error'])

</source>
</class>

<class classid="83" nclones="2" nlines="12" similarity="83">
<source file="systems/deepchem-2.3.0/deepchem/models/seqtoseq.py" startline="197" endline="209" pcid="1256">
  def _create_decoder(self, n_layers, dropout):
    """Create the decoder as a tf.keras.Model."""
    input = Input(shape=(self._embedding_dimension,))
    prev_layer = layers.Stack()(self._max_output_length * [input])
    for i in range(n_layers):
      if dropout > 0.0:
        prev_layer = Dropout(dropout)(prev_layer)
      prev_layer = GRU(
          self._embedding_dimension, return_sequences=True)(prev_layer)
    output = Dense(
        len(self._output_tokens), activation=tf.nn.softmax)(prev_layer)
    return tf.keras.Model(inputs=input, outputs=output)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/seqtoseq.py" startline="525" endline="538" pcid="1271">
  def _create_decoder(self, n_layers, dropout):
    """Create the decoder as a tf.keras.Model."""
    input = Input(shape=(self._embedding_dimension,))
    prev_layer = Dense(self._embedding_dimension, activation=tf.nn.relu)(input)
    prev_layer = layers.Stack()(self._max_output_length * [prev_layer])
    for i in range(3):
      if dropout > 0.0:
        prev_layer = Dropout(dropout)(prev_layer)
      prev_layer = GRU(
          self._decoder_dimension, return_sequences=True)(prev_layer)
    output = Dense(
        len(self._output_tokens), activation=tf.nn.softmax)(prev_layer)
    return tf.keras.Model(inputs=input, outputs=output)

</source>
</class>

<class classid="84" nclones="2" nlines="42" similarity="90">
<source file="systems/deepchem-2.3.0/deepchem/models/graph_models.py" startline="120" endline="171" pcid="1287">
  def default_generator(self,
                        dataset,
                        epochs=1,
                        mode='fit',
                        deterministic=True,
                        pad_batches=True):
    for epoch in range(epochs):
      for (X_b, y_b, w_b, ids_b) in dataset.iterbatches(
          batch_size=self.batch_size,
          deterministic=deterministic,
          pad_batches=pad_batches):
        if y_b is not None:
          if self.mode == 'classification':
            y_b = to_one_hot(y_b.flatten(), self.n_classes).reshape(
                -1, self.n_tasks, self.n_classes)
        atom_feat = []
        pair_feat = []
        atom_split = []
        atom_to_pair = []
        pair_split = []
        start = 0
        for im, mol in enumerate(X_b):
          n_atoms = mol.get_num_atoms()
          # number of atoms in each molecule
          atom_split.extend([im] * n_atoms)
          # index of pair features
          C0, C1 = np.meshgrid(np.arange(n_atoms), np.arange(n_atoms))
          atom_to_pair.append(
              np.transpose(
                  np.array([C1.flatten() + start,
                            C0.flatten() + start])))
          # number of pairs for each atom
          pair_split.extend(C1.flatten() + start)
          start = start + n_atoms

          # atom features
          atom_feat.append(mol.get_atom_features())
          # pair features
          pair_feat.append(
              np.reshape(mol.get_pair_features(),
                         (n_atoms * n_atoms, self.n_pair_feat)))

        inputs = [
            np.concatenate(atom_feat, axis=0),
            np.concatenate(pair_feat, axis=0),
            np.array(pair_split),
            np.array(atom_split),
            np.concatenate(atom_to_pair, axis=0)
        ]
        yield (inputs, [y_b], [w_b])


</source>
<source file="systems/deepchem-2.3.0/deepchem/models/graph_models.py" startline="763" endline="818" pcid="1299">
  def default_generator(self,
                        dataset,
                        epochs=1,
                        mode='fit',
                        deterministic=True,
                        pad_batches=True):
    for epoch in range(epochs):
      for (X_b, y_b, w_b, ids_b) in dataset.iterbatches(
          batch_size=self.batch_size,
          deterministic=deterministic,
          pad_batches=pad_batches):

        n_samples = np.array(X_b.shape[0])
        X_b = pad_features(self.batch_size, X_b)
        if y_b is not None and self.mode == 'classification':
          y_b = to_one_hot(y_b.flatten(), self.n_classes).reshape(
              -1, self.n_tasks, self.n_classes)

        atom_feat = []
        pair_feat = []
        atom_split = []
        atom_to_pair = []
        pair_split = []
        start = 0
        for im, mol in enumerate(X_b):
          n_atoms = mol.get_num_atoms()
          # number of atoms in each molecule
          atom_split.extend([im] * n_atoms)
          # index of pair features
          C0, C1 = np.meshgrid(np.arange(n_atoms), np.arange(n_atoms))
          atom_to_pair.append(
              np.transpose(
                  np.array([C1.flatten() + start,
                            C0.flatten() + start])))
          # number of pairs for each atom
          pair_split.extend(C1.flatten() + start)
          start = start + n_atoms

          # atom features
          atom_feat.append(mol.get_atom_features())
          # pair features
          pair_feat.append(
              np.reshape(mol.get_pair_features(),
                         (n_atoms * n_atoms, self.n_pair_feat)))

        inputs = [
            np.concatenate(atom_feat, axis=0),
            np.concatenate(pair_feat, axis=0),
            np.array(atom_split),
            np.concatenate(atom_to_pair, axis=0), n_samples
        ]
        yield (inputs, [y_b], [w_b])


#################### Deprecation warnings for renamed TensorGraph models ####################

</source>
</class>

<class classid="85" nclones="2" nlines="88" similarity="87">
<source file="systems/deepchem-2.3.0/deepchem/models/robust_multitask.py" startline="22" endline="166" pcid="1321">
  def __init__(self,
               n_tasks,
               n_features,
               layer_sizes=[1000],
               weight_init_stddevs=0.02,
               bias_init_consts=1.0,
               weight_decay_penalty=0.0,
               weight_decay_penalty_type="l2",
               dropouts=0.5,
               activation_fns=tf.nn.relu,
               n_classes=2,
               bypass_layer_sizes=[100],
               bypass_weight_init_stddevs=[.02],
               bypass_bias_init_consts=[1.],
               bypass_dropouts=[.5],
               **kwargs):
    """  Create a RobustMultitaskClassifier.

    Parameters
    ----------
    n_tasks: int
      number of tasks
    n_features: int
      number of features
    layer_sizes: list
      the size of each dense layer in the network.  The length of this list determines the number of layers.
    weight_init_stddevs: list or float
      the standard deviation of the distribution to use for weight initialization of each layer.  The length
      of this list should equal len(layer_sizes).  Alternatively this may be a single value instead of a list,
      in which case the same value is used for every layer.
    bias_init_consts: list or loat
      the value to initialize the biases in each layer to.  The length of this list should equal len(layer_sizes).
      Alternatively this may be a single value instead of a list, in which case the same value is used for every layer.
    weight_decay_penalty: float
      the magnitude of the weight decay penalty to use
    weight_decay_penalty_type: str
      the type of penalty to use for weight decay, either 'l1' or 'l2'
    dropouts: list or float
      the dropout probablity to use for each layer.  The length of this list should equal len(layer_sizes).
      Alternatively this may be a single value instead of a list, in which case the same value is used for every layer.
    activation_fns: list or object
      the Tensorflow activation function to apply to each layer.  The length of this list should equal
      len(layer_sizes).  Alternatively this may be a single value instead of a list, in which case the
      same value is used for every layer.
    n_classes: int
      the number of classes
    bypass_layer_sizes: list
      the size of each dense layer in the bypass network. The length of this list determines the number of bypass layers.
    bypass_weight_init_stddevs: list or float
      the standard deviation of the distribution to use for weight initialization of bypass layers.
      same requirements as weight_init_stddevs
    bypass_bias_init_consts: list or float
      the value to initialize the biases in bypass layers
      same requirements as bias_init_consts
    bypass_dropouts: list or float
      the dropout probablity to use for bypass layers.
      same requirements as dropouts
    """
    self.n_tasks = n_tasks
    self.n_features = n_features
    self.n_classes = n_classes
    n_layers = len(layer_sizes)
    if not isinstance(weight_init_stddevs, collections.Sequence):
      weight_init_stddevs = [weight_init_stddevs] * n_layers
    if not isinstance(bias_init_consts, collections.Sequence):
      bias_init_consts = [bias_init_consts] * n_layers
    if not isinstance(dropouts, collections.Sequence):
      dropouts = [dropouts] * n_layers
    if not isinstance(activation_fns, collections.Sequence):
      activation_fns = [activation_fns] * n_layers
    if weight_decay_penalty != 0.0:
      if weight_decay_penalty_type == 'l1':
        regularizer = tf.keras.regularizers.l1(weight_decay_penalty)
      else:
        regularizer = tf.keras.regularizers.l2(weight_decay_penalty)
    else:
      regularizer = None

    n_bypass_layers = len(bypass_layer_sizes)
    if not isinstance(bypass_weight_init_stddevs, collections.Sequence):
      bypass_weight_init_stddevs = [bypass_weight_init_stddevs
                                   ] * n_bypass_layers
    if not isinstance(bypass_bias_init_consts, collections.Sequence):
      bypass_bias_init_consts = [bypass_bias_init_consts] * n_bypass_layers
    if not isinstance(bypass_dropouts, collections.Sequence):
      bypass_dropouts = [bypass_dropouts] * n_bypass_layers
    bypass_activation_fns = [activation_fns[0]] * n_bypass_layers

    # Add the input features.
    mol_features = tf.keras.Input(shape=(n_features,))
    prev_layer = mol_features

    # Add the shared dense layers
    for size, weight_stddev, bias_const, dropout, activation_fn in zip(
        layer_sizes, weight_init_stddevs, bias_init_consts, dropouts,
        activation_fns):
      layer = tf.keras.layers.Dense(
          size,
          activation=activation_fn,
          kernel_initializer=tf.truncated_normal_initializer(
              stddev=weight_stddev),
          bias_initializer=tf.constant_initializer(value=bias_const),
          kernel_regularizer=regularizer)(prev_layer)
      if dropout > 0.0:
        layer = tf.keras.layers.Dropout(rate=dropout)(layer)
      prev_layer = layer
    top_multitask_layer = prev_layer

    task_outputs = []
    for i in range(self.n_tasks):
      prev_layer = mol_features
      # Add task-specific bypass layers
      for size, weight_stddev, bias_const, dropout, activation_fn in zip(
          bypass_layer_sizes, bypass_weight_init_stddevs,
          bypass_bias_init_consts, bypass_dropouts, bypass_activation_fns):
        layer = tf.keras.layers.Dense(
            size,
            activation=activation_fn,
            kernel_initializer=tf.truncated_normal_initializer(
                stddev=weight_stddev),
            bias_initializer=tf.constant_initializer(value=bias_const),
            kernel_regularizer=regularizer)(prev_layer)
        if dropout > 0.0:
          layer = tf.keras.layers.Dropout(rate=dropout)(layer)
        prev_layer = layer
      top_bypass_layer = prev_layer

      if n_bypass_layers > 0:
        task_layer = tf.keras.layers.Concatenate(axis=1)(
            [top_multitask_layer, top_bypass_layer])
      else:
        task_layer = top_multitask_layer

      task_out = tf.keras.layers.Dense(n_classes)(task_layer)
      task_outputs.append(task_out)

    logits = Stack(axis=1)(task_outputs)
    output = tf.keras.layers.Softmax()(logits)
    model = tf.keras.Model(inputs=mol_features, outputs=[output, logits])
    super(RobustMultitaskClassifier, self).__init__(
        model,
        SoftmaxCrossEntropy(),
        output_types=['prediction', 'loss'],
        **kwargs)

</source>
<source file="systems/deepchem-2.3.0/deepchem/models/robust_multitask.py" startline="205" endline="339" pcid="1324">
  def __init__(self,
               n_tasks,
               n_features,
               layer_sizes=[1000],
               weight_init_stddevs=0.02,
               bias_init_consts=1.0,
               weight_decay_penalty=0.0,
               weight_decay_penalty_type="l2",
               dropouts=0.5,
               activation_fns=tf.nn.relu,
               bypass_layer_sizes=[100],
               bypass_weight_init_stddevs=[.02],
               bypass_bias_init_consts=[1.],
               bypass_dropouts=[.5],
               **kwargs):
    """ Create a RobustMultitaskRegressor.

    Parameters
    ----------
    n_tasks: int
      number of tasks
    n_features: int
      number of features
    layer_sizes: list
      the size of each dense layer in the network.  The length of this list determines the number of layers.
    weight_init_stddevs: list or float
      the standard deviation of the distribution to use for weight initialization of each layer.  The length
      of this list should equal len(layer_sizes).  Alternatively this may be a single value instead of a list,
      in which case the same value is used for every layer.
    bias_init_consts: list or loat
      the value to initialize the biases in each layer to.  The length of this list should equal len(layer_sizes).
      Alternatively this may be a single value instead of a list, in which case the same value is used for every layer.
    weight_decay_penalty: float
      the magnitude of the weight decay penalty to use
    weight_decay_penalty_type: str
      the type of penalty to use for weight decay, either 'l1' or 'l2'
    dropouts: list or float
      the dropout probablity to use for each layer.  The length of this list should equal len(layer_sizes).
      Alternatively this may be a single value instead of a list, in which case the same value is used for every layer.
    activation_fns: list or object
      the Tensorflow activation function to apply to each layer.  The length of this list should equal
      len(layer_sizes).  Alternatively this may be a single value instead of a list, in which case the
      same value is used for every layer.
    bypass_layer_sizes: list
      the size of each dense layer in the bypass network. The length of this list determines the number of bypass layers.
    bypass_weight_init_stddevs: list or float
      the standard deviation of the distribution to use for weight initialization of bypass layers.
      same requirements as weight_init_stddevs
    bypass_bias_init_consts: list or float
      the value to initialize the biases in bypass layers
      same requirements as bias_init_consts
    bypass_dropouts: list or float
      the dropout probablity to use for bypass layers.
      same requirements as dropouts
    """
    self.n_tasks = n_tasks
    self.n_features = n_features
    n_layers = len(layer_sizes)
    if not isinstance(weight_init_stddevs, collections.Sequence):
      weight_init_stddevs = [weight_init_stddevs] * n_layers
    if not isinstance(bias_init_consts, collections.Sequence):
      bias_init_consts = [bias_init_consts] * n_layers
    if not isinstance(dropouts, collections.Sequence):
      dropouts = [dropouts] * n_layers
    if not isinstance(activation_fns, collections.Sequence):
      activation_fns = [activation_fns] * n_layers
    if weight_decay_penalty != 0.0:
      if weight_decay_penalty_type == 'l1':
        regularizer = tf.keras.regularizers.l1(weight_decay_penalty)
      else:
        regularizer = tf.keras.regularizers.l2(weight_decay_penalty)
    else:
      regularizer = None

    n_bypass_layers = len(bypass_layer_sizes)
    if not isinstance(bypass_weight_init_stddevs, collections.Sequence):
      bypass_weight_init_stddevs = [bypass_weight_init_stddevs
                                   ] * n_bypass_layers
    if not isinstance(bypass_bias_init_consts, collections.Sequence):
      bypass_bias_init_consts = [bypass_bias_init_consts] * n_bypass_layers
    if not isinstance(bypass_dropouts, collections.Sequence):
      bypass_dropouts = [bypass_dropouts] * n_bypass_layers
    bypass_activation_fns = [activation_fns[0]] * n_bypass_layers

    # Add the input features.
    mol_features = tf.keras.Input(shape=(n_features,))
    prev_layer = mol_features

    # Add the shared dense layers
    for size, weight_stddev, bias_const, dropout, activation_fn in zip(
        layer_sizes, weight_init_stddevs, bias_init_consts, dropouts,
        activation_fns):
      layer = tf.keras.layers.Dense(
          size,
          activation=activation_fn,
          kernel_initializer=tf.truncated_normal_initializer(
              stddev=weight_stddev),
          bias_initializer=tf.constant_initializer(value=bias_const),
          kernel_regularizer=regularizer)(prev_layer)
      if dropout > 0.0:
        layer = tf.keras.layers.Dropout(rate=dropout)(layer)
      prev_layer = layer
    top_multitask_layer = prev_layer

    task_outputs = []
    for i in range(self.n_tasks):
      prev_layer = mol_features
      # Add task-specific bypass layers
      for size, weight_stddev, bias_const, dropout, activation_fn in zip(
          bypass_layer_sizes, bypass_weight_init_stddevs,
          bypass_bias_init_consts, bypass_dropouts, bypass_activation_fns):
        layer = tf.keras.layers.Dense(
            size,
            activation=activation_fn,
            kernel_initializer=tf.truncated_normal_initializer(
                stddev=weight_stddev),
            bias_initializer=tf.constant_initializer(value=bias_const),
            kernel_regularizer=regularizer)(prev_layer)
        if dropout > 0.0:
          layer = tf.keras.layers.Dropout(rate=dropout)(layer)
        prev_layer = layer
      top_bypass_layer = prev_layer

      if n_bypass_layers > 0:
        task_layer = tf.keras.layers.Concatenate(axis=1)(
            [top_multitask_layer, top_bypass_layer])
      else:
        task_layer = top_multitask_layer

      task_out = tf.keras.layers.Dense(1)(task_layer)
      task_outputs.append(task_out)

    outputs = tf.keras.layers.Concatenate(axis=1)(task_outputs)
    model = tf.keras.Model(inputs=mol_features, outputs=outputs)
    super(RobustMultitaskRegressor, self).__init__(model, L2Loss(), **kwargs)
</source>
</class>

<class classid="86" nclones="2" nlines="16" similarity="75">
<source file="systems/deepchem-2.3.0/deepchem/feat/tests/test_rdkit_grid_features.py" startline="219" endline="239" pcid="1379">
  def test_is_pi_parallel(self):
    ring1_center = np.array([0.0, 0.0, 0.0])
    ring2_center_true = np.array([4.0, 0.0, 0.0])
    ring2_center_false = np.array([10.0, 0.0, 0.0])
    ring1_normal_true = np.array([1.0, 0.0, 0.0])
    ring1_normal_false = np.array([0.0, 1.0, 0.0])

    for ring2_normal in (np.array([2.0, 0, 0]), np.array([-3.0, 0, 0])):
      # parallel normals
      self.assertTrue(
          rgf.is_pi_parallel(ring1_center, ring1_normal_true, ring2_center_true,
                             ring2_normal))
      # perpendicular normals
      self.assertFalse(
          rgf.is_pi_parallel(ring1_center, ring1_normal_false,
                             ring2_center_true, ring2_normal))
      # too far away
      self.assertFalse(
          rgf.is_pi_parallel(ring1_center, ring1_normal_true,
                             ring2_center_false, ring2_normal))

</source>
<source file="systems/deepchem-2.3.0/deepchem/feat/tests/test_rdkit_grid_features.py" startline="240" endline="260" pcid="1380">
  def test_is_pi_t(self):
    ring1_center = np.array([0.0, 0.0, 0.0])
    ring2_center_true = np.array([4.0, 0.0, 0.0])
    ring2_center_false = np.array([10.0, 0.0, 0.0])
    ring1_normal_true = np.array([0.0, 1.0, 0.0])
    ring1_normal_false = np.array([1.0, 0.0, 0.0])

    for ring2_normal in (np.array([2.0, 0, 0]), np.array([-3.0, 0, 0])):
      # perpendicular normals
      self.assertTrue(
          rgf.is_pi_t(ring1_center, ring1_normal_true, ring2_center_true,
                      ring2_normal))
      # parallel normals
      self.assertFalse(
          rgf.is_pi_t(ring1_center, ring1_normal_false, ring2_center_true,
                      ring2_normal))
      # too far away
      self.assertFalse(
          rgf.is_pi_t(ring1_center, ring1_normal_true, ring2_center_false,
                      ring2_normal))

</source>
</class>

<class classid="87" nclones="3" nlines="17" similarity="77">
<source file="systems/deepchem-2.3.0/deepchem/feat/tests/test_graph_features.py" startline="23" endline="50" pcid="1402">
  def test_carbon_nitrogen(self):
    """Test on carbon nitrogen molecule"""
    # Note there is a central nitrogen of degree 4, with 4 carbons
    # of degree 1 (connected only to central nitrogen).
    raw_smiles = ['C[N+](C)(C)C']
    import rdkit
    mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
    featurizer = ConvMolFeaturizer()
    mols = featurizer.featurize(mols)
    mol = mols[0]

    # 5 atoms in compound
    assert mol.get_num_atoms() == 5

    # Get the adjacency lists grouped by degree
    deg_adj_lists = mol.get_deg_adjacency_lists()
    assert np.array_equal(deg_adj_lists[0], np.zeros([0, 0], dtype=np.int32))
    # The 4 outer atoms connected to central nitrogen
    assert np.array_equal(deg_adj_lists[1],
                          np.array([[4], [4], [4], [4]], dtype=np.int32))
    assert np.array_equal(deg_adj_lists[2], np.zeros([0, 2], dtype=np.int32))
    assert np.array_equal(deg_adj_lists[3], np.zeros([0, 3], dtype=np.int32))
    # Central nitrogen connected to everything else.
    assert np.array_equal(deg_adj_lists[4],
                          np.array([[0, 1, 2, 3]], dtype=np.int32))
    assert np.array_equal(deg_adj_lists[5], np.zeros([0, 5], dtype=np.int32))
    assert np.array_equal(deg_adj_lists[6], np.zeros([0, 6], dtype=np.int32))

</source>
<source file="systems/deepchem-2.3.0/deepchem/feat/tests/test_graph_features.py" startline="73" endline="97" pcid="1404">
  def test_alkane(self):
    """Test on simple alkane"""
    raw_smiles = ['CCC']
    import rdkit
    mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
    featurizer = ConvMolFeaturizer()
    mol_list = featurizer.featurize(mols)
    mol = mol_list[0]

    # 3 carbonds in alkane
    assert mol.get_num_atoms() == 3

    deg_adj_lists = mol.get_deg_adjacency_lists()
    assert np.array_equal(deg_adj_lists[0], np.zeros([0, 0], dtype=np.int32))
    # Outer two carbonds are connected to central carbon
    assert np.array_equal(deg_adj_lists[1], np.array(
        [[2], [2]], dtype=np.int32))
    # Central carbon connected to outer two
    assert np.array_equal(deg_adj_lists[2], np.array([[0, 1]], dtype=np.int32))
    assert np.array_equal(deg_adj_lists[3], np.zeros([0, 3], dtype=np.int32))
    assert np.array_equal(deg_adj_lists[4], np.zeros([0, 4], dtype=np.int32))
    assert np.array_equal(deg_adj_lists[5], np.zeros([0, 5], dtype=np.int32))
    assert np.array_equal(deg_adj_lists[6], np.zeros([0, 6], dtype=np.int32))


</source>
<source file="systems/deepchem-2.3.0/deepchem/feat/tests/test_graph_features.py" startline="51" endline="72" pcid="1403">
  def test_single_carbon(self):
    """Test that single carbon atom is featurized properly."""
    raw_smiles = ['C']
    import rdkit
    mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
    featurizer = ConvMolFeaturizer()
    mol_list = featurizer.featurize(mols)
    mol = mol_list[0]

    # Only one carbon
    assert mol.get_num_atoms() == 1

    # No bonds, so degree adjacency lists are empty
    deg_adj_lists = mol.get_deg_adjacency_lists()
    assert np.array_equal(deg_adj_lists[0], np.zeros([1, 0], dtype=np.int32))
    assert np.array_equal(deg_adj_lists[1], np.zeros([0, 1], dtype=np.int32))
    assert np.array_equal(deg_adj_lists[2], np.zeros([0, 2], dtype=np.int32))
    assert np.array_equal(deg_adj_lists[3], np.zeros([0, 3], dtype=np.int32))
    assert np.array_equal(deg_adj_lists[4], np.zeros([0, 4], dtype=np.int32))
    assert np.array_equal(deg_adj_lists[5], np.zeros([0, 5], dtype=np.int32))
    assert np.array_equal(deg_adj_lists[6], np.zeros([0, 6], dtype=np.int32))

</source>
</class>

<class classid="88" nclones="2" nlines="12" similarity="83">
<source file="systems/deepchem-2.3.0/deepchem/feat/tests/test_features.py" startline="39" endline="54" pcid="1415">
  def test_convmol_hashable(self):
    featurizer1 = ConvMolFeaturizer(atom_properties=['feature'])
    featurizer2 = ConvMolFeaturizer(atom_properties=['feature'])
    featurizer3 = ConvMolFeaturizer()

    d = set()
    d.add(featurizer1)
    d.add(featurizer2)
    d.add(featurizer3)

    self.assertEqual(2, len(d))
    featurizers = [featurizer1, featurizer2, featurizer3]

    for featurizer in featurizers:
      self.assertTrue(featurizer in featurizers)

</source>
<source file="systems/deepchem-2.3.0/deepchem/feat/tests/test_features.py" startline="55" endline="69" pcid="1416">
  def test_circularfingerprint_hashable(self):
    featurizer1 = CircularFingerprint()
    featurizer2 = CircularFingerprint()
    featurizer3 = CircularFingerprint(size=5)

    d = set()
    d.add(featurizer1)
    d.add(featurizer2)
    d.add(featurizer3)

    self.assertEqual(2, len(d))
    featurizers = [featurizer1, featurizer2, featurizer3]

    for featurizer in featurizers:
      self.assertTrue(featurizer in featurizers)
</source>
</class>

<class classid="89" nclones="2" nlines="14" similarity="86">
<source file="systems/deepchem-2.3.0/deepchem/feat/coulomb_matrices.py" startline="75" endline="90" pcid="1457">
  def __init__(self,
               max_atoms,
               remove_hydrogens=False,
               randomize=False,
               upper_tri=False,
               n_samples=1,
               seed=None):
    self.max_atoms = int(max_atoms)
    self.remove_hydrogens = remove_hydrogens
    self.randomize = randomize
    self.upper_tri = upper_tri
    self.n_samples = n_samples
    if seed is not None:
      seed = int(seed)
    self.seed = seed

</source>
<source file="systems/deepchem-2.3.0/deepchem/feat/coulomb_matrices.py" startline="238" endline="251" pcid="1462">
  def __init__(self,
               max_atoms,
               remove_hydrogens=False,
               randomize=False,
               n_samples=1,
               seed=None):
    self.max_atoms = int(max_atoms)
    self.remove_hydrogens = remove_hydrogens
    self.randomize = randomize
    self.n_samples = n_samples
    if seed is not None:
      seed = int(seed)
    self.seed = seed

</source>
</class>

<class classid="90" nclones="2" nlines="42" similarity="97">
<source file="systems/deepchem-2.3.0/deepchem/rl/tests/test_ppo.py" startline="101" endline="164" pcid="1547">
  def test_recurrent_states(self):
    """Test a policy that involves recurrent layers."""

    # The environment just has a constant state.

    class TestEnvironment(dc.rl.Environment):

      def __init__(self):
        super(TestEnvironment, self).__init__((10,), 10)
        self._state = np.random.random(10)

      def step(self, action):
        self._state = np.random.random(10)
        return 0.0

      def reset(self):
        pass

    # The policy includes a single recurrent layer.

    class TestPolicy(dc.rl.Policy):

      def __init__(self):
        super(TestPolicy, self).__init__(['action_prob', 'value', 'rnn_state'],
                                         [np.zeros(10)])

      def create_model(self, **kwargs):
        state = Input(shape=(10,))
        rnn_state = Input(shape=(10,))
        reshaped = Reshape((1, 10))(state)
        gru, rnn_final_state = GRU(
            10, return_state=True, return_sequences=True)(
                reshaped, initial_state=rnn_state)
        output = Softmax()(Reshape((10,))(gru))
        value = dc.models.layers.Variable([0.0])([])
        return tf.keras.Model(
            inputs=[state, rnn_state], outputs=[output, value, rnn_final_state])

    # We don't care about actually optimizing it, so just run a few rollouts to make
    # sure fit() doesn't crash, then check the behavior of the GRU state.

    env = TestEnvironment()
    ppo = dc.rl.PPO(env, TestPolicy(), batch_size=0)
    ppo.fit(100)
    # On the first call, the initial state should be all zeros.
    prob1, value1 = ppo.predict(
        env.state, use_saved_states=True, save_states=False)
    # It should still be zeros since we didn't save it last time.
    prob2, value2 = ppo.predict(
        env.state, use_saved_states=True, save_states=True)
    # It should be different now.
    prob3, value3 = ppo.predict(
        env.state, use_saved_states=True, save_states=False)
    # This should be the same as the previous one.
    prob4, value4 = ppo.predict(
        env.state, use_saved_states=True, save_states=False)
    # Now we reset it, so we should get the same result as initially.
    prob5, value5 = ppo.predict(
        env.state, use_saved_states=False, save_states=True)
    assert np.array_equal(prob1, prob2)
    assert np.array_equal(prob1, prob5)
    assert np.array_equal(prob3, prob4)
    assert not np.array_equal(prob2, prob3)

</source>
<source file="systems/deepchem-2.3.0/deepchem/rl/tests/test_a3c.py" startline="101" endline="164" pcid="1568">
  def test_recurrent_states(self):
    """Test a policy that involves recurrent layers."""

    # The environment just has a constant state.

    class TestEnvironment(dc.rl.Environment):

      def __init__(self):
        super(TestEnvironment, self).__init__((10,), 10)
        self._state = np.random.random(10)

      def step(self, action):
        self._state = np.random.random(10)
        return 0.0

      def reset(self):
        pass

    # The policy includes a single recurrent layer.

    class TestPolicy(dc.rl.Policy):

      def __init__(self):
        super(TestPolicy, self).__init__(['action_prob', 'value', 'rnn_state'],
                                         [np.zeros(10)])

      def create_model(self, **kwargs):
        state = Input(shape=(10,))
        rnn_state = Input(shape=(10,))
        reshaped = Reshape((1, 10))(state)
        gru, rnn_final_state = GRU(
            10, return_state=True, return_sequences=True)(
                reshaped, initial_state=rnn_state)
        output = Softmax()(Reshape((10,))(gru))
        value = dc.models.layers.Variable([0.0])([])
        return tf.keras.Model(
            inputs=[state, rnn_state], outputs=[output, value, rnn_final_state])

    # We don't care about actually optimizing it, so just run a few rollouts to make
    # sure fit() doesn't crash, then check the behavior of the GRU state.

    env = TestEnvironment()
    a3c = dc.rl.A3C(env, TestPolicy())
    a3c.fit(100)
    # On the first call, the initial state should be all zeros.
    prob1, value1 = a3c.predict(
        env.state, use_saved_states=True, save_states=False)
    # It should still be zeros since we didn't save it last time.
    prob2, value2 = a3c.predict(
        env.state, use_saved_states=True, save_states=True)
    # It should be different now.
    prob3, value3 = a3c.predict(
        env.state, use_saved_states=True, save_states=False)
    # This should be the same as the previous one.
    prob4, value4 = a3c.predict(
        env.state, use_saved_states=True, save_states=False)
    # Now we reset it, so we should get the same result as initially.
    prob5, value5 = a3c.predict(
        env.state, use_saved_states=False, save_states=True)
    assert np.array_equal(prob1, prob2)
    assert np.array_equal(prob1, prob5)
    assert np.array_equal(prob3, prob4)
    assert not np.array_equal(prob2, prob3)

</source>
</class>

<class classid="91" nclones="2" nlines="63" similarity="98">
<source file="systems/deepchem-2.3.0/deepchem/rl/tests/test_ppo.py" startline="166" endline="250" pcid="1553">
  def test_hindsight(self):
    """Test Hindsight Experience Replay."""

    # The environment is a plane in which the agent moves by steps until it reaches a randomly
    # positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
    # to learn by standard methods, since it may take a very long time to receive any feedback
    # at all.  Using hindsight makes it much easier.

    class TestEnvironment(dc.rl.Environment):

      def __init__(self):
        super(TestEnvironment, self).__init__((4,), 4)
        self.moves = [(-1, 0), (1, 0), (0, -1), (0, 1)]

      def reset(self):
        self._state = np.concatenate([[0, 0], np.random.randint(-50, 50, 2)])
        self._terminated = False
        self.count = 0

      def step(self, action):
        new_state = self._state.copy()
        new_state[:2] += self.moves[action]
        self._state = new_state
        self.count += 1
        reward = 0
        if np.array_equal(new_state[:2], new_state[2:]):
          self._terminated = True
          reward = 1
        elif self.count == 1000:
          self._terminated = True
        return reward

      def apply_hindsight(self, states, actions, goal):
        new_states = []
        rewards = []
        goal_pos = goal[:2]
        for state, action in zip(states, actions):
          new_state = state.copy()
          new_state[2:] = goal_pos
          new_states.append(new_state)
          pos_after_action = new_state[:2] + self.moves[action]
          if np.array_equal(pos_after_action, goal_pos):
            rewards.append(1)
          else:
            rewards.append(0)
        return new_states, rewards

    # A simple policy with two hidden layers.

    class TestPolicy(dc.rl.Policy):

      def __init__(self):
        super(TestPolicy, self).__init__(['action_prob', 'value'])

      def create_model(self, **kwargs):
        state = Input(shape=(4,))
        dense1 = Dense(6, activation=tf.nn.relu)(state)
        dense2 = Dense(6, activation=tf.nn.relu)(dense1)
        output = Dense(4, activation=tf.nn.softmax, use_bias=False)(dense2)
        value = Dense(1)(dense2)
        return tf.keras.Model(inputs=state, outputs=[output, value])

    # Optimize it.

    env = TestEnvironment()
    learning_rate = PolynomialDecay(
        initial_rate=0.0001, final_rate=0.00005, decay_steps=1500000)
    ppo = dc.rl.PPO(
        env,
        TestPolicy(),
        use_hindsight=True,
        optimization_epochs=8,
        optimizer=Adam(learning_rate=learning_rate))
    ppo.fit(1500000)

    # Try running it a few times and see if it succeeds.

    pass_count = 0
    for i in range(5):
      env.reset()
      while not env.terminated:
        env.step(ppo.select_action(env.state))
      if np.array_equal(env.state[:2], env.state[2:]):
        pass_count += 1
    assert pass_count >= 3
</source>
<source file="systems/deepchem-2.3.0/deepchem/rl/tests/test_a3c.py" startline="166" endline="250" pcid="1574">
  def test_hindsight(self):
    """Test Hindsight Experience Replay."""

    # The environment is a plane in which the agent moves by steps until it reaches a randomly
    # positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
    # to learn by standard methods, since it may take a very long time to receive any feedback
    # at all.  Using hindsight makes it much easier.

    class TestEnvironment(dc.rl.Environment):

      def __init__(self):
        super(TestEnvironment, self).__init__((4,), 4)
        self.moves = [(-1, 0), (1, 0), (0, -1), (0, 1)]

      def reset(self):
        self._state = np.concatenate([[0, 0], np.random.randint(-50, 50, 2)])
        self._terminated = False
        self.count = 0

      def step(self, action):
        new_state = self._state.copy()
        new_state[:2] += self.moves[action]
        self._state = new_state
        self.count += 1
        reward = 0
        if np.array_equal(new_state[:2], new_state[2:]):
          self._terminated = True
          reward = 1
        elif self.count == 1000:
          self._terminated = True
        return reward

      def apply_hindsight(self, states, actions, goal):
        new_states = []
        rewards = []
        goal_pos = goal[:2]
        for state, action in zip(states, actions):
          new_state = state.copy()
          new_state[2:] = goal_pos
          new_states.append(new_state)
          pos_after_action = new_state[:2] + self.moves[action]
          if np.array_equal(pos_after_action, goal_pos):
            rewards.append(1)
          else:
            rewards.append(0)
        return new_states, rewards

    # A simple policy with two hidden layers.

    class TestPolicy(dc.rl.Policy):

      def __init__(self):
        super(TestPolicy, self).__init__(['action_prob', 'value'])

      def create_model(self, **kwargs):
        state = Input(shape=(4,))
        dense1 = Dense(6, activation=tf.nn.relu)(state)
        dense2 = Dense(6, activation=tf.nn.relu)(dense1)
        output = Dense(4, activation=tf.nn.softmax, use_bias=False)(dense2)
        value = Dense(1)(dense2)
        return tf.keras.Model(inputs=state, outputs=[output, value])

    # Optimize it.

    env = TestEnvironment()
    learning_rate = PolynomialDecay(
        initial_rate=0.0005, final_rate=0.0002, decay_steps=2000000)
    a3c = dc.rl.A3C(
        env,
        TestPolicy(),
        use_hindsight=True,
        optimizer=Adam(learning_rate=learning_rate))
    a3c.fit(2000000)

    # Try running it a few times and see if it succeeds.

    pass_count = 0
    for i in range(5):
      env.reset()
      while not env.terminated:
        env.step(a3c.select_action(env.state))
      if np.array_equal(env.state[:2], env.state[2:]):
        pass_count += 1
    assert pass_count >= 3

</source>
</class>

<class classid="92" nclones="2" nlines="46" similarity="72">
<source file="systems/deepchem-2.3.0/deepchem/rl/ppo.py" startline="90" endline="177" pcid="1604">
  def __init__(self,
               env,
               policy,
               max_rollout_length=20,
               optimization_rollouts=8,
               optimization_epochs=4,
               batch_size=64,
               clipping_width=0.2,
               discount_factor=0.99,
               advantage_lambda=0.98,
               value_weight=1.0,
               entropy_weight=0.01,
               optimizer=None,
               model_dir=None,
               use_hindsight=False):
    """Create an object for optimizing a policy.

    Parameters
    ----------
    env: Environment
      the Environment to interact with
    policy: Policy
      the Policy to optimize.  It must have outputs with the names 'action_prob'
      and 'value', corresponding to the action probabilities and value estimate
    max_rollout_length: int
      the maximum length of rollouts to generate
    optimization_rollouts: int
      the number of rollouts to generate for each iteration of optimization
    optimization_epochs: int
      the number of epochs of optimization to perform within each iteration
    batch_size: int
      the batch size to use during optimization.  If this is 0, each rollout will be used as a
      separate batch.
    clipping_width: float
      in computing the PPO loss function, the probability ratio is clipped to the range
      (1-clipping_width, 1+clipping_width)
    discount_factor: float
      the discount factor to use when computing rewards
    advantage_lambda: float
      the parameter for trading bias vs. variance in Generalized Advantage Estimation
    value_weight: float
      a scale factor for the value loss term in the loss function
    entropy_weight: float
      a scale factor for the entropy term in the loss function
    optimizer: Optimizer
      the optimizer to use.  If None, a default optimizer is used.
    model_dir: str
      the directory in which the model will be saved.  If None, a temporary directory will be created.
    use_hindsight: bool
      if True, use Hindsight Experience Replay
    """
    self._env = env
    self._policy = policy
    self.max_rollout_length = max_rollout_length
    self.optimization_rollouts = optimization_rollouts
    self.optimization_epochs = optimization_epochs
    self.batch_size = batch_size
    self.clipping_width = clipping_width
    self.discount_factor = discount_factor
    self.advantage_lambda = advantage_lambda
    self.value_weight = value_weight
    self.entropy_weight = entropy_weight
    self.use_hindsight = use_hindsight
    self._state_is_list = isinstance(env.state_shape[0], collections.Sequence)
    if optimizer is None:
      self._optimizer = Adam(learning_rate=0.001, beta1=0.9, beta2=0.999)
    else:
      self._optimizer = optimizer
    self._model = self._build_model(model_dir)
    output_names = policy.output_names
    output_tensors = self._model._output_tensors
    self._value = output_tensors[output_names.index('value')]
    self._action_prob = output_tensors[output_names.index('action_prob')]
    rnn_outputs = [i for i, n in enumerate(output_names) if n == 'rnn_state']
    self._rnn_final_states = [output_tensors[i] for i in rnn_outputs]
    self._session = tf.Session()
    self._train_op = self._model._tf_optimizer.minimize(
        self._model._loss_tensor)
    self._rnn_states = policy.rnn_initial_states
    if len(self._rnn_states) > 0 and batch_size != 0:
      raise ValueError(
          'Cannot batch rollouts when the policy contains a recurrent layer.  Set batch_size to 0.'
      )
    self._checkpoint = tf.train.Checkpoint()
    self._checkpoint.save_counter  # Ensure the variable has been created
    self._checkpoint.listed = self._model.model.trainable_variables
    self._session.run(self._checkpoint.save_counter.initializer)

</source>
<source file="systems/deepchem-2.3.0/deepchem/rl/a3c.py" startline="114" endline="182" pcid="1622">
  def __init__(self,
               env,
               policy,
               max_rollout_length=20,
               discount_factor=0.99,
               advantage_lambda=0.98,
               value_weight=1.0,
               entropy_weight=0.01,
               optimizer=None,
               model_dir=None,
               use_hindsight=False):
    """Create an object for optimizing a policy.

    Parameters
    ----------
    env: Environment
      the Environment to interact with
    policy: Policy
      the Policy to optimize.  It must have outputs with the names 'action_prob'
      and 'value' (for discrete action spaces) or 'action_mean', 'action_std',
      and 'value' (for continuous action spaces)
    max_rollout_length: int
      the maximum length of rollouts to generate
    discount_factor: float
      the discount factor to use when computing rewards
    advantage_lambda: float
      the parameter for trading bias vs. variance in Generalized Advantage Estimation
    value_weight: float
      a scale factor for the value loss term in the loss function
    entropy_weight: float
      a scale factor for the entropy term in the loss function
    optimizer: Optimizer
      the optimizer to use.  If None, a default optimizer is used.
    model_dir: str
      the directory in which the model will be saved.  If None, a temporary directory will be created.
    use_hindsight: bool
      if True, use Hindsight Experience Replay
    """
    self._env = env
    self._policy = policy
    self.max_rollout_length = max_rollout_length
    self.discount_factor = discount_factor
    self.advantage_lambda = advantage_lambda
    self.value_weight = value_weight
    self.entropy_weight = entropy_weight
    self.use_hindsight = use_hindsight
    self._state_is_list = isinstance(env.state_shape[0], collections.Sequence)
    if optimizer is None:
      self._optimizer = Adam(learning_rate=0.001, beta1=0.9, beta2=0.999)
    else:
      self._optimizer = optimizer
    self._model = self._build_model(model_dir)
    output_names = policy.output_names
    output_tensors = self._model._output_tensors
    self._value = output_tensors[output_names.index('value')]
    if self.continuous:
      self._action_mean = output_tensors[output_names.index('action_mean')]
      self._action_std = output_tensors[output_names.index('action_std')]
    else:
      self._action_prob = output_tensors[output_names.index('action_prob')]
    rnn_outputs = [i for i, n in enumerate(output_names) if n == 'rnn_state']
    self._rnn_final_states = [output_tensors[i] for i in rnn_outputs]
    self._session = self._model.session
    self._rnn_states = policy.rnn_initial_states
    self._checkpoint = tf.train.Checkpoint()
    self._checkpoint.save_counter  # Ensure the variable has been created
    self._checkpoint.listed = self._model.model.trainable_variables
    self._session.run(self._checkpoint.save_counter.initializer)

</source>
</class>

<class classid="93" nclones="2" nlines="36" similarity="70">
<source file="systems/deepchem-2.3.0/deepchem/rl/ppo.py" startline="178" endline="210" pcid="1605">
  def _build_model(self, model_dir):
    """Construct a KerasModel containing the policy and loss calculations."""
    state_shape = self._env.state_shape
    state_dtype = self._env.state_dtype
    if not self._state_is_list:
      state_shape = [state_shape]
      state_dtype = [state_dtype]
    features = []
    for s, d in zip(state_shape, state_dtype):
      features.append(
          tf.keras.layers.Input(shape=list(s), dtype=tf.as_dtype(d)))
    policy_model = self._policy.create_model()
    output_names = self._policy.output_names
    loss = PPOLoss(self.value_weight, self.entropy_weight, self.clipping_width,
                   output_names.index('action_prob'),
                   output_names.index('value'))
    model = KerasModel(
        policy_model,
        loss,
        batch_size=self.max_rollout_length,
        model_dir=model_dir,
        optimize=self._optimizer)
    env = self._env
    example_inputs = [
        np.zeros([model.batch_size] + list(shape), dtype)
        for shape, dtype in zip(state_shape, state_dtype)
    ]
    example_labels = [np.zeros((model.batch_size, env.n_actions))]
    example_weights = [np.zeros(model.batch_size)] * 3
    model._create_training_ops((example_inputs, example_labels,
                                example_weights))
    return model

</source>
<source file="systems/deepchem-2.3.0/deepchem/rl/a3c.py" startline="183" endline="226" pcid="1623">
  def _build_model(self, model_dir):
    """Construct a KerasModel containing the policy and loss calculations."""
    state_shape = self._env.state_shape
    state_dtype = self._env.state_dtype
    if not self._state_is_list:
      state_shape = [state_shape]
      state_dtype = [state_dtype]
    features = []
    for s, d in zip(state_shape, state_dtype):
      features.append(
          tf.keras.layers.Input(shape=list(s), dtype=tf.as_dtype(d)))
    policy_model = self._policy.create_model()
    output_names = self._policy.output_names
    if 'action_prob' in output_names:
      self.continuous = False
      loss = A3CLossDiscrete(self.value_weight, self.entropy_weight,
                             output_names.index('action_prob'),
                             output_names.index('value'))
    else:
      self.continuous = True
      loss = A3CLossContinuous(self.value_weight, self.entropy_weight,
                               output_names.index('action_mean'),
                               output_names.index('action_std'),
                               output_names.index('value'))
    model = KerasModel(
        policy_model,
        loss,
        batch_size=self.max_rollout_length,
        model_dir=model_dir,
        optimize=self._optimizer)
    env = self._env
    example_inputs = [
        np.zeros([model.batch_size] + list(shape), dtype)
        for shape, dtype in zip(state_shape, state_dtype)
    ]
    if self.continuous:
      example_labels = [np.zeros([model.batch_size] + list(env.action_shape))]
    else:
      example_labels = [np.zeros((model.batch_size, env.n_actions))]
    example_weights = [np.zeros(model.batch_size)] * 2
    model._create_training_ops((example_inputs, example_labels,
                                example_weights))
    return model

</source>
</class>

<class classid="94" nclones="2" nlines="18" similarity="77">
<source file="systems/deepchem-2.3.0/deepchem/data/data_loader.py" startline="55" endline="84" pcid="1652">
def featurize_smiles_df(df, featurizer, field, log_every_N=1000, verbose=True):
  """Featurize individual compounds in dataframe.

  Given a featurizer that operates on individual chemical compounds
  or macromolecules, compute & add features for that compound to the
  features dataframe
  """
  sample_elems = df[field].tolist()

  features = []
  from rdkit import Chem
  from rdkit.Chem import rdmolfiles
  from rdkit.Chem import rdmolops
  for ind, elem in enumerate(sample_elems):
    mol = Chem.MolFromSmiles(elem)
    # TODO (ytz) this is a bandage solution to reorder the atoms so
    # that they're always in the same canonical order. Presumably this
    # should be correctly implemented in the future for graph mols.
    if mol:
      new_order = rdmolfiles.CanonicalRankAtoms(mol)
      mol = rdmolops.RenumberAtoms(mol, new_order)
    if ind % log_every_N == 0:
      log("Featurizing sample %d" % ind, verbose)
    features.append(featurizer.featurize([mol]))
  valid_inds = np.array(
      [1 if elt.size > 0 else 0 for elt in features], dtype=bool)
  features = [elt for (is_valid, elt) in zip(valid_inds, features) if is_valid]
  return np.squeeze(np.array(features), axis=1), valid_inds


</source>
<source file="systems/deepchem-2.3.0/deepchem/data/data_loader.py" startline="85" endline="111" pcid="1653">
def featurize_smiles_np(arr, featurizer, log_every_N=1000, verbose=True):
  """Featurize individual compounds in a numpy array.

  Given a featurizer that operates on individual chemical compounds
  or macromolecules, compute & add features for that compound to the
  features array
  """
  features = []
  from rdkit import Chem
  from rdkit.Chem import rdmolfiles
  from rdkit.Chem import rdmolops
  for ind, elem in enumerate(arr.tolist()):
    mol = Chem.MolFromSmiles(elem)
    if mol:
      new_order = rdmolfiles.CanonicalRankAtoms(mol)
      mol = rdmolops.RenumberAtoms(mol, new_order)
    if ind % log_every_N == 0:
      log("Featurizing sample %d" % ind, verbose)
    features.append(featurizer.featurize([mol]))

  valid_inds = np.array(
      [1 if elt.size > 0 else 0 for elt in features], dtype=bool)
  features = [elt for (is_valid, elt) in zip(valid_inds, features) if is_valid]
  features = np.squeeze(np.array(features))
  return features.reshape(-1,)


</source>
</class>

<class classid="95" nclones="2" nlines="21" similarity="100">
<source file="systems/deepchem-2.3.0/deepchem/data/tests/test_shuffle.py" startline="85" endline="112" pcid="1691">
  def test_shuffle_each_shard(self):
    """Test that shuffle_each_shard works."""
    n_samples = 100
    n_tasks = 10
    n_features = 10

    X = np.random.rand(n_samples, n_features)
    y = np.random.randint(2, size=(n_samples, n_tasks))
    w = np.random.randint(2, size=(n_samples, n_tasks))
    ids = np.arange(n_samples)
    dataset = dc.data.DiskDataset.from_numpy(X, y, w, ids)
    dataset.reshard(shard_size=10)

    dataset.shuffle_each_shard()
    X_s, y_s, w_s, ids_s = (dataset.X, dataset.y, dataset.w, dataset.ids)
    assert X_s.shape == X.shape
    assert y_s.shape == y.shape
    assert ids_s.shape == ids.shape
    assert w_s.shape == w.shape

    # The ids should now store the performed permutation. Check that the
    # original dataset is recoverable.
    for i in range(n_samples):
      np.testing.assert_array_equal(X_s[i], X[ids_s[i]])
      np.testing.assert_array_equal(y_s[i], y[ids_s[i]])
      np.testing.assert_array_equal(w_s[i], w[ids_s[i]])
      np.testing.assert_array_equal(ids_s[i], ids[ids_s[i]])

</source>
<source file="systems/deepchem-2.3.0/deepchem/data/tests/test_shuffle.py" startline="113" endline="140" pcid="1692">
  def test_shuffle_shards(self):
    """Test that shuffle_shards works."""
    n_samples = 100
    n_tasks = 10
    n_features = 10

    X = np.random.rand(n_samples, n_features)
    y = np.random.randint(2, size=(n_samples, n_tasks))
    w = np.random.randint(2, size=(n_samples, n_tasks))
    ids = np.arange(n_samples)
    dataset = dc.data.DiskDataset.from_numpy(X, y, w, ids)
    dataset.reshard(shard_size=10)
    dataset.shuffle_shards()

    X_s, y_s, w_s, ids_s = (dataset.X, dataset.y, dataset.w, dataset.ids)

    assert X_s.shape == X.shape
    assert y_s.shape == y.shape
    assert ids_s.shape == ids.shape
    assert w_s.shape == w.shape

    # The ids should now store the performed permutation. Check that the
    # original dataset is recoverable.
    for i in range(n_samples):
      np.testing.assert_array_equal(X_s[i], X[ids_s[i]])
      np.testing.assert_array_equal(y_s[i], y[ids_s[i]])
      np.testing.assert_array_equal(w_s[i], w[ids_s[i]])
      np.testing.assert_array_equal(ids_s[i], ids[ids_s[i]])
</source>
</class>

<class classid="96" nclones="3" nlines="11" similarity="100">
<source file="systems/deepchem-2.3.0/deepchem/data/tests/test_datasets.py" startline="248" endline="261" pcid="1702">
    def shard_generator():
      for sz in shard_sizes:
        X_b = np.random.rand(sz, 1)
        y_b = np.random.rand(sz, 1)
        w_b = np.random.rand(sz, 1)
        ids_b = np.random.rand(sz)

        all_Xs.append(X_b)
        all_ys.append(y_b)
        all_ws.append(w_b)
        all_ids.append(ids_b)

        yield X_b, y_b, w_b, ids_b

</source>
<source file="systems/deepchem-2.3.0/deepchem/data/tests/test_datasets.py" startline="562" endline="575" pcid="1720">

      def shard_generator():
        for sz in shard_sizes:
          X_b = np.random.rand(sz, 1)
          y_b = np.random.rand(sz, 1)
          w_b = np.random.rand(sz, 1)
          ids_b = np.random.rand(sz)

          all_Xs.append(X_b)
          all_ys.append(y_b)
          all_ws.append(w_b)
          all_ids.append(ids_b)

          yield X_b, y_b, w_b, ids_b
</source>
<source file="systems/deepchem-2.3.0/deepchem/data/tests/test_datasets.py" startline="444" endline="457" pcid="1716">

    def shard_generator():
      for sz in shard_sizes:
        X_b = np.random.rand(sz, 1)
        y_b = np.random.rand(sz, 1)
        w_b = np.random.rand(sz, 1)
        ids_b = np.random.rand(sz)

        all_Xs.append(X_b)
        all_ys.append(y_b)
        all_ws.append(w_b)
        all_ids.append(ids_b)

        yield X_b, y_b, w_b, ids_b
</source>
</class>

<class classid="97" nclones="2" nlines="15" similarity="93">
<source file="systems/deepchem-2.3.0/deepchem/data/tests/test_reload.py" startline="71" endline="88" pcid="1725">

  def test_reload_after_gen(self):
    """Check num samples for loaded and reloaded datasets is equal."""
    reload = False
    current_dir = os.path.dirname(os.path.abspath(__file__))
    dataset_file = os.path.join(current_dir,
                                "../../../datasets/mini_muv.csv.gz")
    logger.info("Running experiment for first time without reload.")
    (len_train, len_valid, len_test) = self._run_muv_experiment(
        dataset_file, reload)

    logger.info("Running experiment for second time with reload.")
    reload = True
    (len_reload_train, len_reload_valid,
     len_reload_test) = (self._run_muv_experiment(dataset_file, reload))
    assert len_train == len_reload_train
    assert len_valid == len_reload_valid
    assert len_test == len_reload_valid
</source>
<source file="systems/deepchem-2.3.0/deepchem/data/tests/test_reload.py" startline="89" endline="104" pcid="1726">

  def test_reload_twice(self):
    """Check ability to repeatedly run experiments with reload set True."""
    reload = True
    current_dir = os.path.dirname(os.path.abspath(__file__))
    dataset_file = os.path.join(current_dir,
                                "../../../datasets/mini_muv.csv.gz")
    logger.info("Running experiment for first time with reload.")
    (len_train, len_valid, len_test) = self._run_muv_experiment(
        dataset_file, reload)

    logger.info("Running experiment for second time with reload.")
    (len_reload_train, len_reload_valid,
     len_reload_test) = (self._run_muv_experiment(dataset_file, reload))
    assert len_train == len_reload_train
    assert len_valid == len_reload_valid
</source>
</class>

<class classid="98" nclones="3" nlines="20" similarity="80">
<source file="systems/deepchem-2.3.0/deepchem/data/tests/test_support_generator.py" startline="46" endline="71" pcid="1729">
  def test_get_task_support_simple(self):
    """Tests that get_task_support samples correctly."""
    n_samples = 20
    n_features = 3
    n_tasks = 1
    n_trials = 10

    # Generate dummy dataset
    np.random.seed(123)
    ids = np.arange(n_samples)
    X = np.random.rand(n_samples, n_features)
    y = np.random.randint(2, size=(n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))
    dataset = dc.data.NumpyDataset(X, y, w, ids)

    n_episodes = 20
    n_pos = 1
    n_neg = 5
    supports = dc.data.get_task_support(
        dataset, n_episodes, n_pos, n_neg, task=0, log_every_n=10)
    assert len(supports) == n_episodes

    for support in supports:
      assert len(support) == n_pos + n_neg
      assert np.count_nonzero(support.y) == n_pos

</source>
<source file="systems/deepchem-2.3.0/deepchem/data/tests/test_support_generator.py" startline="103" endline="126" pcid="1731">
  def test_get_task_test(self):
    """Tests that get_task_testsamples correctly."""
    n_samples = 20
    n_features = 3
    n_tasks = 1
    n_trials = 10

    # Generate dummy dataset
    np.random.seed(123)
    ids = np.arange(n_samples)
    X = np.random.rand(n_samples, n_features)
    y = np.random.randint(2, size=(n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))
    dataset = dc.data.NumpyDataset(X, y, w, ids)

    n_episodes = 20
    n_test = 10
    tests = dc.data.get_task_test(
        dataset, n_episodes, n_test, task=0, log_every_n=10)

    assert len(tests) == n_episodes
    for test in tests:
      assert len(test) == n_test

</source>
<source file="systems/deepchem-2.3.0/deepchem/data/tests/test_support_generator.py" startline="72" endline="102" pcid="1730">
  def test_get_task_support_missing(self):
    """Test that task support works in presence of missing data."""
    n_samples = 20
    n_features = 3
    n_tasks = 1
    n_trials = 10

    # Generate dummy dataset
    np.random.seed(123)
    ids = np.arange(n_samples)
    X = np.random.rand(n_samples, n_features)
    y = np.random.randint(2, size=(n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))
    # Set last n_samples/2 weights to 0
    w[n_samples // 2:] = 0
    dataset = dc.data.NumpyDataset(X, y, w, ids)

    n_episodes = 20
    n_pos = 1
    n_neg = 2
    supports = dc.data.get_task_support(
        dataset, n_episodes, n_pos, n_neg, task=0, log_every_n=10)
    assert len(supports) == n_episodes

    for support in supports:
      assert len(support) == n_pos + n_neg
      assert np.count_nonzero(support.y) == n_pos
      # Check that no support elements are sample from zero-weight samples
      for identifier in support.ids:
        assert identifier < n_samples / 2

</source>
</class>

<class classid="99" nclones="2" nlines="20" similarity="90">
<source file="systems/deepchem-2.3.0/deepchem/data/tests/test_support_generator.py" startline="179" endline="206" pcid="1734">
  def test_get_task_minus_support_simple(self):
    """Test that fixed index support can be removed from dataset."""
    n_samples = 20
    n_support = 5
    n_features = 3
    n_tasks = 1

    # Generate dummy dataset
    np.random.seed(123)
    ids = np.arange(n_samples)
    X = np.random.rand(n_samples, n_features)
    y = np.random.randint(2, size=(n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))
    dataset = dc.data.NumpyDataset(X, y, w, ids)

    support_dataset = dc.data.NumpyDataset(X[:n_support], y[:n_support],
                                           w[:n_support], ids[:n_support])

    task_dataset = dc.data.get_task_dataset_minus_support(
        dataset, support_dataset, task=0)

    # Assert all support elements have been removed
    assert len(task_dataset) == n_samples - n_support
    np.testing.assert_array_equal(task_dataset.X, X[n_support:])
    np.testing.assert_array_equal(task_dataset.y, y[n_support:])
    np.testing.assert_array_equal(task_dataset.w, w[n_support:])
    np.testing.assert_array_equal(task_dataset.ids, ids[n_support:])

</source>
<source file="systems/deepchem-2.3.0/deepchem/data/tests/test_support_generator.py" startline="207" endline="233" pcid="1735">
  def test_dataset_difference_simple(self):
    """Test that fixed index can be removed from dataset."""
    n_samples = 20
    n_remove = 5
    n_features = 3
    n_tasks = 1

    # Generate dummy dataset
    np.random.seed(123)
    ids = np.arange(n_samples)
    X = np.random.rand(n_samples, n_features)
    y = np.random.randint(2, size=(n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))
    dataset = dc.data.NumpyDataset(X, y, w, ids)

    remove_dataset = dc.data.NumpyDataset(X[:n_remove], y[:n_remove],
                                          w[:n_remove], ids[:n_remove])

    out_dataset = dc.data.dataset_difference(dataset, remove_dataset)

    # Assert all remove elements have been removed
    assert len(out_dataset) == n_samples - n_remove
    np.testing.assert_array_equal(out_dataset.X, X[n_remove:])
    np.testing.assert_array_equal(out_dataset.y, y[n_remove:])
    np.testing.assert_array_equal(out_dataset.w, w[n_remove:])
    np.testing.assert_array_equal(out_dataset.ids, ids[n_remove:])

</source>
</class>

<class classid="100" nclones="2" nlines="23" similarity="91">
<source file="systems/deepchem-2.3.0/deepchem/data/tests/test_support_generator.py" startline="234" endline="264" pcid="1736">
  def test_get_task_minus_support(self):
    """Test that random index support can be removed from dataset."""
    n_samples = 10
    n_support = 4
    n_features = 3
    n_tasks = 1

    # Generate dummy dataset
    np.random.seed(123)
    ids = np.arange(n_samples)
    X = np.random.rand(n_samples, n_features)
    y = np.random.randint(2, size=(n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))
    dataset = dc.data.NumpyDataset(X, y, w, ids)

    support_inds = sorted(
        np.random.choice(np.arange(n_samples), (n_support,), replace=False))
    support_dataset = dc.data.NumpyDataset(X[support_inds], y[support_inds],
                                           w[support_inds], ids[support_inds])

    task_dataset = dc.data.get_task_dataset_minus_support(
        dataset, support_dataset, task=0)

    # Assert all support elements have been removed
    data_inds = sorted(list(set(range(n_samples)) - set(support_inds)))
    assert len(task_dataset) == n_samples - n_support
    np.testing.assert_array_equal(task_dataset.X, X[data_inds])
    np.testing.assert_array_equal(task_dataset.y, y[data_inds])
    np.testing.assert_array_equal(task_dataset.w, w[data_inds])
    np.testing.assert_array_equal(task_dataset.ids, ids[data_inds])

</source>
<source file="systems/deepchem-2.3.0/deepchem/data/tests/test_support_generator.py" startline="265" endline="294" pcid="1737">
  def test_dataset_difference(self):
    """Test that random index can be removed from dataset."""
    n_samples = 10
    n_remove = 4
    n_features = 3
    n_tasks = 1

    # Generate dummy dataset
    np.random.seed(123)
    ids = np.arange(n_samples)
    X = np.random.rand(n_samples, n_features)
    y = np.random.randint(2, size=(n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))
    dataset = dc.data.NumpyDataset(X, y, w, ids)

    remove_inds = sorted(
        np.random.choice(np.arange(n_samples), (n_remove,), replace=False))
    remove_dataset = dc.data.NumpyDataset(X[remove_inds], y[remove_inds],
                                          w[remove_inds], ids[remove_inds])

    out_dataset = dc.data.dataset_difference(dataset, remove_dataset)

    # Assert all remove elements have been removed
    data_inds = sorted(list(set(range(n_samples)) - set(remove_inds)))
    assert len(out_dataset) == n_samples - n_remove
    np.testing.assert_array_equal(out_dataset.X, X[data_inds])
    np.testing.assert_array_equal(out_dataset.y, y[data_inds])
    np.testing.assert_array_equal(out_dataset.w, w[data_inds])
    np.testing.assert_array_equal(out_dataset.ids, ids[data_inds])

</source>
</class>

<class classid="101" nclones="3" nlines="11" similarity="72">
<source file="systems/deepchem-2.3.0/deepchem/data/tests/__init__.py" startline="47" endline="62" pcid="1745">
def load_multitask_data():
  """Load example multitask data."""
  current_dir = os.path.dirname(os.path.abspath(__file__))
  featurizer = dc.feat.CircularFingerprint(size=1024)
  tasks = [
      "task0", "task1", "task2", "task3", "task4", "task5", "task6", "task7",
      "task8", "task9", "task10", "task11", "task12", "task13", "task14",
      "task15", "task16"
  ]
  input_file = os.path.join(current_dir,
                            "../../models/tests/multitask_example.csv")
  loader = dc.data.CSVLoader(
      tasks=tasks, smiles_field="smiles", featurizer=featurizer)
  return loader.featurize(input_file)


</source>
<source file="systems/deepchem-2.3.0/deepchem/data/tests/__init__.py" startline="76" endline="90" pcid="1747">
def load_sparse_multitask_dataset():
  """Load sparse tox multitask data, sample dataset."""
  current_dir = os.path.dirname(os.path.abspath(__file__))
  featurizer = dc.feat.CircularFingerprint(size=1024)
  tasks = [
      "task1", "task2", "task3", "task4", "task5", "task6", "task7", "task8",
      "task9"
  ]
  input_file = os.path.join(current_dir,
                            "../../models/tests/sparse_multitask_example.csv")
  loader = dc.data.CSVLoader(
      tasks=tasks, smiles_field="smiles", featurizer=featurizer)
  return loader.featurize(input_file)


</source>
<source file="systems/deepchem-2.3.0/deepchem/data/tests/__init__.py" startline="63" endline="75" pcid="1746">
def load_classification_data():
  """Loads classification data from example.csv"""
  current_dir = os.path.dirname(os.path.abspath(__file__))
  featurizer = dc.feat.CircularFingerprint(size=1024)
  tasks = ["outcome"]
  task_type = "classification"
  input_file = os.path.join(current_dir,
                            "../../models/tests/example_classification.csv")
  loader = dc.data.CSVLoader(
      tasks=tasks, smiles_field="smiles", featurizer=featurizer)
  return loader.featurize(input_file)


</source>
</class>

<class classid="102" nclones="2" nlines="10" similarity="80">
<source file="systems/deepchem-2.3.0/deepchem/data/tests/__init__.py" startline="91" endline="103" pcid="1748">
def load_feat_multitask_data():
  """Load example with numerical features, tasks."""
  current_dir = os.path.dirname(os.path.abspath(__file__))
  features = ["feat0", "feat1", "feat2", "feat3", "feat4", "feat5"]
  featurizer = dc.feat.UserDefinedFeaturizer(features)
  tasks = ["task0", "task1", "task2", "task3", "task4", "task5"]
  input_file = os.path.join(current_dir,
                            "../../models/tests/feat_multitask_example.csv")
  loader = dc.data.UserCSVLoader(
      tasks=tasks, featurizer=featurizer, id_field="id")
  return loader.featurize(input_file)


</source>
<source file="systems/deepchem-2.3.0/deepchem/data/tests/__init__.py" startline="104" endline="118" pcid="1749">
def load_gaussian_cdf_data():
  """Load example with numbers sampled from Gaussian normal distribution.
     Each feature and task is a column of values that is sampled
     from a normal distribution of mean 0, stdev 1."""
  current_dir = os.path.dirname(os.path.abspath(__file__))
  features = ["feat0", "feat1"]
  featurizer = dc.feat.UserDefinedFeaturizer(features)
  tasks = ["task0", "task1"]
  input_file = os.path.join(current_dir,
                            "../../models/tests/gaussian_cdf_example.csv")
  loader = dc.data.UserCSVLoader(
      tasks=tasks, featurizer=featurizer, id_field="id")
  return loader.featurize(input_file)


</source>
</class>

<class classid="103" nclones="4" nlines="15" similarity="87">
<source file="systems/deepchem-2.3.0/deepchem/trans/tests/test_transformers.py" startline="46" endline="69" pcid="1918">
  def test_y_log_transformer(self):
    """Tests logarithmic data transformer."""
    solubility_dataset = dc.data.tests.load_solubility_data()
    log_transformer = dc.trans.LogTransformer(
        transform_y=True, dataset=solubility_dataset)
    X, y, w, ids = (solubility_dataset.X, solubility_dataset.y,
                    solubility_dataset.w, solubility_dataset.ids)
    solubility_dataset = log_transformer.transform(solubility_dataset)
    X_t, y_t, w_t, ids_t = (solubility_dataset.X, solubility_dataset.y,
                            solubility_dataset.w, solubility_dataset.ids)

    # Check ids are unchanged.
    for id_elt, id_t_elt in zip(ids, ids_t):
      assert id_elt == id_t_elt
    # Check X is unchanged since this is a y transformer
    np.testing.assert_allclose(X, X_t)
    # Check w is unchanged since this is a y transformer
    np.testing.assert_allclose(w, w_t)
    # Check y is now a logarithmic version of itself
    np.testing.assert_allclose(y_t, np.log(y + 1))

    # Check that untransform does the right thing.
    np.testing.assert_allclose(log_transformer.untransform(y_t), y)

</source>
<source file="systems/deepchem-2.3.0/deepchem/trans/tests/test_transformers.py" startline="237" endline="262" pcid="1924">
  def test_X_minmax_transformer(self):
    solubility_dataset = dc.data.tests.load_solubility_data()
    minmax_transformer = dc.trans.MinMaxTransformer(
        transform_X=True, dataset=solubility_dataset)
    X, y, w, ids = (solubility_dataset.X, solubility_dataset.y,
                    solubility_dataset.w, solubility_dataset.ids)
    solubility_dataset = minmax_transformer.transform(solubility_dataset)
    X_t, y_t, w_t, ids_t = (solubility_dataset.X, solubility_dataset.y,
                            solubility_dataset.w, solubility_dataset.ids)

    # Check ids are unchanged before and after transformation
    for id_elt, id_t_elt in zip(ids, ids_t):
      assert id_elt == id_t_elt

    # Check X is unchanged since transform_y is true
    np.testing.assert_allclose(y, y_t)
    # Check w is unchanged since transform_y is true
    np.testing.assert_allclose(w, w_t)

    # Check minimum and maximum values of transformed y are 0 and 1
    np.testing.assert_allclose(X_t.min(), 0.)
    np.testing.assert_allclose(X_t.max(), 1.)

    # Check untransform works correctly
    np.testing.assert_allclose(minmax_transformer.untransform(X_t), X)

</source>
<source file="systems/deepchem-2.3.0/deepchem/trans/tests/test_transformers.py" startline="263" endline="286" pcid="1925">
  def test_y_normalization_transformer(self):
    """Tests normalization transformer."""
    solubility_dataset = dc.data.tests.load_solubility_data()
    normalization_transformer = dc.trans.NormalizationTransformer(
        transform_y=True, dataset=solubility_dataset)
    X, y, w, ids = (solubility_dataset.X, solubility_dataset.y,
                    solubility_dataset.w, solubility_dataset.ids)
    solubility_dataset = normalization_transformer.transform(solubility_dataset)
    X_t, y_t, w_t, ids_t = (solubility_dataset.X, solubility_dataset.y,
                            solubility_dataset.w, solubility_dataset.ids)
    # Check ids are unchanged.
    for id_elt, id_t_elt in zip(ids, ids_t):
      assert id_elt == id_t_elt
    # Check X is unchanged since this is a y transformer
    np.testing.assert_allclose(X, X_t)
    # Check w is unchanged since this is a y transformer
    np.testing.assert_allclose(w, w_t)
    # Check that y_t has zero mean, unit std.
    assert np.isclose(y_t.mean(), 0.)
    assert np.isclose(y_t.std(), 1.)

    # Check that untransform does the right thing.
    np.testing.assert_allclose(normalization_transformer.untransform(y_t), y)

</source>
<source file="systems/deepchem-2.3.0/deepchem/trans/tests/test_transformers.py" startline="84" endline="107" pcid="1920">
  def test_X_log_transformer(self):
    """Tests logarithmic data transformer."""
    solubility_dataset = dc.data.tests.load_solubility_data()
    log_transformer = dc.trans.LogTransformer(
        transform_X=True, dataset=solubility_dataset)
    X, y, w, ids = (solubility_dataset.X, solubility_dataset.y,
                    solubility_dataset.w, solubility_dataset.ids)
    solubility_dataset = log_transformer.transform(solubility_dataset)
    X_t, y_t, w_t, ids_t = (solubility_dataset.X, solubility_dataset.y,
                            solubility_dataset.w, solubility_dataset.ids)

    # Check ids are unchanged.
    for id_elt, id_t_elt in zip(ids, ids_t):
      assert id_elt == id_t_elt
    # Check y is unchanged since this is a X transformer
    np.testing.assert_allclose(y, y_t)
    # Check w is unchanged since this is a y transformer
    np.testing.assert_allclose(w, w_t)
    # Check y is now a logarithmic version of itself
    np.testing.assert_allclose(X_t, np.log(X + 1))

    # Check that untransform does the right thing.
    np.testing.assert_allclose(log_transformer.untransform(X_t), X)

</source>
</class>

<class classid="104" nclones="2" nlines="25" similarity="96">
<source file="systems/deepchem-2.3.0/deepchem/trans/tests/test_transformers.py" startline="108" endline="141" pcid="1921">
  def test_y_log_transformer_select(self):
    """Tests logarithmic data transformer with selection."""
    multitask_dataset = dc.data.tests.load_feat_multitask_data()
    dfe = pd.read_csv(
        os.path.join(self.current_dir,
                     "../../models/tests/feat_multitask_example.csv"))
    tid = []
    tasklist = ["task0", "task3", "task4", "task5"]
    first_task = "task0"
    for task in tasklist:
      tiid = dfe.columns.get_loc(task) - dfe.columns.get_loc(first_task)
      tid = np.concatenate((tid, np.array([tiid])))
    tasks = tid.astype(int)
    log_transformer = dc.trans.LogTransformer(
        transform_y=True, tasks=tasks, dataset=multitask_dataset)
    X, y, w, ids = (multitask_dataset.X, multitask_dataset.y,
                    multitask_dataset.w, multitask_dataset.ids)
    multitask_dataset = log_transformer.transform(multitask_dataset)
    X_t, y_t, w_t, ids_t = (multitask_dataset.X, multitask_dataset.y,
                            multitask_dataset.w, multitask_dataset.ids)

    # Check ids are unchanged.
    for id_elt, id_t_elt in zip(ids, ids_t):
      assert id_elt == id_t_elt
    # Check X is unchanged since this is a y transformer
    np.testing.assert_allclose(X, X_t)
    # Check w is unchanged since this is a y transformer
    np.testing.assert_allclose(w, w_t)
    # Check y is now a logarithmic version of itself
    np.testing.assert_allclose(y_t[:, tasks], np.log(y[:, tasks] + 1))

    # Check that untransform does the right thing.
    np.testing.assert_allclose(log_transformer.untransform(y_t), y)

</source>
<source file="systems/deepchem-2.3.0/deepchem/trans/tests/test_transformers.py" startline="142" endline="175" pcid="1922">
  def test_X_log_transformer_select(self):
    # Tests logarithmic data transformer with selection.
    multitask_dataset = dc.data.tests.load_feat_multitask_data()
    dfe = pd.read_csv(
        os.path.join(self.current_dir,
                     "../../models/tests/feat_multitask_example.csv"))
    fid = []
    featurelist = ["feat0", "feat1", "feat2", "feat3", "feat5"]
    first_feature = "feat0"
    for feature in featurelist:
      fiid = dfe.columns.get_loc(feature) - dfe.columns.get_loc(first_feature)
      fid = np.concatenate((fid, np.array([fiid])))
    features = fid.astype(int)
    log_transformer = dc.trans.LogTransformer(
        transform_X=True, features=features, dataset=multitask_dataset)
    X, y, w, ids = (multitask_dataset.X, multitask_dataset.y,
                    multitask_dataset.w, multitask_dataset.ids)
    multitask_dataset = log_transformer.transform(multitask_dataset)
    X_t, y_t, w_t, ids_t = (multitask_dataset.X, multitask_dataset.y,
                            multitask_dataset.w, multitask_dataset.ids)

    # Check ids are unchanged.
    for id_elt, id_t_elt in zip(ids, ids_t):
      assert id_elt == id_t_elt
    # Check y is unchanged since this is a X transformer
    np.testing.assert_allclose(y, y_t)
    # Check w is unchanged since this is a y transformer
    np.testing.assert_allclose(w, w_t)
    # Check y is now a logarithmic version of itself
    np.testing.assert_allclose(X_t[:, features], np.log(X[:, features] + 1))

    # Check that untransform does the right thing.
    np.testing.assert_allclose(log_transformer.untransform(X_t), X)

</source>
</class>

<class classid="105" nclones="2" nlines="19" similarity="94">
<source file="systems/deepchem-2.3.0/deepchem/trans/tests/test_transformers.py" startline="320" endline="344" pcid="1927">
  def test_cdf_X_transformer(self):
    """Test CDF transformer on Gaussian normal dataset."""
    target = np.array(np.transpose(np.linspace(0., 1., 1001)))
    target = np.transpose(np.array(np.append([target], [target], axis=0)))
    gaussian_dataset = dc.data.tests.load_gaussian_cdf_data()
    bins = 1001
    cdf_transformer = dc.trans.CDFTransformer(
        transform_X=True, dataset=gaussian_dataset, bins=bins)
    X, y, w, ids = (gaussian_dataset.X, gaussian_dataset.y, gaussian_dataset.w,
                    gaussian_dataset.ids)
    gaussian_dataset = cdf_transformer.transform(gaussian_dataset, bins=bins)
    X_t, y_t, w_t, ids_t = (gaussian_dataset.X, gaussian_dataset.y,
                            gaussian_dataset.w, gaussian_dataset.ids)

    # Check ids are unchanged.
    for id_elt, id_t_elt in zip(ids, ids_t):
      assert id_elt == id_t_elt
    # Check y is unchanged since this is an X transformer
    np.testing.assert_allclose(y, y_t)
    # Check w is unchanged since this is an X transformer
    np.testing.assert_allclose(w, w_t)
    # Check X is now holding the proper values when sorted.
    sorted = np.sort(X_t, axis=0)
    np.testing.assert_allclose(sorted, target)

</source>
<source file="systems/deepchem-2.3.0/deepchem/trans/tests/test_transformers.py" startline="345" endline="372" pcid="1928">
  def test_cdf_y_transformer(self):
    # Test CDF transformer on Gaussian normal dataset.
    target = np.array(np.transpose(np.linspace(0., 1., 1001)))
    target = np.transpose(np.array(np.append([target], [target], axis=0)))
    gaussian_dataset = dc.data.tests.load_gaussian_cdf_data()
    bins = 1001
    cdf_transformer = dc.trans.CDFTransformer(
        transform_y=True, dataset=gaussian_dataset, bins=bins)
    X, y, w, ids = (gaussian_dataset.X, gaussian_dataset.y, gaussian_dataset.w,
                    gaussian_dataset.ids)
    gaussian_dataset = cdf_transformer.transform(gaussian_dataset, bins=bins)
    X_t, y_t, w_t, ids_t = (gaussian_dataset.X, gaussian_dataset.y,
                            gaussian_dataset.w, gaussian_dataset.ids)

    # Check ids are unchanged.
    for id_elt, id_t_elt in zip(ids, ids_t):
      assert id_elt == id_t_elt
    # Check X is unchanged since this is an y transformer
    np.testing.assert_allclose(X, X_t)
    # Check w is unchanged since this is an y transformer
    np.testing.assert_allclose(w, w_t)
    # Check y is now holding the proper values when sorted.
    sorted = np.sort(y_t, axis=0)
    np.testing.assert_allclose(sorted, target)

    # Check that untransform does the right thing.
    np.testing.assert_allclose(cdf_transformer.untransform(y_t), y)

</source>
</class>

<class classid="106" nclones="2" nlines="20" similarity="95">
<source file="systems/deepchem-2.3.0/deepchem/trans/tests/test_transformers.py" startline="373" endline="398" pcid="1929">
  def test_clipping_X_transformer(self):
    """Test clipping transformer on X of singletask dataset."""
    n_samples = 10
    n_features = 3
    n_tasks = 1
    ids = np.arange(n_samples)
    X = np.ones((n_samples, n_features))
    target = 5. * X
    X *= 6.
    y = np.zeros((n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))
    dataset = dc.data.NumpyDataset(X, y, w, ids)
    transformer = dc.trans.ClippingTransformer(transform_X=True, x_max=5.)
    clipped_dataset = transformer.transform(dataset)
    X_t, y_t, w_t, ids_t = (clipped_dataset.X, clipped_dataset.y,
                            clipped_dataset.w, clipped_dataset.ids)
    # Check ids are unchanged.
    for id_elt, id_t_elt in zip(ids, ids_t):
      assert id_elt == id_t_elt
    # Check y is unchanged since this is an X transformer
    np.testing.assert_allclose(y, y_t)
    # Check w is unchanged since this is an X transformer
    np.testing.assert_allclose(w, w_t)
    # Check X is now holding the proper values when sorted.
    np.testing.assert_allclose(X_t, target)

</source>
<source file="systems/deepchem-2.3.0/deepchem/trans/tests/test_transformers.py" startline="399" endline="424" pcid="1930">
  def test_clipping_y_transformer(self):
    """Test clipping transformer on y of singletask dataset."""
    n_samples = 10
    n_features = 3
    n_tasks = 1
    ids = np.arange(n_samples)
    X = np.zeros((n_samples, n_features))
    y = np.ones((n_samples, n_tasks))
    target = 5. * y
    y *= 6.
    w = np.ones((n_samples, n_tasks))
    dataset = dc.data.NumpyDataset(X, y, w, ids)
    transformer = dc.trans.ClippingTransformer(transform_y=True, y_max=5.)
    clipped_dataset = transformer.transform(dataset)
    X_t, y_t, w_t, ids_t = (clipped_dataset.X, clipped_dataset.y,
                            clipped_dataset.w, clipped_dataset.ids)
    # Check ids are unchanged.
    for id_elt, id_t_elt in zip(ids, ids_t):
      assert id_elt == id_t_elt
    # Check X is unchanged since this is a y transformer
    np.testing.assert_allclose(X, X_t)
    # Check w is unchanged since this is a y transformer
    np.testing.assert_allclose(w, w_t)
    # Check y is now holding the proper values when sorted.
    np.testing.assert_allclose(y_t, target)

</source>
</class>

<class classid="107" nclones="2" nlines="19" similarity="94">
<source file="systems/deepchem-2.3.0/deepchem/trans/tests/test_transformers.py" startline="425" endline="449" pcid="1931">
  def test_power_X_transformer(self):
    """Test Power transformer on Gaussian normal dataset."""
    gaussian_dataset = dc.data.tests.load_gaussian_cdf_data()
    powers = [1, 2, 0.5]
    power_transformer = dc.trans.PowerTransformer(
        transform_X=True, powers=powers)
    X, y, w, ids = (gaussian_dataset.X, gaussian_dataset.y, gaussian_dataset.w,
                    gaussian_dataset.ids)
    gaussian_dataset2 = power_transformer.transform(gaussian_dataset)
    X_t, y_t, w_t, ids_t = (gaussian_dataset2.X, gaussian_dataset2.y,
                            gaussian_dataset2.w, gaussian_dataset2.ids)

    # Check ids are unchanged.
    for id_elt, id_t_elt in zip(ids, ids_t):
      assert id_elt == id_t_elt
    # Check y is unchanged since this is an X transformer
    np.testing.assert_allclose(y, y_t)
    # Check w is unchanged since this is an X transformer
    np.testing.assert_allclose(w, w_t)
    # Check X is now holding the proper values in each column.
    np.testing.assert_allclose(X_t.shape[1], len(powers) * X.shape[1])
    np.testing.assert_allclose(X, X_t[:, :2])
    np.testing.assert_allclose(np.power(X, 2), X_t[:, 2:4])
    np.testing.assert_allclose(np.power(X, 0.5), X_t[:, 4:])

</source>
<source file="systems/deepchem-2.3.0/deepchem/trans/tests/test_transformers.py" startline="450" endline="477" pcid="1932">
  def test_power_y_transformer(self):
    """Test Power transformer on Gaussian normal dataset."""
    gaussian_dataset = dc.data.tests.load_gaussian_cdf_data()
    powers = [1, 2, 0.5]
    power_transformer = dc.trans.PowerTransformer(
        transform_y=True, powers=powers)
    X, y, w, ids = (gaussian_dataset.X, gaussian_dataset.y, gaussian_dataset.w,
                    gaussian_dataset.ids)
    gaussian_dataset2 = power_transformer.transform(gaussian_dataset)
    X_t, y_t, w_t, ids_t = (gaussian_dataset2.X, gaussian_dataset2.y,
                            gaussian_dataset2.w, gaussian_dataset2.ids)

    # Check ids are unchanged.
    for id_elt, id_t_elt in zip(ids, ids_t):
      assert id_elt == id_t_elt
    # Check X is unchanged since this is an X transformer
    np.testing.assert_allclose(X, X_t)
    # Check w is unchanged since this is an X transformer
    np.testing.assert_allclose(w, w_t)
    # Check y is now holding the proper values in each column.
    np.testing.assert_allclose(y_t.shape[1], len(powers) * y.shape[1])
    np.testing.assert_allclose(y, y_t[:, :2])
    np.testing.assert_allclose(np.power(y, 2), y_t[:, 2:4])
    np.testing.assert_allclose(np.power(y, 0.5), y_t[:, 4:])

    # Check that untransform does the right thing.
    np.testing.assert_allclose(power_transformer.untransform(y_t), y)

</source>
</class>

<class classid="108" nclones="2" nlines="22" similarity="82">
<source file="systems/deepchem-2.3.0/deepchem/trans/tests/test_transformers.py" startline="478" endline="508" pcid="1933">
  def test_singletask_balancing_transformer(self):
    """Test balancing transformer on single-task dataset."""

    classification_dataset = dc.data.tests.load_classification_data()
    balancing_transformer = dc.trans.BalancingTransformer(
        transform_w=True, dataset=classification_dataset)
    X, y, w, ids = (classification_dataset.X, classification_dataset.y,
                    classification_dataset.w, classification_dataset.ids)
    classification_dataset = balancing_transformer.transform(
        classification_dataset)
    X_t, y_t, w_t, ids_t = (classification_dataset.X, classification_dataset.y,
                            classification_dataset.w,
                            classification_dataset.ids)
    # Check ids are unchanged.
    for id_elt, id_t_elt in zip(ids, ids_t):
      assert id_elt == id_t_elt
    # Check X is unchanged since this is a w transformer
    np.testing.assert_allclose(X, X_t)
    # Check y is unchanged since this is a w transformer
    np.testing.assert_allclose(y, y_t)
    for ind, task in enumerate(classification_dataset.get_task_names()):
      y_task = y_t[:, ind]
      w_task = w_t[:, ind]
      w_orig_task = w[:, ind]
      # Assert that entries with zero weight retain zero weight
      np.testing.assert_allclose(w_task[w_orig_task == 0],
                                 np.zeros_like(w_task[w_orig_task == 0]))
      # Check that sum of 0s equals sum of 1s in transformed for each task
      assert np.isclose(
          np.sum(w_task[y_task == 0]), np.sum(w_task[y_task == 1]))

</source>
<source file="systems/deepchem-2.3.0/deepchem/trans/tests/test_transformers.py" startline="509" endline="536" pcid="1934">
  def test_multitask_balancing_transformer(self):
    """Test balancing transformer on multitask dataset."""
    multitask_dataset = dc.data.tests.load_multitask_data()
    balancing_transformer = dc.trans.BalancingTransformer(
        transform_w=True, dataset=multitask_dataset)
    X, y, w, ids = (multitask_dataset.X, multitask_dataset.y,
                    multitask_dataset.w, multitask_dataset.ids)
    multitask_dataset = balancing_transformer.transform(multitask_dataset)
    X_t, y_t, w_t, ids_t = (multitask_dataset.X, multitask_dataset.y,
                            multitask_dataset.w, multitask_dataset.ids)
    # Check ids are unchanged.
    for id_elt, id_t_elt in zip(ids, ids_t):
      assert id_elt == id_t_elt
    # Check X is unchanged since this is a w transformer
    np.testing.assert_allclose(X, X_t)
    # Check y is unchanged since this is a w transformer
    np.testing.assert_allclose(y, y_t)
    for ind, task in enumerate(multitask_dataset.get_task_names()):
      y_task = y_t[:, ind]
      w_task = w_t[:, ind]
      w_orig_task = w[:, ind]
      # Assert that entries with zero weight retain zero weight
      np.testing.assert_allclose(w_task[w_orig_task == 0],
                                 np.zeros_like(w_task[w_orig_task == 0]))
      # Check that sum of 0s equals sum of 1s in transformed for each task
      assert np.isclose(
          np.sum(w_task[y_task == 0]), np.sum(w_task[y_task == 1]))

</source>
</class>

<class classid="109" nclones="23" nlines="78" similarity="70">
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/bbbp_datasets.py" startline="17" endline="109" pcid="1945">
def load_bbbp(featurizer='ECFP',
              split='random',
              reload=True,
              data_dir=None,
              save_dir=None,
              **kwargs):
  """Load blood-brain barrier penetration datasets """
  # Featurize bbb dataset
  logger.info("About to featurize bbbp dataset.")
  if data_dir is None:
    data_dir = DEFAULT_DIR
  if save_dir is None:
    save_dir = DEFAULT_DIR

  bbbp_tasks = ["p_np"]

  if reload:
    save_folder = os.path.join(save_dir, "bbbp-featurized", featurizer)
    if featurizer == "smiles2img":
      img_spec = kwargs.get("img_spec", "std")
      save_folder = os.path.join(save_folder, img_spec)
    save_folder = os.path.join(save_folder, str(split))

    loaded, all_dataset, transformers = deepchem.utils.save.load_dataset_from_disk(
        save_folder)
    if loaded:
      return bbbp_tasks, all_dataset, transformers

  dataset_file = os.path.join(data_dir, "BBBP.csv")
  if not os.path.exists(dataset_file):
    deepchem.utils.download_url(url=BBBP_URL, dest_dir=data_dir)

  if featurizer == 'ECFP':
    featurizer = deepchem.feat.CircularFingerprint(size=1024)
  elif featurizer == 'GraphConv':
    featurizer = deepchem.feat.ConvMolFeaturizer()
  elif featurizer == 'Weave':
    featurizer = deepchem.feat.WeaveFeaturizer()
  elif featurizer == 'Raw':
    featurizer = deepchem.feat.RawFeaturizer()
  elif featurizer == "smiles2img":
    img_spec = kwargs.get("img_spec", "std")
    img_size = kwargs.get("img_size", 80)
    featurizer = deepchem.feat.SmilesToImage(
        img_size=img_size, img_spec=img_spec)

  loader = deepchem.data.CSVLoader(
      tasks=bbbp_tasks, smiles_field="smiles", featurizer=featurizer)
  dataset = loader.featurize(dataset_file, shard_size=8192)

  if split is None:
    # Initialize transformers
    transformers = [
        deepchem.trans.BalancingTransformer(transform_w=True, dataset=dataset)
    ]

    logger.info("Split is None, about to transform data")
    for transformer in transformers:
      dataset = transformer.transform(dataset)

    return bbbp_tasks, (dataset, None, None), transformers

  splitters = {
      'index': deepchem.splits.IndexSplitter(),
      'random': deepchem.splits.RandomSplitter(),
      'scaffold': deepchem.splits.ScaffoldSplitter()
  }
  splitter = splitters[split]
  logger.info("About to split data with {} splitter.".format(split))
  frac_train = kwargs.get("frac_train", 0.8)
  frac_valid = kwargs.get('frac_valid', 0.1)
  frac_test = kwargs.get('frac_test', 0.1)

  train, valid, test = splitter.train_valid_test_split(
      dataset,
      frac_train=frac_train,
      frac_valid=frac_valid,
      frac_test=frac_test)

  # Initialize transformers
  transformers = [
      deepchem.trans.BalancingTransformer(transform_w=True, dataset=train)
  ]

  for transformer in transformers:
    train = transformer.transform(train)
    valid = transformer.transform(valid)
    test = transformer.transform(test)

  if reload:
    deepchem.utils.save.save_dataset_to_disk(save_folder, train, valid, test,
                                             transformers)
  return bbbp_tasks, (train, valid, test), transformers
</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/bace_datasets.py" startline="125" endline="223" pcid="1965">
def load_bace_classification(featurizer='ECFP',
                             split='random',
                             reload=True,
                             data_dir=None,
                             save_dir=None,
                             **kwargs):
  """Load bace datasets."""
  # Featurize bace dataset
  logger.info("About to featurize bace dataset.")
  if data_dir is None:
    data_dir = DEFAULT_DIR
  if save_dir is None:
    save_dir = DEFAULT_DIR

  bace_tasks = ["Class"]

  if reload:
    save_folder = os.path.join(save_dir, "bace_c-featurized", str(featurizer))
    if featurizer == "smiles2img":
      img_spec = kwargs.get("img_spec", "std")
      save_folder = os.path.join(save_folder, img_spec)
    save_folder = os.path.join(save_folder, str(split))

    loaded, all_dataset, transformers = deepchem.utils.save.load_dataset_from_disk(
        save_folder)
    if loaded:
      return bace_tasks, all_dataset, transformers

  dataset_file = os.path.join(data_dir, "bace.csv")
  if not os.path.exists(dataset_file):
    deepchem.utils.download_url(url=BACE_URL, dest_dir=data_dir)

  if featurizer == 'ECFP':
    featurizer = deepchem.feat.CircularFingerprint(size=1024)
  elif featurizer == 'GraphConv':
    featurizer = deepchem.feat.ConvMolFeaturizer()
  elif featurizer == 'Weave':
    featurizer = deepchem.feat.WeaveFeaturizer()
  elif featurizer == 'Raw':
    featurizer = deepchem.feat.RawFeaturizer()
  elif featurizer == 'UserDefined':
    featurizer = deepchem.feat.UserDefinedFeaturizer(
        bace_user_specified_features)
  elif featurizer == "smiles2img":
    img_spec = kwargs.get("img_spec", "std")
    img_size = kwargs.get("img_size", 80)
    featurizer = deepchem.feat.SmilesToImage(
        img_size=img_size, img_spec=img_spec)

  loader = deepchem.data.CSVLoader(
      tasks=bace_tasks, smiles_field="mol", featurizer=featurizer)

  dataset = loader.featurize(dataset_file, shard_size=8192)

  if split is None:
    # Initialize transformers
    transformers = [
        deepchem.trans.BalancingTransformer(transform_w=True, dataset=dataset)
    ]

    logger.info("Split is None, about to transform data")
    for transformer in transformers:
      dataset = transformer.transform(dataset)

    return bace_tasks, (dataset, None, None), transformers

  splitters = {
      'index': deepchem.splits.IndexSplitter(),
      'random': deepchem.splits.RandomSplitter(),
      'scaffold': deepchem.splits.ScaffoldSplitter(),
      'stratified': deepchem.splits.RandomStratifiedSplitter()
  }

  splitter = splitters[split]
  logger.info("About to split data using {} splitter".format(split))
  frac_train = kwargs.get("frac_train", 0.8)
  frac_valid = kwargs.get('frac_valid', 0.1)
  frac_test = kwargs.get('frac_test', 0.1)

  train, valid, test = splitter.train_valid_test_split(
      dataset,
      frac_train=frac_train,
      frac_valid=frac_valid,
      frac_test=frac_test)

  transformers = [
      deepchem.trans.BalancingTransformer(transform_w=True, dataset=train)
  ]

  logger.info("About to transform data.")
  for transformer in transformers:
    train = transformer.transform(train)
    valid = transformer.transform(valid)
    test = transformer.transform(test)

  if reload:
    deepchem.utils.save.save_dataset_to_disk(save_folder, train, valid, test,
                                             transformers)
  return bace_tasks, (train, valid, test), transformers
</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/lipo_datasets.py" startline="17" endline="119" pcid="1978">
def load_lipo(featurizer='ECFP',
              split='index',
              reload=True,
              move_mean=True,
              data_dir=None,
              save_dir=None,
              **kwargs):
  """Load Lipophilicity datasets."""
  # Featurize Lipophilicity dataset
  logger.info("About to featurize Lipophilicity dataset.")
  logger.info("About to load Lipophilicity dataset.")

  Lipo_tasks = ['exp']

  if data_dir is None:
    data_dir = DEFAULT_DIR
  if save_dir is None:
    save_dir = DEFAULT_DIR

  if reload:
    save_folder = os.path.join(save_dir, "lipo-featurized")
    if not move_mean:
      save_folder = os.path.join(save_folder, str(featurizer) + "_mean_unmoved")
    else:
      save_folder = os.path.join(save_folder, str(featurizer))

    if featurizer == "smiles2img":
      img_spec = kwargs.get("img_spec", "std")
      save_folder = os.path.join(save_folder, img_spec)
    save_folder = os.path.join(save_folder, str(split))

    loaded, all_dataset, transformers = deepchem.utils.save.load_dataset_from_disk(
        save_folder)
    if loaded:
      return Lipo_tasks, all_dataset, transformers

  dataset_file = os.path.join(data_dir, "Lipophilicity.csv")
  if not os.path.exists(dataset_file):
    deepchem.utils.download_url(url=LIPO_URL, dest_dir=data_dir)

  if featurizer == 'ECFP':
    featurizer = deepchem.feat.CircularFingerprint(size=1024)
  elif featurizer == 'GraphConv':
    featurizer = deepchem.feat.ConvMolFeaturizer()
  elif featurizer == 'Weave':
    featurizer = deepchem.feat.WeaveFeaturizer()
  elif featurizer == 'Raw':
    featurizer = deepchem.feat.RawFeaturizer()
  elif featurizer == "smiles2img":
    img_spec = kwargs.get("img_spec", "std")
    img_size = kwargs.get("img_size", 80)
    featurizer = deepchem.feat.SmilesToImage(
        img_size=img_size, img_spec=img_spec)

  loader = deepchem.data.CSVLoader(
      tasks=Lipo_tasks, smiles_field="smiles", featurizer=featurizer)
  dataset = loader.featurize(dataset_file, shard_size=8192)

  if split is None:
    transformers = [
        deepchem.trans.NormalizationTransformer(
            transform_y=True, dataset=dataset, move_mean=move_mean)
    ]

    logger.info("Split is None, about to transform data")
    for transformer in transformers:
      dataset = transformer.transform(dataset)

    return Lipo_tasks, (dataset, None, None), transformers

  splitters = {
      'index': deepchem.splits.IndexSplitter(),
      'random': deepchem.splits.RandomSplitter(),
      'scaffold': deepchem.splits.ScaffoldSplitter(),
      'stratified': deepchem.splits.SingletaskStratifiedSplitter()
  }
  splitter = splitters[split]
  logger.info("About to split data with {} splitter.".format(split))
  frac_train = kwargs.get("frac_train", 0.8)
  frac_valid = kwargs.get('frac_valid', 0.1)
  frac_test = kwargs.get('frac_test', 0.1)

  train, valid, test = splitter.train_valid_test_split(
      dataset,
      frac_train=frac_train,
      frac_valid=frac_valid,
      frac_test=frac_test)

  transformers = [
      deepchem.trans.NormalizationTransformer(
          transform_y=True, dataset=train, move_mean=move_mean)
  ]

  logger.info("About to transform data.")
  for transformer in transformers:
    train = transformer.transform(train)
    valid = transformer.transform(valid)
    test = transformer.transform(test)

  if reload:
    deepchem.utils.save.save_dataset_to_disk(save_folder, train, valid, test,
                                             transformers)
  return Lipo_tasks, (train, valid, test), transformers
</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/sampl_datasets.py" startline="17" endline="122" pcid="1976">
def load_sampl(featurizer='ECFP',
               split='index',
               reload=True,
               move_mean=True,
               data_dir=None,
               save_dir=None,
               **kwargs):
  """Load SAMPL datasets."""
  # Featurize SAMPL dataset
  logger.info("About to featurize SAMPL dataset.")
  logger.info("About to load SAMPL dataset.")

  if data_dir is None:
    data_dir = DEFAULT_DIR
  if save_dir is None:
    save_dir = DEFAULT_DIR

  if reload:
    save_folder = os.path.join(save_dir, "sampl-featurized")
    if not move_mean:
      save_folder = os.path.join(save_folder, str(featurizer) + "_mean_unmoved")
    else:
      save_folder = os.path.join(save_folder, str(featurizer))

    if featurizer == "smiles2img":
      img_spec = kwargs.get("img_spec", "std")
      save_folder = os.path.join(save_folder, img_spec)
    save_folder = os.path.join(save_folder, str(split))

  dataset_file = os.path.join(data_dir, "SAMPL.csv")
  if not os.path.exists(dataset_file):
    deepchem.utils.download_url(url=SAMPL_URL, dest_dir=data_dir)

  SAMPL_tasks = ['expt']

  if reload:
    loaded, all_dataset, transformers = deepchem.utils.save.load_dataset_from_disk(
        save_folder)
    if loaded:
      return SAMPL_tasks, all_dataset, transformers

  if featurizer == 'ECFP':
    featurizer = deepchem.feat.CircularFingerprint(size=1024)
  elif featurizer == 'GraphConv':
    featurizer = deepchem.feat.ConvMolFeaturizer()
  elif featurizer == 'Weave':
    featurizer = deepchem.feat.WeaveFeaturizer()
  elif featurizer == 'Raw':
    featurizer = deepchem.feat.RawFeaturizer()
  elif featurizer == 'smiles2img':
    img_size = kwargs.get("img_size", 80)
    img_spec = kwargs.get("img_spec", "std")
    featurizer = deepchem.feat.SmilesToImage(
        img_size=img_size, img_spec=img_spec)

  loader = deepchem.data.CSVLoader(
      tasks=SAMPL_tasks, smiles_field="smiles", featurizer=featurizer)
  dataset = loader.featurize(dataset_file, shard_size=8192)

  if split == None:
    transformers = [
        deepchem.trans.NormalizationTransformer(
            transform_y=True, dataset=dataset, move_mean=move_mean)
    ]

    logger.info("Split is None, about to transform data")
    for transformer in transformers:
      dataset = transformer.transform(dataset)

    return SAMPL_tasks, (dataset, None, None), transformers

  splitters = {
      'index': deepchem.splits.IndexSplitter(),
      'random': deepchem.splits.RandomSplitter(),
      'scaffold': deepchem.splits.ScaffoldSplitter(),
      'stratified': deepchem.splits.SingletaskStratifiedSplitter(task_number=0)
  }

  splitter = splitters[split]
  logger.info("About to split dataset with {} splitter.".format(split))
  frac_train = kwargs.get("frac_train", 0.8)
  frac_valid = kwargs.get('frac_valid', 0.1)
  frac_test = kwargs.get('frac_test', 0.1)

  train, valid, test = splitter.train_valid_test_split(
      dataset,
      frac_train=frac_train,
      frac_valid=frac_valid,
      frac_test=frac_test)
  train, valid, test = splitter.train_valid_test_split(dataset)

  transformers = [
      deepchem.trans.NormalizationTransformer(
          transform_y=True, dataset=train, move_mean=move_mean)
  ]

  logger.info("About to transform dataset.")
  for transformer in transformers:
    train = transformer.transform(train)
    valid = transformer.transform(valid)
    test = transformer.transform(test)

  if reload:
    deepchem.utils.save.save_dataset_to_disk(save_folder, train, valid, test,
                                             transformers)
  return SAMPL_tasks, (train, valid, test), transformers
</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/clintox_datasets.py" startline="19" endline="109" pcid="1949">
def load_clintox(featurizer='ECFP',
                 split='index',
                 reload=True,
                 data_dir=None,
                 save_dir=None,
                 **kwargs):
  """Load clintox datasets."""
  if data_dir is None:
    data_dir = DEFAULT_DIR
  if save_dir is None:
    save_dir = DEFAULT_DIR

  if reload:
    save_folder = os.path.join(save_dir, "clintox-featurized", featurizer)
    if featurizer == "smiles2img":
      img_spec = kwargs.get("img_spec", "std")
      save_folder = os.path.join(save_folder, img_spec)
    save_folder = os.path.join(save_folder, str(split))

  dataset_file = os.path.join(data_dir, "clintox.csv.gz")
  if not os.path.exists(dataset_file):
    deepchem.utils.download_url(url=CLINTOX_URL, dest_dir=data_dir)

  logger.info("About to load clintox dataset.")
  dataset = deepchem.utils.save.load_from_disk(dataset_file)
  clintox_tasks = dataset.columns.values[1:].tolist()
  logger.info("Tasks in dataset: %s" % (clintox_tasks))
  logger.info("Number of tasks in dataset: %s" % str(len(clintox_tasks)))
  logger.info("Number of examples in dataset: %s" % str(dataset.shape[0]))
  if reload:
    loaded, all_dataset, transformers = deepchem.utils.save.load_dataset_from_disk(
        save_folder)
    if loaded:
      return clintox_tasks, all_dataset, transformers
  # Featurize clintox dataset
  logger.info("About to featurize clintox dataset.")
  if featurizer == 'ECFP':
    featurizer = deepchem.feat.CircularFingerprint(size=1024)
  elif featurizer == 'GraphConv':
    featurizer = deepchem.feat.ConvMolFeaturizer()
  elif featurizer == 'Weave':
    featurizer = deepchem.feat.WeaveFeaturizer()
  elif featurizer == 'Raw':
    featurizer = deepchem.feat.RawFeaturizer()
  elif featurizer == "smiles2img":
    img_spec = kwargs.get("img_spec", "std")
    img_size = kwargs.get("img_size", 80)
    featurizer = deepchem.feat.SmilesToImage(
        img_size=img_size, img_spec=img_spec)

  loader = deepchem.data.CSVLoader(
      tasks=clintox_tasks, smiles_field="smiles", featurizer=featurizer)
  dataset = loader.featurize(dataset_file, shard_size=8192)

  # Transform clintox dataset
  if split is None:
    transformers = [
        deepchem.trans.BalancingTransformer(transform_w=True, dataset=dataset)
    ]

    logger.info("Split is None, about to transform data.")
    for transformer in transformers:
      dataset = transformer.transform(dataset)

    return clintox_tasks, (dataset, None, None), transformers

  splitters = {
      'index': deepchem.splits.IndexSplitter(),
      'random': deepchem.splits.RandomSplitter(),
      'scaffold': deepchem.splits.ScaffoldSplitter(),
      'stratified': deepchem.splits.RandomStratifiedSplitter()
  }
  splitter = splitters[split]
  logger.info("About to split data with {} splitter.".format(split))
  train, valid, test = splitter.train_valid_test_split(dataset)

  transformers = [
      deepchem.trans.BalancingTransformer(transform_w=True, dataset=train)
  ]

  logger.info("About to transform data.")
  for transformer in transformers:
    train = transformer.transform(train)
    valid = transformer.transform(valid)
    test = transformer.transform(test)

  if reload:
    deepchem.utils.save.save_dataset_to_disk(save_folder, train, valid, test,
                                             transformers)

  return clintox_tasks, (train, valid, test), transformers
</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/nci_datasets.py" startline="19" endline="125" pcid="1967">
def load_nci(featurizer='ECFP',
             shard_size=1000,
             split='random',
             reload=True,
             data_dir=None,
             save_dir=None,
             **kwargs):

  # Load nci dataset
  logger.info("About to load NCI dataset.")

  if data_dir is None:
    data_dir = DEFAULT_DIR
  if save_dir is None:
    save_dir = DEFAULT_DIR

  all_nci_tasks = [
      'CCRF-CEM', 'HL-60(TB)', 'K-562', 'MOLT-4', 'RPMI-8226', 'SR',
      'A549/ATCC', 'EKVX', 'HOP-62', 'HOP-92', 'NCI-H226', 'NCI-H23',
      'NCI-H322M', 'NCI-H460', 'NCI-H522', 'COLO 205', 'HCC-2998', 'HCT-116',
      'HCT-15', 'HT29', 'KM12', 'SW-620', 'SF-268', 'SF-295', 'SF-539',
      'SNB-19', 'SNB-75', 'U251', 'LOX IMVI', 'MALME-3M', 'M14', 'MDA-MB-435',
      'SK-MEL-2', 'SK-MEL-28', 'SK-MEL-5', 'UACC-257', 'UACC-62', 'IGR-OV1',
      'OVCAR-3', 'OVCAR-4', 'OVCAR-5', 'OVCAR-8', 'NCI/ADR-RES', 'SK-OV-3',
      '786-0', 'A498', 'ACHN', 'CAKI-1', 'RXF 393', 'SN12C', 'TK-10', 'UO-31',
      'PC-3', 'DU-145', 'MCF7', 'MDA-MB-231/ATCC', 'MDA-MB-468', 'HS 578T',
      'BT-549', 'T-47D'
  ]

  if reload:
    save_folder = os.path.join(save_dir, "nci-featurized", featurizer)
    if featurizer == "smiles2img":
      img_spec = kwargs.get("img_spec", "std")
      save_folder = os.path.join(save_folder, img_spec)
    save_folder = os.path.join(save_folder, str(split))

    loaded, all_dataset, transformers = deepchem.utils.save.load_dataset_from_disk(
        save_folder)
    if loaded:
      return all_nci_tasks, all_dataset, transformers

  dataset_file = os.path.join(data_dir, "nci_unique.csv")
  if not os.path.exists(dataset_file):
    deepchem.utils.download_url(url=NCI_URL, dest_dir=data_dir)

  # Featurize nci dataset
  logger.info("About to featurize nci dataset.")
  if featurizer == 'ECFP':
    featurizer = deepchem.feat.CircularFingerprint(size=1024)
  elif featurizer == 'GraphConv':
    featurizer = deepchem.feat.ConvMolFeaturizer()
  elif featurizer == 'Weave':
    featurizer = deepchem.feat.WeaveFeaturizer()
  elif featurizer == 'Raw':
    featurizer = deepchem.feat.RawFeaturizer()
  elif featurizer == "smiles2img":
    img_spec = kwargs.get("img_spec", "std")
    img_size = kwargs.get("img_size", 80)
    featurizer = deepchem.feat.SmilesToImage(
        img_size=img_size, img_spec=img_spec)

  loader = deepchem.data.CSVLoader(
      tasks=all_nci_tasks, smiles_field="smiles", featurizer=featurizer)

  dataset = loader.featurize(dataset_file, shard_size=shard_size)

  if split == None:
    logger.info("Split is None, about to transform data")
    transformers = [
        deepchem.trans.NormalizationTransformer(
            transform_y=True, dataset=dataset)
    ]
    for transformer in transformers:
      dataset = transformer.transform(dataset)
    return all_nci_tasks, (dataset, None, None), transformers

  splitters = {
      'index': deepchem.splits.IndexSplitter(),
      'random': deepchem.splits.RandomSplitter(),
      'scaffold': deepchem.splits.ScaffoldSplitter()
  }
  splitter = splitters[split]
  logger.info("About to split data with {} splitter.".format(splitter))
  frac_train = kwargs.get("frac_train", 0.8)
  frac_valid = kwargs.get('frac_valid', 0.1)
  frac_test = kwargs.get('frac_test', 0.1)

  train, valid, test = splitter.train_valid_test_split(
      dataset,
      frac_train=frac_train,
      frac_valid=frac_valid,
      frac_test=frac_test)

  transformers = [
      deepchem.trans.NormalizationTransformer(transform_y=True, dataset=train)
  ]

  logger.info("About to transform dataset.")
  for transformer in transformers:
    train = transformer.transform(train)
    valid = transformer.transform(valid)
    test = transformer.transform(test)

  if reload:
    deepchem.utils.save.save_dataset_to_disk(save_folder, train, valid, test,
                                             transformers)
  return all_nci_tasks, (train, valid, test), transformers
</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/sweetlead_datasets.py" startline="20" endline="102" pcid="1975">
def load_sweet(featurizer='ECFP',
               split='index',
               reload=True,
               frac_train=.8,
               data_dir=None,
               save_dir=None,
               **kwargs):
  """Load sweet datasets."""
  # Load Sweetlead dataset
  logger.info("About to load Sweetlead dataset.")
  SWEET_tasks = ["task"]

  if data_dir is None:
    data_dir = DEFAULT_DIR
  if save_dir is None:
    save_dir = DEFAULT_DIR

  if reload:
    save_folder = os.path.join(save_dir, "sweet-featurized", featurizer)
    if featurizer == "smiles2img":
      img_spec = kwargs.get("img_spec", "std")
      save_folder = os.path.join(save_folder, img_spec)
    save_folder = os.path.join(save_folder, str(split))

    loaded, all_dataset, transformers = dc.utils.save.load_dataset_from_disk(
        save_folder)
    if loaded:
      return SWEET_tasks, all_dataset, transformers

  # Featurize SWEET dataset
  logger.info("About to featurize SWEET dataset.")
  if featurizer == 'ECFP':
    featurizer = dc.feat.CircularFingerprint(size=1024)
  elif featurizer == "smiles2img":
    img_spec = kwargs.get("img_spec", "std")
    img_size = kwargs.get("img_size", 80)
    featurizer = deepchem.feat.SmilesToImage(
        img_size=img_size, img_spec=img_spec)
  else:
    raise ValueError("Other featurizations not supported")

  dataset_file = os.path.join(data_dir, "sweet.csv.gz")
  if not os.path.exists(dataset_file):
    dc.utils.download_url(SWEETLEAD_URL)
  loader = dc.data.CSVLoader(
      tasks=SWEET_tasks, smiles_field="smiles", featurizer=featurizer)
  dataset = loader.featurize(dataset_file)

  # Initialize transformers
  transformers = [
      dc.trans.BalancingTransformer(transform_w=True, dataset=dataset)
  ]
  logger.info("About to transform data")
  for transformer in transformers:
    dataset = transformer.transform(dataset)

  if split == None:
    return SWEET_tasks, (dataset, None, None), transformers

  splitters = {
      'index': dc.splits.IndexSplitter(),
      'random': dc.splits.RandomSplitter(),
      'scaffold': dc.splits.ScaffoldSplitter(),
      'task': dc.splits.TaskSplitter(),
      'stratified': dc.splits.RandomStratifiedSplitter()
  }
  splitter = splitters[split]
  frac_train = kwargs.get("frac_train", 0.8)
  frac_valid = kwargs.get('frac_valid', 0.1)
  frac_test = kwargs.get('frac_test', 0.1)

  train, valid, test = splitter.train_valid_test_split(
      dataset,
      frac_train=frac_train,
      frac_valid=frac_valid,
      frac_test=frac_test)

  if reload:
    dc.utils.save.save_dataset_to_disk(save_folder, train, valid, test,
                                       transformers)
    all_dataset = (train, valid, test)

  return SWEET_tasks, (train, valid, test), transformers
</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/delaney_datasets.py" startline="17" endline="109" pcid="1966">
def load_delaney(featurizer='ECFP',
                 split='index',
                 reload=True,
                 move_mean=True,
                 data_dir=None,
                 save_dir=None,
                 **kwargs):
  """Load delaney datasets."""
  # Featurize Delaney dataset
  logger.info("About to featurize Delaney dataset.")
  if data_dir is None:
    data_dir = DEFAULT_DIR
  if save_dir is None:
    save_dir = DEFAULT_DIR
  if reload:
    save_folder = os.path.join(save_dir, "delaney-featurized")
    if not move_mean:
      save_folder = os.path.join(save_folder, str(featurizer) + "_mean_unmoved")
    else:
      save_folder = os.path.join(save_folder, str(featurizer))

    if featurizer == "smiles2img":
      img_spec = kwargs.get("img_spec", "std")
      save_folder = os.path.join(save_folder, img_spec)
    save_folder = os.path.join(save_folder, str(split))

  dataset_file = os.path.join(data_dir, "delaney-processed.csv")

  if not os.path.exists(dataset_file):
    deepchem.utils.download_url(url=DELANEY_URL, dest_dir=data_dir)

  delaney_tasks = ['measured log solubility in mols per litre']
  if reload:
    loaded, all_dataset, transformers = deepchem.utils.save.load_dataset_from_disk(
        save_folder)
    if loaded:
      return delaney_tasks, all_dataset, transformers

  if featurizer == 'ECFP':
    featurizer = deepchem.feat.CircularFingerprint(size=1024)
  elif featurizer == 'GraphConv':
    featurizer = deepchem.feat.ConvMolFeaturizer()
  elif featurizer == 'Weave':
    featurizer = deepchem.feat.WeaveFeaturizer()
  elif featurizer == 'Raw':
    featurizer = deepchem.feat.RawFeaturizer()
  elif featurizer == "smiles2img":
    img_spec = kwargs.get("img_spec", "std")
    img_size = kwargs.get("img_size", 80)
    featurizer = deepchem.feat.SmilesToImage(
        img_size=img_size, img_spec=img_spec)

  loader = deepchem.data.CSVLoader(
      tasks=delaney_tasks, smiles_field="smiles", featurizer=featurizer)
  dataset = loader.featurize(dataset_file, shard_size=8192)

  if split is None:
    transformers = [
        deepchem.trans.NormalizationTransformer(
            transform_y=True, dataset=dataset, move_mean=move_mean)
    ]

    logger.info("Split is None, about to transform data")
    for transformer in transformers:
      dataset = transformer.transform(dataset)

    return delaney_tasks, (dataset, None, None), transformers

  splitters = {
      'index': deepchem.splits.IndexSplitter(),
      'random': deepchem.splits.RandomSplitter(),
      'scaffold': deepchem.splits.ScaffoldSplitter(),
      'stratified': deepchem.splits.SingletaskStratifiedSplitter()
  }
  splitter = splitters[split]
  logger.info("About to split dataset with {} splitter.".format(split))
  train, valid, test = splitter.train_valid_test_split(dataset)

  transformers = [
      deepchem.trans.NormalizationTransformer(
          transform_y=True, dataset=train, move_mean=move_mean)
  ]

  logger.info("About to transform data.")
  for transformer in transformers:
    train = transformer.transform(train)
    valid = transformer.transform(valid)
    test = transformer.transform(test)

  if reload:
    deepchem.utils.save.save_dataset_to_disk(save_folder, train, valid, test,
                                             transformers)
  return delaney_tasks, (train, valid, test), transformers
</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/hopv_datasets.py" startline="17" endline="114" pcid="1979">
def load_hopv(featurizer='ECFP',
              split='index',
              reload=True,
              data_dir=None,
              save_dir=None,
              **kwargs):
  """Load HOPV datasets. Does not do train/test split"""
  # Featurize HOPV dataset
  logger.info("About to featurize HOPV dataset.")
  if data_dir is None:
    data_dir = DEFAULT_DIR
  if save_dir is None:
    save_dir = DEFAULT_DIR

  hopv_tasks = [
      'HOMO', 'LUMO', 'electrochemical_gap', 'optical_gap', 'PCE', 'V_OC',
      'J_SC', 'fill_factor'
  ]

  if reload:
    save_folder = os.path.join(save_dir, "hopv-featurized", str(featurizer))
    if featurizer == "smiles2img":
      img_spec = kwargs.get("img_spec", "std")
      save_folder = os.path.join(save_folder, img_spec)
    save_folder = os.path.join(save_folder, str(split))

    loaded, all_dataset, transformers = deepchem.utils.save.load_dataset_from_disk(
        save_folder)
    if loaded:
      return hopv_tasks, all_dataset, transformers

  dataset_file = os.path.join(data_dir, "hopv.csv")
  if not os.path.exists(dataset_file):
    deepchem.utils.download_url(url=HOPV_URL, dest_dir=data_dir)
    deepchem.utils.untargz_file(os.path.join(data_dir, 'hopv.tar.gz'), data_dir)

  if featurizer == 'ECFP':
    featurizer = deepchem.feat.CircularFingerprint(size=1024)
  elif featurizer == 'GraphConv':
    featurizer = deepchem.feat.ConvMolFeaturizer()
  elif featurizer == 'Weave':
    featurizer = deepchem.feat.WeaveFeaturizer()
  elif featurizer == 'Raw':
    featurizer = deepchem.feat.RawFeaturizer()
  elif featurizer == "smiles2img":
    img_spec = kwargs.get("img_spec", "std")
    img_size = kwargs.get("img_size", 80)
    featurizer = deepchem.feat.SmilesToImage(
        img_size=img_size, img_spec=img_spec)

  loader = deepchem.data.CSVLoader(
      tasks=hopv_tasks, smiles_field="smiles", featurizer=featurizer)
  dataset = loader.featurize(dataset_file, shard_size=8192)

  if split == None:
    transformers = [
        deepchem.trans.NormalizationTransformer(
            transform_y=True, dataset=dataset)
    ]

    logger.info("Split is None, about to transform data")
    for transformer in transformers:
      dataset = transformer.transform(dataset)

    return hopv_tasks, (dataset, None, None), transformers

  splitters = {
      'index': deepchem.splits.IndexSplitter(),
      'random': deepchem.splits.RandomSplitter(),
      'scaffold': deepchem.splits.ScaffoldSplitter(),
      'butina': deepchem.splits.ButinaSplitter()
  }
  splitter = splitters[split]
  logger.info("About to split dataset with {} splitter.".format(split))
  frac_train = kwargs.get("frac_train", 0.8)
  frac_valid = kwargs.get('frac_valid', 0.1)
  frac_test = kwargs.get('frac_test', 0.1)

  train, valid, test = splitter.train_valid_test_split(
      dataset,
      frac_train=frac_train,
      frac_valid=frac_valid,
      frac_test=frac_test)

  transformers = [
      deepchem.trans.NormalizationTransformer(transform_y=True, dataset=train)
  ]

  logger.info("About to transform data.")
  for transformer in transformers:
    train = transformer.transform(train)
    valid = transformer.transform(valid)
    test = transformer.transform(test)

  if reload:
    deepchem.utils.save.save_dataset_to_disk(save_folder, train, valid, test,
                                             transformers)
  return hopv_tasks, (train, valid, test), transformers
</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/sider_datasets.py" startline="17" endline="112" pcid="1982">
def load_sider(featurizer='ECFP',
               split='index',
               reload=True,
               K=4,
               data_dir=None,
               save_dir=None,
               **kwargs):
  logger.info("About to load SIDER dataset.")
  if data_dir is None:
    data_dir = DEFAULT_DIR
  if save_dir is None:
    save_dir = DEFAULT_DIR

  if reload:
    save_folder = os.path.join(save_dir, "sider-featurized", str(featurizer))
    if featurizer == "smiles2img":
      img_spec = kwargs.get("img_spec", "std")
      save_folder = os.path.join(save_folder, img_spec)
    save_folder = os.path.join(save_folder, str(split))

  dataset_file = os.path.join(data_dir, "sider.csv.gz")
  if not os.path.exists(dataset_file):
    deepchem.utils.download_url(url=SIDER_URL, dest_dir=data_dir)

  dataset = deepchem.utils.save.load_from_disk(dataset_file)
  logger.info("Columns of dataset: %s" % str(dataset.columns.values))
  logger.info("Number of examples in dataset: %s" % str(dataset.shape[0]))
  SIDER_tasks = dataset.columns.values[1:].tolist()

  if reload:
    loaded, all_dataset, transformers = deepchem.utils.save.load_dataset_from_disk(
        save_folder)
    if loaded:
      return SIDER_tasks, all_dataset, transformers

  # Featurize SIDER dataset
  logger.info("About to featurize SIDER dataset.")
  if featurizer == 'ECFP':
    featurizer = deepchem.feat.CircularFingerprint(size=1024)
  elif featurizer == 'GraphConv':
    featurizer = deepchem.feat.ConvMolFeaturizer()
  elif featurizer == 'Weave':
    featurizer = deepchem.feat.WeaveFeaturizer()
  elif featurizer == 'Raw':
    featurizer = deepchem.feat.RawFeaturizer()
  elif featurizer == "smiles2img":
    img_spec = kwargs.get("img_spec", "std")
    img_size = kwargs.get("img_size", 80)
    featurizer = deepchem.feat.SmilesToImage(
        img_size=img_size, img_spec=img_spec)

  logger.info("SIDER tasks: %s" % str(SIDER_tasks))
  logger.info("%d tasks in total" % len(SIDER_tasks))

  loader = deepchem.data.CSVLoader(
      tasks=SIDER_tasks, smiles_field="smiles", featurizer=featurizer)
  dataset = loader.featurize(dataset_file)
  logger.info("%d datapoints in SIDER dataset" % len(dataset))

  # Initialize transformers
  transformers = [
      deepchem.trans.BalancingTransformer(transform_w=True, dataset=dataset)
  ]
  logger.info("About to transform data")
  for transformer in transformers:
    dataset = transformer.transform(dataset)

  if split == None:
    return SIDER_tasks, (dataset, None, None), transformers

  splitters = {
      'index': deepchem.splits.IndexSplitter(),
      'random': deepchem.splits.RandomSplitter(),
      'scaffold': deepchem.splits.ScaffoldSplitter(),
      'task': deepchem.splits.TaskSplitter(),
      'stratified': deepchem.splits.RandomStratifiedSplitter()
  }
  splitter = splitters[split]
  if split == 'task':
    fold_datasets = splitter.k_fold_split(dataset, K)
    all_dataset = fold_datasets
  else:
    frac_train = kwargs.get("frac_train", 0.8)
    frac_valid = kwargs.get('frac_valid', 0.1)
    frac_test = kwargs.get('frac_test', 0.1)

    train, valid, test = splitter.train_valid_test_split(
        dataset,
        frac_train=frac_train,
        frac_valid=frac_valid,
        frac_test=frac_test)
    if reload:
      deepchem.utils.save.save_dataset_to_disk(save_folder, train, valid, test,
                                               transformers)
    all_dataset = (train, valid, test)
  return SIDER_tasks, all_dataset, transformers
</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/hiv_datasets.py" startline="17" endline="112" pcid="1963">
def load_hiv(featurizer='ECFP',
             split='index',
             reload=True,
             data_dir=None,
             save_dir=None,
             **kwargs):
  """Load hiv datasets. Does not do train/test split"""
  # Featurize hiv dataset
  logger.info("About to featurize hiv dataset.")
  if data_dir is None:
    data_dir = DEFAULT_DIR
  if save_dir is None:
    save_dir = DEFAULT_DIR

  hiv_tasks = ["HIV_active"]

  if reload:
    save_folder = os.path.join(save_dir, "hiv-featurized", str(featurizer))
    if featurizer == "smiles2img":
      img_spec = kwargs.get("img_spec", "std")
      save_folder = os.path.join(save_folder, img_spec)
    save_folder = os.path.join(save_folder, str(split))

  if reload:
    loaded, all_dataset, transformers = deepchem.utils.save.load_dataset_from_disk(
        save_folder)
    if loaded:
      return hiv_tasks, all_dataset, transformers

  dataset_file = os.path.join(data_dir, "HIV.csv")
  if not os.path.exists(dataset_file):
    deepchem.utils.download_url(url=HIV_URL, dest_dir=data_dir)

  if featurizer == 'ECFP':
    featurizer = deepchem.feat.CircularFingerprint(size=1024)
  elif featurizer == 'GraphConv':
    featurizer = deepchem.feat.ConvMolFeaturizer()
  elif featurizer == 'Weave':
    featurizer = deepchem.feat.WeaveFeaturizer()
  elif featurizer == 'Raw':
    featurizer = deepchem.feat.RawFeaturizer()
  elif featurizer == "smiles2img":
    img_spec = kwargs.get("img_spec", "std")
    img_size = kwargs.get("img_size", 80)
    featurizer = deepchem.feat.SmilesToImage(
        img_size=img_size, img_spec=img_spec)

  loader = deepchem.data.CSVLoader(
      tasks=hiv_tasks, smiles_field="smiles", featurizer=featurizer)
  dataset = loader.featurize(dataset_file, shard_size=8192)

  if split is None:
    transformers = [
        deepchem.trans.BalancingTransformer(transform_w=True, dataset=dataset)
    ]

    logger.info("Split is None, about to transform data")
    for transformer in transformers:
      dataset = transformer.transform(dataset)

    return hiv_tasks, (dataset, None, None), transformers

  splitters = {
      'index': deepchem.splits.IndexSplitter(),
      'random': deepchem.splits.RandomSplitter(),
      'scaffold': deepchem.splits.ScaffoldSplitter(),
      'butina': deepchem.splits.ButinaSplitter(),
      'stratified': deepchem.splits.RandomStratifiedSplitter()
  }
  splitter = splitters[split]
  logger.info("About to split dataset with {} splitter.".format(split))
  frac_train = kwargs.get("frac_train", 0.8)
  frac_valid = kwargs.get('frac_valid', 0.1)
  frac_test = kwargs.get('frac_test', 0.1)

  train, valid, test = splitter.train_valid_test_split(
      dataset,
      frac_train=frac_train,
      frac_valid=frac_valid,
      frac_test=frac_test)
  train, valid, test = splitter.train_valid_test_split(dataset)

  transformers = [
      deepchem.trans.BalancingTransformer(transform_w=True, dataset=train)
  ]

  logger.info("About to transform data.")
  for transformer in transformers:
    train = transformer.transform(train)
    valid = transformer.transform(valid)
    test = transformer.transform(test)

  if reload:
    deepchem.utils.save.save_dataset_to_disk(save_folder, train, valid, test,
                                             transformers)
  return hiv_tasks, (train, valid, test), transformers
</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/muv_datasets.py" startline="17" endline="117" pcid="1960">
def load_muv(featurizer='ECFP',
             split='index',
             reload=True,
             K=4,
             data_dir=None,
             save_dir=None,
             **kwargs):
  """Load MUV datasets. Does not do train/test split"""
  # Load MUV dataset
  logger.info("About to load MUV dataset.")

  MUV_tasks = sorted([
      'MUV-692', 'MUV-689', 'MUV-846', 'MUV-859', 'MUV-644', 'MUV-548',
      'MUV-852', 'MUV-600', 'MUV-810', 'MUV-712', 'MUV-737', 'MUV-858',
      'MUV-713', 'MUV-733', 'MUV-652', 'MUV-466', 'MUV-832'
  ])

  if data_dir is None:
    data_dir = DEFAULT_DIR
  if save_dir is None:
    save_dir = DEFAULT_DIR

  if reload:
    save_folder = os.path.join(save_dir, "muv-featurized", str(featurizer))
    if featurizer == "smiles2img":
      img_spec = kwargs.get("img_spec", "std")
      save_folder = os.path.join(save_folder, img_spec)
    save_folder = os.path.join(save_folder, str(split))

    loaded, all_dataset, transformers = deepchem.utils.save.load_dataset_from_disk(
        save_folder)
    if loaded:
      return MUV_tasks, all_dataset, transformers

  dataset_file = os.path.join(data_dir, "muv.csv.gz")
  if not os.path.exists(dataset_file):
    deepchem.utils.download_url(url=MUV_URL, dest_dir=data_dir)

  # Featurize MUV dataset
  logger.info("About to featurize MUV dataset.")

  if featurizer == 'ECFP':
    featurizer = deepchem.feat.CircularFingerprint(size=1024)
  elif featurizer == 'GraphConv':
    featurizer = deepchem.feat.ConvMolFeaturizer()
  elif featurizer == 'Weave':
    featurizer = deepchem.feat.WeaveFeaturizer()
  elif featurizer == 'Raw':
    featurizer = deepchem.feat.RawFeaturizer()
  elif featurizer == "smiles2img":
    img_spec = kwargs.get("img_spec", "std")
    img_size = kwargs.get("img_size", 80)
    featurizer = deepchem.feat.SmilesToImage(
        img_size=img_size, img_spec=img_spec)

  loader = deepchem.data.CSVLoader(
      tasks=MUV_tasks, smiles_field="smiles", featurizer=featurizer)
  dataset = loader.featurize(dataset_file)

  if split == None:
    transformers = [
        deepchem.trans.BalancingTransformer(transform_w=True, dataset=dataset)
    ]

    logger.info("Split is None, about to transform data")
    for transformer in transformers:
      dataset = transformer.transform(dataset)

    return MUV_tasks, (dataset, None, None), transformers

  splitters = {
      'index': deepchem.splits.IndexSplitter(),
      'random': deepchem.splits.RandomSplitter(),
      'scaffold': deepchem.splits.ScaffoldSplitter(),
      'task': deepchem.splits.TaskSplitter(),
      'stratified': deepchem.splits.RandomStratifiedSplitter()
  }
  splitter = splitters[split]
  if split == 'task':
    fold_datasets = splitter.k_fold_split(dataset, K)
    all_dataset = fold_datasets
    logger.info(
        "K-Fold split complete. Use the transformers for this dataset on the returned folds."
    )
    return MUV_tasks, all_dataset, []

  else:
    frac_train = kwargs.get("frac_train", 0.8)
    frac_valid = kwargs.get('frac_valid', 0.1)
    frac_test = kwargs.get('frac_test', 0.1)

    train, valid, test = splitter.train_valid_test_split(
        dataset,
        frac_train=frac_train,
        frac_valid=frac_valid,
        frac_test=frac_test)
    all_dataset = (train, valid, test)
    if reload:
      deepchem.utils.save.save_dataset_to_disk(save_folder, train, valid, test,
                                               transformers)
    return MUV_tasks, all_dataset, transformers
</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/clearance_datasets.py" startline="17" endline="119" pcid="1977">
def load_clearance(featurizer='ECFP',
                   split='random',
                   reload=True,
                   move_mean=True,
                   data_dir=None,
                   save_dir=None,
                   **kwargs):
  """Load clearance datasets."""
  # Featurize clearance dataset
  logger.info("About to featurize clearance dataset.")
  logger.info("About to load clearance dataset.")
  if data_dir is None:
    data_dir = DEFAULT_DIR
  if save_dir is None:
    save_dir = DEFAULT_DIR

  clearance_tasks = ['exp']

  if reload:
    save_folder = os.path.join(save_dir, "clearance-featurized")
    if not move_mean:
      save_folder = os.path.join(save_folder, str(featurizer) + "_mean_unmoved")
    else:
      save_folder = os.path.join(save_folder, str(featurizer))

    if featurizer == "smiles2img":
      img_spec = kwargs.get("img_spec", "std")
      save_folder = os.path.join(save_folder, img_spec)
    save_folder = os.path.join(save_folder, str(split))

    loaded, all_dataset, transformers = deepchem.utils.save.load_dataset_from_disk(
        save_folder)
    if loaded:
      return clearance_tasks, all_dataset, transformers

  dataset_file = os.path.join(data_dir, "clearance.csv")
  if not os.path.exists(dataset_file):
    deepchem.utils.download_url(url=CLEARANCE_URL, dest_dir=data_dir)

  if featurizer == 'ECFP':
    featurizer = deepchem.feat.CircularFingerprint(size=1024)
  elif featurizer == 'GraphConv':
    featurizer = deepchem.feat.ConvMolFeaturizer()
  elif featurizer == 'Weave':
    featurizer = deepchem.feat.WeaveFeaturizer()
  elif featurizer == 'Raw':
    featurizer = deepchem.feat.RawFeaturizer()
  elif featurizer == "smiles2img":
    img_spec = kwargs.get("img_spec", "std")
    img_size = kwargs.get("img_size", 80)
    featurizer = deepchem.feat.SmilesToImage(
        img_size=img_size, img_spec=img_spec)

  loader = deepchem.data.CSVLoader(
      tasks=clearance_tasks, smiles_field="smiles", featurizer=featurizer)
  dataset = loader.featurize(dataset_file, shard_size=8192)

  if split is None:
    # Initialize transformers
    transformers = [
        deepchem.trans.NormalizationTransformer(
            transform_y=True, dataset=dataset, move_mean=move_mean)
    ]

    logger.info("Split is None, about to transform data")
    for transformer in transformers:
      dataset = transformer.transform(dataset)

    return clearance_tasks, (dataset, None, None), transformers

  splitters = {
      'index': deepchem.splits.IndexSplitter(),
      'random': deepchem.splits.RandomSplitter(),
      'scaffold': deepchem.splits.ScaffoldSplitter(),
      'stratified': deepchem.splits.SingletaskStratifiedSplitter()
  }
  splitter = splitters[split]
  logger.info("About to split data with {} splitter.".format(split))
  frac_train = kwargs.get("frac_train", 0.8)
  frac_valid = kwargs.get('frac_valid', 0.1)
  frac_test = kwargs.get('frac_test', 0.1)

  train, valid, test = splitter.train_valid_test_split(
      dataset,
      frac_train=frac_train,
      frac_valid=frac_valid,
      frac_test=frac_test)

  transformers = [
      deepchem.trans.NormalizationTransformer(
          transform_y=True, dataset=train, move_mean=move_mean)
  ]

  logger.info("About to transform data")
  for transformer in transformers:
    train = transformer.transform(train)
    valid = transformer.transform(valid)
    test = transformer.transform(test)

  if reload:
    deepchem.utils.save.save_dataset_to_disk(save_folder, train, valid, test,
                                             transformers)
  return clearance_tasks, (train, valid, test), transformers
</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/toxcast_datasets.py" startline="17" endline="112" pcid="1951">
def load_toxcast(featurizer='ECFP',
                 split='index',
                 reload=True,
                 data_dir=None,
                 save_dir=None,
                 **kwargs):
  if data_dir is None:
    data_dir = DEFAULT_DIR
  if save_dir is None:
    save_dir = DEFAULT_DIR

  if reload:
    save_folder = os.path.join(save_dir, "toxcast-featurized", str(featurizer))
    if featurizer == "smiles2img":
      img_spec = kwargs.get("img_spec", "std")
      save_folder = os.path.join(save_folder, img_spec)
    save_folder = os.path.join(save_folder, str(split))

  dataset_file = os.path.join(data_dir, "toxcast_data.csv.gz")
  if not os.path.exists(dataset_file):
    deepchem.utils.download_url(url=TOXCAST_URL, dest_dir=data_dir)

  dataset = deepchem.utils.save.load_from_disk(dataset_file)
  logger.info("Columns of dataset: %s" % str(dataset.columns.values))
  logger.info("Number of examples in dataset: %s" % str(dataset.shape[0]))
  TOXCAST_tasks = dataset.columns.values[1:].tolist()

  if reload:
    loaded, all_dataset, transformers = deepchem.utils.save.load_dataset_from_disk(
        save_folder)
    if loaded:
      return TOXCAST_tasks, all_dataset, transformers

  # Featurize TOXCAST dataset
  logger.info("About to featurize TOXCAST dataset.")

  if featurizer == 'ECFP':
    featurizer = deepchem.feat.CircularFingerprint(size=1024)
  elif featurizer == 'GraphConv':
    featurizer = deepchem.feat.ConvMolFeaturizer()
  elif featurizer == 'Weave':
    featurizer = deepchem.feat.WeaveFeaturizer()
  elif featurizer == 'Raw':
    featurizer = deepchem.feat.RawFeaturizer()
  elif featurizer == "smiles2img":
    img_spec = kwargs.get("img_spec", "std")
    img_size = kwargs.get("img_size", 80)
    featurizer = deepchem.feat.SmilesToImage(
        img_size=img_size, img_spec=img_spec)

  loader = deepchem.data.CSVLoader(
      tasks=TOXCAST_tasks, smiles_field="smiles", featurizer=featurizer)
  dataset = loader.featurize(dataset_file)

  if split == None:
    transformers = [
        deepchem.trans.BalancingTransformer(transform_w=True, dataset=dataset)
    ]
    logger.info("Split is None, about to transform data.")
    for transformer in transformers:
      dataset = transformer.transform(dataset)
    return TOXCAST_tasks, (dataset, None, None), transformers

  splitters = {
      'index': deepchem.splits.IndexSplitter(),
      'random': deepchem.splits.RandomSplitter(),
      'scaffold': deepchem.splits.ScaffoldSplitter(),
      'stratified': deepchem.splits.RandomStratifiedSplitter()
  }
  splitter = splitters[split]
  logger.info("About to split dataset with {} splitter.".format(split))
  frac_train = kwargs.get("frac_train", 0.8)
  frac_valid = kwargs.get('frac_valid', 0.1)
  frac_test = kwargs.get('frac_test', 0.1)

  train, valid, test = splitter.train_valid_test_split(
      dataset,
      frac_train=frac_train,
      frac_valid=frac_valid,
      frac_test=frac_test)

  transformers = [
      deepchem.trans.BalancingTransformer(transform_w=True, dataset=train)
  ]

  logger.info("About to transform dataset.")
  for transformer in transformers:
    train = transformer.transform(train)
    valid = transformer.transform(valid)
    test = transformer.transform(test)

  if reload:
    deepchem.utils.save.save_dataset_to_disk(save_folder, train, valid, test,
                                             transformers)

  return TOXCAST_tasks, (train, valid, test), transformers
</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/ppb_datasets.py" startline="17" endline="112" pcid="1993">
def load_ppb(featurizer='ECFP',
             split='index',
             reload=True,
             data_dir=None,
             save_dir=None,
             **kwargs):
  """Load PPB datasets."""
  # Featurize PPB dataset
  logger.info("About to featurize PPB dataset.")
  logger.info("About to load PPB dataset.")

  PPB_tasks = ['exp']

  if data_dir is None:
    data_dir = DEFAULT_DIR
  if save_dir is None:
    save_dir = DEFAULT_DIR

  if reload:
    save_folder = os.path.join(save_dir, "ppb-featurized", str(featurizer))
    if featurizer == "smiles2img":
      img_spec = kwargs.get("img_spec", "std")
      save_folder = os.path.join(save_folder, img_spec)
    save_folder = os.path.join(save_folder, str(split))

    loaded, all_dataset, transformers = deepchem.utils.save.load_dataset_from_disk(
        save_folder)
    if loaded:
      return PPB_tasks, all_dataset, transformers

  dataset_file = os.path.join(data_dir, "PPB.csv")
  if not os.path.exists(dataset_file):
    deepchem.utils.download_url(url=PPB_URL, dest_dir=data_dir)

  if featurizer == 'ECFP':
    featurizer = deepchem.feat.CircularFingerprint(size=1024)
  elif featurizer == 'GraphConv':
    featurizer = deepchem.feat.ConvMolFeaturizer()
  elif featurizer == 'Weave':
    featurizer = deepchem.feat.WeaveFeaturizer()
  elif featurizer == 'Raw':
    featurizer = deepchem.feat.RawFeaturizer()
  elif featurizer == "smiles2img":
    img_spec = kwargs.get("img_spec", "std")
    img_size = kwargs.get("img_size", 80)
    featurizer = deepchem.feat.SmilesToImage(
        img_size=img_size, img_spec=img_spec)

  loader = deepchem.data.CSVLoader(
      tasks=PPB_tasks, smiles_field="smiles", featurizer=featurizer)
  dataset = loader.featurize(dataset_file, shard_size=8192)

  if split == None:
    transformers = [
        deepchem.trans.NormalizationTransformer(
            transform_y=True, dataset=dataset)
    ]

    logger.info("Split is None, about to transform data")
    for transformer in transformers:
      dataset = transformer.transform(dataset)

    return PPB_tasks, (dataset, None, None), transformers

  splitters = {
      'index': deepchem.splits.IndexSplitter(),
      'random': deepchem.splits.RandomSplitter(),
      'scaffold': deepchem.splits.ScaffoldSplitter(),
      'stratified': deepchem.splits.SingletaskStratifiedSplitter()
  }
  splitter = splitters[split]
  logger.info("About to split dataset with {} splitter.".format(split))
  frac_train = kwargs.get("frac_train", 0.8)
  frac_valid = kwargs.get('frac_valid', 0.1)
  frac_test = kwargs.get('frac_test', 0.1)

  train, valid, test = splitter.train_valid_test_split(
      dataset,
      frac_train=frac_train,
      frac_valid=frac_valid,
      frac_test=frac_test)

  transformers = [
      deepchem.trans.NormalizationTransformer(transform_y=True, dataset=train)
  ]

  logger.info("About to transform dataset.")
  for transformer in transformers:
    train = transformer.transform(train)
    valid = transformer.transform(valid)
    test = transformer.transform(test)

  if reload:
    deepchem.utils.save.save_dataset_to_disk(save_folder, train, valid, test,
                                             transformers)
  return PPB_tasks, (train, valid, test), transformers
</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/thermosol_datasets.py" startline="36" endline="135" pcid="1953">
def load_thermosol(featurizer="ECFP",
                   data_dir=None,
                   save_dir=None,
                   split=None,
                   split_seed=None,
                   reload=True):
  """Loads the thermodynamic solubility datasets."""
  # Featurizer thermosol dataset
  logger.info("About to featurize thermosol dataset...")
  thermosol_tasks = ["target"]  #Task is solubility in pH 7.4 buffer

  if data_dir is None:
    data_dir = DEFAULT_DATA_DIR
  if save_dir is None:
    save_dir = DEFAULT_DATA_DIR

  if reload:
    save_folder = os.path.join(save_dir, "thermosol-featurized", featurizer)
    if featurizer == "smiles2img":
      img_spec = kwargs.get("img_spec", "std")
      save_folder = os.path.join(save_folder, img_spec)
    save_folder = os.path.join(save_folder, str(split))

    loaded, all_dataset, transformers = deepchem.utils.save.load_dataset_from_disk(
        save_folder)
    if loaded:
      return thermosol_tasks, all_dataset, transformers

  dataset_file = os.path.join(data_dir, "thermosol.csv")
  if not os.path.exists(dataset_file):
    logger.info("{} does not exist. Downloading it.".format(dataset_file))
    deepchem.utils.download_url(url=THERMOSOL_URL, dest_dir=data_dir)

  if featurizer == 'ECFP':
    featurizer = deepchem.feat.CircularFingerprint(size=1024)
  elif featurizer == 'GraphConv':
    featurizer = deepchem.feat.ConvMolFeaturizer()
  elif featurizer == 'Weave':
    featurizer = deepchem.feat.WeaveFeaturizer()
  elif featurizer == 'Raw':
    featurizer = deepchem.feat.RawFeaturizer()
  elif featurizer == 'AdjacencyConv':
    featurizer = deepchem.feat.AdjacencyFingerprint(
        max_n_atoms=150, max_valence=6)
  elif featurizer == "smiles2img":
    img_spec = kwargs.get("img_spec", "std")
    img_size = kwargs.get("img_size", 80)
    featurizer = deepchem.feat.SmilesToImage(
        img_size=img_size, img_spec=img_spec)

  logger.info("Featurizing datasets.")
  loader = deepchem.data.CSVLoader(
      tasks=thermosol_tasks, smiles_field='smile', featurizer=featurizer)
  dataset = loader.featurize(input_files=[dataset_file], shard_size=2000)

  logger.info("Removing missing entries...")
  remove_missing_entries(dataset)

  if split == None:
    logger.info("About to transform the data...")
    transformers = []
    for transformer in transformers:
      logger.info("Transforming the dataset with transformer ",
                  transformer.__class__.__name__)
      dataset = transformer.transform(dataset)
    return thermosol_tasks, (dataset, None, None), transformers

  splitters = {
      'index': deepchem.splits.IndexSplitter(),
      'random': deepchem.splits.RandomSplitter(),
      'scaffold': deepchem.splits.ScaffoldSplitter(),
      'butina': deepchem.splits.ButinaSplitter(),
      'stratified': deepchem.splits.SingletaskStratifiedSplitter()
  }
  splitter = splitters[split]
  frac_train = kwargs.get("frac_train", 0.8)
  frac_valid = kwargs.get('frac_valid', 0.1)
  frac_test = kwargs.get('frac_test', 0.1)

  train, valid, test = splitter.train_valid_test_split(
      dataset,
      frac_train=frac_train,
      frac_valid=frac_valid,
      frac_test=frac_test,
      seed=split_seed)
  transformers = []

  logger.info("About to transform the data...")
  for transformer in transformers:
    logger.info("Transforming the data with transformer ",
                transformer.__class__.__name__)
    train = transformer.transform(train)
    valid = transformer.transform(valid)
    test = transformer.transform(test)

  if reload:
    logger.info("Saving file to {}.".format(save_folder))
    deepchem.utils.save.save_dataset_to_disk(save_folder, train, valid, test,
                                             transformers)
  return thermosol_tasks, (train, valid, test), transformers
</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/bace_datasets.py" startline="18" endline="124" pcid="1964">
def load_bace_regression(featurizer='ECFP',
                         split='random',
                         reload=True,
                         move_mean=True,
                         data_dir=None,
                         save_dir=None,
                         **kwargs):
  """Load bace datasets."""
  # Featurize bace dataset
  logger.info("About to featurize bace dataset.")
  if data_dir is None:
    data_dir = DEFAULT_DIR
  if save_dir is None:
    save_dir = DEFAULT_DIR

  bace_tasks = ["pIC50"]

  if reload:
    save_folder = os.path.join(save_dir, "bace_r-featurized")
    if not move_mean:
      save_folder = os.path.join(save_folder, str(featurizer) + "_mean_unmoved")
    else:
      save_folder = os.path.join(save_folder, str(featurizer))

    if featurizer == "smiles2img":
      img_spec = kwargs.get("img_spec", "std")
      save_folder = os.path.join(save_folder, img_spec)
    save_folder = os.path.join(save_folder, str(split))

    loaded, all_dataset, transformers = deepchem.utils.save.load_dataset_from_disk(
        save_folder)
    if loaded:
      return bace_tasks, all_dataset, transformers

  dataset_file = os.path.join(data_dir, "bace.csv")
  if not os.path.exists(dataset_file):
    deepchem.utils.download_url(url=BACE_URL, dest_dir=data_dir)

  if featurizer == 'ECFP':
    featurizer = deepchem.feat.CircularFingerprint(size=1024)
  elif featurizer == 'GraphConv':
    featurizer = deepchem.feat.ConvMolFeaturizer()
  elif featurizer == 'Weave':
    featurizer = deepchem.feat.WeaveFeaturizer()
  elif featurizer == 'Raw':
    featurizer = deepchem.feat.RawFeaturizer()
  elif featurizer == 'UserDefined':
    featurizer = deepchem.feat.UserDefinedFeaturizer(
        bace_user_specified_features)
  elif featurizer == "smiles2img":
    img_spec = kwargs.get("img_spec", "std")
    img_size = kwargs.get("img_size", 80)
    featurizer = deepchem.feat.SmilesToImage(
        img_size=img_size, img_spec=img_spec)

  loader = deepchem.data.CSVLoader(
      tasks=bace_tasks, smiles_field="mol", featurizer=featurizer)

  dataset = loader.featurize(dataset_file, shard_size=8192)
  if split is None:
    # Initialize transformers
    transformers = [
        deepchem.trans.NormalizationTransformer(
            transform_y=True, dataset=dataset, move_mean=move_mean)
    ]

    logger.info("Split is None, about to transform data")
    for transformer in transformers:
      dataset = transformer.transform(dataset)

    return bace_tasks, (dataset, None, None), transformers

  splitters = {
      'index': deepchem.splits.IndexSplitter(),
      'random': deepchem.splits.RandomSplitter(),
      'scaffold': deepchem.splits.ScaffoldSplitter(),
      'stratified': deepchem.splits.SingletaskStratifiedSplitter()
  }
  splitter = splitters[split]
  logger.info("About to split data using {} splitter".format(split))
  frac_train = kwargs.get("frac_train", 0.8)
  frac_valid = kwargs.get('frac_valid', 0.1)
  frac_test = kwargs.get('frac_test', 0.1)

  train, valid, test = splitter.train_valid_test_split(
      dataset,
      frac_train=frac_train,
      frac_valid=frac_valid,
      frac_test=frac_test)

  transformers = [
      deepchem.trans.NormalizationTransformer(
          transform_y=True, dataset=train, move_mean=move_mean)
  ]

  logger.info("About to transform data.")
  for transformer in transformers:
    train = transformer.transform(train)
    valid = transformer.transform(valid)
    test = transformer.transform(test)

  if reload:
    deepchem.utils.save.save_dataset_to_disk(save_folder, train, valid, test,
                                             transformers)
  return bace_tasks, (train, valid, test), transformers


</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/tox21_datasets.py" startline="17" endline="123" pcid="1998">
def load_tox21(featurizer='ECFP',
               split='index',
               reload=True,
               K=4,
               data_dir=None,
               save_dir=None,
               **kwargs):
  """Load Tox21 datasets. Does not do train/test split"""
  # Featurize Tox21 dataset

  tox21_tasks = [
      'NR-AR', 'NR-AR-LBD', 'NR-AhR', 'NR-Aromatase', 'NR-ER', 'NR-ER-LBD',
      'NR-PPAR-gamma', 'SR-ARE', 'SR-ATAD5', 'SR-HSE', 'SR-MMP', 'SR-p53'
  ]

  if data_dir is None:
    data_dir = DEFAULT_DIR
  if save_dir is None:
    save_dir = DEFAULT_DIR

  if reload:
    save_folder = os.path.join(save_dir, "tox21-featurized", str(featurizer))
    if featurizer == "smiles2img":
      img_spec = kwargs.get("img_spec", "std")
      save_folder = os.path.join(save_folder, img_spec)
    save_folder = os.path.join(save_folder, str(split))

    loaded, all_dataset, transformers = deepchem.utils.save.load_dataset_from_disk(
        save_folder)
    if loaded:
      return tox21_tasks, all_dataset, transformers

  dataset_file = os.path.join(data_dir, "tox21.csv.gz")
  if not os.path.exists(dataset_file):
    deepchem.utils.download_url(url=TOX21_URL, dest_dir=data_dir)

  if featurizer == 'ECFP':
    featurizer = deepchem.feat.CircularFingerprint(size=1024)
  elif featurizer == 'GraphConv':
    featurizer = deepchem.feat.ConvMolFeaturizer()
  elif featurizer == 'Weave':
    featurizer = deepchem.feat.WeaveFeaturizer()
  elif featurizer == 'Raw':
    featurizer = deepchem.feat.RawFeaturizer()
  elif featurizer == 'AdjacencyConv':
    featurizer = deepchem.feat.AdjacencyFingerprint(
        max_n_atoms=150, max_valence=6)
  elif featurizer == "smiles2img":
    img_size = kwargs.get("img_size", 80)
    img_spec = kwargs.get("img_spec", "std")
    featurizer = deepchem.feat.SmilesToImage(
        img_size=img_size, img_spec=img_spec)

  loader = deepchem.data.CSVLoader(
      tasks=tox21_tasks, smiles_field="smiles", featurizer=featurizer)
  dataset = loader.featurize(dataset_file, shard_size=8192)

  if split == None:
    # Initialize transformers
    transformers = [
        deepchem.trans.BalancingTransformer(transform_w=True, dataset=dataset)
    ]

    logger.info("About to transform data")
    for transformer in transformers:
      dataset = transformer.transform(dataset)

    return tox21_tasks, (dataset, None, None), transformers

  splitters = {
      'index': deepchem.splits.IndexSplitter(),
      'random': deepchem.splits.RandomSplitter(),
      'scaffold': deepchem.splits.ScaffoldSplitter(),
      'butina': deepchem.splits.ButinaSplitter(),
      'task': deepchem.splits.TaskSplitter(),
      'stratified': deepchem.splits.RandomStratifiedSplitter()
  }
  splitter = splitters[split]
  if split == 'task':
    fold_datasets = splitter.k_fold_split(dataset, K)
    all_dataset = fold_datasets
  else:
    frac_train = kwargs.get("frac_train", 0.8)
    frac_valid = kwargs.get('frac_valid', 0.1)
    frac_test = kwargs.get('frac_test', 0.1)

    train, valid, test = splitter.train_valid_test_split(
        dataset,
        frac_train=frac_train,
        frac_valid=frac_valid,
        frac_test=frac_test)
    all_dataset = (train, valid, test)

    transformers = [
        deepchem.trans.BalancingTransformer(transform_w=True, dataset=train)
    ]

    logger.info("About to transform data")
    for transformer in transformers:
      train = transformer.transform(train)
      valid = transformer.transform(valid)
      test = transformer.transform(test)

    if reload:
      deepchem.utils.save.save_dataset_to_disk(save_folder, train, valid, test,
                                               transformers)
  return tox21_tasks, all_dataset, transformers
</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/pcba_datasets.py" startline="65" endline="172" pcid="1974">
def load_pcba_dataset(featurizer='ECFP',
                      split='random',
                      reload=True,
                      assay_file_name="pcba.csv.gz",
                      data_dir=None,
                      save_dir=None,
                      **kwargs):
  """Load PCBA datasets. Does not do train/test split"""
  if data_dir is None:
    data_dir = DEFAULT_DIR
  if save_dir is None:
    save_dir = DEFAULT_DIR

  if reload:
    save_folder = os.path.join(save_dir,
                               assay_file_name.split(".")[0] + "-featurized",
                               featurizer)
    if featurizer == "smiles2img":
      img_spec = kwargs.get("img_spec", "std")
      save_folder = os.path.join(save_folder, img_spec)
    save_folder = os.path.join(save_folder, str(split))

  dataset_file = os.path.join(data_dir, assay_file_name)

  if not os.path.exists(dataset_file):
    deepchem.utils.download_url(
        url="http://deepchem.io.s3-website-us-west-1.amazonaws.com/datasets/{0}".
        format(assay_file_name),
        dest_dir=data_dir)

  # Featurize PCBA dataset
  logger.info("About to featurize PCBA dataset.")
  if featurizer == 'ECFP':
    featurizer = deepchem.feat.CircularFingerprint(size=1024)
  elif featurizer == 'GraphConv':
    featurizer = deepchem.feat.ConvMolFeaturizer()
  elif featurizer == 'Weave':
    featurizer = deepchem.feat.WeaveFeaturizer()
  elif featurizer == 'Raw':
    featurizer = deepchem.feat.RawFeaturizer()
  elif featurizer == "smiles2img":
    img_spec = kwargs.get("img_spec", "std")
    img_size = kwargs.get("img_size", 80)
    featurizer = deepchem.feat.SmilesToImage(
        img_size=img_size, img_spec=img_spec)

  with gzip.GzipFile(dataset_file, "r") as fin:
    header = fin.readline().rstrip().decode("utf-8")
    columns = header.split(",")
    columns.remove("mol_id")
    columns.remove("smiles")
    PCBA_tasks = columns

  if reload:
    loaded, all_dataset, transformers = deepchem.utils.save.load_dataset_from_disk(
        save_folder)
    if loaded:
      return PCBA_tasks, all_dataset, transformers

  loader = deepchem.data.CSVLoader(
      tasks=PCBA_tasks, smiles_field="smiles", featurizer=featurizer)

  dataset = loader.featurize(dataset_file)

  if split == None:
    transformers = [
        deepchem.trans.BalancingTransformer(transform_w=True, dataset=dataset)
    ]

    logger.info("Split is None, about to transform data")
    for transformer in transformers:
      dataset = transformer.transform(dataset)

    return PCBA_tasks, (dataset, None, None), transformers

  splitters = {
      'index': deepchem.splits.IndexSplitter(),
      'random': deepchem.splits.RandomSplitter(),
      'scaffold': deepchem.splits.ScaffoldSplitter(),
      'stratified': deepchem.splits.SingletaskStratifiedSplitter()
  }
  splitter = splitters[split]
  logger.info("About to split dataset using {} splitter.".format(split))
  frac_train = kwargs.get("frac_train", 0.8)
  frac_valid = kwargs.get('frac_valid', 0.1)
  frac_test = kwargs.get('frac_test', 0.1)

  train, valid, test = splitter.train_valid_test_split(
      dataset,
      frac_train=frac_train,
      frac_valid=frac_valid,
      frac_test=frac_test)

  transformers = [
      deepchem.trans.BalancingTransformer(transform_w=True, dataset=train)
  ]

  logger.info("About to transform dataset.")
  for transformer in transformers:
    train = transformer.transform(train)
    valid = transformer.transform(valid)
    test = transformer.transform(test)

  if reload:
    deepchem.utils.save.save_dataset_to_disk(save_folder, train, valid, test,
                                             transformers)

  return PCBA_tasks, (train, valid, test), transformers
</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/hppb_datasets.py" startline="36" endline="135" pcid="1981">
def load_hppb(featurizer="ECFP",
              data_dir=None,
              save_dir=None,
              split=None,
              split_seed=None,
              reload=True,
              **kwargs):
  """Loads the thermodynamic solubility datasets."""
  # Featurizer hppb dataset
  logger.info("About to featurize hppb dataset...")
  hppb_tasks = ["target"]  #Task is solubility in pH 7.4 buffer

  if data_dir is None:
    data_dir = DEFAULT_DATA_DIR
  if save_dir is None:
    save_dir = DEFAULT_DATA_DIR

  if reload:
    save_folder = os.path.join(save_dir, "hppb-featurized", str(featurizer))
    if featurizer == "smiles2img":
      img_spec = kwargs.get("img_spec", "std")
      save_folder = os.path.join(save_folder, img_spec)
    save_folder = os.path.join(save_folder, str(split))

    loaded, all_dataset, transformers = deepchem.utils.save.load_dataset_from_disk(
        save_folder)
    if loaded:
      return hppb_tasks, all_dataset, transformers

  dataset_file = os.path.join(data_dir, "hppb.csv")
  if not os.path.exists(dataset_file):
    logger.info("{} does not exist. Downloading it.".format(dataset_file))
    deepchem.utils.download_url(url=hppb_URL, dest_dir=data_dir)

  if featurizer == 'ECFP':
    featurizer = deepchem.feat.CircularFingerprint(size=1024)
  elif featurizer == 'GraphConv':
    featurizer = deepchem.feat.ConvMolFeaturizer()
  elif featurizer == 'Weave':
    featurizer = deepchem.feat.WeaveFeaturizer()
  elif featurizer == 'Raw':
    featurizer = deepchem.feat.RawFeaturizer()
  elif featurizer == 'AdjacencyConv':
    featurizer = deepchem.feat.AdjacencyFingerprint(
        max_n_atoms=150, max_valence=6)
  elif featurizer == "smiles2img":
    img_spec = kwargs.get("img_spec", "std")
    img_size = kwargs.get("img_size", 80)
    featurizer = deepchem.feat.SmilesToImage(
        img_size=img_size, img_spec=img_spec)

  logger.info("Featurizing datasets.")
  loader = deepchem.data.CSVLoader(
      tasks=hppb_tasks, smiles_field='smile', featurizer=featurizer)
  dataset = loader.featurize(input_files=[dataset_file], shard_size=2000)

  logger.info("Removing missing entries...")
  remove_missing_entries(dataset)

  if split == None:
    logger.info("About to transform the data...")
    transformers = []
    for transformer in transformers:
      logger.info("Transforming the dataset with transformer ",
                  transformer.__class__.__name__)
      dataset = transformer.transform(dataset)
    return hppb_tasks, (dataset, None, None), transformers

  splitters = {
      'index': deepchem.splits.IndexSplitter(),
      'random': deepchem.splits.RandomSplitter(),
      'scaffold': deepchem.splits.ScaffoldSplitter(),
      'butina': deepchem.splits.ButinaSplitter(),
      'stratified': deepchem.splits.SingletaskStratifiedSplitter()
  }
  splitter = splitters[split]
  frac_train = kwargs.get("frac_train", 0.8)
  frac_valid = kwargs.get('frac_valid', 0.1)
  frac_test = kwargs.get('frac_test', 0.1)

  train, valid, test = splitter.train_valid_test_split(
      dataset,
      frac_train=frac_train,
      frac_valid=frac_valid,
      frac_test=frac_test)
  transformers = []

  logger.info("About to transform the data...")
  for transformer in transformers:
    logger.info("Transforming the data with transformer ",
                transformer.__class__.__name__)
    train = transformer.transform(train)
    valid = transformer.transform(valid)
    test = transformer.transform(test)

  if reload:
    logger.info("Saving file to {}.".format(save_folder))
    deepchem.utils.save.save_dataset_to_disk(save_folder, train, valid, test,
                                             transformers)
  return hppb_tasks, (train, valid, test), transformers
</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/qm7_datasets.py" startline="22" endline="132" pcid="1968">
def load_qm7_from_mat(featurizer='CoulombMatrix',
                      split='stratified',
                      reload=True,
                      move_mean=True,
                      data_dir=None,
                      save_dir=None,
                      **kwargs):

  qm7_tasks = ["u0_atom"]

  if data_dir is None:
    data_dir = DEFAULT_DIR
  if save_dir is None:
    save_dir = DEFAULT_DIR

  if reload:
    save_folder = os.path.join(save_dir, "qm7-featurized")
    if not move_mean:
      save_folder = os.path.join(save_folder, str(featurizer) + "_mean_unmoved")
    else:
      save_folder = os.path.join(save_folder, str(featurizer))

    if featurizer == "smiles2img":
      img_spec = kwargs.get("img_spec", "std")
      save_folder = os.path.join(save_folder, img_spec)
    save_folder = os.path.join(save_folder, str(split))

    loaded, all_dataset, transformers = deepchem.utils.save.load_dataset_from_disk(
        save_folder)
    if loaded:
      return qm7_tasks, all_dataset, transformers

  if featurizer == 'CoulombMatrix':
    dataset_file = os.path.join(data_dir, "qm7.mat")

    if not os.path.exists(dataset_file):
      deepchem.utils.download_url(url=QM7_MAT_URL, dest_dir=data_dir)

    dataset = scipy.io.loadmat(dataset_file)
    X = dataset['X']
    y = dataset['T'].T
    w = np.ones_like(y)
    dataset = deepchem.data.DiskDataset.from_numpy(X, y, w, ids=None)
  elif featurizer == 'BPSymmetryFunctionInput':
    dataset_file = os.path.join(data_dir, "qm7.mat")

    if not os.path.exists(dataset_file):
      deepchem.utils.download_url(url=QM7_MAT_URL, dest_dir=data_dir)
    dataset = scipy.io.loadmat(dataset_file)
    X = np.concatenate([np.expand_dims(dataset['Z'], 2), dataset['R']], axis=2)
    y = dataset['T'].reshape(-1, 1)  # scipy.io.loadmat puts samples on axis 1
    w = np.ones_like(y)
    dataset = deepchem.data.DiskDataset.from_numpy(X, y, w, ids=None)
  else:
    dataset_file = os.path.join(data_dir, "qm7.csv")
    if not os.path.exists(dataset_file):
      deepchem.utils.download_url(url=QM7_CSV_URL, dest_dir=data_dir)
    if featurizer == 'ECFP':
      featurizer = deepchem.feat.CircularFingerprint(size=1024)
    elif featurizer == 'GraphConv':
      featurizer = deepchem.feat.ConvMolFeaturizer()
    elif featurizer == 'Weave':
      featurizer = deepchem.feat.WeaveFeaturizer()
    elif featurizer == 'Raw':
      featurizer = deepchem.feat.RawFeaturizer()
    elif featurizer == "smiles2img":
      img_spec = kwargs.get("img_spec", "std")
      img_size = kwargs.get("img_size", 80)
      featurizer = deepchem.feat.SmilesToImage(
          img_size=img_size, img_spec=img_spec)
    loader = deepchem.data.CSVLoader(
        tasks=qm7_tasks, smiles_field="smiles", featurizer=featurizer)
    dataset = loader.featurize(dataset_file)

  if split == None:
    raise ValueError()
  else:
    splitters = {
        'index': deepchem.splits.IndexSplitter(),
        'random': deepchem.splits.RandomSplitter(),
        'stratified':
        deepchem.splits.SingletaskStratifiedSplitter(task_number=0)
    }

    splitter = splitters[split]
    frac_train = kwargs.get("frac_train", 0.8)
    frac_valid = kwargs.get('frac_valid', 0.1)
    frac_test = kwargs.get('frac_test', 0.1)

    train_dataset, valid_dataset, test_dataset = splitter.train_valid_test_split(
        dataset,
        frac_train=frac_train,
        frac_valid=frac_valid,
        frac_test=frac_test)

    transformers = [
        deepchem.trans.NormalizationTransformer(
            transform_y=True, dataset=train_dataset, move_mean=move_mean)
    ]

    for transformer in transformers:
      train_dataset = transformer.transform(train_dataset)
      valid_dataset = transformer.transform(valid_dataset)
      test_dataset = transformer.transform(test_dataset)
    if reload:
      deepchem.utils.save.save_dataset_to_disk(
          save_folder, train_dataset, valid_dataset, test_dataset, transformers)

    return qm7_tasks, (train_dataset, valid_dataset, test_dataset), transformers


</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/qm9_datasets.py" startline="18" endline="128" pcid="1954">
def load_qm9(featurizer='CoulombMatrix',
             split='random',
             reload=True,
             move_mean=True,
             data_dir=None,
             save_dir=None,
             **kwargs):
  """Load qm9 datasets."""
  # Featurize qm9 dataset
  logger.info("About to featurize qm9 dataset.")
  qm9_tasks = [
      "mu", "alpha", "homo", "lumo", "gap", "r2", "zpve", "cv", "u0", "u298",
      "h298", "g298"
  ]

  if data_dir is None:
    data_dir = DEFAULT_DIR
  if save_dir is None:
    save_dir = DEFAULT_DIR

  if reload:
    save_folder = os.path.join(save_dir, "qm9-featurized")
    if not move_mean:
      save_folder = os.path.join(save_folder, str(featurizer) + "_mean_unmoved")
    else:
      save_folder = os.path.join(save_folder, str(featurizer))

    if featurizer == "smiles2img":
      img_spec = kwargs.get("img_spec", "std")
      save_folder = os.path.join(save_folder, img_spec)
    save_folder = os.path.join(save_folder, str(split))

    loaded, all_dataset, transformers = deepchem.utils.save.load_dataset_from_disk(
        save_folder)
    if loaded:
      return qm7_tasks, all_dataset, transformers

  if featurizer in ['CoulombMatrix', 'BPSymmetryFunctionInput', 'MP', 'Raw']:
    dataset_file = os.path.join(data_dir, "gdb9.sdf")

    if not os.path.exists(dataset_file):
      deepchem.utils.download_url(url=GDB9_URL, dest_dir=data_dir)
      deepchem.utils.untargz_file(
          os.path.join(data_dir, 'gdb9.tar.gz'), data_dir)
  else:
    dataset_file = os.path.join(data_dir, "qm9.csv")
    if not os.path.exists(dataset_file):
      deepchem.utils.download_url(url=QM9_CSV_URL, dest_dir=data_dir)

  if featurizer in ['CoulombMatrix', 'BPSymmetryFunctionInput', 'MP', 'Raw']:
    if featurizer == 'CoulombMatrix':
      featurizer = deepchem.feat.CoulombMatrix(29)
    elif featurizer == 'BPSymmetryFunctionInput':
      featurizer = deepchem.feat.BPSymmetryFunctionInput(29)
    elif featurizer == 'Raw':
      featurizer = deepchem.feat.RawFeaturizer()
    elif featurizer == 'MP':
      featurizer = deepchem.feat.WeaveFeaturizer(
          graph_distance=False, explicit_H=True)
    loader = deepchem.data.SDFLoader(
        tasks=qm9_tasks,
        smiles_field="smiles",
        mol_field="mol",
        featurizer=featurizer)
  else:
    if featurizer == 'ECFP':
      featurizer = deepchem.feat.CircularFingerprint(size=1024)
    elif featurizer == 'GraphConv':
      featurizer = deepchem.feat.ConvMolFeaturizer()
    elif featurizer == 'Weave':
      featurizer = deepchem.feat.WeaveFeaturizer()
    elif featurizer == "smiles2img":
      img_spec = kwargs.get("img_spec", "std")
      img_size = kwargs.get("img_size", 80)
      featurizer = deepchem.feat.SmilesToImage(
          img_size=img_size, img_spec=img_spec)
    loader = deepchem.data.CSVLoader(
        tasks=qm9_tasks, smiles_field="smiles", featurizer=featurizer)

  dataset = loader.featurize(dataset_file)
  if split == None:
    raise ValueError()

  splitters = {
      'index': deepchem.splits.IndexSplitter(),
      'random': deepchem.splits.RandomSplitter(),
      'stratified': deepchem.splits.SingletaskStratifiedSplitter(task_number=11)
  }
  splitter = splitters[split]
  frac_train = kwargs.get("frac_train", 0.8)
  frac_valid = kwargs.get('frac_valid', 0.1)
  frac_test = kwargs.get('frac_test', 0.1)

  train_dataset, valid_dataset, test_dataset = splitter.train_valid_test_split(
      dataset,
      frac_train=frac_train,
      frac_valid=frac_valid,
      frac_test=frac_test)
  transformers = [
      deepchem.trans.NormalizationTransformer(
          transform_y=True, dataset=train_dataset, move_mean=move_mean)
  ]
  for transformer in transformers:
    train_dataset = transformer.transform(train_dataset)
    valid_dataset = transformer.transform(valid_dataset)
    test_dataset = transformer.transform(test_dataset)

  if reload:
    deepchem.utils.save.save_dataset_to_disk(
        save_folder, train_dataset, valid_dataset, test_dataset, transformers)
  return qm9_tasks, (train_dataset, valid_dataset, test_dataset), transformers
</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/qm8_datasets.py" startline="18" endline="124" pcid="1987">
def load_qm8(featurizer='CoulombMatrix',
             split='random',
             reload=True,
             move_mean=True,
             data_dir=None,
             save_dir=None,
             **kwargs):
  qm8_tasks = [
      "E1-CC2", "E2-CC2", "f1-CC2", "f2-CC2", "E1-PBE0", "E2-PBE0", "f1-PBE0",
      "f2-PBE0", "E1-PBE0", "E2-PBE0", "f1-PBE0", "f2-PBE0", "E1-CAM", "E2-CAM",
      "f1-CAM", "f2-CAM"
  ]

  if data_dir is None:
    data_dir = DEFAULT_DIR
  if save_dir is None:
    save_dir = DEFAULT_DIR

  if reload:
    save_folder = os.path.join(save_dir, "qm8-featurized")
    if not move_mean:
      save_folder = os.path.join(save_folder, str(featurizer) + "_mean_unmoved")
    else:
      save_folder = os.path.join(save_folder, str(featurizer))

    if featurizer == "smiles2img":
      img_spec = kwargs.get("img_spec", "std")
      save_folder = os.path.join(save_folder, img_spec)
    save_folder = os.path.join(save_folder, str(split))

    loaded, all_dataset, transformers = deepchem.utils.save.load_dataset_from_disk(
        save_folder)
    if loaded:
      return qm8_tasks, all_dataset, transformers

  if featurizer in ['CoulombMatrix', 'BPSymmetryFunctionInput', 'MP', 'Raw']:
    dataset_file = os.path.join(data_dir, "qm8.sdf")
    if not os.path.exists(dataset_file):
      deepchem.utils.download_url(url=GDB8_URL, dest_dir=data_dir)
      deepchem.utils.untargz_file(
          os.path.join(data_dir, 'gdb8.tar.gz'), data_dir)
  else:
    dataset_file = os.path.join(data_dir, "qm8.csv")
    if not os.path.exists(dataset_file):
      deepchem.utils.download_url(url=QM8_CSV_URL, dest_dir=data_dir)

  if featurizer in ['CoulombMatrix', 'BPSymmetryFunctionInput', 'MP', 'Raw']:
    if featurizer == 'CoulombMatrix':
      featurizer = deepchem.feat.CoulombMatrix(26)
    elif featurizer == 'BPSymmetryFunctionInput':
      featurizer = deepchem.feat.BPSymmetryFunctionInput(26)
    elif featurizer == 'Raw':
      featurizer = deepchem.feat.RawFeaturizer()
    elif featurizer == 'MP':
      featurizer = deepchem.feat.WeaveFeaturizer(
          graph_distance=False, explicit_H=True)
    loader = deepchem.data.SDFLoader(
        tasks=qm8_tasks,
        smiles_field="smiles",
        mol_field="mol",
        featurizer=featurizer)
  else:
    if featurizer == 'ECFP':
      featurizer = deepchem.feat.CircularFingerprint(size=1024)
    elif featurizer == 'GraphConv':
      featurizer = deepchem.feat.ConvMolFeaturizer()
    elif featurizer == 'Weave':
      featurizer = deepchem.feat.WeaveFeaturizer()
    elif featurizer == "smiles2img":
      img_spec = kwargs.get("img_spec", "std")
      img_size = kwargs.get("img_size", 80)
      featurizer = deepchem.feat.SmilesToImage(
          img_size=img_size, img_spec=img_spec)
    loader = deepchem.data.CSVLoader(
        tasks=qm8_tasks, smiles_field="smiles", featurizer=featurizer)
  dataset = loader.featurize(dataset_file)

  if split == None:
    raise ValueError()

  splitters = {
      'index': deepchem.splits.IndexSplitter(),
      'random': deepchem.splits.RandomSplitter(),
      'stratified': deepchem.splits.SingletaskStratifiedSplitter(task_number=0)
  }
  splitter = splitters[split]
  frac_train = kwargs.get("frac_train", 0.8)
  frac_valid = kwargs.get('frac_valid', 0.1)
  frac_test = kwargs.get('frac_test', 0.1)

  train_dataset, valid_dataset, test_dataset = splitter.train_valid_test_split(
      dataset,
      frac_train=frac_train,
      frac_valid=frac_valid,
      frac_test=frac_test)
  transformers = [
      deepchem.trans.NormalizationTransformer(
          transform_y=True, dataset=train_dataset, move_mean=move_mean)
  ]
  for transformer in transformers:
    train_dataset = transformer.transform(train_dataset)
    valid_dataset = transformer.transform(valid_dataset)
    test_dataset = transformer.transform(test_dataset)
  if reload:
    deepchem.utils.save.save_dataset_to_disk(
        save_folder, train_dataset, valid_dataset, test_dataset, transformers)
  return qm8_tasks, (train_dataset, valid_dataset, test_dataset), transformers
</source>
</class>

<class classid="110" nclones="9" nlines="10" similarity="80">
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/thermosol_datasets.py" startline="19" endline="35" pcid="1952">
def remove_missing_entries(dataset):
  """Remove missing entries.

  Some of the datasets have missing entries that sneak in as zero'd out
  feature vectors. Get rid of them.
  """
  for i, (X, y, w, ids) in enumerate(dataset.itershards()):
    available_rows = X.any(axis=1)
    logger.info("Shard %d has %d missing entries." %
                (i, np.count_nonzero(~available_rows)))
    X = X[available_rows]
    y = y[available_rows]
    w = w[available_rows]
    ids = ids[available_rows]
    dataset.set_shard(i, X, y, w, ids)


</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/uv_datasets.py" startline="28" endline="44" pcid="1988">
def remove_missing_entries(dataset):
  """Remove missing entries.

  Some of the datasets have missing entries that sneak in as zero'd out
  feature vectors. Get rid of them.
  """
  for i, (X, y, w, ids) in enumerate(dataset.itershards()):
    available_rows = X.any(axis=1)
    logger.info("Shard %d has %d missing entries." %
                (i, np.count_nonzero(~available_rows)))
    X = X[available_rows]
    y = y[available_rows]
    w = w[available_rows]
    ids = ids[available_rows]
    dataset.set_shard(i, X, y, w, ids)


</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/factors_datasets.py" startline="27" endline="43" pcid="1956">
def remove_missing_entries(dataset):
  """Remove missing entries.

  Some of the datasets have missing entries that sneak in as zero'd out
  feature vectors. Get rid of them.
  """
  for i, (X, y, w, ids) in enumerate(dataset.itershards()):
    available_rows = X.any(axis=1)
    logger.info("Shard %d has %d missing entries." %
                (i, np.count_nonzero(~available_rows)))
    X = X[available_rows]
    y = y[available_rows]
    w = w[available_rows]
    ids = ids[available_rows]
    dataset.set_shard(i, X, y, w, ids)


</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/kaggle_datasets.py" startline="18" endline="34" pcid="1983">
def remove_missing_entries(dataset):
  """Remove missing entries.

  Some of the datasets have missing entries that sneak in as zero'd out
  feature vectors. Get rid of them.
  """
  for i, (X, y, w, ids) in enumerate(dataset.itershards()):
    available_rows = X.any(axis=1)
    logger.info("Shard %d has %d missing entries." %
                (i, np.count_nonzero(~available_rows)))
    X = X[available_rows]
    y = y[available_rows]
    w = w[available_rows]
    ids = ids[available_rows]
    dataset.set_shard(i, X, y, w, ids)


</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/kinase_datasets.py" startline="27" endline="43" pcid="1994">
def remove_missing_entries(dataset):
  """Remove missing entries.

  Some of the datasets have missing entries that sneak in as zero'd out
  feature vectors. Get rid of them.
  """
  for i, (X, y, w, ids) in enumerate(dataset.itershards()):
    available_rows = X.any(axis=1)
    logger.info("Shard %d has %d missing entries." %
                (i, np.count_nonzero(~available_rows)))
    X = X[available_rows]
    y = y[available_rows]
    w = w[available_rows]
    ids = ids[available_rows]
    dataset.set_shard(i, X, y, w, ids)


</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/hppb_datasets.py" startline="19" endline="35" pcid="1980">
def remove_missing_entries(dataset):
  """Remove missing entries.

  Some of the datasets have missing entries that sneak in as zero'd out
  feature vectors. Get rid of them.
  """
  for i, (X, y, w, ids) in enumerate(dataset.itershards()):
    available_rows = X.any(axis=1)
    logger.info("Shard %d has %d missing entries." %
                (i, np.count_nonzero(~available_rows)))
    X = X[available_rows]
    y = y[available_rows]
    w = w[available_rows]
    ids = ids[available_rows]
    dataset.set_shard(i, X, y, w, ids)


</source>
<source file="systems/deepchem-2.3.0/examples/factors/FACTORS_datasets.py" startline="15" endline="30" pcid="2174">
def remove_missing_entries(dataset):
  """Remove missing entries.

  Some of the datasets have missing entries that sneak in as zero'd out
  feature vectors. Get rid of them.
  """
  for i, (X, y, w, ids) in enumerate(dataset.itershards()):
    available_rows = X.any(axis=1)
    print("Shard %d has %d missing entries."
        % (i, np.count_nonzero(~available_rows)))
    X = X[available_rows]
    y = y[available_rows]
    w = w[available_rows]
    ids = ids[available_rows]
    dataset.set_shard(i, X, y, w, ids)

</source>
<source file="systems/deepchem-2.3.0/examples/uv/UV_datasets.py" startline="15" endline="30" pcid="2154">
def remove_missing_entries(dataset):
  """Remove missing entries.

  Some of the datasets have missing entries that sneak in as zero'd out
  feature vectors. Get rid of them.
  """
  for i, (X, y, w, ids) in enumerate(dataset.itershards()):
    available_rows = X.any(axis=1)
    print("Shard %d has %d missing entries."
        % (i, np.count_nonzero(~available_rows)))
    X = X[available_rows]
    y = y[available_rows]
    w = w[available_rows]
    ids = ids[available_rows]
    dataset.set_shard(i, X, y, w, ids)

</source>
<source file="systems/deepchem-2.3.0/examples/kinase/KINASE_datasets.py" startline="15" endline="30" pcid="2147">
def remove_missing_entries(dataset):
  """Remove missing entries.

  Some of the datasets have missing entries that sneak in as zero'd out
  feature vectors. Get rid of them.
  """
  for i, (X, y, w, ids) in enumerate(dataset.itershards()):
    available_rows = X.any(axis=1)
    print("Shard %d has %d missing entries."
        % (i, np.count_nonzero(~available_rows)))
    X = X[available_rows]
    y = y[available_rows]
    w = w[available_rows]
    ids = ids[available_rows]
    dataset.set_shard(i, X, y, w, ids)

</source>
</class>

<class classid="111" nclones="3" nlines="53" similarity="71">
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/cell_counting_datasets.py" startline="21" endline="80" pcid="1955">
def load_cell_counting(split=None,
                       reload=True,
                       data_dir=None,
                       save_dir=None,
                       **kwargs):
  """Load Cell Counting dataset.

  Loads the cell counting dataset from http://www.robots.ox.ac.uk/~vgg/research/counting/index_org.html.
  """
  if data_dir is None:
    data_dir = DEFAULT_DIR
  if save_dir is None:
    save_dir = DEFAULT_DIR
  # No tasks since no labels provided.
  cell_counting_tasks = []
  # For now images are loaded directly by ImageLoader
  featurizer = ""
  if reload:
    save_folder = os.path.join(save_dir, "cell_counting-featurized", str(split))
    loaded, all_dataset, transformers = deepchem.utils.save.load_dataset_from_disk(
        save_folder)
    if loaded:
      return cell_counting_tasks, all_dataset, transformers
  dataset_file = os.path.join(data_dir, "cells.zip")
  if not os.path.exists(dataset_file):
    deepchem.utils.download_url(url=DATASET_URL, dest_dir=data_dir)

  loader = deepchem.data.ImageLoader()
  dataset = loader.featurize(dataset_file)

  transformers = []

  if split == None:
    logger.info("Split is None, no transformers used.")
    return cell_counting_tasks, (dataset, None, None), transformers

  splitters = {
      'index': deepchem.splits.IndexSplitter(),
      'random': deepchem.splits.RandomSplitter(),
  }
  if split not in splitters:
    raise ValueError("Only index and random splits supported.")
  splitter = splitters[split]

  logger.info("About to split dataset with {} splitter.".format(split))
  frac_train = kwargs.get("frac_train", 0.8)
  frac_valid = kwargs.get('frac_valid', 0.1)
  frac_test = kwargs.get('frac_test', 0.1)

  train, valid, test = splitter.train_valid_test_split(
      dataset,
      frac_train=frac_train,
      frac_valid=frac_valid,
      frac_test=frac_test)
  transformers = []
  all_dataset = (train, valid, test)
  if reload:
    deepchem.utils.save.save_dataset_to_disk(save_folder, train, valid, test,
                                             transformers)
  return cell_counting_tasks, all_dataset, transformers
</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/bbbc_datasets.py" startline="106" endline="186" pcid="1962">
def load_bbbc002(split='index',
                 reload=True,
                 data_dir=None,
                 save_dir=None,
                 **kwargs):
  """Load BBBC002 dataset

  This dataset contains data corresponding to 5 samples of Drosophilia Kc167
  cells. There are 10 fields of view for each sample, each an image of size
  512x512. Ground truth labels contain cell counts for this dataset. Full
  details about this dataset are present at
  https://data.broadinstitute.org/bbbc/BBBC002/.
  """
  # Featurize BBBC002 dataset
  bbbc002_tasks = ["cell-count"]

  if data_dir is None:
    data_dir = DEFAULT_DIR
  if save_dir is None:
    save_dir = DEFAULT_DIR

  if reload:
    save_folder = os.path.join(save_dir, "bbbc002-featurized", str(split))
    loaded, all_dataset, transformers = deepchem.utils.save.load_dataset_from_disk(
        save_folder)
    if loaded:
      return bbbc002_tasks, all_dataset, transformers
  dataset_file = os.path.join(data_dir, "BBBC002_v1_images.zip")
  labels_file = os.path.join(data_dir, "BBBC002_v1_counts.txt")

  if not os.path.exists(dataset_file):
    deepchem.utils.download_url(url=BBBC2_IMAGE_URL, dest_dir=data_dir)
  if not os.path.exists(labels_file):
    deepchem.utils.download_url(url=BBBC2_LABEL_URL, dest_dir=data_dir)
  # Featurize Images into NumpyArrays
  loader = deepchem.data.ImageLoader()
  dataset = loader.featurize(dataset_file, in_memory=False)

  # Load text file with labels
  with open(labels_file) as f:
    content = f.readlines()
  # Strip the first line which holds field labels
  lines = [x.strip() for x in content][1:]
  # Format is: Image_name count1 count2
  lines = [x.split("\t") for x in lines]
  counts = [(float(x[1]) + float(x[2])) / 2.0 for x in lines]
  y = np.reshape(np.array(counts), (len(counts), 1))
  ids = [x[0] for x in lines]

  # This is kludgy way to add y to dataset. Can be done better?
  dataset = deepchem.data.DiskDataset.from_numpy(dataset.X, y, ids=ids)

  if split == None:
    transformers = []
    logger.info("Split is None, no transformers used for the dataset.")
    return bbbc002_tasks, (dataset, None, None), transformers

  splitters = {
      'index': deepchem.splits.IndexSplitter(),
      'random': deepchem.splits.RandomSplitter(),
  }
  if split not in splitters:
    raise ValueError("Only index and random splits supported.")
  splitter = splitters[split]

  logger.info("About to split dataset with {} splitter.".format(split))
  frac_train = kwargs.get("frac_train", 0.8)
  frac_valid = kwargs.get('frac_valid', 0.1)
  frac_test = kwargs.get('frac_test', 0.1)

  train, valid, test = splitter.train_valid_test_split(
      dataset,
      frac_train=frac_train,
      frac_valid=frac_valid,
      frac_test=frac_test)
  all_dataset = (train, valid, test)
  transformers = []
  if reload:
    deepchem.utils.save.save_dataset_to_disk(save_folder, train, valid, test,
                                             transformers)
  return bbbc002_tasks, all_dataset, transformers
</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/bbbc_datasets.py" startline="25" endline="105" pcid="1961">
def load_bbbc001(split='index',
                 reload=True,
                 data_dir=None,
                 save_dir=None,
                 **kwargs):
  """Load BBBC001 dataset

  This dataset contains 6 images of human HT29 colon cancer cells. The task is
  to learn to predict the cell counts in these images. This dataset is too small
   to serve to train algorithms, but might serve as a good test dataset.
   https://data.broadinstitute.org/bbbc/BBBC001/
  """
  # Featurize BBBC001 dataset
  bbbc001_tasks = ["cell-count"]

  if data_dir is None:
    data_dir = DEFAULT_DIR
  if save_dir is None:
    save_dir = DEFAULT_DIR

  if reload:
    save_folder = os.path.join(save_dir, "bbbc001-featurized", str(split))
    loaded, all_dataset, transformers = deepchem.utils.save.load_dataset_from_disk(
        save_folder)
    if loaded:
      return bbbc001_tasks, all_dataset, transformers
  dataset_file = os.path.join(data_dir, "BBBC001_v1_images_tif.zip")
  labels_file = os.path.join(data_dir, "BBBC001_v1_counts.txt")

  if not os.path.exists(dataset_file):
    deepchem.utils.download_url(url=BBBC1_IMAGE_URL, dest_dir=data_dir)
  if not os.path.exists(labels_file):
    deepchem.utils.download_url(url=BBBC1_LABEL_URL, dest_dir=data_dir)
  # Featurize Images into NumpyArrays
  loader = deepchem.data.ImageLoader()
  dataset = loader.featurize(dataset_file, in_memory=False)

  # Load text file with labels
  with open(labels_file) as f:
    content = f.readlines()
  # Strip the first line which holds field labels
  lines = [x.strip() for x in content][1:]
  # Format is: Image_name count1 count2
  lines = [x.split("\t") for x in lines]
  counts = [(float(x[1]) + float(x[2])) / 2.0 for x in lines]
  y = np.array(counts)

  # This is kludgy way to add y to dataset. Can be done better?
  dataset = deepchem.data.DiskDataset.from_numpy(dataset.X, y)

  if split == None:
    transformers = []
    logger.info("Split is None, no transformers used for the dataset.")
    return bbbc001_tasks, (dataset, None, None), transformers

  splitters = {
      'index': deepchem.splits.IndexSplitter(),
      'random': deepchem.splits.RandomSplitter(),
  }
  if split not in splitters:
    raise ValueError("Only index and random splits supported.")
  splitter = splitters[split]

  logger.info("About to split dataset with {} splitter.".format(split))
  frac_train = kwargs.get("frac_train", 0.8)
  frac_valid = kwargs.get('frac_valid', 0.1)
  frac_test = kwargs.get('frac_test', 0.1)

  train, valid, test = splitter.train_valid_test_split(
      dataset,
      frac_train=frac_train,
      frac_valid=frac_valid,
      frac_test=frac_test)
  transformers = []
  all_dataset = (train, valid, test)
  if reload:
    deepchem.utils.save.save_dataset_to_disk(save_folder, train, valid, test,
                                             transformers)
  return bbbc001_tasks, all_dataset, transformers


</source>
</class>

<class classid="112" nclones="3" nlines="57" similarity="84">
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/factors_datasets.py" startline="53" endline="140" pcid="1958">
def gen_factors(FACTORS_tasks,
                data_dir,
                train_dir,
                valid_dir,
                test_dir,
                shard_size=2000):
  """Loads the FACTORS dataset; does not do train/test split"""

  time1 = time.time()

  train_files = os.path.join(data_dir, TRAIN_FILENAME)
  valid_files = os.path.join(data_dir, VALID_FILENAME)
  test_files = os.path.join(data_dir, TEST_FILENAME)

  if not os.path.exists(train_files):
    logger.info("Downloading train file...")
    deepchem.utils.download_url(url=TRAIN_URL, dest_dir=data_dir)
    logger.info("Training file download complete.")

    logger.info("Downloading validation file...")
    deepchem.utils.download_url(url=VALID_URL, dest_dir=data_dir)
    logger.info("Validation file download complete.")

    logger.info("Downloading test file...")
    deepchem.utils.download_url(url=TEST_URL, dest_dir=data_dir)
    logger.info("Test file download complete")

  # Featurize the FACTORS dataset
  logger.info("About to featurize the FACTORS dataset")
  featurizer = deepchem.feat.UserDefinedFeaturizer(merck_descriptors)
  loader = deepchem.data.UserCSVLoader(
      tasks=FACTORS_tasks, id_field="Molecule", featurizer=featurizer)

  logger.info("Featurizing the train dataset...")
  train_dataset = loader.featurize(train_files, shard_size=shard_size)

  logger.info("Featurizing the validation dataset...")
  valid_dataset = loader.featurize(valid_files, shard_size=shard_size)

  logger.info("Featurizing the test dataset...")
  test_dataset = loader.featurize(test_files, shard_size=shard_size)

  logger.info("Remove missing entries from dataset")
  remove_missing_entries(train_dataset)
  remove_missing_entries(valid_dataset)
  remove_missing_entries(test_dataset)

  # Shuffle the training data
  logger.info("Shuffling the training dataset")
  train_dataset.sparse_shuffle()

  # Apply transformations
  logger.info("Transforming datasets with transformers")
  transformers = get_transformers(train_dataset)

  for transformer in transformers:
    logger.info("Performing transformations with {}".format(
        transformer.__class__.__name__))

    logger.info("Transforming the training dataset...")
    train_dataset = transformer.transform(train_dataset)

    logger.info("Transforming the validation dataset...")
    valid_dataset = transformer.transform(valid_dataset)

    logger.info("Transforming the test dataset...")
    test_dataset = transformer.transform(test_dataset)

  logger.info("Transformations complete.")
  logger.info("Moving datasets to corresponding directories")

  train_dataset.move(train_dir)
  logger.info("Train dataset moved.")

  valid_dataset.move(valid_dir)
  logger.info("Validation dataset moved.")

  test_dataset.move(test_dir)
  logger.info("Test dataset moved.")

  time2 = time.time()

  ########## TIMING ################
  logger.info("TIMING: FACTORS fitting took %0.3f s" % (time2 - time1))

  return train_dataset, valid_dataset, test_dataset


</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/uv_datasets.py" startline="54" endline="142" pcid="1990">
def gen_uv(UV_tasks, data_dir, train_dir, valid_dir, test_dir, shard_size=2000):
  """Loading the UV dataset; does not do train/test split"""

  time1 = time.time()

  train_files = os.path.join(data_dir, TRAIN_FILENAME)
  valid_files = os.path.join(data_dir, VALID_FILENAME)
  test_files = os.path.join(data_dir, TEST_FILENAME)

  # Download files if they don't exist

  if not os.path.exists(train_files):

    logger.info("Downloading training file...")
    deepchem.utils.download_url(url=TRAIN_URL, dest_dir=data_dir)
    logger.info("Training file download complete.")

    logger.info("Downloading validation file...")
    deepchem.utils.download_url(url=VALID_URL, dest_dir=data_dir)
    logger.info("Validation file download complete.")

    logger.info("Downloading test file...")
    deepchem.utils.download_url(url=TEST_URL, dest_dir=data_dir)
    logger.info("Test file download complete")

  # Featurizing datasets
  logger.info("About to featurize UV dataset.")
  featurizer = deepchem.feat.UserDefinedFeaturizer(merck_descriptors)
  loader = deepchem.data.UserCSVLoader(
      tasks=UV_tasks, id_field="Molecule", featurizer=featurizer)

  logger.info("Featurizing train datasets...")
  train_dataset = loader.featurize(
      input_files=train_files, shard_size=shard_size)

  logger.info("Featurizing validation datasets...")
  valid_dataset = loader.featurize(
      input_files=valid_files, shard_size=shard_size)

  logger.info("Featurizing test datasets....")
  test_dataset = loader.featurize(input_files=test_files, shard_size=shard_size)

  # Missing entry removal
  logger.info("Removing missing entries from dataset.")
  remove_missing_entries(train_dataset)
  remove_missing_entries(valid_dataset)
  remove_missing_entries(test_dataset)

  # Shuffle the training data
  logger.info("Shuffling the training dataset")
  train_dataset.sparse_shuffle()

  # Apply transformations
  logger.info("Starting transformations")
  transformers = get_transformers(train_dataset)

  for transformer in transformers:
    logger.info("Performing transformations with {}".format(
        transformer.__class__.__name__))

    logger.info("Transforming the training dataset...")
    train_dataset = transformer.transform(train_dataset)

    logger.info("Transforming the validation dataset...")
    valid_dataset = transformer.transform(valid_dataset)

    logger.info("Transforming the test dataset...")
    test_dataset = transformer.transform(test_dataset)

  logger.info("Transformations complete.")
  logger.info("Moving datasets to corresponding directories")

  train_dataset.move(train_dir)
  logger.info("Train dataset moved.")

  valid_dataset.move(valid_dir)
  logger.info("Validation dataset moved.")

  test_dataset.move(test_dir)
  logger.info("Test dataset moved.")

  time2 = time.time()

  ##### TIMING ###########
  logger.info("TIMING: UV fitting took %0.3f s" % (time2 - time1))

  return train_dataset, valid_dataset, test_dataset


</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/kinase_datasets.py" startline="53" endline="146" pcid="1996">
def gen_kinase(KINASE_tasks,
               train_dir,
               valid_dir,
               test_dir,
               data_dir,
               shard_size=2000):

  time1 = time.time()

  train_files = os.path.join(data_dir, TRAIN_FILENAME)
  valid_files = os.path.join(data_dir, VALID_FILENAME)
  test_files = os.path.join(data_dir, TEST_FILENAME)

  # Download files if they don't exist

  if not os.path.exists(train_files):

    logger.info("Downloading training file...")
    deepchem.utils.download_url(url=TRAIN_URL, dest_dir=data_dir)
    logger.info("Training file download complete.")

    logger.info("Downloading validation file...")
    deepchem.utils.download_url(url=VALID_URL, dest_dir=data_dir)
    logger.info("Validation file download complete.")

    logger.info("Downloading test file...")
    deepchem.utils.download_url(url=TEST_URL, dest_dir=data_dir)
    logger.info("Test file download complete")

  # Featurize the KINASE dataset
  logger.info("About to featurize KINASE dataset.")
  featurizer = deepchem.feat.UserDefinedFeaturizer(merck_descriptors)

  loader = deepchem.data.UserCSVLoader(
      tasks=KINASE_tasks, id_field="Molecule", featurizer=featurizer)

  logger.info("Featurizing train datasets...")
  train_dataset = loader.featurize(
      input_files=train_files, shard_size=shard_size)

  logger.info("Featurizing validation datasets...")
  valid_dataset = loader.featurize(
      input_files=valid_files, shard_size=shard_size)

  logger.info("Featurizing test datasets....")
  test_dataset = loader.featurize(input_files=test_files, shard_size=shard_size)

  logger.info("Remove missing entries from dataset")
  remove_missing_entries(train_dataset)
  remove_missing_entries(valid_dataset)
  remove_missing_entries(test_dataset)

  # Shuffle the training data
  logger.info("Shuffling the training dataset")
  train_dataset.sparse_shuffle()

  # Apply transformations
  logger.info("Transformating datasets with transformers")
  transformers = get_transformers(train_dataset)

  for transformer in transformers:
    logger.info("Performing transformations with {}".format(
        transformer.__class__.__name__))

    logger.info("Transforming the training dataset...")
    train_dataset = transformer.transform(train_dataset)

    logger.info("Transforming the validation dataset...")
    valid_dataset = transformer.transform(valid_dataset)

    logger.info("Transforming the test dataset...")
    test_dataset = transformer.transform(test_dataset)

  logger.info("Transformations complete.")
  logger.info("Moving datasets to corresponding directories")

  train_dataset.move(train_dir)
  logger.info("Train dataset moved.")

  valid_dataset.move(valid_dir)
  logger.info("Validation dataset moved.")

  test_dataset.move(test_dir)
  logger.info("Test dataset moved.")

  time2 = time.time()

  ##### TIMING ######

  logger.info("TIMING: KINASE fitting took %0.3f s" % (time2 - time1))

  return train_dataset, valid_dataset, test_dataset


</source>
</class>

<class classid="113" nclones="3" nlines="22" similarity="79">
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/factors_datasets.py" startline="141" endline="175" pcid="1959">
def load_factors(shard_size=2000, featurizer=None, split=None, reload=True):
  """Loads FACTOR dataset; does not do train/test split"""

  FACTORS_tasks = [
      'T_00001', 'T_00002', 'T_00003', 'T_00004', 'T_00005', 'T_00006',
      'T_00007', 'T_00008', 'T_00009', 'T_00010', 'T_00011', 'T_00012'
  ]

  data_dir = deepchem.utils.get_data_dir()
  data_dir = os.path.join(data_dir, "factors")

  if not os.path.exists(data_dir):
    os.mkdir(data_dir)

  train_dir = os.path.join(data_dir, "train_dir")
  valid_dir = os.path.join(data_dir, "valid_dir")
  test_dir = os.path.join(data_dir, "test_dir")

  if (os.path.exists(train_dir) and os.path.exists(valid_dir) and
      os.path.exists(test_dir)):

    logger.info("Reloading existing datasets")
    train_dataset = deepchem.data.DiskDataset(train_dir)
    valid_dataset = deepchem.data.DiskDataset(valid_dir)
    test_dataset = deepchem.data.DiskDataset(test_dir)

  else:
    logger.info("Featurizing datasets")
    train_dataset, valid_dataset, test_dataset = \
    gen_factors(FACTORS_tasks=FACTORS_tasks, data_dir=data_dir, train_dir=train_dir,
                valid_dir=valid_dir, test_dir=test_dir, shard_size=shard_size)

  transformers = get_transformers(train_dataset)

  return FACTORS_tasks, (train_dataset, valid_dataset,
</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/uv_datasets.py" startline="143" endline="171" pcid="1991">
def load_uv(shard_size=2000, featurizer=None, split=None, reload=True):
  """Load UV dataset; does not do train/test split"""

  data_dir = deepchem.utils.get_data_dir()
  data_dir = os.path.join(data_dir, "UV")

  if not os.path.exists(data_dir):
    os.mkdir(data_dir)

  train_dir = os.path.join(data_dir, "train_dir")
  valid_dir = os.path.join(data_dir, "valid_dir")
  test_dir = os.path.join(data_dir, "test_dir")

  if (os.path.exists(train_dir) and os.path.exists(valid_dir) and
      os.path.exists(test_dir)):

    logger.info("Reloading existing datasets")
    train_dataset = deepchem.data.DiskDataset(train_dir)
    valid_dataset = deepchem.data.DiskDataset(valid_dir)
    test_dataset = deepchem.data.DiskDataset(test_dir)

  else:
    logger.info("Featurizing datasets")
    train_dataset, valid_dataset, test_dataset = \
    gen_uv(UV_tasks=UV_tasks, data_dir=data_dir, train_dir=train_dir,
           valid_dir=valid_dir, test_dir=test_dir, shard_size=shard_size)

  transformers = get_transformers(train_dataset)

</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/kaggle_datasets.py" startline="122" endline="150" pcid="1986">
def load_kaggle(shard_size=2000, featurizer=None, split=None, reload=True):
  """Loads kaggle datasets. Generates if not stored already."""
  KAGGLE_tasks = [
      '3A4', 'CB1', 'DPP4', 'HIVINT', 'HIV_PROT', 'LOGD', 'METAB', 'NK1', 'OX1',
      'OX2', 'PGP', 'PPB', 'RAT_F', 'TDI', 'THROMBIN'
  ]
  data_dir = deepchem.utils.get_data_dir()

  data_dir = os.path.join(data_dir, "kaggle")
  if not os.path.exists(data_dir):
    os.mkdir(data_dir)
  train_dir = os.path.join(data_dir, "train_dir")
  valid_dir = os.path.join(data_dir, "valid_dir")
  test_dir = os.path.join(data_dir, "test_dir")

  if (os.path.exists(train_dir) and os.path.exists(valid_dir) and
      os.path.exists(test_dir)):
    logger.info("Reloading existing datasets")
    train_dataset = deepchem.data.DiskDataset(train_dir)
    valid_dataset = deepchem.data.DiskDataset(valid_dir)
    test_dataset = deepchem.data.DiskDataset(test_dir)
  else:
    logger.info("Featurizing datasets")
    train_dataset, valid_dataset, test_dataset = \
      gen_kaggle(KAGGLE_tasks, train_dir, valid_dir, test_dir, data_dir,
                  shard_size=shard_size)

  transformers = get_transformers(train_dataset)
  return KAGGLE_tasks, (train_dataset, valid_dataset,
</source>
</class>

<class classid="114" nclones="2" nlines="47" similarity="77">
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/qm7_datasets.py" startline="133" endline="188" pcid="1969">
def load_qm7b_from_mat(featurizer='CoulombMatrix',
                       split='stratified',
                       reload=True,
                       move_mean=True,
                       data_dir=None,
                       save_dir=None,
                       **kwargs):
  if data_dir is None:
    data_dir = DEFAULT_DIR
  if save_dir is None:
    save_dir = DEFAULT_DIR
  dataset_file = os.path.join(data_dir, "qm7b.mat")

  if not os.path.exists(dataset_file):
    deepchem.utils.download_url(url=QM7B_MAT_URL, dest_dir=data_dir)
  dataset = scipy.io.loadmat(dataset_file)

  X = dataset['X']
  y = dataset['T']
  w = np.ones_like(y)
  dataset = deepchem.data.DiskDataset.from_numpy(X, y, w, ids=None)

  if split == None:
    raise ValueError()
  else:
    splitters = {
        'index': deepchem.splits.IndexSplitter(),
        'random': deepchem.splits.RandomSplitter(),
        'stratified':
        deepchem.splits.SingletaskStratifiedSplitter(task_number=0)
    }
    splitter = splitters[split]
    frac_train = kwargs.get("frac_train", 0.8)
    frac_valid = kwargs.get('frac_valid', 0.1)
    frac_test = kwargs.get('frac_test', 0.1)

    train_dataset, valid_dataset, test_dataset = splitter.train_valid_test_split(
        dataset,
        frac_train=frac_train,
        frac_valid=frac_valid,
        frac_test=frac_test)

    transformers = [
        deepchem.trans.NormalizationTransformer(
            transform_y=True, dataset=train_dataset, move_mean=move_mean)
    ]

    for transformer in transformers:
      train_dataset = transformer.transform(train_dataset)
      valid_dataset = transformer.transform(valid_dataset)
      test_dataset = transformer.transform(test_dataset)

    qm7_tasks = np.arange(y.shape[1])
    return qm7_tasks, (train_dataset, valid_dataset, test_dataset), transformers


</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/qm7_datasets.py" startline="189" endline="248" pcid="1970">
def load_qm7(featurizer='CoulombMatrix',
             split='random',
             reload=True,
             move_mean=True,
             data_dir=None,
             save_dir=None,
             **kwargs):
  """Load qm7 datasets."""
  # Featurize qm7 dataset
  logger.info("About to featurize qm7 dataset.")
  if data_dir is None:
    data_dir = DEFAULT_DIR
  if save_dir is None:
    save_dir = DEFAULT_DIR
  dataset_file = os.path.join(data_dir, "gdb7.sdf")

  if not os.path.exists(dataset_file):
    deepchem.utils.download_url(url=GDB7_URL, dest_dir=data_dir)
    deepchem.utils.untargz_file(os.path.join(data_dir, 'gdb7.tar.gz'), data_dir)

  qm7_tasks = ["u0_atom"]
  if featurizer == 'CoulombMatrix':
    featurizer = deepchem.feat.CoulombMatrixEig(23)
  loader = deepchem.data.SDFLoader(
      tasks=qm7_tasks,
      smiles_field="smiles",
      mol_field="mol",
      featurizer=featurizer)
  dataset = loader.featurize(dataset_file)

  if split == None:
    raise ValueError()

  splitters = {
      'index': deepchem.splits.IndexSplitter(),
      'random': deepchem.splits.RandomSplitter(),
      'stratified': deepchem.splits.SingletaskStratifiedSplitter(task_number=0)
  }
  splitter = splitters[split]
  frac_train = kwargs.get("frac_train", 0.8)
  frac_valid = kwargs.get('frac_valid', 0.1)
  frac_test = kwargs.get('frac_test', 0.1)

  train_dataset, valid_dataset, test_dataset = splitter.train_valid_test_split(
      dataset,
      frac_train=frac_train,
      frac_valid=frac_valid,
      frac_test=frac_test)

  transformers = [
      deepchem.trans.NormalizationTransformer(
          transform_y=True, dataset=train_dataset, move_mean=move_mean)
  ]

  for transformer in transformers:
    train_dataset = transformer.transform(train_dataset)
    valid_dataset = transformer.transform(valid_dataset)
    test_dataset = transformer.transform(test_dataset)

  return qm7_tasks, (train_dataset, valid_dataset, test_dataset), transformers
</source>
</class>

<class classid="115" nclones="3" nlines="14" similarity="100">
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/pcba_datasets.py" startline="17" endline="32" pcid="1971">
def load_pcba(featurizer='ECFP',
              split='random',
              reload=True,
              data_dir=None,
              save_dir=None,
              **kwargs):
  return load_pcba_dataset(
      featurizer=featurizer,
      split=split,
      reload=reload,
      assay_file_name="pcba.csv.gz",
      data_dir=data_dir,
      save_dir=save_dir,
      **kwargs)


</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/pcba_datasets.py" startline="49" endline="64" pcid="1973">
def load_pcba_2475(featurizer='ECFP',
                   split='random',
                   reload=True,
                   data_dir=None,
                   save_dir=None,
                   **kwargs):
  return load_pcba_dataset(
      featurizer=featurizer,
      split=split,
      reload=reload,
      assay_file_name="pcba_2475.csv.gz",
      data_dir=data_dir,
      save_dir=save_dir,
      **kwargs)


</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/load_function/pcba_datasets.py" startline="33" endline="48" pcid="1972">
def load_pcba_146(featurizer='ECFP',
                  split='random',
                  reload=True,
                  data_dir=None,
                  save_dir=None,
                  **kwargs):
  return load_pcba_dataset(
      featurizer=featurizer,
      split=split,
      reload=reload,
      assay_file_name="pcba_146.csv.gz",
      data_dir=data_dir,
      save_dir=save_dir,
      **kwargs)


</source>
</class>

<class classid="116" nclones="3" nlines="20" similarity="95">
<source file="systems/deepchem-2.3.0/deepchem/molnet/tests/test_molnet.py" startline="28" endline="49" pcid="2011">
  def test_delaney_graphconvreg(self):
    """Tests molnet benchmarking on delaney with graphconvreg."""
    datasets = ['delaney']
    model = 'graphconvreg'
    split = 'random'
    out_path = tempfile.mkdtemp()
    metric = [dc.metrics.Metric(dc.metrics.pearson_r2_score, np.mean)]
    dc.molnet.run_benchmark(
        datasets,
        str(model),
        metric=metric,
        split=split,
        out_path=out_path,
        reload=False)
    with open(os.path.join(out_path, 'results.csv'), 'r') as f:
      reader = csv.reader(f)
      for lastrow in reader:
        pass
      assert lastrow[-4] == 'valid'
      assert float(lastrow[-3]) > 0.75
    os.remove(os.path.join(out_path, 'results.csv'))

</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/tests/test_molnet.py" startline="73" endline="94" pcid="2013">
  def test_clintox_multitask(self):
    """Tests molnet benchmarking on clintox with multitask network."""
    datasets = ['clintox']
    model = 'tf'
    split = 'random'
    out_path = tempfile.mkdtemp()
    metric = [dc.metrics.Metric(dc.metrics.roc_auc_score, np.mean)]
    dc.molnet.run_benchmark(
        datasets,
        str(model),
        metric=metric,
        split=split,
        out_path=out_path,
        test=True,
        reload=False)
    with open(os.path.join(out_path, 'results.csv'), 'r') as f:
      reader = csv.reader(f)
      for lastrow in reader:
        pass
      assert lastrow[-4] == 'test'
      assert float(lastrow[-3]) > 0.7
    os.remove(os.path.join(out_path, 'results.csv'))
</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/tests/test_molnet.py" startline="51" endline="72" pcid="2012">
  def test_qm7_multitask(self):
    """Tests molnet benchmarking on qm7 with multitask network."""
    datasets = ['qm7']
    model = 'tf_regression_ft'
    split = 'random'
    out_path = tempfile.mkdtemp()
    metric = [dc.metrics.Metric(dc.metrics.pearson_r2_score, np.mean)]
    dc.molnet.run_benchmark(
        datasets,
        str(model),
        metric=metric,
        split=split,
        out_path=out_path,
        reload=False)
    with open(os.path.join(out_path, 'results.csv'), 'r') as f:
      reader = csv.reader(f)
      for lastrow in reader:
        pass
      assert lastrow[-4] == 'valid'
      assert float(lastrow[-3]) > 0.95
    os.remove(os.path.join(out_path, 'results.csv'))

</source>
</class>

<class classid="117" nclones="3" nlines="11" similarity="81">
<source file="systems/deepchem-2.3.0/deepchem/molnet/tests/test_dnasim.py" startline="25" endline="39" pcid="2015">
  def test_motif_counting_simulation(self):
    "Test motif counting"
    params = {
        "motif_name": "TAL1_known4",
        "seq_length": 1000,
        "pos_counts": [5, 10],
        "neg_counts": [1, 2],
        "num_pos": 30,
        "num_neg": 30,
        "GC_fraction": 0.4
    }
    sequences, y, embed = dc.molnet.simulate_motif_counting(**params)
    assert sequences.shape == (60,)
    assert y.shape == (60, 1)

</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/tests/test_dnasim.py" startline="64" endline="75" pcid="2018">
  def test_single_motif_detection(self):
    "Test single motif detection"
    params = {
        "motif_name": "TAL1_known4",
        "seq_length": 1000,
        "num_pos": 30,
        "num_neg": 30,
        "GC_fraction": 0.4
    }
    sequences, y, embed = dc.molnet.simulate_single_motif_detection(**params)
    assert sequences.shape == (60,)
    assert y.shape == (60, 1)
</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/tests/test_dnasim.py" startline="51" endline="63" pcid="2017">
  def test_motif_density(self):
    "Test motif density"
    params = {
        "motif_name": "TAL1_known4",
        "seq_length": 1000,
        "num_seqs": 30,
        "min_counts": 2,
        "max_counts": 4,
        "GC_fraction": 0.4
    }
    sequences, embed = dc.molnet.motif_density(**params)
    assert sequences.shape == (30,)

</source>
</class>

<class classid="118" nclones="2" nlines="288" similarity="79">
<source file="systems/deepchem-2.3.0/deepchem/molnet/run_benchmark_models.py" startline="22" endline="385" pcid="2019">
def benchmark_classification(train_dataset,
                             valid_dataset,
                             test_dataset,
                             tasks,
                             transformers,
                             n_features,
                             metric,
                             model,
                             test=False,
                             hyper_parameters=None,
                             seed=123):
  """
  Calculate performance of different models on the specific dataset & tasks

  Parameters
  ----------
  train_dataset: dataset struct
      dataset used for model training and evaluation
  valid_dataset: dataset struct
      dataset only used for model evaluation (and hyperparameter tuning)
  test_dataset: dataset struct
      dataset only used for model evaluation
  tasks: list of string
      list of targets(tasks, datasets)
  transformers: dc.trans.Transformer struct
      transformer used for model evaluation
  n_features: integer
      number of features, or length of binary fingerprints
  metric: list of dc.metrics.Metric objects
      metrics used for evaluation
  model: string,  optional
      choice of model
      'rf', 'tf', 'tf_robust', 'logreg', 'irv', 'graphconv', 'dag', 'xgb',
      'weave', 'kernelsvm', 'textcnn', 'mpnn'
  test: boolean, optional
      whether to calculate test_set performance
  hyper_parameters: dict, optional (default=None)
      hyper parameters for designated model, None = use preset values


  Returns
  -------
  train_scores : dict
	predicting results(AUC) on training set
  valid_scores : dict
	predicting results(AUC) on valid set
  test_scores : dict
	predicting results(AUC) on test set


  """
  train_scores = {}
  valid_scores = {}
  test_scores = {}

  assert model in [
      'rf', 'tf', 'tf_robust', 'logreg', 'irv', 'graphconv', 'dag', 'xgb',
      'weave', 'kernelsvm', 'textcnn', 'mpnn'
  ]
  if hyper_parameters is None:
    hyper_parameters = hps[model]
  model_name = model

  if model_name == 'tf':
    layer_sizes = hyper_parameters['layer_sizes']
    weight_init_stddevs = hyper_parameters['weight_init_stddevs']
    bias_init_consts = hyper_parameters['bias_init_consts']
    dropouts = hyper_parameters['dropouts']
    penalty = hyper_parameters['penalty']
    penalty_type = hyper_parameters['penalty_type']
    batch_size = hyper_parameters['batch_size']
    nb_epoch = hyper_parameters['nb_epoch']
    learning_rate = hyper_parameters['learning_rate']

    # Building tensorflow MultitaskDNN model
    model = deepchem.models.MultitaskClassifier(
        len(tasks),
        n_features,
        layer_sizes=layer_sizes,
        weight_init_stddevs=weight_init_stddevs,
        bias_init_consts=bias_init_consts,
        dropouts=dropouts,
        weight_decay_penalty=penalty,
        weight_decay_penalty_type=penalty_type,
        batch_size=batch_size,
        learning_rate=learning_rate,
        random_seed=seed)

  elif model_name == 'tf_robust':
    layer_sizes = hyper_parameters['layer_sizes']
    weight_init_stddevs = hyper_parameters['weight_init_stddevs']
    bias_init_consts = hyper_parameters['bias_init_consts']
    dropouts = hyper_parameters['dropouts']

    bypass_layer_sizes = hyper_parameters['bypass_layer_sizes']
    bypass_weight_init_stddevs = hyper_parameters['bypass_weight_init_stddevs']
    bypass_bias_init_consts = hyper_parameters['bypass_bias_init_consts']
    bypass_dropouts = hyper_parameters['bypass_dropouts']

    penalty = hyper_parameters['penalty']
    penalty_type = hyper_parameters['penalty_type']
    batch_size = hyper_parameters['batch_size']
    nb_epoch = hyper_parameters['nb_epoch']
    learning_rate = hyper_parameters['learning_rate']

    # Building tensorflow robust MultitaskDNN model
    model = deepchem.models.RobustMultitaskClassifier(
        len(tasks),
        n_features,
        layer_sizes=layer_sizes,
        weight_init_stddevs=weight_init_stddevs,
        bias_init_consts=bias_init_consts,
        dropouts=dropouts,
        bypass_layer_sizes=bypass_layer_sizes,
        bypass_weight_init_stddevs=bypass_weight_init_stddevs,
        bypass_bias_init_consts=bypass_bias_init_consts,
        bypass_dropouts=bypass_dropouts,
        weight_decay_penalty=penalty,
        weight_decay_penalty_type=penalty_type,
        batch_size=batch_size,
        learning_rate=learning_rate,
        random_seed=seed)

  elif model_name == 'logreg':
    penalty = hyper_parameters['penalty']
    penalty_type = hyper_parameters['penalty_type']
    nb_epoch = None

    # Building scikit logistic regression model
    def model_builder(model_dir_logreg):
      sklearn_model = LogisticRegression(
          penalty=penalty_type,
          C=1. / penalty,
          class_weight="balanced",
          n_jobs=-1)
      return deepchem.models.sklearn_models.SklearnModel(
          sklearn_model, model_dir_logreg)

    model = deepchem.models.multitask.SingletaskToMultitask(
        tasks, model_builder)

  elif model_name == 'irv':
    penalty = hyper_parameters['penalty']
    batch_size = hyper_parameters['batch_size']
    nb_epoch = hyper_parameters['nb_epoch']
    learning_rate = hyper_parameters['learning_rate']
    n_K = hyper_parameters['n_K']

    # Transform fingerprints to IRV features
    transformer = deepchem.trans.IRVTransformer(n_K, len(tasks), train_dataset)
    train_dataset = transformer.transform(train_dataset)
    valid_dataset = transformer.transform(valid_dataset)
    if test:
      test_dataset = transformer.transform(test_dataset)

    # Building tensorflow IRV model
    model = deepchem.models.TensorflowMultitaskIRVClassifier(
        len(tasks),
        K=n_K,
        penalty=penalty,
        batch_size=batch_size,
        learning_rate=learning_rate,
        random_seed=seed,
        mode='classification')

  elif model_name == 'graphconv':
    batch_size = hyper_parameters['batch_size']
    nb_epoch = hyper_parameters['nb_epoch']
    learning_rate = hyper_parameters['learning_rate']
    n_filters = hyper_parameters['n_filters']
    n_fully_connected_nodes = hyper_parameters['n_fully_connected_nodes']

    model = deepchem.models.GraphConvModel(
        len(tasks),
        graph_conv_layers=[n_filters] * 2,
        dense_layer_size=n_fully_connected_nodes,
        batch_size=batch_size,
        learning_rate=learning_rate,
        random_seed=seed,
        mode='classification')

  elif model_name == 'dag':
    batch_size = hyper_parameters['batch_size']
    nb_epoch = hyper_parameters['nb_epoch']
    learning_rate = hyper_parameters['learning_rate']
    n_graph_feat = hyper_parameters['n_graph_feat']
    default_max_atoms = hyper_parameters['default_max_atoms']

    max_atoms_train = max([mol.get_num_atoms() for mol in train_dataset.X])
    max_atoms_valid = max([mol.get_num_atoms() for mol in valid_dataset.X])
    max_atoms_test = max([mol.get_num_atoms() for mol in test_dataset.X])
    max_atoms = max([max_atoms_train, max_atoms_valid, max_atoms_test])
    max_atoms = min([max_atoms, default_max_atoms])
    print('Maximum number of atoms: %i' % max_atoms)
    reshard_size = 256
    transformer = deepchem.trans.DAGTransformer(max_atoms=max_atoms)
    train_dataset.reshard(reshard_size)
    train_dataset = transformer.transform(train_dataset)
    valid_dataset.reshard(reshard_size)
    valid_dataset = transformer.transform(valid_dataset)
    if test:
      test_dataset.reshard(reshard_size)
      test_dataset = transformer.transform(test_dataset)

    model = deepchem.models.DAGModel(
        len(tasks),
        max_atoms=max_atoms,
        n_atom_feat=n_features,
        n_graph_feat=n_graph_feat,
        n_outputs=30,
        batch_size=batch_size,
        learning_rate=learning_rate,
        random_seed=seed,
        use_queue=False,
        mode='classification')

  elif model_name == 'weave':
    batch_size = hyper_parameters['batch_size']
    nb_epoch = hyper_parameters['nb_epoch']
    learning_rate = hyper_parameters['learning_rate']
    n_graph_feat = hyper_parameters['n_graph_feat']
    n_pair_feat = hyper_parameters['n_pair_feat']

    model = deepchem.models.WeaveModel(
        len(tasks),
        n_atom_feat=n_features,
        n_pair_feat=n_pair_feat,
        n_hidden=50,
        n_graph_feat=n_graph_feat,
        batch_size=batch_size,
        learning_rate=learning_rate,
        use_queue=False,
        random_seed=seed,
        mode='classification')

  elif model_name == 'textcnn':
    batch_size = hyper_parameters['batch_size']
    nb_epoch = hyper_parameters['nb_epoch']
    learning_rate = hyper_parameters['learning_rate']
    n_embedding = hyper_parameters['n_embedding']
    filter_sizes = hyper_parameters['filter_sizes']
    num_filters = hyper_parameters['num_filters']

    all_data = deepchem.data.DiskDataset.merge(
        [train_dataset, valid_dataset, test_dataset])
    char_dict, length = deepchem.models.TextCNNModel.build_char_dict(all_data)

    model = deepchem.models.TextCNNModel(
        len(tasks),
        char_dict,
        seq_length=length,
        n_embedding=n_embedding,
        filter_sizes=filter_sizes,
        num_filters=num_filters,
        learning_rate=learning_rate,
        batch_size=batch_size,
        use_queue=False,
        random_seed=seed,
        mode='classification')

  elif model_name == 'mpnn':
    batch_size = hyper_parameters['batch_size']
    nb_epoch = hyper_parameters['nb_epoch']
    learning_rate = hyper_parameters['learning_rate']
    T = hyper_parameters['T']
    M = hyper_parameters['M']

    model = deepchem.models.MPNNModel(
        len(tasks),
        n_atom_feat=n_features[0],
        n_pair_feat=n_features[1],
        n_hidden=n_features[0],
        T=T,
        M=M,
        batch_size=batch_size,
        learning_rate=learning_rate,
        use_queue=False,
        mode="classification")

  elif model_name == 'rf':
    n_estimators = hyper_parameters['n_estimators']
    nb_epoch = None

    # Building scikit random forest model
    def model_builder(model_dir_rf):
      sklearn_model = RandomForestClassifier(
          class_weight="balanced", n_estimators=n_estimators, n_jobs=-1)
      return deepchem.models.sklearn_models.SklearnModel(
          sklearn_model, model_dir_rf)

    model = deepchem.models.multitask.SingletaskToMultitask(
        tasks, model_builder)

  elif model_name == 'kernelsvm':
    C = hyper_parameters['C']
    gamma = hyper_parameters['gamma']
    nb_epoch = None

    # Building scikit learn Kernel SVM model
    def model_builder(model_dir_kernelsvm):
      sklearn_model = SVC(
          C=C, gamma=gamma, class_weight="balanced", probability=True)
      return deepchem.models.SklearnModel(sklearn_model, model_dir_kernelsvm)

    model = deepchem.models.multitask.SingletaskToMultitask(
        tasks, model_builder)

  elif model_name == 'xgb':
    max_depth = hyper_parameters['max_depth']
    learning_rate = hyper_parameters['learning_rate']
    n_estimators = hyper_parameters['n_estimators']
    gamma = hyper_parameters['gamma']
    min_child_weight = hyper_parameters['min_child_weight']
    max_delta_step = hyper_parameters['max_delta_step']
    subsample = hyper_parameters['subsample']
    colsample_bytree = hyper_parameters['colsample_bytree']
    colsample_bylevel = hyper_parameters['colsample_bylevel']
    reg_alpha = hyper_parameters['reg_alpha']
    reg_lambda = hyper_parameters['reg_lambda']
    scale_pos_weight = hyper_parameters['scale_pos_weight']
    base_score = hyper_parameters['base_score']
    seed = hyper_parameters['seed']
    early_stopping_rounds = hyper_parameters['early_stopping_rounds']
    nb_epoch = None

    esr = {'early_stopping_rounds': early_stopping_rounds}

    # Building xgboost classification model
    def model_builder(model_dir_xgb):
      import xgboost
      xgboost_model = xgboost.XGBClassifier(
          max_depth=max_depth,
          learning_rate=learning_rate,
          n_estimators=n_estimators,
          gamma=gamma,
          min_child_weight=min_child_weight,
          max_delta_step=max_delta_step,
          subsample=subsample,
          colsample_bytree=colsample_bytree,
          colsample_bylevel=colsample_bylevel,
          reg_alpha=reg_alpha,
          reg_lambda=reg_lambda,
          scale_pos_weight=scale_pos_weight,
          base_score=base_score,
          seed=seed)
      return deepchem.models.xgboost_models.XGBoostModel(
          xgboost_model, model_dir_xgb, **esr)

    model = deepchem.models.multitask.SingletaskToMultitask(
        tasks, model_builder)

  if nb_epoch is None:
    model.fit(train_dataset)
  else:
    model.fit(train_dataset, nb_epoch=nb_epoch)

  train_scores[model_name] = model.evaluate(train_dataset, metric, transformers)
  valid_scores[model_name] = model.evaluate(valid_dataset, metric, transformers)
  if test:
    test_scores[model_name] = model.evaluate(test_dataset, metric, transformers)

  return train_scores, valid_scores, test_scores


</source>
<source file="systems/deepchem-2.3.0/deepchem/molnet/run_benchmark_models.py" startline="386" endline="774" pcid="2024">
def benchmark_regression(train_dataset,
                         valid_dataset,
                         test_dataset,
                         tasks,
                         transformers,
                         n_features,
                         metric,
                         model,
                         test=False,
                         hyper_parameters=None,
                         seed=123):
  """
  Calculate performance of different models on the specific dataset & tasks

  Parameters
  ----------
  train_dataset: dataset struct
      dataset used for model training and evaluation
  valid_dataset: dataset struct
      dataset only used for model evaluation (and hyperparameter tuning)
  test_dataset: dataset struct
      dataset only used for model evaluation
  tasks: list of string
      list of targets(tasks, datasets)
  transformers: dc.trans.Transformer struct
      transformer used for model evaluation
  n_features: integer
      number of features, or length of binary fingerprints
  metric: list of dc.metrics.Metric objects
      metrics used for evaluation
  model: string, optional
      choice of model
      'tf_regression', 'tf_regression_ft', 'rf_regression', 'graphconvreg',
      'dtnn', 'dag_regression', 'xgb_regression', 'weave_regression',
      'textcnn_regression', 'krr', 'ani', 'krr_ft', 'mpnn'
  test: boolean, optional
      whether to calculate test_set performance
  hyper_parameters: dict, optional (default=None)
      hyper parameters for designated model, None = use preset values


  Returns
  -------
  train_scores : dict
	predicting results(R2) on training set
  valid_scores : dict
	predicting results(R2) on valid set
  test_scores : dict
	predicting results(R2) on test set

  """
  train_scores = {}
  valid_scores = {}
  test_scores = {}

  assert model in [
      'tf_regression', 'tf_regression_ft', 'rf_regression', 'graphconvreg',
      'dtnn', 'dag_regression', 'xgb_regression', 'weave_regression',
      'textcnn_regression', 'krr', 'ani', 'krr_ft', 'mpnn'
  ]
  import xgboost
  if hyper_parameters is None:
    hyper_parameters = hps[model]
  model_name = model

  if model_name == 'tf_regression':
    layer_sizes = hyper_parameters['layer_sizes']
    weight_init_stddevs = hyper_parameters['weight_init_stddevs']
    bias_init_consts = hyper_parameters['bias_init_consts']
    dropouts = hyper_parameters['dropouts']
    penalty = hyper_parameters['penalty']
    penalty_type = hyper_parameters['penalty_type']
    batch_size = hyper_parameters['batch_size']
    nb_epoch = hyper_parameters['nb_epoch']
    learning_rate = hyper_parameters['learning_rate']

    model = deepchem.models.MultitaskRegressor(
        len(tasks),
        n_features,
        layer_sizes=layer_sizes,
        weight_init_stddevs=weight_init_stddevs,
        bias_init_consts=bias_init_consts,
        dropouts=dropouts,
        weight_decay_penalty=penalty,
        weight_decay_penalty_type=penalty_type,
        batch_size=batch_size,
        learning_rate=learning_rate,
        seed=seed)

  elif model_name == 'tf_regression_ft':
    layer_sizes = hyper_parameters['layer_sizes']
    weight_init_stddevs = hyper_parameters['weight_init_stddevs']
    bias_init_consts = hyper_parameters['bias_init_consts']
    dropouts = hyper_parameters['dropouts']
    penalty = hyper_parameters['penalty']
    penalty_type = hyper_parameters['penalty_type']
    batch_size = hyper_parameters['batch_size']
    nb_epoch = hyper_parameters['nb_epoch']
    learning_rate = hyper_parameters['learning_rate']
    fit_transformers = [hyper_parameters['fit_transformers'](train_dataset)]

    model = deepchem.models.MultitaskFitTransformRegressor(
        n_tasks=len(tasks),
        n_features=n_features,
        layer_sizes=layer_sizes,
        weight_init_stddevs=weight_init_stddevs,
        bias_init_consts=bias_init_consts,
        dropouts=dropouts,
        weight_decay_penalty=penalty,
        weight_decay_penalty_type=penalty_type,
        batch_size=batch_size,
        learning_rate=learning_rate,
        fit_transformers=fit_transformers,
        n_eval=10,
        seed=seed)

  elif model_name == 'graphconvreg':
    batch_size = hyper_parameters['batch_size']
    nb_epoch = hyper_parameters['nb_epoch']
    learning_rate = hyper_parameters['learning_rate']
    n_filters = hyper_parameters['n_filters']
    n_fully_connected_nodes = hyper_parameters['n_fully_connected_nodes']

    model = deepchem.models.GraphConvModel(
        len(tasks),
        graph_conv_layers=[n_filters] * 2,
        dense_layer_size=n_fully_connected_nodes,
        batch_size=batch_size,
        learning_rate=learning_rate,
        random_seed=seed,
        mode='regression')

  elif model_name == 'dtnn':
    batch_size = hyper_parameters['batch_size']
    nb_epoch = hyper_parameters['nb_epoch']
    learning_rate = hyper_parameters['learning_rate']
    n_embedding = hyper_parameters['n_embedding']
    n_distance = hyper_parameters['n_distance']
    assert len(n_features) == 2, 'DTNN is only applicable to qm datasets'

    model = deepchem.models.DTNNModel(
        len(tasks),
        n_embedding=n_embedding,
        n_distance=n_distance,
        batch_size=batch_size,
        learning_rate=learning_rate,
        random_seed=seed,
        output_activation=False,
        use_queue=False,
        mode='regression')

  elif model_name == 'dag_regression':
    batch_size = hyper_parameters['batch_size']
    nb_epoch = hyper_parameters['nb_epoch']
    learning_rate = hyper_parameters['learning_rate']
    n_graph_feat = hyper_parameters['n_graph_feat']
    default_max_atoms = hyper_parameters['default_max_atoms']

    max_atoms_train = max([mol.get_num_atoms() for mol in train_dataset.X])
    max_atoms_valid = max([mol.get_num_atoms() for mol in valid_dataset.X])
    max_atoms_test = max([mol.get_num_atoms() for mol in test_dataset.X])
    max_atoms = max([max_atoms_train, max_atoms_valid, max_atoms_test])
    max_atoms = min([max_atoms, default_max_atoms])
    print('Maximum number of atoms: %i' % max_atoms)
    reshard_size = 256
    transformer = deepchem.trans.DAGTransformer(max_atoms=max_atoms)
    train_dataset.reshard(reshard_size)
    train_dataset = transformer.transform(train_dataset)
    valid_dataset.reshard(reshard_size)
    valid_dataset = transformer.transform(valid_dataset)
    if test:
      test_dataset.reshard(reshard_size)
      test_dataset = transformer.transform(test_dataset)

    model = deepchem.models.DAGModel(
        len(tasks),
        max_atoms=max_atoms,
        n_atom_feat=n_features,
        n_graph_feat=n_graph_feat,
        n_outputs=30,
        batch_size=batch_size,
        learning_rate=learning_rate,
        random_seed=seed,
        use_queue=False,
        mode='regression')

  elif model_name == 'weave_regression':
    batch_size = hyper_parameters['batch_size']
    nb_epoch = hyper_parameters['nb_epoch']
    learning_rate = hyper_parameters['learning_rate']
    n_graph_feat = hyper_parameters['n_graph_feat']
    n_pair_feat = hyper_parameters['n_pair_feat']

    model = deepchem.models.WeaveModel(
        len(tasks),
        n_atom_feat=n_features,
        n_pair_feat=n_pair_feat,
        n_hidden=50,
        n_graph_feat=n_graph_feat,
        batch_size=batch_size,
        learning_rate=learning_rate,
        use_queue=False,
        random_seed=seed,
        mode='regression')

  elif model_name == 'textcnn_regression':
    batch_size = hyper_parameters['batch_size']
    nb_epoch = hyper_parameters['nb_epoch']
    learning_rate = hyper_parameters['learning_rate']
    n_embedding = hyper_parameters['n_embedding']
    filter_sizes = hyper_parameters['filter_sizes']
    num_filters = hyper_parameters['num_filters']

    char_dict, length = deepchem.models.TextCNNModel.build_char_dict(
        train_dataset)

    model = deepchem.models.TextCNNModel(
        len(tasks),
        char_dict,
        seq_length=length,
        n_embedding=n_embedding,
        filter_sizes=filter_sizes,
        num_filters=num_filters,
        learning_rate=learning_rate,
        batch_size=batch_size,
        use_queue=False,
        random_seed=seed,
        mode='regression')

  elif model_name == 'ani':
    batch_size = hyper_parameters['batch_size']
    nb_epoch = hyper_parameters['nb_epoch']
    learning_rate = hyper_parameters['learning_rate']
    layer_structures = hyper_parameters['layer_structures']

    assert len(n_features) == 2, 'ANI is only applicable to qm datasets'
    max_atoms = n_features[0]
    atom_number_cases = np.unique(
        np.concatenate([
            train_dataset.X[:, :, 0], valid_dataset.X[:, :, 0],
            test_dataset.X[:, :, 0]
        ]))

    atom_number_cases = atom_number_cases.astype(int).tolist()
    try:
      # Remove token for paddings
      atom_number_cases.remove(0)
    except:
      pass
    ANItransformer = deepchem.trans.ANITransformer(
        max_atoms=max_atoms, atom_cases=atom_number_cases)
    train_dataset = ANItransformer.transform(train_dataset)
    valid_dataset = ANItransformer.transform(valid_dataset)
    if test:
      test_dataset = ANItransformer.transform(test_dataset)
    n_feat = ANItransformer.get_num_feats() - 1

    model = deepchem.models.ANIRegression(
        len(tasks),
        max_atoms,
        n_feat,
        layer_structures=layer_structures,
        atom_number_cases=atom_number_cases,
        batch_size=batch_size,
        learning_rate=learning_rate,
        use_queue=False,
        mode="regression",
        random_seed=seed)

  elif model_name == 'mpnn':
    batch_size = hyper_parameters['batch_size']
    nb_epoch = hyper_parameters['nb_epoch']
    learning_rate = hyper_parameters['learning_rate']
    T = hyper_parameters['T']
    M = hyper_parameters['M']

    model = deepchem.models.MPNNModel(
        len(tasks),
        n_atom_feat=n_features[0],
        n_pair_feat=n_features[1],
        n_hidden=n_features[0],
        T=T,
        M=M,
        batch_size=batch_size,
        learning_rate=learning_rate,
        use_queue=False,
        mode="regression")

  elif model_name == 'rf_regression':
    n_estimators = hyper_parameters['n_estimators']
    nb_epoch = None

    # Building scikit random forest model
    def model_builder(model_dir_rf_regression):
      sklearn_model = RandomForestRegressor(
          n_estimators=n_estimators, n_jobs=-1)
      return deepchem.models.sklearn_models.SklearnModel(
          sklearn_model, model_dir_rf_regression)

    model = deepchem.models.multitask.SingletaskToMultitask(
        tasks, model_builder)

  elif model_name == 'krr':
    alpha = hyper_parameters['alpha']
    nb_epoch = None

    # Building scikit learn Kernel Ridge Regression model
    def model_builder(model_dir_krr):
      sklearn_model = KernelRidge(kernel="rbf", alpha=alpha)
      return deepchem.models.SklearnModel(sklearn_model, model_dir_krr)

    model = deepchem.models.multitask.SingletaskToMultitask(
        tasks, model_builder)

  elif model_name == 'krr_ft':
    alpha = hyper_parameters['alpha']
    nb_epoch = None

    ft_transformer = deepchem.trans.CoulombFitTransformer(train_dataset)
    train_dataset = ft_transformer.transform(train_dataset)
    valid_dataset = ft_transformer.transform(valid_dataset)
    test_dataset = ft_transformer.transform(test_dataset)

    # Building scikit learn Kernel Ridge Regression model
    def model_builder(model_dir_krr):
      sklearn_model = KernelRidge(kernel="rbf", alpha=alpha)
      return deepchem.models.SklearnModel(sklearn_model, model_dir_krr)

    model = deepchem.models.multitask.SingletaskToMultitask(
        tasks, model_builder)

  elif model_name == 'xgb_regression':
    max_depth = hyper_parameters['max_depth']
    learning_rate = hyper_parameters['learning_rate']
    n_estimators = hyper_parameters['n_estimators']
    gamma = hyper_parameters['gamma']
    min_child_weight = hyper_parameters['min_child_weight']
    max_delta_step = hyper_parameters['max_delta_step']
    subsample = hyper_parameters['subsample']
    colsample_bytree = hyper_parameters['colsample_bytree']
    colsample_bylevel = hyper_parameters['colsample_bylevel']
    reg_alpha = hyper_parameters['reg_alpha']
    reg_lambda = hyper_parameters['reg_lambda']
    scale_pos_weight = hyper_parameters['scale_pos_weight']
    base_score = hyper_parameters['base_score']
    seed = hyper_parameters['seed']
    early_stopping_rounds = hyper_parameters['early_stopping_rounds']
    nb_epoch = None

    esr = {'early_stopping_rounds': early_stopping_rounds}

    # Building xgboost regression model
    def model_builder(model_dir_xgb):
      xgboost_model = xgboost.XGBRegressor(
          max_depth=max_depth,
          learning_rate=learning_rate,
          n_estimators=n_estimators,
          gamma=gamma,
          min_child_weight=min_child_weight,
          max_delta_step=max_delta_step,
          subsample=subsample,
          colsample_bytree=colsample_bytree,
          colsample_bylevel=colsample_bylevel,
          reg_alpha=reg_alpha,
          reg_lambda=reg_lambda,
          scale_pos_weight=scale_pos_weight,
          base_score=base_score,
          seed=seed)
      return deepchem.models.xgboost_models.XGBoostModel(
          xgboost_model, model_dir_xgb, **esr)

    model = deepchem.models.multitask.SingletaskToMultitask(
        tasks, model_builder)

  print('-----------------------------')
  print('Start fitting: %s' % model_name)
  if nb_epoch is None:
    model.fit(train_dataset)
  else:
    model.fit(train_dataset, nb_epoch=nb_epoch)

  train_scores[model_name] = model.evaluate(train_dataset, metric, transformers)
  valid_scores[model_name] = model.evaluate(valid_dataset, metric, transformers)
  if test:
    test_scores[model_name] = model.evaluate(test_dataset, metric, transformers)

  return train_scores, valid_scores, test_scores


</source>
</class>

<class classid="119" nclones="2" nlines="13" similarity="92">
<source file="systems/deepchem-2.3.0/deepchem/metrics/tests/test_genomics.py" startline="49" endline="69" pcid="2057">
  def test_in_silico_mutagenesis_shape(self):
    """Test in-silico mutagenesis returns correct shape."""
    # Construct and train SequenceDNN model
    sequences = np.array(["ACGTA", "GATAG", "CGCGC"])
    sequences = dc.utils.genomics.seq_one_hot_encode(sequences, letters=LETTERS)
    labels = np.array([1, 0, 0])
    labels = np.reshape(labels, (3, 1))
    self.assertEqual(sequences.shape, (3, 4, 5, 1))

    #X = np.random.rand(10, 1, 4, 50)
    #y = np.random.randint(0, 2, size=(10, 1))
    #dataset = dc.data.NumpyDataset(X, y)
    dataset = dc.data.NumpyDataset(sequences, labels)
    model = dc.models.SequenceDNN(
        5, "binary_crossentropy", num_filters=[1, 1], kernel_size=[15, 15])
    model.fit(dataset, nb_epoch=1)

    # Call in-silico mutagenesis
    mutagenesis_scores = in_silico_mutagenesis(model, sequences)
    self.assertEqual(mutagenesis_scores.shape, (1, 3, 4, 5, 1))

</source>
<source file="systems/deepchem-2.3.0/deepchem/metrics/tests/test_genomics.py" startline="70" endline="92" pcid="2058">
  def test_in_silico_mutagenesis_nonzero(self):
    """Test in-silico mutagenesis returns nonzero output."""
    # Construct and train SequenceDNN model
    sequences = np.array(["ACGTA", "GATAG", "CGCGC"])
    sequences = dc.utils.genomics.seq_one_hot_encode(sequences, letters=LETTERS)
    labels = np.array([1, 0, 0])
    labels = np.reshape(labels, (3, 1))
    self.assertEqual(sequences.shape, (3, 4, 5, 1))

    #X = np.random.rand(10, 1, 4, 50)
    #y = np.random.randint(0, 2, size=(10, 1))
    #dataset = dc.data.NumpyDataset(X, y)
    dataset = dc.data.NumpyDataset(sequences, labels)
    model = dc.models.SequenceDNN(
        5, "binary_crossentropy", num_filters=[1, 1], kernel_size=[15, 15])
    model.fit(dataset, nb_epoch=1)

    # Call in-silico mutagenesis
    mutagenesis_scores = in_silico_mutagenesis(model, sequences)
    self.assertEqual(mutagenesis_scores.shape, (1, 3, 4, 5, 1))

    # Check nonzero elements exist
    assert np.count_nonzero(mutagenesis_scores) > 0
</source>
</class>

<class classid="120" nclones="2" nlines="30" similarity="90">
<source file="systems/deepchem-2.3.0/examples/sider/sider_datasets.py" startline="13" endline="53" pcid="2097">
def load_sider(featurizer='ECFP', split='index'):
  current_dir = os.path.dirname(os.path.realpath(__file__))

	  # Load SIDER dataset
  print("About to load SIDER dataset.")
  dataset_file = os.path.join(
      current_dir, "./sider.csv.gz")
  dataset = dc.utils.save.load_from_disk(dataset_file)
  print("Columns of dataset: %s" % str(dataset.columns.values))
  print("Number of examples in dataset: %s" % str(dataset.shape[0]))

  # Featurize SIDER dataset
  print("About to featurize SIDER dataset.")
  if featurizer == 'ECFP':
    featurizer = dc.feat.CircularFingerprint(size=1024)
  elif featurizer == 'GraphConv':
    featurizer = dc.feat.ConvMolFeaturizer()

  SIDER_tasks = dataset.columns.values[1:].tolist()
  print("SIDER tasks: %s" % str(SIDER_tasks))
  print("%d tasks in total" % len(SIDER_tasks))

  loader = dc.data.CSVLoader(
      tasks=SIDER_tasks, smiles_field="smiles", featurizer=featurizer)
  dataset = loader.featurize(dataset_file)
  print("%d datapoints in SIDER dataset" % len(dataset))

  # Initialize transformers
  transformers = [
      dc.trans.BalancingTransformer(transform_w=True, dataset=dataset)]
  print("About to transform data")
  for transformer in transformers:
    dataset = transformer.transform(dataset)

  splitters = {'index': dc.splits.IndexSplitter(),
               'random': dc.splits.RandomSplitter(),
               'scaffold': dc.splits.ScaffoldSplitter()}
  splitter = splitters[split]
  train, valid, test = splitter.train_valid_test_split(dataset)

  return SIDER_tasks, (train, valid, test), transformers
</source>
<source file="systems/deepchem-2.3.0/examples/toxcast/toxcast_datasets.py" startline="13" endline="53" pcid="2100">
def load_toxcast(featurizer='ECFP', split='index'):

  current_dir = os.path.dirname(os.path.realpath(__file__))

  # Load TOXCAST dataset
  print("About to load TOXCAST dataset.")
  dataset_file = os.path.join(
      current_dir, "./processing/toxcast_data.csv.gz")
  dataset = dc.utils.save.load_from_disk(dataset_file)
  print("Columns of dataset: %s" % str(dataset.columns.values))
  print("Number of examples in dataset: %s" % str(dataset.shape[0]))

  # Featurize TOXCAST dataset
  print("About to featurize TOXCAST dataset.")

  if featurizer == 'ECFP':
      featurizer = dc.feat.CircularFingerprint(size=1024)
  elif featurizer == 'GraphConv':
      featurizer = dc.feat.ConvMolFeaturizer()

  TOXCAST_tasks = dataset.columns.values[1:].tolist()

  loader = dc.data.CSVLoader(
      tasks=TOXCAST_tasks, smiles_field="smiles", featurizer=featurizer)
  dataset = loader.featurize(dataset_file)

  # Initialize transformers 
  transformers = [
      dc.trans.BalancingTransformer(transform_w=True, dataset=dataset)]
  print("About to transform data")
  for transformer in transformers:
    dataset = transformer.transform(dataset)

  splitters = {'index': dc.splits.IndexSplitter(),
               'random': dc.splits.RandomSplitter(),
               'scaffold': dc.splits.ScaffoldSplitter()}
  splitter = splitters[split]

  train, valid, test = splitter.train_valid_test_split(dataset)
  
  return TOXCAST_tasks, (train, valid, test), transformers
</source>
</class>

<class classid="121" nclones="2" nlines="19" similarity="89">
<source file="systems/deepchem-2.3.0/examples/low_data/datasets.py" startline="19" endline="46" pcid="2108">
def load_tox21_ecfp(num_train=7200):
  """Load Tox21 datasets. Does not do train/test split"""
  # Set some global variables up top
  current_dir = os.path.dirname(os.path.realpath(__file__))
  dataset_file = os.path.join(
      current_dir, "../../datasets/tox21.csv.gz")
  # Featurize Tox21 dataset
  print("About to featurize Tox21 dataset.")
  featurizer = dc.feat.CircularFingerprint(size=1024)
  tox21_tasks = ['NR-AR', 'NR-AR-LBD', 'NR-AhR', 'NR-Aromatase', 'NR-ER',
                 'NR-ER-LBD', 'NR-PPAR-gamma', 'SR-ARE', 'SR-ATAD5',
                 'SR-HSE', 'SR-MMP', 'SR-p53']

  loader = dc.data.CSVLoader(
      tasks=tox21_tasks, smiles_field="smiles", featurizer=featurizer)
  dataset = loader.featurize(
      dataset_file, shard_size=8192)

  # Initialize transformers 
  transformers = [
      dc.trans.BalancingTransformer(transform_w=True, dataset=dataset)]

  print("About to transform data")
  for transformer in transformers:
    dataset = transformer.transform(dataset)

  return tox21_tasks, dataset, transformers

</source>
<source file="systems/deepchem-2.3.0/examples/low_data/datasets.py" startline="47" endline="75" pcid="2109">
def load_tox21_convmol(base_dir=None, num_train=7200):
  """Load Tox21 datasets. Does not do train/test split"""
  # Set some global variables up top
  current_dir = os.path.dirname(os.path.realpath(__file__))
  dataset_file = os.path.join(
      current_dir, "../../datasets/tox21.csv.gz")

  # Featurize Tox21 dataset
  print("About to featurize Tox21 dataset.")
  featurizer = dc.feat.ConvMolFeaturizer()
  tox21_tasks = ['NR-AR', 'NR-AR-LBD', 'NR-AhR', 'NR-Aromatase', 'NR-ER',
                 'NR-ER-LBD', 'NR-PPAR-gamma', 'SR-ARE', 'SR-ATAD5',
                 'SR-HSE', 'SR-MMP', 'SR-p53']

  loader = dc.data.CSVLoader(
      tasks=tox21_tasks, smiles_field="smiles", featurizer=featurizer)
  dataset = loader.featurize(
      dataset_file, shard_size=8192)

  # Initialize transformers 
  transformers = [
      dc.trans.BalancingTransformer(transform_w=True, dataset=dataset)]

  print("About to transform data")
  for transformer in transformers:
    dataset = transformer.transform(dataset)

  return tox21_tasks, dataset, transformers

</source>
</class>

<class classid="122" nclones="4" nlines="20" similarity="70">
<source file="systems/deepchem-2.3.0/examples/low_data/datasets.py" startline="76" endline="103" pcid="2110">
def load_muv_ecfp():
  """Load MUV datasets. Does not do train/test split"""
  # Load MUV dataset
  print("About to load MUV dataset.")
  current_dir = os.path.dirname(os.path.realpath(__file__))
  dataset_file = os.path.join(
      current_dir, "../../datasets/muv.csv.gz")
  # Featurize MUV dataset
  print("About to featurize MUV dataset.")
  featurizer = dc.feat.CircularFingerprint(size=1024)
  MUV_tasks = sorted(['MUV-692', 'MUV-689', 'MUV-846', 'MUV-859', 'MUV-644',
                      'MUV-548', 'MUV-852', 'MUV-600', 'MUV-810', 'MUV-712',
                      'MUV-737', 'MUV-858', 'MUV-713', 'MUV-733', 'MUV-652',
                      'MUV-466', 'MUV-832'])

  loader = dc.data.CSVLoader(
      tasks=MUV_tasks, smiles_field="smiles", featurizer=featurizer)
  dataset = loader.featurize(dataset_file)

  # Initialize transformers 
  transformers = [
      dc.trans.BalancingTransformer(transform_w=True, dataset=dataset)]
  print("About to transform data")
  for transformer in transformers:
      dataset = transformer.transform(dataset)

  return MUV_tasks, dataset, transformers

</source>
<source file="systems/deepchem-2.3.0/examples/low_data/datasets.py" startline="132" endline="160" pcid="2112">
def load_sider_ecfp():
  """Load SIDER datasets. Does not do train/test split"""
  # Featurize SIDER dataset
  print("About to featurize SIDER dataset.")
  current_dir = os.path.dirname(os.path.realpath(__file__))
  dataset_file = os.path.join(
      current_dir, "../sider/sider.csv.gz")
  featurizer = dc.feat.CircularFingerprint(size=1024)

  dataset = dc.utils.save.load_from_disk(dataset_file)
  SIDER_tasks = dataset.columns.values[1:].tolist()
  print("SIDER tasks: %s" % str(SIDER_tasks))
  print("%d tasks in total" % len(SIDER_tasks))


  loader = dc.data.CSVLoader(
      tasks=SIDER_tasks, smiles_field="smiles", featurizer=featurizer)
  dataset = loader.featurize(dataset_file)
  print("%d datapoints in SIDER dataset" % len(dataset))

  # Initialize transformers
  transformers = [
      dc.trans.BalancingTransformer(transform_w=True, dataset=dataset)]
  print("About to transform data")
  for transformer in transformers:
    dataset = transformer.transform(dataset)

  return SIDER_tasks, dataset, transformers

</source>
<source file="systems/deepchem-2.3.0/examples/low_data/datasets.py" startline="161" endline="188" pcid="2113">
def load_sider_convmol():
  """Load SIDER datasets. Does not do train/test split"""
  # Featurize SIDER dataset
  print("About to featurize SIDER dataset.")
  current_dir = os.path.dirname(os.path.realpath(__file__))
  dataset_file = os.path.join(
      current_dir, "../sider/sider.csv.gz")
  featurizer = dc.feat.ConvMolFeaturizer()

  dataset = dc.utils.save.load_from_disk(dataset_file)
  SIDER_tasks = dataset.columns.values[1:].tolist()
  print("SIDER tasks: %s" % str(SIDER_tasks))
  print("%d tasks in total" % len(SIDER_tasks))


  loader = dc.data.CSVLoader(
      tasks=SIDER_tasks, smiles_field="smiles", featurizer=featurizer)
  dataset = loader.featurize(dataset_file)
  print("%d datapoints in SIDER dataset" % len(dataset))

  # Initialize transformers
  transformers = [
      dc.trans.BalancingTransformer(transform_w=True, dataset=dataset)]
  print("About to transform data")
  for transformer in transformers:
    dataset = transformer.transform(dataset)

  return SIDER_tasks, dataset, transformers
</source>
<source file="systems/deepchem-2.3.0/examples/low_data/datasets.py" startline="104" endline="131" pcid="2111">
def load_muv_convmol():
  """Load MUV datasets. Does not do train/test split"""
  # Load MUV dataset
  print("About to load MUV dataset.")
  current_dir = os.path.dirname(os.path.realpath(__file__))
  dataset_file = os.path.join(
      current_dir, "../../datasets/muv.csv.gz")
  # Featurize MUV dataset
  print("About to featurize MUV dataset.")
  featurizer = dc.feat.ConvMolFeaturizer()
  MUV_tasks = sorted(['MUV-692', 'MUV-689', 'MUV-846', 'MUV-859', 'MUV-644',
                      'MUV-548', 'MUV-852', 'MUV-600', 'MUV-810', 'MUV-712',
                      'MUV-737', 'MUV-858', 'MUV-713', 'MUV-733', 'MUV-652',
                      'MUV-466', 'MUV-832'])

  loader = dc.data.CSVLoader(
      tasks=MUV_tasks, smiles_field="smiles", featurizer=featurizer)
  dataset = loader.featurize(dataset_file)

  # Initialize transformers 
  transformers = [
      dc.trans.BalancingTransformer(transform_w=True, dataset=dataset)]
  print("About to transform data")
  for transformer in transformers:
      dataset = transformer.transform(dataset)

  return MUV_tasks, dataset, transformers

</source>
</class>

<class classid="123" nclones="3" nlines="39" similarity="90">
<source file="systems/deepchem-2.3.0/examples/kinase/KINASE_datasets.py" startline="36" endline="87" pcid="2149">
def gen_kinase(KINASE_tasks, raw_train_dir, train_dir, valid_dir, test_dir,
                shard_size=10000):
  """Load Kinase datasets."""
  train_files = ("KINASE_training_disguised_combined_full.csv.gz")
  valid_files = ("KINASE_test1_disguised_combined_full.csv.gz")
  test_files = ("KINASE_test2_disguised_combined_full.csv.gz")

  # Featurize Kinase dataset
  print("About to featurize KINASE dataset.")
  featurizer = dc.feat.UserDefinedFeaturizer(kinase_descriptors)

  loader = dc.data.UserCSVLoader(
      tasks=KINASE_tasks, id_field="Molecule", featurizer=featurizer)

  train_datasets, valid_datasets, test_datasets = [], [], []
  print("Featurizing train datasets")
  train_dataset = loader.featurize(train_files, shard_size=shard_size)

  print("Featurizing valid datasets")
  valid_dataset = loader.featurize(valid_files, shard_size=shard_size)

  print("Featurizing test datasets")
  test_dataset = loader.featurize(test_files, shard_size=shard_size)

  print("Remove missing entries from datasets.")
  remove_missing_entries(train_dataset)
  remove_missing_entries(valid_dataset)
  remove_missing_entries(test_dataset)

  print("Transforming datasets with transformers.")
  transformers = get_transformers(train_dataset)
  raw_train_dataset = train_dataset

  for transformer in transformers:
    print("Performing transformations with %s"
          % transformer.__class__.__name__)
    print("Transforming datasets")
    train_dataset = transformer.transform(train_dataset)
    valid_dataset = transformer.transform(valid_dataset)
    test_dataset = transformer.transform(test_dataset)

  print("Shuffling order of train dataset.")
  train_dataset.sparse_shuffle()

  print("Moving directories")
  raw_train_dataset.move(raw_train_dir)
  train_dataset.move(train_dir)
  valid_dataset.move(valid_dir)
  test_dataset.move(test_dir)
  
  return (raw_train_dataset, train_dataset, valid_dataset, test_dataset)

</source>
<source file="systems/deepchem-2.3.0/examples/uv/UV_datasets.py" startline="51" endline="107" pcid="2157">
def gen_uv(UV_tasks, raw_train_dir, train_dir, valid_dir, test_dir,
           shard_size=10000):
  """Load UV datasets."""
  train_files = ("UV_training_disguised_combined_full.csv.gz")
  valid_files = ("UV_test1_disguised_combined_full.csv.gz")
  test_files = ("UV_test2_disguised_combined_full.csv.gz")

  # Featurize UV dataset
  print("About to featurize UV dataset.")
  featurizer = dc.feat.UserDefinedFeaturizer(uv_descriptors)

  loader = dc.data.UserCSVLoader(
      tasks=UV_tasks, id_field="Molecule", featurizer=featurizer)

  train_datasets, valid_datasets, test_datasets = [], [], []
  print("Featurizing train datasets")
  train_dataset = loader.featurize(train_files, shard_size=shard_size)

  print("Featurizing valid datasets")
  valid_dataset = loader.featurize(valid_files, shard_size=shard_size)

  print("Featurizing test datasets")
  test_dataset = loader.featurize(test_files, shard_size=shard_size)

  print("Remove missing entries from datasets.")
  remove_missing_entries(train_dataset)
  remove_missing_entries(valid_dataset)
  remove_missing_entries(test_dataset)

  print("Remove malformed datapoints from UV dataset.")
  remove_UV_negative_entries(train_dataset)
  remove_UV_negative_entries(valid_dataset)
  remove_UV_negative_entries(test_dataset)

  print("Transforming datasets with transformers.")
  transformers = get_transformers(train_dataset)
  raw_train_dataset = train_dataset

  for transformer in transformers:
    print("Performing transformations with %s"
          % transformer.__class__.__name__)
    print("Transforming dataset")
    train_dataset = transformer.transform(train_dataset)
    valid_dataset = transformer.transform(valid_dataset)
    test_dataset = transformer.transform(test_dataset)

  print("Shuffling order of train dataset.")
  train_dataset.sparse_shuffle()

  print("Moving directories")
  raw_train_dataset.move(raw_train_dir)
  train_dataset.move(train_dir)
  valid_dataset.move(valid_dir)
  test_dataset.move(test_dir)
  
  return (raw_train_dataset, train_dataset, valid_dataset, test_dataset)

</source>
<source file="systems/deepchem-2.3.0/examples/factors/FACTORS_datasets.py" startline="36" endline="87" pcid="2176">
def gen_factors(FACTORS_tasks, raw_train_dir, train_dir, valid_dir, test_dir,
                shard_size=10000):
  """Load Factor datasets."""
  train_files = ("FACTORS_training_disguised_combined_full.csv.gz")
  valid_files = ("FACTORS_test1_disguised_combined_full.csv.gz")
  test_files = ("FACTORS_test2_disguised_combined_full.csv.gz")

  # Featurize FACTORS dataset
  print("About to featurize FACTORS dataset.")
  featurizer = dc.feat.UserDefinedFeaturizer(factors_descriptors)

  loader = dc.data.UserCSVLoader(
      tasks=FACTORS_tasks, id_field="Molecule", featurizer=featurizer)

  train_datasets, valid_datasets, test_datasets = [], [], []
  print("Featurizing train datasets")
  train_dataset = loader.featurize(train_files, shard_size=shard_size)

  print("Featurizing valid datasets")
  valid_dataset = loader.featurize(valid_files, shard_size=shard_size)

  print("Featurizing test datasets")
  test_dataset = loader.featurize(test_files, shard_size=shard_size)

  print("Remove missing entries from datasets.")
  remove_missing_entries(train_dataset)
  remove_missing_entries(valid_dataset)
  remove_missing_entries(test_dataset)

  print("Transforming datasets with transformers.")
  transformers = get_transformers(train_dataset)
  raw_train_dataset = train_dataset

  for transformer in transformers:
    print("Performing transformations with %s"
          % transformer.__class__.__name__)
    print("Transforming datasets")
    train_dataset = transformer.transform(train_dataset)
    valid_dataset = transformer.transform(valid_dataset)
    test_dataset = transformer.transform(test_dataset)

  print("Shuffling order of train dataset.")
  train_dataset.sparse_shuffle()

  print("Moving directories")
  raw_train_dataset.move(raw_train_dir)
  train_dataset.move(train_dir)
  valid_dataset.move(valid_dir)
  test_dataset.move(test_dir)
  
  return (raw_train_dataset, train_dataset, valid_dataset, test_dataset)

</source>
</class>

<class classid="124" nclones="3" nlines="23" similarity="91">
<source file="systems/deepchem-2.3.0/examples/kinase/KINASE_datasets.py" startline="88" endline="114" pcid="2150">
def load_kinase(shard_size):
  """Loads kinase datasets. Generates if not stored already."""
  KINASE_tasks = (['T_000%d' % i for i in range(13, 100)]
                  + ['T_00%d' % i for i in range(100, 112)])

  current_dir = os.path.dirname(os.path.realpath(__file__))
  raw_train_dir = os.path.join(current_dir, "raw_train_dir")
  train_dir = os.path.join(current_dir, "train_dir") 
  valid_dir = os.path.join(current_dir, "valid_dir") 
  test_dir = os.path.join(current_dir, "test_dir") 

  if (os.path.exists(raw_train_dir) and
      os.path.exists(train_dir) and
      os.path.exists(valid_dir) and
      os.path.exists(test_dir)):
    print("Reloading existing datasets")
    raw_train_dataset = dc.data.DiskDataset(raw_train_dir)
    train_dataset = dc.data.DiskDataset(train_dir)
    valid_dataset = dc.data.DiskDataset(valid_dir)
    test_dataset = dc.data.DiskDataset(test_dir)
  else:
    print("Featurizing datasets")
    (raw_train_dataset, train_dataset, valid_dataset, test_dataset) = \
      gen_kinase(KINASE_tasks, raw_train_dir, train_dir, valid_dir, test_dir,
                  shard_size=shard_size)

  transformers = get_transformers(raw_train_dataset)
</source>
<source file="systems/deepchem-2.3.0/examples/factors/FACTORS_datasets.py" startline="88" endline="114" pcid="2177">
def load_factors(shard_size):
  """Loads factors datasets. Generates if not stored already."""
  FACTORS_tasks = (['T_0000%d' % i for i in range(1, 10)]
                   + ['T_000%d' % i for i in range(10, 13)])

  current_dir = os.path.dirname(os.path.realpath(__file__))
  raw_train_dir = os.path.join(current_dir, "raw_train_dir")
  train_dir = os.path.join(current_dir, "train_dir") 
  valid_dir = os.path.join(current_dir, "valid_dir") 
  test_dir = os.path.join(current_dir, "test_dir") 

  if (os.path.exists(raw_train_dir) and
      os.path.exists(train_dir) and
      os.path.exists(valid_dir) and
      os.path.exists(test_dir)):
    print("Reloading existing datasets")
    raw_train_dataset = dc.data.DiskDataset(raw_train_dir)
    train_dataset = dc.data.DiskDataset(train_dir)
    valid_dataset = dc.data.DiskDataset(valid_dir)
    test_dataset = dc.data.DiskDataset(test_dir)
  else:
    print("Featurizing datasets")
    (raw_train_dataset, train_dataset, valid_dataset, test_dataset) = \
      gen_factors(FACTORS_tasks, raw_train_dir, train_dir, valid_dir, test_dir,
                  shard_size=shard_size)

  transformers = get_transformers(raw_train_dataset)
</source>
<source file="systems/deepchem-2.3.0/examples/uv/UV_datasets.py" startline="108" endline="134" pcid="2158">
def load_uv(shard_size):
  """Loads uv datasets. Generates if not stored already."""
  UV_tasks = (['logTIC'] +
                  ['w__%d' % i for i in range(210, 401)])

  current_dir = os.path.dirname(os.path.realpath(__file__))
  raw_train_dir = os.path.join(current_dir, "raw_train_dir")
  train_dir = os.path.join(current_dir, "train_dir") 
  valid_dir = os.path.join(current_dir, "valid_dir") 
  test_dir = os.path.join(current_dir, "test_dir") 

  if (os.path.exists(raw_train_dir) and
      os.path.exists(train_dir) and
      os.path.exists(valid_dir) and
      os.path.exists(test_dir)):
    print("Reloading existing datasets")
    raw_train_dataset = dc.data.DiskDataset(raw_train_dir)
    train_dataset = dc.data.DiskDataset(train_dir)
    valid_dataset = dc.data.DiskDataset(valid_dir)
    test_dataset = dc.data.DiskDataset(test_dir)
  else:
    print("Featurizing datasets")
    (raw_train_dataset, train_dataset, valid_dataset, test_dataset) = \
      gen_uv(UV_tasks, raw_train_dir, train_dir, valid_dir, test_dir,
                  shard_size=shard_size)

  transformers = get_transformers(raw_train_dataset)
</source>
</class>

<class classid="125" nclones="3" nlines="14" similarity="100">
<source file="systems/deepchem-2.3.0/examples/kinase/KINASE_tf_singletask.py" startline="38" endline="53" pcid="2151">
def task_model_builder(m_dir):
  return dc.models.TensorflowMultitaskRegressor(
      n_tasks=1,
      n_features=n_features,
      logdir=m_dir,
      layer_sizes=[1000] * n_layers,
      dropouts=[.25] * n_layers,
      weight_init_stddevs=[.02] * n_layers,
      bias_init_consts=[1.] * n_layers,
      learning_rate=.0003,
      penalty=.0001,
      penalty_type="l2",
      optimizer="adam",
      batch_size=100)


</source>
<source file="systems/deepchem-2.3.0/examples/factors/FACTORS_tf_singletask.py" startline="38" endline="53" pcid="2173">
def task_model_builder(m_dir):
  return dc.models.TensorflowMultitaskRegressor(
      n_tasks=1,
      n_features=n_features,
      logdir=m_dir,
      layer_sizes=[1000] * n_layers,
      dropouts=[.25] * n_layers,
      weight_init_stddevs=[.02] * n_layers,
      bias_init_consts=[1.] * n_layers,
      learning_rate=.0003,
      penalty=.0001,
      penalty_type="l2",
      optimizer="adam",
      batch_size=100)


</source>
<source file="systems/deepchem-2.3.0/examples/uv/UV_tf_singletask.py" startline="37" endline="52" pcid="2159">
def task_model_builder(m_dir):
  return dc.models.TensorflowMultitaskRegressor(
      n_tasks=1,
      n_features=n_features,
      logdir=m_dir,
      layer_sizes=[1000] * n_layers,
      dropouts=[.25] * n_layers,
      weight_init_stddevs=[.02] * n_layers,
      bias_init_consts=[1.] * n_layers,
      learning_rate=.0003,
      penalty=.0001,
      penalty_type="l2",
      optimizer="adam",
      batch_size=100)


</source>
</class>

</clones>

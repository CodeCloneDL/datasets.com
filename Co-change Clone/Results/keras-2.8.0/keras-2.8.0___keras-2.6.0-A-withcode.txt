<clonepair1>
<source file="systems/keras-2.8.0/keras/layers/local.py" startline="161" endline="259" pcid="3974"></source>
  def build(self, input_shape):
    if self.data_format == 'channels_first':
      input_dim, input_length = input_shape[1], input_shape[2]
    else:
      input_dim, input_length = input_shape[2], input_shape[1]

    if input_dim is None:
      raise ValueError(
          'Axis 2 of input should be fully-defined. '
          'Found shape:', input_shape)
    self.output_length = conv_utils.conv_output_length(input_length,
                                                       self.kernel_size[0],
                                                       self.padding,
                                                       self.strides[0])

    if self.output_length <= 0:
      raise ValueError(
          f'One of the dimensions in the output is <= 0 '
          f'due to downsampling in {self.name}. Consider '
          f'increasing the input size. '
          f'Received input shape {input_shape} which would produce '
          f'output shape with a zero or negative value in a '
          f'dimension.')

    if self.implementation == 1:
      self.kernel_shape = (self.output_length, self.kernel_size[0] * input_dim,
                           self.filters)

      self.kernel = self.add_weight(
          shape=self.kernel_shape,
          initializer=self.kernel_initializer,
          name='kernel',
          regularizer=self.kernel_regularizer,
          constraint=self.kernel_constraint)

    elif self.implementation == 2:
      if self.data_format == 'channels_first':
        self.kernel_shape = (input_dim, input_length, self.filters,
                             self.output_length)
      else:
        self.kernel_shape = (input_length, input_dim, self.output_length,
                             self.filters)

      self.kernel = self.add_weight(
          shape=self.kernel_shape,
          initializer=self.kernel_initializer,
          name='kernel',
          regularizer=self.kernel_regularizer,
          constraint=self.kernel_constraint)

      self.kernel_mask = get_locallyconnected_mask(
          input_shape=(input_length,),
          kernel_shape=self.kernel_size,
          strides=self.strides,
          padding=self.padding,
          data_format=self.data_format,
      )

    elif self.implementation == 3:
      self.kernel_shape = (self.output_length * self.filters,
                           input_length * input_dim)

      self.kernel_idxs = sorted(
          conv_utils.conv_kernel_idxs(
              input_shape=(input_length,),
              kernel_shape=self.kernel_size,
              strides=self.strides,
              padding=self.padding,
              filters_in=input_dim,
              filters_out=self.filters,
              data_format=self.data_format))

      self.kernel = self.add_weight(
          shape=(len(self.kernel_idxs),),
          initializer=self.kernel_initializer,
          name='kernel',
          regularizer=self.kernel_regularizer,
          constraint=self.kernel_constraint)

    else:
      raise ValueError('Unrecognized implementation mode: %d.' %
                       self.implementation)

    if self.use_bias:
      self.bias = self.add_weight(
          shape=(self.output_length, self.filters),
          initializer=self.bias_initializer,
          name='bias',
          regularizer=self.bias_regularizer,
          constraint=self.bias_constraint)
    else:
      self.bias = None

    if self.data_format == 'channels_first':
      self.input_spec = InputSpec(ndim=3, axes={1: input_dim})
    else:
      self.input_spec = InputSpec(ndim=3, axes={-1: input_dim})
    self.built = True

</clonepair1>

<clonepair1>
<source file="systems/keras-2.8.0/keras/layers/local.py" startline="478" endline="579" pcid="3980"></source>
  def build(self, input_shape):
    if self.data_format == 'channels_last':
      input_row, input_col = input_shape[1:-1]
      input_filter = input_shape[3]
    else:
      input_row, input_col = input_shape[2:]
      input_filter = input_shape[1]
    if input_row is None or input_col is None:
      raise ValueError('The spatial dimensions of the inputs to '
                       ' a LocallyConnected2D layer '
                       'should be fully-defined, but layer received '
                       'the inputs shape ' + str(input_shape))
    output_row = conv_utils.conv_output_length(input_row, self.kernel_size[0],
                                               self.padding, self.strides[0])
    output_col = conv_utils.conv_output_length(input_col, self.kernel_size[1],
                                               self.padding, self.strides[1])
    self.output_row = output_row
    self.output_col = output_col

    if self.output_row <= 0 or self.output_col <= 0:
      raise ValueError(
          f'One of the dimensions in the output is <= 0 '
          f'due to downsampling in {self.name}. Consider '
          f'increasing the input size. '
          f'Received input shape {input_shape} which would produce '
          f'output shape with a zero or negative value in a '
          f'dimension.')

    if self.implementation == 1:
      self.kernel_shape = (output_row * output_col, self.kernel_size[0] *
                           self.kernel_size[1] * input_filter, self.filters)

      self.kernel = self.add_weight(
          shape=self.kernel_shape,
          initializer=self.kernel_initializer,
          name='kernel',
          regularizer=self.kernel_regularizer,
          constraint=self.kernel_constraint)

    elif self.implementation == 2:
      if self.data_format == 'channels_first':
        self.kernel_shape = (input_filter, input_row, input_col, self.filters,
                             self.output_row, self.output_col)
      else:
        self.kernel_shape = (input_row, input_col, input_filter,
                             self.output_row, self.output_col, self.filters)

      self.kernel = self.add_weight(
          shape=self.kernel_shape,
          initializer=self.kernel_initializer,
          name='kernel',
          regularizer=self.kernel_regularizer,
          constraint=self.kernel_constraint)

      self.kernel_mask = get_locallyconnected_mask(
          input_shape=(input_row, input_col),
          kernel_shape=self.kernel_size,
          strides=self.strides,
          padding=self.padding,
          data_format=self.data_format,
      )

    elif self.implementation == 3:
      self.kernel_shape = (self.output_row * self.output_col * self.filters,
                           input_row * input_col * input_filter)

      self.kernel_idxs = sorted(
          conv_utils.conv_kernel_idxs(
              input_shape=(input_row, input_col),
              kernel_shape=self.kernel_size,
              strides=self.strides,
              padding=self.padding,
              filters_in=input_filter,
              filters_out=self.filters,
              data_format=self.data_format))

      self.kernel = self.add_weight(
          shape=(len(self.kernel_idxs),),
          initializer=self.kernel_initializer,
          name='kernel',
          regularizer=self.kernel_regularizer,
          constraint=self.kernel_constraint)

    else:
      raise ValueError('Unrecognized implementation mode: %d.' %
                       self.implementation)

    if self.use_bias:
      self.bias = self.add_weight(
          shape=(output_row, output_col, self.filters),
          initializer=self.bias_initializer,
          name='bias',
          regularizer=self.bias_regularizer,
          constraint=self.bias_constraint)
    else:
      self.bias = None
    if self.data_format == 'channels_first':
      self.input_spec = InputSpec(ndim=4, axes={1: input_filter})
    else:
      self.input_spec = InputSpec(ndim=4, axes={-1: input_filter})
    self.built = True

</clonepair1>
<clonepair2>
<source file="systems/keras-2.8.0/keras/layers/recurrent_v2.py" startline="700" endline="828" pcid="4838"></source>
def gru_with_backend_selection(inputs, init_h, kernel, recurrent_kernel, bias,
                               mask, time_major, go_backwards, sequence_lengths,
                               zero_output_for_mask):
  """Call the GRU with optimized backend kernel selection.

  Under the hood, this function will create two TF function, one with the most
  generic kernel and can run on all device condition, and the second one with
  cuDNN specific kernel, which can only run on GPU.

  The first function will be called with normal_lstm_params, while the second
  function is not called, but only registered in the graph. The Grappler will
  do the proper graph rewrite and swap the optimized TF function based on the
  device placement.

  Args:
    inputs: Input tensor of GRU layer.
    init_h: Initial state tensor for the cell output.
    kernel: Weights for cell kernel.
    recurrent_kernel: Weights for cell recurrent kernel.
    bias: Weights for cell kernel bias and recurrent bias. Only recurrent bias
      is used in this case.
    mask: Boolean tensor for mask out the steps within sequence.
      An individual `True` entry indicates that the corresponding timestep
      should be utilized, while a `False` entry indicates that the corresponding
      timestep should be ignored.
    time_major: Boolean, whether the inputs are in the format of
      [time, batch, feature] or [batch, time, feature].
    go_backwards: Boolean (default False). If True, process the input sequence
      backwards and return the reversed sequence.
    sequence_lengths: The lengths of all sequences coming from a variable length
      input, such as ragged tensors. If the input has a fixed timestep size,
      this should be None.
    zero_output_for_mask: Boolean, whether to output zero for masked timestep.

  Returns:
    List of output tensors, same as standard_gru.
  """
  params = {
      'inputs': inputs,
      'init_h': init_h,
      'kernel': kernel,
      'recurrent_kernel': recurrent_kernel,
      'bias': bias,
      'mask': mask,
      'time_major': time_major,
      'go_backwards': go_backwards,
      'sequence_lengths': sequence_lengths,
      'zero_output_for_mask': zero_output_for_mask,
  }

  def gpu_gru_with_fallback(inputs, init_h, kernel, recurrent_kernel, bias,
                            mask, time_major, go_backwards, sequence_lengths,
                            zero_output_for_mask):
    """Use cuDNN kernel when mask is none or strictly right padded."""
    if mask is None:
      return gpu_gru(
          inputs=inputs,
          init_h=init_h,
          kernel=kernel,
          recurrent_kernel=recurrent_kernel,
          bias=bias,
          mask=mask,
          time_major=time_major,
          go_backwards=go_backwards,
          sequence_lengths=sequence_lengths)

    def cudnn_gru_fn():
      return gpu_gru(
          inputs=inputs,
          init_h=init_h,
          kernel=kernel,
          recurrent_kernel=recurrent_kernel,
          bias=bias,
          mask=mask,
          time_major=time_major,
          go_backwards=go_backwards,
          sequence_lengths=sequence_lengths)

    def standard_gru_fn():
      return standard_gru(
          inputs=inputs,
          init_h=init_h,
          kernel=kernel,
          recurrent_kernel=recurrent_kernel,
          bias=bias,
          mask=mask,
          time_major=time_major,
          go_backwards=go_backwards,
          sequence_lengths=sequence_lengths,
          zero_output_for_mask=zero_output_for_mask)

    return tf.cond(
        is_cudnn_supported_inputs(mask, time_major),
        true_fn=cudnn_gru_fn,
        false_fn=standard_gru_fn)

  if _use_new_code():
    # Chooses the implementation dynamically based on the running device.
    (last_output, outputs, new_h,
     runtime) = tf.__internal__.execute_fn_for_device(
         {
             _CPU_DEVICE_NAME: lambda: standard_gru(**params),
             _GPU_DEVICE_NAME: lambda: gpu_gru_with_fallback(**params)
         }, lambda: standard_gru(**params))
  else:
    # Each time a `tf.function` is called, we will give it a unique
    # identifiable API name, so that Grappler won't get confused when it
    # sees multiple GRU layers added into same graph, and it will be able
    # to pair up the different implementations across them.
    api_name = 'gru_' + str(uuid.uuid4())
    supportive_attribute = {
        'time_major': time_major,
        'go_backwards': go_backwards,
    }
    defun_standard_gru = _generate_defun_backend(api_name, _CPU_DEVICE_NAME,
                                                 standard_gru,
                                                 supportive_attribute)
    defun_gpu_gru = _generate_defun_backend(api_name, _GPU_DEVICE_NAME,
                                            gpu_gru_with_fallback,
                                            supportive_attribute)

    # Call the normal GRU impl and register the cuDNN impl function. The
    # grappler will kick in during session execution to optimize the graph.
    last_output, outputs, new_h, runtime = defun_standard_gru(**params)
    _function_register(defun_gpu_gru, **params)

  return last_output, outputs, new_h, runtime


</clonepair2>

<clonepair2>
<source file="systems/keras-2.8.0/keras/layers/recurrent_v2.py" startline="1521" endline="1655" pcid="4851"></source>
def lstm_with_backend_selection(inputs, init_h, init_c, kernel,
                                recurrent_kernel, bias, mask, time_major,
                                go_backwards, sequence_lengths,
                                zero_output_for_mask):
  """Call the LSTM with optimized backend kernel selection.

  Under the hood, this function will create two TF function, one with the most
  generic kernel and can run on all device condition, and the second one with
  cuDNN specific kernel, which can only run on GPU.

  The first function will be called with normal_lstm_params, while the second
  function is not called, but only registered in the graph. The Grappler will
  do the proper graph rewrite and swap the optimized TF function based on the
  device placement.

  Args:
    inputs: Input tensor of LSTM layer.
    init_h: Initial state tensor for the cell output.
    init_c: Initial state tensor for the cell hidden state.
    kernel: Weights for cell kernel.
    recurrent_kernel: Weights for cell recurrent kernel.
    bias: Weights for cell kernel bias and recurrent bias. Only recurrent bias
      is used in this case.
    mask: Boolean tensor for mask out the steps within sequence.
      An individual `True` entry indicates that the corresponding timestep
      should be utilized, while a `False` entry indicates that the corresponding
      timestep should be ignored.
    time_major: Boolean, whether the inputs are in the format of
      [time, batch, feature] or [batch, time, feature].
    go_backwards: Boolean (default False). If True, process the input sequence
      backwards and return the reversed sequence.
    sequence_lengths: The lengths of all sequences coming from a variable length
      input, such as ragged tensors. If the input has a fixed timestep size,
      this should be None.
    zero_output_for_mask: Boolean, whether to output zero for masked timestep.

  Returns:
    List of output tensors, same as standard_lstm.
  """
  params = {
      'inputs': inputs,
      'init_h': init_h,
      'init_c': init_c,
      'kernel': kernel,
      'recurrent_kernel': recurrent_kernel,
      'bias': bias,
      'mask': mask,
      'time_major': time_major,
      'go_backwards': go_backwards,
      'sequence_lengths': sequence_lengths,
      'zero_output_for_mask': zero_output_for_mask,
  }

  def gpu_lstm_with_fallback(inputs, init_h, init_c, kernel, recurrent_kernel,
                             bias, mask, time_major, go_backwards,
                             sequence_lengths, zero_output_for_mask):
    """Use cuDNN kernel when mask is none or strictly right padded."""
    if mask is None:
      return gpu_lstm(
          inputs=inputs,
          init_h=init_h,
          init_c=init_c,
          kernel=kernel,
          recurrent_kernel=recurrent_kernel,
          bias=bias,
          mask=mask,
          time_major=time_major,
          go_backwards=go_backwards,
          sequence_lengths=sequence_lengths)

    def cudnn_lstm_fn():
      return gpu_lstm(
          inputs=inputs,
          init_h=init_h,
          init_c=init_c,
          kernel=kernel,
          recurrent_kernel=recurrent_kernel,
          bias=bias,
          mask=mask,
          time_major=time_major,
          go_backwards=go_backwards,
          sequence_lengths=sequence_lengths)

    def stardard_lstm_fn():
      return standard_lstm(
          inputs=inputs,
          init_h=init_h,
          init_c=init_c,
          kernel=kernel,
          recurrent_kernel=recurrent_kernel,
          bias=bias,
          mask=mask,
          time_major=time_major,
          go_backwards=go_backwards,
          sequence_lengths=sequence_lengths,
          zero_output_for_mask=zero_output_for_mask)

    return tf.cond(
        is_cudnn_supported_inputs(mask, time_major),
        true_fn=cudnn_lstm_fn,
        false_fn=stardard_lstm_fn)

  if _use_new_code():
    # Chooses the implementation dynamically based on the running device.
    (last_output, outputs, new_h, new_c,
     runtime) = tf.__internal__.execute_fn_for_device(
         {
             _CPU_DEVICE_NAME: lambda: standard_lstm(**params),
             _GPU_DEVICE_NAME: lambda: gpu_lstm_with_fallback(**params)
         }, lambda: standard_lstm(**params))
  else:
    # Each time a `tf.function` is called, we will give it a unique
    # identifiable API name, so that Grappler won't get confused when it
    # sees multiple LSTM layers added into same graph, and it will be able
    # to pair up the different implementations across them.
    api_name = 'lstm_' + str(uuid.uuid4())
    supportive_attribute = {
        'time_major': time_major,
        'go_backwards': go_backwards,
    }
    defun_standard_lstm = _generate_defun_backend(api_name, _CPU_DEVICE_NAME,
                                                  standard_lstm,
                                                  supportive_attribute)
    defun_gpu_lstm = _generate_defun_backend(api_name, _GPU_DEVICE_NAME,
                                             gpu_lstm_with_fallback,
                                             supportive_attribute)

    # Call the normal LSTM impl and register the cuDNN impl function. The
    # grappler will kick in during session execution to optimize the graph.
    last_output, outputs, new_h, new_c, runtime = defun_standard_lstm(**params)
    _function_register(defun_gpu_lstm, **params)

  return last_output, outputs, new_h, new_c, runtime


</clonepair2>
<clonepair3>
<source file="systems/keras-2.8.0/keras/layers/local_test.py" startline="292" endline="388" pcid="4251"></source>
  def test_locallyconnected_implementation(self, width, data_format):
    with self.cached_session():
      num_samples = 4
      num_classes = 3
      num_epochs = 2

      np.random.seed(1)
      tf_test_util.random_seed.set_seed(1)
      # Following code generates sparse targets and converts them
      # to one-hot encoded vectors
      # Create sparse targets eg. [0,1,2]
      sparse_targets = np.random.randint(0, num_classes, (num_samples,))

      # Convert to one-hot encoding
      # Final targets:
      # [[ 1. 0. 0. ]
      #  [ 0. 1. 0. ]
      #  [ 0. 0. 1. ]]

      targets = np.zeros((sparse_targets.size, num_classes))
      targets[np.arange(sparse_targets.size), sparse_targets] = 1
      height = 7
      filters = 2
      inputs = get_inputs(data_format, filters, height, num_samples, width)

      kernel_x = (3,)
      kernel_y = () if width == 1 else (2,)
      stride_x = (1,)
      stride_y = () if width == 1 else (3,)
      layers = 2

      kwargs = {
          'layers': layers,
          'filters': filters,
          'kernel_size': kernel_x + kernel_y,
          'strides': stride_x + stride_y,
          'data_format': data_format,
          'num_classes': num_classes
      }

      model_1 = get_model(implementation=1, **kwargs)
      model_2 = get_model(implementation=2, **kwargs)
      model_3 = get_model(implementation=3, **kwargs)

      # Build models.
      model_1.train_on_batch(inputs, targets)
      model_2.train_on_batch(inputs, targets)
      model_3.train_on_batch(inputs, targets)

      # Copy weights.
      copy_model_weights(model_from=model_2, model_to=model_1)
      copy_model_weights(model_from=model_2, model_to=model_3)

      # Compare outputs at initialization.
      out_1 = model_1(inputs)
      out_2 = model_2(inputs)
      out_3 = model_3(inputs)

      self.assertAllCloseAccordingToType(
          out_2, out_1, rtol=1e-5, atol=1e-5)
      self.assertAllCloseAccordingToType(
          out_2, out_3, rtol=1e-5, atol=1e-5)
      self.assertAllCloseAccordingToType(
          out_1, out_3, rtol=1e-5, atol=1e-5)

      # Train.
      model_1.fit(
          x=inputs,
          y=targets,
          epochs=num_epochs,
          batch_size=num_samples,
          shuffle=False)
      model_2.fit(
          x=inputs,
          y=targets,
          epochs=num_epochs,
          batch_size=num_samples,
          shuffle=False)
      model_3.fit(
          x=inputs,
          y=targets,
          epochs=num_epochs,
          batch_size=num_samples,
          shuffle=False)

      # Compare outputs after a few training steps.
      out_1 = model_1(inputs)
      out_2 = model_2(inputs)
      out_3 = model_3(inputs)

      self.assertAllCloseAccordingToType(
          out_2, out_1, atol=2e-4)
      self.assertAllCloseAccordingToType(
          out_2, out_3, atol=2e-4)
      self.assertAllCloseAccordingToType(
          out_1, out_3, atol=2e-4)

</clonepair3>

<clonepair3>
<source file="systems/keras-2.8.0/keras/layers/local_test.py" startline="407" endline="493" pcid="4252"></source>
  def test_locallyconnected_save(self, width, data_format):
    with self.cached_session():
      num_samples = 4
      num_classes = 3
      num_epochs = 2

      np.random.seed(1)
      tf_test_util.random_seed.set_seed(1)
      # Following code generates sparse targets and converts them
      # to one-hot encoded vectors
      # Create sparse targets eg. [0,1,2]
      sparse_targets = np.random.randint(0, num_classes, (num_samples,))

      # Convert to one-hot encoding
      # Final targets:
      # [[ 1. 0. 0. ]
      #  [ 0. 1. 0. ]
      #  [ 0. 0. 1. ]]

      targets = np.zeros((sparse_targets.size, num_classes))
      targets[np.arange(sparse_targets.size), sparse_targets] = 1

      height = 7
      filters = 2
      inputs = get_inputs(data_format, filters, height, num_samples, width)

      kernel_x = (3,)
      kernel_y = () if width == 1 else (2,)
      stride_x = (1,)
      stride_y = () if width == 1 else (3,)
      layers = 2

      kwargs = {
          'layers': layers,
          'filters': filters,
          'kernel_size': kernel_x + kernel_y,
          'strides': stride_x + stride_y,
          'data_format': data_format,
          'num_classes': num_classes
      }

      model_1 = get_model_saveable(implementation=1, **kwargs)
      model_2 = get_model_saveable(implementation=2, **kwargs)
      model_3 = get_model_saveable(implementation=3, **kwargs)

      # Train.
      model_1.fit(
          x=inputs,
          y=targets,
          epochs=num_epochs,
          batch_size=num_samples,
          shuffle=False)
      model_2.fit(
          x=inputs,
          y=targets,
          epochs=num_epochs,
          batch_size=num_samples,
          shuffle=False)
      model_3.fit(
          x=inputs,
          y=targets,
          epochs=num_epochs,
          batch_size=num_samples,
          shuffle=False)

      out_1_before = model_1(inputs)
      out_2_before = model_2(inputs)
      out_3_before = model_3(inputs)

      path_1 = os.path.join(self.get_temp_dir(), 'model_1_path')
      model_1.save(path_1)
      model_1 = keras.models.load_model(path_1, custom_objects={'xent': xent})
      path_2 = os.path.join(self.get_temp_dir(), 'model_2_path')
      model_2.save(path_2)
      model_2 = keras.models.load_model(path_2, custom_objects={'xent': xent})
      path_3 = os.path.join(self.get_temp_dir(), 'model_3_path')
      model_3.save(path_3)
      model_3 = keras.models.load_model(path_3, custom_objects={'xent': xent})

      out_1_after = model_1(inputs)
      out_2_after = model_2(inputs)
      out_3_after = model_3(inputs)

      self.assertAllCloseAccordingToType(out_1_before, out_1_after, atol=2e-4)
      self.assertAllCloseAccordingToType(out_2_before, out_2_after, atol=2e-4)
      self.assertAllCloseAccordingToType(out_3_before, out_3_after, atol=2e-4)

</clonepair3>
<clonepair4>
<source file="systems/keras-2.8.0/keras/layers/recurrent_v2.py" startline="1060" endline="1134" pcid="4843"></source>
  def __init__(self,
               units,
               activation='tanh',
               recurrent_activation='sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               unit_forget_bias=True,
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               return_sequences=False,
               return_state=False,
               go_backwards=False,
               stateful=False,
               time_major=False,
               unroll=False,
               **kwargs):
    # return_runtime is a flag for testing, which shows the real backend
    # implementation chosen by grappler in graph mode.
    self.return_runtime = kwargs.pop('return_runtime', False)

    super(LSTM, self).__init__(
        units,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        unit_forget_bias=unit_forget_bias,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        implementation=kwargs.pop('implementation', 2),
        return_sequences=return_sequences,
        return_state=return_state,
        go_backwards=go_backwards,
        stateful=stateful,
        time_major=time_major,
        unroll=unroll,
        **kwargs)

    self.state_spec = [
        InputSpec(shape=(None, dim)) for dim in (self.units, self.units)
    ]
    self._could_use_gpu_kernel = (
        self.activation in (activations.tanh, tf.tanh) and
        self.recurrent_activation in (activations.sigmoid, tf.sigmoid) and
        recurrent_dropout == 0 and not unroll and use_bias and
        tf.compat.v1.executing_eagerly_outside_functions())
    if tf.config.list_logical_devices('GPU'):
      # Only show the message when there is GPU available, user will not care
      # about the cuDNN if there isn't any GPU.
      if self._could_use_gpu_kernel:
        logging.debug(_CUDNN_AVAILABLE_MSG % self.name)
      else:
        logging.warning(_CUDNN_NOT_AVAILABLE_MSG % self.name)

    if _use_new_code():
      self._defun_wrapper = _DefunWrapper(time_major, go_backwards, 'lstm')

</clonepair4>

<clonepair4>
<source file="systems/keras-2.8.0/keras/layers/recurrent.py" startline="2763" endline="2828" pcid="4370"></source>
  def __init__(self,
               units,
               activation='tanh',
               recurrent_activation='hard_sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               unit_forget_bias=True,
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               return_sequences=False,
               return_state=False,
               go_backwards=False,
               stateful=False,
               unroll=False,
               **kwargs):
    implementation = kwargs.pop('implementation', 1)
    if implementation == 0:
      logging.warning('`implementation=0` has been deprecated, '
                      'and now defaults to `implementation=1`.'
                      'Please update your layer call.')
    if 'enable_caching_device' in kwargs:
      cell_kwargs = {'enable_caching_device':
                     kwargs.pop('enable_caching_device')}
    else:
      cell_kwargs = {}
    cell = LSTMCell(
        units,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        unit_forget_bias=unit_forget_bias,
        bias_initializer=bias_initializer,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        implementation=implementation,
        dtype=kwargs.get('dtype'),
        trainable=kwargs.get('trainable', True),
        **cell_kwargs)
    super(LSTM, self).__init__(
        cell,
        return_sequences=return_sequences,
        return_state=return_state,
        go_backwards=go_backwards,
        stateful=stateful,
        unroll=unroll,
        **kwargs)
    self.activity_regularizer = regularizers.get(activity_regularizer)
    self.input_spec = [InputSpec(ndim=3)]

</clonepair4>
<clonepair5>
<source file="systems/keras-2.8.0/keras/layers/recurrent_v2.py" startline="336" endline="407" pcid="4831"></source>
  def __init__(self,
               units,
               activation='tanh',
               recurrent_activation='sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               return_sequences=False,
               return_state=False,
               go_backwards=False,
               stateful=False,
               unroll=False,
               time_major=False,
               reset_after=True,
               **kwargs):
    # return_runtime is a flag for testing, which shows the real backend
    # implementation chosen by grappler in graph mode.
    self._return_runtime = kwargs.pop('return_runtime', False)

    super(GRU, self).__init__(
        units,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        implementation=kwargs.pop('implementation', 2),
        return_sequences=return_sequences,
        return_state=return_state,
        go_backwards=go_backwards,
        stateful=stateful,
        unroll=unroll,
        time_major=time_major,
        reset_after=reset_after,
        **kwargs)
    # GPU kernel uses following setting by default and not configurable.
    self._could_use_gpu_kernel = (
        self.activation in (activations.tanh, tf.tanh) and
        self.recurrent_activation in (activations.sigmoid, tf.sigmoid) and
        recurrent_dropout == 0 and not unroll and use_bias and
        reset_after and tf.compat.v1.executing_eagerly_outside_functions())
    if tf.config.list_logical_devices('GPU'):
      # Only show the message when there is GPU available, user will not care
      # about the cuDNN if there isn't any GPU.
      if self._could_use_gpu_kernel:
        logging.debug(_CUDNN_AVAILABLE_MSG % self.name)
      else:
        logging.warning(_CUDNN_NOT_AVAILABLE_MSG % self.name)

    if _use_new_code():
      self._defun_wrapper = _DefunWrapper(time_major, go_backwards, 'gru')

</clonepair5>

<clonepair5>
<source file="systems/keras-2.8.0/keras/layers/recurrent.py" startline="2763" endline="2828" pcid="4370"></source>
  def __init__(self,
               units,
               activation='tanh',
               recurrent_activation='hard_sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               unit_forget_bias=True,
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               return_sequences=False,
               return_state=False,
               go_backwards=False,
               stateful=False,
               unroll=False,
               **kwargs):
    implementation = kwargs.pop('implementation', 1)
    if implementation == 0:
      logging.warning('`implementation=0` has been deprecated, '
                      'and now defaults to `implementation=1`.'
                      'Please update your layer call.')
    if 'enable_caching_device' in kwargs:
      cell_kwargs = {'enable_caching_device':
                     kwargs.pop('enable_caching_device')}
    else:
      cell_kwargs = {}
    cell = LSTMCell(
        units,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        unit_forget_bias=unit_forget_bias,
        bias_initializer=bias_initializer,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        implementation=implementation,
        dtype=kwargs.get('dtype'),
        trainable=kwargs.get('trainable', True),
        **cell_kwargs)
    super(LSTM, self).__init__(
        cell,
        return_sequences=return_sequences,
        return_state=return_state,
        go_backwards=go_backwards,
        stateful=stateful,
        unroll=unroll,
        **kwargs)
    self.activity_regularizer = regularizers.get(activity_regularizer)
    self.input_spec = [InputSpec(ndim=3)]

</clonepair5>
<clonepair6>
<source file="systems/keras-2.8.0/keras/layers/recurrent_v2.py" startline="336" endline="407" pcid="4831"></source>
  def __init__(self,
               units,
               activation='tanh',
               recurrent_activation='sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               return_sequences=False,
               return_state=False,
               go_backwards=False,
               stateful=False,
               unroll=False,
               time_major=False,
               reset_after=True,
               **kwargs):
    # return_runtime is a flag for testing, which shows the real backend
    # implementation chosen by grappler in graph mode.
    self._return_runtime = kwargs.pop('return_runtime', False)

    super(GRU, self).__init__(
        units,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        implementation=kwargs.pop('implementation', 2),
        return_sequences=return_sequences,
        return_state=return_state,
        go_backwards=go_backwards,
        stateful=stateful,
        unroll=unroll,
        time_major=time_major,
        reset_after=reset_after,
        **kwargs)
    # GPU kernel uses following setting by default and not configurable.
    self._could_use_gpu_kernel = (
        self.activation in (activations.tanh, tf.tanh) and
        self.recurrent_activation in (activations.sigmoid, tf.sigmoid) and
        recurrent_dropout == 0 and not unroll and use_bias and
        reset_after and tf.compat.v1.executing_eagerly_outside_functions())
    if tf.config.list_logical_devices('GPU'):
      # Only show the message when there is GPU available, user will not care
      # about the cuDNN if there isn't any GPU.
      if self._could_use_gpu_kernel:
        logging.debug(_CUDNN_AVAILABLE_MSG % self.name)
      else:
        logging.warning(_CUDNN_NOT_AVAILABLE_MSG % self.name)

    if _use_new_code():
      self._defun_wrapper = _DefunWrapper(time_major, go_backwards, 'gru')

</clonepair6>

<clonepair6>
<source file="systems/keras-2.8.0/keras/layers/recurrent.py" startline="2088" endline="2153" pcid="4337"></source>
  def __init__(self,
               units,
               activation='tanh',
               recurrent_activation='hard_sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               return_sequences=False,
               return_state=False,
               go_backwards=False,
               stateful=False,
               unroll=False,
               reset_after=False,
               **kwargs):
    implementation = kwargs.pop('implementation', 1)
    if implementation == 0:
      logging.warning('`implementation=0` has been deprecated, '
                      'and now defaults to `implementation=1`.'
                      'Please update your layer call.')
    if 'enable_caching_device' in kwargs:
      cell_kwargs = {'enable_caching_device':
                     kwargs.pop('enable_caching_device')}
    else:
      cell_kwargs = {}
    cell = GRUCell(
        units,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        implementation=implementation,
        reset_after=reset_after,
        dtype=kwargs.get('dtype'),
        trainable=kwargs.get('trainable', True),
        **cell_kwargs)
    super(GRU, self).__init__(
        cell,
        return_sequences=return_sequences,
        return_state=return_state,
        go_backwards=go_backwards,
        stateful=stateful,
        unroll=unroll,
        **kwargs)
    self.activity_regularizer = regularizers.get(activity_regularizer)
    self.input_spec = [InputSpec(ndim=3)]

</clonepair6>
<clonepair7>
<source file="systems/keras-2.8.0/keras/layers/convolutional_recurrent.py" startline="1410" endline="1466" pcid="4032"></source>
  def __init__(self,
               filters,
               kernel_size,
               strides=(1, 1, 1),
               padding='valid',
               data_format=None,
               dilation_rate=(1, 1, 1),
               activation='tanh',
               recurrent_activation='hard_sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               unit_forget_bias=True,
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               return_sequences=False,
               return_state=False,
               go_backwards=False,
               stateful=False,
               dropout=0.0,
               recurrent_dropout=0.0,
               **kwargs):
    super(ConvLSTM3D, self).__init__(
        rank=3,
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        unit_forget_bias=unit_forget_bias,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        return_sequences=return_sequences,
        return_state=return_state,
        go_backwards=go_backwards,
        stateful=stateful,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        **kwargs)
</clonepair7>

<clonepair7>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1314" endline="1478" pcid="3282"></source>
def separable_conv2d(inputs,
                     filters,
                     kernel_size,
                     strides=(1, 1),
                     padding='valid',
                     data_format='channels_last',
                     dilation_rate=(1, 1),
                     depth_multiplier=1,
                     activation=None,
                     use_bias=True,
                     depthwise_initializer=None,
                     pointwise_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     depthwise_regularizer=None,
                     pointwise_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     depthwise_constraint=None,
                     pointwise_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for the depthwise separable 2D convolution layer.

  This layer performs a depthwise convolution that acts separately on
  channels, followed by a pointwise convolution that mixes channels.
  If `use_bias` is True and a bias initializer is provided,
  it adds a bias vector to the output.
  It then optionally applies an activation function to produce the final output.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A tuple or list of 2 integers specifying the spatial
      dimensions of the filters. Can be a single integer to specify the same
      value for all spatial dimensions.
    strides: A tuple or list of 2 positive integers specifying the strides
      of the convolution. Can be a single integer to specify the same value for
      all spatial dimensions.
      Specifying any `stride` value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, height, width)`.

    dilation_rate: An integer or tuple/list of 2 integers, specifying
      the dilation rate to use for dilated convolution.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    depth_multiplier: The number of depthwise convolution output channels for
      each input channel. The total number of depthwise convolution output
      channels will be equal to `num_filters_in * depth_multiplier`.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    depthwise_initializer: An initializer for the depthwise convolution kernel.
    pointwise_initializer: An initializer for the pointwise convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    depthwise_regularizer: Optional regularizer for the depthwise
      convolution kernel.
    pointwise_regularizer: Optional regularizer for the pointwise
      convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    depthwise_constraint: Optional projection function to be applied to the
        depthwise kernel after being updated by an `Optimizer` (e.g. used for
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    pointwise_constraint: Optional projection function to be applied to the
        pointwise kernel after being updated by an `Optimizer`.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.SeparableConv2D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.separable_conv2d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.SeparableConv2D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.separable_conv2d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.SeparableConv2D` instead.',
      stacklevel=2)
  layer = SeparableConv2D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      depth_multiplier=depth_multiplier,
      activation=activation,
      use_bias=use_bias,
      depthwise_initializer=depthwise_initializer,
      pointwise_initializer=pointwise_initializer,
      bias_initializer=bias_initializer,
      depthwise_regularizer=depthwise_regularizer,
      pointwise_regularizer=pointwise_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      depthwise_constraint=depthwise_constraint,
      pointwise_constraint=pointwise_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair7>
<clonepair8>
<source file="systems/keras-2.8.0/keras/layers/convolutional_recurrent.py" startline="1410" endline="1466" pcid="4032"></source>
  def __init__(self,
               filters,
               kernel_size,
               strides=(1, 1, 1),
               padding='valid',
               data_format=None,
               dilation_rate=(1, 1, 1),
               activation='tanh',
               recurrent_activation='hard_sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               unit_forget_bias=True,
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               return_sequences=False,
               return_state=False,
               go_backwards=False,
               stateful=False,
               dropout=0.0,
               recurrent_dropout=0.0,
               **kwargs):
    super(ConvLSTM3D, self).__init__(
        rank=3,
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        unit_forget_bias=unit_forget_bias,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        return_sequences=return_sequences,
        return_state=return_state,
        go_backwards=go_backwards,
        stateful=stateful,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        **kwargs)
</clonepair8>

<clonepair8>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1152" endline="1311" pcid="3281"></source>
def separable_conv1d(inputs,
                     filters,
                     kernel_size,
                     strides=1,
                     padding='valid',
                     data_format='channels_last',
                     dilation_rate=1,
                     depth_multiplier=1,
                     activation=None,
                     use_bias=True,
                     depthwise_initializer=None,
                     pointwise_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     depthwise_regularizer=None,
                     pointwise_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     depthwise_constraint=None,
                     pointwise_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for the depthwise separable 1D convolution layer.

  This layer performs a depthwise convolution that acts separately on
  channels, followed by a pointwise convolution that mixes channels.
  If `use_bias` is True and a bias initializer is provided,
  it adds a bias vector to the output.
  It then optionally applies an activation function to produce the final output.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A single integer specifying the spatial
      dimensions of the filters.
    strides: A single integer specifying the strides
      of the convolution.
      Specifying any `stride` value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, length, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, length)`.
    dilation_rate: A single integer, specifying
      the dilation rate to use for dilated convolution.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    depth_multiplier: The number of depthwise convolution output channels for
      each input channel. The total number of depthwise convolution output
      channels will be equal to `num_filters_in * depth_multiplier`.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    depthwise_initializer: An initializer for the depthwise convolution kernel.
    pointwise_initializer: An initializer for the pointwise convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    depthwise_regularizer: Optional regularizer for the depthwise
      convolution kernel.
    pointwise_regularizer: Optional regularizer for the pointwise
      convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    depthwise_constraint: Optional projection function to be applied to the
        depthwise kernel after being updated by an `Optimizer` (e.g. used for
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    pointwise_constraint: Optional projection function to be applied to the
        pointwise kernel after being updated by an `Optimizer`.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.SeparableConv1D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.separable_conv1d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.SeparableConv1D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.separable_conv1d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.SeparableConv1D` instead.',
      stacklevel=2)
  layer = SeparableConv1D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      depth_multiplier=depth_multiplier,
      activation=activation,
      use_bias=use_bias,
      depthwise_initializer=depthwise_initializer,
      pointwise_initializer=pointwise_initializer,
      bias_initializer=bias_initializer,
      depthwise_regularizer=depthwise_regularizer,
      pointwise_regularizer=pointwise_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      depthwise_constraint=depthwise_constraint,
      pointwise_constraint=pointwise_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair8>
<clonepair9>
<source file="systems/keras-2.8.0/keras/layers/convolutional_recurrent.py" startline="1078" endline="1136" pcid="4030"></source>
  def __init__(self,
               filters,
               kernel_size,
               strides=1,
               padding='valid',
               data_format=None,
               dilation_rate=1,
               activation='tanh',
               recurrent_activation='hard_sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               unit_forget_bias=True,
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               return_sequences=False,
               return_state=False,
               go_backwards=False,
               stateful=False,
               dropout=0.0,
               recurrent_dropout=0.0,
               **kwargs):
    super(ConvLSTM1D, self).__init__(
        rank=1,
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        unit_forget_bias=unit_forget_bias,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        return_sequences=return_sequences,
        return_state=return_state,
        go_backwards=go_backwards,
        stateful=stateful,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        **kwargs)


</clonepair9>

<clonepair9>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1314" endline="1478" pcid="3282"></source>
def separable_conv2d(inputs,
                     filters,
                     kernel_size,
                     strides=(1, 1),
                     padding='valid',
                     data_format='channels_last',
                     dilation_rate=(1, 1),
                     depth_multiplier=1,
                     activation=None,
                     use_bias=True,
                     depthwise_initializer=None,
                     pointwise_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     depthwise_regularizer=None,
                     pointwise_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     depthwise_constraint=None,
                     pointwise_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for the depthwise separable 2D convolution layer.

  This layer performs a depthwise convolution that acts separately on
  channels, followed by a pointwise convolution that mixes channels.
  If `use_bias` is True and a bias initializer is provided,
  it adds a bias vector to the output.
  It then optionally applies an activation function to produce the final output.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A tuple or list of 2 integers specifying the spatial
      dimensions of the filters. Can be a single integer to specify the same
      value for all spatial dimensions.
    strides: A tuple or list of 2 positive integers specifying the strides
      of the convolution. Can be a single integer to specify the same value for
      all spatial dimensions.
      Specifying any `stride` value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, height, width)`.

    dilation_rate: An integer or tuple/list of 2 integers, specifying
      the dilation rate to use for dilated convolution.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    depth_multiplier: The number of depthwise convolution output channels for
      each input channel. The total number of depthwise convolution output
      channels will be equal to `num_filters_in * depth_multiplier`.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    depthwise_initializer: An initializer for the depthwise convolution kernel.
    pointwise_initializer: An initializer for the pointwise convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    depthwise_regularizer: Optional regularizer for the depthwise
      convolution kernel.
    pointwise_regularizer: Optional regularizer for the pointwise
      convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    depthwise_constraint: Optional projection function to be applied to the
        depthwise kernel after being updated by an `Optimizer` (e.g. used for
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    pointwise_constraint: Optional projection function to be applied to the
        pointwise kernel after being updated by an `Optimizer`.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.SeparableConv2D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.separable_conv2d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.SeparableConv2D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.separable_conv2d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.SeparableConv2D` instead.',
      stacklevel=2)
  layer = SeparableConv2D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      depth_multiplier=depth_multiplier,
      activation=activation,
      use_bias=use_bias,
      depthwise_initializer=depthwise_initializer,
      pointwise_initializer=pointwise_initializer,
      bias_initializer=bias_initializer,
      depthwise_regularizer=depthwise_regularizer,
      pointwise_regularizer=pointwise_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      depthwise_constraint=depthwise_constraint,
      pointwise_constraint=pointwise_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair9>
<clonepair10>
<source file="systems/keras-2.8.0/keras/layers/convolutional_recurrent.py" startline="1078" endline="1136" pcid="4030"></source>
  def __init__(self,
               filters,
               kernel_size,
               strides=1,
               padding='valid',
               data_format=None,
               dilation_rate=1,
               activation='tanh',
               recurrent_activation='hard_sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               unit_forget_bias=True,
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               return_sequences=False,
               return_state=False,
               go_backwards=False,
               stateful=False,
               dropout=0.0,
               recurrent_dropout=0.0,
               **kwargs):
    super(ConvLSTM1D, self).__init__(
        rank=1,
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        unit_forget_bias=unit_forget_bias,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        return_sequences=return_sequences,
        return_state=return_state,
        go_backwards=go_backwards,
        stateful=stateful,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        **kwargs)


</clonepair10>

<clonepair10>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1152" endline="1311" pcid="3281"></source>
def separable_conv1d(inputs,
                     filters,
                     kernel_size,
                     strides=1,
                     padding='valid',
                     data_format='channels_last',
                     dilation_rate=1,
                     depth_multiplier=1,
                     activation=None,
                     use_bias=True,
                     depthwise_initializer=None,
                     pointwise_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     depthwise_regularizer=None,
                     pointwise_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     depthwise_constraint=None,
                     pointwise_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for the depthwise separable 1D convolution layer.

  This layer performs a depthwise convolution that acts separately on
  channels, followed by a pointwise convolution that mixes channels.
  If `use_bias` is True and a bias initializer is provided,
  it adds a bias vector to the output.
  It then optionally applies an activation function to produce the final output.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A single integer specifying the spatial
      dimensions of the filters.
    strides: A single integer specifying the strides
      of the convolution.
      Specifying any `stride` value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, length, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, length)`.
    dilation_rate: A single integer, specifying
      the dilation rate to use for dilated convolution.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    depth_multiplier: The number of depthwise convolution output channels for
      each input channel. The total number of depthwise convolution output
      channels will be equal to `num_filters_in * depth_multiplier`.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    depthwise_initializer: An initializer for the depthwise convolution kernel.
    pointwise_initializer: An initializer for the pointwise convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    depthwise_regularizer: Optional regularizer for the depthwise
      convolution kernel.
    pointwise_regularizer: Optional regularizer for the pointwise
      convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    depthwise_constraint: Optional projection function to be applied to the
        depthwise kernel after being updated by an `Optimizer` (e.g. used for
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    pointwise_constraint: Optional projection function to be applied to the
        pointwise kernel after being updated by an `Optimizer`.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.SeparableConv1D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.separable_conv1d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.SeparableConv1D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.separable_conv1d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.SeparableConv1D` instead.',
      stacklevel=2)
  layer = SeparableConv1D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      depth_multiplier=depth_multiplier,
      activation=activation,
      use_bias=use_bias,
      depthwise_initializer=depthwise_initializer,
      pointwise_initializer=pointwise_initializer,
      bias_initializer=bias_initializer,
      depthwise_regularizer=depthwise_regularizer,
      pointwise_regularizer=pointwise_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      depthwise_constraint=depthwise_constraint,
      pointwise_constraint=pointwise_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair10>
<clonepair11>
<source file="systems/keras-2.8.0/keras/layers/convolutional_recurrent.py" startline="1078" endline="1136" pcid="4030"></source>
  def __init__(self,
               filters,
               kernel_size,
               strides=1,
               padding='valid',
               data_format=None,
               dilation_rate=1,
               activation='tanh',
               recurrent_activation='hard_sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               unit_forget_bias=True,
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               return_sequences=False,
               return_state=False,
               go_backwards=False,
               stateful=False,
               dropout=0.0,
               recurrent_dropout=0.0,
               **kwargs):
    super(ConvLSTM1D, self).__init__(
        rank=1,
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        unit_forget_bias=unit_forget_bias,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        return_sequences=return_sequences,
        return_state=return_state,
        go_backwards=go_backwards,
        stateful=stateful,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        **kwargs)


</clonepair11>

<clonepair11>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="956" endline="1002" pcid="3279"></source>
  def __init__(self, filters,
               kernel_size,
               strides=1,
               padding='valid',
               data_format='channels_last',
               dilation_rate=1,
               depth_multiplier=1,
               activation=None,
               use_bias=True,
               depthwise_initializer=None,
               pointwise_initializer=None,
               bias_initializer=tf.compat.v1.zeros_initializer(),
               depthwise_regularizer=None,
               pointwise_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               depthwise_constraint=None,
               pointwise_constraint=None,
               bias_constraint=None,
               trainable=True,
               name=None,
               **kwargs):
    super(SeparableConv1D, self).__init__(
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        depth_multiplier=depth_multiplier,
        activation=activation,
        use_bias=use_bias,
        depthwise_initializer=depthwise_initializer,
        pointwise_initializer=pointwise_initializer,
        bias_initializer=bias_initializer,
        depthwise_regularizer=depthwise_regularizer,
        pointwise_regularizer=pointwise_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        depthwise_constraint=depthwise_constraint,
        pointwise_constraint=pointwise_constraint,
        bias_constraint=bias_constraint,
        trainable=trainable,
        name=name,
        **kwargs)


</clonepair11>
<clonepair12>
<source file="systems/keras-2.8.0/keras/layers/recurrent_v2.py" startline="614" endline="699" pcid="4837"></source>
def gpu_gru(inputs, init_h, kernel, recurrent_kernel, bias, mask, time_major,
            go_backwards, sequence_lengths):
  """GRU with cuDNN implementation which is only available for GPU."""
  if mask is not None:
    sequence_lengths = calculate_sequence_by_mask(mask, time_major)

  if not time_major and sequence_lengths is None:
    inputs = tf.transpose(inputs, perm=(1, 0, 2))
    seq_axis, batch_axis = (0, 1)
  else:
    seq_axis, batch_axis = (0, 1) if time_major else (1, 0)
  # For init_h, cuDNN expects one more dim of num_layers before or after batch
  # dim for time major or batch major inputs respectively
  init_h = tf.expand_dims(init_h, axis=seq_axis)

  weights = tf.split(kernel, 3, axis=1)
  weights += tf.split(recurrent_kernel, 3, axis=1)
  # Note that the bias was initialized as shape (2, 3 * units), flat it into
  # (6 * units)
  bias = tf.split(backend.flatten(bias), 6)

  if tf.sysconfig.get_build_info()['is_cuda_build']:
    # Note that the gate order for cuDNN is different from the canonical format.
    # canonical format is [z, r, h], whereas cuDNN is [r, z, h]. The swap need
    # to be done for kernel, recurrent_kernel, input_bias, recurrent_bias.
    # z is update gate weights.
    # r is reset gate weights.
    # h is output gate weights.
    weights[0], weights[1] = weights[1], weights[0]
    weights[3], weights[4] = weights[4], weights[3]
    bias[0], bias[1] = bias[1], bias[0]
    bias[3], bias[4] = bias[4], bias[3]

  params = _canonical_to_params(
      weights=weights,
      biases=bias,
      shape=tf.constant([-1]),
      transpose_weights=True)

  if sequence_lengths is not None:
    if go_backwards:
      # Three reversals are required. E.g.,
      # normal input = [1, 2, 3, 0, 0]  # where 0 need to be masked
      # reversed_input_to_cudnn = [3, 2, 1, 0, 0]
      # output_from_cudnn = [6, 5, 4, 0, 0]
      # expected_output = [0, 0, 6, 5 ,4]
      inputs = tf.reverse_sequence(
          inputs, sequence_lengths, seq_axis=seq_axis, batch_axis=batch_axis)
    outputs, h, _, _, _ = tf.raw_ops.CudnnRNNV3(
        input=inputs,
        input_h=init_h,
        input_c=0,
        params=params,
        is_training=True,
        rnn_mode='gru',
        sequence_lengths=sequence_lengths,
        time_major=time_major)
    if go_backwards:
      outputs = tf.reverse_sequence(
          outputs, sequence_lengths, seq_axis=seq_axis, batch_axis=batch_axis)
      outputs = tf.reverse(outputs, axis=[seq_axis])
  else:
    if go_backwards:
      # Reverse axis 0 since the input is already convert to time major.
      inputs = tf.reverse(inputs, axis=[0])
    outputs, h, _, _ = tf.raw_ops.CudnnRNN(
        input=inputs, input_h=init_h, input_c=0, params=params,
        is_training=True, rnn_mode='gru')

  last_output = outputs[-1]
  if not time_major and sequence_lengths is None:
    outputs = tf.transpose(outputs, perm=[1, 0, 2])
  h = tf.squeeze(h, axis=seq_axis)

  # In the case of variable length input, the cudnn kernel will fill zeros for
  # the output, whereas the default keras behavior is to bring over the previous
  # output for t-1, so that in the return_sequence=False case, user can quickly
  # get the final effect output instead just 0s at the last timestep.
  # In order to mimic the default keras behavior, we copy the final h state as
  # the last_output, since it is numerically same as the output.
  if sequence_lengths is not None:
    last_output = h

  return last_output, outputs, h, _runtime(_RUNTIME_GPU)


</clonepair12>

<clonepair12>
<source file="systems/keras-2.8.0/keras/layers/recurrent_v2.py" startline="1396" endline="1520" pcid="4850"></source>
def gpu_lstm(inputs, init_h, init_c, kernel, recurrent_kernel, bias, mask,
             time_major, go_backwards, sequence_lengths):
  """LSTM with either cuDNN or ROCm implementation which is only available for GPU.

  Note that currently only right padded data is supported, or the result will be
  polluted by the unmasked data which should be filtered.

  Args:
    inputs: Input tensor of LSTM layer.
    init_h: Initial state tensor for the cell output.
    init_c: Initial state tensor for the cell hidden state.
    kernel: Weights for cell kernel.
    recurrent_kernel: Weights for cell recurrent kernel.
    bias: Weights for cell kernel bias and recurrent bias. Only recurrent bias
      is used in this case.
    mask: Boolean tensor for mask out the steps within sequence. An individual
      `True` entry indicates that the corresponding timestep should be utilized,
      while a `False` entry indicates that the corresponding timestep should be
      ignored.
    time_major: Boolean, whether the inputs are in the format of [time, batch,
      feature] or [batch, time, feature].
    go_backwards: Boolean (default False). If True, process the input sequence
      backwards and return the reversed sequence.
    sequence_lengths: The lengths of all sequences coming from a variable length
      input, such as ragged tensors. If the input has a fixed timestep size,
      this should be None.

  Returns:
    last_output: Output tensor for the last timestep, which has shape
      [batch, units].
    outputs: Output tensor for all timesteps, which has shape
      [batch, time, units].
    state_0: The cell output, which has same shape as init_h.
    state_1: The cell hidden state, which has same shape as init_c.
    runtime: Constant string tensor which indicate real runtime hardware. This
      value is for testing purpose and should not be used by user.
  """
  if mask is not None:
    sequence_lengths = calculate_sequence_by_mask(mask, time_major)

  if not time_major and sequence_lengths is None:
    inputs = tf.transpose(inputs, perm=(1, 0, 2))
    seq_axis, batch_axis = (0, 1)
  else:
    seq_axis, batch_axis = (0, 1) if time_major else (1, 0)
  # For init_h and init_c, cuDNN expects one more dim of num_layers before or
  # after batch dim for time major or batch major inputs respectively
  init_h = tf.expand_dims(init_h, axis=seq_axis)
  init_c = tf.expand_dims(init_c, axis=seq_axis)

  weights = tf.split(kernel, 4, axis=1)
  weights += tf.split(recurrent_kernel, 4, axis=1)
  # cuDNN has an extra set of bias for inputs, we disable them (setting to 0),
  # so that mathematically it is same as the canonical LSTM implementation.
  full_bias = tf.concat((tf.zeros_like(bias), bias), 0)

  if tf.sysconfig.get_build_info()['is_rocm_build']:
    # ROCm MIOpen's weight sequence for LSTM is different from both canonical
    # and Cudnn format
    # MIOpen: [i, f, o, c] Cudnn/Canonical: [i, f, c, o]
    # i is input gate weights.
    # f is forget gate weights.
    # o is output gate weights.
    # c is cell gate weights.
    weights = [weights[x] for x in (0, 1, 3, 2, 4, 5, 7, 6)]
    # full_bias is a tensor of shape (8*n,)
    full_bias = tf.split(full_bias, 8, axis=0)
    full_bias = [full_bias[x] for x in (0, 1, 3, 2, 4, 5, 7, 6)]

  params = _canonical_to_params(
      weights=weights,
      biases=tf.split(full_bias, 8),
      shape=tf.constant([-1]),
      transpose_weights=True)

  if sequence_lengths is not None:
    if go_backwards:
      # Three reversals are required. E.g.,
      # normal input = [1, 2, 3, 0, 0]  # where 0 need to be masked
      # reversed_input_to_cudnn = [3, 2, 1, 0, 0]
      # output_from_cudnn = [6, 5, 4, 0, 0]
      # expected_output = [0, 0, 6, 5 ,4]
      inputs = tf.reverse_sequence(
          inputs, sequence_lengths, seq_axis=seq_axis, batch_axis=batch_axis)
    outputs, h, c, _, _ = tf.raw_ops.CudnnRNNV3(
        input=inputs,
        input_h=init_h,
        input_c=init_c,
        params=params,
        is_training=True,
        rnn_mode='lstm',
        sequence_lengths=sequence_lengths,
        time_major=time_major)
    if go_backwards:
      outputs = tf.reverse_sequence(
          outputs, sequence_lengths, seq_axis=seq_axis, batch_axis=batch_axis)
      outputs = tf.reverse(outputs, axis=[seq_axis])
  else:
    # # Fill the array with shape [batch] with value of max timesteps.
    # sequence_length = array_ops.fill([array_ops.shape(inputs)[1]],
    #                                  array_ops.shape(inputs)[0])
    if go_backwards:
      # Reverse axis 0 since the input is already convert to time major.
      inputs = tf.reverse(inputs, axis=[0])
    outputs, h, c, _ = tf.raw_ops.CudnnRNN(
        input=inputs, input_h=init_h, input_c=init_c, params=params,
        is_training=True, rnn_mode='lstm')

  last_output = outputs[-1]
  if not time_major and sequence_lengths is None:
    outputs = tf.transpose(outputs, perm=[1, 0, 2])
  h = tf.squeeze(h, axis=seq_axis)
  c = tf.squeeze(c, axis=seq_axis)

  # In the case of variable length input, the cudnn kernel will fill zeros for
  # the output, whereas the default keras behavior is to bring over the previous
  # output for t-1, so that in the return_sequence=False case, user can quickly
  # get the final effect output instead just 0s at the last timestep.
  # In order to mimic the default keras behavior, we copy the final h state as
  # the last_output, since it is numerically same as the output.
  if sequence_lengths is not None:
    last_output = h
  return last_output, outputs, h, c, _runtime(_RUNTIME_GPU)


</clonepair12>
<clonepair13>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/normalization.py" startline="236" endline="467" pcid="3215"></source>
def batch_normalization(inputs,
                        axis=-1,
                        momentum=0.99,
                        epsilon=1e-3,
                        center=True,
                        scale=True,
                        beta_initializer=tf.compat.v1.zeros_initializer(),
                        gamma_initializer=tf.compat.v1.ones_initializer(),
                        moving_mean_initializer=tf.compat.v1.zeros_initializer(),
                        moving_variance_initializer=tf.compat.v1.ones_initializer(),
                        beta_regularizer=None,
                        gamma_regularizer=None,
                        beta_constraint=None,
                        gamma_constraint=None,
                        training=False,
                        trainable=True,
                        name=None,
                        reuse=None,
                        renorm=False,
                        renorm_clipping=None,
                        renorm_momentum=0.99,
                        fused=None,
                        virtual_batch_size=None,
                        adjustment=None):
  """Functional interface for the batch normalization layer from_config(Ioffe et al., 2015).

  Note: when training, the moving_mean and moving_variance need to be updated.
  By default the update ops are placed in `tf.GraphKeys.UPDATE_OPS`, so they
  need to be executed alongside the `train_op`. Also, be sure to add any
  batch_normalization ops before getting the update_ops collection. Otherwise,
  update_ops will be empty, and training/inference will not work properly. For
  example:

  ```python
    x_norm = tf.compat.v1.layers.batch_normalization(x, training=training)

    # ...

    update_ops = tf.compat.v1.get_collection(tf.GraphKeys.UPDATE_OPS)
    train_op = optimizer.minimize(loss)
    train_op = tf.group([train_op, update_ops])
  ```

  Args:
    inputs: Tensor input.
    axis: An `int`, the axis that should be normalized (typically the features
      axis). For instance, after a `Convolution2D` layer with
      `data_format="channels_first"`, set `axis=1` in `BatchNormalization`.
    momentum: Momentum for the moving average.
    epsilon: Small float added to variance to avoid dividing by zero.
    center: If True, add offset of `beta` to normalized tensor. If False, `beta`
      is ignored.
    scale: If True, multiply by `gamma`. If False, `gamma` is not used. When the
      next layer is linear (also e.g. `nn.relu`), this can be disabled since the
      scaling can be done by the next layer.
    beta_initializer: Initializer for the beta weight.
    gamma_initializer: Initializer for the gamma weight.
    moving_mean_initializer: Initializer for the moving mean.
    moving_variance_initializer: Initializer for the moving variance.
    beta_regularizer: Optional regularizer for the beta weight.
    gamma_regularizer: Optional regularizer for the gamma weight.
    beta_constraint: An optional projection function to be applied to the `beta`
      weight after being updated by an `Optimizer` (e.g. used to implement norm
      constraints or value constraints for layer weights). The function must
      take as input the unprojected variable and must return the projected
      variable (which must have the same shape). Constraints are not safe to use
      when doing asynchronous distributed training.
    gamma_constraint: An optional projection function to be applied to the
      `gamma` weight after being updated by an `Optimizer`.
    training: Either a Python boolean, or a TensorFlow boolean scalar tensor
      (e.g. a placeholder). Whether to return the output in training mode
      (normalized with statistics of the current batch) or in inference mode
      (normalized with moving statistics). **NOTE**: make sure to set this
        parameter correctly, or else your training/inference will not work
        properly.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable).
    name: String, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer by the same
      name.
    renorm: Whether to use Batch Renormalization (Ioffe, 2017). This adds extra
      variables during training. The inference is the same for either value of
      this parameter.
    renorm_clipping: A dictionary that may map keys 'rmax', 'rmin', 'dmax' to
      scalar `Tensors` used to clip the renorm correction. The correction `(r,
      d)` is used as `corrected_value = normalized_value * r + d`, with `r`
      clipped to [rmin, rmax], and `d` to [-dmax, dmax]. Missing rmax, rmin,
      dmax are set to inf, 0, inf, respectively.
    renorm_momentum: Momentum used to update the moving means and standard
      deviations with renorm. Unlike `momentum`, this affects training and
      should be neither too small (which would add noise) nor too large (which
      would give stale estimates). Note that `momentum` is still applied to get
      the means and variances for inference.
    fused: if `None` or `True`, use a faster, fused implementation if possible.
      If `False`, use the system recommended implementation.
    virtual_batch_size: An `int`. By default, `virtual_batch_size` is `None`,
      which means batch normalization is performed across the whole batch. When
      `virtual_batch_size` is not `None`, instead perform "Ghost Batch
      Normalization", which creates virtual sub-batches which are each
      normalized separately (with shared gamma, beta, and moving statistics).
      Must divide the actual batch size during execution.
    adjustment: A function taking the `Tensor` containing the (dynamic) shape of
      the input tensor and returning a pair (scale, bias) to apply to the
      normalized values (before gamma and beta), only during training. For
      example, if axis==-1,
        `adjustment = lambda shape: (
          tf.random.uniform(shape[-1:], 0.93, 1.07),
          tf.random.uniform(shape[-1:], -0.1, 0.1))` will scale the normalized
            value by up to 7% up or down, then shift the result by up to 0.1
            (with independent scaling and bias for each feature but shared
            across all examples), and finally apply gamma and/or beta. If
            `None`, no adjustment is applied. Cannot be specified if
            virtual_batch_size is specified.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.

  References:
    Batch Normalization - Accelerating Deep Network Training by Reducing
    Internal Covariate Shift:
      [Ioffe et al., 2015](http://proceedings.mlr.press/v37/ioffe15.html)
      ([pdf](http://proceedings.mlr.press/v37/ioffe15.pdf))
    Batch Renormalization - Towards Reducing Minibatch Dependence in
    Batch-Normalized Models:
      [Ioffe,
      2017](http://papers.nips.cc/paper/6790-batch-renormalization-towards-reducing-minibatch-dependence-in-batch-normalized-models)
      ([pdf](http://papers.nips.cc/paper/6790-batch-renormalization-towards-reducing-minibatch-dependence-in-batch-normalized-models.pdf))

  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.BatchNormalization`.

  The batch updating pattern with
  `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used in
  native TF2. Consult the `tf.keras.layers.BatchNormalization` documentation
  for further information.

  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   x_norm = tf.compat.v1.layers.batch_normalization(x)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input(shape=(28, 28, 1),)
   y = tf.keras.layers.BatchNormalization()(x)
   model = tf.keras.Model(x, y)
  ```
  #### How to Map Arguments

  TF1 Arg Name              | TF2 Arg Name              | Note
  :------------------------ | :------------------------ | :---------------
  `name`                    | `name`                    | Layer base class
  `trainable`               | `trainable`               | Layer base class
  `axis`                    | `axis`                    | -
  `momentum`                | `momentum`                | -
  `epsilon`                 | `epsilon`                 | -
  `center`                  | `center`                  | -
  `scale`                   | `scale`                   | -
  `beta_initializer`        | `beta_initializer`        | -
  `gamma_initializer`       | `gamma_initializer`       | -
  `moving_mean_initializer` | `moving_mean_initializer` | -
  `beta_regularizer`        | `beta_regularizer'        | -
  `gamma_regularizer`       | `gamma_regularizer'       | -
  `beta_constraint`         | `beta_constraint'         | -
  `gamma_constraint`        | `gamma_constraint'        | -
  `renorm`                  | Not supported             | -
  `renorm_clipping`         | Not supported             | -
  `renorm_momentum`         | Not supported             | -
  `fused`                   | Not supported             | -
  `virtual_batch_size`      | Not supported             | -
  `adjustment`              | Not supported             | -

  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.batch_normalization` is deprecated and '
      'will be removed in a future version. '
      'Please use `tf.keras.layers.BatchNormalization` instead. '
      'In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` '
      'should not be used (consult the `tf.keras.layers.BatchNormalization` '
      'documentation).',
      stacklevel=2)
  layer = BatchNormalization(
      axis=axis,
      momentum=momentum,
      epsilon=epsilon,
      center=center,
      scale=scale,
      beta_initializer=beta_initializer,
      gamma_initializer=gamma_initializer,
      moving_mean_initializer=moving_mean_initializer,
      moving_variance_initializer=moving_variance_initializer,
      beta_regularizer=beta_regularizer,
      gamma_regularizer=gamma_regularizer,
      beta_constraint=beta_constraint,
      gamma_constraint=gamma_constraint,
      renorm=renorm,
      renorm_clipping=renorm_clipping,
      renorm_momentum=renorm_momentum,
      fused=fused,
      trainable=trainable,
      virtual_batch_size=virtual_batch_size,
      adjustment=adjustment,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs, training=training)


# Aliases

</clonepair13>

<clonepair13>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1314" endline="1478" pcid="3282"></source>
def separable_conv2d(inputs,
                     filters,
                     kernel_size,
                     strides=(1, 1),
                     padding='valid',
                     data_format='channels_last',
                     dilation_rate=(1, 1),
                     depth_multiplier=1,
                     activation=None,
                     use_bias=True,
                     depthwise_initializer=None,
                     pointwise_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     depthwise_regularizer=None,
                     pointwise_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     depthwise_constraint=None,
                     pointwise_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for the depthwise separable 2D convolution layer.

  This layer performs a depthwise convolution that acts separately on
  channels, followed by a pointwise convolution that mixes channels.
  If `use_bias` is True and a bias initializer is provided,
  it adds a bias vector to the output.
  It then optionally applies an activation function to produce the final output.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A tuple or list of 2 integers specifying the spatial
      dimensions of the filters. Can be a single integer to specify the same
      value for all spatial dimensions.
    strides: A tuple or list of 2 positive integers specifying the strides
      of the convolution. Can be a single integer to specify the same value for
      all spatial dimensions.
      Specifying any `stride` value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, height, width)`.

    dilation_rate: An integer or tuple/list of 2 integers, specifying
      the dilation rate to use for dilated convolution.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    depth_multiplier: The number of depthwise convolution output channels for
      each input channel. The total number of depthwise convolution output
      channels will be equal to `num_filters_in * depth_multiplier`.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    depthwise_initializer: An initializer for the depthwise convolution kernel.
    pointwise_initializer: An initializer for the pointwise convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    depthwise_regularizer: Optional regularizer for the depthwise
      convolution kernel.
    pointwise_regularizer: Optional regularizer for the pointwise
      convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    depthwise_constraint: Optional projection function to be applied to the
        depthwise kernel after being updated by an `Optimizer` (e.g. used for
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    pointwise_constraint: Optional projection function to be applied to the
        pointwise kernel after being updated by an `Optimizer`.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.SeparableConv2D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.separable_conv2d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.SeparableConv2D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.separable_conv2d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.SeparableConv2D` instead.',
      stacklevel=2)
  layer = SeparableConv2D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      depth_multiplier=depth_multiplier,
      activation=activation,
      use_bias=use_bias,
      depthwise_initializer=depthwise_initializer,
      pointwise_initializer=pointwise_initializer,
      bias_initializer=bias_initializer,
      depthwise_regularizer=depthwise_regularizer,
      pointwise_regularizer=pointwise_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      depthwise_constraint=depthwise_constraint,
      pointwise_constraint=pointwise_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair13>
<clonepair14>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/normalization.py" startline="236" endline="467" pcid="3215"></source>
def batch_normalization(inputs,
                        axis=-1,
                        momentum=0.99,
                        epsilon=1e-3,
                        center=True,
                        scale=True,
                        beta_initializer=tf.compat.v1.zeros_initializer(),
                        gamma_initializer=tf.compat.v1.ones_initializer(),
                        moving_mean_initializer=tf.compat.v1.zeros_initializer(),
                        moving_variance_initializer=tf.compat.v1.ones_initializer(),
                        beta_regularizer=None,
                        gamma_regularizer=None,
                        beta_constraint=None,
                        gamma_constraint=None,
                        training=False,
                        trainable=True,
                        name=None,
                        reuse=None,
                        renorm=False,
                        renorm_clipping=None,
                        renorm_momentum=0.99,
                        fused=None,
                        virtual_batch_size=None,
                        adjustment=None):
  """Functional interface for the batch normalization layer from_config(Ioffe et al., 2015).

  Note: when training, the moving_mean and moving_variance need to be updated.
  By default the update ops are placed in `tf.GraphKeys.UPDATE_OPS`, so they
  need to be executed alongside the `train_op`. Also, be sure to add any
  batch_normalization ops before getting the update_ops collection. Otherwise,
  update_ops will be empty, and training/inference will not work properly. For
  example:

  ```python
    x_norm = tf.compat.v1.layers.batch_normalization(x, training=training)

    # ...

    update_ops = tf.compat.v1.get_collection(tf.GraphKeys.UPDATE_OPS)
    train_op = optimizer.minimize(loss)
    train_op = tf.group([train_op, update_ops])
  ```

  Args:
    inputs: Tensor input.
    axis: An `int`, the axis that should be normalized (typically the features
      axis). For instance, after a `Convolution2D` layer with
      `data_format="channels_first"`, set `axis=1` in `BatchNormalization`.
    momentum: Momentum for the moving average.
    epsilon: Small float added to variance to avoid dividing by zero.
    center: If True, add offset of `beta` to normalized tensor. If False, `beta`
      is ignored.
    scale: If True, multiply by `gamma`. If False, `gamma` is not used. When the
      next layer is linear (also e.g. `nn.relu`), this can be disabled since the
      scaling can be done by the next layer.
    beta_initializer: Initializer for the beta weight.
    gamma_initializer: Initializer for the gamma weight.
    moving_mean_initializer: Initializer for the moving mean.
    moving_variance_initializer: Initializer for the moving variance.
    beta_regularizer: Optional regularizer for the beta weight.
    gamma_regularizer: Optional regularizer for the gamma weight.
    beta_constraint: An optional projection function to be applied to the `beta`
      weight after being updated by an `Optimizer` (e.g. used to implement norm
      constraints or value constraints for layer weights). The function must
      take as input the unprojected variable and must return the projected
      variable (which must have the same shape). Constraints are not safe to use
      when doing asynchronous distributed training.
    gamma_constraint: An optional projection function to be applied to the
      `gamma` weight after being updated by an `Optimizer`.
    training: Either a Python boolean, or a TensorFlow boolean scalar tensor
      (e.g. a placeholder). Whether to return the output in training mode
      (normalized with statistics of the current batch) or in inference mode
      (normalized with moving statistics). **NOTE**: make sure to set this
        parameter correctly, or else your training/inference will not work
        properly.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable).
    name: String, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer by the same
      name.
    renorm: Whether to use Batch Renormalization (Ioffe, 2017). This adds extra
      variables during training. The inference is the same for either value of
      this parameter.
    renorm_clipping: A dictionary that may map keys 'rmax', 'rmin', 'dmax' to
      scalar `Tensors` used to clip the renorm correction. The correction `(r,
      d)` is used as `corrected_value = normalized_value * r + d`, with `r`
      clipped to [rmin, rmax], and `d` to [-dmax, dmax]. Missing rmax, rmin,
      dmax are set to inf, 0, inf, respectively.
    renorm_momentum: Momentum used to update the moving means and standard
      deviations with renorm. Unlike `momentum`, this affects training and
      should be neither too small (which would add noise) nor too large (which
      would give stale estimates). Note that `momentum` is still applied to get
      the means and variances for inference.
    fused: if `None` or `True`, use a faster, fused implementation if possible.
      If `False`, use the system recommended implementation.
    virtual_batch_size: An `int`. By default, `virtual_batch_size` is `None`,
      which means batch normalization is performed across the whole batch. When
      `virtual_batch_size` is not `None`, instead perform "Ghost Batch
      Normalization", which creates virtual sub-batches which are each
      normalized separately (with shared gamma, beta, and moving statistics).
      Must divide the actual batch size during execution.
    adjustment: A function taking the `Tensor` containing the (dynamic) shape of
      the input tensor and returning a pair (scale, bias) to apply to the
      normalized values (before gamma and beta), only during training. For
      example, if axis==-1,
        `adjustment = lambda shape: (
          tf.random.uniform(shape[-1:], 0.93, 1.07),
          tf.random.uniform(shape[-1:], -0.1, 0.1))` will scale the normalized
            value by up to 7% up or down, then shift the result by up to 0.1
            (with independent scaling and bias for each feature but shared
            across all examples), and finally apply gamma and/or beta. If
            `None`, no adjustment is applied. Cannot be specified if
            virtual_batch_size is specified.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.

  References:
    Batch Normalization - Accelerating Deep Network Training by Reducing
    Internal Covariate Shift:
      [Ioffe et al., 2015](http://proceedings.mlr.press/v37/ioffe15.html)
      ([pdf](http://proceedings.mlr.press/v37/ioffe15.pdf))
    Batch Renormalization - Towards Reducing Minibatch Dependence in
    Batch-Normalized Models:
      [Ioffe,
      2017](http://papers.nips.cc/paper/6790-batch-renormalization-towards-reducing-minibatch-dependence-in-batch-normalized-models)
      ([pdf](http://papers.nips.cc/paper/6790-batch-renormalization-towards-reducing-minibatch-dependence-in-batch-normalized-models.pdf))

  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.BatchNormalization`.

  The batch updating pattern with
  `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used in
  native TF2. Consult the `tf.keras.layers.BatchNormalization` documentation
  for further information.

  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   x_norm = tf.compat.v1.layers.batch_normalization(x)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input(shape=(28, 28, 1),)
   y = tf.keras.layers.BatchNormalization()(x)
   model = tf.keras.Model(x, y)
  ```
  #### How to Map Arguments

  TF1 Arg Name              | TF2 Arg Name              | Note
  :------------------------ | :------------------------ | :---------------
  `name`                    | `name`                    | Layer base class
  `trainable`               | `trainable`               | Layer base class
  `axis`                    | `axis`                    | -
  `momentum`                | `momentum`                | -
  `epsilon`                 | `epsilon`                 | -
  `center`                  | `center`                  | -
  `scale`                   | `scale`                   | -
  `beta_initializer`        | `beta_initializer`        | -
  `gamma_initializer`       | `gamma_initializer`       | -
  `moving_mean_initializer` | `moving_mean_initializer` | -
  `beta_regularizer`        | `beta_regularizer'        | -
  `gamma_regularizer`       | `gamma_regularizer'       | -
  `beta_constraint`         | `beta_constraint'         | -
  `gamma_constraint`        | `gamma_constraint'        | -
  `renorm`                  | Not supported             | -
  `renorm_clipping`         | Not supported             | -
  `renorm_momentum`         | Not supported             | -
  `fused`                   | Not supported             | -
  `virtual_batch_size`      | Not supported             | -
  `adjustment`              | Not supported             | -

  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.batch_normalization` is deprecated and '
      'will be removed in a future version. '
      'Please use `tf.keras.layers.BatchNormalization` instead. '
      'In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` '
      'should not be used (consult the `tf.keras.layers.BatchNormalization` '
      'documentation).',
      stacklevel=2)
  layer = BatchNormalization(
      axis=axis,
      momentum=momentum,
      epsilon=epsilon,
      center=center,
      scale=scale,
      beta_initializer=beta_initializer,
      gamma_initializer=gamma_initializer,
      moving_mean_initializer=moving_mean_initializer,
      moving_variance_initializer=moving_variance_initializer,
      beta_regularizer=beta_regularizer,
      gamma_regularizer=gamma_regularizer,
      beta_constraint=beta_constraint,
      gamma_constraint=gamma_constraint,
      renorm=renorm,
      renorm_clipping=renorm_clipping,
      renorm_momentum=renorm_momentum,
      fused=fused,
      trainable=trainable,
      virtual_batch_size=virtual_batch_size,
      adjustment=adjustment,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs, training=training)


# Aliases

</clonepair14>

<clonepair14>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1152" endline="1311" pcid="3281"></source>
def separable_conv1d(inputs,
                     filters,
                     kernel_size,
                     strides=1,
                     padding='valid',
                     data_format='channels_last',
                     dilation_rate=1,
                     depth_multiplier=1,
                     activation=None,
                     use_bias=True,
                     depthwise_initializer=None,
                     pointwise_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     depthwise_regularizer=None,
                     pointwise_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     depthwise_constraint=None,
                     pointwise_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for the depthwise separable 1D convolution layer.

  This layer performs a depthwise convolution that acts separately on
  channels, followed by a pointwise convolution that mixes channels.
  If `use_bias` is True and a bias initializer is provided,
  it adds a bias vector to the output.
  It then optionally applies an activation function to produce the final output.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A single integer specifying the spatial
      dimensions of the filters.
    strides: A single integer specifying the strides
      of the convolution.
      Specifying any `stride` value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, length, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, length)`.
    dilation_rate: A single integer, specifying
      the dilation rate to use for dilated convolution.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    depth_multiplier: The number of depthwise convolution output channels for
      each input channel. The total number of depthwise convolution output
      channels will be equal to `num_filters_in * depth_multiplier`.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    depthwise_initializer: An initializer for the depthwise convolution kernel.
    pointwise_initializer: An initializer for the pointwise convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    depthwise_regularizer: Optional regularizer for the depthwise
      convolution kernel.
    pointwise_regularizer: Optional regularizer for the pointwise
      convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    depthwise_constraint: Optional projection function to be applied to the
        depthwise kernel after being updated by an `Optimizer` (e.g. used for
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    pointwise_constraint: Optional projection function to be applied to the
        pointwise kernel after being updated by an `Optimizer`.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.SeparableConv1D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.separable_conv1d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.SeparableConv1D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.separable_conv1d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.SeparableConv1D` instead.',
      stacklevel=2)
  layer = SeparableConv1D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      depth_multiplier=depth_multiplier,
      activation=activation,
      use_bias=use_bias,
      depthwise_initializer=depthwise_initializer,
      pointwise_initializer=pointwise_initializer,
      bias_initializer=bias_initializer,
      depthwise_regularizer=depthwise_regularizer,
      pointwise_regularizer=pointwise_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      depthwise_constraint=depthwise_constraint,
      pointwise_constraint=pointwise_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair14>
<clonepair15>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1152" endline="1311" pcid="3281"></source>
def separable_conv1d(inputs,
                     filters,
                     kernel_size,
                     strides=1,
                     padding='valid',
                     data_format='channels_last',
                     dilation_rate=1,
                     depth_multiplier=1,
                     activation=None,
                     use_bias=True,
                     depthwise_initializer=None,
                     pointwise_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     depthwise_regularizer=None,
                     pointwise_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     depthwise_constraint=None,
                     pointwise_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for the depthwise separable 1D convolution layer.

  This layer performs a depthwise convolution that acts separately on
  channels, followed by a pointwise convolution that mixes channels.
  If `use_bias` is True and a bias initializer is provided,
  it adds a bias vector to the output.
  It then optionally applies an activation function to produce the final output.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A single integer specifying the spatial
      dimensions of the filters.
    strides: A single integer specifying the strides
      of the convolution.
      Specifying any `stride` value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, length, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, length)`.
    dilation_rate: A single integer, specifying
      the dilation rate to use for dilated convolution.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    depth_multiplier: The number of depthwise convolution output channels for
      each input channel. The total number of depthwise convolution output
      channels will be equal to `num_filters_in * depth_multiplier`.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    depthwise_initializer: An initializer for the depthwise convolution kernel.
    pointwise_initializer: An initializer for the pointwise convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    depthwise_regularizer: Optional regularizer for the depthwise
      convolution kernel.
    pointwise_regularizer: Optional regularizer for the pointwise
      convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    depthwise_constraint: Optional projection function to be applied to the
        depthwise kernel after being updated by an `Optimizer` (e.g. used for
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    pointwise_constraint: Optional projection function to be applied to the
        pointwise kernel after being updated by an `Optimizer`.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.SeparableConv1D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.separable_conv1d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.SeparableConv1D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.separable_conv1d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.SeparableConv1D` instead.',
      stacklevel=2)
  layer = SeparableConv1D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      depth_multiplier=depth_multiplier,
      activation=activation,
      use_bias=use_bias,
      depthwise_initializer=depthwise_initializer,
      pointwise_initializer=pointwise_initializer,
      bias_initializer=bias_initializer,
      depthwise_regularizer=depthwise_regularizer,
      pointwise_regularizer=pointwise_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      depthwise_constraint=depthwise_constraint,
      pointwise_constraint=pointwise_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair15>

<clonepair15>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1314" endline="1478" pcid="3282"></source>
def separable_conv2d(inputs,
                     filters,
                     kernel_size,
                     strides=(1, 1),
                     padding='valid',
                     data_format='channels_last',
                     dilation_rate=(1, 1),
                     depth_multiplier=1,
                     activation=None,
                     use_bias=True,
                     depthwise_initializer=None,
                     pointwise_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     depthwise_regularizer=None,
                     pointwise_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     depthwise_constraint=None,
                     pointwise_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for the depthwise separable 2D convolution layer.

  This layer performs a depthwise convolution that acts separately on
  channels, followed by a pointwise convolution that mixes channels.
  If `use_bias` is True and a bias initializer is provided,
  it adds a bias vector to the output.
  It then optionally applies an activation function to produce the final output.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A tuple or list of 2 integers specifying the spatial
      dimensions of the filters. Can be a single integer to specify the same
      value for all spatial dimensions.
    strides: A tuple or list of 2 positive integers specifying the strides
      of the convolution. Can be a single integer to specify the same value for
      all spatial dimensions.
      Specifying any `stride` value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, height, width)`.

    dilation_rate: An integer or tuple/list of 2 integers, specifying
      the dilation rate to use for dilated convolution.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    depth_multiplier: The number of depthwise convolution output channels for
      each input channel. The total number of depthwise convolution output
      channels will be equal to `num_filters_in * depth_multiplier`.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    depthwise_initializer: An initializer for the depthwise convolution kernel.
    pointwise_initializer: An initializer for the pointwise convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    depthwise_regularizer: Optional regularizer for the depthwise
      convolution kernel.
    pointwise_regularizer: Optional regularizer for the pointwise
      convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    depthwise_constraint: Optional projection function to be applied to the
        depthwise kernel after being updated by an `Optimizer` (e.g. used for
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    pointwise_constraint: Optional projection function to be applied to the
        pointwise kernel after being updated by an `Optimizer`.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.SeparableConv2D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.separable_conv2d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.SeparableConv2D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.separable_conv2d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.SeparableConv2D` instead.',
      stacklevel=2)
  layer = SeparableConv2D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      depth_multiplier=depth_multiplier,
      activation=activation,
      use_bias=use_bias,
      depthwise_initializer=depthwise_initializer,
      pointwise_initializer=pointwise_initializer,
      bias_initializer=bias_initializer,
      depthwise_regularizer=depthwise_regularizer,
      pointwise_regularizer=pointwise_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      depthwise_constraint=depthwise_constraint,
      pointwise_constraint=pointwise_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair15>
<clonepair16>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="956" endline="1002" pcid="3279"></source>
  def __init__(self, filters,
               kernel_size,
               strides=1,
               padding='valid',
               data_format='channels_last',
               dilation_rate=1,
               depth_multiplier=1,
               activation=None,
               use_bias=True,
               depthwise_initializer=None,
               pointwise_initializer=None,
               bias_initializer=tf.compat.v1.zeros_initializer(),
               depthwise_regularizer=None,
               pointwise_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               depthwise_constraint=None,
               pointwise_constraint=None,
               bias_constraint=None,
               trainable=True,
               name=None,
               **kwargs):
    super(SeparableConv1D, self).__init__(
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        depth_multiplier=depth_multiplier,
        activation=activation,
        use_bias=use_bias,
        depthwise_initializer=depthwise_initializer,
        pointwise_initializer=pointwise_initializer,
        bias_initializer=bias_initializer,
        depthwise_regularizer=depthwise_regularizer,
        pointwise_regularizer=pointwise_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        depthwise_constraint=depthwise_constraint,
        pointwise_constraint=pointwise_constraint,
        bias_constraint=bias_constraint,
        trainable=trainable,
        name=name,
        **kwargs)


</clonepair16>

<clonepair16>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1314" endline="1478" pcid="3282"></source>
def separable_conv2d(inputs,
                     filters,
                     kernel_size,
                     strides=(1, 1),
                     padding='valid',
                     data_format='channels_last',
                     dilation_rate=(1, 1),
                     depth_multiplier=1,
                     activation=None,
                     use_bias=True,
                     depthwise_initializer=None,
                     pointwise_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     depthwise_regularizer=None,
                     pointwise_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     depthwise_constraint=None,
                     pointwise_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for the depthwise separable 2D convolution layer.

  This layer performs a depthwise convolution that acts separately on
  channels, followed by a pointwise convolution that mixes channels.
  If `use_bias` is True and a bias initializer is provided,
  it adds a bias vector to the output.
  It then optionally applies an activation function to produce the final output.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A tuple or list of 2 integers specifying the spatial
      dimensions of the filters. Can be a single integer to specify the same
      value for all spatial dimensions.
    strides: A tuple or list of 2 positive integers specifying the strides
      of the convolution. Can be a single integer to specify the same value for
      all spatial dimensions.
      Specifying any `stride` value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, height, width)`.

    dilation_rate: An integer or tuple/list of 2 integers, specifying
      the dilation rate to use for dilated convolution.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    depth_multiplier: The number of depthwise convolution output channels for
      each input channel. The total number of depthwise convolution output
      channels will be equal to `num_filters_in * depth_multiplier`.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    depthwise_initializer: An initializer for the depthwise convolution kernel.
    pointwise_initializer: An initializer for the pointwise convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    depthwise_regularizer: Optional regularizer for the depthwise
      convolution kernel.
    pointwise_regularizer: Optional regularizer for the pointwise
      convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    depthwise_constraint: Optional projection function to be applied to the
        depthwise kernel after being updated by an `Optimizer` (e.g. used for
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    pointwise_constraint: Optional projection function to be applied to the
        pointwise kernel after being updated by an `Optimizer`.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.SeparableConv2D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.separable_conv2d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.SeparableConv2D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.separable_conv2d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.SeparableConv2D` instead.',
      stacklevel=2)
  layer = SeparableConv2D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      depth_multiplier=depth_multiplier,
      activation=activation,
      use_bias=use_bias,
      depthwise_initializer=depthwise_initializer,
      pointwise_initializer=pointwise_initializer,
      bias_initializer=bias_initializer,
      depthwise_regularizer=depthwise_regularizer,
      pointwise_regularizer=pointwise_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      depthwise_constraint=depthwise_constraint,
      pointwise_constraint=pointwise_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair16>
<clonepair17>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="155" endline="296" pcid="3274"></source>
def conv1d(inputs,
           filters,
           kernel_size,
           strides=1,
           padding='valid',
           data_format='channels_last',
           dilation_rate=1,
           activation=None,
           use_bias=True,
           kernel_initializer=None,
           bias_initializer=tf.compat.v1.zeros_initializer(),
           kernel_regularizer=None,
           bias_regularizer=None,
           activity_regularizer=None,
           kernel_constraint=None,
           bias_constraint=None,
           trainable=True,
           name=None,
           reuse=None):
  """Functional interface for 1D convolution layer (e.g. temporal convolution).

  This layer creates a convolution kernel that is convolved
  (actually cross-correlated) with the layer input to produce a tensor of
  outputs. If `use_bias` is True (and a `bias_initializer` is provided),
  a bias vector is created and added to the outputs. Finally, if
  `activation` is not `None`, it is applied to the outputs as well.

  Args:
    inputs: Tensor input.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: An integer or tuple/list of a single integer, specifying the
      length of the 1D convolution window.
    strides: An integer or tuple/list of a single integer,
      specifying the stride length of the convolution.
      Specifying any stride value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, length, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, length)`.
    dilation_rate: An integer or tuple/list of a single integer, specifying
      the dilation rate to use for dilated convolution.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any `strides` value != 1.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is `tf.keras.layers.Conv1D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv1d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv1D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv1d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv1D` instead.',
      stacklevel=2)
  layer = Conv1D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair17>

<clonepair17>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1314" endline="1478" pcid="3282"></source>
def separable_conv2d(inputs,
                     filters,
                     kernel_size,
                     strides=(1, 1),
                     padding='valid',
                     data_format='channels_last',
                     dilation_rate=(1, 1),
                     depth_multiplier=1,
                     activation=None,
                     use_bias=True,
                     depthwise_initializer=None,
                     pointwise_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     depthwise_regularizer=None,
                     pointwise_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     depthwise_constraint=None,
                     pointwise_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for the depthwise separable 2D convolution layer.

  This layer performs a depthwise convolution that acts separately on
  channels, followed by a pointwise convolution that mixes channels.
  If `use_bias` is True and a bias initializer is provided,
  it adds a bias vector to the output.
  It then optionally applies an activation function to produce the final output.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A tuple or list of 2 integers specifying the spatial
      dimensions of the filters. Can be a single integer to specify the same
      value for all spatial dimensions.
    strides: A tuple or list of 2 positive integers specifying the strides
      of the convolution. Can be a single integer to specify the same value for
      all spatial dimensions.
      Specifying any `stride` value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, height, width)`.

    dilation_rate: An integer or tuple/list of 2 integers, specifying
      the dilation rate to use for dilated convolution.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    depth_multiplier: The number of depthwise convolution output channels for
      each input channel. The total number of depthwise convolution output
      channels will be equal to `num_filters_in * depth_multiplier`.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    depthwise_initializer: An initializer for the depthwise convolution kernel.
    pointwise_initializer: An initializer for the pointwise convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    depthwise_regularizer: Optional regularizer for the depthwise
      convolution kernel.
    pointwise_regularizer: Optional regularizer for the pointwise
      convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    depthwise_constraint: Optional projection function to be applied to the
        depthwise kernel after being updated by an `Optimizer` (e.g. used for
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    pointwise_constraint: Optional projection function to be applied to the
        pointwise kernel after being updated by an `Optimizer`.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.SeparableConv2D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.separable_conv2d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.SeparableConv2D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.separable_conv2d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.SeparableConv2D` instead.',
      stacklevel=2)
  layer = SeparableConv2D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      depth_multiplier=depth_multiplier,
      activation=activation,
      use_bias=use_bias,
      depthwise_initializer=depthwise_initializer,
      pointwise_initializer=pointwise_initializer,
      bias_initializer=bias_initializer,
      depthwise_regularizer=depthwise_regularizer,
      pointwise_regularizer=pointwise_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      depthwise_constraint=depthwise_constraint,
      pointwise_constraint=pointwise_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair17>
<clonepair18>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="429" endline="577" pcid="3276"></source>
def conv2d(inputs,
           filters,
           kernel_size,
           strides=(1, 1),
           padding='valid',
           data_format='channels_last',
           dilation_rate=(1, 1),
           activation=None,
           use_bias=True,
           kernel_initializer=None,
           bias_initializer=tf.compat.v1.zeros_initializer(),
           kernel_regularizer=None,
           bias_regularizer=None,
           activity_regularizer=None,
           kernel_constraint=None,
           bias_constraint=None,
           trainable=True,
           name=None,
           reuse=None):
  """Functional interface for the 2D convolution layer.

  This layer creates a convolution kernel that is convolved
  (actually cross-correlated) with the layer input to produce a tensor of
  outputs. If `use_bias` is True (and a `bias_initializer` is provided),
  a bias vector is created and added to the outputs. Finally, if
  `activation` is not `None`, it is applied to the outputs as well.

  Args:
    inputs: Tensor input.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: An integer or tuple/list of 2 integers, specifying the
      height and width of the 2D convolution window.
      Can be a single integer to specify the same value for
      all spatial dimensions.
    strides: An integer or tuple/list of 2 integers,
      specifying the strides of the convolution along the height and width.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Specifying any stride value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, height, width)`.

    dilation_rate: An integer or tuple/list of 2 integers, specifying
      the dilation rate to use for dilated convolution.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is `tf.keras.layers.Conv2D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv2d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv2D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv2d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv2D` instead.',
      stacklevel=2)
  layer = Conv2D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair18>

<clonepair18>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1314" endline="1478" pcid="3282"></source>
def separable_conv2d(inputs,
                     filters,
                     kernel_size,
                     strides=(1, 1),
                     padding='valid',
                     data_format='channels_last',
                     dilation_rate=(1, 1),
                     depth_multiplier=1,
                     activation=None,
                     use_bias=True,
                     depthwise_initializer=None,
                     pointwise_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     depthwise_regularizer=None,
                     pointwise_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     depthwise_constraint=None,
                     pointwise_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for the depthwise separable 2D convolution layer.

  This layer performs a depthwise convolution that acts separately on
  channels, followed by a pointwise convolution that mixes channels.
  If `use_bias` is True and a bias initializer is provided,
  it adds a bias vector to the output.
  It then optionally applies an activation function to produce the final output.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A tuple or list of 2 integers specifying the spatial
      dimensions of the filters. Can be a single integer to specify the same
      value for all spatial dimensions.
    strides: A tuple or list of 2 positive integers specifying the strides
      of the convolution. Can be a single integer to specify the same value for
      all spatial dimensions.
      Specifying any `stride` value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, height, width)`.

    dilation_rate: An integer or tuple/list of 2 integers, specifying
      the dilation rate to use for dilated convolution.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    depth_multiplier: The number of depthwise convolution output channels for
      each input channel. The total number of depthwise convolution output
      channels will be equal to `num_filters_in * depth_multiplier`.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    depthwise_initializer: An initializer for the depthwise convolution kernel.
    pointwise_initializer: An initializer for the pointwise convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    depthwise_regularizer: Optional regularizer for the depthwise
      convolution kernel.
    pointwise_regularizer: Optional regularizer for the pointwise
      convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    depthwise_constraint: Optional projection function to be applied to the
        depthwise kernel after being updated by an `Optimizer` (e.g. used for
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    pointwise_constraint: Optional projection function to be applied to the
        pointwise kernel after being updated by an `Optimizer`.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.SeparableConv2D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.separable_conv2d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.SeparableConv2D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.separable_conv2d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.SeparableConv2D` instead.',
      stacklevel=2)
  layer = SeparableConv2D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      depth_multiplier=depth_multiplier,
      activation=activation,
      use_bias=use_bias,
      depthwise_initializer=depthwise_initializer,
      pointwise_initializer=pointwise_initializer,
      bias_initializer=bias_initializer,
      depthwise_regularizer=depthwise_regularizer,
      pointwise_regularizer=pointwise_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      depthwise_constraint=depthwise_constraint,
      pointwise_constraint=pointwise_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair18>
<clonepair19>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="711" endline="860" pcid="3278"></source>
def conv3d(inputs,
           filters,
           kernel_size,
           strides=(1, 1, 1),
           padding='valid',
           data_format='channels_last',
           dilation_rate=(1, 1, 1),
           activation=None,
           use_bias=True,
           kernel_initializer=None,
           bias_initializer=tf.compat.v1.zeros_initializer(),
           kernel_regularizer=None,
           bias_regularizer=None,
           activity_regularizer=None,
           kernel_constraint=None,
           bias_constraint=None,
           trainable=True,
           name=None,
           reuse=None):
  """Functional interface for the 3D convolution layer.

  This layer creates a convolution kernel that is convolved
  (actually cross-correlated) with the layer input to produce a tensor of
  outputs. If `use_bias` is True (and a `bias_initializer` is provided),
  a bias vector is created and added to the outputs. Finally, if
  `activation` is not `None`, it is applied to the outputs as well.

  Args:
    inputs: Tensor input.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: An integer or tuple/list of 3 integers, specifying the
      depth, height and width of the 3D convolution window.
      Can be a single integer to specify the same value for
      all spatial dimensions.
    strides: An integer or tuple/list of 3 integers,
      specifying the strides of the convolution along the depth,
      height and width.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Specifying any stride value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, depth, height, width, channels)` while `channels_first`
      corresponds to inputs with shape
      `(batch, channels, depth, height, width)`.
    dilation_rate: An integer or tuple/list of 3 integers, specifying
      the dilation rate to use for dilated convolution.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is `tf.keras.layers.Conv3D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv3d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv3D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv3d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv3D` instead.',
      stacklevel=2)
  layer = Conv3D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair19>

<clonepair19>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1314" endline="1478" pcid="3282"></source>
def separable_conv2d(inputs,
                     filters,
                     kernel_size,
                     strides=(1, 1),
                     padding='valid',
                     data_format='channels_last',
                     dilation_rate=(1, 1),
                     depth_multiplier=1,
                     activation=None,
                     use_bias=True,
                     depthwise_initializer=None,
                     pointwise_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     depthwise_regularizer=None,
                     pointwise_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     depthwise_constraint=None,
                     pointwise_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for the depthwise separable 2D convolution layer.

  This layer performs a depthwise convolution that acts separately on
  channels, followed by a pointwise convolution that mixes channels.
  If `use_bias` is True and a bias initializer is provided,
  it adds a bias vector to the output.
  It then optionally applies an activation function to produce the final output.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A tuple or list of 2 integers specifying the spatial
      dimensions of the filters. Can be a single integer to specify the same
      value for all spatial dimensions.
    strides: A tuple or list of 2 positive integers specifying the strides
      of the convolution. Can be a single integer to specify the same value for
      all spatial dimensions.
      Specifying any `stride` value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, height, width)`.

    dilation_rate: An integer or tuple/list of 2 integers, specifying
      the dilation rate to use for dilated convolution.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    depth_multiplier: The number of depthwise convolution output channels for
      each input channel. The total number of depthwise convolution output
      channels will be equal to `num_filters_in * depth_multiplier`.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    depthwise_initializer: An initializer for the depthwise convolution kernel.
    pointwise_initializer: An initializer for the pointwise convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    depthwise_regularizer: Optional regularizer for the depthwise
      convolution kernel.
    pointwise_regularizer: Optional regularizer for the pointwise
      convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    depthwise_constraint: Optional projection function to be applied to the
        depthwise kernel after being updated by an `Optimizer` (e.g. used for
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    pointwise_constraint: Optional projection function to be applied to the
        pointwise kernel after being updated by an `Optimizer`.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.SeparableConv2D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.separable_conv2d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.SeparableConv2D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.separable_conv2d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.SeparableConv2D` instead.',
      stacklevel=2)
  layer = SeparableConv2D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      depth_multiplier=depth_multiplier,
      activation=activation,
      use_bias=use_bias,
      depthwise_initializer=depthwise_initializer,
      pointwise_initializer=pointwise_initializer,
      bias_initializer=bias_initializer,
      depthwise_regularizer=depthwise_regularizer,
      pointwise_regularizer=pointwise_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      depthwise_constraint=depthwise_constraint,
      pointwise_constraint=pointwise_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair19>
<clonepair20>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1314" endline="1478" pcid="3282"></source>
def separable_conv2d(inputs,
                     filters,
                     kernel_size,
                     strides=(1, 1),
                     padding='valid',
                     data_format='channels_last',
                     dilation_rate=(1, 1),
                     depth_multiplier=1,
                     activation=None,
                     use_bias=True,
                     depthwise_initializer=None,
                     pointwise_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     depthwise_regularizer=None,
                     pointwise_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     depthwise_constraint=None,
                     pointwise_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for the depthwise separable 2D convolution layer.

  This layer performs a depthwise convolution that acts separately on
  channels, followed by a pointwise convolution that mixes channels.
  If `use_bias` is True and a bias initializer is provided,
  it adds a bias vector to the output.
  It then optionally applies an activation function to produce the final output.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A tuple or list of 2 integers specifying the spatial
      dimensions of the filters. Can be a single integer to specify the same
      value for all spatial dimensions.
    strides: A tuple or list of 2 positive integers specifying the strides
      of the convolution. Can be a single integer to specify the same value for
      all spatial dimensions.
      Specifying any `stride` value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, height, width)`.

    dilation_rate: An integer or tuple/list of 2 integers, specifying
      the dilation rate to use for dilated convolution.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    depth_multiplier: The number of depthwise convolution output channels for
      each input channel. The total number of depthwise convolution output
      channels will be equal to `num_filters_in * depth_multiplier`.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    depthwise_initializer: An initializer for the depthwise convolution kernel.
    pointwise_initializer: An initializer for the pointwise convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    depthwise_regularizer: Optional regularizer for the depthwise
      convolution kernel.
    pointwise_regularizer: Optional regularizer for the pointwise
      convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    depthwise_constraint: Optional projection function to be applied to the
        depthwise kernel after being updated by an `Optimizer` (e.g. used for
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    pointwise_constraint: Optional projection function to be applied to the
        pointwise kernel after being updated by an `Optimizer`.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.SeparableConv2D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.separable_conv2d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.SeparableConv2D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.separable_conv2d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.SeparableConv2D` instead.',
      stacklevel=2)
  layer = SeparableConv2D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      depth_multiplier=depth_multiplier,
      activation=activation,
      use_bias=use_bias,
      depthwise_initializer=depthwise_initializer,
      pointwise_initializer=pointwise_initializer,
      bias_initializer=bias_initializer,
      depthwise_regularizer=depthwise_regularizer,
      pointwise_regularizer=pointwise_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      depthwise_constraint=depthwise_constraint,
      pointwise_constraint=pointwise_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair20>

<clonepair20>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1858" endline="1991" pcid="3286"></source>
def conv3d_transpose(inputs,
                     filters,
                     kernel_size,
                     strides=(1, 1, 1),
                     padding='valid',
                     data_format='channels_last',
                     activation=None,
                     use_bias=True,
                     kernel_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     kernel_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     kernel_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for transposed 3D convolution layer.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A tuple or list of 3 positive integers specifying the spatial
      dimensions of the filters. Can be a single integer to specify the same
      value for all spatial dimensions.
    strides: A tuple or list of 3 positive integers specifying the strides
      of the convolution. Can be a single integer to specify the same value for
      all spatial dimensions.
    padding: one of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, depth, height, width, channels)` while `channels_first`
      corresponds to inputs with shape
      `(batch, channels, depth, height, width)`.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.Conv3DTranspose`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv3d_transpose(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv3DTranspose(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv3d_transpose` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv3DTranspose` instead.',
      stacklevel=2)
  layer = Conv3DTranspose(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


# Aliases

</clonepair20>
<clonepair21>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1314" endline="1478" pcid="3282"></source>
def separable_conv2d(inputs,
                     filters,
                     kernel_size,
                     strides=(1, 1),
                     padding='valid',
                     data_format='channels_last',
                     dilation_rate=(1, 1),
                     depth_multiplier=1,
                     activation=None,
                     use_bias=True,
                     depthwise_initializer=None,
                     pointwise_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     depthwise_regularizer=None,
                     pointwise_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     depthwise_constraint=None,
                     pointwise_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for the depthwise separable 2D convolution layer.

  This layer performs a depthwise convolution that acts separately on
  channels, followed by a pointwise convolution that mixes channels.
  If `use_bias` is True and a bias initializer is provided,
  it adds a bias vector to the output.
  It then optionally applies an activation function to produce the final output.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A tuple or list of 2 integers specifying the spatial
      dimensions of the filters. Can be a single integer to specify the same
      value for all spatial dimensions.
    strides: A tuple or list of 2 positive integers specifying the strides
      of the convolution. Can be a single integer to specify the same value for
      all spatial dimensions.
      Specifying any `stride` value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, height, width)`.

    dilation_rate: An integer or tuple/list of 2 integers, specifying
      the dilation rate to use for dilated convolution.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    depth_multiplier: The number of depthwise convolution output channels for
      each input channel. The total number of depthwise convolution output
      channels will be equal to `num_filters_in * depth_multiplier`.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    depthwise_initializer: An initializer for the depthwise convolution kernel.
    pointwise_initializer: An initializer for the pointwise convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    depthwise_regularizer: Optional regularizer for the depthwise
      convolution kernel.
    pointwise_regularizer: Optional regularizer for the pointwise
      convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    depthwise_constraint: Optional projection function to be applied to the
        depthwise kernel after being updated by an `Optimizer` (e.g. used for
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    pointwise_constraint: Optional projection function to be applied to the
        pointwise kernel after being updated by an `Optimizer`.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.SeparableConv2D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.separable_conv2d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.SeparableConv2D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.separable_conv2d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.SeparableConv2D` instead.',
      stacklevel=2)
  layer = SeparableConv2D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      depth_multiplier=depth_multiplier,
      activation=activation,
      use_bias=use_bias,
      depthwise_initializer=depthwise_initializer,
      pointwise_initializer=pointwise_initializer,
      bias_initializer=bias_initializer,
      depthwise_regularizer=depthwise_regularizer,
      pointwise_regularizer=pointwise_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      depthwise_constraint=depthwise_constraint,
      pointwise_constraint=pointwise_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair21>

<clonepair21>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1601" endline="1738" pcid="3284"></source>
def conv2d_transpose(inputs,
                     filters,
                     kernel_size,
                     strides=(1, 1),
                     padding='valid',
                     data_format='channels_last',
                     activation=None,
                     use_bias=True,
                     kernel_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     kernel_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     kernel_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for transposed 2D convolution layer.

  The need for transposed convolutions generally arises
  from the desire to use a transformation going in the opposite direction
  of a normal convolution, i.e., from something that has the shape of the
  output of some convolution to something that has the shape of its input
  while maintaining a connectivity pattern that is compatible with
  said convolution.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A tuple or list of 2 positive integers specifying the spatial
      dimensions of the filters. Can be a single integer to specify the same
      value for all spatial dimensions.
    strides: A tuple or list of 2 positive integers specifying the strides
      of the convolution. Can be a single integer to specify the same value for
      all spatial dimensions.
    padding: one of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, height, width)`.
    activation: Activation function. Set it to `None` to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If `None`, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.Conv2DTranspose`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv2d_transpose(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv2DTranspose(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv2d_transpose` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv2DTranspose` instead.',
      stacklevel=2)
  layer = Conv2DTranspose(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair21>
<clonepair22>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="956" endline="1002" pcid="3279"></source>
  def __init__(self, filters,
               kernel_size,
               strides=1,
               padding='valid',
               data_format='channels_last',
               dilation_rate=1,
               depth_multiplier=1,
               activation=None,
               use_bias=True,
               depthwise_initializer=None,
               pointwise_initializer=None,
               bias_initializer=tf.compat.v1.zeros_initializer(),
               depthwise_regularizer=None,
               pointwise_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               depthwise_constraint=None,
               pointwise_constraint=None,
               bias_constraint=None,
               trainable=True,
               name=None,
               **kwargs):
    super(SeparableConv1D, self).__init__(
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        depth_multiplier=depth_multiplier,
        activation=activation,
        use_bias=use_bias,
        depthwise_initializer=depthwise_initializer,
        pointwise_initializer=pointwise_initializer,
        bias_initializer=bias_initializer,
        depthwise_regularizer=depthwise_regularizer,
        pointwise_regularizer=pointwise_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        depthwise_constraint=depthwise_constraint,
        pointwise_constraint=pointwise_constraint,
        bias_constraint=bias_constraint,
        trainable=trainable,
        name=name,
        **kwargs)


</clonepair22>

<clonepair22>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1152" endline="1311" pcid="3281"></source>
def separable_conv1d(inputs,
                     filters,
                     kernel_size,
                     strides=1,
                     padding='valid',
                     data_format='channels_last',
                     dilation_rate=1,
                     depth_multiplier=1,
                     activation=None,
                     use_bias=True,
                     depthwise_initializer=None,
                     pointwise_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     depthwise_regularizer=None,
                     pointwise_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     depthwise_constraint=None,
                     pointwise_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for the depthwise separable 1D convolution layer.

  This layer performs a depthwise convolution that acts separately on
  channels, followed by a pointwise convolution that mixes channels.
  If `use_bias` is True and a bias initializer is provided,
  it adds a bias vector to the output.
  It then optionally applies an activation function to produce the final output.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A single integer specifying the spatial
      dimensions of the filters.
    strides: A single integer specifying the strides
      of the convolution.
      Specifying any `stride` value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, length, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, length)`.
    dilation_rate: A single integer, specifying
      the dilation rate to use for dilated convolution.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    depth_multiplier: The number of depthwise convolution output channels for
      each input channel. The total number of depthwise convolution output
      channels will be equal to `num_filters_in * depth_multiplier`.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    depthwise_initializer: An initializer for the depthwise convolution kernel.
    pointwise_initializer: An initializer for the pointwise convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    depthwise_regularizer: Optional regularizer for the depthwise
      convolution kernel.
    pointwise_regularizer: Optional regularizer for the pointwise
      convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    depthwise_constraint: Optional projection function to be applied to the
        depthwise kernel after being updated by an `Optimizer` (e.g. used for
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    pointwise_constraint: Optional projection function to be applied to the
        pointwise kernel after being updated by an `Optimizer`.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.SeparableConv1D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.separable_conv1d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.SeparableConv1D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.separable_conv1d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.SeparableConv1D` instead.',
      stacklevel=2)
  layer = SeparableConv1D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      depth_multiplier=depth_multiplier,
      activation=activation,
      use_bias=use_bias,
      depthwise_initializer=depthwise_initializer,
      pointwise_initializer=pointwise_initializer,
      bias_initializer=bias_initializer,
      depthwise_regularizer=depthwise_regularizer,
      pointwise_regularizer=pointwise_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      depthwise_constraint=depthwise_constraint,
      pointwise_constraint=pointwise_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair22>
<clonepair23>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="155" endline="296" pcid="3274"></source>
def conv1d(inputs,
           filters,
           kernel_size,
           strides=1,
           padding='valid',
           data_format='channels_last',
           dilation_rate=1,
           activation=None,
           use_bias=True,
           kernel_initializer=None,
           bias_initializer=tf.compat.v1.zeros_initializer(),
           kernel_regularizer=None,
           bias_regularizer=None,
           activity_regularizer=None,
           kernel_constraint=None,
           bias_constraint=None,
           trainable=True,
           name=None,
           reuse=None):
  """Functional interface for 1D convolution layer (e.g. temporal convolution).

  This layer creates a convolution kernel that is convolved
  (actually cross-correlated) with the layer input to produce a tensor of
  outputs. If `use_bias` is True (and a `bias_initializer` is provided),
  a bias vector is created and added to the outputs. Finally, if
  `activation` is not `None`, it is applied to the outputs as well.

  Args:
    inputs: Tensor input.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: An integer or tuple/list of a single integer, specifying the
      length of the 1D convolution window.
    strides: An integer or tuple/list of a single integer,
      specifying the stride length of the convolution.
      Specifying any stride value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, length, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, length)`.
    dilation_rate: An integer or tuple/list of a single integer, specifying
      the dilation rate to use for dilated convolution.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any `strides` value != 1.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is `tf.keras.layers.Conv1D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv1d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv1D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv1d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv1D` instead.',
      stacklevel=2)
  layer = Conv1D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair23>

<clonepair23>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1152" endline="1311" pcid="3281"></source>
def separable_conv1d(inputs,
                     filters,
                     kernel_size,
                     strides=1,
                     padding='valid',
                     data_format='channels_last',
                     dilation_rate=1,
                     depth_multiplier=1,
                     activation=None,
                     use_bias=True,
                     depthwise_initializer=None,
                     pointwise_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     depthwise_regularizer=None,
                     pointwise_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     depthwise_constraint=None,
                     pointwise_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for the depthwise separable 1D convolution layer.

  This layer performs a depthwise convolution that acts separately on
  channels, followed by a pointwise convolution that mixes channels.
  If `use_bias` is True and a bias initializer is provided,
  it adds a bias vector to the output.
  It then optionally applies an activation function to produce the final output.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A single integer specifying the spatial
      dimensions of the filters.
    strides: A single integer specifying the strides
      of the convolution.
      Specifying any `stride` value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, length, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, length)`.
    dilation_rate: A single integer, specifying
      the dilation rate to use for dilated convolution.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    depth_multiplier: The number of depthwise convolution output channels for
      each input channel. The total number of depthwise convolution output
      channels will be equal to `num_filters_in * depth_multiplier`.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    depthwise_initializer: An initializer for the depthwise convolution kernel.
    pointwise_initializer: An initializer for the pointwise convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    depthwise_regularizer: Optional regularizer for the depthwise
      convolution kernel.
    pointwise_regularizer: Optional regularizer for the pointwise
      convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    depthwise_constraint: Optional projection function to be applied to the
        depthwise kernel after being updated by an `Optimizer` (e.g. used for
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    pointwise_constraint: Optional projection function to be applied to the
        pointwise kernel after being updated by an `Optimizer`.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.SeparableConv1D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.separable_conv1d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.SeparableConv1D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.separable_conv1d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.SeparableConv1D` instead.',
      stacklevel=2)
  layer = SeparableConv1D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      depth_multiplier=depth_multiplier,
      activation=activation,
      use_bias=use_bias,
      depthwise_initializer=depthwise_initializer,
      pointwise_initializer=pointwise_initializer,
      bias_initializer=bias_initializer,
      depthwise_regularizer=depthwise_regularizer,
      pointwise_regularizer=pointwise_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      depthwise_constraint=depthwise_constraint,
      pointwise_constraint=pointwise_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair23>
<clonepair24>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="429" endline="577" pcid="3276"></source>
def conv2d(inputs,
           filters,
           kernel_size,
           strides=(1, 1),
           padding='valid',
           data_format='channels_last',
           dilation_rate=(1, 1),
           activation=None,
           use_bias=True,
           kernel_initializer=None,
           bias_initializer=tf.compat.v1.zeros_initializer(),
           kernel_regularizer=None,
           bias_regularizer=None,
           activity_regularizer=None,
           kernel_constraint=None,
           bias_constraint=None,
           trainable=True,
           name=None,
           reuse=None):
  """Functional interface for the 2D convolution layer.

  This layer creates a convolution kernel that is convolved
  (actually cross-correlated) with the layer input to produce a tensor of
  outputs. If `use_bias` is True (and a `bias_initializer` is provided),
  a bias vector is created and added to the outputs. Finally, if
  `activation` is not `None`, it is applied to the outputs as well.

  Args:
    inputs: Tensor input.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: An integer or tuple/list of 2 integers, specifying the
      height and width of the 2D convolution window.
      Can be a single integer to specify the same value for
      all spatial dimensions.
    strides: An integer or tuple/list of 2 integers,
      specifying the strides of the convolution along the height and width.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Specifying any stride value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, height, width)`.

    dilation_rate: An integer or tuple/list of 2 integers, specifying
      the dilation rate to use for dilated convolution.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is `tf.keras.layers.Conv2D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv2d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv2D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv2d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv2D` instead.',
      stacklevel=2)
  layer = Conv2D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair24>

<clonepair24>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1152" endline="1311" pcid="3281"></source>
def separable_conv1d(inputs,
                     filters,
                     kernel_size,
                     strides=1,
                     padding='valid',
                     data_format='channels_last',
                     dilation_rate=1,
                     depth_multiplier=1,
                     activation=None,
                     use_bias=True,
                     depthwise_initializer=None,
                     pointwise_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     depthwise_regularizer=None,
                     pointwise_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     depthwise_constraint=None,
                     pointwise_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for the depthwise separable 1D convolution layer.

  This layer performs a depthwise convolution that acts separately on
  channels, followed by a pointwise convolution that mixes channels.
  If `use_bias` is True and a bias initializer is provided,
  it adds a bias vector to the output.
  It then optionally applies an activation function to produce the final output.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A single integer specifying the spatial
      dimensions of the filters.
    strides: A single integer specifying the strides
      of the convolution.
      Specifying any `stride` value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, length, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, length)`.
    dilation_rate: A single integer, specifying
      the dilation rate to use for dilated convolution.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    depth_multiplier: The number of depthwise convolution output channels for
      each input channel. The total number of depthwise convolution output
      channels will be equal to `num_filters_in * depth_multiplier`.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    depthwise_initializer: An initializer for the depthwise convolution kernel.
    pointwise_initializer: An initializer for the pointwise convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    depthwise_regularizer: Optional regularizer for the depthwise
      convolution kernel.
    pointwise_regularizer: Optional regularizer for the pointwise
      convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    depthwise_constraint: Optional projection function to be applied to the
        depthwise kernel after being updated by an `Optimizer` (e.g. used for
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    pointwise_constraint: Optional projection function to be applied to the
        pointwise kernel after being updated by an `Optimizer`.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.SeparableConv1D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.separable_conv1d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.SeparableConv1D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.separable_conv1d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.SeparableConv1D` instead.',
      stacklevel=2)
  layer = SeparableConv1D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      depth_multiplier=depth_multiplier,
      activation=activation,
      use_bias=use_bias,
      depthwise_initializer=depthwise_initializer,
      pointwise_initializer=pointwise_initializer,
      bias_initializer=bias_initializer,
      depthwise_regularizer=depthwise_regularizer,
      pointwise_regularizer=pointwise_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      depthwise_constraint=depthwise_constraint,
      pointwise_constraint=pointwise_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair24>
<clonepair25>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="711" endline="860" pcid="3278"></source>
def conv3d(inputs,
           filters,
           kernel_size,
           strides=(1, 1, 1),
           padding='valid',
           data_format='channels_last',
           dilation_rate=(1, 1, 1),
           activation=None,
           use_bias=True,
           kernel_initializer=None,
           bias_initializer=tf.compat.v1.zeros_initializer(),
           kernel_regularizer=None,
           bias_regularizer=None,
           activity_regularizer=None,
           kernel_constraint=None,
           bias_constraint=None,
           trainable=True,
           name=None,
           reuse=None):
  """Functional interface for the 3D convolution layer.

  This layer creates a convolution kernel that is convolved
  (actually cross-correlated) with the layer input to produce a tensor of
  outputs. If `use_bias` is True (and a `bias_initializer` is provided),
  a bias vector is created and added to the outputs. Finally, if
  `activation` is not `None`, it is applied to the outputs as well.

  Args:
    inputs: Tensor input.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: An integer or tuple/list of 3 integers, specifying the
      depth, height and width of the 3D convolution window.
      Can be a single integer to specify the same value for
      all spatial dimensions.
    strides: An integer or tuple/list of 3 integers,
      specifying the strides of the convolution along the depth,
      height and width.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Specifying any stride value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, depth, height, width, channels)` while `channels_first`
      corresponds to inputs with shape
      `(batch, channels, depth, height, width)`.
    dilation_rate: An integer or tuple/list of 3 integers, specifying
      the dilation rate to use for dilated convolution.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is `tf.keras.layers.Conv3D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv3d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv3D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv3d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv3D` instead.',
      stacklevel=2)
  layer = Conv3D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair25>

<clonepair25>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1152" endline="1311" pcid="3281"></source>
def separable_conv1d(inputs,
                     filters,
                     kernel_size,
                     strides=1,
                     padding='valid',
                     data_format='channels_last',
                     dilation_rate=1,
                     depth_multiplier=1,
                     activation=None,
                     use_bias=True,
                     depthwise_initializer=None,
                     pointwise_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     depthwise_regularizer=None,
                     pointwise_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     depthwise_constraint=None,
                     pointwise_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for the depthwise separable 1D convolution layer.

  This layer performs a depthwise convolution that acts separately on
  channels, followed by a pointwise convolution that mixes channels.
  If `use_bias` is True and a bias initializer is provided,
  it adds a bias vector to the output.
  It then optionally applies an activation function to produce the final output.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A single integer specifying the spatial
      dimensions of the filters.
    strides: A single integer specifying the strides
      of the convolution.
      Specifying any `stride` value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, length, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, length)`.
    dilation_rate: A single integer, specifying
      the dilation rate to use for dilated convolution.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    depth_multiplier: The number of depthwise convolution output channels for
      each input channel. The total number of depthwise convolution output
      channels will be equal to `num_filters_in * depth_multiplier`.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    depthwise_initializer: An initializer for the depthwise convolution kernel.
    pointwise_initializer: An initializer for the pointwise convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    depthwise_regularizer: Optional regularizer for the depthwise
      convolution kernel.
    pointwise_regularizer: Optional regularizer for the pointwise
      convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    depthwise_constraint: Optional projection function to be applied to the
        depthwise kernel after being updated by an `Optimizer` (e.g. used for
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    pointwise_constraint: Optional projection function to be applied to the
        pointwise kernel after being updated by an `Optimizer`.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.SeparableConv1D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.separable_conv1d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.SeparableConv1D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.separable_conv1d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.SeparableConv1D` instead.',
      stacklevel=2)
  layer = SeparableConv1D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      depth_multiplier=depth_multiplier,
      activation=activation,
      use_bias=use_bias,
      depthwise_initializer=depthwise_initializer,
      pointwise_initializer=pointwise_initializer,
      bias_initializer=bias_initializer,
      depthwise_regularizer=depthwise_regularizer,
      pointwise_regularizer=pointwise_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      depthwise_constraint=depthwise_constraint,
      pointwise_constraint=pointwise_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair25>
<clonepair26>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1152" endline="1311" pcid="3281"></source>
def separable_conv1d(inputs,
                     filters,
                     kernel_size,
                     strides=1,
                     padding='valid',
                     data_format='channels_last',
                     dilation_rate=1,
                     depth_multiplier=1,
                     activation=None,
                     use_bias=True,
                     depthwise_initializer=None,
                     pointwise_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     depthwise_regularizer=None,
                     pointwise_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     depthwise_constraint=None,
                     pointwise_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for the depthwise separable 1D convolution layer.

  This layer performs a depthwise convolution that acts separately on
  channels, followed by a pointwise convolution that mixes channels.
  If `use_bias` is True and a bias initializer is provided,
  it adds a bias vector to the output.
  It then optionally applies an activation function to produce the final output.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A single integer specifying the spatial
      dimensions of the filters.
    strides: A single integer specifying the strides
      of the convolution.
      Specifying any `stride` value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, length, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, length)`.
    dilation_rate: A single integer, specifying
      the dilation rate to use for dilated convolution.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    depth_multiplier: The number of depthwise convolution output channels for
      each input channel. The total number of depthwise convolution output
      channels will be equal to `num_filters_in * depth_multiplier`.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    depthwise_initializer: An initializer for the depthwise convolution kernel.
    pointwise_initializer: An initializer for the pointwise convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    depthwise_regularizer: Optional regularizer for the depthwise
      convolution kernel.
    pointwise_regularizer: Optional regularizer for the pointwise
      convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    depthwise_constraint: Optional projection function to be applied to the
        depthwise kernel after being updated by an `Optimizer` (e.g. used for
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    pointwise_constraint: Optional projection function to be applied to the
        pointwise kernel after being updated by an `Optimizer`.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.SeparableConv1D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.separable_conv1d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.SeparableConv1D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.separable_conv1d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.SeparableConv1D` instead.',
      stacklevel=2)
  layer = SeparableConv1D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      depth_multiplier=depth_multiplier,
      activation=activation,
      use_bias=use_bias,
      depthwise_initializer=depthwise_initializer,
      pointwise_initializer=pointwise_initializer,
      bias_initializer=bias_initializer,
      depthwise_regularizer=depthwise_regularizer,
      pointwise_regularizer=pointwise_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      depthwise_constraint=depthwise_constraint,
      pointwise_constraint=pointwise_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair26>

<clonepair26>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1858" endline="1991" pcid="3286"></source>
def conv3d_transpose(inputs,
                     filters,
                     kernel_size,
                     strides=(1, 1, 1),
                     padding='valid',
                     data_format='channels_last',
                     activation=None,
                     use_bias=True,
                     kernel_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     kernel_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     kernel_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for transposed 3D convolution layer.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A tuple or list of 3 positive integers specifying the spatial
      dimensions of the filters. Can be a single integer to specify the same
      value for all spatial dimensions.
    strides: A tuple or list of 3 positive integers specifying the strides
      of the convolution. Can be a single integer to specify the same value for
      all spatial dimensions.
    padding: one of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, depth, height, width, channels)` while `channels_first`
      corresponds to inputs with shape
      `(batch, channels, depth, height, width)`.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.Conv3DTranspose`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv3d_transpose(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv3DTranspose(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv3d_transpose` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv3DTranspose` instead.',
      stacklevel=2)
  layer = Conv3DTranspose(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


# Aliases

</clonepair26>
<clonepair27>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1152" endline="1311" pcid="3281"></source>
def separable_conv1d(inputs,
                     filters,
                     kernel_size,
                     strides=1,
                     padding='valid',
                     data_format='channels_last',
                     dilation_rate=1,
                     depth_multiplier=1,
                     activation=None,
                     use_bias=True,
                     depthwise_initializer=None,
                     pointwise_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     depthwise_regularizer=None,
                     pointwise_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     depthwise_constraint=None,
                     pointwise_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for the depthwise separable 1D convolution layer.

  This layer performs a depthwise convolution that acts separately on
  channels, followed by a pointwise convolution that mixes channels.
  If `use_bias` is True and a bias initializer is provided,
  it adds a bias vector to the output.
  It then optionally applies an activation function to produce the final output.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A single integer specifying the spatial
      dimensions of the filters.
    strides: A single integer specifying the strides
      of the convolution.
      Specifying any `stride` value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, length, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, length)`.
    dilation_rate: A single integer, specifying
      the dilation rate to use for dilated convolution.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    depth_multiplier: The number of depthwise convolution output channels for
      each input channel. The total number of depthwise convolution output
      channels will be equal to `num_filters_in * depth_multiplier`.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    depthwise_initializer: An initializer for the depthwise convolution kernel.
    pointwise_initializer: An initializer for the pointwise convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    depthwise_regularizer: Optional regularizer for the depthwise
      convolution kernel.
    pointwise_regularizer: Optional regularizer for the pointwise
      convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    depthwise_constraint: Optional projection function to be applied to the
        depthwise kernel after being updated by an `Optimizer` (e.g. used for
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    pointwise_constraint: Optional projection function to be applied to the
        pointwise kernel after being updated by an `Optimizer`.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.SeparableConv1D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.separable_conv1d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.SeparableConv1D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.separable_conv1d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.SeparableConv1D` instead.',
      stacklevel=2)
  layer = SeparableConv1D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      depth_multiplier=depth_multiplier,
      activation=activation,
      use_bias=use_bias,
      depthwise_initializer=depthwise_initializer,
      pointwise_initializer=pointwise_initializer,
      bias_initializer=bias_initializer,
      depthwise_regularizer=depthwise_regularizer,
      pointwise_regularizer=pointwise_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      depthwise_constraint=depthwise_constraint,
      pointwise_constraint=pointwise_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair27>

<clonepair27>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1601" endline="1738" pcid="3284"></source>
def conv2d_transpose(inputs,
                     filters,
                     kernel_size,
                     strides=(1, 1),
                     padding='valid',
                     data_format='channels_last',
                     activation=None,
                     use_bias=True,
                     kernel_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     kernel_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     kernel_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for transposed 2D convolution layer.

  The need for transposed convolutions generally arises
  from the desire to use a transformation going in the opposite direction
  of a normal convolution, i.e., from something that has the shape of the
  output of some convolution to something that has the shape of its input
  while maintaining a connectivity pattern that is compatible with
  said convolution.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A tuple or list of 2 positive integers specifying the spatial
      dimensions of the filters. Can be a single integer to specify the same
      value for all spatial dimensions.
    strides: A tuple or list of 2 positive integers specifying the strides
      of the convolution. Can be a single integer to specify the same value for
      all spatial dimensions.
    padding: one of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, height, width)`.
    activation: Activation function. Set it to `None` to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If `None`, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.Conv2DTranspose`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv2d_transpose(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv2DTranspose(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv2d_transpose` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv2DTranspose` instead.',
      stacklevel=2)
  layer = Conv2DTranspose(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair27>
<clonepair28>
<source file="systems/keras-2.8.0/keras/layers/recurrent.py" startline="1766" endline="1822" pcid="4332"></source>
  def __init__(self,
               units,
               activation='tanh',
               recurrent_activation='hard_sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               reset_after=False,
               **kwargs):
    if units < 0:
      raise ValueError(f'Received an invalid value for argument `units`, '
                       f'expected a positive integer, got {units}.')
    # By default use cached variable under v2 mode, see b/143699808.
    if tf.compat.v1.executing_eagerly_outside_functions():
      self._enable_caching_device = kwargs.pop('enable_caching_device', True)
    else:
      self._enable_caching_device = kwargs.pop('enable_caching_device', False)
    super(GRUCell, self).__init__(**kwargs)
    self.units = units
    self.activation = activations.get(activation)
    self.recurrent_activation = activations.get(recurrent_activation)
    self.use_bias = use_bias

    self.kernel_initializer = initializers.get(kernel_initializer)
    self.recurrent_initializer = initializers.get(recurrent_initializer)
    self.bias_initializer = initializers.get(bias_initializer)

    self.kernel_regularizer = regularizers.get(kernel_regularizer)
    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)
    self.bias_regularizer = regularizers.get(bias_regularizer)

    self.kernel_constraint = constraints.get(kernel_constraint)
    self.recurrent_constraint = constraints.get(recurrent_constraint)
    self.bias_constraint = constraints.get(bias_constraint)

    self.dropout = min(1., max(0., dropout))
    self.recurrent_dropout = min(1., max(0., recurrent_dropout))

    implementation = kwargs.pop('implementation', 1)
    if self.recurrent_dropout != 0 and implementation != 1:
      logging.debug(RECURRENT_DROPOUT_WARNING_MSG)
      self.implementation = 1
    else:
      self.implementation = implementation
    self.reset_after = reset_after
    self.state_size = self.units
    self.output_size = self.units

</clonepair28>

<clonepair28>
<source file="systems/keras-2.8.0/keras/layers/recurrent.py" startline="2329" endline="2384" pcid="4358"></source>
  def __init__(self,
               units,
               activation='tanh',
               recurrent_activation='hard_sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               unit_forget_bias=True,
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               **kwargs):
    if units < 0:
      raise ValueError(f'Received an invalid value for argument `units`, '
                       f'expected a positive integer, got {units}.')
    # By default use cached variable under v2 mode, see b/143699808.
    if tf.compat.v1.executing_eagerly_outside_functions():
      self._enable_caching_device = kwargs.pop('enable_caching_device', True)
    else:
      self._enable_caching_device = kwargs.pop('enable_caching_device', False)
    super(LSTMCell, self).__init__(**kwargs)
    self.units = units
    self.activation = activations.get(activation)
    self.recurrent_activation = activations.get(recurrent_activation)
    self.use_bias = use_bias

    self.kernel_initializer = initializers.get(kernel_initializer)
    self.recurrent_initializer = initializers.get(recurrent_initializer)
    self.bias_initializer = initializers.get(bias_initializer)
    self.unit_forget_bias = unit_forget_bias

    self.kernel_regularizer = regularizers.get(kernel_regularizer)
    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)
    self.bias_regularizer = regularizers.get(bias_regularizer)

    self.kernel_constraint = constraints.get(kernel_constraint)
    self.recurrent_constraint = constraints.get(recurrent_constraint)
    self.bias_constraint = constraints.get(bias_constraint)

    self.dropout = min(1., max(0., dropout))
    self.recurrent_dropout = min(1., max(0., recurrent_dropout))
    implementation = kwargs.pop('implementation', 1)
    if self.recurrent_dropout != 0 and implementation != 1:
      logging.debug(RECURRENT_DROPOUT_WARNING_MSG)
      self.implementation = 1
    else:
      self.implementation = implementation
    self.state_size = [self.units, self.units]
    self.output_size = self.units

</clonepair28>
<clonepair29>
<source file="systems/keras-2.8.0/keras/layers/recurrent.py" startline="1327" endline="1372" pcid="4309"></source>
  def __init__(self,
               units,
               activation='tanh',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               **kwargs):
    if units < 0:
      raise ValueError(f'Received an invalid value for argument `units`, '
                       f'expected a positive integer, got {units}.')
    # By default use cached variable under v2 mode, see b/143699808.
    if tf.compat.v1.executing_eagerly_outside_functions():
      self._enable_caching_device = kwargs.pop('enable_caching_device', True)
    else:
      self._enable_caching_device = kwargs.pop('enable_caching_device', False)
    super(SimpleRNNCell, self).__init__(**kwargs)
    self.units = units
    self.activation = activations.get(activation)
    self.use_bias = use_bias

    self.kernel_initializer = initializers.get(kernel_initializer)
    self.recurrent_initializer = initializers.get(recurrent_initializer)
    self.bias_initializer = initializers.get(bias_initializer)

    self.kernel_regularizer = regularizers.get(kernel_regularizer)
    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)
    self.bias_regularizer = regularizers.get(bias_regularizer)

    self.kernel_constraint = constraints.get(kernel_constraint)
    self.recurrent_constraint = constraints.get(recurrent_constraint)
    self.bias_constraint = constraints.get(bias_constraint)

    self.dropout = min(1., max(0., dropout))
    self.recurrent_dropout = min(1., max(0., recurrent_dropout))
    self.state_size = self.units
    self.output_size = self.units

</clonepair29>

<clonepair29>
<source file="systems/keras-2.8.0/keras/layers/recurrent.py" startline="1766" endline="1822" pcid="4332"></source>
  def __init__(self,
               units,
               activation='tanh',
               recurrent_activation='hard_sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               reset_after=False,
               **kwargs):
    if units < 0:
      raise ValueError(f'Received an invalid value for argument `units`, '
                       f'expected a positive integer, got {units}.')
    # By default use cached variable under v2 mode, see b/143699808.
    if tf.compat.v1.executing_eagerly_outside_functions():
      self._enable_caching_device = kwargs.pop('enable_caching_device', True)
    else:
      self._enable_caching_device = kwargs.pop('enable_caching_device', False)
    super(GRUCell, self).__init__(**kwargs)
    self.units = units
    self.activation = activations.get(activation)
    self.recurrent_activation = activations.get(recurrent_activation)
    self.use_bias = use_bias

    self.kernel_initializer = initializers.get(kernel_initializer)
    self.recurrent_initializer = initializers.get(recurrent_initializer)
    self.bias_initializer = initializers.get(bias_initializer)

    self.kernel_regularizer = regularizers.get(kernel_regularizer)
    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)
    self.bias_regularizer = regularizers.get(bias_regularizer)

    self.kernel_constraint = constraints.get(kernel_constraint)
    self.recurrent_constraint = constraints.get(recurrent_constraint)
    self.bias_constraint = constraints.get(bias_constraint)

    self.dropout = min(1., max(0., dropout))
    self.recurrent_dropout = min(1., max(0., recurrent_dropout))

    implementation = kwargs.pop('implementation', 1)
    if self.recurrent_dropout != 0 and implementation != 1:
      logging.debug(RECURRENT_DROPOUT_WARNING_MSG)
      self.implementation = 1
    else:
      self.implementation = implementation
    self.reset_after = reset_after
    self.state_size = self.units
    self.output_size = self.units

</clonepair29>
<clonepair30>
<source file="systems/keras-2.8.0/keras/layers/recurrent.py" startline="1327" endline="1372" pcid="4309"></source>
  def __init__(self,
               units,
               activation='tanh',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               **kwargs):
    if units < 0:
      raise ValueError(f'Received an invalid value for argument `units`, '
                       f'expected a positive integer, got {units}.')
    # By default use cached variable under v2 mode, see b/143699808.
    if tf.compat.v1.executing_eagerly_outside_functions():
      self._enable_caching_device = kwargs.pop('enable_caching_device', True)
    else:
      self._enable_caching_device = kwargs.pop('enable_caching_device', False)
    super(SimpleRNNCell, self).__init__(**kwargs)
    self.units = units
    self.activation = activations.get(activation)
    self.use_bias = use_bias

    self.kernel_initializer = initializers.get(kernel_initializer)
    self.recurrent_initializer = initializers.get(recurrent_initializer)
    self.bias_initializer = initializers.get(bias_initializer)

    self.kernel_regularizer = regularizers.get(kernel_regularizer)
    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)
    self.bias_regularizer = regularizers.get(bias_regularizer)

    self.kernel_constraint = constraints.get(kernel_constraint)
    self.recurrent_constraint = constraints.get(recurrent_constraint)
    self.bias_constraint = constraints.get(bias_constraint)

    self.dropout = min(1., max(0., dropout))
    self.recurrent_dropout = min(1., max(0., recurrent_dropout))
    self.state_size = self.units
    self.output_size = self.units

</clonepair30>

<clonepair30>
<source file="systems/keras-2.8.0/keras/layers/recurrent.py" startline="2329" endline="2384" pcid="4358"></source>
  def __init__(self,
               units,
               activation='tanh',
               recurrent_activation='hard_sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               unit_forget_bias=True,
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               **kwargs):
    if units < 0:
      raise ValueError(f'Received an invalid value for argument `units`, '
                       f'expected a positive integer, got {units}.')
    # By default use cached variable under v2 mode, see b/143699808.
    if tf.compat.v1.executing_eagerly_outside_functions():
      self._enable_caching_device = kwargs.pop('enable_caching_device', True)
    else:
      self._enable_caching_device = kwargs.pop('enable_caching_device', False)
    super(LSTMCell, self).__init__(**kwargs)
    self.units = units
    self.activation = activations.get(activation)
    self.recurrent_activation = activations.get(recurrent_activation)
    self.use_bias = use_bias

    self.kernel_initializer = initializers.get(kernel_initializer)
    self.recurrent_initializer = initializers.get(recurrent_initializer)
    self.bias_initializer = initializers.get(bias_initializer)
    self.unit_forget_bias = unit_forget_bias

    self.kernel_regularizer = regularizers.get(kernel_regularizer)
    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)
    self.bias_regularizer = regularizers.get(bias_regularizer)

    self.kernel_constraint = constraints.get(kernel_constraint)
    self.recurrent_constraint = constraints.get(recurrent_constraint)
    self.bias_constraint = constraints.get(bias_constraint)

    self.dropout = min(1., max(0., dropout))
    self.recurrent_dropout = min(1., max(0., recurrent_dropout))
    implementation = kwargs.pop('implementation', 1)
    if self.recurrent_dropout != 0 and implementation != 1:
      logging.debug(RECURRENT_DROPOUT_WARNING_MSG)
      self.implementation = 1
    else:
      self.implementation = implementation
    self.state_size = [self.units, self.units]
    self.output_size = self.units

</clonepair30>
<clonepair31>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="155" endline="296" pcid="3274"></source>
def conv1d(inputs,
           filters,
           kernel_size,
           strides=1,
           padding='valid',
           data_format='channels_last',
           dilation_rate=1,
           activation=None,
           use_bias=True,
           kernel_initializer=None,
           bias_initializer=tf.compat.v1.zeros_initializer(),
           kernel_regularizer=None,
           bias_regularizer=None,
           activity_regularizer=None,
           kernel_constraint=None,
           bias_constraint=None,
           trainable=True,
           name=None,
           reuse=None):
  """Functional interface for 1D convolution layer (e.g. temporal convolution).

  This layer creates a convolution kernel that is convolved
  (actually cross-correlated) with the layer input to produce a tensor of
  outputs. If `use_bias` is True (and a `bias_initializer` is provided),
  a bias vector is created and added to the outputs. Finally, if
  `activation` is not `None`, it is applied to the outputs as well.

  Args:
    inputs: Tensor input.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: An integer or tuple/list of a single integer, specifying the
      length of the 1D convolution window.
    strides: An integer or tuple/list of a single integer,
      specifying the stride length of the convolution.
      Specifying any stride value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, length, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, length)`.
    dilation_rate: An integer or tuple/list of a single integer, specifying
      the dilation rate to use for dilated convolution.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any `strides` value != 1.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is `tf.keras.layers.Conv1D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv1d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv1D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv1d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv1D` instead.',
      stacklevel=2)
  layer = Conv1D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair31>

<clonepair31>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1103" endline="1149" pcid="3280"></source>
  def __init__(self, filters,
               kernel_size,
               strides=(1, 1),
               padding='valid',
               data_format='channels_last',
               dilation_rate=(1, 1),
               depth_multiplier=1,
               activation=None,
               use_bias=True,
               depthwise_initializer=None,
               pointwise_initializer=None,
               bias_initializer=tf.compat.v1.zeros_initializer(),
               depthwise_regularizer=None,
               pointwise_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               depthwise_constraint=None,
               pointwise_constraint=None,
               bias_constraint=None,
               trainable=True,
               name=None,
               **kwargs):
    super(SeparableConv2D, self).__init__(
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        depth_multiplier=depth_multiplier,
        activation=activation,
        use_bias=use_bias,
        depthwise_initializer=depthwise_initializer,
        pointwise_initializer=pointwise_initializer,
        bias_initializer=bias_initializer,
        depthwise_regularizer=depthwise_regularizer,
        pointwise_regularizer=pointwise_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        depthwise_constraint=depthwise_constraint,
        pointwise_constraint=pointwise_constraint,
        bias_constraint=bias_constraint,
        trainable=trainable,
        name=name,
        **kwargs)


</clonepair31>
<clonepair32>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="429" endline="577" pcid="3276"></source>
def conv2d(inputs,
           filters,
           kernel_size,
           strides=(1, 1),
           padding='valid',
           data_format='channels_last',
           dilation_rate=(1, 1),
           activation=None,
           use_bias=True,
           kernel_initializer=None,
           bias_initializer=tf.compat.v1.zeros_initializer(),
           kernel_regularizer=None,
           bias_regularizer=None,
           activity_regularizer=None,
           kernel_constraint=None,
           bias_constraint=None,
           trainable=True,
           name=None,
           reuse=None):
  """Functional interface for the 2D convolution layer.

  This layer creates a convolution kernel that is convolved
  (actually cross-correlated) with the layer input to produce a tensor of
  outputs. If `use_bias` is True (and a `bias_initializer` is provided),
  a bias vector is created and added to the outputs. Finally, if
  `activation` is not `None`, it is applied to the outputs as well.

  Args:
    inputs: Tensor input.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: An integer or tuple/list of 2 integers, specifying the
      height and width of the 2D convolution window.
      Can be a single integer to specify the same value for
      all spatial dimensions.
    strides: An integer or tuple/list of 2 integers,
      specifying the strides of the convolution along the height and width.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Specifying any stride value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, height, width)`.

    dilation_rate: An integer or tuple/list of 2 integers, specifying
      the dilation rate to use for dilated convolution.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is `tf.keras.layers.Conv2D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv2d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv2D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv2d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv2D` instead.',
      stacklevel=2)
  layer = Conv2D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair32>

<clonepair32>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1103" endline="1149" pcid="3280"></source>
  def __init__(self, filters,
               kernel_size,
               strides=(1, 1),
               padding='valid',
               data_format='channels_last',
               dilation_rate=(1, 1),
               depth_multiplier=1,
               activation=None,
               use_bias=True,
               depthwise_initializer=None,
               pointwise_initializer=None,
               bias_initializer=tf.compat.v1.zeros_initializer(),
               depthwise_regularizer=None,
               pointwise_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               depthwise_constraint=None,
               pointwise_constraint=None,
               bias_constraint=None,
               trainable=True,
               name=None,
               **kwargs):
    super(SeparableConv2D, self).__init__(
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        depth_multiplier=depth_multiplier,
        activation=activation,
        use_bias=use_bias,
        depthwise_initializer=depthwise_initializer,
        pointwise_initializer=pointwise_initializer,
        bias_initializer=bias_initializer,
        depthwise_regularizer=depthwise_regularizer,
        pointwise_regularizer=pointwise_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        depthwise_constraint=depthwise_constraint,
        pointwise_constraint=pointwise_constraint,
        bias_constraint=bias_constraint,
        trainable=trainable,
        name=name,
        **kwargs)


</clonepair32>
<clonepair33>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="711" endline="860" pcid="3278"></source>
def conv3d(inputs,
           filters,
           kernel_size,
           strides=(1, 1, 1),
           padding='valid',
           data_format='channels_last',
           dilation_rate=(1, 1, 1),
           activation=None,
           use_bias=True,
           kernel_initializer=None,
           bias_initializer=tf.compat.v1.zeros_initializer(),
           kernel_regularizer=None,
           bias_regularizer=None,
           activity_regularizer=None,
           kernel_constraint=None,
           bias_constraint=None,
           trainable=True,
           name=None,
           reuse=None):
  """Functional interface for the 3D convolution layer.

  This layer creates a convolution kernel that is convolved
  (actually cross-correlated) with the layer input to produce a tensor of
  outputs. If `use_bias` is True (and a `bias_initializer` is provided),
  a bias vector is created and added to the outputs. Finally, if
  `activation` is not `None`, it is applied to the outputs as well.

  Args:
    inputs: Tensor input.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: An integer or tuple/list of 3 integers, specifying the
      depth, height and width of the 3D convolution window.
      Can be a single integer to specify the same value for
      all spatial dimensions.
    strides: An integer or tuple/list of 3 integers,
      specifying the strides of the convolution along the depth,
      height and width.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Specifying any stride value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, depth, height, width, channels)` while `channels_first`
      corresponds to inputs with shape
      `(batch, channels, depth, height, width)`.
    dilation_rate: An integer or tuple/list of 3 integers, specifying
      the dilation rate to use for dilated convolution.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is `tf.keras.layers.Conv3D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv3d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv3D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv3d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv3D` instead.',
      stacklevel=2)
  layer = Conv3D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair33>

<clonepair33>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1103" endline="1149" pcid="3280"></source>
  def __init__(self, filters,
               kernel_size,
               strides=(1, 1),
               padding='valid',
               data_format='channels_last',
               dilation_rate=(1, 1),
               depth_multiplier=1,
               activation=None,
               use_bias=True,
               depthwise_initializer=None,
               pointwise_initializer=None,
               bias_initializer=tf.compat.v1.zeros_initializer(),
               depthwise_regularizer=None,
               pointwise_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               depthwise_constraint=None,
               pointwise_constraint=None,
               bias_constraint=None,
               trainable=True,
               name=None,
               **kwargs):
    super(SeparableConv2D, self).__init__(
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        depth_multiplier=depth_multiplier,
        activation=activation,
        use_bias=use_bias,
        depthwise_initializer=depthwise_initializer,
        pointwise_initializer=pointwise_initializer,
        bias_initializer=bias_initializer,
        depthwise_regularizer=depthwise_regularizer,
        pointwise_regularizer=pointwise_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        depthwise_constraint=depthwise_constraint,
        pointwise_constraint=pointwise_constraint,
        bias_constraint=bias_constraint,
        trainable=trainable,
        name=name,
        **kwargs)


</clonepair33>
<clonepair34>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1103" endline="1149" pcid="3280"></source>
  def __init__(self, filters,
               kernel_size,
               strides=(1, 1),
               padding='valid',
               data_format='channels_last',
               dilation_rate=(1, 1),
               depth_multiplier=1,
               activation=None,
               use_bias=True,
               depthwise_initializer=None,
               pointwise_initializer=None,
               bias_initializer=tf.compat.v1.zeros_initializer(),
               depthwise_regularizer=None,
               pointwise_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               depthwise_constraint=None,
               pointwise_constraint=None,
               bias_constraint=None,
               trainable=True,
               name=None,
               **kwargs):
    super(SeparableConv2D, self).__init__(
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        depth_multiplier=depth_multiplier,
        activation=activation,
        use_bias=use_bias,
        depthwise_initializer=depthwise_initializer,
        pointwise_initializer=pointwise_initializer,
        bias_initializer=bias_initializer,
        depthwise_regularizer=depthwise_regularizer,
        pointwise_regularizer=pointwise_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        depthwise_constraint=depthwise_constraint,
        pointwise_constraint=pointwise_constraint,
        bias_constraint=bias_constraint,
        trainable=trainable,
        name=name,
        **kwargs)


</clonepair34>

<clonepair34>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1601" endline="1738" pcid="3284"></source>
def conv2d_transpose(inputs,
                     filters,
                     kernel_size,
                     strides=(1, 1),
                     padding='valid',
                     data_format='channels_last',
                     activation=None,
                     use_bias=True,
                     kernel_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     kernel_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     kernel_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for transposed 2D convolution layer.

  The need for transposed convolutions generally arises
  from the desire to use a transformation going in the opposite direction
  of a normal convolution, i.e., from something that has the shape of the
  output of some convolution to something that has the shape of its input
  while maintaining a connectivity pattern that is compatible with
  said convolution.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A tuple or list of 2 positive integers specifying the spatial
      dimensions of the filters. Can be a single integer to specify the same
      value for all spatial dimensions.
    strides: A tuple or list of 2 positive integers specifying the strides
      of the convolution. Can be a single integer to specify the same value for
      all spatial dimensions.
    padding: one of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, height, width)`.
    activation: Activation function. Set it to `None` to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If `None`, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.Conv2DTranspose`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv2d_transpose(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv2DTranspose(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv2d_transpose` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv2DTranspose` instead.',
      stacklevel=2)
  layer = Conv2DTranspose(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair34>
<clonepair35>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="155" endline="296" pcid="3274"></source>
def conv1d(inputs,
           filters,
           kernel_size,
           strides=1,
           padding='valid',
           data_format='channels_last',
           dilation_rate=1,
           activation=None,
           use_bias=True,
           kernel_initializer=None,
           bias_initializer=tf.compat.v1.zeros_initializer(),
           kernel_regularizer=None,
           bias_regularizer=None,
           activity_regularizer=None,
           kernel_constraint=None,
           bias_constraint=None,
           trainable=True,
           name=None,
           reuse=None):
  """Functional interface for 1D convolution layer (e.g. temporal convolution).

  This layer creates a convolution kernel that is convolved
  (actually cross-correlated) with the layer input to produce a tensor of
  outputs. If `use_bias` is True (and a `bias_initializer` is provided),
  a bias vector is created and added to the outputs. Finally, if
  `activation` is not `None`, it is applied to the outputs as well.

  Args:
    inputs: Tensor input.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: An integer or tuple/list of a single integer, specifying the
      length of the 1D convolution window.
    strides: An integer or tuple/list of a single integer,
      specifying the stride length of the convolution.
      Specifying any stride value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, length, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, length)`.
    dilation_rate: An integer or tuple/list of a single integer, specifying
      the dilation rate to use for dilated convolution.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any `strides` value != 1.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is `tf.keras.layers.Conv1D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv1d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv1D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv1d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv1D` instead.',
      stacklevel=2)
  layer = Conv1D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair35>

<clonepair35>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="956" endline="1002" pcid="3279"></source>
  def __init__(self, filters,
               kernel_size,
               strides=1,
               padding='valid',
               data_format='channels_last',
               dilation_rate=1,
               depth_multiplier=1,
               activation=None,
               use_bias=True,
               depthwise_initializer=None,
               pointwise_initializer=None,
               bias_initializer=tf.compat.v1.zeros_initializer(),
               depthwise_regularizer=None,
               pointwise_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               depthwise_constraint=None,
               pointwise_constraint=None,
               bias_constraint=None,
               trainable=True,
               name=None,
               **kwargs):
    super(SeparableConv1D, self).__init__(
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        depth_multiplier=depth_multiplier,
        activation=activation,
        use_bias=use_bias,
        depthwise_initializer=depthwise_initializer,
        pointwise_initializer=pointwise_initializer,
        bias_initializer=bias_initializer,
        depthwise_regularizer=depthwise_regularizer,
        pointwise_regularizer=pointwise_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        depthwise_constraint=depthwise_constraint,
        pointwise_constraint=pointwise_constraint,
        bias_constraint=bias_constraint,
        trainable=trainable,
        name=name,
        **kwargs)


</clonepair35>
<clonepair36>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="429" endline="577" pcid="3276"></source>
def conv2d(inputs,
           filters,
           kernel_size,
           strides=(1, 1),
           padding='valid',
           data_format='channels_last',
           dilation_rate=(1, 1),
           activation=None,
           use_bias=True,
           kernel_initializer=None,
           bias_initializer=tf.compat.v1.zeros_initializer(),
           kernel_regularizer=None,
           bias_regularizer=None,
           activity_regularizer=None,
           kernel_constraint=None,
           bias_constraint=None,
           trainable=True,
           name=None,
           reuse=None):
  """Functional interface for the 2D convolution layer.

  This layer creates a convolution kernel that is convolved
  (actually cross-correlated) with the layer input to produce a tensor of
  outputs. If `use_bias` is True (and a `bias_initializer` is provided),
  a bias vector is created and added to the outputs. Finally, if
  `activation` is not `None`, it is applied to the outputs as well.

  Args:
    inputs: Tensor input.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: An integer or tuple/list of 2 integers, specifying the
      height and width of the 2D convolution window.
      Can be a single integer to specify the same value for
      all spatial dimensions.
    strides: An integer or tuple/list of 2 integers,
      specifying the strides of the convolution along the height and width.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Specifying any stride value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, height, width)`.

    dilation_rate: An integer or tuple/list of 2 integers, specifying
      the dilation rate to use for dilated convolution.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is `tf.keras.layers.Conv2D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv2d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv2D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv2d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv2D` instead.',
      stacklevel=2)
  layer = Conv2D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair36>

<clonepair36>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="956" endline="1002" pcid="3279"></source>
  def __init__(self, filters,
               kernel_size,
               strides=1,
               padding='valid',
               data_format='channels_last',
               dilation_rate=1,
               depth_multiplier=1,
               activation=None,
               use_bias=True,
               depthwise_initializer=None,
               pointwise_initializer=None,
               bias_initializer=tf.compat.v1.zeros_initializer(),
               depthwise_regularizer=None,
               pointwise_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               depthwise_constraint=None,
               pointwise_constraint=None,
               bias_constraint=None,
               trainable=True,
               name=None,
               **kwargs):
    super(SeparableConv1D, self).__init__(
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        depth_multiplier=depth_multiplier,
        activation=activation,
        use_bias=use_bias,
        depthwise_initializer=depthwise_initializer,
        pointwise_initializer=pointwise_initializer,
        bias_initializer=bias_initializer,
        depthwise_regularizer=depthwise_regularizer,
        pointwise_regularizer=pointwise_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        depthwise_constraint=depthwise_constraint,
        pointwise_constraint=pointwise_constraint,
        bias_constraint=bias_constraint,
        trainable=trainable,
        name=name,
        **kwargs)


</clonepair36>
<clonepair37>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="711" endline="860" pcid="3278"></source>
def conv3d(inputs,
           filters,
           kernel_size,
           strides=(1, 1, 1),
           padding='valid',
           data_format='channels_last',
           dilation_rate=(1, 1, 1),
           activation=None,
           use_bias=True,
           kernel_initializer=None,
           bias_initializer=tf.compat.v1.zeros_initializer(),
           kernel_regularizer=None,
           bias_regularizer=None,
           activity_regularizer=None,
           kernel_constraint=None,
           bias_constraint=None,
           trainable=True,
           name=None,
           reuse=None):
  """Functional interface for the 3D convolution layer.

  This layer creates a convolution kernel that is convolved
  (actually cross-correlated) with the layer input to produce a tensor of
  outputs. If `use_bias` is True (and a `bias_initializer` is provided),
  a bias vector is created and added to the outputs. Finally, if
  `activation` is not `None`, it is applied to the outputs as well.

  Args:
    inputs: Tensor input.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: An integer or tuple/list of 3 integers, specifying the
      depth, height and width of the 3D convolution window.
      Can be a single integer to specify the same value for
      all spatial dimensions.
    strides: An integer or tuple/list of 3 integers,
      specifying the strides of the convolution along the depth,
      height and width.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Specifying any stride value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, depth, height, width, channels)` while `channels_first`
      corresponds to inputs with shape
      `(batch, channels, depth, height, width)`.
    dilation_rate: An integer or tuple/list of 3 integers, specifying
      the dilation rate to use for dilated convolution.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is `tf.keras.layers.Conv3D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv3d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv3D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv3d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv3D` instead.',
      stacklevel=2)
  layer = Conv3D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair37>

<clonepair37>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="956" endline="1002" pcid="3279"></source>
  def __init__(self, filters,
               kernel_size,
               strides=1,
               padding='valid',
               data_format='channels_last',
               dilation_rate=1,
               depth_multiplier=1,
               activation=None,
               use_bias=True,
               depthwise_initializer=None,
               pointwise_initializer=None,
               bias_initializer=tf.compat.v1.zeros_initializer(),
               depthwise_regularizer=None,
               pointwise_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               depthwise_constraint=None,
               pointwise_constraint=None,
               bias_constraint=None,
               trainable=True,
               name=None,
               **kwargs):
    super(SeparableConv1D, self).__init__(
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        depth_multiplier=depth_multiplier,
        activation=activation,
        use_bias=use_bias,
        depthwise_initializer=depthwise_initializer,
        pointwise_initializer=pointwise_initializer,
        bias_initializer=bias_initializer,
        depthwise_regularizer=depthwise_regularizer,
        pointwise_regularizer=pointwise_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        depthwise_constraint=depthwise_constraint,
        pointwise_constraint=pointwise_constraint,
        bias_constraint=bias_constraint,
        trainable=trainable,
        name=name,
        **kwargs)


</clonepair37>
<clonepair38>
<source file="systems/keras-2.8.0/keras/preprocessing/image_dataset_test.py" startline="292" endline="348" pcid="3734"></source>
  def test_image_dataset_from_directory_errors(self):
    if PIL is None:
      return  # Skip test if PIL is not available.

    directory = self._prepare_directory(num_classes=3, count=5)

    with self.assertRaisesRegex(ValueError, '`labels` argument should be'):
      _ = image_dataset.image_dataset_from_directory(
          directory, labels='other')

    with self.assertRaisesRegex(ValueError, '`label_mode` argument must be'):
      _ = image_dataset.image_dataset_from_directory(
          directory, label_mode='other')

    with self.assertRaisesRegex(ValueError, '`color_mode` must be one of'):
      _ = image_dataset.image_dataset_from_directory(
          directory, color_mode='other')

    with self.assertRaisesRegex(
        ValueError, 'only pass `class_names` if `labels="inferred"`'):
      _ = image_dataset.image_dataset_from_directory(
          directory, labels=[0, 0, 1, 1, 1],
          class_names=['class_0', 'class_1', 'class_2'])

    with self.assertRaisesRegex(
        ValueError,
        'Expected the lengths of `labels` to match the number of files'):
      _ = image_dataset.image_dataset_from_directory(
          directory, labels=[0, 0, 1, 1])

    with self.assertRaisesRegex(
        ValueError, '`class_names` passed did not match'):
      _ = image_dataset.image_dataset_from_directory(
          directory, class_names=['class_0', 'class_2'])

    with self.assertRaisesRegex(ValueError, 'there must be exactly 2'):
      _ = image_dataset.image_dataset_from_directory(
          directory, label_mode='binary')

    with self.assertRaisesRegex(ValueError,
                                '`validation_split` must be between 0 and 1'):
      _ = image_dataset.image_dataset_from_directory(
          directory, validation_split=2)

    with self.assertRaisesRegex(ValueError,
                                '`subset` must be either "training" or'):
      _ = image_dataset.image_dataset_from_directory(
          directory, validation_split=0.2, subset='other')

    with self.assertRaisesRegex(ValueError, '`validation_split` must be set'):
      _ = image_dataset.image_dataset_from_directory(
          directory, validation_split=0, subset='training')

    with self.assertRaisesRegex(ValueError, 'must provide a `seed`'):
      _ = image_dataset.image_dataset_from_directory(
          directory, validation_split=0.2, subset='training')

</clonepair38>

<clonepair38>
<source file="systems/keras-2.8.0/keras/preprocessing/text_dataset_test.py" startline="201" endline="250" pcid="3707"></source>
  def test_text_dataset_from_directory_errors(self):
    directory = self._prepare_directory(num_classes=3, count=5)

    with self.assertRaisesRegex(ValueError, '`labels` argument should be'):
      _ = text_dataset.text_dataset_from_directory(
          directory, labels='other')

    with self.assertRaisesRegex(ValueError, '`label_mode` argument must be'):
      _ = text_dataset.text_dataset_from_directory(
          directory, label_mode='other')

    with self.assertRaisesRegex(
        ValueError, 'only pass `class_names` if `labels="inferred"`'):
      _ = text_dataset.text_dataset_from_directory(
          directory, labels=[0, 0, 1, 1, 1],
          class_names=['class_0', 'class_1', 'class_2'])

    with self.assertRaisesRegex(
        ValueError,
        'Expected the lengths of `labels` to match the number of files'):
      _ = text_dataset.text_dataset_from_directory(
          directory, labels=[0, 0, 1, 1])

    with self.assertRaisesRegex(
        ValueError, '`class_names` passed did not match'):
      _ = text_dataset.text_dataset_from_directory(
          directory, class_names=['class_0', 'class_2'])

    with self.assertRaisesRegex(ValueError, 'there must be exactly 2'):
      _ = text_dataset.text_dataset_from_directory(
          directory, label_mode='binary')

    with self.assertRaisesRegex(ValueError,
                                '`validation_split` must be between 0 and 1'):
      _ = text_dataset.text_dataset_from_directory(
          directory, validation_split=2)

    with self.assertRaisesRegex(ValueError,
                                '`subset` must be either "training" or'):
      _ = text_dataset.text_dataset_from_directory(
          directory, validation_split=0.2, subset='other')

    with self.assertRaisesRegex(ValueError, '`validation_split` must be set'):
      _ = text_dataset.text_dataset_from_directory(
          directory, validation_split=0, subset='training')

    with self.assertRaisesRegex(ValueError, 'must provide a `seed`'):
      _ = text_dataset.text_dataset_from_directory(
          directory, validation_split=0.2, subset='training')

</clonepair38>
<clonepair39>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="155" endline="296" pcid="3274"></source>
def conv1d(inputs,
           filters,
           kernel_size,
           strides=1,
           padding='valid',
           data_format='channels_last',
           dilation_rate=1,
           activation=None,
           use_bias=True,
           kernel_initializer=None,
           bias_initializer=tf.compat.v1.zeros_initializer(),
           kernel_regularizer=None,
           bias_regularizer=None,
           activity_regularizer=None,
           kernel_constraint=None,
           bias_constraint=None,
           trainable=True,
           name=None,
           reuse=None):
  """Functional interface for 1D convolution layer (e.g. temporal convolution).

  This layer creates a convolution kernel that is convolved
  (actually cross-correlated) with the layer input to produce a tensor of
  outputs. If `use_bias` is True (and a `bias_initializer` is provided),
  a bias vector is created and added to the outputs. Finally, if
  `activation` is not `None`, it is applied to the outputs as well.

  Args:
    inputs: Tensor input.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: An integer or tuple/list of a single integer, specifying the
      length of the 1D convolution window.
    strides: An integer or tuple/list of a single integer,
      specifying the stride length of the convolution.
      Specifying any stride value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, length, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, length)`.
    dilation_rate: An integer or tuple/list of a single integer, specifying
      the dilation rate to use for dilated convolution.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any `strides` value != 1.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is `tf.keras.layers.Conv1D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv1d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv1D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv1d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv1D` instead.',
      stacklevel=2)
  layer = Conv1D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair39>

<clonepair39>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="429" endline="577" pcid="3276"></source>
def conv2d(inputs,
           filters,
           kernel_size,
           strides=(1, 1),
           padding='valid',
           data_format='channels_last',
           dilation_rate=(1, 1),
           activation=None,
           use_bias=True,
           kernel_initializer=None,
           bias_initializer=tf.compat.v1.zeros_initializer(),
           kernel_regularizer=None,
           bias_regularizer=None,
           activity_regularizer=None,
           kernel_constraint=None,
           bias_constraint=None,
           trainable=True,
           name=None,
           reuse=None):
  """Functional interface for the 2D convolution layer.

  This layer creates a convolution kernel that is convolved
  (actually cross-correlated) with the layer input to produce a tensor of
  outputs. If `use_bias` is True (and a `bias_initializer` is provided),
  a bias vector is created and added to the outputs. Finally, if
  `activation` is not `None`, it is applied to the outputs as well.

  Args:
    inputs: Tensor input.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: An integer or tuple/list of 2 integers, specifying the
      height and width of the 2D convolution window.
      Can be a single integer to specify the same value for
      all spatial dimensions.
    strides: An integer or tuple/list of 2 integers,
      specifying the strides of the convolution along the height and width.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Specifying any stride value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, height, width)`.

    dilation_rate: An integer or tuple/list of 2 integers, specifying
      the dilation rate to use for dilated convolution.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is `tf.keras.layers.Conv2D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv2d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv2D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv2d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv2D` instead.',
      stacklevel=2)
  layer = Conv2D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair39>
<clonepair40>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="155" endline="296" pcid="3274"></source>
def conv1d(inputs,
           filters,
           kernel_size,
           strides=1,
           padding='valid',
           data_format='channels_last',
           dilation_rate=1,
           activation=None,
           use_bias=True,
           kernel_initializer=None,
           bias_initializer=tf.compat.v1.zeros_initializer(),
           kernel_regularizer=None,
           bias_regularizer=None,
           activity_regularizer=None,
           kernel_constraint=None,
           bias_constraint=None,
           trainable=True,
           name=None,
           reuse=None):
  """Functional interface for 1D convolution layer (e.g. temporal convolution).

  This layer creates a convolution kernel that is convolved
  (actually cross-correlated) with the layer input to produce a tensor of
  outputs. If `use_bias` is True (and a `bias_initializer` is provided),
  a bias vector is created and added to the outputs. Finally, if
  `activation` is not `None`, it is applied to the outputs as well.

  Args:
    inputs: Tensor input.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: An integer or tuple/list of a single integer, specifying the
      length of the 1D convolution window.
    strides: An integer or tuple/list of a single integer,
      specifying the stride length of the convolution.
      Specifying any stride value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, length, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, length)`.
    dilation_rate: An integer or tuple/list of a single integer, specifying
      the dilation rate to use for dilated convolution.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any `strides` value != 1.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is `tf.keras.layers.Conv1D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv1d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv1D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv1d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv1D` instead.',
      stacklevel=2)
  layer = Conv1D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair40>

<clonepair40>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="711" endline="860" pcid="3278"></source>
def conv3d(inputs,
           filters,
           kernel_size,
           strides=(1, 1, 1),
           padding='valid',
           data_format='channels_last',
           dilation_rate=(1, 1, 1),
           activation=None,
           use_bias=True,
           kernel_initializer=None,
           bias_initializer=tf.compat.v1.zeros_initializer(),
           kernel_regularizer=None,
           bias_regularizer=None,
           activity_regularizer=None,
           kernel_constraint=None,
           bias_constraint=None,
           trainable=True,
           name=None,
           reuse=None):
  """Functional interface for the 3D convolution layer.

  This layer creates a convolution kernel that is convolved
  (actually cross-correlated) with the layer input to produce a tensor of
  outputs. If `use_bias` is True (and a `bias_initializer` is provided),
  a bias vector is created and added to the outputs. Finally, if
  `activation` is not `None`, it is applied to the outputs as well.

  Args:
    inputs: Tensor input.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: An integer or tuple/list of 3 integers, specifying the
      depth, height and width of the 3D convolution window.
      Can be a single integer to specify the same value for
      all spatial dimensions.
    strides: An integer or tuple/list of 3 integers,
      specifying the strides of the convolution along the depth,
      height and width.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Specifying any stride value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, depth, height, width, channels)` while `channels_first`
      corresponds to inputs with shape
      `(batch, channels, depth, height, width)`.
    dilation_rate: An integer or tuple/list of 3 integers, specifying
      the dilation rate to use for dilated convolution.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is `tf.keras.layers.Conv3D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv3d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv3D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv3d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv3D` instead.',
      stacklevel=2)
  layer = Conv3D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair40>
<clonepair41>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="155" endline="296" pcid="3274"></source>
def conv1d(inputs,
           filters,
           kernel_size,
           strides=1,
           padding='valid',
           data_format='channels_last',
           dilation_rate=1,
           activation=None,
           use_bias=True,
           kernel_initializer=None,
           bias_initializer=tf.compat.v1.zeros_initializer(),
           kernel_regularizer=None,
           bias_regularizer=None,
           activity_regularizer=None,
           kernel_constraint=None,
           bias_constraint=None,
           trainable=True,
           name=None,
           reuse=None):
  """Functional interface for 1D convolution layer (e.g. temporal convolution).

  This layer creates a convolution kernel that is convolved
  (actually cross-correlated) with the layer input to produce a tensor of
  outputs. If `use_bias` is True (and a `bias_initializer` is provided),
  a bias vector is created and added to the outputs. Finally, if
  `activation` is not `None`, it is applied to the outputs as well.

  Args:
    inputs: Tensor input.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: An integer or tuple/list of a single integer, specifying the
      length of the 1D convolution window.
    strides: An integer or tuple/list of a single integer,
      specifying the stride length of the convolution.
      Specifying any stride value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, length, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, length)`.
    dilation_rate: An integer or tuple/list of a single integer, specifying
      the dilation rate to use for dilated convolution.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any `strides` value != 1.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is `tf.keras.layers.Conv1D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv1d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv1D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv1d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv1D` instead.',
      stacklevel=2)
  layer = Conv1D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair41>

<clonepair41>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1858" endline="1991" pcid="3286"></source>
def conv3d_transpose(inputs,
                     filters,
                     kernel_size,
                     strides=(1, 1, 1),
                     padding='valid',
                     data_format='channels_last',
                     activation=None,
                     use_bias=True,
                     kernel_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     kernel_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     kernel_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for transposed 3D convolution layer.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A tuple or list of 3 positive integers specifying the spatial
      dimensions of the filters. Can be a single integer to specify the same
      value for all spatial dimensions.
    strides: A tuple or list of 3 positive integers specifying the strides
      of the convolution. Can be a single integer to specify the same value for
      all spatial dimensions.
    padding: one of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, depth, height, width, channels)` while `channels_first`
      corresponds to inputs with shape
      `(batch, channels, depth, height, width)`.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.Conv3DTranspose`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv3d_transpose(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv3DTranspose(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv3d_transpose` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv3DTranspose` instead.',
      stacklevel=2)
  layer = Conv3DTranspose(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


# Aliases

</clonepair41>
<clonepair42>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="155" endline="296" pcid="3274"></source>
def conv1d(inputs,
           filters,
           kernel_size,
           strides=1,
           padding='valid',
           data_format='channels_last',
           dilation_rate=1,
           activation=None,
           use_bias=True,
           kernel_initializer=None,
           bias_initializer=tf.compat.v1.zeros_initializer(),
           kernel_regularizer=None,
           bias_regularizer=None,
           activity_regularizer=None,
           kernel_constraint=None,
           bias_constraint=None,
           trainable=True,
           name=None,
           reuse=None):
  """Functional interface for 1D convolution layer (e.g. temporal convolution).

  This layer creates a convolution kernel that is convolved
  (actually cross-correlated) with the layer input to produce a tensor of
  outputs. If `use_bias` is True (and a `bias_initializer` is provided),
  a bias vector is created and added to the outputs. Finally, if
  `activation` is not `None`, it is applied to the outputs as well.

  Args:
    inputs: Tensor input.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: An integer or tuple/list of a single integer, specifying the
      length of the 1D convolution window.
    strides: An integer or tuple/list of a single integer,
      specifying the stride length of the convolution.
      Specifying any stride value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, length, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, length)`.
    dilation_rate: An integer or tuple/list of a single integer, specifying
      the dilation rate to use for dilated convolution.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any `strides` value != 1.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is `tf.keras.layers.Conv1D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv1d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv1D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv1d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv1D` instead.',
      stacklevel=2)
  layer = Conv1D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair42>

<clonepair42>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1601" endline="1738" pcid="3284"></source>
def conv2d_transpose(inputs,
                     filters,
                     kernel_size,
                     strides=(1, 1),
                     padding='valid',
                     data_format='channels_last',
                     activation=None,
                     use_bias=True,
                     kernel_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     kernel_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     kernel_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for transposed 2D convolution layer.

  The need for transposed convolutions generally arises
  from the desire to use a transformation going in the opposite direction
  of a normal convolution, i.e., from something that has the shape of the
  output of some convolution to something that has the shape of its input
  while maintaining a connectivity pattern that is compatible with
  said convolution.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A tuple or list of 2 positive integers specifying the spatial
      dimensions of the filters. Can be a single integer to specify the same
      value for all spatial dimensions.
    strides: A tuple or list of 2 positive integers specifying the strides
      of the convolution. Can be a single integer to specify the same value for
      all spatial dimensions.
    padding: one of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, height, width)`.
    activation: Activation function. Set it to `None` to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If `None`, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.Conv2DTranspose`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv2d_transpose(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv2DTranspose(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv2d_transpose` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv2DTranspose` instead.',
      stacklevel=2)
  layer = Conv2DTranspose(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair42>
<clonepair43>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="155" endline="296" pcid="3274"></source>
def conv1d(inputs,
           filters,
           kernel_size,
           strides=1,
           padding='valid',
           data_format='channels_last',
           dilation_rate=1,
           activation=None,
           use_bias=True,
           kernel_initializer=None,
           bias_initializer=tf.compat.v1.zeros_initializer(),
           kernel_regularizer=None,
           bias_regularizer=None,
           activity_regularizer=None,
           kernel_constraint=None,
           bias_constraint=None,
           trainable=True,
           name=None,
           reuse=None):
  """Functional interface for 1D convolution layer (e.g. temporal convolution).

  This layer creates a convolution kernel that is convolved
  (actually cross-correlated) with the layer input to produce a tensor of
  outputs. If `use_bias` is True (and a `bias_initializer` is provided),
  a bias vector is created and added to the outputs. Finally, if
  `activation` is not `None`, it is applied to the outputs as well.

  Args:
    inputs: Tensor input.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: An integer or tuple/list of a single integer, specifying the
      length of the 1D convolution window.
    strides: An integer or tuple/list of a single integer,
      specifying the stride length of the convolution.
      Specifying any stride value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, length, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, length)`.
    dilation_rate: An integer or tuple/list of a single integer, specifying
      the dilation rate to use for dilated convolution.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any `strides` value != 1.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is `tf.keras.layers.Conv1D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv1d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv1D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv1d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv1D` instead.',
      stacklevel=2)
  layer = Conv1D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair43>

<clonepair43>
<source file="systems/keras-2.8.0/keras/layers/recurrent.py" startline="2587" endline="2630" pcid="4366"></source>
  def __init__(self,
               units,
               activation='tanh',
               recurrent_activation='hard_sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               unit_forget_bias=True,
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               **kwargs):
    warnings.warn(
        '`tf.keras.experimental.PeepholeLSTMCell` is deprecated '
        'and will be removed in a future version. '
        'Please use tensorflow_addons.rnn.PeepholeLSTMCell '
        'instead.',
        stacklevel=2)
    super(PeepholeLSTMCell, self).__init__(
        units=units,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        unit_forget_bias=unit_forget_bias,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        implementation=kwargs.pop('implementation', 1),
        **kwargs)

</clonepair43>
<clonepair44>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="115" endline="152" pcid="3273"></source>
  def __init__(self, filters,
               kernel_size,
               strides=1,
               padding='valid',
               data_format='channels_last',
               dilation_rate=1,
               activation=None,
               use_bias=True,
               kernel_initializer=None,
               bias_initializer=tf.compat.v1.zeros_initializer(),
               kernel_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               bias_constraint=None,
               trainable=True,
               name=None,
               **kwargs):
    super(Conv1D, self).__init__(
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        activation=activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        bias_initializer=bias_initializer,
        kernel_regularizer=kernel_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        kernel_constraint=kernel_constraint,
        bias_constraint=bias_constraint,
        trainable=trainable,
        name=name, **kwargs)


</clonepair44>

<clonepair44>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="155" endline="296" pcid="3274"></source>
def conv1d(inputs,
           filters,
           kernel_size,
           strides=1,
           padding='valid',
           data_format='channels_last',
           dilation_rate=1,
           activation=None,
           use_bias=True,
           kernel_initializer=None,
           bias_initializer=tf.compat.v1.zeros_initializer(),
           kernel_regularizer=None,
           bias_regularizer=None,
           activity_regularizer=None,
           kernel_constraint=None,
           bias_constraint=None,
           trainable=True,
           name=None,
           reuse=None):
  """Functional interface for 1D convolution layer (e.g. temporal convolution).

  This layer creates a convolution kernel that is convolved
  (actually cross-correlated) with the layer input to produce a tensor of
  outputs. If `use_bias` is True (and a `bias_initializer` is provided),
  a bias vector is created and added to the outputs. Finally, if
  `activation` is not `None`, it is applied to the outputs as well.

  Args:
    inputs: Tensor input.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: An integer or tuple/list of a single integer, specifying the
      length of the 1D convolution window.
    strides: An integer or tuple/list of a single integer,
      specifying the stride length of the convolution.
      Specifying any stride value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, length, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, length)`.
    dilation_rate: An integer or tuple/list of a single integer, specifying
      the dilation rate to use for dilated convolution.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any `strides` value != 1.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is `tf.keras.layers.Conv1D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv1d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv1D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv1d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv1D` instead.',
      stacklevel=2)
  layer = Conv1D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair44>
<clonepair45>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="429" endline="577" pcid="3276"></source>
def conv2d(inputs,
           filters,
           kernel_size,
           strides=(1, 1),
           padding='valid',
           data_format='channels_last',
           dilation_rate=(1, 1),
           activation=None,
           use_bias=True,
           kernel_initializer=None,
           bias_initializer=tf.compat.v1.zeros_initializer(),
           kernel_regularizer=None,
           bias_regularizer=None,
           activity_regularizer=None,
           kernel_constraint=None,
           bias_constraint=None,
           trainable=True,
           name=None,
           reuse=None):
  """Functional interface for the 2D convolution layer.

  This layer creates a convolution kernel that is convolved
  (actually cross-correlated) with the layer input to produce a tensor of
  outputs. If `use_bias` is True (and a `bias_initializer` is provided),
  a bias vector is created and added to the outputs. Finally, if
  `activation` is not `None`, it is applied to the outputs as well.

  Args:
    inputs: Tensor input.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: An integer or tuple/list of 2 integers, specifying the
      height and width of the 2D convolution window.
      Can be a single integer to specify the same value for
      all spatial dimensions.
    strides: An integer or tuple/list of 2 integers,
      specifying the strides of the convolution along the height and width.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Specifying any stride value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, height, width)`.

    dilation_rate: An integer or tuple/list of 2 integers, specifying
      the dilation rate to use for dilated convolution.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is `tf.keras.layers.Conv2D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv2d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv2D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv2d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv2D` instead.',
      stacklevel=2)
  layer = Conv2D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair45>

<clonepair45>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="711" endline="860" pcid="3278"></source>
def conv3d(inputs,
           filters,
           kernel_size,
           strides=(1, 1, 1),
           padding='valid',
           data_format='channels_last',
           dilation_rate=(1, 1, 1),
           activation=None,
           use_bias=True,
           kernel_initializer=None,
           bias_initializer=tf.compat.v1.zeros_initializer(),
           kernel_regularizer=None,
           bias_regularizer=None,
           activity_regularizer=None,
           kernel_constraint=None,
           bias_constraint=None,
           trainable=True,
           name=None,
           reuse=None):
  """Functional interface for the 3D convolution layer.

  This layer creates a convolution kernel that is convolved
  (actually cross-correlated) with the layer input to produce a tensor of
  outputs. If `use_bias` is True (and a `bias_initializer` is provided),
  a bias vector is created and added to the outputs. Finally, if
  `activation` is not `None`, it is applied to the outputs as well.

  Args:
    inputs: Tensor input.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: An integer or tuple/list of 3 integers, specifying the
      depth, height and width of the 3D convolution window.
      Can be a single integer to specify the same value for
      all spatial dimensions.
    strides: An integer or tuple/list of 3 integers,
      specifying the strides of the convolution along the depth,
      height and width.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Specifying any stride value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, depth, height, width, channels)` while `channels_first`
      corresponds to inputs with shape
      `(batch, channels, depth, height, width)`.
    dilation_rate: An integer or tuple/list of 3 integers, specifying
      the dilation rate to use for dilated convolution.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is `tf.keras.layers.Conv3D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv3d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv3D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv3d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv3D` instead.',
      stacklevel=2)
  layer = Conv3D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair45>
<clonepair46>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="429" endline="577" pcid="3276"></source>
def conv2d(inputs,
           filters,
           kernel_size,
           strides=(1, 1),
           padding='valid',
           data_format='channels_last',
           dilation_rate=(1, 1),
           activation=None,
           use_bias=True,
           kernel_initializer=None,
           bias_initializer=tf.compat.v1.zeros_initializer(),
           kernel_regularizer=None,
           bias_regularizer=None,
           activity_regularizer=None,
           kernel_constraint=None,
           bias_constraint=None,
           trainable=True,
           name=None,
           reuse=None):
  """Functional interface for the 2D convolution layer.

  This layer creates a convolution kernel that is convolved
  (actually cross-correlated) with the layer input to produce a tensor of
  outputs. If `use_bias` is True (and a `bias_initializer` is provided),
  a bias vector is created and added to the outputs. Finally, if
  `activation` is not `None`, it is applied to the outputs as well.

  Args:
    inputs: Tensor input.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: An integer or tuple/list of 2 integers, specifying the
      height and width of the 2D convolution window.
      Can be a single integer to specify the same value for
      all spatial dimensions.
    strides: An integer or tuple/list of 2 integers,
      specifying the strides of the convolution along the height and width.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Specifying any stride value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, height, width)`.

    dilation_rate: An integer or tuple/list of 2 integers, specifying
      the dilation rate to use for dilated convolution.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is `tf.keras.layers.Conv2D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv2d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv2D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv2d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv2D` instead.',
      stacklevel=2)
  layer = Conv2D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair46>

<clonepair46>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1858" endline="1991" pcid="3286"></source>
def conv3d_transpose(inputs,
                     filters,
                     kernel_size,
                     strides=(1, 1, 1),
                     padding='valid',
                     data_format='channels_last',
                     activation=None,
                     use_bias=True,
                     kernel_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     kernel_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     kernel_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for transposed 3D convolution layer.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A tuple or list of 3 positive integers specifying the spatial
      dimensions of the filters. Can be a single integer to specify the same
      value for all spatial dimensions.
    strides: A tuple or list of 3 positive integers specifying the strides
      of the convolution. Can be a single integer to specify the same value for
      all spatial dimensions.
    padding: one of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, depth, height, width, channels)` while `channels_first`
      corresponds to inputs with shape
      `(batch, channels, depth, height, width)`.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.Conv3DTranspose`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv3d_transpose(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv3DTranspose(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv3d_transpose` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv3DTranspose` instead.',
      stacklevel=2)
  layer = Conv3DTranspose(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


# Aliases

</clonepair46>
<clonepair47>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="429" endline="577" pcid="3276"></source>
def conv2d(inputs,
           filters,
           kernel_size,
           strides=(1, 1),
           padding='valid',
           data_format='channels_last',
           dilation_rate=(1, 1),
           activation=None,
           use_bias=True,
           kernel_initializer=None,
           bias_initializer=tf.compat.v1.zeros_initializer(),
           kernel_regularizer=None,
           bias_regularizer=None,
           activity_regularizer=None,
           kernel_constraint=None,
           bias_constraint=None,
           trainable=True,
           name=None,
           reuse=None):
  """Functional interface for the 2D convolution layer.

  This layer creates a convolution kernel that is convolved
  (actually cross-correlated) with the layer input to produce a tensor of
  outputs. If `use_bias` is True (and a `bias_initializer` is provided),
  a bias vector is created and added to the outputs. Finally, if
  `activation` is not `None`, it is applied to the outputs as well.

  Args:
    inputs: Tensor input.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: An integer or tuple/list of 2 integers, specifying the
      height and width of the 2D convolution window.
      Can be a single integer to specify the same value for
      all spatial dimensions.
    strides: An integer or tuple/list of 2 integers,
      specifying the strides of the convolution along the height and width.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Specifying any stride value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, height, width)`.

    dilation_rate: An integer or tuple/list of 2 integers, specifying
      the dilation rate to use for dilated convolution.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is `tf.keras.layers.Conv2D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv2d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv2D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv2d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv2D` instead.',
      stacklevel=2)
  layer = Conv2D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair47>

<clonepair47>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1601" endline="1738" pcid="3284"></source>
def conv2d_transpose(inputs,
                     filters,
                     kernel_size,
                     strides=(1, 1),
                     padding='valid',
                     data_format='channels_last',
                     activation=None,
                     use_bias=True,
                     kernel_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     kernel_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     kernel_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for transposed 2D convolution layer.

  The need for transposed convolutions generally arises
  from the desire to use a transformation going in the opposite direction
  of a normal convolution, i.e., from something that has the shape of the
  output of some convolution to something that has the shape of its input
  while maintaining a connectivity pattern that is compatible with
  said convolution.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A tuple or list of 2 positive integers specifying the spatial
      dimensions of the filters. Can be a single integer to specify the same
      value for all spatial dimensions.
    strides: A tuple or list of 2 positive integers specifying the strides
      of the convolution. Can be a single integer to specify the same value for
      all spatial dimensions.
    padding: one of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, height, width)`.
    activation: Activation function. Set it to `None` to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If `None`, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.Conv2DTranspose`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv2d_transpose(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv2DTranspose(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv2d_transpose` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv2DTranspose` instead.',
      stacklevel=2)
  layer = Conv2DTranspose(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair47>
<clonepair48>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="429" endline="577" pcid="3276"></source>
def conv2d(inputs,
           filters,
           kernel_size,
           strides=(1, 1),
           padding='valid',
           data_format='channels_last',
           dilation_rate=(1, 1),
           activation=None,
           use_bias=True,
           kernel_initializer=None,
           bias_initializer=tf.compat.v1.zeros_initializer(),
           kernel_regularizer=None,
           bias_regularizer=None,
           activity_regularizer=None,
           kernel_constraint=None,
           bias_constraint=None,
           trainable=True,
           name=None,
           reuse=None):
  """Functional interface for the 2D convolution layer.

  This layer creates a convolution kernel that is convolved
  (actually cross-correlated) with the layer input to produce a tensor of
  outputs. If `use_bias` is True (and a `bias_initializer` is provided),
  a bias vector is created and added to the outputs. Finally, if
  `activation` is not `None`, it is applied to the outputs as well.

  Args:
    inputs: Tensor input.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: An integer or tuple/list of 2 integers, specifying the
      height and width of the 2D convolution window.
      Can be a single integer to specify the same value for
      all spatial dimensions.
    strides: An integer or tuple/list of 2 integers,
      specifying the strides of the convolution along the height and width.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Specifying any stride value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, height, width)`.

    dilation_rate: An integer or tuple/list of 2 integers, specifying
      the dilation rate to use for dilated convolution.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is `tf.keras.layers.Conv2D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv2d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv2D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv2d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv2D` instead.',
      stacklevel=2)
  layer = Conv2D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair48>

<clonepair48>
<source file="systems/keras-2.8.0/keras/layers/recurrent.py" startline="2587" endline="2630" pcid="4366"></source>
  def __init__(self,
               units,
               activation='tanh',
               recurrent_activation='hard_sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               unit_forget_bias=True,
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               **kwargs):
    warnings.warn(
        '`tf.keras.experimental.PeepholeLSTMCell` is deprecated '
        'and will be removed in a future version. '
        'Please use tensorflow_addons.rnn.PeepholeLSTMCell '
        'instead.',
        stacklevel=2)
    super(PeepholeLSTMCell, self).__init__(
        units=units,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        unit_forget_bias=unit_forget_bias,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        implementation=kwargs.pop('implementation', 1),
        **kwargs)

</clonepair48>
<clonepair49>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="389" endline="426" pcid="3275"></source>
  def __init__(self, filters,
               kernel_size,
               strides=(1, 1),
               padding='valid',
               data_format='channels_last',
               dilation_rate=(1, 1),
               activation=None,
               use_bias=True,
               kernel_initializer=None,
               bias_initializer=tf.compat.v1.zeros_initializer(),
               kernel_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               bias_constraint=None,
               trainable=True,
               name=None,
               **kwargs):
    super(Conv2D, self).__init__(
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        activation=activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        bias_initializer=bias_initializer,
        kernel_regularizer=kernel_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        kernel_constraint=kernel_constraint,
        bias_constraint=bias_constraint,
        trainable=trainable,
        name=name, **kwargs)


</clonepair49>

<clonepair49>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="429" endline="577" pcid="3276"></source>
def conv2d(inputs,
           filters,
           kernel_size,
           strides=(1, 1),
           padding='valid',
           data_format='channels_last',
           dilation_rate=(1, 1),
           activation=None,
           use_bias=True,
           kernel_initializer=None,
           bias_initializer=tf.compat.v1.zeros_initializer(),
           kernel_regularizer=None,
           bias_regularizer=None,
           activity_regularizer=None,
           kernel_constraint=None,
           bias_constraint=None,
           trainable=True,
           name=None,
           reuse=None):
  """Functional interface for the 2D convolution layer.

  This layer creates a convolution kernel that is convolved
  (actually cross-correlated) with the layer input to produce a tensor of
  outputs. If `use_bias` is True (and a `bias_initializer` is provided),
  a bias vector is created and added to the outputs. Finally, if
  `activation` is not `None`, it is applied to the outputs as well.

  Args:
    inputs: Tensor input.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: An integer or tuple/list of 2 integers, specifying the
      height and width of the 2D convolution window.
      Can be a single integer to specify the same value for
      all spatial dimensions.
    strides: An integer or tuple/list of 2 integers,
      specifying the strides of the convolution along the height and width.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Specifying any stride value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, height, width)`.

    dilation_rate: An integer or tuple/list of 2 integers, specifying
      the dilation rate to use for dilated convolution.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is `tf.keras.layers.Conv2D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv2d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv2D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv2d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv2D` instead.',
      stacklevel=2)
  layer = Conv2D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair49>
<clonepair50>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="711" endline="860" pcid="3278"></source>
def conv3d(inputs,
           filters,
           kernel_size,
           strides=(1, 1, 1),
           padding='valid',
           data_format='channels_last',
           dilation_rate=(1, 1, 1),
           activation=None,
           use_bias=True,
           kernel_initializer=None,
           bias_initializer=tf.compat.v1.zeros_initializer(),
           kernel_regularizer=None,
           bias_regularizer=None,
           activity_regularizer=None,
           kernel_constraint=None,
           bias_constraint=None,
           trainable=True,
           name=None,
           reuse=None):
  """Functional interface for the 3D convolution layer.

  This layer creates a convolution kernel that is convolved
  (actually cross-correlated) with the layer input to produce a tensor of
  outputs. If `use_bias` is True (and a `bias_initializer` is provided),
  a bias vector is created and added to the outputs. Finally, if
  `activation` is not `None`, it is applied to the outputs as well.

  Args:
    inputs: Tensor input.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: An integer or tuple/list of 3 integers, specifying the
      depth, height and width of the 3D convolution window.
      Can be a single integer to specify the same value for
      all spatial dimensions.
    strides: An integer or tuple/list of 3 integers,
      specifying the strides of the convolution along the depth,
      height and width.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Specifying any stride value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, depth, height, width, channels)` while `channels_first`
      corresponds to inputs with shape
      `(batch, channels, depth, height, width)`.
    dilation_rate: An integer or tuple/list of 3 integers, specifying
      the dilation rate to use for dilated convolution.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is `tf.keras.layers.Conv3D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv3d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv3D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv3d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv3D` instead.',
      stacklevel=2)
  layer = Conv3D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair50>

<clonepair50>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1858" endline="1991" pcid="3286"></source>
def conv3d_transpose(inputs,
                     filters,
                     kernel_size,
                     strides=(1, 1, 1),
                     padding='valid',
                     data_format='channels_last',
                     activation=None,
                     use_bias=True,
                     kernel_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     kernel_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     kernel_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for transposed 3D convolution layer.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A tuple or list of 3 positive integers specifying the spatial
      dimensions of the filters. Can be a single integer to specify the same
      value for all spatial dimensions.
    strides: A tuple or list of 3 positive integers specifying the strides
      of the convolution. Can be a single integer to specify the same value for
      all spatial dimensions.
    padding: one of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, depth, height, width, channels)` while `channels_first`
      corresponds to inputs with shape
      `(batch, channels, depth, height, width)`.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.Conv3DTranspose`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv3d_transpose(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv3DTranspose(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv3d_transpose` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv3DTranspose` instead.',
      stacklevel=2)
  layer = Conv3DTranspose(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


# Aliases

</clonepair50>
<clonepair51>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="711" endline="860" pcid="3278"></source>
def conv3d(inputs,
           filters,
           kernel_size,
           strides=(1, 1, 1),
           padding='valid',
           data_format='channels_last',
           dilation_rate=(1, 1, 1),
           activation=None,
           use_bias=True,
           kernel_initializer=None,
           bias_initializer=tf.compat.v1.zeros_initializer(),
           kernel_regularizer=None,
           bias_regularizer=None,
           activity_regularizer=None,
           kernel_constraint=None,
           bias_constraint=None,
           trainable=True,
           name=None,
           reuse=None):
  """Functional interface for the 3D convolution layer.

  This layer creates a convolution kernel that is convolved
  (actually cross-correlated) with the layer input to produce a tensor of
  outputs. If `use_bias` is True (and a `bias_initializer` is provided),
  a bias vector is created and added to the outputs. Finally, if
  `activation` is not `None`, it is applied to the outputs as well.

  Args:
    inputs: Tensor input.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: An integer or tuple/list of 3 integers, specifying the
      depth, height and width of the 3D convolution window.
      Can be a single integer to specify the same value for
      all spatial dimensions.
    strides: An integer or tuple/list of 3 integers,
      specifying the strides of the convolution along the depth,
      height and width.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Specifying any stride value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, depth, height, width, channels)` while `channels_first`
      corresponds to inputs with shape
      `(batch, channels, depth, height, width)`.
    dilation_rate: An integer or tuple/list of 3 integers, specifying
      the dilation rate to use for dilated convolution.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is `tf.keras.layers.Conv3D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv3d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv3D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv3d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv3D` instead.',
      stacklevel=2)
  layer = Conv3D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair51>

<clonepair51>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1601" endline="1738" pcid="3284"></source>
def conv2d_transpose(inputs,
                     filters,
                     kernel_size,
                     strides=(1, 1),
                     padding='valid',
                     data_format='channels_last',
                     activation=None,
                     use_bias=True,
                     kernel_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     kernel_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     kernel_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for transposed 2D convolution layer.

  The need for transposed convolutions generally arises
  from the desire to use a transformation going in the opposite direction
  of a normal convolution, i.e., from something that has the shape of the
  output of some convolution to something that has the shape of its input
  while maintaining a connectivity pattern that is compatible with
  said convolution.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A tuple or list of 2 positive integers specifying the spatial
      dimensions of the filters. Can be a single integer to specify the same
      value for all spatial dimensions.
    strides: A tuple or list of 2 positive integers specifying the strides
      of the convolution. Can be a single integer to specify the same value for
      all spatial dimensions.
    padding: one of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, height, width)`.
    activation: Activation function. Set it to `None` to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If `None`, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.Conv2DTranspose`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv2d_transpose(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv2DTranspose(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv2d_transpose` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv2DTranspose` instead.',
      stacklevel=2)
  layer = Conv2DTranspose(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair51>
<clonepair52>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="711" endline="860" pcid="3278"></source>
def conv3d(inputs,
           filters,
           kernel_size,
           strides=(1, 1, 1),
           padding='valid',
           data_format='channels_last',
           dilation_rate=(1, 1, 1),
           activation=None,
           use_bias=True,
           kernel_initializer=None,
           bias_initializer=tf.compat.v1.zeros_initializer(),
           kernel_regularizer=None,
           bias_regularizer=None,
           activity_regularizer=None,
           kernel_constraint=None,
           bias_constraint=None,
           trainable=True,
           name=None,
           reuse=None):
  """Functional interface for the 3D convolution layer.

  This layer creates a convolution kernel that is convolved
  (actually cross-correlated) with the layer input to produce a tensor of
  outputs. If `use_bias` is True (and a `bias_initializer` is provided),
  a bias vector is created and added to the outputs. Finally, if
  `activation` is not `None`, it is applied to the outputs as well.

  Args:
    inputs: Tensor input.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: An integer or tuple/list of 3 integers, specifying the
      depth, height and width of the 3D convolution window.
      Can be a single integer to specify the same value for
      all spatial dimensions.
    strides: An integer or tuple/list of 3 integers,
      specifying the strides of the convolution along the depth,
      height and width.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Specifying any stride value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, depth, height, width, channels)` while `channels_first`
      corresponds to inputs with shape
      `(batch, channels, depth, height, width)`.
    dilation_rate: An integer or tuple/list of 3 integers, specifying
      the dilation rate to use for dilated convolution.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is `tf.keras.layers.Conv3D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv3d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv3D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv3d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv3D` instead.',
      stacklevel=2)
  layer = Conv3D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair52>

<clonepair52>
<source file="systems/keras-2.8.0/keras/layers/recurrent.py" startline="2587" endline="2630" pcid="4366"></source>
  def __init__(self,
               units,
               activation='tanh',
               recurrent_activation='hard_sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               unit_forget_bias=True,
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               **kwargs):
    warnings.warn(
        '`tf.keras.experimental.PeepholeLSTMCell` is deprecated '
        'and will be removed in a future version. '
        'Please use tensorflow_addons.rnn.PeepholeLSTMCell '
        'instead.',
        stacklevel=2)
    super(PeepholeLSTMCell, self).__init__(
        units=units,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        unit_forget_bias=unit_forget_bias,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        implementation=kwargs.pop('implementation', 1),
        **kwargs)

</clonepair52>
<clonepair53>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="671" endline="708" pcid="3277"></source>
  def __init__(self, filters,
               kernel_size,
               strides=(1, 1, 1),
               padding='valid',
               data_format='channels_last',
               dilation_rate=(1, 1, 1),
               activation=None,
               use_bias=True,
               kernel_initializer=None,
               bias_initializer=tf.compat.v1.zeros_initializer(),
               kernel_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               bias_constraint=None,
               trainable=True,
               name=None,
               **kwargs):
    super(Conv3D, self).__init__(
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        activation=activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        bias_initializer=bias_initializer,
        kernel_regularizer=kernel_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        kernel_constraint=kernel_constraint,
        bias_constraint=bias_constraint,
        trainable=trainable,
        name=name, **kwargs)


</clonepair53>

<clonepair53>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="711" endline="860" pcid="3278"></source>
def conv3d(inputs,
           filters,
           kernel_size,
           strides=(1, 1, 1),
           padding='valid',
           data_format='channels_last',
           dilation_rate=(1, 1, 1),
           activation=None,
           use_bias=True,
           kernel_initializer=None,
           bias_initializer=tf.compat.v1.zeros_initializer(),
           kernel_regularizer=None,
           bias_regularizer=None,
           activity_regularizer=None,
           kernel_constraint=None,
           bias_constraint=None,
           trainable=True,
           name=None,
           reuse=None):
  """Functional interface for the 3D convolution layer.

  This layer creates a convolution kernel that is convolved
  (actually cross-correlated) with the layer input to produce a tensor of
  outputs. If `use_bias` is True (and a `bias_initializer` is provided),
  a bias vector is created and added to the outputs. Finally, if
  `activation` is not `None`, it is applied to the outputs as well.

  Args:
    inputs: Tensor input.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: An integer or tuple/list of 3 integers, specifying the
      depth, height and width of the 3D convolution window.
      Can be a single integer to specify the same value for
      all spatial dimensions.
    strides: An integer or tuple/list of 3 integers,
      specifying the strides of the convolution along the depth,
      height and width.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Specifying any stride value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, depth, height, width, channels)` while `channels_first`
      corresponds to inputs with shape
      `(batch, channels, depth, height, width)`.
    dilation_rate: An integer or tuple/list of 3 integers, specifying
      the dilation rate to use for dilated convolution.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is `tf.keras.layers.Conv3D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv3d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv3D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv3d` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv3D` instead.',
      stacklevel=2)
  layer = Conv3D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair53>
<clonepair54>
<source file="systems/keras-2.8.0/keras/layers/recurrent.py" startline="1672" endline="1709" pcid="4330"></source>
  def get_config(self):
    config = {
        'units':
            self.units,
        'activation':
            activations.serialize(self.activation),
        'use_bias':
            self.use_bias,
        'kernel_initializer':
            initializers.serialize(self.kernel_initializer),
        'recurrent_initializer':
            initializers.serialize(self.recurrent_initializer),
        'bias_initializer':
            initializers.serialize(self.bias_initializer),
        'kernel_regularizer':
            regularizers.serialize(self.kernel_regularizer),
        'recurrent_regularizer':
            regularizers.serialize(self.recurrent_regularizer),
        'bias_regularizer':
            regularizers.serialize(self.bias_regularizer),
        'activity_regularizer':
            regularizers.serialize(self.activity_regularizer),
        'kernel_constraint':
            constraints.serialize(self.kernel_constraint),
        'recurrent_constraint':
            constraints.serialize(self.recurrent_constraint),
        'bias_constraint':
            constraints.serialize(self.bias_constraint),
        'dropout':
            self.dropout,
        'recurrent_dropout':
            self.recurrent_dropout
    }
    base_config = super(SimpleRNN, self).get_config()
    config.update(_config_for_enable_caching_device(self.cell))
    del base_config['cell']
    return dict(list(base_config.items()) + list(config.items()))

</clonepair54>

<clonepair54>
<source file="systems/keras-2.8.0/keras/layers/recurrent.py" startline="2226" endline="2269" pcid="4356"></source>
  def get_config(self):
    config = {
        'units':
            self.units,
        'activation':
            activations.serialize(self.activation),
        'recurrent_activation':
            activations.serialize(self.recurrent_activation),
        'use_bias':
            self.use_bias,
        'kernel_initializer':
            initializers.serialize(self.kernel_initializer),
        'recurrent_initializer':
            initializers.serialize(self.recurrent_initializer),
        'bias_initializer':
            initializers.serialize(self.bias_initializer),
        'kernel_regularizer':
            regularizers.serialize(self.kernel_regularizer),
        'recurrent_regularizer':
            regularizers.serialize(self.recurrent_regularizer),
        'bias_regularizer':
            regularizers.serialize(self.bias_regularizer),
        'activity_regularizer':
            regularizers.serialize(self.activity_regularizer),
        'kernel_constraint':
            constraints.serialize(self.kernel_constraint),
        'recurrent_constraint':
            constraints.serialize(self.recurrent_constraint),
        'bias_constraint':
            constraints.serialize(self.bias_constraint),
        'dropout':
            self.dropout,
        'recurrent_dropout':
            self.recurrent_dropout,
        'implementation':
            self.implementation,
        'reset_after':
            self.reset_after
    }
    config.update(_config_for_enable_caching_device(self.cell))
    base_config = super(GRU, self).get_config()
    del base_config['cell']
    return dict(list(base_config.items()) + list(config.items()))

</clonepair54>
<clonepair55>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1601" endline="1738" pcid="3284"></source>
def conv2d_transpose(inputs,
                     filters,
                     kernel_size,
                     strides=(1, 1),
                     padding='valid',
                     data_format='channels_last',
                     activation=None,
                     use_bias=True,
                     kernel_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     kernel_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     kernel_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for transposed 2D convolution layer.

  The need for transposed convolutions generally arises
  from the desire to use a transformation going in the opposite direction
  of a normal convolution, i.e., from something that has the shape of the
  output of some convolution to something that has the shape of its input
  while maintaining a connectivity pattern that is compatible with
  said convolution.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A tuple or list of 2 positive integers specifying the spatial
      dimensions of the filters. Can be a single integer to specify the same
      value for all spatial dimensions.
    strides: A tuple or list of 2 positive integers specifying the strides
      of the convolution. Can be a single integer to specify the same value for
      all spatial dimensions.
    padding: one of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, height, width)`.
    activation: Activation function. Set it to `None` to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If `None`, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.Conv2DTranspose`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv2d_transpose(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv2DTranspose(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv2d_transpose` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv2DTranspose` instead.',
      stacklevel=2)
  layer = Conv2DTranspose(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair55>

<clonepair55>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1858" endline="1991" pcid="3286"></source>
def conv3d_transpose(inputs,
                     filters,
                     kernel_size,
                     strides=(1, 1, 1),
                     padding='valid',
                     data_format='channels_last',
                     activation=None,
                     use_bias=True,
                     kernel_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     kernel_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     kernel_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for transposed 3D convolution layer.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A tuple or list of 3 positive integers specifying the spatial
      dimensions of the filters. Can be a single integer to specify the same
      value for all spatial dimensions.
    strides: A tuple or list of 3 positive integers specifying the strides
      of the convolution. Can be a single integer to specify the same value for
      all spatial dimensions.
    padding: one of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, depth, height, width, channels)` while `channels_first`
      corresponds to inputs with shape
      `(batch, channels, depth, height, width)`.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.Conv3DTranspose`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv3d_transpose(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv3DTranspose(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv3d_transpose` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv3DTranspose` instead.',
      stacklevel=2)
  layer = Conv3DTranspose(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


# Aliases

</clonepair55>
<clonepair56>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1858" endline="1991" pcid="3286"></source>
def conv3d_transpose(inputs,
                     filters,
                     kernel_size,
                     strides=(1, 1, 1),
                     padding='valid',
                     data_format='channels_last',
                     activation=None,
                     use_bias=True,
                     kernel_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     kernel_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     kernel_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for transposed 3D convolution layer.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A tuple or list of 3 positive integers specifying the spatial
      dimensions of the filters. Can be a single integer to specify the same
      value for all spatial dimensions.
    strides: A tuple or list of 3 positive integers specifying the strides
      of the convolution. Can be a single integer to specify the same value for
      all spatial dimensions.
    padding: one of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, depth, height, width, channels)` while `channels_first`
      corresponds to inputs with shape
      `(batch, channels, depth, height, width)`.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.Conv3DTranspose`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv3d_transpose(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv3DTranspose(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv3d_transpose` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv3DTranspose` instead.',
      stacklevel=2)
  layer = Conv3DTranspose(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


# Aliases

</clonepair56>

<clonepair56>
<source file="systems/keras-2.8.0/keras/layers/recurrent.py" startline="2587" endline="2630" pcid="4366"></source>
  def __init__(self,
               units,
               activation='tanh',
               recurrent_activation='hard_sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               unit_forget_bias=True,
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               **kwargs):
    warnings.warn(
        '`tf.keras.experimental.PeepholeLSTMCell` is deprecated '
        'and will be removed in a future version. '
        'Please use tensorflow_addons.rnn.PeepholeLSTMCell '
        'instead.',
        stacklevel=2)
    super(PeepholeLSTMCell, self).__init__(
        units=units,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        unit_forget_bias=unit_forget_bias,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        implementation=kwargs.pop('implementation', 1),
        **kwargs)

</clonepair56>
<clonepair57>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="115" endline="152" pcid="3273"></source>
  def __init__(self, filters,
               kernel_size,
               strides=1,
               padding='valid',
               data_format='channels_last',
               dilation_rate=1,
               activation=None,
               use_bias=True,
               kernel_initializer=None,
               bias_initializer=tf.compat.v1.zeros_initializer(),
               kernel_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               bias_constraint=None,
               trainable=True,
               name=None,
               **kwargs):
    super(Conv1D, self).__init__(
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        activation=activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        bias_initializer=bias_initializer,
        kernel_regularizer=kernel_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        kernel_constraint=kernel_constraint,
        bias_constraint=bias_constraint,
        trainable=trainable,
        name=name, **kwargs)


</clonepair57>

<clonepair57>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1858" endline="1991" pcid="3286"></source>
def conv3d_transpose(inputs,
                     filters,
                     kernel_size,
                     strides=(1, 1, 1),
                     padding='valid',
                     data_format='channels_last',
                     activation=None,
                     use_bias=True,
                     kernel_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     kernel_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     kernel_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for transposed 3D convolution layer.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A tuple or list of 3 positive integers specifying the spatial
      dimensions of the filters. Can be a single integer to specify the same
      value for all spatial dimensions.
    strides: A tuple or list of 3 positive integers specifying the strides
      of the convolution. Can be a single integer to specify the same value for
      all spatial dimensions.
    padding: one of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, depth, height, width, channels)` while `channels_first`
      corresponds to inputs with shape
      `(batch, channels, depth, height, width)`.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.Conv3DTranspose`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv3d_transpose(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv3DTranspose(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv3d_transpose` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv3DTranspose` instead.',
      stacklevel=2)
  layer = Conv3DTranspose(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


# Aliases

</clonepair57>
<clonepair58>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="389" endline="426" pcid="3275"></source>
  def __init__(self, filters,
               kernel_size,
               strides=(1, 1),
               padding='valid',
               data_format='channels_last',
               dilation_rate=(1, 1),
               activation=None,
               use_bias=True,
               kernel_initializer=None,
               bias_initializer=tf.compat.v1.zeros_initializer(),
               kernel_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               bias_constraint=None,
               trainable=True,
               name=None,
               **kwargs):
    super(Conv2D, self).__init__(
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        activation=activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        bias_initializer=bias_initializer,
        kernel_regularizer=kernel_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        kernel_constraint=kernel_constraint,
        bias_constraint=bias_constraint,
        trainable=trainable,
        name=name, **kwargs)


</clonepair58>

<clonepair58>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1858" endline="1991" pcid="3286"></source>
def conv3d_transpose(inputs,
                     filters,
                     kernel_size,
                     strides=(1, 1, 1),
                     padding='valid',
                     data_format='channels_last',
                     activation=None,
                     use_bias=True,
                     kernel_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     kernel_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     kernel_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for transposed 3D convolution layer.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A tuple or list of 3 positive integers specifying the spatial
      dimensions of the filters. Can be a single integer to specify the same
      value for all spatial dimensions.
    strides: A tuple or list of 3 positive integers specifying the strides
      of the convolution. Can be a single integer to specify the same value for
      all spatial dimensions.
    padding: one of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, depth, height, width, channels)` while `channels_first`
      corresponds to inputs with shape
      `(batch, channels, depth, height, width)`.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.Conv3DTranspose`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv3d_transpose(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv3DTranspose(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv3d_transpose` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv3DTranspose` instead.',
      stacklevel=2)
  layer = Conv3DTranspose(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


# Aliases

</clonepair58>
<clonepair59>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="671" endline="708" pcid="3277"></source>
  def __init__(self, filters,
               kernel_size,
               strides=(1, 1, 1),
               padding='valid',
               data_format='channels_last',
               dilation_rate=(1, 1, 1),
               activation=None,
               use_bias=True,
               kernel_initializer=None,
               bias_initializer=tf.compat.v1.zeros_initializer(),
               kernel_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               bias_constraint=None,
               trainable=True,
               name=None,
               **kwargs):
    super(Conv3D, self).__init__(
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        activation=activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        bias_initializer=bias_initializer,
        kernel_regularizer=kernel_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        kernel_constraint=kernel_constraint,
        bias_constraint=bias_constraint,
        trainable=trainable,
        name=name, **kwargs)


</clonepair59>

<clonepair59>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1858" endline="1991" pcid="3286"></source>
def conv3d_transpose(inputs,
                     filters,
                     kernel_size,
                     strides=(1, 1, 1),
                     padding='valid',
                     data_format='channels_last',
                     activation=None,
                     use_bias=True,
                     kernel_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     kernel_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     kernel_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for transposed 3D convolution layer.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A tuple or list of 3 positive integers specifying the spatial
      dimensions of the filters. Can be a single integer to specify the same
      value for all spatial dimensions.
    strides: A tuple or list of 3 positive integers specifying the strides
      of the convolution. Can be a single integer to specify the same value for
      all spatial dimensions.
    padding: one of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, depth, height, width, channels)` while `channels_first`
      corresponds to inputs with shape
      `(batch, channels, depth, height, width)`.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.Conv3DTranspose`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv3d_transpose(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv3DTranspose(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv3d_transpose` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv3DTranspose` instead.',
      stacklevel=2)
  layer = Conv3DTranspose(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


# Aliases

</clonepair59>
<clonepair60>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1601" endline="1738" pcid="3284"></source>
def conv2d_transpose(inputs,
                     filters,
                     kernel_size,
                     strides=(1, 1),
                     padding='valid',
                     data_format='channels_last',
                     activation=None,
                     use_bias=True,
                     kernel_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     kernel_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     kernel_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for transposed 2D convolution layer.

  The need for transposed convolutions generally arises
  from the desire to use a transformation going in the opposite direction
  of a normal convolution, i.e., from something that has the shape of the
  output of some convolution to something that has the shape of its input
  while maintaining a connectivity pattern that is compatible with
  said convolution.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A tuple or list of 2 positive integers specifying the spatial
      dimensions of the filters. Can be a single integer to specify the same
      value for all spatial dimensions.
    strides: A tuple or list of 2 positive integers specifying the strides
      of the convolution. Can be a single integer to specify the same value for
      all spatial dimensions.
    padding: one of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, height, width)`.
    activation: Activation function. Set it to `None` to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If `None`, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.Conv2DTranspose`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv2d_transpose(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv2DTranspose(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv2d_transpose` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv2DTranspose` instead.',
      stacklevel=2)
  layer = Conv2DTranspose(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair60>

<clonepair60>
<source file="systems/keras-2.8.0/keras/layers/recurrent.py" startline="2587" endline="2630" pcid="4366"></source>
  def __init__(self,
               units,
               activation='tanh',
               recurrent_activation='hard_sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               unit_forget_bias=True,
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               **kwargs):
    warnings.warn(
        '`tf.keras.experimental.PeepholeLSTMCell` is deprecated '
        'and will be removed in a future version. '
        'Please use tensorflow_addons.rnn.PeepholeLSTMCell '
        'instead.',
        stacklevel=2)
    super(PeepholeLSTMCell, self).__init__(
        units=units,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        unit_forget_bias=unit_forget_bias,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        implementation=kwargs.pop('implementation', 1),
        **kwargs)

</clonepair60>
<clonepair61>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="115" endline="152" pcid="3273"></source>
  def __init__(self, filters,
               kernel_size,
               strides=1,
               padding='valid',
               data_format='channels_last',
               dilation_rate=1,
               activation=None,
               use_bias=True,
               kernel_initializer=None,
               bias_initializer=tf.compat.v1.zeros_initializer(),
               kernel_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               bias_constraint=None,
               trainable=True,
               name=None,
               **kwargs):
    super(Conv1D, self).__init__(
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        activation=activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        bias_initializer=bias_initializer,
        kernel_regularizer=kernel_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        kernel_constraint=kernel_constraint,
        bias_constraint=bias_constraint,
        trainable=trainable,
        name=name, **kwargs)


</clonepair61>

<clonepair61>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1601" endline="1738" pcid="3284"></source>
def conv2d_transpose(inputs,
                     filters,
                     kernel_size,
                     strides=(1, 1),
                     padding='valid',
                     data_format='channels_last',
                     activation=None,
                     use_bias=True,
                     kernel_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     kernel_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     kernel_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for transposed 2D convolution layer.

  The need for transposed convolutions generally arises
  from the desire to use a transformation going in the opposite direction
  of a normal convolution, i.e., from something that has the shape of the
  output of some convolution to something that has the shape of its input
  while maintaining a connectivity pattern that is compatible with
  said convolution.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A tuple or list of 2 positive integers specifying the spatial
      dimensions of the filters. Can be a single integer to specify the same
      value for all spatial dimensions.
    strides: A tuple or list of 2 positive integers specifying the strides
      of the convolution. Can be a single integer to specify the same value for
      all spatial dimensions.
    padding: one of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, height, width)`.
    activation: Activation function. Set it to `None` to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If `None`, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.Conv2DTranspose`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv2d_transpose(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv2DTranspose(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv2d_transpose` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv2DTranspose` instead.',
      stacklevel=2)
  layer = Conv2DTranspose(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair61>
<clonepair62>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="389" endline="426" pcid="3275"></source>
  def __init__(self, filters,
               kernel_size,
               strides=(1, 1),
               padding='valid',
               data_format='channels_last',
               dilation_rate=(1, 1),
               activation=None,
               use_bias=True,
               kernel_initializer=None,
               bias_initializer=tf.compat.v1.zeros_initializer(),
               kernel_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               bias_constraint=None,
               trainable=True,
               name=None,
               **kwargs):
    super(Conv2D, self).__init__(
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        activation=activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        bias_initializer=bias_initializer,
        kernel_regularizer=kernel_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        kernel_constraint=kernel_constraint,
        bias_constraint=bias_constraint,
        trainable=trainable,
        name=name, **kwargs)


</clonepair62>

<clonepair62>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1601" endline="1738" pcid="3284"></source>
def conv2d_transpose(inputs,
                     filters,
                     kernel_size,
                     strides=(1, 1),
                     padding='valid',
                     data_format='channels_last',
                     activation=None,
                     use_bias=True,
                     kernel_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     kernel_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     kernel_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for transposed 2D convolution layer.

  The need for transposed convolutions generally arises
  from the desire to use a transformation going in the opposite direction
  of a normal convolution, i.e., from something that has the shape of the
  output of some convolution to something that has the shape of its input
  while maintaining a connectivity pattern that is compatible with
  said convolution.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A tuple or list of 2 positive integers specifying the spatial
      dimensions of the filters. Can be a single integer to specify the same
      value for all spatial dimensions.
    strides: A tuple or list of 2 positive integers specifying the strides
      of the convolution. Can be a single integer to specify the same value for
      all spatial dimensions.
    padding: one of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, height, width)`.
    activation: Activation function. Set it to `None` to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If `None`, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.Conv2DTranspose`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv2d_transpose(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv2DTranspose(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv2d_transpose` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv2DTranspose` instead.',
      stacklevel=2)
  layer = Conv2DTranspose(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair62>
<clonepair63>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="671" endline="708" pcid="3277"></source>
  def __init__(self, filters,
               kernel_size,
               strides=(1, 1, 1),
               padding='valid',
               data_format='channels_last',
               dilation_rate=(1, 1, 1),
               activation=None,
               use_bias=True,
               kernel_initializer=None,
               bias_initializer=tf.compat.v1.zeros_initializer(),
               kernel_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               bias_constraint=None,
               trainable=True,
               name=None,
               **kwargs):
    super(Conv3D, self).__init__(
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        activation=activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        bias_initializer=bias_initializer,
        kernel_regularizer=kernel_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        kernel_constraint=kernel_constraint,
        bias_constraint=bias_constraint,
        trainable=trainable,
        name=name, **kwargs)


</clonepair63>

<clonepair63>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="1601" endline="1738" pcid="3284"></source>
def conv2d_transpose(inputs,
                     filters,
                     kernel_size,
                     strides=(1, 1),
                     padding='valid',
                     data_format='channels_last',
                     activation=None,
                     use_bias=True,
                     kernel_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     kernel_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     kernel_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for transposed 2D convolution layer.

  The need for transposed convolutions generally arises
  from the desire to use a transformation going in the opposite direction
  of a normal convolution, i.e., from something that has the shape of the
  output of some convolution to something that has the shape of its input
  while maintaining a connectivity pattern that is compatible with
  said convolution.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A tuple or list of 2 positive integers specifying the spatial
      dimensions of the filters. Can be a single integer to specify the same
      value for all spatial dimensions.
    strides: A tuple or list of 2 positive integers specifying the strides
      of the convolution. Can be a single integer to specify the same value for
      all spatial dimensions.
    padding: one of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, height, width)`.
    activation: Activation function. Set it to `None` to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If `None`, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.Conv2DTranspose`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv2d_transpose(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv2DTranspose(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.conv2d_transpose` is deprecated and '
      'will be removed in a future version. '
      'Please Use `tf.keras.layers.Conv2DTranspose` instead.',
      stacklevel=2)
  layer = Conv2DTranspose(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</clonepair63>
<clonepair64>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="2023" endline="2089" pcid="5598"></source>
  def test_persistence_file_vocab_keras_save_keras_load_tf_save_tf_load(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    # Build and validate a golden model.
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string,
        vocabulary=vocab_file)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(output_dataset, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model")
    model.save(output_path, save_format="tf")

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()
    tf.io.gfile.remove(vocab_file)

    loaded_model = keras.models.load_model(
        output_path, custom_objects={"IndexLookup": index_lookup.IndexLookup})

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_dataset = loaded_model.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

    # Try re-saving the layer. This simulates saving a layer contained at
    # a hub Module.
    input_data_2 = keras.Input(shape=(None,), dtype=tf.string)
    output_2 = loaded_model(input_data_2)
    model_2 = keras.Model(inputs=input_data_2, outputs=output_2)
    new_output_dataset = model_2.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model_2")
    tf.saved_model.save(model_2, output_path)

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()

    loaded_model = tf.saved_model.load(output_path)
    f = loaded_model.signatures["serving_default"]

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_dataset = f(tf.constant(input_array))["model"]
    self.assertAllEqual(new_output_dataset, expected_output)

</clonepair64>

<clonepair64>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="2090" endline="2157" pcid="5599"></source>
  def test_persistence_file_vocab_keras_save_keras_load_keras_save_keras_load(
      self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    # Build and validate a golden model.
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string,
        vocabulary=vocab_file)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(output_dataset, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model")
    model.save(output_path, save_format="tf")

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()
    tf.io.gfile.remove(vocab_file)

    loaded_model = keras.models.load_model(
        output_path, custom_objects={"IndexLookup": index_lookup.IndexLookup})

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_dataset = loaded_model.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

    # Try re-saving the layer. This simulates saving a layer contained at
    # a hub Module.
    input_data_2 = keras.Input(shape=(None,), dtype=tf.string)
    output_2 = loaded_model(input_data_2)
    model_2 = keras.Model(inputs=input_data_2, outputs=output_2)
    new_output_dataset = model_2.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model_2")
    model_2.save(output_path, save_format="tf")

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()

    loaded_model = keras.models.load_model(
        output_path, custom_objects={"IndexLookup": index_lookup.IndexLookup})

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_dataset = model_2.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

</clonepair64>
<clonepair65>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1956" endline="2022" pcid="5597"></source>
  def test_persistence_file_vocab_keras_save_keras_load(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    # Build and validate a golden model.
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string,
        vocabulary=vocab_file)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(output_dataset, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model")
    model.save(output_path, save_format="tf")

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()
    tf.io.gfile.remove(vocab_file)

    loaded_model = keras.models.load_model(
        output_path, custom_objects={"IndexLookup": index_lookup.IndexLookup})

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_dataset = loaded_model.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

    # Try re-saving the layer. This simulates saving a layer contained at
    # a hub Module.
    input_data_2 = keras.Input(shape=(None,), dtype=tf.string)
    output_2 = loaded_model(input_data_2)
    model_2 = keras.Model(inputs=input_data_2, outputs=output_2)
    new_output_dataset = model_2.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model_2")
    model_2.save(output_path, save_format="tf")

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()

    loaded_model = keras.models.load_model(
        output_path, custom_objects={"IndexLookup": index_lookup.IndexLookup})

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_dataset = loaded_model.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

</clonepair65>

<clonepair65>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="2090" endline="2157" pcid="5599"></source>
  def test_persistence_file_vocab_keras_save_keras_load_keras_save_keras_load(
      self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    # Build and validate a golden model.
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string,
        vocabulary=vocab_file)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(output_dataset, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model")
    model.save(output_path, save_format="tf")

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()
    tf.io.gfile.remove(vocab_file)

    loaded_model = keras.models.load_model(
        output_path, custom_objects={"IndexLookup": index_lookup.IndexLookup})

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_dataset = loaded_model.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

    # Try re-saving the layer. This simulates saving a layer contained at
    # a hub Module.
    input_data_2 = keras.Input(shape=(None,), dtype=tf.string)
    output_2 = loaded_model(input_data_2)
    model_2 = keras.Model(inputs=input_data_2, outputs=output_2)
    new_output_dataset = model_2.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model_2")
    model_2.save(output_path, save_format="tf")

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()

    loaded_model = keras.models.load_model(
        output_path, custom_objects={"IndexLookup": index_lookup.IndexLookup})

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_dataset = model_2.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

</clonepair65>
<clonepair66>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1956" endline="2022" pcid="5597"></source>
  def test_persistence_file_vocab_keras_save_keras_load(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    # Build and validate a golden model.
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string,
        vocabulary=vocab_file)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(output_dataset, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model")
    model.save(output_path, save_format="tf")

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()
    tf.io.gfile.remove(vocab_file)

    loaded_model = keras.models.load_model(
        output_path, custom_objects={"IndexLookup": index_lookup.IndexLookup})

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_dataset = loaded_model.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

    # Try re-saving the layer. This simulates saving a layer contained at
    # a hub Module.
    input_data_2 = keras.Input(shape=(None,), dtype=tf.string)
    output_2 = loaded_model(input_data_2)
    model_2 = keras.Model(inputs=input_data_2, outputs=output_2)
    new_output_dataset = model_2.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model_2")
    model_2.save(output_path, save_format="tf")

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()

    loaded_model = keras.models.load_model(
        output_path, custom_objects={"IndexLookup": index_lookup.IndexLookup})

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_dataset = loaded_model.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

</clonepair66>

<clonepair66>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="2023" endline="2089" pcid="5598"></source>
  def test_persistence_file_vocab_keras_save_keras_load_tf_save_tf_load(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    # Build and validate a golden model.
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string,
        vocabulary=vocab_file)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(output_dataset, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model")
    model.save(output_path, save_format="tf")

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()
    tf.io.gfile.remove(vocab_file)

    loaded_model = keras.models.load_model(
        output_path, custom_objects={"IndexLookup": index_lookup.IndexLookup})

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_dataset = loaded_model.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

    # Try re-saving the layer. This simulates saving a layer contained at
    # a hub Module.
    input_data_2 = keras.Input(shape=(None,), dtype=tf.string)
    output_2 = loaded_model(input_data_2)
    model_2 = keras.Model(inputs=input_data_2, outputs=output_2)
    new_output_dataset = model_2.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model_2")
    tf.saved_model.save(model_2, output_path)

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()

    loaded_model = tf.saved_model.load(output_path)
    f = loaded_model.signatures["serving_default"]

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_dataset = f(tf.constant(input_array))["model"]
    self.assertAllEqual(new_output_dataset, expected_output)

</clonepair66>
<clonepair67>
<source file="systems/keras-2.8.0/keras/layers/recurrent_v2.py" startline="167" endline="205" pcid="4830"></source>
  def __init__(self,
               units,
               activation='tanh',
               recurrent_activation='sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               reset_after=True,
               **kwargs):
    super(GRUCell, self).__init__(
        units,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        implementation=kwargs.pop('implementation', 2),
        reset_after=reset_after,
        **kwargs)


</clonepair67>

<clonepair67>
<source file="systems/keras-2.8.0/keras/layers/recurrent.py" startline="2587" endline="2630" pcid="4366"></source>
  def __init__(self,
               units,
               activation='tanh',
               recurrent_activation='hard_sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               unit_forget_bias=True,
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               **kwargs):
    warnings.warn(
        '`tf.keras.experimental.PeepholeLSTMCell` is deprecated '
        'and will be removed in a future version. '
        'Please use tensorflow_addons.rnn.PeepholeLSTMCell '
        'instead.',
        stacklevel=2)
    super(PeepholeLSTMCell, self).__init__(
        units=units,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        unit_forget_bias=unit_forget_bias,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        implementation=kwargs.pop('implementation', 1),
        **kwargs)

</clonepair67>
<clonepair68>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="115" endline="152" pcid="3273"></source>
  def __init__(self, filters,
               kernel_size,
               strides=1,
               padding='valid',
               data_format='channels_last',
               dilation_rate=1,
               activation=None,
               use_bias=True,
               kernel_initializer=None,
               bias_initializer=tf.compat.v1.zeros_initializer(),
               kernel_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               bias_constraint=None,
               trainable=True,
               name=None,
               **kwargs):
    super(Conv1D, self).__init__(
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        activation=activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        bias_initializer=bias_initializer,
        kernel_regularizer=kernel_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        kernel_constraint=kernel_constraint,
        bias_constraint=bias_constraint,
        trainable=trainable,
        name=name, **kwargs)


</clonepair68>

<clonepair68>
<source file="systems/keras-2.8.0/keras/layers/recurrent.py" startline="2587" endline="2630" pcid="4366"></source>
  def __init__(self,
               units,
               activation='tanh',
               recurrent_activation='hard_sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               unit_forget_bias=True,
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               **kwargs):
    warnings.warn(
        '`tf.keras.experimental.PeepholeLSTMCell` is deprecated '
        'and will be removed in a future version. '
        'Please use tensorflow_addons.rnn.PeepholeLSTMCell '
        'instead.',
        stacklevel=2)
    super(PeepholeLSTMCell, self).__init__(
        units=units,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        unit_forget_bias=unit_forget_bias,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        implementation=kwargs.pop('implementation', 1),
        **kwargs)

</clonepair68>
<clonepair69>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="389" endline="426" pcid="3275"></source>
  def __init__(self, filters,
               kernel_size,
               strides=(1, 1),
               padding='valid',
               data_format='channels_last',
               dilation_rate=(1, 1),
               activation=None,
               use_bias=True,
               kernel_initializer=None,
               bias_initializer=tf.compat.v1.zeros_initializer(),
               kernel_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               bias_constraint=None,
               trainable=True,
               name=None,
               **kwargs):
    super(Conv2D, self).__init__(
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        activation=activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        bias_initializer=bias_initializer,
        kernel_regularizer=kernel_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        kernel_constraint=kernel_constraint,
        bias_constraint=bias_constraint,
        trainable=trainable,
        name=name, **kwargs)


</clonepair69>

<clonepair69>
<source file="systems/keras-2.8.0/keras/layers/recurrent.py" startline="2587" endline="2630" pcid="4366"></source>
  def __init__(self,
               units,
               activation='tanh',
               recurrent_activation='hard_sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               unit_forget_bias=True,
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               **kwargs):
    warnings.warn(
        '`tf.keras.experimental.PeepholeLSTMCell` is deprecated '
        'and will be removed in a future version. '
        'Please use tensorflow_addons.rnn.PeepholeLSTMCell '
        'instead.',
        stacklevel=2)
    super(PeepholeLSTMCell, self).__init__(
        units=units,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        unit_forget_bias=unit_forget_bias,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        implementation=kwargs.pop('implementation', 1),
        **kwargs)

</clonepair69>
<clonepair70>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="671" endline="708" pcid="3277"></source>
  def __init__(self, filters,
               kernel_size,
               strides=(1, 1, 1),
               padding='valid',
               data_format='channels_last',
               dilation_rate=(1, 1, 1),
               activation=None,
               use_bias=True,
               kernel_initializer=None,
               bias_initializer=tf.compat.v1.zeros_initializer(),
               kernel_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               bias_constraint=None,
               trainable=True,
               name=None,
               **kwargs):
    super(Conv3D, self).__init__(
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        activation=activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        bias_initializer=bias_initializer,
        kernel_regularizer=kernel_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        kernel_constraint=kernel_constraint,
        bias_constraint=bias_constraint,
        trainable=trainable,
        name=name, **kwargs)


</clonepair70>

<clonepair70>
<source file="systems/keras-2.8.0/keras/layers/recurrent.py" startline="2587" endline="2630" pcid="4366"></source>
  def __init__(self,
               units,
               activation='tanh',
               recurrent_activation='hard_sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               unit_forget_bias=True,
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               **kwargs):
    warnings.warn(
        '`tf.keras.experimental.PeepholeLSTMCell` is deprecated '
        'and will be removed in a future version. '
        'Please use tensorflow_addons.rnn.PeepholeLSTMCell '
        'instead.',
        stacklevel=2)
    super(PeepholeLSTMCell, self).__init__(
        units=units,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        unit_forget_bias=unit_forget_bias,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        implementation=kwargs.pop('implementation', 1),
        **kwargs)

</clonepair70>
<clonepair71>
<source file="systems/keras-2.8.0/keras/layers/local.py" startline="116" endline="155" pcid="3972"></source>
  def __init__(self,
               filters,
               kernel_size,
               strides=1,
               padding='valid',
               data_format=None,
               activation=None,
               use_bias=True,
               kernel_initializer='glorot_uniform',
               bias_initializer='zeros',
               kernel_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               bias_constraint=None,
               implementation=1,
               **kwargs):
    super(LocallyConnected1D, self).__init__(**kwargs)
    self.filters = filters
    self.kernel_size = conv_utils.normalize_tuple(kernel_size, 1, 'kernel_size')
    self.strides = conv_utils.normalize_tuple(
        strides, 1, 'strides', allow_zero=True)
    self.padding = conv_utils.normalize_padding(padding)
    if self.padding != 'valid' and implementation == 1:
      raise ValueError('Invalid border mode for LocallyConnected1D '
                       '(only "valid" is supported if implementation is 1): ' +
                       padding)
    self.data_format = conv_utils.normalize_data_format(data_format)
    self.activation = activations.get(activation)
    self.use_bias = use_bias
    self.kernel_initializer = initializers.get(kernel_initializer)
    self.bias_initializer = initializers.get(bias_initializer)
    self.kernel_regularizer = regularizers.get(kernel_regularizer)
    self.bias_regularizer = regularizers.get(bias_regularizer)
    self.activity_regularizer = regularizers.get(activity_regularizer)
    self.kernel_constraint = constraints.get(kernel_constraint)
    self.bias_constraint = constraints.get(bias_constraint)
    self.implementation = implementation
    self.input_spec = InputSpec(ndim=3)

</clonepair71>

<clonepair71>
<source file="systems/keras-2.8.0/keras/layers/local.py" startline="433" endline="472" pcid="3978"></source>
  def __init__(self,
               filters,
               kernel_size,
               strides=(1, 1),
               padding='valid',
               data_format=None,
               activation=None,
               use_bias=True,
               kernel_initializer='glorot_uniform',
               bias_initializer='zeros',
               kernel_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               bias_constraint=None,
               implementation=1,
               **kwargs):
    super(LocallyConnected2D, self).__init__(**kwargs)
    self.filters = filters
    self.kernel_size = conv_utils.normalize_tuple(kernel_size, 2, 'kernel_size')
    self.strides = conv_utils.normalize_tuple(
        strides, 2, 'strides', allow_zero=True)
    self.padding = conv_utils.normalize_padding(padding)
    if self.padding != 'valid' and implementation == 1:
      raise ValueError('Invalid border mode for LocallyConnected2D '
                       '(only "valid" is supported if implementation is 1): ' +
                       padding)
    self.data_format = conv_utils.normalize_data_format(data_format)
    self.activation = activations.get(activation)
    self.use_bias = use_bias
    self.kernel_initializer = initializers.get(kernel_initializer)
    self.bias_initializer = initializers.get(bias_initializer)
    self.kernel_regularizer = regularizers.get(kernel_regularizer)
    self.bias_regularizer = regularizers.get(bias_regularizer)
    self.activity_regularizer = regularizers.get(activity_regularizer)
    self.kernel_constraint = constraints.get(kernel_constraint)
    self.bias_constraint = constraints.get(bias_constraint)
    self.implementation = implementation
    self.input_spec = InputSpec(ndim=4)

</clonepair71>
<clonepair72>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/convolutional.py" startline="389" endline="426" pcid="3275"></source>
  def __init__(self, filters,
               kernel_size,
               strides=(1, 1),
               padding='valid',
               data_format='channels_last',
               dilation_rate=(1, 1),
               activation=None,
               use_bias=True,
               kernel_initializer=None,
               bias_initializer=tf.compat.v1.zeros_initializer(),
               kernel_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               bias_constraint=None,
               trainable=True,
               name=None,
               **kwargs):
    super(Conv2D, self).__init__(
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        activation=activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        bias_initializer=bias_initializer,
        kernel_regularizer=kernel_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        kernel_constraint=kernel_constraint,
        bias_constraint=bias_constraint,
        trainable=trainable,
        name=name, **kwargs)


</clonepair72>

<clonepair72>
<source file="systems/keras-2.8.0/keras/layers/recurrent_v2.py" startline="167" endline="205" pcid="4830"></source>
  def __init__(self,
               units,
               activation='tanh',
               recurrent_activation='sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               reset_after=True,
               **kwargs):
    super(GRUCell, self).__init__(
        units,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        implementation=kwargs.pop('implementation', 2),
        reset_after=reset_after,
        **kwargs)


</clonepair72>
<clonepair73>
<source file="systems/keras-2.8.0/keras/layers/local.py" startline="625" endline="661" pcid="3983"></source>
  def get_config(self):
    config = {
        'filters':
            self.filters,
        'kernel_size':
            self.kernel_size,
        'strides':
            self.strides,
        'padding':
            self.padding,
        'data_format':
            self.data_format,
        'activation':
            activations.serialize(self.activation),
        'use_bias':
            self.use_bias,
        'kernel_initializer':
            initializers.serialize(self.kernel_initializer),
        'bias_initializer':
            initializers.serialize(self.bias_initializer),
        'kernel_regularizer':
            regularizers.serialize(self.kernel_regularizer),
        'bias_regularizer':
            regularizers.serialize(self.bias_regularizer),
        'activity_regularizer':
            regularizers.serialize(self.activity_regularizer),
        'kernel_constraint':
            constraints.serialize(self.kernel_constraint),
        'bias_constraint':
            constraints.serialize(self.bias_constraint),
        'implementation':
            self.implementation
    }
    base_config = super(LocallyConnected2D, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))


</clonepair73>

<clonepair73>
<source file="systems/keras-2.8.0/keras/layers/recurrent.py" startline="1672" endline="1709" pcid="4330"></source>
  def get_config(self):
    config = {
        'units':
            self.units,
        'activation':
            activations.serialize(self.activation),
        'use_bias':
            self.use_bias,
        'kernel_initializer':
            initializers.serialize(self.kernel_initializer),
        'recurrent_initializer':
            initializers.serialize(self.recurrent_initializer),
        'bias_initializer':
            initializers.serialize(self.bias_initializer),
        'kernel_regularizer':
            regularizers.serialize(self.kernel_regularizer),
        'recurrent_regularizer':
            regularizers.serialize(self.recurrent_regularizer),
        'bias_regularizer':
            regularizers.serialize(self.bias_regularizer),
        'activity_regularizer':
            regularizers.serialize(self.activity_regularizer),
        'kernel_constraint':
            constraints.serialize(self.kernel_constraint),
        'recurrent_constraint':
            constraints.serialize(self.recurrent_constraint),
        'bias_constraint':
            constraints.serialize(self.bias_constraint),
        'dropout':
            self.dropout,
        'recurrent_dropout':
            self.recurrent_dropout
    }
    base_config = super(SimpleRNN, self).get_config()
    config.update(_config_for_enable_caching_device(self.cell))
    del base_config['cell']
    return dict(list(base_config.items()) + list(config.items()))

</clonepair73>
<clonepair74>
<source file="systems/keras-2.8.0/keras/layers/local_test.py" startline="562" endline="598" pcid="4256"></source>
def get_model(implementation,
              filters,
              kernel_size,
              strides,
              layers,
              num_classes,
              data_format):
  model = keras.Sequential()

  if len(kernel_size) == 1:
    lc_layer = keras.layers.LocallyConnected1D
  elif len(kernel_size) == 2:
    lc_layer = keras.layers.LocallyConnected2D
  else:
    raise NotImplementedError(kernel_size)

  for _ in range(layers):
    model.add(lc_layer(
        padding='valid',
        kernel_initializer=keras.initializers.random_normal(),
        bias_initializer=keras.initializers.random_normal(),
        filters=filters,
        strides=strides,
        kernel_size=kernel_size,
        activation=keras.activations.relu,
        data_format=data_format,
        implementation=implementation))

  model.add(keras.layers.Flatten())
  model.add(keras.layers.Dense(num_classes))
  model.compile(
      optimizer=RMSPropOptimizer(0.01),
      metrics=[keras.metrics.categorical_accuracy],
      loss=keras.losses.CategoricalCrossentropy(from_logits=True))
  return model


</clonepair74>

<clonepair74>
<source file="systems/keras-2.8.0/keras/layers/local_test.py" startline="599" endline="631" pcid="4257"></source>
def get_model_saveable(implementation, filters, kernel_size, strides, layers,
                       num_classes, data_format):
  model = keras.Sequential()

  if len(kernel_size) == 1:
    lc_layer = keras.layers.LocallyConnected1D
  elif len(kernel_size) == 2:
    lc_layer = keras.layers.LocallyConnected2D
  else:
    raise NotImplementedError(kernel_size)

  for _ in range(layers):
    model.add(
        lc_layer(
            padding='valid',
            kernel_initializer=keras.initializers.random_normal(),
            bias_initializer=keras.initializers.random_normal(),
            filters=filters,
            strides=strides,
            kernel_size=kernel_size,
            activation=keras.activations.relu,
            data_format=data_format,
            implementation=implementation))

  model.add(keras.layers.Flatten())
  model.add(keras.layers.Dense(num_classes))
  model.compile(
      optimizer=rmsprop.RMSProp(learning_rate=0.01),
      metrics=[keras.metrics.categorical_accuracy],
      loss=keras.losses.CategoricalCrossentropy(from_logits=True))
  return model


</clonepair74>
<clonepair75>
<source file="systems/keras-2.8.0/keras/feature_column/dense_features_test.py" startline="49" endline="90" pcid="3069"></source>
  def test_reuses_variables(self):
    sparse_input = tf.SparseTensor(
        indices=((0, 0), (1, 0), (2, 0)), values=(0, 1, 2), dense_shape=(3, 3))

    # Create feature columns (categorical and embedding).
    categorical_column = tf.feature_column.categorical_column_with_identity(
        key='a', num_buckets=3)
    embedding_dimension = 2

    def _embedding_column_initializer(shape, dtype, partition_info=None):
      del shape  # unused
      del dtype  # unused
      del partition_info  # unused
      embedding_values = (
          (1, 0),  # id 0
          (0, 1),  # id 1
          (1, 1))  # id 2
      return embedding_values

    embedding_column = tf.feature_column.embedding_column(
        categorical_column,
        dimension=embedding_dimension,
        initializer=_embedding_column_initializer)

    dense_features = df.DenseFeatures([embedding_column])
    features = {'a': sparse_input}

    inputs = dense_features(features)
    variables = dense_features.variables

    # Sanity check: test that the inputs are correct.
    self.assertAllEqual([[1, 0], [0, 1], [1, 1]], inputs)

    # Check that only one variable was created.
    self.assertEqual(1, len(variables))

    # Check that invoking dense_features on the same features does not create
    # additional variables
    _ = dense_features(features)
    self.assertEqual(1, len(variables))
    self.assertIs(variables[0], dense_features.variables[0])

</clonepair75>

<clonepair75>
<source file="systems/keras-2.8.0/keras/feature_column/dense_features_test.py" startline="143" endline="185" pcid="3073"></source>
  def test_feature_column_dense_features_gradient(self):
    sparse_input = tf.SparseTensor(
        indices=((0, 0), (1, 0), (2, 0)), values=(0, 1, 2), dense_shape=(3, 3))

    # Create feature columns (categorical and embedding).
    categorical_column = tf.feature_column.categorical_column_with_identity(
        key='a', num_buckets=3)
    embedding_dimension = 2

    def _embedding_column_initializer(shape, dtype, partition_info=None):
      del shape  # unused
      del dtype  # unused
      del partition_info  # unused
      embedding_values = (
          (1, 0),  # id 0
          (0, 1),  # id 1
          (1, 1))  # id 2
      return embedding_values

    embedding_column = tf.feature_column.embedding_column(
        categorical_column,
        dimension=embedding_dimension,
        initializer=_embedding_column_initializer)

    dense_features = df.DenseFeatures([embedding_column])
    features = {'a': sparse_input}

    def scale_matrix():
      matrix = dense_features(features)
      return 2 * matrix

    # Sanity check: Verify that scale_matrix returns the correct output.
    self.assertAllEqual([[2, 0], [0, 2], [2, 2]], scale_matrix())

    # Check that the returned gradient is correct.
    grad_function = backprop.implicit_grad(scale_matrix)
    grads_and_vars = grad_function()
    indexed_slice = grads_and_vars[0][0]
    gradient = grads_and_vars[0][0].values

    self.assertAllEqual([0, 1, 2], indexed_slice.indices)
    self.assertAllEqual([[2, 2], [2, 2], [2, 2]], gradient)

</clonepair75>
<clonepair76>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/integer_lookup_test.py" startline="76" endline="111" pcid="4970"></source>
  def test_layer_end_to_end_with_adapt(self, vocab_data, input_data, kwargs,
                                       use_dataset, expected_output,
                                       input_dtype):
    cls = integer_lookup.IntegerLookup
    expected_output_dtype = tf.int64
    input_shape = input_data.shape

    if use_dataset:
      # Keras APIs expect batched datasets.
      # TODO(rachelim): `model.predict` predicts the result on each
      # dataset batch separately, then tries to concatenate the results
      # together. When the results have different shapes on the non-concat
      # axis (which can happen in the output_mode = INT case for
      # IntegerLookup), the concatenation fails. In real use cases, this may
      # not be an issue because users are likely to pipe the preprocessing layer
      # into other keras layers instead of predicting it directly. A workaround
      # for these unit tests is to have the dataset only contain one batch, so
      # no concatenation needs to happen with the result. For consistency with
      # numpy input, we should make `predict` join differently shaped results
      # together sensibly, with 0 padding.
      input_data = tf.data.Dataset.from_tensor_slices(input_data).batch(
          input_shape[0])
      vocab_data = tf.data.Dataset.from_tensor_slices(vocab_data).batch(
          input_shape[0])

    output_data = testing_utils.layer_test(
        cls,
        kwargs=kwargs,
        input_shape=input_shape,
        input_data=input_data,
        input_dtype=input_dtype,
        expected_output_dtype=expected_output_dtype,
        validate_training=False,
        adapt_data=vocab_data)
    self.assertAllClose(expected_output, output_data)

</clonepair76>

<clonepair76>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="302" endline="348" pcid="5513"></source>
  def test_layer_end_to_end_with_adapt(self, vocab_data, input_data, kwargs,
                                       use_dataset, expected_output,
                                       input_dtype):
    cls = index_lookup.IndexLookup
    if "invert" in kwargs and kwargs["invert"]:
      expected_output_dtype = kwargs["vocabulary_dtype"]
    elif "output_mode" in kwargs and kwargs["output_mode"] != index_lookup.INT:
      expected_output_dtype = tf.float32
    else:
      expected_output_dtype = tf.int64

    input_shape = input_data.shape

    if use_dataset:
      # Keras APIs expect batched datasets.
      # TODO(rachelim): `model.predict` predicts the result on each
      # dataset batch separately, then tries to concatenate the results
      # together. When the results have different shapes on the non-concat
      # axis (which can happen in the output_mode = INT case for
      # IndexLookup), the concatenation fails. In real use cases, this may
      # not be an issue because users are likely to pipe the preprocessing layer
      # into other keras layers instead of predicting it directly. A workaround
      # for these unit tests is to have the dataset only contain one batch, so
      # no concatenation needs to happen with the result. For consistency with
      # numpy input, we should make `predict` join differently shaped results
      # together sensibly, with 0 padding.
      input_data = tf.data.Dataset.from_tensor_slices(input_data).batch(
          input_shape[0])
      vocab_data = tf.data.Dataset.from_tensor_slices(vocab_data).batch(
          input_shape[0])

    with CustomObjectScope({"IndexLookup": cls}):
      output_data = testing_utils.layer_test(
          cls,
          kwargs=kwargs,
          input_shape=input_shape,
          input_data=input_data,
          input_dtype=input_dtype,
          expected_output_dtype=expected_output_dtype,
          validate_training=False,
          adapt_data=vocab_data)
    if "invert" in kwargs and kwargs["invert"]:
      self.assertAllEqual(expected_output, output_data)
    else:
      self.assertAllClose(expected_output, output_data)


</clonepair76>
<clonepair77>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/string_lookup_test.py" startline="70" endline="106" pcid="5191"></source>
  def test_layer_end_to_end_with_adapt(self, vocab_data, input_data, kwargs,
                                       use_dataset, expected_output,
                                       input_dtype):
    cls = string_lookup.StringLookup
    expected_output_dtype = tf.int64
    input_shape = input_data.shape

    if use_dataset:
      # Keras APIs expect batched datasets.
      # TODO(rachelim): `model.predict` predicts the result on each
      # dataset batch separately, then tries to concatenate the results
      # together. When the results have different shapes on the non-concat
      # axis (which can happen in the output_mode = INT case for
      # StringLookup), the concatenation fails. In real use cases, this may
      # not be an issue because users are likely to pipe the preprocessing layer
      # into other keras layers instead of predicting it directly. A workaround
      # for these unit tests is to have the dataset only contain one batch, so
      # no concatenation needs to happen with the result. For consistency with
      # numpy input, we should make `predict` join differently shaped results
      # together sensibly, with 0 padding.
      input_data = tf.data.Dataset.from_tensor_slices(input_data).batch(
          input_shape[0])
      vocab_data = tf.data.Dataset.from_tensor_slices(vocab_data).batch(
          input_shape[0])

    output_data = testing_utils.layer_test(
        cls,
        kwargs=kwargs,
        input_shape=input_shape,
        input_data=input_data,
        input_dtype=input_dtype,
        expected_output_dtype=expected_output_dtype,
        validate_training=False,
        adapt_data=vocab_data)
    self.assertAllClose(expected_output, output_data)


</clonepair77>

<clonepair77>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="302" endline="348" pcid="5513"></source>
  def test_layer_end_to_end_with_adapt(self, vocab_data, input_data, kwargs,
                                       use_dataset, expected_output,
                                       input_dtype):
    cls = index_lookup.IndexLookup
    if "invert" in kwargs and kwargs["invert"]:
      expected_output_dtype = kwargs["vocabulary_dtype"]
    elif "output_mode" in kwargs and kwargs["output_mode"] != index_lookup.INT:
      expected_output_dtype = tf.float32
    else:
      expected_output_dtype = tf.int64

    input_shape = input_data.shape

    if use_dataset:
      # Keras APIs expect batched datasets.
      # TODO(rachelim): `model.predict` predicts the result on each
      # dataset batch separately, then tries to concatenate the results
      # together. When the results have different shapes on the non-concat
      # axis (which can happen in the output_mode = INT case for
      # IndexLookup), the concatenation fails. In real use cases, this may
      # not be an issue because users are likely to pipe the preprocessing layer
      # into other keras layers instead of predicting it directly. A workaround
      # for these unit tests is to have the dataset only contain one batch, so
      # no concatenation needs to happen with the result. For consistency with
      # numpy input, we should make `predict` join differently shaped results
      # together sensibly, with 0 padding.
      input_data = tf.data.Dataset.from_tensor_slices(input_data).batch(
          input_shape[0])
      vocab_data = tf.data.Dataset.from_tensor_slices(vocab_data).batch(
          input_shape[0])

    with CustomObjectScope({"IndexLookup": cls}):
      output_data = testing_utils.layer_test(
          cls,
          kwargs=kwargs,
          input_shape=input_shape,
          input_data=input_data,
          input_dtype=input_dtype,
          expected_output_dtype=expected_output_dtype,
          validate_training=False,
          adapt_data=vocab_data)
    if "invert" in kwargs and kwargs["invert"]:
      self.assertAllEqual(expected_output, output_data)
    else:
      self.assertAllClose(expected_output, output_data)


</clonepair77>
<clonepair78>
<source file="systems/keras-2.8.0/keras/layers/lstm_v2_test.py" startline="526" endline="563" pcid="3813"></source>
  def test_lstm_output_on_multiple_kernel(self):
    input_shape = 10
    rnn_state_size = 8
    timestep = 4
    batch = 100

    x_train = np.random.random((batch, timestep, input_shape))

    inputs = keras.layers.Input(
        shape=[timestep, input_shape], dtype=tf.float32)
    with testing_utils.device(should_use_gpu=False):
      layer = rnn.LSTM(rnn_state_size)
      output = layer(inputs)
      cpu_model = keras.models.Model(inputs, output)
      weights = cpu_model.get_weights()
    y_1 = cpu_model.predict(x_train)

    with testing_utils.device(should_use_gpu=True):
      layer = rnn.LSTM(rnn_state_size)
      output = layer(inputs)
      gpu_model = keras.models.Model(inputs, output)
      gpu_model.set_weights(weights)
    y_2 = gpu_model.predict(x_train)

    # Note that cuDNN uses 'sigmoid' as activation, so the LSTM V2 uses
    # 'sigmoid' as default. Construct the canonical LSTM with sigmoid to achieve
    # the same output.
    with testing_utils.device(should_use_gpu=True):
      layer = rnn_v1.LSTM(rnn_state_size, recurrent_activation='sigmoid')
      output = layer(inputs)
      canonical_model = keras.models.Model(inputs, output)
      # Remove the extra cudnn bias since canonical lstm will not use it.
      canonical_model.set_weights(weights[:3])
    y_3 = canonical_model.predict(x_train)

    self.assertAllClose(y_1, y_2)
    self.assertAllClose(y_2, y_3)

</clonepair78>

<clonepair78>
<source file="systems/keras-2.8.0/keras/layers/gru_v2_test.py" startline="224" endline="262" pcid="4410"></source>
  def test_gru_v2_output_on_multiple_kernel(self):
    input_shape = 10
    rnn_state_size = 8
    timestep = 4
    batch = 100

    x_train = np.random.random((batch, timestep, input_shape))

    inputs = keras.layers.Input(
        shape=[timestep, input_shape], dtype=tf.float32)
    with testing_utils.device(should_use_gpu=False):
      layer = rnn.GRU(rnn_state_size)
      output = layer(inputs)
      cpu_model = keras.models.Model(inputs, output)
      weights = cpu_model.get_weights()
      y_1 = cpu_model.predict(x_train)

    with testing_utils.device(should_use_gpu=True):
      layer = rnn.GRU(rnn_state_size)
      output = layer(inputs)
      gpu_model = keras.models.Model(inputs, output)
      gpu_model.set_weights(weights)
      y_2 = gpu_model.predict(x_train)

    # Note that cuDNN uses 'sigmoid' as activation, so the GRU V2 uses
    # 'sigmoid' as default. Construct the canonical GRU with sigmoid to achieve
    # the same output.
    with testing_utils.device(should_use_gpu=True):
      layer = rnn_v1.GRU(rnn_state_size,
                         recurrent_activation='sigmoid',
                         reset_after=True)
      output = layer(inputs)
      canonical_model = keras.models.Model(inputs, output)
      canonical_model.set_weights(weights)
      y_3 = canonical_model.predict(x_train)

    self.assertAllClose(y_1, y_2, rtol=1e-5, atol=1e-5)
    self.assertAllClose(y_2, y_3, rtol=1e-5, atol=1e-5)

</clonepair78>
<clonepair79>
<source file="systems/keras-2.8.0/keras/layers/legacy_rnn/rnn_cell_impl.py" startline="519" endline="553" pcid="4562"></source>
  def __init__(self,
               num_units,
               activation=None,
               reuse=None,
               kernel_initializer=None,
               bias_initializer=None,
               name=None,
               dtype=None,
               **kwargs):
    warnings.warn(
        "`tf.nn.rnn_cell.GRUCell` is deprecated and will be removed "
        "in a future version. This class "
        "is equivalent as `tf.keras.layers.GRUCell`, "
        "and will be replaced by that in Tensorflow 2.0.",
        stacklevel=2)
    super(GRUCell, self).__init__(
        _reuse=reuse, name=name, dtype=dtype, **kwargs)
    _check_supported_dtypes(self.dtype)

    if tf.executing_eagerly() and tf.config.list_logical_devices("GPU"):
      logging.warning(
          "%s: Note that this cell is not optimized for performance. "
          "Please use tf.contrib.cudnn_rnn.CudnnGRU for better "
          "performance on GPU.", self)
    # Inputs must be 2-dimensional.
    self.input_spec = input_spec.InputSpec(ndim=2)

    self._num_units = num_units
    if activation:
      self._activation = activations.get(activation)
    else:
      self._activation = tf.tanh
    self._kernel_initializer = initializers.get(kernel_initializer)
    self._bias_initializer = initializers.get(bias_initializer)

</clonepair79>

<clonepair79>
<source file="systems/keras-2.8.0/keras/layers/legacy_rnn/rnn_cell_impl.py" startline="674" endline="735" pcid="4569"></source>
  def __init__(self,
               num_units,
               forget_bias=1.0,
               state_is_tuple=True,
               activation=None,
               reuse=None,
               name=None,
               dtype=None,
               **kwargs):
    """Initialize the basic LSTM cell.

    Args:
      num_units: int, The number of units in the LSTM cell.
      forget_bias: float, The bias added to forget gates (see above). Must set
        to `0.0` manually when restoring from CudnnLSTM-trained checkpoints.
      state_is_tuple: If True, accepted and returned states are 2-tuples of the
        `c_state` and `m_state`.  If False, they are concatenated along the
        column axis.  The latter behavior will soon be deprecated.
      activation: Activation function of the inner states.  Default: `tanh`. It
        could also be string that is within Keras activation function names.
      reuse: (optional) Python boolean describing whether to reuse variables in
        an existing scope.  If not `True`, and the existing scope already has
        the given variables, an error is raised.
      name: String, the name of the layer. Layers with the same name will share
        weights, but to avoid mistakes we require reuse=True in such cases.
      dtype: Default dtype of the layer (default of `None` means use the type of
        the first input). Required when `build` is called before `call`.
      **kwargs: Dict, keyword named properties for common layer attributes, like
        `trainable` etc when constructing the cell from configs of get_config().
        When restoring from CudnnLSTM-trained checkpoints, must use
        `CudnnCompatibleLSTMCell` instead.
    """
    warnings.warn(
        "`tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be "
        "removed in a future version. This class "
        "is equivalent as `tf.keras.layers.LSTMCell`, "
        "and will be replaced by that in Tensorflow 2.0.",
        stacklevel=2)
    super(BasicLSTMCell, self).__init__(
        _reuse=reuse, name=name, dtype=dtype, **kwargs)
    _check_supported_dtypes(self.dtype)
    if not state_is_tuple:
      logging.warning(
          "%s: Using a concatenated state is slower and will soon be "
          "deprecated.  Use state_is_tuple=True.", self)
    if tf.executing_eagerly() and tf.config.list_logical_devices("GPU"):
      logging.warning(
          "%s: Note that this cell is not optimized for performance. "
          "Please use tf.contrib.cudnn_rnn.CudnnLSTM for better "
          "performance on GPU.", self)

    # Inputs must be 2-dimensional.
    self.input_spec = input_spec.InputSpec(ndim=2)

    self._num_units = num_units
    self._forget_bias = forget_bias
    self._state_is_tuple = state_is_tuple
    if activation:
      self._activation = activations.get(activation)
    else:
      self._activation = tf.tanh

</clonepair79>
<clonepair80>
<source file="systems/keras-2.8.0/keras/layers/legacy_rnn/rnn_cell_impl.py" startline="409" endline="439" pcid="4556"></source>
  def __init__(self,
               num_units,
               activation=None,
               reuse=None,
               name=None,
               dtype=None,
               **kwargs):
    warnings.warn(
        "`tf.nn.rnn_cell.BasicRNNCell` is deprecated and will be "
        "removed in a future version. This class "
        "is equivalent as `tf.keras.layers.SimpleRNNCell`, "
        "and will be replaced by that in Tensorflow 2.0.",
        stacklevel=2)
    super(BasicRNNCell, self).__init__(
        _reuse=reuse, name=name, dtype=dtype, **kwargs)
    _check_supported_dtypes(self.dtype)
    if tf.executing_eagerly() and tf.config.list_logical_devices("GPU"):
      logging.warning(
          "%s: Note that this cell is not optimized for performance. "
          "Please use tf.contrib.cudnn_rnn.CudnnRNNTanh for better "
          "performance on GPU.", self)

    # Inputs must be 2-dimensional.
    self.input_spec = input_spec.InputSpec(ndim=2)

    self._num_units = num_units
    if activation:
      self._activation = activations.get(activation)
    else:
      self._activation = tf.tanh

</clonepair80>

<clonepair80>
<source file="systems/keras-2.8.0/keras/layers/legacy_rnn/rnn_cell_impl.py" startline="674" endline="735" pcid="4569"></source>
  def __init__(self,
               num_units,
               forget_bias=1.0,
               state_is_tuple=True,
               activation=None,
               reuse=None,
               name=None,
               dtype=None,
               **kwargs):
    """Initialize the basic LSTM cell.

    Args:
      num_units: int, The number of units in the LSTM cell.
      forget_bias: float, The bias added to forget gates (see above). Must set
        to `0.0` manually when restoring from CudnnLSTM-trained checkpoints.
      state_is_tuple: If True, accepted and returned states are 2-tuples of the
        `c_state` and `m_state`.  If False, they are concatenated along the
        column axis.  The latter behavior will soon be deprecated.
      activation: Activation function of the inner states.  Default: `tanh`. It
        could also be string that is within Keras activation function names.
      reuse: (optional) Python boolean describing whether to reuse variables in
        an existing scope.  If not `True`, and the existing scope already has
        the given variables, an error is raised.
      name: String, the name of the layer. Layers with the same name will share
        weights, but to avoid mistakes we require reuse=True in such cases.
      dtype: Default dtype of the layer (default of `None` means use the type of
        the first input). Required when `build` is called before `call`.
      **kwargs: Dict, keyword named properties for common layer attributes, like
        `trainable` etc when constructing the cell from configs of get_config().
        When restoring from CudnnLSTM-trained checkpoints, must use
        `CudnnCompatibleLSTMCell` instead.
    """
    warnings.warn(
        "`tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be "
        "removed in a future version. This class "
        "is equivalent as `tf.keras.layers.LSTMCell`, "
        "and will be replaced by that in Tensorflow 2.0.",
        stacklevel=2)
    super(BasicLSTMCell, self).__init__(
        _reuse=reuse, name=name, dtype=dtype, **kwargs)
    _check_supported_dtypes(self.dtype)
    if not state_is_tuple:
      logging.warning(
          "%s: Using a concatenated state is slower and will soon be "
          "deprecated.  Use state_is_tuple=True.", self)
    if tf.executing_eagerly() and tf.config.list_logical_devices("GPU"):
      logging.warning(
          "%s: Note that this cell is not optimized for performance. "
          "Please use tf.contrib.cudnn_rnn.CudnnLSTM for better "
          "performance on GPU.", self)

    # Inputs must be 2-dimensional.
    self.input_spec = input_spec.InputSpec(ndim=2)

    self._num_units = num_units
    self._forget_bias = forget_bias
    self._state_is_tuple = state_is_tuple
    if activation:
      self._activation = activations.get(activation)
    else:
      self._activation = tf.tanh

</clonepair80>
<clonepair81>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_distribution_test.py" startline="52" endline="84" pcid="5014"></source>
  def test_strategy(self, strategy):
    if (backend.is_tpu_strategy(strategy) and
        not test_util.is_mlir_bridge_enabled()):
      self.skipTest("TPU tests require MLIR bridge")

    vocab_data = [[
        "earth", "earth", "earth", "earth", "wind", "wind", "wind", "and",
        "and", "fire"
    ]]
    vocab_dataset = tf.data.Dataset.from_tensors(vocab_data)
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    input_dataset = tf.data.Dataset.from_tensor_slices(input_array).batch(
        2, drop_remainder=True)
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    tf.config.set_soft_device_placement(True)

    with strategy.scope():
      input_data = keras.Input(shape=(None,), dtype=tf.string)
      layer = index_lookup.IndexLookup(
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          vocabulary_dtype=tf.string)
      layer.adapt(vocab_dataset)
      int_data = layer(input_data)
      model = keras.Model(inputs=input_data, outputs=int_data)
    model.compile(loss="mse")
    output_dataset = model.predict(input_dataset)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair81>

<clonepair81>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_distribution_test.py" startline="85" endline="115" pcid="5015"></source>
  def test_strategy_with_file(self, strategy):
    if (backend.is_tpu_strategy(strategy) and
        not test_util.is_mlir_bridge_enabled()):
      self.skipTest("TPU tests require MLIR bridge")

    vocab_data = ["earth", "wind", "and", "fire"]
    vocab_file = self._write_to_temp_file("temp", vocab_data)

    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    input_dataset = tf.data.Dataset.from_tensor_slices(input_array).batch(
        2, drop_remainder=True)
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    tf.config.set_soft_device_placement(True)

    with strategy.scope():
      input_data = keras.Input(shape=(None,), dtype=tf.string)
      layer = index_lookup.IndexLookup(
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          vocabulary_dtype=tf.string,
          vocabulary=vocab_file)
      int_data = layer(input_data)
      model = keras.Model(inputs=input_data, outputs=int_data)
    model.compile(loss="mse")
    output_dataset = model.predict(input_dataset)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair81>
<clonepair82>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_distribution_test.py" startline="52" endline="84" pcid="5014"></source>
  def test_strategy(self, strategy):
    if (backend.is_tpu_strategy(strategy) and
        not test_util.is_mlir_bridge_enabled()):
      self.skipTest("TPU tests require MLIR bridge")

    vocab_data = [[
        "earth", "earth", "earth", "earth", "wind", "wind", "wind", "and",
        "and", "fire"
    ]]
    vocab_dataset = tf.data.Dataset.from_tensors(vocab_data)
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    input_dataset = tf.data.Dataset.from_tensor_slices(input_array).batch(
        2, drop_remainder=True)
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    tf.config.set_soft_device_placement(True)

    with strategy.scope():
      input_data = keras.Input(shape=(None,), dtype=tf.string)
      layer = index_lookup.IndexLookup(
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          vocabulary_dtype=tf.string)
      layer.adapt(vocab_dataset)
      int_data = layer(input_data)
      model = keras.Model(inputs=input_data, outputs=int_data)
    model.compile(loss="mse")
    output_dataset = model.predict(input_dataset)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair82>

<clonepair82>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_distribution_test.py" startline="116" endline="148" pcid="5016"></source>
  def test_tpu_with_multiple_oov(self, strategy):
    # TODO(b/180614455): remove this check when MLIR bridge is always enabled.
    if backend.is_tpu_strategy(strategy):
      self.skipTest("This test needs MLIR bridge on TPU.")

    vocab_data = [[
        "earth", "earth", "earth", "earth", "wind", "wind", "wind", "and",
        "and", "fire"
    ]]
    vocab_dataset = tf.data.Dataset.from_tensors(vocab_data)
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    input_dataset = tf.data.Dataset.from_tensor_slices(input_array).batch(
        2, drop_remainder=True)
    expected_output = [[3, 4, 5, 6], [6, 5, 3, 1]]

    tf.config.set_soft_device_placement(True)

    with strategy.scope():
      input_data = keras.Input(shape=(None,), dtype=tf.string)
      layer = index_lookup.IndexLookup(
          max_tokens=None,
          num_oov_indices=2,
          mask_token="",
          oov_token="[OOV]",
          vocabulary_dtype=tf.string)
      layer.adapt(vocab_dataset)
      int_data = layer(input_data)
      model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_dataset)
    self.assertAllEqual(expected_output, output_dataset)


</clonepair82>
<clonepair83>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="833" endline="859" pcid="5538"></source>
  def test_int_output_no_oov_sparse(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    valid_input = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", ""]])
    invalid_input = np.array([["earth", "wind", "and", "michigan"],
                              ["fire", "and", "earth", "michigan"]])
    valid_input = tf.sparse.from_dense(valid_input)
    invalid_input = tf.sparse.from_dense(invalid_input)
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=0,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(valid_input)
    self.assertAllEqual(expected_output,
                        tf.sparse.to_dense(output_data))
    with self.assertRaisesRegex(tf.errors.InvalidArgumentError,
                                "found OOV values.*michigan"):
      _ = model.predict(invalid_input)

</clonepair83>

<clonepair83>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1027" endline="1056" pcid="5545"></source>
  def test_multi_hot_output_no_oov(self):
    """Check multi hot output when num_oov_indices=0."""
    vocab_data = ["earth", "wind", "and", "fire"]
    valid_input = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", ""]])
    invalid_input = np.array([["earth", "wind", "and", "michigan"],
                              ["fire", "and", "earth", "michigan"]])
    expected_output = [
        [1, 1, 1, 1, 0],
        [1, 0, 1, 1, 0],
    ]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=5,
        num_oov_indices=0,
        mask_token="",
        oov_token="[OOV]",
        output_mode=index_lookup.MULTI_HOT,
        pad_to_max_tokens=True,
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    binary_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=binary_data)
    output_data = model.predict(valid_input)
    self.assertAllEqual(expected_output, output_data)
    with self.assertRaisesRegex(tf.errors.InvalidArgumentError,
                                "found OOV values.*michigan"):
      _ = model.predict(invalid_input)

</clonepair83>
<clonepair84>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="807" endline="832" pcid="5537"></source>
  def test_int_output_no_oov_ragged(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    valid_input = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", ""]])
    invalid_input = np.array([["earth", "wind", "and", "michigan"],
                              ["fire", "and", "earth", "michigan"]])
    valid_input = tf.RaggedTensor.from_tensor(valid_input)
    invalid_input = tf.RaggedTensor.from_tensor(invalid_input)
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=0,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(valid_input)
    self.assertAllEqual(expected_output, output_data)
    with self.assertRaisesRegex(tf.errors.InvalidArgumentError,
                                "found OOV values.*michigan"):
      _ = model.predict(invalid_input)

</clonepair84>

<clonepair84>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1027" endline="1056" pcid="5545"></source>
  def test_multi_hot_output_no_oov(self):
    """Check multi hot output when num_oov_indices=0."""
    vocab_data = ["earth", "wind", "and", "fire"]
    valid_input = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", ""]])
    invalid_input = np.array([["earth", "wind", "and", "michigan"],
                              ["fire", "and", "earth", "michigan"]])
    expected_output = [
        [1, 1, 1, 1, 0],
        [1, 0, 1, 1, 0],
    ]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=5,
        num_oov_indices=0,
        mask_token="",
        oov_token="[OOV]",
        output_mode=index_lookup.MULTI_HOT,
        pad_to_max_tokens=True,
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    binary_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=binary_data)
    output_data = model.predict(valid_input)
    self.assertAllEqual(expected_output, output_data)
    with self.assertRaisesRegex(tf.errors.InvalidArgumentError,
                                "found OOV values.*michigan"):
      _ = model.predict(invalid_input)

</clonepair84>
<clonepair85>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="783" endline="806" pcid="5536"></source>
  def test_int_output_no_oov(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    valid_input = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", ""]])
    invalid_input = np.array([["earth", "wind", "and", "michigan"],
                              ["fire", "and", "earth", "michigan"]])
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=0,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(valid_input)
    self.assertAllEqual(expected_output, output_data)
    with self.assertRaisesRegex(tf.errors.InvalidArgumentError,
                                "found OOV values.*michigan"):
      _ = model.predict(invalid_input)

</clonepair85>

<clonepair85>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1027" endline="1056" pcid="5545"></source>
  def test_multi_hot_output_no_oov(self):
    """Check multi hot output when num_oov_indices=0."""
    vocab_data = ["earth", "wind", "and", "fire"]
    valid_input = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", ""]])
    invalid_input = np.array([["earth", "wind", "and", "michigan"],
                              ["fire", "and", "earth", "michigan"]])
    expected_output = [
        [1, 1, 1, 1, 0],
        [1, 0, 1, 1, 0],
    ]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=5,
        num_oov_indices=0,
        mask_token="",
        oov_token="[OOV]",
        output_mode=index_lookup.MULTI_HOT,
        pad_to_max_tokens=True,
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    binary_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=binary_data)
    output_data = model.predict(valid_input)
    self.assertAllEqual(expected_output, output_data)
    with self.assertRaisesRegex(tf.errors.InvalidArgumentError,
                                "found OOV values.*michigan"):
      _ = model.predict(invalid_input)

</clonepair85>
<clonepair86>
<source file="systems/keras-2.8.0/keras/layers/legacy_rnn/rnn_cell_impl.py" startline="409" endline="439" pcid="4556"></source>
  def __init__(self,
               num_units,
               activation=None,
               reuse=None,
               name=None,
               dtype=None,
               **kwargs):
    warnings.warn(
        "`tf.nn.rnn_cell.BasicRNNCell` is deprecated and will be "
        "removed in a future version. This class "
        "is equivalent as `tf.keras.layers.SimpleRNNCell`, "
        "and will be replaced by that in Tensorflow 2.0.",
        stacklevel=2)
    super(BasicRNNCell, self).__init__(
        _reuse=reuse, name=name, dtype=dtype, **kwargs)
    _check_supported_dtypes(self.dtype)
    if tf.executing_eagerly() and tf.config.list_logical_devices("GPU"):
      logging.warning(
          "%s: Note that this cell is not optimized for performance. "
          "Please use tf.contrib.cudnn_rnn.CudnnRNNTanh for better "
          "performance on GPU.", self)

    # Inputs must be 2-dimensional.
    self.input_spec = input_spec.InputSpec(ndim=2)

    self._num_units = num_units
    if activation:
      self._activation = activations.get(activation)
    else:
      self._activation = tf.tanh

</clonepair86>

<clonepair86>
<source file="systems/keras-2.8.0/keras/layers/legacy_rnn/rnn_cell_impl.py" startline="519" endline="553" pcid="4562"></source>
  def __init__(self,
               num_units,
               activation=None,
               reuse=None,
               kernel_initializer=None,
               bias_initializer=None,
               name=None,
               dtype=None,
               **kwargs):
    warnings.warn(
        "`tf.nn.rnn_cell.GRUCell` is deprecated and will be removed "
        "in a future version. This class "
        "is equivalent as `tf.keras.layers.GRUCell`, "
        "and will be replaced by that in Tensorflow 2.0.",
        stacklevel=2)
    super(GRUCell, self).__init__(
        _reuse=reuse, name=name, dtype=dtype, **kwargs)
    _check_supported_dtypes(self.dtype)

    if tf.executing_eagerly() and tf.config.list_logical_devices("GPU"):
      logging.warning(
          "%s: Note that this cell is not optimized for performance. "
          "Please use tf.contrib.cudnn_rnn.CudnnGRU for better "
          "performance on GPU.", self)
    # Inputs must be 2-dimensional.
    self.input_spec = input_spec.InputSpec(ndim=2)

    self._num_units = num_units
    if activation:
      self._activation = activations.get(activation)
    else:
      self._activation = tf.tanh
    self._kernel_initializer = initializers.get(kernel_initializer)
    self._bias_initializer = initializers.get(bias_initializer)

</clonepair86>
<clonepair87>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1876" endline="1915" pcid="5595"></source>
  def test_persistence_file_vocabs_tf_save_tf_load(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    # Build and validate a golden model.
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string,
        vocabulary=vocab_file)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(output_dataset, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model")
    tf.saved_model.save(obj=model, export_dir=output_path)

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()

    loaded_model = tf.saved_model.load(output_path)
    f = loaded_model.signatures["serving_default"]

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_dataset = f(tf.constant(input_array))["index_lookup"]
    self.assertAllEqual(new_output_dataset, expected_output)

</clonepair87>

<clonepair87>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1916" endline="1955" pcid="5596"></source>
  def test_vocabulary_persistence_file_vocab_keras_save_tf_load(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    # Build and validate a golden model.
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string,
        vocabulary=vocab_file)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(output_dataset, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model")
    model.save(output_path, save_format="tf")

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()

    loaded_model = tf.saved_model.load(output_path)
    f = loaded_model.signatures["serving_default"]

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_dataset = f(tf.constant(input_array))["index_lookup"]
    self.assertAllEqual(new_output_dataset, expected_output)

</clonepair87>
<clonepair88>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1806" endline="1843" pcid="5593"></source>
  def test_vocabulary_persistence_across_saving(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    # Build and validate a golden model.
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(output_dataset, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model")
    model.save(output_path, save_format="tf")

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()

    loaded_model = keras.models.load_model(
        output_path, custom_objects={"IndexLookup": index_lookup.IndexLookup})

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_dataset = loaded_model.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

</clonepair88>

<clonepair88>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1916" endline="1955" pcid="5596"></source>
  def test_vocabulary_persistence_file_vocab_keras_save_tf_load(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    # Build and validate a golden model.
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string,
        vocabulary=vocab_file)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(output_dataset, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model")
    model.save(output_path, save_format="tf")

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()

    loaded_model = tf.saved_model.load(output_path)
    f = loaded_model.signatures["serving_default"]

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_dataset = f(tf.constant(input_array))["index_lookup"]
    self.assertAllEqual(new_output_dataset, expected_output)

</clonepair88>
<clonepair89>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1844" endline="1875" pcid="5594"></source>
  def test_vocabulary_persistence_file_across_cloning(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]
    vocab_file = self._write_to_temp_file("temp", vocab_data)

    # Build and validate a golden model.
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string,
        vocabulary=vocab_file)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(output_dataset, expected_output)

    # Clone the model and set weights.
    new_model = keras.models.clone_model(model)
    new_model.set_weights(model.get_weights())

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, new_model)

    # Validate correctness of the new model.
    new_output_dataset = new_model.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

</clonepair89>

<clonepair89>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1916" endline="1955" pcid="5596"></source>
  def test_vocabulary_persistence_file_vocab_keras_save_tf_load(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    # Build and validate a golden model.
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string,
        vocabulary=vocab_file)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(output_dataset, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model")
    model.save(output_path, save_format="tf")

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()

    loaded_model = tf.saved_model.load(output_path)
    f = loaded_model.signatures["serving_default"]

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_dataset = f(tf.constant(input_array))["index_lookup"]
    self.assertAllEqual(new_output_dataset, expected_output)

</clonepair89>
<clonepair90>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1806" endline="1843" pcid="5593"></source>
  def test_vocabulary_persistence_across_saving(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    # Build and validate a golden model.
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(output_dataset, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model")
    model.save(output_path, save_format="tf")

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()

    loaded_model = keras.models.load_model(
        output_path, custom_objects={"IndexLookup": index_lookup.IndexLookup})

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_dataset = loaded_model.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

</clonepair90>

<clonepair90>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1876" endline="1915" pcid="5595"></source>
  def test_persistence_file_vocabs_tf_save_tf_load(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    # Build and validate a golden model.
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string,
        vocabulary=vocab_file)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(output_dataset, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model")
    tf.saved_model.save(obj=model, export_dir=output_path)

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()

    loaded_model = tf.saved_model.load(output_path)
    f = loaded_model.signatures["serving_default"]

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_dataset = f(tf.constant(input_array))["index_lookup"]
    self.assertAllEqual(new_output_dataset, expected_output)

</clonepair90>
<clonepair91>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1844" endline="1875" pcid="5594"></source>
  def test_vocabulary_persistence_file_across_cloning(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]
    vocab_file = self._write_to_temp_file("temp", vocab_data)

    # Build and validate a golden model.
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string,
        vocabulary=vocab_file)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(output_dataset, expected_output)

    # Clone the model and set weights.
    new_model = keras.models.clone_model(model)
    new_model.set_weights(model.get_weights())

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, new_model)

    # Validate correctness of the new model.
    new_output_dataset = new_model.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

</clonepair91>

<clonepair91>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1876" endline="1915" pcid="5595"></source>
  def test_persistence_file_vocabs_tf_save_tf_load(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    # Build and validate a golden model.
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string,
        vocabulary=vocab_file)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(output_dataset, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model")
    tf.saved_model.save(obj=model, export_dir=output_path)

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()

    loaded_model = tf.saved_model.load(output_path)
    f = loaded_model.signatures["serving_default"]

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_dataset = f(tf.constant(input_array))["index_lookup"]
    self.assertAllEqual(new_output_dataset, expected_output)

</clonepair91>
<clonepair92>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_distribution_test.py" startline="85" endline="115" pcid="5015"></source>
  def test_strategy_with_file(self, strategy):
    if (backend.is_tpu_strategy(strategy) and
        not test_util.is_mlir_bridge_enabled()):
      self.skipTest("TPU tests require MLIR bridge")

    vocab_data = ["earth", "wind", "and", "fire"]
    vocab_file = self._write_to_temp_file("temp", vocab_data)

    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    input_dataset = tf.data.Dataset.from_tensor_slices(input_array).batch(
        2, drop_remainder=True)
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    tf.config.set_soft_device_placement(True)

    with strategy.scope():
      input_data = keras.Input(shape=(None,), dtype=tf.string)
      layer = index_lookup.IndexLookup(
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          vocabulary_dtype=tf.string,
          vocabulary=vocab_file)
      int_data = layer(input_data)
      model = keras.Model(inputs=input_data, outputs=int_data)
    model.compile(loss="mse")
    output_dataset = model.predict(input_dataset)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair92>

<clonepair92>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_distribution_test.py" startline="116" endline="148" pcid="5016"></source>
  def test_tpu_with_multiple_oov(self, strategy):
    # TODO(b/180614455): remove this check when MLIR bridge is always enabled.
    if backend.is_tpu_strategy(strategy):
      self.skipTest("This test needs MLIR bridge on TPU.")

    vocab_data = [[
        "earth", "earth", "earth", "earth", "wind", "wind", "wind", "and",
        "and", "fire"
    ]]
    vocab_dataset = tf.data.Dataset.from_tensors(vocab_data)
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    input_dataset = tf.data.Dataset.from_tensor_slices(input_array).batch(
        2, drop_remainder=True)
    expected_output = [[3, 4, 5, 6], [6, 5, 3, 1]]

    tf.config.set_soft_device_placement(True)

    with strategy.scope():
      input_data = keras.Input(shape=(None,), dtype=tf.string)
      layer = index_lookup.IndexLookup(
          max_tokens=None,
          num_oov_indices=2,
          mask_token="",
          oov_token="[OOV]",
          vocabulary_dtype=tf.string)
      layer.adapt(vocab_dataset)
      int_data = layer(input_data)
      model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_dataset)
    self.assertAllEqual(expected_output, output_dataset)


</clonepair92>
<clonepair93>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_distribution_test.py" startline="85" endline="115" pcid="5015"></source>
  def test_strategy_with_file(self, strategy):
    if (backend.is_tpu_strategy(strategy) and
        not test_util.is_mlir_bridge_enabled()):
      self.skipTest("TPU tests require MLIR bridge")

    vocab_data = ["earth", "wind", "and", "fire"]
    vocab_file = self._write_to_temp_file("temp", vocab_data)

    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    input_dataset = tf.data.Dataset.from_tensor_slices(input_array).batch(
        2, drop_remainder=True)
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    tf.config.set_soft_device_placement(True)

    with strategy.scope():
      input_data = keras.Input(shape=(None,), dtype=tf.string)
      layer = index_lookup.IndexLookup(
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          vocabulary_dtype=tf.string,
          vocabulary=vocab_file)
      int_data = layer(input_data)
      model = keras.Model(inputs=input_data, outputs=int_data)
    model.compile(loss="mse")
    output_dataset = model.predict(input_dataset)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair93>

<clonepair93>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/text_vectorization_distribution_test.py" startline="41" endline="69" pcid="5009"></source>
  def test_distribution_strategy_output(self, strategy):
    if (backend.is_tpu_strategy(strategy) and
        not test_util.is_mlir_bridge_enabled()):
      self.skipTest("TPU tests require MLIR bridge")

    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    input_dataset = tf.data.Dataset.from_tensor_slices(input_array).batch(
        2, drop_remainder=True)

    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    tf.config.set_soft_device_placement(True)

    with strategy.scope():
      input_data = keras.Input(shape=(None,), dtype=tf.string)
      layer = text_vectorization.TextVectorization(
          max_tokens=None,
          standardize=None,
          split=None,
          output_mode=text_vectorization.INT,
          vocabulary=vocab_data)
      int_data = layer(input_data)
      model = keras.Model(inputs=input_data, outputs=int_data)

    output_dataset = model.predict(input_dataset)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair93>
<clonepair94>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1146" endline="1175" pcid="5550"></source>
  def test_file_vocab_and_list_vocab_identical_attrs(self):
    vocab_data = ["earth", "wind", "and", "fire"]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    file_layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)

    list_layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)

    expected_vocab = ["", "[OOV]", "earth", "wind", "and", "fire"]
    self.assertAllEqual(expected_vocab, list_layer.get_vocabulary())
    expected_vocab_size = 6
    self.assertAllEqual(expected_vocab_size, list_layer.vocabulary_size())
    self.assertAllEqual(list_layer.get_vocabulary(),
                        file_layer.get_vocabulary())
    self.assertAllEqual(list_layer.vocabulary_size(),
                        file_layer.vocabulary_size())

</clonepair94>

<clonepair94>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1176" endline="1205" pcid="5551"></source>
  def test_file_vocab_and_list_vocab_identical_attrs_multi_oov(self):
    vocab_data = ["earth", "wind", "and", "fire"]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    file_layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        num_oov_indices=2,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)

    list_layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=2,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)

    expected_vocab = ["", "[OOV]", "[OOV]", "earth", "wind", "and", "fire"]
    self.assertAllEqual(expected_vocab, list_layer.get_vocabulary())
    expected_vocab_size = 7
    self.assertAllEqual(expected_vocab_size, list_layer.vocabulary_size())
    self.assertAllEqual(list_layer.get_vocabulary(),
                        file_layer.get_vocabulary())
    self.assertAllEqual(list_layer.vocabulary_size(),
                        file_layer.vocabulary_size())

</clonepair94>
<clonepair95>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1176" endline="1205" pcid="5551"></source>
  def test_file_vocab_and_list_vocab_identical_attrs_multi_oov(self):
    vocab_data = ["earth", "wind", "and", "fire"]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    file_layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        num_oov_indices=2,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)

    list_layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=2,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)

    expected_vocab = ["", "[OOV]", "[OOV]", "earth", "wind", "and", "fire"]
    self.assertAllEqual(expected_vocab, list_layer.get_vocabulary())
    expected_vocab_size = 7
    self.assertAllEqual(expected_vocab_size, list_layer.vocabulary_size())
    self.assertAllEqual(list_layer.get_vocabulary(),
                        file_layer.get_vocabulary())
    self.assertAllEqual(list_layer.vocabulary_size(),
                        file_layer.vocabulary_size())

</clonepair95>

<clonepair95>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1206" endline="1235" pcid="5552"></source>
  def test_file_vocab_and_list_vocab_identical_attrs_no_mask(self):
    vocab_data = ["earth", "wind", "and", "fire"]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    file_layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        num_oov_indices=2,
        mask_token=None,
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)

    list_layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=2,
        mask_token=None,
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)

    expected_vocab = ["[OOV]", "[OOV]", "earth", "wind", "and", "fire"]
    self.assertAllEqual(expected_vocab, list_layer.get_vocabulary())
    expected_vocab_size = 6
    self.assertAllEqual(expected_vocab_size, list_layer.vocabulary_size())
    self.assertAllEqual(list_layer.get_vocabulary(),
                        file_layer.get_vocabulary())
    self.assertAllEqual(list_layer.vocabulary_size(),
                        file_layer.vocabulary_size())

</clonepair95>
<clonepair96>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1146" endline="1175" pcid="5550"></source>
  def test_file_vocab_and_list_vocab_identical_attrs(self):
    vocab_data = ["earth", "wind", "and", "fire"]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    file_layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)

    list_layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)

    expected_vocab = ["", "[OOV]", "earth", "wind", "and", "fire"]
    self.assertAllEqual(expected_vocab, list_layer.get_vocabulary())
    expected_vocab_size = 6
    self.assertAllEqual(expected_vocab_size, list_layer.vocabulary_size())
    self.assertAllEqual(list_layer.get_vocabulary(),
                        file_layer.get_vocabulary())
    self.assertAllEqual(list_layer.vocabulary_size(),
                        file_layer.vocabulary_size())

</clonepair96>

<clonepair96>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1206" endline="1235" pcid="5552"></source>
  def test_file_vocab_and_list_vocab_identical_attrs_no_mask(self):
    vocab_data = ["earth", "wind", "and", "fire"]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    file_layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        num_oov_indices=2,
        mask_token=None,
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)

    list_layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=2,
        mask_token=None,
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)

    expected_vocab = ["[OOV]", "[OOV]", "earth", "wind", "and", "fire"]
    self.assertAllEqual(expected_vocab, list_layer.get_vocabulary())
    expected_vocab_size = 6
    self.assertAllEqual(expected_vocab_size, list_layer.vocabulary_size())
    self.assertAllEqual(list_layer.get_vocabulary(),
                        file_layer.get_vocabulary())
    self.assertAllEqual(list_layer.vocabulary_size(),
                        file_layer.vocabulary_size())

</clonepair96>
<clonepair97>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="807" endline="832" pcid="5537"></source>
  def test_int_output_no_oov_ragged(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    valid_input = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", ""]])
    invalid_input = np.array([["earth", "wind", "and", "michigan"],
                              ["fire", "and", "earth", "michigan"]])
    valid_input = tf.RaggedTensor.from_tensor(valid_input)
    invalid_input = tf.RaggedTensor.from_tensor(invalid_input)
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=0,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(valid_input)
    self.assertAllEqual(expected_output, output_data)
    with self.assertRaisesRegex(tf.errors.InvalidArgumentError,
                                "found OOV values.*michigan"):
      _ = model.predict(invalid_input)

</clonepair97>

<clonepair97>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1806" endline="1843" pcid="5593"></source>
  def test_vocabulary_persistence_across_saving(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    # Build and validate a golden model.
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(output_dataset, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model")
    model.save(output_path, save_format="tf")

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()

    loaded_model = keras.models.load_model(
        output_path, custom_objects={"IndexLookup": index_lookup.IndexLookup})

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_dataset = loaded_model.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

</clonepair97>
<clonepair98>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1806" endline="1843" pcid="5593"></source>
  def test_vocabulary_persistence_across_saving(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    # Build and validate a golden model.
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(output_dataset, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model")
    model.save(output_path, save_format="tf")

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()

    loaded_model = keras.models.load_model(
        output_path, custom_objects={"IndexLookup": index_lookup.IndexLookup})

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_dataset = loaded_model.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

</clonepair98>

<clonepair98>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1844" endline="1875" pcid="5594"></source>
  def test_vocabulary_persistence_file_across_cloning(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]
    vocab_file = self._write_to_temp_file("temp", vocab_data)

    # Build and validate a golden model.
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string,
        vocabulary=vocab_file)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(output_dataset, expected_output)

    # Clone the model and set weights.
    new_model = keras.models.clone_model(model)
    new_model.set_weights(model.get_weights())

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, new_model)

    # Validate correctness of the new model.
    new_output_dataset = new_model.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

</clonepair98>
<clonepair99>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="783" endline="806" pcid="5536"></source>
  def test_int_output_no_oov(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    valid_input = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", ""]])
    invalid_input = np.array([["earth", "wind", "and", "michigan"],
                              ["fire", "and", "earth", "michigan"]])
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=0,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(valid_input)
    self.assertAllEqual(expected_output, output_data)
    with self.assertRaisesRegex(tf.errors.InvalidArgumentError,
                                "found OOV values.*michigan"):
      _ = model.predict(invalid_input)

</clonepair99>

<clonepair99>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1806" endline="1843" pcid="5593"></source>
  def test_vocabulary_persistence_across_saving(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    # Build and validate a golden model.
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(output_dataset, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model")
    model.save(output_path, save_format="tf")

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()

    loaded_model = keras.models.load_model(
        output_path, custom_objects={"IndexLookup": index_lookup.IndexLookup})

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_dataset = loaded_model.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

</clonepair99>
<clonepair100>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="807" endline="832" pcid="5537"></source>
  def test_int_output_no_oov_ragged(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    valid_input = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", ""]])
    invalid_input = np.array([["earth", "wind", "and", "michigan"],
                              ["fire", "and", "earth", "michigan"]])
    valid_input = tf.RaggedTensor.from_tensor(valid_input)
    invalid_input = tf.RaggedTensor.from_tensor(invalid_input)
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=0,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(valid_input)
    self.assertAllEqual(expected_output, output_data)
    with self.assertRaisesRegex(tf.errors.InvalidArgumentError,
                                "found OOV values.*michigan"):
      _ = model.predict(invalid_input)

</clonepair100>

<clonepair100>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="833" endline="859" pcid="5538"></source>
  def test_int_output_no_oov_sparse(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    valid_input = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", ""]])
    invalid_input = np.array([["earth", "wind", "and", "michigan"],
                              ["fire", "and", "earth", "michigan"]])
    valid_input = tf.sparse.from_dense(valid_input)
    invalid_input = tf.sparse.from_dense(invalid_input)
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=0,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(valid_input)
    self.assertAllEqual(expected_output,
                        tf.sparse.to_dense(output_data))
    with self.assertRaisesRegex(tf.errors.InvalidArgumentError,
                                "found OOV values.*michigan"):
      _ = model.predict(invalid_input)

</clonepair100>
<clonepair101>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="783" endline="806" pcid="5536"></source>
  def test_int_output_no_oov(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    valid_input = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", ""]])
    invalid_input = np.array([["earth", "wind", "and", "michigan"],
                              ["fire", "and", "earth", "michigan"]])
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=0,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(valid_input)
    self.assertAllEqual(expected_output, output_data)
    with self.assertRaisesRegex(tf.errors.InvalidArgumentError,
                                "found OOV values.*michigan"):
      _ = model.predict(invalid_input)

</clonepair101>

<clonepair101>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="833" endline="859" pcid="5538"></source>
  def test_int_output_no_oov_sparse(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    valid_input = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", ""]])
    invalid_input = np.array([["earth", "wind", "and", "michigan"],
                              ["fire", "and", "earth", "michigan"]])
    valid_input = tf.sparse.from_dense(valid_input)
    invalid_input = tf.sparse.from_dense(invalid_input)
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=0,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(valid_input)
    self.assertAllEqual(expected_output,
                        tf.sparse.to_dense(output_data))
    with self.assertRaisesRegex(tf.errors.InvalidArgumentError,
                                "found OOV values.*michigan"):
      _ = model.predict(invalid_input)

</clonepair101>
<clonepair102>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/text_vectorization_test.py" startline="964" endline="991" pcid="5053"></source>
  def test_int_output_densifies_with_zeros_and_pads(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    # Create an input array that has 5 elements in the first example and 4 in
    # the second. This should output a 2x6 tensor with a padding value in the
    # second example, since output_sequence_length is set to 6.
    input_array = np.array([["earth wind and also fire"],
                            ["fire and earth michigan"]])
    expected_output = [[2, 3, 4, 1, 5, 0], [5, 4, 2, 1, 0, 0]]

    output_sequence_length = 6
    expected_output_shape = [None, output_sequence_length]

    # The input shape here is explicitly 1 because we're tokenizing.
    input_data = keras.Input(shape=(1,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=text_vectorization.WHITESPACE,
        output_mode=text_vectorization.INT,
        output_sequence_length=output_sequence_length)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair102>

<clonepair102>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/text_vectorization_test.py" startline="1019" endline="1053" pcid="5055"></source>
  def test_int_output_dynamically_strips_and_pads(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    # Create an input array that has 5 elements in the first example and 4 in
    # the second. This should output a 2x3 tensor with a padding value in the
    # second example, since output_sequence_length is set to 3.
    input_array = np.array([["earth wind and also fire"],
                            ["fire and earth michigan"]])
    expected_output = [[2, 3, 4], [5, 4, 2]]
    output_sequence_length = 3
    expected_output_shape = [None, output_sequence_length]

    # The input shape here is explicitly 1 because we're tokenizing.
    input_data = keras.Input(shape=(1,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=text_vectorization.WHITESPACE,
        output_mode=text_vectorization.INT,
        output_sequence_length=output_sequence_length)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

    # Create an input array that has 1 element in the first example and 2 in
    # the second. This should output a 2x3 tensor with a padding value in the
    # second example, since output_sequence_length is set to 3.
    input_array_2 = np.array([["wind"], ["fire and"]])
    expected_output_2 = [[3, 0, 0], [5, 4, 0]]
    output_dataset = model.predict(input_array_2)
    self.assertAllEqual(expected_output_2, output_dataset)

</clonepair102>
<clonepair103>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/text_vectorization_test.py" startline="992" endline="1018" pcid="5054"></source>
  def test_int_output_densifies_with_zeros_and_strips(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    # Create an input array that has 5 elements in the first example and 4 in
    # the second. This should output a 2x3 tensor with a padding value in the
    # second example, since output_sequence_length is set to 3.
    input_array = np.array([["earth wind and also fire"],
                            ["fire and earth michigan"]])
    expected_output = [[2, 3, 4], [5, 4, 2]]
    output_sequence_length = 3
    expected_output_shape = [None, output_sequence_length]

    # The input shape here is explicitly 1 because we're tokenizing.
    input_data = keras.Input(shape=(1,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=text_vectorization.WHITESPACE,
        output_mode=text_vectorization.INT,
        output_sequence_length=output_sequence_length)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair103>

<clonepair103>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/text_vectorization_test.py" startline="1019" endline="1053" pcid="5055"></source>
  def test_int_output_dynamically_strips_and_pads(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    # Create an input array that has 5 elements in the first example and 4 in
    # the second. This should output a 2x3 tensor with a padding value in the
    # second example, since output_sequence_length is set to 3.
    input_array = np.array([["earth wind and also fire"],
                            ["fire and earth michigan"]])
    expected_output = [[2, 3, 4], [5, 4, 2]]
    output_sequence_length = 3
    expected_output_shape = [None, output_sequence_length]

    # The input shape here is explicitly 1 because we're tokenizing.
    input_data = keras.Input(shape=(1,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=text_vectorization.WHITESPACE,
        output_mode=text_vectorization.INT,
        output_sequence_length=output_sequence_length)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

    # Create an input array that has 1 element in the first example and 2 in
    # the second. This should output a 2x3 tensor with a padding value in the
    # second example, since output_sequence_length is set to 3.
    input_array_2 = np.array([["wind"], ["fire and"]])
    expected_output_2 = [[3, 0, 0], [5, 4, 0]]
    output_dataset = model.predict(input_array_2)
    self.assertAllEqual(expected_output_2, output_dataset)

</clonepair103>
<clonepair104>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="783" endline="806" pcid="5536"></source>
  def test_int_output_no_oov(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    valid_input = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", ""]])
    invalid_input = np.array([["earth", "wind", "and", "michigan"],
                              ["fire", "and", "earth", "michigan"]])
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=0,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(valid_input)
    self.assertAllEqual(expected_output, output_data)
    with self.assertRaisesRegex(tf.errors.InvalidArgumentError,
                                "found OOV values.*michigan"):
      _ = model.predict(invalid_input)

</clonepair104>

<clonepair104>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="807" endline="832" pcid="5537"></source>
  def test_int_output_no_oov_ragged(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    valid_input = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", ""]])
    invalid_input = np.array([["earth", "wind", "and", "michigan"],
                              ["fire", "and", "earth", "michigan"]])
    valid_input = tf.RaggedTensor.from_tensor(valid_input)
    invalid_input = tf.RaggedTensor.from_tensor(invalid_input)
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=0,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(valid_input)
    self.assertAllEqual(expected_output, output_data)
    with self.assertRaisesRegex(tf.errors.InvalidArgumentError,
                                "found OOV values.*michigan"):
      _ = model.predict(invalid_input)

</clonepair104>
<clonepair105>
<source file="systems/keras-2.8.0/keras/distribute/saved_model_test_base.py" startline="191" endline="219" pcid="326"></source>
  def run_test_save_strategy_restore_no_strategy(self, model_and_input,
                                                 distribution, save_in_scope):
    """Save a model with DS, and restore it without DS."""

    saved_dir = os.path.join(self.get_temp_dir(), '1')

    with distribution.scope():
      model = model_and_input.get_model()
      x_train, y_train, x_predict = model_and_input.get_data()
      batch_size = model_and_input.get_batch_size()

      self._train_model(model, x_train, y_train, batch_size)
      predict_dataset = self._get_predict_dataset(x_predict, batch_size)
      result_before_save = self._predict_with_model(
          distribution, model, predict_dataset)

    if save_in_scope:
      with distribution.scope():
        self._save_model(model, saved_dir)
    else:
      self._save_model(model, saved_dir)

    load_result = self._load_and_run_model(
        distribution=None,
        saved_dir=saved_dir,
        predict_dataset=predict_dataset)

    self.assertAllClose(result_before_save, load_result)

</clonepair105>

<clonepair105>
<source file="systems/keras-2.8.0/keras/distribute/saved_model_test_base.py" startline="220" endline="251" pcid="327"></source>
  def run_test_save_strategy_restore_strategy(self, model_and_input,
                                              distribution_for_saving,
                                              distribution_for_restoring,
                                              save_in_scope):
    """Save a model with DS, and restore it with potentially different DS."""
    saved_dir = os.path.join(self.get_temp_dir(), '2')

    with distribution_for_saving.scope():
      model = model_and_input.get_model()
      x_train, y_train, x_predict = model_and_input.get_data()
      batch_size = model_and_input.get_batch_size()

      self._train_model(model, x_train, y_train, batch_size)
      predict_dataset = self._get_predict_dataset(x_predict, batch_size)
      result_before_save = self._predict_with_model(
          distribution_for_saving, model, predict_dataset)

    if save_in_scope:
      with distribution_for_saving.scope():
        self._save_model(model, saved_dir)
    else:
      self._save_model(model, saved_dir)

    with distribution_for_restoring.scope():

      load_result = self._load_and_run_model(
          distribution=distribution_for_restoring,
          saved_dir=saved_dir,
          predict_dataset=predict_dataset)

    self.assertAllClose(result_before_save, load_result)

</clonepair105>
<clonepair106>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="493" endline="518" pcid="5520"></source>
  def test_sparse_int_input_multi_bucket(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.SparseTensor(
        indices=[[0, 0], [1, 2]],
        values=np.array([13, 133], dtype=np.int64),
        dense_shape=[3, 4])

    expected_indices = [[0, 0], [1, 2]]
    expected_values = [6, 2]
    expected_dense_shape = [3, 4]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, sparse=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        vocabulary_dtype=tf.int64,
        num_oov_indices=2,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array, steps=1)
    self.assertAllEqual(expected_indices, output_data.indices)
    self.assertAllEqual(expected_values, output_data.values)
    self.assertAllEqual(expected_dense_shape, output_data.dense_shape)

</clonepair106>

<clonepair106>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="595" endline="620" pcid="5525"></source>
  def test_sparse_int_input(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.SparseTensor(
        indices=[[0, 0], [1, 2]],
        values=np.array([13, 32], dtype=np.int64),
        dense_shape=[3, 4])

    expected_indices = [[0, 0], [1, 2]]
    expected_values = [5, 1]
    expected_dense_shape = [3, 4]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, sparse=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        vocabulary_dtype=tf.int64,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array, steps=1)
    self.assertAllEqual(expected_indices, output_data.indices)
    self.assertAllEqual(expected_values, output_data.values)
    self.assertAllEqual(expected_dense_shape, output_data.dense_shape)

</clonepair106>
<clonepair107>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="380" endline="405" pcid="5515"></source>
  def test_sparse_int_input(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.SparseTensor(
        indices=[[0, 0], [1, 2]],
        values=np.array([13, 32], dtype=np.int64),
        dense_shape=[3, 4])

    expected_indices = [[0, 0], [1, 2]]
    expected_values = [5, 1]
    expected_dense_shape = [3, 4]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, sparse=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        vocabulary_dtype=tf.int64,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array, steps=1)
    self.assertAllEqual(expected_indices, output_data.indices)
    self.assertAllEqual(expected_values, output_data.values)
    self.assertAllEqual(expected_dense_shape, output_data.dense_shape)

</clonepair107>

<clonepair107>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="493" endline="518" pcid="5520"></source>
  def test_sparse_int_input_multi_bucket(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.SparseTensor(
        indices=[[0, 0], [1, 2]],
        values=np.array([13, 133], dtype=np.int64),
        dense_shape=[3, 4])

    expected_indices = [[0, 0], [1, 2]]
    expected_values = [6, 2]
    expected_dense_shape = [3, 4]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, sparse=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        vocabulary_dtype=tf.int64,
        num_oov_indices=2,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array, steps=1)
    self.assertAllEqual(expected_indices, output_data.indices)
    self.assertAllEqual(expected_values, output_data.values)
    self.assertAllEqual(expected_dense_shape, output_data.dense_shape)

</clonepair107>
<clonepair108>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="354" endline="379" pcid="5514"></source>
  def test_sparse_string_input(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = tf.SparseTensor(
        indices=[[0, 0], [1, 2]],
        values=["fire", "michigan"],
        dense_shape=[3, 4])

    expected_indices = [[0, 0], [1, 2]]
    expected_values = [5, 1]
    expected_dense_shape = [3, 4]

    input_data = keras.Input(shape=(None,), dtype=tf.string, sparse=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array, steps=1)
    self.assertAllEqual(expected_indices, output_data.indices)
    self.assertAllEqual(expected_values, output_data.values)
    self.assertAllEqual(expected_dense_shape, output_data.dense_shape)

</clonepair108>

<clonepair108>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="493" endline="518" pcid="5520"></source>
  def test_sparse_int_input_multi_bucket(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.SparseTensor(
        indices=[[0, 0], [1, 2]],
        values=np.array([13, 133], dtype=np.int64),
        dense_shape=[3, 4])

    expected_indices = [[0, 0], [1, 2]]
    expected_values = [6, 2]
    expected_dense_shape = [3, 4]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, sparse=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        vocabulary_dtype=tf.int64,
        num_oov_indices=2,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array, steps=1)
    self.assertAllEqual(expected_indices, output_data.indices)
    self.assertAllEqual(expected_values, output_data.values)
    self.assertAllEqual(expected_dense_shape, output_data.dense_shape)

</clonepair108>
<clonepair109>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="380" endline="405" pcid="5515"></source>
  def test_sparse_int_input(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.SparseTensor(
        indices=[[0, 0], [1, 2]],
        values=np.array([13, 32], dtype=np.int64),
        dense_shape=[3, 4])

    expected_indices = [[0, 0], [1, 2]]
    expected_values = [5, 1]
    expected_dense_shape = [3, 4]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, sparse=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        vocabulary_dtype=tf.int64,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array, steps=1)
    self.assertAllEqual(expected_indices, output_data.indices)
    self.assertAllEqual(expected_values, output_data.values)
    self.assertAllEqual(expected_dense_shape, output_data.dense_shape)

</clonepair109>

<clonepair109>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="595" endline="620" pcid="5525"></source>
  def test_sparse_int_input(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.SparseTensor(
        indices=[[0, 0], [1, 2]],
        values=np.array([13, 32], dtype=np.int64),
        dense_shape=[3, 4])

    expected_indices = [[0, 0], [1, 2]]
    expected_values = [5, 1]
    expected_dense_shape = [3, 4]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, sparse=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        vocabulary_dtype=tf.int64,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array, steps=1)
    self.assertAllEqual(expected_indices, output_data.indices)
    self.assertAllEqual(expected_values, output_data.values)
    self.assertAllEqual(expected_dense_shape, output_data.dense_shape)

</clonepair109>
<clonepair110>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="354" endline="379" pcid="5514"></source>
  def test_sparse_string_input(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = tf.SparseTensor(
        indices=[[0, 0], [1, 2]],
        values=["fire", "michigan"],
        dense_shape=[3, 4])

    expected_indices = [[0, 0], [1, 2]]
    expected_values = [5, 1]
    expected_dense_shape = [3, 4]

    input_data = keras.Input(shape=(None,), dtype=tf.string, sparse=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array, steps=1)
    self.assertAllEqual(expected_indices, output_data.indices)
    self.assertAllEqual(expected_values, output_data.values)
    self.assertAllEqual(expected_dense_shape, output_data.dense_shape)

</clonepair110>

<clonepair110>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="595" endline="620" pcid="5525"></source>
  def test_sparse_int_input(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.SparseTensor(
        indices=[[0, 0], [1, 2]],
        values=np.array([13, 32], dtype=np.int64),
        dense_shape=[3, 4])

    expected_indices = [[0, 0], [1, 2]]
    expected_values = [5, 1]
    expected_dense_shape = [3, 4]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, sparse=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        vocabulary_dtype=tf.int64,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array, steps=1)
    self.assertAllEqual(expected_indices, output_data.indices)
    self.assertAllEqual(expected_values, output_data.values)
    self.assertAllEqual(expected_dense_shape, output_data.dense_shape)

</clonepair110>
<clonepair111>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="354" endline="379" pcid="5514"></source>
  def test_sparse_string_input(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = tf.SparseTensor(
        indices=[[0, 0], [1, 2]],
        values=["fire", "michigan"],
        dense_shape=[3, 4])

    expected_indices = [[0, 0], [1, 2]]
    expected_values = [5, 1]
    expected_dense_shape = [3, 4]

    input_data = keras.Input(shape=(None,), dtype=tf.string, sparse=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array, steps=1)
    self.assertAllEqual(expected_indices, output_data.indices)
    self.assertAllEqual(expected_values, output_data.values)
    self.assertAllEqual(expected_dense_shape, output_data.dense_shape)

</clonepair111>

<clonepair111>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="380" endline="405" pcid="5515"></source>
  def test_sparse_int_input(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.SparseTensor(
        indices=[[0, 0], [1, 2]],
        values=np.array([13, 32], dtype=np.int64),
        dense_shape=[3, 4])

    expected_indices = [[0, 0], [1, 2]]
    expected_values = [5, 1]
    expected_dense_shape = [3, 4]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, sparse=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        vocabulary_dtype=tf.int64,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array, steps=1)
    self.assertAllEqual(expected_indices, output_data.indices)
    self.assertAllEqual(expected_values, output_data.values)
    self.assertAllEqual(expected_dense_shape, output_data.dense_shape)

</clonepair111>
<clonepair112>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="354" endline="379" pcid="5514"></source>
  def test_sparse_string_input(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = tf.SparseTensor(
        indices=[[0, 0], [1, 2]],
        values=["fire", "michigan"],
        dense_shape=[3, 4])

    expected_indices = [[0, 0], [1, 2]]
    expected_values = [5, 1]
    expected_dense_shape = [3, 4]

    input_data = keras.Input(shape=(None,), dtype=tf.string, sparse=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array, steps=1)
    self.assertAllEqual(expected_indices, output_data.indices)
    self.assertAllEqual(expected_values, output_data.values)
    self.assertAllEqual(expected_dense_shape, output_data.dense_shape)

</clonepair112>

<clonepair112>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="469" endline="492" pcid="5519"></source>
  def test_sparse_string_input_multi_bucket(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = tf.SparseTensor(
        indices=[[0, 0], [1, 2]], values=["fire", "ohio"], dense_shape=[3, 4])

    expected_indices = [[0, 0], [1, 2]]
    expected_values = [6, 2]
    expected_dense_shape = [3, 4]

    input_data = keras.Input(shape=(None,), dtype=tf.string, sparse=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=2,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array, steps=1)
    self.assertAllEqual(expected_indices, output_data.indices)
    self.assertAllEqual(expected_values, output_data.values)
    self.assertAllEqual(expected_dense_shape, output_data.dense_shape)

</clonepair112>
<clonepair113>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="783" endline="806" pcid="5536"></source>
  def test_int_output_no_oov(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    valid_input = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", ""]])
    invalid_input = np.array([["earth", "wind", "and", "michigan"],
                              ["fire", "and", "earth", "michigan"]])
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=0,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(valid_input)
    self.assertAllEqual(expected_output, output_data)
    with self.assertRaisesRegex(tf.errors.InvalidArgumentError,
                                "found OOV values.*michigan"):
      _ = model.predict(invalid_input)

</clonepair113>

<clonepair113>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1097" endline="1117" pcid="5547"></source>
  def test_int_output_file_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 0, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair113>
<clonepair114>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="783" endline="806" pcid="5536"></source>
  def test_int_output_no_oov(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    valid_input = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", ""]])
    invalid_input = np.array([["earth", "wind", "and", "michigan"],
                              ["fire", "and", "earth", "michigan"]])
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=0,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(valid_input)
    self.assertAllEqual(expected_output, output_data)
    with self.assertRaisesRegex(tf.errors.InvalidArgumentError,
                                "found OOV values.*michigan"):
      _ = model.predict(invalid_input)

</clonepair114>

<clonepair114>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1365" endline="1383" pcid="5560"></source>
  def test_int_output_explicit_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair114>
<clonepair115>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="783" endline="806" pcid="5536"></source>
  def test_int_output_no_oov(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    valid_input = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", ""]])
    invalid_input = np.array([["earth", "wind", "and", "michigan"],
                              ["fire", "and", "earth", "michigan"]])
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=0,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(valid_input)
    self.assertAllEqual(expected_output, output_data)
    with self.assertRaisesRegex(tf.errors.InvalidArgumentError,
                                "found OOV values.*michigan"):
      _ = model.predict(invalid_input)

</clonepair115>

<clonepair115>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="860" endline="878" pcid="5539"></source>
  def test_int_output_explicit_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair115>
<clonepair116>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="764" endline="782" pcid="5535"></source>
  def test_int_output_no_reserved_zero(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token=None,
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair116>

<clonepair116>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="783" endline="806" pcid="5536"></source>
  def test_int_output_no_oov(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    valid_input = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", ""]])
    invalid_input = np.array([["earth", "wind", "and", "michigan"],
                              ["fire", "and", "earth", "michigan"]])
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=0,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(valid_input)
    self.assertAllEqual(expected_output, output_data)
    with self.assertRaisesRegex(tf.errors.InvalidArgumentError,
                                "found OOV values.*michigan"):
      _ = model.predict(invalid_input)

</clonepair116>
<clonepair117>
<source file="systems/keras-2.8.0/keras/integration_test/preprocessing_applied_in_model_test.py" startline="49" endline="76" pcid="1729"></source>
  def testDistributedModelFit(self, strategy):
    if (not tf.__internal__.tf2.enabled()
        and isinstance(strategy,
                       tf.distribute.experimental.ParameterServerStrategy)):
      self.skipTest(
          "Parameter Server strategy with dataset creator need to be run when "
          "eager execution is enabled.")
    with strategy.scope():
      preprocessing_model = utils.make_preprocessing_model(self.get_temp_dir())
      training_model = utils.make_training_model()
      # Merge the two separate models into a single model for training.
      inputs = preprocessing_model.inputs
      outputs = training_model(preprocessing_model(inputs))
      merged_model = tf.keras.Model(inputs, outputs)
      merged_model.compile(optimizer="sgd", loss="binary_crossentropy")

    def dataset_fn(input_context):
      dataset = utils.make_dataset()
      dataset = dataset.shard(input_context.num_input_pipelines,
                              input_context.input_pipeline_id)
      batch_size = input_context.get_per_replica_batch_size(
          global_batch_size=utils.BATCH_SIZE)
      return dataset.batch(batch_size).repeat().prefetch(2)

    dataset_creator = tf.keras.utils.experimental.DatasetCreator(dataset_fn)
    merged_model.fit(dataset_creator, epochs=2, steps_per_epoch=utils.STEPS)


</clonepair117>

<clonepair117>
<source file="systems/keras-2.8.0/keras/integration_test/preprocessing_applied_in_dataset_creator_test.py" startline="48" endline="72" pcid="1818"></source>
  def testDistributedModelFit(self, strategy):
    if (not tf.__internal__.tf2.enabled()
        and isinstance(strategy,
                       tf.distribute.experimental.ParameterServerStrategy)):
      self.skipTest(
          "Parameter Server strategy with dataset creator need to be run when "
          "eager execution is enabled.")
    with strategy.scope():
      preprocessing_model = utils.make_preprocessing_model(self.get_temp_dir())
      training_model = utils.make_training_model()
      training_model.compile(optimizer="sgd", loss="binary_crossentropy")

    def dataset_fn(input_context):
      dataset = utils.make_dataset()
      dataset = dataset.shard(input_context.num_input_pipelines,
                              input_context.input_pipeline_id)
      batch_size = input_context.get_per_replica_batch_size(
          global_batch_size=utils.BATCH_SIZE)
      dataset = dataset.batch(batch_size).repeat().prefetch(2)
      return dataset.map(lambda x, y: (preprocessing_model(x), y))

    dataset_creator = tf.keras.utils.experimental.DatasetCreator(dataset_fn)
    training_model.fit(dataset_creator, epochs=2, steps_per_epoch=utils.STEPS)


</clonepair117>
<clonepair118>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/string_lookup_test.py" startline="70" endline="106" pcid="5191"></source>
  def test_layer_end_to_end_with_adapt(self, vocab_data, input_data, kwargs,
                                       use_dataset, expected_output,
                                       input_dtype):
    cls = string_lookup.StringLookup
    expected_output_dtype = tf.int64
    input_shape = input_data.shape

    if use_dataset:
      # Keras APIs expect batched datasets.
      # TODO(rachelim): `model.predict` predicts the result on each
      # dataset batch separately, then tries to concatenate the results
      # together. When the results have different shapes on the non-concat
      # axis (which can happen in the output_mode = INT case for
      # StringLookup), the concatenation fails. In real use cases, this may
      # not be an issue because users are likely to pipe the preprocessing layer
      # into other keras layers instead of predicting it directly. A workaround
      # for these unit tests is to have the dataset only contain one batch, so
      # no concatenation needs to happen with the result. For consistency with
      # numpy input, we should make `predict` join differently shaped results
      # together sensibly, with 0 padding.
      input_data = tf.data.Dataset.from_tensor_slices(input_data).batch(
          input_shape[0])
      vocab_data = tf.data.Dataset.from_tensor_slices(vocab_data).batch(
          input_shape[0])

    output_data = testing_utils.layer_test(
        cls,
        kwargs=kwargs,
        input_shape=input_shape,
        input_data=input_data,
        input_dtype=input_dtype,
        expected_output_dtype=expected_output_dtype,
        validate_training=False,
        adapt_data=vocab_data)
    self.assertAllClose(expected_output, output_data)


</clonepair118>

<clonepair118>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/integer_lookup_test.py" startline="76" endline="111" pcid="4970"></source>
  def test_layer_end_to_end_with_adapt(self, vocab_data, input_data, kwargs,
                                       use_dataset, expected_output,
                                       input_dtype):
    cls = integer_lookup.IntegerLookup
    expected_output_dtype = tf.int64
    input_shape = input_data.shape

    if use_dataset:
      # Keras APIs expect batched datasets.
      # TODO(rachelim): `model.predict` predicts the result on each
      # dataset batch separately, then tries to concatenate the results
      # together. When the results have different shapes on the non-concat
      # axis (which can happen in the output_mode = INT case for
      # IntegerLookup), the concatenation fails. In real use cases, this may
      # not be an issue because users are likely to pipe the preprocessing layer
      # into other keras layers instead of predicting it directly. A workaround
      # for these unit tests is to have the dataset only contain one batch, so
      # no concatenation needs to happen with the result. For consistency with
      # numpy input, we should make `predict` join differently shaped results
      # together sensibly, with 0 padding.
      input_data = tf.data.Dataset.from_tensor_slices(input_data).batch(
          input_shape[0])
      vocab_data = tf.data.Dataset.from_tensor_slices(vocab_data).batch(
          input_shape[0])

    output_data = testing_utils.layer_test(
        cls,
        kwargs=kwargs,
        input_shape=input_shape,
        input_data=input_data,
        input_dtype=input_dtype,
        expected_output_dtype=expected_output_dtype,
        validate_training=False,
        adapt_data=vocab_data)
    self.assertAllClose(expected_output, output_data)

</clonepair118>
<clonepair119>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/text_vectorization_test.py" startline="964" endline="991" pcid="5053"></source>
  def test_int_output_densifies_with_zeros_and_pads(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    # Create an input array that has 5 elements in the first example and 4 in
    # the second. This should output a 2x6 tensor with a padding value in the
    # second example, since output_sequence_length is set to 6.
    input_array = np.array([["earth wind and also fire"],
                            ["fire and earth michigan"]])
    expected_output = [[2, 3, 4, 1, 5, 0], [5, 4, 2, 1, 0, 0]]

    output_sequence_length = 6
    expected_output_shape = [None, output_sequence_length]

    # The input shape here is explicitly 1 because we're tokenizing.
    input_data = keras.Input(shape=(1,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=text_vectorization.WHITESPACE,
        output_mode=text_vectorization.INT,
        output_sequence_length=output_sequence_length)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair119>

<clonepair119>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/text_vectorization_test.py" startline="992" endline="1018" pcid="5054"></source>
  def test_int_output_densifies_with_zeros_and_strips(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    # Create an input array that has 5 elements in the first example and 4 in
    # the second. This should output a 2x3 tensor with a padding value in the
    # second example, since output_sequence_length is set to 3.
    input_array = np.array([["earth wind and also fire"],
                            ["fire and earth michigan"]])
    expected_output = [[2, 3, 4], [5, 4, 2]]
    output_sequence_length = 3
    expected_output_shape = [None, output_sequence_length]

    # The input shape here is explicitly 1 because we're tokenizing.
    input_data = keras.Input(shape=(1,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=text_vectorization.WHITESPACE,
        output_mode=text_vectorization.INT,
        output_sequence_length=output_sequence_length)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair119>
<clonepair120>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/text_vectorization_test.py" startline="911" endline="938" pcid="5051"></source>
  def test_int_output_densifies_with_zeros(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    # Create an input array that has 5 elements in the first example and 4 in
    # the second. This should output a 2x5 tensor with a padding value in the
    # second example.
    input_array = np.array([["earth wind and also fire"],
                            ["fire and earth michigan"]])
    expected_output = [[2, 3, 4, 1, 5], [5, 4, 2, 1, 0]]

    # This test doesn't explicitly set an output shape, so the 2nd dimension
    # should stay 'None'.
    expected_output_shape = [None, None]

    # The input shape here is explicitly 1 because we're tokenizing.
    input_data = keras.Input(shape=(1,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=text_vectorization.WHITESPACE,
        output_mode=text_vectorization.INT)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair120>

<clonepair120>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/text_vectorization_test.py" startline="964" endline="991" pcid="5053"></source>
  def test_int_output_densifies_with_zeros_and_pads(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    # Create an input array that has 5 elements in the first example and 4 in
    # the second. This should output a 2x6 tensor with a padding value in the
    # second example, since output_sequence_length is set to 6.
    input_array = np.array([["earth wind and also fire"],
                            ["fire and earth michigan"]])
    expected_output = [[2, 3, 4, 1, 5, 0], [5, 4, 2, 1, 0, 0]]

    output_sequence_length = 6
    expected_output_shape = [None, output_sequence_length]

    # The input shape here is explicitly 1 because we're tokenizing.
    input_data = keras.Input(shape=(1,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=text_vectorization.WHITESPACE,
        output_mode=text_vectorization.INT,
        output_sequence_length=output_sequence_length)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair120>
<clonepair121>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/text_vectorization_test.py" startline="911" endline="938" pcid="5051"></source>
  def test_int_output_densifies_with_zeros(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    # Create an input array that has 5 elements in the first example and 4 in
    # the second. This should output a 2x5 tensor with a padding value in the
    # second example.
    input_array = np.array([["earth wind and also fire"],
                            ["fire and earth michigan"]])
    expected_output = [[2, 3, 4, 1, 5], [5, 4, 2, 1, 0]]

    # This test doesn't explicitly set an output shape, so the 2nd dimension
    # should stay 'None'.
    expected_output_shape = [None, None]

    # The input shape here is explicitly 1 because we're tokenizing.
    input_data = keras.Input(shape=(1,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=text_vectorization.WHITESPACE,
        output_mode=text_vectorization.INT)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair121>

<clonepair121>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/text_vectorization_test.py" startline="992" endline="1018" pcid="5054"></source>
  def test_int_output_densifies_with_zeros_and_strips(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    # Create an input array that has 5 elements in the first example and 4 in
    # the second. This should output a 2x3 tensor with a padding value in the
    # second example, since output_sequence_length is set to 3.
    input_array = np.array([["earth wind and also fire"],
                            ["fire and earth michigan"]])
    expected_output = [[2, 3, 4], [5, 4, 2]]
    output_sequence_length = 3
    expected_output_shape = [None, output_sequence_length]

    # The input shape here is explicitly 1 because we're tokenizing.
    input_data = keras.Input(shape=(1,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=text_vectorization.WHITESPACE,
        output_mode=text_vectorization.INT,
        output_sequence_length=output_sequence_length)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair121>
<clonepair122>
<source file="systems/keras-2.8.0/keras/optimizer_v2/rmsprop.py" startline="90" endline="145" pcid="193"></source>
  def __init__(self,
               learning_rate=0.001,
               rho=0.9,
               momentum=0.0,
               epsilon=1e-7,
               centered=False,
               name="RMSprop",
               **kwargs):
    """Construct a new RMSprop optimizer.

    Args:
      learning_rate: A `Tensor`, floating point value, or a schedule that is a
        `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable
        that takes no arguments and returns the actual value to use. The
        learning rate. Defaults to 0.001.
      rho: Discounting factor for the history/coming gradient. Defaults to 0.9.
      momentum: A scalar or a scalar `Tensor`. Defaults to 0.0.
      epsilon: A small constant for numerical stability. This epsilon is
        "epsilon hat" in the Kingma and Ba paper (in the formula just before
        Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to
        1e-7.
      centered: Boolean. If `True`, gradients are normalized by the estimated
        variance of the gradient; if False, by the uncentered second moment.
        Setting this to `True` may help with training, but is slightly more
        expensive in terms of computation and memory. Defaults to `False`.
      name: Optional name prefix for the operations created when applying
        gradients. Defaults to "RMSprop".
      **kwargs: keyword arguments. Allowed to be {`clipnorm`, `clipvalue`, `lr`,
        `decay`}. `clipnorm` is clip gradients by norm; `clipvalue` is clip
        gradients by value, `decay` is included for backward compatibility to
        allow time inverse decay of learning rate. `lr` is included for backward
        compatibility, recommended to use `learning_rate` instead.

    @compatibility(eager)
    When eager execution is enabled, `learning_rate`, `decay`, `momentum`, and
    `epsilon` can each be a callable that takes no arguments and returns the
    actual value to use. This can be useful for changing these values across
    different invocations of optimizer functions.
    @end_compatibility
    """
    super(RMSprop, self).__init__(name, **kwargs)
    self._set_hyper("learning_rate", kwargs.get("lr", learning_rate))
    self._set_hyper("decay", self._initial_decay)
    self._set_hyper("rho", rho)

    self._momentum = False
    if isinstance(momentum, tf.Tensor) or callable(momentum) or momentum > 0:
      self._momentum = True
    if isinstance(momentum, (int, float)) and (momentum < 0 or momentum > 1):
      raise ValueError(f"`momentum` must be between [0, 1]. Received: "
                       f"momentum={momentum} (of type {type(momentum)}).")
    self._set_hyper("momentum", momentum)

    self.epsilon = epsilon or backend_config.epsilon()
    self.centered = centered

</clonepair122>

<clonepair122>
<source file="systems/keras-2.8.0/keras/optimizer_v2/gradient_descent.py" startline="96" endline="115" pcid="207"></source>
  def __init__(self,
               learning_rate=0.01,
               momentum=0.0,
               nesterov=False,
               name="SGD",
               **kwargs):
    super(SGD, self).__init__(name, **kwargs)
    self._set_hyper("learning_rate", kwargs.get("lr", learning_rate))
    self._set_hyper("decay", self._initial_decay)

    self._momentum = False
    if isinstance(momentum, tf.Tensor) or callable(momentum) or momentum > 0:
      self._momentum = True
    if isinstance(momentum, (int, float)) and (momentum < 0 or momentum > 1):
      raise ValueError(f"`momentum` must be between [0, 1]. Received: "
                       f"momentum={momentum} (of type {type(momentum)}).")
    self._set_hyper("momentum", momentum)

    self.nesterov = nesterov

</clonepair122>
<clonepair123>
<source file="systems/keras-2.8.0/keras/optimizer_v2/rmsprop.py" startline="90" endline="145" pcid="193"></source>
  def __init__(self,
               learning_rate=0.001,
               rho=0.9,
               momentum=0.0,
               epsilon=1e-7,
               centered=False,
               name="RMSprop",
               **kwargs):
    """Construct a new RMSprop optimizer.

    Args:
      learning_rate: A `Tensor`, floating point value, or a schedule that is a
        `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable
        that takes no arguments and returns the actual value to use. The
        learning rate. Defaults to 0.001.
      rho: Discounting factor for the history/coming gradient. Defaults to 0.9.
      momentum: A scalar or a scalar `Tensor`. Defaults to 0.0.
      epsilon: A small constant for numerical stability. This epsilon is
        "epsilon hat" in the Kingma and Ba paper (in the formula just before
        Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to
        1e-7.
      centered: Boolean. If `True`, gradients are normalized by the estimated
        variance of the gradient; if False, by the uncentered second moment.
        Setting this to `True` may help with training, but is slightly more
        expensive in terms of computation and memory. Defaults to `False`.
      name: Optional name prefix for the operations created when applying
        gradients. Defaults to "RMSprop".
      **kwargs: keyword arguments. Allowed to be {`clipnorm`, `clipvalue`, `lr`,
        `decay`}. `clipnorm` is clip gradients by norm; `clipvalue` is clip
        gradients by value, `decay` is included for backward compatibility to
        allow time inverse decay of learning rate. `lr` is included for backward
        compatibility, recommended to use `learning_rate` instead.

    @compatibility(eager)
    When eager execution is enabled, `learning_rate`, `decay`, `momentum`, and
    `epsilon` can each be a callable that takes no arguments and returns the
    actual value to use. This can be useful for changing these values across
    different invocations of optimizer functions.
    @end_compatibility
    """
    super(RMSprop, self).__init__(name, **kwargs)
    self._set_hyper("learning_rate", kwargs.get("lr", learning_rate))
    self._set_hyper("decay", self._initial_decay)
    self._set_hyper("rho", rho)

    self._momentum = False
    if isinstance(momentum, tf.Tensor) or callable(momentum) or momentum > 0:
      self._momentum = True
    if isinstance(momentum, (int, float)) and (momentum < 0 or momentum > 1):
      raise ValueError(f"`momentum` must be between [0, 1]. Received: "
                       f"momentum={momentum} (of type {type(momentum)}).")
    self._set_hyper("momentum", momentum)

    self.epsilon = epsilon or backend_config.epsilon()
    self.centered = centered

</clonepair123>

<clonepair123>
<source file="systems/keras-2.8.0/keras/optimizer_v2/adam.py" startline="321" endline="364" pcid="11"></source>
  def __init__(self,
               learning_rate=0.001,
               beta_1=0.9,
               beta_2=0.999,
               epsilon=1e-7,
               amsgrad=False,
               name='Adam',
               **kwargs):
    """Construct a new Adam optimizer.

    Args:
      learning_rate: A `Tensor`, floating point value, or a schedule that is a
        `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable that
        takes no arguments and returns the actual value to use, The learning
        rate. Defaults to 0.001.
      beta_1: A float value or a constant float tensor, or a callable that takes
        no arguments and returns the actual value to use. The exponential decay
        rate for the 1st moment estimates. Defaults to 0.9.
      beta_2: A float value or a constant float tensor, or a callable that takes
        no arguments and returns the actual value to use, The exponential decay
        rate for the 2nd moment estimates. Defaults to 0.999.
      epsilon: A small constant for numerical stability. This epsilon is
        "epsilon hat" in the Kingma and Ba paper (in the formula just before
        Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to
        1e-7.
      amsgrad: Boolean. Whether to apply AMSGrad variant of this algorithm from
        the paper "On the Convergence of Adam and beyond". Defaults to `False`.
      name: Optional name for the operations created when applying gradients.
        Defaults to "Adam".
      **kwargs: keyword arguments. Allowed to be {`clipnorm`, `clipvalue`, `lr`,
        `decay`}. `clipnorm` is clip gradients by norm; `clipvalue` is clip
        gradients by value, `decay` is included for backward compatibility to
        allow time inverse decay of learning rate. `lr` is included for backward
        compatibility, recommended to use `learning_rate` instead.
    """

    super(NonFusedAdam, self).__init__(name, **kwargs)
    self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))
    self._set_hyper('decay', self._initial_decay)
    self._set_hyper('beta_1', beta_1)
    self._set_hyper('beta_2', beta_2)
    self.epsilon = epsilon or backend_config.epsilon()
    self.amsgrad = amsgrad

</clonepair123>
<clonepair124>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1236" endline="1256" pcid="5553"></source>
  def test_int_output_file_vocab_no_mask(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "", "earth", "michigan"]])
    expected_output = [[1, 2, 3, 4], [4, 0, 1, 0]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        mask_token=None,
        num_oov_indices=1,
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair124>

<clonepair124>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1257" endline="1277" pcid="5554"></source>
  def test_int_output_file_vocab_no_oov_or_mask(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "wind", "earth", "and"]])
    expected_output = [[0, 1, 2, 3], [3, 1, 0, 2]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        mask_token=None,
        num_oov_indices=0,
        oov_token=None,
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair124>
<clonepair125>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1097" endline="1117" pcid="5547"></source>
  def test_int_output_file_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 0, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair125>

<clonepair125>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1257" endline="1277" pcid="5554"></source>
  def test_int_output_file_vocab_no_oov_or_mask(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "wind", "earth", "and"]])
    expected_output = [[0, 1, 2, 3], [3, 1, 0, 2]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        mask_token=None,
        num_oov_indices=0,
        oov_token=None,
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair125>
<clonepair126>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1257" endline="1277" pcid="5554"></source>
  def test_int_output_file_vocab_no_oov_or_mask(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "wind", "earth", "and"]])
    expected_output = [[0, 1, 2, 3], [3, 1, 0, 2]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        mask_token=None,
        num_oov_indices=0,
        oov_token=None,
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair126>

<clonepair126>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1384" endline="1402" pcid="5561"></source>
  def test_int_output_explicit_vocab_with_special_tokens(self):
    vocab_data = ["", "[OOV]", "earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair126>
<clonepair127>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1257" endline="1277" pcid="5554"></source>
  def test_int_output_file_vocab_no_oov_or_mask(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "wind", "earth", "and"]])
    expected_output = [[0, 1, 2, 3], [3, 1, 0, 2]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        mask_token=None,
        num_oov_indices=0,
        oov_token=None,
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair127>

<clonepair127>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1365" endline="1383" pcid="5560"></source>
  def test_int_output_explicit_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair127>
<clonepair128>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1257" endline="1277" pcid="5554"></source>
  def test_int_output_file_vocab_no_oov_or_mask(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "wind", "earth", "and"]])
    expected_output = [[0, 1, 2, 3], [3, 1, 0, 2]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        mask_token=None,
        num_oov_indices=0,
        oov_token=None,
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair128>

<clonepair128>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1310" endline="1328" pcid="5556"></source>
  def test_int_output_int_file_vocab(self):
    vocab_data = ["10", "20", "30", "40"]
    input_array = np.array([[10, 20, 30, 40], [40, 0, 10, 42]])
    expected_output = [[2, 3, 4, 5], [5, 0, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)
    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1,
        vocabulary_dtype=tf.int64)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair128>
<clonepair129>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="860" endline="878" pcid="5539"></source>
  def test_int_output_explicit_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair129>

<clonepair129>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1257" endline="1277" pcid="5554"></source>
  def test_int_output_file_vocab_no_oov_or_mask(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "wind", "earth", "and"]])
    expected_output = [[0, 1, 2, 3], [3, 1, 0, 2]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        mask_token=None,
        num_oov_indices=0,
        oov_token=None,
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair129>
<clonepair130>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="764" endline="782" pcid="5535"></source>
  def test_int_output_no_reserved_zero(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token=None,
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair130>

<clonepair130>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1257" endline="1277" pcid="5554"></source>
  def test_int_output_file_vocab_no_oov_or_mask(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "wind", "earth", "and"]])
    expected_output = [[0, 1, 2, 3], [3, 1, 0, 2]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        mask_token=None,
        num_oov_indices=0,
        oov_token=None,
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair130>
<clonepair131>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1097" endline="1117" pcid="5547"></source>
  def test_int_output_file_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 0, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair131>

<clonepair131>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1236" endline="1256" pcid="5553"></source>
  def test_int_output_file_vocab_no_mask(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "", "earth", "michigan"]])
    expected_output = [[1, 2, 3, 4], [4, 0, 1, 0]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        mask_token=None,
        num_oov_indices=1,
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair131>
<clonepair132>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1236" endline="1256" pcid="5553"></source>
  def test_int_output_file_vocab_no_mask(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "", "earth", "michigan"]])
    expected_output = [[1, 2, 3, 4], [4, 0, 1, 0]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        mask_token=None,
        num_oov_indices=1,
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair132>

<clonepair132>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1648" endline="1667" pcid="5581"></source>
  def test_int_output_explicit_vocab(self):
    vocab_data = ["", "[OOV]", "earth", "wind", "and", "fire"]
    input_array = np.array([[2, 3, 4, 5], [5, 4, 2, 1]])
    expected_output = np.array([["earth", "wind", "and", "fire"],
                                ["fire", "and", "earth", "[OOV]"]])

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string,
        invert=True)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair132>
<clonepair133>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1236" endline="1256" pcid="5553"></source>
  def test_int_output_file_vocab_no_mask(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "", "earth", "michigan"]])
    expected_output = [[1, 2, 3, 4], [4, 0, 1, 0]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        mask_token=None,
        num_oov_indices=1,
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair133>

<clonepair133>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1384" endline="1402" pcid="5561"></source>
  def test_int_output_explicit_vocab_with_special_tokens(self):
    vocab_data = ["", "[OOV]", "earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair133>
<clonepair134>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1236" endline="1256" pcid="5553"></source>
  def test_int_output_file_vocab_no_mask(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "", "earth", "michigan"]])
    expected_output = [[1, 2, 3, 4], [4, 0, 1, 0]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        mask_token=None,
        num_oov_indices=1,
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair134>

<clonepair134>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1365" endline="1383" pcid="5560"></source>
  def test_int_output_explicit_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair134>
<clonepair135>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1236" endline="1256" pcid="5553"></source>
  def test_int_output_file_vocab_no_mask(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "", "earth", "michigan"]])
    expected_output = [[1, 2, 3, 4], [4, 0, 1, 0]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        mask_token=None,
        num_oov_indices=1,
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair135>

<clonepair135>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1310" endline="1328" pcid="5556"></source>
  def test_int_output_int_file_vocab(self):
    vocab_data = ["10", "20", "30", "40"]
    input_array = np.array([[10, 20, 30, 40], [40, 0, 10, 42]])
    expected_output = [[2, 3, 4, 5], [5, 0, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)
    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1,
        vocabulary_dtype=tf.int64)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair135>
<clonepair136>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="860" endline="878" pcid="5539"></source>
  def test_int_output_explicit_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair136>

<clonepair136>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1236" endline="1256" pcid="5553"></source>
  def test_int_output_file_vocab_no_mask(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "", "earth", "michigan"]])
    expected_output = [[1, 2, 3, 4], [4, 0, 1, 0]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        mask_token=None,
        num_oov_indices=1,
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair136>
<clonepair137>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="764" endline="782" pcid="5535"></source>
  def test_int_output_no_reserved_zero(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token=None,
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair137>

<clonepair137>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1236" endline="1256" pcid="5553"></source>
  def test_int_output_file_vocab_no_mask(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "", "earth", "michigan"]])
    expected_output = [[1, 2, 3, 4], [4, 0, 1, 0]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        mask_token=None,
        num_oov_indices=1,
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair137>
<clonepair138>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1097" endline="1117" pcid="5547"></source>
  def test_int_output_file_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 0, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair138>

<clonepair138>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1648" endline="1667" pcid="5581"></source>
  def test_int_output_explicit_vocab(self):
    vocab_data = ["", "[OOV]", "earth", "wind", "and", "fire"]
    input_array = np.array([[2, 3, 4, 5], [5, 4, 2, 1]])
    expected_output = np.array([["earth", "wind", "and", "fire"],
                                ["fire", "and", "earth", "[OOV]"]])

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string,
        invert=True)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair138>
<clonepair139>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1097" endline="1117" pcid="5547"></source>
  def test_int_output_file_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 0, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair139>

<clonepair139>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1384" endline="1402" pcid="5561"></source>
  def test_int_output_explicit_vocab_with_special_tokens(self):
    vocab_data = ["", "[OOV]", "earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair139>
<clonepair140>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1097" endline="1117" pcid="5547"></source>
  def test_int_output_file_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 0, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair140>

<clonepair140>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1365" endline="1383" pcid="5560"></source>
  def test_int_output_explicit_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair140>
<clonepair141>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1097" endline="1117" pcid="5547"></source>
  def test_int_output_file_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 0, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair141>

<clonepair141>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1310" endline="1328" pcid="5556"></source>
  def test_int_output_int_file_vocab(self):
    vocab_data = ["10", "20", "30", "40"]
    input_array = np.array([[10, 20, 30, 40], [40, 0, 10, 42]])
    expected_output = [[2, 3, 4, 5], [5, 0, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)
    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1,
        vocabulary_dtype=tf.int64)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair141>
<clonepair142>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="860" endline="878" pcid="5539"></source>
  def test_int_output_explicit_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair142>

<clonepair142>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1097" endline="1117" pcid="5547"></source>
  def test_int_output_file_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 0, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair142>
<clonepair143>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="764" endline="782" pcid="5535"></source>
  def test_int_output_no_reserved_zero(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token=None,
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair143>

<clonepair143>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1097" endline="1117" pcid="5547"></source>
  def test_int_output_file_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 0, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair143>
<clonepair144>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1384" endline="1402" pcid="5561"></source>
  def test_int_output_explicit_vocab_with_special_tokens(self):
    vocab_data = ["", "[OOV]", "earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair144>

<clonepair144>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1648" endline="1667" pcid="5581"></source>
  def test_int_output_explicit_vocab(self):
    vocab_data = ["", "[OOV]", "earth", "wind", "and", "fire"]
    input_array = np.array([[2, 3, 4, 5], [5, 4, 2, 1]])
    expected_output = np.array([["earth", "wind", "and", "fire"],
                                ["fire", "and", "earth", "[OOV]"]])

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string,
        invert=True)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair144>
<clonepair145>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="860" endline="878" pcid="5539"></source>
  def test_int_output_explicit_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair145>

<clonepair145>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1648" endline="1667" pcid="5581"></source>
  def test_int_output_explicit_vocab(self):
    vocab_data = ["", "[OOV]", "earth", "wind", "and", "fire"]
    input_array = np.array([[2, 3, 4, 5], [5, 4, 2, 1]])
    expected_output = np.array([["earth", "wind", "and", "fire"],
                                ["fire", "and", "earth", "[OOV]"]])

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string,
        invert=True)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair145>
<clonepair146>
<source file="systems/keras-2.8.0/keras/metrics_confusion_matrix_test.py" startline="860" endline="882" pcid="3593"></source>
  def test_value_is_idempotent(self):
    s_obj = metrics.SpecificityAtSensitivity(0.7)
    y_pred = tf.random.uniform((10, 3),
                                       maxval=1,
                                       dtype=tf.float32,
                                       seed=1)
    y_true = tf.random.uniform((10, 3),
                                       maxval=2,
                                       dtype=tf.int64,
                                       seed=1)
    update_op = s_obj.update_state(y_true, y_pred)
    self.evaluate(tf.compat.v1.variables_initializer(s_obj.variables))

    # Run several updates.
    for _ in range(10):
      self.evaluate(update_op)

    # Then verify idempotency.
    initial_specificity = self.evaluate(s_obj.result())
    for _ in range(10):
      self.assertAlmostEqual(initial_specificity, self.evaluate(s_obj.result()),
                             1e-3)

</clonepair146>

<clonepair146>
<source file="systems/keras-2.8.0/keras/metrics_confusion_matrix_test.py" startline="970" endline="992" pcid="3602"></source>
  def test_value_is_idempotent(self):
    s_obj = metrics.PrecisionAtRecall(0.7)
    y_pred = tf.random.uniform((10, 3),
                                       maxval=1,
                                       dtype=tf.float32,
                                       seed=1)
    y_true = tf.random.uniform((10, 3),
                                       maxval=2,
                                       dtype=tf.int64,
                                       seed=1)
    update_op = s_obj.update_state(y_true, y_pred)
    self.evaluate(tf.compat.v1.variables_initializer(s_obj.variables))

    # Run several updates.
    for _ in range(10):
      self.evaluate(update_op)

    # Then verify idempotency.
    initial_precision = self.evaluate(s_obj.result())
    for _ in range(10):
      self.assertAlmostEqual(initial_precision, self.evaluate(s_obj.result()),
                             1e-3)

</clonepair146>
<clonepair147>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1365" endline="1383" pcid="5560"></source>
  def test_int_output_explicit_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair147>

<clonepair147>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1384" endline="1402" pcid="5561"></source>
  def test_int_output_explicit_vocab_with_special_tokens(self):
    vocab_data = ["", "[OOV]", "earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair147>
<clonepair148>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1310" endline="1328" pcid="5556"></source>
  def test_int_output_int_file_vocab(self):
    vocab_data = ["10", "20", "30", "40"]
    input_array = np.array([[10, 20, 30, 40], [40, 0, 10, 42]])
    expected_output = [[2, 3, 4, 5], [5, 0, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)
    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1,
        vocabulary_dtype=tf.int64)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair148>

<clonepair148>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1384" endline="1402" pcid="5561"></source>
  def test_int_output_explicit_vocab_with_special_tokens(self):
    vocab_data = ["", "[OOV]", "earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair148>
<clonepair149>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="860" endline="878" pcid="5539"></source>
  def test_int_output_explicit_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair149>

<clonepair149>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1384" endline="1402" pcid="5561"></source>
  def test_int_output_explicit_vocab_with_special_tokens(self):
    vocab_data = ["", "[OOV]", "earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair149>
<clonepair150>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="764" endline="782" pcid="5535"></source>
  def test_int_output_no_reserved_zero(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token=None,
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair150>

<clonepair150>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1384" endline="1402" pcid="5561"></source>
  def test_int_output_explicit_vocab_with_special_tokens(self):
    vocab_data = ["", "[OOV]", "earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair150>
<clonepair151>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1310" endline="1328" pcid="5556"></source>
  def test_int_output_int_file_vocab(self):
    vocab_data = ["10", "20", "30", "40"]
    input_array = np.array([[10, 20, 30, 40], [40, 0, 10, 42]])
    expected_output = [[2, 3, 4, 5], [5, 0, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)
    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1,
        vocabulary_dtype=tf.int64)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair151>

<clonepair151>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1365" endline="1383" pcid="5560"></source>
  def test_int_output_explicit_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair151>
<clonepair152>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="860" endline="878" pcid="5539"></source>
  def test_int_output_explicit_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair152>

<clonepair152>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1365" endline="1383" pcid="5560"></source>
  def test_int_output_explicit_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair152>
<clonepair153>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="764" endline="782" pcid="5535"></source>
  def test_int_output_no_reserved_zero(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token=None,
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair153>

<clonepair153>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1365" endline="1383" pcid="5560"></source>
  def test_int_output_explicit_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair153>
<clonepair154>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="621" endline="639" pcid="5526"></source>
  def test_ragged_string_input(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = tf.ragged.constant(
        [["earth", "wind", "fire"], ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair154>

<clonepair154>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1365" endline="1383" pcid="5560"></source>
  def test_int_output_explicit_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair154>
<clonepair155>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="406" endline="424" pcid="5516"></source>
  def test_ragged_string_input(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = tf.ragged.constant(
        [["earth", "wind", "fire"], ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair155>

<clonepair155>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1365" endline="1383" pcid="5560"></source>
  def test_int_output_explicit_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair155>
<clonepair156>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="860" endline="878" pcid="5539"></source>
  def test_int_output_explicit_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair156>

<clonepair156>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1310" endline="1328" pcid="5556"></source>
  def test_int_output_int_file_vocab(self):
    vocab_data = ["10", "20", "30", "40"]
    input_array = np.array([[10, 20, 30, 40], [40, 0, 10, 42]])
    expected_output = [[2, 3, 4, 5], [5, 0, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)
    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1,
        vocabulary_dtype=tf.int64)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair156>
<clonepair157>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="764" endline="782" pcid="5535"></source>
  def test_int_output_no_reserved_zero(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token=None,
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair157>

<clonepair157>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1310" endline="1328" pcid="5556"></source>
  def test_int_output_int_file_vocab(self):
    vocab_data = ["10", "20", "30", "40"]
    input_array = np.array([[10, 20, 30, 40], [40, 0, 10, 42]])
    expected_output = [[2, 3, 4, 5], [5, 0, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)
    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1,
        vocabulary_dtype=tf.int64)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair157>
<clonepair158>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="764" endline="782" pcid="5535"></source>
  def test_int_output_no_reserved_zero(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token=None,
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair158>

<clonepair158>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="860" endline="878" pcid="5539"></source>
  def test_int_output_explicit_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair158>
<clonepair159>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="621" endline="639" pcid="5526"></source>
  def test_ragged_string_input(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = tf.ragged.constant(
        [["earth", "wind", "fire"], ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair159>

<clonepair159>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="860" endline="878" pcid="5539"></source>
  def test_int_output_explicit_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair159>
<clonepair160>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="406" endline="424" pcid="5516"></source>
  def test_ragged_string_input(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = tf.ragged.constant(
        [["earth", "wind", "fire"], ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair160>

<clonepair160>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="860" endline="878" pcid="5539"></source>
  def test_int_output_explicit_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair160>
<clonepair161>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="621" endline="639" pcid="5526"></source>
  def test_ragged_string_input(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = tf.ragged.constant(
        [["earth", "wind", "fire"], ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair161>

<clonepair161>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="764" endline="782" pcid="5535"></source>
  def test_int_output_no_reserved_zero(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token=None,
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair161>
<clonepair162>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="406" endline="424" pcid="5516"></source>
  def test_ragged_string_input(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = tf.ragged.constant(
        [["earth", "wind", "fire"], ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair162>

<clonepair162>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="764" endline="782" pcid="5535"></source>
  def test_int_output_no_reserved_zero(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token=None,
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair162>
<clonepair163>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="538" endline="557" pcid="5522"></source>
  def test_ragged_int_input_multi_bucket(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.ragged.constant([[10, 11, 13], [13, 12, 10, 133]],
                                     dtype=np.int64)
    expected_output = [[3, 4, 6], [6, 5, 3, 2]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        vocabulary_dtype=tf.int64,
        num_oov_indices=2,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)


</clonepair163>

<clonepair163>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="640" endline="658" pcid="5527"></source>
  def test_ragged_int_input(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.ragged.constant([[10, 11, 13], [13, 12, 10, 42]],
                                     dtype=np.int64)
    expected_output = [[2, 3, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        vocabulary_dtype=tf.int64,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair163>
<clonepair164>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="444" endline="463" pcid="5518"></source>
  def test_int32_input_with_int64_keys(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.ragged.constant([[10, 11, 13], [13, 12, 10, 42]],
                                     dtype=np.int32)
    expected_output = [[2, 3, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.int32, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        vocabulary_dtype=tf.int64,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)


</clonepair164>

<clonepair164>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="640" endline="658" pcid="5527"></source>
  def test_ragged_int_input(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.ragged.constant([[10, 11, 13], [13, 12, 10, 42]],
                                     dtype=np.int64)
    expected_output = [[2, 3, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        vocabulary_dtype=tf.int64,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair164>
<clonepair165>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="425" endline="443" pcid="5517"></source>
  def test_ragged_int_input(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.ragged.constant([[10, 11, 13], [13, 12, 10, 42]],
                                     dtype=np.int64)
    expected_output = [[2, 3, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        vocabulary_dtype=tf.int64,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair165>

<clonepair165>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="640" endline="658" pcid="5527"></source>
  def test_ragged_int_input(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.ragged.constant([[10, 11, 13], [13, 12, 10, 42]],
                                     dtype=np.int64)
    expected_output = [[2, 3, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        vocabulary_dtype=tf.int64,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair165>
<clonepair166>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="519" endline="537" pcid="5521"></source>
  def test_ragged_string_input_multi_bucket(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = tf.ragged.constant([["earth", "wind", "fire"],
                                      ["fire", "and", "earth", "ohio"]])
    expected_output = [[3, 4, 6], [6, 5, 3, 2]]

    input_data = keras.Input(shape=(None,), dtype=tf.string, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=2,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair166>

<clonepair166>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="621" endline="639" pcid="5526"></source>
  def test_ragged_string_input(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = tf.ragged.constant(
        [["earth", "wind", "fire"], ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair166>
<clonepair167>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="406" endline="424" pcid="5516"></source>
  def test_ragged_string_input(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = tf.ragged.constant(
        [["earth", "wind", "fire"], ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair167>

<clonepair167>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="621" endline="639" pcid="5526"></source>
  def test_ragged_string_input(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = tf.ragged.constant(
        [["earth", "wind", "fire"], ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair167>
<clonepair168>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="444" endline="463" pcid="5518"></source>
  def test_int32_input_with_int64_keys(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.ragged.constant([[10, 11, 13], [13, 12, 10, 42]],
                                     dtype=np.int32)
    expected_output = [[2, 3, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.int32, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        vocabulary_dtype=tf.int64,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)


</clonepair168>

<clonepair168>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="538" endline="557" pcid="5522"></source>
  def test_ragged_int_input_multi_bucket(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.ragged.constant([[10, 11, 13], [13, 12, 10, 133]],
                                     dtype=np.int64)
    expected_output = [[3, 4, 6], [6, 5, 3, 2]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        vocabulary_dtype=tf.int64,
        num_oov_indices=2,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)


</clonepair168>
<clonepair169>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="425" endline="443" pcid="5517"></source>
  def test_ragged_int_input(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.ragged.constant([[10, 11, 13], [13, 12, 10, 42]],
                                     dtype=np.int64)
    expected_output = [[2, 3, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        vocabulary_dtype=tf.int64,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair169>

<clonepair169>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="538" endline="557" pcid="5522"></source>
  def test_ragged_int_input_multi_bucket(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.ragged.constant([[10, 11, 13], [13, 12, 10, 133]],
                                     dtype=np.int64)
    expected_output = [[3, 4, 6], [6, 5, 3, 2]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        vocabulary_dtype=tf.int64,
        num_oov_indices=2,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)


</clonepair169>
<clonepair170>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="406" endline="424" pcid="5516"></source>
  def test_ragged_string_input(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = tf.ragged.constant(
        [["earth", "wind", "fire"], ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair170>

<clonepair170>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="519" endline="537" pcid="5521"></source>
  def test_ragged_string_input_multi_bucket(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = tf.ragged.constant([["earth", "wind", "fire"],
                                      ["fire", "and", "earth", "ohio"]])
    expected_output = [[3, 4, 6], [6, 5, 3, 2]]

    input_data = keras.Input(shape=(None,), dtype=tf.string, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=2,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair170>
<clonepair171>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="425" endline="443" pcid="5517"></source>
  def test_ragged_int_input(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.ragged.constant([[10, 11, 13], [13, 12, 10, 42]],
                                     dtype=np.int64)
    expected_output = [[2, 3, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        vocabulary_dtype=tf.int64,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</clonepair171>

<clonepair171>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="444" endline="463" pcid="5518"></source>
  def test_int32_input_with_int64_keys(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.ragged.constant([[10, 11, 13], [13, 12, 10, 42]],
                                     dtype=np.int32)
    expected_output = [[2, 3, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.int32, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        vocabulary_dtype=tf.int64,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)


</clonepair171>
<clonepair172>
<source file="systems/keras-2.8.0/keras/layers/advanced_activations_test.py" startline="91" endline="109" pcid="4878"></source>
  def test_relu_with_invalid_negative_slope(self):
    with self.assertRaisesRegex(
        ValueError, 'negative_slope of a ReLU layer cannot be a negative '
        'value. Received: None'):
      testing_utils.layer_test(
          keras.layers.ReLU,
          kwargs={'negative_slope': None},
          input_shape=(2, 3, 4),
          supports_masking=True)

    with self.assertRaisesRegex(
        ValueError, 'negative_slope of a ReLU layer cannot be a negative '
        'value. Received: -10'):
      testing_utils.layer_test(
          keras.layers.ReLU,
          kwargs={'negative_slope': -10},
          input_shape=(2, 3, 4),
          supports_masking=True)

</clonepair172>

<clonepair172>
<source file="systems/keras-2.8.0/keras/layers/advanced_activations_test.py" startline="110" endline="128" pcid="4879"></source>
  def test_relu_with_invalid_threshold(self):
    with self.assertRaisesRegex(
        ValueError, 'threshold of a ReLU layer cannot be a negative '
        'value. Received: None'):
      testing_utils.layer_test(
          keras.layers.ReLU,
          kwargs={'threshold': None},
          input_shape=(2, 3, 4),
          supports_masking=True)

    with self.assertRaisesRegex(
        ValueError, 'threshold of a ReLU layer cannot be a negative '
        'value. Received: -10'):
      testing_utils.layer_test(
          keras.layers.ReLU,
          kwargs={'threshold': -10},
          input_shape=(2, 3, 4),
          supports_masking=True)

</clonepair172>
<clonepair173>
<source file="systems/keras-2.8.0/keras/layers/advanced_activations_test.py" startline="91" endline="109" pcid="4878"></source>
  def test_relu_with_invalid_negative_slope(self):
    with self.assertRaisesRegex(
        ValueError, 'negative_slope of a ReLU layer cannot be a negative '
        'value. Received: None'):
      testing_utils.layer_test(
          keras.layers.ReLU,
          kwargs={'negative_slope': None},
          input_shape=(2, 3, 4),
          supports_masking=True)

    with self.assertRaisesRegex(
        ValueError, 'negative_slope of a ReLU layer cannot be a negative '
        'value. Received: -10'):
      testing_utils.layer_test(
          keras.layers.ReLU,
          kwargs={'negative_slope': -10},
          input_shape=(2, 3, 4),
          supports_masking=True)

</clonepair173>

<clonepair173>
<source file="systems/keras-2.8.0/keras/layers/advanced_activations_test.py" startline="161" endline="180" pcid="4883"></source>
  def test_threshold_relu_with_invalid_theta(self):
    with self.assertRaisesRegex(
        ValueError, 'Theta of a Thresholded ReLU layer cannot '
        'be None, expecting a float. Received: None'):
      testing_utils.layer_test(
          keras.layers.ThresholdedReLU,
          kwargs={'theta': None},
          input_shape=(2, 3, 4),
          supports_masking=True)

    with self.assertRaisesRegex(
        ValueError, 'The theta value of a Thresholded ReLU '
        'layer should be >=0. Received: -10'):
      testing_utils.layer_test(
          keras.layers.ThresholdedReLU,
          kwargs={'theta': -10},
          input_shape=(2, 3, 4),
          supports_masking=True)


</clonepair173>
<clonepair174>
<source file="systems/keras-2.8.0/keras/layers/advanced_activations_test.py" startline="110" endline="128" pcid="4879"></source>
  def test_relu_with_invalid_threshold(self):
    with self.assertRaisesRegex(
        ValueError, 'threshold of a ReLU layer cannot be a negative '
        'value. Received: None'):
      testing_utils.layer_test(
          keras.layers.ReLU,
          kwargs={'threshold': None},
          input_shape=(2, 3, 4),
          supports_masking=True)

    with self.assertRaisesRegex(
        ValueError, 'threshold of a ReLU layer cannot be a negative '
        'value. Received: -10'):
      testing_utils.layer_test(
          keras.layers.ReLU,
          kwargs={'threshold': -10},
          input_shape=(2, 3, 4),
          supports_masking=True)

</clonepair174>

<clonepair174>
<source file="systems/keras-2.8.0/keras/layers/advanced_activations_test.py" startline="161" endline="180" pcid="4883"></source>
  def test_threshold_relu_with_invalid_theta(self):
    with self.assertRaisesRegex(
        ValueError, 'Theta of a Thresholded ReLU layer cannot '
        'be None, expecting a float. Received: None'):
      testing_utils.layer_test(
          keras.layers.ThresholdedReLU,
          kwargs={'theta': None},
          input_shape=(2, 3, 4),
          supports_masking=True)

    with self.assertRaisesRegex(
        ValueError, 'The theta value of a Thresholded ReLU '
        'layer should be >=0. Received: -10'):
      testing_utils.layer_test(
          keras.layers.ThresholdedReLU,
          kwargs={'theta': -10},
          input_shape=(2, 3, 4),
          supports_masking=True)


</clonepair174>
<clonepair175>
<source file="systems/keras-2.8.0/keras/layers/legacy_rnn/rnn_cell_impl.py" startline="449" endline="466" pcid="4559"></source>
  def build(self, inputs_shape):
    if inputs_shape[-1] is None:
      raise ValueError(
          "Expected inputs.shape[-1] to be known, "
          f"received shape: {inputs_shape}")
    _check_supported_dtypes(self.dtype)

    input_depth = inputs_shape[-1]
    self._kernel = self.add_variable(
        _WEIGHTS_VARIABLE_NAME,
        shape=[input_depth + self._num_units, self._num_units])
    self._bias = self.add_variable(
        _BIAS_VARIABLE_NAME,
        shape=[self._num_units],
        initializer=tf.compat.v1.zeros_initializer(dtype=self.dtype))

    self.built = True

</clonepair175>

<clonepair175>
<source file="systems/keras-2.8.0/keras/layers/legacy_rnn/rnn_cell_impl.py" startline="746" endline="763" pcid="4572"></source>
  def build(self, inputs_shape):
    if inputs_shape[-1] is None:
      raise ValueError(
          "Expected inputs.shape[-1] to be known, "
          f"received shape: {inputs_shape}")
    _check_supported_dtypes(self.dtype)
    input_depth = inputs_shape[-1]
    h_depth = self._num_units
    self._kernel = self.add_variable(
        _WEIGHTS_VARIABLE_NAME,
        shape=[input_depth + h_depth, 4 * self._num_units])
    self._bias = self.add_variable(
        _BIAS_VARIABLE_NAME,
        shape=[4 * self._num_units],
        initializer=tf.compat.v1.zeros_initializer(dtype=self.dtype))

    self.built = True

</clonepair175>
<clonepair176>
<source file="systems/keras-2.8.0/keras/layers/pooling.py" startline="334" endline="349" pcid="4439"></source>
  def __init__(self, pool_function, pool_size, strides,
               padding='valid', data_format=None,
               name=None, **kwargs):
    super(Pooling2D, self).__init__(name=name, **kwargs)
    if data_format is None:
      data_format = backend.image_data_format()
    if strides is None:
      strides = pool_size
    self.pool_function = pool_function
    self.pool_size = conv_utils.normalize_tuple(pool_size, 2, 'pool_size')
    self.strides = conv_utils.normalize_tuple(
        strides, 2, 'strides', allow_zero=True)
    self.padding = conv_utils.normalize_padding(padding)
    self.data_format = conv_utils.normalize_data_format(data_format)
    self.input_spec = InputSpec(ndim=4)

</clonepair176>

<clonepair176>
<source file="systems/keras-2.8.0/keras/layers/pooling.py" startline="673" endline="688" pcid="4445"></source>
  def __init__(self, pool_function, pool_size, strides,
               padding='valid', data_format='channels_last',
               name=None, **kwargs):
    super(Pooling3D, self).__init__(name=name, **kwargs)
    if data_format is None:
      data_format = backend.image_data_format()
    if strides is None:
      strides = pool_size
    self.pool_function = pool_function
    self.pool_size = conv_utils.normalize_tuple(pool_size, 3, 'pool_size')
    self.strides = conv_utils.normalize_tuple(
        strides, 3, 'strides', allow_zero=True)
    self.padding = conv_utils.normalize_padding(padding)
    self.data_format = conv_utils.normalize_data_format(data_format)
    self.input_spec = InputSpec(ndim=5)

</clonepair176>
<clonepair177>
<source file="systems/keras-2.8.0/keras/layers/pooling.py" startline="50" endline="65" pcid="4433"></source>
  def __init__(self, pool_function, pool_size, strides,
               padding='valid', data_format='channels_last',
               name=None, **kwargs):
    super(Pooling1D, self).__init__(name=name, **kwargs)
    if data_format is None:
      data_format = backend.image_data_format()
    if strides is None:
      strides = pool_size
    self.pool_function = pool_function
    self.pool_size = conv_utils.normalize_tuple(pool_size, 1, 'pool_size')
    self.strides = conv_utils.normalize_tuple(
        strides, 1, 'strides', allow_zero=True)
    self.padding = conv_utils.normalize_padding(padding)
    self.data_format = conv_utils.normalize_data_format(data_format)
    self.input_spec = InputSpec(ndim=3)

</clonepair177>

<clonepair177>
<source file="systems/keras-2.8.0/keras/layers/pooling.py" startline="673" endline="688" pcid="4445"></source>
  def __init__(self, pool_function, pool_size, strides,
               padding='valid', data_format='channels_last',
               name=None, **kwargs):
    super(Pooling3D, self).__init__(name=name, **kwargs)
    if data_format is None:
      data_format = backend.image_data_format()
    if strides is None:
      strides = pool_size
    self.pool_function = pool_function
    self.pool_size = conv_utils.normalize_tuple(pool_size, 3, 'pool_size')
    self.strides = conv_utils.normalize_tuple(
        strides, 3, 'strides', allow_zero=True)
    self.padding = conv_utils.normalize_padding(padding)
    self.data_format = conv_utils.normalize_data_format(data_format)
    self.input_spec = InputSpec(ndim=5)

</clonepair177>
<clonepair178>
<source file="systems/keras-2.8.0/keras/layers/pooling.py" startline="50" endline="65" pcid="4433"></source>
  def __init__(self, pool_function, pool_size, strides,
               padding='valid', data_format='channels_last',
               name=None, **kwargs):
    super(Pooling1D, self).__init__(name=name, **kwargs)
    if data_format is None:
      data_format = backend.image_data_format()
    if strides is None:
      strides = pool_size
    self.pool_function = pool_function
    self.pool_size = conv_utils.normalize_tuple(pool_size, 1, 'pool_size')
    self.strides = conv_utils.normalize_tuple(
        strides, 1, 'strides', allow_zero=True)
    self.padding = conv_utils.normalize_padding(padding)
    self.data_format = conv_utils.normalize_data_format(data_format)
    self.input_spec = InputSpec(ndim=3)

</clonepair178>

<clonepair178>
<source file="systems/keras-2.8.0/keras/layers/pooling.py" startline="334" endline="349" pcid="4439"></source>
  def __init__(self, pool_function, pool_size, strides,
               padding='valid', data_format=None,
               name=None, **kwargs):
    super(Pooling2D, self).__init__(name=name, **kwargs)
    if data_format is None:
      data_format = backend.image_data_format()
    if strides is None:
      strides = pool_size
    self.pool_function = pool_function
    self.pool_size = conv_utils.normalize_tuple(pool_size, 2, 'pool_size')
    self.strides = conv_utils.normalize_tuple(
        strides, 2, 'strides', allow_zero=True)
    self.padding = conv_utils.normalize_padding(padding)
    self.data_format = conv_utils.normalize_data_format(data_format)
    self.input_spec = InputSpec(ndim=4)

</clonepair178>
<clonepair179>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="563" endline="579" pcid="5523"></source>
  def test_sparse_adapt(self):
    vocab_data = tf.SparseTensor(
        indices=[[0, 0], [0, 1], [1, 2]],
        values=["michigan", "fire", "michigan"],
        dense_shape=[3, 4])
    vocab_dataset = tf.data.Dataset.from_tensors(vocab_data)

    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.adapt(vocab_dataset)
    expected_vocabulary = ["", "[OOV]", "michigan", "fire"]
    self.assertAllEqual(expected_vocabulary, layer.get_vocabulary())

</clonepair179>

<clonepair179>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="580" endline="594" pcid="5524"></source>
  def test_ragged_adapt(self):
    vocab_data = tf.ragged.constant([["michigan"],
                                     ["fire", "michigan"]])
    vocab_dataset = tf.data.Dataset.from_tensors(vocab_data)

    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.adapt(vocab_dataset)
    expected_vocabulary = ["", "[OOV]", "michigan", "fire"]
    self.assertAllEqual(expected_vocabulary, layer.get_vocabulary())

</clonepair179>
<clonepair180>
<source file="systems/keras-2.8.0/keras/optimizer_v2/learning_rate_schedule.py" startline="143" endline="170" pcid="223"></source>
  def __init__(
      self,
      initial_learning_rate,
      decay_steps,
      decay_rate,
      staircase=False,
      name=None):
    """Applies exponential decay to the learning rate.

    Args:
      initial_learning_rate: A scalar `float32` or `float64` `Tensor` or a
        Python number.  The initial learning rate.
      decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number.
        Must be positive.  See the decay computation above.
      decay_rate: A scalar `float32` or `float64` `Tensor` or a
        Python number.  The decay rate.
      staircase: Boolean.  If `True` decay the learning rate at discrete
        intervals
      name: String.  Optional name of the operation.  Defaults to
        'ExponentialDecay'.
    """
    super(ExponentialDecay, self).__init__()
    self.initial_learning_rate = initial_learning_rate
    self.decay_steps = decay_steps
    self.decay_rate = decay_rate
    self.staircase = staircase
    self.name = name

</clonepair180>

<clonepair180>
<source file="systems/keras-2.8.0/keras/optimizer_v2/learning_rate_schedule.py" startline="687" endline="718" pcid="238"></source>
  def __init__(
      self,
      initial_learning_rate,
      first_decay_steps,
      t_mul=2.0,
      m_mul=1.0,
      alpha=0.0,
      name=None):
    """Applies cosine decay with restarts to the learning rate.

    Args:
      initial_learning_rate: A scalar `float32` or `float64` Tensor or a Python
        number. The initial learning rate.
      first_decay_steps: A scalar `int32` or `int64` `Tensor` or a Python
        number. Number of steps to decay over.
      t_mul: A scalar `float32` or `float64` `Tensor` or a Python number.
        Used to derive the number of iterations in the i-th period.
      m_mul: A scalar `float32` or `float64` `Tensor` or a Python number.
        Used to derive the initial learning rate of the i-th period.
      alpha: A scalar `float32` or `float64` Tensor or a Python number.
        Minimum learning rate value as a fraction of the initial_learning_rate.
      name: String. Optional name of the operation.  Defaults to 'SGDRDecay'.
    """
    super(CosineDecayRestarts, self).__init__()

    self.initial_learning_rate = initial_learning_rate
    self.first_decay_steps = first_decay_steps
    self._t_mul = t_mul
    self._m_mul = m_mul
    self.alpha = alpha
    self.name = name

</clonepair180>
<clonepair181>
<source file="systems/keras-2.8.0/keras/optimizer_v2/learning_rate_schedule.py" startline="595" endline="618" pcid="235"></source>
  def __init__(
      self,
      initial_learning_rate,
      decay_steps,
      alpha=0.0,
      name=None):
    """Applies cosine decay to the learning rate.

    Args:
      initial_learning_rate: A scalar `float32` or `float64` Tensor or a
        Python number. The initial learning rate.
      decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number.
        Number of steps to decay over.
      alpha: A scalar `float32` or `float64` Tensor or a Python number.
        Minimum learning rate value as a fraction of initial_learning_rate.
      name: String. Optional name of the operation.  Defaults to 'CosineDecay'.
    """
    super(CosineDecay, self).__init__()

    self.initial_learning_rate = initial_learning_rate
    self.decay_steps = decay_steps
    self.alpha = alpha
    self.name = name

</clonepair181>

<clonepair181>
<source file="systems/keras-2.8.0/keras/optimizer_v2/learning_rate_schedule.py" startline="687" endline="718" pcid="238"></source>
  def __init__(
      self,
      initial_learning_rate,
      first_decay_steps,
      t_mul=2.0,
      m_mul=1.0,
      alpha=0.0,
      name=None):
    """Applies cosine decay with restarts to the learning rate.

    Args:
      initial_learning_rate: A scalar `float32` or `float64` Tensor or a Python
        number. The initial learning rate.
      first_decay_steps: A scalar `int32` or `int64` `Tensor` or a Python
        number. Number of steps to decay over.
      t_mul: A scalar `float32` or `float64` `Tensor` or a Python number.
        Used to derive the number of iterations in the i-th period.
      m_mul: A scalar `float32` or `float64` `Tensor` or a Python number.
        Used to derive the initial learning rate of the i-th period.
      alpha: A scalar `float32` or `float64` Tensor or a Python number.
        Minimum learning rate value as a fraction of the initial_learning_rate.
      name: String. Optional name of the operation.  Defaults to 'SGDRDecay'.
    """
    super(CosineDecayRestarts, self).__init__()

    self.initial_learning_rate = initial_learning_rate
    self.first_decay_steps = first_decay_steps
    self._t_mul = t_mul
    self._m_mul = m_mul
    self.alpha = alpha
    self.name = name

</clonepair181>
<clonepair182>
<source file="systems/keras-2.8.0/keras/premade/wide_deep_test.py" startline="35" endline="50" pcid="3781"></source>
  def test_wide_deep_model(self):
    linear_model = linear.LinearModel(units=1)
    dnn_model = sequential.Sequential([core.Dense(units=1, input_dim=3)])
    wide_deep_model = wide_deep.WideDeepModel(linear_model, dnn_model)
    linear_inp = np.random.uniform(low=-5., high=5., size=(64, 2))
    dnn_inp = np.random.uniform(low=-5., high=5., size=(64, 3))
    inputs = [linear_inp, dnn_inp]
    output = .3 * linear_inp[:, 0] + .2 * dnn_inp[:, 1]
    wide_deep_model.compile(
        optimizer=['sgd', 'adam'],
        loss='mse',
        metrics=[],
        run_eagerly=testing_utils.should_run_eagerly())
    wide_deep_model.fit(inputs, output, epochs=5)
    self.assertTrue(wide_deep_model.built)

</clonepair182>

<clonepair182>
<source file="systems/keras-2.8.0/keras/premade/wide_deep_test.py" startline="114" endline="129" pcid="3785"></source>
  def test_wide_deep_model_with_single_optimizer(self):
    linear_model = linear.LinearModel(units=1)
    dnn_model = sequential.Sequential([core.Dense(units=1, input_dim=3)])
    wide_deep_model = wide_deep.WideDeepModel(linear_model, dnn_model)
    linear_inp = np.random.uniform(low=-5., high=5., size=(64, 2))
    dnn_inp = np.random.uniform(low=-5., high=5., size=(64, 3))
    inputs = [linear_inp, dnn_inp]
    output = .3 * linear_inp[:, 0] + .2 * dnn_inp[:, 1]
    wide_deep_model.compile(
        optimizer='sgd',
        loss='mse',
        metrics=[],
        run_eagerly=testing_utils.should_run_eagerly())
    wide_deep_model.fit(inputs, output, epochs=5)
    self.assertTrue(wide_deep_model.built)

</clonepair182>
<clonepair183>
<source file="systems/keras-2.8.0/keras/premade/wide_deep_test.py" startline="35" endline="50" pcid="3781"></source>
  def test_wide_deep_model(self):
    linear_model = linear.LinearModel(units=1)
    dnn_model = sequential.Sequential([core.Dense(units=1, input_dim=3)])
    wide_deep_model = wide_deep.WideDeepModel(linear_model, dnn_model)
    linear_inp = np.random.uniform(low=-5., high=5., size=(64, 2))
    dnn_inp = np.random.uniform(low=-5., high=5., size=(64, 3))
    inputs = [linear_inp, dnn_inp]
    output = .3 * linear_inp[:, 0] + .2 * dnn_inp[:, 1]
    wide_deep_model.compile(
        optimizer=['sgd', 'adam'],
        loss='mse',
        metrics=[],
        run_eagerly=testing_utils.should_run_eagerly())
    wide_deep_model.fit(inputs, output, epochs=5)
    self.assertTrue(wide_deep_model.built)

</clonepair183>

<clonepair183>
<source file="systems/keras-2.8.0/keras/premade/wide_deep_test.py" startline="77" endline="89" pcid="3783"></source>
  def test_wide_deep_model_with_single_input(self):
    linear_model = linear.LinearModel(units=1)
    dnn_model = sequential.Sequential([core.Dense(units=1, input_dim=3)])
    wide_deep_model = wide_deep.WideDeepModel(linear_model, dnn_model)
    inputs = np.random.uniform(low=-5., high=5., size=(64, 3))
    output = .3 * inputs[:, 0]
    wide_deep_model.compile(
        optimizer=['sgd', 'adam'],
        loss='mse',
        metrics=[],
        run_eagerly=testing_utils.should_run_eagerly())
    wide_deep_model.fit(inputs, output, epochs=5)

</clonepair183>
<clonepair184>
<source file="systems/keras-2.8.0/keras/losses_test.py" startline="97" endline="117" pcid="1853"></source>
  def test_categorical_crossentropy_loss_with_unknown_rank_tensor(self):
    t = backend.placeholder()
    p = backend.placeholder()
    o = losses.categorical_crossentropy(t, p)

    t_val = tf.convert_to_tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])
    p_val = tf.convert_to_tensor([[.9, .05, .05], [.05, .89, .06],
                                  [.05, .01, .94]])
    f = backend.function([t, p], o)

    result = f([t_val, p_val])
    self.assertArrayNear(result, [.105, .116, .062], 1e-3)

    # from logits
    p_val = tf.convert_to_tensor([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])
    o = losses.categorical_crossentropy(t, p, from_logits=True)
    f = backend.function([t, p], o)

    result = f([t_val, p_val])
    self.assertArrayNear(result, [.002, 0, .17], 1e-3)

</clonepair184>

<clonepair184>
<source file="systems/keras-2.8.0/keras/losses_test.py" startline="133" endline="155" pcid="1855"></source>
  def test_sparse_categorical_crossentropy_loss_with_unknown_rank_tensor(self):
    # This test only runs in graph because the TF op layer is not supported yet
    # for sparse ops.
    t = backend.placeholder()
    p = backend.placeholder()
    o = losses.sparse_categorical_crossentropy(t, p)

    t_val = tf.convert_to_tensor([0, 1, 2])
    p_val = tf.convert_to_tensor([[.9, .05, .05], [.05, .89, .06],
                                  [.05, .01, .94]])
    f = backend.function([t, p], o)

    result = f([t_val, p_val])
    self.assertArrayNear(result, [.105, .116, .062], 1e-3)

    # from logits
    p_val = tf.convert_to_tensor([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])
    o = losses.sparse_categorical_crossentropy(t, p, from_logits=True)
    f = backend.function([t, p], o)

    result = f([t_val, p_val])
    self.assertArrayNear(result, [.002, 0, .17], 1e-3)

</clonepair184>
<clonepair185>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1555" endline="1569" pcid="5574"></source>
  def test_vocab_with_idf_weights_non_tfidf_output_fails(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    weight_data = [1, 1, 1, 1, 1]
    with self.assertRaisesRegex(ValueError,
                                "`idf_weights` should only be set if"):
      index_lookup.IndexLookup(
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          output_mode=index_lookup.MULTI_HOT,
          vocabulary_dtype=tf.string,
          vocabulary=vocab_data,
          idf_weights=weight_data)

</clonepair185>

<clonepair185>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1570" endline="1584" pcid="5575"></source>
  def test_vocab_with_idf_weights_length_mismatch_fails(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    weight_data = [1, 1, 1, 1, 1]  # too long
    with self.assertRaisesRegex(
        ValueError, "`idf_weights` must be the same length as vocab"):
      index_lookup.IndexLookup(
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          output_mode=index_lookup.TF_IDF,
          vocabulary_dtype=tf.string,
          vocabulary=vocab_data,
          idf_weights=weight_data)

</clonepair185>
<clonepair186>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1570" endline="1584" pcid="5575"></source>
  def test_vocab_with_idf_weights_length_mismatch_fails(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    weight_data = [1, 1, 1, 1, 1]  # too long
    with self.assertRaisesRegex(
        ValueError, "`idf_weights` must be the same length as vocab"):
      index_lookup.IndexLookup(
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          output_mode=index_lookup.TF_IDF,
          vocabulary_dtype=tf.string,
          vocabulary=vocab_data,
          idf_weights=weight_data)

</clonepair186>

<clonepair186>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1585" endline="1597" pcid="5576"></source>
  def test_vocab_without_idf_weights_tfidf_output_fails(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    with self.assertRaisesRegex(
        ValueError, "`idf_weights` must be set if output_mode is TF_IDF"):
      index_lookup.IndexLookup(
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          output_mode=index_lookup.TF_IDF,
          vocabulary_dtype=tf.string,
          vocabulary=vocab_data)

</clonepair186>
<clonepair187>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1329" endline="1341" pcid="5557"></source>
  def test_dataset_map_output(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=0,
        mask_token=None,
        oov_token="[OOV]",
        vocabulary=vocab_data,
        vocabulary_dtype=tf.string)
    ds = tf.data.Dataset.from_tensor_slices([["earth"], ["wind"], ["and"]])
    ds = ds.map(layer)
    self.assertAllEqual(list(ds.as_numpy_iterator()), [[0], [1], [2]])

</clonepair187>

<clonepair187>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1342" endline="1359" pcid="5558"></source>
  def test_dataset_map_output_layer_created_in_function(self):
    vocab_data = ["earth", "wind", "and", "fire"]

    def apply_lookup(data):
      layer = index_lookup.IndexLookup(
          max_tokens=None,
          num_oov_indices=0,
          mask_token=None,
          oov_token="[OOV]",
          vocabulary=vocab_data,
          vocabulary_dtype=tf.string)
      return layer(data)

    ds = tf.data.Dataset.from_tensor_slices([["earth"], ["wind"], ["and"]])
    ds = ds.map(apply_lookup)
    self.assertAllEqual(list(ds.as_numpy_iterator()), [[0], [1], [2]])


</clonepair187>
<clonepair188>
<source file="systems/keras-2.8.0/keras/optimizer_v2/utils.py" startline="84" endline="105" pcid="153"></source>
def make_gradient_clipnorm_fn(clipnorm):
  """Creates a gradient transformation function for clipping by norm."""
  if clipnorm is None:
    return lambda grads_and_vars: grads_and_vars

  def gradient_clipnorm_fn(grads_and_vars):

    if isinstance(tf.distribute.get_strategy(),
                  (tf.distribute.experimental.CentralStorageStrategy,
                   tf.compat.v1.distribute.experimental.CentralStorageStrategy)):
      raise ValueError(
          "`clipnorm` is not supported with `CenteralStorageStrategy`. "
          f"The strategy used is {tf.distribute.get_strategy()}.")

    clipped_grads_and_vars = [
        (tf.clip_by_norm(g, clipnorm), v) for g, v in grads_and_vars
    ]
    return clipped_grads_and_vars

  return gradient_clipnorm_fn


</clonepair188>

<clonepair188>
<source file="systems/keras-2.8.0/keras/optimizer_v2/utils.py" startline="106" endline="127" pcid="155"></source>
def make_global_gradient_clipnorm_fn(clipnorm):
  """Creates a gradient transformation function for clipping by norm."""
  if clipnorm is None:
    return lambda grads_and_vars: grads_and_vars

  def gradient_clipnorm_fn(grads_and_vars):

    if isinstance(tf.distribute.get_strategy(),
                  (tf.distribute.experimental.CentralStorageStrategy,
                   tf.compat.v1.distribute.experimental.CentralStorageStrategy)):
      raise ValueError(
          "`global_clipnorm` is not supported with `CenteralStorageStrategy`. "
          f"The strategy used is {tf.distribute.get_strategy()}.")

    grads, variables = zip(*grads_and_vars)
    clipped_grads, _ = tf.clip_by_global_norm(grads, clipnorm)
    clipped_grads_and_vars = list(zip(clipped_grads, variables))
    return clipped_grads_and_vars

  return gradient_clipnorm_fn


</clonepair188>
<clonepair189>
<source file="systems/keras-2.8.0/keras/optimizer_v2/utils.py" startline="84" endline="105" pcid="153"></source>
def make_gradient_clipnorm_fn(clipnorm):
  """Creates a gradient transformation function for clipping by norm."""
  if clipnorm is None:
    return lambda grads_and_vars: grads_and_vars

  def gradient_clipnorm_fn(grads_and_vars):

    if isinstance(tf.distribute.get_strategy(),
                  (tf.distribute.experimental.CentralStorageStrategy,
                   tf.compat.v1.distribute.experimental.CentralStorageStrategy)):
      raise ValueError(
          "`clipnorm` is not supported with `CenteralStorageStrategy`. "
          f"The strategy used is {tf.distribute.get_strategy()}.")

    clipped_grads_and_vars = [
        (tf.clip_by_norm(g, clipnorm), v) for g, v in grads_and_vars
    ]
    return clipped_grads_and_vars

  return gradient_clipnorm_fn


</clonepair189>

<clonepair189>
<source file="systems/keras-2.8.0/keras/optimizer_v2/utils.py" startline="128" endline="149" pcid="157"></source>
def make_gradient_clipvalue_fn(clipvalue):
  """Creates a gradient transformation function for clipping by value."""
  if clipvalue is None:
    return lambda grads_and_vars: grads_and_vars

  def gradient_clipvalue_fn(grads_and_vars):

    if isinstance(tf.distribute.get_strategy(),
                  (tf.distribute.experimental.CentralStorageStrategy,
                   tf.compat.v1.distribute.experimental.CentralStorageStrategy)):
      raise ValueError(
          "`clipvalue` is not supported with `CenteralStorageStrategy`. "
          f"The strategy used is {tf.distribute.get_strategy()}.")

    clipped_grads_and_vars = [(tf.clip_by_value(g, -clipvalue,
                                                      clipvalue), v)
                              for g, v in grads_and_vars]
    return clipped_grads_and_vars

  return gradient_clipvalue_fn


</clonepair189>
<clonepair190>
<source file="systems/keras-2.8.0/keras/optimizer_v2/utils.py" startline="106" endline="127" pcid="155"></source>
def make_global_gradient_clipnorm_fn(clipnorm):
  """Creates a gradient transformation function for clipping by norm."""
  if clipnorm is None:
    return lambda grads_and_vars: grads_and_vars

  def gradient_clipnorm_fn(grads_and_vars):

    if isinstance(tf.distribute.get_strategy(),
                  (tf.distribute.experimental.CentralStorageStrategy,
                   tf.compat.v1.distribute.experimental.CentralStorageStrategy)):
      raise ValueError(
          "`global_clipnorm` is not supported with `CenteralStorageStrategy`. "
          f"The strategy used is {tf.distribute.get_strategy()}.")

    grads, variables = zip(*grads_and_vars)
    clipped_grads, _ = tf.clip_by_global_norm(grads, clipnorm)
    clipped_grads_and_vars = list(zip(clipped_grads, variables))
    return clipped_grads_and_vars

  return gradient_clipnorm_fn


</clonepair190>

<clonepair190>
<source file="systems/keras-2.8.0/keras/optimizer_v2/utils.py" startline="128" endline="149" pcid="157"></source>
def make_gradient_clipvalue_fn(clipvalue):
  """Creates a gradient transformation function for clipping by value."""
  if clipvalue is None:
    return lambda grads_and_vars: grads_and_vars

  def gradient_clipvalue_fn(grads_and_vars):

    if isinstance(tf.distribute.get_strategy(),
                  (tf.distribute.experimental.CentralStorageStrategy,
                   tf.compat.v1.distribute.experimental.CentralStorageStrategy)):
      raise ValueError(
          "`clipvalue` is not supported with `CenteralStorageStrategy`. "
          f"The strategy used is {tf.distribute.get_strategy()}.")

    clipped_grads_and_vars = [(tf.clip_by_value(g, -clipvalue,
                                                      clipvalue), v)
                              for g, v in grads_and_vars]
    return clipped_grads_and_vars

  return gradient_clipvalue_fn


</clonepair190>
<clonepair191>
<source file="systems/keras-2.8.0/keras/regularizers.py" startline="367" endline="379" pcid="807"></source>
def get(identifier):
  """Retrieve a regularizer instance from a config or identifier."""
  if identifier is None:
    return None
  if isinstance(identifier, dict):
    return deserialize(identifier)
  elif isinstance(identifier, str):
    return deserialize(str(identifier))
  elif callable(identifier):
    return identifier
  else:
    raise ValueError(
        f'Could not interpret regularizer identifier: {identifier}')
</clonepair191>

<clonepair191>
<source file="systems/keras-2.8.0/keras/activations.py" startline="564" endline="600" pcid="5640"></source>
def get(identifier):
  """Returns function.

  Args:
      identifier: Function or string

  Returns:
      Function corresponding to the input string or input function.

  For example:

  >>> tf.keras.activations.get('softmax')
   <function softmax at 0x1222a3d90>
  >>> tf.keras.activations.get(tf.keras.activations.softmax)
   <function softmax at 0x1222a3d90>
  >>> tf.keras.activations.get(None)
   <function linear at 0x1239596a8>
  >>> tf.keras.activations.get(abs)
   <built-in function abs>
  >>> tf.keras.activations.get('abcd')
  Traceback (most recent call last):
  ...
  ValueError: Unknown activation function:abcd

  Raises:
      ValueError: Input is an unknown function or string, i.e., the input does
      not denote any defined function.
  """
  if identifier is None:
    return linear
  if isinstance(identifier, (str, dict)):
    return deserialize(identifier)
  elif callable(identifier):
    return identifier
  else:
    raise TypeError(
        f'Could not interpret activation function identifier: {identifier}')
</clonepair191>
<clonepair192>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/pooling.py" startline="96" endline="170" pcid="3242"></source>
def average_pooling1d(inputs, pool_size, strides,
                      padding='valid', data_format='channels_last',
                      name=None):
  """Average Pooling layer for 1D inputs.

  Args:
    inputs: The tensor over which to pool. Must have rank 3.
    pool_size: An integer or tuple/list of a single integer,
      representing the size of the pooling window.
    strides: An integer or tuple/list of a single integer, specifying the
      strides of the pooling operation.
    padding: A string. The padding method, either 'valid' or 'same'.
      Case-insensitive.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, length, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, length)`.
    name: A string, the name of the layer.

  Returns:
    The output tensor, of rank 3.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.AveragePooling1D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.average_pooling1d(x, pool_size=2, strides=2)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.AveragePooling1D(pool_size=2, strides=2)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.average_pooling1d` is deprecated and '
      'will be removed in a future version. '
      'Please use `tf.keras.layers.AveragePooling1D` instead.',
      stacklevel=2)
  layer = AveragePooling1D(pool_size=pool_size,
                           strides=strides,
                           padding=padding,
                           data_format=data_format,
                           name=name)
  return layer.apply(inputs)


</clonepair192>

<clonepair192>
<source file="systems/keras-2.8.0/keras/legacy_tf_layers/pooling.py" startline="238" endline="312" pcid="3244"></source>
def max_pooling1d(inputs, pool_size, strides,
                  padding='valid', data_format='channels_last',
                  name=None):
  """Max Pooling layer for 1D inputs.

  Args:
    inputs: The tensor over which to pool. Must have rank 3.
    pool_size: An integer or tuple/list of a single integer,
      representing the size of the pooling window.
    strides: An integer or tuple/list of a single integer, specifying the
      strides of the pooling operation.
    padding: A string. The padding method, either 'valid' or 'same'.
      Case-insensitive.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, length, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, length)`.
    name: A string, the name of the layer.

  Returns:
    The output tensor, of rank 3.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is a legacy api that is only compatible with eager execution and
  `tf.function` if you combine it with
  `tf.compat.v1.keras.utils.track_tf1_style_variables`

  Please refer to [tf.layers model mapping section of the migration guide]
  (https://www.tensorflow.org/guide/migrate/model_mapping)
  to learn how to use your TensorFlow v1 model in TF2 with Keras.

  The corresponding TensorFlow v2 layer is
  `tf.keras.layers.MaxPooling1D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.max_pooling1d(x, pool_size=2, strides=2)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.MaxPooling1D(pool_size=2, strides=2)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.max_pooling1d` is deprecated and '
      'will be removed in a future version. '
      'Please use `tf.keras.layers.MaxPooling1D` instead.',
      stacklevel=2)
  layer = MaxPooling1D(pool_size=pool_size,
                       strides=strides,
                       padding=padding,
                       data_format=data_format,
                       name=name)
  return layer.apply(inputs)


</clonepair192>
<clonepair193>
<source file="systems/keras-2.8.0/keras/feature_column/dense_features_v2_test.py" startline="372" endline="386" pcid="3144"></source>
  def test_runtime_batch_size_mismatch(self):
    price1 = tf.feature_column.numeric_column('price1')
    price2 = tf.feature_column.numeric_column('price2')
    with tf.Graph().as_default():
      features = {
          'price1': tf.compat.v1.placeholder(dtype=tf.int64),  # batchsize = 3
          'price2': [[3.], [4.]]  # batchsize = 2
      }
      net = df.DenseFeatures([price1, price2])(features)
      with _initialized_session() as sess:
        with self.assertRaisesRegex(tf.errors.OpError,
                                    'Dimension 0 in both shapes must be equal|'
                                    'Dimensions of inputs should match'):
          sess.run(net, feed_dict={features['price1']: [[1.], [5.], [7.]]})

</clonepair193>

<clonepair193>
<source file="systems/keras-2.8.0/keras/feature_column/dense_features_test.py" startline="387" endline="401" pcid="3093"></source>
  def test_runtime_batch_size_mismatch(self):
    price1 = tf.feature_column.numeric_column('price1')
    price2 = tf.feature_column.numeric_column('price2')
    with tf.Graph().as_default():
      features = {
          'price1': tf.compat.v1.placeholder(dtype=tf.int64),  # batchsize = 3
          'price2': [[3.], [4.]]  # batchsize = 2
      }
      net = df.DenseFeatures([price1, price2])(features)
      with _initialized_session() as sess:
        with self.assertRaisesRegex(tf.errors.OpError,
                                    'Dimension 0 in both shapes must be equal|'
                                    'Dimensions of inputs should match'):
          sess.run(net, feed_dict={features['price1']: [[1.], [5.], [7.]]})

</clonepair193>
<clonepair194>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1668" endline="1680" pcid="5582"></source>
  def test_vocab_with_max_cap(self):
    vocab_data = ["", "[OOV]", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=5,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string,
        invert=True)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(vocab_data, returned_vocab)

</clonepair194>

<clonepair194>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1681" endline="1693" pcid="5583"></source>
  def test_int_vocab_with_max_cap(self):
    vocab_data = [0, -1, 42, 1276, 1138]
    layer = index_lookup.IndexLookup(
        max_tokens=5,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1,
        vocabulary_dtype=tf.int64,
        invert=True)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(vocab_data, returned_vocab)

</clonepair194>
<clonepair195>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1668" endline="1680" pcid="5582"></source>
  def test_vocab_with_max_cap(self):
    vocab_data = ["", "[OOV]", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=5,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string,
        invert=True)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(vocab_data, returned_vocab)

</clonepair195>

<clonepair195>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1729" endline="1740" pcid="5587"></source>
  def test_vocab_with_reserved_mask_element_fails(self):
    vocab_data = ["earth", "mask_token", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="mask_token",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string,
        invert=True)
    with self.assertRaisesRegex(ValueError, "reserved mask"):
      layer.set_vocabulary(vocab_data)

</clonepair195>
<clonepair196>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1668" endline="1680" pcid="5582"></source>
  def test_vocab_with_max_cap(self):
    vocab_data = ["", "[OOV]", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=5,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string,
        invert=True)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(vocab_data, returned_vocab)

</clonepair196>

<clonepair196>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1717" endline="1728" pcid="5586"></source>
  def test_vocab_with_repeated_element_fails(self):
    vocab_data = ["earth", "earth", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string,
        invert=True)
    with self.assertRaisesRegex(ValueError, "repeated term.*earth"):
      layer.set_vocabulary(vocab_data)

</clonepair196>
<clonepair197>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1467" endline="1478" pcid="5567"></source>
  def test_vocab_with_multiple_oov_indices(self):
    vocab_data = ["", "[OOV]", "[OOV]", "[OOV]", "wind"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=3,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(vocab_data, returned_vocab)

</clonepair197>

<clonepair197>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1668" endline="1680" pcid="5582"></source>
  def test_vocab_with_max_cap(self):
    vocab_data = ["", "[OOV]", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=5,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string,
        invert=True)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(vocab_data, returned_vocab)

</clonepair197>
<clonepair198>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/text_vectorization_test.py" startline="1591" endline="1603" pcid="5076"></source>
  def test_idf_weights_length_mismatch_fails(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    idf_weights = [1, 2, 3]
    with self.assertRaisesRegex(
        ValueError, "`idf_weights` must be the same length as vocab"):
      text_vectorization.TextVectorization(
          max_tokens=5,
          standardize=None,
          split=None,
          output_mode=text_vectorization.TF_IDF,
          vocabulary=vocab_data,
          idf_weights=idf_weights)

</clonepair198>

<clonepair198>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/text_vectorization_test.py" startline="1604" endline="1616" pcid="5077"></source>
  def test_set_tfidf_in_non_tfidf_fails(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    idf_weights = [1, 2, 3, 4]
    with self.assertRaisesRegex(ValueError,
                                "`idf_weights` should only be set if"):
      text_vectorization.TextVectorization(
          max_tokens=5,
          standardize=None,
          split=None,
          output_mode=text_vectorization.MULTI_HOT,
          vocabulary=vocab_data,
          idf_weights=idf_weights)

</clonepair198>
<clonepair199>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/text_vectorization_test.py" startline="1579" endline="1590" pcid="5075"></source>
  def test_setting_vocab_without_idf_weights_fails_in_tfidf_mode(self):
    vocab_data = ["earth", "wind", "and", "fire"]

    with self.assertRaisesRegex(
        ValueError, "`idf_weights` must be set if output_mode is TF_IDF"):
      text_vectorization.TextVectorization(
          max_tokens=5,
          standardize=None,
          split=None,
          output_mode=text_vectorization.TF_IDF,
          vocabulary=vocab_data)

</clonepair199>

<clonepair199>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/text_vectorization_test.py" startline="1591" endline="1603" pcid="5076"></source>
  def test_idf_weights_length_mismatch_fails(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    idf_weights = [1, 2, 3]
    with self.assertRaisesRegex(
        ValueError, "`idf_weights` must be the same length as vocab"):
      text_vectorization.TextVectorization(
          max_tokens=5,
          standardize=None,
          split=None,
          output_mode=text_vectorization.TF_IDF,
          vocabulary=vocab_data,
          idf_weights=idf_weights)

</clonepair199>
<clonepair200>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1441" endline="1453" pcid="5565"></source>
  def test_vocab_with_max_cap(self):
    vocab_data = ["", "[OOV]", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=5,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(vocab_data, returned_vocab)
    self.assertAllEqual(layer.vocabulary_size(), 5)

</clonepair200>

<clonepair200>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1454" endline="1466" pcid="5566"></source>
  def test_int_vocab_with_max_cap(self):
    vocab_data = [0, -1, 42, 1276, 1138]
    layer = index_lookup.IndexLookup(
        max_tokens=5,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1,
        vocabulary_dtype=tf.int64)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(vocab_data, returned_vocab)
    self.assertAllEqual(layer.vocabulary_size(), 5)

</clonepair200>
<clonepair201>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1454" endline="1466" pcid="5566"></source>
  def test_int_vocab_with_max_cap(self):
    vocab_data = [0, -1, 42, 1276, 1138]
    layer = index_lookup.IndexLookup(
        max_tokens=5,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1,
        vocabulary_dtype=tf.int64)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(vocab_data, returned_vocab)
    self.assertAllEqual(layer.vocabulary_size(), 5)

</clonepair201>

<clonepair201>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1479" endline="1490" pcid="5568"></source>
  def test_int_vocab_with_multiple_oov_indices(self):
    vocab_data = [0, -1, -1, -1, 42]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=3,
        mask_token=0,
        oov_token=-1,
        vocabulary_dtype=tf.int64)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(vocab_data, returned_vocab)

</clonepair201>
<clonepair202>
<source file="systems/keras-2.8.0/keras/engine/compile_utils_test.py" startline="349" endline="366" pcid="2893"></source>
  def test_custom_loss_callables(self):

    def custom_loss_fn(y_true, y_pred):
      return tf.reduce_sum(y_true - y_pred)

    class CustomLossClass:

      def __call__(self, y_true, y_pred):
        return tf.reduce_sum(y_true - y_pred)

    loss_container = compile_utils.LossesContainer(
        [custom_loss_fn, CustomLossClass()])
    y_t, y_p = tf.ones((10, 5)), tf.zeros((10, 5))
    loss_container(y_t, y_p)

    self.assertEqual(loss_container._losses[0].name, 'custom_loss_fn')
    self.assertEqual(loss_container._losses[1].name, 'custom_loss_class')

</clonepair202>

<clonepair202>
<source file="systems/keras-2.8.0/keras/engine/compile_utils_test.py" startline="775" endline="792" pcid="2916"></source>
  def test_custom_metric_callables(self):

    def custom_metric_fn(y_true, y_pred):
      return tf.reduce_sum(y_true - y_pred)

    class CustomMetricClass:

      def __call__(self, y_true, y_pred):
        return tf.reduce_sum(y_true - y_pred)

    metric_container = compile_utils.MetricsContainer(
        [custom_metric_fn, CustomMetricClass()])
    y_t, y_p = tf.ones((10, 5)), tf.zeros((10, 5))
    metric_container.update_state(y_t, y_p)

    self.assertEqual(metric_container.metrics[0].name, 'custom_metric_fn')
    self.assertEqual(metric_container.metrics[1].name, 'custom_metric_class')

</clonepair202>
<clonepair203>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1403" endline="1415" pcid="5562"></source>
  def test_get_vocabulary_no_special_tokens(self):
    vocab_data = ["", "[OOV]", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=5,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary(include_special_tokens=False)
    self.assertAllEqual(returned_vocab, ["wind", "and", "fire"])
    self.assertAllEqual(layer.vocabulary_size(), 5)

</clonepair203>

<clonepair203>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1441" endline="1453" pcid="5565"></source>
  def test_vocab_with_max_cap(self):
    vocab_data = ["", "[OOV]", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=5,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(vocab_data, returned_vocab)
    self.assertAllEqual(layer.vocabulary_size(), 5)

</clonepair203>
<clonepair204>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1441" endline="1453" pcid="5565"></source>
  def test_vocab_with_max_cap(self):
    vocab_data = ["", "[OOV]", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=5,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(vocab_data, returned_vocab)
    self.assertAllEqual(layer.vocabulary_size(), 5)

</clonepair204>

<clonepair204>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1467" endline="1478" pcid="5567"></source>
  def test_vocab_with_multiple_oov_indices(self):
    vocab_data = ["", "[OOV]", "[OOV]", "[OOV]", "wind"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=3,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(vocab_data, returned_vocab)

</clonepair204>
<clonepair205>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1416" endline="1427" pcid="5563"></source>
  def test_vocab_multi_oov(self):
    vocab_data = ["", "[OOV]", "[OOV]", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=2,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(returned_vocab, vocab_data)

</clonepair205>

<clonepair205>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1441" endline="1453" pcid="5565"></source>
  def test_vocab_with_max_cap(self):
    vocab_data = ["", "[OOV]", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=5,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(vocab_data, returned_vocab)
    self.assertAllEqual(layer.vocabulary_size(), 5)

</clonepair205>
<clonepair206>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1428" endline="1440" pcid="5564"></source>
  def test_vocab_multi_oov_not_present(self):
    vocab_data = ["wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=10,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(returned_vocab,
                        [""] + ["[OOV]"] * 10 + ["wind", "and", "fire"])

</clonepair206>

<clonepair206>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1467" endline="1478" pcid="5567"></source>
  def test_vocab_with_multiple_oov_indices(self):
    vocab_data = ["", "[OOV]", "[OOV]", "[OOV]", "wind"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=3,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(vocab_data, returned_vocab)

</clonepair206>
<clonepair207>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1416" endline="1427" pcid="5563"></source>
  def test_vocab_multi_oov(self):
    vocab_data = ["", "[OOV]", "[OOV]", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=2,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(returned_vocab, vocab_data)

</clonepair207>

<clonepair207>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1428" endline="1440" pcid="5564"></source>
  def test_vocab_multi_oov_not_present(self):
    vocab_data = ["wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=10,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(returned_vocab,
                        [""] + ["[OOV]"] * 10 + ["wind", "and", "fire"])

</clonepair207>
<clonepair208>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="727" endline="737" pcid="5532"></source>
  def test_int_output_shape(self):
    input_data = keras.Input(batch_size=16, shape=(4,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=2,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    int_data = layer(input_data)
    self.assertAllEqual(int_data.shape.as_list(), [16, 4])

</clonepair208>

<clonepair208>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="951" endline="963" pcid="5543"></source>
  def test_one_hot_output_shape(self):
    inputs = keras.Input(batch_size=16, shape=(1,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=["earth"],
        max_tokens=2,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        output_mode=index_lookup.ONE_HOT,
        vocabulary_dtype=tf.string)
    outputs = layer(inputs)
    self.assertAllEqual(outputs.shape.as_list(), [16, 2])

</clonepair208>
<clonepair209>
<source file="systems/keras-2.8.0/keras/losses_test.py" startline="1000" endline="1037" pcid="1932"></source>
  def test_scalar_weighted(self):
    y_true = np.asarray([1, 0, 1, 0]).reshape([2, 2])
    y_pred = np.asarray([0.9, 0.8, 0.7, 0.2], dtype=np.float32).reshape([2, 2])
    obj = losses.BinaryFocalCrossentropy(gamma=2.0)
    loss = obj(y_true, y_pred, sample_weight=1.23)

    # p_t = y_true y_pred + (1 - y_true) (1 - y_pred) = [[0.9, 0.2], [0.7, 0.8]]
    # focal = (1 - p_t) ** gamma = [[0.01, 0.64], [0.09, 0.04]]

    # bceLoss = -log(p_t) = [[0.105, 1.609] ,[0.357, 0.223]] * sample_weight
    # focalLoss = focal bceLoss
    #           = [[0.001, 1.03], [0.032, 0.009]] * sample_weight
    # Reduced loss = (0.001 + 1.03 + 0.032 + 0.009) * 1.23 / 4 = 0.3296

    self.assertAlmostEqual(self.evaluate(loss), 0.3296, 3)

    # Test with logits.
    y_true = tf.constant([[1, 1, 0], [0, 1, 0]], dtype=tf.float32)
    logits = tf.constant([[1.5, -2.7, 2.9], [-3.8, 1.2, -4.5]])
    obj = losses.BinaryFocalCrossentropy(gamma=3.0, from_logits=True)
    loss = obj(y_true, logits, sample_weight=3.21)

    # sigmoidal = sigmoid(logits)
    #           = [[0.8176, 0.063, 0.9478], [0.0219, 0.7685, 0.011]]
    # p_t = y_true sigmoidal + (1 - y_true) (1 - sigmoidal)
    #     = [[0.8176, 0.063, 0.0522], [0.9781, 0.7685, 0.989]]
    # focal = (1 - p_t) ** gamma
    #       = [[0.006, 0.823, 0.851], [0.00001, 0.0124, 0.000001]]

    # bceLoss = -log(p_t) * sample_weight
    # = [[0.2014, 2.7646 , 2.9527], [0.0221, 0.2633, 0.01106]] * sample_weight

    # focalLoss = focal * bceLoss =
    # [[0.0012, 2.2743, 2.514], [0.0000002, 0.0033, 0.00000001]] * sample_weight
    # Reduced loss = 0.799 * 3.21 = 2.565

    self.assertAlmostEqual(self.evaluate(loss), 2.565, 3)

</clonepair209>

<clonepair209>
<source file="systems/keras-2.8.0/keras/losses_test.py" startline="1038" endline="1079" pcid="1933"></source>
  def test_sample_weighted(self):
    y_true = np.asarray([1, 0, 1, 0]).reshape([2, 2])
    y_pred = np.asarray([0.9, 0.8, 0.7, 0.2], dtype=np.float32).reshape([2, 2])
    sample_weight = tf.constant([1.2, 3.4], shape=(2, 1))
    obj = losses.BinaryFocalCrossentropy(gamma=2.0)
    loss = obj(y_true, y_pred, sample_weight=sample_weight)

    # p_t = y_true y_pred + (1 - y_true) (1 - y_pred) = [[0.9, 0.2], [0.7, 0.8]]
    # focal = (1 - p_t) ** gamma = [[0.01, 0.64], [0.09, 0.04]]

    # bceLoss = -log(p_t) * sample_weight
    #         = [[0.105, 1.609] ,[0.357, 0.223]] * sample_weight
    # focalLoss = focal * bceLoss
    #           = [[0.001, 1.03], [0.032, 0.009]] * sample_weight
    #           = [[0.0012, 1.236], [0.1088, 0.0306]]
    # Reduced loss = (0.0012 + 1.236 + 0.1088 + 0.0306) / 4 = 0.34415

    self.assertAlmostEqual(self.evaluate(loss), 0.34415, 3)

    # Test with logits.
    y_true = tf.constant([[1, 1, 0], [0, 1, 0]], dtype=tf.float32)
    logits = tf.constant([[1.5, -2.7, 2.9], [-3.8, 1.2, -4.5]])
    obj = losses.BinaryFocalCrossentropy(gamma=3.0, from_logits=True)
    loss = obj(y_true, logits, sample_weight=sample_weight)

    # sigmoidal = sigmoid(logits)
    #           = [[0.8176, 0.063, 0.9478], [0.0219, 0.7685, 0.011]]
    # p_t = y_true sigmoidal + (1 - y_true) (1 - sigmoidal)
    #     = [[0.8176, 0.063, 0.0522], [0.9781, 0.7685, 0.989]]
    # focal = (1 - p_t) ** gamma
    #       = [[0.006, 0.823, 0.851], [0.00001, 0.0124, 0.000001]]

    # bceLoss = -log(p_t) * sample_weight
    # = [[0.2014, 2.7646 , 2.9527], [0.0221, 0.2633, 0.01106]] * sample_weight

    # focalLoss = focal * bceLoss =
    # [[0.0012, 2.2743, 2.514], [0.0000002, 0.0033, 0.00000001]] * sample_weight
    # focalLoss = [[0.00144, 2.72916, 3.0168], [6.8e-7, 0.01122, 3.4e-8]]
    # Reduced loss = 0.799

    self.assertAlmostEqual(self.evaluate(loss), 0.95977, 3)

</clonepair209>
<clonepair210>
<source file="systems/keras-2.8.0/keras/losses_test.py" startline="963" endline="999" pcid="1931"></source>
  def test_unweighted(self):
    y_true = np.asarray([1, 0, 1, 0]).reshape([2, 2])
    y_pred = np.asarray([0.9, 0.8, 0.7, 0.2], dtype=np.float32).reshape([2, 2])
    obj = losses.BinaryFocalCrossentropy(gamma=2.0)
    loss = obj(y_true, y_pred)

    # p_t = y_true y_pred + (1 - y_true) (1 - y_pred) = [[0.9, 0.2], [0.7, 0.8]]
    # focal = (1 - p_t) ** gamma = [[0.01, 0.64], [0.09, 0.04]]

    # bceLoss = -log(p_t) = [[0.105, 1.609] ,[0.357, 0.223]]
    # focalLoss = focal bceLoss = [[0.001, 1.03], [0.032, 0.009]]
    # Reduced loss = (0.001 + 1.03 + 0.032 + 0.009) / 4 = 0.268

    self.assertAlmostEqual(self.evaluate(loss), 0.268, 3)

    # Test with logits.
    y_true = tf.constant([[1, 1, 0], [0, 1, 0]], dtype=tf.float32)
    logits = tf.constant([[1.5, -2.7, 2.9], [-3.8, 1.2, -4.5]])
    obj = losses.BinaryFocalCrossentropy(gamma=3.0, from_logits=True)
    loss = obj(y_true, logits)

    # sigmoidal = sigmoid(logits)
    #           = [[0.8176, 0.063, 0.9478], [0.0219, 0.7685, 0.011]]
    # p_t = y_true sigmoidal + (1 - y_true) (1 - sigmoidal)
    #     = [[0.8176, 0.063, 0.0522], [0.9781, 0.7685, 0.989]]
    # focal = (1 - p_t) ** gamma
    #       = [[0.006, 0.823, 0.851], [0.00001, 0.0124, 0.000001]]

    # bceLoss = -log(p_t)
    #         = [[0.2014, 2.7646 , 2.9527], [0.0221, 0.2633, 0.01106]]

    # focalLoss = focal bceLoss
    #           = [[0.0012, 2.2743, 2.514], [0.0000002, 0.0033, 0.00000001]]
    # Reduced loss = 0.799

    self.assertAlmostEqual(self.evaluate(loss), 0.799, 3)

</clonepair210>

<clonepair210>
<source file="systems/keras-2.8.0/keras/losses_test.py" startline="1038" endline="1079" pcid="1933"></source>
  def test_sample_weighted(self):
    y_true = np.asarray([1, 0, 1, 0]).reshape([2, 2])
    y_pred = np.asarray([0.9, 0.8, 0.7, 0.2], dtype=np.float32).reshape([2, 2])
    sample_weight = tf.constant([1.2, 3.4], shape=(2, 1))
    obj = losses.BinaryFocalCrossentropy(gamma=2.0)
    loss = obj(y_true, y_pred, sample_weight=sample_weight)

    # p_t = y_true y_pred + (1 - y_true) (1 - y_pred) = [[0.9, 0.2], [0.7, 0.8]]
    # focal = (1 - p_t) ** gamma = [[0.01, 0.64], [0.09, 0.04]]

    # bceLoss = -log(p_t) * sample_weight
    #         = [[0.105, 1.609] ,[0.357, 0.223]] * sample_weight
    # focalLoss = focal * bceLoss
    #           = [[0.001, 1.03], [0.032, 0.009]] * sample_weight
    #           = [[0.0012, 1.236], [0.1088, 0.0306]]
    # Reduced loss = (0.0012 + 1.236 + 0.1088 + 0.0306) / 4 = 0.34415

    self.assertAlmostEqual(self.evaluate(loss), 0.34415, 3)

    # Test with logits.
    y_true = tf.constant([[1, 1, 0], [0, 1, 0]], dtype=tf.float32)
    logits = tf.constant([[1.5, -2.7, 2.9], [-3.8, 1.2, -4.5]])
    obj = losses.BinaryFocalCrossentropy(gamma=3.0, from_logits=True)
    loss = obj(y_true, logits, sample_weight=sample_weight)

    # sigmoidal = sigmoid(logits)
    #           = [[0.8176, 0.063, 0.9478], [0.0219, 0.7685, 0.011]]
    # p_t = y_true sigmoidal + (1 - y_true) (1 - sigmoidal)
    #     = [[0.8176, 0.063, 0.0522], [0.9781, 0.7685, 0.989]]
    # focal = (1 - p_t) ** gamma
    #       = [[0.006, 0.823, 0.851], [0.00001, 0.0124, 0.000001]]

    # bceLoss = -log(p_t) * sample_weight
    # = [[0.2014, 2.7646 , 2.9527], [0.0221, 0.2633, 0.01106]] * sample_weight

    # focalLoss = focal * bceLoss =
    # [[0.0012, 2.2743, 2.514], [0.0000002, 0.0033, 0.00000001]] * sample_weight
    # focalLoss = [[0.00144, 2.72916, 3.0168], [6.8e-7, 0.01122, 3.4e-8]]
    # Reduced loss = 0.799

    self.assertAlmostEqual(self.evaluate(loss), 0.95977, 3)

</clonepair210>
<clonepair211>
<source file="systems/keras-2.8.0/keras/losses_test.py" startline="1151" endline="1164" pcid="1939"></source>
  def test_scalar_weighted(self):
    cce_obj = losses.CategoricalCrossentropy()
    y_true = tf.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]])
    y_pred = tf.constant([[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]],
                         dtype=tf.float32)
    loss = cce_obj(y_true, y_pred, sample_weight=2.3)
    self.assertAlmostEqual(self.evaluate(loss), .7449, 3)

    # Test with logits.
    logits = tf.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])
    cce_obj = losses.CategoricalCrossentropy(from_logits=True)
    loss = cce_obj(y_true, logits, sample_weight=2.3)
    self.assertAlmostEqual(self.evaluate(loss), .1317, 3)

</clonepair211>

<clonepair211>
<source file="systems/keras-2.8.0/keras/losses_test.py" startline="1165" endline="1179" pcid="1940"></source>
  def test_sample_weighted(self):
    cce_obj = losses.CategoricalCrossentropy()
    y_true = tf.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]])
    y_pred = tf.constant([[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]],
                         dtype=tf.float32)
    sample_weight = tf.constant([[1.2], [3.4], [5.6]], shape=(3, 1))
    loss = cce_obj(y_true, y_pred, sample_weight=sample_weight)
    self.assertAlmostEqual(self.evaluate(loss), 1.0696, 3)

    # Test with logits.
    logits = tf.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])
    cce_obj = losses.CategoricalCrossentropy(from_logits=True)
    loss = cce_obj(y_true, logits, sample_weight=sample_weight)
    self.assertAlmostEqual(self.evaluate(loss), 0.31829, 3)

</clonepair211>
<clonepair212>
<source file="systems/keras-2.8.0/keras/losses_test.py" startline="1137" endline="1150" pcid="1938"></source>
  def test_unweighted(self):
    cce_obj = losses.CategoricalCrossentropy()
    y_true = tf.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]])
    y_pred = tf.constant([[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]],
                         dtype=tf.float32)
    loss = cce_obj(y_true, y_pred)
    self.assertAlmostEqual(self.evaluate(loss), .3239, 3)

    # Test with logits.
    logits = tf.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])
    cce_obj = losses.CategoricalCrossentropy(from_logits=True)
    loss = cce_obj(y_true, logits)
    self.assertAlmostEqual(self.evaluate(loss), .0573, 3)

</clonepair212>

<clonepair212>
<source file="systems/keras-2.8.0/keras/losses_test.py" startline="1165" endline="1179" pcid="1940"></source>
  def test_sample_weighted(self):
    cce_obj = losses.CategoricalCrossentropy()
    y_true = tf.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]])
    y_pred = tf.constant([[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]],
                         dtype=tf.float32)
    sample_weight = tf.constant([[1.2], [3.4], [5.6]], shape=(3, 1))
    loss = cce_obj(y_true, y_pred, sample_weight=sample_weight)
    self.assertAlmostEqual(self.evaluate(loss), 1.0696, 3)

    # Test with logits.
    logits = tf.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])
    cce_obj = losses.CategoricalCrossentropy(from_logits=True)
    loss = cce_obj(y_true, logits, sample_weight=sample_weight)
    self.assertAlmostEqual(self.evaluate(loss), 0.31829, 3)

</clonepair212>
<clonepair213>
<source file="systems/keras-2.8.0/keras/losses_test.py" startline="1317" endline="1330" pcid="1950"></source>
  def test_scalar_weighted(self):
    cce_obj = losses.SparseCategoricalCrossentropy()
    y_true = tf.constant([[0], [1], [2]])
    y_pred = tf.constant([[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]],
                         dtype=tf.float32)
    loss = cce_obj(y_true, y_pred, sample_weight=2.3)
    self.assertAlmostEqual(self.evaluate(loss), .7449, 3)

    # Test with logits.
    logits = tf.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])
    cce_obj = losses.SparseCategoricalCrossentropy(from_logits=True)
    loss = cce_obj(y_true, logits, sample_weight=2.3)
    self.assertAlmostEqual(self.evaluate(loss), .1317, 3)

</clonepair213>

<clonepair213>
<source file="systems/keras-2.8.0/keras/losses_test.py" startline="1331" endline="1345" pcid="1951"></source>
  def test_sample_weighted(self):
    cce_obj = losses.SparseCategoricalCrossentropy()
    y_true = tf.constant([[0], [1], [2]])
    y_pred = tf.constant([[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]],
                         dtype=tf.float32)
    sample_weight = tf.constant([[1.2], [3.4], [5.6]], shape=(3, 1))
    loss = cce_obj(y_true, y_pred, sample_weight=sample_weight)
    self.assertAlmostEqual(self.evaluate(loss), 1.0696, 3)

    # Test with logits.
    logits = tf.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])
    cce_obj = losses.SparseCategoricalCrossentropy(from_logits=True)
    loss = cce_obj(y_true, logits, sample_weight=sample_weight)
    self.assertAlmostEqual(self.evaluate(loss), 0.31829, 3)

</clonepair213>
<clonepair214>
<source file="systems/keras-2.8.0/keras/losses_test.py" startline="1303" endline="1316" pcid="1949"></source>
  def test_unweighted(self):
    cce_obj = losses.SparseCategoricalCrossentropy()
    y_true = tf.constant([0, 1, 2])
    y_pred = tf.constant([[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]],
                         dtype=tf.float32)
    loss = cce_obj(y_true, y_pred)
    self.assertAlmostEqual(self.evaluate(loss), .3239, 3)

    # Test with logits.
    logits = tf.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])
    cce_obj = losses.SparseCategoricalCrossentropy(from_logits=True)
    loss = cce_obj(y_true, logits)
    self.assertAlmostEqual(self.evaluate(loss), .0573, 3)

</clonepair214>

<clonepair214>
<source file="systems/keras-2.8.0/keras/losses_test.py" startline="1317" endline="1330" pcid="1950"></source>
  def test_scalar_weighted(self):
    cce_obj = losses.SparseCategoricalCrossentropy()
    y_true = tf.constant([[0], [1], [2]])
    y_pred = tf.constant([[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]],
                         dtype=tf.float32)
    loss = cce_obj(y_true, y_pred, sample_weight=2.3)
    self.assertAlmostEqual(self.evaluate(loss), .7449, 3)

    # Test with logits.
    logits = tf.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])
    cce_obj = losses.SparseCategoricalCrossentropy(from_logits=True)
    loss = cce_obj(y_true, logits, sample_weight=2.3)
    self.assertAlmostEqual(self.evaluate(loss), .1317, 3)

</clonepair214>
<clonepair215>
<source file="systems/keras-2.8.0/keras/losses_test.py" startline="1151" endline="1164" pcid="1939"></source>
  def test_scalar_weighted(self):
    cce_obj = losses.CategoricalCrossentropy()
    y_true = tf.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]])
    y_pred = tf.constant([[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]],
                         dtype=tf.float32)
    loss = cce_obj(y_true, y_pred, sample_weight=2.3)
    self.assertAlmostEqual(self.evaluate(loss), .7449, 3)

    # Test with logits.
    logits = tf.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])
    cce_obj = losses.CategoricalCrossentropy(from_logits=True)
    loss = cce_obj(y_true, logits, sample_weight=2.3)
    self.assertAlmostEqual(self.evaluate(loss), .1317, 3)

</clonepair215>

<clonepair215>
<source file="systems/keras-2.8.0/keras/losses_test.py" startline="1303" endline="1316" pcid="1949"></source>
  def test_unweighted(self):
    cce_obj = losses.SparseCategoricalCrossentropy()
    y_true = tf.constant([0, 1, 2])
    y_pred = tf.constant([[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]],
                         dtype=tf.float32)
    loss = cce_obj(y_true, y_pred)
    self.assertAlmostEqual(self.evaluate(loss), .3239, 3)

    # Test with logits.
    logits = tf.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])
    cce_obj = losses.SparseCategoricalCrossentropy(from_logits=True)
    loss = cce_obj(y_true, logits)
    self.assertAlmostEqual(self.evaluate(loss), .0573, 3)

</clonepair215>
<clonepair216>
<source file="systems/keras-2.8.0/keras/losses_test.py" startline="1123" endline="1136" pcid="1937"></source>
  def test_all_correct_unweighted(self):
    y_true = tf.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype=tf.int64)
    y_pred = tf.constant([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]],
                         dtype=tf.float32)
    cce_obj = losses.CategoricalCrossentropy()
    loss = cce_obj(y_true, y_pred)
    self.assertAlmostEqual(self.evaluate(loss), 0.0, 3)

    # Test with logits.
    logits = tf.constant([[10., 0., 0.], [0., 10., 0.], [0., 0., 10.]])
    cce_obj = losses.CategoricalCrossentropy(from_logits=True)
    loss = cce_obj(y_true, logits)
    self.assertAlmostEqual(self.evaluate(loss), 0.0, 3)

</clonepair216>

<clonepair216>
<source file="systems/keras-2.8.0/keras/losses_test.py" startline="1303" endline="1316" pcid="1949"></source>
  def test_unweighted(self):
    cce_obj = losses.SparseCategoricalCrossentropy()
    y_true = tf.constant([0, 1, 2])
    y_pred = tf.constant([[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]],
                         dtype=tf.float32)
    loss = cce_obj(y_true, y_pred)
    self.assertAlmostEqual(self.evaluate(loss), .3239, 3)

    # Test with logits.
    logits = tf.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])
    cce_obj = losses.SparseCategoricalCrossentropy(from_logits=True)
    loss = cce_obj(y_true, logits)
    self.assertAlmostEqual(self.evaluate(loss), .0573, 3)

</clonepair216>
<clonepair217>
<source file="systems/keras-2.8.0/keras/losses_test.py" startline="1137" endline="1150" pcid="1938"></source>
  def test_unweighted(self):
    cce_obj = losses.CategoricalCrossentropy()
    y_true = tf.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]])
    y_pred = tf.constant([[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]],
                         dtype=tf.float32)
    loss = cce_obj(y_true, y_pred)
    self.assertAlmostEqual(self.evaluate(loss), .3239, 3)

    # Test with logits.
    logits = tf.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])
    cce_obj = losses.CategoricalCrossentropy(from_logits=True)
    loss = cce_obj(y_true, logits)
    self.assertAlmostEqual(self.evaluate(loss), .0573, 3)

</clonepair217>

<clonepair217>
<source file="systems/keras-2.8.0/keras/losses_test.py" startline="1317" endline="1330" pcid="1950"></source>
  def test_scalar_weighted(self):
    cce_obj = losses.SparseCategoricalCrossentropy()
    y_true = tf.constant([[0], [1], [2]])
    y_pred = tf.constant([[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]],
                         dtype=tf.float32)
    loss = cce_obj(y_true, y_pred, sample_weight=2.3)
    self.assertAlmostEqual(self.evaluate(loss), .7449, 3)

    # Test with logits.
    logits = tf.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])
    cce_obj = losses.SparseCategoricalCrossentropy(from_logits=True)
    loss = cce_obj(y_true, logits, sample_weight=2.3)
    self.assertAlmostEqual(self.evaluate(loss), .1317, 3)

</clonepair217>
<clonepair218>
<source file="systems/keras-2.8.0/keras/losses_test.py" startline="1137" endline="1150" pcid="1938"></source>
  def test_unweighted(self):
    cce_obj = losses.CategoricalCrossentropy()
    y_true = tf.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]])
    y_pred = tf.constant([[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]],
                         dtype=tf.float32)
    loss = cce_obj(y_true, y_pred)
    self.assertAlmostEqual(self.evaluate(loss), .3239, 3)

    # Test with logits.
    logits = tf.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])
    cce_obj = losses.CategoricalCrossentropy(from_logits=True)
    loss = cce_obj(y_true, logits)
    self.assertAlmostEqual(self.evaluate(loss), .0573, 3)

</clonepair218>

<clonepair218>
<source file="systems/keras-2.8.0/keras/losses_test.py" startline="1151" endline="1164" pcid="1939"></source>
  def test_scalar_weighted(self):
    cce_obj = losses.CategoricalCrossentropy()
    y_true = tf.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]])
    y_pred = tf.constant([[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]],
                         dtype=tf.float32)
    loss = cce_obj(y_true, y_pred, sample_weight=2.3)
    self.assertAlmostEqual(self.evaluate(loss), .7449, 3)

    # Test with logits.
    logits = tf.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])
    cce_obj = losses.CategoricalCrossentropy(from_logits=True)
    loss = cce_obj(y_true, logits, sample_weight=2.3)
    self.assertAlmostEqual(self.evaluate(loss), .1317, 3)

</clonepair218>
<clonepair219>
<source file="systems/keras-2.8.0/keras/losses_test.py" startline="1123" endline="1136" pcid="1937"></source>
  def test_all_correct_unweighted(self):
    y_true = tf.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype=tf.int64)
    y_pred = tf.constant([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]],
                         dtype=tf.float32)
    cce_obj = losses.CategoricalCrossentropy()
    loss = cce_obj(y_true, y_pred)
    self.assertAlmostEqual(self.evaluate(loss), 0.0, 3)

    # Test with logits.
    logits = tf.constant([[10., 0., 0.], [0., 10., 0.], [0., 0., 10.]])
    cce_obj = losses.CategoricalCrossentropy(from_logits=True)
    loss = cce_obj(y_true, logits)
    self.assertAlmostEqual(self.evaluate(loss), 0.0, 3)

</clonepair219>

<clonepair219>
<source file="systems/keras-2.8.0/keras/losses_test.py" startline="1137" endline="1150" pcid="1938"></source>
  def test_unweighted(self):
    cce_obj = losses.CategoricalCrossentropy()
    y_true = tf.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]])
    y_pred = tf.constant([[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]],
                         dtype=tf.float32)
    loss = cce_obj(y_true, y_pred)
    self.assertAlmostEqual(self.evaluate(loss), .3239, 3)

    # Test with logits.
    logits = tf.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])
    cce_obj = losses.CategoricalCrossentropy(from_logits=True)
    loss = cce_obj(y_true, logits)
    self.assertAlmostEqual(self.evaluate(loss), .0573, 3)

</clonepair219>
<clonepair220>
<source file="systems/keras-2.8.0/keras/losses_test.py" startline="732" endline="744" pcid="1921"></source>
  def test_all_correct_unweighted(self):
    y_true = tf.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype=tf.float32)
    bce_obj = losses.BinaryCrossentropy()
    loss = bce_obj(y_true, y_true)
    self.assertAlmostEqual(self.evaluate(loss), 0.0, 3)

    # Test with logits.
    logits = tf.constant([[100.0, -100.0, -100.0], [-100.0, 100.0, -100.0],
                          [-100.0, -100.0, 100.0]])
    bce_obj = losses.BinaryCrossentropy(from_logits=True)
    loss = bce_obj(y_true, logits)
    self.assertAlmostEqual(self.evaluate(loss), 0.0, 3)

</clonepair220>

<clonepair220>
<source file="systems/keras-2.8.0/keras/losses_test.py" startline="1123" endline="1136" pcid="1937"></source>
  def test_all_correct_unweighted(self):
    y_true = tf.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype=tf.int64)
    y_pred = tf.constant([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]],
                         dtype=tf.float32)
    cce_obj = losses.CategoricalCrossentropy()
    loss = cce_obj(y_true, y_pred)
    self.assertAlmostEqual(self.evaluate(loss), 0.0, 3)

    # Test with logits.
    logits = tf.constant([[10., 0., 0.], [0., 10., 0.], [0., 0., 10.]])
    cce_obj = losses.CategoricalCrossentropy(from_logits=True)
    loss = cce_obj(y_true, logits)
    self.assertAlmostEqual(self.evaluate(loss), 0.0, 3)

</clonepair220>
<clonepair221>
<source file="systems/keras-2.8.0/keras/losses_test.py" startline="963" endline="999" pcid="1931"></source>
  def test_unweighted(self):
    y_true = np.asarray([1, 0, 1, 0]).reshape([2, 2])
    y_pred = np.asarray([0.9, 0.8, 0.7, 0.2], dtype=np.float32).reshape([2, 2])
    obj = losses.BinaryFocalCrossentropy(gamma=2.0)
    loss = obj(y_true, y_pred)

    # p_t = y_true y_pred + (1 - y_true) (1 - y_pred) = [[0.9, 0.2], [0.7, 0.8]]
    # focal = (1 - p_t) ** gamma = [[0.01, 0.64], [0.09, 0.04]]

    # bceLoss = -log(p_t) = [[0.105, 1.609] ,[0.357, 0.223]]
    # focalLoss = focal bceLoss = [[0.001, 1.03], [0.032, 0.009]]
    # Reduced loss = (0.001 + 1.03 + 0.032 + 0.009) / 4 = 0.268

    self.assertAlmostEqual(self.evaluate(loss), 0.268, 3)

    # Test with logits.
    y_true = tf.constant([[1, 1, 0], [0, 1, 0]], dtype=tf.float32)
    logits = tf.constant([[1.5, -2.7, 2.9], [-3.8, 1.2, -4.5]])
    obj = losses.BinaryFocalCrossentropy(gamma=3.0, from_logits=True)
    loss = obj(y_true, logits)

    # sigmoidal = sigmoid(logits)
    #           = [[0.8176, 0.063, 0.9478], [0.0219, 0.7685, 0.011]]
    # p_t = y_true sigmoidal + (1 - y_true) (1 - sigmoidal)
    #     = [[0.8176, 0.063, 0.0522], [0.9781, 0.7685, 0.989]]
    # focal = (1 - p_t) ** gamma
    #       = [[0.006, 0.823, 0.851], [0.00001, 0.0124, 0.000001]]

    # bceLoss = -log(p_t)
    #         = [[0.2014, 2.7646 , 2.9527], [0.0221, 0.2633, 0.01106]]

    # focalLoss = focal bceLoss
    #           = [[0.0012, 2.2743, 2.514], [0.0000002, 0.0033, 0.00000001]]
    # Reduced loss = 0.799

    self.assertAlmostEqual(self.evaluate(loss), 0.799, 3)

</clonepair221>

<clonepair221>
<source file="systems/keras-2.8.0/keras/losses_test.py" startline="1000" endline="1037" pcid="1932"></source>
  def test_scalar_weighted(self):
    y_true = np.asarray([1, 0, 1, 0]).reshape([2, 2])
    y_pred = np.asarray([0.9, 0.8, 0.7, 0.2], dtype=np.float32).reshape([2, 2])
    obj = losses.BinaryFocalCrossentropy(gamma=2.0)
    loss = obj(y_true, y_pred, sample_weight=1.23)

    # p_t = y_true y_pred + (1 - y_true) (1 - y_pred) = [[0.9, 0.2], [0.7, 0.8]]
    # focal = (1 - p_t) ** gamma = [[0.01, 0.64], [0.09, 0.04]]

    # bceLoss = -log(p_t) = [[0.105, 1.609] ,[0.357, 0.223]] * sample_weight
    # focalLoss = focal bceLoss
    #           = [[0.001, 1.03], [0.032, 0.009]] * sample_weight
    # Reduced loss = (0.001 + 1.03 + 0.032 + 0.009) * 1.23 / 4 = 0.3296

    self.assertAlmostEqual(self.evaluate(loss), 0.3296, 3)

    # Test with logits.
    y_true = tf.constant([[1, 1, 0], [0, 1, 0]], dtype=tf.float32)
    logits = tf.constant([[1.5, -2.7, 2.9], [-3.8, 1.2, -4.5]])
    obj = losses.BinaryFocalCrossentropy(gamma=3.0, from_logits=True)
    loss = obj(y_true, logits, sample_weight=3.21)

    # sigmoidal = sigmoid(logits)
    #           = [[0.8176, 0.063, 0.9478], [0.0219, 0.7685, 0.011]]
    # p_t = y_true sigmoidal + (1 - y_true) (1 - sigmoidal)
    #     = [[0.8176, 0.063, 0.0522], [0.9781, 0.7685, 0.989]]
    # focal = (1 - p_t) ** gamma
    #       = [[0.006, 0.823, 0.851], [0.00001, 0.0124, 0.000001]]

    # bceLoss = -log(p_t) * sample_weight
    # = [[0.2014, 2.7646 , 2.9527], [0.0221, 0.2633, 0.01106]] * sample_weight

    # focalLoss = focal * bceLoss =
    # [[0.0012, 2.2743, 2.514], [0.0000002, 0.0033, 0.00000001]] * sample_weight
    # Reduced loss = 0.799 * 3.21 = 2.565

    self.assertAlmostEqual(self.evaluate(loss), 2.565, 3)

</clonepair221>
<clonepair222>
<source file="systems/keras-2.8.0/keras/losses_test.py" startline="745" endline="781" pcid="1922"></source>
  def test_unweighted(self):
    y_true = np.asarray([1, 0, 1, 0]).reshape([2, 2])
    y_pred = np.asarray([1, 1, 1, 0], dtype=np.float32).reshape([2, 2])
    bce_obj = losses.BinaryCrossentropy()
    loss = bce_obj(y_true, y_pred)

    # EPSILON = 1e-7, y = y_true, y` = y_pred, Y_MAX = 0.9999999
    # y` = clip_ops.clip_by_value(output, EPSILON, 1. - EPSILON)
    # y` = [Y_MAX, Y_MAX, Y_MAX, EPSILON]

    # Loss = -(y log(y` + EPSILON) + (1 - y) log(1 - y` + EPSILON))
    #      = [-log(Y_MAX + EPSILON), -log(1 - Y_MAX + EPSILON),
    #         -log(Y_MAX + EPSILON), -log(1)]
    #      = [0, 15.33, 0, 0]
    # Reduced loss = 15.33 / 4

    self.assertAlmostEqual(self.evaluate(loss), 3.833, 3)

    # Test with logits.
    y_true = tf.constant([[1, 0, 1], [0, 1, 1]])
    logits = tf.constant([[100.0, -100.0, 100.0], [100.0, 100.0, -100.0]])
    bce_obj = losses.BinaryCrossentropy(from_logits=True)
    loss = bce_obj(y_true, logits)

    # Loss = max(x, 0) - x * z + log(1 + exp(-abs(x)))
    #            (where x = logits and z = y_true)
    #      = [((100 - 100 * 1 + log(1 + exp(-100))) +
    #          (0 + 100 * 0 + log(1 + exp(-100))) +
    #          (100 - 100 * 1 + log(1 + exp(-100))),
    #         ((100 - 100 * 0 + log(1 + exp(-100))) +
    #          (100 - 100 * 1 + log(1 + exp(-100))) +
    #          (0 + 100 * 1 + log(1 + exp(-100))))]
    #      = [(0 + 0 + 0) / 3, 200 / 3]
    # Reduced loss = (0 + 66.666) / 2

    self.assertAlmostEqual(self.evaluate(loss), 33.333, 3)

</clonepair222>

<clonepair222>
<source file="systems/keras-2.8.0/keras/losses_test.py" startline="782" endline="814" pcid="1923"></source>
  def test_scalar_weighted(self):
    bce_obj = losses.BinaryCrossentropy()
    y_true = np.asarray([1, 0, 1, 0]).reshape([2, 2])
    y_pred = np.asarray([1, 1, 1, 0], dtype=np.float32).reshape([2, 2])
    loss = bce_obj(y_true, y_pred, sample_weight=2.3)

    # EPSILON = 1e-7, y = y_true, y` = y_pred, Y_MAX = 0.9999999
    # y` = clip_ops.clip_by_value(output, EPSILON, 1. - EPSILON)
    # y` = [Y_MAX, Y_MAX, Y_MAX, EPSILON]

    # Loss = -(y log(y` + EPSILON) + (1 - y) log(1 - y` + EPSILON))
    #      = [-log(Y_MAX + EPSILON), -log(1 - Y_MAX + EPSILON),
    #         -log(Y_MAX + EPSILON), -log(1)]
    #      = [0, 15.33, 0, 0]
    # Weighted loss = [0, 15.33 * 2.3, 0, 0]
    # Reduced loss = 15.33 * 2.3 / 4

    self.assertAlmostEqual(self.evaluate(loss), 8.817, 3)

    # Test with logits.
    y_true = tf.constant([[1, 0, 1], [0, 1, 1]])
    logits = tf.constant([[100.0, -100.0, 100.0], [100.0, 100.0, -100.0]])
    bce_obj = losses.BinaryCrossentropy(from_logits=True)
    loss = bce_obj(y_true, logits, sample_weight=2.3)

    # Loss = max(x, 0) - x * z + log(1 + exp(-abs(x)))
    #            (where x = logits and z = y_true)
    # Loss = [(0 + 0 + 0) / 3, 200 / 3]
    # Weighted loss = [0 * 2.3, 66.666 * 2.3]
    # Reduced loss = (0 + 66.666 * 2.3) / 2

    self.assertAlmostEqual(self.evaluate(loss), 76.667, 3)

</clonepair222>
<clonepair223>
<source file="systems/keras-2.8.0/keras/wrappers/scikit_learn_test.py" startline="114" endline="129" pcid="840"></source>
  def test_classify_class_build_fn(self):

    class ClassBuildFnClf:

      def __call__(self, hidden_dim):
        return build_fn_clf(hidden_dim)

    with self.cached_session():
      clf = scikit_learn.KerasClassifier(
          build_fn=ClassBuildFnClf(),
          hidden_dim=HIDDEN_DIM,
          batch_size=BATCH_SIZE,
          epochs=EPOCHS)

      assert_classification_works(clf)

</clonepair223>

<clonepair223>
<source file="systems/keras-2.8.0/keras/wrappers/scikit_learn_test.py" startline="156" endline="171" pcid="845"></source>
  def test_regression_class_build_fn(self):

    class ClassBuildFnReg:

      def __call__(self, hidden_dim):
        return build_fn_reg(hidden_dim)

    with self.cached_session():
      reg = scikit_learn.KerasRegressor(
          build_fn=ClassBuildFnReg(),
          hidden_dim=HIDDEN_DIM,
          batch_size=BATCH_SIZE,
          epochs=EPOCHS)

      assert_regression_works(reg)

</clonepair223>
<clonepair224>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1741" endline="1752" pcid="5588"></source>
  def test_non_unique_int_vocab_fails(self):
    vocab_data = [12, 13, 14, 15, 15]
    with self.assertRaisesRegex(ValueError, "repeated term.*15"):
      _ = index_lookup.IndexLookup(
          vocabulary=vocab_data,
          max_tokens=None,
          num_oov_indices=1,
          mask_token=0,
          oov_token=-1,
          vocabulary_dtype=tf.int64,
          invert=True)

</clonepair224>

<clonepair224>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1753" endline="1765" pcid="5589"></source>
  def test_int_vocab_with_repeated_element_fails(self):
    vocab_data = [11, 11, 34, 23, 124]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1,
        vocabulary_dtype=tf.int64,
        invert=True)
    with self.assertRaisesRegex(ValueError, "repeated term.*11"):
      layer.set_vocabulary(vocab_data)


</clonepair224>
<clonepair225>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1729" endline="1740" pcid="5587"></source>
  def test_vocab_with_reserved_mask_element_fails(self):
    vocab_data = ["earth", "mask_token", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="mask_token",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string,
        invert=True)
    with self.assertRaisesRegex(ValueError, "reserved mask"):
      layer.set_vocabulary(vocab_data)

</clonepair225>

<clonepair225>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1753" endline="1765" pcid="5589"></source>
  def test_int_vocab_with_repeated_element_fails(self):
    vocab_data = [11, 11, 34, 23, 124]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1,
        vocabulary_dtype=tf.int64,
        invert=True)
    with self.assertRaisesRegex(ValueError, "repeated term.*11"):
      layer.set_vocabulary(vocab_data)


</clonepair225>
<clonepair226>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1717" endline="1728" pcid="5586"></source>
  def test_vocab_with_repeated_element_fails(self):
    vocab_data = ["earth", "earth", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string,
        invert=True)
    with self.assertRaisesRegex(ValueError, "repeated term.*earth"):
      layer.set_vocabulary(vocab_data)

</clonepair226>

<clonepair226>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1753" endline="1765" pcid="5589"></source>
  def test_int_vocab_with_repeated_element_fails(self):
    vocab_data = [11, 11, 34, 23, 124]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1,
        vocabulary_dtype=tf.int64,
        invert=True)
    with self.assertRaisesRegex(ValueError, "repeated term.*11"):
      layer.set_vocabulary(vocab_data)


</clonepair226>
<clonepair227>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1621" endline="1631" pcid="5579"></source>
  def test_int_vocab_with_reserved_mask_element_fails(self):
    vocab_data = [125, 0, 3, 4, 94]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1,
        vocabulary_dtype=tf.int64)
    with self.assertRaisesRegex(ValueError, "reserved mask"):
      layer.set_vocabulary(vocab_data)

</clonepair227>

<clonepair227>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1753" endline="1765" pcid="5589"></source>
  def test_int_vocab_with_repeated_element_fails(self):
    vocab_data = [11, 11, 34, 23, 124]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1,
        vocabulary_dtype=tf.int64,
        invert=True)
    with self.assertRaisesRegex(ValueError, "repeated term.*11"):
      layer.set_vocabulary(vocab_data)


</clonepair227>
<clonepair228>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1694" endline="1705" pcid="5584"></source>
  def test_non_unique_vocab_fails(self):
    vocab_data = ["earth", "wind", "and", "fire", "fire"]
    with self.assertRaisesRegex(ValueError, "repeated term.*fire"):
      _ = index_lookup.IndexLookup(
          vocabulary=vocab_data,
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          vocabulary_dtype=tf.string,
          invert=True)

</clonepair228>

<clonepair228>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1741" endline="1752" pcid="5588"></source>
  def test_non_unique_int_vocab_fails(self):
    vocab_data = [12, 13, 14, 15, 15]
    with self.assertRaisesRegex(ValueError, "repeated term.*15"):
      _ = index_lookup.IndexLookup(
          vocabulary=vocab_data,
          max_tokens=None,
          num_oov_indices=1,
          mask_token=0,
          oov_token=-1,
          vocabulary_dtype=tf.int64,
          invert=True)

</clonepair228>
<clonepair229>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1717" endline="1728" pcid="5586"></source>
  def test_vocab_with_repeated_element_fails(self):
    vocab_data = ["earth", "earth", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string,
        invert=True)
    with self.assertRaisesRegex(ValueError, "repeated term.*earth"):
      layer.set_vocabulary(vocab_data)

</clonepair229>

<clonepair229>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1729" endline="1740" pcid="5587"></source>
  def test_vocab_with_reserved_mask_element_fails(self):
    vocab_data = ["earth", "mask_token", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="mask_token",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string,
        invert=True)
    with self.assertRaisesRegex(ValueError, "reserved mask"):
      layer.set_vocabulary(vocab_data)

</clonepair229>
<clonepair230>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1694" endline="1705" pcid="5584"></source>
  def test_non_unique_vocab_fails(self):
    vocab_data = ["earth", "wind", "and", "fire", "fire"]
    with self.assertRaisesRegex(ValueError, "repeated term.*fire"):
      _ = index_lookup.IndexLookup(
          vocabulary=vocab_data,
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          vocabulary_dtype=tf.string,
          invert=True)

</clonepair230>

<clonepair230>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1729" endline="1740" pcid="5587"></source>
  def test_vocab_with_reserved_mask_element_fails(self):
    vocab_data = ["earth", "mask_token", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="mask_token",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string,
        invert=True)
    with self.assertRaisesRegex(ValueError, "reserved mask"):
      layer.set_vocabulary(vocab_data)

</clonepair230>
<clonepair231>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1467" endline="1478" pcid="5567"></source>
  def test_vocab_with_multiple_oov_indices(self):
    vocab_data = ["", "[OOV]", "[OOV]", "[OOV]", "wind"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=3,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(vocab_data, returned_vocab)

</clonepair231>

<clonepair231>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1729" endline="1740" pcid="5587"></source>
  def test_vocab_with_reserved_mask_element_fails(self):
    vocab_data = ["earth", "mask_token", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="mask_token",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string,
        invert=True)
    with self.assertRaisesRegex(ValueError, "reserved mask"):
      layer.set_vocabulary(vocab_data)

</clonepair231>
<clonepair232>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1706" endline="1716" pcid="5585"></source>
  def test_non_int_output_fails(self):
    with self.assertRaisesRegex(ValueError, "`output_mode` must be `'int'`"):
      _ = index_lookup.IndexLookup(
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          vocabulary_dtype=tf.string,
          output_mode=index_lookup.COUNT,
          invert=True)

</clonepair232>

<clonepair232>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1729" endline="1740" pcid="5587"></source>
  def test_vocab_with_reserved_mask_element_fails(self):
    vocab_data = ["earth", "mask_token", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="mask_token",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string,
        invert=True)
    with self.assertRaisesRegex(ValueError, "reserved mask"):
      layer.set_vocabulary(vocab_data)

</clonepair232>
<clonepair233>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1502" endline="1512" pcid="5570"></source>
  def test_vocab_with_repeated_element_fails(self):
    vocab_data = ["earth", "earth", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    with self.assertRaisesRegex(ValueError, "repeated term.*earth"):
      layer.set_vocabulary(vocab_data)

</clonepair233>

<clonepair233>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1729" endline="1740" pcid="5587"></source>
  def test_vocab_with_reserved_mask_element_fails(self):
    vocab_data = ["earth", "mask_token", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="mask_token",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string,
        invert=True)
    with self.assertRaisesRegex(ValueError, "reserved mask"):
      layer.set_vocabulary(vocab_data)

</clonepair233>
<clonepair234>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1694" endline="1705" pcid="5584"></source>
  def test_non_unique_vocab_fails(self):
    vocab_data = ["earth", "wind", "and", "fire", "fire"]
    with self.assertRaisesRegex(ValueError, "repeated term.*fire"):
      _ = index_lookup.IndexLookup(
          vocabulary=vocab_data,
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          vocabulary_dtype=tf.string,
          invert=True)

</clonepair234>

<clonepair234>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1717" endline="1728" pcid="5586"></source>
  def test_vocab_with_repeated_element_fails(self):
    vocab_data = ["earth", "earth", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string,
        invert=True)
    with self.assertRaisesRegex(ValueError, "repeated term.*earth"):
      layer.set_vocabulary(vocab_data)

</clonepair234>
<clonepair235>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1467" endline="1478" pcid="5567"></source>
  def test_vocab_with_multiple_oov_indices(self):
    vocab_data = ["", "[OOV]", "[OOV]", "[OOV]", "wind"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=3,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(vocab_data, returned_vocab)

</clonepair235>

<clonepair235>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1717" endline="1728" pcid="5586"></source>
  def test_vocab_with_repeated_element_fails(self):
    vocab_data = ["earth", "earth", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string,
        invert=True)
    with self.assertRaisesRegex(ValueError, "repeated term.*earth"):
      layer.set_vocabulary(vocab_data)

</clonepair235>
<clonepair236>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1706" endline="1716" pcid="5585"></source>
  def test_non_int_output_fails(self):
    with self.assertRaisesRegex(ValueError, "`output_mode` must be `'int'`"):
      _ = index_lookup.IndexLookup(
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          vocabulary_dtype=tf.string,
          output_mode=index_lookup.COUNT,
          invert=True)

</clonepair236>

<clonepair236>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1717" endline="1728" pcid="5586"></source>
  def test_vocab_with_repeated_element_fails(self):
    vocab_data = ["earth", "earth", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string,
        invert=True)
    with self.assertRaisesRegex(ValueError, "repeated term.*earth"):
      layer.set_vocabulary(vocab_data)

</clonepair236>
<clonepair237>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1525" endline="1535" pcid="5572"></source>
  def test_vocab_with_reserved_mask_element_fails(self):
    vocab_data = ["earth", "mask_token", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="mask_token",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    with self.assertRaisesRegex(ValueError, "reserved mask"):
      layer.set_vocabulary(vocab_data)

</clonepair237>

<clonepair237>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1717" endline="1728" pcid="5586"></source>
  def test_vocab_with_repeated_element_fails(self):
    vocab_data = ["earth", "earth", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string,
        invert=True)
    with self.assertRaisesRegex(ValueError, "repeated term.*earth"):
      layer.set_vocabulary(vocab_data)

</clonepair237>
<clonepair238>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1694" endline="1705" pcid="5584"></source>
  def test_non_unique_vocab_fails(self):
    vocab_data = ["earth", "wind", "and", "fire", "fire"]
    with self.assertRaisesRegex(ValueError, "repeated term.*fire"):
      _ = index_lookup.IndexLookup(
          vocabulary=vocab_data,
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          vocabulary_dtype=tf.string,
          invert=True)

</clonepair238>

<clonepair238>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1706" endline="1716" pcid="5585"></source>
  def test_non_int_output_fails(self):
    with self.assertRaisesRegex(ValueError, "`output_mode` must be `'int'`"):
      _ = index_lookup.IndexLookup(
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          vocabulary_dtype=tf.string,
          output_mode=index_lookup.COUNT,
          invert=True)

</clonepair238>
<clonepair239>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1132" endline="1142" pcid="5549"></source>
    def compute(data):
      layer = index_lookup.IndexLookup(
          vocabulary=vocab_file,
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          output_mode=index_lookup.MULTI_HOT,
          vocabulary_dtype=tf.string)
      return layer(data)

</clonepair239>

<clonepair239>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1694" endline="1705" pcid="5584"></source>
  def test_non_unique_vocab_fails(self):
    vocab_data = ["earth", "wind", "and", "fire", "fire"]
    with self.assertRaisesRegex(ValueError, "repeated term.*fire"):
      _ = index_lookup.IndexLookup(
          vocabulary=vocab_data,
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          vocabulary_dtype=tf.string,
          invert=True)

</clonepair239>
<clonepair240>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1467" endline="1478" pcid="5567"></source>
  def test_vocab_with_multiple_oov_indices(self):
    vocab_data = ["", "[OOV]", "[OOV]", "[OOV]", "wind"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=3,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(vocab_data, returned_vocab)

</clonepair240>

<clonepair240>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1479" endline="1490" pcid="5568"></source>
  def test_int_vocab_with_multiple_oov_indices(self):
    vocab_data = [0, -1, -1, -1, 42]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=3,
        mask_token=0,
        oov_token=-1,
        vocabulary_dtype=tf.int64)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(vocab_data, returned_vocab)

</clonepair240>
<clonepair241>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1416" endline="1427" pcid="5563"></source>
  def test_vocab_multi_oov(self):
    vocab_data = ["", "[OOV]", "[OOV]", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=2,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(returned_vocab, vocab_data)

</clonepair241>

<clonepair241>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1479" endline="1490" pcid="5568"></source>
  def test_int_vocab_with_multiple_oov_indices(self):
    vocab_data = [0, -1, -1, -1, 42]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=3,
        mask_token=0,
        oov_token=-1,
        vocabulary_dtype=tf.int64)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(vocab_data, returned_vocab)

</clonepair241>
<clonepair242>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1479" endline="1490" pcid="5568"></source>
  def test_int_vocab_with_multiple_oov_indices(self):
    vocab_data = [0, -1, -1, -1, 42]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=3,
        mask_token=0,
        oov_token=-1,
        vocabulary_dtype=tf.int64)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(vocab_data, returned_vocab)

</clonepair242>

<clonepair242>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1621" endline="1631" pcid="5579"></source>
  def test_int_vocab_with_reserved_mask_element_fails(self):
    vocab_data = [125, 0, 3, 4, 94]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1,
        vocabulary_dtype=tf.int64)
    with self.assertRaisesRegex(ValueError, "reserved mask"):
      layer.set_vocabulary(vocab_data)

</clonepair242>
<clonepair243>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1416" endline="1427" pcid="5563"></source>
  def test_vocab_multi_oov(self):
    vocab_data = ["", "[OOV]", "[OOV]", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=2,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(returned_vocab, vocab_data)

</clonepair243>

<clonepair243>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1467" endline="1478" pcid="5567"></source>
  def test_vocab_with_multiple_oov_indices(self):
    vocab_data = ["", "[OOV]", "[OOV]", "[OOV]", "wind"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=3,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(vocab_data, returned_vocab)

</clonepair243>
<clonepair244>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1467" endline="1478" pcid="5567"></source>
  def test_vocab_with_multiple_oov_indices(self):
    vocab_data = ["", "[OOV]", "[OOV]", "[OOV]", "wind"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=3,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(vocab_data, returned_vocab)

</clonepair244>

<clonepair244>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1525" endline="1535" pcid="5572"></source>
  def test_vocab_with_reserved_mask_element_fails(self):
    vocab_data = ["earth", "mask_token", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="mask_token",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    with self.assertRaisesRegex(ValueError, "reserved mask"):
      layer.set_vocabulary(vocab_data)

</clonepair244>
<clonepair245>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1467" endline="1478" pcid="5567"></source>
  def test_vocab_with_multiple_oov_indices(self):
    vocab_data = ["", "[OOV]", "[OOV]", "[OOV]", "wind"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=3,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(vocab_data, returned_vocab)

</clonepair245>

<clonepair245>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1502" endline="1512" pcid="5570"></source>
  def test_vocab_with_repeated_element_fails(self):
    vocab_data = ["earth", "earth", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    with self.assertRaisesRegex(ValueError, "repeated term.*earth"):
      layer.set_vocabulary(vocab_data)

</clonepair245>
<clonepair246>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1467" endline="1478" pcid="5567"></source>
  def test_vocab_with_multiple_oov_indices(self):
    vocab_data = ["", "[OOV]", "[OOV]", "[OOV]", "wind"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=3,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(vocab_data, returned_vocab)

</clonepair246>

<clonepair246>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1491" endline="1501" pcid="5569"></source>
  def test_non_unique_vocab_fails(self):
    vocab_data = ["earth", "wind", "and", "fire", "fire"]
    with self.assertRaisesRegex(ValueError, "repeated term.*fire"):
      _ = index_lookup.IndexLookup(
          vocabulary=vocab_data,
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          vocabulary_dtype=tf.string)

</clonepair246>
<clonepair247>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1416" endline="1427" pcid="5563"></source>
  def test_vocab_multi_oov(self):
    vocab_data = ["", "[OOV]", "[OOV]", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=2,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(returned_vocab, vocab_data)

</clonepair247>

<clonepair247>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1525" endline="1535" pcid="5572"></source>
  def test_vocab_with_reserved_mask_element_fails(self):
    vocab_data = ["earth", "mask_token", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="mask_token",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    with self.assertRaisesRegex(ValueError, "reserved mask"):
      layer.set_vocabulary(vocab_data)

</clonepair247>
<clonepair248>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1416" endline="1427" pcid="5563"></source>
  def test_vocab_multi_oov(self):
    vocab_data = ["", "[OOV]", "[OOV]", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=2,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(returned_vocab, vocab_data)

</clonepair248>

<clonepair248>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1502" endline="1512" pcid="5570"></source>
  def test_vocab_with_repeated_element_fails(self):
    vocab_data = ["earth", "earth", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    with self.assertRaisesRegex(ValueError, "repeated term.*earth"):
      layer.set_vocabulary(vocab_data)

</clonepair248>
<clonepair249>
<source file="systems/keras-2.8.0/keras/losses_test.py" startline="1710" endline="1722" pcid="1979"></source>
  def test_sample_weighted(self):
    self.setup()
    logcosh_obj = losses.LogCosh()

    sample_weight = tf.constant([1.2, 3.4], shape=(2, 1))
    loss = logcosh_obj(self.y_true, self.y_pred, sample_weight=sample_weight)

    expected_loss = np.multiply(
        self.expected_losses,
        np.asarray([1.2, 1.2, 1.2, 3.4, 3.4, 3.4]).reshape((2, 3)))
    expected_loss = np.sum(expected_loss) / self.batch_size
    self.assertAlmostEqual(self.evaluate(loss), expected_loss, 3)

</clonepair249>

<clonepair249>
<source file="systems/keras-2.8.0/keras/losses_test.py" startline="1790" endline="1802" pcid="1986"></source>
  def test_sample_weighted(self):
    self.setup()
    poisson_obj = losses.Poisson()

    sample_weight = tf.constant([1.2, 3.4], shape=(2, 1))
    loss = poisson_obj(self.y_true, self.y_pred, sample_weight=sample_weight)

    expected_loss = np.multiply(
        self.expected_losses,
        np.asarray([1.2, 1.2, 1.2, 3.4, 3.4, 3.4]).reshape((2, 3)))
    expected_loss = np.sum(expected_loss) / self.batch_size
    self.assertAlmostEqual(self.evaluate(loss), expected_loss, 3)

</clonepair249>
<clonepair250>
<source file="systems/keras-2.8.0/keras/losses_test.py" startline="1710" endline="1722" pcid="1979"></source>
  def test_sample_weighted(self):
    self.setup()
    logcosh_obj = losses.LogCosh()

    sample_weight = tf.constant([1.2, 3.4], shape=(2, 1))
    loss = logcosh_obj(self.y_true, self.y_pred, sample_weight=sample_weight)

    expected_loss = np.multiply(
        self.expected_losses,
        np.asarray([1.2, 1.2, 1.2, 3.4, 3.4, 3.4]).reshape((2, 3)))
    expected_loss = np.sum(expected_loss) / self.batch_size
    self.assertAlmostEqual(self.evaluate(loss), expected_loss, 3)

</clonepair250>

<clonepair250>
<source file="systems/keras-2.8.0/keras/losses_test.py" startline="1868" endline="1879" pcid="1993"></source>
  def test_sample_weighted(self):
    self.setup()
    k_obj = losses.KLDivergence()
    sample_weight = tf.constant([1.2, 3.4], shape=(2, 1))
    loss = k_obj(self.y_true, self.y_pred, sample_weight=sample_weight)

    expected_loss = np.multiply(
        self.expected_losses,
        np.asarray([1.2, 1.2, 1.2, 3.4, 3.4, 3.4]).reshape(2, 3))
    expected_loss = np.sum(expected_loss) / self.batch_size
    self.assertAlmostEqual(self.evaluate(loss), expected_loss, 3)

</clonepair250>
<clonepair251>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1491" endline="1501" pcid="5569"></source>
  def test_non_unique_vocab_fails(self):
    vocab_data = ["earth", "wind", "and", "fire", "fire"]
    with self.assertRaisesRegex(ValueError, "repeated term.*fire"):
      _ = index_lookup.IndexLookup(
          vocabulary=vocab_data,
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          vocabulary_dtype=tf.string)

</clonepair251>

<clonepair251>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1706" endline="1716" pcid="5585"></source>
  def test_non_int_output_fails(self):
    with self.assertRaisesRegex(ValueError, "`output_mode` must be `'int'`"):
      _ = index_lookup.IndexLookup(
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          vocabulary_dtype=tf.string,
          output_mode=index_lookup.COUNT,
          invert=True)

</clonepair251>
<clonepair252>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1132" endline="1142" pcid="5549"></source>
    def compute(data):
      layer = index_lookup.IndexLookup(
          vocabulary=vocab_file,
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          output_mode=index_lookup.MULTI_HOT,
          vocabulary_dtype=tf.string)
      return layer(data)

</clonepair252>

<clonepair252>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1706" endline="1716" pcid="5585"></source>
  def test_non_int_output_fails(self):
    with self.assertRaisesRegex(ValueError, "`output_mode` must be `'int'`"):
      _ = index_lookup.IndexLookup(
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          vocabulary_dtype=tf.string,
          output_mode=index_lookup.COUNT,
          invert=True)

</clonepair252>
<clonepair253>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1598" endline="1608" pcid="5577"></source>
  def test_non_unique_int_vocab_fails(self):
    vocab_data = [12, 13, 14, 15, 15]
    with self.assertRaisesRegex(ValueError, "repeated term.*15"):
      _ = index_lookup.IndexLookup(
          vocabulary=vocab_data,
          max_tokens=None,
          num_oov_indices=1,
          mask_token=0,
          oov_token=-1,
          vocabulary_dtype=tf.int64)

</clonepair253>

<clonepair253>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1621" endline="1631" pcid="5579"></source>
  def test_int_vocab_with_reserved_mask_element_fails(self):
    vocab_data = [125, 0, 3, 4, 94]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1,
        vocabulary_dtype=tf.int64)
    with self.assertRaisesRegex(ValueError, "reserved mask"):
      layer.set_vocabulary(vocab_data)

</clonepair253>
<clonepair254>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1525" endline="1535" pcid="5572"></source>
  def test_vocab_with_reserved_mask_element_fails(self):
    vocab_data = ["earth", "mask_token", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="mask_token",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    with self.assertRaisesRegex(ValueError, "reserved mask"):
      layer.set_vocabulary(vocab_data)

</clonepair254>

<clonepair254>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1621" endline="1631" pcid="5579"></source>
  def test_int_vocab_with_reserved_mask_element_fails(self):
    vocab_data = [125, 0, 3, 4, 94]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1,
        vocabulary_dtype=tf.int64)
    with self.assertRaisesRegex(ValueError, "reserved mask"):
      layer.set_vocabulary(vocab_data)

</clonepair254>
<clonepair255>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1502" endline="1512" pcid="5570"></source>
  def test_vocab_with_repeated_element_fails(self):
    vocab_data = ["earth", "earth", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    with self.assertRaisesRegex(ValueError, "repeated term.*earth"):
      layer.set_vocabulary(vocab_data)

</clonepair255>

<clonepair255>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1621" endline="1631" pcid="5579"></source>
  def test_int_vocab_with_reserved_mask_element_fails(self):
    vocab_data = [125, 0, 3, 4, 94]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1,
        vocabulary_dtype=tf.int64)
    with self.assertRaisesRegex(ValueError, "reserved mask"):
      layer.set_vocabulary(vocab_data)

</clonepair255>
<clonepair256>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1491" endline="1501" pcid="5569"></source>
  def test_non_unique_vocab_fails(self):
    vocab_data = ["earth", "wind", "and", "fire", "fire"]
    with self.assertRaisesRegex(ValueError, "repeated term.*fire"):
      _ = index_lookup.IndexLookup(
          vocabulary=vocab_data,
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          vocabulary_dtype=tf.string)

</clonepair256>

<clonepair256>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1598" endline="1608" pcid="5577"></source>
  def test_non_unique_int_vocab_fails(self):
    vocab_data = [12, 13, 14, 15, 15]
    with self.assertRaisesRegex(ValueError, "repeated term.*15"):
      _ = index_lookup.IndexLookup(
          vocabulary=vocab_data,
          max_tokens=None,
          num_oov_indices=1,
          mask_token=0,
          oov_token=-1,
          vocabulary_dtype=tf.int64)

</clonepair256>
<clonepair257>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1502" endline="1512" pcid="5570"></source>
  def test_vocab_with_repeated_element_fails(self):
    vocab_data = ["earth", "earth", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    with self.assertRaisesRegex(ValueError, "repeated term.*earth"):
      layer.set_vocabulary(vocab_data)

</clonepair257>

<clonepair257>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1525" endline="1535" pcid="5572"></source>
  def test_vocab_with_reserved_mask_element_fails(self):
    vocab_data = ["earth", "mask_token", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="mask_token",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    with self.assertRaisesRegex(ValueError, "reserved mask"):
      layer.set_vocabulary(vocab_data)

</clonepair257>
<clonepair258>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1491" endline="1501" pcid="5569"></source>
  def test_non_unique_vocab_fails(self):
    vocab_data = ["earth", "wind", "and", "fire", "fire"]
    with self.assertRaisesRegex(ValueError, "repeated term.*fire"):
      _ = index_lookup.IndexLookup(
          vocabulary=vocab_data,
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          vocabulary_dtype=tf.string)

</clonepair258>

<clonepair258>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1525" endline="1535" pcid="5572"></source>
  def test_vocab_with_reserved_mask_element_fails(self):
    vocab_data = ["earth", "mask_token", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="mask_token",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    with self.assertRaisesRegex(ValueError, "reserved mask"):
      layer.set_vocabulary(vocab_data)

</clonepair258>
<clonepair259>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1132" endline="1142" pcid="5549"></source>
    def compute(data):
      layer = index_lookup.IndexLookup(
          vocabulary=vocab_file,
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          output_mode=index_lookup.MULTI_HOT,
          vocabulary_dtype=tf.string)
      return layer(data)

</clonepair259>

<clonepair259>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1525" endline="1535" pcid="5572"></source>
  def test_vocab_with_reserved_mask_element_fails(self):
    vocab_data = ["earth", "mask_token", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="mask_token",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    with self.assertRaisesRegex(ValueError, "reserved mask"):
      layer.set_vocabulary(vocab_data)

</clonepair259>
<clonepair260>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1491" endline="1501" pcid="5569"></source>
  def test_non_unique_vocab_fails(self):
    vocab_data = ["earth", "wind", "and", "fire", "fire"]
    with self.assertRaisesRegex(ValueError, "repeated term.*fire"):
      _ = index_lookup.IndexLookup(
          vocabulary=vocab_data,
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          vocabulary_dtype=tf.string)

</clonepair260>

<clonepair260>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1502" endline="1512" pcid="5570"></source>
  def test_vocab_with_repeated_element_fails(self):
    vocab_data = ["earth", "earth", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    with self.assertRaisesRegex(ValueError, "repeated term.*earth"):
      layer.set_vocabulary(vocab_data)

</clonepair260>
<clonepair261>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1132" endline="1142" pcid="5549"></source>
    def compute(data):
      layer = index_lookup.IndexLookup(
          vocabulary=vocab_file,
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          output_mode=index_lookup.MULTI_HOT,
          vocabulary_dtype=tf.string)
      return layer(data)

</clonepair261>

<clonepair261>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1502" endline="1512" pcid="5570"></source>
  def test_vocab_with_repeated_element_fails(self):
    vocab_data = ["earth", "earth", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        vocabulary_dtype=tf.string)
    with self.assertRaisesRegex(ValueError, "repeated term.*earth"):
      layer.set_vocabulary(vocab_data)

</clonepair261>
<clonepair262>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1132" endline="1142" pcid="5549"></source>
    def compute(data):
      layer = index_lookup.IndexLookup(
          vocabulary=vocab_file,
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          output_mode=index_lookup.MULTI_HOT,
          vocabulary_dtype=tf.string)
      return layer(data)

</clonepair262>

<clonepair262>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/index_lookup_test.py" startline="1491" endline="1501" pcid="5569"></source>
  def test_non_unique_vocab_fails(self):
    vocab_data = ["earth", "wind", "and", "fire", "fire"]
    with self.assertRaisesRegex(ValueError, "repeated term.*fire"):
      _ = index_lookup.IndexLookup(
          vocabulary=vocab_data,
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          vocabulary_dtype=tf.string)

</clonepair262>
<clonepair263>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/image_preprocessing_test.py" startline="361" endline="371" pcid="5421"></source>
  def test_predicting_with_mock_longer_height(self):
    np.random.seed(1337)
    height, width = 3, 3
    inp = np.random.random((12, 10, 6, 3))
    with testing_utils.use_gpu():
      layer = image_preprocessing.RandomCrop(height, width)
      actual_output = layer(inp, training=False)
      resized_inp = tf.image.resize(inp, size=[5, 3])
      expected_output = resized_inp[:, 1:4, :, :]
      self.assertAllClose(expected_output, actual_output)

</clonepair263>

<clonepair263>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/image_preprocessing_test.py" startline="372" endline="382" pcid="5422"></source>
  def test_predicting_with_mock_longer_width(self):
    np.random.seed(1337)
    height, width = 4, 6
    inp = np.random.random((12, 8, 16, 3))
    with testing_utils.use_gpu():
      layer = image_preprocessing.RandomCrop(height, width)
      actual_output = layer(inp, training=False)
      resized_inp = tf.image.resize(inp, size=[4, 8])
      expected_output = resized_inp[:, :, 1:7, :]
      self.assertAllClose(expected_output, actual_output)

</clonepair263>
<clonepair264>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/text_vectorization_test.py" startline="747" endline="757" pcid="5041"></source>
  def test_string_splitting_with_non_1d_array_fails(self):
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        vocabulary=["a"],
        max_tokens=None,
        standardize=None,
        split=text_vectorization.WHITESPACE,
        output_mode=None)
    with self.assertRaisesRegex(ValueError, "last shape dimension must be 1"):
      _ = layer(input_data)

</clonepair264>

<clonepair264>
<source file="systems/keras-2.8.0/keras/layers/preprocessing/text_vectorization_test.py" startline="758" endline="768" pcid="5042"></source>
  def test_string_splitting_with_non_1d_raggedarray_fails(self):
    input_data = keras.Input(shape=(None,), ragged=True, dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        vocabulary=["a"],
        max_tokens=None,
        standardize=None,
        split=text_vectorization.WHITESPACE,
        output_mode=None)
    with self.assertRaisesRegex(ValueError, "last shape dimension must be 1"):
      _ = layer(input_data)

</clonepair264>
<clonepair265>
<source file="systems/keras-2.8.0/keras/layers/convolutional_test.py" startline="508" endline="519" pcid="3862"></source>
  def _run_test(self, kwargs, expected_output_shape):
    num_samples = 2
    stack_size = 3
    num_col = 6

    with testing_utils.use_gpu():
      testing_utils.layer_test(
          keras.layers.Conv1DTranspose,
          kwargs=kwargs,
          input_shape=(num_samples, num_col, stack_size),
          expected_output_shape=expected_output_shape)

</clonepair265>

<clonepair265>
<source file="systems/keras-2.8.0/keras/layers/convolutional_test.py" startline="1169" endline="1180" pcid="3881"></source>
  def _run_test(self, kwargs, expected_output_shape=None):
    num_samples = 2
    stack_size = 3
    num_row = 7

    with self.cached_session():
      testing_utils.layer_test(
          keras.layers.DepthwiseConv1D,
          kwargs=kwargs,
          input_shape=(num_samples, num_row, stack_size),
          expected_output_shape=expected_output_shape)

</clonepair265>
<clonepair266>
<source file="systems/keras-2.8.0/keras/metrics_confusion_matrix_test.py" startline="140" endline="150" pcid="3542"></source>
  def test_weighted(self):
    fn_obj = metrics.FalseNegatives()
    self.evaluate(tf.compat.v1.variables_initializer(fn_obj.variables))
    y_true = tf.constant(((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),
                                   (1, 1, 1, 1, 0), (0, 0, 0, 0, 1)))
    y_pred = tf.constant(((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),
                                   (0, 1, 0, 1, 0), (1, 1, 1, 1, 1)))
    sample_weight = tf.constant((1., 1.5, 2., 2.5))
    result = fn_obj(y_true, y_pred, sample_weight=sample_weight)
    self.assertAllClose(5., self.evaluate(result))

</clonepair266>

<clonepair266>
<source file="systems/keras-2.8.0/keras/metrics_confusion_matrix_test.py" startline="208" endline="218" pcid="3547"></source>
  def test_weighted(self):
    tn_obj = metrics.TrueNegatives()
    self.evaluate(tf.compat.v1.variables_initializer(tn_obj.variables))
    y_true = tf.constant(((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),
                                   (1, 1, 1, 1, 0), (0, 0, 0, 0, 1)))
    y_pred = tf.constant(((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),
                                   (0, 1, 0, 1, 0), (1, 1, 1, 1, 1)))
    sample_weight = tf.constant((1., 1.5, 2., 2.5))
    result = tn_obj(y_true, y_pred, sample_weight=sample_weight)
    self.assertAllClose(4., self.evaluate(result))

</clonepair266>

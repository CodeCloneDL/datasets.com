<clonepair1>
<source file="systems/catalyst-21.09/catalyst/callbacks/metrics/segmentation.py" startline="199" endline="226" pcid="487"></source>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        class_dim: int = 1,
        weights: Optional[List[float]] = None,
        class_names: Optional[List[str]] = None,
        threshold: Optional[float] = None,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=DiceMetric(
                class_dim=class_dim,
                weights=weights,
                class_names=class_names,
                threshold=threshold,
                prefix=prefix,
                suffix=suffix,
            ),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</clonepair1>

<clonepair1>
<source file="systems/catalyst-21.09/catalyst/callbacks/metrics/segmentation.py" startline="313" endline="344" pcid="488"></source>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        alpha: float,
        beta: Optional[float] = None,
        class_dim: int = 1,
        weights: Optional[List[float]] = None,
        class_names: Optional[List[str]] = None,
        threshold: Optional[float] = None,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=TrevskyMetric(
                alpha=alpha,
                beta=beta,
                class_dim=class_dim,
                weights=weights,
                class_names=class_names,
                threshold=threshold,
                prefix=prefix,
                suffix=suffix,
            ),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</clonepair1>
<clonepair2>
<source file="systems/catalyst-21.09/catalyst/callbacks/metrics/segmentation.py" startline="89" endline="116" pcid="486"></source>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        class_dim: int = 1,
        weights: Optional[List[float]] = None,
        class_names: Optional[List[str]] = None,
        threshold: Optional[float] = None,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=IOUMetric(
                class_dim=class_dim,
                weights=weights,
                class_names=class_names,
                threshold=threshold,
                prefix=prefix,
                suffix=suffix,
            ),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</clonepair2>

<clonepair2>
<source file="systems/catalyst-21.09/catalyst/callbacks/metrics/segmentation.py" startline="313" endline="344" pcid="488"></source>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        alpha: float,
        beta: Optional[float] = None,
        class_dim: int = 1,
        weights: Optional[List[float]] = None,
        class_names: Optional[List[str]] = None,
        threshold: Optional[float] = None,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=TrevskyMetric(
                alpha=alpha,
                beta=beta,
                class_dim=class_dim,
                weights=weights,
                class_names=class_names,
                threshold=threshold,
                prefix=prefix,
                suffix=suffix,
            ),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</clonepair2>
<clonepair3>
<source file="systems/catalyst-21.09/catalyst/callbacks/metrics/segmentation.py" startline="89" endline="116" pcid="486"></source>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        class_dim: int = 1,
        weights: Optional[List[float]] = None,
        class_names: Optional[List[str]] = None,
        threshold: Optional[float] = None,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=IOUMetric(
                class_dim=class_dim,
                weights=weights,
                class_names=class_names,
                threshold=threshold,
                prefix=prefix,
                suffix=suffix,
            ),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</clonepair3>

<clonepair3>
<source file="systems/catalyst-21.09/catalyst/callbacks/metrics/segmentation.py" startline="199" endline="226" pcid="487"></source>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        class_dim: int = 1,
        weights: Optional[List[float]] = None,
        class_names: Optional[List[str]] = None,
        threshold: Optional[float] = None,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=DiceMetric(
                class_dim=class_dim,
                weights=weights,
                class_names=class_names,
                threshold=threshold,
                prefix=prefix,
                suffix=suffix,
            ),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</clonepair3>
<clonepair4>
<source file="systems/catalyst-21.09/catalyst/callbacks/metrics/cmc_score.py" startline="132" endline="155" pcid="477"></source>
    def __init__(
        self,
        embeddings_key: str,
        labels_key: str,
        is_query_key: str,
        topk_args: List[int] = None,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=CMCMetric(
                embeddings_key=embeddings_key,
                labels_key=labels_key,
                is_query_key=is_query_key,
                topk_args=topk_args,
                prefix=prefix,
                suffix=suffix,
            ),
            input_key=[embeddings_key, is_query_key],
            target_key=[labels_key],
        )


</clonepair4>

<clonepair4>
<source file="systems/catalyst-21.09/catalyst/callbacks/metrics/segmentation.py" startline="199" endline="226" pcid="487"></source>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        class_dim: int = 1,
        weights: Optional[List[float]] = None,
        class_names: Optional[List[str]] = None,
        threshold: Optional[float] = None,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=DiceMetric(
                class_dim=class_dim,
                weights=weights,
                class_names=class_names,
                threshold=threshold,
                prefix=prefix,
                suffix=suffix,
            ),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</clonepair4>
<clonepair5>
<source file="systems/catalyst-21.09/catalyst/contrib/utils/thresholds.py" startline="373" endline="409" pcid="352"></source>
def get_best_multilabel_thresholds(
    scores: np.ndarray, labels: np.ndarray, objective: METRIC_FN
) -> Tuple[float, List[float]]:
    """Finds best thresholds for multilabel classification task.

    Args:
        scores: estimated per-class scores/probabilities predicted by the model
        labels: ground truth labels
        objective: callable function, metric which we want to maximize

    Returns:
        tuple with best found objective score and per-class thresholds
    """
    num_classes = scores.shape[1]
    best_metric, best_thresholds = 0.0, []

    for baseline_thresholds_fn in [
        get_baseline_thresholds,
        get_multiclass_thresholds,
        get_binary_threshold,
        get_multilabel_thresholds,
    ]:
        _, baseline_thresholds = baseline_thresholds_fn(
            labels=labels, scores=scores, objective=objective
        )
        if isinstance(baseline_thresholds, (int, float)):
            baseline_thresholds = [baseline_thresholds] * num_classes
        metric_value, thresholds_value = get_multilabel_thresholds_greedy(
            labels=labels, scores=scores, objective=objective, thresholds=baseline_thresholds
        )
        if metric_value > best_metric:
            best_metric = metric_value
            best_thresholds = thresholds_value

    return best_metric, best_thresholds


</clonepair5>

<clonepair5>
<source file="systems/catalyst-21.09/catalyst/contrib/utils/thresholds.py" startline="410" endline="448" pcid="353"></source>
def get_best_multiclass_thresholds(
    scores: np.ndarray, labels: np.ndarray, objective: METRIC_FN
) -> Tuple[float, List[float]]:
    """Finds best thresholds for multiclass classification task.

    Args:
        scores: estimated per-class scores/probabilities predicted by the model
        labels: ground truth labels
        objective: callable function, metric which we want to maximize

    Returns:
        tuple with best found objective score and per-class thresholds
    """
    num_classes = scores.shape[1]
    best_metric, best_thresholds = 0.0, []
    labels_onehot = np.zeros((labels.size, labels.max() + 1))
    labels_onehot[np.arange(labels.size), labels] = 1

    for baseline_thresholds_fn in [
        get_baseline_thresholds,
        get_multiclass_thresholds,
        get_binary_threshold,
        get_multilabel_thresholds,
    ]:
        _, baseline_thresholds = baseline_thresholds_fn(
            labels=labels_onehot, scores=scores, objective=objective
        )
        if isinstance(baseline_thresholds, (int, float)):
            baseline_thresholds = [baseline_thresholds] * num_classes
        metric_value, thresholds_value = get_multiclass_thresholds_greedy(
            labels=labels, scores=scores, objective=objective, thresholds=baseline_thresholds
        )
        if metric_value > best_metric:
            best_metric = metric_value
            best_thresholds = thresholds_value

    return best_metric, best_thresholds


</clonepair5>
<clonepair6>
<source file="systems/catalyst-21.09/catalyst/callbacks/metrics/cmc_score.py" startline="174" endline="199" pcid="478"></source>
    def __init__(
        self,
        embeddings_key: str,
        pids_key: str,
        cids_key: str,
        is_query_key: str,
        topk_args: List[int] = None,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=ReidCMCMetric(
                embeddings_key=embeddings_key,
                pids_key=pids_key,
                cids_key=cids_key,
                is_query_key=is_query_key,
                topk_args=topk_args,
                prefix=prefix,
                suffix=suffix,
            ),
            input_key=[embeddings_key, is_query_key],
            target_key=[pids_key, cids_key],
        )


</clonepair6>

<clonepair6>
<source file="systems/catalyst-21.09/catalyst/callbacks/metrics/segmentation.py" startline="199" endline="226" pcid="487"></source>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        class_dim: int = 1,
        weights: Optional[List[float]] = None,
        class_names: Optional[List[str]] = None,
        threshold: Optional[float] = None,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=DiceMetric(
                class_dim=class_dim,
                weights=weights,
                class_names=class_names,
                threshold=threshold,
                prefix=prefix,
                suffix=suffix,
            ),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</clonepair6>
<clonepair7>
<source file="systems/catalyst-21.09/catalyst/callbacks/metrics/classification.py" startline="78" endline="98" pcid="475"></source>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        num_classes: int,
        zero_division: int = 0,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=MulticlassPrecisionRecallF1SupportMetric(
                num_classes=num_classes, zero_division=zero_division, prefix=prefix, suffix=suffix
            ),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</clonepair7>

<clonepair7>
<source file="systems/catalyst-21.09/catalyst/callbacks/metrics/classification.py" startline="175" endline="195" pcid="476"></source>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        num_classes: int,
        zero_division: int = 0,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=MultilabelPrecisionRecallF1SupportMetric(
                num_classes=num_classes, zero_division=zero_division, prefix=prefix, suffix=suffix
            ),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</clonepair7>
<clonepair8>
<source file="systems/catalyst-21.09/catalyst/callbacks/metrics/accuracy.py" startline="82" endline="102" pcid="479"></source>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        topk_args: List[int] = None,
        num_classes: int = None,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=AccuracyMetric(
                topk_args=topk_args, num_classes=num_classes, prefix=prefix, suffix=suffix
            ),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</clonepair8>

<clonepair8>
<source file="systems/catalyst-21.09/catalyst/callbacks/metrics/classification.py" startline="175" endline="195" pcid="476"></source>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        num_classes: int,
        zero_division: int = 0,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=MultilabelPrecisionRecallF1SupportMetric(
                num_classes=num_classes, zero_division=zero_division, prefix=prefix, suffix=suffix
            ),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</clonepair8>
<clonepair9>
<source file="systems/catalyst-21.09/catalyst/callbacks/metrics/accuracy.py" startline="168" endline="185" pcid="480"></source>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        threshold: Union[float, torch.Tensor] = 0.5,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=MultilabelAccuracyMetric(threshold=threshold, prefix=prefix, suffix=suffix),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</clonepair9>

<clonepair9>
<source file="systems/catalyst-21.09/catalyst/callbacks/metrics/classification.py" startline="175" endline="195" pcid="476"></source>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        num_classes: int,
        zero_division: int = 0,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=MultilabelPrecisionRecallF1SupportMetric(
                num_classes=num_classes, zero_division=zero_division, prefix=prefix, suffix=suffix
            ),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</clonepair9>
<clonepair10>
<source file="systems/catalyst-21.09/catalyst/callbacks/metrics/cmc_score.py" startline="132" endline="155" pcid="477"></source>
    def __init__(
        self,
        embeddings_key: str,
        labels_key: str,
        is_query_key: str,
        topk_args: List[int] = None,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=CMCMetric(
                embeddings_key=embeddings_key,
                labels_key=labels_key,
                is_query_key=is_query_key,
                topk_args=topk_args,
                prefix=prefix,
                suffix=suffix,
            ),
            input_key=[embeddings_key, is_query_key],
            target_key=[labels_key],
        )


</clonepair10>

<clonepair10>
<source file="systems/catalyst-21.09/catalyst/callbacks/metrics/cmc_score.py" startline="174" endline="199" pcid="478"></source>
    def __init__(
        self,
        embeddings_key: str,
        pids_key: str,
        cids_key: str,
        is_query_key: str,
        topk_args: List[int] = None,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=ReidCMCMetric(
                embeddings_key=embeddings_key,
                pids_key=pids_key,
                cids_key=cids_key,
                is_query_key=is_query_key,
                topk_args=topk_args,
                prefix=prefix,
                suffix=suffix,
            ),
            input_key=[embeddings_key, is_query_key],
            target_key=[pids_key, cids_key],
        )


</clonepair10>
<clonepair11>
<source file="systems/catalyst-21.09/catalyst/contrib/utils/thresholds.py" startline="288" endline="326" pcid="349"></source>
def get_multilabel_thresholds_greedy(
    scores: np.ndarray,
    labels: np.ndarray,
    objective: METRIC_FN,
    num_iterations: int = 100,
    num_thresholds: int = 100,
    thresholds: np.ndarray = None,
    patience: int = 3,
    atol: float = 0.01,
) -> Tuple[float, List[float]]:
    """Finds best thresholds for multilabel classification task with brute-force algorithm.

    Args:
        scores: estimated per-class scores/probabilities predicted by the model
        labels: ground truth labels
        objective: callable function, metric which we want to maximize
        num_iterations: number of iteration for brute-force algorithm
        num_thresholds: number of thresholds ot try for each class
        thresholds: baseline thresholds, which we want to optimize
        patience: maximum number of iteration before early stop exit
        atol: minimum required improvement per iteration for early stop exit

    Returns:
        tuple with best found objective score and per-class thresholds
    """
    best_metric, thresholds = get_thresholds_greedy(
        scores=scores,
        labels=labels,
        score_fn=partial(_multilabel_score_fn, objective=objective),
        num_iterations=num_iterations,
        num_thresholds=num_thresholds,
        thresholds=thresholds,
        patience=patience,
        atol=atol,
    )

    return best_metric, thresholds


</clonepair11>

<clonepair11>
<source file="systems/catalyst-21.09/catalyst/contrib/utils/thresholds.py" startline="334" endline="372" pcid="351"></source>
def get_multiclass_thresholds_greedy(
    scores: np.ndarray,
    labels: np.ndarray,
    objective: METRIC_FN,
    num_iterations: int = 100,
    num_thresholds: int = 100,
    thresholds: np.ndarray = None,
    patience: int = 3,
    atol: float = 0.01,
) -> Tuple[float, List[float]]:
    """Finds best thresholds for multiclass classification task with brute-force algorithm.

    Args:
        scores: estimated per-class scores/probabilities predicted by the model
        labels: ground truth labels
        objective: callable function, metric which we want to maximize
        num_iterations: number of iteration for brute-force algorithm
        num_thresholds: number of thresholds ot try for each class
        thresholds: baseline thresholds, which we want to optimize
        patience: maximum number of iteration before early stop exit
        atol: minimum required improvement per iteration for early stop exit

    Returns:
        tuple with best found objective score and per-class thresholds
    """
    best_metric, thresholds = get_thresholds_greedy(
        scores=scores,
        labels=labels,
        score_fn=partial(_multiclass_score_fn, objective=objective),
        num_iterations=num_iterations,
        num_thresholds=num_thresholds,
        thresholds=thresholds,
        patience=patience,
        atol=atol,
    )

    return best_metric, thresholds


</clonepair11>
<clonepair12>
<source file="systems/catalyst-21.09/catalyst/callbacks/metrics/accuracy.py" startline="168" endline="185" pcid="480"></source>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        threshold: Union[float, torch.Tensor] = 0.5,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=MultilabelAccuracyMetric(threshold=threshold, prefix=prefix, suffix=suffix),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</clonepair12>

<clonepair12>
<source file="systems/catalyst-21.09/catalyst/callbacks/metrics/recsys.py" startline="92" endline="109" pcid="481"></source>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        topk_args: List[int] = None,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=HitrateMetric(topk_args=topk_args, prefix=prefix, suffix=suffix),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</clonepair12>
<clonepair13>
<source file="systems/catalyst-21.09/catalyst/callbacks/metrics/accuracy.py" startline="168" endline="185" pcid="480"></source>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        threshold: Union[float, torch.Tensor] = 0.5,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=MultilabelAccuracyMetric(threshold=threshold, prefix=prefix, suffix=suffix),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</clonepair13>

<clonepair13>
<source file="systems/catalyst-21.09/catalyst/callbacks/metrics/recsys.py" startline="92" endline="109" pcid="481"></source>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        topk_args: List[int] = None,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=HitrateMetric(topk_args=topk_args, prefix=prefix, suffix=suffix),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</clonepair13>
<clonepair14>
<source file="systems/catalyst-21.09/catalyst/callbacks/metrics/accuracy.py" startline="168" endline="185" pcid="480"></source>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        threshold: Union[float, torch.Tensor] = 0.5,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=MultilabelAccuracyMetric(threshold=threshold, prefix=prefix, suffix=suffix),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</clonepair14>

<clonepair14>
<source file="systems/catalyst-21.09/catalyst/callbacks/metrics/recsys.py" startline="92" endline="109" pcid="481"></source>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        topk_args: List[int] = None,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=HitrateMetric(topk_args=topk_args, prefix=prefix, suffix=suffix),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</clonepair14>
<clonepair15>
<source file="systems/catalyst-21.09/catalyst/callbacks/metrics/accuracy.py" startline="168" endline="185" pcid="480"></source>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        threshold: Union[float, torch.Tensor] = 0.5,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=MultilabelAccuracyMetric(threshold=threshold, prefix=prefix, suffix=suffix),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</clonepair15>

<clonepair15>
<source file="systems/catalyst-21.09/catalyst/callbacks/metrics/recsys.py" startline="92" endline="109" pcid="481"></source>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        topk_args: List[int] = None,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=HitrateMetric(topk_args=topk_args, prefix=prefix, suffix=suffix),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</clonepair15>
<clonepair16>
<source file="systems/catalyst-21.09/catalyst/callbacks/metrics/recsys.py" startline="92" endline="109" pcid="481"></source>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        topk_args: List[int] = None,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=HitrateMetric(topk_args=topk_args, prefix=prefix, suffix=suffix),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</clonepair16>

<clonepair16>
<source file="systems/catalyst-21.09/catalyst/callbacks/metrics/recsys.py" startline="193" endline="210" pcid="482"></source>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        topk_args: List[int] = None,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=MAPMetric(topk_args=topk_args, prefix=prefix, suffix=suffix),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</clonepair16>
<clonepair17>
<source file="systems/catalyst-21.09/catalyst/callbacks/metrics/recsys.py" startline="92" endline="109" pcid="481"></source>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        topk_args: List[int] = None,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=HitrateMetric(topk_args=topk_args, prefix=prefix, suffix=suffix),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</clonepair17>

<clonepair17>
<source file="systems/catalyst-21.09/catalyst/callbacks/metrics/recsys.py" startline="294" endline="311" pcid="483"></source>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        topk_args: List[int] = None,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=MRRMetric(topk_args=topk_args, prefix=prefix, suffix=suffix),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</clonepair17>
<clonepair18>
<source file="systems/catalyst-21.09/catalyst/callbacks/metrics/recsys.py" startline="92" endline="109" pcid="481"></source>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        topk_args: List[int] = None,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=HitrateMetric(topk_args=topk_args, prefix=prefix, suffix=suffix),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</clonepair18>

<clonepair18>
<source file="systems/catalyst-21.09/catalyst/callbacks/metrics/recsys.py" startline="395" endline="412" pcid="484"></source>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        topk_args: List[int] = None,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=NDCGMetric(topk_args=topk_args, prefix=prefix, suffix=suffix),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</clonepair18>
<clonepair19>
<source file="systems/catalyst-21.09/catalyst/callbacks/metrics/recsys.py" startline="193" endline="210" pcid="482"></source>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        topk_args: List[int] = None,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=MAPMetric(topk_args=topk_args, prefix=prefix, suffix=suffix),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</clonepair19>

<clonepair19>
<source file="systems/catalyst-21.09/catalyst/callbacks/metrics/recsys.py" startline="294" endline="311" pcid="483"></source>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        topk_args: List[int] = None,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=MRRMetric(topk_args=topk_args, prefix=prefix, suffix=suffix),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</clonepair19>
<clonepair20>
<source file="systems/catalyst-21.09/catalyst/callbacks/metrics/recsys.py" startline="193" endline="210" pcid="482"></source>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        topk_args: List[int] = None,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=MAPMetric(topk_args=topk_args, prefix=prefix, suffix=suffix),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</clonepair20>

<clonepair20>
<source file="systems/catalyst-21.09/catalyst/callbacks/metrics/recsys.py" startline="395" endline="412" pcid="484"></source>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        topk_args: List[int] = None,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=NDCGMetric(topk_args=topk_args, prefix=prefix, suffix=suffix),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</clonepair20>
<clonepair21>
<source file="systems/catalyst-21.09/catalyst/callbacks/metrics/recsys.py" startline="294" endline="311" pcid="483"></source>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        topk_args: List[int] = None,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=MRRMetric(topk_args=topk_args, prefix=prefix, suffix=suffix),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</clonepair21>

<clonepair21>
<source file="systems/catalyst-21.09/catalyst/callbacks/metrics/recsys.py" startline="395" endline="412" pcid="484"></source>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        topk_args: List[int] = None,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=NDCGMetric(topk_args=topk_args, prefix=prefix, suffix=suffix),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</clonepair21>
<clonepair22>
<source file="systems/catalyst-21.09/catalyst/metrics/functional/_precision.py" startline="8" endline="63" pcid="654"></source>
def precision(
    outputs: torch.Tensor,
    targets: torch.Tensor,
    argmax_dim: int = -1,
    eps: float = 1e-7,
    num_classes: Optional[int] = None,
) -> Union[float, torch.Tensor]:
    """
    Multiclass precision score.

    Args:
        outputs: estimated targets as predicted by a model
            with shape [bs; ..., (num_classes or 1)]
        targets: ground truth (correct) target values
            with shape [bs; ..., 1]
        argmax_dim: int, that specifies dimension for argmax transformation
            in case of scores/probabilities in ``outputs``
        eps: float. Epsilon to avoid zero division.
        num_classes: int, that specifies number of classes if it known

    Returns:
        Tensor: precision for every class

    Examples:

    .. code-block:: python

        import torch
        from catalyst import metrics
        metrics.precision(
            outputs=torch.tensor([
                [1, 0, 0],
                [0, 1, 0],
                [0, 0, 1],
            ]),
            targets=torch.tensor([0, 1, 2]),
        )
        # tensor([1., 1., 1.])


    .. code-block:: python

        import torch
        from catalyst import metrics
        metrics.precision(
            outputs=torch.tensor([[0, 0, 1, 1, 0, 1, 0, 1]]),
            targets=torch.tensor([[0, 1, 0, 1, 0, 0, 1, 1]]),
        )
        # tensor([0.5000, 0.5000]
    """
    precision_score, _, _, _, = precision_recall_fbeta_support(
        outputs=outputs, targets=targets, argmax_dim=argmax_dim, eps=eps, num_classes=num_classes
    )
    return precision_score


</clonepair22>

<clonepair22>
<source file="systems/catalyst-21.09/catalyst/metrics/functional/_recall.py" startline="8" endline="64" pcid="670"></source>
def recall(
    outputs: torch.Tensor,
    targets: torch.Tensor,
    argmax_dim: int = -1,
    eps: float = 1e-7,
    num_classes: Optional[int] = None,
) -> Union[float, torch.Tensor]:
    """
    Multiclass recall score.

    Args:
        outputs: estimated targets as predicted by a model
            with shape [bs; ..., (num_classes or 1)]
        targets: ground truth (correct) target values
            with shape [bs; ..., 1]
        argmax_dim: int, that specifies dimension for argmax transformation
            in case of scores/probabilities in ``outputs``
        eps: float. Epsilon to avoid zero division.
        num_classes: int, that specifies number of classes if it known

    Returns:
        Tensor: recall for every class

    Examples:

    .. code-block:: python

        import torch
        from catalyst import metrics
        metrics.recall(
            outputs=torch.tensor([
                [1, 0, 0],
                [0, 1, 0],
                [0, 0, 1],
            ]),
            targets=torch.tensor([0, 1, 2]),
        )
        # tensor([1., 1., 1.])


    .. code-block:: python

        import torch
        from catalyst import metrics
        metrics.recall(
            outputs=torch.tensor([[0, 0, 1, 1, 0, 1, 0, 1]]),
            targets=torch.tensor([[0, 1, 0, 1, 0, 0, 1, 1]]),
        )
        # tensor([0.5000, 0.5000]
    """
    _, recall_score, _, _ = precision_recall_fbeta_support(
        outputs=outputs, targets=targets, argmax_dim=argmax_dim, eps=eps, num_classes=num_classes
    )

    return recall_score


</clonepair22>
<clonepair23>
<source file="systems/catalyst-21.09/tests/catalyst/metrics/test_additive.py" startline="56" endline="75" pcid="1002"></source>
def test_additive_std(
    values_list: Iterable[float],
    num_samples_list: Iterable[int],
    true_values_list: Iterable[float],
):
    """
    Test additive metric std computation

    Args:
        values_list: list of values to update metric
        num_samples_list: list of num_samples
        true_values_list: list of metric intermediate value
    """
    metric = AdditiveMetric()
    for value, num_samples, true_value in zip(values_list, num_samples_list, true_values_list):
        metric.update(value=value, num_samples=num_samples)
        _, std = metric.compute()
        assert np.isclose(std, true_value)


</clonepair23>

<clonepair23>
<source file="systems/catalyst-21.09/tests/catalyst/metrics/test_additive.py" startline="93" endline="112" pcid="1003"></source>
def test_additive_mode(
    values_list: Union[Iterable[float], Iterable[torch.Tensor]],
    num_samples_list: Iterable[int],
    true_values_list: Iterable[float],
    mode: Iterable[str],
):
    """
    Test additive metric std computation

    Args:
        values_list: list of values to update metric
        num_samples_list: list of num_samples
        true_values_list: list of metric intermediate value
        mode: `AdditiveMetric` mode
    """
    metric = AdditiveMetric(mode=mode)
    for value, num_samples, true_value in zip(values_list, num_samples_list, true_values_list):
        metric.update(value=value, num_samples=num_samples)
        mean, _ = metric.compute()
        assert np.isclose(mean, true_value)
</clonepair23>
<clonepair24>
<source file="systems/catalyst-21.09/tests/catalyst/metrics/test_additive.py" startline="24" endline="43" pcid="1001"></source>
def test_additive_mean(
    values_list: Iterable[float],
    num_samples_list: Iterable[int],
    true_values_list: Iterable[float],
) -> None:
    """
    Test additive metric mean computation

    Args:
        values_list: list of values to update metric
        num_samples_list: list of num_samples
        true_values_list: list of metric intermediate value
    """
    metric = AdditiveMetric()
    for value, num_samples, true_value in zip(values_list, num_samples_list, true_values_list):
        metric.update(value=value, num_samples=num_samples)
        mean, _ = metric.compute()
        assert np.isclose(mean, true_value)


</clonepair24>

<clonepair24>
<source file="systems/catalyst-21.09/tests/catalyst/metrics/test_additive.py" startline="93" endline="112" pcid="1003"></source>
def test_additive_mode(
    values_list: Union[Iterable[float], Iterable[torch.Tensor]],
    num_samples_list: Iterable[int],
    true_values_list: Iterable[float],
    mode: Iterable[str],
):
    """
    Test additive metric std computation

    Args:
        values_list: list of values to update metric
        num_samples_list: list of num_samples
        true_values_list: list of metric intermediate value
        mode: `AdditiveMetric` mode
    """
    metric = AdditiveMetric(mode=mode)
    for value, num_samples, true_value in zip(values_list, num_samples_list, true_values_list):
        metric.update(value=value, num_samples=num_samples)
        mean, _ = metric.compute()
        assert np.isclose(mean, true_value)
</clonepair24>
<clonepair25>
<source file="systems/catalyst-21.09/tests/catalyst/metrics/test_additive.py" startline="24" endline="43" pcid="1001"></source>
def test_additive_mean(
    values_list: Iterable[float],
    num_samples_list: Iterable[int],
    true_values_list: Iterable[float],
) -> None:
    """
    Test additive metric mean computation

    Args:
        values_list: list of values to update metric
        num_samples_list: list of num_samples
        true_values_list: list of metric intermediate value
    """
    metric = AdditiveMetric()
    for value, num_samples, true_value in zip(values_list, num_samples_list, true_values_list):
        metric.update(value=value, num_samples=num_samples)
        mean, _ = metric.compute()
        assert np.isclose(mean, true_value)


</clonepair25>

<clonepair25>
<source file="systems/catalyst-21.09/tests/catalyst/metrics/test_additive.py" startline="56" endline="75" pcid="1002"></source>
def test_additive_std(
    values_list: Iterable[float],
    num_samples_list: Iterable[int],
    true_values_list: Iterable[float],
):
    """
    Test additive metric std computation

    Args:
        values_list: list of values to update metric
        num_samples_list: list of num_samples
        true_values_list: list of metric intermediate value
    """
    metric = AdditiveMetric()
    for value, num_samples, true_value in zip(values_list, num_samples_list, true_values_list):
        metric.update(value=value, num_samples=num_samples)
        _, std = metric.compute()
        assert np.isclose(std, true_value)


</clonepair25>

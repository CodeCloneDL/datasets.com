<html>
<head>
    <title>NiCad6 Clone Report</title>
    <style type="text/css">
        body {font-family:sans-serif;}
        table {background-color:white; border:0px; padding:0px; border-spacing:4px; width:auto; margin-left:30px; margin-right:auto;}
        td {background-color:rgba(192,212,238,0.8); border:0px; padding:8px; width:auto; vertical-align:top; border-radius:8px}
        pre {background-color:white; padding:4px;}
        a {color:darkblue;}
    </style>
</head>
<body>
<h2>NiCad6 Clone Report</h2>
<table>
<tr style="font-size:14pt">
<td><b>System:</b> &nbsp; catalyst-21.09</td>
<td><b>Clone pairs:</b> &nbsp; 196</td>
<td><b>Clone classes:</b> &nbsp; 42</td>
</tr>
<tr style="font-size:12pt">
<td style="background-color:white">Clone type: &nbsp; 3-2</td>
<td style="background-color:white">Granularity: &nbsp; functions-blind</td>
<td style="background-color:white">Max diff threshold: &nbsp; 30%</td>
<td style="background-color:white">Clone size: &nbsp; 10 - 2500 lines</td>
<td style="background-color:white">Total functions-blind: &nbsp; 1283</td>
</tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 1:</b> &nbsp; 2 fragments, nominal size 22 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7')" href="javascript:;">
catalyst-21.09/catalyst/contrib/nn/optimizers/adamp.py: 42-84
</a>
<div class="mid" id="frag7" style="display:none"><pre>
    def __init__(
        self,
        params,
        lr=1e-3,
        betas=(0.9, 0.999),
        eps=1e-8,
        weight_decay=0,
        delta=0.1,
        wd_ratio=0.1,
        nesterov=False,
    ):
        """

        Args:
            params: iterable of parameters to optimize
                or dicts defining parameter groups
            lr (float, optional): learning rate (default: 1e-3)
            betas (Tuple[float, float], optional): coefficients
                used for computing running averages of gradient
                and its square (default: (0.9, 0.999))
            eps (float, optional): term added to the denominator to improve
                numerical stability (default: 1e-8)
            weight_decay (float, optional): weight decay coefficient
                (default: 1e-2)
            delta: threshold that determines whether
                a set of parameters is scale invariant or not (default: 0.1)
            wd_ratio: relative weight decay applied on scale-invariant
                parameters compared to that applied on scale-variant parameters
                (default: 0.1)
            nesterov (boolean, optional): enables Nesterov momentum
                (default: False)
        """
        defaults = dict(  # noqa: C408
            lr=lr,
            betas=betas,
            eps=eps,
            weight_decay=weight_decay,
            delta=delta,
            wd_ratio=wd_ratio,
            nesterov=nesterov,
        )
        super(AdamP, self).__init__(params, defaults)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag16')" href="javascript:;">
catalyst-21.09/catalyst/contrib/nn/optimizers/sgdp.py: 42-85
</a>
<div class="mid" id="frag16" style="display:none"><pre>
    def __init__(
        self,
        params,
        lr=required,
        momentum=0,
        weight_decay=0,
        dampening=0,
        nesterov=False,
        eps=1e-8,
        delta=0.1,
        wd_ratio=0.1,
    ):
        """

        Args:
            params: iterable of parameters to optimize
                or dicts defining parameter groups
            lr: learning rate
            momentum (float, optional): momentum factor (default: 0)
            weight_decay (float, optional): weight decay (L2 penalty)
                (default: 0)
            dampening (float, optional): dampening for momentum (default: 0)
            nesterov (bool, optional): enables Nesterov momentum
                (default: False)
            eps (float, optional): term added to the denominator to improve
                numerical stability (default: 1e-8)
            delta: threshold that determines whether
                a set of parameters is scale invariant or not (default: 0.1)
            wd_ratio: relative weight decay applied on scale-invariant
                parameters compared to that applied on scale-variant parameters
                (default: 0.1)
        """
        defaults = dict(  # noqa: C408
            lr=lr,
            momentum=momentum,
            dampening=dampening,
            weight_decay=weight_decay,
            nesterov=nesterov,
            eps=eps,
            delta=delta,
            wd_ratio=wd_ratio,
        )
        super(SGDP, self).__init__(params, defaults)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 2:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag11')" href="javascript:;">
catalyst-21.09/catalyst/contrib/nn/optimizers/adamp.py: 97-112
</a>
<div class="mid" id="frag11" style="display:none"><pre>
    def _projection(self, p, grad, perturb, delta, wd_ratio, eps):
        wd = 1
        expand_size = [-1] + [1] * (len(p.shape) - 1)
        for view_func in [self._channel_view, self._layer_view]:

            cosine_sim = self._cosine_similarity(grad, p.data, eps, view_func)

            if cosine_sim.max() &lt; delta / math.sqrt(view_func(p.data).size(1)):
                p_n = p.data / view_func(p.data).norm(dim=1).view(expand_size).add_(eps)
                perturb -= p_n * view_func(p_n * perturb).sum(dim=1).view(expand_size)
                wd = wd_ratio

                return perturb, wd

        return perturb, wd

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag20')" href="javascript:;">
catalyst-21.09/catalyst/contrib/nn/optimizers/sgdp.py: 98-113
</a>
<div class="mid" id="frag20" style="display:none"><pre>
    def _projection(self, p, grad, perturb, delta, wd_ratio, eps):
        wd = 1
        expand_size = [-1] + [1] * (len(p.shape) - 1)
        for view_func in [self._channel_view, self._layer_view]:

            cosine_sim = self._cosine_similarity(grad, p.data, eps, view_func)

            if cosine_sim.max() &lt; delta / math.sqrt(view_func(p.data).size(1)):
                p_n = p.data / view_func(p.data).norm(dim=1).view(expand_size).add_(eps)
                perturb -= p_n * view_func(p_n * perturb).sum(dim=1).view(expand_size)
                wd = wd_ratio

                return perturb, wd

        return perturb, wd

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 3:</b> &nbsp; 2 fragments, nominal size 64 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag15')" href="javascript:;">
catalyst-21.09/catalyst/contrib/nn/optimizers/radam.py: 38-126
</a>
<div class="mid" id="frag15" style="display:none"><pre>
    def step(self, closure: Optional[Callable] = None):
        """Makes optimizer step.

        Args:
            closure (callable, optional): A closure that reevaluates
                the model and returns the loss.

        Returns:
            computed loss

        Raises:
            RuntimeError: RAdam does not support sparse gradients
        """
        loss = None
        if closure is not None:
            loss = closure()

        for group in self.param_groups:

            for p in group["params"]:
                if p.grad is None:
                    continue
                grad = p.grad.data.float()
                if grad.is_sparse:
                    raise RuntimeError("RAdam does not support sparse gradients")

                p_data_fp32 = p.data.float()

                state = self.state[p]

                if len(state) == 0:
                    state["step"] = 0
                    state["exp_avg"] = torch.zeros_like(p_data_fp32)
                    state["exp_avg_sq"] = torch.zeros_like(p_data_fp32)
                else:
                    state["exp_avg"] = state["exp_avg"].type_as(p_data_fp32)
                    state["exp_avg_sq"] = state["exp_avg_sq"].type_as(p_data_fp32)

                exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
                beta1, beta2 = group["betas"]

                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
                exp_avg.mul_(beta1).add_(1 - beta1, grad)

                state["step"] += 1
                buffered = self.buffer[int(state["step"] % 10)]
                if state["step"] == buffered[0]:
                    n_sma, step_size = buffered[1], buffered[2]
                else:
                    buffered[0] = state["step"]
                    beta2_t = beta2 ** state["step"]
                    n_sma_max = 2 / (1 - beta2) - 1
                    n_sma = n_sma_max - 2 * state["step"] * beta2_t / (1 - beta2_t)
                    buffered[1] = n_sma

                    # more conservative since it's an approximated value
                    if n_sma &gt;= 5:
                        step_size = (
                            group["lr"]
                            * math.sqrt(
                                (1 - beta2_t)
                                * (n_sma - 4)
                                / (n_sma_max - 4)
                                * (n_sma - 2)
                                / n_sma
                                * n_sma_max
                                / (n_sma_max - 2)
                            )
                            / (1 - beta1 ** state["step"])
                        )
                    else:
                        step_size = group["lr"] / (1 - beta1 ** state["step"])
                    buffered[2] = step_size

                if group["weight_decay"] != 0:
                    p_data_fp32.add_(-group["weight_decay"] * group["lr"], p_data_fp32)

                # more conservative since it's an approximated value
                if n_sma &gt;= 5:
                    denom = exp_avg_sq.sqrt().add_(group["eps"])
                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)
                else:
                    p_data_fp32.add_(-step_size, exp_avg)

                p.data.copy_(p_data_fp32)

        return loss


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag24')" href="javascript:;">
catalyst-21.09/catalyst/contrib/nn/optimizers/ralamb.py: 50-157
</a>
<div class="mid" id="frag24" style="display:none"><pre>
    def step(self, closure: Optional[Callable] = None):
        """Makes optimizer step.

        Args:
            closure (callable, optional): A closure that reevaluates
                the model and returns the loss.

        Returns:
            computed loss

        Raises:
            RuntimeError: Ralamb does not support sparse gradients
        """
        loss = None
        if closure is not None:
            loss = closure()

        for group in self.param_groups:

            for p in group["params"]:
                if p.grad is None:
                    continue
                grad = p.grad.data.float()
                if grad.is_sparse:
                    raise RuntimeError("Ralamb does not support sparse gradients")

                p_data_fp32 = p.data.float()

                state = self.state[p]

                if len(state) == 0:
                    state["step"] = 0
                    state["exp_avg"] = torch.zeros_like(p_data_fp32)
                    state["exp_avg_sq"] = torch.zeros_like(p_data_fp32)
                else:
                    state["exp_avg"] = state["exp_avg"].type_as(p_data_fp32)
                    state["exp_avg_sq"] = state["exp_avg_sq"].type_as(p_data_fp32)

                exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
                beta1, beta2 = group["betas"]

                # Decay the first and second moment running average coefficient
                # m_t
                exp_avg.mul_(beta1).add_(1 - beta1, grad)
                # v_t
                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)

                state["step"] += 1
                buffered = self.buffer[int(state["step"] % 10)]

                if state["step"] == buffered[0]:
                    n_sma, radam_step_size = buffered[1], buffered[2]
                else:
                    buffered[0] = state["step"]
                    beta2_t = beta2 ** state["step"]
                    n_sma_max = 2 / (1 - beta2) - 1
                    n_sma = n_sma_max - 2 * state["step"] * beta2_t / (1 - beta2_t)
                    buffered[1] = n_sma

                    # more conservative since it"s an approximated value
                    if n_sma &gt;= 5:
                        radam_step_size = math.sqrt(
                            (1 - beta2_t)
                            * (n_sma - 4)
                            / (n_sma_max - 4)
                            * (n_sma - 2)
                            / n_sma
                            * n_sma_max
                            / (n_sma_max - 2)
                        ) / (1 - beta1 ** state["step"])
                    else:
                        radam_step_size = 1.0 / (1 - beta1 ** state["step"])
                    buffered[2] = radam_step_size

                if group["weight_decay"] != 0:
                    p_data_fp32.add_(-group["weight_decay"] * group["lr"], p_data_fp32)

                # more conservative since it"s an approximated value
                radam_step = p_data_fp32.clone()
                if n_sma &gt;= 5:
                    denom = exp_avg_sq.sqrt().add_(group["eps"])
                    radam_step.addcdiv_(-radam_step_size * group["lr"], exp_avg, denom)
                else:
                    radam_step.add_(-radam_step_size * group["lr"], exp_avg)

                radam_norm = radam_step.pow(2).sum().sqrt()
                weight_norm = p.data.pow(2).sum().sqrt().clamp(0, 10)
                if weight_norm == 0 or radam_norm == 0:
                    trust_ratio = 1
                else:
                    trust_ratio = weight_norm / radam_norm

                state["weight_norm"] = weight_norm
                state["adam_norm"] = radam_norm
                state["trust_ratio"] = trust_ratio

                if n_sma &gt;= 5:
                    p_data_fp32.addcdiv_(
                        -radam_step_size * group["lr"] * trust_ratio, exp_avg, denom
                    )
                else:
                    p_data_fp32.add_(-radam_step_size * group["lr"] * trust_ratio, exp_avg)

                p.data.copy_(p_data_fp32)

        return loss


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 4:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag44')" href="javascript:;">
catalyst-21.09/catalyst/contrib/nn/criterion/dice.py: 17-42
</a>
<div class="mid" id="frag44" style="display:none"><pre>
    def __init__(
        self,
        class_dim: int = 1,
        mode: str = "macro",
        weights: List[float] = None,
        eps: float = 1e-7,
    ):
        """
        Args:
            class_dim: indicates class dimention (K) for
                ``outputs`` and ``targets`` tensors (default = 1)
            mode: class summation strategy. Must be one of ['micro', 'macro',
                'weighted']. If mode='micro', classes are ignored, and metric
                are calculated generally. If mode='macro', metric are
                calculated per-class and than are averaged over all classes.
                If mode='weighted', metric are calculated per-class and than
                summed over all classes with weights.
            weights: class weights(for mode="weighted")
            eps: epsilon to avoid zero division
        """
        super().__init__()
        assert mode in ["micro", "macro", "weighted"]
        self.loss_fn = partial(
            dice, eps=eps, class_dim=class_dim, threshold=None, mode=mode, weights=weights
        )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag93')" href="javascript:;">
catalyst-21.09/catalyst/contrib/nn/criterion/iou.py: 16-41
</a>
<div class="mid" id="frag93" style="display:none"><pre>
    def __init__(
        self,
        class_dim: int = 1,
        mode: str = "macro",
        weights: List[float] = None,
        eps: float = 1e-7,
    ):
        """
        Args:
            class_dim: indicates class dimention (K) for
                ``outputs`` and ``targets`` tensors (default = 1)
            mode: class summation strategy. Must be one of ['micro', 'macro',
                'weighted']. If mode='micro', classes are ignored, and metric
                are calculated generally. If mode='macro', metric are
                calculated per-class and than are averaged over all classes.
                If mode='weighted', metric are calculated per-class and than
                summed over all classes with weights.
            weights: class weights(for mode="weighted")
            eps: epsilon to avoid zero division
        """
        super().__init__()
        assert mode in ["micro", "macro", "weighted"]
        self.loss_fn = partial(
            iou, eps=eps, class_dim=class_dim, threshold=None, mode=mode, weights=weights
        )

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 5:</b> &nbsp; 3 fragments, nominal size 17 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag128')" href="javascript:;">
catalyst-21.09/catalyst/contrib/nn/modules/amsoftmax.py: 40-57
</a>
<div class="mid" id="frag128" style="display:none"><pre>
    def __init__(  # noqa: D107
        self,
        in_features: int,
        out_features: int,
        s: float = 64.0,
        m: float = 0.5,
        eps: float = 1e-6,
    ):
        super(AMSoftmax, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.s = s
        self.m = m
        self.eps = eps

        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))
        nn.init.xavier_uniform_(self.weight)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag134')" href="javascript:;">
catalyst-21.09/catalyst/contrib/nn/modules/arcface.py: 42-60
</a>
<div class="mid" id="frag134" style="display:none"><pre>
    def __init__(  # noqa: D107
        self,
        in_features: int,
        out_features: int,
        s: float = 64.0,
        m: float = 0.5,
        eps: float = 1e-6,
    ):
        super(ArcFace, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.s = s
        self.m = m
        self.threshold = math.pi - m
        self.eps = eps

        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))
        nn.init.xavier_uniform_(self.weight)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag137')" href="javascript:;">
catalyst-21.09/catalyst/contrib/nn/modules/arcface.py: 149-171
</a>
<div class="mid" id="frag137" style="display:none"><pre>
    def __init__(  # noqa: D107
        self,
        in_features: int,
        out_features: int,
        s: float = 64.0,
        m: float = 0.5,
        k: int = 3,
        eps: float = 1e-6,
    ):
        super(SubCenterArcFace, self).__init__()
        self.in_features = in_features
        self.out_features = out_features

        self.s = s
        self.m = m
        self.k = k
        self.eps = eps

        self.weight = nn.Parameter(torch.FloatTensor(k, in_features, out_features))
        nn.init.xavier_uniform_(self.weight)

        self.threshold = math.pi - self.m

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 6:</b> &nbsp; 3 fragments, nominal size 10 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag130')" href="javascript:;">
catalyst-21.09/catalyst/contrib/nn/modules/amsoftmax.py: 71-105
</a>
<div class="mid" id="frag130" style="display:none"><pre>
    def forward(self, input: torch.Tensor, target: torch.LongTensor = None) -&gt; torch.Tensor:
        """
        Args:
            input: input features,
                expected shapes ``BxF`` where ``B``
                is batch dimension and ``F`` is an
                input feature dimension.
            target: target classes,
                expected shapes ``B`` where
                ``B`` is batch dimension.
                If `None` then will be returned
                projection on centroids.
                Default is `None`.

        Returns:
            tensor (logits) with shapes ``BxC``
            where ``C`` is a number of classes
            (out_features).
        """
        cos_theta = F.linear(F.normalize(input), F.normalize(self.weight))

        if target is None:
            return cos_theta

        cos_theta = torch.clamp(cos_theta, -1.0 + self.eps, 1.0 - self.eps)

        one_hot = torch.zeros_like(cos_theta)
        one_hot.scatter_(1, target.view(-1, 1).long(), 1)

        logits = torch.where(one_hot.bool(), cos_theta - self.m, cos_theta)
        logits *= self.s

        return logits


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag200')" href="javascript:;">
catalyst-21.09/catalyst/contrib/nn/modules/cosface.py: 64-97
</a>
<div class="mid" id="frag200" style="display:none"><pre>
    def forward(self, input: torch.Tensor, target: torch.LongTensor = None) -&gt; torch.Tensor:
        """
        Args:
            input: input features,
                expected shapes ``BxF`` where ``B``
                is batch dimension and ``F`` is an
                input feature dimension.
            target: target classes,
                expected shapes ``B`` where
                ``B`` is batch dimension.
                If `None` then will be returned
                projection on centroids.
                Default is `None`.

        Returns:
            tensor (logits) with shapes ``BxC``
            where ``C`` is a number of classes
            (out_features).
        """
        cosine = F.linear(F.normalize(input), F.normalize(self.weight))
        phi = cosine - self.m

        if target is None:
            return cosine

        one_hot = torch.zeros_like(cosine)
        one_hot.scatter_(1, target.view(-1, 1).long(), 1)

        logits = (one_hot * phi) + ((1.0 - one_hot) * cosine)
        logits *= self.s

        return logits


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag136')" href="javascript:;">
catalyst-21.09/catalyst/contrib/nn/modules/arcface.py: 74-110
</a>
<div class="mid" id="frag136" style="display:none"><pre>
    def forward(self, input: torch.Tensor, target: torch.LongTensor = None) -&gt; torch.Tensor:
        """
        Args:
            input: input features,
                expected shapes ``BxF`` where ``B``
                is batch dimension and ``F`` is an
                input feature dimension.
            target: target classes,
                expected shapes ``B`` where
                ``B`` is batch dimension.
                If `None` then will be returned
                projection on centroids.
                Default is `None`.

        Returns:
            tensor (logits) with shapes ``BxC``
            where ``C`` is a number of classes
            (out_features).
        """
        cos_theta = F.linear(F.normalize(input), F.normalize(self.weight))

        if target is None:
            return cos_theta

        theta = torch.acos(torch.clamp(cos_theta, -1.0 + self.eps, 1.0 - self.eps))

        one_hot = torch.zeros_like(cos_theta)
        one_hot.scatter_(1, target.view(-1, 1).long(), 1)

        mask = torch.where(theta &gt; self.threshold, torch.zeros_like(one_hot), one_hot)

        logits = torch.cos(torch.where(mask.bool(), theta + self.m, theta))
        logits *= self.s

        return logits


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 7:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag198')" href="javascript:;">
catalyst-21.09/catalyst/contrib/nn/modules/cosface.py: 40-51
</a>
<div class="mid" id="frag198" style="display:none"><pre>
    def __init__(  # noqa: D107
        self, in_features: int, out_features: int, s: float = 64.0, m: float = 0.35
    ):
        super(CosFace, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.s = s
        self.m = m

        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))
        nn.init.xavier_uniform_(self.weight)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag201')" href="javascript:;">
catalyst-21.09/catalyst/contrib/nn/modules/cosface.py: 132-143
</a>
<div class="mid" id="frag201" style="display:none"><pre>
    def __init__(  # noqa: D107
        self, in_features: int, out_features: int, dynamical_s: bool = True, eps: float = 1e-6
    ):
        super(AdaCos, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.s = math.sqrt(2) * math.log(out_features - 1)
        self.eps = eps

        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))
        nn.init.xavier_uniform_(self.weight)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 8:</b> &nbsp; 8 fragments, nominal size 25 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag258')" href="javascript:;">
catalyst-21.09/catalyst/contrib/models/cv/segmentation/unet.py: 21-52
</a>
<div class="mid" id="frag258" style="display:none"><pre>
    def _get_components(
        self,
        encoder: UnetEncoder,
        num_classes: int,
        bridge_params: Dict,
        decoder_params: Dict,
        head_params: Dict,
    ):
        bridge = UnetBridge(
            in_channels=encoder.out_channels,
            in_strides=encoder.out_strides,
            out_channels=encoder.out_channels[-1] * 2,
            block_fn=EncoderDownsampleBlock,
            **bridge_params,
        )
        decoder = UNetDecoder(
            in_channels=bridge.out_channels,
            in_strides=bridge.out_strides,
            block_fn=DecoderConcatBlock,
            **decoder_params,
        )
        head = UnetHead(
            in_channels=decoder.out_channels,
            in_strides=decoder.out_strides,
            out_channels=num_classes,
            num_upsample_blocks=int(np.log2(decoder.out_strides[-1])),
            **head_params,
        )

        return encoder, bridge, decoder, head


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag318')" href="javascript:;">
catalyst-21.09/catalyst/contrib/models/cv/segmentation/linknet.py: 17-47
</a>
<div class="mid" id="frag318" style="display:none"><pre>
    def _get_components(
        self,
        encoder: UnetEncoder,
        num_classes: int,
        bridge_params: Dict,
        decoder_params: Dict,
        head_params: Dict,
    ):
        bridge = UnetBridge(
            in_channels=encoder.out_channels,
            in_strides=encoder.out_strides,
            out_channels=encoder.out_channels[-1] * 2,
            block_fn=EncoderDownsampleBlock,
            **bridge_params
        )
        decoder = UNetDecoder(
            in_channels=bridge.out_channels,
            in_strides=bridge.out_strides,
            block_fn=DecoderSumBlock,
            **decoder_params
        )
        head = UnetHead(
            in_channels=decoder.out_channels,
            in_strides=decoder.out_strides,
            out_channels=num_classes,
            num_upsample_blocks=int(np.log2(decoder.out_strides[-1])),
            **head_params
        )
        return encoder, bridge, decoder, head


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag259')" href="javascript:;">
catalyst-21.09/catalyst/contrib/models/cv/segmentation/unet.py: 56-86
</a>
<div class="mid" id="frag259" style="display:none"><pre>
    def _get_components(
        self,
        encoder: ResnetEncoder,
        num_classes: int,
        bridge_params: Dict,
        decoder_params: Dict,
        head_params: Dict,
    ):
        bridge = UnetBridge(
            in_channels=encoder.out_channels,
            in_strides=encoder.out_strides,
            out_channels=encoder.out_channels[-1],
            block_fn=partial(EncoderUpsampleBlock, pool_first=True),
            **bridge_params,
        )
        decoder = UNetDecoder(
            in_channels=bridge.out_channels,
            in_strides=bridge.out_strides,
            block_fn=partial(DecoderConcatBlock, aggregate_first=True, upsample_scale=2),
            **decoder_params,
        )
        head = UnetHead(
            in_channels=decoder.out_channels,
            in_strides=decoder.out_strides,
            out_channels=num_classes,
            num_upsample_blocks=int(np.log2(decoder.out_strides[-1])),
            **head_params,
        )
        return encoder, bridge, decoder, head


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag314')" href="javascript:;">
catalyst-21.09/catalyst/contrib/models/cv/segmentation/fpn.py: 14-43
</a>
<div class="mid" id="frag314" style="display:none"><pre>
    def _get_components(
        self,
        encoder: UnetEncoder,
        num_classes: int,
        bridge_params: Dict,
        decoder_params: Dict,
        head_params: Dict,
    ):
        bridge = UnetBridge(
            in_channels=encoder.out_channels,
            in_strides=encoder.out_strides,
            out_channels=encoder.out_channels[-1] * 2,
            block_fn=EncoderDownsampleBlock,
            **bridge_params
        )
        decoder = FPNDecoder(
            in_channels=bridge.out_channels, in_strides=bridge.out_strides, **decoder_params
        )
        head = FPNHead(
            in_channels=decoder.out_channels,
            in_strides=decoder.out_strides,
            out_channels=num_classes,
            upsample_scale=decoder.out_strides[-1],
            interpolation_mode="bilinear",
            align_corners=True,
            **head_params
        )
        return encoder, bridge, decoder, head


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag319')" href="javascript:;">
catalyst-21.09/catalyst/contrib/models/cv/segmentation/linknet.py: 51-75
</a>
<div class="mid" id="frag319" style="display:none"><pre>
    def _get_components(
        self,
        encoder: ResnetEncoder,
        num_classes: int,
        bridge_params: Dict,
        decoder_params: Dict,
        head_params: Dict,
    ):
        bridge = None
        decoder = UNetDecoder(
            in_channels=encoder.out_channels,
            in_strides=encoder.out_strides,
            block_fn=partial(DecoderSumBlock, aggregate_first=False, upsample_scale=None),
            **decoder_params
        )
        head = UnetHead(
            in_channels=decoder.out_channels,
            in_strides=decoder.out_strides,
            out_channels=num_classes,
            num_upsample_blocks=int(np.log2(decoder.out_strides[-1])),
            **head_params
        )
        return encoder, bridge, decoder, head


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag274')" href="javascript:;">
catalyst-21.09/catalyst/contrib/models/cv/segmentation/psp.py: 39-62
</a>
<div class="mid" id="frag274" style="display:none"><pre>
    def _get_components(
        self,
        encoder: ResnetEncoder,
        num_classes: int,
        bridge_params: Dict,
        decoder_params: Dict,
        head_params: Dict,
    ):
        bridge = None
        decoder = PSPDecoder(
            in_channels=encoder.out_channels, in_strides=encoder.out_strides, **decoder_params
        )
        head = UnetHead(
            in_channels=decoder.out_channels,
            in_strides=decoder.out_strides,
            out_channels=num_classes,
            upsample_scale=decoder.out_strides[-1],
            interpolation_mode="bilinear",
            align_corners=True,
            **head_params
        )
        return encoder, bridge, decoder, head


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag315')" href="javascript:;">
catalyst-21.09/catalyst/contrib/models/cv/segmentation/fpn.py: 47-70
</a>
<div class="mid" id="frag315" style="display:none"><pre>
    def _get_components(
        self,
        encoder: ResnetEncoder,
        num_classes: int,
        bridge_params: Dict,
        decoder_params: Dict,
        head_params: Dict,
    ):
        bridge = None
        decoder = FPNDecoder(
            in_channels=encoder.out_channels, in_strides=encoder.out_strides, **decoder_params
        )
        head = FPNHead(
            in_channels=decoder.out_channels,
            in_strides=decoder.out_strides,
            out_channels=num_classes,
            upsample_scale=decoder.out_strides[-1],
            interpolation_mode="bilinear",
            align_corners=True,
            **head_params
        )
        return encoder, bridge, decoder, head


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag273')" href="javascript:;">
catalyst-21.09/catalyst/contrib/models/cv/segmentation/psp.py: 12-35
</a>
<div class="mid" id="frag273" style="display:none"><pre>
    def _get_components(
        self,
        encoder: UnetEncoder,
        num_classes: int,
        bridge_params: Dict,
        decoder_params: Dict,
        head_params: Dict,
    ):
        bridge = None
        decoder = PSPDecoder(
            in_channels=encoder.out_channels, in_strides=encoder.out_strides, **decoder_params
        )
        head = UnetHead(
            in_channels=decoder.out_channels,
            in_strides=decoder.out_strides,
            out_channels=num_classes,
            upsample_scale=decoder.out_strides[-1],
            interpolation_mode="bilinear",
            align_corners=True,
            **head_params
        )
        return encoder, bridge, decoder, head


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 9:</b> &nbsp; 5 fragments, nominal size 10 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag270')" href="javascript:;">
catalyst-21.09/catalyst/contrib/models/cv/segmentation/head/unet.py: 41-54
</a>
<div class="mid" id="frag270" style="display:none"><pre>
    def forward(self, x: List[torch.Tensor]) -&gt; torch.Tensor:
        """Forward call."""
        x_last = x[-1]
        x = self.head(x_last)
        if self.upsample_scale &gt; 1:
            x = F.interpolate(
                x,
                scale_factor=self.upsample_scale,
                mode=self.interpolation_mode,
                align_corners=self.align_corners,
            )
        return x


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag311')" href="javascript:;">
catalyst-21.09/catalyst/contrib/models/cv/segmentation/blocks/fpn.py: 79-91
</a>
<div class="mid" id="frag311" style="display:none"><pre>
    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
        """Forward call."""
        x = self.block(x)
        if self.upsample:
            x = F.interpolate(
                x,
                scale_factor=self.upsample_scale,
                mode=self.interpolation_mode,
                align_corners=self.align_corners,
            )
        return x


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag272')" href="javascript:;">
catalyst-21.09/catalyst/contrib/models/cv/segmentation/head/fpn.py: 52-66
</a>
<div class="mid" id="frag272" style="display:none"><pre>
    def forward(self, x: List[torch.Tensor]) -&gt; torch.Tensor:
        """Forward call."""
        x = list(map(lambda block, features: block(features), self.segmentation_blocks, x))
        x = sum(x)
        x = self.head(x)
        if self.upsample_scale &gt; 1:
            x = F.interpolate(
                x,
                scale_factor=self.upsample_scale,
                mode=self.interpolation_mode,
                align_corners=self.align_corners,
            )
        return x


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag297')" href="javascript:;">
catalyst-21.09/catalyst/contrib/models/cv/segmentation/blocks/unet.py: 106-118
</a>
<div class="mid" id="frag297" style="display:none"><pre>
    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
        """Forward call."""
        if self.pool_first:
            x = F.max_pool2d(x, kernel_size=self.upsample_scale, stride=self.upsample_scale)
        x = F.interpolate(
            x,
            scale_factor=self.upsample_scale,
            mode=self.interpolation_mode,
            align_corners=self.align_corners,
        )
        return self.block(x)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag309')" href="javascript:;">
catalyst-21.09/catalyst/contrib/models/cv/segmentation/blocks/fpn.py: 38-50
</a>
<div class="mid" id="frag309" style="display:none"><pre>
    def forward(self, bottom: torch.Tensor, left: torch.Tensor) -&gt; torch.Tensor:
        """Forward call."""
        x = F.interpolate(
            bottom,
            scale_factor=self.upsample_scale,
            mode=self.interpolation_mode,
            align_corners=self.align_corners,
        )
        left = self.block(left)
        x = x + left
        return x


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 10:</b> &nbsp; 2 fragments, nominal size 20 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag300')" href="javascript:;">
catalyst-21.09/catalyst/contrib/models/cv/segmentation/blocks/unet.py: 184-206
</a>
<div class="mid" id="frag300" style="display:none"><pre>
    def forward(self, bottom: torch.Tensor, left: torch.Tensor) -&gt; torch.Tensor:
        """Forward call."""
        if self.aggregate_first:
            x = torch.cat([bottom, left], 1)
            x = _upsample(
                x,
                scale=self.upsample_scale,
                interpolation_mode=self.interpolation_mode,
                align_corners=self.align_corners,
            )
        else:
            x = _upsample(
                bottom,
                scale=self.upsample_scale,
                size=left.shape[2:],
                interpolation_mode=self.interpolation_mode,
                align_corners=self.align_corners,
            )
            x = torch.cat([x, left], 1)

        return self.block(x)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag302')" href="javascript:;">
catalyst-21.09/catalyst/contrib/models/cv/segmentation/blocks/unet.py: 217-241
</a>
<div class="mid" id="frag302" style="display:none"><pre>
    def forward(self, bottom: torch.Tensor, left: torch.Tensor) -&gt; torch.Tensor:
        """Forward call."""
        if self.aggregate_first:
            x = bottom + left
            x = _upsample(
                x,
                scale=self.upsample_scale,
                interpolation_mode=self.interpolation_mode,
                align_corners=self.align_corners,
            )
            x = self.block(x)
        else:
            x = _upsample(
                bottom,
                scale=self.upsample_scale,
                size=left.shape[2:],
                interpolation_mode=self.interpolation_mode,
                align_corners=self.align_corners,
            )
            x = self.block(x)
            x = x + left

        return x


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 11:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag322')" href="javascript:;">
catalyst-21.09/catalyst/contrib/models/mnist.py: 11-31
</a>
<div class="mid" id="frag322" style="display:none"><pre>
    def __init__(self, out_features: int, normalize: bool = True):
        """
        Args:
            out_features: size of the output tensor
        """
        super().__init__()
        layers = [
            nn.Conv2d(1, 32, 3, 1),
            nn.ReLU(),
            nn.Conv2d(32, 64, 3, 1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            Flatten(),
            nn.Linear(9216, 128),
            nn.ReLU(),
            nn.Linear(128, out_features),
        ]
        if normalize:
            layers.append(Normalize())
        self._net = nn.Sequential(*layers)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag324')" href="javascript:;">
catalyst-21.09/catalyst/contrib/models/mnist.py: 46-68
</a>
<div class="mid" id="frag324" style="display:none"><pre>
    def __init__(self, out_features: int):
        """
        Args:
            out_features: size of the output tensor
        """
        super().__init__()
        layers = [
            nn.Conv2d(1, 32, 3, 1),
            nn.LeakyReLU(),
            nn.BatchNorm2d(32),
            nn.Conv2d(32, 64, 3, 1),
            nn.LeakyReLU(),
            nn.MaxPool2d(2),
            Flatten(),
            nn.BatchNorm1d(9216),
            nn.Linear(9216, 128),
            nn.LeakyReLU(),
            nn.Linear(128, out_features),
            nn.BatchNorm1d(out_features),
        ]

        self._net = nn.Sequential(*layers)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 12:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag335')" href="javascript:;">
catalyst-21.09/catalyst/contrib/data/cv/reader.py: 10-30
</a>
<div class="mid" id="frag335" style="display:none"><pre>
    def __init__(
        self,
        input_key: str,
        output_key: Optional[str] = None,
        rootpath: Optional[str] = None,
        grayscale: bool = False,
    ):
        """
        Args:
            input_key: key to use from annotation dict
            output_key: key to use to store the result,
                default: ``input_key``
            rootpath: path to images dataset root directory
                (so your can use relative paths in annotations)
            grayscale: flag if you need to work only
                with grayscale images
        """
        super().__init__(input_key, output_key or input_key)
        self.rootpath = rootpath
        self.grayscale = grayscale

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag337')" href="javascript:;">
catalyst-21.09/catalyst/contrib/data/cv/reader.py: 51-72
</a>
<div class="mid" id="frag337" style="display:none"><pre>
    def __init__(
        self,
        input_key: str,
        output_key: Optional[str] = None,
        rootpath: Optional[str] = None,
        clip_range: Tuple[Union[int, float], Union[int, float]] = (0, 1),
    ):
        """
        Args:
            input_key: key to use from annotation dict
            output_key: key to use to store the result,
                default: ``input_key``
            rootpath: path to images dataset root directory
                (so your can use relative paths in annotations)
            clip_range (Tuple[int, int]): lower and upper interval edges,
                image values outside the interval are clipped
                to the interval edges
        """
        super().__init__(input_key, output_key or input_key)
        self.rootpath = rootpath
        self.clip = clip_range

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 13:</b> &nbsp; 2 fragments, nominal size 21 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag349')" href="javascript:;">
catalyst-21.09/catalyst/contrib/utils/thresholds.py: 288-326
</a>
<div class="mid" id="frag349" style="display:none"><pre>
def get_multilabel_thresholds_greedy(
    scores: np.ndarray,
    labels: np.ndarray,
    objective: METRIC_FN,
    num_iterations: int = 100,
    num_thresholds: int = 100,
    thresholds: np.ndarray = None,
    patience: int = 3,
    atol: float = 0.01,
) -&gt; Tuple[float, List[float]]:
    """Finds best thresholds for multilabel classification task with brute-force algorithm.

    Args:
        scores: estimated per-class scores/probabilities predicted by the model
        labels: ground truth labels
        objective: callable function, metric which we want to maximize
        num_iterations: number of iteration for brute-force algorithm
        num_thresholds: number of thresholds ot try for each class
        thresholds: baseline thresholds, which we want to optimize
        patience: maximum number of iteration before early stop exit
        atol: minimum required improvement per iteration for early stop exit

    Returns:
        tuple with best found objective score and per-class thresholds
    """
    best_metric, thresholds = get_thresholds_greedy(
        scores=scores,
        labels=labels,
        score_fn=partial(_multilabel_score_fn, objective=objective),
        num_iterations=num_iterations,
        num_thresholds=num_thresholds,
        thresholds=thresholds,
        patience=patience,
        atol=atol,
    )

    return best_metric, thresholds


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag351')" href="javascript:;">
catalyst-21.09/catalyst/contrib/utils/thresholds.py: 334-372
</a>
<div class="mid" id="frag351" style="display:none"><pre>
def get_multiclass_thresholds_greedy(
    scores: np.ndarray,
    labels: np.ndarray,
    objective: METRIC_FN,
    num_iterations: int = 100,
    num_thresholds: int = 100,
    thresholds: np.ndarray = None,
    patience: int = 3,
    atol: float = 0.01,
) -&gt; Tuple[float, List[float]]:
    """Finds best thresholds for multiclass classification task with brute-force algorithm.

    Args:
        scores: estimated per-class scores/probabilities predicted by the model
        labels: ground truth labels
        objective: callable function, metric which we want to maximize
        num_iterations: number of iteration for brute-force algorithm
        num_thresholds: number of thresholds ot try for each class
        thresholds: baseline thresholds, which we want to optimize
        patience: maximum number of iteration before early stop exit
        atol: minimum required improvement per iteration for early stop exit

    Returns:
        tuple with best found objective score and per-class thresholds
    """
    best_metric, thresholds = get_thresholds_greedy(
        scores=scores,
        labels=labels,
        score_fn=partial(_multiclass_score_fn, objective=objective),
        num_iterations=num_iterations,
        num_thresholds=num_thresholds,
        thresholds=thresholds,
        patience=patience,
        atol=atol,
    )

    return best_metric, thresholds


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 14:</b> &nbsp; 2 fragments, nominal size 22 lines, similarity 91%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag352')" href="javascript:;">
catalyst-21.09/catalyst/contrib/utils/thresholds.py: 373-409
</a>
<div class="mid" id="frag352" style="display:none"><pre>
def get_best_multilabel_thresholds(
    scores: np.ndarray, labels: np.ndarray, objective: METRIC_FN
) -&gt; Tuple[float, List[float]]:
    """Finds best thresholds for multilabel classification task.

    Args:
        scores: estimated per-class scores/probabilities predicted by the model
        labels: ground truth labels
        objective: callable function, metric which we want to maximize

    Returns:
        tuple with best found objective score and per-class thresholds
    """
    num_classes = scores.shape[1]
    best_metric, best_thresholds = 0.0, []

    for baseline_thresholds_fn in [
        get_baseline_thresholds,
        get_multiclass_thresholds,
        get_binary_threshold,
        get_multilabel_thresholds,
    ]:
        _, baseline_thresholds = baseline_thresholds_fn(
            labels=labels, scores=scores, objective=objective
        )
        if isinstance(baseline_thresholds, (int, float)):
            baseline_thresholds = [baseline_thresholds] * num_classes
        metric_value, thresholds_value = get_multilabel_thresholds_greedy(
            labels=labels, scores=scores, objective=objective, thresholds=baseline_thresholds
        )
        if metric_value &gt; best_metric:
            best_metric = metric_value
            best_thresholds = thresholds_value

    return best_metric, best_thresholds


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag353')" href="javascript:;">
catalyst-21.09/catalyst/contrib/utils/thresholds.py: 410-448
</a>
<div class="mid" id="frag353" style="display:none"><pre>
def get_best_multiclass_thresholds(
    scores: np.ndarray, labels: np.ndarray, objective: METRIC_FN
) -&gt; Tuple[float, List[float]]:
    """Finds best thresholds for multiclass classification task.

    Args:
        scores: estimated per-class scores/probabilities predicted by the model
        labels: ground truth labels
        objective: callable function, metric which we want to maximize

    Returns:
        tuple with best found objective score and per-class thresholds
    """
    num_classes = scores.shape[1]
    best_metric, best_thresholds = 0.0, []
    labels_onehot = np.zeros((labels.size, labels.max() + 1))
    labels_onehot[np.arange(labels.size), labels] = 1

    for baseline_thresholds_fn in [
        get_baseline_thresholds,
        get_multiclass_thresholds,
        get_binary_threshold,
        get_multilabel_thresholds,
    ]:
        _, baseline_thresholds = baseline_thresholds_fn(
            labels=labels_onehot, scores=scores, objective=objective
        )
        if isinstance(baseline_thresholds, (int, float)):
            baseline_thresholds = [baseline_thresholds] * num_classes
        metric_value, thresholds_value = get_multiclass_thresholds_greedy(
            labels=labels, scores=scores, objective=objective, thresholds=baseline_thresholds
        )
        if metric_value &gt; best_metric:
            best_metric = metric_value
            best_thresholds = thresholds_value

    return best_metric, best_thresholds


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 15:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag401')" href="javascript:;">
catalyst-21.09/catalyst/contrib/__main__.py: 78-98
</a>
<div class="mid" id="frag401" style="display:none"><pre>
def build_parser() -&gt; ArgumentParser:
    """Builds parser.

    Returns:
        parser
    """
    parser = ArgumentParser("catalyst-contrib", formatter_class=RawTextHelpFormatter)
    parser.add_argument("-v", "--version", action="version", version=f"%(prog)s {__version__}")
    all_commands = ", \n".join(map(lambda x: f"    {x}", COMMANDS.keys()))

    subparsers = parser.add_subparsers(
        metavar="{command}", dest="command", help=f"available commands: \n{all_commands}"
    )
    subparsers.required = True

    for key, value in COMMANDS.items():
        value.build_args(subparsers.add_parser(key))

    return parser


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag678')" href="javascript:;">
catalyst-21.09/catalyst/dl/__main__.py: 25-45
</a>
<div class="mid" id="frag678" style="display:none"><pre>
def build_parser() -&gt; ArgumentParser:
    """Builds parser.

    Returns:
        parser
    """
    parser = ArgumentParser("catalyst-dl", formatter_class=RawTextHelpFormatter)
    parser.add_argument("-v", "--version", action="version", version=f"%(prog)s {__version__}")
    all_commands = ", \n".join(map(lambda x: f"    {x}", COMMANDS.keys()))

    subparsers = parser.add_subparsers(
        metavar="{command}", dest="command", help=f"available commands: \n{all_commands}"
    )
    subparsers.required = True

    for key, value in COMMANDS.items():
        value.build_args(subparsers.add_parser(key))

    return parser


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 16:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag461')" href="javascript:;">
catalyst-21.09/catalyst/callbacks/pruning.py: 87-110
</a>
<div class="mid" id="frag461" style="display:none"><pre>
    def on_stage_end(self, runner: "IRunner") -&gt; None:
        """Event handler.

        Active if prune_on_stage_end or remove_reparametrization is True.

        Args:
            runner: runner for your experiment
        """
        if self.prune_on_stage_end:
            prune_model(
                model=runner.model,
                pruning_fn=self.pruning_fn,
                keys_to_prune=self.keys_to_prune,
                amount=self.amount,
                layers_to_prune=self.layers_to_prune,
            )
        if self.remove_reparametrization_on_stage_end:
            remove_reparametrization(
                model=runner.model,
                keys_to_prune=self.keys_to_prune,
                layers_to_prune=self.layers_to_prune,
            )


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag526')" href="javascript:;">
catalyst-21.09/catalyst/callbacks/onnx.py: 103-125
</a>
<div class="mid" id="frag526" style="display:none"><pre>
    def on_stage_end(self, runner: "IRunner") -&gt; None:
        """
        On stage end action.

        Args:
            runner: runner for experiment
        """
        model = runner.model
        batch = runner.engine.sync_device(runner.batch[self.input_key])
        onnx_export(
            model=model,
            file=self.filename,
            batch=batch,
            method_name=self.method_name,
            input_names=self.input_names,
            output_names=self.output_names,
            dynamic_axes=self.dynamic_axes,
            opset_version=self.opset_version,
            do_constant_folding=self.do_constant_folding,
            verbose=self.verbose,
        )


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 17:</b> &nbsp; 7 fragments, nominal size 22 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag470')" href="javascript:;">
catalyst-21.09/catalyst/callbacks/criterion.py: 84-106
</a>
<div class="mid" id="frag470" style="display:none"><pre>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        metric_key: str,
        criterion_key: str = None,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            input_key=input_key,
            target_key=target_key,
            metric_fn=self._metric_fn,
            metric_key=metric_key,
            compute_on_call=True,
            log_on_batch=True,
            prefix=prefix,
            suffix=suffix,
        )
        self.criterion_key = criterion_key
        self.criterion = None

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag477')" href="javascript:;">
catalyst-21.09/catalyst/callbacks/metrics/cmc_score.py: 132-155
</a>
<div class="mid" id="frag477" style="display:none"><pre>
    def __init__(
        self,
        embeddings_key: str,
        labels_key: str,
        is_query_key: str,
        topk_args: List[int] = None,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=CMCMetric(
                embeddings_key=embeddings_key,
                labels_key=labels_key,
                is_query_key=is_query_key,
                topk_args=topk_args,
                prefix=prefix,
                suffix=suffix,
            ),
            input_key=[embeddings_key, is_query_key],
            target_key=[labels_key],
        )


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag474')" href="javascript:;">
catalyst-21.09/catalyst/callbacks/metrics/functional_metric.py: 22-47
</a>
<div class="mid" id="frag474" style="display:none"><pre>
    def __init__(
        self,
        input_key: Union[str, Iterable[str], Dict[str, str]],
        target_key: Union[str, Iterable[str], Dict[str, str]],
        metric_fn: Callable,
        metric_key: str,
        compute_on_call: bool = True,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=FunctionalBatchMetric(
                metric_fn=metric_fn,
                metric_key=metric_key,
                compute_on_call=compute_on_call,
                prefix=prefix,
                suffix=suffix,
            ),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag478')" href="javascript:;">
catalyst-21.09/catalyst/callbacks/metrics/cmc_score.py: 174-199
</a>
<div class="mid" id="frag478" style="display:none"><pre>
    def __init__(
        self,
        embeddings_key: str,
        pids_key: str,
        cids_key: str,
        is_query_key: str,
        topk_args: List[int] = None,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=ReidCMCMetric(
                embeddings_key=embeddings_key,
                pids_key=pids_key,
                cids_key=cids_key,
                is_query_key=is_query_key,
                topk_args=topk_args,
                prefix=prefix,
                suffix=suffix,
            ),
            input_key=[embeddings_key, is_query_key],
            target_key=[pids_key, cids_key],
        )


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag486')" href="javascript:;">
catalyst-21.09/catalyst/callbacks/metrics/segmentation.py: 89-116
</a>
<div class="mid" id="frag486" style="display:none"><pre>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        class_dim: int = 1,
        weights: Optional[List[float]] = None,
        class_names: Optional[List[str]] = None,
        threshold: Optional[float] = None,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=IOUMetric(
                class_dim=class_dim,
                weights=weights,
                class_names=class_names,
                threshold=threshold,
                prefix=prefix,
                suffix=suffix,
            ),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag487')" href="javascript:;">
catalyst-21.09/catalyst/callbacks/metrics/segmentation.py: 199-226
</a>
<div class="mid" id="frag487" style="display:none"><pre>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        class_dim: int = 1,
        weights: Optional[List[float]] = None,
        class_names: Optional[List[str]] = None,
        threshold: Optional[float] = None,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=DiceMetric(
                class_dim=class_dim,
                weights=weights,
                class_names=class_names,
                threshold=threshold,
                prefix=prefix,
                suffix=suffix,
            ),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag488')" href="javascript:;">
catalyst-21.09/catalyst/callbacks/metrics/segmentation.py: 313-344
</a>
<div class="mid" id="frag488" style="display:none"><pre>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        alpha: float,
        beta: Optional[float] = None,
        class_dim: int = 1,
        weights: Optional[List[float]] = None,
        class_names: Optional[List[str]] = None,
        threshold: Optional[float] = None,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=TrevskyMetric(
                alpha=alpha,
                beta=beta,
                class_dim=class_dim,
                weights=weights,
                class_names=class_names,
                threshold=threshold,
                prefix=prefix,
                suffix=suffix,
            ),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 18:</b> &nbsp; 9 fragments, nominal size 14 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag473')" href="javascript:;">
catalyst-21.09/catalyst/callbacks/metrics/r2_squared.py: 60-74
</a>
<div class="mid" id="frag473" style="display:none"><pre>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=R2Squared(prefix=prefix, suffix=suffix),
            input_key=input_key,
            target_key=target_key,
        )


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag482')" href="javascript:;">
catalyst-21.09/catalyst/callbacks/metrics/recsys.py: 193-210
</a>
<div class="mid" id="frag482" style="display:none"><pre>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        topk_args: List[int] = None,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=MAPMetric(topk_args=topk_args, prefix=prefix, suffix=suffix),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag481')" href="javascript:;">
catalyst-21.09/catalyst/callbacks/metrics/recsys.py: 92-109
</a>
<div class="mid" id="frag481" style="display:none"><pre>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        topk_args: List[int] = None,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=HitrateMetric(topk_args=topk_args, prefix=prefix, suffix=suffix),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag480')" href="javascript:;">
catalyst-21.09/catalyst/callbacks/metrics/accuracy.py: 168-185
</a>
<div class="mid" id="frag480" style="display:none"><pre>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        threshold: Union[float, torch.Tensor] = 0.5,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=MultilabelAccuracyMetric(threshold=threshold, prefix=prefix, suffix=suffix),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag483')" href="javascript:;">
catalyst-21.09/catalyst/callbacks/metrics/recsys.py: 294-311
</a>
<div class="mid" id="frag483" style="display:none"><pre>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        topk_args: List[int] = None,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=MRRMetric(topk_args=topk_args, prefix=prefix, suffix=suffix),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag484')" href="javascript:;">
catalyst-21.09/catalyst/callbacks/metrics/recsys.py: 395-412
</a>
<div class="mid" id="frag484" style="display:none"><pre>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        topk_args: List[int] = None,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=NDCGMetric(topk_args=topk_args, prefix=prefix, suffix=suffix),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag479')" href="javascript:;">
catalyst-21.09/catalyst/callbacks/metrics/accuracy.py: 82-102
</a>
<div class="mid" id="frag479" style="display:none"><pre>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        topk_args: List[int] = None,
        num_classes: int = None,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=AccuracyMetric(
                topk_args=topk_args, num_classes=num_classes, prefix=prefix, suffix=suffix
            ),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag476')" href="javascript:;">
catalyst-21.09/catalyst/callbacks/metrics/classification.py: 175-195
</a>
<div class="mid" id="frag476" style="display:none"><pre>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        num_classes: int,
        zero_division: int = 0,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=MultilabelPrecisionRecallF1SupportMetric(
                num_classes=num_classes, zero_division=zero_division, prefix=prefix, suffix=suffix
            ),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag475')" href="javascript:;">
catalyst-21.09/catalyst/callbacks/metrics/classification.py: 78-98
</a>
<div class="mid" id="frag475" style="display:none"><pre>
    def __init__(
        self,
        input_key: str,
        target_key: str,
        num_classes: int,
        zero_division: int = 0,
        log_on_batch: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init."""
        super().__init__(
            metric=MulticlassPrecisionRecallF1SupportMetric(
                num_classes=num_classes, zero_division=zero_division, prefix=prefix, suffix=suffix
            ),
            input_key=input_key,
            target_key=target_key,
            log_on_batch=log_on_batch,
        )


</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 19:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 82%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag489')" href="javascript:;">
catalyst-21.09/catalyst/callbacks/metrics/scikit_learn.py: 107-126
</a>
<div class="mid" id="frag489" style="display:none"><pre>
    def __init__(
        self,
        keys: Mapping[str, Any],
        metric_fn: Union[Callable, str],
        metric_key: str,
        log_on_batch: bool = True,
        **metric_kwargs
    ):
        """Init."""
        if isinstance(metric_fn, str):
            metric_fn = sklearn.metrics.__dict__[metric_fn]
        metric_fn = partial(metric_fn, **metric_kwargs)

        super().__init__(
            metric=FunctionalBatchMetric(metric_fn=metric_fn, metric_key=metric_key),
            input_key=keys,
            target_key=keys,
            log_on_batch=log_on_batch,
        )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag491')" href="javascript:;">
catalyst-21.09/catalyst/callbacks/metrics/scikit_learn.py: 232-251
</a>
<div class="mid" id="frag491" style="display:none"><pre>
    def __init__(
        self,
        keys: Mapping[str, Any],
        metric_fn: Union[Callable, str],
        metric_key: str,
        **metric_kwargs
    ):
        """Init."""
        if isinstance(metric_fn, str):
            metric_fn = sklearn.metrics.__dict__[metric_fn]
        metric_fn = partial(metric_fn, **metric_kwargs)

        super().__init__(
            metric=FunctionalLoaderMetric(
                metric_fn=metric_fn, metric_key=metric_key, accumulative_fields=keys
            ),
            input_key=keys,
            target_key=keys,
        )

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 20:</b> &nbsp; 6 fragments, nominal size 23 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag529')" href="javascript:;">
catalyst-21.09/catalyst/loggers/console.py: 34-66
</a>
<div class="mid" id="frag529" style="display:none"><pre>
    def log_metrics(
        self,
        metrics: Dict[str, float],
        scope: str = None,
        # experiment info
        run_key: str = None,
        global_epoch_step: int = 0,
        global_batch_step: int = 0,
        global_sample_step: int = 0,
        # stage info
        stage_key: str = None,
        stage_epoch_len: int = 0,
        stage_epoch_step: int = 0,
        stage_batch_step: int = 0,
        stage_sample_step: int = 0,
        # loader info
        loader_key: str = None,
        loader_batch_len: int = 0,
        loader_sample_len: int = 0,
        loader_batch_step: int = 0,
        loader_sample_step: int = 0,
    ) -&gt; None:
        """Logs loader and epoch metrics to stdout."""
        if scope == "loader" and self._log_loader_metrics:
            prefix = f"{loader_key} ({stage_epoch_step}/{stage_epoch_len}) "
            msg = prefix + _format_metrics(metrics)
            print(msg)
        elif scope == "epoch" and self._log_epoch_metrics:
            # @TODO: trick to save pure epoch-based metrics, like lr/momentum
            prefix = f"* Epoch ({stage_epoch_step}/{stage_epoch_len}) "
            msg = prefix + _format_metrics(metrics["_epoch_"])
            print(msg)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag557')" href="javascript:;">
catalyst-21.09/catalyst/core/logger.py: 17-41
</a>
<div class="mid" id="frag557" style="display:none"><pre>
    def log_metrics(
        self,
        metrics: Dict[str, float],
        scope: str = None,
        # experiment info
        run_key: str = None,
        global_epoch_step: int = 0,
        global_batch_step: int = 0,
        global_sample_step: int = 0,
        # stage info
        stage_key: str = None,
        stage_epoch_len: int = 0,
        stage_epoch_step: int = 0,
        stage_batch_step: int = 0,
        stage_sample_step: int = 0,
        # loader info
        loader_key: str = None,
        loader_batch_len: int = 0,
        loader_sample_len: int = 0,
        loader_batch_step: int = 0,
        loader_sample_step: int = 0,
    ) -&gt; None:
        """Logs metrics to the logger."""
        pass

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag534')" href="javascript:;">
catalyst-21.09/catalyst/loggers/csv.py: 95-128
</a>
<div class="mid" id="frag534" style="display:none"><pre>
    def log_metrics(
        self,
        metrics: Dict[str, Any],
        scope: str = None,
        # experiment info
        run_key: str = None,
        global_epoch_step: int = 0,
        global_batch_step: int = 0,
        global_sample_step: int = 0,
        # stage info
        stage_key: str = None,
        stage_epoch_len: int = 0,
        stage_epoch_step: int = 0,
        stage_batch_step: int = 0,
        stage_sample_step: int = 0,
        # loader info
        loader_key: str = None,
        loader_batch_len: int = 0,
        loader_sample_len: int = 0,
        loader_batch_step: int = 0,
        loader_sample_step: int = 0,
    ) -&gt; None:
        """@TODO: docs."""
        if scope == "epoch":
            for loader_key, per_loader_metrics in metrics.items():
                if loader_key not in self.loggers.keys():
                    self.loggers[loader_key] = open(
                        os.path.join(self.logdir, f"{loader_key}.csv"), "a+"
                    )
                    self._make_header(metrics=per_loader_metrics, loader_key=loader_key)
                self._log_metrics(
                    metrics=per_loader_metrics, step=stage_epoch_step, loader_key=loader_key
                )

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag560')" href="javascript:;">
catalyst-21.09/catalyst/core/logger.py: 79-105
</a>
<div class="mid" id="frag560" style="display:none"><pre>
    def log_artifact(
        self,
        tag: str,
        artifact: object = None,
        path_to_artifact: str = None,
        scope: str = None,
        # experiment info
        run_key: str = None,
        global_epoch_step: int = 0,
        global_batch_step: int = 0,
        global_sample_step: int = 0,
        # stage info
        stage_key: str = None,
        stage_epoch_len: int = 0,
        stage_epoch_step: int = 0,
        stage_batch_step: int = 0,
        stage_sample_step: int = 0,
        # loader info
        loader_key: str = None,
        loader_batch_len: int = 0,
        loader_sample_len: int = 0,
        loader_batch_step: int = 0,
        loader_sample_step: int = 0,
    ) -&gt; None:
        """Logs artifact (arbitrary file like audio, video, model weights) to the logger."""
        pass

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag558')" href="javascript:;">
catalyst-21.09/catalyst/core/logger.py: 42-67
</a>
<div class="mid" id="frag558" style="display:none"><pre>
    def log_image(
        self,
        tag: str,
        image: np.ndarray,
        scope: str = None,
        # experiment info
        run_key: str = None,
        global_epoch_step: int = 0,
        global_batch_step: int = 0,
        global_sample_step: int = 0,
        # stage info
        stage_key: str = None,
        stage_epoch_len: int = 0,
        stage_epoch_step: int = 0,
        stage_batch_step: int = 0,
        stage_sample_step: int = 0,
        # loader info
        loader_key: str = None,
        loader_batch_len: int = 0,
        loader_sample_len: int = 0,
        loader_batch_step: int = 0,
        loader_sample_step: int = 0,
    ) -&gt; None:
        """Logs image to the logger."""
        pass

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag542')" href="javascript:;">
catalyst-21.09/catalyst/loggers/tensorboard.py: 140-168
</a>
<div class="mid" id="frag542" style="display:none"><pre>
    def log_image(
        self,
        tag: str,
        image: np.ndarray,
        scope: str = None,
        # experiment info
        run_key: str = None,
        global_epoch_step: int = 0,
        global_batch_step: int = 0,
        global_sample_step: int = 0,
        # stage info
        stage_key: str = None,
        stage_epoch_len: int = 0,
        stage_epoch_step: int = 0,
        stage_batch_step: int = 0,
        stage_sample_step: int = 0,
        # loader info
        loader_key: str = None,
        loader_batch_len: int = 0,
        loader_sample_len: int = 0,
        loader_batch_step: int = 0,
        loader_sample_step: int = 0,
    ) -&gt; None:
        """Logs image to Tensorboard for current scope on current step."""
        assert loader_key is not None
        self._check_loader_key(loader_key=loader_key)
        tensor = image_to_tensor(image)
        self.loggers[loader_key].add_image(f"{tag}/{scope}", tensor, global_step=global_epoch_step)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 21:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag620')" href="javascript:;">
catalyst-21.09/catalyst/metrics/_functional_metric.py: 50-63
</a>
<div class="mid" id="frag620" style="display:none"><pre>
    def __init__(
        self,
        metric_fn: Callable,
        metric_key: str,
        compute_on_call: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init"""
        super().__init__(compute_on_call=compute_on_call, prefix=prefix, suffix=suffix)
        self.metric_fn = metric_fn
        self.metric_name = f"{self.prefix}{metric_key}{self.suffix}"
        self.additive_metric = AdditiveMetric()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag626')" href="javascript:;">
catalyst-21.09/catalyst/metrics/_functional_metric.py: 169-185
</a>
<div class="mid" id="frag626" style="display:none"><pre>
    def __init__(
        self,
        metric_fn: Callable,
        metric_key: str,
        accumulative_fields: Iterable[str] = None,
        compute_on_call: bool = True,
        prefix: str = None,
        suffix: str = None,
    ):
        """Init"""
        super().__init__(compute_on_call=compute_on_call, prefix=prefix, suffix=suffix)
        self.metric_fn = metric_fn
        self.metric_name = f"{self.prefix}{metric_key}{self.suffix}"
        self.accumulative_metric = AccumulativeMetric(
            keys=accumulative_fields, compute_on_call=compute_on_call
        )

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 22:</b> &nbsp; 3 fragments, nominal size 21 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag643')" href="javascript:;">
catalyst-21.09/catalyst/metrics/functional/_segmentation.py: 167-262
</a>
<div class="mid" id="frag643" style="display:none"><pre>
def iou(
    outputs: torch.Tensor,
    targets: torch.Tensor,
    class_dim: int = 1,
    threshold: float = None,
    mode: str = "per-class",
    weights: Optional[List[float]] = None,
    eps: float = 1e-7,
) -&gt; torch.Tensor:
    """
    Computes the iou/jaccard score, iou score = intersection / union = tp / (tp + fp + fn)

    Args:
        outputs: [N; K; ...] tensor that for each of the N examples
            indicates the probability of the example belonging to each of
            the K classes, according to the model.
        targets:  binary [N; K; ...] tensor that encodes which of the K
            classes are associated with the N-th input
        class_dim: indicates class dimention (K) for
            ``outputs`` and ``targets`` tensors (default = 1), if
            mode = "micro" means nothing
        threshold: threshold for outputs binarization
        mode: class summation strategy. Must be one of ['micro', 'macro',
            'weighted', 'per-class']. If mode='micro', classes are ignored,
            and metric are calculated generally. If mode='macro', metric are
            calculated per-class and than are averaged over all classes. If
            mode='weighted', metric are calculated per-class and than summed
            over all classes with weights. If mode='per-class', metric are
            calculated separately for all classes
        weights: class weights(for mode="weighted")
        eps: epsilon to avoid zero division

    Returns:
        IoU (Jaccard) score for each class(if mode='weighted') or aggregated IOU

    Example:

    .. code-block:: python

        import torch
        from catalyst import metrics

        size = 4
        half_size = size // 2
        shape = (1, 1, size, size)
        empty = torch.zeros(shape)
        full = torch.ones(shape)
        left = torch.ones(shape)
        left[:, :, :, half_size:] = 0
        right = torch.ones(shape)
        right[:, :, :, :half_size] = 0
        top_left = torch.zeros(shape)
        top_left[:, :, :half_size, :half_size] = 1
        pred = torch.cat([empty, left, empty, full, left, top_left], dim=1)
        targets = torch.cat([full, right, empty, full, left, left], dim=1)

        metrics.iou(
            outputs=pred,
            targets=targets,
            class_dim=1,
            threshold=0.5,
            mode="per-class"
        )
        # tensor([0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.5])

        metrics.iou(
            outputs=pred,
            targets=targets,
            class_dim=1,
            threshold=0.5,
            mode="macro"
        )
        # tensor(0.5833)

        metrics.iou(
            outputs=pred,
            targets=targets,
            class_dim=1,
            threshold=0.5,
            mode="micro"
        )
        # tensor(0.4375)
    """
    metric_fn = partial(_iou, eps=eps)
    score = _get_region_based_metrics(
        outputs=outputs,
        targets=targets,
        metric_fn=metric_fn,
        class_dim=class_dim,
        threshold=threshold,
        mode=mode,
        weights=weights,
    )
    return score


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag644')" href="javascript:;">
catalyst-21.09/catalyst/metrics/functional/_segmentation.py: 263-360
</a>
<div class="mid" id="frag644" style="display:none"><pre>
def dice(
    outputs: torch.Tensor,
    targets: torch.Tensor,
    class_dim: int = 1,
    threshold: float = None,
    mode: str = "per-class",
    weights: Optional[List[float]] = None,
    eps: float = 1e-7,
) -&gt; torch.Tensor:
    """
    Computes the dice score,
    dice score = 2 * intersection / (intersection + union)) = \
    = 2 * tp / (2 * tp + fp + fn)

    Args:
        outputs: [N; K; ...] tensor that for each of the N examples
            indicates the probability of the example belonging to each of
            the K classes, according to the model.
        targets:  binary [N; K; ...] tensor that encodes which of the K
            classes are associated with the N-th input
        class_dim: indicates class dimention (K) for
            ``outputs`` and ``targets`` tensors (default = 1), if
            mode = "micro" means nothing
        threshold: threshold for outputs binarization
        mode: class summation strategy. Must be one of ['micro', 'macro',
            'weighted', 'per-class']. If mode='micro', classes are ignored,
            and metric are calculated generally. If mode='macro', metric are
            calculated per-class and than are averaged over all classes. If
            mode='weighted', metric are calculated per-class and than summed
            over all classes with weights. If mode='per-class', metric are
            calculated separately for all classes
        weights: class weights(for mode="weighted")
        eps: epsilon to avoid zero division

    Returns:
        Dice score for each class(if mode='weighted') or aggregated Dice

    Example:

    .. code-block:: python

        import torch
        from catalyst import metrics

        size = 4
        half_size = size // 2
        shape = (1, 1, size, size)
        empty = torch.zeros(shape)
        full = torch.ones(shape)
        left = torch.ones(shape)
        left[:, :, :, half_size:] = 0
        right = torch.ones(shape)
        right[:, :, :, :half_size] = 0
        top_left = torch.zeros(shape)
        top_left[:, :, :half_size, :half_size] = 1
        pred = torch.cat([empty, left, empty, full, left, top_left], dim=1)
        targets = torch.cat([full, right, empty, full, left, left], dim=1)

        metrics.dice(
            outputs=pred,
            targets=targets,
            class_dim=1,
            threshold=0.5,
            mode="per-class"
        )
        # tensor([0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.6667])

        metrics.dice(
            outputs=pred,
            targets=targets,
            class_dim=1,
            threshold=0.5,
            mode="macro"
        )
        # tensor(0.6111)

        metrics.dice(
            outputs=pred,
            targets=targets,
            class_dim=1,
            threshold=0.5,
            mode="micro"
        )
        # tensor(0.6087)
    """
    metric_fn = partial(_dice, eps=eps)
    score = _get_region_based_metrics(
        outputs=outputs,
        targets=targets,
        metric_fn=metric_fn,
        class_dim=class_dim,
        threshold=threshold,
        mode=mode,
        weights=weights,
    )
    return score


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag645')" href="javascript:;">
catalyst-21.09/catalyst/metrics/functional/_segmentation.py: 361-469
</a>
<div class="mid" id="frag645" style="display:none"><pre>
def trevsky(
    outputs: torch.Tensor,
    targets: torch.Tensor,
    alpha: float,
    beta: Optional[float] = None,
    class_dim: int = 1,
    threshold: float = None,
    mode: str = "per-class",
    weights: Optional[List[float]] = None,
    eps: float = 1e-7,
) -&gt; torch.Tensor:
    """
    Computes the trevsky score,
    trevsky score = tp / (tp + fp * beta + fn * alpha)

    Args:
        outputs: [N; K; ...] tensor that for each of the N examples
            indicates the probability of the example belonging to each of
            the K classes, according to the model.
        targets:  binary [N; K; ...] tensor that encodes which of the K
            classes are associated with the N-th input
        alpha: false negative coefficient, bigger alpha bigger penalty for
            false negative. Must be in (0, 1)
        beta: false positive coefficient, bigger alpha bigger penalty for false
            positive. Must be in (0, 1), if None beta = (1 - alpha)
        class_dim: indicates class dimention (K) for
            ``outputs`` and ``targets`` tensors (default = 1)
        threshold: threshold for outputs binarization
        mode: class summation strategy. Must be one of ['micro', 'macro',
            'weighted', 'per-class']. If mode='micro', classes are ignored,
            and metric are calculated generally. If mode='macro', metric are
            calculated per-class and than are averaged over all classes. If
            mode='weighted', metric are calculated per-class and than summed
            over all classes with weights. If mode='per-class', metric are
            calculated separately for all classes
        weights: class weights(for mode="weighted")
        eps: epsilon to avoid zero division

    Returns:
        Trevsky score for each class(if mode='weighted') or aggregated score

    Example:

    .. code-block:: python

        import torch
        from catalyst import metrics

        size = 4
        half_size = size // 2
        shape = (1, 1, size, size)
        empty = torch.zeros(shape)
        full = torch.ones(shape)
        left = torch.ones(shape)
        left[:, :, :, half_size:] = 0
        right = torch.ones(shape)
        right[:, :, :, :half_size] = 0
        top_left = torch.zeros(shape)
        top_left[:, :, :half_size, :half_size] = 1
        pred = torch.cat([empty, left, empty, full, left, top_left], dim=1)
        targets = torch.cat([full, right, empty, full, left, left], dim=1)

        metrics.trevsky(
            outputs=pred,
            targets=targets,
            alpha=0.2,
            class_dim=1,
            threshold=0.5,
            mode="per-class"
        )
        # tensor([0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.8333])

        metrics.trevsky(
            outputs=pred,
            targets=targets,
            alpha=0.2,
            class_dim=1,
            threshold=0.5,
            mode="macro"
        )
        # tensor(0.6389)

        metrics.trevsky(
            outputs=pred,
            targets=targets,
            alpha=0.2,
            class_dim=1,
            threshold=0.5,
            mode="micro"
        )
        # tensor(0.7000)
    """
    # assert 0 &lt; alpha &lt; 1  # I am not sure about this
    if beta is None:
        assert 0 &lt; alpha &lt; 1, "if beta=None, alpha must be in (0, 1)"
        beta = 1 - alpha
    metric_fn = partial(_trevsky, alpha=alpha, beta=beta, eps=eps)
    score = _get_region_based_metrics(
        outputs=outputs,
        targets=targets,
        metric_fn=metric_fn,
        class_dim=class_dim,
        threshold=threshold,
        mode=mode,
        weights=weights,
    )
    return score


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 23:</b> &nbsp; 2 fragments, nominal size 20 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag652')" href="javascript:;">
catalyst-21.09/catalyst/metrics/functional/_focal.py: 5-53
</a>
<div class="mid" id="frag652" style="display:none"><pre>
def sigmoid_focal_loss(
    outputs: torch.Tensor,
    targets: torch.Tensor,
    gamma: float = 2.0,
    alpha: float = 0.25,
    reduction: str = "mean",
):
    """
    Compute binary focal loss between target and output logits.

    Args:
        outputs: tensor of arbitrary shape
        targets: tensor of the same shape as input
        gamma: gamma for focal loss
        alpha: alpha for focal loss
        reduction (string, optional):
            specifies the reduction to apply to the output:
            ``"none"`` | ``"mean"`` | ``"sum"`` | ``"batchwise_mean"``.
            ``"none"``: no reduction will be applied,
            ``"mean"``: the sum of the output will be divided by the number of
            elements in the output,
            ``"sum"``: the output will be summed.

    Returns:
        computed loss

    Source: https://github.com/BloodAxe/pytorch-toolbelt
    """
    targets = targets.type(outputs.type())

    logpt = -F.binary_cross_entropy_with_logits(outputs, targets, reduction="none")
    pt = torch.exp(logpt)

    # compute the loss
    loss = -((1 - pt).pow(gamma)) * logpt

    if alpha is not None:
        loss = loss * (alpha * targets + (1 - alpha) * (1 - targets))

    if reduction == "mean":
        loss = loss.mean()
    if reduction == "sum":
        loss = loss.sum()
    if reduction == "batchwise_mean":
        loss = loss.sum(0)

    return loss


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag653')" href="javascript:;">
catalyst-21.09/catalyst/metrics/functional/_focal.py: 54-113
</a>
<div class="mid" id="frag653" style="display:none"><pre>
def reduced_focal_loss(
    outputs: torch.Tensor,
    targets: torch.Tensor,
    threshold: float = 0.5,
    gamma: float = 2.0,
    reduction="mean",
) -&gt; torch.Tensor:
    """Compute reduced focal loss between target and output logits.

    It has been proposed in `Reduced Focal Loss\: 1st Place Solution to xView
    object detection in Satellite Imagery`_ paper.

    .. note::
        ``size_average`` and ``reduce`` params are in the process of being
        deprecated, and in the meantime, specifying either of those two args
        will override ``reduction``.

    Source: https://github.com/BloodAxe/pytorch-toolbelt

    .. _Reduced Focal Loss\: 1st Place Solution to xView object detection
        in Satellite Imagery: https://arxiv.org/abs/1903.01347

    Args:
        outputs: tensor of arbitrary shape
        targets: tensor of the same shape as input
        threshold: threshold for focal reduction
        gamma: gamma for focal reduction
        reduction: specifies the reduction to apply to the output:
            ``"none"`` | ``"mean"`` | ``"sum"`` | ``"batchwise_mean"``.
            ``"none"``: no reduction will be applied,
            ``"mean"``: the sum of the output will be divided by the number of
            elements in the output,
            ``"sum"``: the output will be summed.
            ``"batchwise_mean"`` computes mean loss per sample in batch.
            Default: "mean"

    Returns:  # noqa: DAR201
        torch.Tensor: computed loss
    """
    targets = targets.type(outputs.type())

    logpt = -F.binary_cross_entropy_with_logits(outputs, targets, reduction="none")
    pt = torch.exp(logpt)

    # compute the loss
    focal_reduction = ((1.0 - pt) / threshold).pow(gamma)
    focal_reduction[pt &lt; threshold] = 1

    loss = -focal_reduction * logpt

    if reduction == "mean":
        loss = loss.mean()
    if reduction == "sum":
        loss = loss.sum()
    if reduction == "batchwise_mean":
        loss = loss.sum(0)

    return loss


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 24:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag654')" href="javascript:;">
catalyst-21.09/catalyst/metrics/functional/_precision.py: 8-63
</a>
<div class="mid" id="frag654" style="display:none"><pre>
def precision(
    outputs: torch.Tensor,
    targets: torch.Tensor,
    argmax_dim: int = -1,
    eps: float = 1e-7,
    num_classes: Optional[int] = None,
) -&gt; Union[float, torch.Tensor]:
    """
    Multiclass precision score.

    Args:
        outputs: estimated targets as predicted by a model
            with shape [bs; ..., (num_classes or 1)]
        targets: ground truth (correct) target values
            with shape [bs; ..., 1]
        argmax_dim: int, that specifies dimension for argmax transformation
            in case of scores/probabilities in ``outputs``
        eps: float. Epsilon to avoid zero division.
        num_classes: int, that specifies number of classes if it known

    Returns:
        Tensor: precision for every class

    Examples:

    .. code-block:: python

        import torch
        from catalyst import metrics
        metrics.precision(
            outputs=torch.tensor([
                [1, 0, 0],
                [0, 1, 0],
                [0, 0, 1],
            ]),
            targets=torch.tensor([0, 1, 2]),
        )
        # tensor([1., 1., 1.])


    .. code-block:: python

        import torch
        from catalyst import metrics
        metrics.precision(
            outputs=torch.tensor([[0, 0, 1, 1, 0, 1, 0, 1]]),
            targets=torch.tensor([[0, 1, 0, 1, 0, 0, 1, 1]]),
        )
        # tensor([0.5000, 0.5000]
    """
    precision_score, _, _, _, = precision_recall_fbeta_support(
        outputs=outputs, targets=targets, argmax_dim=argmax_dim, eps=eps, num_classes=num_classes
    )
    return precision_score


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag670')" href="javascript:;">
catalyst-21.09/catalyst/metrics/functional/_recall.py: 8-64
</a>
<div class="mid" id="frag670" style="display:none"><pre>
def recall(
    outputs: torch.Tensor,
    targets: torch.Tensor,
    argmax_dim: int = -1,
    eps: float = 1e-7,
    num_classes: Optional[int] = None,
) -&gt; Union[float, torch.Tensor]:
    """
    Multiclass recall score.

    Args:
        outputs: estimated targets as predicted by a model
            with shape [bs; ..., (num_classes or 1)]
        targets: ground truth (correct) target values
            with shape [bs; ..., 1]
        argmax_dim: int, that specifies dimension for argmax transformation
            in case of scores/probabilities in ``outputs``
        eps: float. Epsilon to avoid zero division.
        num_classes: int, that specifies number of classes if it known

    Returns:
        Tensor: recall for every class

    Examples:

    .. code-block:: python

        import torch
        from catalyst import metrics
        metrics.recall(
            outputs=torch.tensor([
                [1, 0, 0],
                [0, 1, 0],
                [0, 0, 1],
            ]),
            targets=torch.tensor([0, 1, 2]),
        )
        # tensor([1., 1., 1.])


    .. code-block:: python

        import torch
        from catalyst import metrics
        metrics.recall(
            outputs=torch.tensor([[0, 0, 1, 1, 0, 1, 0, 1]]),
            targets=torch.tensor([[0, 1, 0, 1, 0, 0, 1, 1]]),
        )
        # tensor([0.5000, 0.5000]
    """
    _, recall_score, _, _ = precision_recall_fbeta_support(
        outputs=outputs, targets=targets, argmax_dim=argmax_dim, eps=eps, num_classes=num_classes
    )

    return recall_score


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 25:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag668')" href="javascript:;">
catalyst-21.09/catalyst/metrics/functional/_f1_score.py: 8-63
</a>
<div class="mid" id="frag668" style="display:none"><pre>
def fbeta_score(
    outputs: torch.Tensor,
    targets: torch.Tensor,
    beta: float = 1.0,
    eps: float = 1e-7,
    argmax_dim: int = -1,
    num_classes: Optional[int] = None,
) -&gt; Union[float, torch.Tensor]:
    """Counts fbeta score for given ``outputs`` and ``targets``.

    Args:
        outputs: A list of predicted elements
        targets:  A list of elements that are to be predicted
        beta: beta param for f_score
        eps: epsilon to avoid zero division
        argmax_dim: int, that specifies dimension for argmax transformation
            in case of scores/probabilities in ``outputs``
        num_classes: int, that specifies number of classes if it known

    Raises:
        ValueError: If ``beta`` is a negative number.

    Returns:
        float: F_beta score.

    Example:

    .. code-block:: python

        import torch
        from catalyst import metrics
        metrics.fbeta_score(
            outputs=torch.tensor([
                [1, 0, 0],
                [0, 1, 0],
                [0, 0, 1],
            ]),
            targets=torch.tensor([0, 1, 2]),
            beta=1,
        )
        # tensor([1., 1., 1.]),  # per class fbeta
    """
    if beta &lt; 0:
        raise ValueError("beta parameter should be non-negative")

    _p, _r, fbeta, _ = precision_recall_fbeta_support(
        outputs=outputs,
        targets=targets,
        beta=beta,
        eps=eps,
        argmax_dim=argmax_dim,
        num_classes=num_classes,
    )
    return fbeta


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag669')" href="javascript:;">
catalyst-21.09/catalyst/metrics/functional/_f1_score.py: 64-111
</a>
<div class="mid" id="frag669" style="display:none"><pre>
def f1_score(
    outputs: torch.Tensor,
    targets: torch.Tensor,
    eps: float = 1e-7,
    argmax_dim: int = -1,
    num_classes: Optional[int] = None,
) -&gt; Union[float, torch.Tensor]:
    """Fbeta_score with beta=1.

    Args:
        outputs: A list of predicted elements
        targets:  A list of elements that are to be predicted
        eps: epsilon to avoid zero division
        argmax_dim: int, that specifies dimension for argmax transformation
            in case of scores/probabilities in ``outputs``
        num_classes: int, that specifies number of classes if it known

    Returns:
        float: F_1 score

    Example:

    .. code-block:: python

        import torch
        from catalyst import metrics
        metrics.f1_score(
            outputs=torch.tensor([
                [1, 0, 0],
                [0, 1, 0],
                [0, 0, 1],
            ]),
            targets=torch.tensor([0, 1, 2]),
        )
        # tensor([1., 1., 1.]),  # per class fbeta
    """
    score = fbeta_score(
        outputs=outputs,
        targets=targets,
        beta=1,
        eps=eps,
        argmax_dim=argmax_dim,
        num_classes=num_classes,
    )

    return score


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 26:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag692')" href="javascript:;">
catalyst-21.09/tests/catalyst/contrib/nn/test_criterion.py: 229-251
</a>
<div class="mid" id="frag692" style="display:none"><pre>
def test_barlow_twins_loss(
    embeddings_left: torch.Tensor,
    embeddings_right: torch.Tensor,
    offdiag_lambda: float,
    eps: float,
    true_value: float,
):
    """
    Test Barlow Twins loss

    Args:
        embeddings_left: left objects embeddings [batch_size, features_dim]
        embeddings_right: right objects embeddings [batch_size, features_dim]
        offdiag_lambda: trade off parametr
        eps: zero varience handler (var + eps)
        true_value: expected loss value
    """
    value = BarlowTwinsLoss(offdiag_lambda=offdiag_lambda, eps=eps)(
        embeddings_left, embeddings_right
    ).item()
    assert np.isclose(value, true_value)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag694')" href="javascript:;">
catalyst-21.09/tests/catalyst/contrib/nn/test_criterion.py: 316-336
</a>
<div class="mid" id="frag694" style="display:none"><pre>
def test_supervised_contrastive_loss(
    features: torch.Tensor,
    targets: torch.Tensor,
    tau: float,
    pos_aggregation: str,
    true_value: float,
):
    """
    Test supervised contrastive loss

    Args:
        features: features of objects
        targets: targets of objects
        pos_aggregation: aggeragation of positive objects
        tau: temperature
        true_value: expected loss value
    """
    value = SupervisedContrastiveLoss(tau=tau, pos_aggregation=pos_aggregation)(
        features, targets
    ).item()
    assert np.isclose(value, true_value)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 27:</b> &nbsp; 2 fragments, nominal size 143 lines, similarity 79%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag698')" href="javascript:;">
catalyst-21.09/tests/catalyst/contrib/models/test_hydra.py: 32-226
</a>
<div class="mid" id="frag698" style="display:none"><pre>
def test_config1():
    """@TODO: Docs. Contribution is welcome."""
    config1 = {
        "encoder_params": {
            "hiddens": [16, 16],
            "layer_fn": {"module": "Linear", "bias": False},
            "norm_fn": "LayerNorm",
        },
        "heads_params": {
            "head1": {"hiddens": [2], "layer_fn": {"module": "Linear", "bias": True}},
            "_head2": {
                "_hidden": {"hiddens": [16], "layer_fn": {"module": "Linear", "bias": False}},
                "head2_1": {
                    "hiddens": [32],
                    "layer_fn": {"module": "Linear", "bias": True},
                    "normalize_output": True,
                },
                "_head2_2": {
                    "_hidden": {
                        "hiddens": [16, 16, 16],
                        "layer_fn": {"module": "Linear", "bias": False},
                    },
                    "head2_2_1": {
                        "hiddens": [32],
                        "layer_fn": {"module": "Linear", "bias": True},
                        "normalize_output": False,
                    },
                },
            },
        },
        "embedders_params": {
            "target1": {"num_embeddings": 2, "normalize_output": True},
            "target2": {"num_embeddings": 2, "normalize_output": False},
        },
    }

    hydra = Hydra.get_from_params(**config1)

    config1_copy = copy.deepcopy(config1)
    _pop_normalization(config1_copy)
    encoder_params = config1_copy["encoder_params"]
    heads_params = config1_copy["heads_params"]
    heads_params["head1"]["hiddens"].insert(0, 16)
    heads_params["_head2"]["_hidden"]["hiddens"].insert(0, 16)
    heads_params["_head2"]["head2_1"]["hiddens"].insert(0, 16)
    heads_params["_head2"]["_head2_2"]["_hidden"]["hiddens"].insert(0, 16)
    heads_params["_head2"]["_head2_2"]["head2_2_1"]["hiddens"].insert(0, 16)

    net = nn.ModuleDict(
        {
            "encoder": SequentialNet(**encoder_params),
            "embedders": nn.ModuleDict(
                {
                    "target1": nn.Sequential(
                        OrderedDict(
                            [
                                ("embedding", nn.Embedding(embedding_dim=16, num_embeddings=2)),
                                ("normalize", Normalize()),
                            ]
                        )
                    ),
                    "target2": nn.Sequential(
                        OrderedDict(
                            [("embedding", nn.Embedding(embedding_dim=16, num_embeddings=2))]
                        )
                    ),
                }
            ),
            "heads": nn.ModuleDict(
                {
                    "head1": nn.Sequential(
                        OrderedDict([("net", SequentialNet(**heads_params["head1"]))])
                    ),
                    "_head2": nn.ModuleDict(
                        {
                            "_hidden": nn.Sequential(
                                OrderedDict(
                                    [("net", SequentialNet(**heads_params["_head2"]["_hidden"]))]
                                )
                            ),
                            "head2_1": nn.Sequential(
                                OrderedDict(
                                    [
                                        (
                                            "net",
                                            SequentialNet(**heads_params["_head2"]["head2_1"]),
                                        ),
                                        ("normalize", Normalize()),
                                    ]
                                )
                            ),
                            "_head2_2": nn.ModuleDict(
                                {
                                    "_hidden": nn.Sequential(
                                        OrderedDict(
                                            [
                                                (
                                                    "net",
                                                    SequentialNet(
                                                        **heads_params["_head2"]["_head2_2"][
                                                            "_hidden"
                                                        ]
                                                    ),
                                                )
                                            ]
                                        )
                                    ),
                                    "head2_2_1": nn.Sequential(
                                        OrderedDict(
                                            [
                                                (
                                                    "net",
                                                    SequentialNet(
                                                        **heads_params["_head2"]["_head2_2"][
                                                            "head2_2_1"
                                                        ]
                                                    ),
                                                )
                                            ]
                                        )
                                    ),
                                }
                            ),
                        }
                    ),
                }
            ),
        }
    )

    _check_named_parameters(hydra.encoder, net["encoder"])
    _check_named_parameters(hydra.heads, net["heads"])
    _check_named_parameters(hydra.embedders, net["embedders"])

    input_ = torch.rand(1, 16)

    output_kv = hydra(input_)
    assert (input_ == output_kv["features"]).sum().item() == 16
    kv_keys = [
        "features",
        "embeddings",
        "head1",
        "_head2/",
        "_head2/head2_1",
        "_head2/_head2_2/",
        "_head2/_head2_2/head2_2_1",
    ]
    _check_lists(output_kv.keys(), kv_keys)

    output_kv = hydra(input_, target1=torch.ones(1, 2).long())
    kv_keys = [
        "features",
        "embeddings",
        "head1",
        "_head2/",
        "_head2/head2_1",
        "_head2/_head2_2/",
        "_head2/_head2_2/head2_2_1",
        "target1_embeddings",
    ]
    _check_lists(output_kv.keys(), kv_keys)

    output_kv = hydra(input_, target2=torch.ones(1, 2).long())
    kv_keys = [
        "features",
        "embeddings",
        "head1",
        "_head2/",
        "_head2/head2_1",
        "_head2/_head2_2/",
        "_head2/_head2_2/head2_2_1",
        "target2_embeddings",
    ]
    _check_lists(output_kv.keys(), kv_keys)

    output_kv = hydra(input_, target1=torch.ones(1, 2).long(), target2=torch.ones(1, 2).long())
    kv_keys = [
        "features",
        "embeddings",
        "head1",
        "_head2/",
        "_head2/head2_1",
        "_head2/_head2_2/",
        "_head2/_head2_2/head2_2_1",
        "target1_embeddings",
        "target2_embeddings",
    ]
    _check_lists(output_kv.keys(), kv_keys)

    output_tuple = hydra.forward_tuple(input_)
    assert len(output_tuple) == 5
    assert (output_tuple[0] == output_kv["features"]).sum().item() == 16
    assert (output_tuple[1] == output_kv["embeddings"]).sum().item() == 16


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag700')" href="javascript:;">
catalyst-21.09/tests/catalyst/contrib/models/test_hydra.py: 364-527
</a>
<div class="mid" id="frag700" style="display:none"><pre>
def test_config3():
    """@TODO: Docs. Contribution is welcome."""
    config_path = Path(__file__).absolute().parent / "config3.yml"
    config3 = utils.load_config(config_path)["model_params"]

    hydra = Hydra.get_from_params(**config3)

    config3_copy = copy.deepcopy(config3)
    _pop_normalization(config3_copy)
    encoder_params = config3_copy["encoder_params"]
    heads_params = config3_copy["heads_params"]
    heads_params["head1"]["hiddens"].insert(0, 16)
    heads_params["_head2"]["_hidden"]["hiddens"].insert(0, 16)
    heads_params["_head2"]["head2_1"]["hiddens"].insert(0, 16)
    heads_params["_head2"]["_head2_2"]["_hidden"]["hiddens"].insert(0, 16)
    heads_params["_head2"]["_head2_2"]["head2_2_1"]["hiddens"].insert(0, 16)

    net = nn.ModuleDict(
        {
            "encoder": SequentialNet(**encoder_params),
            "embedders": nn.ModuleDict(
                {
                    "target1": nn.Sequential(
                        OrderedDict(
                            [
                                ("embedding", nn.Embedding(embedding_dim=16, num_embeddings=2)),
                                ("normalize", Normalize()),
                            ]
                        )
                    ),
                    "target2": nn.Sequential(
                        OrderedDict(
                            [("embedding", nn.Embedding(embedding_dim=16, num_embeddings=2))]
                        )
                    ),
                }
            ),
            "heads": nn.ModuleDict(
                {
                    "head1": nn.Sequential(
                        OrderedDict([("net", SequentialNet(**heads_params["head1"]))])
                    ),
                    "_head2": nn.ModuleDict(
                        {
                            "_hidden": nn.Sequential(
                                OrderedDict(
                                    [("net", SequentialNet(**heads_params["_head2"]["_hidden"]))]
                                )
                            ),
                            "head2_1": nn.Sequential(
                                OrderedDict(
                                    [
                                        (
                                            "net",
                                            SequentialNet(**heads_params["_head2"]["head2_1"]),
                                        ),
                                        ("normalize", Normalize()),
                                    ]
                                )
                            ),
                            "_head2_2": nn.ModuleDict(
                                {
                                    "_hidden": nn.Sequential(
                                        OrderedDict(
                                            [
                                                (
                                                    "net",
                                                    SequentialNet(
                                                        **heads_params["_head2"]["_head2_2"][
                                                            "_hidden"
                                                        ]
                                                    ),
                                                )
                                            ]
                                        )
                                    ),
                                    "head2_2_1": nn.Sequential(
                                        OrderedDict(
                                            [
                                                (
                                                    "net",
                                                    SequentialNet(
                                                        **heads_params["_head2"]["_head2_2"][
                                                            "head2_2_1"
                                                        ]
                                                    ),
                                                )
                                            ]
                                        )
                                    ),
                                }
                            ),
                        }
                    ),
                }
            ),
        }
    )

    _check_named_parameters(hydra.encoder, net["encoder"])
    _check_named_parameters(hydra.heads, net["heads"])
    _check_named_parameters(hydra.embedders, net["embedders"])

    input_ = torch.rand(1, 16)

    output_kv = hydra(input_)
    assert (input_ == output_kv["features"]).sum().item() == 16
    kv_keys = [
        "features",
        "embeddings",
        "head1",
        "_head2/",
        "_head2/head2_1",
        "_head2/_head2_2/",
        "_head2/_head2_2/head2_2_1",
    ]
    _check_lists(output_kv.keys(), kv_keys)

    output_kv = hydra(input_, target1=torch.ones(1, 2).long())
    kv_keys = [
        "features",
        "embeddings",
        "head1",
        "_head2/",
        "_head2/head2_1",
        "_head2/_head2_2/",
        "_head2/_head2_2/head2_2_1",
        "target1_embeddings",
    ]
    _check_lists(output_kv.keys(), kv_keys)

    output_kv = hydra(input_, target2=torch.ones(1, 2).long())
    kv_keys = [
        "features",
        "embeddings",
        "head1",
        "_head2/",
        "_head2/head2_1",
        "_head2/_head2_2/",
        "_head2/_head2_2/head2_2_1",
        "target2_embeddings",
    ]
    _check_lists(output_kv.keys(), kv_keys)

    output_kv = hydra(input_, target1=torch.ones(1, 2).long(), target2=torch.ones(1, 2).long())
    kv_keys = [
        "features",
        "embeddings",
        "head1",
        "_head2/",
        "_head2/head2_1",
        "_head2/_head2_2/",
        "_head2/_head2_2/head2_2_1",
        "target1_embeddings",
        "target2_embeddings",
    ]
    _check_lists(output_kv.keys(), kv_keys)

    output_tuple = hydra.forward_tuple(input_)
    assert len(output_tuple) == 5
    assert (output_tuple[0] == output_kv["features"]).sum().item() == 16
    assert (output_tuple[1] == output_kv["embeddings"]).sum().item() == 16


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 28:</b> &nbsp; 2 fragments, nominal size 97 lines, similarity 77%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag699')" href="javascript:;">
catalyst-21.09/tests/catalyst/contrib/models/test_hydra.py: 227-363
</a>
<div class="mid" id="frag699" style="display:none"><pre>
def test_config2():
    """@TODO: Docs. Contribution is welcome."""
    config2 = {
        "in_features": 16,
        "heads_params": {
            "head1": {"hiddens": [2], "layer_fn": {"module": "Linear", "bias": True}},
            "_head2": {
                "_hidden": {"hiddens": [16], "layer_fn": {"module": "Linear", "bias": False}},
                "head2_1": {
                    "hiddens": [32],
                    "layer_fn": {"module": "Linear", "bias": True},
                    "normalize_output": True,
                },
                "_head2_2": {
                    "_hidden": {
                        "hiddens": [16, 16, 16],
                        "layer_fn": {"module": "Linear", "bias": False},
                    },
                    "head2_2_1": {
                        "hiddens": [32],
                        "layer_fn": {"module": "Linear", "bias": True},
                        "normalize_output": False,
                    },
                },
            },
        },
    }

    hydra = Hydra.get_from_params(**config2)

    config2_copy = copy.deepcopy(config2)
    _pop_normalization(config2_copy)
    heads_params = config2_copy["heads_params"]
    heads_params["head1"]["hiddens"].insert(0, 16)
    heads_params["_head2"]["_hidden"]["hiddens"].insert(0, 16)
    heads_params["_head2"]["head2_1"]["hiddens"].insert(0, 16)
    heads_params["_head2"]["_head2_2"]["_hidden"]["hiddens"].insert(0, 16)
    heads_params["_head2"]["_head2_2"]["head2_2_1"]["hiddens"].insert(0, 16)

    net = nn.ModuleDict(
        {
            "encoder": nn.Sequential(),
            "heads": nn.ModuleDict(
                {
                    "head1": nn.Sequential(
                        OrderedDict([("net", SequentialNet(**heads_params["head1"]))])
                    ),
                    "_head2": nn.ModuleDict(
                        {
                            "_hidden": nn.Sequential(
                                OrderedDict(
                                    [("net", SequentialNet(**heads_params["_head2"]["_hidden"]))]
                                )
                            ),
                            "head2_1": nn.Sequential(
                                OrderedDict(
                                    [
                                        (
                                            "net",
                                            SequentialNet(**heads_params["_head2"]["head2_1"]),
                                        ),
                                        ("normalize", Normalize()),
                                    ]
                                )
                            ),
                            "_head2_2": nn.ModuleDict(
                                {
                                    "_hidden": nn.Sequential(
                                        OrderedDict(
                                            [
                                                (
                                                    "net",
                                                    SequentialNet(
                                                        **heads_params["_head2"]["_head2_2"][
                                                            "_hidden"
                                                        ]
                                                    ),
                                                )
                                            ]
                                        )
                                    ),
                                    "head2_2_1": nn.Sequential(
                                        OrderedDict(
                                            [
                                                (
                                                    "net",
                                                    SequentialNet(
                                                        **heads_params["_head2"]["_head2_2"][
                                                            "head2_2_1"
                                                        ]
                                                    ),
                                                )
                                            ]
                                        )
                                    ),
                                }
                            ),
                        }
                    ),
                }
            ),
        }
    )

    _check_named_parameters(hydra.encoder, net["encoder"])
    _check_named_parameters(hydra.heads, net["heads"])
    assert hydra.embedders == {}

    input_ = torch.rand(1, 16)

    output_kv = hydra(input_)
    assert (input_ == output_kv["features"]).sum().item() == 16
    assert (input_ == output_kv["embeddings"]).sum().item() == 16
    kv_keys = [
        "features",
        "embeddings",
        "head1",
        "_head2/",
        "_head2/head2_1",
        "_head2/_head2_2/",
        "_head2/_head2_2/head2_2_1",
    ]
    _check_lists(output_kv.keys(), kv_keys)

    with pytest.raises(KeyError):
        output_kv = hydra(input_, target1=torch.ones(1, 2).long())
    with pytest.raises(KeyError):
        output_kv = hydra(input_, target2=torch.ones(1, 2).long())
    with pytest.raises(KeyError):
        output_kv = hydra(input_, target1=torch.ones(1, 2).long(), target2=torch.ones(1, 2).long())

    output_tuple = hydra.forward_tuple(input_)
    assert len(output_tuple) == 5
    assert (output_tuple[0] == output_kv["features"]).sum().item() == 16
    assert (output_tuple[1] == output_kv["embeddings"]).sum().item() == 16


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag701')" href="javascript:;">
catalyst-21.09/tests/catalyst/contrib/models/test_hydra.py: 528-642
</a>
<div class="mid" id="frag701" style="display:none"><pre>
def test_config4():
    """@TODO: Docs. Contribution is welcome."""
    config_path = Path(__file__).absolute().parent / "config4.yml"
    config4 = utils.load_config(config_path)["model_params"]

    with pytest.raises(AssertionError):
        hydra = Hydra.get_from_params(**config4)
    config4["in_features"] = 16
    hydra = Hydra.get_from_params(**config4)

    config4_copy = copy.deepcopy(config4)
    _pop_normalization(config4_copy)
    heads_params = config4_copy["heads_params"]
    heads_params["head1"]["hiddens"].insert(0, 16)
    heads_params["_head2"]["_hidden"]["hiddens"].insert(0, 16)
    heads_params["_head2"]["head2_1"]["hiddens"].insert(0, 16)
    heads_params["_head2"]["_head2_2"]["_hidden"]["hiddens"].insert(0, 16)
    heads_params["_head2"]["_head2_2"]["head2_2_1"]["hiddens"].insert(0, 16)

    net = nn.ModuleDict(
        {
            "encoder": nn.Sequential(),
            "heads": nn.ModuleDict(
                {
                    "head1": nn.Sequential(
                        OrderedDict([("net", SequentialNet(**heads_params["head1"]))])
                    ),
                    "_head2": nn.ModuleDict(
                        {
                            "_hidden": nn.Sequential(
                                OrderedDict(
                                    [("net", SequentialNet(**heads_params["_head2"]["_hidden"]))]
                                )
                            ),
                            "head2_1": nn.Sequential(
                                OrderedDict(
                                    [
                                        (
                                            "net",
                                            SequentialNet(**heads_params["_head2"]["head2_1"]),
                                        ),
                                        ("normalize", Normalize()),
                                    ]
                                )
                            ),
                            "_head2_2": nn.ModuleDict(
                                {
                                    "_hidden": nn.Sequential(
                                        OrderedDict(
                                            [
                                                (
                                                    "net",
                                                    SequentialNet(
                                                        **heads_params["_head2"]["_head2_2"][
                                                            "_hidden"
                                                        ]
                                                    ),
                                                )
                                            ]
                                        )
                                    ),
                                    "head2_2_1": nn.Sequential(
                                        OrderedDict(
                                            [
                                                (
                                                    "net",
                                                    SequentialNet(
                                                        **heads_params["_head2"]["_head2_2"][
                                                            "head2_2_1"
                                                        ]
                                                    ),
                                                )
                                            ]
                                        )
                                    ),
                                }
                            ),
                        }
                    ),
                }
            ),
        }
    )

    _check_named_parameters(hydra.encoder, net["encoder"])
    _check_named_parameters(hydra.heads, net["heads"])
    assert hydra.embedders == {}

    input_ = torch.rand(1, 16)

    output_kv = hydra(input_)
    assert (input_ == output_kv["features"]).sum().item() == 16
    assert (input_ == output_kv["embeddings"]).sum().item() == 16
    kv_keys = [
        "features",
        "embeddings",
        "head1",
        "_head2/",
        "_head2/head2_1",
        "_head2/_head2_2/",
        "_head2/_head2_2/head2_2_1",
    ]
    _check_lists(output_kv.keys(), kv_keys)

    with pytest.raises(KeyError):
        output_kv = hydra(input_, target1=torch.ones(1, 2).long())
    with pytest.raises(KeyError):
        output_kv = hydra(input_, target2=torch.ones(1, 2).long())
    with pytest.raises(KeyError):
        output_kv = hydra(input_, target1=torch.ones(1, 2).long(), target2=torch.ones(1, 2).long())

    output_tuple = hydra.forward_tuple(input_)
    assert len(output_tuple) == 5
    assert (output_tuple[0] == output_kv["features"]).sum().item() == 16
    assert (output_tuple[1] == output_kv["embeddings"]).sum().item() == 16
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 29:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag702')" href="javascript:;">
catalyst-21.09/tests/catalyst/contrib/models/test_functional.py: 5-18
</a>
<div class="mid" id="frag702" style="display:none"><pre>
def test_linear():
    net = get_linear_net(
        in_features=32,
        features=[128, 64, 64],
        use_bias=[True, False, False],
        normalization=[None, "BatchNorm1d", "LayerNorm"],
        dropout_rate=[None, 0.5, 0.8],
        activation=[None, "ReLU", {"module": "ELU", "alpha": 0.5}],
        residual="soft",
    )

    print(net)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag703')" href="javascript:;">
catalyst-21.09/tests/catalyst/contrib/models/test_functional.py: 19-33
</a>
<div class="mid" id="frag703" style="display:none"><pre>
def test_convolution():
    net = get_convolution_net(
        in_channels=3,
        channels=[128, 64, 64],
        kernel_sizes=[8, 4, 3],
        strides=[4, 2, 1],
        groups=[1, 2, 2],
        use_bias=[True, False, False],
        normalization=[None, "BatchNorm2d", "BatchNorm2d"],
        dropout_rate=[None, 0.5, 0.8],
        activation=[None, "ReLU", {"module": "ELU", "alpha": 0.5}],
        residual="soft",
    )

    print(net)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 30:</b> &nbsp; 9 fragments, nominal size 11 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag722')" href="javascript:;">
catalyst-21.09/tests/catalyst/engines/test_parallel_apex.py: 50-64
</a>
<div class="mid" id="frag722" style="display:none"><pre>
    def get_callbacks(self, stage: str):
        return {
            "criterion": CriterionCallback(
                metric_key="loss", input_key="logits", target_key="targets"
            ),
            "optimizer": OptimizerCallback(metric_key="loss"),
            # "scheduler": dl.SchedulerCallback(loader_key="valid", metric_key="loss"),
            "checkpoint": CheckpointCallback(
                self._logdir, loader_key="valid", metric_key="loss", minimize=True, save_n_best=3
            ),
            "test_nn_parallel_data_parallel": DataParallelTypeChecker(),
            "test_loss_minimization": LossMinimizationCallback("loss", logger=logger),
            "test_logits_type": OPTTensorTypeChecker("logits", self._opt_level),
        }

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag755')" href="javascript:;">
catalyst-21.09/tests/catalyst/engines/test_distributed_amp.py: 50-64
</a>
<div class="mid" id="frag755" style="display:none"><pre>
    def get_callbacks(self, stage: str):
        return {
            "criterion": CriterionCallback(
                metric_key="loss", input_key="logits", target_key="targets"
            ),
            "optimizer": OptimizerCallback(metric_key="loss"),
            # "scheduler": dl.SchedulerCallback(loader_key="valid", metric_key="loss"),
            "checkpoint": CheckpointCallback(
                self._logdir, loader_key="valid", metric_key="loss", minimize=True, save_n_best=3
            ),
            "test_nn_parallel_distributed_data_parallel": DistributedDataParallelTypeChecker(),
            "test_loss_minimization": LossMinimizationCallback("loss", logger=logger),
            "test_world_size": WorldSizeCheckCallback(NUM_CUDA_DEVICES, logger=logger),
        }

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag894')" href="javascript:;">
catalyst-21.09/tests/catalyst/engines/test_parallel.py: 32-45
</a>
<div class="mid" id="frag894" style="display:none"><pre>
    def get_callbacks(self, stage: str):
        return {
            "criterion": CriterionCallback(
                metric_key="loss", input_key="logits", target_key="targets"
            ),
            "optimizer": OptimizerCallback(metric_key="loss"),
            # "scheduler": dl.SchedulerCallback(loader_key="valid", metric_key="loss"),
            "checkpoint": CheckpointCallback(
                self._logdir, loader_key="valid", metric_key="loss", minimize=True, save_n_best=3
            ),
            "test_nn_parallel_data_parallel": DataParallelTypeChecker(),
            "test_loss_minimization": LossMinimizationCallback("loss", logger=logger),
        }

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag788')" href="javascript:;">
catalyst-21.09/tests/catalyst/engines/test_device.py: 42-56
</a>
<div class="mid" id="frag788" style="display:none"><pre>
    def get_callbacks(self, stage: str):
        return {
            "criterion": CriterionCallback(
                metric_key="loss", input_key="logits", target_key="targets"
            ),
            "optimizer": OptimizerCallback(metric_key="loss"),
            # "scheduler": dl.SchedulerCallback(loader_key="valid", metric_key="loss"),
            "checkpoint": CheckpointCallback(
                self._logdir, loader_key="valid", metric_key="loss", minimize=True, save_n_best=3
            ),
            "test_nn_module": ModuleTypeChecker(),
            "test_device": DeviceCheckCallback(self._device, logger=logger),
            "test_loss_minimization": LossMinimizationCallback("loss", logger=logger),
        }

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag831')" href="javascript:;">
catalyst-21.09/tests/catalyst/engines/test_distributed_apex.py: 56-71
</a>
<div class="mid" id="frag831" style="display:none"><pre>
    def get_callbacks(self, stage: str):
        return {
            "criterion": CriterionCallback(
                metric_key="loss", input_key="logits", target_key="targets"
            ),
            "optimizer": OptimizerCallback(metric_key="loss"),
            # "scheduler": dl.SchedulerCallback(loader_key="valid", metric_key="loss"),
            "checkpoint": CheckpointCallback(
                self._logdir, loader_key="valid", metric_key="loss", minimize=True, save_n_best=3
            ),
            # "test_nn_parallel_distributed_data_parallel": DistributedDataParallelTypeChecker(),
            "test_loss_minimization": LossMinimizationCallback("loss", logger=logger),
            "test_world_size": WorldSizeCheckCallback(NUM_CUDA_DEVICES, logger=logger),
            "test_logits_type": OPTTensorTypeChecker("logits", self._opt_level),
        }

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag739')" href="javascript:;">
catalyst-21.09/tests/catalyst/engines/test_distributed.py: 48-62
</a>
<div class="mid" id="frag739" style="display:none"><pre>
    def get_callbacks(self, stage: str):
        return {
            "criterion": CriterionCallback(
                metric_key="loss", input_key="logits", target_key="targets"
            ),
            "optimizer": OptimizerCallback(metric_key="loss"),
            # "scheduler": dl.SchedulerCallback(loader_key="valid", metric_key="loss"),
            "checkpoint": CheckpointCallback(
                self._logdir, loader_key="valid", metric_key="loss", minimize=True, save_n_best=3
            ),
            "test_nn_parallel_distributed_data_parallel": DistributedDataParallelTypeChecker(),
            "test_loss_minimization": LossMinimizationCallback("loss", logger=logger),
            "test_world_size": WorldSizeCheckCallback(NUM_CUDA_DEVICES, logger=logger),
        }

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag805')" href="javascript:;">
catalyst-21.09/tests/catalyst/engines/test_amp.py: 41-57
</a>
<div class="mid" id="frag805" style="display:none"><pre>
    def get_callbacks(self, stage: str):
        return {
            "criterion": CriterionCallback(
                metric_key="loss", input_key="logits", target_key="targets"
            ),
            "optimizer": OptimizerCallback(metric_key="loss"),
            # "scheduler": dl.SchedulerCallback(loader_key="valid", metric_key="loss"),
            "checkpoint": CheckpointCallback(
                self._logdir, loader_key="valid", metric_key="loss", minimize=True, save_n_best=3
            ),
            "test_nn_module": ModuleTypeChecker(),
            "test_device": DeviceCheckCallback(self._device, logger=logger),
            "test_loss_minimization": LossMinimizationCallback("loss", logger=logger),
            "test_logits_type": TensorTypeChecker("logits"),
            # "loss_type_checker": TensorTypeChecker("loss", True),
        }

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag771')" href="javascript:;">
catalyst-21.09/tests/catalyst/engines/test_parallel_amp.py: 40-54
</a>
<div class="mid" id="frag771" style="display:none"><pre>
    def get_callbacks(self, stage: str):
        return {
            "criterion": CriterionCallback(
                metric_key="loss", input_key="logits", target_key="targets"
            ),
            "optimizer": OptimizerCallback(metric_key="loss"),
            # "scheduler": dl.SchedulerCallback(loader_key="valid", metric_key="loss"),
            "checkpoint": CheckpointCallback(
                self._logdir, loader_key="valid", metric_key="loss", minimize=True, save_n_best=3
            ),
            "test_nn_parallel_data_parallel": DataParallelTypeChecker(),
            "test_loss_minimization": LossMinimizationCallback("loss", logger=logger),
            "test_logits_type": TensorTypeChecker("logits"),
        }

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag877')" href="javascript:;">
catalyst-21.09/tests/catalyst/engines/test_apex.py: 50-65
</a>
<div class="mid" id="frag877" style="display:none"><pre>
    def get_callbacks(self, stage: str):
        return {
            "criterion": CriterionCallback(
                metric_key="loss", input_key="logits", target_key="targets"
            ),
            "optimizer": OptimizerCallback(metric_key="loss"),
            # "scheduler": dl.SchedulerCallback(loader_key="valid", metric_key="loss"),
            "checkpoint": CheckpointCallback(
                self._logdir, loader_key="valid", metric_key="loss", minimize=True, save_n_best=3
            ),
            "test_nn_module": ModuleTypeChecker(),
            "test_device": DeviceCheckCallback(self._device, logger=logger),
            "test_loss_minimization": LossMinimizationCallback("loss", logger=logger),
            "test_logits_type": OPTTensorTypeChecker("logits", self._opt_level),
        }

</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 31:</b> &nbsp; 9 fragments, nominal size 44 lines, similarity 77%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag734')" href="javascript:;">
catalyst-21.09/tests/catalyst/engines/test_parallel_apex.py: 108-157
</a>
<div class="mid" id="frag734" style="display:none"><pre>
def train_from_config(opt_level):
    with TemporaryDirectory() as logdir:
        dataset = DummyDataset(6)
        runner = SupervisedConfigRunner(
            config={
                "args": {"logdir": logdir},
                "model": {"_target_": "DummyModel", "in_features": 4, "out_features": 2},
                "engine": {
                    "_target_": "DataParallelApexEngine",
                    "apex_kwargs": {"opt_level": opt_level},
                },
                "args": {"logdir": logdir},
                "stages": {
                    "stage1": {
                        "num_epochs": 10,
                        "loaders": {"batch_size": 4, "num_workers": 0},
                        "criterion": {"_target_": "MSELoss"},
                        "optimizer": {"_target_": "Adam", "lr": 1e-3},
                        "callbacks": {
                            "criterion": {
                                "_target_": "CriterionCallback",
                                "metric_key": "loss",
                                "input_key": "logits",
                                "target_key": "targets",
                            },
                            "optimizer": {"_target_": "OptimizerCallback", "metric_key": "loss"},
                            "test_nn_parallel_data_parallel": {
                                "_target_": "DataParallelTypeChecker"
                            },
                            "test_loss_minimization": {
                                "_target_": "LossMinimizationCallback",
                                "key": "loss",
                            },
                            "test_logits_type": {
                                "_target_": "OPTTensorTypeChecker",
                                "key": "logits",
                                "opt_level": opt_level,
                            },
                        },
                    },
                },
            }
        )
        runner.get_datasets = lambda *args, **kwargs: {
            "train": dataset,
            "valid": dataset,
        }
        runner.run()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag889')" href="javascript:;">
catalyst-21.09/tests/catalyst/engines/test_apex.py: 109-161
</a>
<div class="mid" id="frag889" style="display:none"><pre>
def train_from_config(device, opt_level):
    with TemporaryDirectory() as logdir:
        dataset = DummyDataset(6)
        runner = SupervisedConfigRunner(
            config={
                "args": {"logdir": logdir},
                "model": {"_target_": "DummyModel", "in_features": 4, "out_features": 2},
                "engine": {
                    "_target_": "APEXEngine",
                    "device": device,
                    "apex_kwargs": {"opt_level": opt_level.upper()},
                },
                "args": {"logdir": logdir},
                "stages": {
                    "stage1": {
                        "num_epochs": 10,
                        "criterion": {"_target_": "MSELoss"},
                        "optimizer": {"_target_": "Adam", "lr": 1e-3},
                        "loaders": {"batch_size": 4, "num_workers": 0},
                        "callbacks": {
                            "criterion": {
                                "_target_": "CriterionCallback",
                                "metric_key": "loss",
                                "input_key": "logits",
                                "target_key": "targets",
                            },
                            "optimizer": {"_target_": "OptimizerCallback", "metric_key": "loss"},
                            "test_nn_module": {"_target_": "ModuleTypeChecker"},
                            "test_device": {
                                "_target_": "DeviceCheckCallback",
                                "assert_device": device,
                            },
                            "test_loss_minimization": {
                                "_target_": "LossMinimizationCallback",
                                "key": "loss",
                            },
                            "test_opt_logits_type": {
                                "_target_": "OPTTensorTypeChecker",
                                "key": "logits",
                                "opt_level": opt_level,
                            },
                        },
                    },
                },
            }
        )
        runner.get_datasets = lambda *args, **kwargs: {
            "train": dataset,
            "valid": dataset,
        }
        runner.run()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag752')" href="javascript:;">
catalyst-21.09/tests/catalyst/engines/test_distributed.py: 121-163
</a>
<div class="mid" id="frag752" style="display:none"><pre>
def test_config_ddp_engine():
    with TemporaryDirectory() as logdir:
        runner = MyConfigRunner(
            config={
                "args": {"logdir": logdir},
                "model": {"_target_": "DummyModel", "in_features": 4, "out_features": 2},
                "engine": {
                    "_target_": "DistributedDataParallelEngine",
                    "port": DDP_ADDRESS + random.randint(100, 200),
                    "process_group_kwargs": {"backend": "nccl"},
                },
                "loggers": {"console": {"_target_": "ConsoleLogger"}},
                "stages": {
                    "stage1": {
                        "num_epochs": 10,
                        "loaders": {"batch_size": 4, "num_workers": 0},
                        "criterion": {"_target_": "MSELoss"},
                        "optimizer": {"_target_": "Adam", "lr": 1e-3},
                        "callbacks": {
                            "criterion": {
                                "_target_": "CriterionCallback",
                                "metric_key": "loss",
                                "input_key": "logits",
                                "target_key": "targets",
                            },
                            "optimizer": {"_target_": "OptimizerCallback", "metric_key": "loss"},
                            "test_nn_parallel_distributed_data_parallel": {
                                "_target_": "DistributedDataParallelTypeChecker"
                            },
                            "test_loss_minimization": {
                                "_target_": "LossMinimizationCallback",
                                "key": "loss",
                            },
                            "test_world_size": {
                                "_target_": "WorldSizeCheckCallback",
                                "assert_world_size": NUM_CUDA_DEVICES,
                            },
                        },
                    },
                },
            }
        )
        runner.run()
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag817')" href="javascript:;">
catalyst-21.09/tests/catalyst/engines/test_amp.py: 101-145
</a>
<div class="mid" id="frag817" style="display:none"><pre>
def train_from_config(device):
    with TemporaryDirectory() as logdir:
        dataset = DummyDataset(6)
        runner = SupervisedConfigRunner(
            config={
                "args": {"logdir": logdir},
                "model": {"_target_": "DummyModel", "in_features": 4, "out_features": 2},
                "engine": {"_target_": "AMPEngine", "device": device},
                "args": {"logdir": logdir},
                "stages": {
                    "stage1": {
                        "num_epochs": 10,
                        "criterion": {"_target_": "MSELoss"},
                        "optimizer": {"_target_": "Adam", "lr": 1e-3},
                        "loaders": {"batch_size": 4, "num_workers": 0},
                        "callbacks": {
                            "criterion": {
                                "_target_": "CriterionCallback",
                                "metric_key": "loss",
                                "input_key": "logits",
                                "target_key": "targets",
                            },
                            "optimizer": {"_target_": "OptimizerCallback", "metric_key": "loss"},
                            "test_nn_module": {"_target_": "ModuleTypeChecker"},
                            "test_device": {
                                "_target_": "DeviceCheckCallback",
                                "assert_device": device,
                            },
                            "test_loss_minimization": {
                                "_target_": "LossMinimizationCallback",
                                "key": "loss",
                            },
                            "test_logits_type": {"_target_": "TensorTypeChecker", "key": "logits"},
                        },
                    },
                },
            }
        )
        runner.get_datasets = lambda *args, **kwargs: {
            "train": dataset,
            "valid": dataset,
        }
        runner.run()


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag843')" href="javascript:;">
catalyst-21.09/tests/catalyst/engines/test_distributed_apex.py: 116-166
</a>
<div class="mid" id="frag843" style="display:none"><pre>
def train_from_config(port, logdir, opt_lvl):
    opt = str(opt_lvl).strip().upper()
    runner = MyConfigRunner(
        config={
            "args": {"logdir": logdir},
            "model": {"_target_": "DummyModel", "in_features": 4, "out_features": 2},
            "engine": {
                "_target_": "DistributedDataParallelApexEngine",
                "port": DDP_ADDRESS + random.randint(100, 200),
                "process_group_kwargs": {"backend": "nccl"},
                "apex_kwargs": {"opt_level": opt},
            },
            "loggers": {"console": {"_target_": "ConsoleLogger"}},
            "stages": {
                "stage1": {
                    "num_epochs": 10,
                    "loaders": {"batch_size": 4, "num_workers": 0},
                    "criterion": {"_target_": "MSELoss"},
                    "optimizer": {"_target_": "Adam", "lr": 1e-3},
                    "callbacks": {
                        "criterion": {
                            "_target_": "CriterionCallback",
                            "metric_key": "loss",
                            "input_key": "logits",
                            "target_key": "targets",
                        },
                        "optimizer": {"_target_": "OptimizerCallback", "metric_key": "loss"},
                        # "test_nn_parallel_distributed_data_parallel": {
                        #     "_target_": "DistributedDataParallelTypeChecker"
                        # },
                        "test_loss_minimization": {
                            "_target_": "LossMinimizationCallback",
                            "key": "loss",
                        },
                        "test_world_size": {
                            "_target_": "WorldSizeCheckCallback",
                            "assert_world_size": NUM_CUDA_DEVICES,
                        },
                        "test_logits_type": {
                            "_target_": "OPTTensorTypeChecker",
                            "key": "logits",
                            "opt_level": opt,
                        },
                    },
                },
            },
        }
    )
    runner.run()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag768')" href="javascript:;">
catalyst-21.09/tests/catalyst/engines/test_distributed_amp.py: 123-166
</a>
<div class="mid" id="frag768" style="display:none"><pre>
def test_train_with_config_experiment_distributed_parallel_amp_device():
    with TemporaryDirectory() as logdir:
        runner = MyConfigRunner(
            config={
                "args": {"logdir": logdir},
                "model": {"_target_": "DummyModel", "in_features": 4, "out_features": 2},
                "engine": {
                    "_target_": "DistributedDataParallelAMPEngine",
                    "port": DDP_ADDRESS + random.randint(100, 200),
                    "process_group_kwargs": {"backend": "nccl"},
                },
                "loggers": {"console": {"_target_": "ConsoleLogger"}},
                "stages": {
                    "stage1": {
                        "num_epochs": 10,
                        "loaders": {"batch_size": 4, "num_workers": 0},
                        "criterion": {"_target_": "MSELoss"},
                        "optimizer": {"_target_": "Adam", "lr": 1e-3},
                        "callbacks": {
                            "criterion": {
                                "_target_": "CriterionCallback",
                                "metric_key": "loss",
                                "input_key": "logits",
                                "target_key": "targets",
                            },
                            "optimizer": {"_target_": "OptimizerCallback", "metric_key": "loss"},
                            "test_nn_parallel_distributed_data_parallel": {
                                "_target_": "DistributedDataParallelTypeChecker"
                            },
                            "test_loss_minimization": {
                                "_target_": "LossMinimizationCallback",
                                "key": "loss",
                            },
                            "test_world_size": {
                                "_target_": "WorldSizeCheckCallback",
                                "assert_world_size": NUM_CUDA_DEVICES,
                            },
                            "test_logits_type": {"_target_": "TensorTypeChecker", "key": "logits"},
                        },
                    },
                },
            }
        )
        runner.run()
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag906')" href="javascript:;">
catalyst-21.09/tests/catalyst/engines/test_parallel.py: 89-130
</a>
<div class="mid" id="frag906" style="display:none"><pre>
def train_from_config():
    with TemporaryDirectory() as logdir:
        dataset = DummyDataset(6)
        runner = SupervisedConfigRunner(
            config={
                "args": {"logdir": logdir},
                "model": {"_target_": "DummyModel", "in_features": 4, "out_features": 2},
                "engine": {"_target_": "DataParallelEngine"},
                "args": {"logdir": logdir},
                "stages": {
                    "stage1": {
                        "num_epochs": 10,
                        "loaders": {"batch_size": 4, "num_workers": 0},
                        "criterion": {"_target_": "MSELoss"},
                        "optimizer": {"_target_": "Adam", "lr": 1e-3},
                        "callbacks": {
                            "criterion": {
                                "_target_": "CriterionCallback",
                                "metric_key": "loss",
                                "input_key": "logits",
                                "target_key": "targets",
                            },
                            "optimizer": {"_target_": "OptimizerCallback", "metric_key": "loss"},
                            "test_nn_parallel_data_parallel": {
                                "_target_": "DataParallelTypeChecker"
                            },
                            "test_loss_minimization": {
                                "_target_": "LossMinimizationCallback",
                                "key": "loss",
                            },
                        },
                    },
                },
            }
        )
        runner.get_datasets = lambda *args, **kwargs: {
            "train": dataset,
            "valid": dataset,
        }
        runner.run()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag783')" href="javascript:;">
catalyst-21.09/tests/catalyst/engines/test_parallel_amp.py: 98-140
</a>
<div class="mid" id="frag783" style="display:none"><pre>
def train_from_config():
    with TemporaryDirectory() as logdir:
        dataset = DummyDataset(6)
        runner = SupervisedConfigRunner(
            config={
                "args": {"logdir": logdir},
                "model": {"_target_": "DummyModel", "in_features": 4, "out_features": 2},
                "engine": {"_target_": "DataParallelAMPEngine"},
                "args": {"logdir": logdir},
                "stages": {
                    "stage1": {
                        "num_epochs": 10,
                        "loaders": {"batch_size": 4, "num_workers": 0},
                        "criterion": {"_target_": "MSELoss"},
                        "optimizer": {"_target_": "Adam", "lr": 1e-3},
                        "callbacks": {
                            "criterion": {
                                "_target_": "CriterionCallback",
                                "metric_key": "loss",
                                "input_key": "logits",
                                "target_key": "targets",
                            },
                            "optimizer": {"_target_": "OptimizerCallback", "metric_key": "loss"},
                            "test_nn_parallel_data_parallel": {
                                "_target_": "DataParallelTypeChecker"
                            },
                            "test_loss_minimization": {
                                "_target_": "LossMinimizationCallback",
                                "key": "loss",
                            },
                            "test_logits_type": {"_target_": "TensorTypeChecker", "key": "logits"},
                        },
                    },
                },
            }
        )
        runner.get_datasets = lambda *args, **kwargs: {
            "train": dataset,
            "valid": dataset,
        }
        runner.run()


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag800')" href="javascript:;">
catalyst-21.09/tests/catalyst/engines/test_device.py: 100-142
</a>
<div class="mid" id="frag800" style="display:none"><pre>
def train_from_config(device):
    with TemporaryDirectory() as logdir:
        dataset = DummyDataset(6)
        runner = SupervisedConfigRunner(
            config={
                "args": {"logdir": logdir},
                "model": {"_target_": "DummyModel", "in_features": 4, "out_features": 2},
                "engine": {"_target_": "DeviceEngine", "device": device},
                "stages": {
                    "stage1": {
                        "num_epochs": 10,
                        "criterion": {"_target_": "MSELoss"},
                        "optimizer": {"_target_": "Adam", "lr": 1e-3},
                        "loaders": {"batch_size": 4, "num_workers": 0},
                        "callbacks": {
                            "criterion": {
                                "_target_": "CriterionCallback",
                                "metric_key": "loss",
                                "input_key": "logits",
                                "target_key": "targets",
                            },
                            "optimizer": {"_target_": "OptimizerCallback", "metric_key": "loss"},
                            "test_nn_module": {"_target_": "ModuleTypeChecker"},
                            "test_device": {
                                "_target_": "DeviceCheckCallback",
                                "assert_device": device,
                            },
                            "test_loss_minimization": {
                                "_target_": "LossMinimizationCallback",
                                "key": "loss",
                            },
                        },
                    },
                },
            }
        )
        runner.get_datasets = lambda *args, **kwargs: {
            "train": dataset,
            "valid": dataset,
        }
        runner.run()


</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 32:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 92%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag909')" href="javascript:;">
catalyst-21.09/tests/catalyst/data/test_loader.py: 8-22
</a>
<div class="mid" id="frag909" style="display:none"><pre>
def test_batch_limit1() -&gt; None:
    for shuffle in (False, True):
        num_samples, num_features = int(1e2), int(1e1)
        X, y = torch.rand(num_samples, num_features), torch.rand(num_samples)
        dataset = TensorDataset(X, y)
        loader = DataLoader(dataset, batch_size=4, num_workers=1, shuffle=shuffle)
        loader = BatchLimitLoaderWrapper(loader, num_batches=1)

        batch1 = next(iter(loader))[0]
        batch2 = next(iter(loader))[0]
        batch3 = next(iter(loader))[0]
        assert all(torch.isclose(x, y).all() for x, y in zip(batch1, batch2))
        assert all(torch.isclose(x, y).all() for x, y in zip(batch2, batch3))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag910')" href="javascript:;">
catalyst-21.09/tests/catalyst/data/test_loader.py: 23-36
</a>
<div class="mid" id="frag910" style="display:none"><pre>
def test_batch_limit2() -&gt; None:
    for shuffle in (False, True):
        num_samples, num_features = int(1e2), int(1e1)
        X, y = torch.rand(num_samples, num_features), torch.rand(num_samples)
        dataset = TensorDataset(X, y)
        loader = DataLoader(dataset, batch_size=4, num_workers=1, shuffle=shuffle)
        loader = BatchLimitLoaderWrapper(loader, num_batches=2)

        batch1 = next(iter(loader))[0]
        batch2 = next(iter(loader))[0]
        batch3 = next(iter(loader))[0]
        batch4 = next(iter(loader))[0]
        assert all(torch.isclose(x, y).all() for x, y in zip(batch1, batch3))
        assert all(torch.isclose(x, y).all() for x, y in zip(batch2, batch4))
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 33:</b> &nbsp; 5 fragments, nominal size 14 lines, similarity 92%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag933')" href="javascript:;">
catalyst-21.09/tests/catalyst/callbacks/test_pruning.py: 42-57
</a>
<div class="mid" id="frag933" style="display:none"><pre>
def test_pruning():
    dataloader = prepare_experiment()
    model = nn.Linear(100, 10, bias=False)
    runner = dl.SupervisedRunner()
    criterion = nn.CrossEntropyLoss()
    runner.train(
        model=model,
        optimizer=torch.optim.Adam(model.parameters()),
        criterion=criterion,
        loaders={"train": dataloader},
        callbacks=[PruningCallback(l1_unstructured, amount=0.5)],
        num_epochs=1,
    )
    assert np.isclose(pruning_factor(model), 0.5)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag937')" href="javascript:;">
catalyst-21.09/tests/catalyst/callbacks/test_pruning.py: 121-136
</a>
<div class="mid" id="frag937" style="display:none"><pre>
def test_pruning_str_structured_f():
    dataloader = prepare_experiment()
    model = nn.Linear(100, 10, bias=False)
    runner = dl.SupervisedRunner()
    criterion = nn.CrossEntropyLoss()
    runner.train(
        model=model,
        optimizer=torch.optim.Adam(model.parameters()),
        criterion=criterion,
        loaders={"train": dataloader},
        callbacks=[PruningCallback("ln_structured", amount=0.5, dim=1)],
        num_epochs=1,
    )
    assert np.isclose(pruning_factor(model), 0.5)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag938')" href="javascript:;">
catalyst-21.09/tests/catalyst/callbacks/test_pruning.py: 139-152
</a>
<div class="mid" id="frag938" style="display:none"><pre>
def test_pruning_str_random_structured_f():
    dataloader = prepare_experiment()
    model = nn.Linear(100, 10, bias=False)
    runner = dl.SupervisedRunner()
    criterion = nn.CrossEntropyLoss()
    runner.train(
        model=model,
        optimizer=torch.optim.Adam(model.parameters()),
        criterion=criterion,
        loaders={"train": dataloader},
        callbacks=[PruningCallback("random_structured", amount=0.5)],
        num_epochs=1,
    )
    assert np.isclose(pruning_factor(model), 0.5)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag936')" href="javascript:;">
catalyst-21.09/tests/catalyst/callbacks/test_pruning.py: 103-118
</a>
<div class="mid" id="frag936" style="display:none"><pre>
def test_pruning_str_structured():
    dataloader = prepare_experiment()
    model = nn.Linear(100, 10, bias=False)
    runner = dl.SupervisedRunner()
    criterion = nn.CrossEntropyLoss()
    runner.train(
        model=model,
        optimizer=torch.optim.Adam(model.parameters()),
        criterion=criterion,
        loaders={"train": dataloader},
        callbacks=[PruningCallback("ln_structured", amount=0.5, dim=1, l_norm=2)],
        num_epochs=1,
    )
    assert np.isclose(pruning_factor(model), 0.5)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag935')" href="javascript:;">
catalyst-21.09/tests/catalyst/callbacks/test_pruning.py: 86-101
</a>
<div class="mid" id="frag935" style="display:none"><pre>
def test_pruning_str_unstructured():
    dataloader = prepare_experiment()
    model = nn.Linear(100, 10, bias=False)
    runner = dl.SupervisedRunner()
    criterion = nn.CrossEntropyLoss()
    runner.train(
        model=model,
        optimizer=torch.optim.Adam(model.parameters()),
        criterion=criterion,
        loaders={"train": dataloader},
        callbacks=[PruningCallback("l1_unstructured", amount=0.5)],
        num_epochs=1,
    )
    assert np.isclose(pruning_factor(model), 0.5)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 34:</b> &nbsp; 2 fragments, nominal size 27 lines, similarity 96%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag941')" href="javascript:;">
catalyst-21.09/tests/catalyst/callbacks/test_wrapper_callback.py: 24-55
</a>
<div class="mid" id="frag941" style="display:none"><pre>
    def test_enabled(self):
        runner = Mock(stage="stage1", loader_key="train", epoch=1)

        orders = (
            CallbackOrder.Internal,
            CallbackOrder.Metric,
            CallbackOrder.MetricAggregation,
            CallbackOrder.Optimizer,
            CallbackOrder.Scheduler,
            CallbackOrder.External,
        )

        events = (
            "on_loader_start",
            "on_loader_end",
            "on_stage_start",
            "on_stage_end",
            "on_epoch_start",
            "on_epoch_end",
            "on_batch_start",
            "on_batch_end",
            "on_exception",
        )

        for event in events:
            for order in orders:
                callback = RaiserCallback(order, event)
                wrapper = CallbackWrapper(callback, enable_callback=True)

                with self.assertRaises(Dummy):
                    wrapper.__getattribute__(event)(runner)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag942')" href="javascript:;">
catalyst-21.09/tests/catalyst/callbacks/test_wrapper_callback.py: 56-84
</a>
<div class="mid" id="frag942" style="display:none"><pre>
    def test_disabled(self):
        runner = Mock(stage="stage1", loader_key="train", epoch=1)

        orders = (
            CallbackOrder.Internal,
            CallbackOrder.Metric,
            CallbackOrder.MetricAggregation,
            CallbackOrder.Optimizer,
            CallbackOrder.Scheduler,
            CallbackOrder.External,
        )

        events = (
            "on_loader_start",
            "on_loader_end",
            "on_stage_start",
            "on_stage_end",
            "on_epoch_start",
            "on_epoch_end",
            "on_batch_start",
            "on_batch_end",
            "on_exception",
        )

        for event in events:
            for order in orders:
                callback = RaiserCallback(order, event)
                wrapper = CallbackWrapper(callback, enable_callback=False)
                wrapper.__getattribute__(event)(runner)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 35:</b> &nbsp; 2 fragments, nominal size 36 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag946')" href="javascript:;">
catalyst-21.09/tests/catalyst/callbacks/test_aggregation.py: 31-71
</a>
<div class="mid" id="frag946" style="display:none"><pre>
def test_aggregation_1():
    """
    Aggregation as weighted_sum
    """
    loaders, model, criterion, optimizer = prepare_experiment()
    runner = dl.SupervisedRunner()
    runner.train(
        model=model,
        criterion=criterion,
        optimizer=optimizer,
        loaders=loaders,
        logdir="./logs/aggregation_1/",
        num_epochs=3,
        callbacks=[
            dl.CriterionCallback(
                input_key="logits",
                target_key="targets",
                metric_key="loss_bce",
                criterion_key="bce",
            ),
            dl.CriterionCallback(
                input_key="logits",
                target_key="targets",
                metric_key="loss_focal",
                criterion_key="focal",
            ),
            # loss aggregation
            dl.MetricAggregationCallback(
                metric_key="loss",
                metrics={"loss_focal": 0.6, "loss_bce": 0.4},
                mode="weighted_sum",
            ),
        ],
    )
    for loader in ["train", "valid"]:
        metrics = runner.epoch_metrics[loader]
        loss_1 = metrics["loss_bce"] * 0.4 + metrics["loss_focal"] * 0.6
        loss_2 = metrics["loss"]
        assert np.abs(loss_1 - loss_2) &lt; 1e-5


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag947')" href="javascript:;">
catalyst-21.09/tests/catalyst/callbacks/test_aggregation.py: 72-114
</a>
<div class="mid" id="frag947" style="display:none"><pre>
def test_aggregation_2():
    """
    Aggregation with custom function
    """
    loaders, model, criterion, optimizer = prepare_experiment()
    runner = dl.SupervisedRunner()

    def aggregation_function(metrics, runner):
        epoch = runner.stage_epoch_step
        loss = (3 / 2 - epoch / 2) * metrics["loss_focal"] + (1 / 2 * epoch - 1 / 2) * metrics[
            "loss_bce"
        ]
        return loss

    runner.train(
        model=model,
        criterion=criterion,
        optimizer=optimizer,
        loaders=loaders,
        logdir="./logs/aggregation_2/",
        num_epochs=3,
        callbacks=[
            dl.CriterionCallback(
                input_key="logits",
                target_key="targets",
                metric_key="loss_bce",
                criterion_key="bce",
            ),
            dl.CriterionCallback(
                input_key="logits",
                target_key="targets",
                metric_key="loss_focal",
                criterion_key="focal",
            ),
            # loss aggregation
            dl.MetricAggregationCallback(metric_key="loss", mode=aggregation_function),
        ],
    )
    for loader in ["train", "valid"]:
        metrics = runner.epoch_metrics[loader]
        loss_1 = metrics["loss_bce"]
        loss_2 = metrics["loss"]
        assert np.abs(loss_1 - loss_2) &lt; 1e-5
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 36:</b> &nbsp; 3 fragments, nominal size 21 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag959')" href="javascript:;">
catalyst-21.09/tests/catalyst/callbacks/test_metric.py: 72-101
</a>
<div class="mid" id="frag959" style="display:none"><pre>
    def handle_batch(self, batch: Dict[str, torch.Tensor]) -&gt; None:
        """
        Process batch

        Args:
            batch: batch data
        """
        if self.is_train_loader:
            images, targets = batch["features"].float(), batch["targets"].long()
            features = self.model(images)
            self.batch = {
                "embeddings": features,
                "targets": targets,
            }
        else:
            images, targets, cids, is_query = (
                batch["features"].float(),
                batch["targets"].long(),
                batch["cids"].long(),
                batch["is_query"].bool(),
            )
            features = self.model(images)
            self.batch = {
                "embeddings": features,
                "targets": targets,
                "cids": cids,
                "is_query": is_query,
            }


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1032')" href="javascript:;">
catalyst-21.09/tests/pipelines/test_metric_learning.py: 18-39
</a>
<div class="mid" id="frag1032" style="display:none"><pre>
    def handle_batch(self, batch) -&gt; None:
        if self.is_train_loader:
            images, targets = batch["features"].float(), batch["targets"].long()
            features = self.model(images)
            self.batch = {
                "embeddings": features,
                "targets": targets,
            }
        else:
            images, targets, is_query = (
                batch["features"].float(),
                batch["targets"].long(),
                batch["is_query"].bool(),
            )
            features = self.model(images)
            self.batch = {
                "embeddings": features,
                "targets": targets,
                "is_query": is_query,
            }


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag962')" href="javascript:;">
catalyst-21.09/tests/catalyst/callbacks/test_metric.py: 183-211
</a>
<div class="mid" id="frag962" style="display:none"><pre>
    def handle_batch(self, batch: Dict[str, torch.Tensor]) -&gt; None:
        """
        Handle batch for train and valid loaders

        Args:
            batch: batch to process
        """
        if self.is_train_loader:
            images, targets = batch["features"].float(), batch["targets"].long()
            features = self.model(images)
            self.batch = {
                "embeddings": features,
                "targets": targets,
                "images": images,
            }
        else:
            images, targets, is_query = (
                batch["features"].float(),
                batch["targets"].long(),
                batch["is_query"].bool(),
            )
            features = self.model(images)
            self.batch = {
                "embeddings": features,
                "targets": targets,
                "is_query": is_query,
            }


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 37:</b> &nbsp; 2 fragments, nominal size 49 lines, similarity 79%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag964')" href="javascript:;">
catalyst-21.09/tests/catalyst/callbacks/test_metric.py: 268-332
</a>
<div class="mid" id="frag964" style="display:none"><pre>
def test_reid_pipeline():
    """This test checks that reid pipeline runs and compute metrics with ReidCMCScoreCallback"""
    with TemporaryDirectory() as logdir:

        # 1. train and valid loaders
        transforms = Compose([ToTensor(), Normalize((0.1307,), (0.3081,))])

        train_dataset = MnistMLDataset(root=os.getcwd(), download=True, transform=transforms)
        sampler = data.BatchBalanceClassSampler(
            labels=train_dataset.get_labels(), num_classes=3, num_samples=10, num_batches=20
        )
        train_loader = DataLoader(dataset=train_dataset, batch_sampler=sampler, num_workers=0)

        valid_dataset = MnistReIDQGDataset(
            root=os.getcwd(), transform=transforms, gallery_fraq=0.2
        )
        valid_loader = DataLoader(dataset=valid_dataset, batch_size=1024)

        # 2. model and optimizer
        model = models.MnistSimpleNet(out_features=16)
        optimizer = Adam(model.parameters(), lr=0.001)

        # 3. criterion with triplets sampling
        sampler_inbatch = data.AllTripletsSampler(max_output_triplets=1000)
        criterion = nn.TripletMarginLossWithSampler(margin=0.5, sampler_inbatch=sampler_inbatch)

        # 4. training with catalyst Runner
        callbacks = [
            dl.ControlFlowCallback(
                dl.CriterionCallback(
                    input_key="embeddings", target_key="targets", metric_key="loss"
                ),
                loaders="train",
            ),
            dl.ControlFlowCallback(
                dl.ReidCMCScoreCallback(
                    embeddings_key="embeddings",
                    pids_key="targets",
                    cids_key="cids",
                    is_query_key="is_query",
                    topk_args=[1],
                ),
                loaders="valid",
            ),
            dl.PeriodicLoaderCallback(
                valid_loader_key="valid", valid_metric_key="cmc01", minimize=False, valid=2
            ),
        ]

        runner = ReIDCustomRunner()
        runner.train(
            model=model,
            criterion=criterion,
            optimizer=optimizer,
            callbacks=callbacks,
            loaders=OrderedDict({"train": train_loader, "valid": valid_loader}),
            verbose=False,
            logdir=logdir,
            valid_loader="valid",
            valid_metric="cmc01",
            minimize_valid_metric=False,
            num_epochs=10,
        )
        assert "cmc01" in runner.loader_metrics
        assert runner.loader_metrics["cmc01"] &gt; 0.7
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1033')" href="javascript:;">
catalyst-21.09/tests/pipelines/test_metric_learning.py: 40-106
</a>
<div class="mid" id="frag1033" style="display:none"><pre>
def train_experiment(device, engine=None):
    with TemporaryDirectory() as logdir:

        # 1. train and valid loaders
        transforms = Compose([ToTensor(), Normalize((0.1307,), (0.3081,))])

        train_dataset = datasets.MnistMLDataset(
            root=os.getcwd(), download=True, transform=transforms
        )
        sampler = data.BatchBalanceClassSampler(
            labels=train_dataset.get_labels(), num_classes=5, num_samples=10, num_batches=10
        )
        train_loader = DataLoader(dataset=train_dataset, batch_sampler=sampler)

        valid_dataset = datasets.MnistQGDataset(
            root=os.getcwd(), transform=transforms, gallery_fraq=0.2
        )
        valid_loader = DataLoader(dataset=valid_dataset, batch_size=1024)

        # 2. model and optimizer
        model = models.MnistSimpleNet(out_features=16)
        optimizer = Adam(model.parameters(), lr=0.001)

        # 3. criterion with triplets sampling
        sampler_inbatch = data.HardTripletsSampler(norm_required=False)
        criterion = nn.TripletMarginLossWithSampler(margin=0.5, sampler_inbatch=sampler_inbatch)

        # 4. training with catalyst Runner
        callbacks = [
            dl.ControlFlowCallback(
                dl.CriterionCallback(
                    input_key="embeddings", target_key="targets", metric_key="loss"
                ),
                loaders="train",
            ),
            dl.ControlFlowCallback(
                dl.CMCScoreCallback(
                    embeddings_key="embeddings",
                    labels_key="targets",
                    is_query_key="is_query",
                    topk_args=[1],
                ),
                loaders="valid",
            ),
            dl.PeriodicLoaderCallback(
                valid_loader_key="valid", valid_metric_key="cmc01", minimize=False, valid=2
            ),
        ]

        runner = CustomRunner(input_key="features", output_key="embeddings")
        runner.train(
            engine=engine or dl.DeviceEngine(device),
            model=model,
            criterion=criterion,
            optimizer=optimizer,
            callbacks=callbacks,
            loaders={"train": train_loader, "valid": valid_loader},
            verbose=False,
            logdir=logdir,
            valid_loader="valid",
            valid_metric="cmc01",
            minimize_valid_metric=False,
            num_epochs=2,
        )


# Torch
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 38:</b> &nbsp; 2 fragments, nominal size 32 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag975')" href="javascript:;">
catalyst-21.09/tests/catalyst/metrics/functional/test_iou.py: 7-50
</a>
<div class="mid" id="frag975" style="display:none"><pre>
def test_iou():
    """
    Tests for catalyst.metrics.iou metric.
    """
    size = 4
    half_size = size // 2
    shape = (1, 1, size, size)

    # check 0: one empty
    empty = torch.zeros(shape)
    full = torch.ones(shape)
    assert iou(empty, full, class_dim=1, mode="per-class").item() == 0

    # check 0: no overlap
    left = torch.ones(shape)
    left[:, :, :, half_size:] = 0
    right = torch.ones(shape)
    right[:, :, :, :half_size] = 0
    assert iou(left, right, class_dim=1, mode="per-class").item() == 0

    # check 1: both empty, both full, complete overlap
    assert iou(empty, empty, class_dim=1, mode="per-class").item() == 1
    assert iou(full, full, class_dim=1, mode="per-class").item() == 1
    assert iou(left, left, class_dim=1, mode="per-class").item() == 1

    # check 0.5: half overlap
    top_left = torch.zeros(shape)
    top_left[:, :, :half_size, :half_size] = 1
    assert torch.isclose(iou(top_left, left, class_dim=1, mode="per-class"), torch.Tensor([[0.5]]))
    assert torch.isclose(iou(top_left, left, class_dim=1, mode="micro"), torch.Tensor([[0.5]]))
    assert torch.isclose(iou(top_left, left, class_dim=1, mode="macro"), torch.Tensor([[0.5]]))

    # check multiclass: 0, 0, 1, 1, 1, 0.5
    a = torch.cat([empty, left, empty, full, left, top_left], dim=1)
    b = torch.cat([full, right, empty, full, left, left], dim=1)
    ans = torch.Tensor([0, 0, 1, 1, 1, 0.5])
    ans_micro = torch.tensor(0.4375)
    assert torch.allclose(iou(a, b, class_dim=1, mode="per-class"), ans)
    assert torch.allclose(iou(a, b, class_dim=1, mode="micro"), ans_micro)

    aaa = torch.cat([a, a, a], dim=0)
    bbb = torch.cat([b, b, b], dim=0)
    assert torch.allclose(iou(aaa, bbb, class_dim=1, mode="per-class"), ans)
    assert torch.allclose(iou(aaa, bbb, class_dim=1, mode="micro"), ans_micro)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag988')" href="javascript:;">
catalyst-21.09/tests/catalyst/metrics/functional/test_dice.py: 7-56
</a>
<div class="mid" id="frag988" style="display:none"><pre>
def test_dice():
    """
    Tests for catalyst.metrics.dice metric.
    """
    size = 4
    half_size = size // 2
    shape = (1, 1, size, size)

    # check 0: one empty
    empty = torch.zeros(shape)
    full = torch.ones(shape)
    assert dice(empty, full, class_dim=1, mode="per-class").item() == 0

    # check 0: no overlap
    left = torch.ones(shape)
    left[:, :, :, half_size:] = 0
    right = torch.ones(shape)
    right[:, :, :, :half_size] = 0
    assert dice(left, right, class_dim=1, mode="per-class").item() == 0

    # check 1: both empty, both full, complete overlap
    assert dice(empty, empty, class_dim=1, mode="per-class").item() == 1
    assert dice(full, full, class_dim=1, mode="per-class").item() == 1
    assert dice(left, left, class_dim=1, mode="per-class").item() == 1

    # check 0.5: half overlap
    top_left = torch.zeros(shape)
    top_left[:, :, :half_size, :half_size] = 1
    assert torch.isclose(
        dice(top_left, left, class_dim=1, mode="per-class"), torch.Tensor([[0.6666666]])
    )
    assert torch.isclose(
        dice(top_left, left, class_dim=1, mode="micro"), torch.Tensor([[0.6666666]])
    )
    assert torch.isclose(
        dice(top_left, left, class_dim=1, mode="macro"), torch.Tensor([[0.6666666]])
    )

    # check multiclass: 0, 0, 1, 1, 1, 0.66667
    a = torch.cat([empty, left, empty, full, left, top_left], dim=1)
    b = torch.cat([full, right, empty, full, left, left], dim=1)
    ans = torch.Tensor([0, 0, 1, 1, 1, 0.6666666])
    ans_micro = torch.tensor(0.6087)
    assert torch.allclose(dice(a, b, class_dim=1, mode="per-class"), ans)
    assert torch.allclose(dice(a, b, class_dim=1, mode="micro"), ans_micro)

    aaa = torch.cat([a, a, a], dim=0)
    bbb = torch.cat([b, b, b], dim=0)
    assert torch.allclose(dice(aaa, bbb, class_dim=1, mode="per-class"), ans)
    assert torch.allclose(dice(aaa, bbb, class_dim=1, mode="micro"), ans_micro)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 39:</b> &nbsp; 3 fragments, nominal size 13 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag979')" href="javascript:;">
catalyst-21.09/tests/catalyst/metrics/functional/test_classification.py: 99-125
</a>
<div class="mid" id="frag979" style="display:none"><pre>
def test_micro(
    tp: np.array,
    fp: np.array,
    fn: np.array,
    support: np.array,
    zero_division: int,
    true_answer: Tuple[float],
):
    """
    Test micro metrics averaging

    Args:
        tp: true positive statistic
        fp: false positive statistic
        fn: false negative statistic
        support: support statistic
        zero_division: 0 or 1
        true_answer: true metric value
    """
    _, micro, _, _ = get_aggregated_metrics(
        tp=tp, fp=fp, fn=fn, support=support, zero_division=zero_division
    )
    assert micro[-1] is None
    for pred, real in zip(micro[:-1], true_answer):
        assert abs(pred - real) &lt; EPS


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag981')" href="javascript:;">
catalyst-21.09/tests/catalyst/metrics/functional/test_classification.py: 211-235
</a>
<div class="mid" id="frag981" style="display:none"><pre>
def test_weighted(
    tp: np.array,
    fp: np.array,
    fn: np.array,
    support: np.array,
    zero_division: int,
    true_answer: Tuple[float],
):
    """
    Test weighted metrics averaging

    Args:
        tp: true positive statistic
        fp: false positive statistic
        fn: false negative statistic
        support: support statistic
        zero_division: 0 or 1
        true_answer: true metric value
    """
    _, _, _, weighted = get_aggregated_metrics(
        tp=tp, fp=fp, fn=fn, support=support, zero_division=zero_division
    )
    assert weighted[-1] is None
    for pred, real in zip(weighted[:-1], true_answer):
        assert abs(pred - real) &lt; EPS
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag980')" href="javascript:;">
catalyst-21.09/tests/catalyst/metrics/functional/test_classification.py: 155-181
</a>
<div class="mid" id="frag980" style="display:none"><pre>
def test_macro_average(
    tp: np.array,
    fp: np.array,
    fn: np.array,
    support: np.array,
    zero_division: int,
    true_answer: Tuple[float],
):
    """
    Test macro metrics averaging

    Args:
        tp: true positive statistic
        fp: false positive statistic
        fn: false negative statistic
        support: support statistic
        zero_division: 0 or 1
        true_answer: true metric value
    """
    _, _, macro, _ = get_aggregated_metrics(
        tp=tp, fp=fp, fn=fn, support=support, zero_division=zero_division
    )
    assert macro[-1] is None
    for pred, real in zip(macro[:-1], true_answer):
        assert abs(pred - real) &lt; EPS


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 40:</b> &nbsp; 3 fragments, nominal size 10 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1001')" href="javascript:;">
catalyst-21.09/tests/catalyst/metrics/test_additive.py: 24-43
</a>
<div class="mid" id="frag1001" style="display:none"><pre>
def test_additive_mean(
    values_list: Iterable[float],
    num_samples_list: Iterable[int],
    true_values_list: Iterable[float],
) -&gt; None:
    """
    Test additive metric mean computation

    Args:
        values_list: list of values to update metric
        num_samples_list: list of num_samples
        true_values_list: list of metric intermediate value
    """
    metric = AdditiveMetric()
    for value, num_samples, true_value in zip(values_list, num_samples_list, true_values_list):
        metric.update(value=value, num_samples=num_samples)
        mean, _ = metric.compute()
        assert np.isclose(mean, true_value)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1002')" href="javascript:;">
catalyst-21.09/tests/catalyst/metrics/test_additive.py: 56-75
</a>
<div class="mid" id="frag1002" style="display:none"><pre>
def test_additive_std(
    values_list: Iterable[float],
    num_samples_list: Iterable[int],
    true_values_list: Iterable[float],
):
    """
    Test additive metric std computation

    Args:
        values_list: list of values to update metric
        num_samples_list: list of num_samples
        true_values_list: list of metric intermediate value
    """
    metric = AdditiveMetric()
    for value, num_samples, true_value in zip(values_list, num_samples_list, true_values_list):
        metric.update(value=value, num_samples=num_samples)
        _, std = metric.compute()
        assert np.isclose(std, true_value)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1003')" href="javascript:;">
catalyst-21.09/tests/catalyst/metrics/test_additive.py: 93-112
</a>
<div class="mid" id="frag1003" style="display:none"><pre>
def test_additive_mode(
    values_list: Union[Iterable[float], Iterable[torch.Tensor]],
    num_samples_list: Iterable[int],
    true_values_list: Iterable[float],
    mode: Iterable[str],
):
    """
    Test additive metric std computation

    Args:
        values_list: list of values to update metric
        num_samples_list: list of num_samples
        true_values_list: list of metric intermediate value
        mode: `AdditiveMetric` mode
    """
    metric = AdditiveMetric(mode=mode)
    for value, num_samples, true_value in zip(values_list, num_samples_list, true_values_list):
        metric.update(value=value, num_samples=num_samples)
        mean, _ = metric.compute()
        assert np.isclose(mean, true_value)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 41:</b> &nbsp; 3 fragments, nominal size 47 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1087')" href="javascript:;">
catalyst-21.09/tests/pipelines/test_classification.py: 14-72
</a>
<div class="mid" id="frag1087" style="display:none"><pre>
def train_experiment(device, engine=None):
    with TemporaryDirectory() as logdir:
        # sample data
        num_samples, num_features, num_classes = int(1e4), int(1e1), 4
        X = torch.rand(num_samples, num_features)
        y = (torch.rand(num_samples) * num_classes).to(torch.int64)

        # pytorch loaders
        dataset = TensorDataset(X, y)
        loader = DataLoader(dataset, batch_size=32, num_workers=1)
        loaders = {"train": loader, "valid": loader}

        # model, criterion, optimizer, scheduler
        model = torch.nn.Linear(num_features, num_classes)
        criterion = torch.nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(model.parameters())
        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [2])

        # model training
        runner = dl.SupervisedRunner(
            input_key="features", output_key="logits", target_key="targets", loss_key="loss"
        )
        callbacks = [
            dl.AccuracyCallback(input_key="logits", target_key="targets", num_classes=num_classes),
            dl.PrecisionRecallF1SupportCallback(
                input_key="logits", target_key="targets", num_classes=4
            ),
        ]
        if SETTINGS.ml_required:
            callbacks.append(
                dl.ConfusionMatrixCallback(input_key="logits", target_key="targets", num_classes=4)
            )
        if SETTINGS.amp_required and (
            engine is None
            or not isinstance(
                engine,
                (dl.AMPEngine, dl.DataParallelAMPEngine, dl.DistributedDataParallelAMPEngine),
            )
        ):
            callbacks.append(dl.AUCCallback(input_key="logits", target_key="targets"))

        runner.train(
            engine=engine or dl.DeviceEngine(device),
            model=model,
            criterion=criterion,
            optimizer=optimizer,
            scheduler=scheduler,
            loaders=loaders,
            logdir=logdir,
            num_epochs=1,
            valid_loader="valid",
            valid_metric="accuracy03",
            minimize_valid_metric=False,
            verbose=False,
            callbacks=callbacks,
        )


# Torch
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1099')" href="javascript:;">
catalyst-21.09/tests/pipelines/test_multilabel_classification.py: 14-73
</a>
<div class="mid" id="frag1099" style="display:none"><pre>
def train_experiment(device, engine=None):
    with TemporaryDirectory() as logdir:
        # sample data
        num_samples, num_features, num_classes = int(1e4), int(1e1), 4
        X = torch.rand(num_samples, num_features)
        y = (torch.rand(num_samples, num_classes) &gt; 0.5).to(torch.float32)

        # pytorch loaders
        dataset = TensorDataset(X, y)
        loader = DataLoader(dataset, batch_size=32, num_workers=1)
        loaders = {"train": loader, "valid": loader}

        # model, criterion, optimizer, scheduler
        model = torch.nn.Linear(num_features, num_classes)
        criterion = torch.nn.BCEWithLogitsLoss()
        optimizer = torch.optim.Adam(model.parameters())
        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [2])

        # model training
        runner = dl.SupervisedRunner(
            input_key="features", output_key="logits", target_key="targets", loss_key="loss"
        )
        callbacks = [
            dl.BatchTransformCallback(
                transform="F.sigmoid",
                scope="on_batch_end",
                input_key="logits",
                output_key="scores",
            ),
            dl.MultilabelAccuracyCallback(input_key="scores", target_key="targets", threshold=0.5),
            dl.MultilabelPrecisionRecallF1SupportCallback(
                input_key="scores", target_key="targets", num_classes=num_classes
            ),
        ]
        if SETTINGS.amp_required and (
            engine is None
            or not isinstance(
                engine,
                (dl.AMPEngine, dl.DataParallelAMPEngine, dl.DistributedDataParallelAMPEngine),
            )
        ):
            callbacks.append(dl.AUCCallback(input_key="scores", target_key="targets"))
        runner.train(
            engine=engine or dl.DeviceEngine(device),
            model=model,
            criterion=criterion,
            optimizer=optimizer,
            scheduler=scheduler,
            loaders=loaders,
            logdir=logdir,
            num_epochs=1,
            valid_loader="valid",
            valid_metric="accuracy",
            minimize_valid_metric=False,
            verbose=False,
            callbacks=callbacks,
        )


# Torch
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1111')" href="javascript:;">
catalyst-21.09/tests/pipelines/test_recsys.py: 14-77
</a>
<div class="mid" id="frag1111" style="display:none"><pre>
def train_experiment(device, engine=None):
    with TemporaryDirectory() as logdir:
        # sample data
        num_users, num_features, num_items = int(1e4), int(1e1), 10
        X = torch.rand(num_users, num_features)
        y = (torch.rand(num_users, num_items) &gt; 0.5).to(torch.float32)

        # pytorch loaders
        dataset = TensorDataset(X, y)
        loader = DataLoader(dataset, batch_size=32, num_workers=1)
        loaders = {"train": loader, "valid": loader}

        # model, criterion, optimizer, scheduler
        model = torch.nn.Linear(num_features, num_items)
        criterion = torch.nn.BCEWithLogitsLoss()
        optimizer = torch.optim.Adam(model.parameters())
        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [2])

        callbacks = [
            dl.BatchTransformCallback(
                input_key="logits",
                output_key="scores",
                transform=torch.sigmoid,
                scope="on_batch_end",
            ),
            dl.CriterionCallback(input_key="logits", target_key="targets", metric_key="loss"),
            dl.AUCCallback(input_key="scores", target_key="targets"),
            dl.HitrateCallback(input_key="scores", target_key="targets", topk_args=(1, 3, 5)),
            dl.MRRCallback(input_key="scores", target_key="targets", topk_args=(1, 3, 5)),
            dl.MAPCallback(input_key="scores", target_key="targets", topk_args=(1, 3, 5)),
            dl.NDCGCallback(input_key="scores", target_key="targets", topk_args=(1, 3, 5)),
            dl.OptimizerCallback(metric_key="loss"),
            dl.SchedulerCallback(),
            dl.CheckpointCallback(
                logdir=logdir, loader_key="valid", metric_key="map01", minimize=False
            ),
        ]
        if SETTINGS.amp_required and (
            engine is None
            or not isinstance(
                engine,
                (dl.AMPEngine, dl.DataParallelAMPEngine, dl.DistributedDataParallelAMPEngine),
            )
        ):
            callbacks.append(dl.AUCCallback(input_key="logits", target_key="targets"))

        # model training
        runner = dl.SupervisedRunner(
            input_key="features", output_key="logits", target_key="targets", loss_key="loss"
        )
        runner.train(
            engine=engine or dl.DeviceEngine(device),
            model=model,
            criterion=criterion,
            optimizer=optimizer,
            scheduler=scheduler,
            loaders=loaders,
            num_epochs=1,
            verbose=False,
            callbacks=callbacks,
        )


# Torch
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 42:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1215')" href="javascript:;">
catalyst-21.09/examples/catalyst_rl/db.py: 124-136
</a>
<div class="mid" id="frag1215" style="display:none"><pre>
        def add_message(self, message: IRLDatabaseMessage):
            if message == IRLDatabaseMessage.ENABLE_SAMPLING:
                self._set_flag("sampling_flag", 1)
            elif message == IRLDatabaseMessage.DISABLE_SAMPLING:
                self._set_flag("sampling_flag", 0)
            elif message == IRLDatabaseMessage.DISABLE_TRAINING:
                self._set_flag("sampling_flag", 0)
                self._set_flag("training_flag", 0)
            elif message == IRLDatabaseMessage.ENABLE_TRAINING:
                self._set_flag("training_flag", 1)
            else:
                raise NotImplementedError("unknown message", message)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1229')" href="javascript:;">
catalyst-21.09/examples/catalyst_rl/db.py: 248-260
</a>
<div class="mid" id="frag1229" style="display:none"><pre>
        def add_message(self, message: IRLDatabaseMessage):
            if message == IRLDatabaseMessage.ENABLE_SAMPLING:
                self._set_flag("sampling_flag", 1)
            elif message == IRLDatabaseMessage.DISABLE_SAMPLING:
                self._set_flag("sampling_flag", 0)
            elif message == IRLDatabaseMessage.DISABLE_TRAINING:
                self._set_flag("sampling_flag", 0)
                self._set_flag("training_flag", 0)
            elif message == IRLDatabaseMessage.ENABLE_TRAINING:
                self._set_flag("training_flag", 1)
            else:
                raise NotImplementedError("unknown message", message)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<script language="JavaScript">
function ShowHide(divId) { 
    if(document.getElementById(divId).style.display == 'none') {
        document.getElementById(divId).style.display='block';
    } else { 
        document.getElementById(divId).style.display = 'none';
    } 
}
</script>
</body>
</html>

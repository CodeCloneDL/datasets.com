<html>
<head>
    <title>NiCad6 Clone Report</title>
    <style type="text/css">
        body {font-family:sans-serif;}
        table {background-color:white; border:0px; padding:0px; border-spacing:4px; width:auto; margin-left:30px; margin-right:auto;}
        td {background-color:rgba(192,212,238,0.8); border:0px; padding:8px; width:auto; vertical-align:top; border-radius:8px}
        pre {background-color:white; padding:4px;}
        a {color:darkblue;}
    </style>
</head>
<body>
<h2>NiCad6 Clone Report</h2>
<table>
<tr style="font-size:14pt">
<td><b>System:</b> &nbsp; sonnet-1.33</td>
<td><b>Clone pairs:</b> &nbsp; 85</td>
<td><b>Clone classes:</b> &nbsp; 33</td>
</tr>
<tr style="font-size:12pt">
<td style="background-color:white">Clone type: &nbsp; 3-2</td>
<td style="background-color:white">Granularity: &nbsp; functions-blind</td>
<td style="background-color:white">Max diff threshold: &nbsp; 30%</td>
<td style="background-color:white">Clone size: &nbsp; 10 - 2500 lines</td>
<td style="background-color:white">Total functions-blind: &nbsp; 874</td>
</tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 1:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag113')" href="javascript:;">
sonnet-1.33/sonnet/examples/rmc_learn_to_execute.py: 58-68
</a>
<div class="mid" id="frag113" style="display:none"><pre>
  def __init__(
      self,
      core,
      target_size,
      final_mlp,
      name="sequence_model"):
    super(SequenceModel, self).__init__(name=name)
    self._core = core
    self._target_size = target_size
    self._final_mlp = final_mlp

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag118')" href="javascript:;">
sonnet-1.33/sonnet/examples/rmc_nth_farthest.py: 59-69
</a>
<div class="mid" id="frag118" style="display:none"><pre>
  def __init__(
      self,
      core,
      target_size,
      final_mlp,
      name="sequence_model"):
    super(SequenceModel, self).__init__(name=name)
    self._core = core
    self._target_size = target_size
    self._final_mlp = final_mlp

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 2:</b> &nbsp; 2 fragments, nominal size 23 lines, similarity 86%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag149')" href="javascript:;">
sonnet-1.33/sonnet/python/ops/initializers_test.py: 129-155
</a>
<div class="mid" id="frag149" style="display:none"><pre>
  def testMoreMultipleRestore(self):
    restore_initializers = {
        'w': initializers.restore_initializer(_checkpoint(), 'w'),
        'b': initializers.restore_initializer(_checkpoint(), 'b')
    }

    with tf.variable_scope('agent'):
      c = convnet.ConvNet2D(
          output_channels=(16, 32),
          kernel_shapes=(8, 4),
          strides=(4, 2),
          paddings=[conv.VALID],
          activation=tf.nn.relu,
          activate_final=True,
          initializers=restore_initializers)

    inputs = tf.constant(1 / 255.0, shape=[1, 86, 86, 3])
    outputs = c(inputs)
    init = tf.global_variables_initializer()
    tf.get_default_graph().finalize()
    with self.test_session() as session:
      session.run(init)
      o = session.run(outputs)

    self.assertAllClose(
        np.linalg.norm(o), _TWO_CONV_LAYERS_RELU, atol=_TOLERANCE)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag150')" href="javascript:;">
sonnet-1.33/sonnet/python/ops/initializers_test.py: 156-183
</a>
<div class="mid" id="frag150" style="display:none"><pre>
  def testFromDifferentScope(self):
    sub = functools.partial(re.sub, r'^[^/]+/', 'agent/')
    restore_initializers = {
        'w': initializers.restore_initializer(_checkpoint(), 'w', sub),
        'b': initializers.restore_initializer(_checkpoint(), 'b', sub)
    }

    with tf.variable_scope('some_random_scope'):
      c = convnet.ConvNet2D(
          output_channels=(16, 32),
          kernel_shapes=(8, 4),
          strides=(4, 2),
          paddings=[conv.VALID],
          activation=tf.nn.relu,
          activate_final=True,
          initializers=restore_initializers)

    inputs = tf.constant(1 / 255.0, shape=[1, 86, 86, 3])
    outputs = c(inputs)
    init = tf.global_variables_initializer()
    tf.get_default_graph().finalize()
    with self.test_session() as session:
      session.run(init)
      o = session.run(outputs)

    self.assertAllClose(
        np.linalg.norm(o), _TWO_CONV_LAYERS_RELU, atol=_TOLERANCE)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 3:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag175')" href="javascript:;">
sonnet-1.33/sonnet/python/custom_getters/context_test.py: 36-50
</a>
<div class="mid" id="frag175" style="display:none"><pre>
  def testContextCallsCustomGetterOnlyWhenInScope(self):
    custom_getter = snt.custom_getters.Context(_suffix_getter, verbose=True)
    with tf.variable_scope('', custom_getter=custom_getter):
      lin = snt.Linear(10, name='linear')

    inputs = tf.placeholder(tf.float32, [10, 10])

    _ = lin(inputs)
    self.assertEqual('linear/w:0', lin.w.name)
    with custom_getter:
      _ = lin(inputs)
      self.assertEqual('linear/w_custom:0', lin.w.name)
    _ = lin(inputs)
    self.assertEqual('linear/w:0', lin.w.name)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag176')" href="javascript:;">
sonnet-1.33/sonnet/python/custom_getters/context_test.py: 51-67
</a>
<div class="mid" id="frag176" style="display:none"><pre>
  def testNestedContextCallsCustomGetterOnlyWhenInScope(self):
    custom_getter = snt.custom_getters.Context(_suffix_getter)
    with tf.variable_scope('', custom_getter=custom_getter):
      lin = snt.Linear(10, name='linear')

    inputs = tf.placeholder(tf.float32, [10, 10])
    with custom_getter:
      _ = lin(inputs)
      self.assertEqual('linear/w_custom:0', lin.w.name)
      with custom_getter:
        _ = lin(inputs)
        self.assertEqual('linear/w_custom:0', lin.w.name)
      _ = lin(inputs)
      self.assertEqual('linear/w_custom:0', lin.w.name)
    _ = lin(inputs)
    self.assertEqual('linear/w:0', lin.w.name)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 4:</b> &nbsp; 3 fragments, nominal size 18 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag183')" href="javascript:;">
sonnet-1.33/sonnet/python/custom_getters/restore_initializer_test.py: 54-76
</a>
<div class="mid" id="frag183" style="display:none"><pre>
  def testSimpleUsage(self):
    checkpoint_path, expected_values = self._save_test_checkpoint()
    checkpoint_path = tf.train.latest_checkpoint(checkpoint_path)

    g = tf.Graph()
    with g.as_default():
      custom_getter = snt.custom_getters.restore_initializer(
          filename=checkpoint_path)

      with tf.variable_scope("", custom_getter=custom_getter):
        inputs = tf.placeholder(tf.float32, [10, 10])
        lin1 = snt.Linear(10, name="linear1")
        lin1(inputs)

      init = tf.global_variables_initializer()

    with self.test_session(graph=g) as sess:
      sess.run(init)
      w_value, b_value = sess.run([lin1.w, lin1.b])

    self.assertAllClose(expected_values["w"], w_value)
    self.assertAllClose(expected_values["b"], b_value)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag184')" href="javascript:;">
sonnet-1.33/sonnet/python/custom_getters/restore_initializer_test.py: 77-103
</a>
<div class="mid" id="frag184" style="display:none"><pre>
  def testNameFn(self):
    checkpoint_path, expected_values = self._save_test_checkpoint()
    checkpoint_path = tf.train.latest_checkpoint(checkpoint_path)

    def name_fn(name):
      return name.replace("linear2", "linear1")

    g = tf.Graph()
    with g.as_default():
      custom_getter = snt.custom_getters.restore_initializer(
          filename=checkpoint_path,
          name_fn=name_fn)

      with tf.variable_scope("", custom_getter=custom_getter):
        inputs = tf.placeholder(tf.float32, [10, 10])
        lin1 = snt.Linear(10, name="linear2")
        lin1(inputs)

      init = tf.global_variables_initializer()

    with self.test_session(graph=g) as sess:
      sess.run(init)
      w_value, b_value = sess.run([lin1.w, lin1.b])

    self.assertAllClose(expected_values["w"], w_value)
    self.assertAllClose(expected_values["b"], b_value)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag186')" href="javascript:;">
sonnet-1.33/sonnet/python/custom_getters/restore_initializer_test.py: 104-130
</a>
<div class="mid" id="frag186" style="display:none"><pre>
  def testCollections(self):
    checkpoint_path, expected_values = self._save_test_checkpoint()
    checkpoint_path = tf.train.latest_checkpoint(checkpoint_path)

    g = tf.Graph()
    with g.as_default():
      custom_getter = snt.custom_getters.restore_initializer(
          filename=checkpoint_path,
          collection="blah")

      with tf.variable_scope("", custom_getter=custom_getter):
        inputs = tf.placeholder(tf.float32, [10, 10])
        lin1 = snt.Linear(10, name="linear1")
        lin1(inputs)

        tf.add_to_collection("blah", lin1.w)

      init = tf.global_variables_initializer()

    with self.test_session(graph=g) as sess:
      sess.run(init)
      w_value = sess.run(lin1.w)

    self.assertFalse(np.allclose(expected_values["w"], w_value))
    # b is initialized to zero always.


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 5:</b> &nbsp; 2 fragments, nominal size 37 lines, similarity 86%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag213')" href="javascript:;">
sonnet-1.33/sonnet/python/custom_getters/bayes_by_backprop_test.py: 332-385
</a>
<div class="mid" id="frag213" style="display:none"><pre>
  def testRecurrentNetSamplesWeightsOnce(self):
    """Test that sampling of the weights is done only once for a sequence.

    Test strategy: Provide an input sequence x whose value is the same at each
    time step. If the outputs from f_theta() are the same at each time step,
    this is evidence (but not proof) that theta is the same at each time step.
    """
    seq_length = 10
    batch_size = 1
    input_dim = 5
    output_dim = 5

    bbb_getter = bbb.bayes_by_backprop_getter(
        posterior_builder=bbb.diagonal_gaussian_posterior_builder,
        prior_builder=bbb.fixed_gaussian_prior_builder,
        kl_builder=bbb.stochastic_kl_builder,
        sampling_mode_tensor=tf.constant(bbb.EstimatorModes.sample))

    class NoStateLSTM(snt.LSTM):
      """An LSTM which ignores hidden state."""

      def _build(self, inputs, state):
        outputs, _ = super(NoStateLSTM, self)._build(inputs, state)
        return outputs, state

    with tf.variable_scope("model", custom_getter=bbb_getter):
      core = NoStateLSTM(output_dim)

    input_seq = tf.ones(shape=(seq_length, batch_size, input_dim))
    output_seq, _ = tf.nn.dynamic_rnn(
        core,
        inputs=input_seq,
        initial_state=core.initial_state(batch_size=batch_size),
        time_major=True)

    init_op = tf.global_variables_initializer()
    with self.test_session() as sess:
      sess.run(init_op)
      output_res_one = sess.run(output_seq)
      output_res_two = sess.run(output_seq)

    # Ensure that the sequence is the same at every time step, a necessary
    # but not sufficient condition for the weights to be the same.
    output_zero = output_res_one[0]
    for time_step_output in output_res_one[1:]:
      self.assertAllClose(output_zero, time_step_output)

    # Ensure that the noise is different in the second run by checking that
    # the output sequence is different now.
    for first_run_elem, second_run_elem in zip(output_res_one, output_res_two):
      distance = np.linalg.norm(
          first_run_elem.flatten() - second_run_elem.flatten())
      self.assertGreater(distance, 0.001)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag217')" href="javascript:;">
sonnet-1.33/sonnet/python/custom_getters/bayes_by_backprop_test.py: 431-489
</a>
<div class="mid" id="frag217" style="display:none"><pre>
  def testWeightsResampledWithKeepControlDeps(self):
    """Test that weights are resampled with `keep_control_dependencies=True`.

    Test strategy: We test the inverse of `testRecurrentNetSamplesWeightsOnce`.
    Provide an input sequence x whose value is the same at each time step. If
    the outputs from f_theta() are the different at each time step, then theta
    is different at each time step. In principle, it is possible that different
    thetas give the same outputs, but this is very unlikely.
    """
    seq_length = 10
    batch_size = 1
    input_dim = 5
    output_dim = 5

    bbb_getter = bbb.bayes_by_backprop_getter(
        posterior_builder=bbb.diagonal_gaussian_posterior_builder,
        prior_builder=bbb.fixed_gaussian_prior_builder,
        kl_builder=bbb.stochastic_kl_builder,
        sampling_mode_tensor=tf.constant(bbb.EstimatorModes.sample),
        keep_control_dependencies=True)

    class NoStateLSTM(snt.LSTM):
      """An LSTM which ignores hidden state."""

      def _build(self, inputs, state):
        outputs, _ = super(NoStateLSTM, self)._build(inputs, state)
        return outputs, state

    with tf.variable_scope("model", custom_getter=bbb_getter):
      core = NoStateLSTM(output_dim)

    input_seq = tf.ones(shape=(seq_length, batch_size, input_dim))
    output_seq, _ = tf.nn.dynamic_rnn(
        core,
        inputs=input_seq,
        initial_state=core.initial_state(batch_size=batch_size),
        time_major=True)

    init_op = tf.global_variables_initializer()
    with self.test_session() as sess:
      sess.run(init_op)
      output_res_one = sess.run(output_seq)
      output_res_two = sess.run(output_seq)

    # Ensure that the sequence is different at every time step
    output_zero = output_res_one[0]
    for time_step_output in output_res_one[1:]:
      distance = np.linalg.norm(
          time_step_output.flatten() - output_zero.flatten())
      self.assertGreater(distance, 0.001)

    # Ensure that the noise is different in the second run by checking that
    # the output sequence is different now.
    for first_run_elem, second_run_elem in zip(output_res_one, output_res_two):
      distance = np.linalg.norm(
          first_run_elem.flatten() - second_run_elem.flatten())
      self.assertGreater(distance, 0.001)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 6:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag222')" href="javascript:;">
sonnet-1.33/sonnet/python/custom_getters/override_args_test.py: 61-77
</a>
<div class="mid" id="frag222" style="display:none"><pre>
  def testNestedWithin(self, custom_getter_fn):
    # Create a module with an 'override args' custom getter, within the scope
    # of another custom getter.
    local_custom_getter = custom_getter_fn(
        collections=[tf.GraphKeys.LOCAL_VARIABLES])
    with tf.variable_scope("", custom_getter=_suffix_custom_getter):
      local_linear = snt.Linear(10, custom_getter=local_custom_getter)

    # Connect the module to the graph, creating its variables.
    inputs = tf.placeholder(dtype=tf.float32, shape=(7, 11))
    local_linear(inputs)

    # Both custom getters should be effective.
    self.assertIn(local_linear.w, tf.local_variables())
    self.assertNotIn(local_linear.w, tf.global_variables())
    self.assertEqual("linear/w_test", local_linear.w.op.name)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag223')" href="javascript:;">
sonnet-1.33/sonnet/python/custom_getters/override_args_test.py: 82-98
</a>
<div class="mid" id="frag223" style="display:none"><pre>
  def testWithNested(self, custom_getter_fn):
    # Create a module with a custom getter, within the scope of an
    # 'override args' custom getter.
    local_custom_getter = custom_getter_fn(
        collections=[tf.GraphKeys.LOCAL_VARIABLES])
    with tf.variable_scope("", custom_getter=local_custom_getter):
      local_linear = snt.Linear(10, custom_getter=_suffix_custom_getter)

    # Connect the module to the graph, creating its variables.
    inputs = tf.placeholder(dtype=tf.float32, shape=(7, 11))
    local_linear(inputs)

    # Both custom getters should be effective.
    self.assertIn(local_linear.w, tf.local_variables())
    self.assertNotIn(local_linear.w, tf.global_variables())
    self.assertEqual("linear/w_test", local_linear.w.op.name)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 7:</b> &nbsp; 3 fragments, nominal size 19 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag293')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/relational_memory_test.py: 138-160
</a>
<div class="mid" id="frag293" style="display:none"><pre>
  def testMemoryUpdating(self):
    """Checks if memory is updating correctly."""
    mem_slots = 2
    head_size = 32
    num_heads = 4
    batch_size = 5
    input_shape = (batch_size, 3, 3)
    mem = relational_memory.RelationalMemory(mem_slots, head_size, num_heads,
                                             gate_style=None)
    inputs = tf.placeholder(tf.float32, input_shape)

    memory_0 = mem.initial_state(batch_size)
    _, memory_1 = mem(inputs, memory_0)

    with self.test_session() as session:
      tf.global_variables_initializer().run()
      results = session.run(
          {"memory_1": memory_1, "memory_0": memory_0},
          feed_dict={inputs: np.zeros(input_shape)})

    self.assertTrue(np.any(np.not_equal(results["memory_0"],
                                        results["memory_1"])))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag294')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/relational_memory_test.py: 164-186
</a>
<div class="mid" id="frag294" style="display:none"><pre>
  def testInputErasureWorking(self, gate_style):
    """Checks if gating is working by ignoring the input."""
    mem_slots = 2
    head_size = 32
    num_heads = 2
    batch_size = 5
    input_shape = (batch_size, 3, 3)
    mem = relational_memory.RelationalMemory(mem_slots, head_size, num_heads,
                                             forget_bias=float("+inf"),
                                             input_bias=float("-inf"),
                                             gate_style=gate_style)
    inputs = tf.placeholder(tf.float32, input_shape)

    memory_0 = mem.initial_state(batch_size)
    _, memory_1 = mem(inputs, memory_0)

    with self.test_session() as session:
      tf.global_variables_initializer().run()
      results = session.run(
          {"memory_1": memory_1, "memory_0": memory_0},
          feed_dict={inputs: np.ones(input_shape)})
    self.assertAllEqual(results["memory_0"], results["memory_1"])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag295')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/relational_memory_test.py: 190-216
</a>
<div class="mid" id="frag295" style="display:none"><pre>
  def testDifferingKeyHeadSizes(self, gate_style):
    """Checks if arbitrary key sizes are still supported."""
    mem_slots = 2
    head_size = 32
    num_heads = 2
    key_size = 128
    batch_size = 5

    input_shape = (batch_size, 3, 3)
    mem = relational_memory.RelationalMemory(mem_slots, head_size, num_heads,
                                             gate_style=gate_style,
                                             key_size=key_size)
    self.assertNotEqual(key_size, mem._head_size)
    inputs = tf.placeholder(tf.float32, input_shape)

    memory_0 = mem.initial_state(batch_size)
    _, memory_1 = mem(inputs, memory_0)

    with self.test_session() as session:
      tf.global_variables_initializer().run()
      results = session.run(
          {"memory_1": memory_1, "memory_0": memory_0},
          feed_dict={inputs: np.ones(input_shape)})

    self.assertTrue(np.any(np.not_equal(results["memory_0"],
                                        results["memory_1"])))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 8:</b> &nbsp; 4 fragments, nominal size 15 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag351')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/base_info_test.py: 167-182
</a>
<div class="mid" id="frag351" style="display:none"><pre>
  def testModuleInfo_tensor(self):
    # pylint: disable=not-callable
    tf.reset_default_graph()
    dumb = DumbModule(name="dumb_a")
    ph_0 = tf.placeholder(dtype=tf.float32, shape=(1, 10,))
    dumb(ph_0)
    def check():
      sonnet_collection = tf.get_default_graph().get_collection(
          base_info.SONNET_COLLECTION_NAME)
      connected_subgraph = sonnet_collection[0].connected_subgraphs[0]
      self.assertIsInstance(connected_subgraph.inputs["inputs"], tf.Tensor)
      self.assertIsInstance(connected_subgraph.outputs, tf.Tensor)
    check()
    _copy_default_graph()
    check()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag355')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/base_info_test.py: 203-219
</a>
<div class="mid" id="frag355" style="display:none"><pre>
  def testModuleInfo_tuple(self):
    # pylint: disable=not-callable
    tf.reset_default_graph()
    dumb = DumbModule(name="dumb_a")
    ph_0 = tf.placeholder(dtype=tf.float32, shape=(1, 10,))
    ph_1 = tf.placeholder(dtype=tf.float32, shape=(1, 10,))
    dumb((ph_0, ph_1))
    def check():
      sonnet_collection = tf.get_default_graph().get_collection(
          base_info.SONNET_COLLECTION_NAME)
      connected_subgraph = sonnet_collection[0].connected_subgraphs[0]
      self.assertIsInstance(connected_subgraph.inputs["inputs"], tuple)
      self.assertIsInstance(connected_subgraph.outputs, tuple)
    check()
    _copy_default_graph()
    check()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag359')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/base_info_test.py: 238-254
</a>
<div class="mid" id="frag359" style="display:none"><pre>
  def testModuleInfo_dict(self):
    # pylint: disable=not-callable
    tf.reset_default_graph()
    dumb = DumbModule(name="dumb_a")
    ph_0 = tf.placeholder(dtype=tf.float32, shape=(1, 10,))
    ph_1 = tf.placeholder(dtype=tf.float32, shape=(1, 10,))
    dumb({"ph_0": ph_0, "ph_1": ph_1})
    def check():
      sonnet_collection = tf.get_default_graph().get_collection(
          base_info.SONNET_COLLECTION_NAME)
      connected_subgraph = sonnet_collection[0].connected_subgraphs[0]
      self.assertIsInstance(connected_subgraph.inputs["inputs"], dict)
      self.assertIsInstance(connected_subgraph.outputs, dict)
    check()
    _copy_default_graph()
    check()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag357')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/base_info_test.py: 220-237
</a>
<div class="mid" id="frag357" style="display:none"><pre>
  def testModuleInfo_namedtuple(self):
    # pylint: disable=not-callable
    tf.reset_default_graph()
    dumb = DumbModule(name="dumb_a")
    ph_0 = tf.placeholder(dtype=tf.float32, shape=(1, 10,))
    ph_1 = tf.placeholder(dtype=tf.float32, shape=(1, 10,))
    dumb(DumbNamedTuple(ph_0, ph_1))
    def check():
      sonnet_collection = tf.get_default_graph().get_collection(
          base_info.SONNET_COLLECTION_NAME)
      connected_subgraph = sonnet_collection[0].connected_subgraphs[0]
      self.assertTrue(
          base_info._is_namedtuple(connected_subgraph.inputs["inputs"]))
      self.assertTrue(base_info._is_namedtuple(connected_subgraph.outputs))
    check()
    _copy_default_graph()
    check()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 9:</b> &nbsp; 2 fragments, nominal size 29 lines, similarity 96%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag365')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/batch_norm_test.py: 33-70
</a>
<div class="mid" id="frag365" style="display:none"><pre>
  def testConstruct(self):
    inputs = tf.placeholder(tf.float32, shape=[None, 64, 64, 3])

    batch_norm1 = snt.BatchNorm(offset=False, scale=False)
    batch_norm1(inputs, is_training=True)

    err = "Batch normalization doesn't have an offset, so no beta"
    with self.assertRaisesRegexp(snt.Error, err):
      _ = batch_norm1.beta

    err = "Batch normalization doesn't have a scale, so no gamma"
    with self.assertRaisesRegexp(snt.Error, err):
      _ = batch_norm1.gamma

    batch_norm2 = snt.BatchNorm(offset=True, scale=False)
    batch_norm2(inputs, is_training=True)
    _ = batch_norm2.beta

    batch_norm3 = snt.BatchNorm(offset=False, scale=True)
    batch_norm3(inputs, is_training=True)
    _ = batch_norm3.gamma

    batch_norm4 = snt.BatchNorm(offset=True, scale=True)
    batch_norm4(inputs, is_training=True)
    _ = batch_norm4.beta
    _ = batch_norm4.gamma

    batch_norm4(inputs, is_training=True, test_local_stats=True)
    batch_norm4(inputs,
                is_training=tf.constant(True),
                test_local_stats=tf.constant(True))

    is_training_ph = tf.placeholder(tf.bool)
    test_local_stats_ph = tf.placeholder(tf.bool)
    batch_norm4(inputs,
                is_training=is_training_ph,
                test_local_stats=test_local_stats_ph)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag405')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/batch_norm_v2_test.py: 33-70
</a>
<div class="mid" id="frag405" style="display:none"><pre>
  def testConstruct(self):
    inputs = tf.placeholder(tf.float32, shape=[None, 64, 64, 3])

    batch_norm1 = snt.BatchNormV2(offset=False, scale=False, fused=False)
    batch_norm1(inputs, is_training=True)

    err = "Batch normalization doesn't have an offset, so no beta"
    with self.assertRaisesRegexp(snt.Error, err):
      _ = batch_norm1.beta

    err = "Batch normalization doesn't have a scale, so no gamma"
    with self.assertRaisesRegexp(snt.Error, err):
      _ = batch_norm1.gamma

    batch_norm2 = snt.BatchNormV2(offset=True, scale=False)
    batch_norm2(inputs, is_training=True)
    _ = batch_norm2.beta

    batch_norm3 = snt.BatchNormV2(offset=False, scale=True)
    batch_norm3(inputs, is_training=True)
    _ = batch_norm3.gamma

    batch_norm4 = snt.BatchNormV2(offset=True, scale=True)
    batch_norm4(inputs, is_training=True)
    _ = batch_norm4.beta
    _ = batch_norm4.gamma

    batch_norm4(inputs, is_training=True, test_local_stats=True)
    batch_norm4(inputs,
                is_training=tf.constant(True),
                test_local_stats=tf.constant(True))

    is_training_ph = tf.placeholder(tf.bool)
    test_local_stats_ph = tf.placeholder(tf.bool)
    batch_norm4(inputs,
                is_training=is_training_ph,
                test_local_stats=test_local_stats_ph)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 10:</b> &nbsp; 2 fragments, nominal size 28 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag371')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/batch_norm_test.py: 175-236
</a>
<div class="mid" id="frag371" style="display:none"><pre>
  def testCheckStatsDouble(self, dtype):
    """The correct statistics are being computed for double connection.

    Connected in parallel, it's ill-defined what order the updates will happen
    in. A double update could happen, or two sequential updates. E.g. If
    decay_rate is 0.9, the start value is 1.0, and the target value is 0.0, the
    value could progress as

      1.00 -&gt; 0.90 -&gt; 0.81,

    if the second update uses the fresh second value. Or as

      1.00 -&gt; 0.90 -&gt; 0.80

    if the second update uses the stale first value.

    We fix this here by running them in sequential run calls to ensure that this
    test is deterministic.

    The two situations are minimally different, especially if decay_rate is
    close to one (e.g. the default of 0.999).

    Args:
      dtype: TensorFlow datatype of input test batch.
    """

    v, _, inputs = self._get_inputs(dtype)
    bn = snt.BatchNorm(offset=False, scale=False, decay_rate=0.9)

    with tf.name_scope("net1"):
      bn(inputs, is_training=True)

    with tf.name_scope("net2"):
      bn(inputs, is_training=True)

    update_ops_1 = tuple(tf.get_collection(tf.GraphKeys.UPDATE_OPS, "net1"))
    self.assertEqual(len(update_ops_1), 2)
    update_ops_2 = tuple(tf.get_collection(tf.GraphKeys.UPDATE_OPS, "net2"))
    self.assertEqual(len(update_ops_2), 2)

    with self.test_session() as sess:
      sess.run(tf.global_variables_initializer())

      mm, mv = sess.run([bn.moving_mean, bn.moving_variance])

      self.assertAllClose(np.zeros([1, 6]), mm)
      self.assertAllClose(np.ones([1, 6]), mv)

      sess.run(update_ops_1)
      sess.run(update_ops_2)

      mm, mv = sess.run([bn.moving_mean,
                         bn.moving_variance])

      correct_mm = (1.0 - bn._decay_rate) * v
      correct_mm = (1.0 - bn._decay_rate) * v + bn._decay_rate * correct_mm
      correct_mv = np.ones([1, 6]) * bn._decay_rate**2

      atol = 1.e-2 if dtype == tf.float16 else 1.e-6
      self.assertAllClose(np.reshape(correct_mm, [1, 6]), mm, atol=atol)
      self.assertAllClose(np.reshape(correct_mv, [1, 6]), mv, atol=atol)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag411')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/batch_norm_v2_test.py: 179-245
</a>
<div class="mid" id="frag411" style="display:none"><pre>
  def testCheckStatsDouble(self, dtype):
    """The correct statistics are being computed for double connection.

    Connected in parallel, it's ill-defined what order the updates will happen
    in. A double update could happen, or two sequential updates. E.g. If
    decay_rate is 0.9, the start value is 1.0, and the target value is 0.0, the
    value could progress as

      1.00 -&gt; 0.90 -&gt; 0.81,

    if the second update uses the fresh second value. Or as

      1.00 -&gt; 0.90 -&gt; 0.80

    if the second update uses the stale first value.

    We fix this here by running them in sequential run calls to ensure that this
    test is deterministic.

    The two situations are minimally different, especially if decay_rate is
    close to one (e.g. the default of 0.999).

    Args:
      dtype: TensorFlow datatype of input test batch.
    """

    v, _, inputs = self._get_inputs(dtype)
    bn = snt.BatchNormV2(
        offset=False,
        scale=False,
        decay_rate=0.9,
        update_ops_collection=tf.GraphKeys.UPDATE_OPS)

    with tf.name_scope("net1"):
      bn(inputs, is_training=True)

    with tf.name_scope("net2"):
      bn(inputs, is_training=True)

    update_ops_1 = tuple(tf.get_collection(tf.GraphKeys.UPDATE_OPS, "net1"))
    self.assertEqual(len(update_ops_1), 2)
    update_ops_2 = tuple(tf.get_collection(tf.GraphKeys.UPDATE_OPS, "net2"))
    self.assertEqual(len(update_ops_2), 2)

    with self.test_session() as sess:
      sess.run(tf.global_variables_initializer())

      mm, mv = sess.run([bn.moving_mean, bn.moving_variance])

      self.assertAllClose(np.zeros([1, 6]), mm)
      self.assertAllClose(np.ones([1, 6]), mv)

      sess.run(update_ops_1)
      sess.run(update_ops_2)

      mm, mv = sess.run([bn.moving_mean,
                         bn.moving_variance])

      correct_mm = (1.0 - bn._decay_rate) * v
      correct_mm = (1.0 - bn._decay_rate) * v + bn._decay_rate * correct_mm
      correct_mv = np.ones([1, 6]) * bn._decay_rate**2

      atol = 1.e-2 if dtype == tf.float16 else 1.e-6

      self.assertAllClose(np.reshape(correct_mm, [1, 6]), mm, atol=atol)
      self.assertAllClose(np.reshape(correct_mv, [1, 6]), mv, atol=atol)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 11:</b> &nbsp; 2 fragments, nominal size 26 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag372')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/batch_norm_test.py: 237-277
</a>
<div class="mid" id="frag372" style="display:none"><pre>
  def testCheckStatsPython(self):
    """The correct normalization is being used for different Python flags."""

    v, input_v, inputs = self._get_inputs()

    bn = snt.BatchNorm(offset=False, scale=False, decay_rate=0.5)
    out1 = bn(inputs, is_training=True, test_local_stats=True)
    out2 = bn(inputs, is_training=False, test_local_stats=True)
    out3 = bn(inputs, is_training=False, test_local_stats=False)

    update_ops = tuple(tf.get_collection(tf.GraphKeys.UPDATE_OPS))
    self.assertEqual(len(update_ops), 2)

    with tf.control_dependencies(update_ops):
      out1 = tf.identity(out1)

    with self.test_session() as sess:
      sess.run(tf.global_variables_initializer())

      out_v = sess.run(out1)
      mm, mv = sess.run([bn.moving_mean, bn.moving_variance])

      # Single moving average steps should have happened.
      correct_mm = (1.0 - bn._decay_rate) * v
      correct_mv = np.ones([1, 6]) * bn._decay_rate

      self.assertAllClose(np.reshape(correct_mm, [1, 6]), mm)
      self.assertAllClose(np.reshape(correct_mv, [1, 6]), mv)
      self.assertAllClose(np.zeros([7, 6]), out_v, rtol=1e-6, atol=1e-5)

      out2_, out3_ = sess.run([out2, out3])

      # Out2: Tested using local batch stats.
      # Better numerical precision due to using shifted estimators.
      self.assertAllClose(np.zeros([7, 6]), out2_)

      # Out3: Tested using moving average stats.
      self.assertAllClose(
          (input_v - mm) / np.sqrt(mv + bn._eps),
          out3_)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag412')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/batch_norm_v2_test.py: 246-291
</a>
<div class="mid" id="frag412" style="display:none"><pre>
  def testCheckStatsPython(self):
    """The correct normalization is being used for different Python flags."""

    v, input_v, inputs = self._get_inputs()

    bn = snt.BatchNormV2(
        offset=False,
        scale=False,
        decay_rate=0.5,
        update_ops_collection=tf.GraphKeys.UPDATE_OPS
    )
    out1 = bn(inputs, is_training=True, test_local_stats=True)
    out2 = bn(inputs, is_training=False, test_local_stats=True)
    out3 = bn(inputs, is_training=False, test_local_stats=False)

    update_ops = tuple(tf.get_collection(tf.GraphKeys.UPDATE_OPS))
    self.assertEqual(len(update_ops), 2)

    with tf.control_dependencies(update_ops):
      out1 = tf.identity(out1)

    with self.test_session() as sess:
      sess.run(tf.global_variables_initializer())

      out_v = sess.run(out1)
      mm, mv = sess.run([bn.moving_mean, bn.moving_variance])

      # Single moving average steps should have happened.
      correct_mm = (1.0 - bn._decay_rate) * v
      correct_mv = np.ones([1, 6]) * bn._decay_rate

      self.assertAllClose(np.reshape(correct_mm, [1, 6]), mm)
      self.assertAllClose(np.reshape(correct_mv, [1, 6]), mv)
      self.assertAllClose(np.zeros([7, 6]), out_v, rtol=1e-6, atol=1e-5)

      out2_, out3_ = sess.run([out2, out3])

      # Out2: Tested using local batch stats.
      # Better numerical precision due to using shifted estimators.
      self.assertAllClose(np.zeros([7, 6]), out2_, rtol=1e-6, atol=1e-5)

      # Out3: Tested using moving average stats.
      self.assertAllClose(
          (input_v - mm) / np.sqrt(mv + bn._eps),
          out3_)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 12:</b> &nbsp; 2 fragments, nominal size 42 lines, similarity 92%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag373')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/batch_norm_test.py: 283-353
</a>
<div class="mid" id="frag373" style="display:none"><pre>
  def testCheckStatsInGraph(self, update_ops_collection):
    """The correct normalization is being used for different TF flags."""

    v, input_v, inputs = self._get_inputs()

    bn = snt.BatchNorm(offset=False,
                       scale=False,
                       decay_rate=0.5,
                       update_ops_collection=update_ops_collection)

    is_training = tf.placeholder(tf.bool)
    test_local_stats = tf.placeholder(tf.bool)

    out = bn(inputs,
             is_training=is_training,
             test_local_stats=test_local_stats)

    if update_ops_collection is not None:
      update_ops = tuple(tf.get_collection(update_ops_collection))
      self.assertEqual(len(update_ops), 2)

      with tf.control_dependencies(update_ops):
        out = tf.identity(out)

    with self.test_session() as sess:
      sess.run(tf.global_variables_initializer())

      # Run with `is_training=True`, `test_local_stats=True`.
      out_v = sess.run(out, feed_dict={is_training: True,
                                       test_local_stats: True})

      # Moving averages not updated until after calculation so shifted
      # stats are poor.
      self.assertAllClose(np.zeros([7, 6]), out_v, rtol=1e-6, atol=1e-5)

      ops = (bn.moving_mean, bn.moving_variance)
      mm1, mv1 = sess.run(ops)

      # Single moving average step should have happened.
      correct_mm = (1.0 - bn._decay_rate) * v
      correct_mv = np.ones([1, 6]) * bn._decay_rate

      self.assertAllClose(np.reshape(correct_mm, [1, 6]), mm1)
      self.assertAllClose(np.reshape(correct_mv, [1, 6]), mv1)

      # Run with `is_training=False`, `test_local_stats=True`.
      # Should have used local batch stats.
      out_v = sess.run(out, feed_dict={is_training: False,
                                       test_local_stats: True})

      # Moving averages should not have changed.
      mm2, mv2 = sess.run(ops)
      self.assertAllClose(mm1, mm2)
      self.assertAllClose(mv1, mv2)

      self.assertAllClose(np.zeros([7, 6]), out_v)

      # Run with `is_training=False`, `test_local_stats=False`.
      # Should have used moving average stats.
      out_v = sess.run(out, feed_dict={is_training: False,
                                       test_local_stats: False})

      # Moving averages should not have changed.
      mm3, mv3 = sess.run(ops)
      self.assertAllClose(mm1, mm3)
      self.assertAllClose(mv1, mv3)

      self.assertAllClose(
          (input_v - mm3) / np.sqrt(mv3 + bn._eps),
          out_v)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag413')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/batch_norm_v2_test.py: 297-368
</a>
<div class="mid" id="frag413" style="display:none"><pre>
  def testCheckStatsInGraph(self, update_ops_collection):
    """The correct normalization is being used for different TF flags."""

    v, input_v, inputs = self._get_inputs()

    bn = snt.BatchNormV2(
        offset=False,
        scale=False,
        decay_rate=0.5,
        update_ops_collection=update_ops_collection)

    is_training = tf.placeholder(tf.bool)
    test_local_stats = tf.placeholder(tf.bool)

    out = bn(inputs,
             is_training=is_training,
             test_local_stats=test_local_stats)

    if update_ops_collection is not None:
      update_ops = tuple(tf.get_collection(update_ops_collection))
      self.assertEqual(len(update_ops), 2)

      with tf.control_dependencies(update_ops):
        out = tf.identity(out)

    with self.test_session() as sess:
      sess.run(tf.global_variables_initializer())

      # Run with `is_training=True`, `test_local_stats=True`.
      out_v = sess.run(out, feed_dict={is_training: True,
                                       test_local_stats: True})

      # Moving averages not updated until after calculation so shifted
      # stats are poor.
      self.assertAllClose(np.zeros([7, 6]), out_v, rtol=1e-6, atol=1e-5)

      ops = (bn.moving_mean, bn.moving_variance)
      mm1, mv1 = sess.run(ops)

      # Single moving average step should have happened.
      correct_mm = (1.0 - bn._decay_rate) * v
      correct_mv = np.ones([1, 6]) * bn._decay_rate

      self.assertAllClose(np.reshape(correct_mm, [1, 6]), mm1)
      self.assertAllClose(np.reshape(correct_mv, [1, 6]), mv1)

      # Run with `is_training=False`, `test_local_stats=True`.
      # Should have used local batch stats.
      out_v = sess.run(out, feed_dict={is_training: False,
                                       test_local_stats: True})

      # Moving averages should not have changed.
      mm2, mv2 = sess.run(ops)
      self.assertAllClose(mm1, mm2)
      self.assertAllClose(mv1, mv2)

      self.assertAllClose(np.zeros([7, 6]), out_v, rtol=1e-6, atol=1e-5)

      # Run with `is_training=False`, `test_local_stats=False`.
      # Should have used moving average stats.
      out_v = sess.run(out, feed_dict={is_training: False,
                                       test_local_stats: False})

      # Moving averages should not have changed.
      mm3, mv3 = sess.run(ops)
      self.assertAllClose(mm1, mm3)
      self.assertAllClose(mv1, mv3)

      self.assertAllClose(
          (input_v - mm3) / np.sqrt(mv3 + bn._eps),
          out_v)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 13:</b> &nbsp; 2 fragments, nominal size 21 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag375')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/batch_norm_test.py: 371-401
</a>
<div class="mid" id="frag375" style="display:none"><pre>
  def testUpdatesInsideCond(self):
    """Demonstrate that updates inside a cond fail.

    """

    _, input_v, inputs = self._get_inputs()
    bn = snt.BatchNorm(offset=False, scale=False, decay_rate=0.5)
    condition = tf.placeholder(tf.bool)
    cond = tf.cond(condition,
                   lambda: bn(inputs, is_training=True),
                   lambda: inputs)

    init = tf.global_variables_initializer()

    with self.test_session() as sess:
      sess.run(init)
      out_v = sess.run(cond, feed_dict={condition: False})
      self.assertAllClose(input_v, out_v)

      out_v = sess.run(cond, feed_dict={condition: True})
      self.assertAllClose(np.zeros([7, 6]), out_v, rtol=1e-4, atol=1e-4)

      # Variables are accessible outside the tf.cond()
      mm, mv = sess.run([bn.moving_mean, bn.moving_variance])
      self.assertAllClose(np.zeros([1, 6]), mm)
      self.assertAllClose(np.ones([1, 6]), mv)

      # Tensors are not accessible outside the tf.cond()
      with self.assertRaisesRegexp(ValueError, "Operation"):
        sess.run(tuple(tf.get_collection(tf.GraphKeys.UPDATE_OPS)))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag415')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/batch_norm_v2_test.py: 389-421
</a>
<div class="mid" id="frag415" style="display:none"><pre>
  def testUpdatesInsideCond(self):
    """Demonstrate that updates inside a cond fail."""

    _, input_v, inputs = self._get_inputs()
    bn = snt.BatchNormV2(
        offset=False,
        scale=False,
        decay_rate=0.5,
        update_ops_collection=tf.GraphKeys.UPDATE_OPS)
    condition = tf.placeholder(tf.bool)
    cond = tf.cond(condition,
                   lambda: bn(inputs, is_training=True),
                   lambda: inputs)

    init = tf.global_variables_initializer()

    with self.test_session() as sess:
      sess.run(init)
      out_v = sess.run(cond, feed_dict={condition: False})
      self.assertAllClose(input_v, out_v)

      out_v = sess.run(cond, feed_dict={condition: True})
      self.assertAllClose(np.zeros([7, 6]), out_v, rtol=1e-4, atol=1e-4)

      # Variables are accessible outside the tf.cond()
      mm, mv = sess.run([bn.moving_mean, bn.moving_variance])
      self.assertAllClose(np.zeros([1, 6]), mm)
      self.assertAllClose(np.ones([1, 6]), mv)

      # Tensors are not accessible outside the tf.cond()
      with self.assertRaisesRegexp(ValueError, "Operation"):
        sess.run(tuple(tf.get_collection(tf.GraphKeys.UPDATE_OPS)))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 14:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 88%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag376')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/batch_norm_test.py: 402-426
</a>
<div class="mid" id="frag376" style="display:none"><pre>
  def testVariableBatchSize(self):
    """Check the inputs batch_size can change."""

    inputs_shape = [10, 10]
    inputs = tf.placeholder(tf.float32, shape=[None] + inputs_shape)
    bn = snt.BatchNorm(offset=False, scale=False)

    # Outputs should be equal to inputs.
    out = bn(inputs,
             is_training=False,
             test_local_stats=False)

    init = tf.global_variables_initializer()
    update_ops = tuple(tf.get_collection(tf.GraphKeys.UPDATE_OPS))

    with self.test_session() as sess:
      sess.run(init)

      for batch_size in [1, 3, 10]:
        input_data = np.random.rand(batch_size, *inputs_shape)
        out_v = sess.run(out, feed_dict={inputs: input_data})
        self.assertAllClose(input_data / np.sqrt(1.0 + bn._eps), out_v)

        sess.run(update_ops, feed_dict={inputs: input_data})

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag416')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/batch_norm_v2_test.py: 422-447
</a>
<div class="mid" id="frag416" style="display:none"><pre>
  def testVariableBatchSize(self):
    """Check the inputs batch_size can change."""

    inputs_shape = [10, 10]
    inputs = tf.placeholder(tf.float32, shape=[None] + inputs_shape)
    bn = snt.BatchNormV2(
        offset=False, scale=False)

    # Outputs should be equal to inputs.
    out = bn(inputs,
             is_training=False,
             test_local_stats=False)

    init = tf.global_variables_initializer()
    update_ops = tuple(tf.get_collection(tf.GraphKeys.UPDATE_OPS))

    with self.test_session() as sess:
      sess.run(init)

      for batch_size in [1, 3, 10]:
        input_data = np.random.rand(batch_size, *inputs_shape)
        out_v = sess.run(out, feed_dict={inputs: input_data})
        self.assertAllClose(input_data / np.sqrt(1.0 + bn._eps), out_v)

        sess.run(update_ops, feed_dict={inputs: input_data})

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 15:</b> &nbsp; 2 fragments, nominal size 26 lines, similarity 84%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag380')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/batch_norm_test.py: 460-489
</a>
<div class="mid" id="frag380" style="display:none"><pre>
  def testInitializers(self, offset, scale):
    initializers = {
        "moving_mean": tf.constant_initializer(2.0),
        "moving_variance": tf.constant_initializer(3.0),
    }

    if scale:
      initializers["gamma"] = tf.constant_initializer(4.0)
    if offset:
      initializers["beta"] = tf.constant_initializer(5.0)

    inputs_shape = [10, 10]
    inputs = tf.placeholder(tf.float32, shape=[None] + inputs_shape)
    bn = snt.BatchNorm(offset=offset, scale=scale, initializers=initializers)
    self.assertEqual(bn.initializers, initializers)
    bn(inputs, is_training=True)

    init = tf.global_variables_initializer()
    with self.test_session() as sess:
      sess.run(init)

      ones_v = np.ones([1, 1, inputs_shape[-1]])
      self.assertAllClose(bn.moving_mean.eval(), ones_v * 2.0)
      self.assertAllClose(bn.moving_variance.eval(), ones_v * 3.0)

      if scale:
        self.assertAllClose(bn.gamma.eval(), ones_v * 4.0)
      if offset:
        self.assertAllClose(bn.beta.eval(), ones_v * 5.0)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag420')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/batch_norm_v2_test.py: 481-513
</a>
<div class="mid" id="frag420" style="display:none"><pre>
  def testInitializers(self, offset, scale):
    initializers = {
        "moving_mean": tf.constant_initializer(2.0),
        "moving_variance": tf.constant_initializer(3.0),
    }

    if scale:
      initializers["gamma"] = tf.constant_initializer(4.0)
    if offset:
      initializers["beta"] = tf.constant_initializer(5.0)

    inputs_shape = [10, 10]
    inputs = tf.placeholder(tf.float32, shape=[None] + inputs_shape)
    bn = snt.BatchNormV2(
        offset=offset,
        scale=scale,
        initializers=initializers)
    self.assertEqual(bn.initializers, initializers)
    bn(inputs, is_training=True)

    init = tf.global_variables_initializer()
    with self.test_session() as sess:
      sess.run(init)

      ones_v = np.ones([1, 1, inputs_shape[-1]])
      self.assertAllClose(bn.moving_mean.eval(), ones_v * 2.0)
      self.assertAllClose(bn.moving_variance.eval(), ones_v * 3.0)

      if scale:
        self.assertAllClose(bn.gamma.eval(), ones_v * 4.0)
      if offset:
        self.assertAllClose(bn.beta.eval(), ones_v * 5.0)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 16:</b> &nbsp; 2 fragments, nominal size 23 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag381')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/batch_norm_test.py: 496-519
</a>
<div class="mid" id="frag381" style="display:none"><pre>
  def testRegularizersInRegularizationLosses(self, offset, scale):
    regularizers = {}
    if offset:
      regularizers["beta"] = tf.contrib.layers.l1_regularizer(scale=0.5)
    if scale:
      regularizers["gamma"] = tf.contrib.layers.l2_regularizer(scale=0.5)

    inputs_shape = [10, 10]
    inputs = tf.placeholder(tf.float32, shape=[None] + inputs_shape)
    bn = snt.BatchNorm(offset=offset, scale=scale, regularizers=regularizers)
    self.assertEqual(bn.regularizers, regularizers)
    bn(inputs, is_training=True)

    graph_regularizers = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)
    if not offset and not scale:
      self.assertFalse(graph_regularizers)
    if offset and not scale:
      self.assertRegexpMatches(graph_regularizers[0].name, ".*l1_regularizer.*")
    if scale and not offset:
      self.assertRegexpMatches(graph_regularizers[0].name, ".*l2_regularizer.*")
    if scale and offset:
      self.assertRegexpMatches(graph_regularizers[0].name, ".*l1_regularizer.*")
      self.assertRegexpMatches(graph_regularizers[1].name, ".*l2_regularizer.*")

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag421')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/batch_norm_v2_test.py: 520-546
</a>
<div class="mid" id="frag421" style="display:none"><pre>
  def testRegularizersInRegularizationLosses(self, offset, scale):
    regularizers = {}
    if offset:
      regularizers["beta"] = tf.contrib.layers.l1_regularizer(scale=0.5)
    if scale:
      regularizers["gamma"] = tf.contrib.layers.l2_regularizer(scale=0.5)

    inputs_shape = [10, 10]
    inputs = tf.placeholder(tf.float32, shape=[None] + inputs_shape)
    bn = snt.BatchNormV2(
        offset=offset,
        scale=scale,
        regularizers=regularizers)
    self.assertEqual(bn.regularizers, regularizers)
    bn(inputs, is_training=True)

    graph_regularizers = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)
    if not offset and not scale:
      self.assertFalse(graph_regularizers)
    if offset and not scale:
      self.assertRegexpMatches(graph_regularizers[0].name, ".*l1_regularizer.*")
    if scale and not offset:
      self.assertRegexpMatches(graph_regularizers[0].name, ".*l2_regularizer.*")
    if scale and offset:
      self.assertRegexpMatches(graph_regularizers[0].name, ".*l1_regularizer.*")
      self.assertRegexpMatches(graph_regularizers[1].name, ".*l2_regularizer.*")

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 17:</b> &nbsp; 2 fragments, nominal size 31 lines, similarity 79%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag383')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/batch_norm_test.py: 558-601
</a>
<div class="mid" id="frag383" style="display:none"><pre>
  def testFusedBatchNorm(self, is_training, test_local_stats, scale,
                         is_training_python_bool):
    input_shape = (32, 9, 9, 8)
    iterations = 5
    x = tf.placeholder(tf.float32, shape=input_shape)
    bn1 = snt.BatchNorm(scale=scale, update_ops_collection=None)

    with self.assertRaises(NotImplementedError):
      # Input does not have 4 dimensions but fused is True.
      xlinear = tf.placeholder(tf.float32, shape=(2, 3))
      snt.BatchNorm(fused=True, scale=scale)(xlinear, is_training=True)

    with self.assertRaises(ValueError):
      # The axis is incorrect
      snt.BatchNorm(axis=(1, 2, 3), fused=True, scale=scale)(
          x, is_training=True)

    bn2 = snt.BatchNorm(scale=scale, fused=True, update_ops_collection=None)

    xx = np.random.random(input_shape)
    feed_dict = {x: xx}
    if not is_training_python_bool:
      is_training_node = tf.placeholder(tf.bool, shape=())
      feed_dict.update({is_training_node: is_training})
      is_training = is_training_node
      test_local_stats_node = tf.placeholder(tf.bool, shape=())
      feed_dict.update({test_local_stats_node: test_local_stats})
      test_local_stats = test_local_stats_node

    o1 = bn1(x, is_training=is_training, test_local_stats=test_local_stats)
    o2 = bn2(x, is_training=is_training, test_local_stats=test_local_stats)

    with self.test_session() as sess:
      sess.run(tf.global_variables_initializer())
      params = [
          o1, o2, bn1._moving_mean, bn1._moving_variance, bn2._moving_mean,
          bn2._moving_variance
      ]
      for _ in range(iterations):
        y1, y2, mean1, var1, mean2, var2 = sess.run(params, feed_dict=feed_dict)
        self.assertAllClose(y1, y2, atol=1e-4)
        self.assertAllClose(mean1, mean2, atol=1e-4)
        self.assertAllClose(var1, var2, atol=1e-4)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag423')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/batch_norm_v2_test.py: 588-620
</a>
<div class="mid" id="frag423" style="display:none"><pre>
  def testFusedBatchNormV2(self, is_training, test_local_stats, scale,
                           is_training_python_bool):
    input_shape = (32, 9, 9, 8)
    iterations = 5
    x = tf.placeholder(tf.float32, shape=input_shape)
    bn1 = snt.BatchNormV2(scale=scale)
    bn2 = snt.BatchNormV2(fused=False, scale=scale)

    xx = np.random.random(input_shape)
    feed_dict = {x: xx}
    if not is_training_python_bool:
      is_training_node = tf.placeholder(tf.bool, shape=())
      feed_dict.update({is_training_node: is_training})
      is_training = is_training_node
      test_local_stats_node = tf.placeholder(tf.bool, shape=())
      feed_dict.update({test_local_stats_node: test_local_stats})
      test_local_stats = test_local_stats_node

    o1 = bn1(x, is_training=is_training, test_local_stats=test_local_stats)
    o2 = bn2(x, is_training=is_training, test_local_stats=test_local_stats)

    with self.test_session() as sess:
      sess.run(tf.global_variables_initializer())
      params = [
          o1, o2, bn1._moving_mean, bn1._moving_variance, bn2._moving_mean,
          bn2._moving_variance
      ]
      for _ in range(iterations):
        y1, y2, mean1, var1, mean2, var2 = sess.run(params, feed_dict=feed_dict)
        self.assertAllClose(y1, y2, atol=1e-4)
        self.assertAllClose(mean1, mean2, atol=1e-4)
        self.assertAllClose(var1, var2, atol=1e-4)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 18:</b> &nbsp; 2 fragments, nominal size 19 lines, similarity 94%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag384')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/batch_norm_test.py: 606-630
</a>
<div class="mid" id="frag384" style="display:none"><pre>
  def testFusedBatchNormFloat16(self, is_training, test_local_stats):
    input_shape = (31, 7, 7, 5)
    iterations = 3
    x = tf.placeholder(tf.float16, shape=input_shape)
    bn1 = snt.BatchNorm(update_ops_collection=None)
    bn2 = snt.BatchNorm(fused=True, update_ops_collection=None)

    feed_dict = {x: np.random.random(input_shape)}

    o1 = bn1(x, is_training=is_training, test_local_stats=test_local_stats)
    o2 = bn2(x, is_training=is_training, test_local_stats=test_local_stats)

    with self.test_session() as sess:
      sess.run(tf.global_variables_initializer())
      params = [
          o1, o2, bn1._moving_mean, bn1._moving_variance, bn2._moving_mean,
          bn2._moving_variance
      ]
      for _ in range(iterations):
        y1, y2, mean1, var1, mean2, var2 = sess.run(params, feed_dict=feed_dict)
        self.assertAllClose(y1, y2, atol=1e-2)
        self.assertAllClose(mean1, mean2, atol=1e-2)
        self.assertAllClose(var1, var2, atol=1e-2)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag424')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/batch_norm_v2_test.py: 625-648
</a>
<div class="mid" id="frag424" style="display:none"><pre>
  def testFusedBatchNormFloat16(self, is_training, test_local_stats):
    input_shape = (31, 7, 7, 5)
    iterations = 3
    x = tf.placeholder(tf.float16, shape=input_shape)
    bn1 = snt.BatchNormV2(fused=False)
    bn2 = snt.BatchNormV2()

    feed_dict = {x: np.random.random(input_shape)}

    o1 = bn1(x, is_training=is_training, test_local_stats=test_local_stats)
    o2 = bn2(x, is_training=is_training, test_local_stats=test_local_stats)

    with self.test_session() as sess:
      sess.run(tf.global_variables_initializer())
      params = [
          o1, o2, bn1._moving_mean, bn1._moving_variance, bn2._moving_mean,
          bn2._moving_variance
      ]
      for _ in range(iterations):
        y1, y2, mean1, var1, mean2, var2 = sess.run(params, feed_dict=feed_dict)
        self.assertAllClose(y1, y2, atol=1e-2)
        self.assertAllClose(mean1, mean2, atol=1e-2)
        self.assertAllClose(var1, var2, atol=1e-2)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 19:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag502')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/basic_test.py: 471-484
</a>
<div class="mid" id="frag502" style="display:none"><pre>
  def testPartitioners(self):
    if tf.executing_eagerly():
      self.skipTest("Partitioned variables are not supported in eager mode.")
    inputs = tf.zeros([1, 100])
    partitioners = {
        "w": tf.variable_axis_size_partitioner(10000),
        "b": tf.variable_axis_size_partitioner(100),
    }
    linear = snt.Linear(100, partitioners=partitioners)
    linear(inputs)

    self.assertEqual(type(linear.w), variables.PartitionedVariable)
    self.assertEqual(type(linear.b), variables.PartitionedVariable)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag517')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/basic_test.py: 730-743
</a>
<div class="mid" id="frag517" style="display:none"><pre>
  def testPartitioners(self):
    if tf.executing_eagerly():
      self.skipTest("Partitioned variables are not supported in eager mode.")
    inputs = tf.zeros([1, 100])
    partitioners = {
        "b": tf.variable_axis_size_partitioner(10000),
    }
    bias = snt.AddBias(partitioners=partitioners)
    bias(inputs)

    self.assertEqual(type(bias.b), variables.PartitionedVariable)


# @tf.contrib.eager.run_all_tests_in_graph_and_eager_modes
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 20:</b> &nbsp; 3 fragments, nominal size 10 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag513')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/basic_test.py: 671-683
</a>
<div class="mid" id="frag513" style="display:none"><pre>
  def testInvalidInitializationParameters(self, bias_dims, unused_bias_shape):
    err = "Invalid initializer keys.*"
    with self.assertRaisesRegexp(KeyError, err):
      snt.AddBias(
          bias_dims=bias_dims,
          initializers={"not_b": tf.truncated_normal_initializer(stddev=1.0)})

    err = "Initializer for 'b' is not a callable function"
    with self.assertRaisesRegexp(TypeError, err):
      snt.AddBias(
          bias_dims=bias_dims,
          initializers={"b": tf.zeros([1, 2, 3])})

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag514')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/basic_test.py: 685-696
</a>
<div class="mid" id="frag514" style="display:none"><pre>
  def testInvalidPartitionerParameters(self, bias_dims, unused_bias_shape):
    with self.assertRaisesRegexp(KeyError, "Invalid partitioner keys.*"):
      snt.AddBias(
          bias_dims=bias_dims,
          partitioners={"not_b": tf.fixed_size_partitioner(num_shards=2)})

    err = "Partitioner for 'b' is not a callable function"
    with self.assertRaisesRegexp(TypeError, err):
      snt.AddBias(
          bias_dims=bias_dims,
          partitioners={"b": tf.zeros([1, 2, 3])})

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag523')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/basic_test.py: 824-835
</a>
<div class="mid" id="frag523" style="display:none"><pre>
  def testInvalidPartitionerParameters(self):
    with self.assertRaisesRegexp(KeyError, "Invalid partitioner keys.*"):
      snt.TrainableVariable(
          shape=[1],
          partitioners={"not_w": tf.fixed_size_partitioner(num_shards=2)})

    err = "Partitioner for 'w' is not a callable function"
    with self.assertRaisesRegexp(TypeError, err):
      snt.TrainableVariable(
          shape=[1],
          partitioners={"w": tf.zeros([1, 2, 3])})

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 21:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag582')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/basic_test.py: 1583-1595
</a>
<div class="mid" id="frag582" style="display:none"><pre>
  def testComputation(self):
    inputs = tf.constant(dtype=tf.int32, value=[[1, 2, 3], [1, 2, 3]])

    dims = [0, 1]
    begin = [0, 1]
    size = [1, 2]
    mod = snt.SliceByDim(dims=dims, begin=begin, size=size)
    output = mod(inputs)

    actual = self.evaluate(output)
    expected = [[2, 3]]
    self.assertAllEqual(actual, expected)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag583')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/basic_test.py: 1596-1608
</a>
<div class="mid" id="frag583" style="display:none"><pre>
  def testNegativeDim(self):
    inputs = tf.constant(dtype=tf.int32, value=[[1, 2, 3], [4, 5, 6]])

    dims = [0, -1]
    begin = [0, 1]
    size = [-1, 2]
    mod = snt.SliceByDim(dims=dims, begin=begin, size=size)
    output = mod(inputs)

    actual = self.evaluate(output)
    expected = [[2, 3], [5, 6]]
    self.assertAllEqual(actual, expected)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 22:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag634')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/nets/convnet_test.py: 220-233
</a>
<div class="mid" id="frag634" style="display:none"><pre>
  def testBatchNormBuildFlag(self, module):
    model = module(output_channels=self.output_channels,
                   kernel_shapes=self.kernel_shapes,
                   strides=self.strides,
                   paddings=self.paddings,
                   use_batch_norm=True)
    self.assertTrue(model.use_batch_norm)
    input_to_net = tf.random_normal(dtype=tf.float32, shape=(1, 100, 100, 3))

    # Check that an error is raised if we don't specify the is_training flag
    err = "is_training flag must be explicitly specified"
    with self.assertRaisesRegexp(ValueError, err):
      model(input_to_net)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag637')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/nets/convnet_test.py: 298-311
</a>
<div class="mid" id="frag637" style="display:none"><pre>
  def testNoBias(self, module):
    model = module(output_channels=self.output_channels,
                   kernel_shapes=self.kernel_shapes,
                   strides=self.strides,
                   paddings=self.paddings,
                   use_bias=False)
    self.assertEqual(model.use_bias, (False,) * len(self.output_channels))
    input_to_net = tf.random_normal(dtype=tf.float32, shape=(1, 100, 100, 3))
    model(input_to_net)

    model_variables = model.get_variables()

    self.assertLen(model_variables, len(self.output_channels))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 23:</b> &nbsp; 2 fragments, nominal size 23 lines, similarity 82%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag649')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/nets/convnet_test.py: 668-694
</a>
<div class="mid" id="frag649" style="display:none"><pre>
  def testCustomGetterTranspose(self):
    """Tests passing a custom getter to the transpose method."""
    conv2d = snt.nets.ConvNet2D(output_channels=self.output_channels,
                                kernel_shapes=self.kernel_shapes,
                                strides=self.strides,
                                paddings=self.paddings)
    input_shape = [10, 100, 100, 3]
    output_of_conv2d = conv2d(tf.zeros(dtype=tf.float32, shape=input_shape))
    # We'll be able to check if the custom_getter was used by checking for
    # gradients.
    conv2d_transpose = conv2d.transpose(
        custom_getter=snt.custom_getters.stop_gradient)
    if tf.executing_eagerly():
      with tf.GradientTape() as tape:
        output_of_transpose = conv2d_transpose(output_of_conv2d)
      conv2d_transpose_vars = conv2d_transpose.get_variables()
      self.assertTrue(len(conv2d_transpose_vars))
      for tensor in tape.gradient(output_of_transpose, conv2d_transpose_vars):
        self.assertIsNone(tensor)

    else:
      output_of_transpose = conv2d_transpose(output_of_conv2d)
      conv2d_transpose_vars = conv2d_transpose.get_variables()
      self.assertTrue(len(conv2d_transpose_vars))
      for tensor in tf.gradients(output_of_transpose, conv2d_transpose_vars):
        self.assertIsNone(tensor)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag662')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/nets/convnet_test.py: 1058-1085
</a>
<div class="mid" id="frag662" style="display:none"><pre>
  def testCustomGetterTranspose(self):
    """Tests passing a custom getter to the transpose method."""
    conv2d_t = snt.nets.ConvNet2DTranspose(
        output_shapes=self.output_shapes,
        output_channels=self.output_channels,
        kernel_shapes=self.kernel_shapes,
        strides=self.strides,
        paddings=self.paddings)
    input_shape = [10, 100, 100, 3]
    output_of_conv2d_t = conv2d_t(tf.zeros(dtype=tf.float32, shape=input_shape))
    # We'll be able to check if the custom_getter was used by checking for
    # gradients.
    conv2d = conv2d_t.transpose(custom_getter=snt.custom_getters.stop_gradient)
    if tf.executing_eagerly():
      with tf.GradientTape() as tape:
        output_of_conv = conv2d(output_of_conv2d_t)
      conv2d_vars = conv2d.get_variables()
      self.assertTrue(len(conv2d_vars))
      for tensor in tape.gradient(output_of_conv, conv2d_vars):
        self.assertIsNone(tensor)

    else:
      output_of_conv = conv2d(output_of_conv2d_t)
      conv2d_vars = conv2d.get_variables()
      self.assertTrue(len(conv2d_vars))
      for tensor in tf.gradients(output_of_conv, conv2d_vars):
        self.assertIsNone(tensor)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 24:</b> &nbsp; 4 fragments, nominal size 32 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag650')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/nets/convnet_test.py: 695-736
</a>
<div class="mid" id="frag650" style="display:none"><pre>
  def testNoCustomGetterTranspose(self):
    """Tests not passing a custom getter to the transpose method."""
    conv2d = snt.nets.ConvNet2D(output_channels=self.output_channels,
                                kernel_shapes=self.kernel_shapes,
                                strides=self.strides,
                                paddings=self.paddings,
                                custom_getter=snt.custom_getters.stop_gradient)
    input_shape = [10, 100, 100, 3]
    input_to_conv2d = tf.zeros(dtype=tf.float32, shape=input_shape)
    if tf.executing_eagerly():
      with tf.GradientTape() as tape0:
        output_of_conv2d = conv2d(input_to_conv2d)
      # Create a transpose without a custom getter
      conv2d_transpose = conv2d.transpose()
      with tf.GradientTape() as tape1:
        output_of_transpose = conv2d_transpose(output_of_conv2d)
      conv2d_vars = conv2d.get_variables()
      conv2d_grads = tape0.gradient(output_of_conv2d, conv2d_vars)
      conv2d_transpose_vars = conv2d_transpose.get_variables()
      conv2d_transpose_grads = tape1.gradient(output_of_transpose,
                                              conv2d_transpose_vars)
    else:
      output_of_conv2d = conv2d(input_to_conv2d)
      conv2d_vars = conv2d.get_variables()
      conv2d_grads = tf.gradients(output_of_conv2d, conv2d_vars)
      # Create a transpose without a custom getter
      conv2d_transpose = conv2d.transpose()
      output_of_transpose = conv2d_transpose(output_of_conv2d)
      conv2d_transpose_vars = conv2d_transpose.get_variables()
      conv2d_transpose_grads = tf.gradients(output_of_transpose,
                                            conv2d_transpose_vars)

    # Sanity check that the custom getter was indeed used for the conv net.
    self.assertTrue(len(conv2d_vars))
    for tensor in conv2d_grads:
      self.assertIsNone(tensor)
    # Check the transpose did not use the custom getter that was passed to the
    # original conv net.
    self.assertTrue(len(conv2d_transpose_vars))
    for tensor in conv2d_transpose_grads:
      self.assertIsNotNone(tensor)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag663')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/nets/convnet_test.py: 1086-1129
</a>
<div class="mid" id="frag663" style="display:none"><pre>
  def testNoCustomGetterTranspose(self):
    """Tests not passing a custom getter to the transpose method."""
    conv2d_t = snt.nets.ConvNet2DTranspose(
        output_shapes=self.output_shapes,
        output_channels=self.output_channels,
        kernel_shapes=self.kernel_shapes,
        strides=self.strides,
        paddings=self.paddings,
        custom_getter=snt.custom_getters.stop_gradient)
    input_shape = [10, 100, 100, 3]
    input_to_conv2d_t = tf.zeros(dtype=tf.float32, shape=input_shape)
    if tf.executing_eagerly():
      with tf.GradientTape() as tape0:
        output_of_conv2d_t = conv2d_t(input_to_conv2d_t)
      # Create a transpose without a custom getter
      conv2d = conv2d_t.transpose()
      with tf.GradientTape() as tape1:
        output_of_conv = conv2d(output_of_conv2d_t)
      conv2d_t_vars = conv2d_t.get_variables()
      conv2d_t_grads = tape0.gradient(output_of_conv2d_t, conv2d_t_vars)
      conv2d_vars = conv2d.get_variables()
      conv2d_grads = tape1.gradient(output_of_conv, conv2d_vars)
    else:
      output_of_conv2d_t = conv2d_t(input_to_conv2d_t)
      conv2d_t_vars = conv2d_t.get_variables()
      conv2d_t_grads = tf.gradients(output_of_conv2d_t, conv2d_t_vars)
      # Create a transpose without a custom getter
      conv2d = conv2d_t.transpose()
      output_of_conv = conv2d(output_of_conv2d_t)
      conv2d_vars = conv2d.get_variables()
      conv2d_grads = tf.gradients(output_of_conv, conv2d_vars)

    # Sanity check that the custom getter was indeed used for the conv net.
    self.assertTrue(len(conv2d_t_vars))
    for tensor in conv2d_t_grads:
      self.assertIsNone(tensor)
    # Check the transpose did not use the custom getter that was passed to the
    # original conv net.
    self.assertTrue(len(conv2d_vars))
    for tensor in conv2d_grads:
      self.assertIsNotNone(tensor)


# @tf.contrib.eager.run_all_tests_in_graph_and_eager_modes
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag661')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/nets/convnet_test.py: 1023-1057
</a>
<div class="mid" id="frag661" style="display:none"><pre>
  def testCustomGetter(self):
    custom_getter = snt.custom_getters.Context(snt.custom_getters.stop_gradient)
    module = snt.nets.ConvNet2DTranspose(
        output_shapes=self.output_shapes,
        output_channels=self.output_channels,
        kernel_shapes=self.kernel_shapes,
        strides=self.strides,
        paddings=self.paddings,
        custom_getter=custom_getter)

    input_shape = [10, 100, 100, 3]
    input_to_net = tf.random_normal(dtype=tf.float32, shape=input_shape)

    if tf.executing_eagerly():
      with tf.GradientTape() as tape0:
        out0 = module(input_to_net)
      with tf.GradientTape() as tape1:
        with custom_getter:
          out1 = module(input_to_net)
      all_vars = tf.trainable_variables()
      out0_grads = tape0.gradient(out0, all_vars)
      out1_grads = tape1.gradient(out1, all_vars)

    else:
      out0 = module(input_to_net)
      with custom_getter:
        out1 = module(input_to_net)
      all_vars = tf.trainable_variables()
      out0_grads = tf.gradients(out0, all_vars)
      out1_grads = tf.gradients(out1, all_vars)

    for grad in out0_grads:
      self.assertIsNotNone(grad)
    self.assertEqual([None] * len(out1_grads), out1_grads)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag653')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/nets/convnet_test.py: 801-834
</a>
<div class="mid" id="frag653" style="display:none"><pre>
  def testCustomGetter(self):
    custom_getter = snt.custom_getters.Context(snt.custom_getters.stop_gradient)
    module = snt.nets.ConvNet2D(output_channels=self.output_channels,
                                kernel_shapes=self.kernel_shapes,
                                rates=self.rates,
                                strides=self.strides,
                                paddings=self.paddings,
                                custom_getter=custom_getter)

    input_shape = [10, 100, 100, 3]
    input_to_net = tf.random_normal(dtype=tf.float32, shape=input_shape)

    if tf.executing_eagerly():
      with tf.GradientTape() as tape0:
        out0 = module(input_to_net)
      with tf.GradientTape() as tape1:
        with custom_getter:
          out1 = module(input_to_net)
      all_vars = tf.trainable_variables()
      out0_grads = tape0.gradient(out0, all_vars)
      out1_grads = tape1.gradient(out1, all_vars)

    else:
      out0 = module(input_to_net)
      with custom_getter:
        out1 = module(input_to_net)
      all_vars = tf.trainable_variables()
      out0_grads = tf.gradients(out0, all_vars)
      out1_grads = tf.gradients(out1, all_vars)

    for grad in out0_grads:
      self.assertNotEqual(None, grad)
    self.assertEqual([None] * len(out1_grads), out1_grads)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 25:</b> &nbsp; 2 fragments, nominal size 19 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag652')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/nets/convnet_test.py: 776-800
</a>
<div class="mid" id="frag652" style="display:none"><pre>
  def testPartitioners(self):
    if tf.executing_eagerly():
      self.skipTest("Eager does not support partitioned variables.")

    partitioners = {
        "w": tf.variable_axis_size_partitioner(10),
        "b": tf.variable_axis_size_partitioner(8),
    }

    module = snt.nets.ConvNet2D(output_channels=self.output_channels,
                                kernel_shapes=self.kernel_shapes,
                                rates=self.rates,
                                strides=self.strides,
                                paddings=self.paddings,
                                partitioners=partitioners)

    input_shape = [10, 100, 100, 3]
    input_to_net = tf.placeholder(tf.float32, shape=input_shape)

    _ = module(input_to_net)

    for layer in module._layers:
      self.assertEqual(type(layer.w), variables.PartitionedVariable)
      self.assertEqual(type(layer.b), variables.PartitionedVariable)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag659')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/nets/convnet_test.py: 974-998
</a>
<div class="mid" id="frag659" style="display:none"><pre>
  def testPartitioners(self):
    if tf.executing_eagerly():
      self.skipTest("Eager does not support partitioned variables.")

    partitioners = {
        "w": tf.variable_axis_size_partitioner(10),
        "b": tf.variable_axis_size_partitioner(8),
    }

    module = snt.nets.ConvNet2DTranspose(output_channels=self.output_channels,
                                         output_shapes=self.output_shapes,
                                         kernel_shapes=self.kernel_shapes,
                                         strides=self.strides,
                                         paddings=self.paddings,
                                         partitioners=partitioners)

    input_shape = [10, 100, 100, 3]
    input_to_net = tf.placeholder(tf.float32, shape=input_shape)

    _ = module(input_to_net)

    for layer in module._layers:
      self.assertEqual(type(layer.w), variables.PartitionedVariable)
      self.assertEqual(type(layer.b), variables.PartitionedVariable)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 26:</b> &nbsp; 3 fragments, nominal size 11 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag724')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/nets/mlp_test.py: 360-372
</a>
<div class="mid" id="frag724" style="display:none"><pre>
  def testDropoutOff(self):
    """Make sure dropout layers aren't added to the computation graph."""
    if tf.executing_eagerly():
      self.skipTest("Test not supported when executing eagerly")
    mlp_name = "test_dropout_on_mlp"
    mlp = snt.nets.MLP([1], use_dropout=False, use_bias=False,
                       activate_final=True, name=mlp_name)
    _ = mlp(tf.ones([1, 1]), is_training=True,
            dropout_keep_prob=0.5)
    op_names = [op.name for op in tf.get_default_graph().get_operations()]
    op_to_look_for = "{}_1/dropout/Shape".format(mlp_name)
    self.assertNotIn(op_to_look_for, op_names)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag725')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/nets/mlp_test.py: 373-384
</a>
<div class="mid" id="frag725" style="display:none"><pre>
  def testDropout(self):
    if tf.executing_eagerly():
      self.skipTest("Test not supported when executing eagerly")
    mlp_name = "test_dropout_on_mlp"
    mlp = snt.nets.MLP([1], use_dropout=True, use_bias=False,
                       activate_final=True, name=mlp_name)
    _ = mlp(tf.ones([1, 1]), is_training=True,
            dropout_keep_prob=0.5)
    op_names = [op.name for op in tf.get_default_graph().get_operations()]
    op_to_look_for = "{}_1/dropout/Shape".format(mlp_name)
    self.assertIn(op_to_look_for, op_names)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag726')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/nets/mlp_test.py: 385-397
</a>
<div class="mid" id="frag726" style="display:none"><pre>
  def testDropoutTensor(self):
    """Checks support for tf.Bool Tensors."""
    if tf.executing_eagerly():
      self.skipTest("Test not supported when executing eagerly")
    mlp_name = "test_dropout_on_mlp"
    mlp = snt.nets.MLP([1], use_dropout=True, use_bias=False,
                       activate_final=True, name=mlp_name)
    _ = mlp(tf.ones([1, 1]), is_training=tf.convert_to_tensor(True, tf.bool),
            dropout_keep_prob=0.5)
    op_names = [op.name for op in tf.get_default_graph().get_operations()]
    op_to_look_for = "{}_1/dropout/Shape".format(mlp_name)
    self.assertIn(op_to_look_for, op_names)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 27:</b> &nbsp; 5 fragments, nominal size 16 lines, similarity 82%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag766')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/block_matrix_test.py: 38-60
</a>
<div class="mid" id="frag766" style="display:none"><pre>
  def test_lower(self):
    """Tests block lower-triangular matrix."""

    btm = block_matrix.BlockTriangularMatrix(
        block_shape=(2, 3), block_rows=3, upper=False)
    self.assertEqual(btm.num_blocks, 6)
    self.assertEqual(btm.block_size, 6)
    self.assertEqual(btm.input_size, 36)

    output = btm(create_input(btm.input_size))
    with self.test_session() as sess:
      result = sess.run(output)

    self._check_output_size(btm, result)

    expected = np.array([[[0, 1, 2, 0, 0, 0, 0, 0, 0],
                          [3, 4, 5, 0, 0, 0, 0, 0, 0],
                          [6, 7, 8, 9, 10, 11, 0, 0, 0],
                          [12, 13, 14, 15, 16, 17, 0, 0, 0],
                          [18, 19, 20, 21, 22, 23, 24, 25, 26],
                          [27, 28, 29, 30, 31, 32, 33, 34, 35]]])
    self.assertAllEqual(result, expected)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag767')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/block_matrix_test.py: 61-83
</a>
<div class="mid" id="frag767" style="display:none"><pre>
  def test_lower_no_diagonal(self):
    """Tests block lower-triangular matrix without diagonal."""

    btm = block_matrix.BlockTriangularMatrix(
        block_shape=(2, 3), block_rows=3, include_diagonal=False)
    self.assertEqual(btm.num_blocks, 3)
    self.assertEqual(btm.block_size, 6)
    self.assertEqual(btm.input_size, 18)

    output = btm(create_input(btm.input_size))
    with self.test_session() as sess:
      result = sess.run(output)

    self._check_output_size(btm, result)

    expected = np.array([[[0, 0, 0, 0, 0, 0, 0, 0, 0],
                          [0, 0, 0, 0, 0, 0, 0, 0, 0],
                          [0, 1, 2, 0, 0, 0, 0, 0, 0],
                          [3, 4, 5, 0, 0, 0, 0, 0, 0],
                          [6, 7, 8, 9, 10, 11, 0, 0, 0],
                          [12, 13, 14, 15, 16, 17, 0, 0, 0]]])
    self.assertAllEqual(result, expected)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag769')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/block_matrix_test.py: 107-129
</a>
<div class="mid" id="frag769" style="display:none"><pre>
  def test_upper_no_diagonal(self):
    """Tests block upper-triangular matrix without diagonal."""

    btm = block_matrix.BlockTriangularMatrix(
        block_shape=(2, 3), block_rows=3, upper=True, include_diagonal=False)
    self.assertEqual(btm.num_blocks, 3)
    self.assertEqual(btm.block_size, 6)
    self.assertEqual(btm.input_size, 18)

    output = btm(create_input(btm.input_size))
    with self.test_session() as sess:
      result = sess.run(output)

    self._check_output_size(btm, result)

    expected = np.array([[[0, 0, 0, 0, 1, 2, 3, 4, 5],
                          [0, 0, 0, 6, 7, 8, 9, 10, 11],
                          [0, 0, 0, 0, 0, 0, 12, 13, 14],
                          [0, 0, 0, 0, 0, 0, 15, 16, 17],
                          [0, 0, 0, 0, 0, 0, 0, 0, 0],
                          [0, 0, 0, 0, 0, 0, 0, 0, 0]]])
    self.assertAllEqual(result, expected)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag768')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/block_matrix_test.py: 84-106
</a>
<div class="mid" id="frag768" style="display:none"><pre>
  def test_upper(self):
    """Tests block upper-triangular matrix."""

    btm = block_matrix.BlockTriangularMatrix(
        block_shape=(2, 3), block_rows=3, upper=True)
    self.assertEqual(btm.num_blocks, 6)
    self.assertEqual(btm.block_size, 6)
    self.assertEqual(btm.input_size, 36)

    output = btm(create_input(btm.input_size))
    with self.test_session() as sess:
      result = sess.run(output)

    self._check_output_size(btm, result)

    expected = np.array([[[0, 1, 2, 3, 4, 5, 6, 7, 8],
                          [9, 10, 11, 12, 13, 14, 15, 16, 17],
                          [0, 0, 0, 18, 19, 20, 21, 22, 23],
                          [0, 0, 0, 24, 25, 26, 27, 28, 29],
                          [0, 0, 0, 0, 0, 0, 30, 31, 32],
                          [0, 0, 0, 0, 0, 0, 33, 34, 35]]])
    self.assertAllEqual(result, expected)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag771')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/block_matrix_test.py: 155-174
</a>
<div class="mid" id="frag771" style="display:none"><pre>
  def test_default(self):
    """Tests BlockDiagonalMatrix."""

    bdm = block_matrix.BlockDiagonalMatrix(block_shape=(2, 3), block_rows=3)
    self.assertEqual(bdm.num_blocks, 3)
    self.assertEqual(bdm.block_size, 6)
    self.assertEqual(bdm.input_size, 18)

    output = bdm(create_input(bdm.input_size))
    with self.test_session() as sess:
      result = sess.run(output)

    expected = np.array([[[0, 1, 2, 0, 0, 0, 0, 0, 0],
                          [3, 4, 5, 0, 0, 0, 0, 0, 0],
                          [0, 0, 0, 6, 7, 8, 0, 0, 0],
                          [0, 0, 0, 9, 10, 11, 0, 0, 0],
                          [0, 0, 0, 0, 0, 0, 12, 13, 14],
                          [0, 0, 0, 0, 0, 0, 15, 16, 17]]])
    self.assertAllEqual(result, expected)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 28:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag786')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/residual_test.py: 98-110
</a>
<div class="mid" id="frag786" style="display:none"><pre>
  def testShape(self):
    inputs = tf.placeholder(tf.float32, shape=[self.batch_size, self.in_size])
    prev_state = tf.placeholder(
        tf.float32, shape=[self.batch_size, self.in_size])
    vanilla_rnn = snt.VanillaRNN(self.in_size)
    residual_wrapper = snt.ResidualCore(vanilla_rnn, name="residual")
    output, next_state = residual_wrapper(inputs, prev_state)
    shape = np.ndarray((self.batch_size, self.in_size))

    self.assertEqual(self.in_size, residual_wrapper.output_size)
    self.assertShapeEqual(shape, output)
    self.assertShapeEqual(shape, next_state)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag791')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/residual_test.py: 189-202
</a>
<div class="mid" id="frag791" style="display:none"><pre>
  def testShape(self):
    inputs = tf.placeholder(tf.float32, shape=[self.batch_size, self.in_size])
    prev_state = tf.placeholder(
        tf.float32, shape=[self.batch_size, self.hidden_size])
    vanilla_rnn = snt.VanillaRNN(self.hidden_size)
    skip_wrapper = snt.SkipConnectionCore(vanilla_rnn, name="skip")
    output, next_state = skip_wrapper(inputs, prev_state)
    output_shape = np.ndarray((self.batch_size,
                               self.in_size + self.hidden_size))
    state_shape = np.ndarray((self.batch_size, self.hidden_size))

    self.assertShapeEqual(output_shape, output)
    self.assertShapeEqual(state_shape, next_state)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 29:</b> &nbsp; 2 fragments, nominal size 25 lines, similarity 96%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag787')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/residual_test.py: 111-142
</a>
<div class="mid" id="frag787" style="display:none"><pre>
  def testComputation(self):
    inputs = tf.placeholder(tf.float32, shape=[self.batch_size, self.in_size])
    prev_state = tf.placeholder(tf.float32,
                                shape=[self.batch_size, self.in_size])

    vanilla_rnn = snt.VanillaRNN(name="rnn", hidden_size=self.in_size)
    residual = snt.ResidualCore(vanilla_rnn, name="residual")

    output, new_state = residual(inputs, prev_state)
    in_to_hid = vanilla_rnn.in_to_hidden_variables
    hid_to_hid = vanilla_rnn.hidden_to_hidden_variables
    with self.test_session() as sess:
      # With random data, check the TF calculation matches the Numpy version.
      input_data = np.random.randn(self.batch_size, self.in_size)
      prev_state_data = np.random.randn(self.batch_size, self.in_size)
      tf.global_variables_initializer().run()

      fetches = [output, new_state, in_to_hid[0], in_to_hid[1],
                 hid_to_hid[0], hid_to_hid[1]]
      output = sess.run(fetches,
                        {inputs: input_data, prev_state: prev_state_data})
    output_v, new_state_v, in_to_hid_w, in_to_hid_b = output[:4]
    hid_to_hid_w, hid_to_hid_b = output[4:]

    real_in_to_hid = np.dot(input_data, in_to_hid_w) + in_to_hid_b
    real_hid_to_hid = np.dot(prev_state_data, hid_to_hid_w) + hid_to_hid_b
    vanilla_output = np.tanh(real_in_to_hid + real_hid_to_hid)
    residual_output = vanilla_output + input_data

    self.assertAllClose(residual_output, output_v)
    self.assertAllClose(vanilla_output, new_state_v)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag792')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/residual_test.py: 203-234
</a>
<div class="mid" id="frag792" style="display:none"><pre>
  def testComputation(self):
    inputs = tf.placeholder(tf.float32, shape=[self.batch_size, self.in_size])
    prev_state = tf.placeholder(tf.float32,
                                shape=[self.batch_size, self.in_size])

    vanilla_rnn = snt.VanillaRNN(name="rnn", hidden_size=self.in_size)
    residual = snt.SkipConnectionCore(vanilla_rnn, name="skip")

    output, new_state = residual(inputs, prev_state)
    in_to_hid = vanilla_rnn.in_to_hidden_variables
    hid_to_hid = vanilla_rnn.hidden_to_hidden_variables
    with self.test_session() as sess:
      # With random data, check the TF calculation matches the Numpy version.
      input_data = np.random.randn(self.batch_size, self.in_size)
      prev_state_data = np.random.randn(self.batch_size, self.in_size)
      tf.global_variables_initializer().run()

      fetches = [output, new_state, in_to_hid[0], in_to_hid[1],
                 hid_to_hid[0], hid_to_hid[1]]
      output = sess.run(fetches,
                        {inputs: input_data, prev_state: prev_state_data})
    output_v, new_state_v, in_to_hid_w, in_to_hid_b = output[:4]
    hid_to_hid_w, hid_to_hid_b = output[4:]

    real_in_to_hid = np.dot(input_data, in_to_hid_w) + in_to_hid_b
    real_hid_to_hid = np.dot(prev_state_data, hid_to_hid_w) + hid_to_hid_b
    vanilla_output = np.tanh(real_in_to_hid + real_hid_to_hid)
    skip_output = np.concatenate((input_data, vanilla_output), -1)

    self.assertAllClose(skip_output, output_v)
    self.assertAllClose(vanilla_output, new_state_v)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 30:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag788')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/residual_test.py: 143-161
</a>
<div class="mid" id="frag788" style="display:none"><pre>
  def testHeterogeneousState(self):
    """Checks that the shape and type of the initial state are preserved."""

    core = HeterogeneousStateCore(name="rnn", hidden_size=self.in_size)
    residual = snt.ResidualCore(core, name="residual")

    core_state = core.initial_state(self.batch_size)
    residual_state = residual.initial_state(self.batch_size)

    self.assertEqual(core_state[0].shape.as_list(),
                     residual_state[0].shape.as_list())
    self.assertEqual(core_state[1].shape.as_list(),
                     residual_state[1].shape.as_list())
    self.assertEqual(core_state[0].dtype,
                     residual_state[0].dtype)
    self.assertEqual(core_state[1].dtype,
                     residual_state[1].dtype)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag793')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/residual_test.py: 235-253
</a>
<div class="mid" id="frag793" style="display:none"><pre>
  def testHeterogeneousState(self):
    """Checks that the shape and type of the initial state are preserved."""

    core = HeterogeneousStateCore(name="rnn", hidden_size=self.hidden_size)
    skip_wrapper = snt.SkipConnectionCore(core, name="skip")

    core_state = core.initial_state(self.batch_size)
    skip_state = skip_wrapper.initial_state(self.batch_size)

    self.assertEqual(core_state[0].shape.as_list(),
                     skip_state[0].shape.as_list())
    self.assertEqual(core_state[1].shape.as_list(),
                     skip_state[1].shape.as_list())
    self.assertEqual(core_state[0].dtype,
                     skip_state[0].dtype)
    self.assertEqual(core_state[1].dtype,
                     skip_state[1].dtype)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 31:</b> &nbsp; 2 fragments, nominal size 43 lines, similarity 82%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag815')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/spatial_transformer_test.py: 109-154
</a>
<div class="mid" id="frag815" style="display:none"><pre>
  def testSameAsNumPyReference(self, output_shape, source_shape, constraints):
    def chain(x):
      return itertools.chain(*x)

    def predict(output_shape, source_shape, inputs):
      ranges = [np.linspace(-1, 1, x, dtype=np.float32)
                for x in reversed(output_shape)]
      n = len(source_shape)
      grid = np.meshgrid(*ranges, indexing="xy")
      for _ in range(len(output_shape), len(source_shape)):
        grid.append(np.zeros_like(grid[0]))
      grid.append(np.ones_like(grid[0]))
      grid = np.array([x.reshape(1, -1) for x in grid]).squeeze()
      predicted_output = []
      for i in range(0, batch_size):
        x = np.dot(inputs[i, :].reshape(n, n+1), grid)
        for k, s in enumerate(reversed(source_shape)):
          s = (s - 1) * 0.5
          x[k, :] = x[k, :] * s + s
        x = np.concatenate([v.reshape(v.shape + (1,)) for v in x], -1)
        predicted_output.append(x.reshape(tuple(output_shape) + (n,)))
      return predicted_output

    batch_size = 20
    agw = snt.AffineGridWarper(source_shape=source_shape,
                               output_shape=output_shape,
                               constraints=constraints)
    inputs = tf.placeholder(tf.float32, [None, constraints.num_free_params])
    warped_grid = agw(inputs)
    full_size = constraints.num_dim * (constraints.num_dim + 1)
    full_input_np = np.random.rand(batch_size, full_size)

    con_i = [i for i, x in enumerate(chain(constraints.mask)) if not x]
    con_val = [x for x in chain(constraints.constraints) if x is not None]
    for i, v in zip(con_i, con_val):
      full_input_np[:, i] = v
    uncon_i = [i for i, x in enumerate(chain(constraints.mask)) if x]
    with self.test_session() as sess:
      output = sess.run(warped_grid,
                        feed_dict={inputs: full_input_np[:, uncon_i]})

    self.assertAllClose(output,
                        predict(output_shape, source_shape, full_input_np),
                        rtol=1e-05,
                        atol=1e-05)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag819')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/spatial_transformer_test.py: 176-228
</a>
<div class="mid" id="frag819" style="display:none"><pre>
  def testInvSameAsNumPyRef(self, output_shape, source_shape, constraints):
    def chain(x):
      return itertools.chain(*x)

    def predict(output_shape, source_shape, inputs):
      ranges = [np.linspace(-1, 1, x, dtype=np.float32)
                for x in reversed(source_shape)]
      n = len(output_shape)
      grid = np.meshgrid(*ranges, indexing="xy")
      for _ in range(len(source_shape), len(output_shape)):
        grid.append(np.zeros_like(grid[0]))
      grid.append(np.ones_like(grid[0]))
      grid = np.array([x.reshape(1, -1) for x in grid]).squeeze()
      predicted_output = []
      for i in range(0, batch_size):
        affine_matrix = inputs[i, :].reshape(n, n+1)
        inv_matrix = np.linalg.inv(affine_matrix[:2, :2])
        inv_transform = np.concatenate(
            [inv_matrix, -np.dot(inv_matrix,
                                 affine_matrix[:, 2].reshape(2, 1))], 1)
        x = np.dot(inv_transform, grid)
        for k, s in enumerate(reversed(output_shape)):
          s = (s - 1) * 0.5
          x[k, :] = x[k, :] * s + s
        x = np.concatenate([v.reshape(v.shape + (1,)) for v in x], -1)
        predicted_output.append(x.reshape(tuple(source_shape) + (n,)))
      return predicted_output

    batch_size = 20
    agw = snt.AffineGridWarper(source_shape=source_shape,
                               output_shape=output_shape,
                               constraints=constraints).inverse()
    inputs = tf.placeholder(tf.float32, [None, constraints.num_free_params])
    warped_grid = agw(inputs)
    full_size = constraints.num_dim * (constraints.num_dim + 1)
    # Adding a bit of mass to the matrix to avoid singular matrices
    full_input_np = np.random.rand(batch_size, full_size) + 0.1

    con_i = [i for i, x in enumerate(chain(constraints.mask)) if not x]
    con_val = [x for x in chain(constraints.constraints) if x is not None]
    for i, v in zip(con_i, con_val):
      full_input_np[:, i] = v
    uncon_i = [i for i, x in enumerate(chain(constraints.mask)) if x]
    with self.test_session() as sess:
      output = sess.run(warped_grid,
                        feed_dict={inputs: full_input_np[:, uncon_i]})

    self.assertAllClose(output,
                        predict(output_shape, source_shape, full_input_np),
                        rtol=1e-05,
                        atol=1e-05)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 32:</b> &nbsp; 7 fragments, nominal size 19 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag830')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/conv_gpu_test.py: 79-103
</a>
<div class="mid" id="frag830" style="display:none"><pre>
  def testConv1DDataFormats(self, use_bias, stride):
    """Check the module produces the same result for supported data formats."""
    func = functools.partial(
        snt.Conv1D,
        output_channels=self.OUT_CHANNELS,
        kernel_shape=self.KERNEL_SHAPE,
        use_bias=use_bias,
        stride=stride,
        initializers=create_initializers(use_bias))

    conv_nwc = func(name="NWC", data_format="NWC")
    x = tf.constant(np.random.random(self.INPUT_SHAPE).astype(np.float32))
    result_nwc = conv_nwc(x)

    # We will force both modules to share the same weights by creating
    # a custom getter that returns the weights from the first conv module when
    # tf.get_variable is called.
    custom_getter = {"w": create_custom_field_getter(conv_nwc, "w"),
                     "b": create_custom_field_getter(conv_nwc, "b")}
    conv_nwc = func(name="NCW", data_format="NCW", custom_getter=custom_getter)
    x_transpose = tf.transpose(x, perm=(0, 2, 1))
    result_ncw = tf.transpose(conv_nwc(x_transpose), perm=(0, 2, 1))

    self.checkEquality(result_nwc, result_ncw)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag840')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/conv_gpu_test.py: 235-260
</a>
<div class="mid" id="frag840" style="display:none"><pre>
  def testConv2DDataFormats(self, use_bias, stride):
    """Check the module produces the same result for supported data formats."""
    func = functools.partial(
        snt.Conv2D,
        output_channels=self.OUT_CHANNELS,
        kernel_shape=self.KERNEL_SHAPE,
        use_bias=use_bias,
        stride=stride,
        initializers=create_initializers(use_bias))

    conv_nhwc = func(name="NHWC", data_format="NHWC")
    x = tf.constant(np.random.random(self.INPUT_SHAPE).astype(np.float32))
    result_nhwc = conv_nhwc(x)

    # We will force both modules to share the same weights by creating
    # a custom getter that returns the weights from the first conv module when
    # tf.get_variable is called.
    custom_getter = {"w": create_custom_field_getter(conv_nhwc, "w"),
                     "b": create_custom_field_getter(conv_nhwc, "b")}
    conv_nchw = func(name="NCHW", data_format="NCHW",
                     custom_getter=custom_getter)
    x_transpose = tf.transpose(x, perm=(0, 3, 1, 2))
    result_nchw = tf.transpose(conv_nchw(x_transpose), perm=(0, 2, 3, 1))

    self.checkEquality(result_nhwc, result_nchw)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag835')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/conv_gpu_test.py: 157-181
</a>
<div class="mid" id="frag835" style="display:none"><pre>
  def testCausalConv1DDataFormats(self, use_bias, stride):
    """Check the module produces the same result for supported data formats."""
    func = functools.partial(
        snt.CausalConv1D,
        output_channels=self.OUT_CHANNELS,
        kernel_shape=self.KERNEL_SHAPE,
        use_bias=use_bias,
        stride=stride,
        initializers=create_initializers(use_bias))

    conv_nwc = func(name="NWC", data_format="NWC")
    x = tf.constant(np.random.random(self.INPUT_SHAPE).astype(np.float32))
    result_nwc = conv_nwc(x)

    # We will force both modules to share the same weights by creating
    # a custom getter that returns the weights from the first conv module when
    # tf.get_variable is called.
    custom_getter = {"w": create_custom_field_getter(conv_nwc, "w"),
                     "b": create_custom_field_getter(conv_nwc, "b")}
    conv_ncw = func(name="NCW", data_format="NCW", custom_getter=custom_getter)
    x_transpose = tf.transpose(x, perm=(0, 2, 1))
    result_ncw = tf.transpose(conv_ncw(x_transpose), perm=(0, 2, 1))

    self.checkEquality(result_nwc, result_ncw)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag850')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/conv_gpu_test.py: 395-424
</a>
<div class="mid" id="frag850" style="display:none"><pre>
  def testConv1DTransposeDataFormats(self, use_bias, stride):
    """Check the module produces the same result for supported data formats."""
    input_shape = (self.INPUT_SHAPE.input_batch,
                   int(np.ceil(self.INPUT_SHAPE.input_width / stride)),
                   self.INPUT_SHAPE.input_channels)

    func = functools.partial(
        snt.Conv1DTranspose,
        output_channels=self.OUT_CHANNELS,
        kernel_shape=self.KERNEL_SHAPE,
        output_shape=(self.INPUT_SHAPE.input_width,),
        use_bias=use_bias,
        stride=stride,
        initializers=create_initializers(use_bias))

    conv_nwc = func(name="NWC", data_format="NWC")
    x = tf.constant(np.random.random(input_shape).astype(np.float32))
    result_nwc = conv_nwc(x)

    # We will force both modules to share the same weights by creating
    # a custom getter that returns the weights from the first conv module when
    # tf.get_variable is called.
    custom_getter = {"w": create_custom_field_getter(conv_nwc, "w"),
                     "b": create_custom_field_getter(conv_nwc, "b")}
    conv_ncw = func(name="NCW", data_format="NCW", custom_getter=custom_getter)
    x_transpose = tf.transpose(x, perm=(0, 2, 1))
    result_ncw = tf.transpose(conv_ncw(x_transpose), perm=(0, 2, 1))

    self.checkEquality(result_nwc, result_ncw)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag845')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/conv_gpu_test.py: 315-340
</a>
<div class="mid" id="frag845" style="display:none"><pre>
  def testConv3DDataFormats(self, use_bias, stride):
    """Check the module produces the same result for supported data formats."""
    func = functools.partial(
        snt.Conv3D,
        output_channels=self.OUT_CHANNELS,
        kernel_shape=self.KERNEL_SHAPE,
        use_bias=use_bias,
        stride=stride,
        initializers=create_initializers(use_bias))

    conv_ndhwc = func(name="NDHWC", data_format="NDHWC")
    x = tf.constant(np.random.random(self.INPUT_SHAPE).astype(np.float32))
    result_ndhwc = conv_ndhwc(x)

    # We will force both modules to share the same weights by creating
    # a custom getter that returns the weights from the first conv module when
    # tf.get_variable is called.
    custom_getter = {"w": create_custom_field_getter(conv_ndhwc, "w"),
                     "b": create_custom_field_getter(conv_ndhwc, "b")}
    conv_ncdhw = func(name="NCDHW", data_format="NCDHW",
                      custom_getter=custom_getter)
    x_transpose = tf.transpose(x, perm=(0, 4, 1, 2, 3))
    result_ncdhw = tf.transpose(conv_ncdhw(x_transpose), perm=(0, 2, 3, 4, 1))

    self.checkEquality(result_ndhwc, result_ncdhw)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag855')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/conv_gpu_test.py: 479-511
</a>
<div class="mid" id="frag855" style="display:none"><pre>
  def testConv2DTransposeDataFormats(self, use_bias, stride):
    """Check the module produces the same result for supported data formats."""
    input_shape = (self.INPUT_SHAPE.input_batch,
                   int(np.ceil(self.INPUT_SHAPE.input_height / stride)),
                   int(np.ceil(self.INPUT_SHAPE.input_width / stride)),
                   self.INPUT_SHAPE.input_channels)

    func = functools.partial(
        snt.Conv2DTranspose,
        output_channels=self.OUT_CHANNELS,
        kernel_shape=self.KERNEL_SHAPE,
        output_shape=(self.INPUT_SHAPE.input_height,
                      self.INPUT_SHAPE.input_width),
        use_bias=use_bias,
        stride=stride,
        initializers=create_initializers(use_bias))

    conv_nhwc = func(name="NHWC", data_format="NHWC")
    x = tf.constant(np.random.random(input_shape).astype(np.float32))
    result_nhwc = conv_nhwc(x)

    # We will force both modules to share the same weights by creating
    # a custom getter that returns the weights from the first conv module when
    # tf.get_variable is called.
    custom_getter = {"w": create_custom_field_getter(conv_nhwc, "w"),
                     "b": create_custom_field_getter(conv_nhwc, "b")}
    conv_nchw = func(name="NCHW", data_format="NCHW",
                     custom_getter=custom_getter)
    x_transpose = tf.transpose(x, perm=(0, 3, 1, 2))
    result_nchw = tf.transpose(conv_nchw(x_transpose), perm=(0, 2, 3, 1))

    self.checkEquality(result_nhwc, result_nchw)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag860')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/conv_gpu_test.py: 568-602
</a>
<div class="mid" id="frag860" style="display:none"><pre>
  def testConv3DTransposeDataFormats(self, use_bias, stride):
    """Check the module produces the same result for supported data formats."""
    input_shape = (self.INPUT_SHAPE.input_batch,
                   int(np.ceil(self.INPUT_SHAPE.input_depth / stride)),
                   int(np.ceil(self.INPUT_SHAPE.input_height / stride)),
                   int(np.ceil(self.INPUT_SHAPE.input_width / stride)),
                   self.INPUT_SHAPE.input_channels)

    func = functools.partial(
        snt.Conv3DTranspose,
        output_channels=self.OUT_CHANNELS,
        kernel_shape=self.KERNEL_SHAPE,
        output_shape=(self.INPUT_SHAPE.input_depth,
                      self.INPUT_SHAPE.input_height,
                      self.INPUT_SHAPE.input_width),
        use_bias=use_bias,
        stride=stride,
        initializers=create_initializers(use_bias))

    conv_ndhwc = func(name="NDHWC", data_format="NDHWC")
    x = tf.constant(np.random.random(input_shape).astype(np.float32))
    result_ndhwc = conv_ndhwc(x)

    # We will force both modules to share the same weights by creating
    # a custom getter that returns the weights from the first conv module when
    # tf.get_variable is called.
    custom_getter = {"w": create_custom_field_getter(conv_ndhwc, "w"),
                     "b": create_custom_field_getter(conv_ndhwc, "b")}
    conv_ncdhw = func(name="NCDHW", data_format="NCDHW",
                      custom_getter=custom_getter)
    x_transpose = tf.transpose(x, perm=(0, 4, 1, 2, 3))
    result_ncdhw = tf.transpose(conv_ncdhw(x_transpose), perm=(0, 2, 3, 4, 1))

    self.checkEquality(result_ndhwc, result_ncdhw)

</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 33:</b> &nbsp; 7 fragments, nominal size 27 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag831')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/conv_gpu_test.py: 105-137
</a>
<div class="mid" id="frag831" style="display:none"><pre>
  def testConv1DDataFormatsBatchNorm(self, use_bias):
    """Similar to `testConv1DDataFormats`, but this checks BatchNorm support."""

    def func(name, data_format, custom_getter=None):
      conv = snt.Conv1D(
          name=name,
          output_channels=self.OUT_CHANNELS,
          kernel_shape=self.KERNEL_SHAPE,
          use_bias=use_bias,
          initializers=create_initializers(use_bias),
          data_format=data_format,
          custom_getter=custom_getter)
      if data_format == "NWC":
        batch_norm = snt.BatchNorm(scale=True, update_ops_collection=None)
      else:  # data_format = "NCW"
        batch_norm = snt.BatchNorm(scale=True, update_ops_collection=None,
                                   axis=(0, 2))
      return snt.Sequential([conv,
                             functools.partial(batch_norm, is_training=True)])

    seq_nwc = func(name="NWC", data_format="NWC")
    x = tf.constant(np.random.random(self.INPUT_SHAPE).astype(np.float32))
    result_nwc = seq_nwc(x)

    custom_getter = {"w": create_custom_field_getter(seq_nwc.layers[0], "w"),
                     "b": create_custom_field_getter(seq_nwc.layers[0], "b")}
    seq_ncw = func(name="NCW", data_format="NCW", custom_getter=custom_getter)
    x_transpose = tf.transpose(x, perm=(0, 2, 1))
    result_ncw = tf.transpose(seq_ncw(x_transpose), perm=(0, 2, 1))

    self.checkEquality(result_nwc, result_ncw)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag836')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/conv_gpu_test.py: 183-215
</a>
<div class="mid" id="frag836" style="display:none"><pre>
  def testCausalConv1DDataFormatsBatchNorm(self, use_bias):
    """Similar to `testCausalConv1DDataFormats`. Checks BatchNorm support."""

    def func(name, data_format, custom_getter=None):
      conv = snt.CausalConv1D(
          name=name,
          output_channels=self.OUT_CHANNELS,
          kernel_shape=self.KERNEL_SHAPE,
          use_bias=use_bias,
          initializers=create_initializers(use_bias),
          data_format=data_format,
          custom_getter=custom_getter)
      if data_format == "NWC":
        batch_norm = snt.BatchNorm(scale=True, update_ops_collection=None)
      else:  # data_format == "NCW"
        batch_norm = snt.BatchNorm(scale=True, update_ops_collection=None,
                                   axis=(0, 2))
      return snt.Sequential([conv,
                             functools.partial(batch_norm, is_training=True)])

    seq_nwc = func(name="NWC", data_format="NWC")
    x = tf.constant(np.random.random(self.INPUT_SHAPE).astype(np.float32))
    result_nwc = seq_nwc(x)

    custom_getter = {"w": create_custom_field_getter(seq_nwc.layers[0], "w"),
                     "b": create_custom_field_getter(seq_nwc.layers[0], "b")}
    seq_ncw = func(name="NCW", data_format="NCW", custom_getter=custom_getter)
    x_transpose = tf.transpose(x, perm=(0, 2, 1))
    result_ncw = tf.transpose(seq_ncw(x_transpose), perm=(0, 2, 1))

    self.checkEquality(result_nwc, result_ncw)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag861')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/conv_gpu_test.py: 604-639
</a>
<div class="mid" id="frag861" style="display:none"><pre>
  def testConv3DTransposeDataFormatsBatchNorm(self, use_bias):
    """Like `testConv3DTransposeDataFormats` but checks BatchNorm support."""

    def func(name, data_format, custom_getter=None):
      conv = snt.Conv3DTranspose(
          name=name,
          output_channels=self.OUT_CHANNELS,
          kernel_shape=self.KERNEL_SHAPE,
          output_shape=(self.INPUT_SHAPE.input_depth,
                        self.INPUT_SHAPE.input_height,
                        self.INPUT_SHAPE.input_width),
          use_bias=use_bias,
          initializers=create_initializers(use_bias),
          data_format=data_format,
          custom_getter=custom_getter)
      if data_format == "NDHWC":
        batch_norm = snt.BatchNorm(scale=True, update_ops_collection=None)
      else:  # data_format == "NCDHW"
        batch_norm = snt.BatchNorm(scale=True, update_ops_collection=None,
                                   axis=(0, 2, 3, 4))
      return snt.Sequential([conv,
                             functools.partial(batch_norm, is_training=True)])

    seq_ndhwc = func(name="NDHWC", data_format="NDHWC")
    x = tf.constant(np.random.random(self.INPUT_SHAPE).astype(np.float32))
    result_ndhwc = seq_ndhwc(x)

    custom_getter = {"w": create_custom_field_getter(seq_ndhwc.layers[0], "w"),
                     "b": create_custom_field_getter(seq_ndhwc.layers[0], "b")}
    seq_ncdhw = func(name="NCDHW", data_format="NCDHW",
                     custom_getter=custom_getter)
    x_transpose = tf.transpose(x, perm=(0, 4, 1, 2, 3))
    result_ncdhw = tf.transpose(seq_ncdhw(x_transpose), perm=(0, 2, 3, 4, 1))

    self.checkEquality(result_ndhwc, result_ncdhw)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag841')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/conv_gpu_test.py: 262-295
</a>
<div class="mid" id="frag841" style="display:none"><pre>
  def testConv2DDataFormatsBatchNorm(self, use_bias):
    """Similar to `testConv2DDataFormats`, but this checks BatchNorm support."""

    def func(name, data_format, custom_getter=None):
      conv = snt.Conv2D(
          name=name,
          output_channels=self.OUT_CHANNELS,
          kernel_shape=self.KERNEL_SHAPE,
          use_bias=use_bias,
          initializers=create_initializers(use_bias),
          data_format=data_format,
          custom_getter=custom_getter)
      if data_format == "NHWC":
        batch_norm = snt.BatchNorm(scale=True, update_ops_collection=None)
      else:  # data_format = "NCHW"
        batch_norm = snt.BatchNorm(scale=True, update_ops_collection=None,
                                   fused=True, axis=(0, 2, 3))
      return snt.Sequential([conv,
                             functools.partial(batch_norm, is_training=True)])

    seq_nhwc = func(name="NHWC", data_format="NHWC")
    x = tf.constant(np.random.random(self.INPUT_SHAPE).astype(np.float32))
    result_nhwc = seq_nhwc(x)

    custom_getter = {"w": create_custom_field_getter(seq_nhwc.layers[0], "w"),
                     "b": create_custom_field_getter(seq_nhwc.layers[0], "b")}
    seq_nchw = func(name="NCHW", data_format="NCHW",
                    custom_getter=custom_getter)
    x_transpose = tf.transpose(x, perm=(0, 3, 1, 2))
    result_nchw = tf.transpose(seq_nchw(x_transpose), perm=(0, 2, 3, 1))

    self.checkEquality(result_nhwc, result_nchw)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag846')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/conv_gpu_test.py: 342-375
</a>
<div class="mid" id="frag846" style="display:none"><pre>
  def testConv3DDataFormatsBatchNorm(self, use_bias):
    """Similar to `testConv3DDataFormats`, but this checks BatchNorm support."""

    def func(name, data_format, custom_getter=None):
      conv = snt.Conv3D(
          name=name,
          output_channels=self.OUT_CHANNELS,
          kernel_shape=self.KERNEL_SHAPE,
          use_bias=use_bias,
          initializers=create_initializers(use_bias),
          data_format=data_format,
          custom_getter=custom_getter)
      if data_format == "NDHWC":
        batch_norm = snt.BatchNorm(scale=True, update_ops_collection=None)
      else:  # data_format = "NCDHW"
        batch_norm = snt.BatchNorm(scale=True, update_ops_collection=None,
                                   axis=(0, 2, 3, 4))
      return snt.Sequential([conv,
                             functools.partial(batch_norm, is_training=True)])

    seq_ndhwc = func(name="NDHWC", data_format="NDHWC")
    x = tf.constant(np.random.random(self.INPUT_SHAPE).astype(np.float32))
    result_ndhwc = seq_ndhwc(x)

    custom_getter = {"w": create_custom_field_getter(seq_ndhwc.layers[0], "w"),
                     "b": create_custom_field_getter(seq_ndhwc.layers[0], "b")}
    seq_ncdhw = func(name="NCDHW", data_format="NCDHW",
                     custom_getter=custom_getter)
    x_transpose = tf.transpose(x, perm=(0, 4, 1, 2, 3))
    result_ncdhw = tf.transpose(seq_ncdhw(x_transpose), perm=(0, 2, 3, 4, 1))

    self.checkEquality(result_ndhwc, result_ncdhw)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag851')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/conv_gpu_test.py: 426-459
</a>
<div class="mid" id="frag851" style="display:none"><pre>
  def testConv1DTransposeDataFormatsBatchNorm(self, use_bias):
    """Like `testConv1DTransposeDataFormats` but checks BatchNorm support."""

    def func(name, data_format, custom_getter=None):
      conv = snt.Conv1DTranspose(
          name=name,
          output_channels=self.OUT_CHANNELS,
          kernel_shape=self.KERNEL_SHAPE,
          output_shape=(self.INPUT_SHAPE.input_width,),
          use_bias=use_bias,
          initializers=create_initializers(use_bias),
          data_format=data_format,
          custom_getter=custom_getter)
      if data_format == "NWC":
        batch_norm = snt.BatchNorm(scale=True, update_ops_collection=None)
      else:  # data_format == "NCW"
        batch_norm = snt.BatchNorm(scale=True, update_ops_collection=None,
                                   axis=(0, 2))
      return snt.Sequential([conv,
                             functools.partial(batch_norm, is_training=True)])

    seq_nwc = func(name="NWC", data_format="NWC")
    x = tf.constant(np.random.random(self.INPUT_SHAPE).astype(np.float32))
    result_nwc = seq_nwc(x)

    custom_getter = {"w": create_custom_field_getter(seq_nwc.layers[0], "w"),
                     "b": create_custom_field_getter(seq_nwc.layers[0], "b")}
    seq_ncw = func(name="NCW", data_format="NCW", custom_getter=custom_getter)
    x_transpose = tf.transpose(x, perm=(0, 2, 1))
    result_ncw = tf.transpose(seq_ncw(x_transpose), perm=(0, 2, 1))

    self.checkEquality(result_nwc, result_ncw)


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag856')" href="javascript:;">
sonnet-1.33/sonnet/python/modules/conv_gpu_test.py: 513-548
</a>
<div class="mid" id="frag856" style="display:none"><pre>
  def testConv2DTransposeDataFormatsBatchNorm(self, use_bias):
    """Like `testConv2DTransposeDataFormats` but checks BatchNorm support."""

    def func(name, data_format, custom_getter=None):
      conv = snt.Conv2DTranspose(
          name=name,
          output_channels=self.OUT_CHANNELS,
          kernel_shape=self.KERNEL_SHAPE,
          output_shape=(self.INPUT_SHAPE.input_height,
                        self.INPUT_SHAPE.input_width),
          use_bias=use_bias,
          initializers=create_initializers(use_bias),
          data_format=data_format,
          custom_getter=custom_getter)
      if data_format == "NHWC":
        batch_norm = snt.BatchNorm(scale=True, update_ops_collection=None)
      else:  # data_format == "NCHW"
        batch_norm = snt.BatchNorm(scale=True, update_ops_collection=None,
                                   fused=True, axis=(0, 2, 3))
      return snt.Sequential([conv,
                             functools.partial(batch_norm, is_training=True)])

    seq_nhwc = func(name="NHWC", data_format="NHWC")
    x = tf.constant(np.random.random(self.INPUT_SHAPE).astype(np.float32))
    result_nhwc = seq_nhwc(x)

    custom_getter = {"w": create_custom_field_getter(seq_nhwc.layers[0], "w"),
                     "b": create_custom_field_getter(seq_nhwc.layers[0], "b")}
    seq_nchw = func(name="NCHW", data_format="NCHW",
                    custom_getter=custom_getter)
    x_transpose = tf.transpose(x, perm=(0, 3, 1, 2))
    result_nchw = tf.transpose(seq_nchw(x_transpose), perm=(0, 2, 3, 1))

    self.checkEquality(result_nhwc, result_nchw)


</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<script language="JavaScript">
function ShowHide(divId) { 
    if(document.getElementById(divId).style.display == 'none') {
        document.getElementById(divId).style.display='block';
    } else { 
        document.getElementById(divId).style.display = 'none';
    } 
}
</script>
</body>
</html>

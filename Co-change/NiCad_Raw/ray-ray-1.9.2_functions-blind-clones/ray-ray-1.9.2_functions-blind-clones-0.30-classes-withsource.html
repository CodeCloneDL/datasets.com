<html>
<head>
    <title>NiCad6 Clone Report</title>
    <style type="text/css">
        body {font-family:sans-serif;}
        table {background-color:white; border:0px; padding:0px; border-spacing:4px; width:auto; margin-left:30px; margin-right:auto;}
        td {background-color:rgba(192,212,238,0.8); border:0px; padding:8px; width:auto; vertical-align:top; border-radius:8px}
        pre {background-color:white; padding:4px;}
        a {color:darkblue;}
    </style>
</head>
<body>
<h2>NiCad6 Clone Report</h2>
<table>
<tr style="font-size:14pt">
<td><b>System:</b> &nbsp; ray-ray-1.9.2</td>
<td><b>Clone pairs:</b> &nbsp; 548</td>
<td><b>Clone classes:</b> &nbsp; 226</td>
</tr>
<tr style="font-size:12pt">
<td style="background-color:white">Clone type: &nbsp; 3-2</td>
<td style="background-color:white">Granularity: &nbsp; functions-blind</td>
<td style="background-color:white">Max diff threshold: &nbsp; 30%</td>
<td style="background-color:white">Clone size: &nbsp; 10 - 2500 lines</td>
<td style="background-color:white">Total functions-blind: &nbsp; 7993</td>
</tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 1:</b> &nbsp; 2 fragments, nominal size 26 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag11')" href="javascript:;">
ray-ray-1.9.2/rllib/offline/is_estimator.py: 12-39
</a>
<div class="mid" id="frag11" style="display:none"><pre>
    @override(OffPolicyEstimator)
    def estimate(self, batch: SampleBatchType) -&gt; OffPolicyEstimate:
        self.check_can_estimate_for(batch)

        rewards, old_prob = batch["rewards"], batch["action_prob"]
        new_prob = self.action_log_likelihood(batch)

        # calculate importance ratios
        p = []
        for t in range(batch.count):
            if t == 0:
                pt_prev = 1.0
            else:
                pt_prev = p[t - 1]
            p.append(pt_prev * new_prob[t] / old_prob[t])

        # calculate stepwise IS estimate
        V_prev, V_step_IS = 0.0, 0.0
        for t in range(batch.count):
            V_prev += rewards[t] * self.gamma**t
            V_step_IS += p[t] * rewards[t] * self.gamma**t

        estimation = OffPolicyEstimate(
            "is", {
                "V_prev": V_prev,
                "V_step_IS": V_step_IS,
                "V_gain_est": V_step_IS / max(1e-8, V_prev),
            })
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag20')" href="javascript:;">
ray-ray-1.9.2/rllib/offline/wis_estimator.py: 18-53
</a>
<div class="mid" id="frag20" style="display:none"><pre>
    @override(OffPolicyEstimator)
    def estimate(self, batch: SampleBatchType) -&gt; OffPolicyEstimate:
        self.check_can_estimate_for(batch)

        rewards, old_prob = batch["rewards"], batch["action_prob"]
        new_prob = self.action_log_likelihood(batch)

        # calculate importance ratios
        p = []
        for t in range(batch.count):
            if t == 0:
                pt_prev = 1.0
            else:
                pt_prev = p[t - 1]
            p.append(pt_prev * new_prob[t] / old_prob[t])
        for t, v in enumerate(p):
            if t &gt;= len(self.filter_values):
                self.filter_values.append(v)
                self.filter_counts.append(1.0)
            else:
                self.filter_values[t] += v
                self.filter_counts[t] += 1.0

        # calculate stepwise weighted IS estimate
        V_prev, V_step_WIS = 0.0, 0.0
        for t in range(batch.count):
            V_prev += rewards[t] * self.gamma**t
            w_t = self.filter_values[t] / self.filter_counts[t]
            V_step_WIS += p[t] / w_t * rewards[t] * self.gamma**t

        estimation = OffPolicyEstimate(
            "wis", {
                "V_prev": V_prev,
                "V_step_WIS": V_step_WIS,
                "V_gain_est": V_step_WIS / max(1e-8, V_prev),
            })
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 2:</b> &nbsp; 2 fragments, nominal size 28 lines, similarity 92%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag40')" href="javascript:;">
ray-ray-1.9.2/rllib/contrib/sumo/utils.py: 107-137
</a>
<div class="mid" id="frag40" style="display:none"><pre>
    def get_timeloss(self, entity, default=float("NaN")):
        """ Returns the timeLoss computed by SUMO for the given entity. """

        if entity in self.tripinfo:
            logger.debug("TRIPINFO for %s", entity)
            if "timeLoss" in self.tripinfo[entity]:
                logger.debug("timeLoss %s", self.tripinfo[entity]["timeLoss"])
                return float(self.tripinfo[entity]["timeLoss"])
            logger.debug("timeLoss not found.")
            return default
        elif entity in self.personinfo:
            logger.debug("PERSONINFO for %s", entity)
            logger.debug("%s", pformat(self.personinfo[entity]))
            time_loss, ts_found = 0.0, False
            for _, stage in self.personinfo[entity]["stages"]:
                if "timeLoss" in stage:
                    logger.debug("timeLoss %s", stage["timeLoss"])
                    time_loss += float(stage["timeLoss"])
                    ts_found = True
            if not ts_found:
                logger.debug("timeLoss not found.")
                return default
            if time_loss &lt;= 0:
                logger.debug("ERROR: timeLoss is %.2f", time_loss)
                return default
            logger.debug("total timeLoss %.2f", time_loss)
            return time_loss
        else:
            logger.debug("Entity %s not found.", entity)
        return default

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag43')" href="javascript:;">
ray-ray-1.9.2/rllib/contrib/sumo/utils.py: 199-235
</a>
<div class="mid" id="frag43" style="display:none"><pre>
    def get_arrival(self, entity, default=float("NaN")):
        """
            Returns the arrival computed by SUMO for the given entity.

            The functions process_tripinfo_file() needs to be called in advance
            to initialize the data structures required.

            If the entity does not exist or does not have the value, it returns
            the default value.
        """
        if entity in self.tripinfo:
            logger.debug("TRIPINFO for %s", entity)
            if "arrival" in self.tripinfo[entity]:
                logger.debug("arrival %s", self.tripinfo[entity]["arrival"])
                return float(self.tripinfo[entity]["arrival"])
            logger.debug("arrival not found.")
            return default
        elif entity in self.personinfo:
            logger.debug("PERSONINFO for %s", entity)
            arrival, arrival_found = 0.0, False
            for _, stage in self.personinfo[entity]["stages"]:
                if "arrival" in stage:
                    logger.debug("arrival %s", stage["arrival"])
                    arrival = float(stage["arrival"])
                    arrival_found = True
            if not arrival_found:
                logger.debug("arrival not found.")
                return default
            if arrival &lt;= 0:
                logger.debug("ERROR: arrival is %.2f", arrival)
                return default
            logger.debug("total arrival %.2f", arrival)
            return arrival
        else:
            logger.debug("Entity %s not found.", entity)
        return default

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 3:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag126')" href="javascript:;">
ray-ray-1.9.2/rllib/contrib/bandits/envs/discrete.py: 23-42
</a>
<div class="mid" id="frag126" style="display:none"><pre>
    def __init__(self, config=None):
        self.config = copy.copy(DEFAULT_CONFIG_LINEAR)
        if config is not None and type(config) == dict:
            self.config.update(config)

        self.feature_dim = self.config["feature_dim"]
        self.num_actions = self.config["num_actions"]
        self.sigma = self.config["reward_noise_std"]

        self.action_space = spaces.Discrete(self.num_actions)
        self.observation_space = spaces.Box(
            low=-10, high=10, shape=(self.feature_dim, ))

        self.thetas = np.random.uniform(-1, 1,
                                        (self.num_actions, self.feature_dim))
        self.thetas /= np.linalg.norm(self.thetas, axis=1, keepdims=True)

        self._elapsed_steps = 0
        self._current_context = None

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag131')" href="javascript:;">
ray-ray-1.9.2/rllib/contrib/bandits/envs/discrete.py: 93-111
</a>
<div class="mid" id="frag131" style="display:none"><pre>

    def __init__(self, config=None):
        self.config = copy.copy(DEFAULT_CONFIG_WHEEL)
        if config is not None and type(config) == dict:
            self.config.update(config)

        self.delta = self.config["delta"]
        self.mu_1 = self.config["mu_1"]
        self.mu_2 = self.config["mu_2"]
        self.mu_3 = self.config["mu_3"]
        self.std = self.config["std"]

        self.action_space = spaces.Discrete(self.num_actions)
        self.observation_space = spaces.Box(
            low=-1, high=1, shape=(self.feature_dim, ))

        self.means = [self.mu_1] + 4 * [self.mu_2]
        self._elapsed_steps = 0
        self._current_context = None
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 4:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag136')" href="javascript:;">
ray-ray-1.9.2/rllib/contrib/bandits/examples/tune_LinTS_train_wheel_env.py: 16-32
</a>
<div class="mid" id="frag136" style="display:none"><pre>
def plot_model_weights(means, covs, ax):
    fmts = ["bo", "ro", "yx", "k+", "gx"]
    labels = ["arm{}".format(i) for i in range(5)]

    ax.set_title("Weights distributions of arms")

    for i in range(0, 5):
        x, y = np.random.multivariate_normal(means[i] / 30, covs[i], 5000).T
        ax.plot(x, y, fmts[i], label=labels[i])

    ax.set_aspect("equal")
    ax.grid(True, which="both")
    ax.axhline(y=0, color="k")
    ax.axvline(x=0, color="k")
    ax.legend(loc="best")


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag137')" href="javascript:;">
ray-ray-1.9.2/rllib/contrib/bandits/examples/LinTS_train_wheel_env.py: 13-31
</a>
<div class="mid" id="frag137" style="display:none"><pre>
def plot_model_weights(means, covs):
    fmts = ["bo", "ro", "yx", "k+", "gx"]
    labels = ["arm{}".format(i) for i in range(5)]

    fig, ax = plt.subplots(figsize=(6, 4))

    ax.set_title("Weights distributions of arms")

    for i in range(0, 5):
        x, y = np.random.multivariate_normal(means[i] / 30, covs[i], 5000).T
        ax.plot(x, y, fmts[i], label=labels[i])

    ax.grid(True, which="both")
    ax.axhline(y=0, color="k")
    ax.axvline(x=0, color="k")
    ax.legend(loc="best")
    plt.show()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 5:</b> &nbsp; 2 fragments, nominal size 44 lines, similarity 93%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag170')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/ars/ars_tf_policy.py: 22-78
</a>
<div class="mid" id="frag170" style="display:none"><pre>
    def __init__(self, obs_space, action_space, config):
        super().__init__(obs_space, action_space, config)
        self.action_noise_std = self.config["action_noise_std"]
        self.preprocessor = ModelCatalog.get_preprocessor_for_space(
            self.observation_space)
        self.observation_filter = get_filter(self.config["observation_filter"],
                                             self.preprocessor.shape)

        self.single_threaded = self.config.get("single_threaded", False)
        if self.config["framework"] == "tf":
            self.sess = make_session(single_threaded=self.single_threaded)

            # Set graph-level seed.
            if config.get("seed") is not None:
                with self.sess.as_default():
                    tf1.set_random_seed(config["seed"])

            self.inputs = tf1.placeholder(
                tf.float32, [None] + list(self.preprocessor.shape))
        else:
            if not tf1.executing_eagerly():
                tf1.enable_eager_execution()
            self.sess = self.inputs = None
            if config.get("seed") is not None:
                # Tf2.x.
                if config.get("framework") == "tf2":
                    tf.random.set_seed(config["seed"])
                # Tf-eager.
                elif tf1 and config.get("framework") == "tfe":
                    tf1.set_random_seed(config["seed"])

        # Policy network.
        self.dist_class, dist_dim = ModelCatalog.get_action_dist(
            self.action_space, self.config["model"], dist_type="deterministic")

        self.model = ModelCatalog.get_model_v2(
            obs_space=self.preprocessor.observation_space,
            action_space=self.action_space,
            num_outputs=dist_dim,
            model_config=self.config["model"])

        self.sampler = None
        if self.sess:
            dist_inputs, _ = self.model({SampleBatch.CUR_OBS: self.inputs})
            dist = self.dist_class(dist_inputs, self.model)
            self.sampler = dist.sample()
            self.variables = ray.experimental.tf_utils.TensorFlowVariables(
                dist_inputs, self.sess)
            self.sess.run(tf1.global_variables_initializer())
        else:
            self.variables = ray.experimental.tf_utils.TensorFlowVariables(
                [], None, self.model.variables())

        self.num_params = sum(
            np.prod(variable.shape.as_list())
            for _, variable in self.variables.variables.items())

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag521')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/es/es_tf_policy.py: 70-125
</a>
<div class="mid" id="frag521" style="display:none"><pre>

class ESTFPolicy(Policy):
    def __init__(self, obs_space, action_space, config):
        super().__init__(obs_space, action_space, config)
        self.action_space_struct = get_base_struct_from_space(action_space)
        self.action_noise_std = self.config["action_noise_std"]
        self.preprocessor = ModelCatalog.get_preprocessor_for_space(obs_space)
        self.observation_filter = get_filter(self.config["observation_filter"],
                                             self.preprocessor.shape)
        self.single_threaded = self.config.get("single_threaded", False)
        if self.config["framework"] == "tf":
            self.sess = make_session(single_threaded=self.single_threaded)

            # Set graph-level seed.
            if config.get("seed") is not None:
                with self.sess.as_default():
                    tf1.set_random_seed(config["seed"])

            self.inputs = tf1.placeholder(
                tf.float32, [None] + list(self.preprocessor.shape))
        else:
            if not tf1.executing_eagerly():
                tf1.enable_eager_execution()
            self.sess = self.inputs = None
            if config.get("seed") is not None:
                # Tf2.x.
                if config.get("framework") == "tf2":
                    tf.random.set_seed(config["seed"])
                # Tf-eager.
                elif tf1 and config.get("framework") == "tfe":
                    tf1.set_random_seed(config["seed"])

        # Policy network.
        self.dist_class, dist_dim = ModelCatalog.get_action_dist(
            self.action_space, self.config["model"], dist_type="deterministic")

        self.model = ModelCatalog.get_model_v2(
            obs_space=self.preprocessor.observation_space,
            action_space=action_space,
            num_outputs=dist_dim,
            model_config=self.config["model"])

        self.sampler = None
        if self.sess:
            dist_inputs, _ = self.model({SampleBatch.CUR_OBS: self.inputs})
            dist = self.dist_class(dist_inputs, self.model)
            self.sampler = dist.sample()
            self.variables = ray.experimental.tf_utils.TensorFlowVariables(
                dist_inputs, self.sess)
            self.sess.run(tf1.global_variables_initializer())
        else:
            self.variables = ray.experimental.tf_utils.TensorFlowVariables(
                [], None, self.model.variables())

        self.num_params = sum(
            np.prod(variable.shape.as_list())
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 6:</b> &nbsp; 2 fragments, nominal size 21 lines, similarity 85%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag171')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/ars/ars_tf_policy.py: 79-106
</a>
<div class="mid" id="frag171" style="display:none"><pre>
    def compute_actions(self,
                        observation,
                        add_noise=False,
                        update=True,
                        **kwargs):
        # Squeeze batch dimension (we always calculate actions for only a
        # single obs).
        observation = observation[0]
        observation = self.preprocessor.transform(observation)
        observation = self.observation_filter(observation[None], update=update)

        # `actions` is a list of (component) batches.
        # Eager mode.
        if not self.sess:
            dist_inputs, _ = self.model({SampleBatch.CUR_OBS: observation})
            dist = self.dist_class(dist_inputs, self.model)
            actions = dist.sample()
            actions = tree.map_structure(lambda a: a.numpy(), actions)
        # Graph mode.
        else:
            actions = self.sess.run(
                self.sampler, feed_dict={self.inputs: observation})

        actions = unbatch(actions)
        if add_noise and isinstance(self.action_space, gym.spaces.Box):
            actions += np.random.randn(*actions.shape) * self.action_noise_std
        return actions, [], {}

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag522')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/es/es_tf_policy.py: 127-156
</a>
<div class="mid" id="frag522" style="display:none"><pre>

    @override(Policy)
    def compute_actions(self,
                        observation,
                        add_noise=False,
                        update=True,
                        **kwargs):
        # Squeeze batch dimension (we always calculate actions for only a
        # single obs).
        observation = observation[0]
        observation = self.preprocessor.transform(observation)
        observation = self.observation_filter(observation[None], update=update)
        # `actions` is a list of (component) batches.
        # Eager mode.
        if not self.sess:
            dist_inputs, _ = self.model({SampleBatch.CUR_OBS: observation})
            dist = self.dist_class(dist_inputs, self.model)
            actions = dist.sample()
            actions = tree.map_structure(lambda a: a.numpy(), actions)
        # Graph mode.
        else:
            actions = self.sess.run(
                self.sampler, feed_dict={self.inputs: observation})

        if add_noise:
            actions = tree.map_structure(self._add_noise, actions,
                                         self.action_space_struct)
        # Convert `flat_actions` to a list of lists of action components
        # (list of single actions).
        actions = unbatch(actions)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 7:</b> &nbsp; 2 fragments, nominal size 22 lines, similarity 95%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag206')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/impala/impala.py: 132-169
</a>
<div class="mid" id="frag206" style="display:none"><pre>
    def default_resource_request(cls, config):
        cf = dict(cls._default_config, **config)

        eval_config = cf["evaluation_config"]

        # Return PlacementGroupFactory containing all needed resources
        # (already properly defined as device bundles).
        return PlacementGroupFactory(
            bundles=[{
                # Driver + Aggregation Workers:
                # Force to be on same node to maximize data bandwidth
                # between aggregation workers and the learner (driver).
                # Aggregation workers tree-aggregate experiences collected
                # from RolloutWorkers (n rollout workers map to m
                # aggregation workers, where m &lt; n) and always use 1 CPU
                # each.
                "CPU": cf["num_cpus_for_driver"] +
                cf["num_aggregation_workers"],
                "GPU": 0 if cf["_fake_gpus"] else cf["num_gpus"],
            }] + [
                {
                    # RolloutWorkers.
                    "CPU": cf["num_cpus_per_worker"],
                    "GPU": cf["num_gpus_per_worker"],
                } for _ in range(cf["num_workers"])
            ] + ([
                {
                    # Evaluation (remote) workers.
                    # Note: The local eval worker is located on the driver CPU.
                    "CPU": eval_config.get("num_cpus_per_worker",
                                           cf["num_cpus_per_worker"]),
                    "GPU": eval_config.get("num_gpus_per_worker",
                                           cf["num_gpus_per_worker"]),
                } for _ in range(cf["evaluation_num_workers"])
            ] if cf["evaluation_interval"] else []),
            strategy=config.get("placement_strategy", "PACK"))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag608')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/dqn/apex.py: 81-117
</a>
<div class="mid" id="frag608" style="display:none"><pre>
    @override(Trainable)
    def default_resource_request(cls, config):
        cf = dict(cls._default_config, **config)

        eval_config = cf["evaluation_config"]

        # Return PlacementGroupFactory containing all needed resources
        # (already properly defined as device bundles).
        return PlacementGroupFactory(
            bundles=[{
                # Local worker + replay buffer actors.
                # Force replay buffers to be on same node to maximize
                # data bandwidth between buffers and the learner (driver).
                # Replay buffer actors each contain one shard of the total
                # replay buffer and use 1 CPU each.
                "CPU": cf["num_cpus_for_driver"] +
                cf["optimizer"]["num_replay_buffer_shards"],
                "GPU": 0 if cf["_fake_gpus"] else cf["num_gpus"],
            }] + [
                {
                    # RolloutWorkers.
                    "CPU": cf["num_cpus_per_worker"],
                    "GPU": cf["num_gpus_per_worker"],
                } for _ in range(cf["num_workers"])
            ] + ([
                {
                    # Evaluation workers.
                    # Note: The local eval worker is located on the driver CPU.
                    "CPU": eval_config.get("num_cpus_per_worker",
                                           cf["num_cpus_per_worker"]),
                    "GPU": eval_config.get("num_gpus_per_worker",
                                           cf["num_gpus_per_worker"]),
                } for _ in range(cf["evaluation_num_workers"])
            ] if cf["evaluation_interval"] else []),
            strategy=config.get("placement_strategy", "PACK"))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 8:</b> &nbsp; 2 fragments, nominal size 70 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag217')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/impala/vtrace_tf_policy.py: 162-233
</a>
<div class="mid" id="frag217" style="display:none"><pre>

def build_vtrace_loss(policy, model, dist_class, train_batch):
    model_out, _ = model(train_batch)
    action_dist = dist_class(model_out, model)

    if isinstance(policy.action_space, gym.spaces.Discrete):
        is_multidiscrete = False
        output_hidden_shape = [policy.action_space.n]
    elif isinstance(policy.action_space, gym.spaces.MultiDiscrete):
        is_multidiscrete = True
        output_hidden_shape = policy.action_space.nvec.astype(np.int32)
    else:
        is_multidiscrete = False
        output_hidden_shape = 1

    def make_time_major(*args, **kw):
        return _make_time_major(policy, train_batch.get(SampleBatch.SEQ_LENS),
                                *args, **kw)

    actions = train_batch[SampleBatch.ACTIONS]
    dones = train_batch[SampleBatch.DONES]
    rewards = train_batch[SampleBatch.REWARDS]
    behaviour_action_logp = train_batch[SampleBatch.ACTION_LOGP]
    behaviour_logits = train_batch[SampleBatch.ACTION_DIST_INPUTS]
    unpacked_behaviour_logits = tf.split(
        behaviour_logits, output_hidden_shape, axis=1)
    unpacked_outputs = tf.split(model_out, output_hidden_shape, axis=1)
    values = model.value_function()

    if policy.is_recurrent():
        max_seq_len = tf.reduce_max(train_batch[SampleBatch.SEQ_LENS])
        mask = tf.sequence_mask(train_batch[SampleBatch.SEQ_LENS], max_seq_len)
        mask = tf.reshape(mask, [-1])
    else:
        mask = tf.ones_like(rewards)

    # Prepare actions for loss
    loss_actions = actions if is_multidiscrete else tf.expand_dims(
        actions, axis=1)

    # Inputs are reshaped from [B * T] =&gt; [(T|T-1), B] for V-trace calc.
    drop_last = policy.config["vtrace_drop_last_ts"]
    policy.loss = VTraceLoss(
        actions=make_time_major(loss_actions, drop_last=drop_last),
        actions_logp=make_time_major(
            action_dist.logp(actions), drop_last=drop_last),
        actions_entropy=make_time_major(
            action_dist.multi_entropy(), drop_last=drop_last),
        dones=make_time_major(dones, drop_last=drop_last),
        behaviour_action_logp=make_time_major(
            behaviour_action_logp, drop_last=drop_last),
        behaviour_logits=make_time_major(
            unpacked_behaviour_logits, drop_last=drop_last),
        target_logits=make_time_major(unpacked_outputs, drop_last=drop_last),
        discount=policy.config["gamma"],
        rewards=make_time_major(rewards, drop_last=drop_last),
        values=make_time_major(values, drop_last=drop_last),
        bootstrap_value=make_time_major(values)[-1],
        dist_class=Categorical if is_multidiscrete else dist_class,
        model=model,
        valid_mask=make_time_major(mask, drop_last=drop_last),
        config=policy.config,
        vf_loss_coeff=policy.config["vf_loss_coeff"],
        entropy_coeff=policy.entropy_coeff,
        clip_rho_threshold=policy.config["vtrace_clip_rho_threshold"],
        clip_pg_rho_threshold=policy.config["vtrace_clip_pg_rho_threshold"])

    if policy.config.get("_separate_vf_optimizer"):
        return policy.loss.loss_wo_vf, policy.loss.vf_loss
    else:
        return policy.loss.total_loss

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag243')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/impala/vtrace_torch_policy.py: 113-205
</a>
<div class="mid" id="frag243" style="display:none"><pre>


def build_vtrace_loss(policy, model, dist_class, train_batch):
    model_out, _ = model(train_batch)
    action_dist = dist_class(model_out, model)

    if isinstance(policy.action_space, gym.spaces.Discrete):
        is_multidiscrete = False
        output_hidden_shape = [policy.action_space.n]
    elif isinstance(policy.action_space, gym.spaces.MultiDiscrete):
        is_multidiscrete = True
        output_hidden_shape = policy.action_space.nvec.astype(np.int32)
    else:
        is_multidiscrete = False
        output_hidden_shape = 1

    def _make_time_major(*args, **kw):
        return make_time_major(policy, train_batch.get(SampleBatch.SEQ_LENS),
                               *args, **kw)

    actions = train_batch[SampleBatch.ACTIONS]
    dones = train_batch[SampleBatch.DONES]
    rewards = train_batch[SampleBatch.REWARDS]
    behaviour_action_logp = train_batch[SampleBatch.ACTION_LOGP]
    behaviour_logits = train_batch[SampleBatch.ACTION_DIST_INPUTS]
    if isinstance(output_hidden_shape, (list, tuple, np.ndarray)):
        unpacked_behaviour_logits = torch.split(
            behaviour_logits, list(output_hidden_shape), dim=1)
        unpacked_outputs = torch.split(
            model_out, list(output_hidden_shape), dim=1)
    else:
        unpacked_behaviour_logits = torch.chunk(
            behaviour_logits, output_hidden_shape, dim=1)
        unpacked_outputs = torch.chunk(model_out, output_hidden_shape, dim=1)
    values = model.value_function()

    if policy.is_recurrent():
        max_seq_len = torch.max(train_batch[SampleBatch.SEQ_LENS])
        mask_orig = sequence_mask(train_batch[SampleBatch.SEQ_LENS],
                                  max_seq_len)
        mask = torch.reshape(mask_orig, [-1])
    else:
        mask = torch.ones_like(rewards)

    # Prepare actions for loss.
    loss_actions = actions if is_multidiscrete else torch.unsqueeze(
        actions, dim=1)

    # Inputs are reshaped from [B * T] =&gt; [(T|T-1), B] for V-trace calc.
    drop_last = policy.config["vtrace_drop_last_ts"]
    loss = VTraceLoss(
        actions=_make_time_major(loss_actions, drop_last=drop_last),
        actions_logp=_make_time_major(
            action_dist.logp(actions), drop_last=drop_last),
        actions_entropy=_make_time_major(
            action_dist.entropy(), drop_last=drop_last),
        dones=_make_time_major(dones, drop_last=drop_last),
        behaviour_action_logp=_make_time_major(
            behaviour_action_logp, drop_last=drop_last),
        behaviour_logits=_make_time_major(
            unpacked_behaviour_logits, drop_last=drop_last),
        target_logits=_make_time_major(unpacked_outputs, drop_last=drop_last),
        discount=policy.config["gamma"],
        rewards=_make_time_major(rewards, drop_last=drop_last),
        values=_make_time_major(values, drop_last=drop_last),
        bootstrap_value=_make_time_major(values)[-1],
        dist_class=TorchCategorical if is_multidiscrete else dist_class,
        model=model,
        valid_mask=_make_time_major(mask, drop_last=drop_last),
        config=policy.config,
        vf_loss_coeff=policy.config["vf_loss_coeff"],
        entropy_coeff=policy.entropy_coeff,
        clip_rho_threshold=policy.config["vtrace_clip_rho_threshold"],
        clip_pg_rho_threshold=policy.config["vtrace_clip_pg_rho_threshold"])

    # Store values for stats function in model (tower), such that for
    # multi-GPU, we do not override them during the parallel loss phase.
    model.tower_stats["pi_loss"] = loss.pi_loss
    model.tower_stats["vf_loss"] = loss.vf_loss
    model.tower_stats["entropy"] = loss.entropy
    model.tower_stats["mean_entropy"] = loss.mean_entropy
    model.tower_stats["total_loss"] = loss.total_loss

    values_batched = make_time_major(
        policy,
        train_batch.get(SampleBatch.SEQ_LENS),
        values,
        drop_last=policy.config["vtrace"] and drop_last)
    model.tower_stats["vf_explained_var"] = explained_variance(
        torch.reshape(loss.value_targets, [-1]),
        torch.reshape(values_batched, [-1]))

    return loss.total_loss
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 9:</b> &nbsp; 2 fragments, nominal size 31 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag226')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/impala/vtrace_tf.py: 94-130
</a>
<div class="mid" id="frag226" style="display:none"><pre>
def from_logits(behaviour_policy_logits,
                target_policy_logits,
                actions,
                discounts,
                rewards,
                values,
                bootstrap_value,
                dist_class=Categorical,
                model=None,
                clip_rho_threshold=1.0,
                clip_pg_rho_threshold=1.0,
                name="vtrace_from_logits"):
    """multi_from_logits wrapper used only for tests"""

    res = multi_from_logits(
        [behaviour_policy_logits], [target_policy_logits], [actions],
        discounts,
        rewards,
        values,
        bootstrap_value,
        dist_class,
        model,
        clip_rho_threshold=clip_rho_threshold,
        clip_pg_rho_threshold=clip_pg_rho_threshold,
        name=name)

    return VTraceFromLogitsReturns(
        vs=res.vs,
        pg_advantages=res.pg_advantages,
        log_rhos=res.log_rhos,
        behaviour_action_log_probs=tf.squeeze(
            res.behaviour_action_log_probs, axis=0),
        target_action_log_probs=tf.squeeze(
            res.target_action_log_probs, axis=0),
    )


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag238')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/impala/vtrace_torch.py: 88-122
</a>
<div class="mid" id="frag238" style="display:none"><pre>

def from_logits(behaviour_policy_logits,
                target_policy_logits,
                actions,
                discounts,
                rewards,
                values,
                bootstrap_value,
                dist_class=TorchCategorical,
                model=None,
                clip_rho_threshold=1.0,
                clip_pg_rho_threshold=1.0):
    """multi_from_logits wrapper used only for tests"""

    res = multi_from_logits(
        [behaviour_policy_logits], [target_policy_logits], [actions],
        discounts,
        rewards,
        values,
        bootstrap_value,
        dist_class,
        model,
        clip_rho_threshold=clip_rho_threshold,
        clip_pg_rho_threshold=clip_pg_rho_threshold)

    assert len(res.behaviour_action_log_probs) == 1
    assert len(res.target_action_log_probs) == 1
    return VTraceFromLogitsReturns(
        vs=res.vs,
        pg_advantages=res.pg_advantages,
        log_rhos=res.log_rhos,
        behaviour_action_log_probs=res.behaviour_action_log_probs[0],
        target_action_log_probs=res.target_action_log_probs[0],
    )

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 10:</b> &nbsp; 2 fragments, nominal size 49 lines, similarity 89%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag253')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/maml/maml_torch_policy.py: 17-76
</a>
<div class="mid" id="frag253" style="display:none"><pre>


def PPOLoss(dist_class,
            actions,
            curr_logits,
            behaviour_logits,
            advantages,
            value_fn,
            value_targets,
            vf_preds,
            cur_kl_coeff,
            entropy_coeff,
            clip_param,
            vf_clip_param,
            vf_loss_coeff,
            clip_loss=False):
    def surrogate_loss(actions, curr_dist, prev_dist, advantages, clip_param,
                       clip_loss):
        pi_new_logp = curr_dist.logp(actions)
        pi_old_logp = prev_dist.logp(actions)

        logp_ratio = torch.exp(pi_new_logp - pi_old_logp)
        if clip_loss:
            return torch.min(
                advantages * logp_ratio,
                advantages * torch.clamp(logp_ratio, 1 - clip_param,
                                         1 + clip_param))
        return advantages * logp_ratio

    def kl_loss(curr_dist, prev_dist):
        return prev_dist.kl(curr_dist)

    def entropy_loss(dist):
        return dist.entropy()

    def vf_loss(value_fn, value_targets, vf_preds, vf_clip_param=0.1):
        # GAE Value Function Loss
        vf_loss1 = torch.pow(value_fn - value_targets, 2.0)
        vf_clipped = vf_preds + torch.clamp(value_fn - vf_preds,
                                            -vf_clip_param, vf_clip_param)
        vf_loss2 = torch.pow(vf_clipped - value_targets, 2.0)
        vf_loss = torch.max(vf_loss1, vf_loss2)
        return vf_loss

    pi_new_dist = dist_class(curr_logits, None)
    pi_old_dist = dist_class(behaviour_logits, None)

    surr_loss = torch.mean(
        surrogate_loss(actions, pi_new_dist, pi_old_dist, advantages,
                       clip_param, clip_loss))
    kl_loss = torch.mean(kl_loss(pi_new_dist, pi_old_dist))
    vf_loss = torch.mean(
        vf_loss(value_fn, value_targets, vf_preds, vf_clip_param))
    entropy_loss = torch.mean(entropy_loss(pi_new_dist))

    total_loss = -surr_loss + cur_kl_coeff * kl_loss
    total_loss += vf_loss_coeff * vf_loss
    total_loss -= entropy_coeff * entropy_loss
    return total_loss, surr_loss, kl_loss, vf_loss, entropy_loss

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag281')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/maml/maml_tf_policy.py: 16-74
</a>
<div class="mid" id="frag281" style="display:none"><pre>


def PPOLoss(dist_class,
            actions,
            curr_logits,
            behaviour_logits,
            advantages,
            value_fn,
            value_targets,
            vf_preds,
            cur_kl_coeff,
            entropy_coeff,
            clip_param,
            vf_clip_param,
            vf_loss_coeff,
            clip_loss=False):
    def surrogate_loss(actions, curr_dist, prev_dist, advantages, clip_param,
                       clip_loss):
        pi_new_logp = curr_dist.logp(actions)
        pi_old_logp = prev_dist.logp(actions)

        logp_ratio = tf.math.exp(pi_new_logp - pi_old_logp)
        if clip_loss:
            return tf.minimum(
                advantages * logp_ratio,
                advantages * tf.clip_by_value(logp_ratio, 1 - clip_param,
                                              1 + clip_param))
        return advantages * logp_ratio

    def kl_loss(curr_dist, prev_dist):
        return prev_dist.kl(curr_dist)

    def entropy_loss(dist):
        return dist.entropy()

    def vf_loss(value_fn, value_targets, vf_preds, vf_clip_param=0.1):
        # GAE Value Function Loss
        vf_loss1 = tf.math.square(value_fn - value_targets)
        vf_clipped = vf_preds + tf.clip_by_value(value_fn - vf_preds,
                                                 -vf_clip_param, vf_clip_param)
        vf_loss2 = tf.math.square(vf_clipped - value_targets)
        vf_loss = tf.maximum(vf_loss1, vf_loss2)
        return vf_loss

    pi_new_dist = dist_class(curr_logits, None)
    pi_old_dist = dist_class(behaviour_logits, None)

    surr_loss = tf.reduce_mean(
        surrogate_loss(actions, pi_new_dist, pi_old_dist, advantages,
                       clip_param, clip_loss))
    kl_loss = tf.reduce_mean(kl_loss(pi_new_dist, pi_old_dist))
    vf_loss = tf.reduce_mean(
        vf_loss(value_fn, value_targets, vf_preds, vf_clip_param))
    entropy_loss = tf.reduce_mean(entropy_loss(pi_new_dist))

    total_loss = -surr_loss + cur_kl_coeff * kl_loss
    total_loss += vf_loss_coeff * vf_loss - entropy_coeff * entropy_loss
    return total_loss, surr_loss, kl_loss, vf_loss, entropy_loss

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 11:</b> &nbsp; 2 fragments, nominal size 31 lines, similarity 96%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag258')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/maml/maml_torch_policy.py: 78-111
</a>
<div class="mid" id="frag258" style="display:none"><pre>
# This is the computation graph for workers (inner adaptation steps)
class WorkerLoss(object):
    def __init__(self,
                 model,
                 dist_class,
                 actions,
                 curr_logits,
                 behaviour_logits,
                 advantages,
                 value_fn,
                 value_targets,
                 vf_preds,
                 cur_kl_coeff,
                 entropy_coeff,
                 clip_param,
                 vf_clip_param,
                 vf_loss_coeff,
                 clip_loss=False):
        self.loss, surr_loss, kl_loss, vf_loss, ent_loss = PPOLoss(
            dist_class=dist_class,
            actions=actions,
            curr_logits=curr_logits,
            behaviour_logits=behaviour_logits,
            advantages=advantages,
            value_fn=value_fn,
            value_targets=value_targets,
            vf_preds=vf_preds,
            cur_kl_coeff=cur_kl_coeff,
            entropy_coeff=entropy_coeff,
            clip_param=clip_param,
            vf_clip_param=vf_clip_param,
            vf_loss_coeff=vf_loss_coeff,
            clip_loss=clip_loss)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag286')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/maml/maml_tf_policy.py: 76-109
</a>
<div class="mid" id="frag286" style="display:none"><pre>
# This is the computation graph for workers (inner adaptation steps)
class WorkerLoss(object):
    def __init__(self,
                 dist_class,
                 actions,
                 curr_logits,
                 behaviour_logits,
                 advantages,
                 value_fn,
                 value_targets,
                 vf_preds,
                 cur_kl_coeff,
                 entropy_coeff,
                 clip_param,
                 vf_clip_param,
                 vf_loss_coeff,
                 clip_loss=False):
        self.loss, surr_loss, kl_loss, vf_loss, ent_loss = PPOLoss(
            dist_class=dist_class,
            actions=actions,
            curr_logits=curr_logits,
            behaviour_logits=behaviour_logits,
            advantages=advantages,
            value_fn=value_fn,
            value_targets=value_targets,
            vf_preds=vf_preds,
            cur_kl_coeff=cur_kl_coeff,
            entropy_coeff=entropy_coeff,
            clip_param=clip_param,
            vf_clip_param=vf_clip_param,
            vf_loss_coeff=vf_loss_coeff,
            clip_loss=clip_loss)
        self.loss = tf1.Print(self.loss, ["Worker Adapt Loss", self.loss])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 12:</b> &nbsp; 2 fragments, nominal size 48 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag262')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/maml/maml_torch_policy.py: 246-304
</a>
<div class="mid" id="frag262" style="display:none"><pre>


def maml_loss(policy, model, dist_class, train_batch):
    logits, state = model(train_batch)
    policy.cur_lr = policy.config["lr"]

    if policy.config["worker_index"]:
        policy.loss_obj = WorkerLoss(
            model=model,
            dist_class=dist_class,
            actions=train_batch[SampleBatch.ACTIONS],
            curr_logits=logits,
            behaviour_logits=train_batch[SampleBatch.ACTION_DIST_INPUTS],
            advantages=train_batch[Postprocessing.ADVANTAGES],
            value_fn=model.value_function(),
            value_targets=train_batch[Postprocessing.VALUE_TARGETS],
            vf_preds=train_batch[SampleBatch.VF_PREDS],
            cur_kl_coeff=0.0,
            entropy_coeff=policy.config["entropy_coeff"],
            clip_param=policy.config["clip_param"],
            vf_clip_param=policy.config["vf_clip_param"],
            vf_loss_coeff=policy.config["vf_loss_coeff"],
            clip_loss=False)
    else:
        policy.var_list = model.named_parameters()

        # `split` may not exist yet (during test-loss call), use a dummy value.
        # Cannot use get here due to train_batch being a TrackingDict.
        if "split" in train_batch:
            split = train_batch["split"]
        else:
            split_shape = (policy.config["inner_adaptation_steps"],
                           policy.config["num_workers"])
            split_const = int(train_batch["obs"].shape[0] //
                              (split_shape[0] * split_shape[1]))
            split = torch.ones(split_shape, dtype=int) * split_const
        policy.loss_obj = MAMLLoss(
            model=model,
            dist_class=dist_class,
            value_targets=train_batch[Postprocessing.VALUE_TARGETS],
            advantages=train_batch[Postprocessing.ADVANTAGES],
            actions=train_batch[SampleBatch.ACTIONS],
            behaviour_logits=train_batch[SampleBatch.ACTION_DIST_INPUTS],
            vf_preds=train_batch[SampleBatch.VF_PREDS],
            cur_kl_coeff=policy.kl_coeff_val,
            policy_vars=policy.var_list,
            obs=train_batch[SampleBatch.CUR_OBS],
            num_tasks=policy.config["num_workers"],
            split=split,
            config=policy.config,
            inner_adaptation_steps=policy.config["inner_adaptation_steps"],
            entropy_coeff=policy.config["entropy_coeff"],
            clip_param=policy.config["clip_param"],
            vf_clip_param=policy.config["vf_clip_param"],
            vf_loss_coeff=policy.config["vf_loss_coeff"],
            use_gae=policy.config["use_gae"],
            meta_opt=policy.meta_opt)

    return policy.loss_obj.loss
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag292')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/maml/maml_tf_policy.py: 308-354
</a>
<div class="mid" id="frag292" style="display:none"><pre>


def maml_loss(policy, model, dist_class, train_batch):
    logits, state = model(train_batch)
    policy.cur_lr = policy.config["lr"]

    if policy.config["worker_index"]:
        policy.loss_obj = WorkerLoss(
            dist_class=dist_class,
            actions=train_batch[SampleBatch.ACTIONS],
            curr_logits=logits,
            behaviour_logits=train_batch[SampleBatch.ACTION_DIST_INPUTS],
            advantages=train_batch[Postprocessing.ADVANTAGES],
            value_fn=model.value_function(),
            value_targets=train_batch[Postprocessing.VALUE_TARGETS],
            vf_preds=train_batch[SampleBatch.VF_PREDS],
            cur_kl_coeff=0.0,
            entropy_coeff=policy.config["entropy_coeff"],
            clip_param=policy.config["clip_param"],
            vf_clip_param=policy.config["vf_clip_param"],
            vf_loss_coeff=policy.config["vf_loss_coeff"],
            clip_loss=False)
    else:
        policy.var_list = tf1.get_collection(tf1.GraphKeys.TRAINABLE_VARIABLES,
                                             tf1.get_variable_scope().name)
        policy.loss_obj = MAMLLoss(
            model=model,
            dist_class=dist_class,
            value_targets=train_batch[Postprocessing.VALUE_TARGETS],
            advantages=train_batch[Postprocessing.ADVANTAGES],
            actions=train_batch[SampleBatch.ACTIONS],
            behaviour_logits=train_batch[SampleBatch.ACTION_DIST_INPUTS],
            vf_preds=train_batch[SampleBatch.VF_PREDS],
            cur_kl_coeff=policy.kl_coeff,
            policy_vars=policy.var_list,
            obs=train_batch[SampleBatch.CUR_OBS],
            num_tasks=policy.config["num_workers"],
            split=train_batch["split"],
            config=policy.config,
            inner_adaptation_steps=policy.config["inner_adaptation_steps"],
            entropy_coeff=policy.config["entropy_coeff"],
            clip_param=policy.config["clip_param"],
            vf_clip_param=policy.config["vf_clip_param"],
            vf_loss_coeff=policy.config["vf_loss_coeff"],
            use_gae=policy.config["use_gae"])

    return policy.loss_obj.loss
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 13:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 84%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag263')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/maml/maml_torch_policy.py: 305-320
</a>
<div class="mid" id="frag263" style="display:none"><pre>


def maml_stats(policy, train_batch):
    if policy.config["worker_index"]:
        return {"worker_loss": policy.loss_obj.loss}
    else:
        return {
            "cur_kl_coeff": policy.kl_coeff_val,
            "cur_lr": policy.cur_lr,
            "total_loss": policy.loss_obj.loss,
            "policy_loss": policy.loss_obj.mean_policy_loss,
            "vf_loss": policy.loss_obj.mean_vf_loss,
            "kl_loss": policy.loss_obj.mean_kl_loss,
            "inner_kl": policy.loss_obj.mean_inner_kl,
            "entropy": policy.loss_obj.mean_entropy,
        }
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag293')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/maml/maml_tf_policy.py: 355-370
</a>
<div class="mid" id="frag293" style="display:none"><pre>


def maml_stats(policy, train_batch):
    if policy.config["worker_index"]:
        return {"worker_loss": policy.loss_obj.loss}
    else:
        return {
            "cur_kl_coeff": tf.cast(policy.kl_coeff, tf.float64),
            "cur_lr": tf.cast(policy.cur_lr, tf.float64),
            "total_loss": policy.loss_obj.loss,
            "policy_loss": policy.loss_obj.mean_policy_loss,
            "vf_loss": policy.loss_obj.mean_vf_loss,
            "kl": policy.loss_obj.mean_kl,
            "inner_kl": policy.loss_obj.mean_inner_kl,
            "entropy": policy.loss_obj.mean_entropy,
        }
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 14:</b> &nbsp; 2 fragments, nominal size 50 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag322')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/sac/sac_tf_model.py: 41-115
</a>
<div class="mid" id="frag322" style="display:none"><pre>
    def __init__(self,
                 obs_space: gym.spaces.Space,
                 action_space: gym.spaces.Space,
                 num_outputs: Optional[int],
                 model_config: ModelConfigDict,
                 name: str,
                 policy_model_config: ModelConfigDict = None,
                 q_model_config: ModelConfigDict = None,
                 twin_q: bool = False,
                 initial_alpha: float = 1.0,
                 target_entropy: Optional[float] = None):
        """Initialize a SACTFModel instance.

        Args:
            policy_model_config (ModelConfigDict): The config dict for the
                policy network.
            q_model_config (ModelConfigDict): The config dict for the
                Q-network(s) (2 if twin_q=True).
            twin_q (bool): Build twin Q networks (Q-net and target) for more
                stable Q-learning.
            initial_alpha (float): The initial value for the to-be-optimized
                alpha parameter (default: 1.0).
            target_entropy (Optional[float]): A target entropy value for
                the to-be-optimized alpha parameter. If None, will use the
                defaults described in the papers for SAC (and discrete SAC).

        Note that the core layers for forward() are not defined here, this
        only defines the layers for the output heads. Those layers for
        forward() should be defined in subclasses of SACModel.
        """
        super(SACTFModel, self).__init__(obs_space, action_space, num_outputs,
                                         model_config, name)
        if isinstance(action_space, Discrete):
            self.action_dim = action_space.n
            self.discrete = True
            action_outs = q_outs = self.action_dim
        elif isinstance(action_space, Box):
            self.action_dim = np.product(action_space.shape)
            self.discrete = False
            action_outs = 2 * self.action_dim
            q_outs = 1
        else:
            assert isinstance(action_space, Simplex)
            self.action_dim = np.product(action_space.shape)
            self.discrete = False
            action_outs = self.action_dim
            q_outs = 1

        self.action_model = self.build_policy_model(
            self.obs_space, action_outs, policy_model_config, "policy_model")

        self.q_net = self.build_q_model(self.obs_space, self.action_space,
                                        q_outs, q_model_config, "q")
        if twin_q:
            self.twin_q_net = self.build_q_model(self.obs_space,
                                                 self.action_space, q_outs,
                                                 q_model_config, "twin_q")
        else:
            self.twin_q_net = None

        self.log_alpha = tf.Variable(
            np.log(initial_alpha), dtype=tf.float32, name="log_alpha")
        self.alpha = tf.exp(self.log_alpha)

        # Auto-calculate the target entropy.
        if target_entropy is None or target_entropy == "auto":
            # See hyperparams in [2] (README.md).
            if self.discrete:
                target_entropy = 0.98 * np.array(
                    -np.log(1.0 / action_space.n), dtype=np.float32)
            # See [1] (README.md).
            else:
                target_entropy = -np.prod(action_space.shape)
        self.target_entropy = target_entropy

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag342')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/sac/sac_torch_model.py: 41-123
</a>
<div class="mid" id="frag342" style="display:none"><pre>
    def __init__(self,
                 obs_space: gym.spaces.Space,
                 action_space: gym.spaces.Space,
                 num_outputs: Optional[int],
                 model_config: ModelConfigDict,
                 name: str,
                 policy_model_config: ModelConfigDict = None,
                 q_model_config: ModelConfigDict = None,
                 twin_q: bool = False,
                 initial_alpha: float = 1.0,
                 target_entropy: Optional[float] = None):
        """Initializes a SACTorchModel instance.
7
        Args:
            policy_model_config (ModelConfigDict): The config dict for the
                policy network.
            q_model_config (ModelConfigDict): The config dict for the
                Q-network(s) (2 if twin_q=True).
            twin_q (bool): Build twin Q networks (Q-net and target) for more
                stable Q-learning.
            initial_alpha (float): The initial value for the to-be-optimized
                alpha parameter (default: 1.0).
            target_entropy (Optional[float]): A target entropy value for
                the to-be-optimized alpha parameter. If None, will use the
                defaults described in the papers for SAC (and discrete SAC).

        Note that the core layers for forward() are not defined here, this
        only defines the layers for the output heads. Those layers for
        forward() should be defined in subclasses of SACModel.
        """
        nn.Module.__init__(self)
        super(SACTorchModel, self).__init__(obs_space, action_space,
                                            num_outputs, model_config, name)

        if isinstance(action_space, Discrete):
            self.action_dim = action_space.n
            self.discrete = True
            action_outs = q_outs = self.action_dim
        elif isinstance(action_space, Box):
            self.action_dim = np.product(action_space.shape)
            self.discrete = False
            action_outs = 2 * self.action_dim
            q_outs = 1
        else:
            assert isinstance(action_space, Simplex)
            self.action_dim = np.product(action_space.shape)
            self.discrete = False
            action_outs = self.action_dim
            q_outs = 1

        # Build the policy network.
        self.action_model = self.build_policy_model(
            self.obs_space, action_outs, policy_model_config, "policy_model")

        # Build the Q-network(s).
        self.q_net = self.build_q_model(self.obs_space, self.action_space,
                                        q_outs, q_model_config, "q")
        if twin_q:
            self.twin_q_net = self.build_q_model(self.obs_space,
                                                 self.action_space, q_outs,
                                                 q_model_config, "twin_q")
        else:
            self.twin_q_net = None

        log_alpha = nn.Parameter(
            torch.from_numpy(np.array([np.log(initial_alpha)])).float())
        self.register_parameter("log_alpha", log_alpha)

        # Auto-calculate the target entropy.
        if target_entropy is None or target_entropy == "auto":
            # See hyperparams in [2] (README.md).
            if self.discrete:
                target_entropy = 0.98 * np.array(
                    -np.log(1.0 / action_space.n), dtype=np.float32)
            # See [1] (README.md).
            else:
                target_entropy = -np.prod(action_space.shape)

        target_entropy = nn.Parameter(
            torch.from_numpy(np.array([target_entropy])).float(),
            requires_grad=False)
        self.register_parameter("target_entropy", target_entropy)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 15:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag324')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/sac/sac_tf_model.py: 128-148
</a>
<div class="mid" id="frag324" style="display:none"><pre>
    def build_policy_model(self, obs_space, num_outputs, policy_model_config,
                           name):
        """Builds the policy model used by this SAC.

        Override this method in a sub-class of SACTFModel to implement your
        own policy net. Alternatively, simply set `custom_model` within the
        top level SAC `policy_model` config key to make this default
        implementation of `build_policy_model` use your custom policy network.

        Returns:
            TFModelV2: The TFModelV2 policy sub-model.
        """
        model = ModelCatalog.get_model_v2(
            obs_space,
            self.action_space,
            num_outputs,
            policy_model_config,
            framework="tf",
            name=name)
        return model

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag344')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/sac/sac_torch_model.py: 136-156
</a>
<div class="mid" id="frag344" style="display:none"><pre>
    def build_policy_model(self, obs_space, num_outputs, policy_model_config,
                           name):
        """Builds the policy model used by this SAC.

        Override this method in a sub-class of SACTFModel to implement your
        own policy net. Alternatively, simply set `custom_model` within the
        top level SAC `policy_model` config key to make this default
        implementation of `build_policy_model` use your custom policy network.

        Returns:
            TorchModelV2: The TorchModelV2 policy sub-model.
        """
        model = ModelCatalog.get_model_v2(
            obs_space,
            self.action_space,
            num_outputs,
            policy_model_config,
            framework="torch",
            name=name)
        return model

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 16:</b> &nbsp; 2 fragments, nominal size 29 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag325')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/sac/sac_tf_model.py: 149-189
</a>
<div class="mid" id="frag325" style="display:none"><pre>
    def build_q_model(self, obs_space, action_space, num_outputs,
                      q_model_config, name):
        """Builds one of the (twin) Q-nets used by this SAC.

        Override this method in a sub-class of SACTFModel to implement your
        own Q-nets. Alternatively, simply set `custom_model` within the
        top level SAC `Q_model` config key to make this default implementation
        of `build_q_model` use your custom Q-nets.

        Returns:
            TFModelV2: The TFModelV2 Q-net sub-model.
        """
        self.concat_obs_and_actions = False
        if self.discrete:
            input_space = obs_space
        else:
            orig_space = getattr(obs_space, "original_space", obs_space)
            if isinstance(orig_space, Box) and len(orig_space.shape) == 1:
                input_space = Box(
                    float("-inf"),
                    float("inf"),
                    shape=(orig_space.shape[0] + action_space.shape[0], ))
                self.concat_obs_and_actions = True
            else:
                if isinstance(orig_space, gym.spaces.Tuple):
                    spaces = list(orig_space.spaces)
                elif isinstance(orig_space, gym.spaces.Dict):
                    spaces = list(orig_space.spaces.values())
                else:
                    spaces = [obs_space]
                input_space = gym.spaces.Tuple(spaces + [action_space])

        model = ModelCatalog.get_model_v2(
            input_space,
            action_space,
            num_outputs,
            q_model_config,
            framework="tf",
            name=name)
        return model

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag345')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/sac/sac_torch_model.py: 157-197
</a>
<div class="mid" id="frag345" style="display:none"><pre>
    def build_q_model(self, obs_space, action_space, num_outputs,
                      q_model_config, name):
        """Builds one of the (twin) Q-nets used by this SAC.

        Override this method in a sub-class of SACTFModel to implement your
        own Q-nets. Alternatively, simply set `custom_model` within the
        top level SAC `Q_model` config key to make this default implementation
        of `build_q_model` use your custom Q-nets.

        Returns:
            TorchModelV2: The TorchModelV2 Q-net sub-model.
        """
        self.concat_obs_and_actions = False
        if self.discrete:
            input_space = obs_space
        else:
            orig_space = getattr(obs_space, "original_space", obs_space)
            if isinstance(orig_space, Box) and len(orig_space.shape) == 1:
                input_space = Box(
                    float("-inf"),
                    float("inf"),
                    shape=(orig_space.shape[0] + action_space.shape[0], ))
                self.concat_obs_and_actions = True
            else:
                if isinstance(orig_space, gym.spaces.Tuple):
                    spaces = list(orig_space.spaces)
                elif isinstance(orig_space, gym.spaces.Dict):
                    spaces = list(orig_space.spaces.values())
                else:
                    spaces = [obs_space]
                input_space = gym.spaces.Tuple(spaces + [action_space])

        model = ModelCatalog.get_model_v2(
            input_space,
            action_space,
            num_outputs,
            q_model_config,
            framework="torch",
            name=name)
        return model

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 17:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag328')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/sac/sac_tf_model.py: 229-255
</a>
<div class="mid" id="frag328" style="display:none"><pre>
    def _get_q_value(self, model_out, actions, net):
        # Model outs may come as original Tuple/Dict observations, concat them
        # here if this is the case.
        if isinstance(net.obs_space, Box):
            if isinstance(model_out, (list, tuple)):
                model_out = tf.concat(model_out, axis=-1)
            elif isinstance(model_out, dict):
                model_out = tf.concat(list(model_out.values()), axis=-1)
        elif isinstance(model_out, dict):
            model_out = list(model_out.values())

        # Continuous case -&gt; concat actions to model_out.
        if actions is not None:
            if self.concat_obs_and_actions:
                input_dict = {"obs": tf.concat([model_out, actions], axis=-1)}
            else:
                input_dict = {"obs": force_list(model_out) + [actions]}
        # Discrete case -&gt; return q-vals for all actions.
        else:
            input_dict = {"obs": model_out}
        # Switch on training mode (when getting Q-values, we are usually in
        # training).
        input_dict["is_training"] = True

        out, _ = net(input_dict, [], None)
        return out

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag348')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/sac/sac_torch_model.py: 237-263
</a>
<div class="mid" id="frag348" style="display:none"><pre>
    def _get_q_value(self, model_out, actions, net):
        # Model outs may come as original Tuple observations, concat them
        # here if this is the case.
        if isinstance(net.obs_space, Box):
            if isinstance(model_out, (list, tuple)):
                model_out = torch.cat(model_out, dim=-1)
            elif isinstance(model_out, dict):
                model_out = torch.cat(list(model_out.values()), dim=-1)
        elif isinstance(model_out, dict):
            model_out = list(model_out.values())

        # Continuous case -&gt; concat actions to model_out.
        if actions is not None:
            if self.concat_obs_and_actions:
                input_dict = {"obs": torch.cat([model_out, actions], dim=-1)}
            else:
                input_dict = {"obs": force_list(model_out) + [actions]}
        # Discrete case -&gt; return q-vals for all actions.
        else:
            input_dict = {"obs": model_out}
        # Switch on training mode (when getting Q-values, we are usually in
        # training).
        input_dict["is_training"] = True

        out, _ = net(input_dict, [], None)
        return out

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 18:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag329')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/sac/sac_tf_model.py: 256-285
</a>
<div class="mid" id="frag329" style="display:none"><pre>
    def get_policy_output(self, model_out: TensorType) -&gt; TensorType:
        """Returns policy outputs, given the output of self.__call__().

        For continuous action spaces, these will be the mean/stddev
        distribution inputs for the (SquashedGaussian) action distribution.
        For discrete action spaces, these will be the logits for a categorical
        distribution.

        Args:
            model_out (TensorType): Feature outputs from the model layers
                (result of doing `self.__call__(obs)`).

        Returns:
            TensorType: Distribution inputs for sampling actions.
        """
        # Model outs may come as original Tuple/Dict observations, concat them
        # here if this is the case.
        if isinstance(self.action_model.obs_space, Box):
            if isinstance(model_out, (list, tuple)):
                model_out = tf.concat(model_out, axis=-1)
            elif isinstance(model_out, dict):
                model_out = tf.concat(
                    [
                        tf.expand_dims(val, 1) if len(val.shape) == 1 else val
                        for val in tree.flatten(model_out.values())
                    ],
                    axis=-1)
        out, _ = self.action_model({"obs": model_out}, [], None)
        return out

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag349')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/sac/sac_torch_model.py: 264-293
</a>
<div class="mid" id="frag349" style="display:none"><pre>
    def get_policy_output(self, model_out: TensorType) -&gt; TensorType:
        """Returns policy outputs, given the output of self.__call__().

        For continuous action spaces, these will be the mean/stddev
        distribution inputs for the (SquashedGaussian) action distribution.
        For discrete action spaces, these will be the logits for a categorical
        distribution.

        Args:
            model_out (TensorType): Feature outputs from the model layers
                (result of doing `self.__call__(obs)`).

        Returns:
            TensorType: Distribution inputs for sampling actions.
        """
        # Model outs may come as original Tuple observations, concat them
        # here if this is the case.
        if isinstance(self.action_model.obs_space, Box):
            if isinstance(model_out, (list, tuple)):
                model_out = torch.cat(model_out, dim=-1)
            elif isinstance(model_out, dict):
                model_out = torch.cat(
                    [
                        torch.unsqueeze(val, 1) if len(val.shape) == 1 else val
                        for val in tree.flatten(model_out.values())
                    ],
                    dim=-1)
        out, _ = self.action_model({"obs": model_out}, [], None)
        return out

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 19:</b> &nbsp; 2 fragments, nominal size 37 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag373')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/marwil/tests/test_marwil.py: 26-82
</a>
<div class="mid" id="frag373" style="display:none"><pre>

    def test_marwil_compilation_and_learning_from_offline_file(self):
        """Test whether a MARWILTrainer can be built with all frameworks.

        Learns from a historic-data file.
        To generate this data, first run:
        $ ./train.py --run=PPO --env=CartPole-v0 \
          --stop='{"timesteps_total": 50000}' \
          --config='{"output": "/tmp/out", "batch_mode": "complete_episodes"}'
        """
        rllib_dir = Path(__file__).parent.parent.parent.parent
        print("rllib dir={}".format(rllib_dir))
        data_file = os.path.join(rllib_dir, "tests/data/cartpole/large.json")
        print("data_file={} exists={}".format(data_file,
                                              os.path.isfile(data_file)))

        config = marwil.DEFAULT_CONFIG.copy()
        config["num_workers"] = 2
        config["evaluation_num_workers"] = 1
        config["evaluation_interval"] = 3
        config["evaluation_num_episodes"] = 5
        config["evaluation_parallel_to_training"] = True
        # Evaluate on actual environment.
        config["evaluation_config"] = {"input": "sampler"}
        # Learn from offline data.
        config["input"] = [data_file]
        num_iterations = 350
        min_reward = 70.0

        # Test for all frameworks.
        for _ in framework_iterator(config, frameworks=("tf", "torch")):
            trainer = marwil.MARWILTrainer(config=config, env="CartPole-v0")
            learnt = False
            for i in range(num_iterations):
                results = trainer.train()
                check_train_results(results)
                print(results)

                eval_results = results.get("evaluation")
                if eval_results:
                    print("iter={} R={} ".format(
                        i, eval_results["episode_reward_mean"]))
                    # Learn until some reward is reached on an actual live env.
                    if eval_results["episode_reward_mean"] &gt; min_reward:
                        print("learnt!")
                        learnt = True
                        break

            if not learnt:
                raise ValueError(
                    "MARWILTrainer did not reach {} reward from expert "
                    "offline data!".format(min_reward))

            check_compute_single_action(
                trainer, include_prev_action_reward=True)

            trainer.stop()
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag378')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/marwil/tests/test_bc.py: 22-77
</a>
<div class="mid" id="frag378" style="display:none"><pre>

    def test_bc_compilation_and_learning_from_offline_file(self):
        """Test whether a BCTrainer can be built with all frameworks.

        And learns from a historic-data file (while being evaluated on an
        actual env using evaluation_num_workers &gt; 0).
        """
        rllib_dir = Path(__file__).parent.parent.parent.parent
        print("rllib dir={}".format(rllib_dir))
        data_file = os.path.join(rllib_dir, "tests/data/cartpole/large.json")
        print("data_file={} exists={}".format(data_file,
                                              os.path.isfile(data_file)))

        config = marwil.BC_DEFAULT_CONFIG.copy()
        config["num_workers"] = 0  # Run locally.

        config["evaluation_interval"] = 3
        config["evaluation_num_workers"] = 1
        config["evaluation_num_episodes"] = 5
        config["evaluation_parallel_to_training"] = True
        # Evaluate on actual environment.
        config["evaluation_config"] = {"input": "sampler"}
        # Learn from offline data.
        config["input"] = [data_file]
        num_iterations = 350
        min_reward = 70.0

        # Test for all frameworks.
        for _ in framework_iterator(config, frameworks=("tf", "torch")):
            trainer = marwil.BCTrainer(config=config, env="CartPole-v0")
            learnt = False
            for i in range(num_iterations):
                results = trainer.train()
                check_train_results(results)
                print(results)

                eval_results = results.get("evaluation")
                if eval_results:
                    print("iter={} R={}".format(
                        i, eval_results["episode_reward_mean"]))
                    # Learn until good reward is reached in the actual env.
                    if eval_results["episode_reward_mean"] &gt; min_reward:
                        print("learnt!")
                        learnt = True
                        break

            if not learnt:
                raise ValueError(
                    "BCTrainer did not reach {} reward from expert offline "
                    "data!".format(min_reward))

            check_compute_single_action(
                trainer, include_prev_action_reward=True)

            trainer.stop()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 20:</b> &nbsp; 3 fragments, nominal size 16 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag390')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/a3c/tests/test_a3c.py: 19-40
</a>
<div class="mid" id="frag390" style="display:none"><pre>
        ray.shutdown()

    def test_a3c_compilation(self):
        """Test whether an A3CTrainer can be built with both frameworks."""
        config = a3c.DEFAULT_CONFIG.copy()
        config["num_workers"] = 2
        config["num_envs_per_worker"] = 2

        num_iterations = 1

        # Test against all frameworks.
        for _ in framework_iterator(config, with_eager_tracing=True):
            for env in ["CartPole-v1", "Pendulum-v1", "PongDeterministic-v0"]:
                print("env={}".format(env))
                config["model"]["use_lstm"] = env == "CartPole-v1"
                trainer = a3c.A3CTrainer(config=config, env=env)
                for i in range(num_iterations):
                    results = trainer.train()
                    check_train_results(results)
                    print(results)
                check_compute_single_action(
                    trainer, include_state=config["model"]["use_lstm"])
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag395')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/a3c/tests/test_a2c.py: 17-35
</a>
<div class="mid" id="frag395" style="display:none"><pre>

    def test_a2c_compilation(self):
        """Test whether an A2CTrainer can be built with both frameworks."""
        config = a3c.a2c.A2C_DEFAULT_CONFIG.copy()
        config["num_workers"] = 2
        config["num_envs_per_worker"] = 2

        num_iterations = 1

        # Test against all frameworks.
        for _ in framework_iterator(config, with_eager_tracing=True):
            for env in ["CartPole-v0", "Pendulum-v1", "PongDeterministic-v0"]:
                trainer = a3c.A2CTrainer(config=config, env=env)
                for i in range(num_iterations):
                    results = trainer.train()
                    check_train_results(results)
                    print(results)
                check_compute_single_action(trainer)
                trainer.stop()
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag492')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/pg/tests/test_pg.py: 23-40
</a>
<div class="mid" id="frag492" style="display:none"><pre>

    def test_pg_compilation(self):
        """Test whether a PGTrainer can be built with all frameworks."""
        config = pg.DEFAULT_CONFIG.copy()
        config["num_workers"] = 1
        config["rollout_fragment_length"] = 500
        num_iterations = 1

        for _ in framework_iterator(config, with_eager_tracing=True):
            for env in ["FrozenLake-v1", "CartPole-v0"]:
                trainer = pg.PGTrainer(config=config, env=env)
                for i in range(num_iterations):
                    results = trainer.train()
                    check_train_results(results)
                    print(results)

                check_compute_single_action(
                    trainer, include_prev_action_reward=True)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 21:</b> &nbsp; 3 fragments, nominal size 28 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag391')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/a3c/tests/test_a3c.py: 41-86
</a>
<div class="mid" id="frag391" style="display:none"><pre>
                trainer.stop()

    def test_a3c_entropy_coeff_schedule(self):
        """Test A3CTrainer entropy coeff schedule support."""
        config = a3c.DEFAULT_CONFIG.copy()
        config["num_workers"] = 1
        config["num_envs_per_worker"] = 1
        config["train_batch_size"] = 20
        config["batch_mode"] = "truncate_episodes"
        config["rollout_fragment_length"] = 10
        config["timesteps_per_iteration"] = 20
        # 0 metrics reporting delay, this makes sure timestep,
        # which entropy coeff depends on, is updated after each worker rollout.
        config["min_iter_time_s"] = 0
        # Initial lr, doesn't really matter because of the schedule below.
        config["entropy_coeff"] = 0.01
        schedule = [
            [0, 0.01],
            [120, 0.0001],
        ]
        config["entropy_coeff_schedule"] = schedule

        def _step_n_times(trainer, n: int):
            """Step trainer n times.

            Returns:
                learning rate at the end of the execution.
            """
            for _ in range(n):
                results = trainer.train()
            return results["info"][LEARNER_INFO][DEFAULT_POLICY_ID][
                LEARNER_STATS_KEY]["entropy_coeff"]

        # Test against all frameworks.
        for _ in framework_iterator(config):
            trainer = a3c.A3CTrainer(config=config, env="CartPole-v1")

            coeff = _step_n_times(trainer, 1)  # 20 timesteps
            # Should be close to the starting coeff of 0.01
            self.assertGreaterEqual(coeff, 0.005)

            coeff = _step_n_times(trainer, 10)  # 200 timesteps
            # Should have annealed to the final coeff of 0.0001.
            self.assertLessEqual(coeff, 0.00011)

            trainer.stop()
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag595')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/dqn/tests/test_apex_dqn.py: 70-118
</a>
<div class="mid" id="frag595" style="display:none"><pre>
            trainer.stop()

    def test_apex_lr_schedule(self):
        config = apex.APEX_DEFAULT_CONFIG.copy()
        config["num_workers"] = 1
        config["num_gpus"] = 0
        config["buffer_size"] = 100
        config["learning_starts"] = 10
        config["train_batch_size"] = 10
        config["rollout_fragment_length"] = 5
        config["prioritized_replay"] = True
        config["timesteps_per_iteration"] = 10
        # 0 metrics reporting delay, this makes sure timestep,
        # which lr depends on, is updated after each worker rollout.
        config["min_iter_time_s"] = 0
        config["optimizer"]["num_replay_buffer_shards"] = 1
        # This makes sure learning schedule is checked every 10 timesteps.
        config["optimizer"]["max_weight_sync_delay"] = 10
        # Initial lr, doesn't really matter because of the schedule below.
        config["lr"] = 0.2
        lr_schedule = [
            [0, 0.2],
            [100, 0.001],
        ]
        config["lr_schedule"] = lr_schedule

        def _step_n_times(trainer, n: int):
            """Step trainer n times.

            Returns:
                learning rate at the end of the execution.
            """
            for _ in range(n):
                results = trainer.train()
            return results["info"][LEARNER_INFO][DEFAULT_POLICY_ID][
                LEARNER_STATS_KEY]["cur_lr"]

        for _ in framework_iterator(config):
            trainer = apex.ApexTrainer(config=config, env="CartPole-v0")

            lr = _step_n_times(trainer, 1)  # 10 timesteps
            # Close to 0.2
            self.assertGreaterEqual(lr, 0.1)

            lr = _step_n_times(trainer, 20)  # 200 timesteps
            # LR Annealed to 0.001
            self.assertLessEqual(lr, 0.0011)

            trainer.stop()
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag429')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/ppo/tests/test_appo.py: 72-115
</a>
<div class="mid" id="frag429" style="display:none"><pre>
            trainer.stop()

    def test_appo_entropy_coeff_schedule(self):
        config = ppo.appo.DEFAULT_CONFIG.copy()
        config["num_workers"] = 1
        config["num_gpus"] = 0
        config["train_batch_size"] = 20
        config["batch_mode"] = "truncate_episodes"
        config["rollout_fragment_length"] = 10
        config["timesteps_per_iteration"] = 20
        # 0 metrics reporting delay, this makes sure timestep,
        # which entropy coeff depends on, is updated after each worker rollout.
        config["min_iter_time_s"] = 0
        # Initial lr, doesn't really matter because of the schedule below.
        config["entropy_coeff"] = 0.01
        schedule = [
            [0, 0.01],
            [120, 0.0001],
        ]
        config["entropy_coeff_schedule"] = schedule

        def _step_n_times(trainer, n: int):
            """Step trainer n times.

            Returns:
                learning rate at the end of the execution.
            """
            for _ in range(n):
                results = trainer.train()
            return results["info"][LEARNER_INFO][DEFAULT_POLICY_ID][
                LEARNER_STATS_KEY]["entropy_coeff"]

        for _ in framework_iterator(config):
            trainer = ppo.APPOTrainer(config=config, env="CartPole-v0")

            coeff = _step_n_times(trainer, 1)  # 20 timesteps
            # Should be close to the starting coeff of 0.01.
            self.assertGreaterEqual(coeff, 0.005)

            coeff = _step_n_times(trainer, 10)  # 200 timesteps
            # Should have annealed to the final coeff of 0.0001.
            self.assertLessEqual(coeff, 0.00011)

            trainer.stop()
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 22:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag403')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/a3c/a3c_torch_policy.py: 83-96
</a>
<div class="mid" id="frag403" style="display:none"><pre>
    return total_loss


def stats(policy: Policy, train_batch: SampleBatch) -&gt; Dict[str, TensorType]:

    return {
        "cur_lr": policy.cur_lr,
        "entropy_coeff": policy.entropy_coeff,
        "policy_entropy": torch.mean(
            torch.stack(policy.get_tower_stats("entropy"))),
        "policy_loss": torch.mean(
            torch.stack(policy.get_tower_stats("pi_err"))),
        "vf_loss": torch.mean(
            torch.stack(policy.get_tower_stats("value_err"))),
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag586')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/dqn/r2d2_torch_policy.py: 236-249
</a>
<div class="mid" id="frag586" style="display:none"><pre>

            # Do forward pass on loss to update td error attribute
            r2d2_loss(self, self.model, None, input_dict)

            return self.model.tower_stats["td_error"]

        self.compute_td_error = compute_td_error


def build_q_stats(policy: Policy, batch: SampleBatch) -&gt; Dict[str, TensorType]:

    return {
        "cur_lr": policy.cur_lr,
        "total_loss": torch.mean(
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 23:</b> &nbsp; 2 fragments, nominal size 27 lines, similarity 82%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag427')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/ppo/tests/test_appo.py: 19-47
</a>
<div class="mid" id="frag427" style="display:none"><pre>
        ray.shutdown()

    def test_appo_compilation(self):
        """Test whether an APPOTrainer can be built with both frameworks."""
        config = ppo.appo.DEFAULT_CONFIG.copy()
        config["num_workers"] = 1
        num_iterations = 2

        for _ in framework_iterator(config, with_eager_tracing=True):
            print("w/o v-trace")
            _config = config.copy()
            _config["vtrace"] = False
            trainer = ppo.APPOTrainer(config=_config, env="CartPole-v0")
            for i in range(num_iterations):
                results = trainer.train()
                check_train_results(results)
                print(results)
            check_compute_single_action(trainer)
            trainer.stop()

            print("w/ v-trace")
            _config = config.copy()
            _config["vtrace"] = True
            trainer = ppo.APPOTrainer(config=_config, env="CartPole-v0")
            for i in range(num_iterations):
                results = trainer.train()
                check_train_results(results)
                print(results)
            check_compute_single_action(trainer)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag599')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/dqn/tests/test_dqn.py: 18-55
</a>
<div class="mid" id="frag599" style="display:none"><pre>

    def test_dqn_compilation(self):
        """Test whether a DQNTrainer can be built on all frameworks."""
        config = dqn.DEFAULT_CONFIG.copy()
        config["num_workers"] = 2

        num_iterations = 1

        for _ in framework_iterator(config, with_eager_tracing=True):
            # Double-dueling DQN.
            print("Double-dueling")
            plain_config = config.copy()
            trainer = dqn.DQNTrainer(config=plain_config, env="CartPole-v0")
            for i in range(num_iterations):
                results = trainer.train()
                check_train_results(results)
                print(results)

            check_compute_single_action(trainer)
            trainer.stop()

            # Rainbow.
            print("Rainbow")
            rainbow_config = config.copy()
            rainbow_config["num_atoms"] = 10
            rainbow_config["noisy"] = True
            rainbow_config["double_q"] = True
            rainbow_config["dueling"] = True
            rainbow_config["n_step"] = 5
            trainer = dqn.DQNTrainer(config=rainbow_config, env="CartPole-v0")
            for i in range(num_iterations):
                results = trainer.train()
                check_train_results(results)
                print(results)

            check_compute_single_action(trainer)

            trainer.stop()
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 24:</b> &nbsp; 2 fragments, nominal size 23 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag676')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/dreamer/dreamer_model.py: 23-53
</a>
<div class="mid" id="frag676" style="display:none"><pre>

    def __init__(self,
                 depth: int = 32,
                 act: ActFunc = None,
                 shape: Tuple[int] = (3, 64, 64)):
        """Initializes Conv Encoder

        Args:
            depth (int): Number of channels in the first conv layer
            act (Any): Activation for Encoder, default ReLU
            shape (List): Shape of observation input
        """
        super().__init__()
        self.act = act
        if not act:
            self.act = nn.ReLU
        self.depth = depth
        self.shape = shape

        init_channels = self.shape[0]
        self.layers = [
            Conv2d(init_channels, self.depth, 4, stride=2),
            self.act(),
            Conv2d(self.depth, 2 * self.depth, 4, stride=2),
            self.act(),
            Conv2d(2 * self.depth, 4 * self.depth, 4, stride=2),
            self.act(),
            Conv2d(4 * self.depth, 8 * self.depth, 4, stride=2),
            self.act(),
        ]
        self.model = nn.Sequential(*self.layers)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag678')" href="javascript:;">
ray-ray-1.9.2/rllib/agents/dreamer/dreamer_model.py: 74-107
</a>
<div class="mid" id="frag678" style="display:none"><pre>

    def __init__(self,
                 input_size: int,
                 depth: int = 32,
                 act: ActFunc = None,
                 shape: Tuple[int] = (3, 64, 64)):
        """Initializes a ConvDecoder instance.

        Args:
            input_size (int): Input size, usually feature size output from
                RSSM.
            depth (int): Number of channels in the first conv layer
            act (Any): Activation for Encoder, default ReLU
            shape (List): Shape of observation input
        """
        super().__init__()
        self.act = act
        if not act:
            self.act = nn.ReLU
        self.depth = depth
        self.shape = shape

        self.layers = [
            Linear(input_size, 32 * self.depth),
            Reshape([-1, 32 * self.depth, 1, 1]),
            ConvTranspose2d(32 * self.depth, 4 * self.depth, 5, stride=2),
            self.act(),
            ConvTranspose2d(4 * self.depth, 2 * self.depth, 5, stride=2),
            self.act(),
            ConvTranspose2d(2 * self.depth, self.depth, 6, stride=2),
            self.act(),
            ConvTranspose2d(self.depth, self.shape[0], 6, stride=2),
        ]
        self.model = nn.Sequential(*self.layers)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 25:</b> &nbsp; 2 fragments, nominal size 71 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag705')" href="javascript:;">
ray-ray-1.9.2/rllib/models/torch/complex_input_net.py: 35-136
</a>
<div class="mid" id="frag705" style="display:none"><pre>

    def __init__(self, obs_space, action_space, num_outputs, model_config,
                 name):
        self.original_space = obs_space.original_space if \
            hasattr(obs_space, "original_space") else obs_space
        assert isinstance(self.original_space, (Dict, Tuple)), \
            "`obs_space.original_space` must be [Dict|Tuple]!"

        self.processed_obs_space = self.original_space if \
            model_config.get("_disable_preprocessor_api") else obs_space

        nn.Module.__init__(self)
        TorchModelV2.__init__(self, self.original_space, action_space,
                              num_outputs, model_config, name)

        self.flattened_input_space = flatten_space(self.original_space)

        # Atari type CNNs or IMPALA type CNNs (with residual layers)?
        # self.cnn_type = self.model_config["custom_model_config"].get(
        #     "conv_type", "atari")

        # Build the CNN(s) given obs_space's image components.
        self.cnns = {}
        self.one_hot = {}
        self.flatten = {}
        concat_size = 0
        for i, component in enumerate(self.flattened_input_space):
            # Image space.
            if len(component.shape) == 3:
                config = {
                    "conv_filters": model_config["conv_filters"]
                    if "conv_filters" in model_config else
                    get_filter_config(obs_space.shape),
                    "conv_activation": model_config.get("conv_activation"),
                    "post_fcnet_hiddens": [],
                }
                # if self.cnn_type == "atari":
                cnn = ModelCatalog.get_model_v2(
                    component,
                    action_space,
                    num_outputs=None,
                    model_config=config,
                    framework="torch",
                    name="cnn_{}".format(i))
                # TODO (sven): add IMPALA-style option.
                # else:
                #    cnn = TorchImpalaVisionNet(
                #        component,
                #        action_space,
                #        num_outputs=None,
                #        model_config=config,
                #        name="cnn_{}".format(i))

                concat_size += cnn.num_outputs
                self.cnns[i] = cnn
                self.add_module("cnn_{}".format(i), cnn)
            # Discrete|MultiDiscrete inputs -&gt; One-hot encode.
            elif isinstance(component, Discrete):
                self.one_hot[i] = True
                concat_size += component.n
            elif isinstance(component, MultiDiscrete):
                self.one_hot[i] = True
                concat_size += sum(component.nvec)
            # Everything else (1D Box).
            else:
                self.flatten[i] = int(np.product(component.shape))
                concat_size += self.flatten[i]

        # Optional post-concat FC-stack.
        post_fc_stack_config = {
            "fcnet_hiddens": model_config.get("post_fcnet_hiddens", []),
            "fcnet_activation": model_config.get("post_fcnet_activation",
                                                 "relu")
        }
        self.post_fc_stack = ModelCatalog.get_model_v2(
            Box(float("-inf"),
                float("inf"),
                shape=(concat_size, ),
                dtype=np.float32),
            self.action_space,
            None,
            post_fc_stack_config,
            framework="torch",
            name="post_fc_stack")

        # Actions and value heads.
        self.logits_layer = None
        self.value_layer = None
        self._value_out = None

        if num_outputs:
            # Action-distribution head.
            self.logits_layer = SlimFC(
                in_size=self.post_fc_stack.num_outputs,
                out_size=num_outputs,
                activation_fn=None,
            )
            # Create the value branch model.
            self.value_layer = SlimFC(
                in_size=self.post_fc_stack.num_outputs,
                out_size=1,
                activation_fn=None,
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag753')" href="javascript:;">
ray-ray-1.9.2/rllib/models/tf/complex_input_net.py: 34-120
</a>
<div class="mid" id="frag753" style="display:none"><pre>
    def __init__(self, obs_space, action_space, num_outputs, model_config,
                 name):
        self.original_space = obs_space.original_space if \
            hasattr(obs_space, "original_space") else obs_space
        assert isinstance(self.original_space, (Dict, Tuple)), \
            "`obs_space.original_space` must be [Dict|Tuple]!"

        self.processed_obs_space = self.original_space if \
            model_config.get("_disable_preprocessor_api") else obs_space
        super().__init__(self.original_space, action_space, num_outputs,
                         model_config, name)

        self.flattened_input_space = flatten_space(self.original_space)

        # Build the CNN(s) given obs_space's image components.
        self.cnns = {}
        self.one_hot = {}
        self.flatten = {}
        concat_size = 0
        for i, component in enumerate(self.flattened_input_space):
            # Image space.
            if len(component.shape) == 3:
                config = {
                    "conv_filters": model_config["conv_filters"]
                    if "conv_filters" in model_config else
                    get_filter_config(obs_space.shape),
                    "conv_activation": model_config.get("conv_activation"),
                    "post_fcnet_hiddens": [],
                }
                cnn = ModelCatalog.get_model_v2(
                    component,
                    action_space,
                    num_outputs=None,
                    model_config=config,
                    framework="tf",
                    name="cnn_{}".format(i))
                concat_size += cnn.num_outputs
                self.cnns[i] = cnn
            # Discrete|MultiDiscrete inputs -&gt; One-hot encode.
            elif isinstance(component, Discrete):
                self.one_hot[i] = True
                concat_size += component.n
            elif isinstance(component, MultiDiscrete):
                self.one_hot[i] = True
                concat_size += sum(component.nvec)
            # Everything else (1D Box).
            else:
                self.flatten[i] = int(np.product(component.shape))
                concat_size += self.flatten[i]

        # Optional post-concat FC-stack.
        post_fc_stack_config = {
            "fcnet_hiddens": model_config.get("post_fcnet_hiddens", []),
            "fcnet_activation": model_config.get("post_fcnet_activation",
                                                 "relu")
        }
        self.post_fc_stack = ModelCatalog.get_model_v2(
            Box(float("-inf"),
                float("inf"),
                shape=(concat_size, ),
                dtype=np.float32),
            self.action_space,
            None,
            post_fc_stack_config,
            framework="tf",
            name="post_fc_stack")

        # Actions and value heads.
        self.logits_and_value_model = None
        self._value_out = None
        if num_outputs:
            # Action-distribution head.
            concat_layer = tf.keras.layers.Input(
                (self.post_fc_stack.num_outputs, ))
            logits_layer = tf.keras.layers.Dense(
                num_outputs,
                activation=tf.keras.activations.linear,
                name="logits")(concat_layer)

            # Create the value branch model.
            value_layer = tf.keras.layers.Dense(
                1,
                name="value_out",
                activation=None,
                kernel_initializer=normc_initializer(0.01))(concat_layer)
            self.logits_and_value_model = tf.keras.models.Model(
                concat_layer, [logits_layer, value_layer])
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 26:</b> &nbsp; 2 fragments, nominal size 30 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag706')" href="javascript:;">
ray-ray-1.9.2/rllib/models/torch/complex_input_net.py: 138-173
</a>
<div class="mid" id="frag706" style="display:none"><pre>
        else:
            self.num_outputs = concat_size

    @override(ModelV2)
    def forward(self, input_dict, state, seq_lens):
        if SampleBatch.OBS in input_dict and "obs_flat" in input_dict:
            orig_obs = input_dict[SampleBatch.OBS]
        else:
            orig_obs = restore_original_dimensions(
                input_dict[SampleBatch.OBS],
                self.processed_obs_space,
                tensorlib="torch")
        # Push image observations through our CNNs.
        outs = []
        for i, component in enumerate(tree.flatten(orig_obs)):
            if i in self.cnns:
                cnn_out, _ = self.cnns[i]({SampleBatch.OBS: component})
                outs.append(cnn_out)
            elif i in self.one_hot:
                if component.dtype in [torch.int32, torch.int64, torch.uint8]:
                    outs.append(
                        one_hot(component, self.flattened_input_space[i]))
                else:
                    outs.append(component)
            else:
                outs.append(torch.reshape(component, [-1, self.flatten[i]]))
        # Concat all outputs and the non-image inputs.
        out = torch.cat(outs, dim=1)
        # Push through (optional) FC-stack (this may be an empty stack).
        out, _ = self.post_fc_stack({SampleBatch.OBS: out}, [], None)

        # No logits/value branches.
        if self.logits_layer is None:
            return out, []

        # Logits- and value branches.
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag754')" href="javascript:;">
ray-ray-1.9.2/rllib/models/tf/complex_input_net.py: 122-161
</a>
<div class="mid" id="frag754" style="display:none"><pre>
            self.num_outputs = self.post_fc_stack.num_outputs

    @override(ModelV2)
    def forward(self, input_dict, state, seq_lens):
        if SampleBatch.OBS in input_dict and "obs_flat" in input_dict:
            orig_obs = input_dict[SampleBatch.OBS]
        else:
            orig_obs = restore_original_dimensions(
                input_dict[SampleBatch.OBS],
                self.processed_obs_space,
                tensorlib="tf")
        # Push image observations through our CNNs.
        outs = []
        for i, component in enumerate(tree.flatten(orig_obs)):
            if i in self.cnns:
                cnn_out, _ = self.cnns[i]({SampleBatch.OBS: component})
                outs.append(cnn_out)
            elif i in self.one_hot:
                if "int" in component.dtype.name:
                    outs.append(
                        one_hot(component, self.flattened_input_space[i]))
                else:
                    outs.append(component)
            else:
                outs.append(
                    tf.cast(
                        tf.reshape(component, [-1, self.flatten[i]]),
                        dtype=tf.float32,
                    ))
        # Concat all outputs and the non-image inputs.
        out = tf.concat(outs, axis=1)
        # Push through (optional) FC-stack (this may be an empty stack).
        out, _ = self.post_fc_stack({SampleBatch.OBS: out}, [], None)

        # No logits/value branches.
        if not self.logits_and_value_model:
            return out, []

        # Logits- and value branches.
        logits, values = self.logits_and_value_model(out)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 27:</b> &nbsp; 2 fragments, nominal size 21 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag737')" href="javascript:;">
ray-ray-1.9.2/rllib/models/torch/modules/multi_head_attention.py: 36-65
</a>
<div class="mid" id="frag737" style="display:none"><pre>
    def forward(self, inputs: TensorType) -&gt; TensorType:
        L = list(inputs.size())[1]  # length of segment
        H = self._num_heads  # number of attention heads
        D = self._head_dim  # attention head dimension

        qkv = self._qkv_layer(inputs)

        queries, keys, values = torch.chunk(input=qkv, chunks=3, dim=-1)
        queries = queries[:, -L:]  # only query based on the segment

        queries = torch.reshape(queries, [-1, L, H, D])
        keys = torch.reshape(keys, [-1, L, H, D])
        values = torch.reshape(values, [-1, L, H, D])

        score = torch.einsum("bihd,bjhd-&gt;bijh", queries, keys)
        score = score / D**0.5

        # causal mask of the same length as the sequence
        mask = sequence_mask(torch.arange(1, L + 1), dtype=score.dtype)
        mask = mask[None, :, :, None]
        mask = mask.float()

        masked_score = score * mask + 1e30 * (mask - 1.)
        wmat = nn.functional.softmax(masked_score, dim=2)

        out = torch.einsum("bijh,bjhd-&gt;bihd", wmat, values)
        shape = list(out.size())[:2] + [H * D]
        #        temp = torch.cat(temp2, [H * D], dim=0)
        out = torch.reshape(out, shape)
        return self._linear_layer(out)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag775')" href="javascript:;">
ray-ray-1.9.2/rllib/models/tf/layers/multi_head_attention.py: 26-53
</a>
<div class="mid" id="frag775" style="display:none"><pre>
    def call(self, inputs: TensorType) -&gt; TensorType:
        L = tf.shape(inputs)[1]  # length of segment
        H = self._num_heads  # number of attention heads
        D = self._head_dim  # attention head dimension

        qkv = self._qkv_layer(inputs)

        queries, keys, values = tf.split(qkv, 3, -1)
        queries = queries[:, -L:]  # only query based on the segment

        queries = tf.reshape(queries, [-1, L, H, D])
        keys = tf.reshape(keys, [-1, L, H, D])
        values = tf.reshape(values, [-1, L, H, D])

        score = tf.einsum("bihd,bjhd-&gt;bijh", queries, keys)
        score = score / D**0.5

        # causal mask of the same length as the sequence
        mask = tf.sequence_mask(tf.range(1, L + 1), dtype=score.dtype)
        mask = mask[None, :, :, None]

        masked_score = score * mask + 1e30 * (mask - 1.)
        wmat = tf.nn.softmax(masked_score, axis=2)

        out = tf.einsum("bijh,bjhd-&gt;bihd", wmat, values)
        shape = tf.concat([tf.shape(out)[:2], [H * D]], axis=0)
        out = tf.reshape(out, shape)
        return self._linear_layer(out)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 28:</b> &nbsp; 2 fragments, nominal size 36 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag740')" href="javascript:;">
ray-ray-1.9.2/rllib/models/utils.py: 6-63
</a>
<div class="mid" id="frag740" style="display:none"><pre>

def get_activation_fn(name: Optional[str] = None, framework: str = "tf"):
    """Returns a framework specific activation function, given a name string.

    Args:
        name (Optional[str]): One of "relu" (default), "tanh", "elu",
            "swish", or "linear" (same as None).
        framework (str): One of "jax", "tf|tfe|tf2" or "torch".

    Returns:
        A framework-specific activtion function. e.g. tf.nn.tanh or
            torch.nn.ReLU. None if name in ["linear", None].

    Raises:
        ValueError: If name is an unknown activation function.
    """
    # Already a callable, return as-is.
    if callable(name):
        return name

    # Infer the correct activation function from the string specifier.
    if framework == "torch":
        if name in ["linear", None]:
            return None
        if name == "swish":
            from ray.rllib.utils.torch_utils import Swish
            return Swish
        _, nn = try_import_torch()
        if name == "relu":
            return nn.ReLU
        elif name == "tanh":
            return nn.Tanh
        elif name == "elu":
            return nn.ELU
    elif framework == "jax":
        if name in ["linear", None]:
            return None
        jax, _ = try_import_jax()
        if name == "swish":
            return jax.nn.swish
        if name == "relu":
            return jax.nn.relu
        elif name == "tanh":
            return jax.nn.hard_tanh
        elif name == "elu":
            return jax.nn.elu
    else:
        assert framework in ["tf", "tfe", "tf2"],\
            "Unsupported framework `{}`!".format(framework)
        if name in ["linear", None]:
            return None
        tf1, tf, tfv = try_import_tf()
        fn = getattr(tf.nn, name, None)
        if fn is not None:
            return fn

    raise ValueError("Unknown activation ({}) for framework={}!".format(
        name, framework))
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1134')" href="javascript:;">
ray-ray-1.9.2/rllib/utils/framework.py: 264-311
</a>
<div class="mid" id="frag1134" style="display:none"><pre>
def get_activation_fn(name: Optional[str] = None, framework: str = "tf"):
    """Returns a framework specific activation function, given a name string.

    Args:
        name (Optional[str]): One of "relu" (default), "tanh", "swish", or
            "linear" or None.
        framework (str): One of "tf" or "torch".

    Returns:
        A framework-specific activtion function. e.g. tf.nn.tanh or
            torch.nn.ReLU. None if name in ["linear", None].

    Raises:
        ValueError: If name is an unknown activation function.
    """
    if framework == "torch":
        if name in ["linear", None]:
            return None
        if name in ["swish", "silu"]:
            from ray.rllib.utils.torch_utils import Swish
            return Swish
        _, nn = try_import_torch()
        if name == "relu":
            return nn.ReLU
        elif name == "tanh":
            return nn.Tanh
    elif framework == "jax":
        if name in ["linear", None]:
            return None
        jax, flax = try_import_jax()
        if name == "swish":
            return jax.nn.swish
        if name == "relu":
            return jax.nn.relu
        elif name == "tanh":
            return jax.nn.hard_tanh
    else:
        if name in ["linear", None]:
            return None
        if name == "swish":
            name = "silu"
        tf1, tf, tfv = try_import_tf()
        fn = getattr(tf.nn, name, None)
        if fn is not None:
            return fn

    raise ValueError("Unknown activation ({}) for framework={}!".format(
        name, framework))
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 29:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 87%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag836')" href="javascript:;">
ray-ray-1.9.2/rllib/models/preprocessors.py: 217-233
</a>
<div class="mid" id="frag836" style="display:none"><pre>
    def _init_shape(self, obs_space: gym.Space, options: dict) -&gt; List[int]:
        assert isinstance(self._obs_space, gym.spaces.Tuple)
        size = 0
        self.preprocessors = []
        for i in range(len(self._obs_space.spaces)):
            space = self._obs_space.spaces[i]
            logger.debug("Creating sub-preprocessor for {}".format(space))
            preprocessor_class = get_preprocessor(space)
            if preprocessor_class is not None:
                preprocessor = preprocessor_class(space, self._options)
                size += preprocessor.size
            else:
                preprocessor = None
                size += int(np.product(space.shape))
            self.preprocessors.append(preprocessor)
        return (size, )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag839')" href="javascript:;">
ray-ray-1.9.2/rllib/models/preprocessors.py: 257-272
</a>
<div class="mid" id="frag839" style="display:none"><pre>
    def _init_shape(self, obs_space: gym.Space, options: dict) -&gt; List[int]:
        assert isinstance(self._obs_space, gym.spaces.Dict)
        size = 0
        self.preprocessors = []
        for space in self._obs_space.spaces.values():
            logger.debug("Creating sub-preprocessor for {}".format(space))
            preprocessor_class = get_preprocessor(space)
            if preprocessor_class is not None:
                preprocessor = preprocessor_class(space, self._options)
                size += preprocessor.size
            else:
                preprocessor = None
                size += int(np.product(space.shape))
            self.preprocessors.append(preprocessor)
        return (size, )

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 30:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag847')" href="javascript:;">
ray-ray-1.9.2/rllib/env/env_context.py: 19-49
</a>
<div class="mid" id="frag847" style="display:none"><pre>
    def __init__(self,
                 env_config: EnvConfigDict,
                 worker_index: int,
                 vector_index: int = 0,
                 remote: bool = False,
                 num_workers: Optional[int] = None):
        """Initializes an EnvContext instance.

        Args:
            env_config: The env's configuration defined under the
                "env_config" key in the Trainer's config.
            worker_index: When there are multiple workers created, this
                uniquely identifies the worker the env is created in.
                0 for local worker, &gt;0 for remote workers.
            num_workers: The total number of (remote) workers in the set.
                0 if only a local worker exists.
            vector_index: When there are multiple envs per worker, this
                uniquely identifies the env index within the worker.
                Starts from 0.
            remote: Whether individual sub-environments (in a vectorized
                env) should be @ray.remote actors or not.
        """
        # Store the env_config in the (super) dict.
        dict.__init__(self, env_config)

        # Set some metadata attributes.
        self.worker_index = worker_index
        self.vector_index = vector_index
        self.remote = remote
        self.num_workers = num_workers

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2022')" href="javascript:;">
ray-ray-1.9.2/rllib/execution/metric_ops.py: 64-75
</a>
<div class="mid" id="frag2022" style="display:none"><pre>
        &gt;&gt;&gt; output_op = train_op.for_each(CollectMetrics(workers))
        &gt;&gt;&gt; print(next(output_op))
        {"episode_reward_max": ..., "episode_reward_mean": ..., ...}
    """

    def __init__(self,
                 workers: WorkerSet,
                 min_history: int = 100,
                 timeout_seconds: int = 180,
                 selected_workers: List[ActorHandle] = None):
        self.workers = workers
        self.episode_history = []
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 31:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag886')" href="javascript:;">
ray-ray-1.9.2/rllib/env/wrappers/pettingzoo_env.py: 69-92
</a>
<div class="mid" id="frag886" style="display:none"><pre>
    def __init__(self, env):
        self.env = env
        # agent idx list
        self.agents = self.env.possible_agents

        # Get dictionaries of obs_spaces and act_spaces
        self.observation_spaces = self.env.observation_spaces
        self.action_spaces = self.env.action_spaces

        # Get first observation space, assuming all agents have equal space
        self.observation_space = self.observation_spaces[self.agents[0]]

        # Get first action space, assuming all agents have equal space
        self.action_space = self.action_spaces[self.agents[0]]

        assert all(obs_space == self.observation_space
                   for obs_space
                   in self.env.observation_spaces.values()), \
            "Observation spaces for all agents must be identical. Perhaps " \
            "SuperSuit's pad_observations wrapper can help (useage: " \
            "`supersuit.aec_wrappers.pad_observations(env)`"

        assert all(act_space == self.action_space
                   for act_space in self.env.action_spaces.values()), \
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag892')" href="javascript:;">
ray-ray-1.9.2/rllib/env/wrappers/pettingzoo_env.py: 134-157
</a>
<div class="mid" id="frag892" style="display:none"><pre>

    def render(self, mode="human"):
        return self.env.render(mode)


class ParallelPettingZooEnv(MultiAgentEnv):
    def __init__(self, env):
        self.par_env = env
        # agent idx list
        self.agents = self.par_env.possible_agents

        # Get dictionaries of obs_spaces and act_spaces
        self.observation_spaces = self.par_env.observation_spaces
        self.action_spaces = self.par_env.action_spaces

        # Get first observation space, assuming all agents have equal space
        self.observation_space = self.observation_spaces[self.agents[0]]

        # Get first action space, assuming all agents have equal space
        self.action_space = self.action_spaces[self.agents[0]]

        assert all(obs_space == self.observation_space
                   for obs_space
                   in self.par_env.observation_spaces.values()), \
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 32:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag947')" href="javascript:;">
ray-ray-1.9.2/rllib/env/external_env.py: 79-109
</a>
<div class="mid" id="frag947" style="display:none"><pre>
    def start_episode(self,
                      episode_id: Optional[str] = None,
                      training_enabled: bool = True) -&gt; str:
        """Record the start of an episode.

        Args:
            episode_id: Unique string id for the episode or
                None for it to be auto-assigned and returned.
            training_enabled: Whether to use experiences for this
                episode to improve the policy.

        Returns:
            Unique string id for the episode.
        """

        if episode_id is None:
            episode_id = uuid.uuid4().hex

        if episode_id in self._finished:
            raise ValueError(
                "Episode {} has already completed.".format(episode_id))

        if episode_id in self._episodes:
            raise ValueError(
                "Episode {} is already started".format(episode_id))

        self._episodes[episode_id] = _ExternalEnvEpisode(
            episode_id, self._results_avail_condition, training_enabled)

        return episode_id

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag978')" href="javascript:;">
ray-ray-1.9.2/rllib/env/external_multi_agent_env.py: 60-81
</a>
<div class="mid" id="frag978" style="display:none"><pre>
    def start_episode(self,
                      episode_id: Optional[str] = None,
                      training_enabled: bool = True) -&gt; str:
        if episode_id is None:
            episode_id = uuid.uuid4().hex

        if episode_id in self._finished:
            raise ValueError(
                "Episode {} has already completed.".format(episode_id))

        if episode_id in self._episodes:
            raise ValueError(
                "Episode {} is already started".format(episode_id))

        self._episodes[episode_id] = _ExternalEnvEpisode(
            episode_id,
            self._results_avail_condition,
            training_enabled,
            multiagent=True)

        return episode_id

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 33:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag962')" href="javascript:;">
ray-ray-1.9.2/rllib/env/tests/test_remote_worker_envs.py: 39-68
</a>
<div class="mid" id="frag962" style="display:none"><pre>

    def test_remote_worker_env(self):
        config = pg.DEFAULT_CONFIG.copy()
        config["remote_worker_envs"] = True
        config["num_envs_per_worker"] = 4

        # Simple string env definition (gym.make(...)).
        config["env"] = "CartPole-v0"
        trainer = pg.PGTrainer(config=config)
        print(trainer.train())
        trainer.stop()

        # Using tune.register.
        config["env"] = "cartpole"
        trainer = pg.PGTrainer(config=config)
        print(trainer.train())
        trainer.stop()

        # Using class directly.
        config["env"] = RandomEnv
        trainer = pg.PGTrainer(config=config)
        print(trainer.train())
        trainer.stop()

        # Using class directly: Sub-class of gym.Env,
        # which implements its own API.
        config["env"] = NonVectorizedEnvToBeVectorizedIntoRemoteVectorEnv
        trainer = pg.PGTrainer(config=config)
        print(trainer.train())
        trainer.stop()
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag963')" href="javascript:;">
ray-ray-1.9.2/rllib/env/tests/test_remote_worker_envs.py: 69-92
</a>
<div class="mid" id="frag963" style="display:none"><pre>

    def test_remote_worker_env_multi_agent(self):
        config = pg.DEFAULT_CONFIG.copy()
        config["remote_worker_envs"] = True
        config["num_envs_per_worker"] = 4

        # Full classpath provided.
        config["env"] = \
            "ray.rllib.examples.env.random_env.RandomMultiAgentEnv"
        trainer = pg.PGTrainer(config=config)
        print(trainer.train())
        trainer.stop()

        # Using tune.register.
        config["env"] = "pistonball"
        trainer = pg.PGTrainer(config=config)
        print(trainer.train())
        trainer.stop()

        # Using class directly.
        config["env"] = RandomMultiAgentEnv
        trainer = pg.PGTrainer(config=config)
        print(trainer.train())
        trainer.stop()
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 34:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 85%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag964')" href="javascript:;">
ray-ray-1.9.2/rllib/env/tests/test_record_env_wrapper.py: 11-31
</a>
<div class="mid" id="frag964" style="display:none"><pre>
    def test_wrap_gym_env(self):
        wrapped = record_env_wrapper(
            env=MockEnv2(10),
            record_env=tempfile.gettempdir(),
            log_dir="",
            policy_config={
                "in_evaluation": False,
            })
        # Type is wrappers.Monitor.
        self.assertTrue(isinstance(wrapped, wrappers.Monitor))
        self.assertFalse(isinstance(wrapped, VideoMonitor))

        wrapped.reset()
        # 10 steps for a complete episode.
        for i in range(10):
            wrapped.step(0)

        # MockEnv2 returns a reward of 100.0 every step.
        # So total reward is 1000.0.
        self.assertEqual(wrapped.get_episode_rewards(), [1000.0])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag965')" href="javascript:;">
ray-ray-1.9.2/rllib/env/tests/test_record_env_wrapper.py: 32-53
</a>
<div class="mid" id="frag965" style="display:none"><pre>
    def test_wrap_multi_agent_env(self):
        wrapped = record_env_wrapper(
            env=BasicMultiAgent(3),
            record_env=tempfile.gettempdir(),
            log_dir="",
            policy_config={
                "in_evaluation": False,
            })
        # Type is VideoMonitor.
        self.assertTrue(isinstance(wrapped, wrappers.Monitor))
        self.assertTrue(isinstance(wrapped, VideoMonitor))

        wrapped.reset()
        # BasicMultiAgent is hardcoded to run 25-step episodes.
        for i in range(25):
            wrapped.step({0: 0, 1: 0, 2: 0})

        # However VideoMonitor's _after_step is overwritten to not
        # use stats_recorder. So nothing to verify here, except that
        # it runs fine.


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 35:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1088')" href="javascript:;">
ray-ray-1.9.2/rllib/utils/tests/test_taskpool.py: 28-47
</a>
<div class="mid" id="frag1088" style="display:none"><pre>
    def test_completed_prefetch_yieldsAllCompleteUpToDefaultLimit(
            self, rayWaitMock):
        # Load the pool with 1000 tasks, mock them all as complete and then
        # check that the first call to completed_prefetch only yields 999
        # items and the second call yields the final one
        pool = TaskPool()
        for i in range(1000):
            task = createMockWorkerAndObjectRef(i)
            pool.add(*task)

        rayWaitMock.return_value = (list(range(1000)), [])

        # For this test, we're only checking the object refs
        fetched = [pair[1] for pair in pool.completed_prefetch()]
        self.assertListEqual(fetched, list(range(999)))

        # Finally, check the next iteration returns the final taks
        fetched = [pair[1] for pair in pool.completed_prefetch()]
        self.assertListEqual(fetched, [999])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1089')" href="javascript:;">
ray-ray-1.9.2/rllib/utils/tests/test_taskpool.py: 49-69
</a>
<div class="mid" id="frag1089" style="display:none"><pre>
    def test_completed_prefetch_yieldsAllCompleteUpToSpecifiedLimit(
            self, rayWaitMock):
        # Load the pool with 1000 tasks, mock them all as complete and then
        # check that the first call to completed_prefetch only yield 999 items
        # and the second call yields the final one
        pool = TaskPool()
        for i in range(1000):
            task = createMockWorkerAndObjectRef(i)
            pool.add(*task)

        rayWaitMock.return_value = (list(range(1000)), [])

        # Verify that only the first 500 tasks are returned, this should leave
        # some tasks in the _fetching deque for later
        fetched = [pair[1] for pair in pool.completed_prefetch(max_yield=500)]
        self.assertListEqual(fetched, list(range(500)))

        # Finally, check the next iteration returns the remaining tasks
        fetched = [pair[1] for pair in pool.completed_prefetch()]
        self.assertListEqual(fetched, list(range(500, 1000)))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 36:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1094')" href="javascript:;">
ray-ray-1.9.2/rllib/utils/schedules/polynomial_schedule.py: 13-37
</a>
<div class="mid" id="frag1094" style="display:none"><pre>
    def __init__(self,
                 schedule_timesteps,
                 final_p,
                 framework,
                 initial_p=1.0,
                 power=2.0):
        """
        Polynomial interpolation between initial_p and final_p over
        schedule_timesteps. After this many time steps, always `final_p` is
        returned.

        Agrs:
            schedule_timesteps (int): Number of time steps for which to
                linearly anneal initial_p to final_p
            final_p (float): Final output value.
            initial_p (float): Initial output value.
            framework (Optional[str]): One of "tf", "torch", or None.
        """
        super().__init__(framework=framework)
        assert schedule_timesteps &gt; 0
        self.schedule_timesteps = schedule_timesteps
        self.final_p = final_p
        self.initial_p = initial_p
        self.power = power

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1097')" href="javascript:;">
ray-ray-1.9.2/rllib/utils/schedules/exponential_schedule.py: 9-34
</a>
<div class="mid" id="frag1097" style="display:none"><pre>
    def __init__(self,
                 schedule_timesteps,
                 framework,
                 initial_p=1.0,
                 decay_rate=0.1):
        """
        Exponential decay schedule from initial_p to final_p over
        schedule_timesteps. After this many time steps always `final_p` is
        returned.

        Agrs:
            schedule_timesteps (int): Number of time steps for which to
                linearly anneal initial_p to final_p
            initial_p (float): Initial output value.
            decay_rate (float): The percentage of the original value after
                100% of the time has been reached (see formula above).
                &gt;0.0: The smaller the decay-rate, the stronger the decay.
                1.0: No decay at all.
            framework (Optional[str]): One of "tf", "torch", or None.
        """
        super().__init__(framework=framework)
        assert schedule_timesteps &gt; 0
        self.schedule_timesteps = schedule_timesteps
        self.initial_p = initial_p
        self.decay_rate = decay_rate

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 37:</b> &nbsp; 3 fragments, nominal size 16 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1100')" href="javascript:;">
ray-ray-1.9.2/rllib/utils/schedules/tests/test_schedules.py: 33-50
</a>
<div class="mid" id="frag1100" style="display:none"><pre>
                check(out, value, decimals=4)

    def test_linear_schedule(self):
        ts = [0, 50, 10, 100, 90, 2, 1, 99, 23, 1000]
        expected = [2.1 - (min(t, 100) / 100) * (2.1 - 0.6) for t in ts]
        config = {"schedule_timesteps": 100, "initial_p": 2.1, "final_p": 0.6}

        for fw in framework_iterator(
                frameworks=["tf2", "tf", "tfe", "torch", None]):
            linear = from_config(LinearSchedule, config, framework=fw)
            for t, e in zip(ts, expected):
                out = linear(t)
                check(out, e, decimals=4)

            ts_as_tensors = self._get_framework_tensors(ts, fw)
            for t, e in zip(ts_as_tensors, expected):
                out = linear(t)
                assert fw != "tf" or isinstance(out, tf.Tensor)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1103')" href="javascript:;">
ray-ray-1.9.2/rllib/utils/schedules/tests/test_schedules.py: 98-117
</a>
<div class="mid" id="frag1103" style="display:none"><pre>
                check(out, e, decimals=4)

    def test_piecewise_schedule(self):
        ts = [0, 5, 10, 100, 90, 2, 1, 99, 27]
        expected = [50.0, 60.0, 70.0, 14.5, 14.5, 54.0, 52.0, 14.5, 140.0]
        config = dict(
            endpoints=[(0, 50.0), (25, 100.0), (30, 200.0)],
            outside_value=14.5)

        for fw in framework_iterator(
                frameworks=["tf2", "tf", "tfe", "torch", None]):
            piecewise = from_config(PiecewiseSchedule, config, framework=fw)
            for t, e in zip(ts, expected):
                out = piecewise(t)
                check(out, e, decimals=4)

            ts_as_tensors = self._get_framework_tensors(ts, fw)
            for t, e in zip(ts_as_tensors, expected):
                out = piecewise(t)
                assert fw != "tf" or isinstance(out, tf.Tensor)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1102')" href="javascript:;">
ray-ray-1.9.2/rllib/utils/schedules/tests/test_schedules.py: 77-97
</a>
<div class="mid" id="frag1102" style="display:none"><pre>
                check(out, e, decimals=4)

    def test_exponential_schedule(self):
        decay_rate = 0.2
        ts = [0, 5, 10, 100, 90, 2, 1, 99, 23]
        expected = [2.0 * decay_rate**(t / 100) for t in ts]
        config = dict(
            initial_p=2.0, decay_rate=decay_rate, schedule_timesteps=100)

        for fw in framework_iterator(
                frameworks=["tf2", "tf", "tfe", "torch", None]):
            exponential = from_config(
                ExponentialSchedule, config, framework=fw)
            for t, e in zip(ts, expected):
                out = exponential(t)
                check(out, e, decimals=4)

            ts_as_tensors = self._get_framework_tensors(ts, fw)
            for t, e in zip(ts_as_tensors, expected):
                out = exponential(t)
                assert fw != "tf" or isinstance(out, tf.Tensor)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 38:</b> &nbsp; 4 fragments, nominal size 12 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1199')" href="javascript:;">
ray-ray-1.9.2/rllib/tests/test_eager_support.py: 107-119
</a>
<div class="mid" id="frag1199" style="display:none"><pre>
    def test_apex_dqn(self):
        check_support(
            "APEX", {
                "num_workers": 2,
                "learning_starts": 0,
                "num_gpus": 0,
                "min_iter_time_s": 1,
                "timesteps_per_iteration": 100,
                "optimizer": {
                    "num_replay_buffer_shards": 1,
                },
            })

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1252')" href="javascript:;">
ray-ray-1.9.2/rllib/tests/test_supported_multi_agent.py: 107-119
</a>
<div class="mid" id="frag1252" style="display:none"><pre>
                },
            })

    def test_apex_ddpg_multiagent(self):
        check_support_multiagent(
            "APEX_DDPG", {
                "num_workers": 2,
                "timesteps_per_iteration": 100,
                "buffer_size": 1000,
                "num_gpus": 0,
                "min_iter_time_s": 1,
                "learning_starts": 10,
                "target_network_update_freq": 100,
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1251')" href="javascript:;">
ray-ray-1.9.2/rllib/tests/test_supported_multi_agent.py: 92-106
</a>
<div class="mid" id="frag1251" style="display:none"><pre>
    def tearDownClass(cls) -&gt; None:
        ray.shutdown()

    def test_apex_multiagent(self):
        check_support_multiagent(
            "APEX", {
                "num_workers": 2,
                "timesteps_per_iteration": 100,
                "num_gpus": 0,
                "buffer_size": 1000,
                "min_iter_time_s": 1,
                "learning_starts": 10,
                "target_network_update_freq": 100,
                "optimizer": {
                    "num_replay_buffer_shards": 1,
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1273')" href="javascript:;">
ray-ray-1.9.2/rllib/tests/test_ignore_worker_failure.py: 91-104
</a>
<div class="mid" id="frag1273" style="display:none"><pre>
    def test_async_replay(self):
        self.do_test(
            "APEX", {
                "timesteps_per_iteration": 1000,
                "num_gpus": 0,
                "min_iter_time_s": 1,
                "explore": False,
                "learning_starts": 1000,
                "target_network_update_freq": 100,
                "optimizer": {
                    "num_replay_buffer_shards": 1,
                },
            })

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 39:</b> &nbsp; 2 fragments, nominal size 24 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1205')" href="javascript:;">
ray-ray-1.9.2/rllib/tests/test_model_imports.py: 25-51
</a>
<div class="mid" id="frag1205" style="display:none"><pre>
    def __init__(self, obs_space, action_space, num_outputs, model_config,
                 name):
        super(MyKerasModel, self).__init__(obs_space, action_space,
                                           num_outputs, model_config, name)
        self.inputs = tf.keras.layers.Input(
            shape=obs_space.shape, name="observations")
        layer_1 = tf.keras.layers.Dense(
            16,
            name="layer1",
            activation=tf.nn.relu,
            kernel_initializer=normc_initializer(1.0))(self.inputs)
        layer_out = tf.keras.layers.Dense(
            num_outputs,
            name="out",
            activation=None,
            kernel_initializer=normc_initializer(0.01))(layer_1)
        if self.model_config["vf_share_layers"]:
            value_out = tf.keras.layers.Dense(
                1,
                name="value",
                activation=None,
                kernel_initializer=normc_initializer(0.01))(layer_1)
            self.base_model = tf.keras.Model(self.inputs,
                                             [layer_out, value_out])
        else:
            self.base_model = tf.keras.Model(self.inputs, layer_out)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1881')" href="javascript:;">
ray-ray-1.9.2/rllib/examples/custom_keras_model.py: 33-55
</a>
<div class="mid" id="frag1881" style="display:none"><pre>
    """Custom model for policy gradient algorithms."""

    def __init__(self, obs_space, action_space, num_outputs, model_config,
                 name):
        super(MyKerasModel, self).__init__(obs_space, action_space,
                                           num_outputs, model_config, name)
        self.inputs = tf.keras.layers.Input(
            shape=obs_space.shape, name="observations")
        layer_1 = tf.keras.layers.Dense(
            128,
            name="my_layer1",
            activation=tf.nn.relu,
            kernel_initializer=normc_initializer(1.0))(self.inputs)
        layer_out = tf.keras.layers.Dense(
            num_outputs,
            name="my_out",
            activation=None,
            kernel_initializer=normc_initializer(0.01))(layer_1)
        value_out = tf.keras.layers.Dense(
            1,
            name="value_out",
            activation=None,
            kernel_initializer=normc_initializer(0.01))(layer_1)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 40:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1269')" href="javascript:;">
ray-ray-1.9.2/rllib/tests/test_ignore_worker_failure.py: 53-68
</a>
<div class="mid" id="frag1269" style="display:none"><pre>
    def _do_test_fault_recover(self, alg, config):
        register_env("fault_env", lambda c: FaultInjectEnv(c))
        agent_cls = get_trainer_class(alg)

        # Test fault handling
        config["num_workers"] = 2
        config["ignore_worker_failures"] = True
        # Make worker idx=1 fail. Other workers will be ok.
        config["env_config"] = {"bad_indices": [1]}

        for _ in framework_iterator(config, frameworks=("torch", "tf")):
            a = agent_cls(config=config, env="fault_env")
            result = a.train()
            self.assertTrue(result["num_healthy_workers"], 1)
            a.stop()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1270')" href="javascript:;">
ray-ray-1.9.2/rllib/tests/test_ignore_worker_failure.py: 69-83
</a>
<div class="mid" id="frag1270" style="display:none"><pre>
    def _do_test_fault_fatal(self, alg, config):
        register_env("fault_env", lambda c: FaultInjectEnv(c))
        agent_cls = get_trainer_class(alg)

        # Test raises real error when out of workers
        config["num_workers"] = 2
        config["ignore_worker_failures"] = True
        # Make both worker idx=1 and 2 fail.
        config["env_config"] = {"bad_indices": [1, 2]}

        for _ in framework_iterator(config, frameworks=("torch", "tf")):
            a = agent_cls(config=config, env="fault_env")
            self.assertRaises(Exception, lambda: a.train())
            a.stop()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 41:</b> &nbsp; 3 fragments, nominal size 14 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1303')" href="javascript:;">
ray-ray-1.9.2/rllib/tests/test_external_env.py: 25-39
</a>
<div class="mid" id="frag1303" style="display:none"><pre>
        def run(self):
            eid = self.start_episode()
            obs = self.env.reset()
            while True:
                action = self.get_action(eid, obs)
                obs, reward, done, info = self.env.step(action)
                if multiagent:
                    self.log_returns(eid, reward)
                else:
                    self.log_returns(eid, reward, info=info)
                if done:
                    self.end_episode(eid, obs)
                    obs = self.env.reset()
                    eid = self.start_episode()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1305')" href="javascript:;">
ray-ray-1.9.2/rllib/tests/test_external_env.py: 53-69
</a>
<div class="mid" id="frag1305" style="display:none"><pre>
    def run(self):
        eid = self.start_episode()
        obs = self.env.reset()
        while True:
            if random.random() &lt; self.off_pol_frac:
                action = self.env.action_space.sample()
                self.log_action(eid, obs, action)
            else:
                action = self.get_action(eid, obs)
            obs, reward, done, info = self.env.step(action)
            self.log_returns(eid, reward, info=info)
            if done:
                self.end_episode(eid, obs)
                obs = self.env.reset()
                eid = self.start_episode()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1307')" href="javascript:;">
ray-ray-1.9.2/rllib/tests/test_external_env.py: 76-89
</a>
<div class="mid" id="frag1307" style="display:none"><pre>
    def run(self):
        eid = self.start_episode()
        obs = self.env.reset()
        while True:
            action = self.fixed_action
            self.log_action(eid, obs, action)
            obs, reward, done, info = self.env.step(action)
            self.log_returns(eid, reward, info=info)
            if done:
                self.end_episode(eid, obs)
                obs = self.env.reset()
                eid = self.start_episode()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 42:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 88%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1317')" href="javascript:;">
ray-ray-1.9.2/rllib/tests/test_external_env.py: 192-208
</a>
<div class="mid" id="frag1317" style="display:none"><pre>
    def test_train_cartpole(self):
        register_env("test", lambda _: SimpleServing(gym.make("CartPole-v0")))
        config = {"num_workers": 0}
        for _ in framework_iterator(config, frameworks=("tf", "torch")):
            pg = PGTrainer(env="test", config=config)
            reached = False
            for i in range(80):
                result = pg.train()
                print("Iteration {}, reward {}, timesteps {}".format(
                    i, result["episode_reward_mean"],
                    result["timesteps_total"]))
                if result["episode_reward_mean"] &gt;= 80:
                    reached = True
                    break
            if not reached:
                raise Exception("failed to improve reward")

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1318')" href="javascript:;">
ray-ray-1.9.2/rllib/tests/test_external_env.py: 209-226
</a>
<div class="mid" id="frag1318" style="display:none"><pre>
    def test_train_cartpole_multi(self):
        register_env("test2",
                     lambda _: MultiServing(lambda: gym.make("CartPole-v0")))
        config = {"num_workers": 0}
        for _ in framework_iterator(config, frameworks=("tf", "torch")):
            pg = PGTrainer(env="test2", config=config)
            reached = False
            for i in range(80):
                result = pg.train()
                print("Iteration {}, reward {}, timesteps {}".format(
                    i, result["episode_reward_mean"],
                    result["timesteps_total"]))
                if result["episode_reward_mean"] &gt;= 80:
                    reached = True
                    break
            if not reached:
                raise Exception("failed to improve reward")

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 43:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1323')" href="javascript:;">
ray-ray-1.9.2/rllib/tests/test_lstm.py: 82-97
</a>
<div class="mid" id="frag1323" style="display:none"><pre>
    def test_batch_id(self):
        eps_ids = [1, 1, 1, 5, 5, 5, 5, 5]
        batch_ids = [1, 1, 2, 2, 3, 3, 4, 4]
        agent_ids = [1, 1, 1, 1, 1, 1, 1, 1]
        f = [[101, 102, 103, 201, 202, 203, 204, 205],
             [[101], [102], [103], [201], [202], [203], [204], [205]]]
        s = [[209, 208, 207, 109, 108, 107, 106, 105]]
        _, _, seq_lens = chop_into_sequences(
            episode_ids=eps_ids,
            unroll_ids=batch_ids,
            agent_indices=agent_ids,
            feature_columns=f,
            state_columns=s,
            max_seq_len=4)
        self.assertEqual(seq_lens.tolist(), [2, 1, 1, 2, 2])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1324')" href="javascript:;">
ray-ray-1.9.2/rllib/tests/test_lstm.py: 98-115
</a>
<div class="mid" id="frag1324" style="display:none"><pre>
    def test_multi_agent(self):
        eps_ids = [1, 1, 1, 5, 5, 5, 5, 5]
        agent_ids = [1, 1, 2, 1, 1, 2, 2, 3]
        f = [[101, 102, 103, 201, 202, 203, 204, 205],
             [[101], [102], [103], [201], [202], [203], [204], [205]]]
        s = [[209, 208, 207, 109, 108, 107, 106, 105]]
        f_pad, s_init, seq_lens = chop_into_sequences(
            episode_ids=eps_ids,
            unroll_ids=np.ones_like(eps_ids),
            agent_indices=agent_ids,
            feature_columns=f,
            state_columns=s,
            max_seq_len=4,
            dynamic_max=False)
        self.assertEqual(seq_lens.tolist(), [2, 1, 2, 2, 1])
        self.assertEqual(len(f_pad[0]), 20)
        self.assertEqual(len(s_init[0]), 5)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 44:</b> &nbsp; 3 fragments, nominal size 13 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1382')" href="javascript:;">
ray-ray-1.9.2/rllib/tests/test_multi_agent_env.py: 185-202
</a>
<div class="mid" id="frag1382" style="display:none"><pre>
                         list(range(25)) * 6)

    def test_multi_agent_sample_sync_remote(self):
        ev = RolloutWorker(
            env_creator=lambda _: BasicMultiAgent(5),
            policy_spec={
                "p0": PolicySpec(policy_class=MockPolicy),
                "p1": PolicySpec(policy_class=MockPolicy),
            },
            # This signature will raise a soft-deprecation warning due
            # to the new signature we are using (agent_id, episode, **kwargs),
            # but should not break this test.
            policy_mapping_fn=(lambda agent_id: "p{}".format(agent_id % 2)),
            rollout_fragment_length=50,
            num_envs=4,
            remote_worker_envs=True,
            remote_env_batch_wait_ms=99999999)
        batch = ev.sample()
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1384')" href="javascript:;">
ray-ray-1.9.2/rllib/tests/test_multi_agent_env.py: 217-229
</a>
<div class="mid" id="frag1384" style="display:none"><pre>
        self.assertEqual(batch.count, 200)

    def test_multi_agent_sample_with_horizon(self):
        ev = RolloutWorker(
            env_creator=lambda _: BasicMultiAgent(5),
            policy_spec={
                "p0": PolicySpec(policy_class=MockPolicy),
                "p1": PolicySpec(policy_class=MockPolicy),
            },
            policy_mapping_fn=(lambda aid, **kwarg: "p{}".format(aid % 2)),
            episode_horizon=10,  # test with episode horizon set
            rollout_fragment_length=50)
        batch = ev.sample()
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1383')" href="javascript:;">
ray-ray-1.9.2/rllib/tests/test_multi_agent_env.py: 203-216
</a>
<div class="mid" id="frag1383" style="display:none"><pre>
        self.assertEqual(batch.count, 200)

    def test_multi_agent_sample_async_remote(self):
        ev = RolloutWorker(
            env_creator=lambda _: BasicMultiAgent(5),
            policy_spec={
                "p0": PolicySpec(policy_class=MockPolicy),
                "p1": PolicySpec(policy_class=MockPolicy),
            },
            policy_mapping_fn=(lambda aid, **kwargs: "p{}".format(aid % 2)),
            rollout_fragment_length=50,
            num_envs=4,
            remote_worker_envs=True)
        batch = ev.sample()
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 45:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1386')" href="javascript:;">
ray-ray-1.9.2/rllib/tests/test_multi_agent_env.py: 255-268
</a>
<div class="mid" id="frag1386" style="display:none"><pre>
        self.assertTrue(ag0_ts[-1] == ag1_ts[-1])

    def test_multi_agent_with_flex_agents(self):
        register_env("flex_agents_multi_agent_cartpole",
                     lambda _: FlexAgentsMultiAgent())
        pg = PGTrainer(
            env="flex_agents_multi_agent_cartpole",
            config={
                "num_workers": 0,
                "framework": "tf",
            })
        for i in range(10):
            result = pg.train()
            print("Iteration {}, reward {}, timesteps {}".format(
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1393')" href="javascript:;">
ray-ray-1.9.2/rllib/tests/test_multi_agent_env.py: 383-400
</a>
<div class="mid" id="frag1393" style="display:none"><pre>
        self.assertEqual(batch.policy_batches["p1"].count, 20)

    def test_train_multi_agent_cartpole_single_policy(self):
        n = 10
        register_env("multi_agent_cartpole",
                     lambda _: MultiAgentCartPole({"num_agents": n}))
        pg = PGTrainer(
            env="multi_agent_cartpole",
            config={
                "num_workers": 0,
                "framework": "tf",
            })
        for i in range(50):
            result = pg.train()
            print("Iteration {}, reward {}, timesteps {}".format(
                i, result["episode_reward_mean"], result["timesteps_total"]))
            if result["episode_reward_mean"] &gt;= 50 * n:
                return
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 46:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1389')" href="javascript:;">
ray-ray-1.9.2/rllib/tests/test_multi_agent_env.py: 297-307
</a>
<div class="mid" id="frag1389" style="display:none"><pre>

        class StatefulPolicy(RandomPolicy):
            def compute_actions(self,
                                obs_batch,
                                state_batches=None,
                                prev_action_batch=None,
                                prev_reward_batch=None,
                                episodes=None,
                                explore=True,
                                timestep=None,
                                **kwargs):
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1490')" href="javascript:;">
ray-ray-1.9.2/rllib/examples/rollout_worker_custom_workflow.py: 40-51
</a>
<div class="mid" id="frag1490" style="display:none"><pre>
    def compute_actions(self,
                        obs_batch,
                        state_batches=None,
                        prev_action_batch=None,
                        prev_reward_batch=None,
                        info_batch=None,
                        episodes=None,
                        **kwargs):
        # return random actions
        return np.array(
            [self.action_space.sample() for _ in obs_batch]), [], {}

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 47:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1432')" href="javascript:;">
ray-ray-1.9.2/rllib/tests/test_external_multi_agent_env.py: 25-37
</a>
<div class="mid" id="frag1432" style="display:none"><pre>
    def test_external_multi_agent_env_complete_episodes(self):
        agents = 4
        ev = RolloutWorker(
            env_creator=lambda _: SimpleMultiServing(BasicMultiAgent(agents)),
            policy_spec=MockPolicy,
            rollout_fragment_length=40,
            batch_mode="complete_episodes")
        for _ in range(3):
            batch = ev.sample()
            self.assertEqual(batch.count, 40)
            self.assertEqual(
                len(np.unique(batch[SampleBatch.AGENT_INDEX])), agents)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1433')" href="javascript:;">
ray-ray-1.9.2/rllib/tests/test_external_multi_agent_env.py: 38-50
</a>
<div class="mid" id="frag1433" style="display:none"><pre>
    def test_external_multi_agent_env_truncate_episodes(self):
        agents = 4
        ev = RolloutWorker(
            env_creator=lambda _: SimpleMultiServing(BasicMultiAgent(agents)),
            policy_spec=MockPolicy,
            rollout_fragment_length=40,
            batch_mode="truncate_episodes")
        for _ in range(3):
            batch = ev.sample()
            self.assertEqual(batch.count, 160)
            self.assertEqual(
                len(np.unique(batch[SampleBatch.AGENT_INDEX])), agents)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 48:</b> &nbsp; 2 fragments, nominal size 31 lines, similarity 93%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1463')" href="javascript:;">
ray-ray-1.9.2/rllib/evaluation/tests/test_postprocessing.py: 44-76
</a>
<div class="mid" id="frag1463" style="display:none"><pre>
    def test_n_step_4(self):
        """Tests, whether n-step adjustments of trajectories work."""
        # n-step = 4
        gamma = 0.99
        obs = np.arange(0, 7)
        actions = np.random.randint(-1, 3, size=(7, ))
        check_actions = actions.copy()
        rewards = [10.0, 0.0, 100.0, 50.0, 60.0, 10.0, 100.0]
        dones = [False, False, False, False, False, False, True]
        next_obs = np.arange(1, 8)
        batch = SampleBatch({
            SampleBatch.OBS: obs,
            SampleBatch.ACTIONS: actions,
            SampleBatch.REWARDS: rewards,
            SampleBatch.DONES: dones,
            SampleBatch.NEXT_OBS: next_obs,
        })
        adjust_nstep(4, gamma, batch)
        check(batch[SampleBatch.OBS], [0, 1, 2, 3, 4, 5, 6])
        check(batch[SampleBatch.ACTIONS], check_actions)
        check(batch[SampleBatch.NEXT_OBS], [4, 5, 6, 7, 7, 7, 7])
        check(batch[SampleBatch.DONES],
              [False, False, False, True, True, True, True])
        check(batch[SampleBatch.REWARDS], [
            discount_cumsum(np.array(rewards[0:4]), gamma)[0],
            discount_cumsum(np.array(rewards[1:5]), gamma)[0],
            discount_cumsum(np.array(rewards[2:6]), gamma)[0],
            discount_cumsum(np.array(rewards[3:7]), gamma)[0],
            discount_cumsum(np.array(rewards[4:]), gamma)[0],
            discount_cumsum(np.array(rewards[5:]), gamma)[0],
            discount_cumsum(np.array(rewards[6:]), gamma)[0],
        ])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1466')" href="javascript:;">
ray-ray-1.9.2/rllib/evaluation/tests/test_postprocessing.py: 117-155
</a>
<div class="mid" id="frag1466" style="display:none"><pre>
    def test_n_step_from_same_obs_source_array(self):
        """Tests, whether n-step also works on a shared obs/new-obs array."""
        gamma = 0.99
        # The underlying observation data. Both obs and next_obs will
        # be references into that same np.array.
        underlying_obs = np.arange(0, 8)
        obs = underlying_obs[:7]
        next_obs = underlying_obs[1:]

        actions = np.random.randint(-1, 3, size=(7, ))
        check_actions = actions.copy()
        rewards = [10.0, 0.0, 100.0, 50.0, 60.0, 10.0, 100.0]
        dones = [False, False, False, False, False, False, True]

        batch = SampleBatch({
            SampleBatch.OBS: obs,
            SampleBatch.ACTIONS: actions,
            SampleBatch.REWARDS: rewards,
            SampleBatch.DONES: dones,
            SampleBatch.NEXT_OBS: next_obs,
        })
        adjust_nstep(4, gamma, batch)

        check(batch[SampleBatch.OBS], [0, 1, 2, 3, 4, 5, 6])
        check(batch[SampleBatch.ACTIONS], check_actions)
        check(batch[SampleBatch.NEXT_OBS], [4, 5, 6, 7, 7, 7, 7])
        check(batch[SampleBatch.DONES],
              [False, False, False, True, True, True, True])
        check(batch[SampleBatch.REWARDS], [
            discount_cumsum(np.array(rewards[0:4]), gamma)[0],
            discount_cumsum(np.array(rewards[1:5]), gamma)[0],
            discount_cumsum(np.array(rewards[2:6]), gamma)[0],
            discount_cumsum(np.array(rewards[3:7]), gamma)[0],
            discount_cumsum(np.array(rewards[4:]), gamma)[0],
            discount_cumsum(np.array(rewards[5:]), gamma)[0],
            discount_cumsum(np.array(rewards[6:]), gamma)[0],
        ])


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 49:</b> &nbsp; 5 fragments, nominal size 49 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1483')" href="javascript:;">
ray-ray-1.9.2/rllib/examples/serving/cartpole_server.py: 46-120
</a>
<div class="mid" id="frag1483" style="display:none"><pre>
def get_cli_args():
    """Create CLI parser and return parsed arguments"""
    parser = argparse.ArgumentParser()

    # Example-specific args.
    parser.add_argument(
        "--port",
        type=int,
        default=SERVER_BASE_PORT,
        help="The base-port to use (on localhost). "
        f"Default is {SERVER_BASE_PORT}.")
    parser.add_argument(
        "--callbacks-verbose",
        action="store_true",
        help="Activates info-messages for different events on "
        "server/client (episode steps, postprocessing, etc..).")
    parser.add_argument(
        "--num-workers",
        type=int,
        default=2,
        help="The number of workers to use. Each worker will create "
        "its own listening socket for incoming experiences.")
    parser.add_argument(
        "--no-restore",
        action="store_true",
        help="Do not restore from a previously saved checkpoint (location of "
        "which is saved in `last_checkpoint_[algo-name].out`).")

    # General args.
    parser.add_argument(
        "--run",
        default="PPO",
        choices=["DQN", "PPO"],
        help="The RLlib-registered algorithm to use.")
    parser.add_argument("--num-cpus", type=int, default=3)
    parser.add_argument(
        "--framework",
        choices=["tf", "tf2", "tfe", "torch"],
        default="tf",
        help="The DL framework specifier.")
    parser.add_argument(
        "--stop-iters",
        type=int,
        default=200,
        help="Number of iterations to train.")
    parser.add_argument(
        "--stop-timesteps",
        type=int,
        default=500000,
        help="Number of timesteps to train.")
    parser.add_argument(
        "--stop-reward",
        type=float,
        default=80.0,
        help="Reward at which we stop training.")
    parser.add_argument(
        "--as-test",
        action="store_true",
        help="Whether this script should be run as a test: --stop-reward must "
        "be achieved within --stop-timesteps AND --stop-iters.")
    parser.add_argument(
        "--no-tune",
        action="store_true",
        help="Run without Tune using a manual train loop instead. Here,"
        "there is no TensorBoard support.")
    parser.add_argument(
        "--local-mode",
        action="store_true",
        help="Init Ray in local mode for easier debugging.")

    args = parser.parse_args()
    print(f"Running with following CLI args: {args}")
    return args


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1990')" href="javascript:;">
ray-ray-1.9.2/rllib/examples/attention_net.py: 55-110
</a>
<div class="mid" id="frag1990" style="display:none"><pre>
def get_cli_args():
    """Create CLI parser and return parsed arguments"""
    parser = argparse.ArgumentParser()

    # example-specific args
    parser.add_argument(
        "--no-attention",
        action="store_true",
        help="Do NOT use attention. For comparison: The agent will not learn.")
    parser.add_argument(
        "--env", choices=SUPPORTED_ENVS, default="RepeatAfterMeEnv")

    # general args
    parser.add_argument(
        "--run", default="PPO", help="The RLlib-registered algorithm to use.")
    parser.add_argument("--num-cpus", type=int, default=3)
    parser.add_argument(
        "--framework",
        choices=["tf", "tf2", "tfe", "torch"],
        default="tf",
        help="The DL framework specifier.")
    parser.add_argument(
        "--stop-iters",
        type=int,
        default=200,
        help="Number of iterations to train.")
    parser.add_argument(
        "--stop-timesteps",
        type=int,
        default=500000,
        help="Number of timesteps to train.")
    parser.add_argument(
        "--stop-reward",
        type=float,
        default=80.0,
        help="Reward at which we stop training.")
    parser.add_argument(
        "--as-test",
        action="store_true",
        help="Whether this script should be run as a test: --stop-reward must "
        "be achieved within --stop-timesteps AND --stop-iters.")
    parser.add_argument(
        "--no-tune",
        action="store_true",
        help="Run without Tune using a manual train loop instead. Here,"
        "there is no TensorBoard support.")
    parser.add_argument(
        "--local-mode",
        action="store_true",
        help="Init Ray in local mode for easier debugging.")

    args = parser.parse_args()
    print(f"Running with following CLI args: {args}")
    return args


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1940')" href="javascript:;">
ray-ray-1.9.2/rllib/examples/autoregressive_action_dist.py: 52-109
</a>
<div class="mid" id="frag1940" style="display:none"><pre>


def get_cli_args():
    """Create CLI parser and return parsed arguments"""
    parser = argparse.ArgumentParser()

    # example-specific arg: disable autoregressive action dist
    parser.add_argument(
        "--no-autoreg",
        action="store_true",
        help="Do NOT use an autoregressive action distribution but normal,"
        "independently distributed actions.")

    # general args
    parser.add_argument(
        "--run",
        type=str,
        default="PPO",
        help="The RLlib-registered algorithm to use.")
    parser.add_argument(
        "--framework",
        choices=["tf", "tf2", "tfe", "torch"],
        default="tf",
        help="The DL framework specifier.")
    parser.add_argument("--num-cpus", type=int, default=0)
    parser.add_argument(
        "--as-test",
        action="store_true",
        help="Whether this script should be run as a test: --stop-reward must "
        "be achieved within --stop-timesteps AND --stop-iters.")
    parser.add_argument(
        "--stop-iters",
        type=int,
        default=200,
        help="Number of iterations to train.")
    parser.add_argument(
        "--stop-timesteps",
        type=int,
        default=100000,
        help="Number of timesteps to train.")
    parser.add_argument(
        "--stop-reward",
        type=float,
        default=200.0,
        help="Reward at which we stop training.")
    parser.add_argument(
        "--no-tune",
        action="store_true",
        help="Run without Tune using a manual train loop instead. Here,"
        "there is no TensorBoard support.")
    parser.add_argument(
        "--local-mode",
        action="store_true",
        help="Init Ray in local mode for easier debugging.")

    args = parser.parse_args()
    print(f"Running with following CLI args: {args}")
    return args
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1889')" href="javascript:;">
ray-ray-1.9.2/rllib/examples/remote_envs_with_inference_done_on_main_node.py: 25-73
</a>
<div class="mid" id="frag1889" style="display:none"><pre>
def get_cli_args():
    """Create CLI parser and return parsed arguments"""
    parser = argparse.ArgumentParser()

    # example-specific args
    # This should be &gt;1, otherwise, remote envs make no sense.
    parser.add_argument("--num-envs-per-worker", type=int, default=4)

    # general args
    parser.add_argument(
        "--framework",
        choices=["tf", "tf2", "tfe", "torch"],
        default="tf",
        help="The DL framework specifier.")
    parser.add_argument(
        "--as-test",
        action="store_true",
        help="Whether this script should be run as a test: --stop-reward must "
        "be achieved within --stop-timesteps AND --stop-iters.")
    parser.add_argument(
        "--stop-iters",
        type=int,
        default=50,
        help="Number of iterations to train.")
    parser.add_argument(
        "--stop-timesteps",
        type=int,
        default=100000,
        help="Number of timesteps to train.")
    parser.add_argument(
        "--stop-reward",
        type=float,
        default=150.0,
        help="Reward at which we stop training.")
    parser.add_argument(
        "--no-tune",
        action="store_true",
        help="Run without Tune using a manual train loop instead. Here,"
        "there is no TensorBoard support.")
    parser.add_argument(
        "--local-mode",
        action="store_true",
        help="Init Ray in local mode for easier debugging.")

    args = parser.parse_args()
    print(f"Running with following CLI args: {args}")
    return args


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1868')" href="javascript:;">
ray-ray-1.9.2/rllib/examples/preprocessing_disabled.py: 21-68
</a>
<div class="mid" id="frag1868" style="display:none"><pre>
def get_cli_args():
    """Create CLI parser and return parsed arguments"""
    parser = argparse.ArgumentParser()

    # general args
    parser.add_argument(
        "--run", default="PPO", help="The RLlib-registered algorithm to use.")
    parser.add_argument("--num-cpus", type=int, default=3)
    parser.add_argument(
        "--framework",
        choices=["tf", "tf2", "tfe", "torch"],
        default="tf",
        help="The DL framework specifier.")
    parser.add_argument(
        "--stop-iters",
        type=int,
        default=200,
        help="Number of iterations to train.")
    parser.add_argument(
        "--stop-timesteps",
        type=int,
        default=500000,
        help="Number of timesteps to train.")
    parser.add_argument(
        "--stop-reward",
        type=float,
        default=80.0,
        help="Reward at which we stop training.")
    parser.add_argument(
        "--as-test",
        action="store_true",
        help="Whether this script should be run as a test: --stop-reward must "
        "be achieved within --stop-timesteps AND --stop-iters.")
    parser.add_argument(
        "--no-tune",
        action="store_true",
        help="Run without Tune using a manual train loop instead. Here,"
        "there is no TensorBoard support.")
    parser.add_argument(
        "--local-mode",
        action="store_true",
        help="Init Ray in local mode for easier debugging.")

    args = parser.parse_args()
    print(f"Running with following CLI args: {args}")
    return args


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 50:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1561')" href="javascript:;">
ray-ray-1.9.2/rllib/examples/models/shared_weights_model.py: 67-84
</a>
<div class="mid" id="frag1561" style="display:none"><pre>
    def __init__(self, observation_space, action_space, num_outputs,
                 model_config, name):
        super().__init__(observation_space, action_space, num_outputs,
                         model_config, name)

        inputs = tf.keras.layers.Input(observation_space.shape)
        with tf1.variable_scope(
                tf1.VariableScope(tf1.AUTO_REUSE, "shared"),
                reuse=tf1.AUTO_REUSE,
                auxiliary_name_scope=False):
            last_layer = tf.keras.layers.Dense(
                units=64, activation=tf.nn.relu, name="fc1")(inputs)
        output = tf.keras.layers.Dense(
            units=num_outputs, activation=None, name="fc_out")(last_layer)
        vf = tf.keras.layers.Dense(
            units=1, activation=None, name="value_out")(last_layer)
        self.base_model = tf.keras.models.Model(inputs, [output, vf])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1564')" href="javascript:;">
ray-ray-1.9.2/rllib/examples/models/shared_weights_model.py: 98-117
</a>
<div class="mid" id="frag1564" style="display:none"><pre>
    def __init__(self, observation_space, action_space, num_outputs,
                 model_config, name):
        super().__init__(observation_space, action_space, num_outputs,
                         model_config, name)

        inputs = tf.keras.layers.Input(observation_space.shape)

        # Weights shared with SharedWeightsModel1.
        with tf1.variable_scope(
                tf1.VariableScope(tf1.AUTO_REUSE, "shared"),
                reuse=tf1.AUTO_REUSE,
                auxiliary_name_scope=False):
            last_layer = tf.keras.layers.Dense(
                units=64, activation=tf.nn.relu, name="fc1")(inputs)
        output = tf.keras.layers.Dense(
            units=num_outputs, activation=None, name="fc_out")(last_layer)
        vf = tf.keras.layers.Dense(
            units=1, activation=None, name="value_out")(last_layer)
        self.base_model = tf.keras.models.Model(inputs, [output, vf])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 51:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 85%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1570')" href="javascript:;">
ray-ray-1.9.2/rllib/examples/models/parametric_actions_model.py: 23-37
</a>
<div class="mid" id="frag1570" style="display:none"><pre>
    """

    def __init__(self,
                 obs_space,
                 action_space,
                 num_outputs,
                 model_config,
                 name,
                 true_obs_shape=(4, ),
                 action_embed_size=2,
                 **kw):
        super(ParametricActionsModel, self).__init__(
            obs_space, action_space, num_outputs, model_config, name, **kw)
        self.action_embed_model = FullyConnectedNetwork(
            Box(-1, 1, shape=true_obs_shape), action_space, action_embed_size,
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1573')" href="javascript:;">
ray-ray-1.9.2/rllib/examples/models/parametric_actions_model.py: 66-81
</a>
<div class="mid" id="frag1573" style="display:none"><pre>
    """PyTorch version of above ParametricActionsModel."""

    def __init__(self,
                 obs_space,
                 action_space,
                 num_outputs,
                 model_config,
                 name,
                 true_obs_shape=(4, ),
                 action_embed_size=2,
                 **kw):
        DQNTorchModel.__init__(self, obs_space, action_space, num_outputs,
                               model_config, name, **kw)

        self.action_embed_model = TorchFC(
            Box(-1, 1, shape=true_obs_shape), action_space, action_embed_size,
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 52:</b> &nbsp; 9 fragments, nominal size 13 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1790')" href="javascript:;">
ray-ray-1.9.2/rllib/examples/env/dm_control_suite.py: 7-21
</a>
<div class="mid" id="frag1790" style="display:none"><pre>
def acrobot_swingup(from_pixels=True,
                    height=64,
                    width=64,
                    frame_skip=2,
                    channels_first=True):
    return DMCEnv(
        "acrobot",
        "swingup",
        from_pixels=from_pixels,
        height=height,
        width=width,
        frame_skip=frame_skip,
        channels_first=channels_first)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1793')" href="javascript:;">
ray-ray-1.9.2/rllib/examples/env/dm_control_suite.py: 52-66
</a>
<div class="mid" id="frag1793" style="display:none"><pre>
def hopper_stand(from_pixels=True,
                 height=64,
                 width=64,
                 frame_skip=2,
                 channels_first=True):
    return DMCEnv(
        "hopper",
        "stand",
        from_pixels=from_pixels,
        height=height,
        width=width,
        frame_skip=frame_skip,
        channels_first=channels_first)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1794')" href="javascript:;">
ray-ray-1.9.2/rllib/examples/env/dm_control_suite.py: 67-81
</a>
<div class="mid" id="frag1794" style="display:none"><pre>
def cheetah_run(from_pixels=True,
                height=64,
                width=64,
                frame_skip=2,
                channels_first=True):
    return DMCEnv(
        "cheetah",
        "run",
        from_pixels=from_pixels,
        height=height,
        width=width,
        frame_skip=frame_skip,
        channels_first=channels_first)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1798')" href="javascript:;">
ray-ray-1.9.2/rllib/examples/env/dm_control_suite.py: 127-139
</a>
<div class="mid" id="frag1798" style="display:none"><pre>
def humanoid_walk(from_pixels=True,
                  height=64,
                  width=64,
                  frame_skip=2,
                  channels_first=True):
    return DMCEnv(
        "humanoid",
        "walk",
        from_pixels=from_pixels,
        height=height,
        width=width,
        frame_skip=frame_skip,
        channels_first=channels_first)
</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1791')" href="javascript:;">
ray-ray-1.9.2/rllib/examples/env/dm_control_suite.py: 22-36
</a>
<div class="mid" id="frag1791" style="display:none"><pre>
def walker_walk(from_pixels=True,
                height=64,
                width=64,
                frame_skip=2,
                channels_first=True):
    return DMCEnv(
        "walker",
        "walk",
        from_pixels=from_pixels,
        height=height,
        width=width,
        frame_skip=frame_skip,
        channels_first=channels_first)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1797')" href="javascript:;">
ray-ray-1.9.2/rllib/examples/env/dm_control_suite.py: 112-126
</a>
<div class="mid" id="frag1797" style="display:none"><pre>
def cartpole_swingup(from_pixels=True,
                     height=64,
                     width=64,
                     frame_skip=2,
                     channels_first=True):
    return DMCEnv(
        "cartpole",
        "swingup",
        from_pixels=from_pixels,
        height=height,
        width=width,
        frame_skip=frame_skip,
        channels_first=channels_first)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1796')" href="javascript:;">
ray-ray-1.9.2/rllib/examples/env/dm_control_suite.py: 97-111
</a>
<div class="mid" id="frag1796" style="display:none"><pre>
def pendulum_swingup(from_pixels=True,
                     height=64,
                     width=64,
                     frame_skip=2,
                     channels_first=True):
    return DMCEnv(
        "pendulum",
        "swingup",
        from_pixels=from_pixels,
        height=height,
        width=width,
        frame_skip=frame_skip,
        channels_first=channels_first)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1795')" href="javascript:;">
ray-ray-1.9.2/rllib/examples/env/dm_control_suite.py: 82-96
</a>
<div class="mid" id="frag1795" style="display:none"><pre>
def walker_run(from_pixels=True,
               height=64,
               width=64,
               frame_skip=2,
               channels_first=True):
    return DMCEnv(
        "walker",
        "run",
        from_pixels=from_pixels,
        height=height,
        width=width,
        frame_skip=frame_skip,
        channels_first=channels_first)


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1792')" href="javascript:;">
ray-ray-1.9.2/rllib/examples/env/dm_control_suite.py: 37-51
</a>
<div class="mid" id="frag1792" style="display:none"><pre>
def hopper_hop(from_pixels=True,
               height=64,
               width=64,
               frame_skip=2,
               channels_first=True):
    return DMCEnv(
        "hopper",
        "hop",
        from_pixels=from_pixels,
        height=height,
        width=width,
        frame_skip=frame_skip,
        channels_first=channels_first)


</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 53:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 84%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1809')" href="javascript:;">
ray-ray-1.9.2/rllib/examples/env/coin_game_vectorized_env.py: 100-117
</a>
<div class="mid" id="frag1809" style="display:none"><pre>
    @override(CoinGame)
    def _get_episode_info(self):

        player_red_info, player_blue_info = {}, {}

        if len(self.red_pick) &gt; 0:
            red_pick = sum(self.red_pick)
            player_red_info["pick_speed"] = \
                red_pick / (len(self.red_pick) * self.batch_size)
            if red_pick &gt; 0:
                player_red_info["pick_own_color"] = \
                    sum(self.red_pick_own) / red_pick

        if len(self.blue_pick) &gt; 0:
            blue_pick = sum(self.blue_pick)
            player_blue_info["pick_speed"] = \
                blue_pick / (len(self.blue_pick) * self.batch_size)
            if blue_pick &gt; 0:
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1848')" href="javascript:;">
ray-ray-1.9.2/rllib/examples/env/coin_game_non_vectorized_env.py: 267-290
</a>
<div class="mid" id="frag1848" style="display:none"><pre>
            info = {
                self.player_red_id: player_red_info,
                self.player_blue_id: player_blue_info,
            }
        else:
            info = {}

        return state, rewards, done, info

    @override(InfoAccumulationInterface)
    def _get_episode_info(self):
        """
        Output the following information:
        pick_speed is the fraction of steps during which the player picked a
        coin.
        pick_own_color is the fraction of coins picked by the player which have
        the same color as the player.
        """
        player_red_info, player_blue_info = {}, {}

        if len(self.red_pick) &gt; 0:
            red_pick = sum(self.red_pick)
            player_red_info["pick_speed"] = red_pick / len(self.red_pick)
            if red_pick &gt; 0:
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 54:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 77%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1826')" href="javascript:;">
ray-ray-1.9.2/rllib/examples/env/parametric_actions_cartpole.py: 63-83
</a>
<div class="mid" id="frag1826" style="display:none"><pre>
    def step(self, action):
        if action == self.left_idx:
            actual_action = 0
        elif action == self.right_idx:
            actual_action = 1
        else:
            raise ValueError(
                "Chosen action was not one of the non-zero action embeddings",
                action, self.action_assignments, self.action_mask,
                self.left_idx, self.right_idx)
        orig_obs, rew, done, info = self.wrapped.step(actual_action)
        self.update_avail_actions()
        self.action_mask = self.action_mask.astype(np.float32)
        obs = {
            "action_mask": self.action_mask,
            "avail_actions": self.action_assignments,
            "cart": orig_obs,
        }
        return obs, rew, done, info


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1829')" href="javascript:;">
ray-ray-1.9.2/rllib/examples/env/parametric_actions_cartpole.py: 118-133
</a>
<div class="mid" id="frag1829" style="display:none"><pre>
    def step(self, action):
        if action == self.left_idx:
            actual_action = 0
        elif action == self.right_idx:
            actual_action = 1
        else:
            raise ValueError(
                "Chosen action was not one of the non-zero action embeddings",
                action, self.valid_avail_actions_mask, self.left_idx,
                self.right_idx)
        orig_obs, rew, done, info = self.wrapped.step(actual_action)
        obs = {
            "valid_avail_actions_mask": self.valid_avail_actions_mask,
            "cart": orig_obs,
        }
        return obs, rew, done, info
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 55:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1916')" href="javascript:;">
ray-ray-1.9.2/rllib/examples/policy/episode_env_aware_policy.py: 59-76
</a>
<div class="mid" id="frag1916" style="display:none"><pre>

    @override(Policy)
    def compute_actions_from_input_dict(self,
                                        input_dict,
                                        explore=None,
                                        timestep=None,
                                        **kwargs):
        ts = input_dict["t"]
        print(ts)
        # Always return [episodeID, envID] as actions.
        actions = np.array([[
            input_dict[SampleBatch.AGENT_INDEX][i],
            input_dict[SampleBatch.EPS_ID][i], input_dict["env_id"][i]
        ] for i, _ in enumerate(input_dict["obs"])])
        states = [
            np.array([[ts[i]] for i in range(len(input_dict["obs"]))])
            for _ in range(2)
        ]
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1920')" href="javascript:;">
ray-ray-1.9.2/rllib/examples/policy/episode_env_aware_policy.py: 125-140
</a>
<div class="mid" id="frag1920" style="display:none"><pre>

    @override(Policy)
    def compute_actions_from_input_dict(self,
                                        input_dict,
                                        explore=None,
                                        timestep=None,
                                        **kwargs):
        ts = input_dict["t"]
        print(ts)
        # Always return [episodeID, envID] as actions.
        actions = np.array([[
            input_dict[SampleBatch.AGENT_INDEX][i],
            input_dict[SampleBatch.EPS_ID][i], input_dict["env_id"][i]
        ] for i, _ in enumerate(input_dict["obs"])])
        states = [np.array([[ts[i]] for i in range(len(input_dict["obs"]))])]
        self.global_timestep += 1
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 56:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2005')" href="javascript:;">
ray-ray-1.9.2/rllib/execution/tests/test_segment_tree.py: 8-21
</a>
<div class="mid" id="frag2005" style="display:none"><pre>
    def test_tree_set(self):
        tree = SumSegmentTree(4)

        tree[2] = 1.0
        tree[3] = 3.0

        assert np.isclose(tree.sum(), 4.0)
        assert np.isclose(tree.sum(0, 2), 0.0)
        assert np.isclose(tree.sum(0, 3), 1.0)
        assert np.isclose(tree.sum(2, 3), 1.0)
        assert np.isclose(tree.sum(2, -1), 1.0)
        assert np.isclose(tree.sum(2, 4), 4.0)
        assert np.isclose(tree.sum(2), 4.0)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2006')" href="javascript:;">
ray-ray-1.9.2/rllib/execution/tests/test_segment_tree.py: 22-34
</a>
<div class="mid" id="frag2006" style="display:none"><pre>
    def test_tree_set_overlap(self):
        tree = SumSegmentTree(4)

        tree[2] = 1.0
        tree[2] = 3.0

        assert np.isclose(tree.sum(), 3.0)
        assert np.isclose(tree.sum(2, 3), 3.0)
        assert np.isclose(tree.sum(2, -1), 3.0)
        assert np.isclose(tree.sum(2, 4), 3.0)
        assert np.isclose(tree.sum(2), 3.0)
        assert np.isclose(tree.sum(1, 2), 0.0)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 57:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2007')" href="javascript:;">
ray-ray-1.9.2/rllib/execution/tests/test_segment_tree.py: 35-47
</a>
<div class="mid" id="frag2007" style="display:none"><pre>
    def test_prefixsum_idx(self):
        tree = SumSegmentTree(4)

        tree[2] = 1.0
        tree[3] = 3.0

        assert tree.find_prefixsum_idx(0.0) == 2
        assert tree.find_prefixsum_idx(0.5) == 2
        assert tree.find_prefixsum_idx(0.99) == 2
        assert tree.find_prefixsum_idx(1.01) == 3
        assert tree.find_prefixsum_idx(3.00) == 3
        assert tree.find_prefixsum_idx(4.00) == 3

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2008')" href="javascript:;">
ray-ray-1.9.2/rllib/execution/tests/test_segment_tree.py: 48-62
</a>
<div class="mid" id="frag2008" style="display:none"><pre>
    def test_prefixsum_idx2(self):
        tree = SumSegmentTree(4)

        tree[0] = 0.5
        tree[1] = 1.0
        tree[2] = 1.0
        tree[3] = 3.0

        assert tree.find_prefixsum_idx(0.00) == 0
        assert tree.find_prefixsum_idx(0.55) == 1
        assert tree.find_prefixsum_idx(0.99) == 1
        assert tree.find_prefixsum_idx(1.51) == 2
        assert tree.find_prefixsum_idx(3.00) == 3
        assert tree.find_prefixsum_idx(5.50) == 3

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 58:</b> &nbsp; 10 fragments, nominal size 12 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2037')" href="javascript:;">
ray-ray-1.9.2/release/lightgbm_tests/workloads/train_small.py: 42-55
</a>
<div class="mid" id="frag2037" style="display:none"><pre>
    def train():
        os.environ["TEST_OUTPUT_JSON"] = output
        os.environ["TEST_STATE_JSON"] = state
        train_ray(
            path="/data/classification.parquet",
            num_workers=4,
            num_boost_rounds=100,
            num_files=25,
            regression=False,
            use_gpu=False,
            ray_params=ray_params,
            lightgbm_params=None,
        )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2039')" href="javascript:;">
ray-ray-1.9.2/release/lightgbm_tests/workloads/tune_small.py: 25-37
</a>
<div class="mid" id="frag2039" style="display:none"><pre>
def train_wrapper(config, ray_params):
    train_ray(
        path="/data/classification.parquet",
        num_workers=4,
        num_boost_rounds=100,
        num_files=25,
        regression=False,
        use_gpu=False,
        ray_params=ray_params,
        lightgbm_params=config,
    )


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2040')" href="javascript:;">
ray-ray-1.9.2/release/lightgbm_tests/workloads/tune_4x32.py: 25-37
</a>
<div class="mid" id="frag2040" style="display:none"><pre>
def train_wrapper(config, ray_params):
    train_ray(
        path="/data/classification.parquet",
        num_workers=32,
        num_boost_rounds=100,
        num_files=128,
        regression=False,
        use_gpu=False,
        ray_params=ray_params,
        lightgbm_params=config,
    )


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2112')" href="javascript:;">
ray-ray-1.9.2/release/xgboost_tests/workloads/train_small.py: 42-55
</a>
<div class="mid" id="frag2112" style="display:none"><pre>
    def train():
        os.environ["TEST_OUTPUT_JSON"] = output
        os.environ["TEST_STATE_JSON"] = state
        train_ray(
            path="/data/classification.parquet",
            num_workers=4,
            num_boost_rounds=100,
            num_files=25,
            regression=False,
            use_gpu=False,
            ray_params=ray_params,
            xgboost_params=None,
        )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2041')" href="javascript:;">
ray-ray-1.9.2/release/lightgbm_tests/workloads/tune_32x4.py: 25-37
</a>
<div class="mid" id="frag2041" style="display:none"><pre>
def train_wrapper(config, ray_params):
    train_ray(
        path="/data/classification.parquet",
        num_workers=4,
        num_boost_rounds=100,
        num_files=64,
        regression=False,
        use_gpu=False,
        ray_params=ray_params,
        lightgbm_params=config,
    )


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2081')" href="javascript:;">
ray-ray-1.9.2/release/ml_user_tests/xgboost/train_gpu_connect.py: 36-49
</a>
<div class="mid" id="frag2081" style="display:none"><pre>
    def train():
        os.environ["RXGB_PLACEMENT_GROUP_TIMEOUT_S"] = "1200"

        train_ray(
            path="/data/classification.parquet",
            num_workers=4,
            num_boost_rounds=100,
            num_files=25,
            regression=False,
            use_gpu=True,
            ray_params=ray_params,
            xgboost_params=None,
        )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2117')" href="javascript:;">
ray-ray-1.9.2/release/xgboost_tests/workloads/tune_32x4.py: 25-37
</a>
<div class="mid" id="frag2117" style="display:none"><pre>
def train_wrapper(config, ray_params):
    train_ray(
        path="/data/classification.parquet",
        num_workers=4,
        num_boost_rounds=100,
        num_files=64,
        regression=False,
        use_gpu=False,
        ray_params=ray_params,
        xgboost_params=config,
    )


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2116')" href="javascript:;">
ray-ray-1.9.2/release/xgboost_tests/workloads/tune_4x32.py: 25-37
</a>
<div class="mid" id="frag2116" style="display:none"><pre>
def train_wrapper(config, ray_params):
    train_ray(
        path="/data/classification.parquet",
        num_workers=32,
        num_boost_rounds=100,
        num_files=128,
        regression=False,
        use_gpu=False,
        ray_params=ray_params,
        xgboost_params=config,
    )


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2042')" href="javascript:;">
ray-ray-1.9.2/release/lightgbm_tests/workloads/train_small_connect.py: 34-45
</a>
<div class="mid" id="frag2042" style="display:none"><pre>
    def train():
        train_ray(
            path="/data/classification.parquet",
            num_workers=4,
            num_boost_rounds=100,
            num_files=25,
            regression=False,
            use_gpu=False,
            ray_params=ray_params,
            lightgbm_params=None,
        )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2115')" href="javascript:;">
ray-ray-1.9.2/release/xgboost_tests/workloads/tune_small.py: 25-37
</a>
<div class="mid" id="frag2115" style="display:none"><pre>
def train_wrapper(config, ray_params):
    train_ray(
        path="/data/classification.parquet",
        num_workers=4,
        num_boost_rounds=100,
        num_files=25,
        regression=False,
        use_gpu=False,
        ray_params=ray_params,
        xgboost_params=config,
    )


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 59:</b> &nbsp; 3 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2063')" href="javascript:;">
ray-ray-1.9.2/release/long_running_tests/workloads/actor_deaths.py: 73-84
</a>
<div class="mid" id="frag2063" style="display:none"><pre>
    def ping(self, num_pings):
        children_outputs = []
        for _ in range(num_pings):
            children_outputs += [
                child.ping.remote() for child in self.children
            ]
        try:
            ray.get(children_outputs)
        except Exception:
            # Replace the children if one of them died.
            self.__init__(len(self.children), self.death_probability)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5943')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_threaded_actor.py: 219-230
</a>
<div class="mid" id="frag5943" style="display:none"><pre>
        def ping(self, num_pings):
            children_outputs = []
            for _ in range(num_pings):
                children_outputs += [
                    child.ping.remote() for child in self.children
                ]
            try:
                ray.get(children_outputs)
            except Exception:
                # Replace the children if one of them died.
                self.__init__(len(self.children), self.death_probability)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2196')" href="javascript:;">
ray-ray-1.9.2/release/nightly_tests/stress_tests/test_dead_actors.py: 39-50
</a>
<div class="mid" id="frag2196" style="display:none"><pre>
    def ping(self, num_pings):
        children_outputs = []
        for _ in range(num_pings):
            children_outputs += [
                child.ping.remote() for child in self.children
            ]
        try:
            ray.get(children_outputs)
        except Exception:
            # Replace the children if one of them died.
            self.__init__(len(self.children), self.death_probability)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 60:</b> &nbsp; 3 fragments, nominal size 10 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2089')" href="javascript:;">
ray-ray-1.9.2/release/serve_tests/workloads/serve_test_utils.py: 16-45
</a>
<div class="mid" id="frag2089" style="display:none"><pre>
def parse_time_to_ms(time_string: str) -&gt; float:
    """Given a time string with various unit, convert
    to ms in float:

    wrk time unit reference
    https://github.com/wg/wrk/blob/master/src/units.c#L17-L21

        Example:
            "71.91ms" -&gt; 71.91
            "50us" -&gt; 0.05
            "1.5s" -&gt; 1500
    """
    # Group 1 - (one or more digits + optional dot + one or more digits)
    # 71.91 / 50 / 1.5
    # Group 2 - (All words)
    # ms / us / s
    parsed = re.split(r"(\d+.?\d+)(\w+)", time_string)
    values = [val for val in parsed if val]

    if values[1] == "ms":
        return float(values[0])
    elif values[1] == "us":
        return float(values[0]) / 1000
    elif values[1] == "s":
        return float(values[0]) * 1000

    # Should not return here in common benchmark
    return values[1]


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2090')" href="javascript:;">
ray-ray-1.9.2/release/serve_tests/workloads/serve_test_utils.py: 46-75
</a>
<div class="mid" id="frag2090" style="display:none"><pre>
def parse_size_to_KB(size_string: str) -&gt; float:
    """Given a size string with various unit, convert
    to KB in float:

    wrk binary unit reference
    https://github.com/wg/wrk/blob/master/src/units.c#L29-L33

        Example:
            "200.56KB" -&gt; 200.56
            "50MB" -&gt; 51200
            "0.5GB" -&gt; 524288
    """
    # Group 1 - (one or more digits + optional dot + one or more digits)
    # 200.56 / 50 / 0.5
    # Group 2 - (All words)
    # KB / MB / GB
    parsed = re.split(r"(\d+.?\d+)(\w*)", size_string)
    values = [val for val in parsed if val]

    if values[1] == "KB":
        return float(values[0])
    elif values[1] == "MB":
        return float(values[0]) * 1024
    elif values[1] == "GB":
        return float(values[0]) * 1024 * 1024

    # Bytes
    return float(values[0]) / 1000


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2091')" href="javascript:;">
ray-ray-1.9.2/release/serve_tests/workloads/serve_test_utils.py: 76-102
</a>
<div class="mid" id="frag2091" style="display:none"><pre>
def parse_metric_to_base(metric_string: str) -&gt; float:
    """Given a metric string with various unit, convert
    to original base

    wrk metric unit reference
    https://github.com/wg/wrk/blob/master/src/units.c#L35-L39

        Example:
            "71.91" -&gt; 71.91
            "1.32k" -&gt; 1320
            "1.5M" -&gt; 1500000
    """

    parsed = re.split(r"(\d+.?\d+)(\w*)", metric_string)
    values = [val for val in parsed if val]

    if len(values) == 1:
        return float(values[0])
    if values[1] == "k":
        return float(values[0]) * 1000
    elif values[1] == "M":
        return float(values[0]) * 1000 * 1000

    # Should not return here in common benchmark
    return values[1]


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 61:</b> &nbsp; 3 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2097')" href="javascript:;">
ray-ray-1.9.2/release/serve_tests/workloads/serve_cluster_fault_tolerance_gcs.py: 28-39
</a>
<div class="mid" id="frag2097" style="display:none"><pre>
def request_with_retries(endpoint, timeout=3):
    start = time.time()
    while True:
        try:
            return requests.get(
                "http://127.0.0.1:8000" + endpoint, timeout=timeout)
        except requests.RequestException:
            if time.time() - start &gt; timeout:
                raise TimeoutError
            time.sleep(0.1)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7841')" href="javascript:;">
ray-ray-1.9.2/python/ray/serve/tests/test_failure.py: 12-23
</a>
<div class="mid" id="frag7841" style="display:none"><pre>
def request_with_retries(endpoint, timeout=30):
    start = time.time()
    while True:
        try:
            return requests.get(
                "http://127.0.0.1:8000" + endpoint, timeout=timeout)
        except requests.RequestException:
            if time.time() - start &gt; timeout:
                raise TimeoutError
            time.sleep(0.1)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2101')" href="javascript:;">
ray-ray-1.9.2/release/serve_tests/workloads/serve_cluster_fault_tolerance.py: 30-41
</a>
<div class="mid" id="frag2101" style="display:none"><pre>
def request_with_retries(endpoint, timeout=3):
    start = time.time()
    while True:
        try:
            return requests.get(
                "http://127.0.0.1:8000" + endpoint, timeout=timeout)
        except requests.RequestException:
            if time.time() - start &gt; timeout:
                raise TimeoutError
            time.sleep(0.1)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 62:</b> &nbsp; 2 fragments, nominal size 38 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2098')" href="javascript:;">
ray-ray-1.9.2/release/serve_tests/workloads/serve_cluster_fault_tolerance_gcs.py: 41-113
</a>
<div class="mid" id="frag2098" style="display:none"><pre>
def main():
    # Setup local cluster, note this cluster setup is the same for both
    # local and product ray cluster env.
    # Each test uses different ray namespace, thus kv storage key for each
    # checkpoint is different to avoid collision.
    namespace = uuid.uuid4().hex

    # IS_SMOKE_TEST is set by args of releaser's e2e.py
    smoke_test = os.environ.get("IS_SMOKE_TEST", "1")
    if smoke_test == "1":
        checkpoint_path = "file://checkpoint.db"
    else:
        checkpoint_path = "gs://kazi_test/test/fault-tolerant-test-checkpoint"  # noqa: E501

    _, cluster = setup_local_single_node_cluster(
        1, checkpoint_path=checkpoint_path, namespace=namespace)

    # Deploy for the first time
    @serve.deployment(name="echo", num_replicas=DEFAULT_NUM_REPLICAS)
    class Echo:
        def __init__(self):
            return True

        def __call__(self, request):
            return "hii"

    Echo.deploy()

    # Ensure endpoint is working
    for _ in range(5):
        response = request_with_retries("/echo/", timeout=3)
        assert response.text == "hii"

    logger.info("Initial deployment successful with working endpoint.")

    # Kill current cluster, recover from remote checkpoint and ensure endpoint
    # is still available with expected results

    ray.kill(serve.api._global_client._controller, no_restart=True)
    ray.shutdown()
    cluster.shutdown()
    serve.api._set_global_client(None)

    # Start another ray cluster with same namespace to resume from previous
    # checkpoints with no new deploy() call.
    setup_local_single_node_cluster(
        1, checkpoint_path=checkpoint_path, namespace=namespace)

    for _ in range(5):
        response = request_with_retries("/echo/", timeout=3)
        assert response.text == "hii"

    logger.info("Deployment recovery from Google Cloud Storage checkpoint "
                "is successful with working endpoint.")

    # Delete dangling checkpoints. If script failed before this step, it's up
    # to the TTL policy on GCS to clean up, but won't lead to collision with
    # subsequent tests since each test run in different uuid namespace.
    serve.shutdown()
    ray.shutdown()
    cluster.shutdown()

    # Checkpoints in GCS bucket are moved after 7 days with explicit lifecycle
    # rules. Each checkpoint is ~260 Bytes in size from this test.

    # Save results
    save_test_results(
        {
            "result": "success"
        },
        default_output_file="/tmp/serve_cluster_fault_tolerance.json")


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2102')" href="javascript:;">
ray-ray-1.9.2/release/serve_tests/workloads/serve_cluster_fault_tolerance.py: 43-115
</a>
<div class="mid" id="frag2102" style="display:none"><pre>
def main():
    # Setup local cluster, note this cluster setup is the same for both
    # local and product ray cluster env.
    # Each test uses different ray namespace, thus kv storage key for each
    # checkpoint is different to avoid collision.
    namespace = uuid.uuid4().hex

    # IS_SMOKE_TEST is set by args of releaser's e2e.py
    smoke_test = os.environ.get("IS_SMOKE_TEST", "1")
    if smoke_test == "1":
        path = Path("checkpoint.db")
        checkpoint_path = f"file://{path}"
        if path.exists():
            path.unlink()
    else:
        checkpoint_path = "s3://serve-nightly-tests/fault-tolerant-test-checkpoint"  # noqa: E501

    _, cluster = setup_local_single_node_cluster(
        1, checkpoint_path=checkpoint_path, namespace=namespace)

    # Deploy for the first time
    @serve.deployment(num_replicas=DEFAULT_NUM_REPLICAS)
    def hello():
        return serve.get_replica_context().deployment

    for name in ["hello", "world"]:
        hello.options(name=name).deploy()

        for _ in range(5):
            response = request_with_retries(f"/{name}/", timeout=3)
            assert response.text == name

    logger.info("Initial deployment successful with working endpoint.")

    # Kill current cluster, recover from remote checkpoint and ensure endpoint
    # is still available with expected results

    ray.kill(serve.api._global_client._controller, no_restart=True)
    ray.shutdown()
    cluster.shutdown()
    serve.api._set_global_client(None)

    # Start another ray cluster with same namespace to resume from previous
    # checkpoints with no new deploy() call.
    setup_local_single_node_cluster(
        1, checkpoint_path=checkpoint_path, namespace=namespace)

    for name in ["hello", "world"]:
        for _ in range(5):
            response = request_with_retries(f"/{name}/", timeout=3)
            assert response.text == name

    logger.info("Deployment recovery from s3 checkpoint is successful "
                "with working endpoint.")

    # Delete dangling checkpoints. If script failed before this step, it's up
    # to the TTL policy on s3 to clean up, but won't lead to collision with
    # subsequent tests since each test run in different uuid namespace.
    serve.shutdown()
    ray.shutdown()
    cluster.shutdown()

    # Checkpoints in S3 bucket are moved after 7 days with explicit lifecycle
    # rules. Each checkpoint is ~260 Bytes in size from this test.

    # Save results
    save_test_results(
        {
            "result": "success"
        },
        default_output_file="/tmp/serve_cluster_fault_tolerance.json")


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 63:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2186')" href="javascript:;">
ray-ray-1.9.2/release/nightly_tests/stress_tests/test_many_tasks.py: 31-51
</a>
<div class="mid" id="frag2186" style="display:none"><pre>
def stage0(smoke=False):
    num_tasks = 1000
    size = 1000000

    if smoke:
        num_tasks //= 25
        size //= 25

    stage_0_iterations = []
    start_time = time.time()
    logger.info("Submitting many tasks with large returns.")
    for i in range(10):
        iteration_start = time.time()
        logger.info("Iteration %s", i)
        ray.get([f.remote(size) for _ in range(num_tasks)])
        stage_0_iterations.append(time.time() - iteration_start)

    return time.time() - start_time


# Stage 1: Launch a bunch of tasks.
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2187')" href="javascript:;">
ray-ray-1.9.2/release/nightly_tests/stress_tests/test_many_tasks.py: 52-72
</a>
<div class="mid" id="frag2187" style="display:none"><pre>
def stage1(smoke=False):
    num_tasks = 100000

    if smoke:
        num_tasks //= 25

    stage_1_iterations = []
    start_time = time.time()
    logger.info("Submitting many tasks.")
    for i in range(10):
        iteration_start = time.time()
        logger.info("Iteration %s", i)
        ray.get([f.remote(0) for _ in range(num_tasks)])
        stage_1_iterations.append(time.time() - iteration_start)

    return time.time() - start_time, stage_1_iterations


# Launch a bunch of tasks, each with a bunch of dependencies. TODO(rkn): This
# test starts to fail if we increase the number of tasks in the inner loop from
# 500 to 1000. (approximately 615 seconds)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 64:</b> &nbsp; 3 fragments, nominal size 13 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2221')" href="javascript:;">
ray-ray-1.9.2/release/tune_tests/scalability_tests/workloads/test_result_throughput_single_node.py: 22-40
</a>
<div class="mid" id="frag2221" style="display:none"><pre>
def main():
    os.environ["TUNE_DISABLE_AUTO_CALLBACK_LOGGERS"] = "1"  # Tweak

    ray.init(address="auto")

    num_samples = 96
    results_per_second = 50
    trial_length_s = 100

    max_runtime = 120

    timed_tune_run(
        name="result throughput single node",
        num_samples=num_samples,
        results_per_second=results_per_second,
        trial_length_s=trial_length_s,
        max_runtime=max_runtime)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2224')" href="javascript:;">
ray-ray-1.9.2/release/tune_tests/scalability_tests/workloads/test_bookkeeping_overhead.py: 22-40
</a>
<div class="mid" id="frag2224" style="display:none"><pre>
def main():
    os.environ["TUNE_GLOBAL_CHECKPOINT_S"] = "100"  # Tweak

    ray.init(address="auto")

    num_samples = 10000
    results_per_second = 1
    trial_length_s = 1

    max_runtime = 800

    timed_tune_run(
        name="bookkeeping overhead",
        num_samples=num_samples,
        results_per_second=results_per_second,
        trial_length_s=trial_length_s,
        max_runtime=max_runtime)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2228')" href="javascript:;">
ray-ray-1.9.2/release/tune_tests/scalability_tests/workloads/test_result_throughput_cluster.py: 24-47
</a>
<div class="mid" id="frag2228" style="display:none"><pre>
def main():
    os.environ["TUNE_DISABLE_AUTO_CALLBACK_LOGGERS"] = "1"  # Tweak

    ray.init(address="auto")

    num_samples = 1000
    results_per_second = 0.5
    trial_length_s = 100

    max_runtime = 120

    if is_ray_cluster():
        # Add constant overhead for SSH connection
        max_runtime = 120

    timed_tune_run(
        name="result throughput cluster",
        num_samples=num_samples,
        results_per_second=results_per_second,
        trial_length_s=trial_length_s,
        max_runtime=max_runtime,
        sync_config=tune.SyncConfig(syncer=None))  # Tweak!


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 65:</b> &nbsp; 4 fragments, nominal size 11 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2238')" href="javascript:;">
ray-ray-1.9.2/doc/kubernetes/example_scripts/job_example.py: 18-32
</a>
<div class="mid" id="frag2238" style="display:none"><pre>
def wait_for_nodes(expected):
    # Wait for all nodes to join the cluster.
    while True:
        resources = ray.cluster_resources()
        node_keys = [key for key in resources if "node" in key]
        num_nodes = sum(resources[node_key] for node_key in node_keys)
        if num_nodes &lt; expected:
            print("{} nodes have joined so far, waiting for {} more.".format(
                num_nodes, expected - num_nodes))
            sys.stdout.flush()
            time.sleep(1)
        else:
            break


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2244')" href="javascript:;">
ray-ray-1.9.2/doc/kubernetes/example_scripts/run_local_example.py: 25-39
</a>
<div class="mid" id="frag2244" style="display:none"><pre>
def wait_for_nodes(expected):
    # Wait for all nodes to join the cluster.
    while True:
        resources = ray.cluster_resources()
        node_keys = [key for key in resources if "node" in key]
        num_nodes = sum(resources[node_key] for node_key in node_keys)
        if num_nodes &lt; expected:
            print("{} nodes have joined so far, waiting for {} more.".format(
                num_nodes, expected - num_nodes))
            sys.stdout.flush()
            time.sleep(1)
        else:
            break


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2356')" href="javascript:;">
ray-ray-1.9.2/doc/yarn/example.py: 15-27
</a>
<div class="mid" id="frag2356" style="display:none"><pre>
def wait_for_nodes(expected):
    # Wait for all nodes to join the cluster.
    while True:
        num_nodes = len(ray.nodes())
        if num_nodes &lt; expected:
            print("{} nodes have joined so far, waiting for {} more.".format(
                num_nodes, expected - num_nodes))
            sys.stdout.flush()
            time.sleep(1)
        else:
            break


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2241')" href="javascript:;">
ray-ray-1.9.2/doc/kubernetes/example_scripts/run_on_head.py: 17-31
</a>
<div class="mid" id="frag2241" style="display:none"><pre>
def wait_for_nodes(expected):
    # Wait for all nodes to join the cluster.
    while True:
        resources = ray.cluster_resources()
        node_keys = [key for key in resources if "node" in key]
        num_nodes = sum(resources[node_key] for node_key in node_keys)
        if num_nodes &lt; expected:
            print("{} nodes have joined so far, waiting for {} more.".format(
                num_nodes, expected - num_nodes))
            sys.stdout.flush()
            time.sleep(1)
        else:
            break


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 66:</b> &nbsp; 4 fragments, nominal size 11 lines, similarity 91%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2239')" href="javascript:;">
ray-ray-1.9.2/doc/kubernetes/example_scripts/job_example.py: 33-48
</a>
<div class="mid" id="frag2239" style="display:none"><pre>
def main():
    wait_for_nodes(3)

    # Check that objects can be transferred from each node to each other node.
    for i in range(10):
        print("Iteration {}".format(i))
        results = [
            gethostname.remote(gethostname.remote(())) for _ in range(100)
        ]
        print(Counter(ray.get(results)))
        sys.stdout.flush()

    print("Success!")
    sys.stdout.flush()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2242')" href="javascript:;">
ray-ray-1.9.2/doc/kubernetes/example_scripts/run_on_head.py: 32-47
</a>
<div class="mid" id="frag2242" style="display:none"><pre>
def main():
    wait_for_nodes(3)

    # Check that objects can be transferred from each node to each other node.
    for i in range(10):
        print("Iteration {}".format(i))
        results = [
            gethostname.remote(gethostname.remote(())) for _ in range(100)
        ]
        print(Counter(ray.get(results)))
        sys.stdout.flush()

    print("Success!")
    sys.stdout.flush()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2357')" href="javascript:;">
ray-ray-1.9.2/doc/yarn/example.py: 28-44
</a>
<div class="mid" id="frag2357" style="display:none"><pre>
def main():
    wait_for_nodes(4)

    # Check that objects can be transferred from each node to each other node.
    for i in range(10):
        print("Iteration {}".format(i))
        results = [
            gethostname.remote(gethostname.remote(())) for _ in range(100)
        ]
        print(Counter(ray.get(results)))
        sys.stdout.flush()

    print("Success!")
    sys.stdout.flush()
    time.sleep(20)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2245')" href="javascript:;">
ray-ray-1.9.2/doc/kubernetes/example_scripts/run_local_example.py: 40-55
</a>
<div class="mid" id="frag2245" style="display:none"><pre>
def main():
    wait_for_nodes(3)

    # Check that objects can be transferred from each node to each other node.
    for i in range(10):
        print("Iteration {}".format(i))
        results = [
            gethostname.remote(gethostname.remote(())) for _ in range(100)
        ]
        print(Counter(ray.get(results)))
        sys.stdout.flush()

    print("Success!")
    sys.stdout.flush()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 67:</b> &nbsp; 5 fragments, nominal size 11 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2253')" href="javascript:;">
ray-ray-1.9.2/doc/source/tune/_tutorials/tune-serve-integration-mnist.py: 189-200
</a>
<div class="mid" id="frag2253" style="display:none"><pre>
def train(model, optimizer, train_loader, device=None):
    device = device or torch.device("cpu")
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2280')" href="javascript:;">
ray-ray-1.9.2/doc/examples/plot_hyperparameter.py: 100-116
</a>
<div class="mid" id="frag2280" style="display:none"><pre>
def train(model, optimizer, train_loader, device=torch.device("cpu")):
    """Optimize the model with one pass over the data.

    Cuts off at 1024 samples to simplify training.
    """
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        if batch_idx * len(data) &gt; 1024:
            return
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3984')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/examples/mnist_pytorch.py: 34-47
</a>
<div class="mid" id="frag3984" style="display:none"><pre>
def train(model, optimizer, train_loader, device=None):
    device = device or torch.device("cpu")
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        if batch_idx * len(data) &gt; EPOCH_SIZE:
            return
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2343')" href="javascript:;">
ray-ray-1.9.2/doc/examples/doc_code/torch_example.py: 46-59
</a>
<div class="mid" id="frag2343" style="display:none"><pre>
def train(model, device, train_loader, optimizer):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        # This break is for speeding up the tutorial.
        if batch_idx * len(data) &gt; 1024:
            return
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3707')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/tutorial.py: 45-59
</a>
<div class="mid" id="frag3707" style="display:none"><pre>
def train(model, optimizer, train_loader):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        # We set this just for the example to run quickly.
        if batch_idx * len(data) &gt; EPOCH_SIZE:
            return
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 68:</b> &nbsp; 4 fragments, nominal size 14 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2254')" href="javascript:;">
ray-ray-1.9.2/doc/source/tune/_tutorials/tune-serve-integration-mnist.py: 201-225
</a>
<div class="mid" id="frag2254" style="display:none"><pre>
def test(model, data_loader, device=None):
    device = device or torch.device("cpu")
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for batch_idx, (data, target) in enumerate(data_loader):
            data, target = data.to(device), target.to(device)
            outputs = model(data)
            _, predicted = torch.max(outputs.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()

    return correct / total


#######################################################################
# Tune trainable for model selection
# ----------------------------------
# We'll now define our Tune trainable function. This function takes
# a ``config`` parameter containing the hyperparameters we should train
# the model on, and will start a full training run. This means it
# will take care of creating the model and optimizer and repeatedly
# call the ``train`` function to train the model. Also, this function
# will report the training progress back to Tune.
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3985')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/examples/mnist_pytorch.py: 48-65
</a>
<div class="mid" id="frag3985" style="display:none"><pre>
def test(model, data_loader, device=None):
    device = device or torch.device("cpu")
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for batch_idx, (data, target) in enumerate(data_loader):
            if batch_idx * len(data) &gt; TEST_SIZE:
                break
            data, target = data.to(device), target.to(device)
            outputs = model(data)
            _, predicted = torch.max(outputs.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()

    return correct / total


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3708')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/tutorial.py: 60-80
</a>
<div class="mid" id="frag3708" style="display:none"><pre>
def test(model, data_loader):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for batch_idx, (data, target) in enumerate(data_loader):
            # We set this just for the example to run quickly.
            if batch_idx * len(data) &gt; TEST_SIZE:
                break
            data, target = data.to(device), target.to(device)
            outputs = model(data)
            _, predicted = torch.max(outputs.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()

    return correct / total
# __train_def_end__


# __train_func_begin__
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2281')" href="javascript:;">
ray-ray-1.9.2/doc/examples/plot_hyperparameter.py: 117-149
</a>
<div class="mid" id="frag2281" style="display:none"><pre>
def test(model, test_loader, device=torch.device("cpu")):
    """Checks the validation accuracy of the model.

    Cuts off at 512 samples for simplicity.
    """
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for batch_idx, (data, target) in enumerate(test_loader):
            if batch_idx * len(data) &gt; 512:
                break
            data, target = data.to(device), target.to(device)
            outputs = model(data)
            _, predicted = torch.max(outputs.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()

    return correct / total


#######################################################################
# Evaluating the Hyperparameters
# -------------------------------
#
# For a given configuration, the neural network created previously
# will be trained and return the accuracy of the model. These trained
# networks will then be tested for accuracy to find the best set of
# hyperparameters.
#
# The ``@ray.remote`` decorator defines a remote process.


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 69:</b> &nbsp; 2 fragments, nominal size 43 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2256')" href="javascript:;">
ray-ray-1.9.2/doc/source/tune/_tutorials/tune-serve-integration-mnist.py: 289-346
</a>
<div class="mid" id="frag2256" style="display:none"><pre>
def tune_from_scratch(num_samples=10, num_epochs=10, gpus_per_trial=0., day=0):
    data_interface = MNISTDataInterface("~/data", max_days=10)
    num_examples = data_interface._get_day_slice(day)

    config = {
        "batch_size": tune.choice([16, 32, 64]),
        "layer_size": tune.choice([32, 64, 128, 192]),
        "lr": tune.loguniform(1e-4, 1e-1),
        "momentum": tune.uniform(0.1, 0.9),
    }

    scheduler = ASHAScheduler(
        metric="mean_accuracy",
        mode="max",
        max_t=num_epochs,
        grace_period=1,
        reduction_factor=2)

    reporter = CLIReporter(
        parameter_columns=["layer_size", "lr", "momentum", "batch_size"],
        metric_columns=["mean_accuracy", "training_iteration"])

    analysis = tune.run(
        partial(
            train_mnist,
            start_model=None,
            data_fn=data_interface.get_data,
            num_epochs=num_epochs,
            use_gpus=True if gpus_per_trial &gt; 0 else False,
            day=day),
        resources_per_trial={
            "cpu": 1,
            "gpu": gpus_per_trial
        },
        config=config,
        num_samples=num_samples,
        scheduler=scheduler,
        progress_reporter=reporter,
        verbose=0,
        name="tune_serve_mnist_fromscratch")

    best_trial = analysis.get_best_trial("mean_accuracy", "max", "last")
    best_accuracy = best_trial.metric_analysis["mean_accuracy"]["last"]
    best_trial_config = best_trial.config
    best_checkpoint = best_trial.checkpoint.value

    return best_accuracy, best_trial_config, best_checkpoint, num_examples


#######################################################################
# To continue training from an existing model, we can use this function
# instead. It takes a starting model (a checkpoint) as a parameter and
# the old config.
#
# Note that this time the search space does _not_ contain the
# layer size parameter. Since we continue to train an existing model,
# we cannot change the layer size mid training, so we just continue
# to use the existing one.
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2257')" href="javascript:;">
ray-ray-1.9.2/doc/source/tune/_tutorials/tune-serve-integration-mnist.py: 347-412
</a>
<div class="mid" id="frag2257" style="display:none"><pre>
def tune_from_existing(start_model,
                       start_config,
                       num_samples=10,
                       num_epochs=10,
                       gpus_per_trial=0.,
                       day=0):
    data_interface = MNISTDataInterface("/tmp/mnist_data", max_days=10)
    num_examples = data_interface._get_day_slice(day) - \
                   data_interface._get_day_slice(day - 1)

    config = start_config.copy()
    config.update({
        "batch_size": tune.choice([16, 32, 64]),
        "lr": tune.loguniform(1e-4, 1e-1),
        "momentum": tune.uniform(0.1, 0.9),
    })

    scheduler = ASHAScheduler(
        metric="mean_accuracy",
        mode="max",
        max_t=num_epochs,
        grace_period=1,
        reduction_factor=2)

    reporter = CLIReporter(
        parameter_columns=["lr", "momentum", "batch_size"],
        metric_columns=["mean_accuracy", "training_iteration"])

    analysis = tune.run(
        partial(
            train_mnist,
            start_model=start_model,
            data_fn=data_interface.get_incremental_data,
            num_epochs=num_epochs,
            use_gpus=True if gpus_per_trial &gt; 0 else False,
            day=day),
        resources_per_trial={
            "cpu": 1,
            "gpu": gpus_per_trial
        },
        config=config,
        num_samples=num_samples,
        scheduler=scheduler,
        progress_reporter=reporter,
        verbose=0,
        name="tune_serve_mnist_fromsexisting")

    best_trial = analysis.get_best_trial("mean_accuracy", "max", "last")
    best_accuracy = best_trial.metric_analysis["mean_accuracy"]["last"]
    best_trial_config = best_trial.config
    best_checkpoint = best_trial.checkpoint.value

    return best_accuracy, best_trial_config, best_checkpoint, num_examples


#######################################################################
# Serving tuned models with Ray Serve
# -----------------------------------
# Let's now turn to the model serving part with Ray Serve. Serve allows
# you to deploy your models as multiple _deployments_. Broadly speaking,
# a deployment handles incoming requests and replies with a result. For
# instance, our MNIST deployment takes an image as input and outputs the
# digit it recognized from it. This deployment can be exposed over HTTP.
#
# First, we will define our deployment. This loads our PyTorch
# MNIST model from a checkpoint, takes an image as an input and
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 70:</b> &nbsp; 2 fragments, nominal size 20 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2283')" href="javascript:;">
ray-ray-1.9.2/doc/examples/dask_xgboost/dask_xgboost.py: 179-214
</a>
<div class="mid" id="frag2283" style="display:none"><pre>


def train_xgboost(config, train_df, test_df, target_column, ray_params):
    train_set = RayDMatrix(train_df, target_column)
    test_set = RayDMatrix(test_df, target_column)

    evals_result = {}

    train_start_time = time.time()

    # Train the classifier
    bst = train(
        params=config,
        dtrain=train_set,
        evals=[(test_set, "eval")],
        evals_result=evals_result,
        ray_params=ray_params)

    train_end_time = time.time()
    train_duration = train_end_time - train_start_time
    print(f"Total time taken: {train_duration} seconds.")

    model_path = "model.xgb"
    bst.save_model(model_path)
    print("Final validation error: {:.4f}".format(
        evals_result["eval"]["error"][-1]))

    return bst, evals_result


###############################################################################
# We can now pass our Dask dataframes and run the function. We will use
# ``RayParams`` to specify that the number of actors and CPUs to train with.
#
# The dataset has to be downloaded onto the cluster, which may take a few
# minutes.
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2332')" href="javascript:;">
ray-ray-1.9.2/doc/examples/modin_xgboost/modin_xgboost.py: 167-201
</a>
<div class="mid" id="frag2332" style="display:none"><pre>


def train_xgboost(config, train_df, test_df, target_column, ray_params):
    train_set = RayDMatrix(train_df, target_column)
    test_set = RayDMatrix(test_df, target_column)

    evals_result = {}

    train_start_time = time.time()

    # Train the classifier
    bst = train(
        params=config,
        dtrain=train_set,
        evals=[(test_set, "eval")],
        evals_result=evals_result,
        verbose_eval=False,
        num_boost_round=100,
        ray_params=ray_params)

    train_end_time = time.time()
    train_duration = train_end_time - train_start_time
    print(f"Total time taken: {train_duration} seconds.")

    model_path = "model.xgb"
    bst.save_model(model_path)
    print("Final validation error: {:.4f}".format(
        evals_result["eval"]["error"][-1]))

    return bst, evals_result


###############################################################################
# We can now pass our Modin dataframes and run the function. We will use
# ``RayParams`` to specify that the number of actors and CPUs to train with.
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 71:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 77%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2396')" href="javascript:;">
ray-ray-1.9.2/streaming/python/function.py: 304-339
</a>
<div class="mid" id="frag2396" style="display:none"><pre>

def load_function(descriptor_func_bytes: bytes):
    """
    Deserialize `descriptor_func_bytes` to get function info, then
    get or load streaming function.
    Note that this function must be kept in sync with
     `io.ray.streaming.runtime.python.GraphPbBuilder.serializeFunction`

    Args:
        descriptor_func_bytes: serialized function info

    Returns:
        a streaming function
    """
    assert len(descriptor_func_bytes) &gt; 0
    function_bytes, module_name, function_name, function_interface \
        = gateway_client.deserialize(descriptor_func_bytes)
    if function_bytes:
        return deserialize(function_bytes)
    else:
        assert module_name
        assert function_interface
        function_interface = getattr(sys.modules[__name__], function_interface)
        mod = importlib.import_module(module_name)
        assert function_name
        func = getattr(mod, function_name)
        # If func is a python function, user function is a simple python
        # function, which will be wrapped as a SimpleXXXFunction.
        # If func is a python class, user function is a sub class
        # of XXXFunction.
        if inspect.isfunction(func):
            simple_func_class = _get_simple_function_class(function_interface)
            return simple_func_class(func)
        else:
            assert issubclass(func, function_interface)
            return func()
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2697')" href="javascript:;">
ray-ray-1.9.2/streaming/python/partition.py: 98-128
</a>
<div class="mid" id="frag2697" style="display:none"><pre>
def load_partition(descriptor_partition_bytes: bytes):
    """
    Deserialize `descriptor_partition_bytes` to get partition info, then
    get or load partition function.
    Note that this function must be kept in sync with
     `io.ray.streaming.runtime.python.GraphPbBuilder.serializePartition`

    Args:
        descriptor_partition_bytes: serialized partition info

    Returns:
        partition function
    """
    assert len(descriptor_partition_bytes) &gt; 0
    partition_bytes, module_name, function_name =\
        gateway_client.deserialize(descriptor_partition_bytes)
    if partition_bytes:
        return deserialize(partition_bytes)
    else:
        assert module_name
        mod = importlib.import_module(module_name)
        assert function_name
        func = getattr(mod, function_name)
        # If func is a python function, user partition is a simple python
        # function, which will be wrapped as a SimplePartition.
        # If func is a python class, user partition is a sub class
        # of Partition.
        if inspect.isfunction(func):
            return SimplePartition(func)
        else:
            assert issubclass(func, Partition)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 72:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 84%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2478')" href="javascript:;">
ray-ray-1.9.2/streaming/python/tests/test_stream.py: 7-20
</a>
<div class="mid" id="frag2478" style="display:none"><pre>
def test_data_stream():
    ray.init(job_config=ray.job_config.JobConfig(code_search_path=sys.path))
    ctx = StreamingContext.Builder().build()
    stream = ctx.from_values(1, 2, 3)
    java_stream = stream.as_java_stream()
    python_stream = java_stream.as_python_stream()
    assert stream.get_id() == java_stream.get_id()
    assert stream.get_id() == python_stream.get_id()
    python_stream.set_parallelism(10)
    assert stream.get_parallelism() == java_stream.get_parallelism()
    assert stream.get_parallelism() == python_stream.get_parallelism()
    ray.shutdown()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2479')" href="javascript:;">
ray-ray-1.9.2/streaming/python/tests/test_stream.py: 21-35
</a>
<div class="mid" id="frag2479" style="display:none"><pre>
def test_key_data_stream():
    ray.init(job_config=ray.job_config.JobConfig(code_search_path=sys.path))
    ctx = StreamingContext.Builder().build()
    key_stream = ctx.from_values(
        "a", "b", "c").map(lambda x: (x, 1)).key_by(lambda x: x[0])
    java_stream = key_stream.as_java_stream()
    python_stream = java_stream.as_python_stream()
    assert key_stream.get_id() == java_stream.get_id()
    assert key_stream.get_id() == python_stream.get_id()
    python_stream.set_parallelism(10)
    assert key_stream.get_parallelism() == java_stream.get_parallelism()
    assert key_stream.get_parallelism() == python_stream.get_parallelism()
    ray.shutdown()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 73:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 91%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2488')" href="javascript:;">
ray-ray-1.9.2/streaming/python/tests/test_direct_transfer.py: 21-33
</a>
<div class="mid" id="frag2488" style="display:none"><pre>
    def init_writer(self, output_channel, reader_actor):
        conf = {Config.CHANNEL_TYPE: Config.NATIVE_CHANNEL}
        reader_async_func = PythonFunctionDescriptor(
            __name__, self.on_reader_message.__name__, self.__class__.__name__)
        reader_sync_func = PythonFunctionDescriptor(
            __name__, self.on_reader_message_sync.__name__,
            self.__class__.__name__)
        transfer.ChannelCreationParametersBuilder.\
            set_python_reader_function_descriptor(
                reader_async_func, reader_sync_func)
        self.writer = transfer.DataWriter([output_channel],
                                          [pickle.loads(reader_actor)], conf)
        self.output_channel_id = transfer.ChannelID(output_channel)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2489')" href="javascript:;">
ray-ray-1.9.2/streaming/python/tests/test_direct_transfer.py: 34-45
</a>
<div class="mid" id="frag2489" style="display:none"><pre>

    def init_reader(self, input_channel, writer_actor):
        conf = {Config.CHANNEL_TYPE: Config.NATIVE_CHANNEL}
        writer_async_func = PythonFunctionDescriptor(
            __name__, self.on_writer_message.__name__, self.__class__.__name__)
        writer_sync_func = PythonFunctionDescriptor(
            __name__, self.on_writer_message_sync.__name__,
            self.__class__.__name__)
        transfer.ChannelCreationParametersBuilder.\
            set_python_writer_function_descriptor(
                writer_async_func, writer_sync_func)
        self.reader = transfer.DataReader([input_channel],
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 74:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2588')" href="javascript:;">
ray-ray-1.9.2/streaming/python/runtime/transfer.py: 135-146
</a>
<div class="mid" id="frag2588" style="display:none"><pre>
    def __init__(self,
                 body,
                 timestamp,
                 message_id,
                 channel_id,
                 is_empty_message=False):
        self.__body = body
        self.__timestamp = timestamp
        self.__channel_id = channel_id
        self.__message_id = message_id
        self.__is_empty_message = is_empty_message

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4254')" href="javascript:;">
ray-ray-1.9.2/python/ray/actor.py: 91-113
</a>
<div class="mid" id="frag4254" style="display:none"><pre>
    def __init__(self,
                 actor,
                 method_name,
                 num_returns,
                 decorator=None,
                 hardref=False):
        self._actor_ref = weakref.ref(actor)
        self._method_name = method_name
        self._num_returns = num_returns
        # This is a decorator that is used to wrap the function invocation (as
        # opposed to the function execution). The decorator must return a
        # function that takes in two arguments ("args" and "kwargs"). In most
        # cases, it should call the function that was passed into the decorator
        # and return the resulting ObjectRefs.
        self._decorator = decorator

        # Acquire a hard ref to the actor, this is useful mainly when passing
        # actor method handles to remote functions.
        if hardref:
            self._actor_hard_ref = actor
        else:
            self._actor_hard_ref = None

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 75:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2682')" href="javascript:;">
ray-ray-1.9.2/streaming/python/runtime/remote_call.py: 60-76
</a>
<div class="mid" id="frag2682" style="display:none"><pre>
    @staticmethod
    def request_job_worker_rollback(master: ActorHandle,
                                    request: WorkerRollbackRequest):
        logger.info("Remote call mst: request job worker rollback start.")
        request_pb = remote_call_pb2.BaseWorkerCmd()
        request_pb.actor_id = request.from_actor_id
        request_pb.timestamp = int(time.time() * 1000.0)
        rollback_request_pb = remote_call_pb2.WorkerRollbackRequest()
        rollback_request_pb.exception_msg = request.exception_msg()
        rollback_request_pb.worker_hostname = os.uname()[1]
        rollback_request_pb.worker_pid = str(os.getpid())
        request_pb.detail.Pack(rollback_request_pb)
        return_ids = master.requestJobWorkerRollback\
            .remote(request_pb.SerializeToString())
        result = remote_call_pb2.BoolResult()
        result.ParseFromString(ray.get(return_ids))
        logger.info("Remote call mst: request job worker rollback finish.")
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2683')" href="javascript:;">
ray-ray-1.9.2/streaming/python/runtime/remote_call.py: 78-92
</a>
<div class="mid" id="frag2683" style="display:none"><pre>

    @staticmethod
    def report_job_worker_commit(master: ActorHandle,
                                 report: WorkerCommitReport):
        logger.info("Remote call mst: report job worker commit start.")
        report_pb = remote_call_pb2.BaseWorkerCmd()

        report_pb.actor_id = report.from_actor_id
        report_pb.timestamp = int(time.time() * 1000.0)
        wk_commit = remote_call_pb2.WorkerCommitReport()
        wk_commit.commit_checkpoint_id = report.commit_checkpoint_id
        report_pb.detail.Pack(wk_commit)
        return_id = master.reportJobWorkerCommit\
            .remote(report_pb.SerializeToString())
        result = remote_call_pb2.BoolResult()
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 76:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2765')" href="javascript:;">
ray-ray-1.9.2/dashboard/modules/job/tests/test_cli.py: 51-65
</a>
<div class="mid" id="frag2765" style="display:none"><pre>
def set_env_var(key: str, val: Optional[str] = None):
    old_val = os.environ.get(key, None)
    if val is not None:
        os.environ[key] = val
    elif key in os.environ:
        del os.environ[key]

    yield

    if key in os.environ:
        del os.environ[key]
    if old_val is not None:
        os.environ[key] = old_val


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2788')" href="javascript:;">
ray-ray-1.9.2/dashboard/modules/job/tests/test_cli_integration.py: 14-28
</a>
<div class="mid" id="frag2788" style="display:none"><pre>
def set_env_var(key: str, val: Optional[str] = None):
    old_val = os.environ.get(key, None)
    if val is not None:
        os.environ[key] = val
    elif key in os.environ:
        del os.environ[key]

    yield

    if key in os.environ:
        del os.environ[key]
    if old_val is not None:
        os.environ[key] = old_val


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 77:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2779')" href="javascript:;">
ray-ray-1.9.2/dashboard/modules/job/tests/test_http_job_server.py: 148-173
</a>
<div class="mid" id="frag2779" style="display:none"><pre>
def test_submit_job_with_exception_in_driver(job_sdk_client):
    """
    Submit a job that's expected to throw exception while executing.
    """
    client = job_sdk_client

    with tempfile.TemporaryDirectory() as tmp_dir:
        path = Path(tmp_dir)
        driver_script = """
print('Hello !')
raise RuntimeError('Intentionally failed.')
        """
        test_script_file = path / "test_script.py"
        with open(test_script_file, "w+") as file:
            file.write(driver_script)

        job_id = client.submit_job(
            entrypoint="python test_script.py",
            runtime_env={"working_dir": tmp_dir})

        wait_for_condition(_check_job_failed, client=client, job_id=job_id)
        logs = client.get_job_logs(job_id)
        assert "Hello !" in logs
        assert "RuntimeError: Intentionally failed." in logs


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2780')" href="javascript:;">
ray-ray-1.9.2/dashboard/modules/job/tests/test_http_job_server.py: 174-198
</a>
<div class="mid" id="frag2780" style="display:none"><pre>
def test_stop_long_running_job(job_sdk_client):
    """
    Submit a job that runs for a while and stop it in the middle.
    """
    client = job_sdk_client

    with tempfile.TemporaryDirectory() as tmp_dir:
        path = Path(tmp_dir)
        driver_script = """
print('Hello !')
import time
time.sleep(300) # This should never finish
raise RuntimeError('Intentionally failed.')
        """
        test_script_file = path / "test_script.py"
        with open(test_script_file, "w+") as file:
            file.write(driver_script)

        job_id = client.submit_job(
            entrypoint="python test_script.py",
            runtime_env={"working_dir": tmp_dir})
        assert client.stop_job(job_id) is True
        wait_for_condition(_check_job_stopped, client=client, job_id=job_id)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 78:</b> &nbsp; 2 fragments, nominal size 22 lines, similarity 79%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2805')" href="javascript:;">
ray-ray-1.9.2/dashboard/modules/job/tests/test_common.py: 40-67
</a>
<div class="mid" id="frag2805" style="display:none"><pre>
    def test_validate_runtime_env(self):
        r = validate_request_type({"entrypoint": "abc"}, JobSubmitRequest)
        assert r.entrypoint == "abc"
        assert r.runtime_env is None

        r = validate_request_type({
            "entrypoint": "abc",
            "runtime_env": {
                "hi": "hi2"
            }
        }, JobSubmitRequest)
        assert r.entrypoint == "abc"
        assert r.runtime_env == {"hi": "hi2"}

        with pytest.raises(TypeError, match="must be a dict"):
            validate_request_type({
                "entrypoint": "abc",
                "runtime_env": 123
            }, JobSubmitRequest)

        with pytest.raises(TypeError, match="keys must be strings"):
            validate_request_type({
                "entrypoint": "abc",
                "runtime_env": {
                    1: "hi"
                }
            }, JobSubmitRequest)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2806')" href="javascript:;">
ray-ray-1.9.2/dashboard/modules/job/tests/test_common.py: 68-104
</a>
<div class="mid" id="frag2806" style="display:none"><pre>
    def test_validate_metadata(self):
        r = validate_request_type({"entrypoint": "abc"}, JobSubmitRequest)
        assert r.entrypoint == "abc"
        assert r.metadata is None

        r = validate_request_type({
            "entrypoint": "abc",
            "metadata": {
                "hi": "hi2"
            }
        }, JobSubmitRequest)
        assert r.entrypoint == "abc"
        assert r.metadata == {"hi": "hi2"}

        with pytest.raises(TypeError, match="must be a dict"):
            validate_request_type({
                "entrypoint": "abc",
                "metadata": 123
            }, JobSubmitRequest)

        with pytest.raises(TypeError, match="keys must be strings"):
            validate_request_type({
                "entrypoint": "abc",
                "metadata": {
                    1: "hi"
                }
            }, JobSubmitRequest)

        with pytest.raises(TypeError, match="values must be strings"):
            validate_request_type({
                "entrypoint": "abc",
                "metadata": {
                    "hi": 1
                }
            }, JobSubmitRequest)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 79:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2826')" href="javascript:;">
ray-ray-1.9.2/ci/travis/py_dep_analysis_test.py: 40-59
</a>
<div class="mid" id="frag2826" style="display:none"><pre>
    def test_import_line_continuation(self):
        graph = pda.DepGraph()
        graph.ids["ray"] = 0

        with tempfile.TemporaryDirectory() as tmpdir:
            src_path = os.path.join(tmpdir, "continuation1.py")
            self.create_tmp_file(
                src_path, """
import ray.rllib.env.\\
    mock_env
b = 2
""")
            pda._process_file(graph, src_path, "ray")

        self.assertEqual(len(graph.ids), 2)
        print(graph.ids)
        # Shoud pick up the full module name.
        self.assertEqual(graph.ids["ray.rllib.env.mock_env"], 1)
        self.assertEqual(graph.edges[0], {1: True})

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2827')" href="javascript:;">
ray-ray-1.9.2/ci/travis/py_dep_analysis_test.py: 60-79
</a>
<div class="mid" id="frag2827" style="display:none"><pre>
    def test_import_line_continuation_parenthesis(self):
        graph = pda.DepGraph()
        graph.ids["ray"] = 0

        with tempfile.TemporaryDirectory() as tmpdir:
            src_path = os.path.join(tmpdir, "continuation1.py")
            self.create_tmp_file(
                src_path, """
from ray.rllib.env import (ClassName,
    module1, module2)
b = 2
""")
            pda._process_file(graph, src_path, "ray")

        self.assertEqual(len(graph.ids), 2)
        print(graph.ids)
        # Shoud pick up the full module name without trailing (.
        self.assertEqual(graph.ids["ray.rllib.env"], 1)
        self.assertEqual(graph.edges[0], {1: True})

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 80:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 93%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2828')" href="javascript:;">
ray-ray-1.9.2/ci/travis/py_dep_analysis_test.py: 80-107
</a>
<div class="mid" id="frag2828" style="display:none"><pre>
    def test_from_import_file_module(self):
        graph = pda.DepGraph()
        graph.ids["ray"] = 0

        with tempfile.TemporaryDirectory() as tmpdir:
            src_path = "multi_line_comment_3.py"
            self.create_tmp_file(
                os.path.join(tmpdir, src_path), """
from ray.rllib.env import mock_env
a = 1
b = 2
""")
            # Touch ray/rllib/env/mock_env.py in tmpdir,
            # so that it looks like a module.
            module_dir = os.path.join(tmpdir, "python", "ray", "rllib", "env")
            os.makedirs(module_dir, exist_ok=True)
            f = open(os.path.join(module_dir, "mock_env.py"), "w")
            f.write("print('hello world!')")
            f.close

            pda._process_file(graph, src_path, "ray", _base_dir=tmpdir)

        self.assertEqual(len(graph.ids), 2)
        self.assertEqual(graph.ids["ray.rllib.env.mock_env"], 1)
        # Only 1 edge from ray to ray.rllib.env.mock_env
        # ray.tune.tune is ignored.
        self.assertEqual(graph.edges[0], {1: True})

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2829')" href="javascript:;">
ray-ray-1.9.2/ci/travis/py_dep_analysis_test.py: 108-137
</a>
<div class="mid" id="frag2829" style="display:none"><pre>
    def test_from_import_class_object(self):
        graph = pda.DepGraph()
        graph.ids["ray"] = 0

        with tempfile.TemporaryDirectory() as tmpdir:
            src_path = "multi_line_comment_3.py"
            self.create_tmp_file(
                os.path.join(tmpdir, src_path), """
from ray.rllib.env import MockEnv
a = 1
b = 2
""")
            # Touch ray/rllib/env.py in tmpdir,
            # MockEnv is a class on env module.
            module_dir = os.path.join(tmpdir, "python", "ray", "rllib")
            os.makedirs(module_dir, exist_ok=True)
            f = open(os.path.join(module_dir, "env.py"), "w")
            f.write("print('hello world!')")
            f.close

            pda._process_file(graph, src_path, "ray", _base_dir=tmpdir)

        self.assertEqual(len(graph.ids), 2)
        # Should depend on env.py instead.
        self.assertEqual(graph.ids["ray.rllib.env"], 1)
        # Only 1 edge from ray to ray.rllib.env.mock_env
        # ray.tune.tune is ignored.
        self.assertEqual(graph.edges[0], {1: True})


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 81:</b> &nbsp; 2 fragments, nominal size 29 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2945')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/automlboard/frontend/view.py: 83-116
</a>
<div class="mid" id="frag2945" style="display:none"><pre>
        .filter(trial_id=trial_id) \
        .order_by("-start_time")[0]
    context = {
        "job_id": job_id,
        "trial_id": trial_id,
        "current_trial": current_trial,
        "recent_results": recent_results,
        "recent_trials": recent_trials
    }
    return render(request, "trial.html", context)


def get_job_info(current_job):
    """Get job information for current job."""
    trials = TrialRecord.objects.filter(job_id=current_job.job_id)
    total_num = len(trials)
    running_num = sum(t.trial_status == Trial.RUNNING for t in trials)
    success_num = sum(t.trial_status == Trial.TERMINATED for t in trials)
    failed_num = sum(t.trial_status == Trial.ERROR for t in trials)

    if total_num == 0:
        progress = 0
    else:
        progress = int(float(success_num) / total_num * 100)

    winner = get_winner(trials)

    job_info = {
        "job_id": current_job.job_id,
        "job_name": current_job.name,
        "user": current_job.user,
        "type": current_job.type,
        "start_time": current_job.start_time,
        "end_time": current_job.end_time,
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2948')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/automlboard/frontend/query.py: 9-68
</a>
<div class="mid" id="frag2948" style="display:none"><pre>
def query_job(request):
    """Rest API to query the job info, with the given job_id.

    The url pattern should be like this:

    curl http://&lt;server&gt;:&lt;port&gt;/query_job?job_id=&lt;job_id&gt;

    The response may be:

    {
        "running_trials": 0,
        "start_time": "2018-07-19 20:49:40",
        "current_round": 1,
        "failed_trials": 0,
        "best_trial_id": "2067R2ZD",
        "name": "asynchyperband_test",
        "job_id": "asynchyperband_test",
        "user": "Grady",
        "type": "RAY TUNE",
        "total_trials": 4,
        "end_time": "2018-07-19 20:50:10",
        "progress": 100,
        "success_trials": 4
    }
    """
    job_id = request.GET.get("job_id")
    jobs = JobRecord.objects.filter(job_id=job_id)
    trials = TrialRecord.objects.filter(job_id=job_id)

    total_num = len(trials)
    running_num = sum(t.trial_status == Trial.RUNNING for t in trials)
    success_num = sum(t.trial_status == Trial.TERMINATED for t in trials)
    failed_num = sum(t.trial_status == Trial.ERROR for t in trials)
    if total_num == 0:
        progress = 0
    else:
        progress = int(float(success_num) / total_num * 100)

    if len(jobs) == 0:
        resp = "Unkonwn job id %s.\n" % job_id
    else:
        job = jobs[0]
        result = {
            "job_id": job.job_id,
            "name": job.name,
            "user": job.user,
            "type": job.type,
            "start_time": job.start_time,
            "end_time": job.end_time,
            "success_trials": success_num,
            "failed_trials": failed_num,
            "running_trials": running_num,
            "total_trials": total_num,
            "best_trial_id": job.best_trial_id,
            "progress": progress
        }
        resp = json.dumps(result)
    return HttpResponse(resp, content_type="application/json;charset=utf-8")


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 82:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3059')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/integration/kubernetes.py: 144-158
</a>
<div class="mid" id="frag3059" style="display:none"><pre>

    def sync_up(self,
                source: str,
                target: Tuple[str, str],
                exclude: Optional[List] = None) -&gt; bool:
        """Here target is a tuple (target_node, target_dir)"""
        target_node, target_dir = target

        # Add trailing slashes for rsync
        source = os.path.join(source, "")
        target_dir = os.path.join(target_dir, "")

        command_runner = self._get_command_runner(target_node)
        command_runner.run_rsync_up(source, target_dir)
        return True
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3060')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/integration/kubernetes.py: 159-173
</a>
<div class="mid" id="frag3060" style="display:none"><pre>

    def sync_down(self,
                  source: Tuple[str, str],
                  target: str,
                  exclude: Optional[List] = None) -&gt; bool:
        """Here source is a tuple (source_node, source_dir)"""
        source_node, source_dir = source

        # Add trailing slashes for rsync
        source_dir = os.path.join(source_dir, "")
        target = os.path.join(target, "")

        command_runner = self._get_command_runner(source_node)
        command_runner.run_rsync_down(source_dir, target)
        return True
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 83:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3085')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/integration/keras.py: 153-166
</a>
<div class="mid" id="frag3085" style="display:none"><pre>
    def _handle(self, logs: Dict, when: str = None):
        if not self._metrics:
            report_dict = logs
        else:
            report_dict = {}
            for key in self._metrics:
                if isinstance(self._metrics, dict):
                    metric = self._metrics[key]
                else:
                    metric = key
                report_dict[key] = logs[metric]
        tune.report(**report_dict)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3127')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/integration/lightgbm.py: 57-70
</a>
<div class="mid" id="frag3127" style="display:none"><pre>
    def _get_report_dict(self, evals_log: Dict[str, Dict[str, list]]) -&gt; dict:
        result_dict = flatten_dict(evals_log, delimiter="-")
        if not self._metrics:
            report_dict = result_dict
        else:
            report_dict = {}
            for key in self._metrics:
                if isinstance(self._metrics, dict):
                    metric = self._metrics[key]
                else:
                    metric = key
                report_dict[key] = result_dict[metric]
        return report_dict

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 84:</b> &nbsp; 2 fragments, nominal size 23 lines, similarity 95%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3123')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/integration/docker.py: 92-120
</a>
<div class="mid" id="frag3123" style="display:none"><pre>
    def sync_up(self,
                source: str,
                target: Tuple[str, str],
                exclude: Optional[List] = None) -&gt; bool:
        """Here target is a tuple (target_node, target_dir)"""
        target_node, target_dir = target

        # Add trailing slashes for rsync
        source = os.path.join(source, "")
        target_dir = os.path.join(target_dir, "")
        import click
        try:
            rsync(
                cluster_config=self._cluster_config_file,
                source=source,
                target=target_dir,
                down=False,
                ip_address=target_node,
                should_bootstrap=self._should_bootstrap,
                use_internal_ip=True)
        except click.ClickException:
            if log_once("docker_rsync_up_fail"):
                logger.warning(
                    "Rsync-up failed. Consider using a durable trainable "
                    "or setting the `TUNE_SYNC_DISABLE_BOOTSTRAP=1` env var.")
            raise

        return True

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3124')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/integration/docker.py: 121-149
</a>
<div class="mid" id="frag3124" style="display:none"><pre>
    def sync_down(self,
                  source: Tuple[str, str],
                  target: str,
                  exclude: Optional[List] = None) -&gt; bool:
        """Here source is a tuple (source_node, source_dir)"""
        source_node, source_dir = source

        # Add trailing slashes for rsync
        source_dir = os.path.join(source_dir, "")
        target = os.path.join(target, "")
        import click
        try:
            rsync(
                cluster_config=self._cluster_config_file,
                source=source_dir,
                target=target,
                down=True,
                ip_address=source_node,
                should_bootstrap=self._should_bootstrap,
                use_internal_ip=True)
        except click.ClickException:
            if log_once("docker_rsync_down_fail"):
                logger.warning(
                    "Rsync-down failed. Consider using a durable trainable "
                    "or setting the `TUNE_SYNC_DISABLE_BOOTSTRAP=1` env var.")
            raise

        return True

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 85:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3152')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_var.py: 190-202
</a>
<div class="mid" id="frag3152" style="display:none"><pre>
    def testDependentLambda(self):
        trials = self.generate_trials({
            "run": "PPO",
            "config": {
                "x": grid_search([1, 2]),
                "y": tune.sample_from(lambda spec: spec.config.x * 100),
            },
        }, "dependent_lambda")
        trials = list(trials)
        self.assertEqual(len(trials), 2)
        self.assertEqual(trials[0].config, {"x": 1, "y": 100})
        self.assertEqual(trials[1].config, {"x": 2, "y": 200})

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3153')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_var.py: 203-218
</a>
<div class="mid" id="frag3153" style="display:none"><pre>
    def testDependentGridSearch(self):
        trials = self.generate_trials({
            "run": "PPO",
            "config": {
                "x": grid_search([
                    tune.sample_from(lambda spec: spec.config.y * 100),
                    tune.sample_from(lambda spec: spec.config.y * 200)
                ]),
                "y": tune.sample_from(lambda spec: 1),
            },
        }, "dependent_grid_search")
        trials = list(trials)
        self.assertEqual(len(trials), 2)
        self.assertEqual(trials[0].config, {"x": 100, "y": 1})
        self.assertEqual(trials[1].config, {"x": 200, "y": 1})

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 86:</b> &nbsp; 3 fragments, nominal size 15 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3165')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_experiment_analysis.py: 35-49
</a>
<div class="mid" id="frag3165" style="display:none"><pre>
    def run_test_exp(self):
        self.ea = tune.run(
            MyTrainableClass,
            name=self.test_name,
            local_dir=self.test_dir,
            stop={"training_iteration": 1},
            checkpoint_freq=1,
            num_samples=self.num_samples,
            config={
                "width": tune.sample_from(
                    lambda spec: 10 + int(90 * random.random())),
                "height": tune.sample_from(
                    lambda spec: int(100 * random.random())),
            })

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3182')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_experiment_analysis.py: 205-220
</a>
<div class="mid" id="frag3182" style="display:none"><pre>
    def testIgnoreOtherExperiment(self):
        analysis = tune.run(
            MyTrainableClass,
            name="test_example",
            local_dir=self.test_dir,
            stop={"training_iteration": 1},
            num_samples=1,
            config={
                "width": tune.sample_from(
                    lambda spec: 10 + int(90 * random.random())),
                "height": tune.sample_from(
                    lambda spec: int(100 * random.random())),
            })
        df = analysis.dataframe(self.metric, mode="max")
        self.assertEquals(df.shape[0], 1)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3166')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_experiment_analysis.py: 50-65
</a>
<div class="mid" id="frag3166" style="display:none"><pre>
    def nan_test_exp(self):
        nan_ea = tune.run(
            lambda x: nan,
            name="testing_nan",
            local_dir=self.test_dir,
            stop={"training_iteration": 1},
            checkpoint_freq=1,
            num_samples=self.num_samples,
            config={
                "width": tune.sample_from(
                    lambda spec: 10 + int(90 * random.random())),
                "height": tune.sample_from(
                    lambda spec: int(100 * random.random())),
            })
        return nan_ea

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 87:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3199')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_torch_trainable.py: 111-126
</a>
<div class="mid" id="frag3199" style="display:none"><pre>
def ray_4_node():
    cluster = Cluster()
    for _ in range(4):
        cluster.add_node(num_cpus=1)

    ray.init(address=cluster.address)

    yield

    ray.shutdown()
    cluster.shutdown()
    # Ensure that tests don't ALL fail
    if dist.is_initialized():
        dist.destroy_process_group()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3200')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_torch_trainable.py: 128-143
</a>
<div class="mid" id="frag3200" style="display:none"><pre>
def ray_4_node_gpu():
    cluster = Cluster()
    for _ in range(4):
        cluster.add_node(num_cpus=2, num_gpus=2)

    ray.init(address=cluster.address)

    yield

    ray.shutdown()
    cluster.shutdown()
    # Ensure that tests don't ALL fail
    if dist.is_initialized():
        dist.destroy_process_group()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 88:</b> &nbsp; 4 fragments, nominal size 12 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3202')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_torch_trainable.py: 154-166
</a>
<div class="mid" id="frag3202" style="display:none"><pre>
def test_colocated_gpu(ray_4_node_gpu):  # noqa: F811
    assert ray.available_resources()["GPU"] == 8
    trainable_cls = DistributedTrainableCreator(
        _train_check_global,
        num_workers=4,
        num_gpus_per_worker=2,
        num_workers_per_host=1)
    trainable = trainable_cls()
    assert ray.available_resources().get("GPU", 0) == 0
    trainable.train()
    trainable.stop()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3203')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_torch_trainable.py: 167-182
</a>
<div class="mid" id="frag3203" style="display:none"><pre>
def test_colocated_gpu_double(ray_4_node_gpu):  # noqa: F811
    assert ray.available_resources()["GPU"] == 8
    trainable_cls = DistributedTrainableCreator(
        _train_check_global,
        num_workers=8,
        num_gpus_per_worker=1,
        num_workers_per_host=2,
        timeout_s=30)
    trainable = trainable_cls()
    print("?????")
    print(ray.available_resources().get("GPU"))
    assert ray.available_resources().get("GPU", 0) == 0
    trainable.train()
    trainable.stop()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3367')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_tensorflow_trainable.py: 128-141
</a>
<div class="mid" id="frag3367" style="display:none"><pre>
def test_colocated_gpu_double(ray_4_node_gpu):  # noqa: F811
    assert ray.available_resources()["GPU"] == 8
    trainable_cls = DistributedTrainableCreator(
        _train_check_global,
        num_workers=8,
        num_gpus_per_worker=1,
        num_cpus_per_worker=1,
        num_workers_per_host=2)
    trainable = trainable_cls()
    assert ray.available_resources().get("GPU", 0) == 0
    trainable.train()
    trainable.stop()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3366')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_tensorflow_trainable.py: 115-127
</a>
<div class="mid" id="frag3366" style="display:none"><pre>
def test_colocated_gpu(ray_4_node_gpu):  # noqa: F811
    assert ray.available_resources()["GPU"] == 8
    trainable_cls = DistributedTrainableCreator(
        _train_check_global,
        num_workers=4,
        num_gpus_per_worker=2,
        num_workers_per_host=1)
    trainable = trainable_cls()
    assert ray.available_resources().get("GPU", 0) == 0
    trainable.train()
    trainable.stop()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 89:</b> &nbsp; 2 fragments, nominal size 39 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3277')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_integration_wandb.py: 318-378
</a>
<div class="mid" id="frag3277" style="display:none"><pre>

        del logger

    def testWandbMixinConfig(self):
        config = {"par1": 4, "par2": 9.12345678}
        trial = Trial(config, 0, "trial_0", "trainable",
                      PlacementGroupFactory([{
                          "CPU": 1
                      }]))
        trial_info = TrialInfo(trial)

        config[TRIAL_INFO] = trial_info

        if WANDB_ENV_VAR in os.environ:
            del os.environ[WANDB_ENV_VAR]

        # Needs at least a project
        with self.assertRaises(ValueError):
            trainable = WandbTestTrainable(config)

        # No API key
        config["wandb"] = {"project": "test_project"}
        with self.assertRaises(ValueError):
            trainable = WandbTestTrainable(config)

        # API Key in config
        config["wandb"] = {"project": "test_project", "api_key": "1234"}
        trainable = WandbTestTrainable(config)
        self.assertEqual(os.environ[WANDB_ENV_VAR], "1234")

        del os.environ[WANDB_ENV_VAR]

        # API Key file
        with tempfile.NamedTemporaryFile("wt") as fp:
            fp.write("5678")
            fp.flush()

            config["wandb"] = {
                "project": "test_project",
                "api_key_file": fp.name
            }

            trainable = WandbTestTrainable(config)
            self.assertEqual(os.environ[WANDB_ENV_VAR], "5678")

        del os.environ[WANDB_ENV_VAR]

        # API Key in env
        os.environ[WANDB_ENV_VAR] = "9012"
        config["wandb"] = {"project": "test_project"}
        trainable = WandbTestTrainable(config)

        # From now on, the API key is in the env variable.

        # Default configuration
        config["wandb"] = {"project": "test_project"}
        config[TRIAL_INFO] = trial_info

        trainable = WandbTestTrainable(config)
        self.assertEqual(trainable.wandb.kwargs["project"], "test_project")
        self.assertEqual(trainable.wandb.kwargs["id"], trial.trial_id)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3278')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_integration_wandb.py: 379-445
</a>
<div class="mid" id="frag3278" style="display:none"><pre>
        self.assertEqual(trainable.wandb.kwargs["name"], trial.trial_name)
        self.assertEqual(trainable.wandb.kwargs["group"], "WandbTestTrainable")

    def testWandbDecoratorConfig(self):
        config = {"par1": 4, "par2": 9.12345678}
        trial = Trial(config, 0, "trial_0", "trainable",
                      PlacementGroupFactory([{
                          "CPU": 1
                      }]))
        trial_info = TrialInfo(trial)

        @wandb_mixin
        def train_fn(config):
            return 1

        train_fn.__mixins__ = (_MockWandbTrainableMixin, )

        config[TRIAL_INFO] = trial_info

        if WANDB_ENV_VAR in os.environ:
            del os.environ[WANDB_ENV_VAR]

        # Needs at least a project
        with self.assertRaises(ValueError):
            wrapped = wrap_function(train_fn)(config)

        # No API key
        config["wandb"] = {"project": "test_project"}
        with self.assertRaises(ValueError):
            wrapped = wrap_function(train_fn)(config)

        # API Key in config
        config["wandb"] = {"project": "test_project", "api_key": "1234"}
        wrapped = wrap_function(train_fn)(config)
        self.assertEqual(os.environ[WANDB_ENV_VAR], "1234")

        del os.environ[WANDB_ENV_VAR]

        # API Key file
        with tempfile.NamedTemporaryFile("wt") as fp:
            fp.write("5678")
            fp.flush()

            config["wandb"] = {
                "project": "test_project",
                "api_key_file": fp.name
            }

            wrapped = wrap_function(train_fn)(config)
            self.assertEqual(os.environ[WANDB_ENV_VAR], "5678")

        del os.environ[WANDB_ENV_VAR]

        # API Key in env
        os.environ[WANDB_ENV_VAR] = "9012"
        config["wandb"] = {"project": "test_project"}
        wrapped = wrap_function(train_fn)(config)

        # From now on, the API key is in the env variable.

        # Default configuration
        config["wandb"] = {"project": "test_project"}
        config[TRIAL_INFO] = trial_info

        wrapped = wrap_function(train_fn)(config)
        self.assertEqual(wrapped.wandb.kwargs["project"], "test_project")
        self.assertEqual(wrapped.wandb.kwargs["id"], trial.trial_id)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 90:</b> &nbsp; 2 fragments, nominal size 67 lines, similarity 89%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3309')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/ext_pytorch.py: 208-295
</a>
<div class="mid" id="frag3309" style="display:none"><pre>
def train_cifar(config, checkpoint_dir=None, data_dir=None):
    net = Net(config["l1"], config["l2"])

    device = "cpu"
    if torch.cuda.is_available():
        device = "cuda:0"
        if torch.cuda.device_count() &gt; 1:
            net = nn.DataParallel(net)
    net.to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(net.parameters(), lr=config["lr"], momentum=0.9)

    if checkpoint_dir:
        model_state, optimizer_state = torch.load(
            os.path.join(checkpoint_dir, "checkpoint"))
        net.load_state_dict(model_state)
        optimizer.load_state_dict(optimizer_state)

    trainset, testset = load_data(data_dir)

    test_abs = int(len(trainset) * 0.8)
    train_subset, val_subset = random_split(
        trainset, [test_abs, len(trainset) - test_abs])

    trainloader = torch.utils.data.DataLoader(
        train_subset,
        batch_size=int(config["batch_size"]),
        shuffle=True,
        num_workers=8)
    valloader = torch.utils.data.DataLoader(
        val_subset,
        batch_size=int(config["batch_size"]),
        shuffle=True,
        num_workers=8)

    for epoch in range(10):  # loop over the dataset multiple times
        running_loss = 0.0
        epoch_steps = 0
        for i, data in enumerate(trainloader, 0):
            # get the inputs; data is a list of [inputs, labels]
            inputs, labels = data
            inputs, labels = inputs.to(device), labels.to(device)

            # zero the parameter gradients
            optimizer.zero_grad()

            # forward + backward + optimize
            outputs = net(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            # print statistics
            running_loss += loss.item()
            epoch_steps += 1
            if i % 2000 == 1999:  # print every 2000 mini-batches
                print("[%d, %5d] loss: %.3f" % (epoch + 1, i + 1,
                                                running_loss / epoch_steps))
                running_loss = 0.0

        # Validation loss
        val_loss = 0.0
        val_steps = 0
        total = 0
        correct = 0
        for i, data in enumerate(valloader, 0):
            with torch.no_grad():
                inputs, labels = data
                inputs, labels = inputs.to(device), labels.to(device)

                outputs = net(inputs)
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

                loss = criterion(outputs, labels)
                val_loss += loss.cpu().numpy()
                val_steps += 1

        with tune.checkpoint_dir(epoch) as checkpoint_dir:
            path = os.path.join(checkpoint_dir, "checkpoint")
            torch.save((net.state_dict(), optimizer.state_dict()), path)

        tune.report(loss=(val_loss / val_steps), accuracy=correct / total)
    print("Finished Training")

######################################################################
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4037')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/examples/cifar10_pytorch.py: 66-160
</a>
<div class="mid" id="frag4037" style="display:none"><pre>
def train_cifar(config, checkpoint_dir=None):
    net = Net(config["l1"], config["l2"])

    device = "cpu"
    if torch.cuda.is_available():
        device = "cuda:0"
        if torch.cuda.device_count() &gt; 1:
            net = nn.DataParallel(net)
    net.to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(net.parameters(), lr=config["lr"], momentum=0.9)

    # The `checkpoint_dir` parameter gets passed by Ray Tune when a checkpoint
    # should be restored.
    if checkpoint_dir:
        checkpoint = os.path.join(checkpoint_dir, "checkpoint")
        model_state, optimizer_state = torch.load(checkpoint)
        net.load_state_dict(model_state)
        optimizer.load_state_dict(optimizer_state)

    data_dir = os.path.abspath("./data")
    trainset, testset = load_data(data_dir)

    test_abs = int(len(trainset) * 0.8)
    train_subset, val_subset = random_split(
        trainset, [test_abs, len(trainset) - test_abs])

    trainloader = torch.utils.data.DataLoader(
        train_subset,
        batch_size=int(config["batch_size"]),
        shuffle=True,
        num_workers=8)
    valloader = torch.utils.data.DataLoader(
        val_subset,
        batch_size=int(config["batch_size"]),
        shuffle=True,
        num_workers=8)

    for epoch in range(10):  # loop over the dataset multiple times
        running_loss = 0.0
        epoch_steps = 0
        for i, data in enumerate(trainloader, 0):
            # get the inputs; data is a list of [inputs, labels]
            inputs, labels = data
            inputs, labels = inputs.to(device), labels.to(device)

            # zero the parameter gradients
            optimizer.zero_grad()

            # forward + backward + optimize
            outputs = net(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            # print statistics
            running_loss += loss.item()
            epoch_steps += 1
            if i % 2000 == 1999:  # print every 2000 mini-batches
                print("[%d, %5d] loss: %.3f" % (epoch + 1, i + 1,
                                                running_loss / epoch_steps))
                running_loss = 0.0

        # Validation loss
        val_loss = 0.0
        val_steps = 0
        total = 0
        correct = 0
        for i, data in enumerate(valloader, 0):
            with torch.no_grad():
                inputs, labels = data
                inputs, labels = inputs.to(device), labels.to(device)

                outputs = net(inputs)
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

                loss = criterion(outputs, labels)
                val_loss += loss.cpu().numpy()
                val_steps += 1

        # Here we save a checkpoint. It is automatically registered with
        # Ray Tune and will potentially be passed as the `checkpoint_dir`
        # parameter in future iterations.
        with tune.checkpoint_dir(step=epoch) as checkpoint_dir:
            path = os.path.join(checkpoint_dir, "checkpoint")
            torch.save(
                (net.state_dict(), optimizer.state_dict()), path)

        tune.report(loss=(val_loss / val_steps), accuracy=correct / total)
    print("Finished Training")
# __train_end__

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 91:</b> &nbsp; 7 fragments, nominal size 19 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3389')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_trial_runner.py: 74-99
</a>
<div class="mid" id="frag3389" style="display:none"><pre>
    def testExtraResources(self):
        ray.init(num_cpus=4, num_gpus=2)
        runner = TrialRunner()
        kwargs = {
            "stopping_criterion": {
                "training_iteration": 1
            },
            "placement_group_factory": PlacementGroupFactory([{
                "CPU": 1
            }, {
                "CPU": 3,
                "GPU": 1
            }]),
        }
        trials = [Trial("__fake", **kwargs), Trial("__fake", **kwargs)]
        for t in trials:
            runner.add_trial(t)

        runner.step()
        self.assertEqual(trials[0].status, Trial.RUNNING)
        self.assertEqual(trials[1].status, Trial.PENDING)

        runner.step()
        self.assertEqual(trials[0].status, Trial.TERMINATED)
        self.assertEqual(trials[1].status, Trial.PENDING)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3391')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_trial_runner.py: 123-148
</a>
<div class="mid" id="frag3391" style="display:none"><pre>
    def testExtraCustomResources(self):
        ray.init(num_cpus=4, num_gpus=2, resources={"a": 2})
        runner = TrialRunner()
        kwargs = {
            "stopping_criterion": {
                "training_iteration": 1
            },
            "placement_group_factory": PlacementGroupFactory([{
                "CPU": 1
            }, {
                "a": 2
            }]),
        }
        trials = [Trial("__fake", **kwargs), Trial("__fake", **kwargs)]
        for t in trials:
            runner.add_trial(t)

        runner.step()
        self.assertEqual(trials[0].status, Trial.RUNNING)
        self.assertEqual(trials[1].status, Trial.PENDING)

        runner.step()
        self.assertTrue(sum(t.status == Trial.RUNNING for t in trials) &lt; 2)
        self.assertEqual(trials[0].status, Trial.TERMINATED)
        self.assertEqual(trials[1].status, Trial.PENDING)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3590')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_trial_runner_2.py: 50-71
</a>
<div class="mid" id="frag3590" style="display:none"><pre>
    def testErrorHandling(self):
        ray.init(num_cpus=4, num_gpus=2)
        runner = TrialRunner()
        kwargs = {
            "stopping_criterion": {
                "training_iteration": 1
            },
            "resources": Resources(cpu=1, gpu=1),
        }
        _global_registry.register(TRAINABLE_CLASS, "asdf", None)
        trials = [Trial("asdf", **kwargs), Trial("__fake", **kwargs)]
        for t in trials:
            runner.add_trial(t)

        runner.step()
        self.assertEqual(trials[0].status, Trial.ERROR)
        self.assertEqual(trials[1].status, Trial.PENDING)

        runner.step()
        self.assertEqual(trials[0].status, Trial.ERROR)
        self.assertEqual(trials[1].status, Trial.RUNNING)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3390')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_trial_runner.py: 100-122
</a>
<div class="mid" id="frag3390" style="display:none"><pre>
    def testCustomResources(self):
        ray.init(num_cpus=4, num_gpus=2, resources={"a": 2})
        runner = TrialRunner()
        kwargs = {
            "stopping_criterion": {
                "training_iteration": 1
            },
            "placement_group_factory": PlacementGroupFactory([{
                "CPU": 1,
                "a": 2
            }]),
        }
        trials = [Trial("__fake", **kwargs), Trial("__fake", **kwargs)]
        for t in trials:
            runner.add_trial(t)

        runner.step()
        self.assertEqual(trials[0].status, Trial.RUNNING)
        self.assertEqual(trials[1].status, Trial.PENDING)
        runner.step()
        self.assertEqual(trials[0].status, Trial.TERMINATED)
        self.assertEqual(trials[1].status, Trial.PENDING)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3396')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_trial_runner.py: 238-261
</a>
<div class="mid" id="frag3396" style="display:none"><pre>
    def testMultiStepRun2(self):
        """Checks that runner.step throws when overstepping."""
        ray.init(num_cpus=1)
        runner = TrialRunner()
        kwargs = {
            "stopping_criterion": {
                "training_iteration": 2
            },
            "resources": Resources(cpu=1, gpu=0),
        }
        trials = [Trial("__fake", **kwargs)]
        for t in trials:
            runner.add_trial(t)

        runner.step()
        self.assertEqual(trials[0].status, Trial.RUNNING)

        runner.step()
        self.assertEqual(trials[0].status, Trial.RUNNING)

        runner.step()
        self.assertEqual(trials[0].status, Trial.TERMINATED)
        self.assertRaises(TuneError, runner.step)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3394')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_trial_runner.py: 180-208
</a>
<div class="mid" id="frag3394" style="display:none"><pre>
    def testResourceScheduler(self):
        ray.init(num_cpus=4, num_gpus=1)
        runner = TrialRunner()
        kwargs = {
            "stopping_criterion": {
                "training_iteration": 1
            },
            "resources": Resources(cpu=1, gpu=1),
        }
        trials = [Trial("__fake", **kwargs), Trial("__fake", **kwargs)]
        for t in trials:
            runner.add_trial(t)

        runner.step()
        self.assertEqual(trials[0].status, Trial.RUNNING)
        self.assertEqual(trials[1].status, Trial.PENDING)

        runner.step()
        self.assertEqual(trials[0].status, Trial.TERMINATED)
        self.assertEqual(trials[1].status, Trial.PENDING)

        runner.step()
        self.assertEqual(trials[0].status, Trial.TERMINATED)
        self.assertEqual(trials[1].status, Trial.RUNNING)

        runner.step()
        self.assertEqual(trials[0].status, Trial.TERMINATED)
        self.assertEqual(trials[1].status, Trial.TERMINATED)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3395')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_trial_runner.py: 209-237
</a>
<div class="mid" id="frag3395" style="display:none"><pre>
    def testMultiStepRun(self):
        ray.init(num_cpus=4, num_gpus=2)
        runner = TrialRunner()
        kwargs = {
            "stopping_criterion": {
                "training_iteration": 5
            },
            "resources": Resources(cpu=1, gpu=1),
        }
        trials = [Trial("__fake", **kwargs), Trial("__fake", **kwargs)]
        for t in trials:
            runner.add_trial(t)

        runner.step()
        self.assertEqual(trials[0].status, Trial.RUNNING)
        self.assertEqual(trials[1].status, Trial.PENDING)

        runner.step()
        self.assertEqual(trials[0].status, Trial.RUNNING)
        self.assertEqual(trials[1].status, Trial.RUNNING)

        runner.step()
        self.assertEqual(trials[0].status, Trial.RUNNING)
        self.assertEqual(trials[1].status, Trial.RUNNING)

        runner.step()
        self.assertEqual(trials[0].status, Trial.RUNNING)
        self.assertEqual(trials[1].status, Trial.RUNNING)

</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 92:</b> &nbsp; 2 fragments, nominal size 22 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3451')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_trainable_util.py: 90-114
</a>
<div class="mid" id="frag3451" style="display:none"><pre>
    def test_multi_level_nested(self):
        ori_in = OrderedDict({
            "a": {
                "b": {
                    "c": {
                        "d": 1,
                    },
                },
            },
            "b": {
                "c": {
                    "d": 2,
                },
            },
            "c": {
                "d": 3,
            },
            "e": 4,
        })
        in_ = copy.deepcopy(ori_in)
        result = flatten_dict(in_)
        assert in_ == ori_in
        assert result == {"a/b/c/d": 1, "b/c/d": 2, "c/d": 3, "e": 4}


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3454')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_trainable_util.py: 125-145
</a>
<div class="mid" id="frag3454" style="display:none"><pre>
    def test_multi_level_nested(self):
        result = unflatten_dict({"a/b/c/d": 1, "b/c/d": 2, "c/d": 3, "e": 4})
        assert result == {
            "a": {
                "b": {
                    "c": {
                        "d": 1,
                    },
                },
            },
            "b": {
                "c": {
                    "d": 2,
                },
            },
            "c": {
                "d": 3,
            },
            "e": 4,
        }

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 93:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 86%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3509')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_tune_restore.py: 69-82
</a>
<div class="mid" id="frag3509" style="display:none"><pre>
    def testTuneRestore(self):
        self.assertTrue(os.path.isfile(self.checkpoint_path))
        tune.run(
            "PG",
            name="TuneRestoreTest",
            stop={"training_iteration": 2},  # train one more iteration.
            checkpoint_freq=1,
            restore=self.checkpoint_path,  # Restore the checkpoint
            config={
                "env": "CartPole-v0",
                "framework": "tf",
            },
        )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3510')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_tune_restore.py: 83-100
</a>
<div class="mid" id="frag3510" style="display:none"><pre>
    def testPostRestoreCheckpointExistence(self):
        """Tests that checkpoint restored from is not deleted post-restore."""
        self.assertTrue(os.path.isfile(self.checkpoint_path))
        tune.run(
            "PG",
            name="TuneRestoreTest",
            stop={"training_iteration": 2},
            checkpoint_freq=1,
            keep_checkpoints_num=1,
            restore=self.checkpoint_path,
            config={
                "env": "CartPole-v0",
                "framework": "tf",
            },
        )
        self.assertTrue(os.path.isfile(self.checkpoint_path))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 94:</b> &nbsp; 2 fragments, nominal size 27 lines, similarity 77%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3524')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_tune_restore.py: 241-272
</a>
<div class="mid" id="frag3524" style="display:none"><pre>
    def testFailResumeGridSearch(self):
        os.environ["TUNE_MAX_PENDING_TRIALS_PG"] = "1"

        config = dict(
            num_samples=3,
            fail_fast=True,
            config={
                "test": tune.grid_search([1, 2, 3]),
                "test2": tune.grid_search([1, 2, 3]),
            },
            stop={"training_iteration": 2},
            local_dir=self.logdir,
            verbose=1)

        with self.assertRaises(RuntimeError):
            tune.run(
                "trainable",
                callbacks=[self.FailureInjectorCallback()],
                **config)

        analysis = tune.run(
            "trainable",
            resume=True,
            callbacks=[self.CheckStateCallback()],
            **config)
        assert len(analysis.trials) == 27
        test_counter = Counter([t.config["test"] for t in analysis.trials])
        assert all(v == 9 for v in test_counter.values())
        test2_counter = Counter([t.config["test2"] for t in analysis.trials])
        assert all(v == 9 for v in test2_counter.values())

    # Unfinished trials' resources should be updated.
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3525')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_tune_restore.py: 273-303
</a>
<div class="mid" id="frag3525" style="display:none"><pre>
    def testResourceUpdateInResume(self):
        os.environ["TUNE_MAX_PENDING_TRIALS_PG"] = "1"

        config = dict(
            num_samples=3,
            fail_fast=True,
            config={
                "test": tune.grid_search([1, 2, 3]),
                "test2": tune.grid_search([1, 2, 3]),
            },
            stop={"training_iteration": 2},
            local_dir=self.logdir,
            verbose=1)

        with self.assertRaises(RuntimeError):
            tune.run(
                "trainable",
                callbacks=[
                    self.FailureInjectorCallback(),
                    self.CheckTrialResourcesCallback(1)
                ],
                **config)

        analysis = tune.run(
            "trainable",
            resume=True,
            resources_per_trial={"cpu": 2},
            callbacks=[self.CheckTrialResourcesCallback(2)],
            **config)
        assert len(analysis.trials) == 27

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 95:</b> &nbsp; 2 fragments, nominal size 36 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3526')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_tune_restore.py: 304-346
</a>
<div class="mid" id="frag3526" style="display:none"><pre>
    def testFailResumeWithPreset(self):
        os.environ["TUNE_MAX_PENDING_TRIALS_PG"] = "1"

        search_alg = BasicVariantGenerator(points_to_evaluate=[{
            "test": -1,
            "test2": -1
        }, {
            "test": -1
        }, {
            "test2": -1
        }])

        config = dict(
            num_samples=3 + 3,  # 3 preset, 3 samples
            fail_fast=True,
            config={
                "test": tune.grid_search([1, 2, 3]),
                "test2": tune.grid_search([1, 2, 3]),
            },
            stop={"training_iteration": 2},
            local_dir=self.logdir,
            verbose=1)
        with self.assertRaises(RuntimeError):
            tune.run(
                "trainable",
                callbacks=[self.FailureInjectorCallback(5)],
                search_alg=search_alg,
                **config)

        analysis = tune.run(
            "trainable",
            resume=True,
            callbacks=[self.CheckStateCallback(expected_trials=5)],
            search_alg=search_alg,
            **config)
        assert len(analysis.trials) == 34
        test_counter = Counter([t.config["test"] for t in analysis.trials])
        assert test_counter.pop(-1) == 4
        assert all(v == 10 for v in test_counter.values())
        test2_counter = Counter([t.config["test2"] for t in analysis.trials])
        assert test2_counter.pop(-1) == 4
        assert all(v == 10 for v in test2_counter.values())

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3527')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_tune_restore.py: 347-390
</a>
<div class="mid" id="frag3527" style="display:none"><pre>
    def testFailResumeAfterPreset(self):
        os.environ["TUNE_MAX_PENDING_TRIALS_PG"] = "1"

        search_alg = BasicVariantGenerator(points_to_evaluate=[{
            "test": -1,
            "test2": -1
        }, {
            "test": -1
        }, {
            "test2": -1
        }])

        config = dict(
            num_samples=3 + 3,  # 3 preset, 3 samples
            fail_fast=True,
            config={
                "test": tune.grid_search([1, 2, 3]),
                "test2": tune.grid_search([1, 2, 3]),
            },
            stop={"training_iteration": 2},
            local_dir=self.logdir,
            verbose=1)

        with self.assertRaises(RuntimeError):
            tune.run(
                "trainable",
                callbacks=[self.FailureInjectorCallback(15)],
                search_alg=search_alg,
                **config)

        analysis = tune.run(
            "trainable",
            resume=True,
            callbacks=[self.CheckStateCallback(expected_trials=15)],
            search_alg=search_alg,
            **config)
        assert len(analysis.trials) == 34
        test_counter = Counter([t.config["test"] for t in analysis.trials])
        assert test_counter.pop(-1) == 4
        assert all(v == 10 for v in test_counter.values())
        test2_counter = Counter([t.config["test2"] for t in analysis.trials])
        assert test2_counter.pop(-1) == 4
        assert all(v == 10 for v in test2_counter.values())

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 96:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3545')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_tune_restore.py: 538-549
</a>
<div class="mid" id="frag3545" style="display:none"><pre>
    def run_explicit_restore(self, random_state, checkpoint_path):
        np.random.set_state(random_state)
        search_alg2, cost = self.set_basic_conf()
        search_alg2 = ConcurrencyLimiter(search_alg2, 1)
        search_alg2.restore(checkpoint_path)
        return tune.run(
            cost,
            num_samples=5,
            search_alg=search_alg2,
            scheduler=self.get_scheduler(),
            verbose=0)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3546')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_tune_restore.py: 550-560
</a>
<div class="mid" id="frag3546" style="display:none"><pre>
    def run_full(self):
        np.random.seed(162)
        search_alg3, cost = self.set_basic_conf()
        search_alg3 = ConcurrencyLimiter(search_alg3, 1)
        return tune.run(
            cost,
            num_samples=10,
            search_alg=search_alg3,
            scheduler=self.get_scheduler(),
            verbose=0)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 97:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3547')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_tune_restore.py: 561-572
</a>
<div class="mid" id="frag3547" style="display:none"><pre>
    def testWarmStart(self):
        results_exp_1, r_state, checkpoint_path = self.run_part_from_scratch()
        results_exp_2 = self.run_explicit_restore(r_state, checkpoint_path)
        results_exp_3 = self.run_full()
        trials_1_config = self.treat_trial_config(
            [trial.config for trial in results_exp_1.trials])
        trials_2_config = self.treat_trial_config(
            [trial.config for trial in results_exp_2.trials])
        trials_3_config = self.treat_trial_config(
            [trial.config for trial in results_exp_3.trials])
        self.assertEqual(trials_1_config + trials_2_config, trials_3_config)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3548')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_tune_restore.py: 573-586
</a>
<div class="mid" id="frag3548" style="display:none"><pre>
    def testRestore(self):
        results_exp_1, r_state, checkpoint_path = self.run_part_from_scratch()
        results_exp_2 = self.run_from_experiment_restore(r_state)
        results_exp_3 = self.run_full()

        trials_1_config = self.treat_trial_config(
            [trial.config for trial in results_exp_1.trials])
        trials_2_config = self.treat_trial_config(
            [trial.config for trial in results_exp_2.trials])
        trials_3_config = self.treat_trial_config(
            [trial.config for trial in results_exp_3.trials])
        self.assertEqual(trials_1_config + trials_2_config, trials_3_config)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 98:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 92%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3554')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_tune_restore.py: 629-647
</a>
<div class="mid" id="frag3554" style="display:none"><pre>
    def set_basic_conf(self):
        space = {
            "height": tune.uniform(-100, 100),
            "width": tune.randint(0, 100),
        }

        def cost(param, reporter):
            reporter(loss=(param["height"] - 14)**2 - abs(param["width"] - 3))

        search_alg = CFO(
            space=space,
            metric="loss",
            mode="min",
            seed=20,
        )

        return search_alg, cost


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3556')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_tune_restore.py: 649-668
</a>
<div class="mid" id="frag3556" style="display:none"><pre>
    def set_basic_conf(self):
        space = {
            "height": tune.uniform(-100, 100),
            "width": tune.randint(0, 100),
            "time_budget_s": 10,
        }

        def cost(param, reporter):
            reporter(loss=(param["height"] - 14)**2 - abs(param["width"] - 3))

        search_alg = BlendSearch(
            space=space,
            metric="loss",
            mode="min",
            seed=20,
        )

        return search_alg, cost


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 99:</b> &nbsp; 5 fragments, nominal size 24 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3592')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_trial_runner_2.py: 78-104
</a>
<div class="mid" id="frag3592" style="display:none"><pre>
    def testFailureRecoveryDisabled(self):
        ray.init(num_cpus=1, num_gpus=1)
        searchalg, scheduler = create_mock_components()

        runner = TrialRunner(searchalg, scheduler=scheduler)
        kwargs = {
            "resources": Resources(cpu=1, gpu=1),
            "checkpoint_freq": 1,
            "max_failures": 0,
            "config": {
                "mock_error": True,
            },
        }
        runner.add_trial(Trial("__fake", **kwargs))
        trials = runner.get_trials()

        runner.step()  # Start trial
        self.assertEqual(trials[0].status, Trial.RUNNING)
        runner.step()  # Process result, dispatch save
        self.assertEqual(trials[0].status, Trial.RUNNING)
        runner.step()  # Process save
        runner.step()  # Error
        self.assertEqual(trials[0].status, Trial.ERROR)
        self.assertEqual(trials[0].num_failures, 1)
        self.assertEqual(len(searchalg.errored_trials), 1)
        self.assertEqual(len(scheduler.errored_trials), 1)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3595')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_trial_runner_2.py: 167-191
</a>
<div class="mid" id="frag3595" style="display:none"><pre>
    def testFailFast(self):
        ray.init(num_cpus=1, num_gpus=1)
        runner = TrialRunner(fail_fast=True)
        kwargs = {
            "resources": Resources(cpu=1, gpu=1),
            "checkpoint_freq": 1,
            "max_failures": 0,
            "config": {
                "mock_error": True,
                "persistent_error": True,
            },
        }
        runner.add_trial(Trial("__fake", **kwargs))
        runner.add_trial(Trial("__fake", **kwargs))
        trials = runner.get_trials()

        runner.step()  # Start trial
        self.assertEqual(trials[0].status, Trial.RUNNING)
        runner.step()  # Process result, dispatch save
        self.assertEqual(trials[0].status, Trial.RUNNING)
        runner.step()  # Process save
        runner.step()  # Error
        self.assertEqual(trials[0].status, Trial.ERROR)
        self.assertRaises(TuneError, lambda: runner.step())

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3596')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_trial_runner_2.py: 192-215
</a>
<div class="mid" id="frag3596" style="display:none"><pre>
    def testFailFastRaise(self):
        ray.init(num_cpus=1, num_gpus=1)
        runner = TrialRunner(fail_fast=TrialRunner.RAISE)
        kwargs = {
            "resources": Resources(cpu=1, gpu=1),
            "checkpoint_freq": 1,
            "max_failures": 0,
            "config": {
                "mock_error": True,
                "persistent_error": True,
            },
        }
        runner.add_trial(Trial("__fake", **kwargs))
        runner.add_trial(Trial("__fake", **kwargs))
        trials = runner.get_trials()

        runner.step()  # Start trial
        self.assertEqual(trials[0].status, Trial.RUNNING)
        runner.step()  # Process result, dispatch save
        self.assertEqual(trials[0].status, Trial.RUNNING)
        runner.step()  # Process save
        with self.assertRaises(Exception):
            runner.step()  # Error

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3593')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_trial_runner_2.py: 105-134
</a>
<div class="mid" id="frag3593" style="display:none"><pre>
    def testFailureRecoveryEnabled(self):
        ray.init(num_cpus=1, num_gpus=1)
        searchalg, scheduler = create_mock_components()

        runner = TrialRunner(searchalg, scheduler=scheduler)

        kwargs = {
            "resources": Resources(cpu=1, gpu=1),
            "checkpoint_freq": 1,
            "max_failures": 1,
            "config": {
                "mock_error": True,
            },
        }
        runner.add_trial(Trial("__fake", **kwargs))
        trials = runner.get_trials()

        runner.step()  # Start trial
        self.assertEqual(trials[0].status, Trial.RUNNING)
        runner.step()  # Process result, dispatch save
        self.assertEqual(trials[0].status, Trial.RUNNING)
        runner.step()  # Process save
        runner.step()  # Error (transient), dispatch restore
        self.assertEqual(trials[0].status, Trial.RUNNING)
        self.assertEqual(trials[0].num_failures, 1)
        runner.step()  # Process restore
        self.assertEqual(trials[0].status, Trial.RUNNING)
        self.assertEqual(len(searchalg.errored_trials), 0)
        self.assertEqual(len(scheduler.errored_trials), 0)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3594')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_trial_runner_2.py: 135-166
</a>
<div class="mid" id="frag3594" style="display:none"><pre>
    def testFailureRecoveryMaxFailures(self):
        ray.init(num_cpus=1, num_gpus=1)
        runner = TrialRunner()
        kwargs = {
            "resources": Resources(cpu=1, gpu=1),
            "checkpoint_freq": 1,
            "max_failures": 2,
            "config": {
                "mock_error": True,
                "persistent_error": True,
            },
        }
        runner.add_trial(Trial("__fake", **kwargs))
        trials = runner.get_trials()

        runner.step()  # Start trial
        self.assertEqual(trials[0].status, Trial.RUNNING)
        runner.step()  # Process result, dispatch save
        self.assertEqual(trials[0].status, Trial.RUNNING)
        runner.step()  # Process save
        runner.step()  # Error (transient), dispatch restore
        self.assertEqual(trials[0].status, Trial.RUNNING)
        self.assertEqual(trials[0].num_failures, 1)
        runner.step()  # Process restore
        runner.step()  # Error (transient), dispatch restore
        self.assertEqual(trials[0].status, Trial.RUNNING)
        self.assertEqual(trials[0].num_failures, 2)
        runner.step()  # Process restore
        runner.step()  # Error (terminal)
        self.assertEqual(trials[0].status, Trial.ERROR)
        self.assertEqual(trials[0].num_failures, 3)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 100:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 82%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3599')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_trial_runner_2.py: 285-305
</a>
<div class="mid" id="frag3599" style="display:none"><pre>
    def testCheckpointingAtEnd(self):
        ray.init(num_cpus=1, num_gpus=1)
        runner = TrialRunner()
        kwargs = {
            "stopping_criterion": {
                "training_iteration": 2
            },
            "checkpoint_at_end": True,
            "resources": Resources(cpu=1, gpu=1),
        }
        runner.add_trial(Trial("__fake", **kwargs))
        trials = runner.get_trials()

        runner.step()  # Start trial
        self.assertEqual(trials[0].status, Trial.RUNNING)
        runner.step()  # Process result
        runner.step()  # Process result, dispatch save
        self.assertEqual(trials[0].last_result[DONE], True)
        runner.step()  # Process save
        self.assertEqual(trials[0].has_checkpoint(), True)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3600')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_trial_runner_2.py: 306-325
</a>
<div class="mid" id="frag3600" style="display:none"><pre>
    def testResultDone(self):
        """Tests that last_result is marked `done` after trial is complete."""
        ray.init(num_cpus=1, num_gpus=1)
        runner = TrialRunner()
        kwargs = {
            "stopping_criterion": {
                "training_iteration": 2
            },
            "resources": Resources(cpu=1, gpu=1),
        }
        runner.add_trial(Trial("__fake", **kwargs))
        trials = runner.get_trials()

        runner.step()
        self.assertEqual(trials[0].status, Trial.RUNNING)
        runner.step()
        self.assertNotEqual(trials[0].last_result[DONE], True)
        runner.step()
        self.assertEqual(trials[0].last_result[DONE], True)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 101:</b> &nbsp; 9 fragments, nominal size 12 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3608')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_searchers.py: 71-86
</a>
<div class="mid" id="frag3608" style="display:none"><pre>
    def testBayesOpt(self):
        from ray.tune.suggest.bayesopt import BayesOptSearch

        out = tune.run(
            _invalid_objective,
            # At least one nan, inf, -inf and float
            search_alg=BayesOptSearch(random_state=1234),
            config=self.config,
            metric="_metric",
            mode="max",
            num_samples=8,
            reuse_actors=False)

        best_trial = out.best_trial
        self.assertLessEqual(best_trial.config["report"], 2.0)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3615')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_searchers.py: 199-215
</a>
<div class="mid" id="frag3615" style="display:none"><pre>
    def testNevergrad(self):
        from ray.tune.suggest.nevergrad import NevergradSearch
        import nevergrad as ng

        np.random.seed(2020)  # At least one nan, inf, -inf and float

        out = tune.run(
            _invalid_objective,
            search_alg=NevergradSearch(optimizer=ng.optimizers.RandomSearch),
            config=self.config,
            mode="max",
            num_samples=16,
            reuse_actors=False)

        best_trial = out.best_trial
        self.assertLessEqual(best_trial.config["report"], 2.0)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3618')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_searchers.py: 251-271
</a>
<div class="mid" id="frag3618" style="display:none"><pre>
    def testZOOpt(self):
        self.skipTest(
            "Recent ZOOpt versions fail handling invalid values gracefully. "
            "Skipping until we or they found a workaround. ")
        from ray.tune.suggest.zoopt import ZOOptSearch

        np.random.seed(1000)  # At least one nan, inf, -inf and float

        out = tune.run(
            _invalid_objective,
            search_alg=ZOOptSearch(budget=100, parallel_num=4),
            config=self.config,
            metric="_metric",
            mode="max",
            num_samples=8,
            reuse_actors=False)

        best_trial = out.best_trial
        self.assertLessEqual(best_trial.config["report"], 2.0)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3616')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_searchers.py: 216-233
</a>
<div class="mid" id="frag3616" style="display:none"><pre>
    def testOptuna(self):
        from ray.tune.suggest.optuna import OptunaSearch
        from optuna.samplers import RandomSampler

        np.random.seed(1000)  # At least one nan, inf, -inf and float

        out = tune.run(
            _invalid_objective,
            search_alg=OptunaSearch(sampler=RandomSampler(seed=1234)),
            config=self.config,
            metric="_metric",
            mode="max",
            num_samples=8,
            reuse_actors=False)

        best_trial = out.best_trial
        self.assertLessEqual(best_trial.config["report"], 2.0)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3617')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_searchers.py: 234-250
</a>
<div class="mid" id="frag3617" style="display:none"><pre>
    def testSkopt(self):
        from ray.tune.suggest.skopt import SkOptSearch

        np.random.seed(1234)  # At least one nan, inf, -inf and float

        out = tune.run(
            _invalid_objective,
            search_alg=SkOptSearch(),
            config=self.config,
            metric="_metric",
            mode="max",
            num_samples=8,
            reuse_actors=False)

        best_trial = out.best_trial
        self.assertLessEqual(best_trial.config["report"], 2.0)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3613')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_searchers.py: 167-182
</a>
<div class="mid" id="frag3613" style="display:none"><pre>
    def testHEBO(self):
        from ray.tune.suggest.hebo import HEBOSearch

        out = tune.run(
            _invalid_objective,
            # At least one nan, inf, -inf and float
            search_alg=HEBOSearch(random_state_seed=123),
            config=self.config,
            metric="_metric",
            mode="max",
            num_samples=8,
            reuse_actors=False)

        best_trial = out.best_trial
        self.assertLessEqual(best_trial.config["report"], 2.0)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3610')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_searchers.py: 110-124
</a>
<div class="mid" id="frag3610" style="display:none"><pre>
    def testBOHB(self):
        from ray.tune.suggest.bohb import TuneBOHB

        out = tune.run(
            _invalid_objective,
            search_alg=TuneBOHB(seed=1000),
            config=self.config,
            metric="_metric",
            mode="max",
            num_samples=8,
            reuse_actors=False)

        best_trial = out.best_trial
        self.assertLessEqual(best_trial.config["report"], 2.0)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3614')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_searchers.py: 183-198
</a>
<div class="mid" id="frag3614" style="display:none"><pre>
    def testHyperopt(self):
        from ray.tune.suggest.hyperopt import HyperOptSearch

        out = tune.run(
            _invalid_objective,
            # At least one nan, inf, -inf and float
            search_alg=HyperOptSearch(random_state_seed=1234),
            config=self.config,
            metric="_metric",
            mode="max",
            num_samples=8,
            reuse_actors=False)

        best_trial = out.best_trial
        self.assertLessEqual(best_trial.config["report"], 2.0)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3612')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_searchers.py: 150-166
</a>
<div class="mid" id="frag3612" style="display:none"><pre>
    def testDragonfly(self):
        from ray.tune.suggest.dragonfly import DragonflySearch

        np.random.seed(1000)  # At least one nan, inf, -inf and float

        out = tune.run(
            _invalid_objective,
            search_alg=DragonflySearch(domain="euclidean", optimizer="random"),
            config=self.config,
            metric="_metric",
            mode="max",
            num_samples=8,
            reuse_actors=False)

        best_trial = out.best_trial
        self.assertLessEqual(best_trial.config["point"], 2.0)

</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 102:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 94%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3609')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_searchers.py: 87-109
</a>
<div class="mid" id="frag3609" style="display:none"><pre>
    def testBlendSearch(self):
        from ray.tune.suggest.flaml import BlendSearch

        out = tune.run(
            _invalid_objective,
            search_alg=BlendSearch(points_to_evaluate=[{
                "report": 1.0
            }, {
                "report": 2.1
            }, {
                "report": 3.1
            }, {
                "report": 4.1
            }]),
            config=self.config,
            metric="_metric",
            mode="max",
            num_samples=16,
            reuse_actors=False)

        best_trial = out.best_trial
        self.assertLessEqual(best_trial.config["report"], 2.0)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3611')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_searchers.py: 125-149
</a>
<div class="mid" id="frag3611" style="display:none"><pre>
    def testCFO(self):
        self.skipTest("Broken in FLAML, reenable once "
                      "https://github.com/microsoft/FLAML/pull/263 is merged")
        from ray.tune.suggest.flaml import CFO

        out = tune.run(
            _invalid_objective,
            search_alg=CFO(points_to_evaluate=[{
                "report": 1.0
            }, {
                "report": 2.1
            }, {
                "report": 3.1
            }, {
                "report": 4.1
            }]),
            config=self.config,
            metric="_metric",
            mode="max",
            num_samples=16,
            reuse_actors=False)

        best_trial = out.best_trial
        self.assertLessEqual(best_trial.config["report"], 2.0)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 103:</b> &nbsp; 3 fragments, nominal size 16 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3637')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_searchers.py: 496-515
</a>
<div class="mid" id="frag3637" style="display:none"><pre>
    def testDragonfly(self):
        from ray.tune.suggest.dragonfly import DragonflySearch

        searcher = DragonflySearch(
            space=self.config,
            metric=self.metric_name,
            mode="max",
            domain="euclidean",
            optimizer="random")

        self._save(searcher)

        searcher = DragonflySearch(
            space=self.config,
            metric=self.metric_name,
            mode="max",
            domain="euclidean",
            optimizer="random")
        self._restore(searcher)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3640')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_searchers.py: 541-559
</a>
<div class="mid" id="frag3640" style="display:none"><pre>
    def testNevergrad(self):
        from ray.tune.suggest.nevergrad import NevergradSearch
        import nevergrad as ng

        searcher = NevergradSearch(
            space=self.config,
            metric=self.metric_name,
            mode="max",
            optimizer=ng.optimizers.RandomSearch)

        self._save(searcher)

        searcher = NevergradSearch(
            space=self.config,
            metric=self.metric_name,
            mode="max",
            optimizer=ng.optimizers.RandomSearch)
        self._restore(searcher)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3643')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_searchers.py: 584-604
</a>
<div class="mid" id="frag3643" style="display:none"><pre>
    def testZOOpt(self):
        from ray.tune.suggest.zoopt import ZOOptSearch

        searcher = ZOOptSearch(
            space=self.config,
            metric=self.metric_name,
            mode="max",
            budget=100,
            parallel_num=4)

        self._save(searcher)

        searcher = ZOOptSearch(
            space=self.config,
            metric=self.metric_name,
            mode="max",
            budget=100,
            parallel_num=4)
        self._restore(searcher)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 104:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3655')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_run_experiment.py: 95-115
</a>
<div class="mid" id="frag3655" style="display:none"><pre>
    def testCheckpointAtEnd(self):
        class train(Trainable):
            def step(self):
                return {"timesteps_this_iter": 1, "done": True}

            def save_checkpoint(self, path):
                checkpoint = os.path.join(path, "checkpoint")
                with open(checkpoint, "w") as f:
                    f.write("OK")
                return checkpoint

        trials = run_experiments({
            "foo": {
                "run": train,
                "checkpoint_at_end": True
            }
        })
        for trial in trials:
            self.assertEqual(trial.status, Trial.TERMINATED)
            self.assertTrue(trial.has_checkpoint())

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3658')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_run_experiment.py: 116-137
</a>
<div class="mid" id="frag3658" style="display:none"><pre>
    def testExportFormats(self):
        class train(Trainable):
            def step(self):
                return {"timesteps_this_iter": 1, "done": True}

            def _export_model(self, export_formats, export_dir):
                path = os.path.join(export_dir, "exported")
                with open(path, "w") as f:
                    f.write("OK")
                return {export_formats[0]: path}

        trials = run_experiments({
            "foo": {
                "run": train,
                "export_formats": ["format"]
            }
        })
        for trial in trials:
            self.assertEqual(trial.status, Trial.TERMINATED)
            self.assertTrue(
                os.path.exists(os.path.join(trial.logdir, "exported")))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 105:</b> &nbsp; 2 fragments, nominal size 33 lines, similarity 93%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3667')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_run_experiment.py: 179-225
</a>
<div class="mid" id="frag3667" style="display:none"><pre>
    def testCustomLoggerNoAutoLogging(self):
        """Does not create CSV/JSON logger callbacks automatically"""
        os.environ["TUNE_DISABLE_AUTO_CALLBACK_LOGGERS"] = "1"

        class CustomLogger(Logger):
            def on_result(self, result):
                with open(os.path.join(self.logdir, "test.log"), "w") as f:
                    f.write("hi")

        [trial] = run_experiments(
            {
                "foo": {
                    "run": "__fake",
                    "stop": {
                        "training_iteration": 1
                    }
                }
            },
            callbacks=[LegacyLoggerCallback(logger_classes=[CustomLogger])])
        self.assertTrue(os.path.exists(os.path.join(trial.logdir, "test.log")))
        self.assertFalse(
            os.path.exists(os.path.join(trial.logdir, "params.json")))

        [trial] = run_experiments({
            "foo": {
                "run": "__fake",
                "stop": {
                    "training_iteration": 1
                }
            }
        })
        self.assertFalse(
            os.path.exists(os.path.join(trial.logdir, "params.json")))

        [trial] = run_experiments(
            {
                "foo": {
                    "run": "__fake",
                    "stop": {
                        "training_iteration": 1
                    }
                }
            },
            callbacks=[LegacyLoggerCallback(logger_classes=[])])
        self.assertFalse(
            os.path.exists(os.path.join(trial.logdir, "params.json")))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3669')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_run_experiment.py: 226-273
</a>
<div class="mid" id="frag3669" style="display:none"><pre>
    def testCustomLoggerWithAutoLogging(self):
        """Creates CSV/JSON logger callbacks automatically"""
        if "TUNE_DISABLE_AUTO_CALLBACK_LOGGERS" in os.environ:
            del os.environ["TUNE_DISABLE_AUTO_CALLBACK_LOGGERS"]

        class CustomLogger(Logger):
            def on_result(self, result):
                with open(os.path.join(self.logdir, "test.log"), "w") as f:
                    f.write("hi")

        [trial] = run_experiments(
            {
                "foo": {
                    "run": "__fake",
                    "stop": {
                        "training_iteration": 1
                    }
                }
            },
            callbacks=[LegacyLoggerCallback(logger_classes=[CustomLogger])])
        self.assertTrue(os.path.exists(os.path.join(trial.logdir, "test.log")))
        self.assertTrue(
            os.path.exists(os.path.join(trial.logdir, "params.json")))

        [trial] = run_experiments({
            "foo": {
                "run": "__fake",
                "stop": {
                    "training_iteration": 1
                }
            }
        })
        self.assertTrue(
            os.path.exists(os.path.join(trial.logdir, "params.json")))

        [trial] = run_experiments(
            {
                "foo": {
                    "run": "__fake",
                    "stop": {
                        "training_iteration": 1
                    }
                }
            },
            callbacks=[LegacyLoggerCallback(logger_classes=[])])
        self.assertTrue(
            os.path.exists(os.path.join(trial.logdir, "params.json")))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 106:</b> &nbsp; 2 fragments, nominal size 21 lines, similarity 95%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3690')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_integration_pytorch_lightning.py: 99-125
</a>
<div class="mid" id="frag3690" style="display:none"><pre>

    def testCheckpointCallback(self):
        tmpdir = tempfile.mkdtemp()
        self.addCleanup(lambda: shutil.rmtree(tmpdir))

        def train(config):
            module = _MockModule(10., 20.)
            trainer = pl.Trainer(
                max_epochs=1,
                callbacks=[
                    _TuneCheckpointCallback(
                        "trainer.ckpt", on=["batch_end", "train_end"])
                ])
            trainer.fit(module)

        analysis = tune.run(
            train,
            stop={TRAINING_ITERATION: 10},
            keep_checkpoints_num=100,
            local_dir=tmpdir)

        checkpoints = [
            dir for dir in os.listdir(analysis.trials[0].logdir)
            if dir.startswith("checkpoint")
        ]
        # 10 checkpoints after each batch, 1 checkpoint at end
        self.assertEqual(len(checkpoints), 11)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3692')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_integration_pytorch_lightning.py: 126-153
</a>
<div class="mid" id="frag3692" style="display:none"><pre>

    def testReportCheckpointCallback(self):
        tmpdir = tempfile.mkdtemp()
        self.addCleanup(lambda: shutil.rmtree(tmpdir))

        def train(config):
            module = _MockModule(10., 20.)
            trainer = pl.Trainer(
                max_epochs=1,
                callbacks=[
                    TuneReportCheckpointCallback(
                        ["avg_val_loss"], "trainer.ckpt", on="validation_end")
                ])
            trainer.fit(module)

        analysis = tune.run(
            train,
            stop={TRAINING_ITERATION: 10},
            keep_checkpoints_num=100,
            local_dir=tmpdir)

        checkpoints = [
            dir for dir in os.listdir(analysis.trials[0].logdir)
            if dir.startswith("checkpoint")
        ]
        # 1 checkpoint after the validation step
        self.assertEqual(len(checkpoints), 1)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 107:</b> &nbsp; 3 fragments, nominal size 10 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3716')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_logger.py: 48-59
</a>
<div class="mid" id="frag3716" style="display:none"><pre>

    def tearDown(self):
        shutil.rmtree(self.test_dir, ignore_errors=True)

    def testLegacyCSV(self):
        config = {"a": 2, "b": 5, "c": {"c": {"D": 123}, "e": None}}
        t = Trial(
            evaluated_params=config, trial_id="csv", logdir=self.test_dir)
        logger = CSVLogger(config=config, logdir=self.test_dir, trial=t)
        logger.on_result(result(2, 4))
        logger.on_result(result(2, 5))
        logger.on_result(result(2, 6, score=[1, 2, 3], hello={"world": 1}))
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3725')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_logger.py: 217-227
</a>
<div class="mid" id="frag3725" style="display:none"><pre>
                        self.assertNotIn(key, v.metadata.plugin_data.content)

        self.assertEqual(len(results), 3)
        self.assertSequenceEqual([int(res) for res in results], [4, 5, 6])

    def testLegacyBadTBX(self):
        config = {"b": (1, 2, 3)}
        t = Trial(
            evaluated_params=config, trial_id="tbx", logdir=self.test_dir)
        logger = TBXLogger(config=config, logdir=self.test_dir, trial=t)
        logger.on_result(result(0, 4))
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3719')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_logger.py: 85-96
</a>
<div class="mid" id="frag3719" style="display:none"><pre>
        self.assertEqual(len(results), 3)
        self.assertSequenceEqual(
            [int(row["episode_reward_mean"]) for row in results], [4, 5, 6])

    def testJSONLegacyLogger(self):
        config = {"a": 2, "b": 5, "c": {"c": {"D": 123}, "e": None}}
        t = Trial(
            evaluated_params=config, trial_id="json", logdir=self.test_dir)
        logger = JsonLogger(config=config, logdir=self.test_dir, trial=t)
        logger.on_result(result(0, 4))
        logger.on_result(result(1, 5))
        logger.on_result(result(2, 6, score=[1, 2, 3], hello={"world": 1}))
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 108:</b> &nbsp; 3 fragments, nominal size 11 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3717')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_logger.py: 60-72
</a>
<div class="mid" id="frag3717" style="display:none"><pre>
        logger.close()

        self._validate_csv_result()

    def testCSV(self):
        config = {"a": 2, "b": 5, "c": {"c": {"D": 123}, "e": None}}
        t = Trial(
            evaluated_params=config, trial_id="csv", logdir=self.test_dir)
        logger = CSVLoggerCallback()
        logger.on_trial_result(0, [], t, result(0, 4))
        logger.on_trial_result(1, [], t, result(1, 5))
        logger.on_trial_result(
            2, [], t, result(2, 6, score=[1, 2, 3], hello={"world": 1}))
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3726')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_logger.py: 228-241
</a>
<div class="mid" id="frag3726" style="display:none"><pre>
        logger.on_result(result(2, 4, score=[1, 2, 3], hello={"world": 1}))
        with self.assertLogs("ray.tune.logger", level="INFO") as cm:
            logger.close()
        assert "INFO" in cm.output[0]

    def testBadTBX(self):
        config = {"b": (1, 2, 3)}
        t = Trial(
            evaluated_params=config, trial_id="tbx", logdir=self.test_dir)
        logger = TBXLoggerCallback()
        logger.on_trial_result(0, [], t, result(0, 4))
        logger.on_trial_result(1, [], t, result(1, 5))
        logger.on_trial_result(
            2, [], t, result(2, 6, score=[1, 2, 3], hello={"world": 1}))
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3720')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/tests/test_logger.py: 97-109
</a>
<div class="mid" id="frag3720" style="display:none"><pre>
        logger.close()

        self._validate_json_result(config)

    def testJSON(self):
        config = {"a": 2, "b": 5, "c": {"c": {"D": 123}, "e": None}}
        t = Trial(
            evaluated_params=config, trial_id="json", logdir=self.test_dir)
        logger = JsonLoggerCallback()
        logger.on_trial_result(0, [], t, result(0, 4))
        logger.on_trial_result(1, [], t, result(1, 5))
        logger.on_trial_result(
            2, [], t, result(2, 6, score=[1, 2, 3], hello={"world": 1}))
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 109:</b> &nbsp; 3 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3751')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/suggest/bohb.py: 170-184
</a>
<div class="mid" id="frag3751" style="display:none"><pre>

        bohb_config = self._bohb_config or {}
        self.bohber = BOHB(self._space, **bohb_config)

    def set_search_properties(self, metric: Optional[str], mode: Optional[str],
                              config: Dict, **spec) -&gt; bool:
        if self._space:
            return False
        space = self.convert_search_space(config)
        self._space = space

        if metric:
            self._metric = metric
        if mode:
            self._mode = mode
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3855')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/suggest/hebo.py: 215-228
</a>
<div class="mid" id="frag3855" style="display:none"><pre>
            else:
                self._initial_points = self._points_to_evaluate

    def set_search_properties(self, metric: Optional[str], mode: Optional[str],
                              config: Dict, **spec) -&gt; bool:
        if self._opt:
            return False
        space = self.convert_search_space(config)
        self._space = space
        if metric:
            self._metric = metric
        if mode:
            self._mode = mode

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3773')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/suggest/dragonfly.py: 292-305
</a>
<div class="mid" id="frag3773" style="display:none"><pre>
            # If only a mode was passed, use anonymous metric
            self._metric = DEFAULT_METRIC

    def set_search_properties(self, metric: Optional[str], mode: Optional[str],
                              config: Dict, **spec) -&gt; bool:
        if self._opt:
            return False
        space = self.convert_search_space(config)
        self._space = space
        if metric:
            self._metric = metric
        if mode:
            self._mode = mode

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 110:</b> &nbsp; 3 fragments, nominal size 17 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3912')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/examples/optuna_example.py: 33-54
</a>
<div class="mid" id="frag3912" style="display:none"><pre>
def run_optuna_tune(smoke_test=False):
    algo = OptunaSearch()
    algo = ConcurrencyLimiter(algo, max_concurrent=4)
    scheduler = AsyncHyperBandScheduler()
    analysis = tune.run(
        easy_objective,
        metric="mean_loss",
        mode="min",
        search_alg=algo,
        scheduler=scheduler,
        num_samples=10 if smoke_test else 100,
        config={
            "steps": 100,
            "width": tune.uniform(0, 20),
            "height": tune.uniform(-100, 100),
            # This is an ignored parameter.
            "activation": tune.choice(["relu", "tanh"])
        })

    print("Best hyperparameters found were: ", analysis.best_config)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3991')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/examples/cfo_example.py: 30-51
</a>
<div class="mid" id="frag3991" style="display:none"><pre>
def run_cfo_tune(smoke_test=False):
    algo = CFO()
    algo = ConcurrencyLimiter(algo, max_concurrent=4)
    scheduler = AsyncHyperBandScheduler()
    analysis = tune.run(
        easy_objective,
        metric="mean_loss",
        mode="min",
        search_alg=algo,
        scheduler=scheduler,
        num_samples=10 if smoke_test else 100,
        config={
            "steps": 100,
            "width": tune.uniform(0, 20),
            "height": tune.uniform(-100, 100),
            # This is an ignored parameter.
            "activation": tune.choice(["relu", "tanh"])
        })

    print("Best hyperparameters found were: ", analysis.best_config)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4092')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/examples/blendsearch_example.py: 30-51
</a>
<div class="mid" id="frag4092" style="display:none"><pre>
def run_blendsearch_tune(smoke_test=False):
    algo = BlendSearch()
    algo = ConcurrencyLimiter(algo, max_concurrent=4)
    scheduler = AsyncHyperBandScheduler()
    analysis = tune.run(
        easy_objective,
        metric="mean_loss",
        mode="min",
        search_alg=algo,
        scheduler=scheduler,
        num_samples=10 if smoke_test else 100,
        config={
            "steps": 100,
            "width": tune.uniform(0, 20),
            "height": tune.uniform(-100, 100),
            # This is an ignored parameter.
            "activation": tune.choice(["relu", "tanh"])
        })

    print("Best hyperparameters found were: ", analysis.best_config)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 111:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3951')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/examples/pbt_dcgan_mnist/common.py: 97-107
</a>
<div class="mid" id="frag3951" style="display:none"><pre>
    def __init__(self):
        super(Discriminator, self).__init__()
        self.main = nn.Sequential(
            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ndf * 2), nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ndf * 4), nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(ndf * 4, 1, 4, 1, 0, bias=False), nn.Sigmoid())

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6926')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/sgd/torch/examples/dcgan.py: 51-61
</a>
<div class="mid" id="frag6926" style="display:none"><pre>
    def __init__(self, features=32, num_channels=1):
        super(Discriminator, self).__init__()
        self.main = nn.Sequential(
            nn.Conv2d(num_channels, features, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(features, features * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(features * 2), nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(features * 2, features * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(features * 4), nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(features * 4, 1, 4, 1, 0, bias=False), nn.Sigmoid())

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 112:</b> &nbsp; 4 fragments, nominal size 10 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3962')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/examples/mnist_ptl_mini.py: 29-39
</a>
<div class="mid" id="frag3962" style="display:none"><pre>
    def forward(self, x):
        batch_size, channels, width, height = x.size()
        x = x.view(batch_size, -1)
        x = self.layer_1(x)
        x = torch.relu(x)
        x = self.layer_2(x)
        x = torch.relu(x)
        x = self.layer_3(x)
        x = torch.log_softmax(x, dim=1)
        return x

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6821')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/sgd/torch/resnet.py: 105-116
</a>
<div class="mid" id="frag6821" style="display:none"><pre>
    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.layer4(out)
        out = F.avg_pool2d(out, 4)
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6885')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/sgd/torch/examples/pytorch-lightning/mnist-ptl.py: 37-50
</a>
<div class="mid" id="frag6885" style="display:none"><pre>
    def forward(self, x):
        batch_size, channels, width, height = x.size()

        # (b, 1, 28, 28) -&gt; (b, 1*28*28)
        x = x.view(batch_size, -1)
        x = self.layer_1(x)
        x = torch.relu(x)
        x = self.layer_2(x)
        x = torch.relu(x)
        x = self.layer_3(x)

        x = torch.log_softmax(x, dim=1)
        return x

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4041')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/examples/mnist_pytorch_lightning.py: 48-62
</a>
<div class="mid" id="frag4041" style="display:none"><pre>

    def forward(self, x):
        batch_size, channels, width, height = x.size()
        x = x.view(batch_size, -1)

        x = self.layer_1(x)
        x = torch.relu(x)

        x = self.layer_2(x)
        x = torch.relu(x)

        x = self.layer_3(x)
        x = torch.log_softmax(x, dim=1)

        return x
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 113:</b> &nbsp; 3 fragments, nominal size 14 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4071')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/examples/tf_distributed_keras_example.py: 30-47
</a>
<div class="mid" id="frag4071" style="display:none"><pre>
def build_and_compile_cnn_model(config):
    model = tf.keras.Sequential([
        tf.keras.Input(shape=(28, 28)),
        tf.keras.layers.Reshape(target_shape=(28, 28, 1)),
        tf.keras.layers.Conv2D(32, 3, activation="relu"),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(config.get("hidden", 16), activation="relu"),
        tf.keras.layers.Dense(10)
    ])
    model.compile(
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        optimizer=tf.keras.optimizers.SGD(
            learning_rate=config.get("lr", 0.05),
            momentum=config.get("momentum", 0.5)),
        metrics=["accuracy"])
    return model


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6723')" href="javascript:;">
ray-ray-1.9.2/python/ray/train/examples/tensorflow_mnist_example.py: 32-48
</a>
<div class="mid" id="frag6723" style="display:none"><pre>
def build_and_compile_cnn_model(config):
    learning_rate = config.get("lr", 0.001)
    model = tf.keras.Sequential([
        tf.keras.Input(shape=(28, 28)),
        tf.keras.layers.Reshape(target_shape=(28, 28, 1)),
        tf.keras.layers.Conv2D(32, 3, activation="relu"),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(128, activation="relu"),
        tf.keras.layers.Dense(10)
    ])
    model.compile(
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),
        metrics=["accuracy"])
    return model


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6727')" href="javascript:;">
ray-ray-1.9.2/python/ray/train/examples/tensorflow_quick_start.py: 21-39
</a>
<div class="mid" id="frag6727" style="display:none"><pre>
def build_and_compile_cnn_model():
    model = tf.keras.Sequential([
        tf.keras.layers.InputLayer(input_shape=(28, 28)),
        tf.keras.layers.Reshape(target_shape=(28, 28, 1)),
        tf.keras.layers.Conv2D(32, 3, activation='relu'),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(10)
    ])
    model.compile(
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
        metrics=['accuracy'])
    return model

# __tf_setup_end__

# __tf_single_begin__

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 114:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4087')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/examples/wandb_example.py: 44-60
</a>
<div class="mid" id="frag4087" style="display:none"><pre>


def tune_decorated(api_key_file):
    """Example for using the @wandb_mixin decorator with the function API"""
    analysis = tune.run(
        decorated_train_function,
        metric="loss",
        mode="min",
        config={
            "mean": tune.grid_search([1, 2, 3, 4, 5]),
            "sd": tune.uniform(0.2, 0.8),
            "wandb": {
                "api_key_file": api_key_file,
                "project": "Wandb_example"
            }
        })
    return analysis.best_config
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4089')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/examples/wandb_example.py: 69-85
</a>
<div class="mid" id="frag4089" style="display:none"><pre>


def tune_trainable(api_key_file):
    """Example for using a WandTrainableMixin with the class API"""
    analysis = tune.run(
        WandbTrainable,
        metric="loss",
        mode="min",
        config={
            "mean": tune.grid_search([1, 2, 3, 4, 5]),
            "sd": tune.uniform(0.2, 0.8),
            "wandb": {
                "api_key_file": api_key_file,
                "project": "Wandb_example"
            }
        })
    return analysis.best_config
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 115:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4099')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/automl/genetic_searcher.py: 137-174
</a>
<div class="mid" id="frag4099" style="display:none"><pre>
    def _selection(candidate):
        """Perform selection action to candidates.

        For example, new gene = sample_1 + the 5th bit of sample2.

        Args:
            candidate: List of candidate genes (encodings).

        Examples:
            &gt;&gt;&gt; # Genes that represent 3 parameters
            &gt;&gt;&gt; gene1 = np.array([[0, 0, 1], [0, 1], [1, 0]])
            &gt;&gt;&gt; gene2 = np.array([[0, 1, 0], [1, 0], [0, 1]])
            &gt;&gt;&gt; new_gene = _selection([gene1, gene2])
            &gt;&gt;&gt; # new_gene could be gene1 overwritten with the
            &gt;&gt;&gt; # 2nd parameter of gene2
            &gt;&gt;&gt; # in which case:
            &gt;&gt;&gt; #   new_gene[0] = gene1[0]
            &gt;&gt;&gt; #   new_gene[1] = gene2[1]
            &gt;&gt;&gt; #   new_gene[2] = gene1[0]

        Returns:
            New gene (encoding)
        """
        sample_index1 = np.random.choice(len(candidate))
        sample_index2 = np.random.choice(len(candidate))
        sample_1 = candidate[sample_index1]
        sample_2 = candidate[sample_index2]
        select_index = np.random.choice(len(sample_1))
        logger.info(
            LOGGING_PREFIX + "Perform selection from %sth to %sth at index=%s",
            sample_index2, sample_index1, select_index)

        next_gen = []
        for i in range(len(sample_1)):
            sample = sample_2[i] if i is select_index else sample_1[i]
            next_gen.append(sample)
        return next_gen

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4100')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/automl/genetic_searcher.py: 176-214
</a>
<div class="mid" id="frag4100" style="display:none"><pre>
    def _crossover(candidate):
        """Perform crossover action to candidates.

        For example, new gene = 60% sample_1 + 40% sample_2.

        Args:
            candidate: List of candidate genes (encodings).

        Examples:
            &gt;&gt;&gt; # Genes that represent 3 parameters
            &gt;&gt;&gt; gene1 = np.array([[0, 0, 1], [0, 1], [1, 0]])
            &gt;&gt;&gt; gene2 = np.array([[0, 1, 0], [1, 0], [0, 1]])
            &gt;&gt;&gt; new_gene = _crossover([gene1, gene2])
            &gt;&gt;&gt; # new_gene could be the first [n=1] parameters of
            &gt;&gt;&gt; # gene1 + the rest of gene2
            &gt;&gt;&gt; # in which case:
            &gt;&gt;&gt; #   new_gene[0] = gene1[0]
            &gt;&gt;&gt; #   new_gene[1] = gene2[1]
            &gt;&gt;&gt; #   new_gene[2] = gene1[1]

        Returns:
            New gene (encoding)
        """
        sample_index1 = np.random.choice(len(candidate))
        sample_index2 = np.random.choice(len(candidate))
        sample_1 = candidate[sample_index1]
        sample_2 = candidate[sample_index2]
        cross_index = int(len(sample_1) * np.random.uniform(low=0.3, high=0.7))
        logger.info(
            LOGGING_PREFIX +
            "Perform crossover between %sth and %sth at index=%s",
            sample_index1, sample_index2, cross_index)

        next_gen = []
        for i in range(len(sample_1)):
            sample = sample_2[i] if i &gt; cross_index else sample_1[i]
            next_gen.append(sample)
        return next_gen

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 116:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4153')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/schedulers/hyperband.py: 123-145
</a>
<div class="mid" id="frag4153" style="display:none"><pre>
    def set_search_properties(self, metric: Optional[str],
                              mode: Optional[str]) -&gt; bool:
        if self._metric and metric:
            return False
        if self._mode and mode:
            return False

        if metric:
            self._metric = metric
        if mode:
            self._mode = mode

        if self._mode == "max":
            self._metric_op = 1.
        elif self._mode == "min":
            self._metric_op = -1.

        if self._metric is None and self._mode:
            # If only a mode was passed, use anonymous metric
            self._metric = DEFAULT_METRIC

        return True

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4186')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/schedulers/median_stopping_rule.py: 75-95
</a>
<div class="mid" id="frag4186" style="display:none"><pre>
    def set_search_properties(self, metric: Optional[str],
                              mode: Optional[str]) -&gt; bool:
        if self._metric and metric:
            return False
        if self._mode and mode:
            return False

        if metric:
            self._metric = metric
        if mode:
            self._mode = mode

        self._worst = float("-inf") if self._mode == "max" else float("inf")
        self._compare_op = max if self._mode == "max" else min

        if self._metric is None and self._mode:
            # If only a mode was passed, use anonymous metric
            self._metric = DEFAULT_METRIC

        return True

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 117:</b> &nbsp; 2 fragments, nominal size 27 lines, similarity 92%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4154')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/schedulers/hyperband.py: 146-187
</a>
<div class="mid" id="frag4154" style="display:none"><pre>
    def on_trial_add(self, trial_runner: "trial_runner.TrialRunner",
                     trial: Trial):
        """Adds new trial.

        On a new trial add, if current bracket is not filled,
        add to current bracket. Else, if current band is not filled,
        create new bracket, add to current bracket.
        Else, create new iteration, create new bracket, add to bracket."""
        if not self._metric or not self._metric_op:
            raise ValueError(
                "{} has been instantiated without a valid `metric` ({}) or "
                "`mode` ({}) parameter. Either pass these parameters when "
                "instantiating the scheduler, or pass them as parameters "
                "to `tune.run()`".format(self.__class__.__name__, self._metric,
                                         self._mode))

        cur_bracket = self._state["bracket"]
        cur_band = self._hyperbands[self._state["band_idx"]]
        if cur_bracket is None or cur_bracket.filled():
            retry = True
            while retry:
                # if current iteration is filled, create new iteration
                if self._cur_band_filled():
                    cur_band = []
                    self._hyperbands.append(cur_band)
                    self._state["band_idx"] += 1

                # cur_band will always be less than s_max_1 or else filled
                s = len(cur_band)
                assert s &lt; self._s_max_1, "Current band is filled!"
                if self._get_r0(s) == 0:
                    logger.info("Bracket too small - Retrying...")
                    cur_bracket = None
                else:
                    retry = False
                    cur_bracket = self._create_bracket(s)
                cur_band.append(cur_bracket)
                self._state["bracket"] = cur_bracket

        self._state["bracket"].add_trial(trial)
        self._trial_info[trial] = cur_bracket, self._state["band_idx"]

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4181')" href="javascript:;">
ray-ray-1.9.2/python/ray/tune/schedulers/hb_bohb.py: 27-70
</a>
<div class="mid" id="frag4181" style="display:none"><pre>
    def on_trial_add(self, trial_runner: "trial_runner.TrialRunner",
                     trial: Trial):
        """Adds new trial.

        On a new trial add, if current bracket is not filled, add to current
        bracket. Else, if current band is not filled, create new bracket, add
        to current bracket. Else, create new iteration, create new bracket,
        add to bracket.
        """
        if not self._metric or not self._metric_op:
            raise ValueError(
                "{} has been instantiated without a valid `metric` ({}) or "
                "`mode` ({}) parameter. Either pass these parameters when "
                "instantiating the scheduler, or pass them as parameters "
                "to `tune.run()`".format(self.__class__.__name__, self._metric,
                                         self._mode))

        cur_bracket = self._state["bracket"]
        cur_band = self._hyperbands[self._state["band_idx"]]
        if cur_bracket is None or cur_bracket.filled():
            retry = True
            while retry:
                # if current iteration is filled, create new iteration
                if self._cur_band_filled():
                    cur_band = []
                    self._hyperbands.append(cur_band)
                    self._state["band_idx"] += 1

                # MAIN CHANGE HERE - largest bracket first!
                # cur_band will always be less than s_max_1 or else filled
                s = self._s_max_1 - len(cur_band) - 1
                assert s &gt;= 0, "Current band is filled!"
                if self._get_r0(s) == 0:
                    logger.debug("BOHB: Bracket too small - Retrying...")
                    cur_bracket = None
                else:
                    retry = False
                    cur_bracket = self._create_bracket(s)
                cur_band.append(cur_bracket)
                self._state["bracket"] = cur_bracket

        self._state["bracket"].add_trial(trial)
        self._trial_info[trial] = cur_bracket, self._state["band_idx"]

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 118:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4432')" href="javascript:;">
ray-ray-1.9.2/python/ray/autoscaler/_private/aliyun/utils.py: 50-67
</a>
<div class="mid" id="frag4432" style="display:none"><pre>

class AcsClient:
    """
    A wrapper around Aliyun SDK. We use this wrapper in aliyun node provider.

    Parameters:
        access_key: The AccessKey ID of your aliyun account.
        access_key_secret: The AccessKey secret of your aliyun account.
        region_id: A region is a geographic area where a data center resides.
                   Region_id is the ID of region (e.g., cn-hangzhou,
                   us-west-1, etc.)
        max_retries: The maximum number of retries each connection.
    """

    def __init__(self, access_key, access_key_secret, region_id, max_retries):
        self.cli = client.AcsClient(
            ak=access_key,
            secret=access_key_secret,
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4436')" href="javascript:;">
ray-ray-1.9.2/python/ray/autoscaler/_private/aliyun/utils.py: 195-214
</a>
<div class="mid" id="frag4436" style="display:none"><pre>
        response = self._send_request(request)
        if response is not None:
            instance_ids = response.get("InstanceIdSets").get("InstanceIdSet")
            return instance_ids
        logging.error("instance created failed.")
        return None

    def create_security_group(self, vpc_id):
        """ Create a security group

        :param vpc_id: The ID of the VPC in which to create
                       the security group.
        :return: The created security group ID.
        """
        request = CreateSecurityGroupRequest()
        request.set_VpcId(vpc_id)
        response = self._send_request(request)
        if response is not None:
            security_group_id = response.get("SecurityGroupId")
            return security_group_id
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 119:</b> &nbsp; 2 fragments, nominal size 32 lines, similarity 79%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4433')" href="javascript:;">
ray-ray-1.9.2/python/ray/autoscaler/_private/aliyun/utils.py: 68-117
</a>
<div class="mid" id="frag4433" style="display:none"><pre>
            max_retry_time=max_retries,
            region_id=region_id,
        )

    def describe_instances(self, tags=None, instance_ids=None):
        """ Query the details of one or more Elastic Compute Service (ECS) instances.

        :param tags: The tags of the instance.
        :param instance_ids: The IDs of ECS instances
        :return: ECS instance list
        """
        request = DescribeInstancesRequest()
        if tags is not None:
            request.set_Tags(tags)
        if instance_ids is not None:
            request.set_InstanceIds(instance_ids)
        response = self._send_request(request)
        if response is not None:
            instance_list = response.get("Instances").get("Instance")
            return instance_list
        return None

    def create_instance(
            self,
            instance_type,
            image_id,
            tags,
            key_pair_name,
            optimized="optimized",
            instance_charge_type="PostPaid",
            spot_strategy="SpotWithPriceLimit",
            internet_charge_type="PayByTraffic",
            internet_max_bandwidth_out=5,
    ):
        """ Create a subscription or pay-as-you-go ECS instance.

        :param instance_type: The instance type of the ECS.
        :param image_id: The ID of the image used to create the instance.
        :param tags: The tags of the instance.
        :param key_pair_name: The name of the key pair to be bound to
                              the instance.
        :param optimized: Specifies whether the instance is I/O optimized
        :param instance_charge_type: The billing method of the instance.
                                     Default value: PostPaid.
        :param spot_strategy: The preemption policy for the pay-as-you-go
                              instance.
        :param internet_charge_type: The billing method for network usage.
                                     Default value: PayByTraffic.
        :param internet_max_bandwidth_out: The maximum inbound public
                                           bandwidth. Unit: Mbit/s.
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4434')" href="javascript:;">
ray-ray-1.9.2/python/ray/autoscaler/_private/aliyun/utils.py: 118-179
</a>
<div class="mid" id="frag4434" style="display:none"><pre>
        :return: The created instance ID.
        """
        request = CreateInstanceRequest()
        request.set_InstanceType(instance_type)
        request.set_ImageId(image_id)
        request.set_IoOptimized(optimized)
        request.set_InstanceChargeType(instance_charge_type)
        request.set_SpotStrategy(spot_strategy)
        request.set_InternetChargeType(internet_charge_type)
        request.set_InternetMaxBandwidthOut(internet_max_bandwidth_out)
        request.set_KeyPairName(key_pair_name)
        request.set_Tags(tags)

        response = self._send_request(request)
        if response is not None:
            instance_id = response.get("InstanceId")
            logging.info("instance %s created task submit successfully.",
                         instance_id)
            return instance_id
        logging.error("instance created failed.")
        return None

    def run_instances(
            self,
            instance_type,
            image_id,
            tags,
            security_group_id,
            vswitch_id,
            key_pair_name,
            amount=1,
            optimized="optimized",
            instance_charge_type="PostPaid",
            spot_strategy="SpotWithPriceLimit",
            internet_charge_type="PayByTraffic",
            internet_max_bandwidth_out=1,
    ):
        """ Create one or more pay-as-you-go or subscription
            Elastic Compute Service (ECS) instances

        :param instance_type: The instance type of the ECS.
        :param image_id: The ID of the image used to create the instance.
        :param tags: The tags of the instance.
        :param security_group_id: The ID of the security group to which to
                                  assign the instance. Instances in the same
                                  security group can communicate with
                                  each other.
        :param vswitch_id: The ID of the vSwitch to which to connect
                           the instance.
        :param key_pair_name: The name of the key pair to be bound to
                              the instance.
        :param amount: The number of instances that you want to create.
        :param optimized: Specifies whether the instance is I/O optimized
        :param instance_charge_type: The billing method of the instance.
                                     Default value: PostPaid.
        :param spot_strategy: The preemption policy for the pay-as-you-go
                              instance.
        :param internet_charge_type: The billing method for network usage.
                                     Default value: PayByTraffic.
        :param internet_max_bandwidth_out: The maximum inbound public
                                           bandwidth. Unit: Mbit/s.
        :return: The created instance IDs.
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 120:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4438')" href="javascript:;">
ray-ray-1.9.2/python/ray/autoscaler/_private/aliyun/utils.py: 233-252
</a>
<div class="mid" id="frag4438" style="display:none"><pre>
            return security_groups
        logging.error("describe security group failed.")
        return None

    def authorize_security_group(self, ip_protocol, port_range,
                                 security_group_id, source_cidr_ip):
        """ Create an inbound security group rule.

        :param ip_protocol: The transport layer protocol.
        :param port_range: The range of destination ports relevant to
                           the transport layer protocol.
        :param security_group_id: The ID of the destination security group.
        :param source_cidr_ip: The range of source IPv4 addresses.
                               CIDR blocks and IPv4 addresses are supported.
        """
        request = AuthorizeSecurityGroupRequest()
        request.set_IpProtocol(ip_protocol)
        request.set_PortRange(port_range)
        request.set_SecurityGroupId(security_group_id)
        request.set_SourceCidrIp(source_cidr_ip)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4441')" href="javascript:;">
ray-ray-1.9.2/python/ray/autoscaler/_private/aliyun/utils.py: 275-291
</a>
<div class="mid" id="frag4441" style="display:none"><pre>
    def create_vpc(self):
        """ Creates a virtual private cloud (VPC).

        :return: The created VPC ID.
        """
        request = CreateVpcRequest()
        response = self._send_request(request)
        if response is not None:
            return response.get("VpcId")
        return None

    def describe_vpcs(self):
        """ Queries one or more VPCs in a region.

        :return: VPC list.
        """
        request = DescribeVpcsRequest()
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 121:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4481')" href="javascript:;">
ray-ray-1.9.2/python/ray/autoscaler/_private/aws/utils.py: 132-145
</a>
<div class="mid" id="frag4481" style="display:none"><pre>
def resource_cache(name, region, max_retries=BOTO_MAX_RETRIES, **kwargs):
    cli_logger.verbose("Creating AWS resource `{}` in `{}`", cf.bold(name),
                       cf.bold(region))
    kwargs.setdefault(
        "config",
        Config(retries={"max_attempts": max_retries}),
    )
    return boto3.resource(
        name,
        region,
        **kwargs,
    )


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4482')" href="javascript:;">
ray-ray-1.9.2/python/ray/autoscaler/_private/aws/utils.py: 147-163
</a>
<div class="mid" id="frag4482" style="display:none"><pre>
def client_cache(name, region, max_retries=BOTO_MAX_RETRIES, **kwargs):
    try:
        # try to re-use a client from the resource cache first
        return resource_cache(name, region, max_retries, **kwargs).meta.client
    except ResourceNotExistsError:
        # fall back for clients without an associated resource
        cli_logger.verbose("Creating AWS client `{}` in `{}`", cf.bold(name),
                           cf.bold(region))
        kwargs.setdefault(
            "config",
            Config(retries={"max_attempts": max_retries}),
        )
        return boto3.client(
            name,
            region,
            **kwargs,
        )
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 122:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4554')" href="javascript:;">
ray-ray-1.9.2/python/ray/data/impl/simple_block.py: 180-193
</a>
<div class="mid" id="frag4554" style="display:none"><pre>
                    has_next_row = True
                next_key = key_fn(next_row)

                def gen():
                    nonlocal iter
                    nonlocal next_row
                    nonlocal has_next_row
                    assert has_next_row
                    while key_fn(next_row) == next_key:
                        yield next_row
                        try:
                            next_row = next(iter)
                        except StopIteration:
                            has_next_row = False
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4557')" href="javascript:;">
ray-ray-1.9.2/python/ray/data/impl/simple_block.py: 251-261
</a>
<div class="mid" id="frag4557" style="display:none"><pre>
                    next_row = next(iter)
                next_key = key_fn(next_row)

                def gen():
                    nonlocal iter
                    nonlocal next_row
                    while key_fn(next_row) == next_key:
                        yield next_row
                        try:
                            next_row = next(iter)
                        except StopIteration:
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 123:</b> &nbsp; 2 fragments, nominal size 25 lines, similarity 88%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4598')" href="javascript:;">
ray-ray-1.9.2/python/ray/data/tests/mock_server.py: 16-49
</a>
<div class="mid" id="frag4598" style="display:none"><pre>
def start_service(service_name, host, port):
    moto_svr_path = shutil.which("moto_server")
    if not moto_svr_path:
        pytest.skip("moto not installed")
    args = [moto_svr_path, service_name, "-H", host, "-p", str(port)]
    # For debugging
    # args = '{0} {1} -H {2} -p {3} 2&gt;&amp;1 | \
    # tee -a /tmp/moto.log'.format(moto_svr_path, service_name, host, port)
    process = sp.Popen(
        args, stdin=sp.PIPE, stdout=sp.PIPE, stderr=sp.PIPE)  # shell=True
    url = "http://{host}:{port}".format(host=host, port=port)

    for i in range(0, 30):
        output = process.poll()
        if output is not None:
            print("moto_server exited status {0}".format(output))
            stdout, stderr = process.communicate()
            print("moto_server stdout: {0}".format(stdout))
            print("moto_server stderr: {0}".format(stderr))
            pytest.fail("Can not start service: {}".format(service_name))

        try:
            # we need to bypass the proxies due to monkeypatches
            requests.get(url, timeout=5, proxies=_proxy_bypass)
            break
        except requests.exceptions.ConnectionError:
            time.sleep(0.5)
    else:
        stop_process(process)  # pytest.fail doesn't call stop_process
        pytest.fail("Can not start service: {}".format(service_name))

    return process


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4688')" href="javascript:;">
ray-ray-1.9.2/python/ray/workflow/tests/mock_server.py: 16-47
</a>
<div class="mid" id="frag4688" style="display:none"><pre>
def start_service(service_name, host, port):
    moto_svr_path = shutil.which("moto_server")
    args = [moto_svr_path, service_name, "-H", host, "-p", str(port)]
    # For debugging
    # args = '{0} {1} -H {2} -p {3} 2&gt;&amp;1 | tee -a /tmp/moto.log'.format(moto_svr_path, service_name, host, port)
    process = sp.Popen(
        args, stdin=sp.PIPE, stdout=sp.DEVNULL,
        stderr=sp.DEVNULL)  # shell=True
    url = "http://{host}:{port}".format(host=host, port=port)

    for i in range(0, 30):
        output = process.poll()
        if output is not None:
            print('moto_server exited status {0}'.format(output))
            stdout, stderr = process.communicate()
            print('moto_server stdout: {0}'.format(stdout))
            print('moto_server stderr: {0}'.format(stderr))
            pytest.fail("Can not start service: {}".format(service_name))

        try:
            # we need to bypass the proxies due to monkeypatches
            requests.get(url, timeout=5, proxies=_proxy_bypass)
            break
        except requests.exceptions.ConnectionError:
            time.sleep(0.5)
    else:
        stop_process(process)  # pytest.fail doesn't call stop_process
        pytest.fail("Can not start service: {}".format(service_name))

    return process


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 124:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4599')" href="javascript:;">
ray-ray-1.9.2/python/ray/data/tests/mock_server.py: 50-67
</a>
<div class="mid" id="frag4599" style="display:none"><pre>
def stop_process(process):
    try:
        process.send_signal(signal.SIGTERM)
        process.communicate(timeout=20)
    except sp.TimeoutExpired:
        process.kill()
        outs, errors = process.communicate(timeout=20)
        exit_code = process.returncode
        msg = "Child process finished {} not in clean way: {} {}" \
            .format(exit_code, outs, errors)
        raise RuntimeError(msg)


# TODO(Clark): We should be able to use "session" scope here, but we've found
# that the s3_fs fixture ends up hanging with S3 ops timing out (or the server
# being unreachable). This appears to only be an issue when using the tmp_dir
# fixture as the S3 dir path. We should fix this since "session" scope should
# reduce a lot of the per-test overhead (2x faster execution for IO methods in
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4689')" href="javascript:;">
ray-ray-1.9.2/python/ray/workflow/tests/mock_server.py: 48-59
</a>
<div class="mid" id="frag4689" style="display:none"><pre>
def stop_process(process):
    try:
        process.send_signal(signal.SIGTERM)
        process.communicate(timeout=20)
    except sp.TimeoutExpired:
        process.kill()
        outs, errors = process.communicate(timeout=20)
        exit_code = process.returncode
        msg = "Child process finished {} not in clean way: {} {}" \
            .format(exit_code, outs, errors)
        raise RuntimeError(msg)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 125:</b> &nbsp; 3 fragments, nominal size 17 lines, similarity 82%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4602')" href="javascript:;">
ray-ray-1.9.2/python/ray/data/tests/test_size_estimation.py: 18-35
</a>
<div class="mid" id="frag4602" style="display:none"><pre>

def test_py_size(ray_start_regular_shared):
    b = SimpleBlockBuilder()
    assert b.get_estimated_memory_usage() == 0
    b.add(SMALL_VALUE)
    assert_close(b.get_estimated_memory_usage(), 111)
    b.add(SMALL_VALUE)
    assert_close(b.get_estimated_memory_usage(), 222)
    for _ in range(8):
        b.add(SMALL_VALUE)
    assert_close(b.get_estimated_memory_usage(), 1110)
    for _ in range(90):
        b.add(SMALL_VALUE)
    assert_close(b.get_estimated_memory_usage(), 11100)
    b.add_block([SMALL_VALUE] * 900)
    assert_close(b.get_estimated_memory_usage(), 111000)
    assert len(b.build()) == 1000

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4604')" href="javascript:;">
ray-ray-1.9.2/python/ray/data/tests/test_size_estimation.py: 56-74
</a>
<div class="mid" id="frag4604" style="display:none"><pre>

def test_arrow_size(ray_start_regular_shared):
    b = ArrowBlockBuilder()
    assert b.get_estimated_memory_usage() == 0
    b.add(ARROW_SMALL_VALUE)
    assert_close(b.get_estimated_memory_usage(), 118)
    b.add(ARROW_SMALL_VALUE)
    assert_close(b.get_estimated_memory_usage(), 236)
    for _ in range(8):
        b.add(ARROW_SMALL_VALUE)
    assert_close(b.get_estimated_memory_usage(), 1180)
    for _ in range(90):
        b.add(ARROW_SMALL_VALUE)
    assert_close(b.get_estimated_memory_usage(), 11800)
    for _ in range(900):
        b.add(ARROW_SMALL_VALUE)
    assert_close(b.get_estimated_memory_usage(), 118000)
    assert b.build().num_rows == 1000

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4603')" href="javascript:;">
ray-ray-1.9.2/python/ray/data/tests/test_size_estimation.py: 36-55
</a>
<div class="mid" id="frag4603" style="display:none"><pre>

def test_py_size_diff_values(ray_start_regular_shared):
    b = SimpleBlockBuilder()
    assert b.get_estimated_memory_usage() == 0
    for _ in range(10):
        b.add(LARGE_VALUE)
    assert_close(b.get_estimated_memory_usage(), 100120)
    for _ in range(100):
        b.add(SMALL_VALUE)
    assert_close(b.get_estimated_memory_usage(), 121120)
    for _ in range(100):
        b.add(LARGE_VALUE)
    assert_close(b.get_estimated_memory_usage(), 1166875)
    for _ in range(100):
        b.add(LARGE_VALUE)
    assert_close(b.get_estimated_memory_usage(), 2182927)
    b.add_block([SMALL_VALUE] * 1000)
    assert_close(b.get_estimated_memory_usage(), 2240613)
    assert len(b.build()) == 1310

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 126:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 92%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4774')" href="javascript:;">
ray-ray-1.9.2/python/ray/workflow/tests/test_recovery.py: 88-105
</a>
<div class="mid" id="frag4774" style="display:none"><pre>
def test_dedupe_downloads_list(workflow_start_regular):
    with tempfile.TemporaryDirectory() as temp_dir:
        debug_store = DebugStorage(get_global_storage(), temp_dir)
        utils._alter_storage(debug_store)

        numbers = [ray.put(i) for i in range(5)]
        workflows = [identity.step(numbers) for _ in range(100)]

        gather.step(*workflows).run()

        ops = debug_store._logged_storage.get_op_counter()
        get_objects_count = 0
        for key in ops["get"]:
            if "objects" in key:
                get_objects_count += 1
        assert get_objects_count == 5


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4775')" href="javascript:;">
ray-ray-1.9.2/python/ray/workflow/tests/test_recovery.py: 112-129
</a>
<div class="mid" id="frag4775" style="display:none"><pre>
def test_dedupe_download_raw_ref(workflow_start_regular):
    with tempfile.TemporaryDirectory() as temp_dir:
        debug_store = DebugStorage(get_global_storage(), temp_dir)
        utils._alter_storage(debug_store)

        ref = ray.put("hello")
        workflows = [identity.step(ref) for _ in range(100)]

        gather.step(*workflows).run()

        ops = debug_store._logged_storage.get_op_counter()
        get_objects_count = 0
        for key in ops["get"]:
            if "objects" in key:
                get_objects_count += 1
        assert get_objects_count == 1


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 127:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 92%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4932')" href="javascript:;">
ray-ray-1.9.2/python/ray/experimental/array/distributed/core.py: 136-151
</a>
<div class="mid" id="frag4932" style="display:none"><pre>
def triu(a):
    if a.ndim != 2:
        raise Exception("Input must have 2 dimensions, but a.ndim is "
                        "{}.".format(a.ndim))
    result = DistArray(a.shape)
    for (i, j) in np.ndindex(*result.num_blocks):
        if i &lt; j:
            result.object_refs[i, j] = ra.copy.remote(a.object_refs[i, j])
        elif i == j:
            result.object_refs[i, j] = ra.triu.remote(a.object_refs[i, j])
        else:
            result.object_refs[i, j] = ra.zeros_like.remote(
                a.object_refs[i, j])
    return result


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4933')" href="javascript:;">
ray-ray-1.9.2/python/ray/experimental/array/distributed/core.py: 153-168
</a>
<div class="mid" id="frag4933" style="display:none"><pre>
def tril(a):
    if a.ndim != 2:
        raise Exception("Input must have 2 dimensions, but a.ndim is "
                        "{}.".format(a.ndim))
    result = DistArray(a.shape)
    for (i, j) in np.ndindex(*result.num_blocks):
        if i &gt; j:
            result.object_refs[i, j] = ra.copy.remote(a.object_refs[i, j])
        elif i == j:
            result.object_refs[i, j] = ra.tril.remote(a.object_refs[i, j])
        else:
            result.object_refs[i, j] = ra.zeros_like.remote(
                a.object_refs[i, j])
    return result


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 128:</b> &nbsp; 2 fragments, nominal size 29 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5025')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_component_failures_3.py: 19-76
</a>
<div class="mid" id="frag5025" style="display:none"><pre>
def test_actor_creation_node_failure(ray_start_cluster):
    # TODO(swang): Refactor test_raylet_failed, etc to reuse the below code.
    cluster = ray_start_cluster

    @ray.remote
    class Child:
        def __init__(self, death_probability):
            self.death_probability = death_probability

        def get_probability(self):
            return self.death_probability

        def ping(self):
            # Exit process with some probability.
            exit_chance = np.random.rand()
            if exit_chance &lt; self.death_probability:
                sys.exit(-1)

    num_children = 25
    # Children actors will die about half the time.
    death_probability = 0.5

    children = [Child.remote(death_probability) for _ in range(num_children)]
    while len(cluster.list_all_nodes()) &gt; 1:
        for j in range(2):
            # Submit some tasks on the actors. About half of the actors will
            # fail.
            children_out = [child.ping.remote() for child in children]
            # Wait a while for all the tasks to complete. This should trigger
            # reconstruction for any actor creation tasks that were forwarded
            # to nodes that then failed.
            ready, _ = ray.wait(
                children_out, num_returns=len(children_out), timeout=5 * 60.0)
            assert len(ready) == len(children_out)

            # Replace any actors that died.
            for i, out in enumerate(children_out):
                try:
                    ray.get(out)
                except ray.exceptions.RayActorError:
                    children[i] = Child.remote(death_probability)

            children_out = [
                child.get_probability.remote() for child in children
            ]
            # Wait for new created actors to finish creation before
            # removing a node. This is needed because right now we don't
            # support reconstructing actors that died in the process of
            # being created.
            ready, _ = ray.wait(
                children_out, num_returns=len(children_out), timeout=5 * 60.0)
            assert len(ready) == len(children_out)

        # Remove a node. Any actor creation tasks that were forwarded to this
        # node must be restarted.
        cluster.remove_node(get_other_nodes(cluster, True)[-1])


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5038')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_multinode_failures_2.py: 82-124
</a>
<div class="mid" id="frag5038" style="display:none"><pre>
def test_actor_creation_node_failure(ray_start_cluster):
    # TODO(swang): Refactor test_raylet_failed, etc to reuse the below code.
    cluster = ray_start_cluster

    @ray.remote
    class Child:
        def __init__(self, death_probability):
            self.death_probability = death_probability

        def ping(self):
            # Exit process with some probability.
            exit_chance = np.random.rand()
            if exit_chance &lt; self.death_probability:
                sys.exit(-1)

    num_children = 25
    # Children actors will die about half the time.
    death_probability = 0.5

    children = [Child.remote(death_probability) for _ in range(num_children)]
    while len(cluster.list_all_nodes()) &gt; 1:
        for j in range(2):
            # Submit some tasks on the actors. About half of the actors will
            # fail.
            children_out = [child.ping.remote() for child in children]
            # Wait a while for all the tasks to complete. This should trigger
            # reconstruction for any actor creation tasks that were forwarded
            # to nodes that then failed.
            ready, _ = ray.wait(
                children_out, num_returns=len(children_out), timeout=5 * 60.0)
            assert len(ready) == len(children_out)

            # Replace any actors that died.
            for i, out in enumerate(children_out):
                try:
                    ray.get(out)
                except ray.exceptions.RayActorError:
                    children[i] = Child.remote(death_probability)
        # Remove a node. Any actor creation tasks that were forwarded to this
        # node must be resubmitted.
        cluster.remove_node(get_other_nodes(cluster, True)[-1])


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 129:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5030')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_component_failures_3.py: 86-108
</a>
<div class="mid" id="frag5030" style="display:none"><pre>
def test_driver_lives_parallel(ray_start_regular):
    all_processes = ray.worker._global_node.all_processes

    process_infos = (all_processes[ray_constants.PROCESS_TYPE_GCS_SERVER] +
                     all_processes[ray_constants.PROCESS_TYPE_RAYLET] +
                     all_processes[ray_constants.PROCESS_TYPE_LOG_MONITOR] +
                     all_processes[ray_constants.PROCESS_TYPE_MONITOR])
    assert len(process_infos) == 4

    # Kill all the components in parallel.
    for process_info in process_infos:
        process_info.process.terminate()

    time.sleep(0.1)
    for process_info in process_infos:
        process_info.process.kill()

    for process_info in process_infos:
        process_info.process.wait()

    # If the driver can reach the tearDown method, then it is still alive.


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5042')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_multinode_failures_2.py: 136-158
</a>
<div class="mid" id="frag5042" style="display:none"><pre>
def test_driver_lives_parallel(ray_start_regular):
    all_processes = ray.worker._global_node.all_processes

    process_infos = (all_processes[ray_constants.PROCESS_TYPE_GCS_SERVER] +
                     all_processes[ray_constants.PROCESS_TYPE_RAYLET] +
                     all_processes[ray_constants.PROCESS_TYPE_LOG_MONITOR] +
                     all_processes[ray_constants.PROCESS_TYPE_MONITOR])
    assert len(process_infos) == 4

    # Kill all the components in parallel.
    for process_info in process_infos:
        process_info.process.terminate()

    time.sleep(0.1)
    for process_info in process_infos:
        process_info.process.kill()

    for process_info in process_infos:
        process_info.process.wait()

    # If the driver can reach the tearDown method, then it is still alive.


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 130:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5104')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_autoscaling_policy.py: 32-47
</a>
<div class="mid" id="frag5104" style="display:none"><pre>
    def __init__(
            self,
            duration: float,
            resources: Dict[str, float],
            start_callback: Callable[[None], None] = None,
            done_callback: Callable[[None], None] = None,
    ):
        self.duration = duration
        self.resources = resources
        self.start_callback = start_callback
        self.done_callback = done_callback
        self.start_time = None
        self.end_time = None
        self.node = None


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5105')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_autoscaling_policy.py: 53-70
</a>
<div class="mid" id="frag5105" style="display:none"><pre>
    def __init__(
            self,
            duration: float,
            bundles: List[Dict[str, float]],
            strategy: int,
            start_callback: Callable[[None], None] = None,
            done_callback: Callable[[None], None] = None,
    ):
        self.duration = duration
        self.bundles = bundles
        self.strategy = strategy
        self.start_callback = start_callback
        self.done_callback = done_callback
        self.start_time = None
        self.end_time = None
        self.node = None


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 131:</b> &nbsp; 2 fragments, nominal size 23 lines, similarity 86%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5135')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_autoscaling_policy.py: 470-503
</a>
<div class="mid" id="frag5135" style="display:none"><pre>
    def testManyTasks(self):
        config = copy.deepcopy(SAMPLE_CLUSTER_CONFIG)
        config_path = self.write_config(config)
        self.provider = MockProvider()
        simulator = Simulator(config_path, self.provider)

        done_count = 0

        def done_callback():
            nonlocal done_count
            done_count += 1

        tasks = [
            Task(
                duration=200,
                resources={"CPU": 1},
                done_callback=done_callback) for _ in range(5000)
        ]
        simulator.submit(tasks)

        time = 0
        while done_count &lt; len(tasks):
            time = simulator.step()

        assert time &lt; 850
        # TODO (Alex): Not clear what's actually worth asserting here.
        assert simulator.node_costs()

        # Check event logs contain add/remove node events.
        assert any("Adding" in x
                   for x in simulator.autoscaler.event_summarizer.summary())
        assert any("Removing" in x
                   for x in simulator.autoscaler.event_summarizer.summary())

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5137')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_autoscaling_policy.py: 504-536
</a>
<div class="mid" id="frag5137" style="display:none"><pre>
    def testManyActors(self):
        config = copy.deepcopy(SAMPLE_CLUSTER_CONFIG)
        config_path = self.write_config(config)
        self.provider = MockProvider()
        simulator = Simulator(config_path, self.provider)

        start_count = 0

        def start_callback():
            nonlocal start_count
            start_count += 1

        tasks = [
            Actor(
                duration=float("inf"),
                resources={"CPU": 1},
                start_callback=start_callback,
            ) for _ in range(5000)
        ]
        simulator.submit(tasks)

        time = 0
        while start_count &lt; len(tasks):
            time = simulator.step()

        assert time &lt; 650

        # Check event logs contain add/remove node events.
        assert any("Adding" in x
                   for x in simulator.autoscaler.event_summarizer.summary())
        assert any("Removing" in x
                   for x in simulator.autoscaler.event_summarizer.summary())

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 132:</b> &nbsp; 2 fragments, nominal size 26 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5152')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_object_spilling_2.py: 16-45
</a>
<div class="mid" id="frag5152" style="display:none"><pre>
def test_delete_objects(object_spilling_config, shutdown_only):
    # Limit our object store to 75 MiB of memory.
    object_spilling_config, temp_folder = object_spilling_config

    address = ray.init(
        object_store_memory=75 * 1024 * 1024,
        _system_config={
            "max_io_workers": 1,
            "min_spilling_size": 0,
            "automatic_object_spilling_enabled": True,
            "object_store_full_delay_ms": 100,
            "object_spilling_config": object_spilling_config,
        })
    arr = np.random.rand(1024 * 1024)  # 8 MB data
    replay_buffer = []

    for _ in range(80):
        ref = None
        while ref is None:
            ref = ray.put(arr)
            replay_buffer.append(ref)

    print("-----------------------------------")

    del replay_buffer
    del ref
    wait_for_condition(lambda: is_dir_empty(temp_folder))
    assert_no_thrashing(address["redis_address"])


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5153')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_object_spilling_2.py: 48-86
</a>
<div class="mid" id="frag5153" style="display:none"><pre>
def test_delete_objects_delete_while_creating(object_spilling_config,
                                              shutdown_only):
    # Limit our object store to 75 MiB of memory.
    object_spilling_config, temp_folder = object_spilling_config

    address = ray.init(
        object_store_memory=75 * 1024 * 1024,
        _system_config={
            "max_io_workers": 4,
            "min_spilling_size": 0,
            "automatic_object_spilling_enabled": True,
            "object_store_full_delay_ms": 100,
            "object_spilling_config": object_spilling_config,
        })
    arr = np.random.rand(1024 * 1024)  # 8 MB data
    replay_buffer = []

    for _ in range(80):
        ref = None
        while ref is None:
            ref = ray.put(arr)
            replay_buffer.append(ref)
        # Remove the replay buffer with 60% probability.
        if random.randint(0, 9) &lt; 6:
            replay_buffer.pop()

    # Do random sampling.
    for _ in range(200):
        ref = random.choice(replay_buffer)
        sample = ray.get(ref, timeout=0)
        assert np.array_equal(sample, arr)

    # After all, make sure all objects are killed without race condition.
    del replay_buffer
    del ref
    wait_for_condition(lambda: is_dir_empty(temp_folder))
    assert_no_thrashing(address["redis_address"])


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 133:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5157')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_object_spilling_2.py: 114-129
</a>
<div class="mid" id="frag5157" style="display:none"><pre>
        def create_objects(self):
            for _ in range(80):
                ref = None
                while ref is None:
                    ref = ray.put(arr)
                    self.replay_buffer.append(ref)
                # Remove the replay buffer with 60% probability.
                if random.randint(0, 9) &lt; 6:
                    self.replay_buffer.pop()

            # Do random sampling.
            for _ in range(200):
                ref = random.choice(self.replay_buffer)
                sample = ray.get(ref, timeout=0)
                assert np.array_equal(sample, arr)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5162')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_object_spilling_2.py: 184-199
</a>
<div class="mid" id="frag5162" style="display:none"><pre>
        def create_objects(self):
            for _ in range(80):
                ref = None
                while ref is None:
                    ref = ray.put(arr)
                    self.replay_buffer.append(ref)
                # Remove the replay buffer with 60% probability.
                if random.randint(0, 9) &lt; 6:
                    self.replay_buffer.pop()

            # Do random sampling.
            for _ in range(50):
                ref = random.choice(self.replay_buffer)
                sample = ray.get(ref, timeout=10)
                assert np.array_equal(sample, arr)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 134:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5202')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_reference_counting.py: 36-52
</a>
<div class="mid" id="frag5202" style="display:none"><pre>
def _fill_object_store_and_get(obj, succeed=True, object_MiB=20,
                               num_objects=5):
    for _ in range(num_objects):
        ray.put(np.zeros(object_MiB * 1024 * 1024, dtype=np.uint8))

    if type(obj) is bytes:
        obj = ray.ObjectRef(obj)

    if succeed:
        wait_for_condition(
            lambda: ray.worker.global_worker.core_worker.object_exists(obj))
    else:
        wait_for_condition(
            lambda: not ray.worker.global_worker.core_worker.object_exists(obj)
        )


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5398')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_reference_counting_2.py: 38-56
</a>
<div class="mid" id="frag5398" style="display:none"><pre>
def _fill_object_store_and_get(obj, succeed=True, object_MiB=20,
                               num_objects=5):
    for _ in range(num_objects):
        ray.put(np.zeros(object_MiB * 1024 * 1024, dtype=np.uint8))

    if type(obj) is bytes:
        obj = ray.ObjectRef(obj)

    if succeed:
        wait_for_condition(
            lambda: ray.worker.global_worker.core_worker.object_exists(obj))
    else:
        wait_for_condition(
            lambda: not ray.worker.global_worker.core_worker.object_exists(obj)
        )


# Test that an object containing object refs within it pins the inner IDs
# recursively and for submitted tasks.
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 135:</b> &nbsp; 2 fragments, nominal size 31 lines, similarity 93%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5270')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_client_terminate.py: 51-92
</a>
<div class="mid" id="frag5270" style="display:none"><pre>
def test_cancel_chain(ray_start_regular, use_force):
    with ray_start_client_server() as ray:
        SignalActor = create_remote_signal_actor(ray)
        signaler = SignalActor.remote()

        @ray.remote
        def wait_for(t):
            return ray.get(t[0])

        obj1 = wait_for.remote([signaler.wait.remote()])
        obj2 = wait_for.remote([obj1])
        obj3 = wait_for.remote([obj2])
        obj4 = wait_for.remote([obj3])

        assert len(ray.wait([obj1], timeout=.1)[0]) == 0
        ray.cancel(obj1, force=use_force)
        for ob in [obj1, obj2, obj3, obj4]:
            with pytest.raises(valid_exceptions(use_force)):
                ray.get(ob)

        signaler2 = SignalActor.remote()
        obj1 = wait_for.remote([signaler2.wait.remote()])
        obj2 = wait_for.remote([obj1])
        obj3 = wait_for.remote([obj2])
        obj4 = wait_for.remote([obj3])

        assert len(ray.wait([obj3], timeout=.1)[0]) == 0
        ray.cancel(obj3, force=use_force)
        for ob in [obj3, obj4]:
            with pytest.raises(valid_exceptions(use_force)):
                ray.get(ob)

        with pytest.raises(GetTimeoutError):
            ray.get(obj1, timeout=.1)

        with pytest.raises(GetTimeoutError):
            ray.get(obj2, timeout=.1)

        signaler2.send.remote()
        ray.get(obj1)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5358')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_cancel.py: 21-60
</a>
<div class="mid" id="frag5358" style="display:none"><pre>

@pytest.mark.parametrize("use_force", [True, False])
def test_cancel_chain(ray_start_regular, use_force):
    signaler = SignalActor.remote()

    @ray.remote
    def wait_for(t):
        return ray.get(t[0])

    obj1 = wait_for.remote([signaler.wait.remote()])
    obj2 = wait_for.remote([obj1])
    obj3 = wait_for.remote([obj2])
    obj4 = wait_for.remote([obj3])

    assert len(ray.wait([obj1], timeout=.1)[0]) == 0
    ray.cancel(obj1, force=use_force)
    for ob in [obj1, obj2, obj3, obj4]:
        with pytest.raises(valid_exceptions(use_force)):
            ray.get(ob)

    signaler2 = SignalActor.remote()
    obj1 = wait_for.remote([signaler2.wait.remote()])
    obj2 = wait_for.remote([obj1])
    obj3 = wait_for.remote([obj2])
    obj4 = wait_for.remote([obj3])

    assert len(ray.wait([obj3], timeout=.1)[0]) == 0
    ray.cancel(obj3, force=use_force)
    for ob in [obj3, obj4]:
        with pytest.raises(valid_exceptions(use_force)):
            ray.get(ob)

    with pytest.raises(GetTimeoutError):
        ray.get(obj1, timeout=.1)

    with pytest.raises(GetTimeoutError):
        ray.get(obj2, timeout=.1)

    signaler2.send.remote()
    ray.get(obj1)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 136:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5307')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_actor_group.py: 36-49
</a>
<div class="mid" id="frag5307" style="display:none"><pre>
def test_actor_shutdown(ray_start_2_cpus):
    assert ray.available_resources()["CPU"] == 2
    ag = ActorGroup(actor_cls=DummyActor, num_actors=2)
    time.sleep(1)
    assert "CPU" not in ray.available_resources()
    assert len(ray.state.actors()) == 2
    ag.shutdown()
    time.sleep(1)
    assert ray.available_resources()["CPU"] == 2

    with pytest.raises(RuntimeError):
        ag.return_arg.remote(1)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6681')" href="javascript:;">
ray-ray-1.9.2/python/ray/train/tests/test_worker_group.py: 36-49
</a>
<div class="mid" id="frag6681" style="display:none"><pre>
def test_worker_shutdown(ray_start_2_cpus):
    assert ray.available_resources()["CPU"] == 2
    wg = WorkerGroup(num_workers=2)
    time.sleep(1)
    assert "CPU" not in ray.available_resources()
    assert len(ray.state.actors()) == 2
    wg.shutdown()
    time.sleep(1)
    assert ray.available_resources()["CPU"] == 2

    with pytest.raises(RuntimeError):
        wg.execute(lambda: 1)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 137:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5542')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_failure.py: 141-167
</a>
<div class="mid" id="frag5542" style="display:none"><pre>
def test_failed_actor_init(ray_start_regular, error_pubsub):
    p = error_pubsub
    error_message1 = "actor constructor failed"
    error_message2 = "actor method failed"

    @ray.remote
    class FailedActor:
        def __init__(self):
            raise Exception(error_message1)

        def fail_method(self):
            raise Exception(error_message2)

    a = FailedActor.remote()

    # Make sure that we get errors from a failed constructor.
    errors = get_error_message(p, 1, ray_constants.TASK_PUSH_ERROR)
    assert len(errors) == 1
    assert errors[0].type == ray_constants.TASK_PUSH_ERROR
    assert error_message1 in errors[0].error_message

    # Incoming methods will get the exception in creation task
    with pytest.raises(ray.exceptions.RayActorError) as e:
        ray.get(a.fail_method.remote())
    assert error_message1 in str(e.value)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5545')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_failure.py: 168-189
</a>
<div class="mid" id="frag5545" style="display:none"><pre>
def test_failed_actor_method(ray_start_regular, error_pubsub):
    p = error_pubsub
    error_message2 = "actor method failed"

    @ray.remote
    class FailedActor:
        def __init__(self):
            pass

        def fail_method(self):
            raise Exception(error_message2)

    a = FailedActor.remote()

    # Make sure that we get errors from a failed method.
    a.fail_method.remote()
    errors = get_error_message(p, 1, ray_constants.TASK_PUSH_ERROR)
    assert len(errors) == 1
    assert errors[0].type == ray_constants.TASK_PUSH_ERROR
    assert error_message2 in errors[0].error_message


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 138:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5566')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_failure.py: 351-366
</a>
<div class="mid" id="frag5566" style="display:none"><pre>
def test_exception_chain(ray_start_regular):
    @ray.remote
    def bar():
        return 1 / 0

    @ray.remote
    def foo():
        return ray.get(bar.remote())

    r = foo.remote()
    try:
        ray.get(r)
    except ZeroDivisionError as ex:
        assert isinstance(ex, RayTaskError)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6299')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_traceback.py: 145-171
</a>
<div class="mid" id="frag6299" style="display:none"><pre>
def test_exception_chain(ray_start_regular):
    """Test the chained stacktrace."""
    expected_output = """ray::foo() (pid=XXX, ip=YYY) # noqa
  File "FILE", line ZZ, in foo
    return ray.get(bar.remote())
ray.exceptions.RayTaskError(ZeroDivisionError): ray::bar() (pid=XXX, ip=YYY)
  File "FILE", line ZZ, in bar
    return 1 / 0
ZeroDivisionError: division by zero"""

    @ray.remote
    def bar():
        return 1 / 0

    @ray.remote
    def foo():
        return ray.get(bar.remote())

    r = foo.remote()
    try:
        ray.get(r)
    except ZeroDivisionError as ex:
        assert isinstance(ex, RayTaskError)
        print(ex)
        assert clean_noqa(expected_output) == scrub_traceback(str(ex))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 139:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5592')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_component_failures_2.py: 21-35
</a>
<div class="mid" id="frag5592" style="display:none"><pre>
def ray_start_workers_separate_multinode(request):
    num_nodes = request.param[0]
    num_initial_workers = request.param[1]
    # Start the Ray processes.
    cluster = Cluster()
    for _ in range(num_nodes):
        cluster.add_node(num_cpus=num_initial_workers)
    ray.init(address=cluster.address)

    yield num_nodes, num_initial_workers
    # The code after the yield will run as teardown code.
    ray.shutdown()
    cluster.shutdown()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5911')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_multinode_failures.py: 17-31
</a>
<div class="mid" id="frag5911" style="display:none"><pre>
def ray_start_workers_separate_multinode(request):
    num_nodes = request.param[0]
    num_initial_workers = request.param[1]
    # Start the Ray processes.
    cluster = Cluster()
    for _ in range(num_nodes):
        cluster.add_node(num_cpus=num_initial_workers)
    ray.init(address=cluster.address)

    yield num_nodes, num_initial_workers
    # The code after the yield will run as teardown code.
    ray.shutdown()
    cluster.shutdown()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 140:</b> &nbsp; 2 fragments, nominal size 37 lines, similarity 94%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5593')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_component_failures_2.py: 36-88
</a>
<div class="mid" id="frag5593" style="display:none"><pre>
def test_worker_failed(ray_start_workers_separate_multinode):
    num_nodes, num_initial_workers = (ray_start_workers_separate_multinode)

    if num_nodes == 4 and sys.platform == "win32":
        pytest.skip("Failing on Windows.")

    @ray.remote
    def get_pids():
        time.sleep(0.25)
        return os.getpid()

    start_time = time.time()
    pids = set()
    while len(pids) &lt; num_nodes * num_initial_workers:
        new_pids = ray.get([
            get_pids.remote()
            for _ in range(2 * num_nodes * num_initial_workers)
        ])
        for pid in new_pids:
            pids.add(pid)
        if time.time() - start_time &gt; 60:
            raise RayTestTimeoutException(
                "Timed out while waiting to get worker PIDs.")

    @ray.remote
    def f(x):
        time.sleep(0.5)
        return x

    # Submit more tasks than there are workers so that all workers and
    # cores are utilized.
    object_refs = [f.remote(i) for i in range(num_initial_workers * num_nodes)]
    object_refs += [f.remote(object_ref) for object_ref in object_refs]
    # Allow the tasks some time to begin executing.
    time.sleep(0.1)
    # Kill the workers as the tasks execute.
    for pid in pids:
        try:
            os.kill(pid, SIGKILL)
        except OSError:
            # The process may have already exited due to worker capping.
            pass
        time.sleep(0.1)
    # Make sure that we either get the object or we get an appropriate
    # exception.
    for object_ref in object_refs:
        try:
            ray.get(object_ref)
        except (ray.exceptions.RayTaskError,
                ray.exceptions.WorkerCrashedError):
            pass


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5912')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_multinode_failures.py: 33-82
</a>
<div class="mid" id="frag5912" style="display:none"><pre>
def test_worker_failed(ray_start_workers_separate_multinode):
    num_nodes, num_initial_workers = (ray_start_workers_separate_multinode)

    @ray.remote
    def get_pids():
        time.sleep(0.25)
        return os.getpid()

    start_time = time.time()
    pids = set()
    while len(pids) &lt; num_nodes * num_initial_workers:
        new_pids = ray.get([
            get_pids.remote()
            for _ in range(2 * num_nodes * num_initial_workers)
        ])
        for pid in new_pids:
            pids.add(pid)
        if time.time() - start_time &gt; 60:
            raise RayTestTimeoutException(
                "Timed out while waiting to get worker PIDs.")

    @ray.remote
    def f(x):
        time.sleep(0.5)
        return x

    # Submit more tasks than there are workers so that all workers and
    # cores are utilized.
    object_refs = [f.remote(i) for i in range(num_initial_workers * num_nodes)]
    object_refs += [f.remote(object_ref) for object_ref in object_refs]
    # Allow the tasks some time to begin executing.
    time.sleep(0.1)
    # Kill the workers as the tasks execute.
    for pid in pids:
        try:
            os.kill(pid, SIGKILL)
        except OSError:
            # The process may have already exited due to worker capping.
            pass
        time.sleep(0.1)
    # Make sure that we either get the object or we get an appropriate
    # exception.
    for object_ref in object_refs:
        try:
            ray.get(object_ref)
        except (ray.exceptions.RayTaskError,
                ray.exceptions.WorkerCrashedError):
            pass


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 141:</b> &nbsp; 2 fragments, nominal size 27 lines, similarity 92%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5596')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_component_failures_2.py: 89-130
</a>
<div class="mid" id="frag5596" style="display:none"><pre>
def _test_component_failed(cluster, component_type):
    """Kill a component on all worker nodes and check workload succeeds."""
    # Submit many tasks with many dependencies.
    @ray.remote
    def f(x):
        return x

    @ray.remote
    def g(*xs):
        return 1

    # Kill the component on all nodes except the head node as the tasks
    # execute. Do this in a loop while submitting tasks between each
    # component failure.
    time.sleep(0.1)
    worker_nodes = get_other_nodes(cluster)
    assert len(worker_nodes) &gt; 0
    for node in worker_nodes:
        process = node.all_processes[component_type][0].process
        # Submit a round of tasks with many dependencies.
        x = 1
        for _ in range(1000):
            x = f.remote(x)

        xs = [g.remote(1)]
        for _ in range(100):
            xs.append(g.remote(*xs))
            xs.append(g.remote(1))

        # Kill a component on one of the nodes.
        process.terminate()
        time.sleep(1)
        process.kill()
        process.wait()
        assert not process.poll() is None

        # Make sure that we can still get the objects after the
        # executing tasks died.
        ray.get(x)
        ray.get(xs)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5915')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_multinode_failures.py: 83-130
</a>
<div class="mid" id="frag5915" style="display:none"><pre>
def _test_component_failed(cluster, component_type):
    """Kill a component on all worker nodes and check workload succeeds."""
    # Submit many tasks with many dependencies.
    @ray.remote
    def f(x):
        # Sleep to make sure that tasks actually fail mid-execution.
        time.sleep(0.01)
        return x

    @ray.remote
    def g(*xs):
        # Sleep to make sure that tasks actually fail mid-execution. We
        # only use it for direct calls because the test already takes a
        # long time to run with the raylet codepath.
        time.sleep(0.01)
        return 1

    # Kill the component on all nodes except the head node as the tasks
    # execute. Do this in a loop while submitting tasks between each
    # component failure.
    time.sleep(0.1)
    worker_nodes = get_other_nodes(cluster)
    assert len(worker_nodes) &gt; 0
    for node in worker_nodes:
        process = node.all_processes[component_type][0].process
        # Submit a round of tasks with many dependencies.
        x = 1
        for _ in range(1000):
            x = f.remote(x)

        xs = [g.remote(1)]
        for _ in range(100):
            xs.append(g.remote(*xs))
            xs.append(g.remote(1))

        # Kill a component on one of the nodes.
        process.terminate()
        time.sleep(1)
        process.kill()
        process.wait()
        assert not process.poll() is None

        # Make sure that we can still get the objects after the
        # executing tasks died.
        ray.get(x)
        ray.get(xs)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 142:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5599')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_component_failures_2.py: 131-147
</a>
<div class="mid" id="frag5599" style="display:none"><pre>
def check_components_alive(cluster, component_type, check_component_alive):
    """Check that a given component type is alive on all worker nodes."""
    worker_nodes = get_other_nodes(cluster)
    assert len(worker_nodes) &gt; 0
    for node in worker_nodes:
        process = node.all_processes[component_type][0].process
        if check_component_alive:
            assert process.poll() is None
        else:
            print("waiting for " + component_type + " with PID " +
                  str(process.pid) + "to terminate")
            process.wait()
            print("done waiting for " + component_type + " with PID " +
                  str(process.pid) + "to terminate")
            assert not process.poll() is None


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5918')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_multinode_failures.py: 131-147
</a>
<div class="mid" id="frag5918" style="display:none"><pre>
def check_components_alive(cluster, component_type, check_component_alive):
    """Check that a given component type is alive on all worker nodes."""
    worker_nodes = get_other_nodes(cluster)
    assert len(worker_nodes) &gt; 0
    for node in worker_nodes:
        process = node.all_processes[component_type][0].process
        if check_component_alive:
            assert process.poll() is None
        else:
            print("waiting for " + component_type + " with PID " +
                  str(process.pid) + "to terminate")
            process.wait()
            print("done waiting for " + component_type + " with PID " +
                  str(process.pid) + "to terminate")
            assert not process.poll() is None


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 143:</b> &nbsp; 2 fragments, nominal size 20 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5607')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_actor_in_container.py: 14-41
</a>
<div class="mid" id="frag5607" style="display:none"><pre>
def test_actor_in_container():
    job_config = ray.job_config.JobConfig(
        runtime_env={
            "container": {
                "image": "rayproject/ray-worker-container:nightly-py36-cpu",
            }
        })
    ray.init(job_config=job_config)

    @ray.remote
    class Counter(object):
        def __init__(self):
            self.value = 0

        def increment(self):
            self.value += 1
            return self.value

        def get_counter(self):
            return self.value

    a1 = Counter.options().remote()
    a1.increment.remote()
    result = ray.get(a1.get_counter.remote())
    assert result == 1
    ray.shutdown()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5611')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_actor_in_container.py: 43-68
</a>
<div class="mid" id="frag5611" style="display:none"><pre>
def test_actor_in_heterogeneous_image():
    job_config = ray.job_config.JobConfig(
        runtime_env={
            "container": {
                "image": "rayproject/ray-worker-container:"
                "nightly-py36-cpu-pandas",
            }
        })
    ray.init(job_config=job_config)

    @ray.remote
    class HeterogeneousActor(object):
        def __init__(self):
            pass

        def run_pandas(self):
            import numpy as np
            import pandas as pd
            return len(pd.Series([1, 3, 5, np.nan, 6]))

    h1 = HeterogeneousActor.options().remote()
    pandas_result = ray.get(h1.run_pandas.remote())
    assert pandas_result == 5
    ray.shutdown()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 144:</b> &nbsp; 3 fragments, nominal size 21 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5649')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_actor_advanced.py: 314-348
</a>
<div class="mid" id="frag5649" style="display:none"><pre>
def test_distributed_handle(ray_start_cluster_2_nodes):
    cluster = ray_start_cluster_2_nodes
    counter, ids = setup_counter_actor(test_checkpoint=False)

    @ray.remote
    def fork_many_incs(counter, num_incs):
        x = None
        for _ in range(num_incs):
            x = counter.inc.remote()
        # Only call ray.get() on the last task submitted.
        return ray.get(x)

    # Fork num_iters times.
    count = ray.get(ids[-1])
    num_incs = 100
    num_iters = 10
    forks = [
        fork_many_incs.remote(counter, num_incs) for _ in range(num_iters)
    ]
    ray.wait(forks, num_returns=len(forks))
    count += num_incs * num_iters

    # Kill the second plasma store to get rid of the cached objects and
    # trigger the corresponding raylet to exit.
    # TODO: kill raylet instead once this test is not skipped.
    get_non_head_nodes(cluster)[0].kill_plasma_store(wait=True)

    # Check that the actor did not restore from a checkpoint.
    assert not ray.get(counter.test_restore.remote())
    # Check that we can submit another call on the actor and get the
    # correct counter result.
    x = ray.get(counter.inc.remote())
    assert x == count + 1


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5653')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_actor_advanced.py: 392-426
</a>
<div class="mid" id="frag5653" style="display:none"><pre>
def test_checkpoint_distributed_handle(ray_start_cluster_2_nodes):
    cluster = ray_start_cluster_2_nodes
    counter, ids = setup_counter_actor(test_checkpoint=True)

    @ray.remote
    def fork_many_incs(counter, num_incs):
        x = None
        for _ in range(num_incs):
            x = counter.inc.remote()
        # Only call ray.get() on the last task submitted.
        return ray.get(x)

    # Fork num_iters times.
    count = ray.get(ids[-1])
    num_incs = 100
    num_iters = 10
    forks = [
        fork_many_incs.remote(counter, num_incs) for _ in range(num_iters)
    ]
    ray.wait(forks, num_returns=len(forks))
    count += num_incs * num_iters

    # Kill the second plasma store to get rid of the cached objects and
    # trigger the corresponding raylet to exit.
    # TODO: kill raylet instead once this test is not skipped.
    get_non_head_nodes(cluster)[0].kill_plasma_store(wait=True)

    # Check that the actor restored from a checkpoint.
    assert ray.get(counter.test_restore.remote())
    # Check that we can submit another call on the actor and get the
    # correct counter result.
    x = ray.get(counter.inc.remote())
    assert x == count + 1


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5651')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_actor_advanced.py: 350-390
</a>
<div class="mid" id="frag5651" style="display:none"><pre>
def test_remote_checkpoint_distributed_handle(ray_start_cluster_2_nodes):
    cluster = ray_start_cluster_2_nodes
    counter, ids = setup_counter_actor(test_checkpoint=True)

    @ray.remote
    def fork_many_incs(counter, num_incs):
        x = None
        for _ in range(num_incs):
            x = counter.inc.remote()
        # Only call ray.get() on the last task submitted.
        return ray.get(x)

    # Fork num_iters times.
    count = ray.get(ids[-1])
    num_incs = 100
    num_iters = 10
    forks = [
        fork_many_incs.remote(counter, num_incs) for _ in range(num_iters)
    ]
    ray.wait(forks, num_returns=len(forks))
    ray.wait([counter.__ray_checkpoint__.remote()])
    count += num_incs * num_iters

    # Kill the second plasma store to get rid of the cached objects and
    # trigger the corresponding raylet to exit.
    # TODO: kill raylet instead once this test is not skipped.
    get_non_head_nodes(cluster)[0].kill_plasma_store(wait=True)

    # Check that the actor restored from a checkpoint.
    assert ray.get(counter.test_restore.remote())
    # Check that the number of inc calls since actor initialization is
    # exactly zero, since there could not have been another inc call since
    # the remote checkpoint.
    num_inc_calls = ray.get(counter.get_num_inc_calls.remote())
    assert num_inc_calls == 0
    # Check that we can submit another call on the actor and get the
    # correct counter result.
    x = ray.get(counter.inc.remote())
    assert x == count + 1


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 145:</b> &nbsp; 2 fragments, nominal size 22 lines, similarity 91%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5661')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_actor_advanced.py: 470-500
</a>
<div class="mid" id="frag5661" style="display:none"><pre>
def test_fork_consistency(setup_queue_actor):
    queue = setup_queue_actor

    @ray.remote
    def fork(queue, key, num_items):
        x = None
        for item in range(num_items):
            x = queue.enqueue.remote(key, item)
        return ray.get(x)

    # Fork num_iters times.
    num_forks = 5
    num_items_per_fork = 100

    # Submit some tasks on new actor handles.
    forks = [
        fork.remote(queue, i, num_items_per_fork) for i in range(num_forks)
    ]
    # Submit some more tasks on the original actor handle.
    for item in range(num_items_per_fork):
        local_fork = queue.enqueue.remote(num_forks, item)
    forks.append(local_fork)
    # Wait for tasks from all handles to complete.
    ray.get(forks)
    # Check that all tasks from all handles have completed.
    items = ray.get(queue.read.remote())
    for i in range(num_forks + 1):
        filtered_items = [item[1] for item in items if item[0] == i]
        assert filtered_items == list(range(num_items_per_fork))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5663')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_actor_advanced.py: 501-533
</a>
<div class="mid" id="frag5663" style="display:none"><pre>
def test_pickled_handle_consistency(setup_queue_actor):
    queue = setup_queue_actor

    @ray.remote
    def fork(pickled_queue, key, num_items):
        queue = ray.worker.pickle.loads(pickled_queue)
        x = None
        for item in range(num_items):
            x = queue.enqueue.remote(key, item)
        return ray.get(x)

    # Fork num_iters times.
    num_forks = 10
    num_items_per_fork = 100

    # Submit some tasks on the pickled actor handle.
    new_queue = ray.worker.pickle.dumps(queue)
    forks = [
        fork.remote(new_queue, i, num_items_per_fork) for i in range(num_forks)
    ]
    # Submit some more tasks on the original actor handle.
    for item in range(num_items_per_fork):
        local_fork = queue.enqueue.remote(num_forks, item)
    forks.append(local_fork)
    # Wait for tasks from all handles to complete.
    ray.get(forks)
    # Check that all tasks from all handles have completed.
    items = ray.get(queue.read.remote())
    for i in range(num_forks + 1):
        filtered_items = [item[1] for item in items if item[0] == i]
        assert filtered_items == list(range(num_items_per_fork))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 146:</b> &nbsp; 3 fragments, nominal size 15 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5721')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_actor_advanced.py: 1214-1233
</a>
<div class="mid" id="frag5721" style="display:none"><pre>
            sys.exit(1)

    def graceful_exit():
        actor = Foo.remote()
        actor_id = ray.get(actor.get_id.remote())

        state_after_starting = ray.state.actors()[actor_id]
        time.sleep(1)
        del actor
        time.sleep(1)
        state_after_ending = ray.state.actors()[actor_id]

        assert state_after_starting["StartTime"] == state_after_ending[
            "StartTime"]

        start_time = state_after_ending["StartTime"]
        end_time = state_after_ending["EndTime"]
        lapsed = end_time - start_time

        assert end_time &gt; start_time &gt; 0, \
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5723')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_actor_advanced.py: 1254-1275
</a>
<div class="mid" id="frag5723" style="display:none"><pre>
        assert end_time &gt; start_time &gt; 0, \
            f"Start: {start_time}, End: {end_time}"
        assert 500 &lt; lapsed &lt; 1500, f"Start: {start_time}, End: {end_time}"

    def restarted():
        actor = Foo.options(max_restarts=1).remote()
        actor_id = ray.get(actor.get_id.remote())

        state_after_starting = ray.state.actors()[actor_id]
        time.sleep(1)
        actor.kill_self.remote()
        time.sleep(1)
        actor.kill_self.remote()
        time.sleep(1)
        state_after_ending = ray.state.actors()[actor_id]

        assert state_after_starting["StartTime"] == state_after_ending[
            "StartTime"]

        start_time = state_after_ending["StartTime"]
        end_time = state_after_ending["EndTime"]
        lapsed = end_time - start_time
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5722')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_actor_advanced.py: 1234-1253
</a>
<div class="mid" id="frag5722" style="display:none"><pre>
            f"Start: {start_time}, End: {end_time}"
        assert 500 &lt; lapsed &lt; 1500, f"Start: {start_time}, End: {end_time}"

    def not_graceful_exit():
        actor = Foo.remote()
        actor_id = ray.get(actor.get_id.remote())

        state_after_starting = ray.state.actors()[actor_id]
        time.sleep(1)
        actor.kill_self.remote()
        time.sleep(1)
        state_after_ending = ray.state.actors()[actor_id]

        assert state_after_starting["StartTime"] == state_after_ending[
            "StartTime"]

        start_time = state_after_ending["StartTime"]
        end_time = state_after_ending["EndTime"]
        lapsed = end_time - start_time

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 147:</b> &nbsp; 2 fragments, nominal size 19 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5781')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_component_failures.py: 66-104
</a>
<div class="mid" id="frag5781" style="display:none"><pre>
def test_dying_driver_get(ray_start_regular):
    # Start the Ray processes.
    address_info = ray_start_regular

    @ray.remote
    def sleep_forever():
        time.sleep(10**6)

    x_id = sleep_forever.remote()

    driver = """
import ray
ray.init("{}")
ray.get(ray.ObjectRef(ray._private.utils.hex_to_binary("{}")))
""".format(address_info["redis_address"], x_id.hex())

    p = run_string_as_driver_nonblocking(driver)
    # Make sure the driver is running.
    time.sleep(1)
    assert p.poll() is None

    # Kill the driver process.
    p.kill()
    p.wait()
    time.sleep(0.1)

    # Make sure the original task hasn't finished.
    ready_ids, _ = ray.wait([x_id], timeout=0)
    assert len(ready_ids) == 0
    # Seal the object so the store attempts to notify the worker that the
    # get has been fulfilled.
    obj = np.ones(200 * 1024, dtype=np.uint8)
    ray.worker.global_worker.put_object(obj, x_id)
    time.sleep(0.1)

    # Make sure that nothing has died.
    assert ray._private.services.remaining_processes_alive()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5787')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_component_failures.py: 145-183
</a>
<div class="mid" id="frag5787" style="display:none"><pre>
def test_dying_driver_wait(ray_start_regular):
    # Start the Ray processes.
    address_info = ray_start_regular

    @ray.remote
    def sleep_forever():
        time.sleep(10**6)

    x_id = sleep_forever.remote()

    driver = """
import ray
ray.init("{}")
ray.wait([ray.ObjectRef(ray._private.utils.hex_to_binary("{}"))])
""".format(address_info["redis_address"], x_id.hex())

    p = run_string_as_driver_nonblocking(driver)
    # Make sure the driver is running.
    time.sleep(1)
    assert p.poll() is None

    # Kill the driver process.
    p.kill()
    p.wait()
    time.sleep(0.1)

    # Make sure the original task hasn't finished.
    ready_ids, _ = ray.wait([x_id], timeout=0)
    assert len(ready_ids) == 0
    # Seal the object so the store attempts to notify the worker that the
    # wait can return.
    obj = np.ones(200 * 1024, dtype=np.uint8)
    ray.worker.global_worker.put_object(obj, x_id)
    time.sleep(0.1)

    # Make sure that nothing has died.
    assert ray._private.services.remaining_processes_alive()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 148:</b> &nbsp; 2 fragments, nominal size 20 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5791')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_ray_debugger.py: 51-80
</a>
<div class="mid" id="frag5791" style="display:none"><pre>
def test_ray_debugger_commands(shutdown_only):
    ray.init(num_cpus=2)

    @ray.remote
    def f():
        """We support unicode too: 🐛"""
        ray.util.pdb.set_trace()

    result1 = f.remote()
    result2 = f.remote()

    # Make sure that calling "continue" in the debugger
    # gives back control to the debugger loop:
    p = pexpect.spawn("ray debug")
    p.expect("Enter breakpoint index or press enter to refresh: ")
    p.sendline("0")
    p.expect("-&gt; ray.util.pdb.set_trace()")
    p.sendline("ll")
    # Cannot use the 🐛 symbol here because pexpect doesn't support
    # unicode, but this test also does nicely:
    p.expect("unicode")
    p.sendline("c")
    p.expect("Enter breakpoint index or press enter to refresh: ")
    p.sendline("0")
    p.expect("-&gt; ray.util.pdb.set_trace()")
    p.sendline("c")

    ray.get([result1, result2])


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5793')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_ray_debugger.py: 83-111
</a>
<div class="mid" id="frag5793" style="display:none"><pre>
def test_ray_debugger_stepping(shutdown_only):
    ray.init(num_cpus=1)

    @ray.remote
    def g():
        return None

    @ray.remote
    def f():
        ray.util.pdb.set_trace()
        x = g.remote()
        return ray.get(x)

    result = f.remote()

    p = pexpect.spawn("ray debug")
    p.expect("Enter breakpoint index or press enter to refresh: ")
    p.sendline("0")
    p.expect("-&gt; x = g.remote()")
    p.sendline("remote")
    p.expect("(Pdb)")
    p.sendline("get")
    p.expect("(Pdb)")
    p.sendline("continue")

    # This should succeed now!
    ray.get(result)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 149:</b> &nbsp; 2 fragments, nominal size 33 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5826')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_stress_failure.py: 88-142
</a>
<div class="mid" id="frag5826" style="display:none"><pre>
def test_recursive(ray_start_reconstruction):
    plasma_store_memory, num_nodes, cluster = ray_start_reconstruction
    # Define the size of one task's return argument so that the combined
    # sum of all objects' sizes is at least twice the plasma stores'
    # combined allotted memory.
    num_objects = 100
    size = int(plasma_store_memory * 1.5 / (num_objects * 8))

    # Define a root task with no dependencies, which returns a numpy array
    # of the given size.
    @ray.remote
    def no_dependency_task(size):
        array = np.zeros(size)
        return array

    # Define a task with a single dependency, which returns its one
    # argument.
    @ray.remote
    def single_dependency(i, arg):
        arg = np.copy(arg)
        arg[0] = i
        return arg

    # Launch num_objects instances of the remote task, each dependent on
    # the one before it.
    arg = no_dependency_task.remote(size)
    args = []
    for i in range(num_objects):
        arg = single_dependency.remote(i, arg)
        args.append(arg)

    # Get each value to force each task to finish. After some number of
    # gets, old values should be evicted.
    for i in range(num_objects):
        value = ray.get(args[i])
        assert value[0] == i
    # Get each value again to force reconstruction.
    for i in range(num_objects):
        value = ray.get(args[i])
        assert value[0] == i
    # Get 10 values randomly.
    random_indexes = sorted_random_indexes(num_objects, 10)
    for i in random_indexes:
        value = ray.get(args[i])
        assert value[0] == i
    # Get values sequentially, in chunks.
    num_chunks = 4 * num_nodes
    chunk = num_objects // num_chunks
    for i in range(num_chunks):
        values = ray.get(args[i * chunk:(i + 1) * chunk])
        del values

    assert cluster.remaining_processes_alive()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5829')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_stress_failure.py: 144-196
</a>
<div class="mid" id="frag5829" style="display:none"><pre>
def test_multiple_recursive(ray_start_reconstruction):
    plasma_store_memory, _, cluster = ray_start_reconstruction
    # Define the size of one task's return argument so that the combined
    # sum of all objects' sizes is at least twice the plasma stores'
    # combined allotted memory.
    num_objects = 100
    size = plasma_store_memory * 2 // (num_objects * 8)

    # Define a root task with no dependencies, which returns a numpy array
    # of the given size.
    @ray.remote
    def no_dependency_task(size):
        array = np.zeros(size)
        return array

    # Define a task with multiple dependencies, which returns its first
    # argument.
    @ray.remote
    def multiple_dependency(i, arg1, arg2, arg3):
        arg1 = np.copy(arg1)
        arg1[0] = i
        return arg1

    # Launch num_args instances of the root task. Then launch num_objects
    # instances of the multi-dependency remote task, each dependent on the
    # num_args tasks before it.
    num_args = 3
    args = []
    for i in range(num_args):
        arg = no_dependency_task.remote(size)
        args.append(arg)
    for i in range(num_objects):
        args.append(multiple_dependency.remote(i, *args[i:i + num_args]))

    # Get each value to force each task to finish. After some number of
    # gets, old values should be evicted.
    args = args[num_args:]
    for i in range(num_objects):
        value = ray.get(args[i])
        assert value[0] == i
    # Get each value again to force reconstruction.
    for i in range(num_objects):
        value = ray.get(args[i])
        assert value[0] == i
    # Get 10 values randomly.
    random_indexes = sorted_random_indexes(num_objects, 10)
    for i in random_indexes:
        value = ray.get(args[i])
        assert value[0] == i

    assert cluster.remaining_processes_alive()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 150:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5886')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_mldataset.py: 39-51
</a>
<div class="mid" id="frag5886" style="display:none"><pre>
def test_from_parallel_it(ray_start_regular_shared):
    para_it = parallel_it.from_range(4).for_each(lambda x: [x])
    ds = ml_data.from_parallel_iter(para_it, batch_size=2)
    assert repr(ds) == ("MLDataset[from_range[4, shards=2]"
                        ".for_each().batch(2).to_pandas()]")
    collected = list(ds.gather_sync())
    assert len(collected) == 2
    assert all(d.shape == (2, 1) for d in collected)
    expected = para_it.flatten().batch(2).gather_sync().flatten()
    flattened = ds.gather_sync().for_each(lambda x: x[0].to_list()).flatten()
    assert list(flattened) == list(expected)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5887')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_mldataset.py: 52-69
</a>
<div class="mid" id="frag5887" style="display:none"><pre>
def test_batch(ray_start_regular_shared):
    para_it = parallel_it.from_range(16).for_each(lambda x: [x])
    ds = ml_data.from_parallel_iter(para_it, batch_size=2)
    collected = list(ds.gather_sync())
    assert len(collected) == 8
    assert all(d.shape == (2, 1) for d in collected)

    ds = ds.batch(4)
    assert repr(ds) == ("MLDataset[from_range[16, shards=2]"
                        ".for_each().batch(2).to_pandas().batch(4)]")
    collected = list(ds.gather_sync())
    assert len(collected) == 4
    assert all(d.shape == (4, 1) for d in collected)
    expected = para_it.flatten().batch(4).gather_sync().flatten()
    flattened = ds.gather_sync().for_each(lambda x: x[0].to_list()).flatten()
    assert list(flattened) == list(expected)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 151:</b> &nbsp; 2 fragments, nominal size 24 lines, similarity 87%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5905')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_client_init.py: 140-170
</a>
<div class="mid" id="frag5905" style="display:none"><pre>
def test_python_version(init_and_serve):
    server_handle = init_and_serve
    ray = _ClientContext()
    info1 = ray.connect("localhost:50051")
    assert info1["python_version"] == ".".join(
        [str(x) for x in list(sys.version_info)[:3]])
    ray.disconnect()
    time.sleep(1)

    def mock_connection_response():
        return ray_client_pb2.ConnectionInfoResponse(
            num_clients=1,
            python_version="2.7.12",
            ray_version="",
            ray_commit="",
            protocol_version=CURRENT_PROTOCOL_VERSION,
        )

    # inject mock connection function
    server_handle.data_servicer._build_connection_response = \
        mock_connection_response

    ray = _ClientContext()
    with pytest.raises(RuntimeError):
        _ = ray.connect("localhost:50051")

    ray = _ClientContext()
    info3 = ray.connect("localhost:50051", ignore_version=True)
    assert info3["num_clients"] == 1, info3
    ray.disconnect()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5907')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_client_init.py: 171-201
</a>
<div class="mid" id="frag5907" style="display:none"><pre>

def test_protocol_version(init_and_serve):
    server_handle = init_and_serve
    ray = _ClientContext()
    info1 = ray.connect("localhost:50051")
    local_py_version = ".".join([str(x) for x in list(sys.version_info)[:3]])
    assert info1["protocol_version"] == CURRENT_PROTOCOL_VERSION, info1
    ray.disconnect()
    time.sleep(1)

    def mock_connection_response():
        return ray_client_pb2.ConnectionInfoResponse(
            num_clients=1,
            python_version=local_py_version,
            ray_version="",
            ray_commit="",
            protocol_version="2050-01-01",  # from the future
        )

    # inject mock connection function
    server_handle.data_servicer._build_connection_response = \
        mock_connection_response

    ray = _ClientContext()
    with pytest.raises(RuntimeError):
        _ = ray.connect("localhost:50051")

    ray = _ClientContext()
    info3 = ray.connect("localhost:50051", ignore_version=True)
    assert info3["num_clients"] == 1, info3
    ray.disconnect()
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 152:</b> &nbsp; 2 fragments, nominal size 33 lines, similarity 82%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5948')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_global_gc.py: 18-60
</a>
<div class="mid" id="frag5948" style="display:none"><pre>
def test_auto_local_gc(shutdown_only):
    ray.init(
        num_cpus=2,
        _system_config={
            "local_gc_interval_s": 10,
            "local_gc_min_interval_s": 5,
            "global_gc_min_interval_s": 10
        })

    class ObjectWithCyclicRef:
        def __init__(self):
            self.loop = self

    @ray.remote(num_cpus=1)
    class GarbageHolder:
        def __init__(self):
            gc.disable()
            x = ObjectWithCyclicRef()
            self.garbage = weakref.ref(x)

        def has_garbage(self):
            return self.garbage() is not None

    try:
        gc.disable()

        # Local driver.
        local_ref = weakref.ref(ObjectWithCyclicRef())

        # Remote workers.
        actors = [GarbageHolder.remote() for _ in range(2)]
        assert local_ref() is not None
        assert all(ray.get([a.has_garbage.remote() for a in actors]))

        def check_refs_gced():
            return (local_ref() is None and
                    not any(ray.get([a.has_garbage.remote() for a in actors])))

        wait_for_condition(check_refs_gced)
    finally:
        gc.enable()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5953')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_global_gc.py: 61-110
</a>
<div class="mid" id="frag5953" style="display:none"><pre>
def test_global_gc(shutdown_only):
    cluster = ray.cluster_utils.Cluster()
    cluster.add_node(
        num_cpus=1,
        num_gpus=0,
        _system_config={
            "local_gc_interval_s": 10,
            "local_gc_min_interval_s": 5,
            "global_gc_min_interval_s": 10
        })
    cluster.add_node(num_cpus=1, num_gpus=0)
    ray.init(address=cluster.address)

    class ObjectWithCyclicRef:
        def __init__(self):
            self.loop = self

    @ray.remote(num_cpus=1)
    class GarbageHolder:
        def __init__(self):
            gc.disable()
            x = ObjectWithCyclicRef()
            self.garbage = weakref.ref(x)

        def has_garbage(self):
            return self.garbage() is not None

    try:
        gc.disable()

        # Local driver.
        local_ref = weakref.ref(ObjectWithCyclicRef())

        # Remote workers.
        actors = [GarbageHolder.remote() for _ in range(2)]
        assert local_ref() is not None
        assert all(ray.get([a.has_garbage.remote() for a in actors]))

        # GC should be triggered for all workers, including the local driver.
        global_gc()

        def check_refs_gced():
            return (local_ref() is None and
                    not any(ray.get([a.has_garbage.remote() for a in actors])))

        wait_for_condition(check_refs_gced, timeout=30)
    finally:
        gc.enable()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 153:</b> &nbsp; 2 fragments, nominal size 24 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6003')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_scheduling.py: 348-388
</a>
<div class="mid" id="frag6003" style="display:none"><pre>
def test_locality_aware_leasing_cached_objects(ray_start_cluster):
    # This test ensures that a task will run where its task dependencies are
    # located, even when those objects aren't primary copies.
    cluster = ray_start_cluster

    # Disable worker caching so worker leases are not reused, and disable
    # inlining of return objects so return objects are always put into Plasma.
    cluster.add_node(
        num_cpus=1,
        _system_config={
            "worker_lease_timeout_milliseconds": 0,
            "max_direct_call_object_size": 0,
        })
    # Use a custom resource for pinning tasks to a node.
    cluster.add_node(num_cpus=1, resources={"pin_worker1": 1})
    worker2 = cluster.add_node(num_cpus=1, resources={"pin_worker2": 1})
    ray.init(address=cluster.address)

    @ray.remote
    def f():
        return ray.worker.global_worker.node.unique_id

    @ray.remote
    def g(x):
        return ray.worker.global_worker.node.unique_id

    @ray.remote
    def h(x, y):
        return ray.worker.global_worker.node.unique_id

    # f_obj1 pinned on worker1.
    f_obj1 = f.options(resources={"pin_worker1": 1}).remote()
    # f_obj2 pinned on worker2.
    f_obj2 = f.options(resources={"pin_worker2": 1}).remote()
    # f_obj1 cached copy pulled to worker 2 in order to execute g() task.
    ray.get(g.options(resources={"pin_worker2": 1}).remote(f_obj1))
    # Confirm that h is scheduled onto worker 2, since it should have the
    # primary copy of f_obj12 and a cached copy of f_obj1.
    assert ray.get(h.remote(f_obj1, f_obj2)) == worker2.unique_id


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6007')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_scheduling.py: 389-427
</a>
<div class="mid" id="frag6007" style="display:none"><pre>
def test_locality_aware_leasing_borrowed_objects(ray_start_cluster):
    # This test ensures that a task will run where its task dependencies are
    # located, even when those objects are borrowed.
    cluster = ray_start_cluster

    # Disable worker caching so worker leases are not reused, and disable
    # inlining of return objects so return objects are always put into Plasma.
    cluster.add_node(
        num_cpus=1,
        resources={"pin_head": 1},
        _system_config={
            "worker_lease_timeout_milliseconds": 0,
            "max_direct_call_object_size": 0,
        })
    # Use a custom resource for pinning tasks to a node.
    worker_node = cluster.add_node(num_cpus=1, resources={"pin_worker": 1})
    ray.init(address=cluster.address)

    @ray.remote
    def f():
        return ray.worker.global_worker.node.unique_id

    @ray.remote
    def g(x):
        return ray.get(h.remote(x[0]))

    @ray.remote
    def h(x):
        return ray.worker.global_worker.node.unique_id

    # f will run on worker, f_obj will be pinned on worker.
    f_obj = f.options(resources={"pin_worker": 1}).remote()
    # g will run on head, f_obj will be borrowed by head, and we confirm that
    # h(f_obj) is scheduled onto worker, the node that has f_obj.
    assert ray.get(g.options(resources={
        "pin_head": 1
    }).remote([f_obj])) == worker_node.unique_id


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 154:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6054')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_plasma_unlimited.py: 50-64
</a>
<div class="mid" id="frag6054" style="display:none"><pre>
def test_fallback_when_spilling_impossible_on_put():
    try:
        address = _init_ray()
        x1 = ray.put(np.zeros(400 * MB, dtype=np.uint8))
        x1p = ray.get(x1)
        # x2 will be fallback allocated on the filesystem.
        x2 = ray.put(np.zeros(400 * MB, dtype=np.uint8))
        x2p = ray.get(x2)
        del x1p
        del x2p
        _check_spilled_mb(address, spilled=None, fallback=400)
    finally:
        ray.shutdown()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6056')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_plasma_unlimited.py: 80-98
</a>
<div class="mid" id="frag6056" style="display:none"><pre>
def test_fallback_when_spilling_impossible_on_get():
    try:
        address = _init_ray()
        x1 = ray.put(np.zeros(400 * MB, dtype=np.uint8))
        # x1 will be spilled.
        x2 = ray.put(np.zeros(400 * MB, dtype=np.uint8))
        _check_spilled_mb(address, spilled=400)
        # x1 will be restored, x2 will be spilled.
        x1p = ray.get(x1)
        _check_spilled_mb(address, spilled=800, restored=400)
        # x2 will be restored, triggering a fallback allocation.
        x2p = ray.get(x2)
        _check_spilled_mb(address, spilled=800, restored=800, fallback=400)
        del x1p
        del x2p
    finally:
        ray.shutdown()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 155:</b> &nbsp; 4 fragments, nominal size 15 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6105')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_actor_pool.py: 16-34
</a>
<div class="mid" id="frag6105" style="display:none"><pre>
def test_get_next(init):
    @ray.remote
    class MyActor:
        def __init__(self):
            pass

        def f(self, x):
            return x + 1

        def double(self, x):
            return 2 * x

    actors = [MyActor.remote() for _ in range(4)]
    pool = ActorPool(actors)
    for i in range(5):
        pool.submit(lambda a, v: a.f.remote(v), i)
        assert pool.get_next() == i + 1


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6117')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_actor_pool.py: 81-102
</a>
<div class="mid" id="frag6117" style="display:none"><pre>
def test_map_unordered(init):
    @ray.remote
    class MyActor:
        def __init__(self):
            pass

        def f(self, x):
            return x + 1

        def double(self, x):
            return 2 * x

    actors = [MyActor.remote() for _ in range(4)]
    pool = ActorPool(actors)

    total = []
    for v in pool.map(lambda a, v: a.double.remote(v), range(5)):
        total += [v]

    assert all(elem in [0, 2, 4, 6, 8] for elem in total)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6109')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_actor_pool.py: 35-59
</a>
<div class="mid" id="frag6109" style="display:none"><pre>
def test_get_next_unordered(init):
    @ray.remote
    class MyActor:
        def __init__(self):
            pass

        def f(self, x):
            return x + 1

        def double(self, x):
            return 2 * x

    actors = [MyActor.remote() for _ in range(4)]
    pool = ActorPool(actors)

    total = []

    for i in range(5):
        pool.submit(lambda a, v: a.f.remote(v), i)
    while pool.has_next():
        total += [pool.get_next_unordered()]

    assert all(elem in [1, 2, 3, 4, 5] for elem in total)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6113')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_actor_pool.py: 60-80
</a>
<div class="mid" id="frag6113" style="display:none"><pre>
def test_map(init):
    @ray.remote
    class MyActor:
        def __init__(self):
            pass

        def f(self, x):
            return x + 1

        def double(self, x):
            return 2 * x

    actors = [MyActor.remote() for _ in range(4)]
    pool = ActorPool(actors)

    index = 0
    for v in pool.map(lambda a, v: a.double.remote(v), range(5)):
        assert v == 2 * index
        index += 1


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 156:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 88%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6121')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_actor_pool.py: 103-124
</a>
<div class="mid" id="frag6121" style="display:none"><pre>
def test_get_next_timeout(init):
    @ray.remote
    class MyActor:
        def __init__(self):
            pass

        def f(self, x):
            while True:
                x = x + 1
                time.sleep(1)
            return None

        def double(self, x):
            return 2 * x

    actors = [MyActor.remote() for _ in range(4)]
    pool = ActorPool(actors)
    pool.submit(lambda a, v: a.f.remote(v), 0)
    with pytest.raises(TimeoutError):
        pool.get_next_unordered(timeout=0.1)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6125')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_actor_pool.py: 125-147
</a>
<div class="mid" id="frag6125" style="display:none"><pre>
def test_get_next_unordered_timeout(init):
    @ray.remote
    class MyActor:
        def __init__(self):
            pass

        def f(self, x):
            while True:
                x + 1
                time.sleep(1)
            return

        def double(self, x):
            return 2 * x

    actors = [MyActor.remote() for _ in range(4)]
    pool = ActorPool(actors)

    pool.submit(lambda a, v: a.f.remote(v), 0)
    with pytest.raises(TimeoutError):
        pool.get_next_unordered(timeout=0.1)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 157:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6283')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_output.py: 259-298
</a>
<div class="mid" id="frag6283" style="display:none"><pre>
def test_multi_stdout_err(file):
    if file == "stdout":
        file_handle = "sys.stdout"
    else:  # sys.stderr
        file_handle = "sys.stderr"

    script = f"""
import ray
import sys

ray.init(num_cpus=1)

@ray.remote
def foo():
    print(file={file_handle})

@ray.remote
def bar():
    print(file={file_handle})

@ray.remote
def baz():
    print(file={file_handle})

ray.get(foo.remote())
ray.get(bar.remote())
ray.get(baz.remote())
    """

    proc = run_string_as_driver_nonblocking(script)
    if file == "stdout":
        out_str = proc.stdout.read().decode("ascii")
    else:
        out_str = proc.stderr.read().decode("ascii")

    assert "(foo pid=" in out_str, out_str
    assert "(bar pid=" in out_str, out_str
    assert "(baz pid=" in out_str, out_str


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6284')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_output.py: 301-349
</a>
<div class="mid" id="frag6284" style="display:none"><pre>
def test_actor_stdout(file):
    if file == "stdout":
        file_handle = "sys.stdout"
    else:  # sys.stderr
        file_handle = "sys.stderr"

    script = f"""
import ray
import sys

ray.init(num_cpus=2)

@ray.remote
class Actor1:
    def f(self):
        print("hi", file={file_handle})

@ray.remote
class Actor2:
    def __init__(self):
        print("init", file={file_handle})
        self.name = "ActorX"
    def f(self):
        print("bye", file={file_handle})
    def __repr__(self):
        return self.name

a = Actor1.remote()
ray.get(a.f.remote())
b = Actor2.remote()
ray.get(b.f.remote())
    """

    proc = run_string_as_driver_nonblocking(script)
    if file == "stdout":
        out_str = proc.stdout.read().decode("ascii")
    else:
        out_str = proc.stderr.read().decode("ascii")
    print(out_str)

    assert "hi" in out_str, out_str
    assert "(Actor1 pid=" in out_str, out_str
    assert "bye" in out_str, out_str
    assert re.search("Actor2 pid=.*init", out_str), out_str
    assert not re.search("ActorX pid=.*init", out_str), out_str
    assert re.search("ActorX pid=.*bye", out_str), out_str
    assert not re.search("Actor2 pid=.*bye", out_str), out_str


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 158:</b> &nbsp; 4 fragments, nominal size 16 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6289')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_traceback.py: 53-80
</a>
<div class="mid" id="frag6289" style="display:none"><pre>
def test_actor_creation_stacktrace(ray_start_regular):
    """Test the actor creation task stacktrace."""
    expected_output = """The actor died because of an error raised in its creation task, ray::A.__init__() (pid=XXX, ip=YYY) # noqa
  File "FILE", line ZZ, in __init__
    g(3)
  File "FILE", line ZZ, in g
    raise ValueError(a)
ValueError: 3"""

    def g(a):
        raise ValueError(a)

    @ray.remote
    class A:
        def __init__(self):
            g(3)

        def ping(self):
            pass

    try:
        a = A.remote()
        ray.get(a.ping.remote())
    except RayActorError as ex:
        print(ex)
        assert clean_noqa(expected_output) == scrub_traceback(str(ex))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6296')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_traceback.py: 114-141
</a>
<div class="mid" id="frag6296" style="display:none"><pre>
def test_actor_task_stacktrace(ray_start_regular):
    """Test the actor task stacktrace."""
    expected_output = """ray::A.f() (pid=XXX, repr=&lt;test_traceback.A object at ADDRESS&gt;) # noqa
  File "FILE", line ZZ, in f
    return g(c)
  File "FILE", line ZZ, in g
    raise ValueError(a)
ValueError: 7"""

    def g(a):
        raise ValueError(a)

    @ray.remote
    class A:
        def f(self):
            a = 3
            b = 4
            c = a + b
            return g(c)

    a = A.remote()
    try:
        ray.get(a.f.remote())
    except ValueError as ex:
        print(ex)
        assert clean_noqa(expected_output) == scrub_traceback(str(ex))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6293')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_traceback.py: 84-110
</a>
<div class="mid" id="frag6293" style="display:none"><pre>
def test_task_stacktrace(ray_start_regular):
    """Test the normal task stacktrace."""
    expected_output = """ray::f() (pid=XXX, ip=YYY)
  File "FILE", line ZZ, in f
    return g(c)
  File "FILE", line ZZ, in g
    raise ValueError(a)
ValueError: 7"""

    def g(a):
        raise ValueError(a)
        # pass

    @ray.remote
    def f():
        a = 3
        b = 4
        c = a + b
        return g(c)

    try:
        ray.get(f.remote())
    except ValueError as ex:
        print(ex)
        assert clean_noqa(expected_output) == scrub_traceback(str(ex))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6313')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_traceback.py: 274-312
</a>
<div class="mid" id="frag6313" style="display:none"><pre>
def test_unpickleable_stacktrace(shutdown_only):
    expected_output = """System error: Failed to unpickle serialized exception
traceback: Traceback (most recent call last):
  File "FILE", line ZZ, in from_bytes
    return pickle.loads(ray_exception.serialized_exception)
TypeError: __init__() missing 1 required positional argument: 'arg'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "FILE", line ZZ, in deserialize_objects
    obj = self._deserialize_object(data, metadata, object_ref)
  File "FILE", line ZZ, in _deserialize_object
    return RayError.from_bytes(obj)
  File "FILE", line ZZ, in from_bytes
    raise RuntimeError(msg) from e
RuntimeError: Failed to unpickle serialized exception"""

    class NoPickleError(OSError):
        def __init__(self, arg):
            pass

    def g(a):
        raise NoPickleError("asdf")

    @ray.remote
    def f():
        a = 3
        b = 4
        c = a + b
        return g(c)

    try:
        ray.get(f.remote())
    except Exception as ex:
        print(repr(scrub_traceback(str(ex))))
        assert clean_noqa(expected_output) == scrub_traceback(str(ex))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 159:</b> &nbsp; 3 fragments, nominal size 10 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6326')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_response_cache.py: 56-75
</a>
<div class="mid" id="frag6326" style="display:none"><pre>
def test_response_cache_incomplete_response():
    """
    Tests case where a cache entry is populated after a long time. Any new
    threads attempting to access that entry should sleep until the response
    is ready.
    """
    cache = ResponseCache()

    def populate_cache():
        time.sleep(2)
        cache.update_cache(123, 15, "abcdef")

    cache.check_cache(123, 15)  # shouldn't block
    t = threading.Thread(target=populate_cache, args=())
    t.start()
    # Should block until other thread populates cache
    assert cache.check_cache(123, 15) == "abcdef"
    t.join()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6328')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_response_cache.py: 76-95
</a>
<div class="mid" id="frag6328" style="display:none"><pre>
def test_ordered_response_cache_incomplete_response():
    """
    Tests case where an ordered cache entry is populated after a long time. Any
    new threads attempting to access that entry should sleep until the response
    is ready.
    """
    cache = OrderedResponseCache()

    def populate_cache():
        time.sleep(2)
        cache.update_cache(15, "vwxyz")

    cache.check_cache(15)  # shouldn't block
    t = threading.Thread(target=populate_cache, args=())
    t.start()
    # Should block until other thread populates cache
    assert cache.check_cache(15) == "vwxyz"
    t.join()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6333')" href="javascript:;">
ray-ray-1.9.2/python/ray/tests/test_response_cache.py: 159-180
</a>
<div class="mid" id="frag6333" style="display:none"><pre>
def test_ordered_response_cache_cleanup_while_waiting():
    """
    Tests that an error is thrown when an ordered cache entry is updated with
    the response for a different request than what was originally being
    checked for.
    """
    # Error when awaiting cache to update, but entry is cleaned up
    cache = OrderedResponseCache()
    assert cache.check_cache(123) is None

    def cleanup_cache():
        time.sleep(2)
        cache.cleanup(123)

    t = threading.Thread(target=cleanup_cache, args=())
    t.start()

    with pytest.raises(RuntimeError):
        cache.check_cache(123)
    t.join()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 160:</b> &nbsp; 2 fragments, nominal size 21 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6583')" href="javascript:;">
ray-ray-1.9.2/python/ray/_private/log_monitor.py: 42-62
</a>
<div class="mid" id="frag6583" style="display:none"><pre>
    def __init__(self,
                 filename=None,
                 size_when_last_opened=None,
                 file_position=None,
                 file_handle=None,
                 is_err_file=False,
                 job_id=None,
                 worker_pid=None):
        assert (filename is not None and size_when_last_opened is not None
                and file_position is not None)
        self.filename = filename
        self.size_when_last_opened = size_when_last_opened
        self.file_position = file_position
        self.file_handle = file_handle
        self.is_err_file = is_err_file
        self.job_id = job_id
        self.worker_pid = worker_pid
        self.actor_name = None
        self.task_name = None


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6789')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/sgd/torch/torch_runner.py: 31-57
</a>
<div class="mid" id="frag6789" style="display:none"><pre>
    def __init__(self,
                 training_operator_cls,
                 config=None,
                 use_gpu=False,
                 serialize_data_creation=True,
                 use_fp16=False,
                 use_tqdm=False,
                 scheduler_step_freq=None):
        self.training_operator_cls = training_operator_cls
        self.config = {} if config is None else config

        self.timers = utils.TimerCollection()
        self.epochs = 0
        self.training_operator = None
        self.serialize_data_creation = serialize_data_creation
        self.use_gpu = use_gpu
        self.use_fp16 = choose_amp_backend(use_fp16, amp, apex_amp)
        self.use_tqdm = use_tqdm
        self.scheduler_step_freq = scheduler_step_freq

        # Training and Validation iterators
        self.train_iterator = None
        self._should_reset_train_loader = True

        self.val_iterator = None
        self._should_reset_val_loader = True

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 161:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6606')" href="javascript:;">
ray-ray-1.9.2/python/ray/train/tests/test_tune.py: 47-66
</a>
<div class="mid" id="frag6606" style="display:none"><pre>


def torch_fashion_mnist(num_workers, use_gpu, num_samples):
    epochs = 2

    trainer = Trainer("torch", num_workers=num_workers, use_gpu=use_gpu)
    MnistTrainable = trainer.to_tune_trainable(fashion_mnist_train_func)

    analysis = tune.run(
        MnistTrainable,
        num_samples=num_samples,
        config={
            "lr": tune.loguniform(1e-4, 1e-1),
            "batch_size": tune.choice([32, 64, 128]),
            "epochs": epochs
        })

    # Check that loss decreases in each trial.
    for path, df in analysis.trial_dataframes.items():
        assert df.loc[1, "loss"] &lt; df.loc[0, "loss"]
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6608')" href="javascript:;">
ray-ray-1.9.2/python/ray/train/tests/test_tune.py: 71-89
</a>
<div class="mid" id="frag6608" style="display:none"><pre>


def tune_tensorflow_mnist(num_workers, use_gpu, num_samples):
    epochs = 2
    trainer = Trainer("tensorflow", num_workers=num_workers, use_gpu=use_gpu)
    MnistTrainable = trainer.to_tune_trainable(tensorflow_mnist_train_func)

    analysis = tune.run(
        MnistTrainable,
        num_samples=num_samples,
        config={
            "lr": tune.loguniform(1e-4, 1e-1),
            "batch_size": tune.choice([32, 64, 128]),
            "epochs": epochs
        })

    # Check that loss decreases in each trial.
    for path, df in analysis.trial_dataframes.items():
        assert df.loc[1, "loss"] &lt; df.loc[0, "loss"]
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 162:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6635')" href="javascript:;">
ray-ray-1.9.2/python/ray/train/tests/test_gpu.py: 91-112
</a>
<div class="mid" id="frag6635" style="display:none"><pre>
    os.environ.pop("CUDA_VISIBLE_DEVICES")


def test_tensorflow_mnist_gpu(ray_start_4_cpus_2_gpus):
    num_workers = 2
    epochs = 3

    trainer = Trainer("tensorflow", num_workers=num_workers, use_gpu=True)
    config = {"lr": 1e-3, "batch_size": 64, "epochs": epochs}
    trainer.start()
    results = trainer.run(tensorflow_mnist_train_func, config)
    trainer.shutdown()

    assert len(results) == num_workers
    result = results[0]

    loss = result["loss"]
    assert len(loss) == epochs
    assert loss[-1] &lt; loss[0]

    accuracy = result["accuracy"]
    assert len(accuracy) == epochs
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6671')" href="javascript:;">
ray-ray-1.9.2/python/ray/train/tests/test_examples.py: 22-43
</a>
<div class="mid" id="frag6671" style="display:none"><pre>
    yield address_info
    # The code after the yield will run as teardown code.
    ray.shutdown()


@pytest.mark.parametrize("num_workers", [1, 2])
def test_tensorflow_mnist(ray_start_2_cpus, num_workers):
    num_workers = num_workers
    epochs = 3

    trainer = Trainer("tensorflow", num_workers=num_workers)
    config = {"lr": 1e-3, "batch_size": 64, "epochs": epochs}
    trainer.start()
    results = trainer.run(tensorflow_mnist_train_func, config)
    trainer.shutdown()

    assert len(results) == num_workers
    result = results[0]

    loss = result["loss"]
    assert len(loss) == epochs
    assert loss[-1] &lt; loss[0]
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 163:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 91%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6636')" href="javascript:;">
ray-ray-1.9.2/python/ray/train/tests/test_gpu.py: 113-129
</a>
<div class="mid" id="frag6636" style="display:none"><pre>
    assert accuracy[-1] &gt; accuracy[0]


def test_torch_fashion_mnist_gpu(ray_start_4_cpus_2_gpus):
    num_workers = 2
    epochs = 3

    trainer = Trainer("torch", num_workers=num_workers, use_gpu=True)
    config = {"lr": 1e-3, "batch_size": 64, "epochs": epochs}
    trainer.start()
    results = trainer.run(fashion_mnist_train_func, config)
    trainer.shutdown()

    assert len(results) == num_workers

    for result in results:
        assert len(result) == epochs
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6674')" href="javascript:;">
ray-ray-1.9.2/python/ray/train/tests/test_examples.py: 71-87
</a>
<div class="mid" id="frag6674" style="display:none"><pre>

    for result in results:
        assert len(result) == epochs
        assert result[-1]["loss"] &lt; result[0]["loss"]


def test_torch_fashion_mnist(ray_start_2_cpus):
    num_workers = 2
    epochs = 3

    trainer = Trainer("torch", num_workers=num_workers)
    config = {"lr": 1e-3, "batch_size": 64, "epochs": epochs}
    trainer.start()
    results = trainer.run(fashion_mnist_train_func, config)
    trainer.shutdown()

    assert len(results) == num_workers
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 164:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 93%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6637')" href="javascript:;">
ray-ray-1.9.2/python/ray/train/tests/test_gpu.py: 130-148
</a>
<div class="mid" id="frag6637" style="display:none"><pre>
        assert result[-1] &lt; result[0]


def test_horovod_torch_mnist_gpu(ray_start_4_cpus_2_gpus):
    num_workers = 2
    num_epochs = 2
    trainer = Trainer("horovod", num_workers, use_gpu=True)
    trainer.start()
    results = trainer.run(
        horovod_torch_train_func,
        config={
            "num_epochs": num_epochs,
            "lr": 1e-3
        })
    trainer.shutdown()

    assert len(results) == num_workers
    for worker_result in results:
        assert len(worker_result) == num_epochs
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6676')" href="javascript:;">
ray-ray-1.9.2/python/ray/train/tests/test_examples.py: 97-115
</a>
<div class="mid" id="frag6676" style="display:none"><pre>
    trainer = Trainer(backend="torch", num_workers=1)
    trainer.start()
    trainer.run(torch_quick_start_train_func)
    trainer.shutdown()


def test_horovod_torch_mnist(ray_start_2_cpus):
    num_workers = 2
    num_epochs = 2
    trainer = Trainer("horovod", num_workers)
    trainer.start()
    results = trainer.run(
        horovod_torch_train_func,
        config={
            "num_epochs": num_epochs,
            "lr": 1e-3
        })
    trainer.shutdown()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 165:</b> &nbsp; 3 fragments, nominal size 14 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6697')" href="javascript:;">
ray-ray-1.9.2/python/ray/train/examples/tune_tensorflow_mnist_example.py: 10-27
</a>
<div class="mid" id="frag6697" style="display:none"><pre>
def tune_tensorflow_mnist(num_workers, num_samples):
    trainer = Trainer(backend="tensorflow", num_workers=num_workers)
    Trainable = trainer.to_tune_trainable(train_func)
    analysis = tune.run(
        Trainable,
        num_samples=num_samples,
        config={
            "lr": tune.loguniform(1e-4, 1e-1),
            "batch_size": tune.choice([32, 64, 128]),
            "epochs": 3
        })
    best_loss = analysis.get_best_config(metric="loss", mode="min")
    best_accuracy = analysis.get_best_config(metric="accuracy", mode="max")
    print(f"Best loss config: {best_loss}")
    print(f"Best accuracy config: {best_accuracy}")
    return analysis


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6704')" href="javascript:;">
ray-ray-1.9.2/python/ray/train/examples/tune_linear_example.py: 10-25
</a>
<div class="mid" id="frag6704" style="display:none"><pre>
def tune_linear(num_workers, num_samples):
    trainer = Trainer("torch", num_workers=num_workers)
    Trainable = trainer.to_tune_trainable(train_func)
    analysis = tune.run(
        Trainable,
        num_samples=num_samples,
        config={
            "lr": tune.loguniform(1e-4, 1e-1),
            "batch_size": tune.choice([4, 16, 32]),
            "epochs": 3
        })
    results = analysis.get_best_config(metric="loss", mode="min")
    print(results)
    return results


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6699')" href="javascript:;">
ray-ray-1.9.2/python/ray/train/examples/tune_linear_dataset_example.py: 10-27
</a>
<div class="mid" id="frag6699" style="display:none"><pre>
def tune_linear(num_workers, num_samples, use_gpu):
    datasets = get_datasets()

    trainer = Trainer("torch", num_workers=num_workers, use_gpu=use_gpu)
    Trainable = trainer.to_tune_trainable(train_func, dataset=datasets)
    analysis = tune.run(
        Trainable,
        num_samples=num_samples,
        config={
            "lr": tune.loguniform(1e-4, 1e-1),
            "batch_size": tune.choice([4, 16, 32]),
            "epochs": 3
        })
    results = analysis.get_best_config(metric="loss", mode="min")
    print(results)
    return results


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 166:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 92%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6702')" href="javascript:;">
ray-ray-1.9.2/python/ray/train/examples/torch_quick_start.py: 33-50
</a>
<div class="mid" id="frag6702" style="display:none"><pre>
def train_func():
    num_epochs = 3
    model = NeuralNetwork()
    loss_fn = nn.MSELoss()
    optimizer = optim.SGD(model.parameters(), lr=0.1)

    for epoch in range(num_epochs):
        output = model(input)
        loss = loss_fn(output, labels)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        print(f"epoch: {epoch}, loss: {loss.item()}")

# __torch_single_end__

# __torch_distributed_begin__

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6703')" href="javascript:;">
ray-ray-1.9.2/python/ray/train/examples/torch_quick_start.py: 53-70
</a>
<div class="mid" id="frag6703" style="display:none"><pre>
def train_func_distributed():
    num_epochs = 3
    model = NeuralNetwork()
    model = train.torch.prepare_model(model)
    loss_fn = nn.MSELoss()
    optimizer = optim.SGD(model.parameters(), lr=0.1)

    for epoch in range(num_epochs):
        output = model(input)
        loss = loss_fn(output, labels)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        print(f"epoch: {epoch}, loss: {loss.item()}")

# __torch_distributed_end__


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 167:</b> &nbsp; 3 fragments, nominal size 13 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6711')" href="javascript:;">
ray-ray-1.9.2/python/ray/train/examples/tensorflow_linear_dataset_example.py: 87-104
</a>
<div class="mid" id="frag6711" style="display:none"><pre>

def train_tensorflow_linear(num_workers=2, use_gpu=False):
    dataset_pipeline = get_dataset_pipeline()
    trainer = Trainer(
        backend="tensorflow", num_workers=num_workers, use_gpu=use_gpu)
    trainer.start()
    results = trainer.run(
        train_func=train_func,
        dataset=dataset_pipeline,
        config={
            "lr": 1e-3,
            "batch_size": 32,
            "epochs": 4
        })
    trainer.shutdown()
    print(f"Results: {results[0]}")
    return results

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6742')" href="javascript:;">
ray-ray-1.9.2/python/ray/train/examples/train_fashion_mnist_example.py: 110-125
</a>
<div class="mid" id="frag6742" style="display:none"><pre>
def train_fashion_mnist(num_workers=2, use_gpu=False):
    trainer = Trainer(
        backend="torch", num_workers=num_workers, use_gpu=use_gpu)
    trainer.start()
    result = trainer.run(
        train_func=train_func,
        config={
            "lr": 1e-3,
            "batch_size": 64,
            "epochs": 4
        },
        callbacks=[JsonLoggerCallback()])
    trainer.shutdown()
    print(f"Loss results: {result}")


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6725')" href="javascript:;">
ray-ray-1.9.2/python/ray/train/examples/tensorflow_mnist_example.py: 75-89
</a>
<div class="mid" id="frag6725" style="display:none"><pre>
def train_tensorflow_mnist(num_workers=2, use_gpu=False, epochs=4):
    trainer = Trainer(
        backend="tensorflow", num_workers=num_workers, use_gpu=use_gpu)
    trainer.start()
    results = trainer.run(
        train_func=train_func,
        config={
            "lr": 1e-3,
            "batch_size": 64,
            "epochs": epochs
        })
    trainer.shutdown()
    print(f"Results: {results[0]}")


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 168:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6739')" href="javascript:;">
ray-ray-1.9.2/python/ray/train/examples/train_fashion_mnist_example.py: 45-61
</a>
<div class="mid" id="frag6739" style="display:none"><pre>
def train_epoch(dataloader, model, loss_fn, optimizer):
    size = len(dataloader.dataset)
    for batch, (X, y) in enumerate(dataloader):
        # Compute prediction error
        pred = model(X)
        loss = loss_fn(pred, y)

        # Backpropagation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if batch % 100 == 0:
            loss, current = loss.item(), batch * len(X)
            print(f"loss: {loss:&gt;7f}  [{current:&gt;5d}/{size:&gt;5d}]")


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6743')" href="javascript:;">
ray-ray-1.9.2/python/ray/train/examples/tune_cifar_pytorch_pbt_example.py: 20-36
</a>
<div class="mid" id="frag6743" style="display:none"><pre>
def train_epoch(dataloader, model, loss_fn, optimizer):
    size = len(dataloader.dataset)
    for batch, (X, y) in enumerate(dataloader):
        # Compute prediction error
        pred = model(X)
        loss = loss_fn(pred, y)

        # Backpropagation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if batch % 100 == 0:
            loss, current = loss.item(), batch * len(X)
            print(f"loss: {loss:&gt;7f}  [{current:&gt;5d}/{size:&gt;5d}]")


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 169:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 92%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6740')" href="javascript:;">
ray-ray-1.9.2/python/ray/train/examples/train_fashion_mnist_example.py: 62-79
</a>
<div class="mid" id="frag6740" style="display:none"><pre>
def validate_epoch(dataloader, model, loss_fn):
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    model.eval()
    test_loss, correct = 0, 0
    with torch.no_grad():
        for X, y in dataloader:
            pred = model(X)
            test_loss += loss_fn(pred, y).item()
            correct += (pred.argmax(1) == y).type(torch.float).sum().item()
    test_loss /= num_batches
    correct /= size
    print(f"Test Error: \n "
          f"Accuracy: {(100 * correct):&gt;0.1f}%, "
          f"Avg loss: {test_loss:&gt;8f} \n")
    return test_loss


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6744')" href="javascript:;">
ray-ray-1.9.2/python/ray/train/examples/tune_cifar_pytorch_pbt_example.py: 37-54
</a>
<div class="mid" id="frag6744" style="display:none"><pre>
def validate_epoch(dataloader, model, loss_fn):
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    model.eval()
    test_loss, correct = 0, 0
    with torch.no_grad():
        for X, y in dataloader:
            pred = model(X)
            test_loss += loss_fn(pred, y).item()
            correct += (pred.argmax(1) == y).type(torch.float).sum().item()
    test_loss /= num_batches
    correct /= size
    print(f"Test Error: \n "
          f"Accuracy: {(100 * correct):&gt;0.1f}%, "
          f"Avg loss: {test_loss:&gt;8f} \n")
    return {"loss": test_loss}


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 170:</b> &nbsp; 3 fragments, nominal size 43 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6745')" href="javascript:;">
ray-ray-1.9.2/python/ray/train/examples/tune_cifar_pytorch_pbt_example.py: 55-117
</a>
<div class="mid" id="frag6745" style="display:none"><pre>
def train_func(config):
    epochs = config.pop("epochs", 3)
    model = ResNet18(config)
    model = train.torch.prepare_model(model)

    # Create optimizer.
    optimizer = torch.optim.SGD(
        model.parameters(),
        lr=config.get("lr", 0.1),
        momentum=config.get("momentum", 0.9))

    # Load in training and validation data.
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465),
                             (0.2023, 0.1994, 0.2010)),
    ])  # meanstd transformation

    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465),
                             (0.2023, 0.1994, 0.2010)),
    ])

    with FileLock(".ray.lock"):
        train_dataset = CIFAR10(
            root="~/data",
            train=True,
            download=True,
            transform=transform_train)
        validation_dataset = CIFAR10(
            root="~/data",
            train=False,
            download=False,
            transform=transform_test)

    if config.get("test_mode"):
        train_dataset = Subset(train_dataset, list(range(64)))
        validation_dataset = Subset(validation_dataset, list(range(64)))

    train_loader = DataLoader(train_dataset, batch_size=config["batch_size"])
    validation_loader = DataLoader(
        validation_dataset, batch_size=config["batch_size"])

    train_loader = train.torch.prepare_data_loader(train_loader)
    validation_loader = train.torch.prepare_data_loader(validation_loader)

    # Create loss.
    criterion = nn.CrossEntropyLoss()

    results = []

    for _ in range(epochs):
        train_epoch(train_loader, model, criterion, optimizer)
        result = validate_epoch(validation_loader, model, criterion)
        train.report(**result)
        results.append(result)

    return results


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6894')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/sgd/torch/examples/cifar_pytorch_pbt.py: 33-87
</a>
<div class="mid" id="frag6894" style="display:none"><pre>
    def setup(self, config):
        # Create model.
        model = ResNet18(config)

        # Create optimizer.
        optimizer = torch.optim.SGD(
            model.parameters(),
            lr=config.get("lr", 0.1),
            momentum=config.get("momentum", 0.9))

        # Load in training and validation data.
        transform_train = transforms.Compose([
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465),
                                 (0.2023, 0.1994, 0.2010)),
        ])  # meanstd transformation

        transform_test = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465),
                                 (0.2023, 0.1994, 0.2010)),
        ])

        with FileLock(".ray.lock"):
            train_dataset = CIFAR10(
                root="~/data",
                train=True,
                download=True,
                transform=transform_train)
            validation_dataset = CIFAR10(
                root="~/data",
                train=False,
                download=False,
                transform=transform_test)

        if config.get("test_mode"):
            train_dataset = Subset(train_dataset, list(range(64)))
            validation_dataset = Subset(validation_dataset, list(range(64)))

        train_loader = DataLoader(
            train_dataset, batch_size=config[BATCH_SIZE], num_workers=2)
        validation_loader = DataLoader(
            validation_dataset, batch_size=config[BATCH_SIZE], num_workers=2)

        # Create loss.
        criterion = nn.CrossEntropyLoss()

        self.model, self.optimizer, self.criterion = \
            self.register(models=model, optimizers=optimizer,
                          criterion=criterion,)
        self.register_data(
            train_loader=train_loader, validation_loader=validation_loader)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6882')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/sgd/torch/examples/cifar_pytorch_example.py: 31-89
</a>
<div class="mid" id="frag6882" style="display:none"><pre>
    def setup(self, config):
        # Create model.
        model = ResNet18(config)

        # Create optimizer.
        optimizer = torch.optim.SGD(
            model.parameters(),
            lr=config.get("lr", 0.1),
            momentum=config.get("momentum", 0.9))

        # Load in training and validation data.
        transform_train = transforms.Compose([
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465),
                                 (0.2023, 0.1994, 0.2010)),
        ])  # meanstd transformation

        transform_test = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465),
                                 (0.2023, 0.1994, 0.2010)),
        ])
        with FileLock(".ray.lock"):
            train_dataset = CIFAR10(
                root="~/data",
                train=True,
                download=True,
                transform=transform_train)
            validation_dataset = CIFAR10(
                root="~/data",
                train=False,
                download=False,
                transform=transform_test)

        if config["test_mode"]:
            train_dataset = Subset(train_dataset, list(range(64)))
            validation_dataset = Subset(validation_dataset, list(range(64)))

        train_loader = DataLoader(
            train_dataset, batch_size=config[BATCH_SIZE], num_workers=2)
        validation_loader = DataLoader(
            validation_dataset, batch_size=config[BATCH_SIZE], num_workers=2)

        # Create scheduler.
        scheduler = torch.optim.lr_scheduler.MultiStepLR(
            optimizer, milestones=[150, 250, 350], gamma=0.1)

        # Create loss.
        criterion = nn.CrossEntropyLoss()

        # Register all components.
        self.model, self.optimizer, self.criterion, self.scheduler = \
            self.register(models=model, optimizers=optimizer,
                          criterion=criterion, schedulers=scheduler)
        self.register_data(
            train_loader=train_loader, validation_loader=validation_loader)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 171:</b> &nbsp; 2 fragments, nominal size 23 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6768')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/lightgbm/simple_example.py: 8-40
</a>
<div class="mid" id="frag6768" style="display:none"><pre>
def main():
    # Load dataset
    data, labels = datasets.load_breast_cancer(return_X_y=True)
    # Split into train and test set
    train_x, test_x, train_y, test_y = train_test_split(
        data, labels, test_size=0.25)

    train_set = RayDMatrix(train_x, train_y)
    test_set = RayDMatrix(test_x, test_y)

    # Set config
    config = {
        "objective": "binary",
        "metric": ["binary_logloss", "binary_error"],
        "max_depth": 3,
    }

    evals_result = {}

    # Train the classifier
    bst = train(
        config,
        train_set,
        evals=[(test_set, "eval")],
        evals_result=evals_result,
        ray_params=RayParams(max_actor_restarts=1, num_actors=1),
        verbose_eval=False)

    bst.booster_.save_model("simple.lgbm")
    print("Final validation error: {:.4f}".format(
        evals_result["eval"]["binary_error"][-1]))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7580')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/xgboost/simple_example.py: 8-41
</a>
<div class="mid" id="frag7580" style="display:none"><pre>
def main():
    # Load dataset
    data, labels = datasets.load_breast_cancer(return_X_y=True)
    # Split into train and test set
    train_x, test_x, train_y, test_y = train_test_split(
        data, labels, test_size=0.25)

    train_set = RayDMatrix(train_x, train_y)
    test_set = RayDMatrix(test_x, test_y)

    # Set config
    config = {
        "tree_method": "approx",
        "objective": "binary:logistic",
        "eval_metric": ["logloss", "error"],
        "max_depth": 3,
    }

    evals_result = {}

    # Train the classifier
    bst = train(
        config,
        train_set,
        evals=[(test_set, "eval")],
        evals_result=evals_result,
        ray_params=RayParams(max_actor_restarts=1, num_actors=1),
        verbose_eval=False)

    bst.save_model("simple.xgb")
    print("Final validation error: {:.4f}".format(
        evals_result["eval"]["error"][-1]))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 172:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 93%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6772')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/lightgbm/simple_tune.py: 11-36
</a>
<div class="mid" id="frag6772" style="display:none"><pre>
def train_model(config):
    # Load dataset
    data, labels = datasets.load_breast_cancer(return_X_y=True)
    # Split into train and test set
    train_x, test_x, train_y, test_y = train_test_split(
        data, labels, test_size=0.25)

    train_set = RayDMatrix(train_x, train_y)
    test_set = RayDMatrix(test_x, test_y)

    evals_result = {}
    bst = train(
        params=config,
        dtrain=train_set,
        evals=[(test_set, "eval")],
        evals_result=evals_result,
        verbose_eval=False,
        ray_params=RayParams(
            num_actors=num_actors, cpus_per_actor=num_cpus_per_actor))
    bst.booster_.save_model("model.lgbm")


# __train_end__


# __load_begin__
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7584')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/xgboost/simple_tune.py: 12-37
</a>
<div class="mid" id="frag7584" style="display:none"><pre>
def train_model(config):
    # Load dataset
    data, labels = datasets.load_breast_cancer(return_X_y=True)
    # Split into train and test set
    train_x, test_x, train_y, test_y = train_test_split(
        data, labels, test_size=0.25)

    train_set = RayDMatrix(train_x, train_y)
    test_set = RayDMatrix(test_x, test_y)

    evals_result = {}
    bst = train(
        params=config,
        dtrain=train_set,
        evals=[(test_set, "eval")],
        evals_result=evals_result,
        verbose_eval=False,
        ray_params=RayParams(
            num_actors=num_actors, cpus_per_actor=num_cpus_per_actor))
    bst.save_model("model.xgb")


# __train_end__


# __load_begin__
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 173:</b> &nbsp; 2 fragments, nominal size 23 lines, similarity 82%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6774')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/lightgbm/simple_tune.py: 48-92
</a>
<div class="mid" id="frag6774" style="display:none"><pre>
def main():
    # __tune_begin__
    from ray import tune

    # Set config
    config = {
        "objective": "binary",
        "metric": ["binary_logloss", "binary_error"],
        "eta": tune.loguniform(1e-4, 1e-1),
        "subsample": tune.uniform(0.5, 1.0),
        "max_depth": tune.randint(1, 9)
    }
    # __tune_end__

    # __tune_run_begin__
    analysis = tune.run(
        train_model,
        config=config,
        metric="eval-binary_error",
        mode="min",
        num_samples=4,
        resources_per_trial={
            "cpu": 1,
            "extra_cpu": num_actors * num_cpus_per_actor
        })

    # Load in the best performing model.
    best_bst = load_best_model(analysis.best_logdir)

    # Use the following code block instead if using Ray Client.
    # import ray
    # if ray.util.client.ray.is_connected():
    #     # If using Ray Client best_logdir is a directory on the server.
    #     # So we want to make sure we wrap model loading in a task.
    #     remote_load_fn = ray.remote(load_best_model)
    #     best_bst = ray.get(remote_load_fn.remote(analysis.best_logdir))

    # Do something with the best model.
    _ = best_bst

    accuracy = 1. - analysis.best_result["eval-binary_error"]
    print(f"Best model parameters: {analysis.best_config}")
    print(f"Best model total accuracy: {accuracy:.4f}")
    # __tune_run_end__

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7586')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/xgboost/simple_tune.py: 50-96
</a>
<div class="mid" id="frag7586" style="display:none"><pre>
def main():
    # __tune_begin__
    from ray import tune

    # Set config
    config = {
        "tree_method": "approx",
        "objective": "binary:logistic",
        "eval_metric": ["logloss", "error"],
        "eta": tune.loguniform(1e-4, 1e-1),
        "subsample": tune.uniform(0.5, 1.0),
        "max_depth": tune.randint(1, 9)
    }
    # __tune_end__

    # __tune_run_begin__
    analysis = tune.run(
        train_model,
        config=config,
        metric="eval-error",
        mode="min",
        num_samples=4,
        resources_per_trial=PlacementGroupFactory([{
            "CPU": 1.0
        }] + [{
            "CPU": float(num_cpus_per_actor)
        }] * num_actors))

    # Load in the best performing model.
    best_bst = load_best_model(analysis.best_logdir)

    # Use the following code block instead if using Ray Client.
    # import ray
    # if ray.util.client.ray.is_connected():
    #     # If using Ray Client best_logdir is a directory on the server.
    #     # So we want to make sure we wrap model loading in a task.
    #     remote_load_fn = ray.remote(load_best_model)
    #     best_bst = ray.get(remote_load_fn.remote(analysis.best_logdir))

    # Do something with the best model.
    _ = best_bst

    accuracy = 1. - analysis.best_result["eval-error"]
    print(f"Best model parameters: {analysis.best_config}")
    print(f"Best model total accuracy: {accuracy:.4f}")
    # __tune_run_end__

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 174:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6779')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/lightgbm/release_test_util.py: 37-51
</a>
<div class="mid" id="frag6779" style="display:none"><pre>

    def __call__(self, env: CallbackEnv):
        if env.iteration == self._iteration:
            rank = 0 if self.is_rank_0 else 1
            if rank in self._ranks:
                if not ray.get(self._state.has_failed.remote(self._id)):
                    success = ray.get(self._state.set_failed.remote(self._id))
                    if not success:
                        # Another rank is already about to fail
                        return

                    pid = os.getpid()
                    print(f"Killing process: {pid} for actor rank {rank}")
                    time.sleep(1)
                    os.kill(pid, 9)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7591')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/xgboost/release_test_util.py: 39-54
</a>
<div class="mid" id="frag7591" style="display:none"><pre>

    def after_iteration(self, model, epoch, evals_log):
        if epoch == self._iteration:
            rank = get_actor_rank()
            if rank in self._ranks:
                if not ray.get(self._state.has_failed.remote(self._id)):
                    success = ray.get(self._state.set_failed.remote(self._id))
                    if not success:
                        # Another rank is already about to fail
                        return

                    pid = os.getpid()
                    print(f"Killing process: {pid} for actor rank {rank}")
                    time.sleep(1)
                    os.kill(pid, 9)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 175:</b> &nbsp; 2 fragments, nominal size 75 lines, similarity 96%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6781')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/lightgbm/release_test_util.py: 63-148
</a>
<div class="mid" id="frag6781" style="display:none"><pre>

def train_ray(path,
              num_workers,
              num_boost_rounds,
              num_files=0,
              regression=False,
              use_gpu=False,
              ray_params=None,
              lightgbm_params=None,
              **kwargs):
    path = os.path.expanduser(path)
    if not os.path.exists(path):
        raise ValueError(f"Path does not exist: {path}")

    if num_files:
        files = sorted(glob.glob(f"{path}/**/*.parquet"))
        while num_files &gt; len(files):
            files = files + files
        path = files[0:num_files]

    use_device_matrix = False
    if use_gpu:
        try:
            import cupy  # noqa: F401
            use_device_matrix = True
        except ImportError:
            use_device_matrix = False

    if use_device_matrix:
        dtrain = RayDeviceQuantileDMatrix(
            path,
            num_actors=num_workers,
            label="labels",
            ignore=["partition"],
            filetype=RayFileType.PARQUET)
    else:
        dtrain = RayDMatrix(
            path,
            num_actors=num_workers,
            label="labels",
            ignore=["partition"],
            filetype=RayFileType.PARQUET)

    config = {"device": "cpu" if not use_gpu else "gpu"}

    if not regression:
        # Classification
        config.update({
            "objective": "binary",
            "metric": ["binary_logloss", "binary_error"],
        })
    else:
        # Regression
        config.update({
            "objective": "regression",
            "metric": ["l2", "rmse"],
        })

    if lightgbm_params:
        config.update(lightgbm_params)

    start = time.time()
    evals_result = {}
    additional_results = {}
    bst = train(
        config,
        dtrain,
        evals_result=evals_result,
        additional_results=additional_results,
        num_boost_round=num_boost_rounds,
        ray_params=ray_params or RayParams(
            max_actor_restarts=2,
            num_actors=num_workers,
            cpus_per_actor=2,
            gpus_per_actor=0 if not use_gpu else 1),
        evals=[(dtrain, "train")],
        **kwargs)
    taken = time.time() - start
    print(f"TRAIN TIME TAKEN: {taken:.2f} seconds")

    out_file = os.path.expanduser(
        "~/benchmark_{}.lgbm".format("cpu" if not use_gpu else "gpu"))
    bst.booster_.save_model(out_file)

    print("Final training error: {:.4f}".format(evals_result["train"][
        "binary_error" if not regression else "rmse"][-1]))
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7593')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/xgboost/release_test_util.py: 62-147
</a>
<div class="mid" id="frag7593" style="display:none"><pre>

def train_ray(path,
              num_workers,
              num_boost_rounds,
              num_files=0,
              regression=False,
              use_gpu=False,
              ray_params=None,
              xgboost_params=None,
              **kwargs):
    path = os.path.expanduser(path)
    if not os.path.exists(path):
        raise ValueError(f"Path does not exist: {path}")

    if num_files:
        files = sorted(glob.glob(f"{path}/**/*.parquet"))
        while num_files &gt; len(files):
            files = files + files
        path = files[0:num_files]

    use_device_matrix = False
    if use_gpu:
        try:
            import cupy  # noqa: F401
            use_device_matrix = True
        except ImportError:
            use_device_matrix = False

    if use_device_matrix:
        dtrain = RayDeviceQuantileDMatrix(
            path,
            num_actors=num_workers,
            label="labels",
            ignore=["partition"],
            filetype=RayFileType.PARQUET)
    else:
        dtrain = RayDMatrix(
            path,
            num_actors=num_workers,
            label="labels",
            ignore=["partition"],
            filetype=RayFileType.PARQUET)

    config = {"tree_method": "hist" if not use_gpu else "gpu_hist"}

    if not regression:
        # Classification
        config.update({
            "objective": "binary:logistic",
            "eval_metric": ["logloss", "error"],
        })
    else:
        # Regression
        config.update({
            "objective": "reg:squarederror",
            "eval_metric": ["logloss", "rmse"],
        })

    if xgboost_params:
        config.update(xgboost_params)

    start = time.time()
    evals_result = {}
    additional_results = {}
    bst = train(
        config,
        dtrain,
        evals_result=evals_result,
        additional_results=additional_results,
        num_boost_round=num_boost_rounds,
        ray_params=ray_params or RayParams(
            max_actor_restarts=2,
            num_actors=num_workers,
            cpus_per_actor=1,
            gpus_per_actor=1 if not use_gpu else 1),
        evals=[(dtrain, "train")],
        **kwargs)
    taken = time.time() - start
    print(f"TRAIN TIME TAKEN: {taken:.2f} seconds")

    out_file = os.path.expanduser(
        "~/benchmark_{}.xgb".format("cpu" if not use_gpu else "gpu"))
    bst.save_model(out_file)

    print("Final training error: {:.4f}".format(
        evals_result["train"]["error"][-1]))
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 176:</b> &nbsp; 2 fragments, nominal size 23 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6815')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/sgd/torch/resnet.py: 13-36
</a>
<div class="mid" id="frag6815" style="display:none"><pre>
    def __init__(self, in_planes, planes, stride=1):
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(
            in_planes,
            planes,
            kernel_size=3,
            stride=stride,
            padding=1,
            bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(
            planes, planes, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != self.expansion * planes:
            self.shortcut = nn.Sequential(
                nn.Conv2d(
                    in_planes,
                    self.expansion * planes,
                    kernel_size=1,
                    stride=stride,
                    bias=False), nn.BatchNorm2d(self.expansion * planes))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6817')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/sgd/torch/resnet.py: 48-73
</a>
<div class="mid" id="frag6817" style="display:none"><pre>
    def __init__(self, in_planes, planes, stride=1):
        super(Bottleneck, self).__init__()
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(
            planes,
            planes,
            kernel_size=3,
            stride=stride,
            padding=1,
            bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv3 = nn.Conv2d(
            planes, self.expansion * planes, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(self.expansion * planes)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != self.expansion * planes:
            self.shortcut = nn.Sequential(
                nn.Conv2d(
                    in_planes,
                    self.expansion * planes,
                    kernel_size=1,
                    stride=stride,
                    bias=False), nn.BatchNorm2d(self.expansion * planes))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 177:</b> &nbsp; 2 fragments, nominal size 20 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7030')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/sgd/tests/test_tensorflow.py: 117-143
</a>
<div class="mid" id="frag7030" style="display:none"><pre>
def test_tf_dataset(ray_start_4_cpus):  # noqa: F811
    num_points = 32 * 100 * 2
    data = [i * (1 / num_points) for i in range(num_points)]
    it = parallel_it.from_items(data, 2, False).for_each(lambda x: [x, x])
    # this will create MLDataset with column RangeIndex(range(2))
    ds = ml_data.from_parallel_iter(it, True, batch_size=32, repeated=False)
    tf_ds = ds.to_tf(feature_columns=[0], label_column=1)
    trainer = TFTrainer(
        model_creator=model_creator,
        data_creator=make_data_creator(tf_ds),
        num_replicas=2,
        config={
            "batch_size": 32,
            "fit_config": {
                "steps_per_epoch": 100,
            }
        })

    for _ in range(10):
        trainer.train()

    model = trainer.get_model()
    prediction = model.predict([0.5])[0][0]
    assert 0.4 &lt;= prediction &lt;= 0.6
    trainer.shutdown()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7227')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/data/examples/mlp_identity_tf.py: 38-63
</a>
<div class="mid" id="frag7227" style="display:none"><pre>
def main():
    num_points = 32 * 100 * 2
    data = [i * (1 / num_points) for i in range(num_points)]
    it = parallel_it.from_items(data, 2, False).for_each(lambda x: [x, x])
    # this will create MLDataset with column RangeIndex(range(2))
    ds = ml_data.from_parallel_iter(it, True, batch_size=32, repeated=False)
    tf_ds = ds.to_tf(feature_columns=[0], label_column=1)

    trainer = TFTrainer(
        model_creator=model_creator,
        data_creator=make_data_creator(tf_ds),
        num_replicas=2,
        config={
            "batch_size": 32,
            "fit_config": {
                "steps_per_epoch": 100,
            }
        })

    for _ in range(10):
        trainer.train()

    model = trainer.get_model()
    print("f(0.5)=", float(model.predict([0.5])))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 178:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7045')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/sgd/tests/test_torch_basic.py: 40-53
</a>
<div class="mid" id="frag7045" style="display:none"><pre>
def test_single_step(ray_start_2_cpus, use_local):  # noqa: F811
    trainer = TorchTrainer(
        training_operator_cls=Operator,
        num_workers=1,
        use_local=use_local,
        use_gpu=False)
    metrics = trainer.train(num_steps=1)
    assert metrics[BATCH_COUNT] == 1

    val_metrics = trainer.validate(num_steps=1)
    assert val_metrics[BATCH_COUNT] == 1
    trainer.shutdown()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7095')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/sgd/tests/test_ptl.py: 87-100
</a>
<div class="mid" id="frag7095" style="display:none"><pre>
@pytest.mark.parametrize("use_local", [True, False])
def test_single_step(ray_start_2_cpus, use_local):  # noqa: F811
    trainer = TorchTrainer(
        training_operator_cls=Operator,
        num_workers=1,
        use_local=use_local,
        use_gpu=False)
    metrics = trainer.train(num_steps=1)
    assert metrics[BATCH_COUNT] == 1

    val_metrics = trainer.validate(num_steps=1)
    assert val_metrics[BATCH_COUNT] == 1
    trainer.shutdown()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 179:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7046')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/sgd/tests/test_torch_basic.py: 57-81
</a>
<div class="mid" id="frag7046" style="display:none"><pre>
def test_train(ray_start_2_cpus, num_workers, use_local,
               use_fp16):  # noqa: F811
    trainer = TorchTrainer(
        training_operator_cls=Operator,
        num_workers=num_workers,
        use_local=use_local,
        use_gpu=False,
        # use_fp16 has no effect here but allows
        # us to check syntax
        use_fp16=use_fp16,
    )
    for i in range(3):
        train_loss1 = trainer.train()["train_loss"]
    validation_loss1 = trainer.validate()["val_loss"]

    for i in range(3):
        train_loss2 = trainer.train()["train_loss"]
    validation_loss2 = trainer.validate()["val_loss"]

    assert train_loss2 &lt;= train_loss1, (train_loss2, train_loss1)
    assert validation_loss2 &lt;= validation_loss1, (validation_loss2,
                                                  validation_loss1)
    trainer.shutdown()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7096')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/sgd/tests/test_ptl.py: 103-122
</a>
<div class="mid" id="frag7096" style="display:none"><pre>
@pytest.mark.parametrize("use_local", [True, False])
def test_train(ray_start_2_cpus, num_workers, use_local):  # noqa: F811
    trainer = TorchTrainer(
        training_operator_cls=Operator,
        num_workers=num_workers,
        use_local=use_local,
        use_gpu=False)
    for i in range(3):
        train_loss1 = trainer.train()["train_loss"]
    validation_loss1 = trainer.validate()["val_loss"]

    for i in range(3):
        train_loss2 = trainer.train()["train_loss"]
    validation_loss2 = trainer.validate()["val_loss"]

    assert train_loss2 &lt;= train_loss1, (train_loss2, train_loss1)
    assert validation_loss2 &lt;= validation_loss1, (validation_loss2,
                                                  validation_loss1)
    trainer.shutdown()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 180:</b> &nbsp; 2 fragments, nominal size 27 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7047')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/sgd/tests/test_torch_basic.py: 84-112
</a>
<div class="mid" id="frag7047" style="display:none"><pre>
def test_tune_train(ray_start_4_cpus, num_workers, use_local):  # noqa: F811
    TorchTrainable = TorchTrainer.as_trainable(
        **{
            "training_operator_cls": Operator,
            "num_workers": num_workers,
            "use_gpu": False,
            "backend": "gloo",
            "use_local": use_local,
            "config": {
                "batch_size": 512,
                "lr": 0.001
            }
        })

    analysis = tune.run(
        TorchTrainable,
        num_samples=2,
        stop={"training_iteration": 2},
        verbose=1)

    # checks loss decreasing for every trials
    for path, df in analysis.trial_dataframes.items():
        mean_train_loss1 = df.loc[0, "train_loss"]
        mean_train_loss2 = df.loc[1, "train_loss"]
        mean_val_loss1 = df.loc[0, "val_loss"]
        mean_val_loss2 = df.loc[1, "val_loss"]

        assert mean_train_loss2 &lt;= mean_train_loss1
        assert mean_val_loss2 &lt;= mean_val_loss1
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7131')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/sgd/tests/test_torch_3.py: 48-86
</a>
<div class="mid" id="frag7131" style="display:none"><pre>
def test_tune_custom_train(ray_start_4_cpus, num_workers,
                           use_local):  # noqa: F811
    def custom_train_func(trainer, info):
        train_stats = trainer.train(profile=True)
        val_stats = trainer.validate(profile=True)
        stats = merge_dicts(train_stats, val_stats)
        return stats

    TorchTrainable = TorchTrainer.as_trainable(
        **{
            "override_tune_step": custom_train_func,
            "training_operator_cls": Operator,
            "num_workers": num_workers,
            "use_gpu": False,
            "backend": "gloo",
            "use_local": use_local,
            "config": {
                "batch_size": 512,
                "lr": 0.001
            }
        })

    analysis = tune.run(
        TorchTrainable,
        num_samples=2,
        stop={"training_iteration": 2},
        verbose=1)

    # checks loss decreasing for every trials
    for path, df in analysis.trial_dataframes.items():
        mean_train_loss1 = df.loc[0, "train_loss"]
        mean_train_loss2 = df.loc[1, "train_loss"]
        mean_val_loss1 = df.loc[0, "val_loss"]
        mean_val_loss2 = df.loc[1, "val_loss"]

        assert mean_train_loss2 &lt;= mean_train_loss1
        assert mean_val_loss2 &lt;= mean_val_loss1


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 181:</b> &nbsp; 3 fragments, nominal size 24 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7097')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/sgd/tests/test_ptl.py: 125-160
</a>
<div class="mid" id="frag7097" style="display:none"><pre>
@pytest.mark.parametrize("use_local", [True, False])
def test_save_and_restore(ray_start_2_cpus, num_workers, use_local,
                          tmp_path):  # noqa: F811
    trainer1 = TorchTrainer(
        training_operator_cls=Operator,
        num_workers=num_workers,
        use_local=use_local)
    trainer1.train()
    checkpoint_path = os.path.join(tmp_path, "checkpoint")
    trainer1.save(checkpoint_path)

    model1 = trainer1.get_model()
    ints1 = trainer1.apply_all_operators(lambda op: op.get_model().rand_int)[0]

    trainer1.shutdown()

    trainer2 = TorchTrainer(
        training_operator_cls=Operator,
        num_workers=num_workers,
        use_local=use_local)
    trainer2.load(checkpoint_path)

    model2 = trainer2.get_model()
    ints2 = trainer2.apply_all_operators(lambda op: op.get_model().rand_int)

    model1_state_dict = model1.state_dict()
    model2_state_dict = model2.state_dict()

    assert set(model1_state_dict.keys()) == set(model2_state_dict.keys())

    for k in model1_state_dict:
        assert torch.equal(model1_state_dict[k], model2_state_dict[k])
    for i in ints2:
        assert i == ints1
    trainer2.shutdown()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7158')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/sgd/tests/test_torch.py: 301-332
</a>
<div class="mid" id="frag7158" style="display:none"><pre>
@pytest.mark.parametrize("use_local", [True, False])
def test_save_and_restore(ray_start_2_cpus, num_workers, use_local,
                          tmp_path):  # noqa: F811
    trainer1 = TorchTrainer(
        training_operator_cls=Operator,
        num_workers=num_workers,
        use_local=use_local)
    trainer1.train()
    checkpoint_path = os.path.join(tmp_path, "checkpoint")
    trainer1.save(checkpoint_path)

    model1 = trainer1.get_model()

    trainer1.shutdown()

    trainer2 = TorchTrainer(
        training_operator_cls=Operator,
        num_workers=num_workers,
        use_local=use_local)
    trainer2.load(checkpoint_path)

    model2 = trainer2.get_model()

    model1_state_dict = model1.state_dict()
    model2_state_dict = model2.state_dict()

    assert set(model1_state_dict.keys()) == set(model2_state_dict.keys())

    for k in model1_state_dict:
        assert torch.equal(model1_state_dict[k], model2_state_dict[k])
    trainer2.shutdown()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7159')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/sgd/tests/test_torch.py: 333-363
</a>
<div class="mid" id="frag7159" style="display:none"><pre>

def test_wrap_ddp(ray_start_2_cpus, tmp_path):  # noqa: F811
    if not dist.is_available():
        return
    trainer1 = TorchTrainer(
        training_operator_cls=Operator,
        wrap_ddp=False,
        num_workers=2,
        use_local=True)
    trainer1.train()
    checkpoint_path = os.path.join(tmp_path, "checkpoint")
    trainer1.save(checkpoint_path)

    model1 = trainer1.get_model()
    trainer1.shutdown()

    trainer2 = TorchTrainer(
        training_operator_cls=Operator, wrap_ddp=False, num_workers=2)
    trainer2.load(checkpoint_path)

    model2 = trainer2.get_model()

    model1_state_dict = model1.state_dict()
    model2_state_dict = model2.state_dict()

    assert set(model1_state_dict.keys()) == set(model2_state_dict.keys())

    for k in model1_state_dict:
        assert torch.equal(model1_state_dict[k], model2_state_dict[k])
    trainer2.shutdown()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 182:</b> &nbsp; 3 fragments, nominal size 22 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7117')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/sgd/tests/test_torch_failure.py: 129-158
</a>
<div class="mid" id="frag7117" style="display:none"><pre>
@patch.object(RemoteWorkerGroup, "_train", remote_worker_train_with_fail)
def test_fail_twice(ray_start_2_cpus, use_local):  # noqa: F811
    if not dist.is_available():
        return

    def single_loader(config):
        dataset = LinearDataset(2, 5, size=1000000)
        return DataLoader(dataset, batch_size=config.get("batch_size", 32))

    TestOperator = TrainingOperator.from_creators(
        model_creator,
        optimizer_creator,
        single_loader,
        loss_creator=lambda config: nn.MSELoss())

    start_with_fail = gen_start_with_fail(2)

    with patch.object(TorchTrainer, "_start_workers", start_with_fail):
        trainer1 = TorchTrainer(
            training_operator_cls=TestOperator,
            config={"batch_size": 100000},
            use_local=use_local,
            num_workers=2)

        # MAX RETRIES SHOULD BE ON BY DEFAULT
        trainer1.train()
        assert trainer1._num_failures == 2
        assert trainer1.worker_group.num_workers == 2
        trainer1.shutdown(force=True)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7127')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/sgd/tests/test_torch_failure.py: 270-297
</a>
<div class="mid" id="frag7127" style="display:none"><pre>
@patch.object(RemoteWorkerGroup, "_train", remote_worker_train_with_fail)
def test_failure_during_resize(ray_start_2_cpus):  # noqa: F811
    """Tests if training succeeds even with failures during worker resizing."""
    if not dist.is_available():
        return

    def single_loader(config):
        dataset = LinearDataset(2, 5, size=1000000)
        return DataLoader(dataset, batch_size=config.get("batch_size", 32))

    TestOperator = TrainingOperator.from_creators(
        model_creator,
        optimizer_creator,
        single_loader,
        loss_creator=lambda config: nn.MSELoss())

    start_with_fail = gen_start_with_startup_fail(1)
    with patch.object(TorchTrainer, "_start_workers", start_with_fail):
        trainer1 = TorchTrainer(
            training_operator_cls=TestOperator,
            config={"batch_size": 100000},
            timeout_s=5,
            use_local=False,
            num_workers=2)
        trainer1.train()

    trainer1.shutdown()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7119')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/sgd/tests/test_torch_failure.py: 161-191
</a>
<div class="mid" id="frag7119" style="display:none"><pre>
@patch.object(RemoteWorkerGroup, "_train", remote_worker_train_with_fail)
def test_fail_with_recover(ray_start_2_cpus, use_local):  # noqa: F811
    print(locals())
    if not dist.is_available():
        return

    def single_loader(config):
        dataset = LinearDataset(2, 5, size=1000000)
        return DataLoader(dataset, batch_size=config.get("batch_size", 32))

    TestOperator = TrainingOperator.from_creators(
        model_creator,
        optimizer_creator,
        single_loader,
        loss_creator=lambda config: nn.MSELoss())

    start_with_fail = gen_start_with_fail(3)

    with patch.object(TorchTrainer, "_start_workers", start_with_fail):
        trainer1 = TorchTrainer(
            training_operator_cls=TestOperator,
            config={"batch_size": 100000},
            timeout_s=5,
            use_local=use_local,
            num_workers=2)

        with pytest.raises(RuntimeError):
            trainer1.train(max_retries=1)

        trainer1.shutdown(force=True)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 183:</b> &nbsp; 2 fragments, nominal size 21 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7246')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_gpu_tests/test_distributed_sendrecv.py: 14-35
</a>
<div class="mid" id="frag7246" style="display:none"><pre>
def test_sendrecv(ray_start_distributed_2_nodes_4_gpus, group_name, array_size,
                  src_rank, dst_rank):
    if src_rank == dst_rank:
        return
    world_size = 4
    actors, _ = create_collective_workers(
        num_workers=world_size, group_name=group_name)
    ray.get([
        a.set_buffer.remote(cp.ones(array_size, dtype=cp.float32) * (i + 1))
        for i, a in enumerate(actors)
    ])
    refs = []
    for i in range(world_size):
        refs.append(actors[i].get_buffer.remote())
    refs[src_rank] = actors[src_rank].do_send.remote(group_name, dst_rank)
    refs[dst_rank] = actors[dst_rank].do_recv.remote(group_name, src_rank)
    results = ray.get(refs)
    assert (results[src_rank] == cp.ones(array_size, dtype=cp.float32) *
            (src_rank + 1)).all()
    assert (results[dst_rank] == cp.ones(array_size, dtype=cp.float32) *
            (src_rank + 1)).all()
    ray.get([a.destroy_group.remote(group_name) for a in actors])
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7421')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_cpu_tests/test_distributed_sendrecv.py: 16-39
</a>
<div class="mid" id="frag7421" style="display:none"><pre>
def test_sendrecv(ray_start_distributed_2_nodes, group_name, array_size,
                  src_rank, dst_rank, backend):
    if src_rank == dst_rank:
        return
    world_size = 8
    actors, _ = create_collective_workers(
        num_workers=world_size, group_name=group_name, backend=backend)
    ray.get([
        a.set_buffer.remote(np.ones(array_size, dtype=np.float32) * (i + 1))
        for i, a in enumerate(actors)
    ])
    refs = []
    for i in range(world_size):
        refs.append(actors[i].get_buffer.remote())
    refs[src_rank] = actors[src_rank].do_send.remote(group_name, dst_rank)
    refs[dst_rank] = actors[dst_rank].do_recv.remote(group_name, src_rank)
    results = ray.get(refs)
    assert (results[src_rank] == np.ones(array_size, dtype=np.float32) *
            (src_rank + 1)).all()
    assert (results[dst_rank] == np.ones(array_size, dtype=np.float32) *
            (src_rank + 1)).all()
    ray.get([a.destroy_group.remote(group_name) for a in actors])


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 184:</b> &nbsp; 4 fragments, nominal size 15 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7247')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_gpu_tests/test_distributed_allgather.py: 14-30
</a>
<div class="mid" id="frag7247" style="display:none"><pre>
                         [2, 2**5, 2**10, 2**15, 2**20, [2, 2], [5, 5, 5]])
def test_allgather_different_array_size(ray_start_distributed_2_nodes_4_gpus,
                                        array_size, tensor_backend):
    world_size = 4
    actors, _ = create_collective_workers(world_size)
    init_tensors_for_gather_scatter(
        actors, array_size=array_size, tensor_backend=tensor_backend)
    results = ray.get([a.do_allgather.remote() for a in actors])
    for i in range(world_size):
        for j in range(world_size):
            if tensor_backend == "cupy":
                assert (results[i][j] == cp.ones(array_size, dtype=cp.float32)
                        * (j + 1)).all()
            else:
                assert (results[i][j] == torch.ones(
                    array_size, dtype=torch.float32).cuda() * (j + 1)).all()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7347')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_cpu_tests/test_allgather.py: 14-30
</a>
<div class="mid" id="frag7347" style="display:none"><pre>
                         [2, 2**5, 2**10, 2**15, 2**20, [2, 2], [5, 5, 5]])
def test_allgather_different_array_size(ray_start_single_node, array_size,
                                        tensor_backend, backend):
    world_size = 2
    actors, _ = create_collective_workers(world_size, backend=backend)
    init_tensors_for_gather_scatter(
        actors, array_size=array_size, tensor_backend=tensor_backend)
    results = ray.get([a.do_allgather.remote() for a in actors])
    for i in range(world_size):
        for j in range(world_size):
            if tensor_backend == "numpy":
                assert (results[i][j] == np.ones(array_size, dtype=np.float32)
                        * (j + 1)).all()
            else:
                assert (results[i][j] == torch.ones(
                    array_size, dtype=torch.float32) * (j + 1)).all()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7281')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_gpu_tests/test_allgather.py: 14-30
</a>
<div class="mid" id="frag7281" style="display:none"><pre>
                         [2, 2**5, 2**10, 2**15, 2**20, [2, 2], [5, 5, 5]])
def test_allgather_different_array_size(ray_start_single_node_2_gpus,
                                        array_size, tensor_backend):
    world_size = 2
    actors, _ = create_collective_workers(world_size)
    init_tensors_for_gather_scatter(
        actors, array_size=array_size, tensor_backend=tensor_backend)
    results = ray.get([a.do_allgather.remote() for a in actors])
    for i in range(world_size):
        for j in range(world_size):
            if tensor_backend == "cupy":
                assert (results[i][j] == cp.ones(array_size, dtype=cp.float32)
                        * (j + 1)).all()
            else:
                assert (results[i][j] == torch.ones(
                    array_size, dtype=torch.float32).cuda() * (j + 1)).all()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7422')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_cpu_tests/test_distributed_allgather.py: 16-32
</a>
<div class="mid" id="frag7422" style="display:none"><pre>
                         [2, 2**5, 2**10, 2**15, 2**20, [2, 2], [5, 5, 5]])
def test_allgather_different_array_size(ray_start_distributed_2_nodes,
                                        array_size, tensor_backend, backend):
    world_size = 8
    actors, _ = create_collective_workers(world_size, backend=backend)
    init_tensors_for_gather_scatter(
        actors, array_size=array_size, tensor_backend=tensor_backend)
    results = ray.get([a.do_allgather.remote() for a in actors])
    for i in range(world_size):
        for j in range(world_size):
            if tensor_backend == "numpy":
                assert (results[i][j] == np.ones(array_size, dtype=np.float32)
                        * (j + 1)).all()
            else:
                assert (results[i][j] == torch.ones(
                    array_size, dtype=torch.float32) * (j + 1)).all()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 185:</b> &nbsp; 6 fragments, nominal size 11 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7249')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_gpu_tests/test_distributed_allgather.py: 45-57
</a>
<div class="mid" id="frag7249" style="display:none"><pre>
@pytest.mark.parametrize("length", [0, 1, 3, 4, 7, 8])
def test_unmatched_tensor_list_length(ray_start_distributed_2_nodes_4_gpus,
                                      length):
    world_size = 4
    actors, _ = create_collective_workers(world_size)
    list_buffer = [cp.ones(10, dtype=cp.float32) for _ in range(length)]
    ray.wait([a.set_list_buffer.remote(list_buffer) for a in actors])
    if length != world_size:
        with pytest.raises(RuntimeError):
            ray.get([a.do_allgather.remote() for a in actors])
    else:
        ray.get([a.do_allgather.remote() for a in actors])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7283')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_gpu_tests/test_allgather.py: 44-55
</a>
<div class="mid" id="frag7283" style="display:none"><pre>
@pytest.mark.parametrize("length", [0, 1, 2, 3])
def test_unmatched_tensor_list_length(ray_start_single_node_2_gpus, length):
    world_size = 2
    actors, _ = create_collective_workers(world_size)
    list_buffer = [cp.ones(10, dtype=cp.float32) for _ in range(length)]
    ray.wait([a.set_list_buffer.remote(list_buffer) for a in actors])
    if length != world_size:
        with pytest.raises(RuntimeError):
            ray.get([a.do_allgather.remote() for a in actors])
    else:
        ray.get([a.do_allgather.remote() for a in actors])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7250')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_gpu_tests/test_distributed_allgather.py: 59-71
</a>
<div class="mid" id="frag7250" style="display:none"><pre>
@pytest.mark.parametrize("shape", [10, 20, [4, 5], [1, 3, 5, 7]])
def test_unmatched_tensor_shape(ray_start_distributed_2_nodes_4_gpus, shape):
    world_size = 4
    actors, _ = create_collective_workers(world_size)
    init_tensors_for_gather_scatter(actors, array_size=10)
    list_buffer = [cp.ones(shape, dtype=cp.float32) for _ in range(world_size)]
    ray.get([a.set_list_buffer.remote(list_buffer) for a in actors])
    if shape != 10:
        with pytest.raises(RuntimeError):
            ray.get([a.do_allgather.remote() for a in actors])
    else:
        ray.get([a.do_allgather.remote() for a in actors])

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7284')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_gpu_tests/test_allgather.py: 57-69
</a>
<div class="mid" id="frag7284" style="display:none"><pre>
@pytest.mark.parametrize("shape", [10, 20, [4, 5], [1, 3, 5, 7]])
def test_unmatched_tensor_shape(ray_start_single_node_2_gpus, shape):
    world_size = 2
    actors, _ = create_collective_workers(world_size)
    init_tensors_for_gather_scatter(actors, array_size=10)
    list_buffer = [cp.ones(shape, dtype=cp.float32) for _ in range(world_size)]
    ray.get([a.set_list_buffer.remote(list_buffer) for a in actors])
    if shape != 10:
        with pytest.raises(RuntimeError):
            ray.get([a.do_allgather.remote() for a in actors])
    else:
        ray.get([a.do_allgather.remote() for a in actors])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7425')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_cpu_tests/test_distributed_allgather.py: 65-77
</a>
<div class="mid" id="frag7425" style="display:none"><pre>
@pytest.mark.parametrize("shape", [10, 20, [4, 5], [1, 3, 5, 7]])
def test_unmatched_tensor_shape(ray_start_distributed_2_nodes, shape, backend):
    world_size = 8
    actors, _ = create_collective_workers(world_size, backend=backend)
    init_tensors_for_gather_scatter(actors, array_size=10)
    list_buffer = [np.ones(shape, dtype=np.float32) for _ in range(world_size)]
    ray.get([a.set_list_buffer.remote(list_buffer, copy=True) for a in actors])
    if shape != 10:
        with pytest.raises(RuntimeError):
            ray.get([a.do_allgather.remote() for a in actors])
    else:
        ray.get([a.do_allgather.remote() for a in actors])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7350')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_cpu_tests/test_allgather.py: 61-73
</a>
<div class="mid" id="frag7350" style="display:none"><pre>
@pytest.mark.parametrize("shape", [10, 20, [4, 5], [1, 3, 5, 7]])
def test_unmatched_tensor_shape(ray_start_single_node, shape, backend):
    world_size = 2
    actors, _ = create_collective_workers(world_size, backend=backend)
    init_tensors_for_gather_scatter(actors, array_size=10)
    list_buffer = [np.ones(shape, dtype=np.float32) for _ in range(world_size)]
    ray.get([a.set_list_buffer.remote(list_buffer, copy=True) for a in actors])
    if shape != 10:
        with pytest.raises(RuntimeError):
            ray.get([a.do_allgather.remote() for a in actors])
    else:
        ray.get([a.do_allgather.remote() for a in actors])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 186:</b> &nbsp; 4 fragments, nominal size 48 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7251')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_gpu_tests/test_distributed_allgather.py: 72-128
</a>
<div class="mid" id="frag7251" style="display:none"><pre>

def test_allgather_torch_cupy(ray_start_distributed_2_nodes_4_gpus):
    world_size = 4
    shape = [10, 10]
    actors, _ = create_collective_workers(world_size)

    # tensor is pytorch, list is cupy
    for i, a in enumerate(actors):
        t = torch.ones(shape, dtype=torch.float32).cuda() * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = [
            cp.ones(shape, dtype=cp.float32) for _ in range(world_size)
        ]
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_allgather.remote() for a in actors])
    for i in range(world_size):
        for j in range(world_size):
            assert (results[i][j] == cp.ones(shape, dtype=cp.float32) *
                    (j + 1)).all()

    # tensor is cupy, list is pytorch
    for i, a in enumerate(actors):
        t = cp.ones(shape, dtype=cp.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = [
            torch.ones(shape, dtype=torch.float32).cuda()
            for _ in range(world_size)
        ]
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_allgather.remote() for a in actors])
    for i in range(world_size):
        for j in range(world_size):
            assert (results[i][j] == torch.ones(
                shape, dtype=torch.float32).cuda() * (j + 1)).all()

    # some tensors in the list are pytorch, some are cupy
    for i, a in enumerate(actors):
        t = cp.ones(shape, dtype=cp.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = []
        for j in range(world_size):
            if j % 2 == 0:
                list_buffer.append(
                    torch.ones(shape, dtype=torch.float32).cuda())
            else:
                list_buffer.append(cp.ones(shape, dtype=cp.float32))
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_allgather.remote() for a in actors])
    for i in range(world_size):
        for j in range(world_size):
            if j % 2 == 0:
                assert (results[i][j] == torch.ones(
                    shape, dtype=torch.float32).cuda() * (j + 1)).all()
            else:
                assert (results[i][j] == cp.ones(shape, dtype=cp.float32) *
                        (j + 1)).all()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7351')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_cpu_tests/test_allgather.py: 75-129
</a>
<div class="mid" id="frag7351" style="display:none"><pre>
@pytest.mark.parametrize("backend", [Backend.GLOO])
def test_allgather_torch_numpy(ray_start_single_node, backend):
    world_size = 2
    shape = [10, 10]
    actors, _ = create_collective_workers(world_size, backend=backend)

    # tensor is pytorch, list is numpy
    for i, a in enumerate(actors):
        t = torch.ones(shape, dtype=torch.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = [
            np.ones(shape, dtype=np.float32) for _ in range(world_size)
        ]
        ray.wait([a.set_list_buffer.remote(list_buffer, copy=True)])
    results = ray.get([a.do_allgather.remote() for a in actors])
    for i in range(world_size):
        for j in range(world_size):
            assert (results[i][j] == np.ones(shape, dtype=np.float32) *
                    (j + 1)).all()

    # tensor is numpy, list is pytorch
    for i, a in enumerate(actors):
        t = np.ones(shape, dtype=np.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = [
            torch.ones(shape, dtype=torch.float32) for _ in range(world_size)
        ]
        ray.wait([a.set_list_buffer.remote(list_buffer, copy=True)])
    results = ray.get([a.do_allgather.remote() for a in actors])
    for i in range(world_size):
        for j in range(world_size):
            assert (results[i][j] == torch.ones(shape, dtype=torch.float32) *
                    (j + 1)).all()

    # some tensors in the list are pytorch, some are numpy
    for i, a in enumerate(actors):
        t = np.ones(shape, dtype=np.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = []
        for j in range(world_size):
            if j % 2 == 0:
                list_buffer.append(torch.ones(shape, dtype=torch.float32))
            else:
                list_buffer.append(np.ones(shape, dtype=np.float32))
        ray.wait([a.set_list_buffer.remote(list_buffer, copy=True)])
    results = ray.get([a.do_allgather.remote() for a in actors])
    for i in range(world_size):
        for j in range(world_size):
            if j % 2 == 0:
                assert (results[i][j] == torch.ones(
                    shape, dtype=torch.float32) * (j + 1)).all()
            else:
                assert (results[i][j] == np.ones(shape, dtype=np.float32) *
                        (j + 1)).all()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7285')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_gpu_tests/test_allgather.py: 70-126
</a>
<div class="mid" id="frag7285" style="display:none"><pre>

def test_allgather_torch_cupy(ray_start_single_node_2_gpus):
    world_size = 2
    shape = [10, 10]
    actors, _ = create_collective_workers(world_size)

    # tensor is pytorch, list is cupy
    for i, a in enumerate(actors):
        t = torch.ones(shape, dtype=torch.float32).cuda() * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = [
            cp.ones(shape, dtype=cp.float32) for _ in range(world_size)
        ]
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_allgather.remote() for a in actors])
    for i in range(world_size):
        for j in range(world_size):
            assert (results[i][j] == cp.ones(shape, dtype=cp.float32) *
                    (j + 1)).all()

    # tensor is cupy, list is pytorch
    for i, a in enumerate(actors):
        t = cp.ones(shape, dtype=cp.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = [
            torch.ones(shape, dtype=torch.float32).cuda()
            for _ in range(world_size)
        ]
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_allgather.remote() for a in actors])
    for i in range(world_size):
        for j in range(world_size):
            assert (results[i][j] == torch.ones(
                shape, dtype=torch.float32).cuda() * (j + 1)).all()

    # some tensors in the list are pytorch, some are cupy
    for i, a in enumerate(actors):
        t = cp.ones(shape, dtype=cp.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = []
        for j in range(world_size):
            if j % 2 == 0:
                list_buffer.append(
                    torch.ones(shape, dtype=torch.float32).cuda())
            else:
                list_buffer.append(cp.ones(shape, dtype=cp.float32))
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_allgather.remote() for a in actors])
    for i in range(world_size):
        for j in range(world_size):
            if j % 2 == 0:
                assert (results[i][j] == torch.ones(
                    shape, dtype=torch.float32).cuda() * (j + 1)).all()
            else:
                assert (results[i][j] == cp.ones(shape, dtype=cp.float32) *
                        (j + 1)).all()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7426')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_cpu_tests/test_distributed_allgather.py: 79-133
</a>
<div class="mid" id="frag7426" style="display:none"><pre>
@pytest.mark.parametrize("backend", [Backend.GLOO])
def test_allgather_torch_numpy(ray_start_distributed_2_nodes, backend):
    world_size = 8
    shape = [10, 10]
    actors, _ = create_collective_workers(world_size, backend=backend)

    # tensor is pytorch, list is numpy
    for i, a in enumerate(actors):
        t = torch.ones(shape, dtype=torch.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = [
            np.ones(shape, dtype=np.float32) for _ in range(world_size)
        ]
        ray.wait([a.set_list_buffer.remote(list_buffer, copy=True)])
    results = ray.get([a.do_allgather.remote() for a in actors])
    for i in range(world_size):
        for j in range(world_size):
            assert (results[i][j] == np.ones(shape, dtype=np.float32) *
                    (j + 1)).all()

    # tensor is numpy, list is pytorch
    for i, a in enumerate(actors):
        t = np.ones(shape, dtype=np.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = [
            torch.ones(shape, dtype=torch.float32) for _ in range(world_size)
        ]
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_allgather.remote() for a in actors])
    for i in range(world_size):
        for j in range(world_size):
            assert (results[i][j] == torch.ones(shape, dtype=torch.float32) *
                    (j + 1)).all()

    # some tensors in the list are pytorch, some are numpy
    for i, a in enumerate(actors):
        t = np.ones(shape, dtype=np.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = []
        for j in range(world_size):
            if j % 2 == 0:
                list_buffer.append(torch.ones(shape, dtype=torch.float32))
            else:
                list_buffer.append(np.ones(shape, dtype=np.float32))
        ray.wait([a.set_list_buffer.remote(list_buffer, copy=True)])
    results = ray.get([a.do_allgather.remote() for a in actors])
    for i in range(world_size):
        for j in range(world_size):
            if j % 2 == 0:
                assert (results[i][j] == torch.ones(
                    shape, dtype=torch.float32) * (j + 1)).all()
            else:
                assert (results[i][j] == np.ones(shape, dtype=np.float32) *
                        (j + 1)).all()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 187:</b> &nbsp; 4 fragments, nominal size 13 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7252')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_gpu_tests/test_distributed_reduce.py: 12-26
</a>
<div class="mid" id="frag7252" style="display:none"><pre>
def test_reduce_different_name(ray_start_distributed_2_nodes_4_gpus,
                               group_name, dst_rank):
    world_size = 4
    actors, _ = create_collective_workers(
        num_workers=world_size, group_name=group_name)
    results = ray.get(
        [a.do_reduce.remote(group_name, dst_rank) for a in actors])
    for i in range(world_size):
        if i == dst_rank:
            assert (results[i] == cp.ones(
                (10, ), dtype=cp.float32) * world_size).all()
        else:
            assert (results[i] == cp.ones((10, ), dtype=cp.float32)).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7368')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_cpu_tests/test_reduce.py: 13-27
</a>
<div class="mid" id="frag7368" style="display:none"><pre>
def test_reduce_different_name(ray_start_single_node, group_name, dst_rank,
                               backend):
    world_size = 2
    actors, _ = create_collective_workers(
        num_workers=world_size, group_name=group_name, backend=backend)
    results = ray.get(
        [a.do_reduce.remote(group_name, dst_rank) for a in actors])
    for i in range(world_size):
        if i == dst_rank:
            assert (results[i] == np.ones(
                (10, ), dtype=np.float32) * world_size).all()
        else:
            assert (results[i] == np.ones((10, ), dtype=np.float32)).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7302')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_gpu_tests/test_reduce.py: 12-26
</a>
<div class="mid" id="frag7302" style="display:none"><pre>
def test_reduce_different_name(ray_start_single_node_2_gpus, group_name,
                               dst_rank):
    world_size = 2
    actors, _ = create_collective_workers(
        num_workers=world_size, group_name=group_name)
    results = ray.get(
        [a.do_reduce.remote(group_name, dst_rank) for a in actors])
    for i in range(world_size):
        if i == dst_rank:
            assert (results[i] == cp.ones(
                (10, ), dtype=cp.float32) * world_size).all()
        else:
            assert (results[i] == cp.ones((10, ), dtype=cp.float32)).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7427')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_cpu_tests/test_distributed_reduce.py: 13-27
</a>
<div class="mid" id="frag7427" style="display:none"><pre>
def test_reduce_different_name(ray_start_distributed_2_nodes, group_name,
                               backend, dst_rank):
    world_size = 8
    actors, _ = create_collective_workers(
        num_workers=world_size, group_name=group_name, backend=backend)
    results = ray.get(
        [a.do_reduce.remote(group_name, dst_rank) for a in actors])
    for i in range(world_size):
        if i == dst_rank:
            assert (results[i] == np.ones(
                (10, ), dtype=np.float32) * world_size).all()
        else:
            assert (results[i] == np.ones((10, ), dtype=np.float32)).all()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 188:</b> &nbsp; 4 fragments, nominal size 15 lines, similarity 86%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7253')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_gpu_tests/test_distributed_reduce.py: 29-46
</a>
<div class="mid" id="frag7253" style="display:none"><pre>
def test_reduce_different_array_size(ray_start_distributed_2_nodes_4_gpus,
                                     array_size, dst_rank):
    world_size = 4
    actors, _ = create_collective_workers(world_size)
    ray.wait([
        a.set_buffer.remote(cp.ones(array_size, dtype=cp.float32))
        for a in actors
    ])
    results = ray.get([a.do_reduce.remote(dst_rank=dst_rank) for a in actors])
    for i in range(world_size):
        if i == dst_rank:
            assert (results[i] == cp.ones(
                (array_size, ), dtype=cp.float32) * world_size).all()
        else:
            assert (results[i] == cp.ones((array_size, ),
                                          dtype=cp.float32)).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7303')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_gpu_tests/test_reduce.py: 29-46
</a>
<div class="mid" id="frag7303" style="display:none"><pre>
def test_reduce_different_array_size(ray_start_single_node_2_gpus, array_size,
                                     dst_rank):
    world_size = 2
    actors, _ = create_collective_workers(world_size)
    ray.wait([
        a.set_buffer.remote(cp.ones(array_size, dtype=cp.float32))
        for a in actors
    ])
    results = ray.get([a.do_reduce.remote(dst_rank=dst_rank) for a in actors])
    for i in range(world_size):
        if i == dst_rank:
            assert (results[i] == cp.ones(
                (array_size, ), dtype=cp.float32) * world_size).all()
        else:
            assert (results[i] == cp.ones((array_size, ),
                                          dtype=cp.float32)).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7428')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_cpu_tests/test_distributed_reduce.py: 31-48
</a>
<div class="mid" id="frag7428" style="display:none"><pre>
def test_reduce_different_array_size(ray_start_distributed_2_nodes, backend,
                                     array_size, dst_rank):
    world_size = 8
    actors, _ = create_collective_workers(world_size, backend=backend)
    ray.wait([
        a.set_buffer.remote(np.ones(array_size, dtype=np.float32))
        for a in actors
    ])
    results = ray.get([a.do_reduce.remote(dst_rank=dst_rank) for a in actors])
    for i in range(world_size):
        if i == dst_rank:
            assert (results[i] == np.ones(
                (array_size, ), dtype=np.float32) * world_size).all()
        else:
            assert (results[i] == np.ones((array_size, ),
                                          dtype=np.float32)).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7369')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_cpu_tests/test_reduce.py: 31-48
</a>
<div class="mid" id="frag7369" style="display:none"><pre>
def test_reduce_different_array_size(ray_start_single_node, array_size,
                                     dst_rank, backend):
    world_size = 2
    actors, _ = create_collective_workers(world_size, backend=backend)
    ray.wait([
        a.set_buffer.remote(np.ones(array_size, dtype=np.float32))
        for a in actors
    ])
    results = ray.get([a.do_reduce.remote(dst_rank=dst_rank) for a in actors])
    for i in range(world_size):
        if i == dst_rank:
            assert (results[i] == np.ones(
                (array_size, ), dtype=np.float32) * world_size).all()
        else:
            assert (results[i] == np.ones((array_size, ),
                                          dtype=np.float32)).all()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 189:</b> &nbsp; 4 fragments, nominal size 41 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7254')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_gpu_tests/test_distributed_reduce.py: 48-99
</a>
<div class="mid" id="frag7254" style="display:none"><pre>
def test_reduce_different_op(ray_start_distributed_2_nodes_4_gpus, dst_rank):
    world_size = 4
    actors, _ = create_collective_workers(world_size)

    # check product
    ray.wait([
        a.set_buffer.remote(cp.ones(10, dtype=cp.float32) * (i + 2))
        for i, a in enumerate(actors)
    ])
    results = ray.get([
        a.do_reduce.remote(dst_rank=dst_rank, op=ReduceOp.PRODUCT)
        for a in actors
    ])
    for i in range(world_size):
        if i == dst_rank:
            assert (results[i] == cp.ones(
                (10, ), dtype=cp.float32) * 120).all()
        else:
            assert (results[i] == cp.ones(
                (10, ), dtype=cp.float32) * (i + 2)).all()

    # check min
    ray.wait([
        a.set_buffer.remote(cp.ones(10, dtype=cp.float32) * (i + 2))
        for i, a in enumerate(actors)
    ])
    results = ray.get([
        a.do_reduce.remote(dst_rank=dst_rank, op=ReduceOp.MIN) for a in actors
    ])
    for i in range(world_size):
        if i == dst_rank:
            assert (results[i] == cp.ones((10, ), dtype=cp.float32) * 2).all()
        else:
            assert (results[i] == cp.ones(
                (10, ), dtype=cp.float32) * (i + 2)).all()

    # check max
    ray.wait([
        a.set_buffer.remote(cp.ones(10, dtype=cp.float32) * (i + 2))
        for i, a in enumerate(actors)
    ])
    results = ray.get([
        a.do_reduce.remote(dst_rank=dst_rank, op=ReduceOp.MAX) for a in actors
    ])
    for i in range(world_size):
        if i == dst_rank:
            assert (results[i] == cp.ones((10, ), dtype=cp.float32) * 5).all()
        else:
            assert (results[i] == cp.ones(
                (10, ), dtype=cp.float32) * (i + 2)).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7429')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_cpu_tests/test_distributed_reduce.py: 51-106
</a>
<div class="mid" id="frag7429" style="display:none"><pre>
def test_reduce_different_op(ray_start_distributed_2_nodes, backend, dst_rank):
    world_size = 8
    actors, _ = create_collective_workers(world_size, backend=backend)

    # check product
    ray.wait([
        a.set_buffer.remote(np.ones(10, dtype=np.float32) * (i + 2))
        for i, a in enumerate(actors)
    ])
    results = ray.get([
        a.do_reduce.remote(dst_rank=dst_rank, op=ReduceOp.PRODUCT)
        for a in actors
    ])

    product = 1
    for i in range(world_size):
        product = product * (i + 2)
    for i in range(world_size):
        if i == dst_rank:
            assert (results[i] == np.ones(
                (10, ), dtype=np.float32) * product).all()
        else:
            assert (results[i] == np.ones(
                (10, ), dtype=np.float32) * (i + 2)).all()
    # check min
    ray.wait([
        a.set_buffer.remote(np.ones(10, dtype=np.float32) * (i + 2))
        for i, a in enumerate(actors)
    ])
    results = ray.get([
        a.do_reduce.remote(dst_rank=dst_rank, op=ReduceOp.MIN) for a in actors
    ])
    for i in range(world_size):
        if i == dst_rank:
            assert (results[i] == np.ones((10, ), dtype=np.float32) * 2).all()
        else:
            assert (results[i] == np.ones(
                (10, ), dtype=np.float32) * (i + 2)).all()

    # check max
    ray.wait([
        a.set_buffer.remote(np.ones(10, dtype=np.float32) * (i + 2))
        for i, a in enumerate(actors)
    ])
    results = ray.get([
        a.do_reduce.remote(dst_rank=dst_rank, op=ReduceOp.MAX) for a in actors
    ])
    for i in range(world_size):
        if i == dst_rank:
            assert (results[i] == np.ones(
                (10, ), dtype=np.float32) * (world_size + 1)).all()
        else:
            assert (results[i] == np.ones(
                (10, ), dtype=np.float32) * (i + 2)).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7305')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_gpu_tests/test_reduce.py: 73-123
</a>
<div class="mid" id="frag7305" style="display:none"><pre>
def test_reduce_different_op(ray_start_single_node_2_gpus, dst_rank):
    world_size = 2
    actors, _ = create_collective_workers(world_size)

    # check product
    ray.wait([
        a.set_buffer.remote(cp.ones(10, dtype=cp.float32) * (i + 2))
        for i, a in enumerate(actors)
    ])
    results = ray.get([
        a.do_reduce.remote(dst_rank=dst_rank, op=ReduceOp.PRODUCT)
        for a in actors
    ])
    for i in range(world_size):
        if i == dst_rank:
            assert (results[i] == cp.ones((10, ), dtype=cp.float32) * 6).all()
        else:
            assert (results[i] == cp.ones(
                (10, ), dtype=cp.float32) * (i + 2)).all()

    # check min
    ray.wait([
        a.set_buffer.remote(cp.ones(10, dtype=cp.float32) * (i + 2))
        for i, a in enumerate(actors)
    ])
    results = ray.get([
        a.do_reduce.remote(dst_rank=dst_rank, op=ReduceOp.MIN) for a in actors
    ])
    for i in range(world_size):
        if i == dst_rank:
            assert (results[i] == cp.ones((10, ), dtype=cp.float32) * 2).all()
        else:
            assert (results[i] == cp.ones(
                (10, ), dtype=cp.float32) * (i + 2)).all()

    # check max
    ray.wait([
        a.set_buffer.remote(cp.ones(10, dtype=cp.float32) * (i + 2))
        for i, a in enumerate(actors)
    ])
    results = ray.get([
        a.do_reduce.remote(dst_rank=dst_rank, op=ReduceOp.MAX) for a in actors
    ])
    for i in range(world_size):
        if i == dst_rank:
            assert (results[i] == cp.ones((10, ), dtype=cp.float32) * 3).all()
        else:
            assert (results[i] == cp.ones(
                (10, ), dtype=cp.float32) * (i + 2)).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7371')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_cpu_tests/test_reduce.py: 78-128
</a>
<div class="mid" id="frag7371" style="display:none"><pre>
def test_reduce_different_op(ray_start_single_node, dst_rank, backend):
    world_size = 2
    actors, _ = create_collective_workers(world_size, backend=backend)

    # check product
    ray.wait([
        a.set_buffer.remote(np.ones(10, dtype=np.float32) * (i + 2))
        for i, a in enumerate(actors)
    ])
    results = ray.get([
        a.do_reduce.remote(dst_rank=dst_rank, op=ReduceOp.PRODUCT)
        for a in actors
    ])
    for i in range(world_size):
        if i == dst_rank:
            assert (results[i] == np.ones((10, ), dtype=np.float32) * 6).all()
        else:
            assert (results[i] == np.ones(
                (10, ), dtype=np.float32) * (i + 2)).all()

    # check min
    ray.wait([
        a.set_buffer.remote(np.ones(10, dtype=np.float32) * (i + 2))
        for i, a in enumerate(actors)
    ])
    results = ray.get([
        a.do_reduce.remote(dst_rank=dst_rank, op=ReduceOp.MIN) for a in actors
    ])
    for i in range(world_size):
        if i == dst_rank:
            assert (results[i] == np.ones((10, ), dtype=np.float32) * 2).all()
        else:
            assert (results[i] == np.ones(
                (10, ), dtype=np.float32) * (i + 2)).all()

    # check max
    ray.wait([
        a.set_buffer.remote(np.ones(10, dtype=np.float32) * (i + 2))
        for i, a in enumerate(actors)
    ])
    results = ray.get([
        a.do_reduce.remote(dst_rank=dst_rank, op=ReduceOp.MAX) for a in actors
    ])
    for i in range(world_size):
        if i == dst_rank:
            assert (results[i] == np.ones((10, ), dtype=np.float32) * 3).all()
        else:
            assert (results[i] == np.ones(
                (10, ), dtype=np.float32) * (i + 2)).all()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 190:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7255')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_gpu_tests/test_distributed_reduce.py: 101-114
</a>
<div class="mid" id="frag7255" style="display:none"><pre>
def test_reduce_torch_cupy(ray_start_distributed_2_nodes_4_gpus, dst_rank):
    import torch
    world_size = 4
    actors, _ = create_collective_workers(world_size)
    ray.wait([actors[1].set_buffer.remote(torch.ones(10, ).cuda())])
    results = ray.get([a.do_reduce.remote(dst_rank=dst_rank) for a in actors])
    if dst_rank == 0:
        assert (results[0] == cp.ones((10, )) * world_size).all()
        assert (results[1] == torch.ones((10, )).cuda()).all()
    else:
        assert (results[0] == cp.ones((10, ))).all()
        assert (results[1] == torch.ones((10, )).cuda() * world_size).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7306')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_gpu_tests/test_reduce.py: 125-138
</a>
<div class="mid" id="frag7306" style="display:none"><pre>
def test_reduce_torch_cupy(ray_start_single_node_2_gpus, dst_rank):
    import torch
    world_size = 2
    actors, _ = create_collective_workers(world_size)
    ray.wait([actors[1].set_buffer.remote(torch.ones(10, ).cuda())])
    results = ray.get([a.do_reduce.remote(dst_rank=dst_rank) for a in actors])
    if dst_rank == 0:
        assert (results[0] == cp.ones((10, )) * world_size).all()
        assert (results[1] == torch.ones((10, )).cuda()).all()
    else:
        assert (results[0] == cp.ones((10, ))).all()
        assert (results[1] == torch.ones((10, )).cuda() * world_size).all()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 191:</b> &nbsp; 4 fragments, nominal size 12 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7258')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_gpu_tests/test_distributed_allreduce.py: 24-38
</a>
<div class="mid" id="frag7258" style="display:none"><pre>
def test_allreduce_different_array_size(ray_start_distributed_2_nodes_4_gpus,
                                        array_size):
    world_size = 4
    actors, _ = create_collective_workers(world_size)
    ray.wait([
        a.set_buffer.remote(cp.ones(array_size, dtype=cp.float32))
        for a in actors
    ])
    results = ray.get([a.do_allreduce.remote() for a in actors])
    assert (results[0] == cp.ones(
        (array_size, ), dtype=cp.float32) * world_size).all()
    assert (results[1] == cp.ones(
        (array_size, ), dtype=cp.float32) * world_size).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7433')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_cpu_tests/test_distributed_allreduce.py: 27-41
</a>
<div class="mid" id="frag7433" style="display:none"><pre>
def test_allreduce_different_array_size(ray_start_distributed_2_nodes,
                                        array_size, backend):
    world_size = 8
    actors, _ = create_collective_workers(world_size, backend=backend)
    ray.wait([
        a.set_buffer.remote(np.ones(array_size, dtype=np.float32))
        for a in actors
    ])
    results = ray.get([a.do_allreduce.remote() for a in actors])
    assert (results[0] == np.ones(
        (array_size, ), dtype=np.float32) * world_size).all()
    assert (results[1] == np.ones(
        (array_size, ), dtype=np.float32) * world_size).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7287')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_gpu_tests/test_allreduce.py: 21-35
</a>
<div class="mid" id="frag7287" style="display:none"><pre>
def test_allreduce_different_array_size(ray_start_single_node_2_gpus,
                                        array_size):
    world_size = 2
    actors, _ = create_collective_workers(world_size)
    ray.wait([
        a.set_buffer.remote(cp.ones(array_size, dtype=cp.float32))
        for a in actors
    ])
    results = ray.get([a.do_allreduce.remote() for a in actors])
    assert (results[0] == cp.ones(
        (array_size, ), dtype=cp.float32) * world_size).all()
    assert (results[1] == cp.ones(
        (array_size, ), dtype=cp.float32) * world_size).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7353')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_cpu_tests/test_allreduce.py: 23-37
</a>
<div class="mid" id="frag7353" style="display:none"><pre>
def test_allreduce_different_array_size(ray_start_single_node, array_size,
                                        backend):
    world_size = 2
    actors, _ = create_collective_workers(world_size, backend=backend)
    ray.wait([
        a.set_buffer.remote(np.ones(array_size, dtype=np.float32))
        for a in actors
    ])
    results = ray.get([a.do_allreduce.remote() for a in actors])
    assert (results[0] == np.ones(
        (array_size, ), dtype=np.float32) * world_size).all()
    assert (results[1] == np.ones(
        (array_size, ), dtype=np.float32) * world_size).all()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 192:</b> &nbsp; 4 fragments, nominal size 19 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7259')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_gpu_tests/test_distributed_allreduce.py: 39-65
</a>
<div class="mid" id="frag7259" style="display:none"><pre>
def test_allreduce_destroy(ray_start_distributed_2_nodes_4_gpus,
                           backend="nccl",
                           group_name="default"):
    world_size = 4
    actors, _ = create_collective_workers(world_size)

    results = ray.get([a.do_allreduce.remote() for a in actors])
    assert (results[0] == cp.ones((10, ), dtype=cp.float32) * world_size).all()
    assert (results[1] == cp.ones((10, ), dtype=cp.float32) * world_size).all()

    # destroy the group and try do work, should fail
    ray.get([a.destroy_group.remote() for a in actors])
    with pytest.raises(RuntimeError):
        results = ray.get([a.do_allreduce.remote() for a in actors])

    # reinit the same group and all reduce
    ray.get([
        actor.init_group.remote(world_size, i, backend, group_name)
        for i, actor in enumerate(actors)
    ])
    results = ray.get([a.do_allreduce.remote() for a in actors])
    assert (results[0] == cp.ones(
        (10, ), dtype=cp.float32) * world_size * world_size).all()
    assert (results[1] == cp.ones(
        (10, ), dtype=cp.float32) * world_size * world_size).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7288')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_gpu_tests/test_allreduce.py: 36-62
</a>
<div class="mid" id="frag7288" style="display:none"><pre>
def test_allreduce_destroy(ray_start_single_node_2_gpus,
                           backend="nccl",
                           group_name="default"):
    world_size = 2
    actors, _ = create_collective_workers(world_size)

    results = ray.get([a.do_allreduce.remote() for a in actors])
    assert (results[0] == cp.ones((10, ), dtype=cp.float32) * world_size).all()
    assert (results[1] == cp.ones((10, ), dtype=cp.float32) * world_size).all()

    # destroy the group and try do work, should fail
    ray.get([a.destroy_group.remote() for a in actors])
    with pytest.raises(RuntimeError):
        results = ray.get([a.do_allreduce.remote() for a in actors])

    # reinit the same group and all reduce
    ray.get([
        actor.init_group.remote(world_size, i, backend, group_name)
        for i, actor in enumerate(actors)
    ])
    results = ray.get([a.do_allreduce.remote() for a in actors])
    assert (results[0] == cp.ones(
        (10, ), dtype=cp.float32) * world_size * 2).all()
    assert (results[1] == cp.ones(
        (10, ), dtype=cp.float32) * world_size * 2).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7354')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_cpu_tests/test_allreduce.py: 39-65
</a>
<div class="mid" id="frag7354" style="display:none"><pre>
def test_allreduce_destroy(ray_start_single_node,
                           backend,
                           group_name="default"):
    world_size = 2
    actors, _ = create_collective_workers(world_size, backend=backend)

    results = ray.get([a.do_allreduce.remote() for a in actors])
    assert (results[0] == np.ones((10, ), dtype=np.float32) * world_size).all()
    assert (results[1] == np.ones((10, ), dtype=np.float32) * world_size).all()

    # destroy the group and try do work, should fail
    ray.get([a.destroy_group.remote() for a in actors])
    with pytest.raises(RuntimeError):
        results = ray.get([a.do_allreduce.remote() for a in actors])

    # reinit the same group and all reduce
    ray.get([
        actor.init_group.remote(world_size, i, backend, group_name)
        for i, actor in enumerate(actors)
    ])
    results = ray.get([a.do_allreduce.remote() for a in actors])
    assert (results[0] == np.ones(
        (10, ), dtype=np.float32) * world_size * 2).all()
    assert (results[1] == np.ones(
        (10, ), dtype=np.float32) * world_size * 2).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7434')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_cpu_tests/test_distributed_allreduce.py: 43-69
</a>
<div class="mid" id="frag7434" style="display:none"><pre>
def test_allreduce_destroy(ray_start_distributed_2_nodes,
                           backend,
                           group_name="default"):
    world_size = 8
    actors, _ = create_collective_workers(world_size, backend=backend)

    results = ray.get([a.do_allreduce.remote() for a in actors])
    assert (results[0] == np.ones((10, ), dtype=np.float32) * world_size).all()
    assert (results[1] == np.ones((10, ), dtype=np.float32) * world_size).all()

    # destroy the group and try do work, should fail
    ray.get([a.destroy_group.remote() for a in actors])
    with pytest.raises(RuntimeError):
        results = ray.get([a.do_allreduce.remote() for a in actors])

    # reinit the same group and all reduce
    ray.get([
        actor.init_group.remote(world_size, i, backend, group_name)
        for i, actor in enumerate(actors)
    ])
    results = ray.get([a.do_allreduce.remote() for a in actors])
    assert (results[0] == np.ones(
        (10, ), dtype=np.float32) * world_size * world_size).all()
    assert (results[1] == np.ones(
        (10, ), dtype=np.float32) * world_size * world_size).all()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 193:</b> &nbsp; 4 fragments, nominal size 14 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7260')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_gpu_tests/test_distributed_allreduce.py: 66-82
</a>
<div class="mid" id="frag7260" style="display:none"><pre>
def test_allreduce_multiple_group(ray_start_distributed_2_nodes_4_gpus,
                                  backend="nccl",
                                  num_groups=5):
    world_size = 4
    actors, _ = create_collective_workers(world_size)
    for group_name in range(1, num_groups):
        ray.get([
            actor.init_group.remote(world_size, i, backend, str(group_name))
            for i, actor in enumerate(actors)
        ])
    for i in range(num_groups):
        group_name = "default" if i == 0 else str(i)
        results = ray.get([a.do_allreduce.remote(group_name) for a in actors])
        assert (results[0] == cp.ones(
            (10, ), dtype=cp.float32) * (world_size**(i + 1))).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7289')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_gpu_tests/test_allreduce.py: 63-79
</a>
<div class="mid" id="frag7289" style="display:none"><pre>
def test_allreduce_multiple_group(ray_start_single_node_2_gpus,
                                  backend="nccl",
                                  num_groups=5):
    world_size = 2
    actors, _ = create_collective_workers(world_size)
    for group_name in range(1, num_groups):
        ray.get([
            actor.init_group.remote(world_size, i, backend, str(group_name))
            for i, actor in enumerate(actors)
        ])
    for i in range(num_groups):
        group_name = "default" if i == 0 else str(i)
        results = ray.get([a.do_allreduce.remote(group_name) for a in actors])
        assert (results[0] == cp.ones(
            (10, ), dtype=cp.float32) * (world_size**(i + 1))).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7355')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_cpu_tests/test_allreduce.py: 67-82
</a>
<div class="mid" id="frag7355" style="display:none"><pre>
def test_allreduce_multiple_group(ray_start_single_node, backend,
                                  num_groups=5):
    world_size = 2
    actors, _ = create_collective_workers(world_size, backend=backend)
    for group_name in range(1, num_groups):
        ray.get([
            actor.init_group.remote(world_size, i, backend, str(group_name))
            for i, actor in enumerate(actors)
        ])
    for i in range(num_groups):
        group_name = "default" if i == 0 else str(i)
        results = ray.get([a.do_allreduce.remote(group_name) for a in actors])
        assert (results[0] == np.ones(
            (10, ), dtype=np.float32) * (world_size**(i + 1))).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7435')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_cpu_tests/test_distributed_allreduce.py: 71-87
</a>
<div class="mid" id="frag7435" style="display:none"><pre>
def test_allreduce_multiple_group(ray_start_distributed_2_nodes,
                                  backend,
                                  num_groups=5):
    world_size = 8
    actors, _ = create_collective_workers(world_size, backend=backend)
    for group_name in range(1, num_groups):
        ray.get([
            actor.init_group.remote(world_size, i, backend, str(group_name))
            for i, actor in enumerate(actors)
        ])
    for i in range(num_groups):
        group_name = "default" if i == 0 else str(i)
        results = ray.get([a.do_allreduce.remote(group_name) for a in actors])
        assert (results[0] == np.ones(
            (10, ), dtype=np.float32) * (world_size**(i + 1))).all()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 194:</b> &nbsp; 4 fragments, nominal size 23 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7261')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_gpu_tests/test_distributed_allreduce.py: 83-115
</a>
<div class="mid" id="frag7261" style="display:none"><pre>
def test_allreduce_different_op(ray_start_distributed_2_nodes_4_gpus):
    world_size = 4
    actors, _ = create_collective_workers(world_size)

    # check product
    ray.wait([
        a.set_buffer.remote(cp.ones(10, dtype=cp.float32) * (i + 2))
        for i, a in enumerate(actors)
    ])
    results = ray.get(
        [a.do_allreduce.remote(op=ReduceOp.PRODUCT) for a in actors])
    assert (results[0] == cp.ones((10, ), dtype=cp.float32) * 120).all()
    assert (results[1] == cp.ones((10, ), dtype=cp.float32) * 120).all()

    # check min
    ray.wait([
        a.set_buffer.remote(cp.ones(10, dtype=cp.float32) * (i + 2))
        for i, a in enumerate(actors)
    ])
    results = ray.get([a.do_allreduce.remote(op=ReduceOp.MIN) for a in actors])
    assert (results[0] == cp.ones((10, ), dtype=cp.float32) * 2).all()
    assert (results[1] == cp.ones((10, ), dtype=cp.float32) * 2).all()

    # check max
    ray.wait([
        a.set_buffer.remote(cp.ones(10, dtype=cp.float32) * (i + 2))
        for i, a in enumerate(actors)
    ])
    results = ray.get([a.do_allreduce.remote(op=ReduceOp.MAX) for a in actors])
    assert (results[0] == cp.ones((10, ), dtype=cp.float32) * 5).all()
    assert (results[1] == cp.ones((10, ), dtype=cp.float32) * 5).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7436')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_cpu_tests/test_distributed_allreduce.py: 89-124
</a>
<div class="mid" id="frag7436" style="display:none"><pre>
def test_allreduce_different_op(ray_start_distributed_2_nodes, backend):
    world_size = 8
    actors, _ = create_collective_workers(world_size, backend=backend)

    # check product
    ray.wait([
        a.set_buffer.remote(np.ones(10, dtype=np.float32) * (i + 2))
        for i, a in enumerate(actors)
    ])
    results = ray.get(
        [a.do_allreduce.remote(op=ReduceOp.PRODUCT) for a in actors])
    product = 1
    for i in range(world_size):
        product = product * (i + 2)
    assert (results[0] == np.ones((10, ), dtype=np.float32) * product).all()
    assert (results[1] == np.ones((10, ), dtype=np.float32) * product).all()

    # check min
    ray.wait([
        a.set_buffer.remote(np.ones(10, dtype=np.float32) * (i + 2))
        for i, a in enumerate(actors)
    ])
    results = ray.get([a.do_allreduce.remote(op=ReduceOp.MIN) for a in actors])
    assert (results[0] == np.ones((10, ), dtype=np.float32) * 2).all()
    assert (results[1] == np.ones((10, ), dtype=np.float32) * 2).all()

    # check max
    ray.wait([
        a.set_buffer.remote(np.ones(10, dtype=np.float32) * (i + 2))
        for i, a in enumerate(actors)
    ])
    results = ray.get([a.do_allreduce.remote(op=ReduceOp.MAX) for a in actors])
    assert (results[0] == np.ones((10, ), dtype=np.float32) * 9).all()
    assert (results[1] == np.ones((10, ), dtype=np.float32) * 9).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7290')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_gpu_tests/test_allreduce.py: 80-112
</a>
<div class="mid" id="frag7290" style="display:none"><pre>
def test_allreduce_different_op(ray_start_single_node_2_gpus):
    world_size = 2
    actors, _ = create_collective_workers(world_size)

    # check product
    ray.wait([
        a.set_buffer.remote(cp.ones(10, dtype=cp.float32) * (i + 2))
        for i, a in enumerate(actors)
    ])
    results = ray.get(
        [a.do_allreduce.remote(op=ReduceOp.PRODUCT) for a in actors])
    assert (results[0] == cp.ones((10, ), dtype=cp.float32) * 6).all()
    assert (results[1] == cp.ones((10, ), dtype=cp.float32) * 6).all()

    # check min
    ray.wait([
        a.set_buffer.remote(cp.ones(10, dtype=cp.float32) * (i + 2))
        for i, a in enumerate(actors)
    ])
    results = ray.get([a.do_allreduce.remote(op=ReduceOp.MIN) for a in actors])
    assert (results[0] == cp.ones((10, ), dtype=cp.float32) * 2).all()
    assert (results[1] == cp.ones((10, ), dtype=cp.float32) * 2).all()

    # check max
    ray.wait([
        a.set_buffer.remote(cp.ones(10, dtype=cp.float32) * (i + 2))
        for i, a in enumerate(actors)
    ])
    results = ray.get([a.do_allreduce.remote(op=ReduceOp.MAX) for a in actors])
    assert (results[0] == cp.ones((10, ), dtype=cp.float32) * 3).all()
    assert (results[1] == cp.ones((10, ), dtype=cp.float32) * 3).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7356')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_cpu_tests/test_allreduce.py: 84-116
</a>
<div class="mid" id="frag7356" style="display:none"><pre>
def test_allreduce_different_op(ray_start_single_node, backend):
    world_size = 2
    actors, _ = create_collective_workers(world_size, backend=backend)

    # check product
    ray.wait([
        a.set_buffer.remote(np.ones(10, dtype=np.float32) * (i + 2))
        for i, a in enumerate(actors)
    ])
    results = ray.get(
        [a.do_allreduce.remote(op=ReduceOp.PRODUCT) for a in actors])
    assert (results[0] == np.ones((10, ), dtype=np.float32) * 6).all()
    assert (results[1] == np.ones((10, ), dtype=np.float32) * 6).all()

    # check min
    ray.wait([
        a.set_buffer.remote(np.ones(10, dtype=np.float32) * (i + 2))
        for i, a in enumerate(actors)
    ])
    results = ray.get([a.do_allreduce.remote(op=ReduceOp.MIN) for a in actors])
    assert (results[0] == np.ones((10, ), dtype=np.float32) * 2).all()
    assert (results[1] == np.ones((10, ), dtype=np.float32) * 2).all()

    # check max
    ray.wait([
        a.set_buffer.remote(np.ones(10, dtype=np.float32) * (i + 2))
        for i, a in enumerate(actors)
    ])
    results = ray.get([a.do_allreduce.remote(op=ReduceOp.MAX) for a in actors])
    assert (results[0] == np.ones((10, ), dtype=np.float32) * 3).all()
    assert (results[1] == np.ones((10, ), dtype=np.float32) * 3).all()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 195:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7263')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_gpu_tests/test_distributed_allreduce.py: 128-139
</a>
<div class="mid" id="frag7263" style="display:none"><pre>
def test_allreduce_torch_cupy(ray_start_distributed_2_nodes_4_gpus):
    # import torch
    world_size = 4
    actors, _ = create_collective_workers(world_size)
    ray.wait([actors[1].set_buffer.remote(torch.ones(10, ).cuda())])
    results = ray.get([a.do_allreduce.remote() for a in actors])
    assert (results[0] == cp.ones((10, )) * world_size).all()

    ray.wait([actors[0].set_buffer.remote(torch.ones(10, ))])
    ray.wait([actors[1].set_buffer.remote(cp.ones(10, ))])
    with pytest.raises(RuntimeError):
        results = ray.get([a.do_allreduce.remote() for a in actors])
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7292')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_gpu_tests/test_allreduce.py: 124-137
</a>
<div class="mid" id="frag7292" style="display:none"><pre>
def test_allreduce_torch_cupy(ray_start_single_node_2_gpus):
    # import torch
    world_size = 2
    actors, _ = create_collective_workers(world_size)
    ray.wait([actors[1].set_buffer.remote(torch.ones(10, ).cuda())])
    results = ray.get([a.do_allreduce.remote() for a in actors])
    assert (results[0] == cp.ones((10, )) * world_size).all()

    ray.wait([actors[0].set_buffer.remote(torch.ones(10, ))])
    ray.wait([actors[1].set_buffer.remote(cp.ones(10, ))])
    with pytest.raises(RuntimeError):
        results = ray.get([a.do_allreduce.remote() for a in actors])


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 196:</b> &nbsp; 4 fragments, nominal size 14 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7264')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_gpu_tests/test_distributed_reducescatter.py: 14-29
</a>
<div class="mid" id="frag7264" style="display:none"><pre>
                         [2, 2**5, 2**10, 2**15, 2**20, [2, 2], [5, 5, 5]])
def test_reducescatter_different_array_size(
        ray_start_distributed_2_nodes_4_gpus, array_size, tensor_backend):
    world_size = 4
    actors, _ = create_collective_workers(world_size)
    init_tensors_for_gather_scatter(
        actors, array_size=array_size, tensor_backend=tensor_backend)
    results = ray.get([a.do_reducescatter.remote() for a in actors])
    for i in range(world_size):
        if tensor_backend == "cupy":
            assert (results[i] == cp.ones(array_size, dtype=cp.float32) *
                    world_size).all()
        else:
            assert (results[i] == torch.ones(
                array_size, dtype=torch.float32).cuda() * world_size).all()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7439')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_cpu_tests/test_distributed_reducescatter.py: 16-31
</a>
<div class="mid" id="frag7439" style="display:none"><pre>
                         [2, 2**5, 2**10, 2**15, 2**20, [2, 2], [5, 5, 5]])
def test_reducescatter_different_array_size(
        ray_start_distributed_2_nodes, array_size, tensor_backend, backend):
    world_size = 8
    actors, _ = create_collective_workers(world_size, backend=backend)
    init_tensors_for_gather_scatter(
        actors, array_size=array_size, tensor_backend=tensor_backend)
    results = ray.get([a.do_reducescatter.remote() for a in actors])
    for i in range(world_size):
        if tensor_backend == "numpy":
            assert (results[i] == np.ones(array_size, dtype=np.float32) *
                    world_size).all()
        else:
            assert (results[i] == torch.ones(array_size, dtype=torch.float32) *
                    world_size).all()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7299')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_gpu_tests/test_reducescatter.py: 14-29
</a>
<div class="mid" id="frag7299" style="display:none"><pre>
                         [2, 2**5, 2**10, 2**15, 2**20, [2, 2], [5, 5, 5]])
def test_reducescatter_different_array_size(ray_start_single_node_2_gpus,
                                            array_size, tensor_backend):
    world_size = 2
    actors, _ = create_collective_workers(world_size)
    init_tensors_for_gather_scatter(
        actors, array_size=array_size, tensor_backend=tensor_backend)
    results = ray.get([a.do_reducescatter.remote() for a in actors])
    for i in range(world_size):
        if tensor_backend == "cupy":
            assert (results[i] == cp.ones(array_size, dtype=cp.float32) *
                    world_size).all()
        else:
            assert (results[i] == torch.ones(
                array_size, dtype=torch.float32).cuda() * world_size).all()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7365')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_cpu_tests/test_reducescatter.py: 16-31
</a>
<div class="mid" id="frag7365" style="display:none"><pre>
                         [2, 2**5, 2**10, 2**15, 2**20, [2, 2], [5, 5, 5]])
def test_reducescatter_different_array_size(ray_start_single_node, array_size,
                                            tensor_backend, backend):
    world_size = 2
    actors, _ = create_collective_workers(world_size, backend=backend)
    init_tensors_for_gather_scatter(
        actors, array_size=array_size, tensor_backend=tensor_backend)
    results = ray.get([a.do_reducescatter.remote() for a in actors])
    for i in range(world_size):
        if tensor_backend == "numpy":
            assert (results[i] == np.ones(array_size, dtype=np.float32) *
                    world_size).all()
        else:
            assert (results[i] == torch.ones(array_size, dtype=torch.float32) *
                    world_size).all()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 197:</b> &nbsp; 4 fragments, nominal size 69 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7266')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_gpu_tests/test_distributed_reducescatter.py: 43-123
</a>
<div class="mid" id="frag7266" style="display:none"><pre>

def test_reducescatter_torch_cupy(ray_start_distributed_2_nodes_4_gpus):
    world_size = 4
    shape = [10, 10]
    actors, _ = create_collective_workers(world_size)

    # tensor is pytorch, list is cupy
    for i, a in enumerate(actors):
        t = torch.ones(shape, dtype=torch.float32).cuda() * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = [
            cp.ones(shape, dtype=cp.float32) for _ in range(world_size)
        ]
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_reducescatter.remote() for a in actors])
    for i in range(world_size):
        assert (results[i] == torch.ones(shape, dtype=torch.float32).cuda() *
                world_size).all()

    # tensor is cupy, list is pytorch
    for i, a in enumerate(actors):
        t = cp.ones(shape, dtype=cp.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = [
            torch.ones(shape, dtype=torch.float32).cuda()
            for _ in range(world_size)
        ]
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_reducescatter.remote() for a in actors])
    for i in range(world_size):
        assert (
            results[i] == cp.ones(shape, dtype=cp.float32) * world_size).all()

    # some tensors in the list are pytorch, some are cupy
    for i, a in enumerate(actors):
        if i % 2 == 0:
            t = torch.ones(shape, dtype=torch.float32).cuda() * (i + 1)
        else:
            t = cp.ones(shape, dtype=cp.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = []
        for j in range(world_size):
            if j % 2 == 0:
                list_buffer.append(
                    torch.ones(shape, dtype=torch.float32).cuda())
            else:
                list_buffer.append(cp.ones(shape, dtype=cp.float32))
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_reducescatter.remote() for a in actors])
    for i in range(world_size):
        if i % 2 == 0:
            assert (results[i] == torch.ones(
                shape, dtype=torch.float32).cuda() * world_size).all()
        else:
            assert (results[i] == cp.ones(shape, dtype=cp.float32) *
                    world_size).all()

    # mixed case
    for i, a in enumerate(actors):
        if i % 2 == 0:
            t = torch.ones(shape, dtype=torch.float32).cuda() * (i + 1)
        else:
            t = cp.ones(shape, dtype=cp.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = []
        for j in range(world_size):
            if j % 2 == 0:
                list_buffer.append(cp.ones(shape, dtype=cp.float32))
            else:
                list_buffer.append(
                    torch.ones(shape, dtype=torch.float32).cuda())
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_reducescatter.remote() for a in actors])
    for i in range(world_size):
        if i % 2 == 0:
            assert (results[i] == torch.ones(
                shape, dtype=torch.float32).cuda() * world_size).all()
        else:
            assert (results[i] == cp.ones(shape, dtype=cp.float32) *
                    world_size).all()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7367')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_cpu_tests/test_reducescatter.py: 46-123
</a>
<div class="mid" id="frag7367" style="display:none"><pre>
@pytest.mark.parametrize("backend", [Backend.GLOO])
def test_reducescatter_torch_numpy(ray_start_single_node, backend):
    world_size = 2
    shape = [10, 10]
    actors, _ = create_collective_workers(world_size, backend=backend)

    # tensor is pytorch, list is numpy
    for i, a in enumerate(actors):
        t = torch.ones(shape, dtype=torch.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = [
            np.ones(shape, dtype=np.float32) for _ in range(world_size)
        ]
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_reducescatter.remote() for a in actors])
    for i in range(world_size):
        assert (results[i] == torch.ones(shape, dtype=torch.float32) *
                world_size).all()

    # tensor is numpy, list is pytorch
    for i, a in enumerate(actors):
        t = np.ones(shape, dtype=np.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = [
            torch.ones(shape, dtype=torch.float32) for _ in range(world_size)
        ]
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_reducescatter.remote() for a in actors])
    for i in range(world_size):
        assert (
            results[i] == np.ones(shape, dtype=np.float32) * world_size).all()

    # some tensors in the list are pytorch, some are numpy
    for i, a in enumerate(actors):
        if i % 2 == 0:
            t = torch.ones(shape, dtype=torch.float32) * (i + 1)
        else:
            t = np.ones(shape, dtype=np.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = []
        for j in range(world_size):
            if j % 2 == 0:
                list_buffer.append(torch.ones(shape, dtype=torch.float32))
            else:
                list_buffer.append(np.ones(shape, dtype=np.float32))
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_reducescatter.remote() for a in actors])
    for i in range(world_size):
        if i % 2 == 0:
            assert (results[i] == torch.ones(shape, dtype=torch.float32) *
                    world_size).all()
        else:
            assert (results[i] == np.ones(shape, dtype=np.float32) *
                    world_size).all()

    # mixed case
    for i, a in enumerate(actors):
        if i % 2 == 0:
            t = torch.ones(shape, dtype=torch.float32) * (i + 1)
        else:
            t = np.ones(shape, dtype=np.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = []
        for j in range(world_size):
            if j % 2 == 0:
                list_buffer.append(np.ones(shape, dtype=np.float32))
            else:
                list_buffer.append(torch.ones(shape, dtype=torch.float32))
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_reducescatter.remote() for a in actors])
    for i in range(world_size):
        if i % 2 == 0:
            assert (results[i] == torch.ones(shape, dtype=torch.float32) *
                    world_size).all()
        else:
            assert (results[i] == np.ones(shape, dtype=np.float32) *
                    world_size).all()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7301')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_gpu_tests/test_reducescatter.py: 42-122
</a>
<div class="mid" id="frag7301" style="display:none"><pre>

def test_reducescatter_torch_cupy(ray_start_single_node_2_gpus):
    world_size = 2
    shape = [10, 10]
    actors, _ = create_collective_workers(world_size)

    # tensor is pytorch, list is cupy
    for i, a in enumerate(actors):
        t = torch.ones(shape, dtype=torch.float32).cuda() * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = [
            cp.ones(shape, dtype=cp.float32) for _ in range(world_size)
        ]
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_reducescatter.remote() for a in actors])
    for i in range(world_size):
        assert (results[i] == torch.ones(shape, dtype=torch.float32).cuda() *
                world_size).all()

    # tensor is cupy, list is pytorch
    for i, a in enumerate(actors):
        t = cp.ones(shape, dtype=cp.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = [
            torch.ones(shape, dtype=torch.float32).cuda()
            for _ in range(world_size)
        ]
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_reducescatter.remote() for a in actors])
    for i in range(world_size):
        assert (
            results[i] == cp.ones(shape, dtype=cp.float32) * world_size).all()

    # some tensors in the list are pytorch, some are cupy
    for i, a in enumerate(actors):
        if i % 2 == 0:
            t = torch.ones(shape, dtype=torch.float32).cuda() * (i + 1)
        else:
            t = cp.ones(shape, dtype=cp.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = []
        for j in range(world_size):
            if j % 2 == 0:
                list_buffer.append(
                    torch.ones(shape, dtype=torch.float32).cuda())
            else:
                list_buffer.append(cp.ones(shape, dtype=cp.float32))
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_reducescatter.remote() for a in actors])
    for i in range(world_size):
        if i % 2 == 0:
            assert (results[i] == torch.ones(
                shape, dtype=torch.float32).cuda() * world_size).all()
        else:
            assert (results[i] == cp.ones(shape, dtype=cp.float32) *
                    world_size).all()

    # mixed case
    for i, a in enumerate(actors):
        if i % 2 == 0:
            t = torch.ones(shape, dtype=torch.float32).cuda() * (i + 1)
        else:
            t = cp.ones(shape, dtype=cp.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = []
        for j in range(world_size):
            if j % 2 == 0:
                list_buffer.append(cp.ones(shape, dtype=cp.float32))
            else:
                list_buffer.append(
                    torch.ones(shape, dtype=torch.float32).cuda())
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_reducescatter.remote() for a in actors])
    for i in range(world_size):
        if i % 2 == 0:
            assert (results[i] == torch.ones(
                shape, dtype=torch.float32).cuda() * world_size).all()
        else:
            assert (results[i] == cp.ones(shape, dtype=cp.float32) *
                    world_size).all()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7441')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_cpu_tests/test_distributed_reducescatter.py: 47-124
</a>
<div class="mid" id="frag7441" style="display:none"><pre>
@pytest.mark.parametrize("backend", [Backend.GLOO])
def test_reducescatter_torch_numpy(ray_start_distributed_2_nodes, backend):
    world_size = 8
    shape = [10, 10]
    actors, _ = create_collective_workers(world_size, backend=backend)

    # tensor is pytorch, list is numpy
    for i, a in enumerate(actors):
        t = torch.ones(shape, dtype=torch.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = [
            np.ones(shape, dtype=np.float32) for _ in range(world_size)
        ]
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_reducescatter.remote() for a in actors])
    for i in range(world_size):
        assert (results[i] == torch.ones(shape, dtype=torch.float32) *
                world_size).all()

    # tensor is numpy, list is pytorch
    for i, a in enumerate(actors):
        t = np.ones(shape, dtype=np.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = [
            torch.ones(shape, dtype=torch.float32) for _ in range(world_size)
        ]
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_reducescatter.remote() for a in actors])
    for i in range(world_size):
        assert (
            results[i] == np.ones(shape, dtype=np.float32) * world_size).all()

    # some tensors in the list are pytorch, some are numpy
    for i, a in enumerate(actors):
        if i % 2 == 0:
            t = torch.ones(shape, dtype=torch.float32) * (i + 1)
        else:
            t = np.ones(shape, dtype=np.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = []
        for j in range(world_size):
            if j % 2 == 0:
                list_buffer.append(torch.ones(shape, dtype=torch.float32))
            else:
                list_buffer.append(np.ones(shape, dtype=np.float32))
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_reducescatter.remote() for a in actors])
    for i in range(world_size):
        if i % 2 == 0:
            assert (results[i] == torch.ones(shape, dtype=torch.float32) *
                    world_size).all()
        else:
            assert (results[i] == np.ones(shape, dtype=np.float32) *
                    world_size).all()

    # mixed case
    for i, a in enumerate(actors):
        if i % 2 == 0:
            t = torch.ones(shape, dtype=torch.float32) * (i + 1)
        else:
            t = np.ones(shape, dtype=np.float32) * (i + 1)
        ray.wait([a.set_buffer.remote(t)])
        list_buffer = []
        for j in range(world_size):
            if j % 2 == 0:
                list_buffer.append(np.ones(shape, dtype=np.float32))
            else:
                list_buffer.append(torch.ones(shape, dtype=torch.float32))
        ray.wait([a.set_list_buffer.remote(list_buffer)])
    results = ray.get([a.do_reducescatter.remote() for a in actors])
    for i in range(world_size):
        if i % 2 == 0:
            assert (results[i] == torch.ones(shape, dtype=torch.float32) *
                    world_size).all()
        else:
            assert (results[i] == np.ones(shape, dtype=np.float32) *
                    world_size).all()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 198:</b> &nbsp; 4 fragments, nominal size 11 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7268')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_gpu_tests/test_distributed_basic_apis.py: 19-32
</a>
<div class="mid" id="frag7268" style="display:none"><pre>
@pytest.mark.parametrize("world_size", [2, 3, 4])
def test_init_multiple_groups(ray_start_distributed_2_nodes_4_gpus,
                              world_size):
    num_groups = 1
    actors = [Worker.remote() for _ in range(world_size)]
    for i in range(num_groups):
        group_name = str(i)
        init_results = ray.get([
            actor.init_group.remote(world_size, k, group_name=group_name)
            for k, actor in enumerate(actors)
        ])
        for j in range(world_size):
            assert init_results[j]

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7294')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_gpu_tests/test_basic_apis.py: 16-29
</a>
<div class="mid" id="frag7294" style="display:none"><pre>

def test_init_multiple_groups(ray_start_single_node_2_gpus):
    world_size = 2
    num_groups = 10
    actors = [Worker.remote() for i in range(world_size)]
    for i in range(num_groups):
        group_name = str(i)
        init_results = ray.get([
            actor.init_group.remote(world_size, k, group_name=group_name)
            for k, actor in enumerate(actors)
        ])
        for j in range(world_size):
            assert init_results[j]

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7443')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_cpu_tests/test_distributed_basic_apis.py: 23-37
</a>
<div class="mid" id="frag7443" style="display:none"><pre>
@pytest.mark.parametrize("world_size", [2, 3, 4])
def test_init_multiple_groups(ray_start_distributed_2_nodes, world_size,
                              backend):
    num_groups = 5
    actors = [Worker.remote() for _ in range(world_size)]
    for i in range(num_groups):
        group_name = str(i)
        init_results = ray.get([
            actor.init_group.remote(
                world_size, i, group_name=group_name, backend=backend)
            for i, actor in enumerate(actors)
        ])
        for j in range(world_size):
            assert init_results[j]

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7360')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_cpu_tests/test_basic_apis.py: 20-34
</a>
<div class="mid" id="frag7360" style="display:none"><pre>
@pytest.mark.parametrize("backend", [Backend.GLOO])
def test_init_multiple_groups(ray_start_single_node, backend):
    world_size = 2
    num_groups = 10
    actors = [Worker.remote() for i in range(world_size)]
    for i in range(num_groups):
        group_name = str(i)
        init_results = ray.get([
            actor.init_group.remote(
                world_size, k, group_name=group_name, backend=backend)
            for k, actor in enumerate(actors)
        ])
        for j in range(world_size):
            assert init_results[j]

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 199:</b> &nbsp; 4 fragments, nominal size 17 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7269')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_gpu_tests/test_distributed_basic_apis.py: 34-56
</a>
<div class="mid" id="frag7269" style="display:none"><pre>
@pytest.mark.parametrize("world_size", [2, 3, 4])
def test_get_rank(ray_start_distributed_2_nodes_4_gpus, world_size):
    actors, _ = create_collective_workers(world_size)
    actor0_rank = ray.get(actors[0].report_rank.remote())
    assert actor0_rank == 0
    actor1_rank = ray.get(actors[1].report_rank.remote())
    assert actor1_rank == 1

    # create a second group with a different name, and different
    # orders of ranks.
    new_group_name = "default2"
    ranks = list(range(world_size))
    shuffle(ranks)
    ray.get([
        actor.init_group.remote(
            world_size, ranks[i], group_name=new_group_name)
        for i, actor in enumerate(actors)
    ])
    actor0_rank = ray.get(actors[0].report_rank.remote(new_group_name))
    assert actor0_rank == ranks[0]
    actor1_rank = ray.get(actors[1].report_rank.remote(new_group_name))
    assert actor1_rank == ranks[1]

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7313')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_multigpu_tests/test_distributed_multigpu_basic_apis.py: 26-49
</a>
<div class="mid" id="frag7313" style="display:none"><pre>
def test_get_rank(ray_start_distributed_multigpu_2_nodes_4_gpus):
    world_size = 2
    actors, _ = create_collective_multigpu_workers(world_size)
    actor0_rank = ray.get(actors[0].report_rank.remote())
    assert actor0_rank == 0
    actor1_rank = ray.get(actors[1].report_rank.remote())
    assert actor1_rank == 1

    # create a second group with a different name, and different
    # orders of ranks.
    new_group_name = "default2"
    ranks = list(range(world_size))
    shuffle(ranks)
    ray.get([
        actor.init_group.remote(
            world_size, ranks[i], group_name=new_group_name)
        for i, actor in enumerate(actors)
    ])
    actor0_rank = ray.get(actors[0].report_rank.remote(new_group_name))
    assert actor0_rank == ranks[0]
    actor1_rank = ray.get(actors[1].report_rank.remote(new_group_name))
    assert actor1_rank == ranks[1]


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7444')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_cpu_tests/test_distributed_basic_apis.py: 40-62
</a>
<div class="mid" id="frag7444" style="display:none"><pre>
@pytest.mark.parametrize("world_size", [5, 6, 7, 8])
def test_get_rank(ray_start_distributed_2_nodes, world_size, backend):
    actors, _ = create_collective_workers(world_size, backend=backend)
    actor0_rank = ray.get(actors[0].report_rank.remote())
    assert actor0_rank == 0
    actor1_rank = ray.get(actors[1].report_rank.remote())
    assert actor1_rank == 1

    # create a second group with a different name, and different
    # orders of ranks.
    new_group_name = "default2"
    ranks = list(range(world_size))
    shuffle(ranks)
    _ = ray.get([
        actor.init_group.remote(
            world_size, ranks[i], group_name=new_group_name, backend=backend)
        for i, actor in enumerate(actors)
    ])
    actor0_rank = ray.get(actors[0].report_rank.remote(new_group_name))
    assert actor0_rank == ranks[0]
    actor1_rank = ray.get(actors[1].report_rank.remote(new_group_name))
    assert actor1_rank == ranks[1]

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7295')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_gpu_tests/test_basic_apis.py: 30-51
</a>
<div class="mid" id="frag7295" style="display:none"><pre>

def test_get_rank(ray_start_single_node_2_gpus):
    world_size = 2
    actors, _ = create_collective_workers(world_size)
    actor0_rank = ray.get(actors[0].report_rank.remote())
    assert actor0_rank == 0
    actor1_rank = ray.get(actors[1].report_rank.remote())
    assert actor1_rank == 1

    # create a second group with a different name,
    # and different order of ranks.
    new_group_name = "default2"
    ray.get([
        actor.init_group.remote(
            world_size, world_size - 1 - i, group_name=new_group_name)
        for i, actor in enumerate(actors)
    ])
    actor0_rank = ray.get(actors[0].report_rank.remote(new_group_name))
    assert actor0_rank == 1
    actor1_rank = ray.get(actors[1].report_rank.remote(new_group_name))
    assert actor1_rank == 0

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 200:</b> &nbsp; 5 fragments, nominal size 16 lines, similarity 87%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7271')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_gpu_tests/test_distributed_basic_apis.py: 66-84
</a>
<div class="mid" id="frag7271" style="display:none"><pre>

def test_is_group_initialized(ray_start_distributed_2_nodes_4_gpus):
    world_size = 4
    actors, _ = create_collective_workers(world_size)
    # check group is_init
    actor0_is_init = ray.get(actors[0].report_is_group_initialized.remote())
    assert actor0_is_init
    actor0_is_init = ray.get(
        actors[0].report_is_group_initialized.remote("random"))
    assert not actor0_is_init
    actor0_is_init = ray.get(
        actors[0].report_is_group_initialized.remote("123"))
    assert not actor0_is_init
    actor1_is_init = ray.get(actors[0].report_is_group_initialized.remote())
    assert actor1_is_init
    actor1_is_init = ray.get(
        actors[0].report_is_group_initialized.remote("456"))
    assert not actor1_is_init

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7297')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_gpu_tests/test_basic_apis.py: 60-78
</a>
<div class="mid" id="frag7297" style="display:none"><pre>

def test_is_group_initialized(ray_start_single_node_2_gpus):
    world_size = 2
    actors, _ = create_collective_workers(world_size)
    # check group is_init
    actor0_is_init = ray.get(actors[0].report_is_group_initialized.remote())
    assert actor0_is_init
    actor0_is_init = ray.get(
        actors[0].report_is_group_initialized.remote("random"))
    assert not actor0_is_init
    actor0_is_init = ray.get(
        actors[0].report_is_group_initialized.remote("123"))
    assert not actor0_is_init
    actor1_is_init = ray.get(actors[0].report_is_group_initialized.remote())
    assert actor1_is_init
    actor1_is_init = ray.get(
        actors[0].report_is_group_initialized.remote("456"))
    assert not actor1_is_init

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7363')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_cpu_tests/test_basic_apis.py: 70-88
</a>
<div class="mid" id="frag7363" style="display:none"><pre>
@pytest.mark.parametrize("backend", [Backend.GLOO])
def test_is_group_initialized(ray_start_single_node, backend):
    world_size = 2
    actors, _ = create_collective_workers(world_size, backend=backend)
    # check group is_init
    actor0_is_init = ray.get(actors[0].report_is_group_initialized.remote())
    assert actor0_is_init
    actor0_is_init = ray.get(
        actors[0].report_is_group_initialized.remote("random"))
    assert not actor0_is_init
    actor0_is_init = ray.get(
        actors[0].report_is_group_initialized.remote("123"))
    assert not actor0_is_init
    actor1_is_init = ray.get(actors[0].report_is_group_initialized.remote())
    assert actor1_is_init
    actor1_is_init = ray.get(
        actors[0].report_is_group_initialized.remote("456"))
    assert not actor1_is_init

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7446')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_cpu_tests/test_distributed_basic_apis.py: 73-91
</a>
<div class="mid" id="frag7446" style="display:none"><pre>
@pytest.mark.parametrize("backend", [Backend.GLOO])
def test_is_group_initialized(ray_start_distributed_2_nodes, backend):
    world_size = 8
    actors, _ = create_collective_workers(world_size, backend=backend)
    # check group is_init
    actor0_is_init = ray.get(actors[0].report_is_group_initialized.remote())
    assert actor0_is_init
    actor0_is_init = ray.get(
        actors[0].report_is_group_initialized.remote("random"))
    assert not actor0_is_init
    actor0_is_init = ray.get(
        actors[0].report_is_group_initialized.remote("123"))
    assert not actor0_is_init
    actor1_is_init = ray.get(actors[0].report_is_group_initialized.remote())
    assert actor1_is_init
    actor1_is_init = ray.get(
        actors[0].report_is_group_initialized.remote("456"))
    assert not actor1_is_init

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7314')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_multigpu_tests/test_distributed_multigpu_basic_apis.py: 50-68
</a>
<div class="mid" id="frag7314" style="display:none"><pre>
def test_is_group_initialized(ray_start_distributed_multigpu_2_nodes_4_gpus):
    world_size = 2
    actors, _ = create_collective_multigpu_workers(world_size)
    # check group is_init
    actor0_is_init = ray.get(actors[0].report_is_group_initialized.remote())
    assert actor0_is_init
    actor0_is_init = ray.get(
        actors[0].report_is_group_initialized.remote("random"))
    assert not actor0_is_init
    actor0_is_init = ray.get(
        actors[0].report_is_group_initialized.remote("123"))
    assert not actor0_is_init
    actor1_is_init = ray.get(actors[0].report_is_group_initialized.remote())
    assert actor1_is_init
    actor1_is_init = ray.get(
        actors[0].report_is_group_initialized.remote("456"))
    assert not actor1_is_init


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 201:</b> &nbsp; 5 fragments, nominal size 25 lines, similarity 84%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7272')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_gpu_tests/test_distributed_basic_apis.py: 85-119
</a>
<div class="mid" id="frag7272" style="display:none"><pre>

def test_destroy_group(ray_start_distributed_2_nodes_4_gpus):
    world_size = 4
    actors, _ = create_collective_workers(world_size)
    # Now destroy the group at actor0
    ray.wait([actors[0].destroy_group.remote()])
    actor0_is_init = ray.get(actors[0].report_is_group_initialized.remote())
    assert not actor0_is_init

    # should go well as the group `random` does not exist at all
    ray.wait([actors[0].destroy_group.remote("random")])

    actor1_is_init = ray.get(actors[1].report_is_group_initialized.remote())
    assert actor1_is_init
    ray.wait([actors[1].destroy_group.remote("random")])
    actor1_is_init = ray.get(actors[1].report_is_group_initialized.remote())
    assert actor1_is_init
    ray.wait([actors[1].destroy_group.remote("default")])
    actor1_is_init = ray.get(actors[1].report_is_group_initialized.remote())
    assert not actor1_is_init
    for i in [2, 3]:
        ray.wait([actors[i].destroy_group.remote("default")])

    # Now reconstruct the group using the same name
    init_results = ray.get([
        actor.init_group.remote(world_size, i)
        for i, actor in enumerate(actors)
    ])
    for i in range(world_size):
        assert init_results[i]
    actor0_is_init = ray.get(actors[0].report_is_group_initialized.remote())
    assert actor0_is_init
    actor1_is_init = ray.get(actors[0].report_is_group_initialized.remote())
    assert actor1_is_init

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7447')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_cpu_tests/test_distributed_basic_apis.py: 93-127
</a>
<div class="mid" id="frag7447" style="display:none"><pre>
@pytest.mark.parametrize("backend", [Backend.GLOO])
def test_destroy_group(ray_start_distributed_2_nodes, backend):
    world_size = 8
    actors, _ = create_collective_workers(world_size, backend=backend)
    # Now destroy the group at actor0
    ray.wait([actors[0].destroy_group.remote()])
    actor0_is_init = ray.get(actors[0].report_is_group_initialized.remote())
    assert not actor0_is_init

    # should go well as the group `random` does not exist at all
    ray.wait([actors[0].destroy_group.remote("random")])

    actor1_is_init = ray.get(actors[1].report_is_group_initialized.remote())
    assert actor1_is_init
    ray.wait([actors[1].destroy_group.remote("random")])
    actor1_is_init = ray.get(actors[1].report_is_group_initialized.remote())
    assert actor1_is_init
    ray.wait([actors[1].destroy_group.remote("default")])
    actor1_is_init = ray.get(actors[1].report_is_group_initialized.remote())
    assert not actor1_is_init
    for i in range(2, world_size):
        ray.wait([actors[i].destroy_group.remote("default")])

    # Now reconstruct the group using the same name
    init_results = ray.get([
        actor.init_group.remote(world_size, i, backend=backend)
        for i, actor in enumerate(actors)
    ])
    for i in range(world_size):
        assert init_results[i]
    actor0_is_init = ray.get(actors[0].report_is_group_initialized.remote())
    assert actor0_is_init
    actor1_is_init = ray.get(actors[0].report_is_group_initialized.remote())
    assert actor1_is_init

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7315')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_multigpu_tests/test_distributed_multigpu_basic_apis.py: 69-101
</a>
<div class="mid" id="frag7315" style="display:none"><pre>
def test_destroy_group(ray_start_distributed_multigpu_2_nodes_4_gpus):
    world_size = 2
    actors, _ = create_collective_multigpu_workers(world_size)
    # Now destroy the group at actor0
    ray.wait([actors[0].destroy_group.remote()])
    actor0_is_init = ray.get(actors[0].report_is_group_initialized.remote())
    assert not actor0_is_init

    # should go well as the group `random` does not exist at all
    ray.wait([actors[0].destroy_group.remote("random")])

    actor1_is_init = ray.get(actors[1].report_is_group_initialized.remote())
    assert actor1_is_init
    ray.wait([actors[1].destroy_group.remote("random")])
    actor1_is_init = ray.get(actors[1].report_is_group_initialized.remote())
    assert actor1_is_init
    ray.wait([actors[1].destroy_group.remote("default")])
    actor1_is_init = ray.get(actors[1].report_is_group_initialized.remote())
    assert not actor1_is_init

    # Now reconstruct the group using the same name
    init_results = ray.get([
        actor.init_group.remote(world_size, i)
        for i, actor in enumerate(actors)
    ])
    for i in range(world_size):
        assert init_results[i]
    actor0_is_init = ray.get(actors[0].report_is_group_initialized.remote())
    assert actor0_is_init
    actor1_is_init = ray.get(actors[0].report_is_group_initialized.remote())
    assert actor1_is_init


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7298')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_gpu_tests/test_basic_apis.py: 79-111
</a>
<div class="mid" id="frag7298" style="display:none"><pre>

def test_destroy_group(ray_start_single_node_2_gpus):
    world_size = 2
    actors, _ = create_collective_workers(world_size)
    # Now destroy the group at actor0
    ray.wait([actors[0].destroy_group.remote()])
    actor0_is_init = ray.get(actors[0].report_is_group_initialized.remote())
    assert not actor0_is_init

    # should go well as the group `random` does not exist at all
    ray.wait([actors[0].destroy_group.remote("random")])

    actor1_is_init = ray.get(actors[1].report_is_group_initialized.remote())
    assert actor1_is_init
    ray.wait([actors[1].destroy_group.remote("random")])
    actor1_is_init = ray.get(actors[1].report_is_group_initialized.remote())
    assert actor1_is_init
    ray.wait([actors[1].destroy_group.remote("default")])
    actor1_is_init = ray.get(actors[1].report_is_group_initialized.remote())
    assert not actor1_is_init

    # Now reconstruct the group using the same name
    init_results = ray.get([
        actor.init_group.remote(world_size, i)
        for i, actor in enumerate(actors)
    ])
    for i in range(world_size):
        assert init_results[i]
    actor0_is_init = ray.get(actors[0].report_is_group_initialized.remote())
    assert actor0_is_init
    actor1_is_init = ray.get(actors[0].report_is_group_initialized.remote())
    assert actor1_is_init

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7364')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_cpu_tests/test_basic_apis.py: 90-122
</a>
<div class="mid" id="frag7364" style="display:none"><pre>
@pytest.mark.parametrize("backend", [Backend.GLOO])
def test_destroy_group(ray_start_single_node, backend):
    world_size = 2
    actors, _ = create_collective_workers(world_size, backend=backend)
    # Now destroy the group at actor0
    ray.wait([actors[0].destroy_group.remote()])
    actor0_is_init = ray.get(actors[0].report_is_group_initialized.remote())
    assert not actor0_is_init

    # should go well as the group `random` does not exist at all
    ray.wait([actors[0].destroy_group.remote("random")])

    actor1_is_init = ray.get(actors[1].report_is_group_initialized.remote())
    assert actor1_is_init
    ray.wait([actors[1].destroy_group.remote("random")])
    actor1_is_init = ray.get(actors[1].report_is_group_initialized.remote())
    assert actor1_is_init
    ray.wait([actors[1].destroy_group.remote("default")])
    actor1_is_init = ray.get(actors[1].report_is_group_initialized.remote())
    assert not actor1_is_init

    # Now reconstruct the group using the same name
    init_results = ray.get([
        actor.init_group.remote(world_size, i)
        for i, actor in enumerate(actors)
    ])
    for i in range(world_size):
        assert init_results[i]
    actor0_is_init = ray.get(actors[0].report_is_group_initialized.remote())
    assert actor0_is_init
    actor1_is_init = ray.get(actors[0].report_is_group_initialized.remote())
    assert actor1_is_init

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 202:</b> &nbsp; 4 fragments, nominal size 14 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7273')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_gpu_tests/test_distributed_broadcast.py: 11-28
</a>
<div class="mid" id="frag7273" style="display:none"><pre>
def test_broadcast_different_name(ray_start_distributed_2_nodes_4_gpus,
                                  group_name, src_rank):
    world_size = 4
    actors, _ = create_collective_workers(
        num_workers=world_size, group_name=group_name)
    ray.wait([
        a.set_buffer.remote(cp.ones((10, ), dtype=cp.float32) * (i + 2))
        for i, a in enumerate(actors)
    ])
    results = ray.get([
        a.do_broadcast.remote(group_name=group_name, src_rank=src_rank)
        for a in actors
    ])
    for i in range(world_size):
        assert (results[i] == cp.ones(
            (10, ), dtype=cp.float32) * (src_rank + 2)).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7448')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_cpu_tests/test_distributed_broadcast.py: 13-30
</a>
<div class="mid" id="frag7448" style="display:none"><pre>
def test_broadcast_different_name(ray_start_distributed_2_nodes, group_name,
                                  src_rank, backend):
    world_size = 8
    actors, _ = create_collective_workers(
        num_workers=world_size, group_name=group_name, backend=backend)
    ray.wait([
        a.set_buffer.remote(np.ones((10, ), dtype=np.float32) * (i + 2))
        for i, a in enumerate(actors)
    ])
    results = ray.get([
        a.do_broadcast.remote(group_name=group_name, src_rank=src_rank)
        for a in actors
    ])
    for i in range(world_size):
        assert (results[i] == np.ones(
            (10, ), dtype=np.float32) * (src_rank + 2)).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7343')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_cpu_tests/test_broadcast.py: 13-30
</a>
<div class="mid" id="frag7343" style="display:none"><pre>
def test_broadcast_different_name(ray_start_single_node, group_name, src_rank,
                                  backend):
    world_size = 2
    actors, _ = create_collective_workers(
        num_workers=world_size, group_name=group_name, backend=backend)
    ray.get([
        a.set_buffer.remote(np.ones((10, ), dtype=np.float32) * (i + 2))
        for i, a in enumerate(actors)
    ])
    results = ray.get([
        a.do_broadcast.remote(group_name=group_name, src_rank=src_rank)
        for a in actors
    ])
    for i in range(world_size):
        assert (results[i] == np.ones(
            (10, ), dtype=np.float32) * (src_rank + 2)).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7277')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_gpu_tests/test_broadcast.py: 11-28
</a>
<div class="mid" id="frag7277" style="display:none"><pre>
def test_broadcast_different_name(ray_start_single_node_2_gpus, group_name,
                                  src_rank):
    world_size = 2
    actors, _ = create_collective_workers(
        num_workers=world_size, group_name=group_name)
    ray.wait([
        a.set_buffer.remote(cp.ones((10, ), dtype=cp.float32) * (i + 2))
        for i, a in enumerate(actors)
    ])
    results = ray.get([
        a.do_broadcast.remote(group_name=group_name, src_rank=src_rank)
        for a in actors
    ])
    for i in range(world_size):
        assert (results[i] == cp.ones(
            (10, ), dtype=cp.float32) * (src_rank + 2)).all()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 203:</b> &nbsp; 4 fragments, nominal size 12 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7274')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_gpu_tests/test_distributed_broadcast.py: 31-45
</a>
<div class="mid" id="frag7274" style="display:none"><pre>
def test_broadcast_different_array_size(ray_start_distributed_2_nodes_4_gpus,
                                        array_size, src_rank):
    world_size = 4
    actors, _ = create_collective_workers(world_size)
    ray.wait([
        a.set_buffer.remote(cp.ones(array_size, dtype=cp.float32) * (i + 2))
        for i, a in enumerate(actors)
    ])
    results = ray.get(
        [a.do_broadcast.remote(src_rank=src_rank) for a in actors])
    for i in range(world_size):
        assert (results[i] == cp.ones(
            (array_size, ), dtype=cp.float32) * (src_rank + 2)).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7278')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_gpu_tests/test_broadcast.py: 31-45
</a>
<div class="mid" id="frag7278" style="display:none"><pre>
def test_broadcast_different_array_size(ray_start_single_node_2_gpus,
                                        array_size, src_rank):
    world_size = 2
    actors, _ = create_collective_workers(world_size)
    ray.wait([
        a.set_buffer.remote(cp.ones(array_size, dtype=cp.float32) * (i + 2))
        for i, a in enumerate(actors)
    ])
    results = ray.get(
        [a.do_broadcast.remote(src_rank=src_rank) for a in actors])
    for i in range(world_size):
        assert (results[i] == cp.ones(
            (array_size, ), dtype=cp.float32) * (src_rank + 2)).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7344')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_cpu_tests/test_broadcast.py: 34-48
</a>
<div class="mid" id="frag7344" style="display:none"><pre>
def test_broadcast_different_array_size(ray_start_single_node, array_size,
                                        src_rank, backend):
    world_size = 2
    actors, _ = create_collective_workers(world_size, backend=backend)
    ray.get([
        a.set_buffer.remote(np.ones(array_size, dtype=np.float32) * (i + 2))
        for i, a in enumerate(actors)
    ])
    results = ray.get(
        [a.do_broadcast.remote(src_rank=src_rank) for a in actors])
    for i in range(world_size):
        assert (results[i] == np.ones(
            (array_size, ), dtype=np.float32) * (src_rank + 2)).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7449')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_cpu_tests/test_distributed_broadcast.py: 34-48
</a>
<div class="mid" id="frag7449" style="display:none"><pre>
def test_broadcast_different_array_size(ray_start_distributed_2_nodes,
                                        array_size, src_rank, backend):
    world_size = 8
    actors, _ = create_collective_workers(world_size, backend=backend)
    ray.wait([
        a.set_buffer.remote(np.ones(array_size, dtype=np.float32) * (i + 2))
        for i, a in enumerate(actors)
    ])
    results = ray.get(
        [a.do_broadcast.remote(src_rank=src_rank) for a in actors])
    for i in range(world_size):
        assert (results[i] == np.ones(
            (array_size, ), dtype=np.float32) * (src_rank + 2)).all()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 204:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7275')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_gpu_tests/test_distributed_broadcast.py: 47-62
</a>
<div class="mid" id="frag7275" style="display:none"><pre>
def test_broadcast_torch_cupy(ray_start_distributed_2_nodes_4_gpus, src_rank):
    import torch
    world_size = 4
    actors, _ = create_collective_workers(world_size)
    ray.wait(
        [actors[1].set_buffer.remote(torch.ones(10, ).cuda() * world_size)])
    results = ray.get(
        [a.do_broadcast.remote(src_rank=src_rank) for a in actors])
    if src_rank == 0:
        assert (results[0] == cp.ones((10, ))).all()
        assert (results[1] == torch.ones((10, )).cuda()).all()
    else:
        assert (results[0] == cp.ones((10, )) * world_size).all()
        assert (results[1] == torch.ones((10, )).cuda() * world_size).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7279')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_gpu_tests/test_broadcast.py: 47-62
</a>
<div class="mid" id="frag7279" style="display:none"><pre>
def test_broadcast_torch_cupy(ray_start_single_node_2_gpus, src_rank):
    import torch
    world_size = 2
    actors, _ = create_collective_workers(world_size)
    ray.wait(
        [actors[1].set_buffer.remote(torch.ones(10, ).cuda() * world_size)])
    results = ray.get(
        [a.do_broadcast.remote(src_rank=src_rank) for a in actors])
    if src_rank == 0:
        assert (results[0] == cp.ones((10, ))).all()
        assert (results[1] == torch.ones((10, )).cuda()).all()
    else:
        assert (results[0] == cp.ones((10, )) * world_size).all()
        assert (results[1] == torch.ones((10, )).cuda() * world_size).all()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 205:</b> &nbsp; 2 fragments, nominal size 21 lines, similarity 85%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7304')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_gpu_tests/test_reduce.py: 48-71
</a>
<div class="mid" id="frag7304" style="display:none"><pre>
def test_reduce_multiple_group(ray_start_single_node_2_gpus,
                               dst_rank,
                               num_groups=5):
    world_size = 2
    actors, _ = create_collective_workers(world_size)
    for group_name in range(1, num_groups):
        ray.get([
            actor.init_group.remote(world_size, i, "nccl", str(group_name))
            for i, actor in enumerate(actors)
        ])
    for i in range(num_groups):
        group_name = "default" if i == 0 else str(i)
        results = ray.get([
            a.do_reduce.remote(dst_rank=dst_rank, group_name=group_name)
            for a in actors
        ])
        for j in range(world_size):
            if j == dst_rank:
                assert (results[j] == cp.ones(
                    (10, ), dtype=cp.float32) * (i + 2)).all()
            else:
                assert (results[j] == cp.ones((10, ), dtype=cp.float32)).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7370')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_cpu_tests/test_reduce.py: 51-75
</a>
<div class="mid" id="frag7370" style="display:none"><pre>
def test_reduce_multiple_group(ray_start_single_node,
                               dst_rank,
                               backend,
                               num_groups=5):
    world_size = 2
    actors, _ = create_collective_workers(world_size, backend=backend)
    for group_name in range(1, num_groups):
        ray.get([
            actor.init_group.remote(world_size, i, backend, str(group_name))
            for i, actor in enumerate(actors)
        ])
    for i in range(num_groups):
        group_name = "default" if i == 0 else str(i)
        results = ray.get([
            a.do_reduce.remote(dst_rank=dst_rank, group_name=group_name)
            for a in actors
        ])
        for j in range(world_size):
            if j == dst_rank:
                assert (results[j] == np.ones(
                    (10, ), dtype=np.float32) * (i + 2)).all()
            else:
                assert (results[j] == np.ones((10, ), dtype=np.float32)).all()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 206:</b> &nbsp; 2 fragments, nominal size 20 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7308')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_gpu_tests/test_sendrecv.py: 13-35
</a>
<div class="mid" id="frag7308" style="display:none"><pre>
def test_reduce_different_name(ray_start_single_node_2_gpus, group_name,
                               array_size, dst_rank):
    world_size = 2
    actors, _ = create_collective_workers(
        num_workers=world_size, group_name=group_name)
    ray.wait([
        a.set_buffer.remote(cp.ones(array_size, dtype=cp.float32) * (i + 1))
        for i, a in enumerate(actors)
    ])
    src_rank = 1 - dst_rank
    refs = []
    for i, actor in enumerate(actors):
        if i != dst_rank:
            ref = actor.do_send.remote(group_name, dst_rank)
        else:
            ref = actor.do_recv.remote(group_name, src_rank)
        refs.append(ref)
    results = ray.get(refs)
    for i in range(world_size):
        assert (results[i] == cp.ones(array_size, dtype=cp.float32) *
                (src_rank + 1)).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7374')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_cpu_tests/test_sendrecv.py: 15-37
</a>
<div class="mid" id="frag7374" style="display:none"><pre>
def test_reduce_different_name(ray_start_single_node, group_name, array_size,
                               dst_rank, backend):
    world_size = 2
    actors, _ = create_collective_workers(
        num_workers=world_size, group_name=group_name, backend=backend)
    ray.wait([
        a.set_buffer.remote(np.ones(array_size, dtype=np.float32) * (i + 1))
        for i, a in enumerate(actors)
    ])
    src_rank = 1 - dst_rank
    refs = []
    for i, actor in enumerate(actors):
        if i != dst_rank:
            ref = actor.do_send.remote(group_name, dst_rank)
        else:
            ref = actor.do_recv.remote(group_name, src_rank)
        refs.append(ref)
    results = ray.get(refs)
    for i in range(world_size):
        assert (results[i] == np.ones(array_size, dtype=np.float32) *
                (src_rank + 1)).all()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 207:</b> &nbsp; 2 fragments, nominal size 20 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7309')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_gpu_tests/test_sendrecv.py: 37-59
</a>
<div class="mid" id="frag7309" style="display:none"><pre>
def test_sendrecv_torch_cupy(ray_start_single_node_2_gpus, dst_rank):
    import torch
    world_size = 2
    actors, _ = create_collective_workers(world_size)
    ray.wait([actors[1].set_buffer.remote(torch.ones(10, ).cuda() * 2)])
    src_rank = 1 - dst_rank

    refs = []
    for i, actor in enumerate(actors):
        if i != dst_rank:
            ref = actor.do_send.remote(dst_rank=dst_rank)
        else:
            ref = actor.do_recv.remote(src_rank=src_rank)
        refs.append(ref)
    results = ray.get(refs)
    if dst_rank == 0:
        assert (results[0] == cp.ones((10, )) * 2).all()
        assert (results[1] == torch.ones((10, )).cuda() * 2).all()
    else:
        assert (results[0] == cp.ones((10, ))).all()
        assert (results[1] == torch.ones((10, )).cuda()).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7375')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_cpu_tests/test_sendrecv.py: 40-62
</a>
<div class="mid" id="frag7375" style="display:none"><pre>
def test_sendrecv_torch_numpy(ray_start_single_node, dst_rank, backend):
    import torch
    world_size = 2
    actors, _ = create_collective_workers(world_size, backend=backend)
    ray.wait([actors[1].set_buffer.remote(torch.ones(10, ) * 2)])
    src_rank = 1 - dst_rank

    refs = []
    for i, actor in enumerate(actors):
        if i != dst_rank:
            ref = actor.do_send.remote(dst_rank=dst_rank)
        else:
            ref = actor.do_recv.remote(src_rank=src_rank)
        refs.append(ref)
    results = ray.get(refs)
    if dst_rank == 0:
        assert (results[0] == np.ones((10, )) * 2).all()
        assert (results[1] == torch.ones((10, )) * 2).all()
    else:
        assert (results[0] == np.ones((10, ))).all()
        assert (results[1] == torch.ones((10, ))).all()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 208:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7322')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_multigpu_tests/test_distributed_multigpu_allreduce.py: 32-45
</a>
<div class="mid" id="frag7322" style="display:none"><pre>
def test_allreduce_multigpu_different_array_size(
        ray_start_distributed_multigpu_2_nodes_4_gpus, array_size):
    world_size = 2
    num_gpu_per_worker = 2
    actual_world_size = world_size * num_gpu_per_worker
    actors, _ = create_collective_multigpu_workers(world_size)
    ray.get([a.set_buffer.remote(array_size) for a in actors])
    results = ray.get([a.do_allreduce_multigpu.remote() for a in actors])
    assert (results[0] == cp.ones(
        (array_size, ), dtype=cp.float32) * actual_world_size).all()
    assert (results[1] == cp.ones(
        (array_size, ), dtype=cp.float32) * actual_world_size).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7326')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_multigpu_tests/test_distributed_multigpu_allreduce.py: 130-143
</a>
<div class="mid" id="frag7326" style="display:none"><pre>
def test_allreduce_multigpu_different_dtype(
        ray_start_distributed_multigpu_2_nodes_4_gpus, dtype):
    world_size = 2
    num_gpu_per_worker = 2
    actual_world_size = world_size * num_gpu_per_worker
    actors, _ = create_collective_multigpu_workers(world_size)
    ray.get([a.set_buffer.remote([10], dtype=dtype) for a in actors])
    results = ray.get([a.do_allreduce_multigpu.remote() for a in actors])
    assert (results[0] == cp.ones(
        (10, ), dtype=dtype) * actual_world_size).all()
    assert (results[1] == cp.ones(
        (10, ), dtype=dtype) * actual_world_size).all()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 209:</b> &nbsp; 2 fragments, nominal size 19 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7328')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_multigpu_tests/test_distributed_multigpu_reducescatter.py: 14-35
</a>
<div class="mid" id="frag7328" style="display:none"><pre>
@pytest.mark.parametrize("array_size",
                         [2, 2**5, 2**10, 2**15, 2**20, [2, 2], [5, 5, 5]])
def test_reducescatter_different_array_size(
        ray_start_distributed_multigpu_2_nodes_4_gpus, array_size,
        tensor_backend):
    world_size = 2
    num_gpu_per_worker = 2
    actual_world_size = world_size * num_gpu_per_worker
    actors, _ = create_collective_multigpu_workers(world_size)

    init_tensors_for_gather_scatter_multigpu(
        actors, array_size=array_size, tensor_backend=tensor_backend)
    results = ray.get([a.do_reducescatter_multigpu.remote() for a in actors])
    for i in range(world_size):
        for j in range(num_gpu_per_worker):
            if tensor_backend == "cupy":
                assert (results[i][j] == cp.ones(array_size, dtype=cp.float32)
                        * actual_world_size).all()
            else:
                assert (results[i][j] == torch.ones(
                    array_size, dtype=torch.float32).cuda(j) *
                        actual_world_size).all()
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7330')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_multigpu_tests/test_distributed_multigpu_allgather.py: 14-34
</a>
<div class="mid" id="frag7330" style="display:none"><pre>
@pytest.mark.parametrize("array_size",
                         [2, 2**5, 2**10, 2**15, 2**20, [2, 2], [5, 5, 5]])
def test_allgather_different_array_size(
        ray_start_distributed_multigpu_2_nodes_4_gpus, array_size,
        tensor_backend):
    world_size = 2
    num_gpu_per_worker = 2
    actual_world_size = world_size * num_gpu_per_worker
    actors, _ = create_collective_multigpu_workers(world_size)
    init_tensors_for_gather_scatter_multigpu(
        actors, array_size=array_size, tensor_backend=tensor_backend)
    results = ray.get([a.do_allgather_multigpu.remote() for a in actors])
    for i in range(world_size):
        for j in range(num_gpu_per_worker):
            for k in range(actual_world_size):
                if tensor_backend == "cupy":
                    assert (results[i][j][k] == cp.ones(
                        array_size, dtype=cp.float32)).all()
                else:
                    assert (results[i][j][k] == torch.ones(
                        array_size, dtype=torch.float32).cuda(j)).all()
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 210:</b> &nbsp; 2 fragments, nominal size 32 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7329')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_multigpu_tests/test_distributed_multigpu_reducescatter.py: 36-76
</a>
<div class="mid" id="frag7329" style="display:none"><pre>


def test_reducescatter_torch_cupy(
        ray_start_distributed_multigpu_2_nodes_4_gpus):
    world_size = 2
    num_gpu_per_worker = 2
    actual_world_size = world_size * num_gpu_per_worker
    shape = [10, 10]
    actors, _ = create_collective_multigpu_workers(world_size)

    # tensor is pytorch, list is cupy
    for i, a in enumerate(actors):
        ray.get([
            a.set_buffer.remote(
                shape, tensor_type0="torch", tensor_type1="torch")
        ])
        ray.get([
            a.set_list_buffer.remote(
                shape, tensor_type0="cupy", tensor_type1="cupy")
        ])
    results = ray.get([a.do_reducescatter_multigpu.remote() for a in actors])
    for i in range(world_size):
        for j in range(num_gpu_per_worker):
            assert (results[i][j] == torch.ones(
                shape, dtype=torch.float32).cuda(j) * actual_world_size).all()

    # tensor is cupy, list is pytorch
    for i, a in enumerate(actors):
        ray.get([
            a.set_buffer.remote(
                shape, tensor_type0="cupy", tensor_type1="cupy")
        ])
        ray.get([
            a.set_list_buffer.remote(
                shape, tensor_type0="torch", tensor_type1="torch")
        ])
    results = ray.get([a.do_reducescatter_multigpu.remote() for a in actors])
    for i in range(world_size):
        for j in range(num_gpu_per_worker):
            assert (results[i][j] == cp.ones(shape, dtype=cp.float32) *
                    actual_world_size).all()
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7331')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_multigpu_tests/test_distributed_multigpu_allgather.py: 35-76
</a>
<div class="mid" id="frag7331" style="display:none"><pre>


def test_allgather_torch_cupy(ray_start_distributed_multigpu_2_nodes_4_gpus):
    world_size = 2
    num_gpu_per_worker = 2
    actual_world_size = world_size * num_gpu_per_worker
    shape = [10, 10]
    actors, _ = create_collective_multigpu_workers(world_size)

    # tensor is pytorch, list is cupy
    for i, a in enumerate(actors):
        ray.get([
            a.set_buffer.remote(
                shape, tensor_type0="torch", tensor_type1="torch")
        ])
        ray.get([
            a.set_list_buffer.remote(
                shape, tensor_type0="cupy", tensor_type1="cupy")
        ])
    results = ray.get([a.do_allgather_multigpu.remote() for a in actors])
    for i in range(world_size):
        for j in range(num_gpu_per_worker):
            for k in range(actual_world_size):
                assert (results[i][j][k] == cp.ones(shape,
                                                    dtype=cp.float32)).all()

    # tensor is cupy, list is pytorch
    for i, a in enumerate(actors):
        ray.get([
            a.set_buffer.remote(
                shape, tensor_type0="cupy", tensor_type1="cupy")
        ])
        ray.get([
            a.set_list_buffer.remote(
                shape, tensor_type0="torch", tensor_type1="torch")
        ])
    results = ray.get([a.do_allgather_multigpu.remote() for a in actors])
    for i in range(world_size):
        for j in range(num_gpu_per_worker):
            for k in range(actual_world_size):
                assert (results[i][j][k] == torch.ones(
                    shape, dtype=torch.float32).cuda(j)).all()
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 211:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 85%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7345')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_cpu_tests/test_broadcast.py: 51-65
</a>
<div class="mid" id="frag7345" style="display:none"><pre>
def test_broadcast_torch_numpy(ray_start_single_node, src_rank, backend):
    import torch
    world_size = 2
    actors, _ = create_collective_workers(world_size, backend=backend)
    ray.wait([actors[1].set_buffer.remote(torch.ones(10, ) * world_size)])
    results = ray.get(
        [a.do_broadcast.remote(src_rank=src_rank) for a in actors])
    if src_rank == 0:
        assert (results[0] == np.ones((10, ))).all()
        assert (results[1] == torch.ones((10, ))).all()
    else:
        assert (results[0] == np.ones((10, )) * world_size).all()
        assert (results[1] == torch.ones((10, )) * world_size).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7450')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_cpu_tests/test_distributed_broadcast.py: 51-66
</a>
<div class="mid" id="frag7450" style="display:none"><pre>
def test_broadcast_torch_numpy(ray_start_distributed_2_nodes, src_rank,
                               backend):
    import torch
    world_size = 8
    actors, _ = create_collective_workers(world_size, backend=backend)
    ray.wait([actors[1].set_buffer.remote(torch.ones(10, ) * world_size)])
    results = ray.get(
        [a.do_broadcast.remote(src_rank=src_rank) for a in actors])
    if src_rank == 0:
        assert (results[0] == np.ones((10, ))).all()
        assert (results[1] == torch.ones((10, ))).all()
    else:
        assert (results[0] == np.ones((10, )) * world_size).all()
        assert (results[1] == torch.ones((10, )) * world_size).all()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 212:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7349')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_cpu_tests/test_allgather.py: 46-58
</a>
<div class="mid" id="frag7349" style="display:none"><pre>
@pytest.mark.parametrize("length", [0, 1, 2, 3])
def test_unmatched_tensor_list_length(ray_start_single_node, length, backend):
    world_size = 2
    actors, _ = create_collective_workers(world_size, backend=backend)
    list_buffer = [np.ones(10, dtype=np.float32) for _ in range(length)]
    ray.wait(
        [a.set_list_buffer.remote(list_buffer, copy=True) for a in actors])
    if length != world_size:
        with pytest.raises(RuntimeError):
            ray.get([a.do_allgather.remote() for a in actors])
    else:
        ray.get([a.do_allgather.remote() for a in actors])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7424')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_cpu_tests/test_distributed_allgather.py: 49-62
</a>
<div class="mid" id="frag7424" style="display:none"><pre>
@pytest.mark.parametrize("length", [0, 1, 3, 4, 7, 8])
def test_unmatched_tensor_list_length(ray_start_distributed_2_nodes, length,
                                      backend):
    world_size = 8
    actors, _ = create_collective_workers(world_size, backend=backend)
    list_buffer = [np.ones(10, dtype=np.float32) for _ in range(length)]
    ray.wait(
        [a.set_list_buffer.remote(list_buffer, copy=True) for a in actors])
    if length != world_size:
        with pytest.raises(RuntimeError):
            ray.get([a.do_allgather.remote() for a in actors])
    else:
        ray.get([a.do_allgather.remote() for a in actors])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 213:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 91%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7372')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/single_node_cpu_tests/test_reduce.py: 131-144
</a>
<div class="mid" id="frag7372" style="display:none"><pre>
def test_reduce_torch_numpy(ray_start_single_node, dst_rank, backend):
    import torch
    world_size = 2
    actors, _ = create_collective_workers(world_size, backend=backend)
    ray.wait([actors[1].set_buffer.remote(torch.ones(10, ))])
    results = ray.get([a.do_reduce.remote(dst_rank=dst_rank) for a in actors])
    if dst_rank == 0:
        assert (results[0] == np.ones((10, )) * world_size).all()
        assert (results[1] == torch.ones((10, ))).all()
    else:
        assert (results[0] == np.ones((10, ))).all()
        assert (results[1] == torch.ones((10, )) * world_size).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7430')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/distributed_cpu_tests/test_distributed_reduce.py: 109-122
</a>
<div class="mid" id="frag7430" style="display:none"><pre>
def test_reduce_torch_numpy(ray_start_distributed_2_nodes, backend, dst_rank):
    import torch
    world_size = 8
    actors, _ = create_collective_workers(world_size, backend=backend)
    ray.get([actors[1].set_buffer.remote(torch.ones(10, ))])
    results = ray.get([a.do_reduce.remote(dst_rank=dst_rank) for a in actors])
    if dst_rank == 0:
        assert (results[0] == np.ones((10, )) * world_size).all()
        assert (results[1] == torch.ones((10, ))).all()
    else:
        assert (results[0] == np.ones((10, ))).all()
        assert (results[1] == torch.ones((10, ))).all()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 214:</b> &nbsp; 3 fragments, nominal size 13 lines, similarity 85%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7396')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/util.py: 100-115
</a>
<div class="mid" id="frag7396" style="display:none"><pre>
def create_collective_workers(num_workers=2,
                              group_name="default",
                              backend="nccl"):
    actors = [None] * num_workers
    for i in range(num_workers):
        actor = Worker.remote()
        ray.get([actor.init_tensors.remote()])
        actors[i] = actor
    world_size = num_workers
    init_results = ray.get([
        actor.init_group.remote(world_size, i, backend, group_name)
        for i, actor in enumerate(actors)
    ])
    return actors, init_results


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7419')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/util.py: 353-369
</a>
<div class="mid" id="frag7419" style="display:none"><pre>
def create_collective_multigpu_workers(num_workers=2,
                                       group_name="default",
                                       backend="nccl"):
    actors = [None] * num_workers
    for i in range(num_workers):
        actor = MultiGPUWorker.remote()
        ray.get([actor.set_buffer.remote([10])], timeout=10)
        ray.get([actor.set_list_buffer.remote([10])], timeout=10)
        actors[i] = actor
    world_size = num_workers
    init_results = ray.get([
        actor.init_group.remote(world_size, i, backend, group_name)
        for i, actor in enumerate(actors)
    ])
    return actors, init_results


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7471')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/cpu_util.py: 107-122
</a>
<div class="mid" id="frag7471" style="display:none"><pre>
def create_collective_workers(num_workers=2,
                              group_name="default",
                              backend="nccl"):
    actors = [None] * num_workers
    for i in range(num_workers):
        actor = Worker.remote()
        ray.get([actor.init_tensors.remote()])
        actors[i] = actor
    world_size = num_workers
    init_results = ray.get([
        actor.init_group.remote(world_size, i, backend, group_name)
        for i, actor in enumerate(actors)
    ])
    return actors, init_results


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 215:</b> &nbsp; 2 fragments, nominal size 24 lines, similarity 87%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7397')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/util.py: 116-142
</a>
<div class="mid" id="frag7397" style="display:none"><pre>
def init_tensors_for_gather_scatter(actors,
                                    array_size=10,
                                    dtype=cp.float32,
                                    tensor_backend="cupy"):
    world_size = len(actors)
    for i, a in enumerate(actors):
        if tensor_backend == "cupy":
            t = cp.ones(array_size, dtype=dtype) * (i + 1)
        elif tensor_backend == "torch":
            t = torch.ones(array_size, dtype=torch.float32).cuda() * (i + 1)
        else:
            raise RuntimeError("Unsupported tensor backend.")
        ray.get([a.set_buffer.remote(t)])
    if tensor_backend == "cupy":
        list_buffer = [
            cp.ones(array_size, dtype=dtype) for _ in range(world_size)
        ]
    elif tensor_backend == "torch":
        list_buffer = [
            torch.ones(array_size, dtype=torch.float32).cuda()
            for _ in range(world_size)
        ]
    else:
        raise RuntimeError("Unsupported tensor backend.")
    ray.get([a.set_list_buffer.remote(list_buffer) for a in actors])


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7472')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/cpu_util.py: 123-147
</a>
<div class="mid" id="frag7472" style="display:none"><pre>
def init_tensors_for_gather_scatter(actors,
                                    array_size=10,
                                    dtype=np.float32,
                                    tensor_backend="numpy"):
    world_size = len(actors)
    for i, a in enumerate(actors):
        if tensor_backend == "numpy":
            t = np.ones(array_size, dtype=dtype) * (i + 1)
        elif tensor_backend == "torch":
            t = torch.ones(array_size, dtype=torch.float32) * (i + 1)
        else:
            raise RuntimeError("Unsupported tensor backend.")
        ray.get([a.set_buffer.remote(t)])
    if tensor_backend == "numpy":
        list_buffer = [
            np.ones(array_size, dtype=dtype) for _ in range(world_size)
        ]
    elif tensor_backend == "torch":
        list_buffer = [
            torch.ones(array_size, dtype=torch.float32)
            for _ in range(world_size)
        ]
    else:
        raise RuntimeError("Unsupported tensor backend.")
    ray.get([a.set_list_buffer.remote(list_buffer, copy=True) for a in actors])
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 216:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7410')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/util.py: 288-305
</a>
<div class="mid" id="frag7410" style="display:none"><pre>
    def do_send_multigpu(self,
                         group_name="default",
                         dst_rank=0,
                         dst_gpu_index=0,
                         src_gpu_index=0):
        if src_gpu_index == 0:
            col.send_multigpu(self.buffer0, dst_rank, dst_gpu_index,
                              group_name)
            cp.cuda.Device(0).synchronize()
            return self.buffer0
        elif src_gpu_index == 1:
            col.send_multigpu(self.buffer1, dst_rank, dst_gpu_index,
                              group_name)
            cp.cuda.Device(1).synchronize()
            return self.buffer1
        else:
            raise RuntimeError()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7411')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/tests/util.py: 306-323
</a>
<div class="mid" id="frag7411" style="display:none"><pre>
    def do_recv_multigpu(self,
                         group_name="default",
                         src_rank=0,
                         src_gpu_index=0,
                         dst_gpu_index=0):
        if dst_gpu_index == 0:
            col.recv_multigpu(self.buffer0, src_rank, src_gpu_index,
                              group_name)
            cp.cuda.Device(0).synchronize()
            return self.buffer0
        elif dst_gpu_index == 1:
            col.recv_multigpu(self.buffer1, src_rank, src_gpu_index,
                              group_name)
            cp.cuda.Device(1).synchronize()
            return self.buffer1
        else:
            raise RuntimeError()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 217:</b> &nbsp; 3 fragments, nominal size 23 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7517')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/collective_group/gloo_collective_group.py: 283-321
</a>
<div class="mid" id="frag7517" style="display:none"><pre>
                             gloo_util.get_tensor_n_elements(input_tensor),
                             gloo_util.get_gloo_tensor_dtype(input_tensor),
                             root_rank)

        self._collective(tensors, tensors, collective_fn)

    def allgather(self,
                  tensor_lists,
                  tensors,
                  allgather_options=AllGatherOptions()):
        """Allgather tensors on CPU into a list of tensors.

        Args:
            tensor_lists (List[List[Tensor]]): allgathered tensors.
            tensors: the list of tensors to allgather across the group.
                     Each tensor must locate on CPU.
            allgather_options: allgather options.

        Returns:
            None
        """

        def collective_fn(input_tensor, output_tensor, context):
            pygloo.allgather(context, gloo_util.get_tensor_ptr(input_tensor),
                             gloo_util.get_tensor_ptr(output_tensor),
                             gloo_util.get_tensor_n_elements(input_tensor),
                             gloo_util.get_gloo_tensor_dtype(input_tensor))

        _check_inputs_compatibility_for_scatter_gather(tensors, tensor_lists)
        output_flattened = [
            _flatten_for_scatter_gather(tensor_list, copy=False)
            for tensor_list in tensor_lists
        ]

        def postprocess_fn():
            for i, tensor_list in enumerate(tensor_lists):
                for j, tensor in enumerate(tensor_list):
                    gloo_util.copy_tensor(tensor, output_flattened[i][j])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7546')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/collective_group/nccl_collective_group.py: 247-287
</a>
<div class="mid" id="frag7546" style="display:none"><pre>
                nccl_util.get_tensor_ptr(output_tensor),
                nccl_util.get_tensor_n_elements(input_tensor),
                nccl_util.get_nccl_tensor_dtype(input_tensor), root_rank,
                stream.ptr)

        self._collective(tensors, tensors, collective_fn)

    def allgather(self,
                  tensor_lists,
                  tensors,
                  allgather_options=AllGatherOptions()):
        """Allgather tensors across gpus into a list of tensors.

        Args:
            tensor_lists (List[List[Tensor]]): allgathered tensors.
            tensors: the list of tensors to allgather across the group.
                     Each tensor must lolcate on a GPU of the process.
            allgather_options: allgather options.

        Returns:
            None
        """

        def collective_fn(input_tensor, output_tensor, comm, stream):
            comm.allGather(
                nccl_util.get_tensor_ptr(input_tensor),
                nccl_util.get_tensor_ptr(output_tensor),
                nccl_util.get_tensor_n_elements(input_tensor),
                nccl_util.get_nccl_tensor_dtype(input_tensor), stream.ptr)

        _check_inputs_compatibility_for_scatter_gather(tensors, tensor_lists)
        output_flattened = [
            _flatten_for_scatter_gather(tensor_list, copy=False)
            for tensor_list in tensor_lists
        ]

        def postprocess_fn(stream):
            # TODO(Hao): designate a copy stream.
            for i, tensor_list in enumerate(tensor_lists):
                for j, tensor in enumerate(tensor_list):
                    nccl_util.copy_tensor(tensor, output_flattened[i][j])
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7549')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/collective_group/nccl_collective_group.py: 288-330
</a>
<div class="mid" id="frag7549" style="display:none"><pre>

        self._collective(
            tensors,
            output_flattened,
            collective_fn,
            postprocess_fn=postprocess_fn)

    def reducescatter(self,
                      tensors,
                      tensor_lists,
                      reducescatter_options=ReduceScatterOptions()):
        """Reduce then scatter a list of tensors across the group.

        Args:
            tensors (List): the output tensors (could be unspecified), each
                            located on a GPU of the current process.
            tensor_lists (List[List]): the list of tensors to be reduced then
                                       scattered.
            reducescatter_options: reduce-scatter options.

        Returns:
            None
        """

        def collective_fn(input_tensor, output_tensor, comm, stream):
            comm.reduceScatter(
                nccl_util.get_tensor_ptr(input_tensor),
                nccl_util.get_tensor_ptr(output_tensor),
                nccl_util.get_tensor_n_elements(output_tensor),
                nccl_util.get_nccl_tensor_dtype(output_tensor),
                nccl_util.get_nccl_reduce_op(reducescatter_options.reduceOp),
                stream.ptr)

        _check_inputs_compatibility_for_scatter_gather(tensors, tensor_lists)
        input_flattened = [
            _flatten_for_scatter_gather(tensor_list, copy=False)
            for tensor_list in tensor_lists
        ]

        def preprocess_fn(stream):
            for i, tensor_list in enumerate(tensor_lists):
                for j, tensor in enumerate(tensor_list):
                    nccl_util.copy_tensor(input_flattened[i][j], tensor)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 218:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 84%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7530')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/collective_group/gloo_collective_group.py: 457-481
</a>
<div class="mid" id="frag7530" style="display:none"><pre>
                           " Got {} != 1.".format(len(tensors)))
    d = gloo_util.get_tensor_device(tensors[0])
    if d != "cpu":
        raise RuntimeError("Gloo only accept cpu tensor." " Got {}.".format(d))


def _flatten_for_scatter_gather(tensor_list, copy=False):
    """Flatten the tensor for gather/scatter operations.

    Args:
        tensor_list: the list of tensors to be scattered/gathered.
        copy: whether the copy the tensors in tensor_list into the buffer.

    Returns:
        The flattened tensor buffer.
    """
    if not tensor_list:
        raise RuntimeError("Received an empty list.")

    t = tensor_list[0]
    # note we need a numpy dtype here.
    dtype = gloo_util.get_numpy_tensor_dtype(t)
    buffer_shape = [len(tensor_list)] + gloo_util.get_tensor_shape(t)

    buffer = numpy.empty(buffer_shape, dtype=dtype)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7564')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/collective_group/nccl_collective_group.py: 633-657
</a>
<div class="mid" id="frag7564" style="display:none"><pre>

        # We have made sure that self.rank != peer_rank during API check.
        peer_p2p_rank = 0 if self.rank &gt; peer_rank else 1
        for i, tensor in enumerate(tensors):
            p2p_fn(tensors[i], comms[i], streams[i], peer_p2p_rank)


def _flatten_for_scatter_gather(tensor_list, copy=False):
    """Flatten the tensor for gather/scatter operations.

    Args:
        tensor_list: the list of tensors to be scattered/gathered.
        copy: whether the copy the tensors in tensor_list into the buffer.

    Returns:
        The flattened tensor buffer.
    """
    if not tensor_list:
        raise RuntimeError("Received an empty list.")
    t = tensor_list[0]
    # note we need a cupy dtype here.
    dtype = nccl_util.get_cupy_tensor_dtype(t)
    buffer_shape = [len(tensor_list)] + nccl_util.get_tensor_shape(t)
    device = nccl_util.get_tensor_device(t)
    with nccl_util.Device(device):
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 219:</b> &nbsp; 3 fragments, nominal size 10 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7539')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/collective_group/nccl_collective_group.py: 158-180
</a>
<div class="mid" id="frag7539" style="display:none"><pre>

    @classmethod
    def backend(cls):
        return Backend.NCCL

    def allreduce(self, tensors, allreduce_options=AllReduceOptions()):
        """AllReduce tensors across the collective group following options.

        Args:
            tensors (List): the list of tensors to be reduced. Each tensor must
                            reside on one GPU of the current process.
            allreduce_options: allreduce options.

        Returns:
            None
        """

        def collective_fn(input_tensor, output_tensor, comm, stream):
            comm.allReduce(
                nccl_util.get_tensor_ptr(input_tensor),
                nccl_util.get_tensor_ptr(output_tensor),
                nccl_util.get_tensor_n_elements(input_tensor),
                nccl_util.get_nccl_tensor_dtype(input_tensor),
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7542')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/collective_group/nccl_collective_group.py: 201-224
</a>
<div class="mid" id="frag7542" style="display:none"><pre>
        for i, d in enumerate(devices):
            with nccl_util.Device(d):
                barrier_tensors[i] = cupy.array([1])
        self.allreduce(barrier_tensors)

    def reduce(self, tensors, reduce_options=ReduceOptions()):
        """Reduce tensors to a destination gpu following options.

        Args:
            tensors (List): the list of tensors to be reduced, each tensor
                            must reside on one gpu of the current process.
            reduce_options: reduce options.

        Returns:
            None
        """
        root_rank = len(tensors) * reduce_options.root_rank \
            + reduce_options.root_tensor

        def collective_fn(input_tensor, output_tensor, comm, stream):
            comm.reduce(
                nccl_util.get_tensor_ptr(input_tensor),
                nccl_util.get_tensor_ptr(output_tensor),
                nccl_util.get_tensor_n_elements(input_tensor),
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7544')" href="javascript:;">
ray-ray-1.9.2/python/ray/util/collective/collective_group/nccl_collective_group.py: 225-246
</a>
<div class="mid" id="frag7544" style="display:none"><pre>
                nccl_util.get_nccl_tensor_dtype(input_tensor),
                nccl_util.get_nccl_reduce_op(reduce_options.reduceOp),
                root_rank, stream.ptr)

        self._collective(tensors, tensors, collective_fn)

    def broadcast(self, tensors, broadcast_options=BroadcastOptions()):
        """Broadcast tensors to all other gpus following options.

        Args:
            tensors (List): tensors to be broadcast or received.
            broadcast_options: broadcast options.

        Returns:
            None
        """
        root_rank = len(tensors) * broadcast_options.root_rank \
            + broadcast_options.root_tensor

        def collective_fn(input_tensor, output_tensor, comm, stream):
            comm.broadcast(
                nccl_util.get_tensor_ptr(input_tensor),
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 220:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7714')" href="javascript:;">
ray-ray-1.9.2/python/ray/cloudpickle/__init__.py: 24-36
</a>
<div class="mid" id="frag7714" style="display:none"><pre>
def dump_debug(obj, *args, **kwargs):
    try:
        return dump(obj, *args, **kwargs)
    except (TypeError, PicklingError) as exc:
        if os.environ.get("RAY_PICKLE_VERBOSE_DEBUG"):
            from ray.util.check_serialize import inspect_serializability
            inspect_serializability(obj)
            raise
        else:
            msg = _warn_msg(obj, "ray.cloudpickle.dump", exc)
            raise type(exc)(msg)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7715')" href="javascript:;">
ray-ray-1.9.2/python/ray/cloudpickle/__init__.py: 37-47
</a>
<div class="mid" id="frag7715" style="display:none"><pre>
def dumps_debug(obj, *args, **kwargs):
    try:
        return dumps(obj, *args, **kwargs)
    except (TypeError, PicklingError) as exc:
        if os.environ.get("RAY_PICKLE_VERBOSE_DEBUG"):
            from ray.util.check_serialize import inspect_serializability
            inspect_serializability(obj)
            raise
        else:
            msg = _warn_msg(obj, "ray.cloudpickle.dumps", exc)
            raise type(exc)(msg)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 221:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7726')" href="javascript:;">
ray-ray-1.9.2/python/ray/serve/pipeline/tests/test_step.py: 8-23
</a>
<div class="mid" id="frag7726" style="display:none"><pre>
def test_decorator_no_args():
    @pipeline.step
    def f():
        pass

    assert isinstance(f, PipelineStep)
    assert f.num_replicas == 1

    @pipeline.step
    class A:
        pass

    assert isinstance(A, PipelineStep)
    assert A.num_replicas == 1


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7728')" href="javascript:;">
ray-ray-1.9.2/python/ray/serve/pipeline/tests/test_step.py: 24-39
</a>
<div class="mid" id="frag7728" style="display:none"><pre>
def test_decorator_with_arg():
    @pipeline.step(num_replicas=2)
    def f():
        pass

    assert isinstance(f, PipelineStep)
    assert f.num_replicas == 2

    @pipeline.step(num_replicas=5)
    class A:
        pass

    assert isinstance(A, PipelineStep)
    assert A.num_replicas == 5


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 222:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7730')" href="javascript:;">
ray-ray-1.9.2/python/ray/serve/pipeline/tests/test_step.py: 40-53
</a>
<div class="mid" id="frag7730" style="display:none"><pre>
def test_pass_step_without_calling():
    @pipeline.step
    def step1():
        pass

    @pipeline.step
    def step2():
        pass

    step2(step1(pipeline.INPUT))
    with pytest.raises(TypeError):
        step2(step1)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7733')" href="javascript:;">
ray-ray-1.9.2/python/ray/serve/pipeline/tests/test_step.py: 54-67
</a>
<div class="mid" id="frag7733" style="display:none"><pre>
def test_input_step_multiple_args_rejected():
    @pipeline.step
    def step1():
        pass

    @pipeline.step
    def step2():
        pass

    step1(pipeline.INPUT)
    with pytest.raises(ValueError):
        step1(pipeline.INPUT, step2(pipeline.INPUT))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 223:</b> &nbsp; 2 fragments, nominal size 31 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7782')" href="javascript:;">
ray-ray-1.9.2/python/ray/serve/tests/test_cluster.py: 25-72
</a>
<div class="mid" id="frag7782" style="display:none"><pre>
def test_scale_up(ray_cluster):
    cluster = ray_cluster
    cluster.add_node(num_cpus=1)
    cluster.connect(namespace="serve")
    # By default, Serve controller and proxy actors use 0 CPUs,
    # so initially there should only be room for 1 replica.

    @serve.deployment(version="1", num_replicas=1)
    def D(*args):
        return os.getpid()

    def get_pids(expected, timeout=30):
        pids = set()
        start = time.time()
        while len(pids) &lt; expected:
            pids.add(requests.get("http://localhost:8000/D").text)
            if time.time() - start &gt;= timeout:
                raise TimeoutError("Timed out waiting for pids.")
        return pids

    serve.start(detached=True)
    client = serve.api._connect()

    D.deploy()
    pids1 = get_pids(1)

    goal_ref = D.options(num_replicas=3).deploy(_blocking=False)

    # Check that a new replica has not started in 1.0 seconds.  This
    # doesn't guarantee that a new replica won't ever be started, but
    # 1.0 seconds is a reasonable upper bound on replica startup time.
    assert not client._wait_for_goal(goal_ref, timeout=1.0)
    assert get_pids(1) == pids1

    # Add a node with another CPU, another replica should get placed.
    cluster.add_node(num_cpus=1)
    assert not client._wait_for_goal(goal_ref, timeout=1.0)
    pids2 = get_pids(2)
    assert pids1.issubset(pids2)

    # Add a node with another CPU, the final replica should get placed
    # and the deploy goal should be done.
    cluster.add_node(num_cpus=1)
    assert client._wait_for_goal(goal_ref)
    pids3 = get_pids(3)
    assert pids2.issubset(pids3)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7785')" href="javascript:;">
ray-ray-1.9.2/python/ray/serve/tests/test_cluster.py: 74-118
</a>
<div class="mid" id="frag7785" style="display:none"><pre>
def test_node_failure(ray_cluster):
    cluster = ray_cluster
    cluster.add_node(num_cpus=3)
    cluster.connect(namespace="serve")

    worker_node = cluster.add_node(num_cpus=2)

    @serve.deployment(version="1", num_replicas=5)
    def D(*args):
        return os.getpid()

    def get_pids(expected, timeout=30):
        pids = set()
        start = time.time()
        while len(pids) &lt; expected:
            pids.add(requests.get("http://localhost:8000/D").text)
            if time.time() - start &gt;= timeout:
                raise TimeoutError("Timed out waiting for pids.")
        return pids

    serve.start(detached=True)

    print("Initial deploy.")
    D.deploy()
    pids1 = get_pids(5)

    # Remove the node. There should still be three replicas running.
    print("Kill node.")
    cluster.remove_node(worker_node)
    pids2 = get_pids(3)
    assert pids2.issubset(pids1)

    # Add a worker node back. One replica should get placed.
    print("Add back first node.")
    cluster.add_node(num_cpus=1)
    pids3 = get_pids(4)
    assert pids2.issubset(pids3)

    # Add another worker node. One more replica should get placed.
    print("Add back second node.")
    cluster.add_node(num_cpus=1)
    pids4 = get_pids(5)
    assert pids3.issubset(pids4)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 224:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7792')" href="javascript:;">
ray-ray-1.9.2/python/ray/serve/tests/test_autoscaling_policy.py: 45-56
</a>
<div class="mid" id="frag7792" style="display:none"><pre>
    def test_scale_up(self):
        config = AutoscalingConfig(
            min_replicas=0,
            max_replicas=100,
            target_num_ongoing_requests_per_replica=1)
        num_replicas = 10
        num_ongoing_requests = [2.0] * num_replicas
        desired_num_replicas = calculate_desired_num_replicas(
            autoscaling_config=config,
            current_num_ongoing_requests=num_ongoing_requests)
        assert 19 &lt;= desired_num_replicas &lt;= 21  # 10 * 2 = 20

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7793')" href="javascript:;">
ray-ray-1.9.2/python/ray/serve/tests/test_autoscaling_policy.py: 57-68
</a>
<div class="mid" id="frag7793" style="display:none"><pre>
    def test_scale_down(self):
        config = AutoscalingConfig(
            min_replicas=0,
            max_replicas=100,
            target_num_ongoing_requests_per_replica=1)
        num_replicas = 10
        num_ongoing_requests = [0.5] * num_replicas
        desired_num_replicas = calculate_desired_num_replicas(
            autoscaling_config=config,
            current_num_ongoing_requests=num_ongoing_requests)
        assert 4 &lt;= desired_num_replicas &lt;= 6  # 10 * 0.5 = 5

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 225:</b> &nbsp; 3 fragments, nominal size 11 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7877')" href="javascript:;">
ray-ray-1.9.2/python/ray/serve/tests/test_runtime_env.py: 122-173
</a>
<div class="mid" id="frag7877" style="display:none"><pre>
def test_working_dir_connect_from_new_driver(ray_start, tmp_dir,
                                             use_ray_client):
    with open("hello", "w") as f:
        f.write("world")

    driver1 = """
import ray
from ray import serve

job_config = ray.job_config.JobConfig(runtime_env={{"working_dir": "."}})
if {use_ray_client}:
    ray.util.connect("{client_addr}", namespace="serve", job_config=job_config)
else:
    ray.init(address="auto", namespace="serve", job_config=job_config)

serve.start(detached=True)

@serve.deployment
class Test:
    def __call__(self, *args):
        return open("hello").read()

Test.deploy()
handle = Test.get_handle()
assert ray.get(handle.remote()) == "world"
""".format(
        use_ray_client=use_ray_client, client_addr=ray_start)

    run_string_as_driver(driver1)

    driver2 = """
import ray
from ray import serve

job_config = ray.job_config.JobConfig(runtime_env={{"working_dir": "."}})
if {use_ray_client}:
    ray.util.connect("{client_addr}", namespace="serve", job_config=job_config)
else:
    ray.init(address="auto", namespace="serve", job_config=job_config)

serve.start(detached=True)

Test = serve.get_deployment("Test")
handle = Test.get_handle()
assert ray.get(handle.remote()) == "world"
Test.delete()
""".format(
        use_ray_client=use_ray_client, client_addr=ray_start)

    run_string_as_driver(driver2)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7878')" href="javascript:;">
ray-ray-1.9.2/python/ray/serve/tests/test_runtime_env.py: 176-238
</a>
<div class="mid" id="frag7878" style="display:none"><pre>
def test_working_dir_scale_up_in_new_driver(ray_start, tmp_dir,
                                            use_ray_client):
    with open("hello", "w") as f:
        f.write("world")

    driver1 = """
import os

import ray
from ray import serve

job_config = ray.job_config.JobConfig(runtime_env={{"working_dir": "."}})
if {use_ray_client}:
    ray.util.connect("{client_addr}", namespace="serve", job_config=job_config)
else:
    ray.init(address="auto", namespace="serve", job_config=job_config)

serve.start(detached=True)

@serve.deployment(version="1")
class Test:
    def __call__(self, *args):
        return os.getpid(), open("hello").read()

Test.deploy()
handle = Test.get_handle()
assert ray.get(handle.remote())[1] == "world"
""".format(
        use_ray_client=use_ray_client, client_addr=ray_start)

    run_string_as_driver(driver1)

    with open("hello", "w") as f:
        f.write("no longer world")

    driver2 = """
import ray
from ray import serve

job_config = ray.job_config.JobConfig(runtime_env={{"working_dir": "."}})
if {use_ray_client}:
    ray.util.connect("{client_addr}", namespace="serve", job_config=job_config)
else:
    ray.init(address="auto", namespace="serve", job_config=job_config)

serve.start(detached=True)

Test = serve.get_deployment("Test")
Test.options(num_replicas=2).deploy()
handle = Test.get_handle()
results = ray.get([handle.remote() for _ in range(1000)])
print(set(results))
assert all(r[1] == "world" for r in results), (
    "results should still come from the first env")
assert len(set(r[0] for r in results)) == 2, (
    "make sure there are two replicas")
Test.delete()
""".format(
        use_ray_client=use_ray_client, client_addr=ray_start)

    run_string_as_driver(driver2)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7879')" href="javascript:;">
ray-ray-1.9.2/python/ray/serve/tests/test_runtime_env.py: 241-299
</a>
<div class="mid" id="frag7879" style="display:none"><pre>
def test_working_dir_deploy_new_version(ray_start, tmp_dir, use_ray_client):
    with open("hello", "w") as f:
        f.write("world")

    driver1 = """
import ray
from ray import serve

job_config = ray.job_config.JobConfig(runtime_env={{"working_dir": "."}})
if {use_ray_client}:
    ray.util.connect("{client_addr}", namespace="serve", job_config=job_config)
else:
    ray.init(address="auto", namespace="serve", job_config=job_config)

serve.start(detached=True)

@serve.deployment(version="1")
class Test:
    def __call__(self, *args):
        return open("hello").read()

Test.deploy()
handle = Test.get_handle()
assert ray.get(handle.remote()) == "world"
""".format(
        use_ray_client=use_ray_client, client_addr=ray_start)

    run_string_as_driver(driver1)

    with open("hello", "w") as f:
        f.write("world2")

    driver2 = """
import ray
from ray import serve

job_config = ray.job_config.JobConfig(runtime_env={{"working_dir": "."}})
if {use_ray_client}:
    ray.util.connect("{client_addr}", namespace="serve", job_config=job_config)
else:
    ray.init(address="auto", namespace="serve", job_config=job_config)

serve.start(detached=True)

@serve.deployment(version="2")
class Test:
    def __call__(self, *args):
        return open("hello").read()

Test.deploy()
handle = Test.get_handle()
assert ray.get(handle.remote()) == "world2"
Test.delete()
""".format(
        use_ray_client=use_ray_client, client_addr=ray_start)

    run_string_as_driver(driver2)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 226:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7909')" href="javascript:;">
ray-ray-1.9.2/python/ray/serve/tests/test_warnings.py: 8-37
</a>
<div class="mid" id="frag7909" style="display:none"><pre>
def test_slow_allocation_warning(serve_instance, capsys):
    # this deployment can never be scheduled
    @serve.deployment(ray_actor_options={"num_cpus": 99999})
    class D:
        def __init__(self):
            pass

    num_replicas = 2
    D.options(num_replicas=num_replicas).deploy(_blocking=False)

    expected_warning = (f"Deployment '{D.name}' has "
                        f"{num_replicas} replicas that have taken "
                        f"more than {SLOW_STARTUP_WARNING_S}s "
                        f"to be scheduled.")

    # wait long enough for the warning to be printed
    # with a small grace period
    time.sleep(SLOW_STARTUP_WARNING_PERIOD_S * 1.5)

    captured = capsys.readouterr()

    print(captured.err)

    assert expected_warning in captured.err

    # make sure that exactly one warning was printed
    # for this deployment
    assert captured.err.count(expected_warning) == 1


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7911')" href="javascript:;">
ray-ray-1.9.2/python/ray/serve/tests/test_warnings.py: 38-64
</a>
<div class="mid" id="frag7911" style="display:none"><pre>
def test_slow_initialization_warning(serve_instance, capsys):
    # this deployment will take a while to allocate

    @serve.deployment
    class D:
        def __init__(self):
            time.sleep(99999)

    num_replicas = 4
    D.options(num_replicas=num_replicas).deploy(_blocking=False)

    expected_warning = (f"Deployment '{D.name}' has "
                        f"{num_replicas} replicas that have taken "
                        f"more than {SLOW_STARTUP_WARNING_S}s "
                        f"to initialize.")

    # wait long enough for the warning to be printed
    # with a small grace period
    time.sleep(SLOW_STARTUP_WARNING_PERIOD_S * 1.5)

    captured = capsys.readouterr()

    assert expected_warning in captured.err

    # make sure that exactly one warning was printed
    # for this deployment
    assert captured.err.count(expected_warning) == 1
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<script language="JavaScript">
function ShowHide(divId) { 
    if(document.getElementById(divId).style.display == 'none') {
        document.getElementById(divId).style.display='block';
    } else { 
        document.getElementById(divId).style.display = 'none';
    } 
}
</script>
</body>
</html>

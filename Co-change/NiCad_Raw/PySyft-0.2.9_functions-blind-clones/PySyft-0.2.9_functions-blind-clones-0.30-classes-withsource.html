<html>
<head>
    <title>NiCad6 Clone Report</title>
    <style type="text/css">
        body {font-family:sans-serif;}
        table {background-color:white; border:0px; padding:0px; border-spacing:4px; width:auto; margin-left:30px; margin-right:auto;}
        td {background-color:rgba(192,212,238,0.8); border:0px; padding:8px; width:auto; vertical-align:top; border-radius:8px}
        pre {background-color:white; padding:4px;}
        a {color:darkblue;}
    </style>
</head>
<body>
<h2>NiCad6 Clone Report</h2>
<table>
<tr style="font-size:14pt">
<td><b>System:</b> &nbsp; PySyft-0.2.9</td>
<td><b>Clone pairs:</b> &nbsp; 130</td>
<td><b>Clone classes:</b> &nbsp; 55</td>
</tr>
<tr style="font-size:12pt">
<td style="background-color:white">Clone type: &nbsp; 3-2</td>
<td style="background-color:white">Granularity: &nbsp; functions-blind</td>
<td style="background-color:white">Max diff threshold: &nbsp; 30%</td>
<td style="background-color:white">Clone size: &nbsp; 10 - 2500 lines</td>
<td style="background-color:white">Total functions-blind: &nbsp; 1409</td>
</tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 1:</b> &nbsp; 3 fragments, nominal size 13 lines, similarity 92%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag64')" href="javascript:;">
PySyft-0.2.9/syft/frameworks/torch/tensors/interpreters/paillier.py: 78-105
</a>
<div class="mid" id="frag64" style="display:none"><pre>
    def __add__(self, *args, **kwargs):
        """
        Here is the version of the add method without the decorator: as you can see
        it is much more complicated. However you misght need sometimes to specify
        some particular behaviour: so here what to start from :)
        """

        if isinstance(args[0], th.Tensor):
            data = self.child + args[0].numpy()
            obj = PaillierTensor()
            obj.child = data
            return obj

        if isinstance(self.child, th.Tensor):
            self.child = self.child.numpy()

        # Replace all syft tensor with their child attribute
        new_self, new_args, new_kwargs = hook_args.unwrap_args_from_method(
            "__add__", self, args, kwargs
        )

        # Send it to the appropriates class and get the response
        response = getattr(new_self, "__add__")(*new_args, **new_kwargs)

        # Put back SyftTensor on the tensors found in the response
        response = hook_args.hook_response("__add__", response, wrap_type=type(self))
        return response

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag65')" href="javascript:;">
PySyft-0.2.9/syft/frameworks/torch/tensors/interpreters/paillier.py: 106-133
</a>
<div class="mid" id="frag65" style="display:none"><pre>
    def __sub__(self, *args, **kwargs):
        """
        Here is the version of the add method without the decorator: as you can see
        it is much more complicated. However you misght need sometimes to specify
        some particular behaviour: so here what to start from :)
        """

        if isinstance(args[0], th.Tensor):
            data = self.child - args[0].numpy()
            obj = PaillierTensor()
            obj.child = data
            return obj

        if isinstance(self.child, th.Tensor):
            self.child = self.child.numpy()

        # Replace all syft tensor with their child attribute
        new_self, new_args, new_kwargs = hook_args.unwrap_args_from_method(
            "__sub__", self, args, kwargs
        )

        # Send it to the appropriate class and get the response
        response = getattr(new_self, "__sub__")(*new_args, **new_kwargs)

        # Put back SyftTensor on the tensors found in the response
        response = hook_args.hook_response("__sub__", response, wrap_type=type(self))
        return response

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag66')" href="javascript:;">
PySyft-0.2.9/syft/frameworks/torch/tensors/interpreters/paillier.py: 134-161
</a>
<div class="mid" id="frag66" style="display:none"><pre>
    def __mul__(self, *args, **kwargs):
        """
        Here is the version of the add method without the decorator: as you can see
        it is much more complicated. However you misght need sometimes to specify
        some particular behaviour: so here what to start from :)
        """

        if isinstance(args[0], th.Tensor):
            data = self.child * args[0].numpy()
            obj = PaillierTensor()
            obj.child = data
            return obj

        if isinstance(self.child, th.Tensor):
            self.child = self.child.numpy()

        # Replace all syft tensor with their child attribute
        new_self, new_args, new_kwargs = hook_args.unwrap_args_from_method(
            "__mul__", self, args, kwargs
        )

        # Send it to the appropriate class and get the response
        response = getattr(new_self, "__mul__")(*new_args, **new_kwargs)

        # Put back SyftTensor on the tensors found in the response
        response = hook_args.hook_response("__mul__", response, wrap_type=type(self))
        return response

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 2:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 91%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag193')" href="javascript:;">
PySyft-0.2.9/syft/frameworks/torch/he/fv/util/operations.py: 222-249
</a>
<div class="mid" id="frag193" style="display:none"><pre>
def multiply_add_plain_with_delta(ct, pt, context_data):
    """Add plaintext to ciphertext.

    Args:
        ct (Ciphertext): ct is pre-computed carrier polynomial where we can add pt data.
        pt (Plaintext): A plaintext representation of integer data to be encrypted.
        context (Context): Context for extracting encryption parameters.

    Returns:
        A Ciphertext object with the encrypted result of encryption process.
    """
    ct_param_id = ct.param_id
    coeff_modulus = context_data.param.coeff_modulus
    pt = pt.data
    plain_coeff_count = len(pt)
    delta = context_data.coeff_div_plain_modulus
    ct0, ct1 = ct.data  # here ct = pk * u * e

    # Coefficients of plain m multiplied by coeff_modulus q, divided by plain_modulus t,
    # and rounded to the nearest integer (rounded up in case of a tie). Equivalent to
    for i in range(plain_coeff_count):
        for j in range(len(coeff_modulus)):
            temp = round(delta[j] * pt[i]) % coeff_modulus[j]
            ct0[j][i] = (ct0[j][i] + temp) % coeff_modulus[j]

    return CipherText([ct0, ct1], ct_param_id)  # ct0 = pk0 * u * e + delta * pt


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag194')" href="javascript:;">
PySyft-0.2.9/syft/frameworks/torch/he/fv/util/operations.py: 250-275
</a>
<div class="mid" id="frag194" style="display:none"><pre>
def multiply_sub_plain_with_delta(ct, pt, context_data):
    """Subtract plaintext from ciphertext.

    Args:
        ct (Ciphertext): ct is pre-computed carrier polynomial where we can add message data.
        pt (Plaintext): A plaintext representation of integer data to be encrypted.
        context (Context): Context for extracting encryption parameters.

    Returns:
        A Ciphertext object with the encrypted result of encryption process.
    """
    ct_param_id = ct.param_id
    coeff_modulus = context_data.param.coeff_modulus
    pt = pt.data
    plain_coeff_count = len(pt)
    delta = context_data.coeff_div_plain_modulus
    ct0, ct1 = ct.data  # here ct = pk * u * e

    # Coefficients of plain m multiplied by coeff_modulus q, divided by plain_modulus t,
    # and rounded to the nearest integer (rounded up in case of a tie). Equivalent to
    for i in range(plain_coeff_count):
        for j in range(len(coeff_modulus)):
            temp = round(delta[j] * pt[i]) % coeff_modulus[j]
            ct0[j][i] = (ct0[j][i] - temp) % coeff_modulus[j]

    return CipherText([ct0, ct1], ct_param_id)  # ct0 = pk0 * u * e - delta * pt
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 3:</b> &nbsp; 3 fragments, nominal size 12 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag207')" href="javascript:;">
PySyft-0.2.9/syft/frameworks/torch/he/fv/evaluator.py: 47-76
</a>
<div class="mid" id="frag207" style="display:none"><pre>
    def add(self, op1, op2):
        """Add two operands using FV scheme.

        Args:
            op1 (Ciphertext/Plaintext): First polynomial argument (Augend).
            op2 (Ciphertext/Plaintext): Second polynomial argument (Addend).

        Returns:
            If both arguments are Plaintext elements then the result will be a Plaintext object
                otherwise a Ciphertext object with value equivalent to the result of addition
                operation of two provided arguments.
        """

        param_type = _typecheck(op1, op2)

        if param_type == ParamTypes.CTCT:
            return self._add_cipher_cipher(op1, op2)

        elif param_type == ParamTypes.PTPT:
            return self._add_plain_plain(op1, op2)

        elif param_type == ParamTypes.CTPT:
            return self._add_cipher_plain(op1, op2)

        elif param_type == ParamTypes.PTCT:
            return self._add_cipher_plain(op2, op1)

        else:
            raise TypeError(f"Addition Operation not supported between {type(op1)} and {type(op2)}")

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag208')" href="javascript:;">
PySyft-0.2.9/syft/frameworks/torch/he/fv/evaluator.py: 77-103
</a>
<div class="mid" id="frag208" style="display:none"><pre>
    def sub(self, op1, op2):
        """Subtracts two operands using FV scheme.

        Args:
            op1 (Ciphertext/Plaintext): First polynomial argument (Minuend).
            op2 (Ciphertext/Plaintext): Second polynomial argument (Subtrahend).

        Returns:
            A ciphertext object with the value equivalent to the result of the subtraction
                of two operands.
        """
        param_type = _typecheck(op1, op2)

        if param_type == ParamTypes.CTCT:
            return self._sub_cipher_cipher(op1, op2)

        elif param_type == ParamTypes.CTPT:
            return self._sub_cipher_plain(op1, op2)

        elif param_type == ParamTypes.PTCT:
            return self._sub_cipher_plain(op2, op1)

        else:
            raise TypeError(
                f"Subtraction Operation not supported between {type(op1)} and {type(op2)}"
            )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag210')" href="javascript:;">
PySyft-0.2.9/syft/frameworks/torch/he/fv/evaluator.py: 124-153
</a>
<div class="mid" id="frag210" style="display:none"><pre>
    def mul(self, op1, op2):
        """Multiply two operands using FV scheme.

        Args:
            op1 (Ciphertext/Plaintext): First polynomial argument (Multiplicand).
            op2 (Ciphertext/Plaintext): Second polynomial argument (Multiplier).

        Returns:
            A Ciphertext object with a value equivalent to the result of the product of two
                operands.
        """
        param_type = _typecheck(op1, op2)

        if param_type == ParamTypes.CTCT:
            return self._mul_cipher_cipher(op1, op2)

        elif param_type == ParamTypes.PTPT:
            return self._mul_plain_plain(op1, op2)

        elif param_type == ParamTypes.CTPT:
            return self._mul_cipher_plain(op1, op2)

        elif param_type == ParamTypes.PTCT:
            return self._mul_cipher_plain(op2, op1)

        else:
            raise TypeError(
                f"Multiplication Operation not supported between {type(op1)} and {type(op2)}"
            )

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 4:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag258')" href="javascript:;">
PySyft-0.2.9/syft/serde/msgpack/torch_serde.py: 31-54
</a>
<div class="mid" id="frag258" style="display:none"><pre>
def _serialize_tensor(worker: AbstractWorker, tensor) -&gt; bin:
    """Serialize the tensor using as default Torch serialization strategy
    This function can be overridden to provide different tensor serialization strategies

    Args
        (torch.Tensor): an input tensor to be serialized

    Returns
        A serialized version of the input tensor

    """
    serializers = {
        TENSOR_SERIALIZATION.TORCH: torch_tensor_serializer,
        TENSOR_SERIALIZATION.NUMPY: numpy_tensor_serializer,
        TENSOR_SERIALIZATION.ALL: simplified_tensor_serializer,
    }
    if worker.serializer not in serializers:
        raise NotImplementedError(
            f"Tensor serialization strategy is not supported: {worker.serializer}"
        )
    serializer = serializers[worker.serializer]
    return serializer(worker, tensor)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag259')" href="javascript:;">
PySyft-0.2.9/syft/serde/msgpack/torch_serde.py: 55-79
</a>
<div class="mid" id="frag259" style="display:none"><pre>
def _deserialize_tensor(worker: AbstractWorker, serializer: str, tensor_bin) -&gt; torch.Tensor:
    """Deserialize the input tensor passed as parameter into a Torch tensor.
    `serializer` parameter selects different deserialization strategies

    Args
        worker: Worker
        serializer: Strategy used for tensor deserialization (e.g.: torch, numpy, all)
        tensor_bin: A simplified representation of a tensor

    Returns
        a Torch tensor
    """
    deserializers = {
        TENSOR_SERIALIZATION.TORCH: torch_tensor_deserializer,
        TENSOR_SERIALIZATION.NUMPY: numpy_tensor_serializer,
        TENSOR_SERIALIZATION.ALL: simplified_tensor_deserializer,
    }
    if serializer not in deserializers:
        raise NotImplementedError(
            f"Cannot deserialize tensor serialized with '{serializer}' strategy"
        )
    deserializer = deserializers[serializer]
    return deserializer(worker, tensor_bin)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 5:</b> &nbsp; 3 fragments, nominal size 25 lines, similarity 96%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag296')" href="javascript:;">
PySyft-0.2.9/syft/grid/utils/autoscale/utils/notebook/terraform_notebook.py: 7-39
</a>
<div class="mid" id="frag296" style="display:none"><pre>
def init():
    """
    args:
    """
    proc = subprocess.Popen(
        "/bin/sh",
        stdout=subprocess.PIPE,
        stdin=subprocess.PIPE,
        stderr=subprocess.STDOUT,
    )

    def outloop():
        running = True
        while running:
            line = proc.stdout.readline().decode(sys.stdout.encoding)
            print(line, end="")
            running = "\n" in line
        print("Exited")

    threading.Thread(target=outloop).start()

    commands = [b"terraform init\n", b"exit\n"]
    i = 0
    while proc.poll() is None and i &lt; len(commands):
        inp = commands[i]
        if inp == "INPUT":
            inp = bytearray(input("") + "\n", sys.stdin.encoding)  # nosec
        if proc.poll() is None:
            proc.stdin.write(inp)
            proc.stdin.flush()
        i += 1


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag298')" href="javascript:;">
PySyft-0.2.9/syft/grid/utils/autoscale/utils/notebook/terraform_notebook.py: 40-72
</a>
<div class="mid" id="frag298" style="display:none"><pre>
def apply():
    """
    args:
    """
    proc = subprocess.Popen(
        "/bin/sh",
        stdout=subprocess.PIPE,
        stdin=subprocess.PIPE,
        stderr=subprocess.STDOUT,
    )

    def outloop():
        running = True
        while running:
            line = proc.stdout.readline().decode(sys.stdout.encoding)
            print(line, end="")
            running = "\n" in line
        print("Exited")

    threading.Thread(target=outloop).start()

    commands = [b"terraform apply\n", "INPUT", b"exit\n"]
    i = 0
    while proc.poll() is None and i &lt; len(commands):
        inp = commands[i]
        if inp == "INPUT":
            inp = bytearray(input("") + "\n", sys.stdin.encoding)  # nosec
        if proc.poll() is None:
            proc.stdin.write(inp)
            proc.stdin.flush()
        i += 1


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag300')" href="javascript:;">
PySyft-0.2.9/syft/grid/utils/autoscale/utils/notebook/terraform_notebook.py: 73-103
</a>
<div class="mid" id="frag300" style="display:none"><pre>
def destroy():
    """
    args:
    """
    proc = subprocess.Popen(
        "/bin/sh",
        stdout=subprocess.PIPE,
        stdin=subprocess.PIPE,
        stderr=subprocess.STDOUT,
    )

    def outloop():
        running = True
        while running:
            line = proc.stdout.readline().decode(sys.stdout.encoding)
            print(line, end="")
            running = "\n" in line
        print("Exited")

    threading.Thread(target=outloop).start()

    commands = [b"terraform destroy\n", "INPUT", b"exit\n"]
    i = 0
    while proc.poll() is None and i &lt; len(commands):
        inp = commands[i]
        if inp == "INPUT":
            inp = bytearray(input("") + "\n", sys.stdin.encoding)  # nosec
        if proc.poll() is None:
            proc.stdin.write(inp)
            proc.stdin.flush()
        i += 1
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 6:</b> &nbsp; 4 fragments, nominal size 22 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag447')" href="javascript:;">
PySyft-0.2.9/syft/generic/pointers/pointer_tensor.py: 52-101
</a>
<div class="mid" id="frag447" style="display:none"><pre>
    def __init__(
        self,
        location: "AbstractWorker" = None,
        id_at_location: Union[str, int] = None,
        owner: "AbstractWorker" = None,
        id: Union[str, int] = None,
        garbage_collect_data: bool = True,
        shape: FrameworkShapeType = None,
        point_to_attr: str = None,
        tags: List[str] = None,
        description: str = None,
    ):
        """Initializes a PointerTensor.

        Args:
            location: An optional AbstractWorker object which points to the worker
                on which this pointer's object can be found.
            id_at_location: An optional string or integer id of the object
                being pointed to.
            owner: An optional AbstractWorker object to specify the worker on which
                the pointer is located. It is also where the pointer is
                registered if register is set to True. Note that this is
                different from the location parameter that specifies where the
                pointer points to.
            id: An optional string or integer id of the PointerTensor.
            garbage_collect_data: If true (default), delete the remote object when the
                pointer is deleted.
            shape: size of the tensor the pointer points to
            point_to_attr: string which can tell a pointer to not point directly to\
                an object, but to point to an attribute of that object such as .child or
                .grad. Note the string can be a chain (i.e., .child.child.child or
                .grad.child.child). Defaults to None, which means don't point to any attr,
                just point to then object corresponding to the id_at_location.
            tags: an optional set of strings corresponding to this tensor
                which this tensor should be searchable for.
            description: an optional string describing the purpose of the tensor.
        """

        super().__init__(
            location=location,
            id_at_location=id_at_location,
            owner=owner,
            id=id,
            garbage_collect_data=garbage_collect_data,
            point_to_attr=point_to_attr,
            tags=tags,
            description=description,
        )
        self._shape = shape

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag511')" href="javascript:;">
PySyft-0.2.9/syft/generic/pointers/pointer_dataset.py: 11-33
</a>
<div class="mid" id="frag511" style="display:none"><pre>
    def __init__(
        self,
        location: "AbstractWorker" = None,
        id_at_location: Union[str, int] = None,
        owner: "AbstractWorker" = None,
        garbage_collect_data: bool = True,
        id: Union[str, int] = None,
        tags: List[str] = None,
        description: str = None,
    ):
        if owner is None:
            owner = sy.framework.hook.local_worker
        self.federated = False  # flag whether it in a federated_dataset object
        super().__init__(
            location=location,
            id_at_location=id_at_location,
            owner=owner,
            garbage_collect_data=garbage_collect_data,
            id=id,
            tags=tags,
            description=description,
        )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag494')" href="javascript:;">
PySyft-0.2.9/syft/generic/pointers/callable_pointer.py: 20-62
</a>
<div class="mid" id="frag494" style="display:none"><pre>
    def __init__(
        self,
        location: "BaseWorker" = None,
        id_at_location: Union[str, int] = None,
        owner: "BaseWorker" = None,
        id: Union[str, int] = None,
        garbage_collect_data: bool = True,
        point_to_attr: str = None,
        tags: List[str] = None,
        description: str = None,
    ):
        """

        Args:
            location: An optional BaseWorker object which points to the worker
                on which this pointer's object can be found.
            id_at_location: An optional string or integer id of the object
                being pointed to.
            owner: An optional BaseWorker object to specify the worker on which
                the pointer is located. It is also where the pointer is
                registered if register is set to True. Note that this is
                different from the location parameter that specifies where the
                pointer points to.
            id: An optional string or integer id of the PointerTensor.
            garbage_collect_data: If true (default), delete the remote object when the
                pointer is deleted.
            point_to_attr: string which can tell a pointer to not point directly to\
                an object, but to point to an attribute of that object such as .child or
                .grad. Note the string can be a chain (i.e., .child.child.child or
                .grad.child.child). Defaults to None, which means don't point to any attr,
                just point to then object corresponding to the id_at_location.
        """
        super().__init__(
            location=location,
            id_at_location=id_at_location,
            owner=owner,
            id=id,
            garbage_collect_data=garbage_collect_data,
            point_to_attr=point_to_attr,
            tags=tags,
            description=description,
        )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag493')" href="javascript:;">
PySyft-0.2.9/syft/generic/pointers/string_pointer.py: 20-39
</a>
<div class="mid" id="frag493" style="display:none"><pre>
    def __init__(
        self,
        location: BaseWorker = None,
        id_at_location: Union[str, int] = None,
        owner: BaseWorker = None,
        id: Union[str, int] = None,
        garbage_collect_data: bool = True,
        tags: List[str] = None,
        description: str = None,
    ):

        super(StringPointer, self).__init__(
            location=location,
            id_at_location=id_at_location,
            owner=owner,
            id=id,
            garbage_collect_data=garbage_collect_data,
            tags=tags,
            description=description,
        )
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 7:</b> &nbsp; 2 fragments, nominal size 27 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag458')" href="javascript:;">
PySyft-0.2.9/syft/generic/pointers/pointer_tensor.py: 181-250
</a>
<div class="mid" id="frag458" style="display:none"><pre>
    def create_pointer(
        tensor,
        location: Union[AbstractWorker, str] = None,
        id_at_location: (str or int) = None,
        owner: Union[AbstractWorker, str] = None,
        ptr_id: (str or int) = None,
        garbage_collect_data=None,
        shape=None,
    ) -&gt; "PointerTensor":
        """Creates a pointer to the "self" FrameworkTensor object.

        This method is called on a FrameworkTensor object, returning a pointer
        to that object. This method is the CORRECT way to create a pointer,
        and the parameters of this method give all possible attributes that
        a pointer can be created with.

        Args:
            location: The AbstractWorker object which points to the worker on which
                this pointer's object can be found. In nearly all cases, this
                is self.owner and so this attribute can usually be left blank.
                Very rarely you may know that you are about to move the Tensor
                to another worker so you can pre-initialize the location
                attribute of the pointer to some other worker, but this is a
                rare exception.
            id_at_location: A string or integer id of the tensor being pointed
                to. Similar to location, this parameter is almost always
                self.id and so you can leave this parameter to None. The only
                exception is if you happen to know that the ID is going to be
                something different than self.id, but again this is very rare
                and most of the time, setting this means that you are probably
                doing something you shouldn't.
            owner: A AbstractWorker parameter to specify the worker on which the
                pointer is located. It is also where the pointer is registered
                if register is set to True.
            ptr_id: A string or integer parameter to specify the id of the pointer
                in case you wish to set it manually for any special reason.
                Otherwise, it will be set randomly.
            garbage_collect_data: If true (default), delete the remote tensor when the
                pointer is deleted.

        Returns:
            A FrameworkTensor[PointerTensor] pointer to self. Note that this
            object itself will likely be wrapped by a FrameworkTensor wrapper.
        """
        if owner is None:
            owner = tensor.owner

        if location is None:
            location = tensor.owner

        owner = tensor.owner.get_worker(owner)
        location = tensor.owner.get_worker(location)

        # previous_pointer = owner.get_pointer_to(location, id_at_location)
        previous_pointer = None

        if previous_pointer is None:
            ptr = PointerTensor(
                location=location,
                id_at_location=id_at_location,
                owner=owner,
                id=ptr_id,
                garbage_collect_data=True if garbage_collect_data is None else garbage_collect_data,
                shape=shape,
                tags=tensor.tags,
                description=tensor.description,
            )

        return ptr

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag498')" href="javascript:;">
PySyft-0.2.9/syft/generic/pointers/object_pointer.py: 82-161
</a>
<div class="mid" id="frag498" style="display:none"><pre>
    def create_pointer(
        obj,
        location: "AbstractWorker" = None,
        id_at_location: (str or int) = None,
        register: bool = False,
        owner: "AbstractWorker" = None,
        ptr_id: (str or int) = None,
        garbage_collect_data=None,
    ) -&gt; "ObjectPointer":
        """Creates a pointer to the "self" FrameworkTensor object.

        This method is called on a FrameworkTensor object, returning a pointer
        to that object. This method is the CORRECT way to create a pointer,
        and the parameters of this method give all possible attributes that
        a pointer can be created with.

        Args:
            location: The AbstractWorker object which points to the worker on which
                this pointer's object can be found. In nearly all cases, this
                is self.owner and so this attribute can usually be left blank.
                Very rarely you may know that you are about to move the Tensor
                to another worker so you can pre-initialize the location
                attribute of the pointer to some other worker, but this is a
                rare exception.
            id_at_location: A string or integer id of the tensor being pointed
                to. Similar to location, this parameter is almost always
                self.id and so you can leave this parameter to None. The only
                exception is if you happen to know that the ID is going to be
                something different than self.id, but again this is very rare
                and most of the time, setting this means that you are probably
                doing something you shouldn't.
            register: A boolean parameter (default False) that determines
                whether to register the new pointer that gets created. This is
                set to false by default because most of the time a pointer is
                initialized in this way so that it can be sent to someone else
                (i.e., "Oh you need to point to my tensor? let me create a
                pointer and send it to you" ). Thus, when a pointer gets
                created, we want to skip being registered on the local worker
                because the pointer is about to be sent elsewhere. However, if
                you are initializing a pointer you intend to keep, then it is
                probably a good idea to register it, especially if there is any
                chance that someone else will initialize a pointer to your
                pointer.
            owner: A AbstractWorker parameter to specify the worker on which the
                pointer is located. It is also where the pointer is registered
                if register is set to True.
            ptr_id: A string or integer parameter to specify the id of the pointer
                in case you wish to set it manually for any special reason.
                Otherwise, it will be set randomly.
            garbage_collect_data: If true (default), delete the remote tensor when the
                pointer is deleted.
            local_autograd: Use autograd system on the local machine instead of PyTorch's
                autograd on the workers.
            preinitialize_grad: Initialize gradient for AutogradTensors to a tensor.

        Returns:
            A FrameworkTensor[ObjectPointer] pointer to self. Note that this
            object itself will likely be wrapped by a FrameworkTensor wrapper.
        """
        if owner is None:
            owner = obj.owner

        if location is None:
            location = obj.owner.id

        owner = obj.owner.get_worker(owner)
        location = obj.owner.get_worker(location)

        ptr = ObjectPointer(
            location=location,
            id_at_location=id_at_location,
            owner=owner,
            id=ptr_id,
            garbage_collect_data=True if garbage_collect_data is None else garbage_collect_data,
            tags=obj.tags,
            description=obj.description,
        )

        return ptr

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 8:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag463')" href="javascript:;">
PySyft-0.2.9/syft/generic/pointers/pointer_tensor.py: 336-346
</a>
<div class="mid" id="frag463" style="display:none"><pre>
    def attr(self, attr_name):
        attr_ptr = PointerTensor(
            id=self.id,
            owner=self.owner,
            location=self.location,
            id_at_location=self.id_at_location,
            point_to_attr=self._create_attr_name_string(attr_name),
        ).wrap(register=False)
        self.__setattr__(attr_name, attr_ptr)
        return attr_ptr

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag507')" href="javascript:;">
PySyft-0.2.9/syft/generic/pointers/object_pointer.py: 356-366
</a>
<div class="mid" id="frag507" style="display:none"><pre>
    def attr(self, attr_name):
        attr_ptr = syft.ObjectPointer(
            id=self.id,
            owner=self.owner,
            location=self.location,
            id_at_location=self.id_at_location,
            point_to_attr=self._create_attr_name_string(attr_name),
        )  # .wrap()
        self.__setattr__(attr_name, attr_ptr)
        return attr_ptr

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 9:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag475')" href="javascript:;">
PySyft-0.2.9/syft/generic/pointers/pointer_tensor.py: 536-549
</a>
<div class="mid" id="frag475" style="display:none"><pre>
    def bufferize(worker: AbstractWorker, ptr: "PointerTensor") -&gt; PointerTensorPB:
        protobuf_pointer = PointerTensorPB()

        syft.serde.protobuf.proto.set_protobuf_id(protobuf_pointer.object_id, ptr.id)
        syft.serde.protobuf.proto.set_protobuf_id(protobuf_pointer.location_id, ptr.location.id)
        syft.serde.protobuf.proto.set_protobuf_id(
            protobuf_pointer.object_id_at_location, ptr.id_at_location
        )

        if ptr.point_to_attr:
            protobuf_pointer.point_to_attr = ptr.point_to_attr
        protobuf_pointer.garbage_collect_data = ptr.garbage_collect_data
        return protobuf_pointer

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag521')" href="javascript:;">
PySyft-0.2.9/syft/generic/pointers/pointer_dataset.py: 148-171
</a>
<div class="mid" id="frag521" style="display:none"><pre>
    def bufferize(worker, pointer_obj):
        """
        This method serializes a PointerDataset into a PointerDatasetPB.

        Args:
            pointer_obj (PointerDataset): input PointerDataset to be serialized.

        Returns:
            protobuf_script (PointerDatasetPB): serialized PointerDataset.
        """
        proto_pointer = PointerDatasetPB()
        sy.serde.protobuf.proto.set_protobuf_id(proto_pointer.object_id, pointer_obj.id)
        sy.serde.protobuf.proto.set_protobuf_id(proto_pointer.location_id, pointer_obj.location.id)
        sy.serde.protobuf.proto.set_protobuf_id(
            proto_pointer.object_id_at_location, pointer_obj.id_at_location
        )
        for tag in pointer_obj.tags:
            proto_pointer.tags.append(tag)

        if pointer_obj.description:
            proto_pointer.description = pointer_obj.description
        proto_pointer.garbage_collect_data = pointer_obj.garbage_collect_data
        return proto_pointer

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 10:</b> &nbsp; 2 fragments, nominal size 29 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag476')" href="javascript:;">
PySyft-0.2.9/syft/generic/pointers/pointer_tensor.py: 551-600
</a>
<div class="mid" id="frag476" style="display:none"><pre>
    def unbufferize(worker: AbstractWorker, protobuf_tensor: PointerTensorPB) -&gt; "PointerTensor":
        # Extract the field values

        obj_id = syft.serde.protobuf.proto.get_protobuf_id(protobuf_tensor.object_id)
        obj_id_at_location = syft.serde.protobuf.proto.get_protobuf_id(
            protobuf_tensor.object_id_at_location
        )
        worker_id = syft.serde.protobuf.proto.get_protobuf_id(protobuf_tensor.location_id)
        point_to_attr = protobuf_tensor.point_to_attr
        shape = syft.hook.create_shape(protobuf_tensor.shape.dims)
        garbage_collect_data = protobuf_tensor.garbage_collect_data

        # If the pointer received is pointing at the current worker, we load the tensor instead
        if worker_id == worker.id:
            tensor = worker.get_obj(obj_id_at_location)

            if point_to_attr is not None and tensor is not None:

                point_to_attrs = point_to_attr.split(".")
                for attr in point_to_attrs:
                    if len(attr) &gt; 0:
                        tensor = getattr(tensor, attr)

                if tensor is not None:

                    if not tensor.is_wrapper and not isinstance(tensor, FrameworkTensor):
                        # if the tensor is a wrapper then it doesn't need to be wrapped
                        # if the tensor isn't a wrapper, BUT it's just a plain torch tensor,
                        # then it doesn't need to be wrapped.
                        # if the tensor is not a wrapper BUT it's also not a torch tensor,
                        # then it needs to be wrapped or else it won't be able to be used
                        # by other interfaces
                        tensor = tensor.wrap()

            return tensor
        # Else we keep the same Pointer
        else:
            location = syft.hook.local_worker.get_worker(worker_id)

            ptr = PointerTensor(
                location=location,
                id_at_location=obj_id_at_location,
                owner=worker,
                id=obj_id,
                shape=shape,
                garbage_collect_data=garbage_collect_data,
            )

            return ptr

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag510')" href="javascript:;">
PySyft-0.2.9/syft/generic/pointers/object_pointer.py: 397-457
</a>
<div class="mid" id="frag510" style="display:none"><pre>
    def detail(worker: "AbstractWorker", object_tuple: tuple) -&gt; "ObjectPointer":
        """
        This function reconstructs an ObjectPointer given it's attributes in form of a dictionary.
        We use the spread operator to pass the dict data as arguments
        to the init method of ObjectPointer
        Args:
            worker: the worker doing the deserialization
            tensor_tuple: a tuple holding the attributes of the ObjectPointer
        Returns:
            ObjectPointer: an ObjectPointer
        Examples:
            ptr = detail(data)
        """
        # TODO: fix comment for this and simplifier
        obj_id, id_at_location, worker_id, point_to_attr, garbage_collect_data = object_tuple

        obj_id = syft.serde.msgpack.serde._detail(worker, obj_id)
        id_at_location = syft.serde.msgpack.serde._detail(worker, id_at_location)
        worker_id = syft.serde.msgpack.serde._detail(worker, worker_id)
        point_to_attr = syft.serde.msgpack.serde._detail(worker, point_to_attr)

        # If the pointer received is pointing at the current worker, we load the tensor instead
        if worker_id == worker.id:
            obj = worker.get_obj(id_at_location)

            if point_to_attr is not None and obj is not None:

                point_to_attrs = point_to_attr.split(".")
                for attr in point_to_attrs:
                    if len(attr) &gt; 0:
                        obj = getattr(obj, attr)

                if obj is not None:

                    if not obj.is_wrapper and not isinstance(obj, FrameworkTensor):
                        # if the object is a wrapper then it doesn't need to be wrapped
                        # i the object isn't a wrapper, BUT it's just a plain torch tensor,
                        # then it doesn't need to be wrapped.
                        # if the object is not a wrapper BUT it's also not a framework object,
                        # then it needs to be wrapped or else it won't be able to be used
                        # by other interfaces
                        obj = obj.wrap()

            return obj
        # Else we keep the same Pointer
        else:

            location = syft.hook.local_worker.get_worker(worker_id)

            ptr = ObjectPointer(
                location=location,
                id_at_location=id_at_location,
                owner=worker,
                id=obj_id,
                garbage_collect_data=garbage_collect_data,
            )

            return ptr


### Register the object with hook_args.py ###
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 11:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag499')" href="javascript:;">
PySyft-0.2.9/syft/generic/pointers/object_pointer.py: 162-192
</a>
<div class="mid" id="frag499" style="display:none"><pre>
    def wrap(self, register=True, type=None, **kwargs):
        """Wraps the class inside framework tensor.

        Because PyTorch/TF do not (yet) support functionality for creating
        arbitrary Tensor types (via subclassing torch.Tensor), in order for our
        new tensor types (such as PointerTensor) to be usable by the rest of
        PyTorch/TF (such as PyTorch's layers and loss functions), we need to
        wrap all of our new tensor types inside of a native PyTorch type.

        This function adds a .wrap() function to all of our tensor types (by
        adding it to AbstractTensor), such that (on any custom tensor
        my_tensor), my_tensor.wrap() will return a tensor that is compatible
        with the rest of the PyTorch/TensorFlow API.

        Returns:
            A wrapper tensor of class `type`, or whatever is specified as
            default by the current syft.framework.Tensor.
        """
        wrapper = syft.framework.hook.create_wrapper(type, **kwargs)
        wrapper.child = self
        wrapper.is_wrapper = True
        wrapper.child.parent = weakref.ref(wrapper)

        if self.id is None:
            self.id = syft.ID_PROVIDER.pop()

        if self.owner is not None and register:
            self.owner.register_obj(wrapper, obj_id=self.id)

        return wrapper

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag530')" href="javascript:;">
PySyft-0.2.9/syft/generic/abstract/tensor.py: 24-54
</a>
<div class="mid" id="frag530" style="display:none"><pre>
    def wrap(self, register=True, type=None, **kwargs):
        """Wraps the class inside an empty object of class `type`.

        Because PyTorch/TF do not (yet) support functionality for creating
        arbitrary Tensor types (via subclassing torch.Tensor), in order for our
        new tensor types (such as PointerTensor) to be usable by the rest of
        PyTorch/TF (such as PyTorch's layers and loss functions), we need to
        wrap all of our new tensor types inside of a native PyTorch type.

        This function adds a .wrap() function to all of our tensor types (by
        adding it to AbstractTensor), such that (on any custom tensor
        my_tensor), my_tensor.wrap() will return a tensor that is compatible
        with the rest of the PyTorch/TensorFlow API.

        Returns:
            A wrapper tensor of class `type`, or whatever is specified as
            default by the current syft.framework.Tensor.
        """
        wrapper = sy.framework.hook.create_wrapper(type, **kwargs)
        wrapper.child = self
        wrapper.is_wrapper = True
        wrapper.child.parent = weakref.ref(wrapper)

        if self.id is None:
            self.id = sy.ID_PROVIDER.pop()

        if self.owner is not None and register:
            self.owner.register_obj(wrapper, obj_id=self.id)

        return wrapper

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 12:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag503')" href="javascript:;">
PySyft-0.2.9/syft/generic/pointers/object_pointer.py: 281-320
</a>
<div class="mid" id="frag503" style="display:none"><pre>
    def __str__(self):
        """Returns a string version of this pointer.

        This is primarily for end users to quickly see things about the object.
        This tostring shouldn't be used for anything else though as it's likely
        to change. (aka, don't try to parse it to extract information. Read the
        attribute you need directly). Also, don't use this to-string as a
        serialized form of the pointer.
        """

        type_name = type(self).__name__
        out = (
            f"["
            f"{type_name} | "
            f"{str(self.owner.id)}:{self.id}"
            " -&gt; "
            f"{str(self.location.id)}:{self.id_at_location}"
            f"]"
        )

        if self.point_to_attr is not None:
            out += "::" + str(self.point_to_attr).replace(".", "::")

        big_str = False

        if self.tags is not None and len(self.tags):
            big_str = True
            out += "\n\tTags: "
            for tag in self.tags:
                out += str(tag) + " "

        if big_str and hasattr(self, "shape"):
            out += "\n\tShape: " + str(self.shape)

        if self.description is not None:
            big_str = True
            out += "\n\tDescription: " + str(self.description).split("\n")[0] + "..."

        return out

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag516')" href="javascript:;">
PySyft-0.2.9/syft/generic/pointers/pointer_dataset.py: 57-87
</a>
<div class="mid" id="frag516" style="display:none"><pre>
    def __repr__(self):
        type_name = type(self).__name__
        out = (
            f"["
            f"{type_name} | "
            f"{str(self.owner.id)}:{self.id}"
            " -&gt; "
            f"{str(self.location.id)}:{self.id_at_location}"
            f"]"
        )

        if self.point_to_attr is not None:
            out += "::" + str(self.point_to_attr).replace(".", "::")

        big_str = False

        if self.tags is not None and len(self.tags):
            big_str = True
            out += "\n\tTags: "
            for tag in self.tags:
                out += str(tag) + " "

        if big_str and hasattr(self, "shape"):
            out += "\n\tShape: " + str(self.shape)

        if self.description is not None:
            big_str = True
            out += "\n\tDescription: " + str(self.description).split("\n")[0] + "..."

        return out

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 13:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag671')" href="javascript:;">
PySyft-0.2.9/syft/exceptions.py: 150-169
</a>
<div class="mid" id="frag671" style="display:none"><pre>
    def simplify(worker: "sy.workers.AbstractWorker", e):
        """
        Serialize information about an Exception which was raised to forward it
        """
        # Get information about the exception: type of error,  traceback
        tp = type(e)
        tb = e.__traceback__
        # Serialize the traceback
        traceback_str = "Traceback (most recent call last):\n" + "".join(traceback.format_tb(tb))
        # Include special attributes if relevant
        try:
            attributes = e.get_attributes()
        except AttributeError:
            attributes = {}
        return (
            sy.serde.msgpack.serde._simplify(worker, tp.__name__),
            sy.serde.msgpack.serde._simplify(worker, traceback_str),
            sy.serde.msgpack.serde._simplify(worker, attributes),
        )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag675')" href="javascript:;">
PySyft-0.2.9/syft/exceptions.py: 241-260
</a>
<div class="mid" id="frag675" style="display:none"><pre>
    def simplify(worker: "sy.workers.AbstractWorker", e):
        """
        Serialize information about an Exception which was raised to forward it
        """
        # Get information about the exception: type of error,  traceback
        tp = type(e)
        tb = e.__traceback__
        # Serialize the traceback
        traceback_str = "Traceback (most recent call last):\n" + "".join(traceback.format_tb(tb))
        # Include special attributes if relevant
        try:
            attributes = e.get_attributes()
        except AttributeError:
            attributes = {}
        return (
            sy.serde.msgpack.serde._simplify(worker, tp.__name__),
            sy.serde.msgpack.serde._simplify(worker, traceback_str),
            sy.serde.msgpack.serde._simplify(worker, attributes),
        )

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 14:</b> &nbsp; 4 fragments, nominal size 14 lines, similarity 85%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag672')" href="javascript:;">
PySyft-0.2.9/syft/exceptions.py: 171-192
</a>
<div class="mid" id="frag672" style="display:none"><pre>
    def detail(worker: "sy.workers.AbstractWorker", error_tuple: Tuple[str, str, dict]):
        """
        Detail and re-raise an Exception forwarded by another worker
        """
        error_name, traceback_str, attributes = error_tuple
        error_name = sy.serde.msgpack.serde._detail(worker, error_name)
        traceback_str = sy.serde.msgpack.serde._detail(worker, traceback_str)
        attributes = sy.serde.msgpack.serde._detail(worker, attributes)
        # De-serialize the traceback
        tb = Traceback.from_string(traceback_str)
        # Check that the error belongs to a valid set of Exceptions
        if error_name in dir(sy.exceptions):
            error_type = getattr(sy.exceptions, error_name)
            error = error_type()
            # Include special attributes if any
            for attr_name, attr in attributes.items():
                setattr(error, attr_name, attr)
            reraise(error_type, error, tb.as_traceback())
        else:
            raise ValueError(f"Invalid Exception returned:\n{traceback_str}")


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag674')" href="javascript:;">
PySyft-0.2.9/syft/exceptions.py: 215-235
</a>
<div class="mid" id="frag674" style="display:none"><pre>
    def detail(worker: "sy.workers.AbstractWorker", error_tuple: Tuple[str, str, dict]):
        """
        Detail and re-raise an Exception forwarded by another worker
        """
        error_name, traceback_str, attributes = error_tuple
        error_name, traceback_str = error_name.decode("utf-8"), traceback_str.decode("utf-8")
        attributes = sy.serde.msgpack.serde._detail(worker, attributes)
        # De-serialize the traceback
        tb = Traceback.from_string(traceback_str)
        # Check that the error belongs to a valid set of Exceptions
        if error_name in dir(sy.exceptions):
            error_type = getattr(sy.exceptions, error_name)
            error = error_type()
            # Include special attributes if any
            for attr_name, attr in attributes.items():
                setattr(error, attr_name, attr)
            reraise(error_type, error, tb.as_traceback())
        else:
            raise ValueError(f"Invalid Exception returned:\n{traceback_str}")


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag681')" href="javascript:;">
PySyft-0.2.9/syft/exceptions.py: 378-399
</a>
<div class="mid" id="frag681" style="display:none"><pre>
    def detail(worker: "sy.workers.AbstractWorker", error_tuple: Tuple[str, str, dict]):
        """
        Detail and re-raise an Exception forwarded by another worker
        """
        error_name, traceback_str, attributes = error_tuple
        error_name = sy.serde.msgpack.serde._detail(worker, error_name)
        traceback_str = sy.serde.msgpack.serde._detail(worker, traceback_str)
        attributes = sy.serde.msgpack.serde._detail(worker, attributes)
        # De-serialize the traceback
        tb = Traceback.from_string(traceback_str)
        # Check that the error belongs to a valid set of Exceptions
        if error_name in dir(sy.exceptions):
            error_type = getattr(sy.exceptions, error_name)
            error = error_type()
            # Include special attributes if any
            for attr_name, attr in attributes.items():
                setattr(error, attr_name, attr)
            reraise(error_type, error, tb.as_traceback())
        else:
            raise ValueError(f"Invalid Exception returned:\n{traceback_str}")


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag676')" href="javascript:;">
PySyft-0.2.9/syft/exceptions.py: 262-283
</a>
<div class="mid" id="frag676" style="display:none"><pre>
    def detail(worker: "sy.workers.AbstractWorker", error_tuple: Tuple[str, str, dict]):
        """
        Detail and re-raise an Exception forwarded by another worker
        """
        error_name, traceback_str, attributes = error_tuple
        error_name = sy.serde.msgpack.serde._detail(worker, error_name)
        traceback_str = sy.serde.msgpack.serde._detail(worker, traceback_str)
        attributes = sy.serde.msgpack.serde._detail(worker, attributes)
        # De-serialize the traceback
        tb = Traceback.from_string(traceback_str)
        # Check that the error belongs to a valid set of Exceptions
        if error_name in dir(sy.exceptions):
            error_type = getattr(sy.exceptions, error_name)
            error = error_type()
            # Include special attributes if any
            for attr_name, attr in attributes.items():
                setattr(error, attr_name, attr)
            reraise(error_type, error, tb.as_traceback())
        else:
            raise ValueError(f"Invalid Exception returned:\n{traceback_str}")


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 15:</b> &nbsp; 2 fragments, nominal size 39 lines, similarity 87%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag705')" href="javascript:;">
PySyft-0.2.9/test/torch/test_hook.py: 251-315
</a>
<div class="mid" id="frag705" style="display:none"><pre>
def test_RNN_grad_set_backpropagation(workers):
    """Perform backpropagation at a remote worker and check if the gradient updates
    and properly computed within the model"""

    alice = workers["alice"]

    class RNN(nn.Module):
        def __init__(self, input_size, hidden_size, output_size):
            super(RNN, self).__init__()
            self.hidden_size = hidden_size
            self.i2h = nn.Linear(input_size + hidden_size, hidden_size)
            self.i2o = nn.Linear(input_size + hidden_size, output_size)
            self.softmax = nn.LogSoftmax(dim=1)

        def forward(self, input, hidden):
            combined = torch.cat((input, hidden), 1)
            hidden = self.i2h(combined)
            output = self.i2o(combined)
            output = self.softmax(output)
            return output, hidden

        def initHidden(self):
            return torch.zeros(1, self.hidden_size)

    # let's initialize a simple RNN
    n_hidden = 128
    n_letters = 57
    n_categories = 18

    rnn = RNN(n_letters, n_hidden, n_categories)

    # Let's send the model to alice, who will be responsible for the tiny computation
    alice_model = rnn.copy().send(alice)

    # Simple input for the Recurrent Neural Network
    input_tensor = torch.zeros(size=(1, 57))
    # Just set a random category for it
    input_tensor[0][20] = 1
    alice_input = input_tensor.copy().send(alice)

    label_tensor = torch.randint(low=0, high=(n_categories - 1), size=(1,))
    alice_label = label_tensor.send(alice)

    hidden_layer = alice_model.initHidden()
    alice_hidden_layer = hidden_layer.send(alice)
    # Forward pass into the NN and its hidden layers, notice how it goes sequentially
    output, alice_hidden_layer = alice_model(alice_input, alice_hidden_layer)
    criterion = nn.NLLLoss()
    loss = criterion(output, alice_label)
    # time to backpropagate...
    loss.backward()

    # now let's get the model and check if its parameters are indeed there
    model_got = alice_model.get()

    learning_rate = 0.005

    # If the gradients are there, then the backpropagation did indeed complete successfully
    for param in model_got.parameters():
        # param.grad.data would raise an exception in case it is none,
        # so we better check it beforehand
        assert param.grad.data is not None
        param.data.add_(-learning_rate, param.grad.data)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag709')" href="javascript:;">
PySyft-0.2.9/test/torch/test_hook.py: 316-382
</a>
<div class="mid" id="frag709" style="display:none"><pre>
def test_local_remote_gradient_clipping(workers):
    """
    Real test case of gradient clipping for the remote and
    local parameters of an RNN
    """
    alice = workers["alice"]

    class RNN(nn.Module):
        def __init__(self, input_size, hidden_size, output_size):
            super(RNN, self).__init__()
            self.hidden_size = hidden_size
            self.i2h = nn.Linear(input_size + hidden_size, hidden_size)
            self.i2o = nn.Linear(input_size + hidden_size, output_size)
            self.softmax = nn.LogSoftmax(dim=1)

        def forward(self, input, hidden):
            combined = torch.cat((input, hidden), 1)
            hidden = self.i2h(combined)
            output = self.i2o(combined)
            output = self.softmax(output)
            return output, hidden

        def initHidden(self):
            return torch.zeros(1, self.hidden_size)

    # let's initialize a simple RNN
    n_hidden = 128
    n_letters = 57
    n_categories = 18

    rnn = RNN(n_letters, n_hidden, n_categories)

    # Let's send the model to alice, who will be responsible for the tiny computation
    alice_model = rnn.copy().send(alice)

    # Simple input for the Recurrent Neural Network
    input_tensor = torch.zeros(size=(1, 57))
    # Just set a random category for it
    input_tensor[0][20] = 1
    alice_input = input_tensor.copy().send(alice)

    label_tensor = torch.randint(low=0, high=(n_categories - 1), size=(1,))
    alice_label = label_tensor.send(alice)

    hidden_layer = alice_model.initHidden()
    alice_hidden_layer = hidden_layer.send(alice)
    # Forward pass into the NN and its hidden layers, notice how it goes sequentially
    output, alice_hidden_layer = alice_model(alice_input, alice_hidden_layer)
    criterion = nn.NLLLoss()
    loss = criterion(output, alice_label)
    # time to backpropagate...
    loss.backward()

    # Remote gradient clipping
    remote_parameters = alice_model.parameters()
    total_norm_remote = nn.utils.clip_grad_norm_(remote_parameters, 2)

    # Local gradient clipping
    local_alice_model = alice_model.get()
    local_parameters = local_alice_model.parameters()
    total_norm_local = nn.utils.clip_grad_norm_(local_parameters, 2)

    # Is the output of the remote gradient clipping version equal to
    # the output of the local gradient clipping version?
    assert torch.isclose(total_norm_remote.get(), total_norm_local, atol=1e-4)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 16:</b> &nbsp; 2 fragments, nominal size 25 lines, similarity 85%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag719')" href="javascript:;">
PySyft-0.2.9/test/torch/nn/test_functional.py: 135-170
</a>
<div class="mid" id="frag719" style="display:none"><pre>
def test_torch_nn_functional_maxpool(workers, protocol):
    bob, alice, james = (workers["bob"], workers["alice"], workers["james"])
    # 4d
    enc_tensor = torch.tensor(
        [[[[1, 1, 2, 4], [5, 6, 7, 8], [3, 2, 1, 0], [1, 2, 3, 4]]]], dtype=torch.float
    )
    enc_tensor = enc_tensor.fix_prec().share(bob, alice, crypto_provider=james)
    r_max = F.max_pool2d(enc_tensor, kernel_size=2)
    r_max = r_max.get().float_prec()
    exp_max = torch.tensor([[[[6.0, 8.0], [3.0, 4.0]]]])
    assert (r_max == exp_max).all()
    # 4d kernel_size = 3
    r_max = F.max_pool2d(enc_tensor, kernel_size=3, stride=1)
    r_max = r_max.get().float_prec()
    exp_max = torch.tensor([[[[7.0, 8.0], [7.0, 8.0]]]])
    assert (r_max == exp_max).all()
    # 3d
    enc_tensor = torch.tensor(
        [[[1, 1, 2, 4], [5, 6, 7, 8], [3, 2, 1, 0], [1, 2, 3, 4]]], dtype=torch.float
    )
    enc_tensor = enc_tensor.fix_prec().share(bob, alice, crypto_provider=james)
    r_max = F.max_pool2d(enc_tensor, kernel_size=2)
    r_max = r_max.get().float_prec()
    exp_max = torch.tensor([[[6.0, 8.0], [3.0, 4.0]]])
    assert (r_max == exp_max).all()
    # 2d
    enc_tensor = torch.tensor(
        [[1, 1, 2, 4], [5, 6, 7, 8], [3, 2, 1, 0], [1, 2, 3, 4]], dtype=torch.float
    )
    enc_tensor = enc_tensor.fix_prec().share(bob, alice, crypto_provider=james)
    r_max = F.max_pool2d(enc_tensor, kernel_size=2)
    r_max = r_max.get().float_prec()
    exp_max = torch.tensor([[6.0, 8.0], [3.0, 4.0]])
    assert (r_max == exp_max).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag720')" href="javascript:;">
PySyft-0.2.9/test/torch/nn/test_functional.py: 172-199
</a>
<div class="mid" id="frag720" style="display:none"><pre>
def test_torch_nn_functional_avgpool(workers, protocol):
    bob, alice, james = (workers["bob"], workers["alice"], workers["james"])
    enc_tensor = torch.tensor(
        [[[[1, 1, 2, 4], [5, 6, 7, 8], [3, 2, 1, 0], [1, 2, 3, 4]]]], dtype=torch.float
    )
    enc_tensor = enc_tensor.fix_prec().share(bob, alice, crypto_provider=james)
    r_avg = F.avg_pool2d(enc_tensor, kernel_size=2)
    r_avg = r_avg.get().float_prec()
    exp_avg = torch.tensor([[[[3.2500, 5.2500], [2.0000, 2.0000]]]])
    assert (r_avg == exp_avg).all()
    # 3d
    enc_tensor = torch.tensor(
        [[[1, 1, 2, 4], [5, 6, 7, 8], [3, 2, 1, 0], [1, 2, 3, 4]]], dtype=torch.float
    )
    enc_tensor = enc_tensor.fix_prec().share(bob, alice, crypto_provider=james)
    r_avg = F.avg_pool2d(enc_tensor, kernel_size=2)
    r_avg = r_avg.get().float_prec()
    exp_avg = torch.tensor([[[3.2500, 5.2500], [2.0000, 2.0000]]])
    assert (r_avg == exp_avg).all()
    # 2d
    enc_tensor = torch.tensor(
        [[1, 1, 2, 4], [5, 6, 7, 8], [3, 2, 1, 0], [1, 2, 3, 4]], dtype=torch.float
    )
    enc_tensor = enc_tensor.fix_prec().share(bob, alice, crypto_provider=james)
    r_avg = F.avg_pool2d(enc_tensor, kernel_size=2)
    r_avg = r_avg.get().float_prec()
    exp_avg = torch.tensor([[3.2500, 5.2500], [2.0000, 2.0000]])
    assert (r_avg == exp_avg).all()
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 17:</b> &nbsp; 3 fragments, nominal size 18 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag727')" href="javascript:;">
PySyft-0.2.9/test/torch/nn/test_nn.py: 128-165
</a>
<div class="mid" id="frag727" style="display:none"><pre>
def test_RNNCell():
    """
    Test the RNNCell module to ensure that it produces the exact same
    output as the primary torch implementation, in the same order.
    """

    # Disable mkldnn to avoid rounding errors due to difference in implementation
    mkldnn_enabled_init = torch._C._get_mkldnn_enabled()
    torch._C._set_mkldnn_enabled(False)

    batch_size = 5
    input_size = 10
    hidden_size = 50

    test_input = torch.rand(batch_size, input_size)
    test_hidden = torch.rand(batch_size, hidden_size)

    # RNNCell implemented in pysyft
    rnn_syft = syft_nn.RNNCell(input_size, hidden_size, True, "tanh")

    # RNNCell implemented in original pytorch
    rnn_torch = nn.RNNCell(input_size, hidden_size, True, "tanh")

    # Make sure the weights of both RNNCell are identical
    rnn_syft.fc_xh.weight = rnn_torch.weight_ih
    rnn_syft.fc_hh.weight = rnn_torch.weight_hh
    rnn_syft.fc_xh.bias = rnn_torch.bias_ih
    rnn_syft.fc_hh.bias = rnn_torch.bias_hh

    output_syft = rnn_syft(test_input, test_hidden)
    output_torch = rnn_torch(test_input, test_hidden)

    assert torch.allclose(output_syft, output_torch, atol=1e-2)

    # Reset mkldnn to the original state
    torch._C._set_mkldnn_enabled(mkldnn_enabled_init)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag728')" href="javascript:;">
PySyft-0.2.9/test/torch/nn/test_nn.py: 166-203
</a>
<div class="mid" id="frag728" style="display:none"><pre>
def test_GRUCell():
    """
    Test the GRUCell module to ensure that it produces the exact same
    output as the primary torch implementation, in the same order.
    """

    # Disable mkldnn to avoid rounding errors due to difference in implementation
    mkldnn_enabled_init = torch._C._get_mkldnn_enabled()
    torch._C._set_mkldnn_enabled(False)

    batch_size = 5
    input_size = 10
    hidden_size = 50

    test_input = torch.rand(batch_size, input_size)
    test_hidden = torch.rand(batch_size, hidden_size)

    # GRUCell implemented in pysyft
    rnn_syft = syft_nn.GRUCell(input_size, hidden_size, True)

    # GRUCell implemented in original pytorch
    rnn_torch = nn.GRUCell(input_size, hidden_size, True)

    # Make sure the weights of both GRUCell are identical
    rnn_syft.fc_xh.weight = rnn_torch.weight_ih
    rnn_syft.fc_hh.weight = rnn_torch.weight_hh
    rnn_syft.fc_xh.bias = rnn_torch.bias_ih
    rnn_syft.fc_hh.bias = rnn_torch.bias_hh

    output_syft = rnn_syft(test_input, test_hidden)
    output_torch = rnn_torch(test_input, test_hidden)

    # Reset mkldnn to the original state
    torch._C._set_mkldnn_enabled(mkldnn_enabled_init)

    assert torch.all(torch.lt(torch.abs(output_syft - output_torch), 1e-6))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag729')" href="javascript:;">
PySyft-0.2.9/test/torch/nn/test_nn.py: 204-244
</a>
<div class="mid" id="frag729" style="display:none"><pre>
def test_LSTMCell():
    """
    Test the LSTMCell module to ensure that it produces the exact same
    output as the primary torch implementation, in the same order.
    """

    # Disable mkldnn to avoid rounding errors due to difference in implementation
    mkldnn_enabled_init = torch._C._get_mkldnn_enabled()
    torch._C._set_mkldnn_enabled(False)

    batch_size = 5
    input_size = 10
    hidden_size = 50

    test_input = torch.rand(batch_size, input_size)
    test_hidden_state = torch.rand(batch_size, hidden_size)
    test_cell_state = torch.rand(batch_size, hidden_size)

    # LSTMCell implemented in pysyft
    rnn_syft = syft_nn.LSTMCell(input_size, hidden_size, True)

    # LSTMCell implemented in original pytorch
    rnn_torch = nn.LSTMCell(input_size, hidden_size, True)

    # Make sure the weights of both LSTMCell are identical
    rnn_syft.fc_xh.weight = rnn_torch.weight_ih
    rnn_syft.fc_hh.weight = rnn_torch.weight_hh
    rnn_syft.fc_xh.bias = rnn_torch.bias_ih
    rnn_syft.fc_hh.bias = rnn_torch.bias_hh

    hidden_syft, cell_syft = rnn_syft(test_input, (test_hidden_state, test_cell_state))
    hidden_torch, cell_torch = rnn_torch(test_input, (test_hidden_state, test_cell_state))

    # Reset mkldnn to the original state
    torch._C._set_mkldnn_enabled(mkldnn_enabled_init)

    # Assert the hidden_state and cell_state of both models are identical separately
    assert torch.all(torch.lt(torch.abs(hidden_syft - hidden_torch), 1e-6))
    assert torch.all(torch.lt(torch.abs(cell_syft - cell_torch), 1e-6))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 18:</b> &nbsp; 3 fragments, nominal size 21 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag730')" href="javascript:;">
PySyft-0.2.9/test/torch/nn/test_nn.py: 245-286
</a>
<div class="mid" id="frag730" style="display:none"><pre>
def test_RNN():
    """
    Test the RNN module to ensure that it produces the exact same
    output as the primary torch implementation, in the same order.
    """

    # Disable mkldnn to avoid rounding errors due to difference in implementation
    mkldnn_enabled_init = torch._C._get_mkldnn_enabled()
    torch._C._set_mkldnn_enabled(False)

    batch_size = 5
    input_size = 10
    hidden_size = 50
    num_layers = 1
    seq_len = 8

    test_input = torch.rand(seq_len, batch_size, input_size)
    test_hidden_state = torch.rand(num_layers, batch_size, hidden_size)

    # RNN implemented in pysyft
    rnn_syft = syft_nn.RNN(input_size, hidden_size, num_layers)

    # RNN implemented in original pytorch
    rnn_torch = nn.RNN(input_size, hidden_size, num_layers)

    # Make sure the weights of both RNN are identical
    rnn_syft.rnn_forward[0].fc_xh.weight = rnn_torch.weight_ih_l0
    rnn_syft.rnn_forward[0].fc_xh.bias = rnn_torch.bias_ih_l0
    rnn_syft.rnn_forward[0].fc_hh.weight = rnn_torch.weight_hh_l0
    rnn_syft.rnn_forward[0].fc_hh.bias = rnn_torch.bias_hh_l0

    output_syft, hidden_syft = rnn_syft(test_input, test_hidden_state)
    output_torch, hidden_torch = rnn_torch(test_input, test_hidden_state)

    # Reset mkldnn to the original state
    torch._C._set_mkldnn_enabled(mkldnn_enabled_init)

    # Assert the hidden_state and output of both models are identical separately
    assert torch.all(torch.lt(torch.abs(output_syft - output_torch), 1e-6))
    assert torch.all(torch.lt(torch.abs(hidden_syft - hidden_torch), 1e-6))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag731')" href="javascript:;">
PySyft-0.2.9/test/torch/nn/test_nn.py: 287-328
</a>
<div class="mid" id="frag731" style="display:none"><pre>
def test_GRU():
    """
    Test the GRU module to ensure that it produces the exact same
    output as the primary torch implementation, in the same order.
    """

    # Disable mkldnn to avoid rounding errors due to difference in implementation
    mkldnn_enabled_init = torch._C._get_mkldnn_enabled()
    torch._C._set_mkldnn_enabled(False)

    batch_size = 5
    input_size = 10
    hidden_size = 50
    num_layers = 1
    seq_len = 8

    test_input = torch.rand(seq_len, batch_size, input_size)
    test_hidden_state = torch.rand(num_layers, batch_size, hidden_size)

    # GRU implemented in pysyft
    rnn_syft = syft_nn.GRU(input_size, hidden_size, num_layers)

    # GRU implemented in original pytorch
    rnn_torch = nn.GRU(input_size, hidden_size, num_layers)

    # Make sure the weights of both GRU are identical
    rnn_syft.rnn_forward[0].fc_xh.weight = rnn_torch.weight_ih_l0
    rnn_syft.rnn_forward[0].fc_xh.bias = rnn_torch.bias_ih_l0
    rnn_syft.rnn_forward[0].fc_hh.weight = rnn_torch.weight_hh_l0
    rnn_syft.rnn_forward[0].fc_hh.bias = rnn_torch.bias_hh_l0

    output_syft, hidden_syft = rnn_syft(test_input, test_hidden_state)
    output_torch, hidden_torch = rnn_torch(test_input, test_hidden_state)

    # Reset mkldnn to the original state
    torch._C._set_mkldnn_enabled(mkldnn_enabled_init)

    # Assert the hidden_state and output of both models are identical separately
    assert torch.all(torch.lt(torch.abs(output_syft - output_torch), 1e-6))
    assert torch.all(torch.lt(torch.abs(hidden_syft - hidden_torch), 1e-6))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag732')" href="javascript:;">
PySyft-0.2.9/test/torch/nn/test_nn.py: 329-374
</a>
<div class="mid" id="frag732" style="display:none"><pre>
def test_LSTM():
    """
    Test the LSTM module to ensure that it produces the exact same
    output as the primary torch implementation, in the same order.
    """

    # Disable mkldnn to avoid rounding errors due to difference in implementation
    mkldnn_enabled_init = torch._C._get_mkldnn_enabled()
    torch._C._set_mkldnn_enabled(False)

    batch_size = 5
    input_size = 10
    hidden_size = 50
    num_layers = 1
    seq_len = 8

    test_input = torch.rand(seq_len, batch_size, input_size)
    test_hidden_state = torch.rand(num_layers, batch_size, hidden_size)
    test_cell_state = torch.rand(num_layers, batch_size, hidden_size)

    # LSTM implemented in pysyft
    rnn_syft = syft_nn.LSTM(input_size, hidden_size, num_layers)

    # LSTM implemented in original pytorch
    rnn_torch = nn.LSTM(input_size, hidden_size, num_layers)

    # Make sure the weights of both LSTM are identical
    rnn_syft.rnn_forward[0].fc_xh.weight = rnn_torch.weight_ih_l0
    rnn_syft.rnn_forward[0].fc_xh.bias = rnn_torch.bias_ih_l0
    rnn_syft.rnn_forward[0].fc_hh.weight = rnn_torch.weight_hh_l0
    rnn_syft.rnn_forward[0].fc_hh.bias = rnn_torch.bias_hh_l0

    output_syft, (hidden_syft, cell_syft) = rnn_syft(
        test_input, (test_hidden_state, test_cell_state)
    )
    output_torch, (hidden_torch, cell_torch) = rnn_torch(
        test_input, (test_hidden_state, test_cell_state)
    )

    # Reset mkldnn to the original state
    torch._C._set_mkldnn_enabled(mkldnn_enabled_init)

    # Assert the hidden_state, cell_state and output of both models are identical separately
    assert torch.all(torch.lt(torch.abs(output_syft - output_torch), 1e-6))
    assert torch.all(torch.lt(torch.abs(hidden_syft - hidden_torch), 1e-6))
    assert torch.all(torch.lt(torch.abs(cell_syft - cell_torch), 1e-6))
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 19:</b> &nbsp; 6 fragments, nominal size 14 lines, similarity 85%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag758')" href="javascript:;">
PySyft-0.2.9/test/torch/tensors/test_fv.py: 413-430
</a>
<div class="mid" id="frag758" style="display:none"><pre>
def test_fv_add_cipher_cipher(int1, int2):
    ctx = Context(EncryptionParams(64, CoeffModulus().create(64, [30, 30]), 64))
    keys = KeyGenerator(ctx).keygen()
    encoder = IntegerEncoder(ctx)
    encryptor = Encryptor(ctx, keys[1])  # keys[1] = public_key
    decryptor = Decryptor(ctx, keys[0])  # keys[0] = secret_key
    evaluator = Evaluator(ctx)

    op1 = encryptor.encrypt(encoder.encode(int1))
    op2 = encryptor.encrypt(encoder.encode(int2))
    assert (
        int1 + int2
        == encoder.decode(decryptor.decrypt(evaluator._add_cipher_cipher(op1, op2)))
        == encoder.decode(decryptor.decrypt(evaluator.add(op1, op2)))
        == encoder.decode(decryptor.decrypt(evaluator.add(op2, op1)))
    )


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag763')" href="javascript:;">
PySyft-0.2.9/test/torch/tensors/test_fv.py: 506-524
</a>
<div class="mid" id="frag763" style="display:none"><pre>
def test_fv_sub_cipher_plain(int1, int2):
    ctx = Context(EncryptionParams(64, CoeffModulus().create(64, [30, 30]), 64))
    keys = KeyGenerator(ctx).keygen()
    encoder = IntegerEncoder(ctx)
    encryptor = Encryptor(ctx, keys[1])  # keys[1] = public_key
    decryptor = Decryptor(ctx, keys[0])  # keys[0] = secret_key
    evaluator = Evaluator(ctx)

    op1 = encryptor.encrypt(encoder.encode(int1))
    op2 = encoder.encode(int2)

    assert (
        int1 - int2
        == encoder.decode(decryptor.decrypt(evaluator._sub_cipher_plain(op1, op2)))
        == encoder.decode(decryptor.decrypt(evaluator.sub(op1, op2)))
        == encoder.decode(decryptor.decrypt(evaluator.sub(op2, op1)))
    )


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag762')" href="javascript:;">
PySyft-0.2.9/test/torch/tensors/test_fv.py: 485-502
</a>
<div class="mid" id="frag762" style="display:none"><pre>
def test_fv_sub_cipher_cipher(int1, int2):
    ctx = Context(EncryptionParams(64, CoeffModulus().create(64, [30, 30]), 64))
    keys = KeyGenerator(ctx).keygen()
    encoder = IntegerEncoder(ctx)
    encryptor = Encryptor(ctx, keys[1])  # keys[1] = public_key
    decryptor = Decryptor(ctx, keys[0])  # keys[0] = secret_key
    evaluator = Evaluator(ctx)

    op1 = encryptor.encrypt(encoder.encode(int1))
    op2 = encryptor.encrypt(encoder.encode(int2))
    assert (
        int1 - int2
        == encoder.decode(decryptor.decrypt(evaluator._sub_cipher_cipher(op1, op2)))
        == encoder.decode(decryptor.decrypt(evaluator.sub(op1, op2)))
        == -encoder.decode(decryptor.decrypt(evaluator.sub(op2, op1)))
    )


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag764')" href="javascript:;">
PySyft-0.2.9/test/torch/tensors/test_fv.py: 538-555
</a>
<div class="mid" id="frag764" style="display:none"><pre>
def test_fv_mul_cipher_cipher(int1, int2):
    ctx = Context(EncryptionParams(64, CoeffModulus().create(64, [30, 30]), 64))
    keys = KeyGenerator(ctx).keygen()
    encoder = IntegerEncoder(ctx)
    encryptor = Encryptor(ctx, keys[1])  # keys[1] = public_key
    decryptor = Decryptor(ctx, keys[0])  # keys[0] = secret_key
    evaluator = Evaluator(ctx)

    op1 = encryptor.encrypt(encoder.encode(int1))
    op2 = encryptor.encrypt(encoder.encode(int2))
    assert (
        int1 * int2
        == encoder.decode(decryptor.decrypt(evaluator._mul_cipher_cipher(op1, op2)))
        == encoder.decode(decryptor.decrypt(evaluator.mul(op1, op2)))
        == encoder.decode(decryptor.decrypt(evaluator.mul(op2, op1)))
    )


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag759')" href="javascript:;">
PySyft-0.2.9/test/torch/tensors/test_fv.py: 434-452
</a>
<div class="mid" id="frag759" style="display:none"><pre>
def test_fv_add_cipher_plain(int1, int2):
    ctx = Context(EncryptionParams(64, CoeffModulus().create(64, [30, 30]), 64))
    keys = KeyGenerator(ctx).keygen()
    encoder = IntegerEncoder(ctx)
    encryptor = Encryptor(ctx, keys[1])  # keys[1] = public_key
    decryptor = Decryptor(ctx, keys[0])  # keys[0] = secret_key
    evaluator = Evaluator(ctx)

    op1 = encryptor.encrypt(encoder.encode(int1))
    op2 = encoder.encode(int2)

    assert (
        int1 + int2
        == encoder.decode(decryptor.decrypt(evaluator._add_cipher_plain(op1, op2)))
        == encoder.decode(decryptor.decrypt(evaluator.add(op1, op2)))
        == encoder.decode(decryptor.decrypt(evaluator.add(op2, op1)))
    )


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag765')" href="javascript:;">
PySyft-0.2.9/test/torch/tensors/test_fv.py: 569-586
</a>
<div class="mid" id="frag765" style="display:none"><pre>
def test_fv_mul_cipher_plain(int1, int2):
    ctx = Context(EncryptionParams(64, CoeffModulus().create(64, [30, 30]), 64))
    keys = KeyGenerator(ctx).keygen()
    encoder = IntegerEncoder(ctx)
    encryptor = Encryptor(ctx, keys[1])  # keys[1] = public_key
    decryptor = Decryptor(ctx, keys[0])  # keys[0] = secret_key
    evaluator = Evaluator(ctx)

    op1 = encryptor.encrypt(encoder.encode(int1))
    op2 = encoder.encode(int2)
    assert (
        int1 * int2
        == encoder.decode(decryptor.decrypt(evaluator._mul_cipher_plain(op1, op2)))
        == encoder.decode(decryptor.decrypt(evaluator.mul(op1, op2)))
        == encoder.decode(decryptor.decrypt(evaluator.mul(op2, op1)))
    )


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 20:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag760')" href="javascript:;">
PySyft-0.2.9/test/torch/tensors/test_fv.py: 456-469
</a>
<div class="mid" id="frag760" style="display:none"><pre>
def test_fv_add_plain_plain(int1, int2):
    ctx = Context(EncryptionParams(64, CoeffModulus().create(64, [30, 30]), 64))
    encoder = IntegerEncoder(ctx)
    evaluator = Evaluator(ctx)
    op1 = encoder.encode(int1)
    op2 = encoder.encode(int2)
    assert (
        int1 + int2
        == encoder.decode(evaluator._add_plain_plain(op1, op2))
        == encoder.decode(evaluator.add(op1, op2))
        == encoder.decode(evaluator.add(op2, op1))
    )


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag766')" href="javascript:;">
PySyft-0.2.9/test/torch/tensors/test_fv.py: 600-614
</a>
<div class="mid" id="frag766" style="display:none"><pre>
def test_fv_mul_plain_plain(int1, int2):
    ctx = Context(EncryptionParams(64, CoeffModulus().create(64, [30, 30]), 64))
    encoder = IntegerEncoder(ctx)
    evaluator = Evaluator(ctx)

    op1 = encoder.encode(int1)
    op2 = encoder.encode(int2)
    assert (
        int1 * int2
        == encoder.decode(evaluator._mul_plain_plain(op1, op2))
        == encoder.decode(evaluator.mul(op1, op2))
        == encoder.decode(evaluator._mul_plain_plain(op2, op1))
    )


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 21:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag770')" href="javascript:;">
PySyft-0.2.9/test/torch/tensors/test_fv.py: 678-695
</a>
<div class="mid" id="frag770" style="display:none"><pre>
def test_fv_relin(val1, val2):
    ctx = Context(EncryptionParams(64, CoeffModulus().create(64, [30, 30]), 64))
    keygenerator = KeyGenerator(ctx)
    keys = keygenerator.keygen()
    relin_key = keygenerator.get_relin_keys()
    encoder = IntegerEncoder(ctx)
    encryptor = Encryptor(ctx, keys[1])  # keys[1] = public_key
    decryptor = Decryptor(ctx, keys[0])  # keys[0] = secret_key
    evaluator = Evaluator(ctx)

    op1 = encryptor.encrypt(encoder.encode(val1))
    op2 = encryptor.encrypt(encoder.encode(val2))
    temp_prod = evaluator.mul(op1, op2)
    relin_prod = evaluator.relin(temp_prod, relin_key)
    assert len(temp_prod.data) - 1 == len(relin_prod.data)
    assert val1 * val2 == encoder.decode(decryptor.decrypt(relin_prod))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag771')" href="javascript:;">
PySyft-0.2.9/test/torch/tensors/test_fv.py: 700-717
</a>
<div class="mid" id="frag771" style="display:none"><pre>
def test_fv_relin_exceptions(val1, val2):
    ctx = Context(EncryptionParams(64, CoeffModulus().create(64, [30, 30]), 64))
    keygenerator = KeyGenerator(ctx)
    keys = keygenerator.keygen()
    relin_key = keygenerator.get_relin_keys()
    encoder = IntegerEncoder(ctx)
    encryptor = Encryptor(ctx, keys[1])  # keys[1] = public_key
    evaluator = Evaluator(ctx)

    op1 = encryptor.encrypt(encoder.encode(val1))
    op2 = encryptor.encrypt(encoder.encode(val2))
    temp_prod = evaluator.mul(op1, op2)

    with pytest.raises(Warning):
        evaluator.relin(op1, relin_key)  # Ciphertext size 2

    with pytest.raises(Exception):
        evaluator.relin(evaluator.mul(temp_prod, val1), relin_key)  # Ciphertext size 4
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 22:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag774')" href="javascript:;">
PySyft-0.2.9/test/torch/tensors/test_parameter.py: 24-41
</a>
<div class="mid" id="frag774" style="display:none"><pre>
def test_param_inplace_send_get(workers):
    tensor = torch.tensor([1.0, -1.0, 3.0, 4.0])
    param = Parameter(data=tensor.clone())
    param_ptr = param.send_(workers["bob"])

    assert param_ptr.id == param.id
    assert id(param_ptr) == id(param)

    param_back = param_ptr.get_()

    assert param_back.id == param_ptr.id
    assert param_back.id == param.id
    assert id(param_back) == id(param_ptr)
    assert id(param_back) == id(param)

    assert (param_back.data == tensor).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1100')" href="javascript:;">
PySyft-0.2.9/test/generic/pointers/test_pointer_tensor.py: 125-143
</a>
<div class="mid" id="frag1100" style="display:none"><pre>
def test_inplace_send_get(workers):
    bob = workers["bob"]

    tensor = torch.tensor([1.0, -1.0, 3.0, 4.0])
    tensor_ptr = tensor.send_(bob)

    assert tensor_ptr.id == tensor.id
    assert id(tensor_ptr) == id(tensor)

    tensor_back = tensor_ptr.get_()

    assert tensor_back.id == tensor_ptr.id
    assert tensor_back.id == tensor.id
    assert id(tensor_back) == id(tensor)
    assert id(tensor_back) == id(tensor)

    assert (tensor_back == tensor).all()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 23:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag833')" href="javascript:;">
PySyft-0.2.9/test/torch/hook/test_hook.py: 17-32
</a>
<div class="mid" id="frag833" style="display:none"><pre>
def test_cuda():  # pragma: no cover
    class Net(torch.nn.Module):
        def __init__(self):
            super(Net, self).__init__()
            self.fc1 = torch.nn.Linear(2, 3)

        def forward(self, x):
            x = torch.nn.functional.relu(self.fc1(x))
            return x

    model = Net()
    assert model.fc1.weight.is_cuda is False
    model = model.cuda()
    assert model.fc1.weight.is_cuda is True


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag836')" href="javascript:;">
PySyft-0.2.9/test/torch/hook/test_hook.py: 34-53
</a>
<div class="mid" id="frag836" style="display:none"><pre>
def test_data():  # pragma: no cover
    class Net(torch.nn.Module):
        def __init__(self):
            super(Net, self).__init__()
            self.fc1 = torch.nn.Linear(2, 3)

        def forward(self, x):
            x = torch.nn.functional.relu(self.fc1(x))
            return x

    model = Net()
    input = torch.tensor([2.0, 4.0])
    out_cpu = model(input)
    assert model.fc1.weight.is_cuda is False
    model = model.cuda()
    assert model.fc1.weight.is_cuda is True
    out_cuda = model(input.cuda())
    assert (out_cpu - out_cuda.cpu() &lt; 1e-3).all()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 24:</b> &nbsp; 2 fragments, nominal size 24 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag852')" href="javascript:;">
PySyft-0.2.9/test/torch/federated/test_utils.py: 29-60
</a>
<div class="mid" id="frag852" style="display:none"><pre>
def test_add_model():
    class Net(th.nn.Module):
        def __init__(self):
            super(Net, self).__init__()
            self.fc1 = th.nn.Linear(2, 2)

    weight1 = th.tensor([1.0, 2.0, 3.0, 4.0])
    weight2 = th.tensor([11.0, 22.0, 33.0, 44.0])

    bias1 = th.tensor([-1.0, -2.0])
    bias2 = th.tensor([1.0, 2.0])

    net1 = Net()
    params1 = net1.named_parameters()
    dict_params1 = dict(params1)
    with th.no_grad():
        dict_params1["fc1.weight"].set_(weight1)
        dict_params1["fc1.bias"].set_(bias1)

    net2 = Net()
    params2 = net2.named_parameters()
    dict_params2 = dict(params2)
    with th.no_grad():
        dict_params2["fc1.weight"].set_(weight2)
        dict_params2["fc1.bias"].set_(bias2)

    new_model = utils.add_model(net1, net2)

    assert (new_model.fc1.weight.data == (weight1 + weight2)).all()
    assert (new_model.fc1.bias.data == (bias1 + bias2)).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag854')" href="javascript:;">
PySyft-0.2.9/test/torch/federated/test_utils.py: 62-93
</a>
<div class="mid" id="frag854" style="display:none"><pre>
def test_add_model_cuda():  # pragma: no cover
    class Net(th.nn.Module):
        def __init__(self):
            super(Net, self).__init__()
            self.fc1 = th.nn.Linear(2, 2)

    weight1 = th.tensor([1.0, 2.0, 3.0, 4.0]).cuda()
    weight2 = th.tensor([11.0, 22.0, 33.0, 44.0]).cuda()

    bias1 = th.tensor([-1.0, -2.0]).cuda()
    bias2 = th.tensor([1.0, 2.0]).cuda()

    net1 = Net().to(th.device("cuda"))
    params1 = net1.named_parameters()
    dict_params1 = dict(params1)
    with th.no_grad():
        dict_params1["fc1.weight"].set_(weight1)
        dict_params1["fc1.bias"].set_(bias1)

    net2 = Net().cuda()
    params2 = net2.named_parameters()
    dict_params2 = dict(params2)
    with th.no_grad():
        dict_params2["fc1.weight"].set_(weight2)
        dict_params2["fc1.bias"].set_(bias2)

    new_model = utils.add_model(net1, net2)

    assert (new_model.fc1.weight.data == (weight1 + weight2)).all()
    assert (new_model.fc1.bias.data == (bias1 + bias2)).all()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 25:</b> &nbsp; 3 fragments, nominal size 14 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag864')" href="javascript:;">
PySyft-0.2.9/test/torch/federated/test_dataset.py: 43-66
</a>
<div class="mid" id="frag864" style="display:none"><pre>
def test_federated_dataset(workers):
    bob = workers["bob"]
    alice = workers["alice"]

    alice_base_dataset = BaseDataset(th.tensor([3, 4, 5, 6]), th.tensor([3, 4, 5, 6]))
    datasets = [
        BaseDataset(th.tensor([1, 2]), th.tensor([1, 2])).send(bob),
        alice_base_dataset.send(alice),
    ]

    fed_dataset = sy.FederatedDataset(datasets)

    assert fed_dataset.workers == ["bob", "alice"]
    assert len(fed_dataset) == 6

    alice_remote_data = fed_dataset.get_dataset("alice")
    assert (alice_remote_data.data == alice_base_dataset.data).all()
    assert alice_remote_data[2] == (5, 5)
    assert len(alice_remote_data) == 4
    assert len(fed_dataset) == 2

    assert isinstance(fed_dataset.__str__(), str)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag868')" href="javascript:;">
PySyft-0.2.9/test/torch/federated/test_dataset.py: 122-137
</a>
<div class="mid" id="frag868" style="display:none"><pre>
def test_get_dataset(workers):
    bob = workers["bob"]
    alice = workers["alice"]

    alice_base_dataset = BaseDataset(th.tensor([3, 4, 5, 6]), th.tensor([3, 4, 5, 6]))
    datasets = [
        BaseDataset(th.tensor([1, 2]), th.tensor([1, 2])).send(bob),
        alice_base_dataset.send(alice),
    ]
    fed_dataset = sy.FederatedDataset(datasets)
    dataset = fed_dataset.get_dataset("alice")

    assert len(fed_dataset) == 2
    assert len(dataset) == 4


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag869')" href="javascript:;">
PySyft-0.2.9/test/torch/federated/test_dataset.py: 138-153
</a>
<div class="mid" id="frag869" style="display:none"><pre>
def test_illegal_get(workers):
    """
    test getting error message when calling .get() on a
    dataset that's a part of fedratedDataset object
    """
    bob = workers["bob"]
    alice = workers["alice"]

    alice_base_dataset = BaseDataset(th.tensor([3, 4, 5, 6]), th.tensor([3, 4, 5, 6]))
    datasets = [
        BaseDataset(th.tensor([1, 2]), th.tensor([1, 2])).send(bob),
        alice_base_dataset.send(alice),
    ]
    fed_dataset = sy.FederatedDataset(datasets)
    with pytest.raises(ValueError):
        fed_dataset["alice"].get()
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 26:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag894')" href="javascript:;">
PySyft-0.2.9/test/torch/mpc/test_multiparty_nn.py: 35-48
</a>
<div class="mid" id="frag894" style="display:none"><pre>
def test_share_convert(workers):
    alice, bob, james = (
        workers["alice"],
        workers["bob"],
        workers["james"],
    )
    tensorA = (
        torch.tensor([10, 20, 30]).share(alice, bob, crypto_provider=james, dtype="long").child
    )
    tensorB = securenn.share_convert(tensorA)

    assert (tensorA.get() == tensorB.get()).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag900')" href="javascript:;">
PySyft-0.2.9/test/torch/mpc/test_multiparty_nn.py: 119-129
</a>
<div class="mid" id="frag900" style="display:none"><pre>
def test_maxpool_deriv(workers):
    alice, bob, james = (
        workers["alice"],
        workers["bob"],
        workers["james"],
    )
    tensorA = (
        torch.tensor([[0, 1, 8, 3]]).share(alice, bob, crypto_provider=james, dtype="long").child
    )
    deriv = securenn.maxpool_deriv(tensorA)
    assert (deriv.get() == torch.tensor([[0, 0, 1, 0]])).all()
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 27:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag908')" href="javascript:;">
PySyft-0.2.9/test/serde/protobuf/test_protobuf_serde.py: 29-59
</a>
<div class="mid" id="frag908" style="display:none"><pre>
def test_protobuf_serde_tensor_roundtrip(str_dtype):
    """Checks that tensors passed through serialization-deserialization stay same"""

    def compare(roundtrip, original):
        assert type(roundtrip) == torch.Tensor
        assert roundtrip.dtype == original.dtype

        # PyTorch doesn't implement equality checking for bfloat16, so convert to float
        if original.dtype == torch.bfloat16:
            roundtrip = roundtrip.float()
            original = original.float()

        # PyTorch doesn't implement equality checking for float16, so use numpy
        assert numpy.array_equal(roundtrip.data.numpy(), original.data.numpy())
        return True

    serde_worker = syft.hook.local_worker
    original_framework = serde_worker.framework
    serde_worker.framework = None

    tensor = torch.rand([10, 10]) * 16
    tensor = tensor.to(TORCH_STR_DTYPE[str_dtype])

    protobuf_tensor = protobuf.serde._bufferize(serde_worker, tensor)
    roundtrip_tensor = protobuf.serde._unbufferize(serde_worker, protobuf_tensor)

    serde_worker.framework = original_framework

    assert compare(roundtrip_tensor, tensor) is True


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag910')" href="javascript:;">
PySyft-0.2.9/test/serde/protobuf/test_protobuf_serde.py: 62-88
</a>
<div class="mid" id="frag910" style="display:none"><pre>
def test_protobuf_serde_tensor_roundtrip_quantized(str_dtype):
    """Checks that tensors passed through serialization-deserialization stay same"""

    def compare(roundtrip, original):
        assert type(roundtrip) == torch.Tensor
        assert roundtrip.dtype == original.dtype
        roundtrip_np = roundtrip.dequantize().numpy()
        original_np = original.dequantize().numpy()
        # PyTorch does implement equality checking for float tensors, but
        # quantized tensors may not be exactly the same after a round trip
        # plus dequantizing so use numpy close checking with a tolerance
        assert numpy.allclose(roundtrip_np, original_np, atol=2 / original.q_scale())
        return True

    serde_worker = syft.hook.local_worker
    original_framework = serde_worker.framework
    serde_worker.framework = None

    tensor = torch.rand([10, 10]) * 16
    tensor = torch.quantize_per_tensor(tensor, 0.1, 10, TORCH_STR_DTYPE[str_dtype])

    protobuf_tensor = protobuf.serde._bufferize(serde_worker, tensor)
    roundtrip_tensor = protobuf.serde._unbufferize(serde_worker, protobuf_tensor)

    serde_worker.framework = original_framework

    assert compare(roundtrip_tensor, tensor) is True
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 28:</b> &nbsp; 2 fragments, nominal size 20 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag938')" href="javascript:;">
PySyft-0.2.9/test/crypten/test_context.py: 37-70
</a>
<div class="mid" id="frag938" style="display:none"><pre>
def test_context_plan(workers):
    # alice and bob
    n_workers = 2

    alice = workers["alice"]
    bob = workers["bob"]

    alice_tensor_ptr = th.tensor([42, 53, 3, 2]).tag("crypten_data").send(alice)
    bob_tensor_ptr = th.tensor([101, 32, 29, 2]).tag("crypten_data").send(bob)

    @run_multiworkers([alice, bob], master_addr="127.0.0.1")
    @sy.func2plan()
    def plan_func(model=None, crypten=crypten):  # pragma: no cover
        alice_tensor = crypten.load("crypten_data", 0)
        bob_tensor = crypten.load("crypten_data", 1)

        crypt = alice_tensor + bob_tensor
        result = crypt.get_plain_text()
        return result

    return_values = plan_func()

    expected_value = th.tensor([143, 85, 32, 4])

    # A toy function is ran at each party, and they should all decrypt
    # a tensor with value [143, 85]
    for rank in range(n_workers):
        assert th.all(
            return_values[rank] == expected_value
        ), "Crypten party with rank {} don't match expected value {} != {}".format(
            rank, return_values[rank], expected_value
        )


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag940')" href="javascript:;">
PySyft-0.2.9/test/crypten/test_context.py: 71-103
</a>
<div class="mid" id="frag940" style="display:none"><pre>
def test_context_jail(workers):
    # alice and bob
    n_workers = 2

    alice = workers["alice"]
    bob = workers["bob"]

    alice_tensor_ptr = th.tensor([42, 53, 3, 2]).tag("crypten_data").send(alice)
    bob_tensor_ptr = th.tensor([101, 32, 29, 2]).tag("crypten_data").send(bob)

    @run_multiworkers([alice, bob], master_addr="127.0.0.1")
    def jail_func(crypten=crypten):  # pragma: no cover
        alice_tensor = crypten.load("crypten_data", 0)
        bob_tensor = crypten.load("crypten_data", 1)

        crypt = alice_tensor + bob_tensor
        result = crypt.get_plain_text()
        return result

    return_values = jail_func()

    expected_value = th.tensor([143, 85, 32, 4])

    # A toy function is ran at each party, and they should all decrypt
    # a tensor with value [143, 85]
    for rank in range(n_workers):
        assert th.all(
            return_values[rank] == expected_value
        ), "Crypten party with rank {} don't match expected value {} != {}".format(
            rank, return_values[rank], expected_value
        )


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 29:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 88%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag942')" href="javascript:;">
PySyft-0.2.9/test/crypten/test_context.py: 104-130
</a>
<div class="mid" id="frag942" style="display:none"><pre>
def test_context_jail_with_model(workers):
    dummy_input = th.empty(1, 1, 28, 28)
    pytorch_model = ExampleNet()

    alice = workers["alice"]
    bob = workers["bob"]

    alice_tensor_ptr = th.tensor(dummy_input).tag("crypten_data").send(alice)

    @run_multiworkers(
        [alice, bob], master_addr="127.0.0.1", model=pytorch_model, dummy_input=dummy_input
    )
    def run_encrypted_eval():  # pragma: no cover
        rank = crypten.communicator.get().get_rank()
        t = crypten.load("crypten_data", 0)

        model.encrypt()  # noqa: F821
        out = model(t)  # noqa: F821
        model.decrypt()  # noqa: F821
        out = out.get_plain_text()
        return model, out  # noqa: F821

    result = run_encrypted_eval()
    # compare out
    assert th.all(result[0][1] == result[1][1])


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag952')" href="javascript:;">
PySyft-0.2.9/test/crypten/test_context.py: 210-235
</a>
<div class="mid" id="frag952" style="display:none"><pre>
def test_context_plan_with_model(workers):
    dummy_input = th.empty(1, 1, 28, 28)
    pytorch_model = ExampleNet()

    alice = workers["alice"]
    bob = workers["bob"]

    alice_tensor_ptr = th.tensor(dummy_input).tag("crypten_data").send(alice)

    @run_multiworkers(
        [alice, bob], master_addr="127.0.0.1", model=pytorch_model, dummy_input=dummy_input
    )
    @sy.func2plan()
    def plan_func_model(model=None, crypten=crypten):  # noqa: F821
        t = crypten.load("crypten_data", 0)

        model.encrypt()
        out = model(t)
        model.decrypt()
        out = out.get_plain_text()
        return model, out

    result = plan_func_model()
    assert th.all(result[0][1] == result[1][1])


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 30:</b> &nbsp; 2 fragments, nominal size 20 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1002')" href="javascript:;">
PySyft-0.2.9/test/conftest.py: 140-175
</a>
<div class="mid" id="frag1002" style="display:none"><pre>
def workers(hook):
    # To run a plan locally the local worker can't be a client worker,
    # since it needs to register objects
    # LaRiffle edit: doing this increases the reference count on pointers and
    # breaks the auto garbage collection for pointer of pointers, see #2150
    # hook.local_worker.is_client_worker = False

    # Reset the hook and the local worker
    syft.local_worker.clear_objects()
    hook_args.hook_method_args_functions = {}
    hook_args.hook_method_response_functions = {}
    hook_args.register_response_functions = {}
    hook_args.get_tensor_type_functions = {}

    # Define 4 virtual workers
    alice = syft.VirtualWorker(id="alice", hook=hook, is_client_worker=False)
    bob = syft.VirtualWorker(id="bob", hook=hook, is_client_worker=False)
    charlie = syft.VirtualWorker(id="charlie", hook=hook, is_client_worker=False)
    james = syft.VirtualWorker(id="james", hook=hook, is_client_worker=False)

    workers = {
        "me": hook.local_worker,
        "alice": alice,
        "bob": bob,
        "charlie": charlie,
        "james": james,
    }

    yield workers

    alice.remove_worker_from_local_worker_registry()
    bob.remove_worker_from_local_worker_registry()
    charlie.remove_worker_from_local_worker_registry()
    james.remove_worker_from_local_worker_registry()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1401')" href="javascript:;">
PySyft-0.2.9/benchmarks/frameworks/torch/mpc/scripts/abstract/workers_initialization.py: 12-36
</a>
<div class="mid" id="frag1401" style="display:none"><pre>
def workers(hook):
    """
    This function defines virtual workers to be used in benchmarking functions.
    """

    # Reset the hook and the local worker
    syft.local_worker.clear_objects()
    hook_args.hook_method_args_functions = {}
    hook_args.hook_method_response_functions = {}
    hook_args.register_response_functions = {}
    hook_args.get_tensor_type_functions = {}

    # Define virtual workers
    alice = syft.VirtualWorker(id="alice", hook=hook, is_client_worker=False)
    bob = syft.VirtualWorker(id="bob", hook=hook, is_client_worker=False)
    james = syft.VirtualWorker(id="james", hook=hook, is_client_worker=False)
    charlie = syft.VirtualWorker(id="charlie", hook=hook, is_client_worker=False)
    workers = {
        "me": hook.local_worker,
        "alice": alice,
        "bob": bob,
        "charlie": charlie,
        "james": james,
    }
    return workers
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 31:</b> &nbsp; 2 fragments, nominal size 19 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1006')" href="javascript:;">
PySyft-0.2.9/test/generic/test_id_provider.py: 8-36
</a>
<div class="mid" id="frag1006" style="display:none"><pre>
def test_pop_no_given_ids(hook):
    provider = id_provider.IdProvider()
    values = [10, 4, 15, 4, 2, 0]

    orig_func = id_provider.create_random_id
    mocked_random_numbers = mock.Mock()
    mocked_random_numbers.side_effect = values
    id_provider.create_random_id = mocked_random_numbers

    val = provider.pop()
    assert val == values[0]

    val = provider.pop()
    assert val == values[1]

    val = provider.pop()
    assert val == values[2]

    # values[3] is skipped, as value already used.

    val = provider.pop()
    assert val == values[4]

    val = provider.pop()
    assert val == values[5]

    id_provider.create_random_id = orig_func


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1007')" href="javascript:;">
PySyft-0.2.9/test/generic/test_id_provider.py: 37-66
</a>
<div class="mid" id="frag1007" style="display:none"><pre>
def test_pop_with_given_ids(hook):
    given_ids = [4, 15, 2]
    provider = id_provider.IdProvider(given_ids=given_ids.copy())
    values = [10, 4, 15, 4, 2, 0]

    orig_func = id_provider.create_random_id
    mocked_random_numbers = mock.Mock()
    mocked_random_numbers.side_effect = values
    id_provider.create_random_id = mocked_random_numbers

    val = provider.pop()
    assert val == given_ids[-1]

    val = provider.pop()
    assert val == given_ids[-2]

    val = provider.pop()
    assert val == given_ids[-3]

    val = provider.pop()
    assert val == values[0]

    # values[1, 2, 3, 4] are skipped, as value already used.

    val = provider.pop()
    assert val == values[5]

    id_provider.create_random_id = orig_func


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 32:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1013')" href="javascript:;">
PySyft-0.2.9/test/generic/frameworks/test_attributes.py: 9-26
</a>
<div class="mid" id="frag1013" style="display:none"><pre>
def test_remote(workers, return_value):
    alice = workers["alice"]

    x = th.tensor([1.0])
    expected = my_awesome_computation(x)

    p = x.send(alice)
    args = (p,)
    results = remote(my_awesome_computation, location=alice)(
        *args, return_value=return_value, return_arity=2
    )

    if not return_value:
        results = tuple(result.get() for result in results)

    assert results == expected


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1014')" href="javascript:;">
PySyft-0.2.9/test/generic/frameworks/test_attributes.py: 28-48
</a>
<div class="mid" id="frag1014" style="display:none"><pre>
def test_remote_wrong_arity(workers, return_value):
    """
    Identical to test_remote except the use didn't set return_arity to
    be the correct number of return values.
    Here it should be 2, not 1.
    """
    alice = workers["alice"]

    x = th.tensor([1.0])
    expected = my_awesome_computation(x)

    p = x.send(alice)
    args = (p,)
    results = remote(my_awesome_computation, location=alice)(
        *args, return_value=return_value, return_arity=1
    )

    if not return_value:
        results = tuple(result.get() for result in results)

    assert results == expected
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 33:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1027')" href="javascript:;">
PySyft-0.2.9/test/generic/test_gc.py: 58-99
</a>
<div class="mid" id="frag1027" style="display:none"><pre>
def test_explicit_garbage_collect_double_pointer(workers):
    """Tests whether deleting a pointer to a pointer garbage collects
    the remote object too"""

    alice, bob = workers["alice"], workers["bob"]

    # create tensor
    x = torch.Tensor([1, 2])

    # send tensor to bob and then pointer to alice
    x_ptr = x.send(bob)
    x_ptr_ptr = x_ptr.send(alice)

    # ensure bob has tensor
    assert x.id in bob.object_store._objects

    # delete pointer to pointer to tensor, which should automatically
    # garbage collect the remote object on Bob's machine
    del x_ptr_ptr

    # ensure bob's object was garbage collected
    assert x.id not in bob.object_store._objects
    # ensure alice's object was garbage collected
    assert x_ptr.id not in workers["alice"].object_store._objects

    # Chained version
    x = torch.Tensor([1, 2])
    x_id = x.id

    # send tensor to bob and then pointer to alice
    # overwriting variable names at sending in the test, is on purpose,
    # to be sure nothing weird happens when people do this
    x = x.send(bob).send(alice)

    # ensure bob has tensor
    assert x_id in bob.object_store._objects
    # delete pointer to pointer to tensor
    del x
    # ensure bob's object was garbage collected
    assert x_id not in bob.object_store._objects


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1029')" href="javascript:;">
PySyft-0.2.9/test/generic/test_gc.py: 122-166
</a>
<div class="mid" id="frag1029" style="display:none"><pre>
def test_implicit_garbage_collect_double_pointer(workers):
    """Tests whether GCing a pointer to a pointer garbage collects
    the remote object too"""

    alice, bob = workers["alice"], workers["bob"]

    # create tensor
    x = torch.Tensor([1, 2])

    # send tensor to bob and then pointer to alice
    x_ptr = x.send(bob)
    x_ptr_ptr = x_ptr.send(alice)

    # ensure bob has tensor
    assert x.id in bob.object_store._objects
    # ensure alice has tensor
    assert x_ptr.id in alice.object_store._objects

    # delete pointer to pointer to tensor, which should automatically
    # garbage collect the remote object on Bob's machine
    x_ptr_ptr = "asdf"

    # ensure bob's object was garbage collected
    assert x.id not in bob.object_store._objects
    # ensure alice's object was garbage collected
    assert x_ptr.id not in alice.object_store._objects

    # Chained version
    x = torch.Tensor([1, 2])
    x_id = x.id
    # send tensor to bob and then pointer to alice
    # overwriting variable names at sending in the test, is on purpose,
    # to be sure nothing weird happens when people do this
    x = x.send(bob).send(alice)

    # ensure bob has tensor
    assert x_id in bob.object_store._objects

    # delete pointer to pointer to tensor
    x = "asdf"

    # ensure bob's object was garbage collected
    assert x_id not in bob.object_store._objects


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 34:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 92%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1035')" href="javascript:;">
PySyft-0.2.9/test/generic/test_functions.py: 5-25
</a>
<div class="mid" id="frag1035" style="display:none"><pre>
def test_combine_pointers(workers):
    """
    Ensure that the sy.combine_pointers works as expected
    """

    bob = workers["bob"]
    alice = workers["alice"]

    x = th.tensor([1, 2, 3, 4, 5]).send(bob)
    y = th.tensor([1, 2, 3, 4, 5]).send(alice)

    a = sy.combine_pointers(*[x, y])
    b = a + a

    c = b.get(sum_results=True)
    assert (c == th.tensor([4, 8, 12, 16, 20])).all()

    b = a + a
    c = b.get(sum_results=False)
    assert len(c) == 2
    assert (c[0] == th.tensor([2, 4, 6, 8, 10])).all
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1107')" href="javascript:;">
PySyft-0.2.9/test/generic/pointers/test_pointer_tensor.py: 339-361
</a>
<div class="mid" id="frag1107" style="display:none"><pre>
def test_combine_pointers(workers):
    """
    Ensure that the sy.combine_pointers works as expected
    """

    bob = workers["bob"]
    alice = workers["alice"]

    x = th.tensor([1, 2, 3, 4, 5]).send(bob)
    y = th.tensor([1, 2, 3, 4, 5]).send(alice)

    a = x.combine(y)
    b = a + a

    c = b.get(sum_results=True)
    assert (c == th.tensor([4, 8, 12, 16, 20])).all()

    b = a + a
    c = b.get(sum_results=False)
    assert len(c) == 2
    assert (c[0] == th.tensor([2, 4, 6, 8, 10])).all


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 35:</b> &nbsp; 2 fragments, nominal size 30 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1064')" href="javascript:;">
PySyft-0.2.9/test/generic/test_autograd.py: 299-338
</a>
<div class="mid" id="frag1064" style="display:none"><pre>
def test_backward_for_linear_model_on_fix_prec_params_with_autograd():
    """
    Test .backward() on Fixed Precision parameters with mixed operations
    """
    x = torch.tensor([[1.0, 2], [1.0, 2]]).fix_prec()
    target = torch.tensor([[1.0], [1.0]]).fix_prec()
    model = nn.Linear(2, 1)
    model.weight = nn.Parameter(torch.tensor([[-1.0, 2]]))
    model.bias = nn.Parameter(torch.tensor([-1.0]))
    model.fix_precision()

    x = syft.AutogradTensor().on(x)
    target = syft.AutogradTensor().on(target)
    model.weight = syft.AutogradTensor().on(model.weight)
    model.bias = syft.AutogradTensor().on(model.bias)

    output = model(x)
    loss = ((output - target) ** 2).sum()
    one = torch.ones(loss.shape).fix_prec()
    one = syft.AutogradTensor().on(one)
    loss.backward(one)

    weight_grad = model.weight.grad.float_precision()
    bias_grad = model.bias.grad.float_precision()

    x = torch.tensor([[1.0, 2], [1.0, 2]])
    target = torch.tensor([[1.0], [1.0]])
    model = nn.Linear(2, 1)
    model.weight = nn.Parameter(torch.tensor([[-1.0, 2]]))
    model.bias = nn.Parameter(torch.tensor([-1.0]))

    output = model(x)
    loss = ((output - target) ** 2).sum()

    one = torch.ones(loss.shape)
    loss.backward(one)
    assert (model.weight.grad == weight_grad).all()
    assert (model.bias.grad == bias_grad).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1069')" href="javascript:;">
PySyft-0.2.9/test/generic/test_autograd.py: 462-503
</a>
<div class="mid" id="frag1069" style="display:none"><pre>
def test_backward_for_linear_model_on_additive_shared_with_autograd(workers):
    """
    Test .backward() on Additive Shared tensors with mixed operations
    """
    bob, alice, james = workers["bob"], workers["alice"], workers["james"]

    x = torch.tensor([[1.0, 2], [1.0, 2]]).fix_prec().share(bob, alice, crypto_provider=james)
    target = torch.tensor([[1.0], [1.0]]).fix_prec().share(bob, alice, crypto_provider=james)
    model = nn.Linear(2, 1)
    model.weight = nn.Parameter(torch.tensor([[-1.0, 2]]))
    model.bias = nn.Parameter(torch.tensor([-1.0]))
    model.fix_precision().share(bob, alice, crypto_provider=james)

    x = syft.AutogradTensor().on(x)
    target = syft.AutogradTensor().on(target)
    model.weight = syft.AutogradTensor().on(model.weight)
    model.bias = syft.AutogradTensor().on(model.bias)

    output = model(x)
    loss = ((output - target) ** 2).sum()
    one = torch.ones(loss.shape).fix_prec().share(bob, alice, crypto_provider=james)
    one = syft.AutogradTensor().on(one)
    loss.backward(one)

    weight_grad = model.weight.grad.get().float_precision()
    bias_grad = model.bias.grad.get().float_precision()

    x = torch.tensor([[1.0, 2], [1.0, 2]])
    target = torch.tensor([[1.0], [1.0]])
    model = nn.Linear(2, 1)
    model.weight = nn.Parameter(torch.tensor([[-1.0, 2]]))
    model.bias = nn.Parameter(torch.tensor([-1.0]))

    output = model(x)
    loss = ((output - target) ** 2).sum()

    one = torch.ones(loss.shape)
    loss.backward(one)
    assert (model.weight.grad == weight_grad).all()
    assert (model.bias.grad == bias_grad).all()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 36:</b> &nbsp; 3 fragments, nominal size 11 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1112')" href="javascript:;">
PySyft-0.2.9/test/generic/pointers/test_pointer_tensor.py: 408-429
</a>
<div class="mid" id="frag1112" style="display:none"><pre>
def test_fix_prec_on_pointer_tensor(workers):
    """
    Ensure .fix_precision() works as expected.
    Also check that fix_precision() is not inplace.
    """
    bob = workers["bob"]

    tensor = torch.tensor([1, 2, 3, 4.0])
    ptr = tensor.send(bob)

    ptr_fp = ptr.fix_precision()

    remote_tensor = bob.object_store.get_obj(ptr.id_at_location)
    remote_fp_tensor = bob.object_store.get_obj(ptr_fp.id_at_location)

    # check that fix_precision is not inplace
    assert (remote_tensor == tensor).all()

    assert isinstance(ptr.child, PointerTensor)
    assert isinstance(remote_fp_tensor.child, FixedPrecisionTensor)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1113')" href="javascript:;">
PySyft-0.2.9/test/generic/pointers/test_pointer_tensor.py: 430-449
</a>
<div class="mid" id="frag1113" style="display:none"><pre>
def test_fix_prec_on_pointer_of_pointer(workers):
    """
    Ensure .fix_precision() works along a chain of pointers.
    """
    bob = workers["bob"]
    alice = workers["alice"]

    tensor = torch.tensor([1, 2, 3, 4.0])
    ptr = tensor.send(bob)
    ptr = ptr.send(alice)

    ptr = ptr.fix_precision()

    alice_tensor = alice.object_store.get_obj(ptr.id_at_location)
    remote_tensor = bob.object_store.get_obj(alice_tensor.id_at_location)

    assert isinstance(ptr.child, PointerTensor)
    assert isinstance(remote_tensor.child, FixedPrecisionTensor)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1115')" href="javascript:;">
PySyft-0.2.9/test/generic/pointers/test_pointer_tensor.py: 467-487
</a>
<div class="mid" id="frag1115" style="display:none"><pre>
def test_float_prec_on_pointer_of_pointer(workers):
    """
    Ensure .float_precision() works along a chain of pointers.
    """
    bob = workers["bob"]
    alice = workers["alice"]

    tensor = torch.tensor([1, 2, 3, 4.0])
    ptr = tensor.send(bob)
    ptr = ptr.send(alice)
    ptr = ptr.fix_precision()

    ptr = ptr.float_precision()

    alice_tensor = alice.object_store.get_obj(ptr.id_at_location)
    remote_tensor = bob.object_store.get_obj(alice_tensor.id_at_location)

    assert isinstance(ptr.child, PointerTensor)
    assert isinstance(remote_tensor, torch.Tensor)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 37:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 85%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1118')" href="javascript:;">
PySyft-0.2.9/test/generic/pointers/test_pointer_tensor.py: 522-543
</a>
<div class="mid" id="frag1118" style="display:none"><pre>
def test_setting_back_grad_to_origin_after_send(workers):
    """
    Calling .backward() on a tensor sent using `.send(..., requires_grad=True)`
    should update the origin tensor gradient
    """
    me = workers["me"]
    alice = workers["alice"]

    with me.registration_enabled():
        x = th.tensor([1.0, 2.0, 3, 4, 5], requires_grad=True)
        y = x + x
        me.register_obj(y)  # registration on the local worker is sometimes buggy

        y_ptr = y.send(alice, requires_grad=True)
        z_ptr = y_ptr * 2

        z = z_ptr.sum()
        z.backward()

        assert (x.grad == th.tensor([4.0, 4.0, 4.0, 4.0, 4.0])).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1119')" href="javascript:;">
PySyft-0.2.9/test/generic/pointers/test_pointer_tensor.py: 544-567
</a>
<div class="mid" id="frag1119" style="display:none"><pre>
def test_setting_back_grad_to_origin_after_move(workers):
    """
    Calling .backward() on a tensor moved using `.move(..., requires_grad=True)`
    should update the origin tensor gradient
    """
    me = workers["me"]
    bob = workers["bob"]
    alice = workers["alice"]

    with me.registration_enabled():
        x = th.tensor([1.0, 2.0, 3, 4, 5], requires_grad=True)
        y = x + x
        me.register_obj(y)  # registration on the local worker is sometimes buggy

        y_ptr = y.send(alice, requires_grad=True)
        z_ptr = y_ptr * 2

        z_ptr2 = z_ptr.move(bob, requires_grad=True)
        z = z_ptr2.sum()
        z.backward()

        assert (x.grad == th.tensor([4.0, 4.0, 4.0, 4.0, 4.0])).all()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 38:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1128')" href="javascript:;">
PySyft-0.2.9/test/generic/pointers/test_pointer_plan.py: 32-56
</a>
<div class="mid" id="frag1128" style="display:none"><pre>
def test_search_plan(hook, workers):

    alice, me = workers["alice"], workers["me"]
    me.is_client_worker = False

    @sy.func2plan(args_shape=[(1,)], state=(th.tensor([1.0]),))
    def plan(x, state):
        (bias,) = state.read()
        return x + bias

    plan.send(alice)
    id_at_location = plan.id

    plan_ptr = me.request_search([id_at_location], location=alice)[0]

    assert isinstance(plan_ptr, PointerPlan)

    x = th.tensor([1.0]).send(alice)
    ptr = plan_ptr(x)

    assert (ptr.get() == th.tensor([2.0])).all()

    me.is_client_worker = True


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1130')" href="javascript:;">
PySyft-0.2.9/test/generic/pointers/test_pointer_plan.py: 57-81
</a>
<div class="mid" id="frag1130" style="display:none"><pre>
def test_get_plan(workers):
    alice, me = workers["alice"], workers["me"]
    me.is_client_worker = False

    @sy.func2plan(args_shape=[(1,)], state=(th.tensor([1.0]),))
    def plan(x, state):
        (bias,) = state.read()
        return x + bias

    plan.send(alice)
    id_at_location = plan.id
    plan_ptr = me.request_search([id_at_location], location=alice)[0]

    plan = plan_ptr.get()

    assert isinstance(plan, Plan)

    x = th.tensor([1.0])
    res = plan(x)

    assert (res == th.tensor([2.0])).all()

    me.is_client_worker = True


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 39:</b> &nbsp; 3 fragments, nominal size 12 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1136')" href="javascript:;">
PySyft-0.2.9/test/notebooks/test_notebooks.py: 91-104
</a>
<div class="mid" id="frag1136" style="display:none"><pre>
def test_notebooks_basic(isolated_filesystem, notebook):
    """Test Notebooks in the tutorial root folder."""
    notebook = notebook.split("/")[-1]
    list_name = Path("examples/tutorials/") / notebook
    tested_notebooks.append(str(list_name))
    res = pm.execute_notebook(
        notebook,
        "/dev/null",
        parameters={"epochs": 1, "n_test_batches": 5, "n_train_items": 64, "n_test_items": 64},
        timeout=300,
    )
    assert isinstance(res, nbformat.notebooknode.NotebookNode)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1137')" href="javascript:;">
PySyft-0.2.9/test/notebooks/test_notebooks.py: 109-123
</a>
<div class="mid" id="frag1137" style="display:none"><pre>
def test_notebooks_basic_translations(isolated_filesystem, translated_notebook):  # pragma: no cover
    """Test Notebooks in the tutorial translations folder."""
    notebook = "/".join(translated_notebook.split("/")[-2:])
    notebook = f"translations/{notebook}"
    list_name = Path(f"examples/tutorials/{notebook}")
    tested_notebooks.append(str(list_name))
    res = pm.execute_notebook(
        notebook,
        "/dev/null",
        parameters={"epochs": 1, "n_test_batches": 5, "n_train_items": 64, "n_test_items": 64},
        timeout=400,
    )
    assert isinstance(res, nbformat.notebooknode.NotebookNode)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1138')" href="javascript:;">
PySyft-0.2.9/test/notebooks/test_notebooks.py: 128-148
</a>
<div class="mid" id="frag1138" style="display:none"><pre>
def test_notebooks_basic_translations_diff(
    isolated_filesystem, translated_notebook
):  # pragma: no cover
    """
    Test Notebooks in the tutorial translations folder if they have been
    modified in the current pull request. This test should not consider any
    notebooks locally. It should be used on Github Actions.
    """
    notebook = "/".join(translated_notebook.split("/")[-2:])
    notebook = f"translations/{notebook}"
    list_name = Path(f"examples/tutorials/{notebook}")
    tested_notebooks.append(str(list_name))
    res = pm.execute_notebook(
        notebook,
        "/dev/null",
        parameters={"epochs": 1, "n_test_batches": 5, "n_train_items": 64, "n_test_items": 64},
        timeout=300,
    )
    assert isinstance(res, nbformat.notebooknode.NotebookNode)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 40:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1149')" href="javascript:;">
PySyft-0.2.9/test/workers/test_websocket_worker.py: 107-132
</a>
<div class="mid" id="frag1149" style="display:none"><pre>
def test_list_objects_remote(hook, start_remote_worker):
    server, remote_proxy = start_remote_worker(id="fed-list-objects", hook=hook, port=8765)
    remote_proxy.clear_objects()

    x = torch.tensor([1, 2, 3]).send(remote_proxy)

    res = remote_proxy.list_tensors_remote()

    res_dict = eval(res.replace("tensor", "torch.tensor"))
    assert len(res_dict) == 1

    y = torch.tensor([4, 5, 6]).send(remote_proxy)
    res = remote_proxy.list_tensors_remote()
    res_dict = eval(res.replace("tensor", "torch.tensor"))
    assert len(res_dict) == 2

    # delete x before terminating the websocket connection
    del x
    del y
    time.sleep(0.1)
    remote_proxy.close()
    time.sleep(0.1)
    remote_proxy.remove_worker_from_local_worker_registry()
    server.terminate()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1150')" href="javascript:;">
PySyft-0.2.9/test/workers/test_websocket_worker.py: 133-158
</a>
<div class="mid" id="frag1150" style="display:none"><pre>
def test_objects_count_remote(hook, start_remote_worker):
    server, remote_proxy = start_remote_worker(id="fed-count-objects", hook=hook, port=8764)
    remote_proxy.clear_objects()

    x = torch.tensor([1, 2, 3]).send(remote_proxy)

    nr_objects = remote_proxy.tensors_count_remote()
    assert nr_objects == 1

    y = torch.tensor([4, 5, 6]).send(remote_proxy)
    nr_objects = remote_proxy.tensors_count_remote()
    assert nr_objects == 2

    x.get()
    nr_objects = remote_proxy.tensors_count_remote()
    assert nr_objects == 1

    # delete remote object before terminating the websocket connection
    del y
    time.sleep(0.1)
    remote_proxy.close()
    time.sleep(0.1)
    remote_proxy.remove_worker_from_local_worker_registry()
    server.terminate()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 41:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1161')" href="javascript:;">
PySyft-0.2.9/test/workers/test_base.py: 106-121
</a>
<div class="mid" id="frag1161" style="display:none"><pre>
def test_send_command_allow_list(hook, workers):
    bob = workers["bob"]
    allow_listed_methods = {
        "torch": {"tensor": [1, 2, 3], "rand": (2, 3), "randn": (2, 3), "zeros": (2, 3)}
    }

    for framework, methods in allow_listed_methods.items():
        attr = getattr(bob.remote, framework)

        for method, inp in methods.items():
            x = getattr(attr, method)(inp)

            if "rand" not in method:
                assert (x.get() == getattr(th, method)(inp)).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1173')" href="javascript:;">
PySyft-0.2.9/test/workers/test_virtual.py: 242-257
</a>
<div class="mid" id="frag1173" style="display:none"><pre>
def test_send_command_allow_list(hook, workers):
    bob = workers["bob"]
    allow_listed_methods = {
        "torch": {"tensor": [1, 2, 3], "rand": (2, 3), "randn": (2, 3), "zeros": (2, 3)}
    }

    for framework, methods in allow_listed_methods.items():
        attr = getattr(bob.remote, framework)

        for method, inp in methods.items():
            x = getattr(attr, method)(inp)

            if "rand" not in method:
                assert (x.get() == getattr(torch, method)(inp)).all()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 42:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 84%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1209')" href="javascript:;">
PySyft-0.2.9/test/execution/test_plan.py: 92-108
</a>
<div class="mid" id="frag1209" style="display:none"><pre>
def test_plan_execute_locally_ambiguous_output(workers):
    bob, alice = workers["bob"], workers["alice"]

    @sy.func2plan(args_shape=[(1,)])
    def serde_plan(x):
        x = x + x
        y = x * 2
        return x

    serde_plan_simplified = serde._simplify(bob, serde_plan)
    serde_plan_detailed = serde._detail(bob, serde_plan_simplified)
    t = th.tensor([2.3])
    expected = serde_plan(t)
    actual = serde_plan_detailed(t)
    assert actual == expected


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1219')" href="javascript:;">
PySyft-0.2.9/test/execution/test_plan.py: 193-210
</a>
<div class="mid" id="frag1219" style="display:none"><pre>
def test_plan_fixed_len_loop(workers):
    bob, alice = workers["bob"], workers["alice"]

    @sy.func2plan(args_shape=[(1,)])
    def serde_plan(x):
        for i in range(10):
            x = x + 1
        return x

    serde_plan_simplified = serde._simplify(bob, serde_plan)
    serde_plan_detailed = serde._detail(bob, serde_plan_simplified)

    t = th.tensor([1.0])
    expected = serde_plan_detailed(t)
    actual = serde_plan_detailed(t)
    assert actual == expected


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 43:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 86%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1226')" href="javascript:;">
PySyft-0.2.9/test/execution/test_plan.py: 254-276
</a>
<div class="mid" id="frag1226" style="display:none"><pre>
def test_plan_multiple_send(workers):
    bob, alice = workers["bob"], workers["alice"]

    @sy.func2plan(args_shape=[(1,)])
    def plan_abs(data):
        return data.abs()

    plan_ptr = plan_abs.send(bob)
    x_ptr = th.tensor([-1, 7, 3]).send(bob)
    p = plan_ptr(x_ptr)
    x_abs = p.get()

    assert (x_abs == th.tensor([1, 7, 3])).all()

    # Test get / send plan
    plan_ptr = plan_abs.send(alice)

    x_ptr = th.tensor([-1, 2, 3]).send(alice)
    p = plan_ptr(x_ptr)
    x_abs = p.get()
    assert (x_abs == th.tensor([1, 2, 3])).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1231')" href="javascript:;">
PySyft-0.2.9/test/execution/test_plan.py: 324-342
</a>
<div class="mid" id="frag1231" style="display:none"><pre>
def test_multiple_workers(workers):
    bob, alice = workers["bob"], workers["alice"]

    @sy.func2plan(args_shape=[(1,)])
    def plan_abs(data):
        return data.abs()

    plan_ptr = plan_abs.send(bob, alice)
    x_ptr = th.tensor([-1, 7, 3]).send(bob)
    p = plan_ptr(x_ptr)
    x_abs = p.get()
    assert (x_abs == th.tensor([1, 7, 3])).all()

    x_ptr = th.tensor([-1, 9, 3]).send(alice)
    p = plan_ptr(x_ptr)
    x_abs = p.get()
    assert (x_abs == th.tensor([1, 9, 3])).all()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 44:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 86%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1262')" href="javascript:;">
PySyft-0.2.9/test/execution/test_plan.py: 717-736
</a>
<div class="mid" id="frag1262" style="display:none"><pre>
def test_plan_list(hook):
    x11 = th.tensor([-1, 2.0])
    x12 = th.tensor([1, -2.0])

    @sy.func2plan()
    def plan_list(data, x):
        y = data[0] + data[1]
        z = data[0] + x
        return y + z

    device_1 = sy.VirtualWorker(hook, id="test_plan_list", data=(x11, x12))

    plan_list.build([th.tensor([1, 2]), th.tensor([2, 3])], th.tensor([0, 0]))
    pointer_to_plan = plan_list.send(device_1)
    pointer_to_data_1 = x11.send(device_1)
    pointer_to_data_2 = x12.send(device_1)
    result = pointer_to_plan([pointer_to_data_1, pointer_to_data_2], th.tensor([1, 1]))
    assert (result.get() == th.tensor([0, 3])).all()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1264')" href="javascript:;">
PySyft-0.2.9/test/execution/test_plan.py: 737-758
</a>
<div class="mid" id="frag1264" style="display:none"><pre>
def test_plan_tuple(hook):
    x11 = th.tensor([-1, 2.0])
    x12 = th.tensor([1, -2.0])

    @sy.func2plan()
    def plan_tuple(data, x):
        y = data[0] + data[1]
        z = data[0] + x
        return y + z

    device_1 = sy.VirtualWorker(hook, id="test_plan_tuple", data=(x11, x12))

    plan_tuple.build((th.tensor([1, 2]), th.tensor([2, 3])), th.tensor([0, 0]))
    pointer_to_plan = plan_tuple.send(device_1)

    pointer_to_data_1 = x11.send(device_1)
    pointer_to_data_2 = x12.send(device_1)

    result = pointer_to_plan((pointer_to_data_1, pointer_to_data_2), th.tensor([1, 1]))
    assert (result.get() == th.tensor([0, 3])).all()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 45:</b> &nbsp; 2 fragments, nominal size 26 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1270')" href="javascript:;">
PySyft-0.2.9/test/execution/test_plan.py: 829-865
</a>
<div class="mid" id="frag1270" style="display:none"><pre>
def test_plan_type_error(hook):
    x1 = th.tensor([-1, 2.0])
    x2 = th.tensor([1, -2.0])

    @sy.func2plan()
    def plan_type_err(dic):
        return dic["k1"]["kk2"]

    dummy_build = {
        "k1": {
            "kk1": [(th.tensor([0, 0]), 1), [th.tensor([0, 0]), 2.5]],
            "kk2": th.tensor([0, 0]),
        },
        "k2": [th.tensor([-1, 2]), "dummy_str"],
    }

    device_1 = sy.VirtualWorker(hook, id="test_nested_structure", data=(x1, x2))
    plan_type_err.build(dummy_build)

    pointer_to_data_1 = x1.send(device_1)
    pointer_to_data_2 = x2.send(device_1)

    call_build = {
        "k1": {"kk1": [(pointer_to_data_1, 1.5), [pointer_to_data_1, True]], "kk2": "warn"},
        "k2": [pointer_to_data_1, pointer_to_data_2],
    }

    pointer_to_plan = plan_type_err.send(device_1)
    with pytest.raises(TypeError) as e:
        _ = pointer_to_plan(call_build)

    assert str(e.value) == (
        "Plan plan_type_err element 1 of element 0 of key kk1 of key k1 of element 0 of "
        "input has type int, while being built with type float."
    )


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1278')" href="javascript:;">
PySyft-0.2.9/test/execution/test_plan.py: 944-980
</a>
<div class="mid" id="frag1278" style="display:none"><pre>
def test_plan_key_error(hook):
    x1 = th.tensor([-1, 2.0])
    x2 = th.tensor([1, -2.0])

    @sy.func2plan()
    def plan_type_warn(dic):
        return dic["k1"]["kk2"]

    dummy_build = {
        "k1": {
            "kk1": [(th.tensor([0, 0]), 1), [th.tensor([0, 0]), 2.5]],
            "kk2": th.tensor([0, 0]),
        },
        "k2": [th.tensor([-1, 2]), "dummy_str"],
    }

    device_1 = sy.VirtualWorker(hook, id="test_nested_structure", data=(x1, x2))
    plan_type_warn.build(dummy_build)

    pointer_to_data_1 = x1.send(device_1)
    pointer_to_data_2 = x2.send(device_1)

    call_build = {
        "k1": {"kk1_wrong": [(pointer_to_data_1, 1.5), [pointer_to_data_1, True]], "kk2": "warn"},
        "k2": [pointer_to_data_1, pointer_to_data_2],
    }

    pointer_to_plan = plan_type_warn.send(device_1)
    with pytest.raises(KeyError) as e:
        _ = pointer_to_plan(call_build)

    assert str(e.value) == (
        "'Plan plan_type_warn key k1 of element 0 of input does not provide the key kk1, "
        "while being build with that key.'"
    )


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 46:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 84%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1274')" href="javascript:;">
PySyft-0.2.9/test/execution/test_plan.py: 897-919
</a>
<div class="mid" id="frag1274" style="display:none"><pre>
def test_wrong_type_err(hook):
    @sy.func2plan()
    def plan_wrong_type_err(x):
        return x

    device_1 = sy.VirtualWorker(hook, id="test_nested_structure")

    call_build = [True, 5]
    dummy_build = ((th.tensor([1, 2, 3]), True),)

    plan_wrong_type_err.build(dummy_build)

    pointer_to_plan = plan_wrong_type_err.send(device_1)

    with pytest.raises(TypeError) as e:
        pointer_to_plan(call_build)

    assert str(e.value) == (
        "Plan plan_wrong_type_err element 0 of input has type tuple, while being "
        "built with type list."
    )


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1276')" href="javascript:;">
PySyft-0.2.9/test/execution/test_plan.py: 920-943
</a>
<div class="mid" id="frag1276" style="display:none"><pre>
def test_wrong_size_dict(hook):
    @sy.func2plan()
    def plan_wrong_size_dict(x):
        return x

    device_1 = sy.VirtualWorker(hook, id="test_nested_structure")

    dummy_build = {"k1": True, "k2": False}

    call_build = {"k1": True, "k2": False, "k3": 1}

    plan_wrong_size_dict.build(dummy_build)

    pointer_to_plan = plan_wrong_size_dict.send(device_1)

    with pytest.raises(TypeError) as e:
        pointer_to_plan(call_build)

    assert str(e.value) == (
        "Plan plan_wrong_size_dict element 0 of input has length 3, while being "
        "build with length 2."
    )


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 47:</b> &nbsp; 2 fragments, nominal size 25 lines, similarity 74%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1307')" href="javascript:;">
PySyft-0.2.9/test/execution/test_state.py: 123-163
</a>
<div class="mid" id="frag1307" style="display:none"><pre>
def test_fetch_stateful_plan(hook, is_func2plan, workers):

    if is_func2plan:

        @sy.func2plan(args_shape=[(1,)], state=(th.tensor([1.0]),))
        def plan(data, state):
            (bias,) = state.read()
            return data * bias

    else:

        class Net(sy.Plan):
            def __init__(self):
                super(Net, self).__init__()
                self.fc1 = nn.Linear(1, 1)

            def forward(self, x):
                return self.fc1(x)

        plan = Net()
        plan.build(th.tensor([1.2]))

    alice = workers["alice"]
    plan_ptr = plan.send(alice)

    # Fetch plan
    fetched_plan = plan.owner.fetch_plan(plan_ptr.id_at_location, alice)

    # Execute it locally
    x = th.tensor([-1.26])
    assert th.all(th.eq(fetched_plan(x), plan(x)))
    # assert fetched_plan.state.state_placeholders != plan.state.state_placeholders #TODO

    # Make sure fetched_plan is using the actions
    assert fetched_plan.forward is None
    assert fetched_plan.is_built

    # Make sure plan is using the blueprint: forward
    assert plan.forward is not None


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1311')" href="javascript:;">
PySyft-0.2.9/test/execution/test_state.py: 165-212
</a>
<div class="mid" id="frag1311" style="display:none"><pre>
def test_fetch_stateful_plan_remote(hook, is_func2plan, start_remote_worker):

    server, remote_proxy = start_remote_worker(
        id=f"test_fetch_stateful_plan_remote_{is_func2plan}", hook=hook, port=8802
    )

    if is_func2plan:

        @sy.func2plan(args_shape=[(1,)], state=(th.tensor([3.0]),))
        def plan(data, state):
            (bias,) = state.read()
            return data * bias

    else:

        class Net(sy.Plan):
            def __init__(self):
                super(Net, self).__init__()
                self.fc1 = nn.Linear(1, 1)

            def forward(self, x):
                return self.fc1(x)

        plan = Net()
        plan.build(th.tensor([1.2]))

    x = th.tensor([-1.26])
    expected = plan(x)
    plan_ptr = plan.send(remote_proxy)

    # Fetch plan
    fetched_plan = plan.owner.fetch_plan(plan_ptr.id_at_location, remote_proxy)

    # Execute it locally
    assert th.all(th.eq(fetched_plan(x), expected))
    # assert fetched_plan.state.state_placeholders != plan.state.state_placeholders #TODO

    # Make sure fetched_plan is using the actions
    assert fetched_plan.forward is None
    assert fetched_plan.is_built

    # Make sure plan is using the blueprint: forward
    assert plan.forward is not None

    remote_proxy.close()
    server.terminate()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 48:</b> &nbsp; 9 fragments, nominal size 11 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1329')" href="javascript:;">
PySyft-0.2.9/test/execution/test_protocol.py: 36-50
</a>
<div class="mid" id="frag1329" style="display:none"><pre>
def test_trace_communication_actions_send():
    @sy.func2protocol(roles=["alice", "bob"], args_shape={"alice": ((1,),)})
    def protocol(alice, bob):
        tensor = alice.torch.tensor([1])

        tensor.send(bob.worker)
        return tensor

    traced_actions = protocol.roles["alice"].actions

    assert protocol.is_built
    assert len(traced_actions) == 2
    assert "send" in (action.name for action in traced_actions)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1333')" href="javascript:;">
PySyft-0.2.9/test/execution/test_protocol.py: 67-82
</a>
<div class="mid" id="frag1333" style="display:none"><pre>
def test_trace_communication_actions_ptr_send():
    @sy.func2protocol(roles=["alice", "bob"], args_shape={"alice": ((1,),)})
    def protocol(alice, bob):
        tensor = alice.torch.tensor([1])

        ptr = tensor.send(bob.worker)
        res = ptr.send(alice.worker)
        return res

    traced_actions = protocol.roles["alice"].actions

    assert protocol.is_built
    assert len(traced_actions) == 3
    assert "send" in (action.name for action in traced_actions)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1337')" href="javascript:;">
PySyft-0.2.9/test/execution/test_protocol.py: 99-115
</a>
<div class="mid" id="frag1337" style="display:none"><pre>
def test_trace_communication_actions_share():
    @sy.func2protocol(roles=["alice", "bob"], args_shape={"alice": ((1,),)})
    def protocol(alice, bob):
        tensor = alice.torch.tensor([1])

        ptr = tensor.send(bob.worker)
        ptr = ptr.fix_prec()
        res = ptr.share(alice.worker, bob.worker)
        return res

    traced_actions = protocol.roles["alice"].actions

    assert protocol.is_built
    assert len(traced_actions) == 4
    assert "share" in (action.name for action in traced_actions)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1339')" href="javascript:;">
PySyft-0.2.9/test/execution/test_protocol.py: 116-132
</a>
<div class="mid" id="frag1339" style="display:none"><pre>
def test_trace_communication_actions_share_():
    @sy.func2protocol(roles=["alice", "bob"], args_shape={"alice": ((1,),)})
    def protocol(alice, bob):
        tensor = alice.torch.tensor([1])

        ptr = tensor.send(bob.worker)
        ptr = ptr.fix_prec()
        res = ptr.share_(alice.worker, bob.worker)
        return res

    traced_actions = protocol.roles["alice"].actions

    assert protocol.is_built
    assert len(traced_actions) == 4
    assert "share_" in (action.name for action in traced_actions)


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1341')" href="javascript:;">
PySyft-0.2.9/test/execution/test_protocol.py: 133-148
</a>
<div class="mid" id="frag1341" style="display:none"><pre>
def test_trace_communication_actions_remote_send():
    @sy.func2protocol(roles=["alice", "bob"], args_shape={"alice": ((1,),)})
    def protocol(alice, bob):
        tensor = alice.torch.tensor([1])

        ptr = tensor.send(bob.worker)
        res = ptr.remote_send(alice.worker)
        return res

    traced_actions = protocol.roles["alice"].actions

    assert protocol.is_built
    assert len(traced_actions) == 3
    assert "remote_send" in (action.name for action in traced_actions)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1331')" href="javascript:;">
PySyft-0.2.9/test/execution/test_protocol.py: 51-66
</a>
<div class="mid" id="frag1331" style="display:none"><pre>
def test_trace_communication_actions_get():
    @sy.func2protocol(roles=["alice", "bob"], args_shape={"alice": ((1,),)})
    def protocol(alice, bob):
        tensor = alice.torch.tensor([1])

        ptr = tensor.send(bob.worker)
        res = ptr.get()
        return res

    traced_actions = protocol.roles["alice"].actions

    assert protocol.is_built
    assert len(traced_actions) == 3
    assert "get" in (action.name for action in traced_actions)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1345')" href="javascript:;">
PySyft-0.2.9/test/execution/test_protocol.py: 165-180
</a>
<div class="mid" id="frag1345" style="display:none"><pre>
def test_trace_communication_actions_remote_get():
    @sy.func2protocol(roles=["alice", "bob"], args_shape={"alice": ((1,),)})
    def protocol(alice, bob):
        tensor = alice.torch.tensor([1])

        ptr = tensor.send(bob.worker).send(alice.worker)
        res = ptr.remote_get()
        return res

    traced_actions = protocol.roles["alice"].actions

    assert protocol.is_built
    assert len(traced_actions) == 4
    assert "remote_get" in (action.name for action in traced_actions)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1343')" href="javascript:;">
PySyft-0.2.9/test/execution/test_protocol.py: 149-164
</a>
<div class="mid" id="frag1343" style="display:none"><pre>
def test_trace_communication_actions_mid_get():
    @sy.func2protocol(roles=["alice", "bob"], args_shape={"alice": ((1,),)})
    def protocol(alice, bob):
        tensor = alice.torch.tensor([1])

        ptr = tensor.send(bob.worker)
        res = ptr.mid_get()
        return res

    traced_actions = protocol.roles["alice"].actions

    assert protocol.is_built
    assert len(traced_actions) == 3
    assert "mid_get" in (action.name for action in traced_actions)


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1335')" href="javascript:;">
PySyft-0.2.9/test/execution/test_protocol.py: 83-98
</a>
<div class="mid" id="frag1335" style="display:none"><pre>
def test_trace_communication_actions_move():
    @sy.func2protocol(roles=["alice", "bob"], args_shape={"alice": ((1,),)})
    def protocol(alice, bob):
        tensor = alice.torch.tensor([1])

        ptr = tensor.send(bob.worker)
        res = ptr.move(alice.worker)
        return res

    traced_actions = protocol.roles["alice"].actions

    assert protocol.is_built
    assert len(traced_actions) == 3
    assert "move" in (action.name for action in traced_actions)


</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 49:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1364')" href="javascript:;">
PySyft-0.2.9/test/execution/test_role.py: 9-29
</a>
<div class="mid" id="frag1364" style="display:none"><pre>
def test_register_computation_action():
    role = Role()
    placeholder = PlaceHolder()
    target = torch.ones([1])

    action = ("__add__", target, (), {})

    role.register_action((action, placeholder), ComputationAction)

    assert len(role.actions) == 1

    registered = role.actions[0]

    assert isinstance(registered, ComputationAction)
    assert registered.name == "__add__"
    assert registered.target == target
    assert registered.args == ()
    assert registered.kwargs == {}
    assert registered.return_ids == (placeholder.id,)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1365')" href="javascript:;">
PySyft-0.2.9/test/execution/test_role.py: 30-51
</a>
<div class="mid" id="frag1365" style="display:none"><pre>
def test_register_communication_action():
    role = Role()
    placeholder = PlaceHolder()
    target = torch.ones([1])

    action = ("get", target, (), {})

    role.register_action((action, placeholder), CommunicationAction)

    assert len(role.actions) == 1

    registered = role.actions[0]

    assert isinstance(registered, CommunicationAction)
    assert registered.name == "get"
    assert registered.target == target
    assert registered.args == ()
    assert registered.kwargs == {}

    assert registered.return_ids == (placeholder.id,)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 50:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1367')" href="javascript:;">
PySyft-0.2.9/examples/tutorials/grid/federated_learning/spam_prediction/handcrafted_GRU.py: 8-27
</a>
<div class="mid" id="frag1367" style="display:none"><pre>
    def __init__(self, input_size, hidden_size, bias=True):
        super(GRUCell, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.bias = bias

        # reset gate
        self.fc_ir = nn.Linear(input_size, hidden_size, bias=bias)
        self.fc_hr = nn.Linear(hidden_size, hidden_size, bias=bias)

        # update gate
        self.fc_iz = nn.Linear(input_size, hidden_size, bias=bias)
        self.fc_hz = nn.Linear(hidden_size, hidden_size, bias=bias)

        # new gate
        self.fc_in = nn.Linear(input_size, hidden_size, bias=bias)
        self.fc_hn = nn.Linear(hidden_size, hidden_size, bias=bias)

        self.init_parameters()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1380')" href="javascript:;">
PySyft-0.2.9/examples/tutorials/advanced/federated_sms_spam_prediction/handcrafted_GRU.py: 8-27
</a>
<div class="mid" id="frag1380" style="display:none"><pre>
    def __init__(self, input_size, hidden_size, bias=True):
        super(GRUCell, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.bias = bias

        # reset gate
        self.fc_ir = nn.Linear(input_size, hidden_size, bias=bias)
        self.fc_hr = nn.Linear(hidden_size, hidden_size, bias=bias)

        # update gate
        self.fc_iz = nn.Linear(input_size, hidden_size, bias=bias)
        self.fc_hz = nn.Linear(hidden_size, hidden_size, bias=bias)

        # new gate
        self.fc_in = nn.Linear(input_size, hidden_size, bias=bias)
        self.fc_hn = nn.Linear(hidden_size, hidden_size, bias=bias)

        self.init_parameters()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 51:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1369')" href="javascript:;">
PySyft-0.2.9/examples/tutorials/grid/federated_learning/spam_prediction/handcrafted_GRU.py: 33-52
</a>
<div class="mid" id="frag1369" style="display:none"><pre>
    def forward(self, x, h):

        x = x.view(-1, x.shape[1])

        i_r = self.fc_ir(x)
        h_r = self.fc_hr(h)
        i_z = self.fc_iz(x)
        h_z = self.fc_hz(h)
        i_n = self.fc_in(x)
        h_n = self.fc_hn(h)

        resetgate = F.sigmoid(i_r + h_r)
        inputgate = F.sigmoid(i_z + h_z)
        newgate = F.tanh(i_n + (resetgate * h_n))

        hy = newgate + inputgate * (h - newgate)

        return hy


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1382')" href="javascript:;">
PySyft-0.2.9/examples/tutorials/advanced/federated_sms_spam_prediction/handcrafted_GRU.py: 33-52
</a>
<div class="mid" id="frag1382" style="display:none"><pre>
    def forward(self, x, h):

        x = x.view(-1, x.shape[1])

        i_r = self.fc_ir(x)
        h_r = self.fc_hr(h)
        i_z = self.fc_iz(x)
        h_z = self.fc_hz(h)
        i_n = self.fc_in(x)
        h_n = self.fc_hn(h)

        resetgate = F.sigmoid(i_r + h_r)
        inputgate = F.sigmoid(i_z + h_z)
        newgate = F.tanh(i_n + (resetgate * h_n))

        hy = newgate + inputgate * (h - newgate)

        return hy


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 52:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1370')" href="javascript:;">
PySyft-0.2.9/examples/tutorials/grid/federated_learning/spam_prediction/handcrafted_GRU.py: 54-72
</a>
<div class="mid" id="frag1370" style="display:none"><pre>
    def __init__(
        self, vocab_size, output_size=1, embedding_dim=50, hidden_dim=10, bias=True, dropout=0.2
    ):
        super(GRU, self).__init__()

        self.hidden_dim = hidden_dim
        self.output_size = output_size

        # Dropout layer
        self.dropout = nn.Dropout(p=dropout)
        # Embedding layer
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        # GRU Cell
        self.gru_cell = GRUCell(embedding_dim, hidden_dim)
        # Fully-connected layer
        self.fc = nn.Linear(hidden_dim, output_size)
        # Sigmoid layer
        self.sigmoid = nn.Sigmoid()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1383')" href="javascript:;">
PySyft-0.2.9/examples/tutorials/advanced/federated_sms_spam_prediction/handcrafted_GRU.py: 54-72
</a>
<div class="mid" id="frag1383" style="display:none"><pre>
    def __init__(
        self, vocab_size, output_size=1, embedding_dim=50, hidden_dim=10, bias=True, dropout=0.2
    ):
        super(GRU, self).__init__()

        self.hidden_dim = hidden_dim
        self.output_size = output_size

        # Dropout layer
        self.dropout = nn.Dropout(p=dropout)
        # Embedding layer
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        # GRU Cell
        self.gru_cell = GRUCell(embedding_dim, hidden_dim)
        # Fully-connected layer
        self.fc = nn.Linear(hidden_dim, output_size)
        # Sigmoid layer
        self.sigmoid = nn.Sigmoid()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 53:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1371')" href="javascript:;">
PySyft-0.2.9/examples/tutorials/grid/federated_learning/spam_prediction/handcrafted_GRU.py: 73-96
</a>
<div class="mid" id="frag1371" style="display:none"><pre>
    def forward(self, x, h):

        batch_size = x.shape[0]

        # Deal with cases were the current batch_size is different from general batch_size
        # It occurrs at the end of iteration with the Dataloaders
        if h.shape[0] != batch_size:
            h = h[:batch_size, :].contiguous()

        # Apply embedding
        x = self.embedding(x)

        # GRU cells
        for t in range(x.shape[1]):
            h = self.gru_cell(x[:, t, :], h)

        # Output corresponds to the last hidden state
        out = h.contiguous().view(-1, self.hidden_dim)

        # Dropout and fully-connected layers
        out = self.dropout(out)
        sig_out = self.sigmoid(self.fc(out))

        return sig_out, h
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1384')" href="javascript:;">
PySyft-0.2.9/examples/tutorials/advanced/federated_sms_spam_prediction/handcrafted_GRU.py: 73-96
</a>
<div class="mid" id="frag1384" style="display:none"><pre>
    def forward(self, x, h):

        batch_size = x.shape[0]

        # Deal with cases were the current batch_size is different from general batch_size
        # It occurrs at the end of iteration with the Dataloaders
        if h.shape[0] != batch_size:
            h = h[:batch_size, :].contiguous()

        # Apply embedding
        x = self.embedding(x)

        # GRU cells
        for t in range(x.shape[1]):
            h = self.gru_cell(x[:, t, :], h)

        # Output corresponds to the last hidden state
        out = h.contiguous().view(-1, self.hidden_dim)

        # Dropout and fully-connected layers
        out = self.dropout(out)
        sig_out = self.sigmoid(self.fc(out))

        return sig_out, h
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 54:</b> &nbsp; 4 fragments, nominal size 27 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1389')" href="javascript:;">
PySyft-0.2.9/benchmarks/frameworks/torch/mpc/scripts/benchmark_ast.py: 44-101
</a>
<div class="mid" id="frag1389" style="display:none"><pre>
def benchmark_share_get_plot(b_data_share_get):
    """
    This function plots the graph for various protocols benchmarks for additive
    shared tensors

    Args:
        b_data_share_get (list): the sample data to approximate

    Returns:
        benchmark_share_get.png (png): plotted graph in graph/ast_benchmarks directory
    """
    # initializing workers
    worker = workers(hook())

    # available protocols
    protocols = ["snn", "fss"]

    # initializing graph plot
    fig, ax = plt.subplots()

    for protocol in protocols:

        # list for handling graph data
        x_data = []
        y_data = []

        for data in b_data_share_get:

            # getting value from b_data_share_get
            dtype, n_workers = data

            # temporary list for calculating average execution time and error
            temp_time_taken = []

            for i in range(10):
                start_time = timeit.default_timer()
                benchmark_share_get(worker, protocol, dtype, n_workers)
                time_taken = timeit.default_timer() - start_time
                temp_time_taken.append(time_taken)

            final_time_taken = sum(temp_time_taken) / len(temp_time_taken)
            final_time_taken *= 1000
            x_data.append(dtype + str(" / ") + str(n_workers))
            y_data.append(final_time_taken)

        ax.plot(x_data, y_data, label=protocol, linestyle="-")
        x_data.clear()
        y_data.clear()

    ax.set_xlabel("dtype / n_workers")
    ax.set_ylabel("Execution Time (ms)")
    ax.legend(bbox_to_anchor=(1, 1.22), loc="upper right", title="Protocols", fontsize="small")
    plt.tight_layout()
    plt.savefig("../graphs/ast_benchmarks/benchmark_share_get.png")
    # plt.show()


# calling benchmark_share_get_plot function
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1393')" href="javascript:;">
PySyft-0.2.9/benchmarks/frameworks/torch/mpc/scripts/benchmark_ast.py: 229-284
</a>
<div class="mid" id="frag1393" style="display:none"><pre>
def benchmark_avg_pool2d_plot(b_data_avg_pool2d):
    """
    This function plots the graph for various protocols benchmarks for
    avg_pool2d.

    Args:
        b_data_avg_pool2d (list): list of protocols to approximate

    Returns:
        benchmark_avg_pool2d.png (png): plotted graph in graph/ast_benchmarks directory
    """

    # initializing workers
    worker = workers(hook())

    # getting data (protocols)
    protocols = b_data_avg_pool2d

    # initializing graph plot
    fig, ax = plt.subplots()

    for protocol in protocols:

        # list for handling graph data
        x_data = []
        y_data = []

        for prec_frac in range(1, 5):
            temp_time_taken = []

            for i in range(10):
                start_time = timeit.default_timer()
                benchmark_avg_pool2d(worker, protocol, prec_frac)
                time_taken = timeit.default_timer() - start_time
                temp_time_taken.append(time_taken)

            final_time_taken = sum(temp_time_taken) / len(temp_time_taken)
            final_time_taken *= 1000
            y_data.append(final_time_taken)
            x_data.append(prec_frac)

        ax.plot(x_data, y_data, label=protocol, linestyle="-")
        x_data.clear()
        y_data.clear()

    # plotting of the data
    plt.title("Benchmark avg_pool2d")
    ax.set_xlabel("Precision Value")
    ax.set_ylabel("Execution Time (ms)")
    ax.legend(bbox_to_anchor=(1, 1.3), loc="upper right", title="Protocol", fontsize="small")
    plt.tight_layout()
    plt.savefig("../graphs/ast_benchmarks/benchmark_avg_pool2d.png")
    # plt.show()


# calling benchmark_avg_pool2d_plot
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1391')" href="javascript:;">
PySyft-0.2.9/benchmarks/frameworks/torch/mpc/scripts/benchmark_ast.py: 138-192
</a>
<div class="mid" id="frag1391" style="display:none"><pre>
def benchmark_max_pool2d_plot(b_data_max_pool2d):
    """
    This function plots the graph for various protocols benchmarks for
    max_pool2d.

    Args:
        b_data_max_pool2d (list): list of protocols to approximate

    Returns:
        benchmark_max_pool2d.png (png): plotted graph in graph/ast_benchmarks directory
    """

    # initializing workers
    worker = workers(hook())

    # getting data (protocols)
    protocols = b_data_max_pool2d

    # initializing graph plot
    fig, ax = plt.subplots()

    for protocol in protocols:

        # list for handling graph data
        x_data = []
        y_data = []

        for prec_frac in range(1, 5):
            temp_time_taken = []

            for i in range(10):
                start_time = timeit.default_timer()
                benchmark_max_pool2d(worker, protocol, prec_frac)
                time_taken = timeit.default_timer() - start_time
                temp_time_taken.append(time_taken)

            final_time_taken = sum(temp_time_taken) / len(temp_time_taken)
            final_time_taken *= 1000
            y_data.append(final_time_taken)
            x_data.append(prec_frac)

        ax.plot(x_data, y_data, label=protocol, linestyle="-")
        x_data.clear()
        y_data.clear()

    # plotting of the data
    plt.title("Benchmark max_pool2d")
    ax.set_xlabel("Precision Value")
    ax.set_ylabel("Execution Time (ms)")
    ax.legend(bbox_to_anchor=(1, 1.3), loc="upper right", title="Protocol", fontsize="small")
    plt.tight_layout()
    plt.savefig("../graphs/ast_benchmarks/benchmark_max_pool2d.png")


# calling benchmark_max_pool2d_plot
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1395')" href="javascript:;">
PySyft-0.2.9/benchmarks/frameworks/torch/mpc/scripts/benchmark_ast.py: 325-380
</a>
<div class="mid" id="frag1395" style="display:none"><pre>
def benchmark_batch_norm_plot(b_data_batch_norm):
    """
    This function plots the graph for various protocols benchmarks for
    batch_norm.

    Args:
        b_data_batch_norm (list): list of protocols to approximate

    Returns:
        benchmark_batch_norm.png (png): plotted graph in graph/ast_benchmarks directory
    """

    # initializing workers
    worker = workers(hook())

    # getting data (protocols)
    protocols = b_data_batch_norm

    # initializing graph plot
    fig, ax = plt.subplots()

    for protocol in protocols:

        # list for handling graph data
        x_data = []
        y_data = []

        for prec_frac in range(1, 5):
            temp_time_taken = []

            for i in range(10):
                start_time = timeit.default_timer()
                benchmark_batch_norm(worker, protocol, True, prec_frac)
                time_taken = timeit.default_timer() - start_time
                temp_time_taken.append(time_taken)

            final_time_taken = sum(temp_time_taken) / len(temp_time_taken)
            final_time_taken *= 1000
            y_data.append(final_time_taken)
            x_data.append(prec_frac)

        ax.plot(x_data, y_data, label=protocol, linestyle="-")
        x_data.clear()
        y_data.clear()

    # plotting of the data
    plt.title("benchmark_batch_norm")
    ax.set_xlabel("Precision Value")
    ax.set_ylabel("Execution Time (ms)")
    ax.legend(bbox_to_anchor=(1, 1.3), loc="upper right", title="Protocol", fontsize="small")
    plt.tight_layout()
    plt.savefig("../graphs/ast_benchmarks/benchmark_batch_norm.png")
    # plt.show()


# calling benchmark_batch_norm_plot
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 55:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1390')" href="javascript:;">
PySyft-0.2.9/benchmarks/frameworks/torch/mpc/scripts/benchmark_ast.py: 105-137
</a>
<div class="mid" id="frag1390" style="display:none"><pre>
def benchmark_max_pool2d(workers, protocol, prec_frac):
    """
    This function benchmarks max_plot2d function.

    Args:
        workers (dict): workers used for sharing data
        protocol (str): the name of the protocol
        prec_frac (int): the precision value (upper limit)
    """

    me, alice, bob, crypto_provider = (
        workers["me"],
        workers["alice"],
        workers["bob"],
        workers["james"],
    )

    args = (alice, bob)
    kwargs = dict(crypto_provider=crypto_provider, protocol=protocol)

    m = 4
    t = torch.tensor(list(range(3 * 7 * m * m))).float().reshape(3, 7, m, m)
    x = t.fix_prec(precision_fractional=prec_frac).share(*args, **kwargs)

    # using maxpool optimization for kernel_size=2
    expected = F.max_pool2d(t, kernel_size=2)
    result = F.max_pool2d(x, kernel_size=2).get().float_prec()

    # # without
    # expected = F.max_pool2d(t, kernel_size=3)
    # result = F.max_pool2d(x, kernel_size=3).get().float_prec()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1392')" href="javascript:;">
PySyft-0.2.9/benchmarks/frameworks/torch/mpc/scripts/benchmark_ast.py: 196-228
</a>
<div class="mid" id="frag1392" style="display:none"><pre>
def benchmark_avg_pool2d(workers, protocol, prec_frac):
    """
    This function benchmarks avg_plot2d function.

    Args:
        workers (dict): workers used for sharing data
        protocol (str): the name of the protocol
        prec_frac (int): the precision value (upper limit)
    """

    me, alice, bob, crypto_provider = (
        workers["me"],
        workers["alice"],
        workers["bob"],
        workers["james"],
    )

    args = (alice, bob)
    kwargs = dict(crypto_provider=crypto_provider, protocol=protocol)

    m = 4
    t = torch.tensor(list(range(3 * 7 * m * m))).float().reshape(3, 7, m, m)
    x = t.fix_prec(precision_fractional=prec_frac).share(*args, **kwargs)

    # using maxpool optimization for kernel_size=2
    expected = F.avg_pool2d(t, kernel_size=2)
    result = F.avg_pool2d(x, kernel_size=2).get().float_prec()

    # # without
    # expected = F.avg_pool2d(t, kernel_size=3)
    # result = F.avg_pool2d(x, kernel_size=3).get().float_prec()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<script language="JavaScript">
function ShowHide(divId) { 
    if(document.getElementById(divId).style.display == 'none') {
        document.getElementById(divId).style.display='block';
    } else { 
        document.getElementById(divId).style.display = 'none';
    } 
}
</script>
</body>
</html>

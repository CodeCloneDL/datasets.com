<clones>
<systeminfo processor="nicad6" system="sonnet-2.0.0" granularity="functions-blind" threshold="0%" minlines="10" maxlines="2500"/>
<cloneinfo npcs="1040" npairs="15"/>
<runinfo ncompares="3440" cputime="39031"/>
<classinfo nclasses="11"/>

<class classid="1" nclones="3" nlines="23" similarity="100">
<source file="systems/sonnet-2.0.0/sonnet/src/conv.py" startline="173" endline="223" pcid="147">
  def __init__(self,
               output_channels: int,
               kernel_shape: Union[int, Sequence[int]],
               stride: Union[int, Sequence[int]] = 1,
               rate: Union[int, Sequence[int]] = 1,
               padding: Union[Text, pad.Paddings] = "SAME",
               with_bias: bool = True,
               w_init: Optional[initializers.Initializer] = None,
               b_init: Optional[initializers.Initializer] = None,
               data_format: Text = "NWC",
               name: Optional[Text] = None):
    """Constructs a ``Conv1D`` module.

    Args:
      output_channels: The number of output channels.
      kernel_shape: Sequence of length 1, or an integer. ``kernel_shape`` will
        be expanded to define a kernel size in all dimensions.
      stride: Sequence of strides of length 1, or an integer. ``stride`` will be
        expanded to define stride in all dimensions.
      rate: Sequence of dilation rates of length 1, or integer that is used to
        define dilation rate in all dimensions. 1 corresponds to standard
        convolution, ``rate > 1`` corresponds to dilated convolution.
      padding: Padding to apply to the input. This can be either ``SAME``,
        ``VALID`` or a callable or sequence of callables of size 1. Any
        callables must take a single integer argument equal to the effective
        kernel size and return a list of two integers representing the padding
        before and after. See snt.pad.* for more details and example functions.
      with_bias: Whether to include bias parameters. Default ``True``.
      w_init: Optional initializer for the weights. By default the weights are
        initialized truncated random normal values with a standard deviation of
        ``1``/``sqrt(input_feature_size)``, which is commonly used when the
        inputs are zero centered (see https://arxiv.org/abs/1502.03167v3).
      b_init: Optional initializer for the bias. By default the bias is
        initialized to zero.
      data_format: The data format of the input.
      name: Name of the module.
    """
    super(Conv1D, self).__init__(
        num_spatial_dims=1,
        output_channels=output_channels,
        kernel_shape=kernel_shape,
        stride=stride,
        rate=rate,
        padding=padding,
        with_bias=with_bias,
        w_init=w_init,
        b_init=b_init,
        data_format=data_format,
        name=name)


</source>
<source file="systems/sonnet-2.0.0/sonnet/src/conv.py" startline="282" endline="331" pcid="149">
  def __init__(self,
               output_channels: int,
               kernel_shape: Union[int, Sequence[int]],
               stride: Union[int, Sequence[int]] = 1,
               rate: Union[int, Sequence[int]] = 1,
               padding: Union[Text, pad.Paddings] = "SAME",
               with_bias: bool = True,
               w_init: Optional[initializers.Initializer] = None,
               b_init: Optional[initializers.Initializer] = None,
               data_format: Text = "NDHWC",
               name: Optional[Text] = None):
    """Constructs a ``Conv3D`` module.

    Args:
      output_channels: The number of output channels.
      kernel_shape: Sequence of kernel sizes (of length 3), or an integer.
        ``kernel_shape`` will be expanded to define a kernel size in all
        dimensions.
      stride: Sequence of strides (of length 3), or an integer. `stride` will be
        expanded to define stride in all dimensions.
      rate: Sequence of dilation rates (of length 3), or integer that is used to
        define dilation rate in all dimensions. 1 corresponds to standard
        convolution, ``rate > 1`` corresponds to dilated convolution.
      padding: Padding to apply to the input. This can either ``SAME``,
        ``VALID`` or a callable or sequence of callables up to size N. Any
        callables must take a single integer argument equal to the effective
        kernel size and return a list of two integers representing the padding
        before and after. See snt.pad.* for more details and example functions.
      with_bias: Whether to include bias parameters. Default ``True``.
      w_init: Optional initializer for the weights. By default the weights are
        initialized truncated random normal values with a standard deviation of
        ``1 / sqrt(input_feature_size)``, which is commonly used when the inputs
        are zero centered (see https://arxiv.org/abs/1502.03167v3).
      b_init: Optional initializer for the bias. By default the bias is
        initialized to zero.
      data_format: The data format of the input.
      name: Name of the module.
    """
    super(Conv3D, self).__init__(
        num_spatial_dims=3,
        output_channels=output_channels,
        kernel_shape=kernel_shape,
        stride=stride,
        rate=rate,
        padding=padding,
        with_bias=with_bias,
        w_init=w_init,
        b_init=b_init,
        data_format=data_format,
        name=name)
</source>
<source file="systems/sonnet-2.0.0/sonnet/src/conv.py" startline="227" endline="278" pcid="148">
  def __init__(self,
               output_channels: int,
               kernel_shape: Union[int, Sequence[int]],
               stride: Union[int, Sequence[int]] = 1,
               rate: Union[int, Sequence[int]] = 1,
               padding: Union[Text, pad.Paddings] = "SAME",
               with_bias: bool = True,
               w_init: Optional[initializers.Initializer] = None,
               b_init: Optional[initializers.Initializer] = None,
               data_format: Text = "NHWC",
               name: Optional[Text] = None):
    """Constructs a ``Conv2D`` module.

    Args:
      output_channels: The number of output channels.
      kernel_shape: Sequence of kernel sizes (of length 2), or an integer.
        ``kernel_shape`` will be expanded to define a kernel size in all
        dimensions.
      stride: Sequence of strides (of length 2), or an integer. ``stride`` will
        be expanded to define stride in all dimensions.
      rate: Sequence of dilation rates (of length 2), or integer that is used to
        define dilation rate in all dimensions. 1 corresponds to standard
        convolution, ``rate > 1`` corresponds to dilated convolution.
      padding: Padding to apply to the input. This can either ``SAME``,
        ``VALID`` or a callable or sequence of callables of size 2. Any
        callables must take a single integer argument equal to the effective
        kernel size and return a list of two integers representing the padding
        before and after. See snt.pad.* for more details and example functions.
      with_bias: Whether to include bias parameters. Default ``True``.
      w_init: Optional initializer for the weights. By default the weights are
        initialized truncated random normal values with a standard deviation of
        ``1 / sqrt(input_feature_size)``, which is commonly used when the inputs
        are zero centered (see https://arxiv.org/abs/1502.03167v3).
      b_init: Optional initializer for the bias. By default the bias is
        initialized to zero.
      data_format: The data format of the input.
      name: Name of the module.
    """
    super(Conv2D, self).__init__(
        num_spatial_dims=2,
        output_channels=output_channels,
        kernel_shape=kernel_shape,
        stride=stride,
        rate=rate,
        padding=padding,
        with_bias=with_bias,
        w_init=w_init,
        b_init=b_init,
        data_format=data_format,
        name=name)


</source>
</class>

<class classid="2" nclones="2" nlines="10" similarity="100">
<source file="systems/sonnet-2.0.0/sonnet/src/base_test.py" startline="180" endline="193" pcid="230">
  def test_get_attr_doesnt_enter_name_scope(self):
    scope_names = []

    class GetAttrModule(base.Module):

      def __getattr__(self, name):
        scope_names.append((name, get_name_scope()))
        return super(GetAttrModule, self).__getattr__(name)

    mod = GetAttrModule()
    with self.assertRaises(AttributeError):
      mod.does_not_exist  # pylint: disable=pointless-statement
    self.assertIn(("does_not_exist", ""), scope_names)

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/base_test.py" startline="194" endline="208" pcid="232">
  def test_get_attribute_doesnt_enter_name_scope(self):
    scope_names = []

    class GetAttributeModule(base.Module):

      def __getattribute__(self, name):
        scope_names.append((name, get_name_scope()))
        return super(GetAttributeModule, self).__getattribute__(name)

    mod = GetAttributeModule()
    with self.assertRaises(AttributeError):
      mod.does_not_exist  # pylint: disable=pointless-statement
    self.assertIn(("does_not_exist", ""), scope_names)


</source>
</class>

<class classid="3" nclones="2" nlines="13" similarity="100">
<source file="systems/sonnet-2.0.0/sonnet/src/optimizers/rmsprop_test.py" startline="66" endline="82" pcid="326">
  def testDense(self):
    parameters = [tf.Variable([1., 2.]), tf.Variable([3., 4.])]
    updates = [tf.constant([5., 5.]), tf.constant([3., 3.])]
    optimizer = self.make_optimizer(learning_rate=0.1)
    # Step 1 of RMSProp
    optimizer.apply(updates, parameters)
    self.assertAllClose([[0.683772, 1.683772], [2.683772, 3.683772]],
                        [x.numpy() for x in parameters])
    # Step 2 of RMSProp
    optimizer.apply(updates, parameters)
    self.assertAllClose([[0.454357, 1.454357], [2.454357, 3.454357]],
                        [x.numpy() for x in parameters])
    # Step 3 of RMSProp
    optimizer.apply(updates, parameters)
    self.assertAllClose([[0.262262, 1.262262], [2.262262, 3.262262]],
                        [x.numpy() for x in parameters])

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/optimizers/adam_test.py" startline="63" endline="79" pcid="360">
  def testDense(self):
    parameters = [tf.Variable([1., 2.]), tf.Variable([3., 4.])]
    updates = [tf.constant([5., 5.]), tf.constant([3., 3.])]
    optimizer = self.make_optimizer(learning_rate=0.001)
    # Step 1 of Adam
    optimizer.apply(updates, parameters)
    self.assertAllClose([[0.999, 1.999], [2.999, 3.999]],
                        [x.numpy() for x in parameters])
    # Step 2 of Adam
    optimizer.apply(updates, parameters)
    self.assertAllClose([[0.998, 1.998], [2.998, 3.998]],
                        [x.numpy() for x in parameters])
    # Step 3 of Adam
    optimizer.apply(updates, parameters)
    self.assertAllClose([[0.997, 1.997], [2.997, 3.997]],
                        [x.numpy() for x in parameters])

</source>
</class>

<class classid="4" nclones="2" nlines="12" similarity="100">
<source file="systems/sonnet-2.0.0/sonnet/src/conv_test.py" startline="158" endline="173" pcid="407">
  def testUnknownBatchSizeNHWC(self):
    x = tf.TensorSpec([None, 5, 5, 3], dtype=tf.float32)

    c = conv.ConvND(
        num_spatial_dims=2,
        output_channels=2,
        kernel_shape=3,
        data_format="NHWC")
    defun_conv = tf.function(c).get_concrete_function(x)

    out1 = defun_conv(tf.ones([3, 5, 5, 3]))
    self.assertEqual(out1.shape, [3, 5, 5, 2])

    out2 = defun_conv(tf.ones([5, 5, 5, 3]))
    self.assertEqual(out2.shape, [5, 5, 5, 2])

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/conv_transpose_test.py" startline="97" endline="112" pcid="575">
  def testUnknownBatchSizeNHWC(self):
    x = tf.TensorSpec([None, 5, 5, 3], dtype=tf.float32)

    c = conv_transpose.ConvNDTranspose(
        num_spatial_dims=2,
        output_channels=2,
        kernel_shape=3,
        data_format="NHWC")
    defun_conv = tf.function(c).get_concrete_function(x)

    out1 = defun_conv(tf.ones([3, 5, 5, 3]))
    self.assertEqual(out1.shape, [3, 5, 5, 2])

    out2 = defun_conv(tf.ones([5, 5, 5, 3]))
    self.assertEqual(out2.shape, [5, 5, 5, 2])

</source>
</class>

<class classid="5" nclones="2" nlines="14" similarity="100">
<source file="systems/sonnet-2.0.0/sonnet/src/conv_test.py" startline="174" endline="191" pcid="408">
  def testUnknownBatchSizeNCHW(self):
    if self.primary_device == "CPU":
      self.skipTest("NCHW not supported on CPU")

    x = tf.TensorSpec([None, 3, 5, 5], dtype=tf.float32)
    c = conv.ConvND(
        num_spatial_dims=2,
        output_channels=2,
        kernel_shape=3,
        data_format="NCHW")
    defun_conv = tf.function(c).get_concrete_function(x)

    out1 = defun_conv(tf.ones([3, 3, 5, 5]))
    self.assertEqual(out1.shape, [3, 2, 5, 5])

    out2 = defun_conv(tf.ones([5, 3, 5, 5]))
    self.assertEqual(out2.shape, [5, 2, 5, 5])

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/conv_transpose_test.py" startline="113" endline="131" pcid="576">
  def testUnknownBatchSizeNCHW(self):
    if self.primary_device == "CPU":
      self.skipTest("NCHW not supported on CPU")

    x = tf.TensorSpec([None, 3, 5, 5], dtype=tf.float32)

    c = conv_transpose.ConvNDTranspose(
        num_spatial_dims=2,
        output_channels=2,
        kernel_shape=3,
        data_format="NCHW")
    defun_conv = tf.function(c).get_concrete_function(x)

    out1 = defun_conv(tf.ones([3, 3, 5, 5]))
    self.assertEqual(out1.shape, [3, 2, 5, 5])

    out2 = defun_conv(tf.ones([5, 3, 5, 5]))
    self.assertEqual(out2.shape, [5, 2, 5, 5])

</source>
</class>

<class classid="6" nclones="2" nlines="11" similarity="100">
<source file="systems/sonnet-2.0.0/sonnet/src/conv_test.py" startline="193" endline="206" pcid="409">
  def testUnknownChannels(self, autograph):
    x = tf.TensorSpec([3, 3, 3, None], dtype=tf.float32)

    c = conv.ConvND(
        num_spatial_dims=2,
        output_channels=1,
        kernel_shape=3,
        data_format="NHWC")
    defun_conv = tf.function(c, autograph=autograph)

    with self.assertRaisesRegex(ValueError,
                                "The number of input channels must be known"):
      defun_conv.get_concrete_function(x)

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/conv_transpose_test.py" startline="133" endline="146" pcid="577">
  def testUnknownChannels(self, autograph):
    x = tf.TensorSpec([3, 3, 3, None], dtype=tf.float32)

    c = conv_transpose.ConvNDTranspose(
        num_spatial_dims=2,
        output_channels=1,
        kernel_shape=3,
        data_format="NHWC")
    defun_conv = tf.function(c, autograph=autograph)

    with self.assertRaisesRegex(ValueError,
                                "The number of input channels must be known"):
      defun_conv.get_concrete_function(x)

</source>
</class>

<class classid="7" nclones="2" nlines="14" similarity="100">
<source file="systems/sonnet-2.0.0/sonnet/src/linear.py" startline="36" endline="63" pcid="465">
  def __init__(self,
               output_size: int,
               with_bias: bool = True,
               w_init: Optional[initializers.Initializer] = None,
               b_init: Optional[initializers.Initializer] = None,
               name: Optional[Text] = None):
    """Constructs a `Linear` module.

    Args:
      output_size: Output dimensionality.
      with_bias: Whether to include bias parameters. Default `True`.
      w_init: Optional initializer for the weights. By default the weights are
        initialized truncated random normal values with a standard deviation of
        `1 / sqrt(input_feature_size)`, which is commonly used when the inputs
        are zero centered (see https://arxiv.org/abs/1502.03167v3).
      b_init: Optional initializer for the bias. By default the bias is
        initialized to zero.
      name: Name of the module.
    """
    super(Linear, self).__init__(name=name)
    self.output_size = output_size
    self.with_bias = with_bias
    self.w_init = w_init
    if with_bias:
      self.b_init = b_init if b_init is not None else initializers.Zeros()
    elif b_init is not None:
      raise ValueError("When not using a bias the b_init must be None.")

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/parallel_linear.py" startline="44" endline="71" pcid="805">
  def __init__(self,
               output_size: int,
               with_bias: bool = True,
               w_init: Optional[initializers.Initializer] = None,
               b_init: Optional[initializers.Initializer] = None,
               name: Optional[Text] = None):
    """Constructs a `ParallelLinear` module.

    Args:
      output_size: Output dimensionality.
      with_bias: Whether to include bias parameters. Default `True`.
      w_init: Optional initializer for the weights. By default the weights are
        initialized truncated random normal values with a standard deviation of
        `1 / sqrt(input_feature_size)`, which is commonly used when the inputs
        are zero centered (see https://arxiv.org/abs/1502.03167v3).
      b_init: Optional initializer for the bias. By default the bias is
        initialized to zero.
      name: Name of the module.
    """
    super(ParallelLinears, self).__init__(name=name)
    self.output_size = output_size
    self.with_bias = with_bias
    self.w_init = w_init
    if with_bias:
      self.b_init = b_init if b_init is not None else initializers.Zeros()
    elif b_init is not None:
      raise ValueError("When not using a bias the b_init must be None.")

</source>
</class>

<class classid="8" nclones="2" nlines="11" similarity="100">
<source file="systems/sonnet-2.0.0/sonnet/src/nets/dnc/write_test.py" startline="84" endline="96" pcid="501">
  def testShape(self):
    batch_size = 1
    num_writes = 2
    memory_size = 5
    word_size = 3

    mem = tf.random.uniform([batch_size, memory_size, word_size])
    write_address = tf.random.uniform([batch_size, num_writes, memory_size])
    reset_weights = tf.random.uniform([batch_size, num_writes, word_size])
    writer = write.erase(mem, write_address, reset_weights)
    self.assertTrue(writer.shape.as_list(),
                    [batch_size, memory_size, word_size])

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/nets/dnc/write_test.py" startline="178" endline="190" pcid="505">
  def testShape(self):
    batch_size = 4
    num_writes = 2
    memory_size = 5
    word_size = 3

    mem = tf.random.uniform([batch_size, memory_size, word_size])
    write_address = tf.random.uniform([batch_size, num_writes, memory_size])
    values = tf.random.uniform([batch_size, num_writes, word_size])
    writer = write.additive_write(mem, write_address, values)
    self.assertAllEqual(writer.shape.as_list(),
                        [batch_size, memory_size, word_size])

</source>
</class>

<class classid="9" nclones="2" nlines="11" similarity="100">
<source file="systems/sonnet-2.0.0/sonnet/src/recurrent_test.py" startline="70" endline="83" pcid="626">
  def testInitialization(self):
    core = recurrent.VanillaRNN(
        hidden_size=self.hidden_size,
        w_i_init=initializers.Ones(),
        w_h_init=initializers.Ones(),
        b_init=initializers.Ones())
    inputs = tf.random.uniform([self.batch_size, self.input_size])
    prev_state = core.initial_state(self.batch_size)
    core(inputs, prev_state)

    for v in core.variables:
      self.assertAllClose(self.evaluate(v), self.evaluate(tf.ones_like(v)))


</source>
<source file="systems/sonnet-2.0.0/sonnet/src/recurrent_test.py" startline="486" endline="499" pcid="654">
  def testInitialization(self):
    core = recurrent.GRU(
        hidden_size=self.hidden_size,
        w_i_init=initializers.Ones(),
        w_h_init=initializers.Ones(),
        b_init=initializers.Ones())
    inputs = tf.random.uniform([self.batch_size, self.input_size])
    prev_state = core.initial_state(self.batch_size)
    core(inputs, prev_state)

    for v in core.variables:
      self.assertAllClose(self.evaluate(v), self.evaluate(tf.ones_like(v)))


</source>
</class>

<class classid="10" nclones="2" nlines="11" similarity="100">
<source file="systems/sonnet-2.0.0/sonnet/src/mixed_precision_test.py" startline="198" endline="212" pcid="728">
  def test_function_create_module_eligible(self, test_class):
    mixed_precision.enable(tf.float16)

    @mixed_precision.modes([tf.float32, tf.float16])
    def model():
      x = tf.Variable([[1., 9.], [8., 9.]])
      d = test_class(x)
      d.check_type = mixed_precision.modes([tf.float32, tf.float16])(
          d.check_type)

      self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)
      self.assertEqual(d.check_type(x, tf.float16).dtype, tf.float32)

    model()

</source>
<source file="systems/sonnet-2.0.0/sonnet/src/mixed_precision_test.py" startline="213" endline="227" pcid="730">
  def test_function_create_module_ineligible(self, test_class):
    mixed_precision.enable(tf.float16)

    @mixed_precision.modes([tf.float32, tf.float16])
    def model():
      x = tf.Variable([[1., 9.], [8., 9.]])
      d = test_class(x)
      d.check_type = mixed_precision.modes([tf.float32, tf.bfloat16])(
          d.check_type)

      self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)
      self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)

    model()

</source>
</class>

<class classid="11" nclones="3" nlines="25" similarity="100">
<source file="systems/sonnet-2.0.0/sonnet/src/conv_transpose.py" startline="197" endline="250" pcid="889">
  def __init__(self,
               output_channels: int,
               kernel_shape: Union[int, Sequence[int]],
               output_shape: Optional[types.ShapeLike] = None,
               stride: Union[int, Sequence[int]] = 1,
               rate: Union[int, Sequence[int]] = 1,
               padding: Text = "SAME",
               with_bias: bool = True,
               w_init: Optional[initializers.Initializer] = None,
               b_init: Optional[initializers.Initializer] = None,
               data_format: Text = "NWC",
               name: Optional[Text] = None):
    """Constructs a `Conv1DTranspose` module.

    Args:
      output_channels: Number of output channels.
      kernel_shape: Sequence of integers (of length 1), or an integer
        representing kernel shape. `kernel_shape` will be expanded to define a
        kernel size in all dimensions.
      output_shape: Output shape of the spatial dimensions of a transpose
        convolution. Can be either an integer or an iterable of integers or
        `Dimension`s, or a `TensorShape` (of length 1). If a `None` value is
        given, a default shape is automatically calculated.
      stride: Sequence of integers (of length 1), or an integer. `stride` will
        be expanded to define stride in all dimensions.
      rate: Sequence of integers (of length 1), or integer that is used to
        define dilation rate in all dimensions. 1 corresponds to standard 1D
        convolution, `rate > 1` corresponds to dilated convolution.
      padding: Padding algorithm, either "SAME" or "VALID".
      with_bias: Boolean, whether to include bias parameters. Default `True`.
      w_init: Optional initializer for the weights. By default the weights are
        initialized truncated random normal values with a standard deviation of
        `1 / sqrt(input_feature_size)`, which is commonly used when the
        inputs are zero centered (see https://arxiv.org/abs/1502.03167v3).
      b_init: Optional initializer for the bias. By default the bias is
        initialized to zero.
      data_format: The data format of the input.
      name: Name of the module.
    """
    super(Conv1DTranspose, self).__init__(
        num_spatial_dims=1,
        output_channels=output_channels,
        kernel_shape=kernel_shape,
        output_shape=output_shape,
        stride=stride,
        rate=rate,
        padding=padding,
        with_bias=with_bias,
        w_init=w_init,
        b_init=b_init,
        data_format=data_format,
        name=name)


</source>
<source file="systems/sonnet-2.0.0/sonnet/src/conv_transpose.py" startline="311" endline="362" pcid="891">
  def __init__(self,
               output_channels: int,
               kernel_shape: Union[int, Sequence[int]],
               output_shape: Optional[types.ShapeLike] = None,
               stride: Union[int, Sequence[int]] = 1,
               rate: Union[int, Sequence[int]] = 1,
               padding: Text = "SAME",
               with_bias: bool = True,
               w_init: Optional[initializers.Initializer] = None,
               b_init: Optional[initializers.Initializer] = None,
               data_format: Text = "NDHWC",
               name: Optional[Text] = None):
    """Constructs a `Conv3DTranspose` module.

    Args:
      output_channels: An integer, The number of output channels.
      kernel_shape: Sequence of integers (of length 3), or an integer
        representing kernel shape. `kernel_shape` will be expanded to define a
        kernel size in all dimensions.
      output_shape: Output shape of the spatial dimensions of a transpose
        convolution. Can be either an integer or an iterable of integers or
        `Dimension`s, or a `TensorShape` (of length 3). If a None value is
        given, a default shape is automatically calculated.
      stride: Sequence of integers (of length 3), or an integer. `stride` will
        be expanded to define stride in all dimensions.
      rate: Sequence of integers (of length 3), or integer that is used to
        define dilation rate in all dimensions. 1 corresponds to standard 3D
        convolution, `rate > 1` corresponds to dilated convolution.
      padding: Padding algorithm, either "SAME" or "VALID".
      with_bias: Boolean, whether to include bias parameters. Default `True`.
      w_init: Optional initializer for the weights. By default the weights are
        initialized truncated random normal values with a standard deviation of
        `1 / sqrt(input_feature_size)`, which is commonly used when the
        inputs are zero centered (see https://arxiv.org/abs/1502.03167v3).
      b_init: Optional initializer for the bias. By default the bias is
        initialized to zero.
      data_format: The data format of the input.
      name: Name of the module.
    """
    super(Conv3DTranspose, self).__init__(
        num_spatial_dims=3,
        output_channels=output_channels,
        kernel_shape=kernel_shape,
        output_shape=output_shape,
        stride=stride,
        rate=rate,
        padding=padding,
        with_bias=with_bias,
        w_init=w_init,
        b_init=b_init,
        data_format=data_format,
        name=name)
</source>
<source file="systems/sonnet-2.0.0/sonnet/src/conv_transpose.py" startline="254" endline="307" pcid="890">
  def __init__(self,
               output_channels: int,
               kernel_shape: Union[int, Sequence[int]],
               output_shape: Optional[types.ShapeLike] = None,
               stride: Union[int, Sequence[int]] = 1,
               rate: Union[int, Sequence[int]] = 1,
               padding: Text = "SAME",
               with_bias: bool = True,
               w_init: Optional[initializers.Initializer] = None,
               b_init: Optional[initializers.Initializer] = None,
               data_format: Text = "NHWC",
               name: Optional[Text] = None):
    """Constructs a `Conv2DTranspose` module.

    Args:
      output_channels: An integer, The number of output channels.
      kernel_shape: Sequence of integers (of length 2), or an integer
        representing kernel shape. `kernel_shape` will be expanded to define a
        kernel size in all dimensions.
      output_shape: Output shape of the spatial dimensions of a transpose
        convolution. Can be either an integer or an iterable of integers or
        `Dimension`s, or a `TensorShape` (of length 2). If a `None` value is
        given, a default shape is automatically calculated.
      stride: Sequence of integers (of length 2), or an integer. `stride` will
        be expanded to define stride in all dimensions.
      rate: Sequence of integers (of length 2), or integer that is used to
        define dilation rate in all dimensions. 1 corresponds to standard 2D
        convolution, `rate > 1` corresponds to dilated convolution.
      padding: Padding algorithm, either "SAME" or "VALID".
      with_bias: Boolean, whether to include bias parameters. Default `True`.
      w_init: Optional initializer for the weights. By default the weights are
        initialized truncated random normal values with a standard deviation of
        `1 / sqrt(input_feature_size)`, which is commonly used when the
        inputs are zero centered (see https://arxiv.org/abs/1502.03167v3).
      b_init: Optional initializer for the bias. By default the bias is
        initialized to zero.
      data_format: The data format of the input.
      name: Name of the module.
    """
    super(Conv2DTranspose, self).__init__(
        num_spatial_dims=2,
        output_channels=output_channels,
        kernel_shape=kernel_shape,
        output_shape=output_shape,
        stride=stride,
        rate=rate,
        padding=padding,
        with_bias=with_bias,
        w_init=w_init,
        b_init=b_init,
        data_format=data_format,
        name=name)


</source>
</class>

</clones>

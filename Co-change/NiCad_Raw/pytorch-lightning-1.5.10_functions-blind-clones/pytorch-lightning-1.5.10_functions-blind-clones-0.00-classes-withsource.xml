<clones>
<systeminfo processor="nicad6" system="pytorch-lightning-1.5.10" granularity="functions-blind" threshold="0%" minlines="10" maxlines="2500"/>
<cloneinfo npcs="1729" npairs="4"/>
<runinfo ncompares="4778" cputime="45197"/>
<classinfo nclasses="4"/>

<class classid="1" nclones="2" nlines="10" similarity="100">
<source file="systems/pytorch-lightning-1.5.10/legacy/simple_classif_training.py" startline="108" endline="118" pcid="14">
    def forward(self, x):
        x = self.layer_0(x)
        x = self.layer_0a(x)
        x = self.layer_1(x)
        x = self.layer_1a(x)
        x = self.layer_2(x)
        x = self.layer_2a(x)
        x = self.layer_end(x)
        logits = F.softmax(x, dim=1)
        return logits

</source>
<source file="systems/pytorch-lightning-1.5.10/tests/helpers/simple_models.py" startline="36" endline="46" pcid="698">
    def forward(self, x):
        x = self.layer_0(x)
        x = self.layer_0a(x)
        x = self.layer_1(x)
        x = self.layer_1a(x)
        x = self.layer_2(x)
        x = self.layer_2a(x)
        x = self.layer_end(x)
        logits = F.softmax(x, dim=1)
        return logits

</source>
</class>

<class classid="2" nclones="2" nlines="14" similarity="100">
<source file="systems/pytorch-lightning-1.5.10/tests/accelerators/test_ddp_spawn.py" startline="45" endline="65" pcid="23">
def test_multi_gpu_model_ddp_spawn(tmpdir):
    tutils.set_random_main_port()

    trainer_options = dict(
        default_root_dir=tmpdir,
        max_epochs=1,
        limit_train_batches=10,
        limit_val_batches=10,
        gpus=[0, 1],
        strategy="ddp_spawn",
        enable_progress_bar=False,
    )

    model = BoringModel()

    tpipes.run_model_test(trainer_options, model)

    # test memory helper functions
    memory.get_memory_profile("min_max")


</source>
<source file="systems/pytorch-lightning-1.5.10/tests/accelerators/test_dp.py" startline="81" endline="101" pcid="44">
def test_multi_gpu_model_dp(tmpdir):
    tutils.set_random_main_port()

    trainer_options = dict(
        default_root_dir=tmpdir,
        max_epochs=1,
        limit_train_batches=10,
        limit_val_batches=10,
        gpus=[0, 1],
        strategy="dp",
        enable_progress_bar=False,
    )

    model = BoringModel()

    tpipes.run_model_test(trainer_options, model)

    # test memory helper functions
    memory.get_memory_profile("min_max")


</source>
</class>

<class classid="3" nclones="2" nlines="17" similarity="100">
<source file="systems/pytorch-lightning-1.5.10/tests/plugins/environments/test_kubeflow_environment.py" startline="54" endline="76" pcid="765">
def test_attributes_from_environment_variables(caplog):
    """Test that the torchelastic cluster environment takes the attributes from the environment variables."""
    env = KubeflowEnvironment()
    assert env.master_address() == "1.2.3.4"
    assert env.master_port() == 500
    assert env.world_size() == 20
    assert env.global_rank() == 1
    assert env.local_rank() == 0
    assert env.node_rank() == 1
    # setter should be no-op
    with caplog.at_level(logging.DEBUG, logger="pytorch_lightning.plugins.environments"):
        env.set_global_rank(100)
    assert env.global_rank() == 1
    assert "setting global rank is not allowed" in caplog.text

    caplog.clear()

    with caplog.at_level(logging.DEBUG, logger="pytorch_lightning.plugins.environments"):
        env.set_world_size(100)
    assert env.world_size() == 20
    assert "setting world size is not allowed" in caplog.text


</source>
<source file="systems/pytorch-lightning-1.5.10/tests/plugins/environments/test_torchelastic_environment.py" startline="48" endline="68" pcid="776">
def test_attributes_from_environment_variables(caplog):
    """Test that the torchelastic cluster environment takes the attributes from the environment variables."""
    env = TorchElasticEnvironment()
    assert env.master_address() == "1.2.3.4"
    assert env.master_port() == 500
    assert env.world_size() == 20
    assert env.global_rank() == 1
    assert env.local_rank() == 2
    assert env.node_rank() == 3
    # setter should be no-op
    with caplog.at_level(logging.DEBUG, logger="pytorch_lightning.plugins.environments"):
        env.set_global_rank(100)
    assert env.global_rank() == 1
    assert "setting global rank is not allowed" in caplog.text

    caplog.clear()

    with caplog.at_level(logging.DEBUG, logger="pytorch_lightning.plugins.environments"):
        env.set_world_size(100)
    assert env.world_size() == 20
    assert "setting world size is not allowed" in caplog.text
</source>
</class>

<class classid="4" nclones="2" nlines="10" similarity="100">
<source file="systems/pytorch-lightning-1.5.10/tests/trainer/test_config_validator.py" startline="22" endline="39" pcid="1177">
def test_wrong_train_setting(tmpdir):
    """
    * Test that an error is thrown when no `train_dataloader()` is defined
    * Test that an error is thrown when no `training_step()` is defined
    """
    trainer = Trainer(default_root_dir=tmpdir, max_epochs=1)

    with pytest.raises(MisconfigurationException, match=r"No `train_dataloader\(\)` method defined."):
        model = BoringModel()
        model.train_dataloader = None
        trainer.fit(model)

    with pytest.raises(MisconfigurationException, match=r"No `training_step\(\)` method defined."):
        model = BoringModel()
        model.training_step = None
        trainer.fit(model)


</source>
<source file="systems/pytorch-lightning-1.5.10/tests/trainer/test_config_validator.py" startline="50" endline="66" pcid="1179">
def test_fit_val_loop_config(tmpdir):
    """When either val loop or val data are missing raise warning."""
    trainer = Trainer(default_root_dir=tmpdir, max_epochs=1)

    # no val data has val loop
    with pytest.warns(UserWarning, match=r"You passed in a `val_dataloader` but have no `validation_step`"):
        model = BoringModel()
        model.validation_step = None
        trainer.fit(model)

    # has val loop but no val data
    with pytest.warns(UserWarning, match=r"You defined a `validation_step` but have no `val_dataloader`"):
        model = BoringModel()
        model.val_dataloader = None
        trainer.fit(model)


</source>
</class>

</clones>

<html>
<head>
    <title>NiCad6 Clone Report</title>
    <style type="text/css">
        body {font-family:sans-serif;}
        table {background-color:white; border:0px; padding:0px; border-spacing:4px; width:auto; margin-left:30px; margin-right:auto;}
        td {background-color:rgba(192,212,238,0.8); border:0px; padding:8px; width:auto; vertical-align:top; border-radius:8px}
        pre {background-color:white; padding:4px;}
        a {color:darkblue;}
    </style>
</head>
<body>
<h2>NiCad6 Clone Report</h2>
<table>
<tr style="font-size:14pt">
<td><b>System:</b> &nbsp; chainer-7.2.0</td>
<td><b>Clone pairs:</b> &nbsp; 784</td>
<td><b>Clone classes:</b> &nbsp; 282</td>
</tr>
<tr style="font-size:12pt">
<td style="background-color:white">Clone type: &nbsp; 3-2</td>
<td style="background-color:white">Granularity: &nbsp; functions-blind</td>
<td style="background-color:white">Max diff threshold: &nbsp; 30%</td>
<td style="background-color:white">Clone size: &nbsp; 10 - 2500 lines</td>
<td style="background-color:white">Total functions-blind: &nbsp; 10723</td>
</tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 1:</b> &nbsp; 7 fragments, nominal size 10 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag156')" href="javascript:;">
chainer-7.2.0/chainer/distributions/geometric.py: 54-65
</a>
<div class="mid" id="frag156" style="display:none"><pre>
    def sample_n(self, n):
        xp = chainer.backend.get_array_module(self.p)
        if xp is cuda.cupy:
            eps = xp.random.geometric(
                self.p.data,
                size=(n,)+self.batch_shape, dtype=self.p.dtype)
        else:
            eps = xp.random.geometric(
                self.p.data,
                size=(n,)+self.batch_shape).astype(self.p.dtype)
        return chainer.Variable(eps)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag348')" href="javascript:;">
chainer-7.2.0/chainer/distributions/gumbel.py: 78-88
</a>
<div class="mid" id="frag348" style="display:none"><pre>
    def sample_n(self, n):
        xp = chainer.backend.get_array_module(self.loc)
        if xp is cuda.cupy:
            eps = xp.random.gumbel(
                size=(n,)+self.batch_shape, dtype=self.loc.dtype)
        else:
            eps = xp.random.gumbel(
                size=(n,)+self.batch_shape).astype(self.loc.dtype)
        noise = self.scale * eps + self.loc
        return noise

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag362')" href="javascript:;">
chainer-7.2.0/chainer/distributions/gamma.py: 76-86
</a>
<div class="mid" id="frag362" style="display:none"><pre>
    def sample_n(self, n):
        xp = chainer.backend.get_array_module(self.k)
        if xp is cuda.cupy:
            eps = xp.random.gamma(
                self.k.data, size=(n,) + self.batch_shape, dtype=self.k.dtype)
        else:
            eps = xp.random.gamma(
                self.k.data, size=(n,) + self.batch_shape).astype(self.k.dtype)
        noise = broadcast.broadcast_to(self.theta, eps.shape) * eps
        return noise

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag173')" href="javascript:;">
chainer-7.2.0/chainer/distributions/chisquare.py: 69-79
</a>
<div class="mid" id="frag173" style="display:none"><pre>
    def sample_n(self, n):
        xp = chainer.backend.get_array_module(self.k)
        if xp is cuda.cupy:
            eps = xp.random.chisquare(
                self.k.data, (n,)+self.k.shape, dtype=self.k.dtype)
        else:
            eps = xp.random.chisquare(
                self.k.data, (n,)+self.k.shape).astype(self.k.dtype)
        noise = chainer.Variable(eps)
        return noise

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag251')" href="javascript:;">
chainer-7.2.0/chainer/distributions/exponential.py: 76-86
</a>
<div class="mid" id="frag251" style="display:none"><pre>
    def sample_n(self, n):
        xp = chainer.backend.get_array_module(self.lam)
        if xp is cuda.cupy:
            eps = xp.random.standard_exponential(
                (n,)+self.lam.shape, dtype=self.lam.dtype)
        else:
            eps = xp.random.standard_exponential(
                (n,)+self.lam.shape).astype(self.lam.dtype)
        noise = eps / self.lam
        return noise

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag189')" href="javascript:;">
chainer-7.2.0/chainer/distributions/cauchy.py: 91-103
</a>
<div class="mid" id="frag189" style="display:none"><pre>
    def sample_n(self, n):
        xp = chainer.backend.get_array_module(self.loc)
        if xp is cuda.cupy:
            eps = xp.random.standard_cauchy(
                (n,)+self.loc.shape, dtype=self.loc.dtype)
        else:
            eps = xp.random.standard_cauchy(
                (n,)+self.loc.shape).astype(self.loc.dtype)

        noise = self.scale * eps + self.loc

        return noise

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag225')" href="javascript:;">
chainer-7.2.0/chainer/distributions/pareto.py: 90-102
</a>
<div class="mid" id="frag225" style="display:none"><pre>
    def sample_n(self, n):
        xp = chainer.backend.get_array_module(self.scale)
        if xp is cuda.cupy:
            eps = xp.random.pareto(
                self.alpha.data, (n,)+self.batch_shape, dtype=self.alpha.dtype)
        else:
            eps = xp.random.pareto(
                self.alpha.data, (n,)+self.batch_shape
            ).astype(self.alpha.dtype)

        noise = self.scale * (eps + 1)
        return noise

</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 2:</b> &nbsp; 4 fragments, nominal size 21 lines, similarity 86%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag447')" href="javascript:;">
chainer-7.2.0/chainer/function_node.py: 1309-1346
</a>
<div class="mid" id="frag447" style="display:none"><pre>

def _extract_apply_in_data(inputs):
    # Extracts arrays from FunctionNode.apply() inputs.
    #
    # A flag that indicates whether inputs are chainerx arrays is also
    # returned.
    #
    # Each object in `inputs` may be `Variable` or an array.
    # If it's a `Variable` and its underlying array is a chainerx array,
    # `Variable._data[0]` (which is backproppable in contrast to
    # `Variable.array`) is returned.
    #
    # If at least one of the arrays is a ChainerX array, all other
    # arrays need to be ChainerX arrays.
    if not inputs:
        return False, ()

    if chainerx.is_available():
        has_chainerx_array = False

        # Unwrap arrays
        arrays = []
        for x in inputs:
            if isinstance(x, variable.Variable):
                arrays.append(x._data[0])
                if x._has_chainerx_array:
                    has_chainerx_array = True
            else:  # x is ndarray
                arrays.append(x)
                if not has_chainerx_array:
                    if isinstance(x, chainerx.ndarray):
                        has_chainerx_array = True
        return has_chainerx_array, tuple(arrays)
    else:
        return False, tuple([
            x.raw_array if isinstance(x, variable.Variable) else x
            for x in inputs])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2425')" href="javascript:;">
chainer-7.2.0/chainer/functions/rnn/n_step_gru.py: 21-48
</a>
<div class="mid" id="frag2425" style="display:none"><pre>
def _extract_apply_in_data(inputs):
    if not inputs:
        return False, ()

    if chainerx.is_available():
        has_chainerx_array = False

        # Unwrap arrays
        arrays = []
        for x in inputs:
            if isinstance(x, variable.Variable):
                if x._has_chainerx_array:
                    arrays.append(x._data[0])
                    has_chainerx_array = True
                else:
                    arrays.append(x.array)
            else:  # x is ndarray
                arrays.append(x)
                if not has_chainerx_array:
                    if isinstance(x, chainerx.ndarray):
                        has_chainerx_array = True
        return has_chainerx_array, tuple(arrays)
    else:
        return False, tuple([
            x.array if isinstance(x, variable.Variable) else x
            for x in inputs])


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2443')" href="javascript:;">
chainer-7.2.0/chainer/functions/rnn/n_step_lstm.py: 20-47
</a>
<div class="mid" id="frag2443" style="display:none"><pre>
def _extract_apply_in_data(inputs):
    if not inputs:
        return False, ()

    if chainerx.is_available():
        has_chainerx_array = False

        # Unwrap arrays
        arrays = []
        for x in inputs:
            if isinstance(x, variable.Variable):
                if x._has_chainerx_array:
                    arrays.append(x._data[0])
                    has_chainerx_array = True
                else:
                    arrays.append(x.array)
            else:  # x is ndarray
                arrays.append(x)
                if not has_chainerx_array:
                    if isinstance(x, chainerx.ndarray):
                        has_chainerx_array = True
        return has_chainerx_array, tuple(arrays)
    else:
        return False, tuple([
            x.array if isinstance(x, variable.Variable) else x
            for x in inputs])


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2453')" href="javascript:;">
chainer-7.2.0/chainer/functions/rnn/n_step_rnn.py: 63-90
</a>
<div class="mid" id="frag2453" style="display:none"><pre>
def _extract_apply_in_data(inputs):
    if not inputs:
        return False, ()

    if chainerx.is_available():
        has_chainerx_array = False

        # Unwrap arrays
        arrays = []
        for x in inputs:
            if isinstance(x, variable.Variable):
                if x._has_chainerx_array:
                    arrays.append(x._data[0])
                    has_chainerx_array = True
                else:
                    arrays.append(x.array)
            else:  # x is ndarray
                arrays.append(x)
                if not has_chainerx_array:
                    if isinstance(x, chainerx.ndarray):
                        has_chainerx_array = True
        return has_chainerx_array, tuple(arrays)
    else:
        return False, tuple([
            x.array if isinstance(x, variable.Variable) else x
            for x in inputs])


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 3:</b> &nbsp; 7 fragments, nominal size 10 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag496')" href="javascript:;">
chainer-7.2.0/chainer/training/extensions/inverse_shift.py: 36-48
</a>
<div class="mid" id="frag496" style="display:none"><pre>
    def __init__(self, attr, gamma, power,
                 init=None, target=None, optimizer=None):
        self._attr = attr
        if gamma &lt; 0:
            raise ValueError('InverseShift does not support negative gamma')
        self._gamma = gamma
        self._power = power
        self._init = init
        self._target = target
        self._optimizer = optimizer
        self._t = 0
        self._last_value = None

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag527')" href="javascript:;">
chainer-7.2.0/chainer/training/extensions/exponential_shift.py: 33-43
</a>
<div class="mid" id="frag527" style="display:none"><pre>
    def __init__(self, attr, rate, init=None, target=None, optimizer=None):
        self._attr = attr
        if rate &lt; 0:
            raise ValueError('ExponentialShift does not support negative rate')
        self._rate = rate
        self._init = init
        self._target = target
        self._optimizer = optimizer
        self._t = 0
        self._last_value = None

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag604')" href="javascript:;">
chainer-7.2.0/chainer/training/extensions/polynomial_shift.py: 39-49
</a>
<div class="mid" id="frag604" style="display:none"><pre>
    def __init__(self, attr, rate, max_count, init=None, target=None,
                 optimizer=None):
        self._attr = attr
        self._rate = rate
        self._init = init
        self._target = target
        self._optimizer = optimizer
        self._t = 0
        self._max_count = max_count
        self._last_value = None

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag571')" href="javascript:;">
chainer-7.2.0/chainer/training/extensions/step_shift.py: 38-48
</a>
<div class="mid" id="frag571" style="display:none"><pre>
    def __init__(self, attr, gamma, step, init=None, target=None,
                 optimizer=None):
        self._attr = attr
        self._gamma = gamma
        self._step = step
        self._init = init
        self._target = target
        self._optimizer = optimizer
        self._t = 0
        self._last_value = None

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag908')" href="javascript:;">
chainer-7.2.0/chainer/functions/normalization/batch_renormalization.py: 21-32
</a>
<div class="mid" id="frag908" style="display:none"><pre>
    def __init__(self, eps=2e-5, mean=None, var=None, decay=0.9,
                 rmax=1, dmax=0, update_statistics=True):
        self._running_mean = mean
        self._running_var = var
        self.rmax = rmax
        self.dmax = dmax
        self.r = None
        self.update_statistics = update_statistics

        self.eps = eps
        self.decay = decay

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag944')" href="javascript:;">
chainer-7.2.0/chainer/functions/normalization/local_response_normalization.py: 125-136
</a>
<div class="mid" id="frag944" style="display:none"><pre>
    def __init__(self, n, k, alpha, beta, use_ideep,
                 scale=None, indexes=None, unit_scale=None):
        self.n = n
        self.k = k
        self.alpha = alpha
        self.beta = beta
        self._use_ideep = use_ideep

        self.scale = scale
        self.indexes = indexes
        self.unit_scale = unit_scale

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag970')" href="javascript:;">
chainer-7.2.0/chainer/functions/normalization/batch_normalization.py: 473-484
</a>
<div class="mid" id="frag970" style="display:none"><pre>
    def __init__(self, eps, expander, axis, mean, var,
                 inv_std, key_axis, impl, forward_data):
        self.eps = eps
        self.expander = expander
        self.axis = axis
        self.mean = mean
        self.var = var  # Only used in iDeep implementation
        self.inv_std = inv_std
        self.key_axis = key_axis
        self._impl = impl
        self.forward_data = forward_data

</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 4:</b> &nbsp; 3 fragments, nominal size 12 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag498')" href="javascript:;">
chainer-7.2.0/chainer/training/extensions/inverse_shift.py: 60-76
</a>
<div class="mid" id="frag498" style="display:none"><pre>
    def __call__(self, trainer):
        self._t += 1

        optimizer = self._get_optimizer(trainer)
        value = self._init * (1 + self._gamma * self._t) ** (-self._power)
        if self._target is not None:
            if self._power &lt; 0:
                # almost same as value = min(value, self._target), but this
                # line supports negative values, too
                if value / self._target &gt; 1:
                    value = self._target
            else:
                # ditto
                if value / self._target &lt; 1:
                    value = self._target
        self._update_value(optimizer, value)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag529')" href="javascript:;">
chainer-7.2.0/chainer/training/extensions/exponential_shift.py: 55-71
</a>
<div class="mid" id="frag529" style="display:none"><pre>
    def __call__(self, trainer):
        self._t += 1

        optimizer = self._get_optimizer(trainer)
        value = self._init * (self._rate ** self._t)
        if self._target is not None:
            if self._rate &gt; 1:
                # almost same as value = min(value, self._target), but this
                # line supports negative values, too
                if value / self._target &gt; 1:
                    value = self._target
            else:
                # ditto
                if value / self._target &lt; 1:
                    value = self._target
        self._update_value(optimizer, value)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag573')" href="javascript:;">
chainer-7.2.0/chainer/training/extensions/step_shift.py: 60-75
</a>
<div class="mid" id="frag573" style="display:none"><pre>
    def __call__(self, trainer):
        self._t += 1
        optimizer = self._get_optimizer(trainer)
        value = self._init * self._gamma ** numpy.floor(self._t / self._step)
        if self._target is not None:
            if self._gamma &gt; 1:
                # almost same as value = min(value, self._target), but this
                # line supports negative values, too
                if value / self._target &gt; 1:
                    value = self._target
            else:
                # ditto
                if value / self._target &lt; 1:
                    value = self._target
        self._update_value(optimizer, value)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 5:</b> &nbsp; 2 fragments, nominal size 26 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag641')" href="javascript:;">
chainer-7.2.0/chainer/training/updaters/multiprocess_parallel_updater.py: 334-362
</a>
<div class="mid" id="frag641" style="display:none"><pre>
def _gather(link, target):
    size, num = size_num_grads(link)

    ptrs = numpy.empty(num, dtype=numpy.uint64)
    dtypes = numpy.empty(num, dtype=numpy.int8)
    info = numpy.empty(num + 1, dtype=numpy.int32)
    info[0] = 0
    i = 0
    for _, param in sorted(link.namedparams()):
        if param.size == 0:
            continue
        ptrs[i] = 0  # NULL pointer
        d = getattr(param, target)
        if d is not None:
            ptrs[i] = d.data.ptr
        dtypes[i] = 0  # fp32
        if param.dtype == numpy.float16:
            dtypes[i] = 1  # fp16
        info[i + 1] = info[i] + param.size
        i += 1
    info[0] = num

    ptrs = cuda.to_gpu(ptrs)
    dtypes = cuda.to_gpu(dtypes)
    info = cuda.to_gpu(info)

    return _memcpy_gather()(ptrs, dtypes, info, size=size)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag645')" href="javascript:;">
chainer-7.2.0/chainer/training/updaters/multiprocess_parallel_updater.py: 426-458
</a>
<div class="mid" id="frag645" style="display:none"><pre>
def _scatter(link, array, target):
    size, num = size_num_grads(link)

    ptrs = numpy.zeros(num, dtype=numpy.uint64)
    dtypes = numpy.zeros(num, dtype=numpy.int8)
    info = numpy.zeros(num + 1, dtype=numpy.int32)
    info[0] = 0
    i = 0
    for _, param in sorted(link.namedparams()):
        if param.size == 0:
            continue
        ptrs[i] = 0  # NULL pointer
        d = getattr(param, target)
        if d is None:
            d = cuda.cupy.zeros(param.shape, dtype=param.dtype)
            setattr(param, target, d)
        ptrs[i] = d.data.ptr
        dtypes[i] = 0  # fp32
        if param.dtype == numpy.float16:
            dtypes[i] = 1  # fp16
        info[i + 1] = info[i] + param.size
        i += 1
    if i != num:
        raise()
    info[0] = num

    ptrs = cuda.to_gpu(ptrs)
    dtypes = cuda.to_gpu(dtypes)
    info = cuda.to_gpu(info)

    return _memcpy_scatter()(ptrs, dtypes, info, array, size=size)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 6:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 92%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag713')" href="javascript:;">
chainer-7.2.0/chainer/graph_optimizations/static_graph.py: 1424-1439
</a>
<div class="mid" id="frag713" style="display:none"><pre>
        return (xs,), inds, 0
    for x in xs:
        if isinstance(x, (list, tuple)):
            x, sub_inds, total = _flatten_args(x, )
            inds.append(('i', i, i+total, sub_inds))
            i += total
        else:
            x = [x]
            inds.append(('f', i))
            i += 1
        ys.extend([y for y in x])
    return tuple(ys), inds, i


# todo: this only outputs tuples of tuples. Any list in the original input
# will be converted to a tuple, changing the types of the input arguments
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag714')" href="javascript:;">
chainer-7.2.0/chainer/graph_optimizations/static_graph.py: 1440-1453
</a>
<div class="mid" id="frag714" style="display:none"><pre>
# to the static chain.
def _unflatten_args(xs, inds):
    ys = []
    for ind in inds:
        code = ind[0]
        if code == 's':
            return xs[0]
        elif code == 'i':
            i_start, i_end, sub_inds = ind[1:]
            y = _unflatten_args(xs[i_start:i_end], sub_inds)
        else:
            i = ind[1]
            y = xs[i]
        ys.append(y)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 7:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag815')" href="javascript:;">
chainer-7.2.0/chainer/link_hooks/timer.py: 64-75
</a>
<div class="mid" id="frag815" style="display:none"><pre>
    def _preprocess(self):
        if self.xp is numpy:
            start = _get_time()
            self._running_stack.append(start)
        else:
            assert self.xp is cuda.cupy
            start = cuda.Event()
            stop = cuda.Event()
            start.record()
            self._running_stack.append((start, stop))
        self._depth += 1

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3097')" href="javascript:;">
chainer-7.2.0/chainer/function_hooks/timer.py: 60-70
</a>
<div class="mid" id="frag3097" style="display:none"><pre>
    def _preprocess(self):
        if self.xp == numpy:
            start = _get_time()
            self._running_stack.append(start)
        else:
            start = cuda.Event()
            stop = cuda.Event()
            start.record()
            self._running_stack.append((start, stop))
        self._depth += 1

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 8:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 82%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag817')" href="javascript:;">
chainer-7.2.0/chainer/link_hooks/timer.py: 80-99
</a>
<div class="mid" id="frag817" style="display:none"><pre>
    def _postprocess(self, link):
        if self.xp is numpy:
            start = self._running_stack.pop()
            stop = _get_time()
            elapsed_time = stop - start
        else:
            assert self.xp is cuda.cupy
            start, stop = self._running_stack.pop()
            stop.record()
            stop.synchronize()
            # Note that `get_elapsed_time` returns result in milliseconds
            elapsed_time = cuda.cupy.cuda.get_elapsed_time(
                start, stop) / 1000
        self.call_history.append((link.__class__.__name__, elapsed_time))

        assert self._depth &gt; 0
        self._depth -= 1
        if self._depth == 0:
            self._total_time += elapsed_time

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3100')" href="javascript:;">
chainer-7.2.0/chainer/function_hooks/timer.py: 79-97
</a>
<div class="mid" id="frag3100" style="display:none"><pre>
    def _postprocess(self, function):
        if self.xp == numpy:
            start = self._running_stack.pop()
            stop = _get_time()
            elapsed_time = stop - start
        else:
            start, stop = self._running_stack.pop()
            stop.record()
            stop.synchronize()
            # Note that `get_elapsed_time` returns result in milliseconds
            elapsed_time = cuda.cupy.cuda.get_elapsed_time(
                start, stop) / 1000
        self.call_history.append((function._impl_name, elapsed_time))

        assert self._depth &gt; 0
        self._depth -= 1
        if self._depth == 0:
            self._total_time += elapsed_time

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 9:</b> &nbsp; 2 fragments, nominal size 27 lines, similarity 92%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag822')" href="javascript:;">
chainer-7.2.0/chainer/link_hooks/timer.py: 135-172
</a>
<div class="mid" id="frag822" style="display:none"><pre>
    def print_report(self, unit='auto', file=sys.stdout):
        """Prints a summary report of time profiling in links.

        Args:
            unit (str): Supplementary units used for computational times.
                `sec`, `ms`, `us`, `ns`, `auto`(default) and `auto_foreach`
                are supported. If `auto`, units of times are aligned to the
                largest, and if `auto_foreach`, units of times are adjusted for
                each element.
        """
        entries = [['LinkName', 'ElapsedTime', 'Occurrence']]
        auto_foreach = (unit == 'auto_foreach')
        if unit == 'auto':
            max_time = max(
                record['elapsed_time'] for record in self.summary().values())
            factor, unit = self._choose_unit(max_time)
        elif not auto_foreach:
            factor = self.table[unit]
        for link_name, record in self.summary().items():
            second = record['elapsed_time']
            if auto_foreach:
                factor, unit = self._choose_unit(second)
            elapsed_time = '%3.2f%s' % (second * factor, unit)
            occurrence = str(record['occurrence'])
            entries.append([link_name, elapsed_time, occurrence])
        entry_widths = []
        entry_widths.append(max(len(f) for f, _, _ in entries))
        entry_widths.append(max(len(e) for _, e, _ in entries))
        entry_widths.append(max(len(o) for _, _, o in entries))
        template = '  '.join('{:&gt;%d}' % w for w in entry_widths)
        for link_name, elapsed_time, occurrence in entries:
            line = template.format(link_name, elapsed_time, occurrence)
            file.write(line)
            file.write('\n')
        file.flush()

    # TODO(crcrpar): Support backward pre/post process.
    # See https://github.com/chainer/chainer/issues/5197
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3106')" href="javascript:;">
chainer-7.2.0/chainer/function_hooks/timer.py: 137-172
</a>
<div class="mid" id="frag3106" style="display:none"><pre>
    def print_report(self, unit='auto', file=sys.stdout):
        """Prints a summary report of time profiling in functions.

        Args:
            unit (str): Supplementary units used for computational times.
                `sec`, `ms`, `us`, `ns`, `auto`(default) and `auto_foreach`
                are supported. If `auto`, units of times are aligned to the
                largest, and if `auto_foreach`, units of times are adjusted for
                each element.
        """
        entries = [['FunctionName', 'ElapsedTime', 'Occurrence']]
        auto_foreach = (unit == 'auto_foreach')
        if unit == 'auto':
            max_time = max(
                record['elapsed_time'] for record in self.summary().values())
            factor, unit = self._choose_unit(max_time)
        elif unit != 'auto_foreach':
            factor = self.table[unit]
        for function_name, record in self.summary().items():
            second = record['elapsed_time']
            if auto_foreach:
                factor, unit = self._choose_unit(second)
            elapsed_time = '%3.2f%s' % (second * factor, unit)
            occurrence = str(record['occurrence'])
            entries.append([function_name, elapsed_time, occurrence])
        entry_widths = []
        entry_widths.append(max(len(f) for f, _, _ in entries))
        entry_widths.append(max(len(e) for _, e, _ in entries))
        entry_widths.append(max(len(o) for _, _, o in entries))
        template = '  '.join('{:&gt;%d}' % w for w in entry_widths)
        for function_name, elapsed_time, occurrence in entries:
            line = template.format(function_name, elapsed_time, occurrence)
            file.write(line)
            file.write('\n')
        if hasattr(file, 'flush'):
            file.flush()
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 10:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag873')" href="javascript:;">
chainer-7.2.0/chainer/link_hook.py: 10-21
</a>
<div class="mid" id="frag873" style="display:none"><pre>
    def __init__(
            self,
            link: 'chainer.link.Link',
            forward_name: str,
            args: tp.Tuple[tp.Any, ...],
            kwargs: tp.Dict[str, tp.Any]
    ) -&gt; None:
        self.link = link
        self.forward_name = forward_name
        self.args = args
        self.kwargs = kwargs

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag875')" href="javascript:;">
chainer-7.2.0/chainer/link_hook.py: 31-44
</a>
<div class="mid" id="frag875" style="display:none"><pre>
    def __init__(
            self,
            link: 'chainer.link.Link',
            forward_name: str,
            args: tp.Tuple[tp.Any, ...],
            kwargs: tp.Dict[str, tp.Any],
            out: tp.Any
    ) -&gt; None:
        self.link = link
        self.forward_name = forward_name
        self.args = args
        self.kwargs = kwargs
        self.out = out

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 11:</b> &nbsp; 8 fragments, nominal size 13 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag889')" href="javascript:;">
chainer-7.2.0/chainer/functions/normalization/group_normalization.py: 28-41
</a>
<div class="mid" id="frag889" style="display:none"><pre>
    def check_type_forward(self, in_types):
        type_check.expect(in_types.size() == 3)
        x_type, gamma_type, beta_type = in_types
        type_check.expect(
            x_type.dtype.kind == 'f',
            x_type.ndim &gt;= 2,
            gamma_type.ndim == 1,
            beta_type.ndim == 1,
            gamma_type.dtype.kind == 'f',
            gamma_type.dtype == beta_type.dtype,
            x_type.shape[1] == gamma_type.shape[0],
            gamma_type.shape == beta_type.shape,
        )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3328')" href="javascript:;">
chainer-7.2.0/chainer/links/loss/hierarchical_softmax.py: 107-122
</a>
<div class="mid" id="frag3328" style="display:none"><pre>
    def check_type_forward(self, in_types):
        type_check.expect(in_types.size() == 3)
        x_type, t_type, w_type = in_types

        type_check.expect(
            x_type.dtype.kind == 'f',
            x_type.ndim == 2,
            t_type.dtype == numpy.int32,
            t_type.ndim == 1,
            x_type.shape[0] == t_type.shape[0],
            w_type.dtype == x_type.dtype,
            w_type.ndim == 2,
            w_type.shape[0] == self.parser_size,
            w_type.shape[1] == x_type.shape[1],
        )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1401')" href="javascript:;">
chainer-7.2.0/chainer/functions/pooling/roi_max_pooling_2d.py: 74-88
</a>
<div class="mid" id="frag1401" style="display:none"><pre>
    def check_type_forward(self, in_types):
        type_check.expect(in_types.size() == 3)

        x_type, roi_type, roi_index_type = in_types
        type_check.expect(
            x_type.dtype.kind == 'f',
            x_type.ndim == 4,
            x_type.dtype == roi_type.dtype,
            roi_type.ndim == 2,
            roi_type.shape[1] == 4,
            roi_index_type.dtype == numpy.int32,
            roi_index_type.ndim == 1,
            roi_type.shape[0] == roi_index_type.shape[0],
        )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1365')" href="javascript:;">
chainer-7.2.0/chainer/functions/pooling/roi_average_pooling_2d.py: 74-88
</a>
<div class="mid" id="frag1365" style="display:none"><pre>
    def check_type_forward(self, in_types):
        type_check.expect(in_types.size() == 3)

        x_type, roi_type, roi_index_type = in_types
        type_check.expect(
            x_type.dtype.kind == 'f',
            x_type.ndim == 4,
            x_type.dtype == roi_type.dtype,
            roi_type.ndim == 2,
            roi_type.shape[1] == 4,
            roi_index_type.dtype == numpy.int32,
            roi_index_type.ndim == 1,
            roi_type.shape[0] == roi_index_type.shape[0],
        )

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1503')" href="javascript:;">
chainer-7.2.0/chainer/functions/pooling/roi_average_align_2d.py: 133-147
</a>
<div class="mid" id="frag1503" style="display:none"><pre>
    def check_type_forward(self, in_types):
        type_check.expect(in_types.size() == 3)

        x_type, roi_type, roi_index_type = in_types
        type_check.expect(
            x_type.dtype == numpy.float32,
            x_type.ndim == 4,
            roi_type.dtype == numpy.float32,
            roi_type.ndim == 2,
            roi_type.shape[1] == 4,
            roi_index_type.dtype == numpy.int32,
            roi_index_type.ndim == 1,
            roi_type.shape[0] == roi_index_type.shape[0],
        )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2197')" href="javascript:;">
chainer-7.2.0/chainer/functions/loss/negative_sampling.py: 43-57
</a>
<div class="mid" id="frag2197" style="display:none"><pre>
    def check_type_forward(self, in_types):
        type_check._argname(in_types, ('x', 't', 'W'))
        x_type, t_type, w_type = in_types

        type_check.expect(
            x_type.dtype.kind == 'f',
            x_type.ndim == 2,
            t_type.dtype == numpy.int32,
            t_type.ndim == 1,
            x_type.shape[0] == t_type.shape[0],
            w_type.dtype == x_type.dtype,
            w_type.ndim == 2,
        )

    # Avoid fp16 computation to keep the precision in reduction operations.
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1393')" href="javascript:;">
chainer-7.2.0/chainer/functions/pooling/roi_max_align_2d.py: 70-84
</a>
<div class="mid" id="frag1393" style="display:none"><pre>
        self.sampling_ratio = sampling_ratio

    def check_type_forward(self, in_types):
        type_check.expect(in_types.size() == 3)

        x_type, roi_type, roi_index_type = in_types
        type_check.expect(
            x_type.dtype == numpy.float32,
            x_type.ndim == 4,
            roi_type.dtype == numpy.float32,
            roi_type.ndim == 2,
            roi_type.shape[1] == 4,
            roi_index_type.dtype == numpy.int32,
            roi_index_type.ndim == 1,
            roi_type.shape[0] == roi_index_type.shape[0],
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10375')" href="javascript:;">
chainer-7.2.0/examples/sentiment/thin_stack.py: 10-24
</a>
<div class="mid" id="frag10375" style="display:none"><pre>
    def check_type_forward(self, in_types):
        type_check.expect(in_types.size() == 3)
        s_type, i_type, v_type = in_types
        type_check.expect(
            s_type.dtype.kind == 'f',
            i_type.dtype.kind == 'i',
            s_type.dtype == v_type.dtype,
            s_type.ndim == 3,
            i_type.ndim == 1,
            v_type.ndim == 2,
            s_type.shape[0] &gt;= i_type.shape[0],
            i_type.shape[0] == v_type.shape[0],
            s_type.shape[2] == v_type.shape[1],
        )

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 12:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag922')" href="javascript:;">
chainer-7.2.0/chainer/functions/normalization/decorrelated_batch_normalization.py: 70-80
</a>
<div class="mid" id="frag922" style="display:none"><pre>
    def check_type_forward(self, in_types):
        type_check.expect(in_types.size() == 1)
        x_type = in_types[0]
        type_check.expect(
            x_type.dtype.kind == 'f',
            x_type.shape[1] % self.groups == 0,
        )
        type_check.expect(
            x_type.ndim &gt;= 2,
        )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag929')" href="javascript:;">
chainer-7.2.0/chainer/functions/normalization/decorrelated_batch_normalization.py: 206-217
</a>
<div class="mid" id="frag929" style="display:none"><pre>
    def check_type_forward(self, in_types):
        type_check.expect(in_types.size() == 3)
        x_type, mean_type, var_type = in_types
        type_check.expect(
            x_type.dtype.kind == 'f',
            mean_type.dtype == x_type.dtype,
            var_type.dtype == x_type.dtype,
        )
        type_check.expect(
            x_type.ndim &gt;= 2,
        )

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 13:</b> &nbsp; 2 fragments, nominal size 19 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag964')" href="javascript:;">
chainer-7.2.0/chainer/functions/normalization/batch_normalization.py: 300-328
</a>
<div class="mid" id="frag964" style="display:none"><pre>
    def __init__(self, eps=2e-5, mean=None, var=None, decay=0.9, axis=None,
                 impl_selector=_impl_selector):
        self.running_mean = mean
        self.running_var = var

        # Note: cuDNN requires that eps be greater than or equals to
        # CUDNN_BN_MIN_EPSILON. Otherwise, an error will occur.
        # See CUDNN_BN_MIN_EPSILON value in cudnn.h to verify minimum allowable
        # value.
        self.eps = eps
        if chainer.should_use_cudnn('&gt;=auto'):
            if eps &lt; libcudnn.CUDNN_BN_MIN_EPSILON:
                raise RuntimeError(
                    'cuDNN does not allow an eps value '
                    'less than {}.'.format(libcudnn.CUDNN_BN_MIN_EPSILON))
        self.decay = decay
        if isinstance(axis, collections_abc.Sequence):
            for i in range(1, len(axis)):
                if axis[i - 1] &gt;= axis[i]:
                    msg = 'numbers in axis must be sorted in ascending order'
                    raise RuntimeError(msg)
        elif isinstance(axis, six.integer_types):
            axis = axis,
        elif axis is not None:
            raise RuntimeError('axis must be int, tuple of int or None')
        self.axis = axis

        self._impl_selector = impl_selector

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag974')" href="javascript:;">
chainer-7.2.0/chainer/functions/normalization/batch_normalization.py: 560-581
</a>
<div class="mid" id="frag974" style="display:none"><pre>
    def __init__(self, eps=2e-5, axis=None):
        # Note: cuDNN requires that eps be greater than or equals to
        # CUDNN_BN_MIN_EPSILON. Otherwise, an error will occur.
        # See CUDNN_BN_MIN_EPSILON value in cudnn.h to verify minimum allowable
        # value.
        self.eps = eps
        if chainer.should_use_cudnn('&gt;=auto'):
            if eps &lt; libcudnn.CUDNN_BN_MIN_EPSILON:
                raise RuntimeError(
                    'cuDNN does not allow an eps value '
                    'less than {}.'.format(libcudnn.CUDNN_BN_MIN_EPSILON))
        if isinstance(axis, collections_abc.Sequence):
            for i in range(1, len(axis)):
                if axis[i - 1] &gt;= axis[i]:
                    msg = 'numbers in axis must be sorted in ascending order'
                    raise RuntimeError(msg)
        elif isinstance(axis, six.integer_types):
            axis = axis,
        elif axis is not None:
            raise RuntimeError('axis must be int, tuple of int or None')
        self.axis = axis

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 14:</b> &nbsp; 2 fragments, nominal size 25 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag965')" href="javascript:;">
chainer-7.2.0/chainer/functions/normalization/batch_normalization.py: 329-352
</a>
<div class="mid" id="frag965" style="display:none"><pre>
    def check_type_forward(self, in_types):
        type_check.expect(in_types.size() == 3)
        x_type, gamma_type, beta_type = in_types
        type_check.expect(
            x_type.dtype.kind == 'f',
            gamma_type.dtype.kind == 'f',
            gamma_type.dtype == beta_type.dtype,
            gamma_type.shape == beta_type.shape,
        )
        _x_ndim = type_check.eval(x_type.ndim)
        _gamma_ndim = type_check.eval(gamma_type.ndim)
        _axis = _compute_axis(_x_ndim, _gamma_ndim, self.axis)
        type_check.expect(
            x_type.ndim &gt;= len(_axis),
        )
        _key_axis = _compute_key_axis(_x_ndim, _gamma_ndim, _axis)
        type_check.expect(
            gamma_type.ndim == len(_key_axis),
        )
        for i in range(len(_key_axis)):
            type_check.expect(
                x_type.shape[_key_axis[i]] == gamma_type.shape[i],
            )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag975')" href="javascript:;">
chainer-7.2.0/chainer/functions/normalization/batch_normalization.py: 582-610
</a>
<div class="mid" id="frag975" style="display:none"><pre>
    def check_type_forward(self, in_types):
        type_check.expect(in_types.size() == 5)
        x_type, gamma_type, beta_type, mean_type, var_type = in_types
        type_check.expect(
            x_type.dtype.kind == 'f',
            # TODO(beam2d): Check shape
            gamma_type.dtype.kind == 'f',
            beta_type.dtype == gamma_type.dtype,
            mean_type.dtype == gamma_type.dtype,
            var_type.dtype == gamma_type.dtype,
            beta_type.shape == gamma_type.shape,
            mean_type.shape == gamma_type.shape,
            var_type.shape == gamma_type.shape,
        )
        _x_ndim = type_check.eval(x_type.ndim)
        _gamma_ndim = type_check.eval(gamma_type.ndim)
        _axis = _compute_axis(_x_ndim, _gamma_ndim, self.axis)
        type_check.expect(
            x_type.ndim &gt;= len(_axis),
        )
        _key_axis = _compute_key_axis(_x_ndim, _gamma_ndim, _axis)
        type_check.expect(
            gamma_type.ndim == len(_key_axis),
        )
        for i in range(len(_key_axis)):
            type_check.expect(
                x_type.shape[_key_axis[i]] == gamma_type.shape[i],
            )

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 15:</b> &nbsp; 2 fragments, nominal size 20 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1037')" href="javascript:;">
chainer-7.2.0/chainer/functions/connection/deconvolution_nd.py: 213-235
</a>
<div class="mid" id="frag1037" style="display:none"><pre>

    def backward(self, indexes, grad_outputs):
        x, W = self.get_retained_inputs()
        gy, = grad_outputs

        ret = []
        if 0 in indexes:
            gx = chainer.functions.convolution_nd(
                gy, W, stride=self.stride, pad=self.pad,
                cover_all=self.cover_all, dilate=self.dilate,
                groups=self.groups)
            ret.append(gx)
        if 1 in indexes:
            gW, = convolution_nd.ConvolutionNDGradW(self).apply((gy, x))
            ret.append(gW)
        if 2 in indexes:
            axis = (0,) + tuple(moves.range(2, gy.ndim))
            gb = chainer.functions.sum(gy, axis=axis)
            if gb.dtype != self.inputs[2].dtype:
                gb = chainer.functions.cast(gb, self.inputs[2].dtype)
            ret.append(gb)

        return ret
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1081')" href="javascript:;">
chainer-7.2.0/chainer/functions/connection/convolution_nd.py: 192-215
</a>
<div class="mid" id="frag1081" style="display:none"><pre>
    def backward(self, indexes, grad_outputs):
        x, W = self.get_retained_inputs()
        gy, = grad_outputs

        ret = []
        if 0 in indexes:
            x_shape = x.shape[2:]
            gx = chainer.functions.deconvolution_nd(
                gy, W, stride=self.stride, pad=self.pad, outsize=x_shape,
                dilate=self.dilate, groups=self.groups)
            ret.append(gx)
        if 1 in indexes:
            gW, = ConvolutionNDGradW(self).apply((x, gy))
            ret.append(gW)
        if 2 in indexes:
            axis = (0,) + tuple(moves.range(2, gy.ndim))
            gb = chainer.functions.sum(gy, axis=axis)
            if gb.dtype != self.inputs[2].dtype:
                gb = chainer.functions.cast(gb, self.inputs[2].dtype)
            ret.append(gb)

        return ret


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 16:</b> &nbsp; 4 fragments, nominal size 19 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1048')" href="javascript:;">
chainer-7.2.0/chainer/functions/connection/convolution_2d.py: 62-83
</a>
<div class="mid" id="frag1048" style="display:none"><pre>
    def check_type_forward(self, in_types):
        n_in = in_types.size()
        type_check.expect(2 &lt;= n_in, n_in &lt;= 3)

        x_type = in_types[0]
        w_type = in_types[1]
        type_check.expect(
            x_type.dtype.kind == 'f',
            w_type.dtype.kind == 'f',
            x_type.ndim == 4,
            w_type.ndim == 4,
            x_type.shape[1] == w_type.shape[1] * self.groups,
        )

        if type_check.eval(n_in) == 3:
            b_type = in_types[2]
            type_check.expect(
                b_type.dtype == x_type.dtype,
                b_type.ndim == 1,
                b_type.shape[0] == w_type.shape[0],
            )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1073')" href="javascript:;">
chainer-7.2.0/chainer/functions/connection/convolution_nd.py: 28-50
</a>
<div class="mid" id="frag1073" style="display:none"><pre>
    def check_type_forward(self, in_types):
        n_in = in_types.size()
        type_check.expect(2 &lt;= n_in, n_in &lt;= 3)

        x_type = in_types[0]
        w_type = in_types[1]
        type_check.expect(
            x_type.dtype.kind == 'f',
            w_type.dtype.kind == 'f',
            x_type.ndim == self.ndim + 2,
            w_type.ndim == self.ndim + 2,
            # Need to consider the case that group count &gt; 1.
            # x_type.shape[1] == w_type.shape[1],
        )

        if type_check.eval(n_in) == 3:
            b_type = in_types[2]
            type_check.expect(
                b_type.dtype.kind == 'f',
                b_type.ndim == 1,
                b_type.shape[0] == w_type.shape[0],
            )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1110')" href="javascript:;">
chainer-7.2.0/chainer/functions/connection/local_convolution_2d.py: 21-40
</a>
<div class="mid" id="frag1110" style="display:none"><pre>
    def check_type_forward(self, in_types):
        n_in = in_types.size()
        type_check.expect(2 &lt;= n_in, n_in &lt;= 3)
        x_type, w_type = in_types[:2]

        type_check.expect(
            x_type.dtype.kind == 'f',
            w_type.dtype.kind == 'f',
            x_type.ndim == 4,
            w_type.ndim == 6,
            x_type.shape[1] == w_type.shape[3],
        )
        if type_check.eval(n_in) == 3:
            b_type = in_types[2]
            type_check.expect(
                b_type.dtype == x_type.dtype,
                b_type.ndim == 3,
                b_type.shape == w_type.shape[:3]
            )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1093')" href="javascript:;">
chainer-7.2.0/chainer/functions/connection/linear.py: 18-39
</a>
<div class="mid" id="frag1093" style="display:none"><pre>
    def check_type_forward(self, in_types):
        n_in = in_types.size()
        type_check.expect(2 &lt;= n_in, n_in &lt;= 3)
        x_type, w_type = in_types[:2]
        type_check._argname((x_type, w_type), ('x', 'W'))

        type_check.expect(
            x_type.dtype.kind == 'f',
            w_type.dtype.kind == 'f',
            x_type.ndim == 2,
            w_type.ndim == 2,
            x_type.shape[1] == w_type.shape[1],
        )
        if type_check.eval(n_in) == 3:
            b_type = in_types[2]
            type_check._argname((b_type,), ('b',))
            type_check.expect(
                b_type.dtype == x_type.dtype,
                b_type.ndim == 1,
                b_type.shape[0] == w_type.shape[0],
            )

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 17:</b> &nbsp; 2 fragments, nominal size 20 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1055')" href="javascript:;">
chainer-7.2.0/chainer/functions/connection/convolution_2d.py: 176-197
</a>
<div class="mid" id="frag1055" style="display:none"><pre>
    def _forward_ideep(self, x, W, b):
        out_c, input_c, kh, kw = W.shape
        n, c, h, w = x.shape

        out_h, out_w = self._get_out_size(x.shape, W.shape)
        pd = (self.sy * (out_h - 1)
              + (kh + (kh - 1) * (self.dy - 1)) - h - self.ph)
        pr = (self.sx * (out_w - 1)
              + (kw + (kw - 1) * (self.dx - 1)) - w - self.pw)
        param = intel64.ideep.convolution2DParam(
            (n, out_c, out_h, out_w),
            self.dy, self.dx,
            self.sy, self.sx,
            self.ph, self.pw,
            pd, pr)
        y = intel64.ideep.convolution2D.Forward(
            intel64.ideep.array(x),
            intel64.ideep.array(W),
            intel64.ideep.array(b) if b is not None else None,
            param)
        return y,

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1065')" href="javascript:;">
chainer-7.2.0/chainer/functions/connection/convolution_2d.py: 384-405
</a>
<div class="mid" id="frag1065" style="display:none"><pre>
    def _forward_ideep(self, x, gy):
        n, input_c, h, w = x.shape
        n, out_c, out_h, out_w = gy.shape
        pd = (self.sy * (out_h - 1)
              + (self.kh + (self.kh - 1) * (self.dy - 1))
              - h - self.ph)
        pr = (self.sx * (out_w - 1)
              + (self.kw + (self.kw - 1) * (self.dx - 1))
              - w - self.pw)

        param = intel64.ideep.convolution2DParam(
            (out_c, input_c, self.kh, self.kw),
            self.dy, self.dx,
            self.sy, self.sx,
            self.ph, self.pw,
            pd, pr)
        gW = intel64.ideep.convolution2D.BackwardWeights(
            intel64.ideep.array(x),
            intel64.ideep.array(gy),
            param)
        return gW,

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 18:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1058')" href="javascript:;">
chainer-7.2.0/chainer/functions/connection/convolution_2d.py: 253-283
</a>
<div class="mid" id="frag1058" style="display:none"><pre>
    def _forward_grouped_convolution(self, x, W, b):
        # G: group count
        # N: batch size
        # kH, kW: kernel height, kernel width
        # iC, iH, iW: input channels, input height, input width
        # oC, oH, oW: output channels, output height, output width
        G = self.groups
        N, iC, iH, iW = x.shape
        oC, _, kH, kW = W.shape  # _ == iCg
        iCg = iC // G
        oCg = oC // G

        # (N, iC, kW, kW, oH, oW)
        x = conv.im2col(x, kH, kW, self.sy, self.sx, self.ph, self.pw,
                        cover_all=self.cover_all, dy=self.dy, dx=self.dx)
        oH, oW = x.shape[-2:]

        x = x.transpose(1, 2, 3, 0, 4, 5)  # (iC, kH, kW, N, oH, oW)
        x = x.reshape(G, iCg * kH * kW, N * oH * oW)

        W = W.reshape(G, oCg, iCg * kH * kW)

        # (G, oCg, N*oH*oW) = (G, oCg, iCg*kH*kW) @ (G, iCg*kH*kW, N*oH*oW)
        y = _matmul(W, x).astype(x.dtype, copy=False)
        y = y.reshape(oC, N, oH, oW)
        y = y.transpose(1, 0, 2, 3)  # (N, oC, oH, oW)
        if b is not None:
            y += b.reshape(1, b.size, 1, 1)

        return y,

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1068')" href="javascript:;">
chainer-7.2.0/chainer/functions/connection/convolution_2d.py: 438-468
</a>
<div class="mid" id="frag1068" style="display:none"><pre>
    def _forward_grouped_convolution(self, x, gy):
        # G: group count
        # N: batch size
        # kH, kW: kernel height, kernel width
        # iC, iH, iW: input channels, input height, input width
        # oC, oH, oW: output channels, output height, output width
        G = self.groups
        N, iC, iH, iW = x.shape
        _, oC, oH, oW = gy.shape  # _ == N
        kH = self.kh
        kW = self.kw
        iCg = iC // G
        oCg = oC // G

        # (N, iC, kH, kW, oH, oW)
        x = conv.im2col(x, kH, kW, self.sy, self.sx, self.ph, self.pw,
                        cover_all=self.cover_all, dy=self.dy, dx=self.dx)

        x = x.transpose(1, 2, 3, 0, 4, 5)  # (iC, kH, kW, N, oH, oW)
        x = x.reshape(G, iCg * kH * kW, N * oH * oW)
        x = x.transpose(0, 2, 1)  # (G, N*oH*oW, iCg*kH*kW)

        gy = gy.transpose(1, 0, 2, 3)  # (oC, N, oH, oW)
        gy = gy.reshape(G, oCg, N * oH * oW)

        # (G, oCg, iCg*kH*kW) = (G, oCg, N*oH*oW) @ (G, N*oH*oW, iCg*kH*kW)
        gW = _matmul(gy, x).astype(self.W_dtype, copy=False)
        gW = gW.reshape(oC, iCg, kH, kW)

        return gW,

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 19:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1075')" href="javascript:;">
chainer-7.2.0/chainer/functions/connection/convolution_nd.py: 69-81
</a>
<div class="mid" id="frag1075" style="display:none"><pre>
    def _use_cudnn(self, x, W):
        if cuda._cudnn_version &lt; 6000 and any(d != 1 for d in self.dilate):
            # cuDNN &lt; 6.0 does not support dilated convolutions
            return False
        if cuda._cudnn_version &lt; 7000 and 1 &lt; self.groups:
            # cuDNN &lt; 7.0 does not support grouped convolutions
            return False
        return (
            chainer.should_use_cudnn('&gt;=auto')
            and not self.cover_all
            and x.dtype == W.dtype
            and self.ndim &gt; 1)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1083')" href="javascript:;">
chainer-7.2.0/chainer/functions/connection/convolution_nd.py: 229-242
</a>
<div class="mid" id="frag1083" style="display:none"><pre>
    def _use_cudnn(self, x, gy):
        if cuda._cudnn_version &lt; 6000 and any(d != 1 for d in self.dilate):
            # cuDNN &lt; 6.0 does not support dilated convolutions
            return False
        if cuda._cudnn_version &lt; 7000 and 1 &lt; self.groups:
            # cuDNN &lt; 7.0 does not support grouped convolutions
            return False
        return (
            chainer.should_use_cudnn('&gt;=auto')
            and not self.cover_all
            and x.dtype == self.W_dtype
            and gy.dtype == self.W_dtype
            and self.ndim &gt; 1)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 20:</b> &nbsp; 4 fragments, nominal size 11 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1082')" href="javascript:;">
chainer-7.2.0/chainer/functions/connection/convolution_nd.py: 218-228
</a>
<div class="mid" id="frag1082" style="display:none"><pre>
    def __init__(self, convnd):
        W_node = convnd.inputs[1]
        self.ndim = convnd.ndim
        self.ksize = W_node.shape[2:]
        self.stride = convnd.stride
        self.pad = convnd.pad
        self.cover_all = convnd.cover_all
        self.dilate = convnd.dilate
        self.groups = convnd.groups
        self.W_dtype = W_node.dtype

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1424')" href="javascript:;">
chainer-7.2.0/chainer/functions/pooling/unpooling_2d.py: 91-102
</a>
<div class="mid" id="frag1424" style="display:none"><pre>
    def __init__(self, unpooling2d):
        self.kh = unpooling2d.kh
        self.kw = unpooling2d.kw
        self.sy = unpooling2d.sy
        self.sx = unpooling2d.sx
        self.ph = unpooling2d.ph
        self.pw = unpooling2d.pw
        self.outh = unpooling2d.outh
        self.outw = unpooling2d.outw
        self.cover_all = unpooling2d.cover_all
        self._use_int_scale_forward = unpooling2d._use_int_scale_forward

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1386')" href="javascript:;">
chainer-7.2.0/chainer/functions/pooling/upsampling_2d.py: 109-121
</a>
<div class="mid" id="frag1386" style="display:none"><pre>
    def __init__(self, upsampling2d):
        self.kh = upsampling2d.kh
        self.kw = upsampling2d.kw
        self.sy = upsampling2d.sy
        self.sx = upsampling2d.sx
        self.ph = upsampling2d.ph
        self.pw = upsampling2d.pw
        self.outh = upsampling2d.outh
        self.outw = upsampling2d.outw
        self.cover_all = upsampling2d.cover_all
        self.indexes = upsampling2d.indexes
        self._in_dtype = upsampling2d._in_dtype

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1412')" href="javascript:;">
chainer-7.2.0/chainer/functions/pooling/average_pooling_2d.py: 105-117
</a>
<div class="mid" id="frag1412" style="display:none"><pre>
    def __init__(self, apool2d):
        self.kh = apool2d.kh
        self.kw = apool2d.kw
        self.sy = apool2d.sy
        self.sx = apool2d.sx
        self.ph = apool2d.ph
        self.pw = apool2d.pw
        self._used_cudnn = apool2d._used_cudnn
        if not self._used_cudnn:
            self._in_shape = apool2d._in_shape
            self._in_dtype = apool2d._in_dtype
        self.apool2d = apool2d

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 21:</b> &nbsp; 3 fragments, nominal size 13 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1099')" href="javascript:;">
chainer-7.2.0/chainer/functions/connection/linear.py: 137-154
</a>
<div class="mid" id="frag1099" style="display:none"><pre>
    def backward(self, indexes, grad_outputs):
        x, W = self.get_retained_inputs()
        gy, = grad_outputs
        ret = []
        with chainer.using_config('use_ideep', self._config_use_ideep):
            if 0 in indexes:
                gx, = LinearGradData().apply((W, gy))
                ret.append(chainer.functions.cast(gx, x.dtype))
            if 1 in indexes:
                gW, = LinearGradWeight(W.dtype).apply((x, gy))
                ret.append(chainer.functions.cast(gW, W.dtype))
            if 2 in indexes:
                gb = chainer.functions.sum(gy, axis=0)
                ret.append(gb)

        return ret


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1106')" href="javascript:;">
chainer-7.2.0/chainer/functions/connection/linear.py: 236-250
</a>
<div class="mid" id="frag1106" style="display:none"><pre>
    def backward(self, indexes, grad_outputs):
        x, gy = self.get_retained_inputs()
        ggW, = grad_outputs

        ret = []
        with chainer.using_config('use_ideep', self._config_use_ideep):
            if 0 in indexes:
                gx, = LinearGradData().apply((ggW, gy))
                ret.append(chainer.functions.cast(gx, x.dtype))
            if 1 in indexes:
                ggy = linear(x, ggW)
                ret.append(chainer.functions.cast(ggy, gy.dtype))
        return ret


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1102')" href="javascript:;">
chainer-7.2.0/chainer/functions/connection/linear.py: 186-200
</a>
<div class="mid" id="frag1102" style="display:none"><pre>
    def backward(self, indexes, grad_outputs):
        W, gy = self.get_retained_inputs()
        ggx, = grad_outputs

        ret = []
        with chainer.using_config('use_ideep', self._config_use_ideep):
            if 0 in indexes:
                gw, = LinearGradWeight(W.dtype).apply((ggx, gy))
                ret.append(chainer.functions.cast(gw, W.dtype))
            if 1 in indexes:
                ggy = linear(ggx, W)
                ret.append(chainer.functions.cast(ggy, gy.dtype))
        return ret


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 22:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 85%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1100')" href="javascript:;">
chainer-7.2.0/chainer/functions/connection/linear.py: 159-177
</a>
<div class="mid" id="frag1100" style="display:none"><pre>
    def forward(self, inputs):
        self._config_use_ideep = chainer.config.use_ideep
        if (intel64.should_use_ideep('&gt;=auto')
                and intel64.inputs_all_ready(inputs)):
            # iDeep implementation
            return self._forward_ideep(inputs)

        # Generic implementation
        self.retain_inputs((0, 1))
        W, gy = inputs

        if (isinstance(gy, numpy.ndarray) and
                not (gy.flags.c_contiguous or gy.flags.f_contiguous) and
                1 in gy.shape):
            gy = numpy.ascontiguousarray(gy)

        gx = gy.dot(W).astype(gy.dtype, copy=False)
        return gx,

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1104')" href="javascript:;">
chainer-7.2.0/chainer/functions/connection/linear.py: 208-227
</a>
<div class="mid" id="frag1104" style="display:none"><pre>
    def forward(self, inputs):
        self._config_use_ideep = chainer.config.use_ideep
        if (intel64.should_use_ideep('&gt;=auto')
                and self._w_dtype == numpy.float32
                and intel64.inputs_all_ready(inputs)):
            # iDeep implementation
            return self._forward_ideep(inputs)

        # Generic implementation
        self.retain_inputs((0, 1))
        x, gy = inputs

        if (isinstance(gy, numpy.ndarray) and
                not (gy.flags.c_contiguous or gy.flags.f_contiguous) and
                1 in gy.shape):
            gy = numpy.ascontiguousarray(gy)

        gW = gy.T.dot(x).astype(self._w_dtype, copy=False)
        return gW,

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 23:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1135')" href="javascript:;">
chainer-7.2.0/chainer/functions/array/spatial_transformer_grid.py: 57-74
</a>
<div class="mid" id="frag1135" style="display:none"><pre>

    def _forward(self, inputs):
        theta, = inputs
        H, W = self.output_shape
        B, _, _ = theta.shape
        xp = backend.get_array_module(theta)

        ys, xs = xp.meshgrid(
            xp.linspace(-1, 1, H, dtype=theta.dtype),
            xp.linspace(-1, 1, W, dtype=theta.dtype), indexing='ij',
            copy=False
        )

        coords = xp.concatenate(
            [xs[None], ys[None], xp.ones((1, H, W), dtype=theta.dtype)],
            axis=0)
        grid = theta.dot(coords.reshape(3, H * W)).reshape(B, 2, H, W)
        return grid,
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1138')" href="javascript:;">
chainer-7.2.0/chainer/functions/array/spatial_transformer_grid.py: 92-112
</a>
<div class="mid" id="frag1138" style="display:none"><pre>

    def _backward(self, inputs, grad_outputs):
        theta, = inputs
        ggrid, = grad_outputs
        H, W = self.output_shape
        B, _, _ = theta.shape
        xp = backend.get_array_module(theta)

        ys, xs = xp.meshgrid(
            xp.linspace(-1, 1, H, dtype=theta.dtype),
            xp.linspace(-1, 1, W, dtype=theta.dtype), indexing='ij',
            copy=False
        )

        coords = xp.concatenate(
            [xs[None], ys[None], xp.ones((1, H, W), dtype=theta.dtype)],
            axis=0)
        coords_T = coords.reshape(3, H * W).transpose(1, 0)
        ggrid = ggrid.reshape(B, 2, H * W)
        gtheta = ggrid.dot(coords_T).reshape(B, 2, 3)
        return gtheta,
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 24:</b> &nbsp; 3 fragments, nominal size 16 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1160')" href="javascript:;">
chainer-7.2.0/chainer/functions/array/hstack.py: 14-31
</a>
<div class="mid" id="frag1160" style="display:none"><pre>
    def check_type_forward(self, in_types):
        type_check.expect(in_types.size() &gt; 0)
        type_check._argname((in_types[0],), ('x0',))

        ndim = type_check.eval(in_types[0].ndim)
        for i in six.moves.range(1, type_check.eval(in_types.size())):
            type_check._argname((in_types[i],), ('x{}'.format(i),))
            type_check.expect(
                in_types[0].dtype == in_types[i].dtype,
                in_types[0].ndim == in_types[i].ndim,
            )
            if ndim &lt;= 1:
                continue
            for d in six.moves.range(0, ndim):
                if d == 1:
                    continue
                type_check.expect(in_types[0].shape[d] == in_types[i].shape[d])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1179')" href="javascript:;">
chainer-7.2.0/chainer/functions/array/dstack.py: 14-32
</a>
<div class="mid" id="frag1179" style="display:none"><pre>
    def check_type_forward(self, in_types):
        type_check.expect(in_types.size() &gt; 0)
        type_check._argname((in_types[0],), ('x0',))

        ndim = type_check.eval(in_types[0].ndim)
        for i in six.moves.range(1, type_check.eval(in_types.size())):
            type_check._argname((in_types[i],), ('x{}'.format(i),))
            type_check.expect(
                in_types[0].dtype == in_types[i].dtype,
                in_types[0].ndim == in_types[i].ndim,
            )
            if ndim &lt;= 2:
                type_check.expect(in_types[0].shape == in_types[i].shape)
                continue
            for d in six.moves.range(0, ndim):
                if d == 2:
                    continue
                type_check.expect(in_types[0].shape[d] == in_types[i].shape[d])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1305')" href="javascript:;">
chainer-7.2.0/chainer/functions/array/vstack.py: 14-28
</a>
<div class="mid" id="frag1305" style="display:none"><pre>
    def check_type_forward(self, in_types):
        type_check.expect(in_types.size() &gt; 0)

        ndim = type_check.eval(in_types[0].ndim)
        for i in six.moves.range(1, type_check.eval(in_types.size())):
            type_check.expect(
                in_types[0].dtype == in_types[i].dtype,
                in_types[0].ndim == in_types[i].ndim,
            )
            if ndim &lt;= 1:
                type_check.expect(in_types[0].shape == in_types[i].shape)
                continue
            for d in six.moves.range(1, ndim):
                type_check.expect(in_types[0].shape[d] == in_types[i].shape[d])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 25:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1181')" href="javascript:;">
chainer-7.2.0/chainer/functions/array/dstack.py: 37-52
</a>
<div class="mid" id="frag1181" style="display:none"><pre>
    def backward(self, indexes, grad_outputs):
        gy, = grad_outputs
        ndim = len(self.inputs[0].shape)
        if len(self.inputs) == 1:
            if ndim &lt;= 2:
                return gy.reshape(self.inputs[0].shape),
            return gy,

        if ndim &lt;= 2:
            gxs = chainer.functions.split_axis(gy, len(self.inputs), axis=2)
            return [gx.reshape(self.inputs[0].shape) for gx in gxs]

        sizes = numpy.array([x.shape[2] for x in self.inputs[:-1]]).cumsum()
        return chainer.functions.split_axis(gy, sizes, axis=2)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1307')" href="javascript:;">
chainer-7.2.0/chainer/functions/array/vstack.py: 33-48
</a>
<div class="mid" id="frag1307" style="display:none"><pre>
    def backward(self, indexes, grad_outputs):
        gy, = grad_outputs
        ndim = len(self.inputs[0].shape)
        if len(self.inputs) == 1:
            if ndim &lt;= 1:
                return gy.reshape(self.inputs[0].shape),
            return gy,

        if ndim &lt;= 1:
            gxs = chainer.functions.split_axis(gy, len(self.inputs), 0)
            return [gx.reshape(self.inputs[0].shape) for gx in gxs]

        sizes = numpy.array([x.shape[0] for x in self.inputs[:-1]]).cumsum()
        return chainer.functions.split_axis(gy, sizes, 0)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 26:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1223')" href="javascript:;">
chainer-7.2.0/chainer/functions/array/scatter_add.py: 13-30
</a>
<div class="mid" id="frag1223" style="display:none"><pre>
    def __init__(self, slices):
        if isinstance(slices, list):
            if all([isinstance(s, int) for s in slices]):
                slices = slices,
            slices = tuple(slices)
        elif not isinstance(slices, tuple):
            slices = slices,

        if chainer.is_debug():
            n_ellipses = 0
            for s in slices:
                if s is Ellipsis:
                    n_ellipses += 1
            if n_ellipses &gt; 1:
                raise ValueError('Only one Ellipsis is allowed')

        self.slices = slices

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1316')" href="javascript:;">
chainer-7.2.0/chainer/functions/array/get_item.py: 19-36
</a>
<div class="mid" id="frag1316" style="display:none"><pre>

    def __init__(self, slices):
        if isinstance(slices, list):
            if all([isinstance(s, int) for s in slices]):
                slices = slices,
            slices = tuple(slices)
        elif not isinstance(slices, tuple):
            slices = slices,

        if chainer.is_debug():
            n_ellipses = 0
            for s in slices:
                if s is Ellipsis:
                    n_ellipses += 1
            if n_ellipses &gt; 1:
                raise ValueError('Only one Ellipsis is allowed')

        self.slices = slices
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 27:</b> &nbsp; 6 fragments, nominal size 13 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1299')" href="javascript:;">
chainer-7.2.0/chainer/functions/array/squeeze.py: 22-33
</a>
<div class="mid" id="frag1299" style="display:none"><pre>
    def __init__(self, axis=None):

        if axis is None:
            self.axis = None
        elif isinstance(axis, six.integer_types):
            self.axis = (axis,)
        elif isinstance(axis, tuple) and all(
                isinstance(x, six.integer_types) for x in axis):
            self.axis = axis
        else:
            raise TypeError('axis must be None, int or tuple of ints')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1710')" href="javascript:;">
chainer-7.2.0/chainer/functions/math/logsumexp.py: 13-26
</a>
<div class="mid" id="frag1710" style="display:none"><pre>
    def __init__(self, axis=None):
        if axis is None:
            self.axis = None
        elif isinstance(axis, six.integer_types):
            self.axis = (axis,)
        elif isinstance(axis, tuple) and all(
                isinstance(a, six.integer_types) for a in axis):
            if len(set(axis)) != len(axis):
                raise ValueError('duplicate value in axis: ({})'.format(
                    ', '.join(map(str, axis))))
            self.axis = axis
        else:
            raise TypeError('None, int or tuple of int are required')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1636')" href="javascript:;">
chainer-7.2.0/chainer/functions/math/average.py: 15-30
</a>
<div class="mid" id="frag1636" style="display:none"><pre>
    def __init__(self, axis, keepdims):
        if axis is None:
            self.axis = None
        elif isinstance(axis, six.integer_types):
            self.axis = (axis,)
        elif isinstance(axis, tuple) and all(
                isinstance(a, six.integer_types) for a in axis):
            if len(set(axis)) != len(axis):
                raise ValueError('duplicate value in axis: ({})'.format(
                    ', '.join(map(str, axis))))
            self.axis = axis
        else:
            raise TypeError('None, int or tuple of int are required')

        self.keepdims = keepdims

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1734')" href="javascript:;">
chainer-7.2.0/chainer/functions/math/prod.py: 15-30
</a>
<div class="mid" id="frag1734" style="display:none"><pre>
    def __init__(self, axis=None, keepdims=False):
        if axis is None:
            self.axis = None
        elif isinstance(axis, six.integer_types):
            self.axis = (axis,)
        elif isinstance(axis, tuple) and all(
                isinstance(a, six.integer_types) for a in axis):
            if len(set(axis)) != len(axis):
                raise ValueError('duplicate value in axis: ({})'.format(
                    ', '.join(map(str, axis))))
            self.axis = axis
        else:
            raise TypeError('None, int or tuple of int are required')

        self.keepdims = keepdims

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1924')" href="javascript:;">
chainer-7.2.0/chainer/functions/math/minmax.py: 15-29
</a>
<div class="mid" id="frag1924" style="display:none"><pre>
    def __init__(self, axis=None, keepdims=False):
        self.keepdims = keepdims
        if axis is None:
            self.axis = None
        elif isinstance(axis, six.integer_types):
            self.axis = (axis,)
        elif isinstance(axis, tuple) and all(
                isinstance(a, six.integer_types) for a in axis):
            if len(set(axis)) != len(axis):
                raise ValueError('duplicate value in axis: ({})'.format(
                    ', '.join(map(str, axis))))
            self.axis = axis
        else:
            raise TypeError('None, int or tuple of int are required')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1895')" href="javascript:;">
chainer-7.2.0/chainer/functions/math/sum.py: 17-32
</a>
<div class="mid" id="frag1895" style="display:none"><pre>
    def __init__(self, axis=None, keepdims=False):
        if axis is None:
            self.axis = None
        elif isinstance(axis, six.integer_types):
            self.axis = (axis,)
        elif isinstance(axis, tuple) and all(
                isinstance(a, six.integer_types) for a in axis):
            if len(set(axis)) != len(axis):
                raise ValueError('duplicate value in axis: ({})'.format(
                    ', '.join(map(str, axis))))
            self.axis = axis
        else:
            raise TypeError('None, int or tuple of int are required')

        self.keepdims = keepdims

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 28:</b> &nbsp; 2 fragments, nominal size 19 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1364')" href="javascript:;">
chainer-7.2.0/chainer/functions/pooling/roi_average_pooling_2d.py: 53-73
</a>
<div class="mid" id="frag1364" style="display:none"><pre>
    def __init__(self, outsize, spatial_scale):
        outh, outw = _pair(outsize)
        if not (isinstance(outh, numbers.Integral) and outh &gt; 0):
            raise TypeError(
                'outsize[0] must be positive integer: {}, {}'
                .format(type(outh), outh))
        if not (isinstance(outw, numbers.Integral) and outw &gt; 0):
            raise TypeError(
                'outsize[1] must be positive integer: {}, {}'
                .format(type(outw), outw))
        if isinstance(spatial_scale, numbers.Integral):
            spatial_scale = float(spatial_scale)
        if not (isinstance(spatial_scale, numbers.Real) and
                spatial_scale &gt; 0):
            raise TypeError(
                'spatial_scale must be a positive float number: {}, {}'
                .format(type(spatial_scale), spatial_scale))

        self.outh, self.outw = outh, outw
        self.spatial_scale = spatial_scale

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1400')" href="javascript:;">
chainer-7.2.0/chainer/functions/pooling/roi_max_pooling_2d.py: 54-73
</a>
<div class="mid" id="frag1400" style="display:none"><pre>
    def __init__(self, outsize, spatial_scale):
        outh, outw = _pair(outsize)
        if not (isinstance(outh, numbers.Integral) and outh &gt; 0):
            raise TypeError(
                'outsize[0] must be positive integer: {}, {}'
                .format(type(outh), outh))
        if not (isinstance(outw, numbers.Integral) and outw &gt; 0):
            raise TypeError(
                'outsize[1] must be positive integer: {}, {}'
                .format(type(outw), outw))
        if isinstance(spatial_scale, numbers.Integral):
            spatial_scale = float(spatial_scale)
        if not (isinstance(spatial_scale, numbers.Real) and
                spatial_scale &gt; 0):
            raise TypeError(
                'spatial_scale must be a positive float number: {}, {}'
                .format(type(spatial_scale), spatial_scale))
        self.outh, self.outw = outh, outw
        self.spatial_scale = spatial_scale

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 29:</b> &nbsp; 4 fragments, nominal size 35 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1366')" href="javascript:;">
chainer-7.2.0/chainer/functions/pooling/roi_average_pooling_2d.py: 89-125
</a>
<div class="mid" id="frag1366" style="display:none"><pre>
    def forward_cpu(self, inputs):
        self.retain_inputs((1, 2))
        self._bottom_data_shape = inputs[0].shape

        bottom_data, bottom_rois, bottom_roi_indices = inputs
        channels, height, width = bottom_data.shape[1:]
        n_rois = bottom_rois.shape[0]
        top_data = numpy.zeros((n_rois, channels, self.outh, self.outw),
                               dtype=bottom_data.dtype)

        for i_roi in six.moves.range(n_rois):
            idx = bottom_roi_indices[i_roi]
            ymin, xmin, ymax, xmax = bottom_rois[i_roi]
            ymin = int(round(ymin * self.spatial_scale))
            xmin = int(round(xmin * self.spatial_scale))
            ymax = int(round(ymax * self.spatial_scale))
            xmax = int(round(xmax * self.spatial_scale))
            roi_height = max(ymax - ymin, 1)
            roi_width = max(xmax - xmin, 1)
            strideh = 1. * roi_height / self.outh
            stridew = 1. * roi_width / self.outw

            for outh in six.moves.range(self.outh):
                sliceh, lenh = _roi_pooling_slice(
                    outh, strideh, height, ymin)
                if sliceh.stop &lt;= sliceh.start:
                    continue
                for outw in six.moves.range(self.outw):
                    slicew, lenw = _roi_pooling_slice(
                        outw, stridew, width, xmin)
                    if slicew.stop &lt;= slicew.start:
                        continue
                    roi_data = bottom_data[int(idx), :, sliceh, slicew]\
                        .reshape(channels, -1)
                    top_data[i_roi, :, outh, outw] =\
                        numpy.average(roi_data, axis=1)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1402')" href="javascript:;">
chainer-7.2.0/chainer/functions/pooling/roi_max_pooling_2d.py: 89-134
</a>
<div class="mid" id="frag1402" style="display:none"><pre>
    def forward_cpu(self, inputs):
        self.retain_inputs((1, 2))
        self._bottom_data_shape = inputs[0].shape

        bottom_data, bottom_rois, bottom_roi_indices = inputs
        channels, height, width = bottom_data.shape[1:]
        n_rois = bottom_rois.shape[0]
        top_data = numpy.full(
            (n_rois, channels, self.outh, self.outw),
            - numpy.inf, dtype=bottom_data.dtype)
        self.argmax_data = - numpy.ones(top_data.shape, numpy.int32)

        for i_roi in six.moves.range(n_rois):
            idx = bottom_roi_indices[i_roi]
            ymin, xmin, ymax, xmax = bottom_rois[i_roi]
            ymin = int(round(ymin * self.spatial_scale))
            xmin = int(round(xmin * self.spatial_scale))
            ymax = int(round(ymax * self.spatial_scale))
            xmax = int(round(xmax * self.spatial_scale))
            roi_height = max(ymax - ymin, 1)
            roi_width = max(xmax - xmin, 1)
            strideh = 1. * roi_height / self.outh
            stridew = 1. * roi_width / self.outw

            for outh in six.moves.range(self.outh):
                sliceh, lenh = _roi_pooling_slice(
                    outh, strideh, height, ymin)
                if sliceh.stop &lt;= sliceh.start:
                    continue
                for outw in six.moves.range(self.outw):
                    slicew, lenw = _roi_pooling_slice(
                        outw, stridew, width, xmin)
                    if slicew.stop &lt;= slicew.start:
                        continue
                    roi_data = bottom_data[int(idx), :, sliceh, slicew]\
                        .reshape(channels, -1)
                    top_data[i_roi, :, outh, outw] =\
                        numpy.max(roi_data, axis=1)

                    # get the max idx respect to feature_maps coordinates
                    max_idx_slice = numpy.unravel_index(
                        numpy.argmax(roi_data, axis=1), (lenh, lenw))
                    max_idx_slice_h = max_idx_slice[0] + sliceh.start
                    max_idx_slice_w = max_idx_slice[1] + slicew.start
                    max_idx_slice = max_idx_slice_h * width + max_idx_slice_w
                    self.argmax_data[i_roi, :, outh, outw] = max_idx_slice
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1368')" href="javascript:;">
chainer-7.2.0/chainer/functions/pooling/roi_average_pooling_2d.py: 196-229
</a>
<div class="mid" id="frag1368" style="display:none"><pre>
        return top_data,

    def backward_cpu(self, inputs, gy):
        bottom_rois, bottom_roi_indices = inputs[1:]
        channels, height, width = self._bottom_data_shape[1:]
        n_rois = bottom_rois.shape[0]
        bottom_diff = numpy.zeros(self._bottom_data_shape, gy[0].dtype)

        for i_roi in six.moves.range(n_rois):
            idx = bottom_roi_indices[i_roi]
            ymin, xmin, ymax, xmax = bottom_rois[i_roi]
            ymin = int(round(ymin * self.spatial_scale))
            xmin = int(round(xmin * self.spatial_scale))
            ymax = int(round(ymax * self.spatial_scale))
            xmax = int(round(xmax * self.spatial_scale))
            roi_height = max(ymax - ymin, 1)
            roi_width = max(xmax - xmin, 1)
            strideh = 1. * roi_height / self.outh
            stridew = 1. * roi_width / self.outw

            for outh in six.moves.range(self.outh):
                sliceh, lenh = _roi_pooling_slice(
                    outh, strideh, height, ymin)
                if sliceh.stop &lt;= sliceh.start:
                    continue
                for outw in six.moves.range(self.outw):
                    slicew, lenw = _roi_pooling_slice(
                        outw, stridew, width, xmin)
                    if slicew.stop &lt;= slicew.start:
                        continue
                    diff_val = gy[0][i_roi, :, outh, outw]\
                        .reshape(channels, 1, 1)
                    diff_val = diff_val / lenh / lenw
                    bottom_diff[int(idx), :, sliceh, slicew] \
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1447')" href="javascript:;">
chainer-7.2.0/chainer/functions/pooling/roi_pooling_2d.py: 69-114
</a>
<div class="mid" id="frag1447" style="display:none"><pre>
    def forward_cpu(self, inputs):
        self.retain_inputs((1,))
        self._bottom_data_shape = inputs[0].shape

        bottom_data, bottom_rois = inputs
        channels, height, width = bottom_data.shape[1:]
        n_rois = bottom_rois.shape[0]
        # `numpy.zeros` needs to be used because the arrays can be
        # returned without having some of its values updated.
        top_data = numpy.zeros((n_rois, channels, self.outh, self.outw),
                               dtype=bottom_data.dtype)
        self.argmax_data = numpy.zeros(top_data.shape, numpy.int32)

        for i_roi in six.moves.range(n_rois):
            idx, xmin, ymin, xmax, ymax = bottom_rois[i_roi]
            xmin = int(round(xmin * self.spatial_scale))
            xmax = int(round(xmax * self.spatial_scale))
            ymin = int(round(ymin * self.spatial_scale))
            ymax = int(round(ymax * self.spatial_scale))
            roi_width = max(xmax - xmin + 1, 1)
            roi_height = max(ymax - ymin + 1, 1)
            strideh = 1. * roi_height / self.outh
            stridew = 1. * roi_width / self.outw

            for outh in six.moves.range(self.outh):
                sliceh, lenh = _roi_pooling_slice(
                    outh, strideh, height, ymin)
                if sliceh.stop &lt;= sliceh.start:
                    continue
                for outw in six.moves.range(self.outw):
                    slicew, lenw = _roi_pooling_slice(
                        outw, stridew, width, xmin)
                    if slicew.stop &lt;= slicew.start:
                        continue
                    roi_data = bottom_data[int(idx), :, sliceh, slicew]\
                        .reshape(channels, -1)
                    top_data[i_roi, :, outh, outw] =\
                        numpy.max(roi_data, axis=1)

                    # get the max idx respect to feature_maps coordinates
                    max_idx_slice = numpy.unravel_index(
                        numpy.argmax(roi_data, axis=1), (lenh, lenw))
                    max_idx_slice_h = max_idx_slice[0] + sliceh.start
                    max_idx_slice_w = max_idx_slice[1] + slicew.start
                    max_idx_slice = max_idx_slice_h * width + max_idx_slice_w
                    self.argmax_data[i_roi, :, outh, outw] = max_idx_slice
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 30:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1367')" href="javascript:;">
chainer-7.2.0/chainer/functions/pooling/roi_average_pooling_2d.py: 126-195
</a>
<div class="mid" id="frag1367" style="display:none"><pre>
        return top_data,

    def forward_gpu(self, inputs):
        self.retain_inputs((1, 2))
        self._bottom_data_shape = inputs[0].shape

        bottom_data, bottom_rois, bottom_roi_indices = inputs
        channels, height, width = bottom_data.shape[1:]
        n_rois = bottom_rois.shape[0]
        top_data = cuda.cupy.empty((n_rois, channels, self.outh,
                                    self.outw), dtype=bottom_data.dtype)
        cuda.elementwise(
            '''
            raw T bottom_data, raw T bottom_rois, raw int32 bottom_roi_indices,
            T spatial_scale, int32 channels, int32 height, int32 width,
            int32 pooled_height, int32 pooled_width
            ''',
            'T top_data',
            '''
            // pos in output filter
            int pw = i % pooled_width;
            int ph = (i / pooled_width) % pooled_height;
            int c = (i / pooled_width / pooled_height) % channels;
            int n = i / pooled_width / pooled_height / channels;

            int roi_batch_ind = bottom_roi_indices[n];
            int roi_start_h = round(bottom_rois[n * 4 + 0] * spatial_scale);
            int roi_start_w = round(bottom_rois[n * 4 + 1] * spatial_scale);
            int roi_end_h = round(bottom_rois[n * 4 + 2] * spatial_scale);
            int roi_end_w = round(bottom_rois[n * 4 + 3] * spatial_scale);

            // Force malformed ROIs to be 1x1
            int roi_height = max(roi_end_h - roi_start_h, 1);
            int roi_width = max(roi_end_w - roi_start_w, 1);
            T bin_size_h = static_cast&lt;T&gt;(roi_height)
                           / static_cast&lt;T&gt;(pooled_height);
            T bin_size_w = static_cast&lt;T&gt;(roi_width)
                           / static_cast&lt;T&gt;(pooled_width);

            int hstart = static_cast&lt;int&gt;(floor(static_cast&lt;T&gt;(ph)
                                          * bin_size_h));
            int wstart = static_cast&lt;int&gt;(floor(static_cast&lt;T&gt;(pw)
                                          * bin_size_w));
            int hend = static_cast&lt;int&gt;(ceil(static_cast&lt;T&gt;(ph + 1)
                                        * bin_size_h));
            int wend = static_cast&lt;int&gt;(ceil(static_cast&lt;T&gt;(pw + 1)
                                        * bin_size_w));

            // Add roi offsets and clip to input boundaries
            hstart = min(max(hstart + roi_start_h, 0), height);
            hend = min(max(hend + roi_start_h, 0), height);
            wstart = min(max(wstart + roi_start_w, 0), width);
            wend = min(max(wend + roi_start_w, 0), width);
            bool is_empty = (hend &lt;= hstart) || (wend &lt;= wstart);

            // Define an empty pooling region to be zero
            T sumval = 0.;
            T count = (hend - hstart) * (wend - wstart);
            int data_offset = (roi_batch_ind * channels + c) * height * width;
            for (int h = hstart; h &lt; hend; ++h) {
                for (int w = wstart; w &lt; wend; ++w) {
                    int bottom_index = h * width + w;
                    sumval += bottom_data[data_offset + bottom_index];
                }
            }
            top_data = is_empty ? 0. : sumval / count;
            ''', 'roi_average_pooling_2d_fwd'
        )(bottom_data, bottom_rois, bottom_roi_indices, self.spatial_scale,
          channels, height, width, self.outh, self.outw, top_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1403')" href="javascript:;">
chainer-7.2.0/chainer/functions/pooling/roi_max_pooling_2d.py: 135-210
</a>
<div class="mid" id="frag1403" style="display:none"><pre>
        return top_data,

    def forward_gpu(self, inputs):
        self.retain_inputs((1, 2))
        self._bottom_data_shape = inputs[0].shape

        bottom_data, bottom_rois, bottom_roi_indices = inputs
        channels, height, width = bottom_data.shape[1:]
        n_rois = bottom_rois.shape[0]
        top_data = cuda.cupy.empty((n_rois, channels, self.outh,
                                    self.outw), dtype=bottom_data.dtype)
        self.argmax_data = cuda.cupy.empty(top_data.shape, numpy.int32)
        cuda.elementwise(
            '''
            raw T bottom_data, raw T bottom_rois, raw int32 bottom_roi_indices,
            T spatial_scale, int32 channels, int32 height, int32 width,
            int32 pooled_height, int32 pooled_width
            ''',
            'T top_data, int32 argmax_data',
            '''
            // pos in output filter
            int pw = i % pooled_width;
            int ph = (i / pooled_width) % pooled_height;
            int c = (i / pooled_width / pooled_height) % channels;
            int n = i / pooled_width / pooled_height / channels;

            int roi_batch_ind = bottom_roi_indices[n];
            int roi_start_h = round(bottom_rois[n * 4 + 0] * spatial_scale);
            int roi_start_w = round(bottom_rois[n * 4 + 1] * spatial_scale);
            int roi_end_h = round(bottom_rois[n * 4 + 2] * spatial_scale);
            int roi_end_w = round(bottom_rois[n * 4 + 3] * spatial_scale);

            // Force malformed ROIs to be 1x1
            int roi_height = max(roi_end_h - roi_start_h , 1);
            int roi_width = max(roi_end_w - roi_start_w, 1);
            T bin_size_h = static_cast&lt;T&gt;(roi_height)
                           / static_cast&lt;T&gt;(pooled_height);
            T bin_size_w = static_cast&lt;T&gt;(roi_width)
                           / static_cast&lt;T&gt;(pooled_width);

            int hstart = static_cast&lt;int&gt;(floor(static_cast&lt;T&gt;(ph)
                                          * bin_size_h));
            int wstart = static_cast&lt;int&gt;(floor(static_cast&lt;T&gt;(pw)
                                          * bin_size_w));
            int hend = static_cast&lt;int&gt;(ceil(static_cast&lt;T&gt;(ph + 1)
                                        * bin_size_h));
            int wend = static_cast&lt;int&gt;(ceil(static_cast&lt;T&gt;(pw + 1)
                                        * bin_size_w));

            // Add roi offsets and clip to input boundaries
            hstart = min(max(hstart + roi_start_h, 0), height);
            hend = min(max(hend + roi_start_h, 0), height);
            wstart = min(max(wstart + roi_start_w, 0), width);
            wend = min(max(wend + roi_start_w, 0), width);

            // Define an empty pooling region to be zero
            T maxval = - (T) (1.0 / 0.0);
            // If nothing is pooled, argmax=-1 causes nothing to be backprop'd
            int maxidx = -1;
            int data_offset = (roi_batch_ind * channels + c) * height * width;
            for (int h = hstart; h &lt; hend; ++h) {
                for (int w = wstart; w &lt; wend; ++w) {
                    int bottom_index = h * width + w;
                    if (bottom_data[data_offset + bottom_index] &gt; maxval) {
                        maxval = bottom_data[data_offset + bottom_index];
                        maxidx = bottom_index;
                    }
                }
            }
            top_data = maxval;
            argmax_data = maxidx;
            ''', 'roi_max_pooling_2d_fwd'
        )(bottom_data, bottom_rois, bottom_roi_indices,
          self.spatial_scale, channels, height, width,
          self.outh, self.outw, top_data, self.argmax_data)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 31:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 94%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1382')" href="javascript:;">
chainer-7.2.0/chainer/functions/pooling/upsampling_2d.py: 20-39
</a>
<div class="mid" id="frag1382" style="display:none"><pre>
    def check_type_forward(self, in_types):
        n_in = in_types.size()
        type_check.expect(n_in == 1)
        x_type = in_types[0]

        type_check.expect(
            x_type.dtype.kind == 'f',
            x_type.ndim == 4,
            x_type.shape == self.indexes.shape,
        )

        if self.outh is not None:
            expected_h = conv.get_conv_outsize(
                self.outh, self.kh, self.sy, self.ph, cover_all=self.cover_all)
            type_check.expect(x_type.shape[2] == expected_h)
        if self.outw is not None:
            expected_w = conv.get_conv_outsize(
                self.outw, self.kw, self.sx, self.pw, cover_all=self.cover_all)
            type_check.expect(x_type.shape[3] == expected_w)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1420')" href="javascript:;">
chainer-7.2.0/chainer/functions/pooling/unpooling_2d.py: 26-44
</a>
<div class="mid" id="frag1420" style="display:none"><pre>
    def check_type_forward(self, in_types):
        n_in = in_types.size()
        type_check.expect(n_in == 1)
        x_type = in_types[0]

        type_check.expect(
            x_type.dtype.kind == 'f',
            x_type.ndim == 4,
        )

        if self.outh is not None:
            expected_h = conv.get_conv_outsize(
                self.outh, self.kh, self.sy, self.ph, cover_all=self.cover_all)
            type_check.expect(x_type.shape[2] == expected_h)
        if self.outw is not None:
            expected_w = conv.get_conv_outsize(
                self.outw, self.kw, self.sx, self.pw, cover_all=self.cover_all)
            type_check.expect(x_type.shape[3] == expected_w)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 32:</b> &nbsp; 2 fragments, nominal size 26 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1392')" href="javascript:;">
chainer-7.2.0/chainer/functions/pooling/roi_max_align_2d.py: 42-69
</a>
<div class="mid" id="frag1392" style="display:none"><pre>
    """ROI max align over a set of 2d planes."""

    def __init__(self, outsize, spatial_scale, sampling_ratio=None):
        outh, outw = _pair(outsize)
        if not (isinstance(outh, numbers.Integral) and outh &gt; 0):
            raise TypeError(
                'outsize[0] must be positive integer: {}, {}'
                .format(type(outh), outh))
        if not (isinstance(outw, numbers.Integral) and outw &gt; 0):
            raise TypeError(
                'outsize[1] must be positive integer: {}, {}'
                .format(type(outw), outw))
        if isinstance(spatial_scale, numbers.Integral):
            spatial_scale = float(spatial_scale)
        if not (isinstance(spatial_scale, numbers.Real) and
                spatial_scale &gt; 0):
            raise TypeError(
                'spatial_scale must be a positive float number: {}, {}'
                .format(type(spatial_scale), spatial_scale))
        sampling_ratio = _pair(sampling_ratio)
        if not all((isinstance(s, numbers.Integral) and s &gt;= 1) or
                   s is None for s in sampling_ratio):
            raise TypeError(
                'sampling_ratio must be integer &gt;= 1 or a pair of it: {}'
                .format(sampling_ratio))

        self.outh, self.outw = outh, outw
        self.spatial_scale = spatial_scale
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1502')" href="javascript:;">
chainer-7.2.0/chainer/functions/pooling/roi_average_align_2d.py: 105-132
</a>
<div class="mid" id="frag1502" style="display:none"><pre>
    def __init__(self, outsize, spatial_scale, sampling_ratio=None):
        outh, outw = _pair(outsize)
        if not (isinstance(outh, numbers.Integral) and outh &gt; 0):
            raise TypeError(
                'outsize[0] must be positive integer: {}, {}'
                .format(type(outh), outh))
        if not (isinstance(outw, numbers.Integral) and outw &gt; 0):
            raise TypeError(
                'outsize[1] must be positive integer: {}, {}'
                .format(type(outw), outw))
        if isinstance(spatial_scale, numbers.Integral):
            spatial_scale = float(spatial_scale)
        if not (isinstance(spatial_scale, numbers.Real) and
                spatial_scale &gt; 0):
            raise TypeError(
                'spatial_scale must be a positive float number: {}, {}'
                .format(type(spatial_scale), spatial_scale))
        sampling_ratio = _pair(sampling_ratio)
        if not all((isinstance(s, numbers.Integral) and s &gt;= 1) or
                   s is None for s in sampling_ratio):
            raise TypeError(
                'sampling_ratio must be integer &gt;= 1 or a pair of it: {}'
                .format(sampling_ratio))

        self.outh, self.outw = outh, outw
        self.spatial_scale = spatial_scale
        self.sampling_ratio = sampling_ratio

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 33:</b> &nbsp; 2 fragments, nominal size 64 lines, similarity 85%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1394')" href="javascript:;">
chainer-7.2.0/chainer/functions/pooling/roi_max_align_2d.py: 85-169
</a>
<div class="mid" id="frag1394" style="display:none"><pre>
        )

    def forward_cpu(self, inputs):
        self.retain_inputs((1, 2))
        self._bottom_data_shape = inputs[0].shape

        bottom_data, bottom_rois, bottom_roi_indices = inputs
        channels, height, width = bottom_data.shape[1:]
        n_rois = bottom_rois.shape[0]
        top_data = numpy.empty((n_rois, channels, self.outh,
                                self.outw), dtype=bottom_data.dtype)
        self.argmax_data = numpy.empty(top_data.shape, numpy.int32)

        pooled_width, pooled_height = self.outw, self.outh
        spatial_scale = self.spatial_scale

        for i in six.moves.range(top_data.size):
            pw = i % pooled_width
            ph = int(i / pooled_width) % pooled_height
            c = int(i / pooled_width / pooled_height) % channels
            n = int(i / pooled_width / pooled_height / channels)

            roi_batch_ind = bottom_roi_indices[n]
            roi_start_h = bottom_rois[n, 0] * spatial_scale
            roi_start_w = bottom_rois[n, 1] * spatial_scale
            roi_end_h = bottom_rois[n, 2] * spatial_scale
            roi_end_w = bottom_rois[n, 3] * spatial_scale

            roi_width = max(roi_end_w - roi_start_w, 1.)
            roi_height = max(roi_end_h - roi_start_h, 1.)
            bin_size_h = roi_height / pooled_height
            bin_size_w = roi_width / pooled_width

            if self.sampling_ratio[0] is None:
                roi_bin_grid_h = int(numpy.ceil(roi_height / pooled_height))
            else:
                roi_bin_grid_h = self.sampling_ratio[0]
            if self.sampling_ratio[1] is None:
                roi_bin_grid_w = int(numpy.ceil(roi_width / pooled_width))
            else:
                roi_bin_grid_w = self.sampling_ratio[1]

            max_val = - numpy.inf
            max_index = -1
            for iy in six.moves.range(roi_bin_grid_h):
                y = roi_start_h + ph * bin_size_h + \
                    (iy + .5) * bin_size_h / roi_bin_grid_h
                y, y_low, y_high = _get_bounds(y, height)
                if y is None or y_low is None or y_high is None:
                    continue
                for ix in six.moves.range(roi_bin_grid_w):
                    x = roi_start_w + pw * bin_size_w + \
                        (ix + .5) * bin_size_w / roi_bin_grid_w
                    x, x_low, x_high = _get_bounds(x, width)
                    if x is None or x_low is None or x_high is None:
                        continue
                    # bilinear interpolation {{

                    w1, w2, w3, w4 = _get_bilinear_interp_params(
                        y, x, y_low, x_low, y_high, x_high)

                    tmp_val = 0.
                    if w1 &gt; 0 and y_low &gt;= 0 and x_low &gt;= 0:
                        v1 = bottom_data[roi_batch_ind, c, y_low, x_low]
                        tmp_val += w1 * v1

                    if w2 &gt; 0 and y_low &gt;= 0 and x_high &lt;= width - 1:
                        v2 = bottom_data[roi_batch_ind, c, y_low, x_high]
                        tmp_val += w2 * v2

                    if w3 &gt; 0 and y_high &lt;= height - 1 and x_low &gt;= 0:
                        v3 = bottom_data[roi_batch_ind, c, y_high, x_low]
                        tmp_val += w3 * v3

                    if w4 &gt; 0 and y_high &lt;= height - 1 and x_high &lt;= width - 1:
                        v4 = bottom_data[roi_batch_ind, c, y_high, x_high]
                        tmp_val += w4 * v4

                    tmp_index = iy * roi_bin_grid_w + ix
                    if tmp_val &gt; max_val:
                        max_val = tmp_val
                        max_index = tmp_index

                    # }}
            top_data[n, c, ph, pw] = max_val
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1504')" href="javascript:;">
chainer-7.2.0/chainer/functions/pooling/roi_average_align_2d.py: 148-227
</a>
<div class="mid" id="frag1504" style="display:none"><pre>
    def forward_cpu(self, inputs):
        self.retain_inputs((1, 2))
        self._bottom_data_shape = inputs[0].shape

        bottom_data, bottom_rois, bottom_roi_indices = inputs
        channels, height, width = bottom_data.shape[1:]
        n_rois = bottom_rois.shape[0]
        top_data = numpy.empty((n_rois, channels, self.outh,
                                self.outw), dtype=bottom_data.dtype)

        pooled_width, pooled_height = self.outw, self.outh
        spatial_scale = self.spatial_scale

        for i in six.moves.range(top_data.size):
            pw = i % pooled_width
            ph = int(i / pooled_width) % pooled_height
            c = int(i / pooled_width / pooled_height) % channels
            n = int(i / pooled_width / pooled_height / channels)

            roi_batch_ind = int(bottom_roi_indices[n])
            roi_start_h = bottom_rois[n, 0] * spatial_scale
            roi_start_w = bottom_rois[n, 1] * spatial_scale
            roi_end_h = bottom_rois[n, 2] * spatial_scale
            roi_end_w = bottom_rois[n, 3] * spatial_scale

            roi_height = max(roi_end_h - roi_start_h, 1.)
            roi_width = max(roi_end_w - roi_start_w, 1.)
            bin_size_h = roi_height / pooled_height
            bin_size_w = roi_width / pooled_width

            if self.sampling_ratio[0] is None:
                roi_bin_grid_h = int(numpy.ceil(roi_height / pooled_height))
            else:
                roi_bin_grid_h = self.sampling_ratio[0]
            if self.sampling_ratio[1] is None:
                roi_bin_grid_w = int(numpy.ceil(roi_width / pooled_width))
            else:
                roi_bin_grid_w = self.sampling_ratio[1]

            count = roi_bin_grid_h * roi_bin_grid_w

            output_val = 0.
            for iy in six.moves.range(roi_bin_grid_h):
                y = roi_start_h + ph * bin_size_h + \
                    (iy + .5) * bin_size_h / roi_bin_grid_h
                y, y_low, y_high = _get_bounds(y, height)
                if y is None or y_low is None or y_high is None:
                    continue
                for ix in six.moves.range(roi_bin_grid_w):
                    x = roi_start_w + pw * bin_size_w + \
                        (ix + .5) * bin_size_w / roi_bin_grid_w
                    x, x_low, x_high = _get_bounds(x, width)
                    if x is None or x_low is None or x_high is None:
                        continue
                    # bilinear interpolation {{

                    w1, w2, w3, w4 = _get_bilinear_interp_params(
                        y, x, y_low, x_low, y_high, x_high)

                    if w1 &gt; 0 and y_low &gt;= 0 and x_low &gt;= 0:
                        v1 = bottom_data[roi_batch_ind, c, y_low, x_low]
                        output_val += w1 * v1

                    if w2 &gt; 0 and y_low &gt;= 0 and x_high &lt;= width - 1:
                        v2 = bottom_data[roi_batch_ind, c, y_low, x_high]
                        output_val += w2 * v2

                    if w3 &gt; 0 and y_high &lt;= height - 1 and x_low &gt;= 0:
                        v3 = bottom_data[roi_batch_ind, c, y_high, x_low]
                        output_val += w3 * v3

                    if w4 &gt; 0 and y_high &lt;= height - 1 and x_high &lt;= width - 1:
                        v4 = bottom_data[roi_batch_ind, c, y_high, x_high]
                        output_val += w4 * v4

                    # }}

            output_val /= count
            top_data[n, c, ph, pw] = output_val

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 34:</b> &nbsp; 2 fragments, nominal size 27 lines, similarity 92%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1395')" href="javascript:;">
chainer-7.2.0/chainer/functions/pooling/roi_max_align_2d.py: 170-295
</a>
<div class="mid" id="frag1395" style="display:none"><pre>
            self.argmax_data[n, c, ph, pw] = max_index

        return top_data,

    def forward_gpu(self, inputs):
        self.retain_inputs((1, 2))
        self._bottom_data_shape = inputs[0].shape

        bottom_data, bottom_rois, bottom_roi_indices = inputs
        channels, height, width = bottom_data.shape[1:]
        n_rois = bottom_rois.shape[0]
        top_data = cuda.cupy.empty((n_rois, channels, self.outh,
                                    self.outw), dtype=bottom_data.dtype)
        self.argmax_data = cuda.cupy.empty(top_data.shape, numpy.int32)

        if self.sampling_ratio[0] is None:
            sampling_ratio_h = 0
        else:
            sampling_ratio_h = self.sampling_ratio[0]
        if self.sampling_ratio[1] is None:
            sampling_ratio_w = 0
        else:
            sampling_ratio_w = self.sampling_ratio[1]
        cuda.elementwise(
            '''
            raw T bottom_data, T spatial_scale, int32 channels,
            int32 height, int32 width, int32 pooled_height, int32 pooled_width,
            int32 sampling_ratio_h, int32 sampling_ratio_w,
            raw T bottom_rois, raw int32 bottom_roi_indices
            ''',
            'T top_data, int32 argmax_data',
            '''
            int pw = i % pooled_width;
            int ph = (i / pooled_width) % pooled_height;
            int c = (i / pooled_width / pooled_height) % channels;
            int n = i / pooled_width / pooled_height / channels;

            int roi_batch_ind = bottom_roi_indices[n];

            T roi_start_h = bottom_rois[n * 4 + 0] * spatial_scale;
            T roi_start_w = bottom_rois[n * 4 + 1] * spatial_scale;
            T roi_end_h = bottom_rois[n * 4 + 2] * spatial_scale;
            T roi_end_w = bottom_rois[n * 4 + 3] * spatial_scale;

            // Force malformed ROIs to be 1x1
            T roi_width = max(roi_end_w - roi_start_w, (T)1.);
            T roi_height = max(roi_end_h - roi_start_h, (T)1.);
            T bin_size_h = static_cast&lt;T&gt;(roi_height)
                            / static_cast&lt;T&gt;(pooled_height);
            T bin_size_w = static_cast&lt;T&gt;(roi_width)
                            / static_cast&lt;T&gt;(pooled_width);

            int bottom_data_offset =
                (roi_batch_ind * channels + c) * height * width;

            // We use roi_bin_grid to sample the grid and mimic integral
            int roi_bin_grid_h = (sampling_ratio_h &gt; 0)
                ? sampling_ratio_h
                : ceil(roi_height / pooled_height);  // e.g. = 2
            int roi_bin_grid_w = (sampling_ratio_w &gt; 0)
                ? sampling_ratio_w
                : ceil(roi_width / pooled_width);

            T max_val = - (T) (1.0 / 0.0);
            int max_index = -1;
            for (int iy = 0; iy &lt; roi_bin_grid_h; iy++)  // e.g. iy = 0, 1
            {
                T y = roi_start_h + ph * bin_size_h +
                    static_cast&lt;T&gt;(iy + .5f) * bin_size_h /
                        static_cast&lt;T&gt;(roi_bin_grid_h);  // e.g. 0.5, 1.5
                int y_low, y_high;
                bool y_ret = get_bounds(y, height, y_low, y_high);
                if (!y_ret) continue;
                for (int ix = 0; ix &lt; roi_bin_grid_w; ix++) {
                    T x = roi_start_w + pw * bin_size_w +
                        static_cast&lt;T&gt;(ix + .5f) * bin_size_w /
                            static_cast&lt;T&gt;(roi_bin_grid_w);

                    int x_low, x_high;
                    bool x_ret = get_bounds(x, width, x_low, x_high);
                    if (!x_ret) continue;
                    // bilinear_interpolation {{
                    T w1, w2, w3, w4;
                    get_bilinear_interp_params(
                        y, x, y_low, x_low, y_high, x_high, w1, w2, w3, w4);

                    T tmp_val = 0.;
                    if (w1 &gt; 0 &amp;&amp; y_low &gt;= 0 &amp;&amp; x_low &gt;= 0) {
                        T v1 = bottom_data[
                            bottom_data_offset + y_low * width + x_low];
                        tmp_val += w1 * v1;
                    }
                    if (w2 &gt; 0 &amp;&amp; y_low &gt;= 0 &amp;&amp; x_high &lt;= width - 1) {
                        T v2 = bottom_data[
                            bottom_data_offset + y_low * width + x_high];
                        tmp_val += w2 * v2;
                    }
                    if (w3 &gt; 0 &amp;&amp; y_high &lt;= height - 1 &amp;&amp; x_low &gt;= 0) {
                        T v3 = bottom_data[
                            bottom_data_offset + y_high * width + x_low];
                        tmp_val += w3 * v3;
                    }
                    if (w4 &gt; 0 &amp;&amp; y_high &lt;= height - 1 &amp;&amp;
                            x_high &lt;= width - 1) {
                        T v4 = bottom_data[
                            bottom_data_offset + y_high * width + x_high];
                        tmp_val += w4 * v4;
                    }

                    int tmp_index = iy * roi_bin_grid_w + ix;
                    if (tmp_val &gt; max_val) {
                        max_val = tmp_val;
                        max_index = tmp_index;
                    }

                    // }}
                }
            }

            top_data = max_val;
            argmax_data = max_index;
            ''',
            'roi_max_align_2d_fwd',
            preamble=_GET_BILINEAR_INTERP_KERNEL,
        )(bottom_data, self.spatial_scale, channels, height, width,
          self.outh, self.outw, sampling_ratio_h, sampling_ratio_w,
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1505')" href="javascript:;">
chainer-7.2.0/chainer/functions/pooling/roi_average_align_2d.py: 228-346
</a>
<div class="mid" id="frag1505" style="display:none"><pre>
        return top_data,

    def forward_gpu(self, inputs):
        self.retain_inputs((1, 2))
        self._bottom_data_shape = inputs[0].shape

        bottom_data, bottom_rois, bottom_roi_indices = inputs
        channels, height, width = bottom_data.shape[1:]
        n_rois = bottom_rois.shape[0]
        top_data = cuda.cupy.empty((n_rois, channels, self.outh,
                                    self.outw), dtype=bottom_data.dtype)
        if self.sampling_ratio[0] is None:
            sampling_ratio_h = 0
        else:
            sampling_ratio_h = self.sampling_ratio[0]
        if self.sampling_ratio[1] is None:
            sampling_ratio_w = 0
        else:
            sampling_ratio_w = self.sampling_ratio[1]
        cuda.elementwise(
            '''
            raw T bottom_data, T spatial_scale, int32 channels,
            int32 height, int32 width, int32 pooled_height, int32 pooled_width,
            int32 sampling_ratio_h, int32 sampling_ratio_w,
            raw T bottom_rois, raw int32 bottom_roi_indices
            ''',
            'T top_data',
            '''
            int pw = i % pooled_width;
            int ph = (i / pooled_width) % pooled_height;
            int c = (i / pooled_width / pooled_height) % channels;
            int n = i / pooled_width / pooled_height / channels;

            int roi_batch_ind = bottom_roi_indices[n];

            T roi_start_h = bottom_rois[n * 4 + 0] * spatial_scale;
            T roi_start_w = bottom_rois[n * 4 + 1] * spatial_scale;
            T roi_end_h = bottom_rois[n * 4 + 2] * spatial_scale;
            T roi_end_w = bottom_rois[n * 4 + 3] * spatial_scale;

            // Force malformed ROIs to be 1x1
            T roi_height = max(roi_end_h - roi_start_h, (T)1.);
            T roi_width = max(roi_end_w - roi_start_w, (T)1.);
            T bin_size_h = static_cast&lt;T&gt;(roi_height)
                            / static_cast&lt;T&gt;(pooled_height);
            T bin_size_w = static_cast&lt;T&gt;(roi_width)
                            / static_cast&lt;T&gt;(pooled_width);

            int bottom_data_offset =
                (roi_batch_ind * channels + c) * height * width;

            // We use roi_bin_grid to sample the grid and mimic integral
            int roi_bin_grid_h = (sampling_ratio_h &gt; 0)
                ? sampling_ratio_h
                : ceil(roi_height / pooled_height);  // e.g. = 2
            int roi_bin_grid_w = (sampling_ratio_w &gt; 0)
                ? sampling_ratio_w
                : ceil(roi_width / pooled_width);

            // We do average (integral) pooling inside a bin
            T count = roi_bin_grid_h * roi_bin_grid_w;  // e.g. = 4

            T output_val = 0.;
            for (int iy = 0; iy &lt; roi_bin_grid_h; iy++)  // e.g. iy = 0, 1
            {
                T y = roi_start_h + ph * bin_size_h +
                    static_cast&lt;T&gt;(iy + .5f) * bin_size_h /
                        static_cast&lt;T&gt;(roi_bin_grid_h);  // e.g. 0.5, 1.5
                int y_low, y_high;
                bool y_ret = get_bounds(y, height, y_low, y_high);
                if (!y_ret) continue;
                for (int ix = 0; ix &lt; roi_bin_grid_w; ix++) {
                    T x = roi_start_w + pw * bin_size_w +
                        static_cast&lt;T&gt;(ix + .5f) * bin_size_w /
                            static_cast&lt;T&gt;(roi_bin_grid_w);

                    int x_low, x_high;
                    bool x_ret = get_bounds(x, width, x_low, x_high);
                    if (!x_ret) continue;
                    // bilinear_interpolation_gradient {{
                    T w1, w2, w3, w4;
                    get_bilinear_interp_params(
                        y, x, y_low, x_low, y_high, x_high, w1, w2, w3, w4);

                    if (w1 &gt; 0 &amp;&amp; y_low &gt;= 0 &amp;&amp; x_low &gt;= 0) {
                        T v1 = bottom_data[
                            bottom_data_offset + y_low * width + x_low];
                        output_val += w1 * v1;
                    }
                    if (w2 &gt; 0 &amp;&amp; y_low &gt;= 0 &amp;&amp; x_high &lt;= width - 1) {
                        T v2 = bottom_data[
                            bottom_data_offset + y_low * width + x_high];
                        output_val += w2 * v2;
                    }
                    if (w3 &gt; 0 &amp;&amp; y_high &lt;= height - 1 &amp;&amp; x_low &gt;= 0) {
                        T v3 = bottom_data[
                            bottom_data_offset + y_high * width + x_low];
                        output_val += w3 * v3;
                    }
                    if (w4 &gt; 0 &amp;&amp; y_high &lt;= height - 1 &amp;&amp;
                            x_high &lt;= width - 1) {
                        T v4 = bottom_data[
                            bottom_data_offset + y_high * width + x_high];
                        output_val += w4 * v4;
                    }

                    // }}
                }
            }
            output_val /= count;

            top_data = output_val;
            ''',
            'roi_average_align_2d_fwd',
            preamble=_GET_BILINEAR_INTERP_KERNEL,
        )(bottom_data, self.spatial_scale, channels, height, width,
          self.outh, self.outw, sampling_ratio_h, sampling_ratio_w,
          bottom_rois, bottom_roi_indices, top_data)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 35:</b> &nbsp; 2 fragments, nominal size 58 lines, similarity 79%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1396')" href="javascript:;">
chainer-7.2.0/chainer/functions/pooling/roi_max_align_2d.py: 296-372
</a>
<div class="mid" id="frag1396" style="display:none"><pre>
          bottom_rois, bottom_roi_indices, top_data, self.argmax_data)

        return top_data,

    def backward_cpu(self, inputs, gy):
        bottom_rois, bottom_roi_indices = inputs[1:]
        channels, height, width = self._bottom_data_shape[1:]
        bottom_diff = numpy.zeros(self._bottom_data_shape, gy[0].dtype)

        spatial_scale = self.spatial_scale
        pooled_height = self.outh
        pooled_width = self.outw
        top_diff = gy[0]

        for i in six.moves.range(top_diff.size):
            pw = i % pooled_width
            ph = int(i / pooled_width) % pooled_height
            c = int(i / pooled_width / pooled_height) % channels
            n = int(i / pooled_width / pooled_height / channels)

            roi_batch_ind = bottom_roi_indices[n]
            roi_start_h = bottom_rois[n, 0] * spatial_scale
            roi_start_w = bottom_rois[n, 1] * spatial_scale
            roi_end_h = bottom_rois[n, 2] * spatial_scale
            roi_end_w = bottom_rois[n, 3] * spatial_scale

            roi_height = max(roi_end_h - roi_start_h, 1.)
            roi_width = max(roi_end_w - roi_start_w, 1.)
            bin_size_h = roi_height / pooled_height
            bin_size_w = roi_width / pooled_width

            top_diff_this_bin = top_diff[n, c, ph, pw]
            max_index = self.argmax_data[n, c, ph, pw]

            if max_index != -1:
                if self.sampling_ratio[0] is None:
                    roi_bin_grid_h = numpy.ceil(roi_height / pooled_height)
                else:
                    roi_bin_grid_h = self.sampling_ratio[0]
                if self.sampling_ratio[1] is None:
                    roi_bin_grid_w = numpy.ceil(roi_width / pooled_width)
                else:
                    roi_bin_grid_w = self.sampling_ratio[1]

                iy = int(max_index / roi_bin_grid_w)
                ix = max_index % roi_bin_grid_w

                y = roi_start_h + ph * bin_size_h + \
                    (iy + .5) * bin_size_h / roi_bin_grid_h
                x = roi_start_w + pw * bin_size_w + \
                    (ix + .5) * bin_size_w / roi_bin_grid_w

                # bilinear_interpolation_gradient {{

                y, y_low, y_high = _get_bounds(y, height)
                if y is None or y_low is None or y_high is None:
                    continue
                x, x_low, x_high = _get_bounds(x, width)
                if x is None or x_low is None or x_high is None:
                    continue
                w1, w2, w3, w4 = _get_bilinear_interp_params(
                    y, x, y_low, x_low, y_high, x_high)

                if w1 &gt; 0 and y_low &gt;= 0 and x_low &gt;= 0:
                    g1 = top_diff_this_bin * w1
                    bottom_diff[roi_batch_ind, c, y_low, x_low] += g1

                if w2 &gt; 0 and y_low &gt;= 0 and x_high &lt;= width - 1:
                    g2 = top_diff_this_bin * w2
                    bottom_diff[roi_batch_ind, c, y_low, x_high] += g2

                if w3 &gt; 0 and y_high &lt;= height - 1 and x_low &gt;= 0:
                    g3 = top_diff_this_bin * w3
                    bottom_diff[roi_batch_ind, c, y_high, x_low] += g3

                if w4 &gt; 0 and y_high &lt;= height - 1 and x_high &lt;= width - 1:
                    g4 = top_diff_this_bin * w4
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1506')" href="javascript:;">
chainer-7.2.0/chainer/functions/pooling/roi_average_align_2d.py: 347-421
</a>
<div class="mid" id="frag1506" style="display:none"><pre>
        return top_data,

    def backward_cpu(self, inputs, gy):
        bottom_rois, bottom_roi_indices = inputs[1:]
        channels, height, width = self._bottom_data_shape[1:]
        bottom_diff = numpy.zeros(self._bottom_data_shape, gy[0].dtype)

        spatial_scale = self.spatial_scale
        pooled_height = self.outh
        pooled_width = self.outw
        top_diff = gy[0]

        for i in six.moves.range(top_diff.size):
            pw = i % pooled_width
            ph = int(i / pooled_width) % pooled_height
            c = int(i / pooled_width / pooled_height) % channels
            n = int(i / pooled_width / pooled_height / channels)

            roi_batch_ind = int(bottom_roi_indices[n])
            roi_start_h = bottom_rois[n, 0] * spatial_scale
            roi_start_w = bottom_rois[n, 1] * spatial_scale
            roi_end_h = bottom_rois[n, 2] * spatial_scale
            roi_end_w = bottom_rois[n, 3] * spatial_scale

            roi_height = max(roi_end_h - roi_start_h, 1.)
            roi_width = max(roi_end_w - roi_start_w, 1.)
            bin_size_h = roi_height / pooled_height
            bin_size_w = roi_width / pooled_width

            top_diff_this_bin = top_diff[n, c, ph, pw]

            if self.sampling_ratio[0] is None:
                roi_bin_grid_h = int(numpy.ceil(roi_height / pooled_height))
            else:
                roi_bin_grid_h = self.sampling_ratio[0]
            if self.sampling_ratio[1] is None:
                roi_bin_grid_w = int(numpy.ceil(roi_width / pooled_width))
            else:
                roi_bin_grid_w = self.sampling_ratio[1]

            count = roi_bin_grid_h * roi_bin_grid_w

            for iy in six.moves.range(roi_bin_grid_h):
                y = roi_start_h + ph * bin_size_h + \
                    (iy + .5) * bin_size_h / roi_bin_grid_h
                y, y_low, y_high = _get_bounds(y, height)
                if y is None or y_low is None or y_high is None:
                    continue
                for ix in six.moves.range(roi_bin_grid_w):
                    x = roi_start_w + pw * bin_size_w + \
                        (ix + .5) * bin_size_w / roi_bin_grid_w
                    x, x_low, x_high = _get_bounds(x, width)
                    if x is None or x_low is None or x_high is None:
                        continue
                    # bilinear_interpolation_gradient {{

                    w1, w2, w3, w4 = _get_bilinear_interp_params(
                        y, x, y_low, x_low, y_high, x_high)

                    if w1 &gt; 0 and y_low &gt;= 0 and x_low &gt;= 0:
                        g1 = top_diff_this_bin * w1 / count
                        bottom_diff[roi_batch_ind, c, y_low, x_low] += g1

                    if w2 &gt; 0 and y_low &gt;= 0 and x_high &lt;= width - 1:
                        g2 = top_diff_this_bin * w2 / count
                        bottom_diff[roi_batch_ind, c, y_low, x_high] += g2

                    if w3 &gt; 0 and y_high &lt;= height - 1 and x_low &gt;= 0:
                        g3 = top_diff_this_bin * w3 / count
                        bottom_diff[roi_batch_ind, c, y_high, x_low] += g3

                    if w4 &gt; 0 and y_high &lt;= height - 1 and x_high &lt;= width - 1:
                        g4 = top_diff_this_bin * w4 / count
                        bottom_diff[roi_batch_ind, c, y_high, x_high] += g4

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 36:</b> &nbsp; 2 fragments, nominal size 24 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1397')" href="javascript:;">
chainer-7.2.0/chainer/functions/pooling/roi_max_align_2d.py: 373-488
</a>
<div class="mid" id="frag1397" style="display:none"><pre>
                    bottom_diff[roi_batch_ind, c, y_high, x_high] += g4

                # }}

        return bottom_diff, None, None

    def backward_gpu(self, inputs, gy):
        utils.nondeterministic('atomicAdd')
        bottom_rois, bottom_roi_indices = inputs[1:]
        channels, height, width = self._bottom_data_shape[1:]
        bottom_diff = cuda.cupy.zeros(self._bottom_data_shape, gy[0].dtype)

        if self.sampling_ratio[0] is None:
            sampling_ratio_h = 0
        else:
            sampling_ratio_h = self.sampling_ratio[0]
        if self.sampling_ratio[1] is None:
            sampling_ratio_w = 0
        else:
            sampling_ratio_w = self.sampling_ratio[1]

        cuda.elementwise(
            '''
            raw T top_diff, T spatial_scale,
            int32 channels, int32 height, int32 width,
            int32 pooled_height, int32 pooled_width,
            int32 sampling_ratio_h, int32 sampling_ratio_w,
            raw T bottom_rois, raw int32 bottom_roi_indices
            ''',
            'raw T bottom_diff, raw int32 argmax_data',
            '''
            // (n, c, h, w) coords in bottom data
            int pw = i % pooled_width;
            int ph = (i / pooled_width) % pooled_height;
            int c = (i / pooled_width / pooled_height) % channels;
            int n = i / pooled_width / pooled_height / channels;

            // Do not using rounding; this implementation detail is critical
            int roi_batch_ind = bottom_roi_indices[n];
            T roi_start_h = bottom_rois[n * 4 + 0] * spatial_scale;
            T roi_start_w = bottom_rois[n * 4 + 1] * spatial_scale;
            T roi_end_h = bottom_rois[n * 4 + 2] * spatial_scale;
            T roi_end_w = bottom_rois[n * 4 + 3] * spatial_scale;

            // Force malformed ROIs to be 1x1
            T roi_width = max(roi_end_w - roi_start_w, (T)1.);
            T roi_height = max(roi_end_h - roi_start_h, (T)1.);
            T bin_size_h = static_cast&lt;T&gt;(roi_height) /
                static_cast&lt;T&gt;(pooled_height);
            T bin_size_w = static_cast&lt;T&gt;(roi_width) /
                static_cast&lt;T&gt;(pooled_width);

            int bottom_diff_offset =
                (roi_batch_ind * channels + c) * height * width;

            int top_offset = (n * channels + c) * pooled_height * pooled_width;
            int max_index = argmax_data[top_offset + ph * pooled_width + pw];

            if (max_index != -1) {
                T top_diff_this_bin =
                    top_diff[top_offset + ph * pooled_width + pw];

                // We use roi_bin_grid to sample the grid and mimic integral
                int roi_bin_grid_h = (sampling_ratio_h &gt; 0)
                    ? sampling_ratio_h
                    : ceil(roi_height / pooled_height); // e.g. = 2
                int roi_bin_grid_w = (sampling_ratio_w &gt; 0)
                    ? sampling_ratio_w
                    : ceil(roi_width / pooled_width);

                int iy = max_index / roi_bin_grid_w;
                int ix = max_index % roi_bin_grid_w;

                T y = roi_start_h + ph * bin_size_h +
                    static_cast&lt;T&gt;(iy + .5f) * bin_size_h /
                        static_cast&lt;T&gt;(roi_bin_grid_h);  // e.g. 0.5, 1.5
                T x = roi_start_w + pw * bin_size_w +
                    static_cast&lt;T&gt;(ix + .5f) * bin_size_w /
                        static_cast&lt;T&gt;(roi_bin_grid_w);

                // bilinear_interpolation_gradient {{
                int y_low, x_low, y_high, x_high;
                T w1, w2, w3, w4;
                bool y_ret = get_bounds(y, height, y_low, y_high);
                bool x_ret = get_bounds(x, width, x_low, x_high);
                if (!x_ret || !y_ret) continue;
                get_bilinear_interp_params(
                    y, x, y_low, x_low, y_high, x_high, w1, w2, w3, w4);

                if (w1 &gt; 0 &amp;&amp; y_low &gt;= 0 &amp;&amp; x_low &gt;= 0) {
                    T g1 = top_diff_this_bin * w1;
                    atomicAdd(&amp;bottom_diff[
                        bottom_diff_offset + y_low * width + x_low], g1);
                }
                if (w2 &gt; 0 &amp;&amp; y_low &gt;= 0 &amp;&amp; x_high &lt;= width - 1) {
                    T g2 = top_diff_this_bin * w2;
                    atomicAdd(&amp;bottom_diff[
                        bottom_diff_offset + y_low * width + x_high], g2);
                }
                if (w3 &gt; 0 &amp;&amp; y_high &lt;= height - 1 &amp;&amp; x_low &gt;= 0) {
                    T g3 = top_diff_this_bin * w3;
                    atomicAdd(&amp;bottom_diff[
                        bottom_diff_offset + y_high * width + x_low], g3);
                }
                if (w4 &gt; 0 &amp;&amp; y_high &lt;= height - 1 &amp;&amp; x_high &lt;= width - 1) {
                    T g4 = top_diff_this_bin * w4;
                    atomicAdd(&amp;bottom_diff[
                        bottom_diff_offset + y_high * width + x_high], g4);
                }
            }
            // }}
            ''',
            'roi_max_align_2d_bwd',
            preamble=_GET_BILINEAR_INTERP_KERNEL,
        )(gy[0], self.spatial_scale, channels, height, width,
          self.outh, self.outw, sampling_ratio_h, sampling_ratio_w,
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1507')" href="javascript:;">
chainer-7.2.0/chainer/functions/pooling/roi_average_align_2d.py: 422-541
</a>
<div class="mid" id="frag1507" style="display:none"><pre>
                    # }}

        return bottom_diff, None, None

    def backward_gpu(self, inputs, gy):
        utils.nondeterministic('atomicAdd')
        bottom_rois, bottom_roi_indices = inputs[1:]
        channels, height, width = self._bottom_data_shape[1:]
        bottom_diff = cuda.cupy.zeros(self._bottom_data_shape, gy[0].dtype)

        if self.sampling_ratio[0] is None:
            sampling_ratio_h = 0
        else:
            sampling_ratio_h = self.sampling_ratio[0]
        if self.sampling_ratio[1] is None:
            sampling_ratio_w = 0
        else:
            sampling_ratio_w = self.sampling_ratio[1]
        cuda.elementwise(
            '''
            raw T top_diff,
            int32 num_rois, T spatial_scale,
            int32 channels, int32 height, int32 width,
            int32 pooled_height, int32 pooled_width,
            int32 sampling_ratio_h, int32 sampling_ratio_w,
            raw T bottom_rois, raw int32 bottom_roi_indices
            ''',
            'raw T bottom_diff',
            '''
            // (n, c, h, w) coords in bottom data
            int pw = i % pooled_width;
            int ph = (i / pooled_width) % pooled_height;
            int c = (i / pooled_width / pooled_height) % channels;
            int n = i / pooled_width / pooled_height / channels;

            // Do not using rounding; this implementation detail is critical
            int roi_batch_ind = bottom_roi_indices[n];
            T roi_start_h = bottom_rois[n * 4 + 0] * spatial_scale;
            T roi_start_w = bottom_rois[n * 4 + 1] * spatial_scale;
            T roi_end_h = bottom_rois[n * 4 + 2] * spatial_scale;
            T roi_end_w = bottom_rois[n * 4 + 3] * spatial_scale;

            // Force malformed ROIs to be 1x1
            T roi_width = max(roi_end_w - roi_start_w, (T)1.);
            T roi_height = max(roi_end_h - roi_start_h, (T)1.);
            T bin_size_h = static_cast&lt;T&gt;(roi_height) /
                static_cast&lt;T&gt;(pooled_height);
            T bin_size_w = static_cast&lt;T&gt;(roi_width) /
                static_cast&lt;T&gt;(pooled_width);

            int bottom_diff_offset =
                (roi_batch_ind * channels + c) * height * width;

            int top_offset = (n * channels + c) * pooled_height * pooled_width;
            T top_diff_this_bin =
                top_diff[top_offset + ph * pooled_width + pw];

            // We use roi_bin_grid to sample the grid and mimic integral
            int roi_bin_grid_h = (sampling_ratio_h &gt; 0)
                ? sampling_ratio_h
                : ceil(roi_height / pooled_height); // e.g. = 2
            int roi_bin_grid_w = (sampling_ratio_w &gt; 0)
                ? sampling_ratio_w
                : ceil(roi_width / pooled_width);

            // We do average (integral) pooling inside a bin
            T count = roi_bin_grid_h * roi_bin_grid_w;  // e.g. = 4

            for (int iy = 0; iy &lt; roi_bin_grid_h; iy++) {
                T y = roi_start_h + ph * bin_size_h +
                    static_cast&lt;T&gt;(iy + .5f) * bin_size_h /
                        static_cast&lt;T&gt;(roi_bin_grid_h);  // e.g. 0.5, 1.5
                int y_low, y_high;
                bool y_ret = get_bounds(y, height, y_low, y_high);
                if (!y_ret) continue;
                for (int ix = 0; ix &lt; roi_bin_grid_w; ix++) {
                    T x = roi_start_w + pw * bin_size_w +
                        static_cast&lt;T&gt;(ix + .5f) * bin_size_w /
                            static_cast&lt;T&gt;(roi_bin_grid_w);

                    int x_low, x_high;
                    bool x_ret = get_bounds(x, width, x_low, x_high);
                    if (!x_ret) continue;
                    // bilinear_interpolation_gradient {{
                    T w1, w2, w3, w4;
                    get_bilinear_interp_params(
                        y, x, y_low, x_low, y_high, x_high, w1, w2, w3, w4);

                    if (w1 &gt; 0 &amp;&amp; y_low &gt;= 0 &amp;&amp; x_low &gt;= 0) {
                        T g1 = top_diff_this_bin * w1 / count;
                        atomicAdd(&amp;bottom_diff[
                            bottom_diff_offset + y_low * width + x_low], g1);
                    }
                    if (w2 &gt; 0 &amp;&amp; y_low &gt;= 0 &amp;&amp; x_high &lt;= width - 1) {
                        T g2 = top_diff_this_bin * w2 / count;
                        atomicAdd(&amp;bottom_diff[
                            bottom_diff_offset + y_low * width + x_high], g2);
                    }
                    if (w3 &gt; 0 &amp;&amp; y_high &lt;= height - 1 &amp;&amp; x_low &gt;= 0) {
                        T g3 = top_diff_this_bin * w3 / count;
                        atomicAdd(&amp;bottom_diff[
                            bottom_diff_offset + y_high * width + x_low], g3);
                    }
                    if (w4 &gt; 0 &amp;&amp; y_high &lt;= height - 1 &amp;&amp;
                            x_high &lt;= width - 1) {
                        T g4 = top_diff_this_bin * w4 / count;
                        atomicAdd(&amp;bottom_diff[
                            bottom_diff_offset + y_high * width + x_high], g4);
                    }

                    // }}
                }
            }
            ''',
            'roi_average_align_2d_bwd',
            preamble=_GET_BILINEAR_INTERP_KERNEL,
        )(gy[0], bottom_rois.shape[0],
          self.spatial_scale, channels, height, width, self.outh, self.outw,
          sampling_ratio_h, sampling_ratio_w, bottom_rois, bottom_roi_indices,
          bottom_diff, size=gy[0].size)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 37:</b> &nbsp; 3 fragments, nominal size 10 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1446')" href="javascript:;">
chainer-7.2.0/chainer/functions/pooling/roi_pooling_2d.py: 57-68
</a>
<div class="mid" id="frag1446" style="display:none"><pre>
    def check_type_forward(self, in_types):
        type_check.expect(in_types.size() == 2)

        x_type, roi_type = in_types
        type_check.expect(
            x_type.dtype.kind == 'f',
            x_type.ndim == 4,
            x_type.dtype == roi_type.dtype,
            roi_type.ndim == 2,
            roi_type.shape[1] == 5,
        )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10379')" href="javascript:;">
chainer-7.2.0/examples/sentiment/thin_stack.py: 46-56
</a>
<div class="mid" id="frag10379" style="display:none"><pre>
    def check_type_forward(self, in_types):
        type_check.expect(in_types.size() == 2)
        s_type, i_type = in_types
        type_check.expect(
            s_type.dtype.kind == 'f',
            i_type.dtype.kind == 'i',
            s_type.ndim == 3,
            i_type.ndim == 1,
            s_type.shape[0] &gt;= i_type.shape[0],
        )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2145')" href="javascript:;">
chainer-7.2.0/chainer/functions/loss/hinge.py: 33-44
</a>
<div class="mid" id="frag2145" style="display:none"><pre>
    def check_type_forward(self, in_types):
        type_check._argname(in_types, ('x', 't'))

        x_type, t_type = in_types
        type_check.expect(
            x_type.dtype.kind == 'f',
            t_type.dtype.kind == 'i',
            x_type.ndim == 2,
            t_type.ndim == 1,
            x_type.shape[0] == t_type.shape[0],
        )

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 38:</b> &nbsp; 2 fragments, nominal size 25 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1487')" href="javascript:;">
chainer-7.2.0/chainer/functions/pooling/max_pooling_nd.py: 309-345
</a>
<div class="mid" id="frag1487" style="display:none"><pre>
        return col.reshape((n, c) + outs),

    def forward_gpu(self, inputs):
        func = self.func

        if func.is_cudnn_used:
            x = func.get_retained_inputs()[0].array
            return self._forward_gpu_compute_indexes_again((x, inputs[0]))

        ndim = func.ndim
        ksize = func.ksize
        stride = func.stride
        pad = func.pad
        cover_all = func.cover_all
        indexes = backend.from_chx(func.indexes)

        x, = inputs
        in_shape = x.shape
        in_dtype = x.dtype

        n, c = in_shape[:2]
        dims = in_shape[2:]

        ys = tuple(conv_nd.get_conv_outsize(d, k, s, p, cover_all)
                   for (d, k, s, p) in six.moves.zip(dims, ksize, stride, pad))
        # (n, c, y_1, y_2, ..., y_N)
        y_shape = (n, c) + ys
        y = cuda.cupy.empty(y_shape, dtype=x.dtype)

        cls = max_pooling_nd_kernel.MaxPoolingNDKernelForwardWithIndexes
        in_params, out_params, operation, name = cls.generate(ndim)
        cuda.elementwise(in_params, out_params, operation, name)(
            x.reduced_view(),
            *(dims + ys + ksize + stride + pad + (indexes.reduced_view(), y)))

        self._in_shape = in_shape
        self._in_dtype = in_dtype
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1488')" href="javascript:;">
chainer-7.2.0/chainer/functions/pooling/max_pooling_nd.py: 346-376
</a>
<div class="mid" id="frag1488" style="display:none"><pre>
        return y,

    def _forward_gpu_compute_indexes_again(self, inputs):
        func = self.func
        ndim = func.ndim
        ksize = func.ksize
        stride = func.stride
        pad = func.pad
        cover_all = func.cover_all

        x, ggx = inputs
        in_shape = x.shape
        in_dtype = x.dtype

        n, c = in_shape[:2]
        dims = in_shape[2:]

        ys = tuple(conv_nd.get_conv_outsize(d, k, s, p, cover_all)
                   for (d, k, s, p) in six.moves.zip(dims, ksize, stride, pad))
        # (n, c, y_1, y_2, ..., y_N)
        y_shape = (n, c) + ys
        y = cuda.cupy.empty(y_shape, dtype=x.dtype)

        cls = max_pooling_nd_kernel.MaxPoolingNDKernelForwardWithIndexes1
        in_params, out_params, operation, name = cls.generate(ndim)
        cuda.elementwise(in_params, out_params, operation, name)(
            x.reduced_view(),
            *(dims + ys + ksize + stride + pad + (ggx.reduced_view(), y)))

        self._in_shape = in_shape
        self._in_dtype = in_dtype
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 39:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1527')" href="javascript:;">
chainer-7.2.0/chainer/functions/pooling/max_pooling_nd_kernel.py: 136-149
</a>
<div class="mid" id="frag1527" style="display:none"><pre>
    def _compile_out(self):
        def aux(offset, d_val, max_val, offset1):
            return 'int {} = {} * ({} + {});'.format(
                offset, d_val, max_val, offset1)

        d_vals = conv_nd_kernel.vars('d', self.ndim)[1:] + [1]
        max_vals = conv_nd_kernel.vars('max', self.ndim)
        offsets = conv_nd_kernel.vars('offset', self.ndim)
        offsets1 = ['d_0 * c0'] + offsets[:-1]
        offset_strs = conv_nd_kernel.map_(
            aux, offsets, d_vals, max_vals, offsets1)
        offset_strs.append('out = in[offset_{}];'.format(self.ndim - 1))
        return offset_strs

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1533')" href="javascript:;">
chainer-7.2.0/chainer/functions/pooling/max_pooling_nd_kernel.py: 191-206
</a>
<div class="mid" id="frag1533" style="display:none"><pre>
    def after(self, out_xs):
        # 2D: int offset_0 = d_1 * (argmax_0 + d_0 * c0);
        #     int offset_1 = 1 * (argmax_1 + offset_0);
        #     out = ggx[offset_1];
        def aux(offset, d_val, max_val, offset1):
            return 'int {} = {} * ({} + {});'.format(
                offset, d_val, max_val, offset1)

        d_vals = conv_nd_kernel.vars('d', self.ndim)[1:] + [1]
        max_vals = conv_nd_kernel.vars('argmax', self.ndim)
        offsets = conv_nd_kernel.vars('offset', self.ndim)
        offsets1 = ['d_0 * c0'] + offsets[:-1]
        offset_strs = conv_nd_kernel.map_(
            aux, offsets, d_vals, max_vals, offsets1)
        offset_strs.append('out = ggx[offset_{}];'.format(self.ndim - 1))
        return '\n'.join(offset_strs)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 40:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1542')" href="javascript:;">
chainer-7.2.0/chainer/functions/pooling/pooling_nd_kernel.py: 32-45
</a>
<div class="mid" id="frag1542" style="display:none"><pre>
    def _generate(self, ndim):
        self.ndim = ndim
        self.ds = conv_nd_kernel.vars('d', ndim)
        self.outs = conv_nd_kernel.vars('out', ndim)
        self.ks = conv_nd_kernel.vars('k', ndim)
        self.ss = conv_nd_kernel.vars('s', ndim)
        self.ps = conv_nd_kernel.vars('p', ndim)

        in_params = self._in_params()
        out_params = self._out_params()
        operation = self._operation()
        name = '{}_pool_{}d_fwd'.format(self.name(), self.ndim)
        return in_params, out_params, operation, name

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1562')" href="javascript:;">
chainer-7.2.0/chainer/functions/pooling/pooling_nd_kernel.py: 181-194
</a>
<div class="mid" id="frag1562" style="display:none"><pre>
    def _generate(self, ndim):
        self.ndim = ndim
        self.ds = conv_nd_kernel.vars('d', ndim)
        self.outs = conv_nd_kernel.vars('out', ndim)
        self.ks = conv_nd_kernel.vars('k', ndim)
        self.ss = conv_nd_kernel.vars('s', ndim)
        self.ps = conv_nd_kernel.vars('p', ndim)

        in_params = self._in_params()
        out_params = self._out_params()
        operation = self._operation()
        name = '{}_pool_{}d_bwd'.format(self.name(), self.ndim)
        return in_params, out_params, operation, name

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 41:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1543')" href="javascript:;">
chainer-7.2.0/chainer/functions/pooling/pooling_nd_kernel.py: 46-61
</a>
<div class="mid" id="frag1543" style="display:none"><pre>
    def _in_params(self):
        # 2D: raw T in, int32 d_0, int32 d_1, int32 out_0, int32 out_1,
        #     int32 k_0, int32 k_1, int32 s_0, int32 s_1, int32 p_0,
        #     int32 p_1, ...
        def aux(x):
            return 'int32 {}'.format(x)
        in_params = self.in_params()
        if type(in_params) is tuple:
            raws = in_params[0]
            in_params = in_params[1]
        else:
            raws = []
        vars = self.ds + self.outs + self.ks + self.ss + self.ps
        return ', '.join(
            ['raw T in'] + raws + conv_nd_kernel.map_(aux, vars) + in_params)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1563')" href="javascript:;">
chainer-7.2.0/chainer/functions/pooling/pooling_nd_kernel.py: 195-210
</a>
<div class="mid" id="frag1563" style="display:none"><pre>
    def _in_params(self):
        # 2D: raw T gy, int32 d_0, int32 d_1, int32 out_0, int32 out_1,
        #     int32 k_0, int32 k_1, int32 s_0, int32 s_1, int32 p_0,
        #     int32 p_1, ...
        def aux(x):
            return 'int32 {}'.format(x)
        in_params = self.in_params()
        if type(in_params) is tuple:
            raws = in_params[0]
            in_params = in_params[1]
        else:
            raws = []
        vars = self.ds + self.outs + self.ks + self.ss + self.ps
        return ', '.join(
            ['raw T gy'] + raws + conv_nd_kernel.map_(aux, vars) + in_params)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 42:</b> &nbsp; 2 fragments, nominal size 30 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1549')" href="javascript:;">
chainer-7.2.0/chainer/functions/pooling/pooling_nd_kernel.py: 87-139
</a>
<div class="mid" id="frag1549" style="display:none"><pre>
    def _compile_loop(self, out_xs):
        # 2D: int in_x0_0 = max(0,   out_x_0 * s_0       - p_0);
        #     int in_x1_0 = min(d_0, out_x_0 * s_0 + k_0 - p_0);
        #     int in_x0_1 = max(0,   out_x_1 * s_1       - p_1);
        #     int in_x1_1 = min(d_1, out_x_1 * s_1 + k_1 - p_1);
        #     ... Before-part here ...
        #     for (int x_0 = in_x0_0; x_0 &lt; in_x1_0; ++x_0) {
        #       int offset_0 = d_1 * (x_0 + d_0 * c0);
        #       for (int x_1 = in_x0_1; x_1 &lt; in_x1_1; ++x_1) {
        #         int offset_1 = 1 * (x_1 + offset_0);
        #         ... Main-part here ...
        #       }
        #     }
        #     ... After-part here ...
        def aux(in_x0, in_x1, d, out, k, s, p):
            return [
                'int {} = max(0, {} * {} - {});'.format(in_x0, out, s, p),
                'int {} = min({}, {} * {} + {} - {});'.format(
                    in_x1, d, out, s, k, p)]
        in_x0s = conv_nd_kernel.vars('in_x0', self.ndim)
        in_x1s = conv_nd_kernel.vars('in_x1', self.ndim)
        bounds = sum(conv_nd_kernel.map_(
            aux, in_x0s, in_x1s, self.ds, out_xs, self.ks, self.ss, self.ps
        ), [])

        def _loop_main(main):
            w = conv_nd_kernel.Writer()

            # Loop openings.
            xs = conv_nd_kernel.vars('x', self.ndim)
            offsets = conv_nd_kernel.vars('offset', self.ndim)
            ds1 = self.ds[1:] + [1]
            offsets1 = ['d_0 * c0'] + offsets[:-1]
            for x, in_x0, in_x1, offset, offset1, d1 in moves.zip(
                    xs, in_x0s, in_x1s, offsets, offsets1, ds1):
                w.write('for (int {} = {}; {} &lt; {}; ++{}) {{'.format(
                    x, in_x0, x, in_x1, x), 'inc')
                w.write(
                    'int {} = {} * ({} + {});'.format(offset, d1, x, offset1))

            # Write main-part.
            offset = offsets[-1]
            for l in main(offset, xs).split('\n'):
                w.write(l)

            # Loop closings.
            for _ in xs:
                w.write('}', 'dec')

            return [w.get()]

        return bounds, _loop_main

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1569')" href="javascript:;">
chainer-7.2.0/chainer/functions/pooling/pooling_nd_kernel.py: 236-288
</a>
<div class="mid" id="frag1569" style="display:none"><pre>
    def _compile_loop(self, xs):
        # 2D: int out_x0_0 = max(0,     (x_0 - k_0 + s_0) / s_0);
        #     int out_x1_0 = min(out_0, (x_0       + s_0) / s_0);
        #     int out_x0_1 = max(0,     (x_1 - k_1 + s_1) / s_1);
        #     int out_x1_1 = min(out_1, (x_1       + s_1) / s_1);
        #     ... Before-part here ...
        #     for (int out_x_0 = out_x0_0; out_x_0 &lt; out_x1_0; ++out_x_0) {
        #       int offset_0 = out_1 * (out_x_0 + out_0 * c0);
        #       for (int out_x_1 = out_x0_1; out_x_1 &lt; out_x1_1; ++out_x_1) {
        #         int offset_1 = 1 * (out_x_1 + offset_0);
        #         ... Main-part here ...
        #       }
        #     }
        #     ... After-part here ...
        def aux(out_x0, out_x1, x, out, k, s):
            return [
                'int {} = max(0, ({} - {} + {}) / {});'.format(
                    out_x0, x, k, s, s),
                'int {} = min({}, ({} + {}) / {});'.format(
                    out_x1, out, x, s, s)]
        out_x0s = conv_nd_kernel.vars('out_x0', self.ndim)
        out_x1s = conv_nd_kernel.vars('out_x1', self.ndim)
        bounds = sum(conv_nd_kernel.map_(
            aux, out_x0s, out_x1s, xs, self.outs, self.ks, self.ss), [])

        def _loop_main(main):
            w = conv_nd_kernel.Writer()

            # Loop openings.
            out_xs = conv_nd_kernel.vars('out_x', self.ndim)
            offsets = conv_nd_kernel.vars('offset', self.ndim)
            outs1 = self.outs[1:] + [1]
            offsets1 = ['out_0 * c0'] + offsets[:-1]
            for out_x, out_x0, out_x1, offset, offset1, out1 in moves.zip(
                    out_xs, out_x0s, out_x1s, offsets, offsets1, outs1):
                w.write('for (int {} = {}; {} &lt; {}; ++{}) {{'.format(
                    out_x, out_x0, out_x, out_x1, out_x), 'inc')
                w.write('int {} = {} * ({} + {});'.format(
                    offset, out1, out_x, offset1))

            # Write main-part.
            offset = offsets[-1]
            for l in main(offset, xs, out_xs).split('\n'):
                w.write(l)

            # Loop closings.
            for _ in out_xs:
                w.write('}', 'dec')

            return [w.get()]

        return bounds, _loop_main

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 43:</b> &nbsp; 3 fragments, nominal size 13 lines, similarity 92%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1577')" href="javascript:;">
chainer-7.2.0/chainer/functions/math/erf.py: 26-40
</a>
<div class="mid" id="frag1577" style="display:none"><pre>
    def forward_cpu(self, x):
        global _erf_cpu
        if _erf_cpu is None:
            try:
                from scipy import special
                _erf_cpu = special.erf
            except ImportError:
                warnings.warn(
                    'SciPy is not available. Forward computation of erf in CPU'
                    ' can be slow without SciPy.',
                    chainer.warnings.PerformanceWarning)
                _erf_cpu = numpy.vectorize(math.erf)
        self.retain_inputs((0,))
        return utils.force_array(_erf_cpu(x[0]), dtype=x[0].dtype),

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2068')" href="javascript:;">
chainer-7.2.0/chainer/functions/math/ndtr.py: 31-45
</a>
<div class="mid" id="frag2068" style="display:none"><pre>
    def forward_cpu(self, x):
        global _ndtr_cpu
        if _ndtr_cpu is None:
            try:
                from scipy import special
                _ndtr_cpu = special.ndtr
            except ImportError:
                warnings.warn(
                    'SciPy is not available. Forward computation of ndtr in'
                    ' CPU can be slow without SciPy.',
                    chainer.warnings.PerformanceWarning)
                _ndtr_cpu = numpy.vectorize(_slow_ndtr_cpu)
        self.retain_inputs((0,))
        return utils.force_array(_ndtr_cpu(x[0]), dtype=x[0].dtype),

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2009')" href="javascript:;">
chainer-7.2.0/chainer/functions/math/erfc.py: 26-40
</a>
<div class="mid" id="frag2009" style="display:none"><pre>
    def forward_cpu(self, x):
        global _erfc_cpu
        if _erfc_cpu is None:
            try:
                from scipy import special
                _erfc_cpu = special.erfc
            except ImportError:
                warnings.warn(
                    'SciPy is not available. Forward computation of erfc in'
                    ' CPU can be slow without SciPy.',
                    chainer.warnings.PerformanceWarning)
                _erfc_cpu = numpy.vectorize(math.erfc)
        self.retain_inputs((0,))
        return utils.force_array(_erfc_cpu(x[0]), dtype=x[0].dtype),

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 44:</b> &nbsp; 5 fragments, nominal size 10 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1583')" href="javascript:;">
chainer-7.2.0/chainer/functions/math/log_ndtr.py: 23-35
</a>
<div class="mid" id="frag1583" style="display:none"><pre>
    def forward_cpu(self, x):
        global _log_ndtr_cpu
        if _log_ndtr_cpu is None:
            try:
                from scipy import special
                _log_ndtr_cpu = special.log_ndtr
            except ImportError:
                raise ImportError('SciPy is not available. Forward computation'
                                  ' of log_ndtr can not be done.')

        self.retain_inputs((0,))
        return utils.force_array(_log_ndtr_cpu(x[0]), dtype=x[0].dtype),

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2075')" href="javascript:;">
chainer-7.2.0/chainer/functions/math/zeta.py: 26-38
</a>
<div class="mid" id="frag2075" style="display:none"><pre>
    def forward_cpu(self, inputs):
        q, = inputs
        global _zeta_cpu
        if _zeta_cpu is None:
            try:
                from scipy import special
                _zeta_cpu = special.zeta
            except ImportError:
                raise ImportError('Scipy is not available. Forward computation'
                                  ' of zeta cannot be done.')
        self.retain_inputs((0,))
        return utils.force_array(_zeta_cpu(self._x, q), dtype=q.dtype),

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1759')" href="javascript:;">
chainer-7.2.0/chainer/functions/math/digamma.py: 22-33
</a>
<div class="mid" id="frag1759" style="display:none"><pre>
    def forward_cpu(self, x):
        global _digamma_cpu
        if _digamma_cpu is None:
            try:
                from scipy import special
                _digamma_cpu = special.digamma
            except ImportError:
                raise ImportError('SciPy is not available. Forward computation'
                                  ' of digamma can not be done.')
        self.retain_inputs((0,))
        return utils.force_array(_digamma_cpu(x[0]), dtype=x[0].dtype),

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1648')" href="javascript:;">
chainer-7.2.0/chainer/functions/math/polygamma.py: 25-37
</a>
<div class="mid" id="frag1648" style="display:none"><pre>
    def forward_cpu(self, inputs):
        n, x = inputs
        global _polygamma_cpu
        if _polygamma_cpu is None:
            try:
                from scipy import special
                _polygamma_cpu = special.polygamma
            except ImportError:
                raise ImportError('SciPy is not available. Forward computation'
                                  ' of polygamma can not be done.')
        self.retain_inputs((0, 1))
        return utils.force_array(_polygamma_cpu(n, x), dtype=x.dtype),

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1632')" href="javascript:;">
chainer-7.2.0/chainer/functions/math/lgamma.py: 20-31
</a>
<div class="mid" id="frag1632" style="display:none"><pre>
    def forward_cpu(self, x):
        global _lgamma_cpu
        if _lgamma_cpu is None:
            try:
                from scipy import special
                _lgamma_cpu = special.gammaln
            except ImportError:
                raise ImportError('SciPy is not available. Forward computation'
                                  ' of lgamma can not be done.')
        self.retain_inputs((0,))
        return utils.force_array(_lgamma_cpu(x[0]), dtype=x[0].dtype),

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 45:</b> &nbsp; 5 fragments, nominal size 13 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1637')" href="javascript:;">
chainer-7.2.0/chainer/functions/math/average.py: 31-48
</a>
<div class="mid" id="frag1637" style="display:none"><pre>
    def check_type_forward(self, in_types):
        type_check._argname(in_types, ('x',))
        type_check.expect(in_types[0].dtype.kind == 'f')

        if self.axis is not None:
            for axis in self.axis:
                if axis &gt;= 0:
                    type_check.expect(
                        axis &lt; in_types[0].ndim,
                    )
                else:
                    type_check.expect(
                        -axis - 1 &lt; in_types[0].ndim,
                    )

    # TODO(kataoka): override `forward_chainerx` if `chainerx.mean` does not
    # overflow for large float16 inputs

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1735')" href="javascript:;">
chainer-7.2.0/chainer/functions/math/prod.py: 31-45
</a>
<div class="mid" id="frag1735" style="display:none"><pre>
    def check_type_forward(self, in_types):
        type_check._argname(in_types, ('x',))
        type_check.expect(in_types[0].dtype.kind == 'f')

        if self.axis is not None:
            for axis in self.axis:
                if axis &gt;= 0:
                    type_check.expect(
                        axis &lt; in_types[0].ndim,
                    )
                else:
                    type_check.expect(
                        -axis - 1 &lt; in_types[0].ndim,
                    )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1896')" href="javascript:;">
chainer-7.2.0/chainer/functions/math/sum.py: 33-47
</a>
<div class="mid" id="frag1896" style="display:none"><pre>
    def check_type_forward(self, in_types):
        type_check._argname(in_types, ('x',))
        type_check.expect(in_types[0].dtype.kind == 'f')

        if self.axis is not None:
            for axis in self.axis:
                if axis &gt;= 0:
                    type_check.expect(
                        axis &lt; in_types[0].ndim,
                    )
                else:
                    type_check.expect(
                        -axis - 1 &lt; in_types[0].ndim,
                    )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1711')" href="javascript:;">
chainer-7.2.0/chainer/functions/math/logsumexp.py: 27-41
</a>
<div class="mid" id="frag1711" style="display:none"><pre>
    def check_type_forward(self, in_types):
        type_check._argname(in_types, ('x',))
        type_check.expect(in_types[0].dtype.kind == 'f')

        if self.axis is not None:
            for axis in self.axis:
                if axis &gt;= 0:
                    type_check.expect(
                        axis &lt; in_types[0].ndim,
                    )
                else:
                    type_check.expect(
                        -axis - 1 &lt; in_types[0].ndim,
                    )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1926')" href="javascript:;">
chainer-7.2.0/chainer/functions/math/minmax.py: 33-47
</a>
<div class="mid" id="frag1926" style="display:none"><pre>
    def check_type_forward(self, in_types):
        type_check._argname(in_types, ('x',))
        type_check.expect(in_types[0].dtype.kind == 'f')

        if self.axis is not None:
            for axis in self.axis:
                if axis &gt;= 0:
                    type_check.expect(
                        axis &lt; in_types[0].ndim,
                    )
                else:
                    type_check.expect(
                        -axis - 1 &lt; in_types[0].ndim,
                    )

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 46:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 91%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1639')" href="javascript:;">
chainer-7.2.0/chainer/functions/math/average.py: 62-75
</a>
<div class="mid" id="frag1639" style="display:none"><pre>
    def backward(self, indexes, grad_outputs):
        gy, = grad_outputs
        gy *= self.multiplier
        ndim = len(self.inputs[0].shape)
        if not (ndim == 0 or self.axis is None or self.keepdims):
            actual_axis = [
                axis if axis &gt;= 0 else axis + ndim
                for axis in self.axis]
            shape = list(gy.shape)
            for axis in sorted(actual_axis):
                shape.insert(axis, 1)
            gy = chainer.functions.reshape(gy, shape)
        return chainer.functions.broadcast_to(gy, self.inputs[0].shape),

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1899')" href="javascript:;">
chainer-7.2.0/chainer/functions/math/sum.py: 59-71
</a>
<div class="mid" id="frag1899" style="display:none"><pre>
    def backward(self, indexes, grad_outputs):
        gy, = grad_outputs
        ndim = len(self.inputs[0].shape)
        if not (ndim == 0 or self.axis is None or self.keepdims):
            actual_axis = [
                axis if axis &gt;= 0 else axis + ndim
                for axis in self.axis]
            shape = list(gy.shape)
            for axis in sorted(actual_axis):
                shape.insert(axis, 1)
            gy = chainer.functions.reshape(gy, shape)
        return chainer.functions.broadcast_to(gy, self.inputs[0].shape),

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 47:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1682')" href="javascript:;">
chainer-7.2.0/chainer/functions/math/trigonometric.py: 184-194
</a>
<div class="mid" id="frag1682" style="display:none"><pre>
    def forward_cpu(self, inputs):
        self.retain_inputs((0, 1))
        x, gy = inputs
        gx = utils.force_array(numpy.square(x))
        numpy.negative(gx, out=gx)
        gx += 1
        numpy.sqrt(gx, out=gx)
        numpy.reciprocal(gx, out=gx)
        gx *= gy
        return gx,

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1690')" href="javascript:;">
chainer-7.2.0/chainer/functions/math/trigonometric.py: 252-263
</a>
<div class="mid" id="frag1690" style="display:none"><pre>
    def forward_cpu(self, inputs):
        self.retain_inputs((0, 1))
        x, gy = inputs
        gx = utils.force_array(numpy.square(x))
        numpy.negative(gx, out=gx)
        gx += 1
        numpy.sqrt(gx, out=gx)
        numpy.reciprocal(gx, out=gx)
        numpy.negative(gx, out=gx)
        gx *= gy
        return gx,

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 48:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1733')" href="javascript:;">
chainer-7.2.0/chainer/functions/math/bias.py: 6-48
</a>
<div class="mid" id="frag1733" style="display:none"><pre>
def bias(x, y, axis=1):
    """Elementwise summation with broadcasting.

    Computes a elementwise summation of two input variables, with the shape of
    the latter variable broadcasted to match the shape of the former. ``axis``
    is the first axis of the first variable along which the second variable is
    applied.

    The term "broadcasting" here comes from Caffe's bias layer so the
    "broadcasting" with the following arguments::

           x : 100 x 3 x 40 x 5 x 6
           y : 3 x 40
        axis : 1

    is equivalent to the following numpy broadcasting::

        x : 100 x  3 x 40 x 5 x 6
        y :  (1 x) 3 x 40 x 1 x 1

    Note that the axis of ``x`` to which we apply ``y`` is specified by the
    argument ``axis``, whose meaning is different from numpy's ``axis``.

    Args:
        x (:class:`~chainer.Variable` or :ref:`ndarray`):
            Input variable to be summed.
        y (:class:`~chainer.Variable` or :ref:`ndarray`):
            Input variable to sum, broadcasted.
        axis (int): The first axis of ``x`` along which ``y`` is applied.

    Returns:
        ~chainer.Variable: Output variable.

    """
    x_shape = x.shape
    y_shape = y.shape
    if chainer.is_debug():
        assert x_shape[axis:axis + len(y_shape)] == y_shape
    y1_shape = tuple([1] * axis + list(y_shape) +
                     [1] * (len(x_shape) - axis - len(y_shape)))
    y1 = reshape.reshape(y, y1_shape)
    y2 = broadcast.broadcast_to(y1, x_shape)
    return x + y2
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2006')" href="javascript:;">
chainer-7.2.0/chainer/functions/math/scale.py: 6-48
</a>
<div class="mid" id="frag2006" style="display:none"><pre>
def scale(x, y, axis=1):
    """Elementwise product with broadcasting.

    Computes a elementwise product of two input variables, with the shape of
    the latter variable broadcasted to match the shape of the former. ``axis``
    is the first axis of the first variable along which the second variable is
    applied.

    The term "broadcasting" here comes from Caffe's scale layer so the
    "broadcasting" with the following arguments::

           x : 100 x 3 x 40 x 5 x 6
           y : 3 x 40
        axis : 1

    is equivalent to the following numpy broadcasting::

        x : 100 x  3 x 40 x 5 x 6
        y :  (1 x) 3 x 40 x 1 x 1

    Note that the axis of ``x`` to which we apply ``y`` is specified by the
    argument ``axis``, whose meaning is different from numpy's ``axis``.

    Args:
        x (:class:`~chainer.Variable` or :ref:`ndarray`):
            Input variable to be scaled.
        y (:class:`~chainer.Variable` or :ref:`ndarray`):
            Input variable to scale, broadcasted.
        axis (int): The first axis of ``x`` along which ``y`` is applied.

    Returns:
        ~chainer.Variable: Output variable.

    """
    x_shape = x.shape
    y_shape = y.shape
    if chainer.is_debug():
        assert x_shape[axis:axis + len(y_shape)] == y_shape
    y1_shape = tuple([1] * axis + list(y_shape) +
                     [1] * (len(x_shape) - axis - len(y_shape)))
    y1 = reshape.reshape(y, y1_shape)
    y2 = broadcast.broadcast_to(y1, x_shape)
    return x * y2
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 49:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1740')" href="javascript:;">
chainer-7.2.0/chainer/functions/math/linear_interpolate.py: 9-20
</a>
<div class="mid" id="frag1740" style="display:none"><pre>
    def check_type_forward(self, in_types):
        type_check._argname(in_types, ('p', 'x', 'y'))
        p_type, x_type, y_type = in_types

        type_check.expect(
            p_type.dtype.kind == 'f',
            x_type.dtype == p_type.dtype,
            y_type.dtype == p_type.dtype,
            p_type.shape == x_type.shape,
            p_type.shape == y_type.shape,
        )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2342')" href="javascript:;">
chainer-7.2.0/chainer/functions/activation/prelu.py: 70-81
</a>
<div class="mid" id="frag2342" style="display:none"><pre>
    def check_type_forward(self, in_types):
        type_check._argname(in_types, ('x', 'W', 'gy'))
        x_type, W_type, gy_type = in_types
        type_check.expect(
            x_type.dtype.kind == 'f',
            W_type.dtype == x_type.dtype,
            gy_type.dtype == x_type.dtype,
            x_type.ndim &gt;= W_type.ndim + 1,
            x_type.shape[1:1 + type_check.eval(W_type.ndim)] == W_type.shape,
            gy_type.shape == x_type.shape
        )

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 50:</b> &nbsp; 2 fragments, nominal size 21 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2040')" href="javascript:;">
chainer-7.2.0/chainer/functions/math/sparse_matmul.py: 165-185
</a>
<div class="mid" id="frag2040" style="display:none"><pre>
class CooMatMul(function_node.FunctionNode):

    def __init__(self, sp_row, sp_col, sp_shape, sp_order='other',
                 transa=False, transb=False, transc=False, dtype=None):
        if sp_row.ndim != sp_col.ndim:
            raise ValueError('ndim of sp_row and sp_col must be the same.')
        if sp_row.ndim != 1 and sp_row.ndim != 2:
            raise ValueError('ndim of sp_row and sp_col must be one or two.')
        for i in range(sp_row.ndim):
            if sp_row.shape[i] != sp_col.shape[i]:
                msg = 'shape of sp_row and sp_col must be the same.'
                raise ValueError(msg)
        if len(sp_shape) != 2:
            raise ValueError('len(sp_shape) must be two.')
        self.sp_row = sp_row  # ((nb,) ldnz)
        self.sp_col = sp_col  # ((nb,) ldnz)
        self.sp_shape = sp_shape  # (_m, _k) when transa is False
        self.sp_order = sp_order
        self.transa = transa
        self.transb = transb
        self.transc = transc
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2048')" href="javascript:;">
chainer-7.2.0/chainer/functions/math/sparse_matmul.py: 353-374
</a>
<div class="mid" id="frag2048" style="display:none"><pre>
class CooMatMulGradSP(function_node.FunctionNode):

    def __init__(self, sp_row, sp_col, sp_shape, sp_order='other',
                 transa=False, transb=False, transc=False,
                 dtype=None):
        if sp_row.ndim != sp_col.ndim:
            raise ValueError('ndim of sp_row and sp_col must be the same.')
        if sp_row.ndim != 1 and sp_row.ndim != 2:
            raise ValueError('ndim of sp_row and sp_col must be one or two.')
        for i in range(sp_row.ndim):
            if sp_row.shape[i] != sp_col.shape[i]:
                msg = 'shape of sp_row and sp_col must be the same.'
                raise ValueError(msg)
        if len(sp_shape) != 2:
            raise ValueError('len(sp_shape) must be two.')
        self.sp_row = sp_row  # ((nb,) ldnz)
        self.sp_col = sp_col  # ((nb,) ldnz)
        self.sp_shape = sp_shape  # (_m, _n) when transc is False
        self.sp_order = sp_order
        self.transa = transa
        self.transb = transb
        self.transc = transc
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 51:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 94%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2043')" href="javascript:;">
chainer-7.2.0/chainer/functions/math/sparse_matmul.py: 221-239
</a>
<div class="mid" id="frag2043" style="display:none"><pre>
        return utils.force_array(c, self.dtype),

    def backward(self, indexes, grad_outputs):
        sp, dn = self.get_retained_inputs()
        g_c, = grad_outputs
        ret = []
        if 0 in indexes:
            g_sp = CooMatMulGradSP(self.sp_row, self.sp_col, self.sp_shape,
                                   self.sp_order,
                                   self.transc, not self.transb, self.transa,
                                   dtype=sp.dtype).apply((g_c, dn))[0]
            ret.append(g_sp)
        if 1 in indexes:
            g_dn = CooMatMul(self.sp_row, self.sp_col, self.sp_shape,
                             self.sp_order,
                             not self.transa, self.transc, self.transb,
                             dtype=dn.dtype).apply((sp, g_c))[0]
            ret.append(g_dn)
        return ret
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2051')" href="javascript:;">
chainer-7.2.0/chainer/functions/math/sparse_matmul.py: 414-432
</a>
<div class="mid" id="frag2051" style="display:none"><pre>
        return utils.force_array(c),

    def backward(self, indexes, grad_outputs):
        a, b = self.get_retained_inputs()
        g_sp, = grad_outputs
        ret = []
        if 0 in indexes:
            g_a = CooMatMul(self.sp_row, self.sp_col, self.sp_shape,
                            self.sp_order,
                            self.transc, not self.transb, self.transa,
                            dtype=a.dtype).apply((g_sp, b))[0]
            ret.append(g_a)
        if 1 in indexes:
            g_b = CooMatMul(self.sp_row, self.sp_col, self.sp_shape,
                            self.sp_order,
                            not self.transc, self.transa, not self.transb,
                            dtype=b.dtype).apply((g_sp, a))[0]
            ret.append(g_b)
        return ret
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 52:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2104')" href="javascript:;">
chainer-7.2.0/chainer/functions/noise/dropout.py: 130-213
</a>
<div class="mid" id="frag2104" style="display:none"><pre>
def dropout(x, ratio=.5, **kwargs):
    """dropout(x, ratio=.5, *, mask=None, return_mask=False)

    Drops elements of input variable randomly.

    This function drops input elements randomly with probability ``ratio`` and
    scales the remaining elements by factor ``1 / (1 - ratio)``. In testing
    mode (i.e., ``chainer.config.train`` is set to ``False``), it does nothing
    and just returns ``x``.

    Args:
        x (:class:`~chainer.Variable` or :ref:`ndarray`):
            Input variable. A :math:`(s_1, s_2, ..., s_N)` -shaped float array.
        ratio (float):
            Dropout ratio. The ``ratio`` must be ``0.0 &lt;= ratio &lt; 1.0``.
        mask (:ref:`ndarray` or None):
            The mask to be used for dropout.
            You do not have to specify this value, unless you need to make
            results deterministic.
            If ``mask`` is not specified or set to ``None``, a mask will be
            generated randomly according to the given ``ratio``.
            If ``mask`` is specified, ``ratio`` will be ignored.
            The shape and dtype must be the same as ``x`` and should be on the
            same device.
            Note that iDeep and cuDNN will not be used for this function if
            mask is specified, as iDeep and cuDNN do not support it.
        return_mask (bool):
            If ``True``, the mask used for dropout is returned together with
            the output variable.
            The returned mask can later be reused by passing it to ``mask``
            argument.

    Returns:
        ~chainer.Variable or tuple:
            When ``return_mask`` is ``False`` (default), returns the output
            variable.
            When ``True``, returns the tuple of the output variable and
            mask (:ref:`ndarray`). The mask will be on the same device as the
            input. The mask will become ``None`` when ``chainer.config.train``
            is set to ``False``.

    See the paper by G. Hinton: `Improving neural networks by preventing
    co-adaptation of feature detectors &lt;https://arxiv.org/abs/1207.0580&gt;`_.

    .. admonition:: Example

        &gt;&gt;&gt; x = np.array([[-1, 0], [2, -3], [-2, 1]], np.float32)
        &gt;&gt;&gt; with chainer.using_config('train', True):
        ...     y = F.dropout(x)
        &gt;&gt;&gt; y.array
        array([[-2.,  0.],
               [ 4., -6.],
               [-0.,  2.]], dtype=float32)
        &gt;&gt;&gt; with chainer.using_config('train', True):
        ...     y = F.dropout(x, ratio=0.0) \
# dropout returns original input if ratio=0.0
        &gt;&gt;&gt; (x == y.array).all()
        True
        &gt;&gt;&gt; with chainer.using_config('train', False):
        ...     y = F.dropout(x) \
# dropout in test mode returns original input
        &gt;&gt;&gt; (x == y.array).all()
        True

    """
    mask = None
    return_mask = False
    if kwargs:
        mask, return_mask = argument.parse_kwargs(
            kwargs, ('mask', mask), ('return_mask', return_mask),
            train='train argument is not supported anymore. '
                  'Use chainer.using_config')

    if configuration.config.train:
        func = Dropout(ratio, mask, return_mask)
        out, = func.apply((x,))
        mask = func.mask
    else:
        out = chainer.as_variable(x)
        mask = None

    if return_mask:
        return out, mask
    return out
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2321')" href="javascript:;">
chainer-7.2.0/chainer/functions/activation/rrelu.py: 92-161
</a>
<div class="mid" id="frag2321" style="display:none"><pre>
def rrelu(x, l=1. / 8, u=1. / 3, **kwargs):
    """rrelu(x, l=1. / 8, u=1. / 3, *, r=None, return_r=False)

    Randomized Leaky Rectified Liner Unit function.

    This function is expressed as

    .. math:: f(x)=\\max(x, rx),

    where :math:`r` is a random number sampled from a uniform distribution
    :math:`U(l, u)`.

    .. note::

        The :math:`r` corresponds to :math:`a` in the original
        paper (https://arxiv.org/pdf/1505.00853.pdf).

    Args:
        x (:class:`~chainer.Variable` or :ref:`ndarray`):
            Input variable. A :math:`(s_1, s_2, ..., s_N)`-shaped float array.
        l (float): The lower bound of the uniform distribution.
        u (float): The upper bound of the uniform distribution.
        r (:ref:`ndarray` or None):
            The r to be used for rrelu.
            The shape and dtype must be the same as ``x[0]`` and should be on
            the same device.
            If ``r``  is not specified or set to ``None``, an ``r`` will be
            generated randomly according to the given ``l`` and ``u``.
            If ``r`` is specified, ``l`` and ``u`` will be ignored.
        return_r (bool):
            If ``True``, the r used for rrelu is returned altogether with
            the output variable.
            The returned ``r`` can latter be reused by passing it to ``r``
            argument.

    Returns:
        ~chainer.Variable or tuple:
            When ``return_r`` is ``False`` (default), return the output
            variable. Otherwise returnes the tuple of the output variable and
            ``r`` (:ref:`ndarray`). The ``r`` will be on the same device as
            the input.
            A :math:`(s_1, s_2, ..., s_N)`-shaped float array.

    .. admonition:: Example

        &gt;&gt;&gt; x = np.array([[-1, 0], [2, -3], [-2, 1]], np.float32)
        &gt;&gt;&gt; x
        array([[-1.,  0.],
               [ 2., -3.],
               [-2.,  1.]], dtype=float32)
        &gt;&gt;&gt; F.rrelu(x).array # doctest: +SKIP
        array([[-0.24850948,  0.        ],
               [ 2.        , -0.50844127],
               [-0.598535  ,  1.        ]], dtype=float32)
    """
    r = None
    return_r = False
    if kwargs:
        r, return_r = argument.parse_kwargs(
            kwargs, ('r', r), ('return_r', r),
            train='train argument is not supported anymore. '
                  'Use chainer.using_config')

    func = RReLU(l, u, r)
    out, = func.apply((x,))
    r = func.r

    if return_r:
        return out, r
    return out
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 53:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 84%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2148')" href="javascript:;">
chainer-7.2.0/chainer/functions/loss/hinge.py: 81-96
</a>
<div class="mid" id="frag2148" style="display:none"><pre>
    def backward_cpu(self, inputs, grad_outputs):
        t, gloss = inputs[1], grad_outputs[0]

        if self.reduce == 'mean':
            gloss /= len(t)

        self.bottom_diff[numpy.arange(len(t)), t] *= -1
        if self.norm == 'L1':
            gx = gloss * numpy.sign(self.bottom_diff)
        elif self.norm == 'L2':
            gx = 2 * gloss * self.bottom_diff
        else:
            raise NotImplementedError()

        return gx, None

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2149')" href="javascript:;">
chainer-7.2.0/chainer/functions/loss/hinge.py: 97-114
</a>
<div class="mid" id="frag2149" style="display:none"><pre>
    def backward_gpu(self, inputs, grad_outputs):
        xp = backend.get_array_module(*inputs)
        t, gloss = inputs[1], grad_outputs[0]

        if self.reduce == 'mean':
            gloss /= len(t)

        self.bottom_diff = _hinge_fwd_kernel()(t, self.bottom_diff)
        if self.norm == 'L1':
            gx = gloss * xp.sign(self.bottom_diff)
        elif self.norm == 'L2':
            gx = 2 * gloss * self.bottom_diff
        else:
            raise NotImplementedError()

        return gx, None


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 54:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2206')" href="javascript:;">
chainer-7.2.0/chainer/functions/loss/vae.py: 9-65
</a>
<div class="mid" id="frag2206" style="display:none"><pre>
def gaussian_kl_divergence(mean, ln_var, reduce='sum'):
    """Computes the KL-divergence of Gaussian variables from the standard one.

    Given two variable ``mean`` representing :math:`\\mu` and ``ln_var``
    representing :math:`\\log(\\sigma^2)`, this function calculates
    the KL-divergence in elementwise manner between the given multi-dimensional
    Gaussian :math:`N(\\mu, S)` and the standard Gaussian :math:`N(0, I)`

    .. math::

       D_{\\mathbf{KL}}(N(\\mu, S) \\| N(0, I)),

    where :math:`S` is a diagonal matrix such that :math:`S_{ii} = \\sigma_i^2`
    and :math:`I` is an identity matrix.

    The output is a variable whose value depends on the value of
    the option ``reduce``. If it is ``'no'``, it holds the elementwise
    loss values. If it is ``'sum'`` or ``'mean'``, loss values are summed up
    or averaged respectively.

    Args:
        mean (:class:`~chainer.Variable` or :ref:`ndarray`):
            A variable representing mean of given
            gaussian distribution, :math:`\\mu`.
        ln_var (:class:`~chainer.Variable` or :ref:`ndarray`):
            A variable representing logarithm of
            variance of given gaussian distribution, :math:`\\log(\\sigma^2)`.
        reduce (str): Reduction option. Its value must be either
            ``'sum'``, ``'mean'`` or ``'no'``. Otherwise, :class:`ValueError`
            is raised.

    Returns:
        ~chainer.Variable:
            A variable representing KL-divergence between
            given gaussian distribution and the standard gaussian.
            If ``reduce`` is ``'no'``, the output variable holds array
            whose shape is same as one of (hence both of) input variables.
            If it is ``'sum'`` or ``'mean'``, the output variable holds a
            scalar value.

    """
    if reduce not in ('sum', 'mean', 'no'):
        raise ValueError(
            'only \'sum\', \'mean\' and \'no\' are valid for \'reduce\', but '
            '\'%s\' is given' % reduce)

    var = exponential.exp(ln_var)
    mean_square = mean * mean
    loss = (mean_square + var - ln_var - 1) * 0.5
    if reduce == 'sum':
        return sum.sum(loss)
    elif reduce == 'mean':
        return average.average(loss)
    else:
        return loss


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2207')" href="javascript:;">
chainer-7.2.0/chainer/functions/loss/vae.py: 66-122
</a>
<div class="mid" id="frag2207" style="display:none"><pre>
def bernoulli_nll(x, y, reduce='sum'):
    """Computes the negative log-likelihood of a Bernoulli distribution.

    This function calculates the negative log-likelihood of a Bernoulli
    distribution.

    .. math::

        -\\log B(x; p) = -\\sum_i \\{x_i \\log(p_i) + \
        (1 - x_i)\\log(1 - p_i)\\},

    where :math:`p = \\sigma(y)`, :math:`\\sigma(\\cdot)` is a sigmoid
    function, and :math:`B(x; p)` is a Bernoulli distribution.


    The output is a variable whose value depends on the value of
    the option ``reduce``. If it is ``'no'``, it holds the elementwise
    loss values. If it is ``'sum'`` or ``'mean'``, loss values are summed up
    or averaged respectively.

    .. note::

       As this function uses a sigmoid function, you can pass a result of
       fully-connected layer (that means :class:`Linear`) to this function
       directly.

    Args:
        x (:class:`~chainer.Variable` or :ref:`ndarray`): Input variable.
        y (:class:`~chainer.Variable` or :ref:`ndarray`): A variable
            representing the parameter of Bernoulli distribution.
        reduce (str): Reduction option. Its value must be either
            ``'sum'``, ``'mean'`` or ``'no'``. Otherwise, :class:`ValueError`
            is raised.

    Returns:
        ~chainer.Variable:
            A variable representing the negative log-likelihood.
            If ``reduce`` is ``'no'``, the output variable holds array
            whose shape is same as one of (hence both of) input variables.
            If it is ``'sum'`` or ``'mean'``, the output variable holds a
            scalar value.

    """
    if reduce not in ('sum', 'mean', 'no'):
        raise ValueError(
            'only \'sum\', \'mean\' and \'no\' are valid for \'reduce\', but '
            '\'%s\' is given' % reduce)

    loss = softplus.softplus(y) - x * y
    if reduce == 'sum':
        return sum.sum(loss)
    elif reduce == 'mean':
        return average.average(loss)
    else:
        return loss


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 55:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2257')" href="javascript:;">
chainer-7.2.0/chainer/functions/activation/tanh.py: 71-83
</a>
<div class="mid" id="frag2257" style="display:none"><pre>
    def forward_gpu(self, inputs):
        self.retain_inputs((0, 1))
        y, gy = inputs
        if (chainer.should_use_cudnn('==always') and
                self.x is not None and gy.flags.c_contiguous):
            gx = cudnn.activation_backward(self.x, y, gy, _mode)
        else:
            gx = cuda.elementwise(
                'T y, T gy', 'T gx',
                'gx = gy * (1 - y * y)',
                'tanh_bwd')(y, gy)
        return gx,

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2364')" href="javascript:;">
chainer-7.2.0/chainer/functions/activation/sigmoid.py: 74-86
</a>
<div class="mid" id="frag2364" style="display:none"><pre>
    def forward_gpu(self, inputs):
        self.retain_inputs((0, 1))
        y, gy = inputs
        if (chainer.should_use_cudnn('==always') and gy.flags.c_contiguous and
                self.x is not None and self.x.flags.c_contiguous):
            gx = cudnn.activation_backward(self.x, y, gy, _mode)
        else:
            gx = cuda.elementwise(
                'T y, T gy', 'T gx',
                'gx = gy * y * (1 - y)',
                'sigmoid_bwd')(y, gy)
        return gx,

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 56:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 84%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2314')" href="javascript:;">
chainer-7.2.0/chainer/functions/activation/rrelu.py: 38-51
</a>
<div class="mid" id="frag2314" style="display:none"><pre>
    def forward_cpu(self, inputs):
        x, = inputs
        if chainer.config.train:
            if self.r is None:
                self.r = np.random.uniform(
                    self.lower, self.upper, x.shape
                ).astype(x.dtype, copy=False)
        else:
            self.r = np.full(
                x.shape, (self.lower + self.upper) / 2, dtype=x.dtype)
        y = np.where(x &gt;= 0, x, x * self.r)
        self.retain_outputs((0,))
        return y,

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2315')" href="javascript:;">
chainer-7.2.0/chainer/functions/activation/rrelu.py: 52-66
</a>
<div class="mid" id="frag2315" style="display:none"><pre>
    def forward_gpu(self, inputs):
        x, = inputs
        xp = cuda.cupy
        if chainer.config.train:
            if self.r is None:
                self.r = xp.random.uniform(
                    self.lower, self.upper, x.shape
                ).astype(x.dtype, copy=False)
        else:
            self.r = xp.full(
                x.shape, (self.lower + self.upper) / 2, dtype=x.dtype)
        y = _kern()(x, x, self.r)
        self.retain_outputs((0,))
        return y,

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 57:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2376')" href="javascript:;">
chainer-7.2.0/chainer/functions/evaluation/classification_summary.py: 26-43
</a>
<div class="mid" id="frag2376" style="display:none"><pre>
    def check_type_forward(self, in_types):
        type_check._argname(in_types, ('x', 't'))
        x_type, t_type = in_types

        type_check.expect(
            x_type.dtype.kind == 'f',
            t_type.dtype.kind == 'i'
        )

        t_ndim = type_check.eval(t_type.ndim)
        type_check.expect(
            x_type.ndim &gt;= t_type.ndim,
            x_type.shape[0] == t_type.shape[0],
            x_type.shape[2: t_ndim + 1] == t_type.shape[1:]
        )
        for i in six.moves.range(t_ndim + 1, type_check.eval(x_type.ndim)):
            type_check.expect(x_type.shape[i] == 1)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2388')" href="javascript:;">
chainer-7.2.0/chainer/functions/evaluation/accuracy.py: 15-32
</a>
<div class="mid" id="frag2388" style="display:none"><pre>
    def check_type_forward(self, in_types):
        type_check._argname(in_types, ('x', 't'))
        x_type, t_type = in_types

        type_check.expect(
            x_type.dtype.kind == 'f',
            t_type.dtype.kind == 'i'
        )

        t_ndim = type_check.eval(t_type.ndim)
        type_check.expect(
            x_type.ndim &gt;= t_type.ndim,
            x_type.shape[0] == t_type.shape[0],
            x_type.shape[2: t_ndim + 1] == t_type.shape[1:]
        )
        for i in six.moves.range(t_ndim + 1, type_check.eval(x_type.ndim)):
            type_check.expect(x_type.shape[i] == 1)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 58:</b> &nbsp; 3 fragments, nominal size 13 lines, similarity 85%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2426')" href="javascript:;">
chainer-7.2.0/chainer/functions/rnn/n_step_gru.py: 49-65
</a>
<div class="mid" id="frag2426" style="display:none"><pre>
def _combine_inputs(hx, ws, bs, xs, num_layers, directions):
    combined = []
    combined.append(hx)
    for x in xs:
        combined.append(x)

    for n in range(num_layers):
        for direction in range(directions):
            idx = directions * n + direction

            for i in range(6):
                combined.append(ws[idx][i])
            for i in range(6):
                combined.append(bs[idx][i])
    return combined


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2454')" href="javascript:;">
chainer-7.2.0/chainer/functions/rnn/n_step_rnn.py: 91-107
</a>
<div class="mid" id="frag2454" style="display:none"><pre>
def _combine_inputs(hx, ws, bs, xs, num_layers, directions):
    combined = []
    combined.append(hx)
    for x in xs:
        combined.append(x)

    for n in range(num_layers):
        for direction in range(directions):
            idx = directions * n + direction

            for i in range(2):
                combined.append(ws[idx][i])
            for i in range(2):
                combined.append(bs[idx][i])
    return combined


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2444')" href="javascript:;">
chainer-7.2.0/chainer/functions/rnn/n_step_lstm.py: 48-65
</a>
<div class="mid" id="frag2444" style="display:none"><pre>
def _combine_inputs(hx, cx, ws, bs, xs, num_layers, directions):
    combined = []
    combined.append(hx)
    combined.append(cx)
    for x in xs:
        combined.append(x)

    for n in range(num_layers):
        for direction in range(directions):
            idx = directions * n + direction

            for i in range(8):
                combined.append(ws[idx][i])
            for i in range(8):
                combined.append(bs[idx][i])
    return combined


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 59:</b> &nbsp; 3 fragments, nominal size 15 lines, similarity 87%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2427')" href="javascript:;">
chainer-7.2.0/chainer/functions/rnn/n_step_gru.py: 66-82
</a>
<div class="mid" id="frag2427" style="display:none"><pre>
def _seperate_inputs(combined, num_layers, seq_length, directions):
    hx = combined[0]
    xs = combined[1: 1 + seq_length]
    ws = []
    bs = []
    index = 1 + seq_length
    for n in range(num_layers):
        ws.append(combined[index: index + 6])
        bs.append(combined[index + 6: index + 12])
        index += 12
        if directions == 2:
            ws.append(combined[index: index + 6])
            bs.append(combined[index + 6: index + 12])
            index += 12
    return hx, ws, bs, xs


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2455')" href="javascript:;">
chainer-7.2.0/chainer/functions/rnn/n_step_rnn.py: 108-124
</a>
<div class="mid" id="frag2455" style="display:none"><pre>
def _seperate_inputs(combined, num_layers, seq_length, directions):
    hx = combined[0]
    xs = combined[1: 1 + seq_length]
    ws = []
    bs = []
    index = 1 + seq_length
    for n in range(num_layers):
        ws.append(combined[index: index + 2])
        bs.append(combined[index + 2: index + 4])
        index += 4
        if directions == 2:
            ws.append(combined[index: index + 2])
            bs.append(combined[index + 2: index + 4])
            index += 4
    return hx, ws, bs, xs


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2445')" href="javascript:;">
chainer-7.2.0/chainer/functions/rnn/n_step_lstm.py: 66-83
</a>
<div class="mid" id="frag2445" style="display:none"><pre>
def _seperate_inputs(combined, num_layers, seq_length, directions):
    hx = combined[0]
    cx = combined[1]
    xs = combined[2: 2 + seq_length]
    ws = []
    bs = []
    index = 2 + seq_length
    for n in range(num_layers):
        ws.append(combined[index: index + 8])
        bs.append(combined[index + 8: index + 16])
        index += 16
        if directions == 2:
            ws.append(combined[index: index + 8])
            bs.append(combined[index + 8: index + 16])
            index += 16
    return hx, cx, ws, bs, xs


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 60:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2519')" href="javascript:;">
chainer-7.2.0/chainer/utils/conv_nd_kernel.py: 98-112
</a>
<div class="mid" id="frag2519" style="display:none"><pre>
    def _compile_out_x(self, ndim, outs):
        # 2D: int out_x0 = i / (out_1) % out_0;
        #     int out_x1 = i % out_1;
        def aux(out_x, xs):
            head = xs[0]
            tail = xs[1:]
            if tail:
                return 'int {} = i / ({}) % {};'.format(
                    out_x, mulexp(tail), head)
            else:
                return 'int {} = i % {};'.format(out_x, head)
        out_xs = vars('out_x', ndim)
        out_x_decls = map_(aux, out_xs, succ_sublists(outs))
        return out_x_decls, out_xs

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2530')" href="javascript:;">
chainer-7.2.0/chainer/utils/conv_nd_kernel.py: 199-213
</a>
<div class="mid" id="frag2530" style="display:none"><pre>
    def _compile_x(self, ndim, ds):
        # 2D: int x_0 = i / (d_1) % d_0;
        #     int x_1 = i % d_1;
        def aux(x, ds):
            head = ds[0]
            tail = ds[1:]
            if tail:
                return 'int {} = i / ({}) % {};'.format(
                    x, mulexp(tail), head)
            else:
                return 'int {} = i % {};'.format(x, head)
        xs = vars('x', ndim)
        x_decls = map_(aux, xs, succ_sublists(ds))
        return x_decls, xs

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 61:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 91%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2524')" href="javascript:;">
chainer-7.2.0/chainer/utils/conv_nd_kernel.py: 155-168
</a>
<div class="mid" id="frag2524" style="display:none"><pre>
    def _generate(self, ndim):
        ds = vars('d', ndim)
        outs = vars('out', ndim)
        ks = vars('k', ndim)
        ss = vars('s', ndim)
        ps = vars('p', ndim)
        dilate = vars('di', ndim)

        in_params = self._in_params(ds, outs, ks, ss, ps, dilate)
        out_params = self._out_params()
        operation = self._operation(ndim, ds, outs, ks, ss, ps, dilate)
        name = name = 'im2col_{}d'.format(ndim)
        return in_params, out_params, operation, name

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2537')" href="javascript:;">
chainer-7.2.0/chainer/utils/conv_nd_kernel.py: 277-290
</a>
<div class="mid" id="frag2537" style="display:none"><pre>
    def _generate(self, ndim):
        ds = vars('d', ndim)
        outs = vars('out', ndim)
        ks = vars('k', ndim)
        ss = vars('s', ndim)
        ps = vars('p', ndim)
        dilate = vars('di', ndim)

        in_params = self._in_params(ds, outs, ks, ss, ps, dilate)
        out_params = self._out_params()
        operation = self._operation(ndim, ds, outs, ks, ss, ps, dilate)
        name = 'col2im_{}d'.format(ndim)
        return in_params, out_params, operation, name

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 62:</b> &nbsp; 3 fragments, nominal size 10 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2582')" href="javascript:;">
chainer-7.2.0/chainer/datasets/kuzushiji_mnist.py: 30-79
</a>
<div class="mid" id="frag2582" style="display:none"><pre>
def get_kuzushiji_mnist(withlabel=True, ndim=1, scale=1., dtype=None,
                        label_dtype=numpy.int32, rgb_format=False):
    """Gets the Kuzushiji-MNIST dataset.

    `Kuzushiji-MNIST (KMNIST) &lt;http://codh.rois.ac.jp/kmnist/&gt;`_ is a set of
    hand-written Japanese characters represented by grey-scale 28x28 images.
    In the original images, each pixel is represented by one-byte unsigned
    integer. This function scales the pixels to floating point values in the
    interval ``[0, scale]``.

    This function returns the training set and the test set of the official
    KMNIST dataset. If ``withlabel`` is ``True``, each dataset consists of
    tuples of images and labels, otherwise it only consists of images.

    Args:
        withlabel (bool): If ``True``, it returns datasets with labels. In this
            case, each example is a tuple of an image and a label. Otherwise,
            the datasets only contain images.
        ndim (int): Number of dimensions of each image. The shape of each image
            is determined depending on ``ndim`` as follows:

            - ``ndim == 1``: the shape is ``(784,)``
            - ``ndim == 2``: the shape is ``(28, 28)``
            - ``ndim == 3``: the shape is ``(1, 28, 28)``

        scale (float): Pixel value scale. If it is 1 (default), pixels are
            scaled to the interval ``[0, 1]``.
        dtype: Data type of resulting image arrays. ``chainer.config.dtype`` is
            used by default (see :ref:`configuration`).
        label_dtype: Data type of the labels.
        rgb_format (bool): if ``ndim == 3`` and ``rgb_format`` is ``True``, the
            image will be converted to rgb format by duplicating the channels
            so the image shape is (3, 28, 28). Default is ``False``.

    Returns:
        A tuple of two datasets. If ``withlabel`` is ``True``, both datasets
        are :class:`~chainer.datasets.TupleDataset` instances. Otherwise, both
        datasets are arrays of images.

    """
    dtype = chainer.get_dtype(dtype)
    train_raw = _retrieve_kuzushiji_mnist_training()
    train = preprocess_mnist(train_raw, withlabel, ndim, scale, dtype,
                             label_dtype, rgb_format)
    test_raw = _retrieve_kuzushiji_mnist_test()
    test = preprocess_mnist(test_raw, withlabel, ndim, scale, dtype,
                            label_dtype, rgb_format)
    return train, test


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2636')" href="javascript:;">
chainer-7.2.0/chainer/datasets/mnist.py: 11-59
</a>
<div class="mid" id="frag2636" style="display:none"><pre>
def get_mnist(withlabel=True, ndim=1, scale=1., dtype=None,
              label_dtype=numpy.int32, rgb_format=False):
    """Gets the MNIST dataset.

    `MNIST &lt;http://yann.lecun.com/exdb/mnist/&gt;`_ is a set of hand-written
    digits represented by grey-scale 28x28 images. In the original images, each
    pixel is represented by one-byte unsigned integer. This function
    scales the pixels to floating point values in the interval ``[0, scale]``.

    This function returns the training set and the test set of the official
    MNIST dataset. If ``withlabel`` is ``True``, each dataset consists of
    tuples of images and labels, otherwise it only consists of images.

    Args:
        withlabel (bool): If ``True``, it returns datasets with labels. In this
            case, each example is a tuple of an image and a label. Otherwise,
            the datasets only contain images.
        ndim (int): Number of dimensions of each image. The shape of each image
            is determined depending on ``ndim`` as follows:

            - ``ndim == 1``: the shape is ``(784,)``
            - ``ndim == 2``: the shape is ``(28, 28)``
            - ``ndim == 3``: the shape is ``(1, 28, 28)``

        scale (float): Pixel value scale. If it is 1 (default), pixels are
            scaled to the interval ``[0, 1]``.
        dtype: Data type of resulting image arrays. ``chainer.config.dtype`` is
            used by default (see :ref:`configuration`).
        label_dtype: Data type of the labels.
        rgb_format (bool): if ``ndim == 3`` and ``rgb_format`` is ``True``, the
            image will be converted to rgb format by duplicating the channels
            so the image shape is (3, 28, 28). Default is ``False``.

    Returns:
        A tuple of two datasets. If ``withlabel`` is ``True``, both datasets
        are :class:`~chainer.datasets.TupleDataset` instances. Otherwise, both
        datasets are arrays of images.

    """
    dtype = chainer.get_dtype(dtype)
    train_raw = _retrieve_mnist_training()
    train = preprocess_mnist(train_raw, withlabel, ndim, scale, dtype,
                             label_dtype, rgb_format)
    test_raw = _retrieve_mnist_test()
    test = preprocess_mnist(test_raw, withlabel, ndim, scale, dtype,
                            label_dtype, rgb_format)
    return train, test


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2648')" href="javascript:;">
chainer-7.2.0/chainer/datasets/fashion_mnist.py: 25-75
</a>
<div class="mid" id="frag2648" style="display:none"><pre>
def get_fashion_mnist(withlabel=True, ndim=1, scale=1., dtype=None,
                      label_dtype=numpy.int32, rgb_format=False):
    """Gets the Fashion-MNIST dataset.

    `Fashion-MNIST &lt;https://github.com/zalandoresearch/fashion-mnist/&gt;`_ is a
    set of fashion articles represented by grey-scale 28x28 images. In the
    original images, each pixel is represented by one-byte unsigned integer.
    This function scales the pixels to floating point values in the interval
    ``[0, scale]``.

    This function returns the training set and the test set of the official
    Fashion-MNIST dataset. If ``withlabel`` is ``True``, each dataset consists
    of tuples of images and labels, otherwise it only consists of images.

    Args:
        withlabel (bool): If ``True``, it returns datasets with labels. In this
            case, each example is a tuple of an image and a label. Otherwise,
            the datasets only contain images.
        ndim (int): Number of dimensions of each image. The shape of each image
            is determined depending on ``ndim`` as follows:

            - ``ndim == 1``: the shape is ``(784,)``
            - ``ndim == 2``: the shape is ``(28, 28)``
            - ``ndim == 3``: the shape is ``(1, 28, 28)``

        scale (float): Pixel value scale. If it is 1 (default), pixels are
            scaled to the interval ``[0, 1]``.
        dtype: Data type of resulting image arrays. ``chainer.config.dtype`` is
            used by default (see :ref:`configuration`).
        label_dtype: Data type of the labels.
        rgb_format (bool): if ``ndim == 3`` and ``rgb_format`` is ``True``, the
            image will be converted to rgb format by duplicating the channels
            so the image shape is (3, 28, 28). Default is ``False``.

    Returns:
        A tuple of two datasets. If ``withlabel`` is ``True``, both datasets
        are :class:`~chainer.datasets.TupleDataset` instances. Otherwise, both
        datasets are arrays of images.

    """
    train_raw = _retrieve_fashion_mnist_training()
    dtype = chainer.get_dtype(dtype)

    train = preprocess_mnist(train_raw, withlabel, ndim, scale, dtype,
                             label_dtype, rgb_format)
    test_raw = _retrieve_fashion_mnist_test()
    test = preprocess_mnist(test_raw, withlabel, ndim, scale, dtype,
                            label_dtype, rgb_format)
    return train, test


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 63:</b> &nbsp; 3 fragments, nominal size 11 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2734')" href="javascript:;">
chainer-7.2.0/chainer/optimizers/adam.py: 480-491
</a>
<div class="mid" id="frag2734" style="display:none"><pre>
    def __init__(self,
                 alpha=_default_hyperparam.alpha,
                 beta1=_default_hyperparam.beta1,
                 beta2=_default_hyperparam.beta2,
                 eps=_default_hyperparam.eps,
                 eta=_default_hyperparam.eta,
                 weight_decay_rate=_default_hyperparam.weight_decay_rate):
        super(AdamW, self).__init__(
            alpha=alpha, beta1=beta1, beta2=beta2, eps=eps, eta=eta,
            weight_decay_rate=weight_decay_rate)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2736')" href="javascript:;">
chainer-7.2.0/chainer/optimizers/adam.py: 539-551
</a>
<div class="mid" id="frag2736" style="display:none"><pre>
    def __init__(self,
                 alpha=_default_hyperparam.alpha,
                 beta1=_default_hyperparam.beta1,
                 beta2=_default_hyperparam.beta2,
                 final_lr=_default_hyperparam.final_lr,
                 gamma=_default_hyperparam.gamma,
                 eps=_default_hyperparam.eps,
                 eta=_default_hyperparam.eta):
        super(AdaBound, self).__init__(
            alpha=alpha, beta1=beta1, beta2=beta2, eps=eps, eta=eta,
            amsgrad=False, adabound=True, final_lr=final_lr, gamma=gamma)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2737')" href="javascript:;">
chainer-7.2.0/chainer/optimizers/adam.py: 571-581
</a>
<div class="mid" id="frag2737" style="display:none"><pre>
    def __init__(self,
                 alpha=_default_hyperparam.alpha,
                 beta1=_default_hyperparam.beta1,
                 beta2=_default_hyperparam.beta2,
                 final_lr=_default_hyperparam.final_lr,
                 gamma=_default_hyperparam.gamma,
                 eps=_default_hyperparam.eps,
                 eta=_default_hyperparam.eta):
        super(AMSBound, self).__init__(
            alpha=alpha, beta1=beta1, beta2=beta2, eps=eps, eta=eta,
            amsgrad=True, adabound=True, final_lr=final_lr, gamma=gamma)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 64:</b> &nbsp; 5 fragments, nominal size 13 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2741')" href="javascript:;">
chainer-7.2.0/chainer/optimizers/ada_grad.py: 68-82
</a>
<div class="mid" id="frag2741" style="display:none"><pre>
    def update_core_gpu(self, param):
        grad = param.grad
        if grad is None:
            return
        if AdaGradRule._kernel is None:
            AdaGradRule._kernel = cuda.elementwise(
                'T grad, T lr, T eps',
                'T param, T h',
                '''h += grad * grad;
                   param -= lr * grad / (sqrt(h) + eps);''',
                'adagrad')
        AdaGradRule._kernel(grad, self.hyperparam.lr, self.hyperparam.eps,
                            param.data, self.state['h'])


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2776')" href="javascript:;">
chainer-7.2.0/chainer/optimizers/nesterov_ag.py: 67-84
</a>
<div class="mid" id="frag2776" style="display:none"><pre>
    def update_core_gpu(self, param):
        grad = param.grad
        if grad is None:
            return
        if NesterovAGRule._kernel is None:
            NesterovAGRule._kernel = cuda.elementwise(
                'T grad, T lr, T momentum',
                'T param, T v',
                '''
                v = v * momentum - lr * grad;
                param += momentum * momentum * v - (1 + momentum) * lr * grad;
                ''',
                'nesterov_ag')
        NesterovAGRule._kernel(
            grad, self.hyperparam.lr, self.hyperparam.momentum,
            param.data, self.state['v'])


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2782')" href="javascript:;">
chainer-7.2.0/chainer/optimizers/smorms3.py: 74-95
</a>
<div class="mid" id="frag2782" style="display:none"><pre>

    def update_core_gpu(self, param):
        grad = param.grad
        if grad is None:
            return
        if SMORMS3Rule._kernel is None:
            SMORMS3Rule._kernel = cuda.elementwise(
                'T grad, T lr, T eps',
                'T param, T mem, T g, T g2',
                '''T r, x;
                   r = 1 / (mem + 1);
                   g = (1 - r) * g + r * grad;
                   g2 = (1 - r) * g2 + r * grad * grad;
                   x = g * g / (g2 + eps);
                   param -= grad * min(lr, x) / (sqrt(g2) + eps);
                   mem = 1 + mem * (1 - x)
                   ''',
                'smorms3')
        SMORMS3Rule._kernel(
            grad, self.hyperparam.lr, self.hyperparam.eps, param.data,
            self.state['mem'], self.state['g'], self.state['g2'])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2753')" href="javascript:;">
chainer-7.2.0/chainer/optimizers/rmsprop_graves.py: 87-106
</a>
<div class="mid" id="frag2753" style="display:none"><pre>
    def update_core_gpu(self, param):
        grad = param.grad
        if grad is None:
            return
        hp = self.hyperparam
        if RMSpropGravesRule._kernel is None:
            RMSpropGravesRule._kernel = cuda.elementwise(
                'T grad, T lr, T alpha, T momentum, T eps',
                'T param, T avg_n, T avg_g, T delta',
                '''avg_n = alpha * avg_n + (1 - alpha) * grad * grad;
                   avg_g = alpha * avg_g + (1 - alpha) * grad;
                   delta = delta * momentum -
                       lr * grad * rsqrt(avg_n - avg_g * avg_g + eps);
                   param += delta;''',
                'rmsprop_graves')
        RMSpropGravesRule._kernel(
            grad, hp.lr, hp.alpha, hp.momentum, hp.eps, param.data,
            self.state['n'], self.state['g'], self.state['delta'])


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2770')" href="javascript:;">
chainer-7.2.0/chainer/optimizers/ada_delta.py: 74-91
</a>
<div class="mid" id="frag2770" style="display:none"><pre>
    def update_core_gpu(self, param):
        grad = param.grad
        if grad is None:
            return
        if AdaDeltaRule._kernel is None:
            AdaDeltaRule._kernel = cuda.elementwise(
                'T grad, T one_minus_rho, T eps',
                'T param, T msg, T msdx',
                '''msg   = msg + one_minus_rho * (grad * grad - msg);
                   T dx  = sqrt((msdx + eps) / (msg + eps)) * grad;
                   msdx  += one_minus_rho * (dx * dx - msdx);
                   param -= dx;''',
                'adadelta')
        AdaDeltaRule._kernel(
            grad, 1 - self.hyperparam.rho, self.hyperparam.eps, param.data,
            self.state['msg'], self.state['msdx'])


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 65:</b> &nbsp; 3 fragments, nominal size 12 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2750')" href="javascript:;">
chainer-7.2.0/chainer/optimizers/rmsprop_graves.py: 52-64
</a>
<div class="mid" id="frag2750" style="display:none"><pre>
    def __init__(self, parent_hyperparam=None,
                 lr=None, alpha=None, momentum=None, eps=None):
        super(RMSpropGravesRule, self).__init__(
            parent_hyperparam or _default_hyperparam)
        if lr is not None:
            self.hyperparam.lr = lr
        if alpha is not None:
            self.hyperparam.alpha = alpha
        if momentum is not None:
            self.hyperparam.momentum = momentum
        if eps is not None:
            self.hyperparam.eps = eps

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2785')" href="javascript:;">
chainer-7.2.0/chainer/optimizers/msvag.py: 59-74
</a>
<div class="mid" id="frag2785" style="display:none"><pre>
    def __init__(self, parent_hyperparam=None,
                 lr=None, beta=None,
                 eta=None, weight_decay_rate=None):
        super(MSVAGRule, self).__init__(
            parent_hyperparam or _default_hyperparam)
        if lr is not None:
            self.hyperparam.lr = lr
        if beta is not None:
            self.hyperparam.beta = beta
        if eta is not None:
            self.hyperparam.eta = eta
        if weight_decay_rate is not None:
            self.hyperparam.weight_decay_rate = weight_decay_rate

        self.beta_power = self.hyperparam.beta

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2756')" href="javascript:;">
chainer-7.2.0/chainer/optimizers/rmsprop.py: 55-67
</a>
<div class="mid" id="frag2756" style="display:none"><pre>
    def __init__(self, parent_hyperparam=None, lr=None, alpha=None, eps=None,
                 eps_inside_sqrt=None):
        super(RMSpropRule, self).__init__(
            parent_hyperparam or _default_hyperparam)
        if lr is not None:
            self.hyperparam.lr = lr
        if alpha is not None:
            self.hyperparam.alpha = alpha
        if eps is not None:
            self.hyperparam.eps = eps
        if eps_inside_sqrt is not None:
            self.hyperparam.eps_inside_sqrt = eps_inside_sqrt

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 66:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2821')" href="javascript:;">
chainer-7.2.0/chainer/iterators/multithread_iterator.py: 45-70
</a>
<div class="mid" id="frag2821" style="display:none"><pre>
    def __init__(self, dataset, batch_size, repeat=True, shuffle=None,
                 n_threads=1, order_sampler=None):
        self.dataset = dataset
        self.batch_size = batch_size
        self._repeat = repeat
        self._shuffle = shuffle

        if self._shuffle is not None:
            if order_sampler is not None:
                raise ValueError('`shuffle` is not `None` and a custom '
                                 '`order_sampler` is set. Please set '
                                 '`shuffle` to `None` to use the custom '
                                 'order sampler.')
            else:
                if self._shuffle:
                    order_sampler = ShuffleOrderSampler()
        else:
            if order_sampler is None:
                order_sampler = ShuffleOrderSampler()
        self.order_sampler = order_sampler

        self.n_threads = n_threads
        self._pool = None

        self.reset()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2890')" href="javascript:;">
chainer-7.2.0/chainer/iterators/serial_iterator.py: 46-68
</a>
<div class="mid" id="frag2890" style="display:none"><pre>
    def __init__(self, dataset, batch_size,
                 repeat=True, shuffle=None, order_sampler=None):
        self.dataset = dataset
        self.batch_size = batch_size
        self._repeat = repeat
        self._shuffle = shuffle

        if self._shuffle is not None:
            if order_sampler is not None:
                raise ValueError('`shuffle` is not `None` and a custom '
                                 '`order_sampler` is set. Please set '
                                 '`shuffle` to `None` to use the custom '
                                 'order sampler.')
            else:
                if self._shuffle:
                    order_sampler = ShuffleOrderSampler()
        else:
            if order_sampler is None:
                order_sampler = ShuffleOrderSampler()
        self.order_sampler = order_sampler

        self.reset()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 67:</b> &nbsp; 3 fragments, nominal size 14 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2841')" href="javascript:;">
chainer-7.2.0/chainer/iterators/dali_iterator.py: 69-85
</a>
<div class="mid" id="frag2841" style="display:none"><pre>
    def serialize(self, serializer):
        self.current_position = serializer('current_position',
                                           self.current_position)
        self.epoch = serializer('epoch', self.epoch)
        self.is_new_epoch = serializer('is_new_epoch', self.is_new_epoch)
        try:
            self._previous_epoch_detail = serializer(
                'previous_epoch_detail', self._previous_epoch_detail)
        except KeyError:
            # guess previous_epoch_detail for older version
            self._previous_epoch_detail = self.epoch + \
                (self.current_position - self.batch_size) / self.epoch_size
            if self.epoch_detail &gt; 0:
                self._previous_epoch_detail = max(
                    self._previous_epoch_detail, 0.)
            else:
                self._previous_epoch_detail = -1.
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10495')" href="javascript:;">
chainer-7.2.0/examples/static_graph_optimizations/ptb/train_ptb_custom_loop.py: 160-176
</a>
<div class="mid" id="frag10495" style="display:none"><pre>
    def serialize(self, serializer):
        # It is important to serialize the state to be recovered on resume.
        self.iteration = serializer('iteration', self.iteration)
        self.epoch = serializer('epoch', self.epoch)
        try:
            self._previous_epoch_detail = serializer(
                'previous_epoch_detail', self._previous_epoch_detail)
        except KeyError:
            # guess previous_epoch_detail for older version
            self._previous_epoch_detail = self.epoch + \
                (self.current_position - self.batch_size) / len(self.dataset)
            if self.epoch_detail &gt; 0:
                self._previous_epoch_detail = max(
                    self._previous_epoch_detail, 0.)
            else:
                self._previous_epoch_detail = -1.

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10319')" href="javascript:;">
chainer-7.2.0/examples/ptb/train_ptb.py: 122-139
</a>
<div class="mid" id="frag10319" style="display:none"><pre>
    def serialize(self, serializer):
        # It is important to serialize the state to be recovered on resume.
        self.iteration = serializer('iteration', self.iteration)
        self.epoch = serializer('epoch', self.epoch)
        try:
            self._previous_epoch_detail = serializer(
                'previous_epoch_detail', self._previous_epoch_detail)
        except KeyError:
            # guess previous_epoch_detail for older version
            self._previous_epoch_detail = self.epoch + \
                (self.current_position - self.batch_size) / len(self.dataset)
            if self.epoch_detail &gt; 0:
                self._previous_epoch_detail = max(
                    self._previous_epoch_detail, 0.)
            else:
                self._previous_epoch_detail = -1.


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 68:</b> &nbsp; 2 fragments, nominal size 22 lines, similarity 82%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2855')" href="javascript:;">
chainer-7.2.0/chainer/iterators/multiprocess_iterator.py: 216-238
</a>
<div class="mid" id="frag2855" style="display:none"><pre>
    def serialize(self, serializer):
        current_position = serializer('current_position',
                                      self.current_position)
        epoch = serializer('epoch', self.epoch)
        is_new_epoch = serializer('is_new_epoch', self.is_new_epoch)
        order = self._state.order.copy()
        try:
            serializer('order', order)
        except KeyError:
            serializer('_order', order)
        self._reset_state(current_position, epoch, is_new_epoch, order)
        try:
            self._previous_epoch_detail = serializer(
                'previous_epoch_detail', self._previous_epoch_detail)
        except KeyError:
            # guess previous_epoch_detail for older version
            self._previous_epoch_detail = self.epoch + \
                (self.current_position - self.batch_size) / self._epoch_size
            if self.epoch_detail &gt; 0:
                self._previous_epoch_detail = max(
                    self._previous_epoch_detail, 0.)
            else:
                self._previous_epoch_detail = -1.
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2897')" href="javascript:;">
chainer-7.2.0/chainer/iterators/serial_iterator.py: 105-129
</a>
<div class="mid" id="frag2897" style="display:none"><pre>
    def serialize(self, serializer):
        current_position = serializer('current_position',
                                      self.current_position)
        epoch = serializer('epoch', self.epoch)
        is_new_epoch = serializer('is_new_epoch', self.is_new_epoch)
        order = self._state.order
        if order is not None:
            try:
                serializer('order', order)
            except KeyError:
                serializer('_order', order)
        self._state = _statemachine.IteratorState(
            current_position, epoch, is_new_epoch, order)
        try:
            self._previous_epoch_detail = serializer(
                'previous_epoch_detail', self._previous_epoch_detail)
        except KeyError:
            # guess previous_epoch_detail for older version
            self._previous_epoch_detail = self.epoch + \
                (self.current_position - self.batch_size) / self._epoch_size
            if self.epoch_detail &gt; 0:
                self._previous_epoch_detail = max(
                    self._previous_epoch_detail, 0.)
            else:
                self._previous_epoch_detail = -1.
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 69:</b> &nbsp; 2 fragments, nominal size 38 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3021')" href="javascript:;">
chainer-7.2.0/chainer/testing/function_link.py: 224-267
</a>
<div class="mid" id="frag3021" style="display:none"><pre>
    def run_test_backward(self, backend_config):
        # Runs the backward test.

        if self.skip_backward_test:
            raise unittest.SkipTest('skip_backward_test is set')

        # avoid cyclic import
        from chainer import gradient_check

        self.backend_config = backend_config
        self.test_name = 'test_backward'
        self.before_test(self.test_name)

        def f(*args):
            return self._forward(args, backend_config)

        def do_check():
            inputs = self._generate_inputs()
            outputs = self._forward_expected(inputs)
            grad_outputs = self._generate_grad_outputs(outputs)

            inputs = backend_config.get_array(inputs)
            grad_outputs = backend_config.get_array(grad_outputs)
            inputs = self._to_noncontiguous_as_needed(inputs)
            grad_outputs = self._to_noncontiguous_as_needed(grad_outputs)

            with FunctionTestError.raise_if_fail(
                    'backward is not implemented correctly'):
                gradient_check.check_backward(
                    f, inputs, grad_outputs, dtype=self.numerical_grad_dtype,
                    detect_nondifferentiable=self.dodge_nondifferentiable,
                    **self.check_backward_options)

        if self.dodge_nondifferentiable:
            while True:
                try:
                    do_check()
                except gradient_check.NondifferentiableError:
                    continue
                else:
                    break
        else:
            do_check()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3024')" href="javascript:;">
chainer-7.2.0/chainer/testing/function_link.py: 268-326
</a>
<div class="mid" id="frag3024" style="display:none"><pre>
    def run_test_double_backward(self, backend_config):
        # Runs the double-backward test.

        if self.skip_double_backward_test:
            raise unittest.SkipTest('skip_double_backward_test is set')

        # avoid cyclic import
        from chainer import gradient_check

        self.backend_config = backend_config
        self.test_name = 'test_double_backward'
        self.before_test(self.test_name)

        def f(*args):
            return self._forward(args, backend_config)

        def do_check():
            inputs = self._generate_inputs()
            outputs = self._forward_expected(inputs)
            grad_outputs = self._generate_grad_outputs(outputs)
            grad_grad_inputs = self._generate_grad_grad_inputs(inputs)

            # Drop ggx corresponding to non-differentiable inputs.
            # Generated `grad_grad_inputs`, the upstream gradients for the
            # double backward test, may contain `None` for omitted gradients.
            # These must be propagated to the gradient check.
            grad_grad_inputs = [
                ggx for ggx in grad_grad_inputs
                if (ggx is None or ggx.dtype.kind == 'f')]

            inputs = backend_config.get_array(inputs)
            grad_outputs = backend_config.get_array(grad_outputs)
            grad_grad_inputs = backend_config.get_array(grad_grad_inputs)
            inputs = self._to_noncontiguous_as_needed(inputs)
            grad_outputs = self._to_noncontiguous_as_needed(grad_outputs)
            grad_grad_inputs = (
                self._to_noncontiguous_as_needed(grad_grad_inputs))

            with backend_config:
                with FunctionTestError.raise_if_fail(
                        'double backward is not implemented correctly'):
                    gradient_check.check_double_backward(
                        f, inputs, grad_outputs, grad_grad_inputs,
                        dtype=self.numerical_grad_dtype,
                        detect_nondifferentiable=self.dodge_nondifferentiable,
                        **self.check_double_backward_options)

        if self.dodge_nondifferentiable:
            while True:
                try:
                    do_check()
                except gradient_check.NondifferentiableError:
                    continue
                else:
                    break
        else:
            do_check()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 70:</b> &nbsp; 2 fragments, nominal size 26 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3268')" href="javascript:;">
chainer-7.2.0/chainer/links/connection/dilated_convolution_2d.py: 94-120
</a>
<div class="mid" id="frag3268" style="display:none"><pre>
    def __init__(self, in_channels, out_channels, ksize=None, stride=1, pad=0,
                 dilate=1, nobias=False, initialW=None, initial_bias=None):
        super(DilatedConvolution2D, self).__init__()

        if ksize is None:
            out_channels, ksize, in_channels = in_channels, out_channels, None

        self.ksize = ksize
        self.stride = _pair(stride)
        self.pad = _pair(pad)
        self.dilate = _pair(dilate)
        self.out_channels = out_channels

        with self.init_scope():
            W_initializer = initializers._get_initializer(initialW)
            self.W = variable.Parameter(W_initializer)
            if in_channels is not None:
                self._initialize_params(in_channels)

            if nobias:
                self.b = None
            else:
                if initial_bias is None:
                    initial_bias = 0
                initial_bias = initializers._get_initializer(initial_bias)
                self.b = variable.Parameter(initial_bias, out_channels)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3307')" href="javascript:;">
chainer-7.2.0/chainer/links/connection/deconvolution_2d.py: 129-166
</a>
<div class="mid" id="frag3307" style="display:none"><pre>
    def __init__(self, in_channels, out_channels, ksize=None, stride=1, pad=0,
                 nobias=False, outsize=None, initialW=None, initial_bias=None,
                 **kwargs):
        super(Deconvolution2D, self).__init__()

        dilate, groups, = argument.parse_kwargs(
            kwargs, ('dilate', 1), ('groups', 1),
            deterministic='deterministic argument is not supported anymore. '
            'Use chainer.using_config(\'cudnn_deterministic\', value) '
            'context where value is either `True` or `False`.')

        if ksize is None:
            out_channels, ksize, in_channels = in_channels, out_channels, None

        self.ksize = ksize
        self.stride = _pair(stride)
        self.pad = _pair(pad)
        self.dilate = _pair(dilate)
        self.outsize = (None, None) if outsize is None else outsize
        self.out_channels = out_channels
        self.groups = int(groups)

        with self.init_scope():
            W_initializer = initializers._get_initializer(initialW)
            self.W = variable.Parameter(W_initializer)
            if in_channels is not None:
                self._initialize_params(in_channels)

            if nobias:
                self.b = None
            else:
                if isinstance(initial_bias, (numpy.ndarray, cuda.ndarray)):
                    assert initial_bias.shape == (out_channels,)
                if initial_bias is None:
                    initial_bias = 0
                bias_initializer = initializers._get_initializer(initial_bias)
                self.b = variable.Parameter(bias_initializer, out_channels)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 71:</b> &nbsp; 2 fragments, nominal size 22 lines, similarity 95%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3281')" href="javascript:;">
chainer-7.2.0/chainer/links/connection/deformable_convolution_2d.py: 88-115
</a>
<div class="mid" id="frag3281" style="display:none"><pre>
    def __init__(self, in_channels, out_channels, ksize, stride=1, pad=0,
                 nobias=False, initialW=None, initial_bias=None):
        super(DeformableConvolution2DSampler, self).__init__()

        self.ksize = ksize
        self.stride = _pair(stride)
        self.pad = _pair(pad)
        self.out_channels = out_channels
        self.initialW = initialW

        if initialW is None:
            initialW = constant.Zero()

        with self.init_scope():
            W_initializer = initializers._get_initializer(initialW)
            self.W = variable.Parameter(W_initializer)

            if nobias:
                self.b = None
            else:
                if initial_bias is None:
                    initial_bias = initializers.Constant(0)
                bias_initializer = initializers._get_initializer(initial_bias)
                self.b = variable.Parameter(bias_initializer)

        if in_channels is not None:
            self._initialize_params(in_channels)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3311')" href="javascript:;">
chainer-7.2.0/chainer/links/connection/depthwise_convolution_2d.py: 45-71
</a>
<div class="mid" id="frag3311" style="display:none"><pre>
    def __init__(self, in_channels, channel_multiplier, ksize, stride=1, pad=0,
                 nobias=False, initialW=None, initial_bias=None):
        super(DepthwiseConvolution2D, self).__init__()
        self.ksize = ksize
        self.stride = _pair(stride)
        self.pad = _pair(pad)
        self.channel_multiplier = channel_multiplier
        self.nobias = nobias

        if initialW is None:
            initialW = initializers.HeNormal(1. / numpy.sqrt(2))

        with self.init_scope():
            W_initializer = initializers._get_initializer(initialW)
            self.W = variable.Parameter(W_initializer)

            if nobias:
                self.b = None
            else:
                if initial_bias is None:
                    initial_bias = initializers.Constant(0)
                bias_initializer = initializers._get_initializer(initial_bias)
                self.b = variable.Parameter(bias_initializer)

        if in_channels is not None:
            self._initialize_params(in_channels)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 72:</b> &nbsp; 2 fragments, nominal size 19 lines, similarity 94%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3366')" href="javascript:;">
chainer-7.2.0/chainer/links/caffe/caffe_function.py: 243-265
</a>
<div class="mid" id="frag3366" style="display:none"><pre>
    def _setup_convolution(self, layer):
        blobs = layer.blobs
        param = layer.convolution_param
        ksize = _get_ksize(param)
        stride = _get_stride(param)
        pad = _get_pad(param)
        num = _get_num(blobs[0])
        channels = _get_channels(blobs[0])
        bias_term = param.bias_term

        n_in = channels * param.group
        n_out = num

        func = convolution_2d.Convolution2D(
            n_in, n_out, ksize, stride, pad, nobias=not bias_term,
            initialW=_ConvolutionBlob(blobs[0], param.group),
            initial_bias=_Blob(blobs[1]) if bias_term else None)

        with self.init_scope():
            setattr(self, layer.name, func)
        self.forwards[layer.name] = _CallChildLink(self, layer.name)
        self._add_layer(layer)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3367')" href="javascript:;">
chainer-7.2.0/chainer/links/caffe/caffe_function.py: 267-289
</a>
<div class="mid" id="frag3367" style="display:none"><pre>
    def _setup_deconvolution(self, layer):
        blobs = layer.blobs
        param = layer.convolution_param
        ksize = _get_ksize(param)
        stride = _get_stride(param)
        pad = _get_pad(param)
        num = _get_num(blobs[0])
        channels = _get_channels(blobs[0])
        bias_term = param.bias_term

        n_in = num
        n_out = channels * param.group

        func = deconvolution_2d.Deconvolution2D(
            n_in, n_out, ksize, stride, pad, nobias=not bias_term,
            initialW=_ConvolutionBlob(blobs[0], param.group),
            initial_bias=_Blob(blobs[1]) if bias_term else None)

        with self.init_scope():
            setattr(self, layer.name, func)
        self.forwards[layer.name] = _CallChildLink(self, layer.name)
        self._add_layer(layer)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 73:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3385')" href="javascript:;">
chainer-7.2.0/chainer/links/caffe/caffe_function.py: 541-553
</a>
<div class="mid" id="frag3385" style="display:none"><pre>
def _get_stride(param):
    if param.stride_h &gt; 0:
        return param.stride_h, param.stride_w
    elif type(param.stride) == int:
        return param.stride
    elif len(param.stride) == 0:
        return 1
    elif len(param.stride) == 1:
        return param.stride[0]
    else:
        return param.stride


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3386')" href="javascript:;">
chainer-7.2.0/chainer/links/caffe/caffe_function.py: 554-566
</a>
<div class="mid" id="frag3386" style="display:none"><pre>
def _get_pad(param):
    if param.pad_h &gt; 0 or param.pad_w &gt; 0:
        return param.pad_h, param.pad_w
    elif type(param.pad) == int:
        return param.pad
    elif len(param.pad) == 0:
        return 0
    elif len(param.pad) == 1:
        return param.pad[0]
    else:
        return param.pad


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 74:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3389')" href="javascript:;">
chainer-7.2.0/chainer/links/caffe/caffe_function.py: 581-593
</a>
<div class="mid" id="frag3389" style="display:none"><pre>
def _get_height(blob):
    if blob.height &gt; 0:
        return blob.height
    elif len(blob.shape.dim) == 2:
        return blob.shape.dim[0]
    elif len(blob.shape.dim) == 4:
        return blob.shape.dim[2]
    else:
        raise RuntimeError(
            '{}-dimensional array is not supported'.format(
                len(blob.shape.dim)))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3390')" href="javascript:;">
chainer-7.2.0/chainer/links/caffe/caffe_function.py: 594-609
</a>
<div class="mid" id="frag3390" style="display:none"><pre>
def _get_width(blob):
    if blob.width &gt; 0:
        return blob.width
    elif len(blob.shape.dim) == 2:
        return blob.shape.dim[1]
    elif len(blob.shape.dim) == 4:
        return blob.shape.dim[3]
    else:
        raise RuntimeError(
            '{}-dimensional array is not supported'.format(
                len(blob.shape.dim)))


# Internal class
# __call__ must return Variable or tuple

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 75:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 94%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3455')" href="javascript:;">
chainer-7.2.0/chainerx_cc/examples/mnist_py/train_mnist.py: 51-74
</a>
<div class="mid" id="frag3455" style="display:none"><pre>
def evaluate(model, X_test, Y_test, eval_size, batch_size):
    N_test = X_test.shape[0] if eval_size is None else eval_size

    if N_test &gt; X_test.shape[0]:
        raise ValueError(
            'Test size can be no larger than {}'.format(X_test.shape[0]))

    with chx.no_backprop_mode():
        total_loss = chx.array(0, dtype=chx.float32)
        num_correct = chx.array(0, dtype=chx.int64)
        for i in range(0, N_test, batch_size):
            x = X_test[i:min(i + batch_size, N_test)]
            t = Y_test[i:min(i + batch_size, N_test)]

            y = model.forward(x)
            total_loss += chx.softmax_cross_entropy(y, t).sum()
            num_correct += (y.argmax(axis=1).astype(t.dtype)
                            == t).astype(chx.int32).sum()

    mean_loss = float(total_loss) / N_test
    accuracy = int(num_correct) / N_test
    return mean_loss, accuracy


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3459')" href="javascript:;">
chainer-7.2.0/chainerx_cc/examples/imagenet_py/train_imagenet.py: 29-52
</a>
<div class="mid" id="frag3459" style="display:none"><pre>
def evaluate(model, X_test, Y_test, eval_size, batch_size):
    N_test = X_test.shape[0] if eval_size is None else eval_size

    if N_test &gt; X_test.shape[0]:
        raise ValueError(
            'Test size can be no larger than {}'.format(X_test.shape[0]))

    with chx.no_backprop_mode():
        total_loss = chx.array(0, dtype=chx.float32)
        num_correct = chx.array(0, dtype=chx.int64)
        for i in range(0, N_test, batch_size):
            x = X_test[i:min(i + batch_size, N_test)]
            t = Y_test[i:min(i + batch_size, N_test)]

            y = model(x)
            total_loss += chx.softmax_cross_entropy(y, t).sum()
            num_correct += (y.argmax(axis=1).astype(t.dtype)
                            == t).astype(chx.int32).sum()

    mean_loss = float(total_loss) / N_test
    accuracy = int(num_correct) / N_test
    return mean_loss, accuracy


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 76:</b> &nbsp; 3 fragments, nominal size 18 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3498')" href="javascript:;">
chainer-7.2.0/chainerx_cc/examples/imagenet_py/image_dataset.py: 18-45
</a>
<div class="mid" id="frag3498" style="display:none"><pre>
    def get_example(self, i):
        # It reads the i-th image/label pair and return a preprocessed image.
        # It applies following preprocesses:
        #     - Cropping (random or center rectangular)
        #     - Random flip
        #     - Scaling to [0, 1] value
        crop_size = self.crop_size

        image, label = self.base[i]
        _, h, w = image.shape

        if self.random:
            # Randomly crop a region and flip the image
            top = random.randint(0, h - crop_size - 1)
            left = random.randint(0, w - crop_size - 1)
            if random.randint(0, 1):
                image = image[:, :, ::-1]
        else:
            # Crop the center
            top = (h - crop_size) // 2
            left = (w - crop_size) // 2
        bottom = top + crop_size
        right = left + crop_size

        image = image[:, top:bottom, left:right]
        image -= self.mean[:, top:bottom, left:right]
        image *= (1.0 / 255.0)  # Scale to [0, 1]
        return image, label
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10436')" href="javascript:;">
chainer-7.2.0/examples/chainermn/imagenet/train_imagenet.py: 49-80
</a>
<div class="mid" id="frag10436" style="display:none"><pre>
    def get_example(self, i):
        # It reads the i-th image/label pair and return a preprocessed image.
        # It applies following preprocesses:
        #     - Cropping (random or center rectangular)
        #     - Random flip
        #     - Scaling to [0, 1] value
        crop_size = self.crop_size

        image, label = self.base[i]
        _, h, w = image.shape

        if self.random:
            # Randomly crop a region and flip the image
            top = random.randint(0, h - crop_size - 1)
            left = random.randint(0, w - crop_size - 1)
            if random.randint(0, 1):
                image = image[:, :, ::-1]
        else:
            # Crop the center
            top = (h - crop_size) // 2
            left = (w - crop_size) // 2
        bottom = top + crop_size
        right = left + crop_size

        image = image[:, top:bottom, left:right]
        image -= self.mean[:, top:bottom, left:right]
        image *= (1.0 / 255.0)  # Scale to [0, 1]
        return image, label


# chainermn.create_multi_node_evaluator can be also used with user customized
# evaluator classes that inherit chainer.training.extensions.Evaluator.
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10576')" href="javascript:;">
chainer-7.2.0/examples/imagenet/dataset_util.py: 19-46
</a>
<div class="mid" id="frag10576" style="display:none"><pre>
    def get_example(self, i):
        # It reads the i-th image/label pair and return a preprocessed image.
        # It applies following preprocesses:
        #     - Cropping (random or center rectangular)
        #     - Random flip
        #     - Scaling to [0, 1] value
        crop_size = self.crop_size

        image, label = self.base[i]
        _, h, w = image.shape

        if self.random:
            # Randomly crop a region and flip the image
            top = random.randint(0, h - crop_size - 1)
            left = random.randint(0, w - crop_size - 1)
            if random.randint(0, 1):
                image = image[:, :, ::-1]
        else:
            # Crop the center
            top = (h - crop_size) // 2
            left = (w - crop_size) // 2
        bottom = top + crop_size
        right = left + crop_size

        image = image[:, top:bottom, left:right]
        image -= self.mean[:, top:bottom, left:right]
        image *= (1.0 / 255.0)  # Scale to [0, 1]
        return image, label
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 77:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3553')" href="javascript:;">
chainer-7.2.0/tests/onnx_chainer_tests/functions_tests/test_normalizations.py: 28-45
</a>
<div class="mid" id="frag3553" style="display:none"><pre>
    def setUp(self):

        class Model(chainer.Chain):

            def __init__(self, ops, args, input_argname):
                super(Model, self).__init__()
                self.ops = ops
                self.args = args
                self.input_argname = input_argname

            def __call__(self, x):
                self.args[self.input_argname] = x
                return self.ops(**self.args)

        ops = getattr(F, self.name)
        self.model = Model(ops, self.args, self.input_argname)
        self.x = input_generator.increasing(2, 5, 3, 3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3594')" href="javascript:;">
chainer-7.2.0/tests/onnx_chainer_tests/functions_tests/test_arrays.py: 349-369
</a>
<div class="mid" id="frag3594" style="display:none"><pre>
    def setUp(self):

        class Model(chainer.Chain):

            def __init__(self, ops, args, input_argname):
                super(Model, self).__init__()
                self.ops = ops
                self.args = args
                self.input_argname = input_argname

            def __call__(self, x):
                self.args[self.input_argname] = x
                return self.ops(**self.args)

        # (batch, channel, height, width) = (1, 1, 2, 2)
        self.x = np.array([[[[64, 32], [64, 32]]]], np.float32)

        # 2x upsampling
        args = {'output_shape': (4, 4)}
        self.model = Model(F.resize_images, args, 'x')

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 78:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3557')" href="javascript:;">
chainer-7.2.0/tests/onnx_chainer_tests/functions_tests/test_normalizations.py: 64-78
</a>
<div class="mid" id="frag3557" style="display:none"><pre>
    def setUp(self):

        class Model(chainer.Chain):

            def __init__(self, **kwargs):
                super(Model, self).__init__()
                with self.init_scope():
                    self.bn = L.BatchNormalization(5, **kwargs)

            def __call__(self, x):
                return self.bn(x)

        self.model = Model(**self.kwargs)
        self.x = input_generator.increasing(2, 5)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3622')" href="javascript:;">
chainer-7.2.0/tests/onnx_chainer_tests/functions_tests/test_activations.py: 59-73
</a>
<div class="mid" id="frag3622" style="display:none"><pre>
    def setUp(self):

        class Model(chainer.Chain):

            def __init__(self):
                super(Model, self).__init__()
                with self.init_scope():
                    self.prelu = L.PReLU()

            def __call__(self, x):
                return self.prelu(x)

        self.model = Model()
        self.x = input_generator.increasing(2, 5)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 79:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3655')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link_hook.py: 188-202
</a>
<div class="mid" id="frag3655" style="display:none"><pre>
    def test_global_hook_delete(self):
        # Deleted hook should not be called

        model, x, dot = self._create_model_and_data()
        hook = MyLinkHook()

        with hook:
            pass
        model(chainer.Variable(x), 'foo', test2='bar')

        assert len(hook.added_args) == 1
        assert len(hook.deleted_args) == 1
        assert len(hook.forward_preprocess_args) == 0
        assert len(hook.forward_postprocess_args) == 0

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3656')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link_hook.py: 203-218
</a>
<div class="mid" id="frag3656" style="display:none"><pre>
    def test_local_hook_delete(self):
        # Deleted hook should not be called

        model, x, dot = self._create_model_and_data()
        hook = MyLinkHook()

        model.add_hook(hook)
        model.delete_hook('MyLinkHook')
        model(chainer.Variable(x), 'foo', test2='bar')

        assert len(hook.added_args) == 1
        assert len(hook.deleted_args) == 1
        assert len(hook.forward_preprocess_args) == 0
        assert len(hook.forward_postprocess_args) == 0


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 80:</b> &nbsp; 3 fragments, nominal size 16 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3663')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/optimizer_hooks_tests/test_weight_decay.py: 37-58
</a>
<div class="mid" id="frag3663" style="display:none"><pre>
    def check_weight_decay(self, backend_configs):
        target = self.target
        assert len(backend_configs) == len(list(target.params()))
        devices = [bc.device for bc in backend_configs]

        decay = 0.2

        # Compute expected
        expects = []
        for param, device in zip(target.params(), devices):
            expects.append(param.array - param.grad - decay * param.array)
            param.to_device(device)

        opt = optimizers.SGD(lr=1)
        opt.setup(self.target)
        opt.add_hook(optimizer_hooks.WeightDecay(decay))
        opt.update()

        # Validate
        for expect, param in zip(expects, target.params()):
            testing.assert_allclose(expect, param.array)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3680')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/optimizer_hooks_tests/test_gradient_hard_clipping.py: 35-59
</a>
<div class="mid" id="frag3680" style="display:none"><pre>
    def check_hardclipping(self, backend_configs):
        target = self.target
        assert len(backend_configs) == len(list(target.params()))
        devices = [bc.device for bc in backend_configs]

        lower_bound = -0.9
        upper_bound = 1.1
        expects = []
        # Compute expected
        for param, device in zip(target.params(), devices):
            expects.append(param.array - np.clip(param.grad,
                                                 lower_bound, upper_bound))
            param.to_device(device)

        # Apply optimizer_hook
        opt = optimizers.SGD(lr=1)
        opt.setup(self.target)
        opt.add_hook(
            optimizer_hooks.GradientHardClipping(lower_bound, upper_bound))
        opt.update()

        # Validate
        for expect, param in zip(expects, target.params()):
            testing.assert_allclose(expect, param.array)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3677')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/optimizer_hooks_tests/test_lasso.py: 34-57
</a>
<div class="mid" id="frag3677" style="display:none"><pre>
    def check_lasso(self, backend_configs):
        target = self.target
        assert len(backend_configs) == len(list(target.params()))
        devices = [bc.device for bc in backend_configs]

        decay = 0.2

        expects = []
        # Compute expected
        for param, device in zip(target.params(), devices):
            expects.append(param.array - param.grad -
                           decay * np.sign(param.array))
            param.to_device(device)

        # Compute using optimizer_hook
        opt = optimizers.SGD(lr=1)
        opt.setup(target)
        opt.add_hook(optimizer_hooks.Lasso(decay))
        opt.update()

        # Validate
        for expect, param in zip(expects, target.params()):
            testing.assert_allclose(expect, param.array)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 81:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3691')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/caffe_tests/test_caffe_function.py: 166-184
</a>
<div class="mid" id="frag3691" style="display:none"><pre>
    def test_convolution(self):
        self.init_func()
        self.assertEqual(len(self.func.layers), 1)
        f = self.func.l1
        self.assertIsInstance(f, links.Convolution2D)
        for i in range(3):  # 3 == group
            in_slice = slice(i * 4, (i + 1) * 4)  # 4 == channels
            out_slice = slice(i * 2, (i + 1) * 2)  # 2 == num / group
            w = f.W.data[out_slice, in_slice]
            numpy.testing.assert_array_equal(
                w.flatten(), range(i * 32, (i + 1) * 32))

        numpy.testing.assert_array_equal(
            f.b.data, range(6))

        self.call(['x'], ['y'])
        self.mock.assert_called_once_with(self.inputs[0])


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3692')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/caffe_tests/test_caffe_function.py: 219-237
</a>
<div class="mid" id="frag3692" style="display:none"><pre>
    def test_deconvolution(self):
        self.init_func()
        self.assertEqual(len(self.func.layers), 1)
        f = self.func.l1
        self.assertIsInstance(f, links.Deconvolution2D)
        for i in range(3):  # 3 == group
            in_slice = slice(i * 4, (i + 1) * 4)  # 4 == channels
            out_slice = slice(i * 2, (i + 1) * 2)  # 2 == num / group
            w = f.W.data[out_slice, in_slice]
            numpy.testing.assert_array_equal(
                w.flatten(), range(i * 32, (i + 1) * 32))

        numpy.testing.assert_array_equal(
            f.b.data, range(12))

        self.call(['x'], ['y'])
        self.mock.assert_called_once_with(self.inputs[0])


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 82:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3777')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/rnn_tests/test_link_peephole.py: 171-184
</a>
<div class="mid" id="frag3777" style="display:none"><pre>
    def check_to_cpu(self, c, h):
        self.link.c = c
        self.link.h = h
        with testing.assert_warns(DeprecationWarning):
            self.link.to_cpu()
        self.assertIs(self.link.xp, numpy)
        self.assertIsInstance(self.link.c.data, self.link.xp.ndarray)
        self.assertIsInstance(self.link.h.data, self.link.xp.ndarray)
        with testing.assert_warns(DeprecationWarning):
            self.link.to_cpu()
        self.assertIs(self.link.xp, numpy)
        self.assertIsInstance(self.link.c.data, self.link.xp.ndarray)
        self.assertIsInstance(self.link.h.data, self.link.xp.ndarray)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4093')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_zoneoutlstm.py: 174-187
</a>
<div class="mid" id="frag4093" style="display:none"><pre>

    def check_to_cpu(self, c, h):
        self.link.c = c
        self.link.h = h
        with testing.assert_warns(DeprecationWarning):
            self.link.to_cpu()
        self.assertIs(self.link.xp, numpy)
        self.assertIsInstance(self.link.c.data, self.link.xp.ndarray)
        self.assertIsInstance(self.link.h.data, self.link.xp.ndarray)
        with testing.assert_warns(DeprecationWarning):
            self.link.to_cpu()
        self.assertIs(self.link.xp, numpy)
        self.assertIsInstance(self.link.c.data, self.link.xp.ndarray)
        self.assertIsInstance(self.link.h.data, self.link.xp.ndarray)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 83:</b> &nbsp; 2 fragments, nominal size 23 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3780')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/rnn_tests/test_link_peephole.py: 194-217
</a>
<div class="mid" id="frag3780" style="display:none"><pre>
    def check_to_cpu_to_gpu(self, c, h):
        self.link.c = c
        self.link.h = h
        with testing.assert_warns(DeprecationWarning):
            self.link.to_gpu()
        self.assertIs(self.link.xp, cuda.cupy)
        self.assertIsInstance(self.link.c.data, self.link.xp.ndarray)
        self.assertIsInstance(self.link.h.data, self.link.xp.ndarray)
        with testing.assert_warns(DeprecationWarning):
            self.link.to_gpu()
        self.assertIs(self.link.xp, cuda.cupy)
        self.assertIsInstance(self.link.c.data, self.link.xp.ndarray)
        self.assertIsInstance(self.link.h.data, self.link.xp.ndarray)
        with testing.assert_warns(DeprecationWarning):
            self.link.to_cpu()
        self.assertIs(self.link.xp, numpy)
        self.assertIsInstance(self.link.c.data, self.link.xp.ndarray)
        self.assertIsInstance(self.link.h.data, self.link.xp.ndarray)
        with testing.assert_warns(DeprecationWarning):
            self.link.to_gpu()
        self.assertIs(self.link.xp, cuda.cupy)
        self.assertIsInstance(self.link.c.data, self.link.xp.ndarray)
        self.assertIsInstance(self.link.h.data, self.link.xp.ndarray)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4096')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_zoneoutlstm.py: 197-220
</a>
<div class="mid" id="frag4096" style="display:none"><pre>

    def check_to_cpu_to_gpu(self, c, h):
        self.link.c = c
        self.link.h = h
        with testing.assert_warns(DeprecationWarning):
            self.link.to_gpu()
        self.assertIs(self.link.xp, cuda.cupy)
        self.assertIsInstance(self.link.c.data, self.link.xp.ndarray)
        self.assertIsInstance(self.link.h.data, self.link.xp.ndarray)
        with testing.assert_warns(DeprecationWarning):
            self.link.to_gpu()
        self.assertIs(self.link.xp, cuda.cupy)
        self.assertIsInstance(self.link.c.data, self.link.xp.ndarray)
        self.assertIsInstance(self.link.h.data, self.link.xp.ndarray)
        with testing.assert_warns(DeprecationWarning):
            self.link.to_cpu()
        self.assertIs(self.link.xp, numpy)
        self.assertIsInstance(self.link.c.data, self.link.xp.ndarray)
        self.assertIsInstance(self.link.h.data, self.link.xp.ndarray)
        with testing.assert_warns(DeprecationWarning):
            self.link.to_gpu()
        self.assertIs(self.link.xp, cuda.cupy)
        self.assertIsInstance(self.link.c.data, self.link.xp.ndarray)
        self.assertIsInstance(self.link.h.data, self.link.xp.ndarray)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 84:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3793')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/rnn_tests/test_link_lstm.py: 31-46
</a>
<div class="mid" id="frag3793" style="display:none"><pre>
    def setUp(self):
        if self.input_none:
            if self.input_omit:
                self.link = links.LSTM(self.out_size)
            else:
                self.link = links.LSTM(None, self.out_size)
        else:
            self.link = links.LSTM(self.in_size, self.out_size)
        self.link.cleargrads()
        x1_shape = (4, self.in_size)
        self.x1 = numpy.random.uniform(-1, 1, x1_shape).astype(numpy.float32)
        x2_shape = (3, self.in_size)
        self.x2 = numpy.random.uniform(-1, 1, x2_shape).astype(numpy.float32)
        x3_shape = (0, self.in_size)
        self.x3 = numpy.random.uniform(-1, 1, x3_shape).astype(numpy.float32)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3822')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/rnn_tests/test_link_lstm.py: 294-306
</a>
<div class="mid" id="frag3822" style="display:none"><pre>
class TestStatelessLSTM(unittest.TestCase):

    def setUp(self):
        if self.input_none:
            if self.input_omit:
                self.link = links.StatelessLSTM(self.out_size)
            else:
                self.link = links.StatelessLSTM(None, self.out_size)
        else:
            self.link = links.StatelessLSTM(self.in_size, self.out_size)
        self.link.cleargrads()

        x_shape = (4, self.in_size)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 85:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3852')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/normalization_tests/test_batch_renormalization.py: 159-177
</a>
<div class="mid" id="frag3852" style="display:none"><pre>
    def check_statistics2(self, x, y):
        x = chainer.Variable(x)
        y = chainer.Variable(y)
        self.link(x, finetune=True)
        self.link(y, finetune=True)
        mean = (self.x.sum(axis=0) + self.y.sum(axis=0)) / (self.nx + self.ny)
        var = (self.x.var(axis=0) * self.nx +
               self.y.var(axis=0) * self.ny) / (self.nx + self.ny)

        # TODO(Kenta Oono)
        # Fix the estimate of the unbiased variance.
        # Unbiased variance should be (nx + ny) / (nx + ny - 1) times of
        # the variance.
        # But the multiplier is ny / (ny - 1) in current implementation
        # these two values are different when nx is not equal to ny.
        unbiased_var = var * self.ny / (self.ny - 1)
        testing.assert_allclose(self.link.avg_mean, mean)
        testing.assert_allclose(self.link.avg_var, unbiased_var)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3871')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/normalization_tests/test_batch_normalization.py: 307-325
</a>
<div class="mid" id="frag3871" style="display:none"><pre>
    def check_statistics2(self, x, y):
        x = chainer.Variable(x)
        y = chainer.Variable(y)
        self.link(x, finetune=True)
        self.link(y, finetune=True)
        mean = (self.x.sum(axis=0) + self.y.sum(axis=0)) / (self.nx + self.ny)
        var = (self.x.var(axis=0) * self.nx +
               self.y.var(axis=0) * self.ny) / (self.nx + self.ny)

        # TODO(Kenta Oono)
        # Fix the estimate of the unbiased variance.
        # Unbiased variance should be (nx + ny) / (nx + ny - 1) times of
        # the variance.
        # But the multiplier is ny / (ny - 1) in current implementation
        # these two values are different when nx is not equal to ny.
        unbiased_var = var * self.ny / (self.ny - 1)
        testing.assert_allclose(mean, self.link.avg_mean)
        testing.assert_allclose(unbiased_var, self.link.avg_var)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 86:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 82%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3918')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/normalization_tests/test_batch_normalization.py: 753-773
</a>
<div class="mid" id="frag3918" style="display:none"><pre>
    def test_forward(self, backend_config):
        with chainer.using_config('compute_mode', 'cudnn_fast'):
            link = self.create_link()
        link.to_device(backend_config.device)

        x = self.create_input_array(backend_config.xp)
        x = chainer.Variable(x, layout=memory_layouts.CUDNN_CHANNEL_LAST_X)
        x.to_device(backend_config.device)
        with backend_config:
            y = link(x)

        assert link.gamma.device == backend_config.device
        assert link.beta.device == backend_config.device
        assert y.layout == memory_layouts.CUDNN_CHANNEL_LAST_X
        assert y.shape == (
            self.batch,
            self.channels,
            self.height,
            self.width)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4220')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_convolution_2d.py: 285-304
</a>
<div class="mid" id="frag4220" style="display:none"><pre>
    def test_forward(self, backend_config):
        with chainer.using_config('compute_mode', 'cudnn_fast'):
            link = self.create_link()
        link.to_device(backend_config.device)

        x = self.create_input_array(backend_config.xp)
        x = chainer.Variable(x, layout=memory_layouts.CUDNN_CHANNEL_LAST_X)
        x.to_device(backend_config.device)
        with backend_config:
            y = link(x)

        assert link.W.device == backend_config.device
        assert y.layout == memory_layouts.CUDNN_CHANNEL_LAST_X
        assert y.shape == (
            self.batch,
            self.out_channels,
            (self.height - self.kernel_height + 1) // self.strides_height,
            (self.width - self.kernel_width + 1) // self.strides_width)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 87:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3922')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/normalization_tests/test_group_normalization.py: 63-78
</a>
<div class="mid" id="frag3922" style="display:none"><pre>
    def generate_inputs(self):
        shape = self.shape

        # sample x such that x.std &gt;= min_std
        min_std = 0.02
        retry = 0
        while True:
            x = numpy.random.uniform(-1, 1, shape).astype(self.dtype)
            x_groups = x.reshape(shape[0], self.groups, -1)
            if x_groups.std(axis=2).min() &gt;= min_std:
                break
            retry += 1
            assert retry &lt;= 20, 'Too many retries to generate inputs'

        return x,

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6708')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/normalization_tests/test_group_normalization.py: 64-81
</a>
<div class="mid" id="frag6708" style="display:none"><pre>
    def generate_inputs(self):
        shape = self.shape

        # sample x such that x.std &gt;= min_std
        min_std = 0.2 if self.dtype == numpy.float16 else 0.02
        retry = 0
        while True:
            x = numpy.random.uniform(-1, 1, shape).astype(self.dtype)
            x_groups = x.reshape(shape[0], self.groups, -1)
            if x_groups.std(axis=2).min() &gt;= min_std:
                break
            retry += 1
            assert retry &lt;= 20, 'Too many retries to generate inputs'

        gamma = numpy.random.uniform(-1, 1, shape[1]).astype(self.dtype)
        beta = numpy.random.uniform(-1, 1, shape[1]).astype(self.dtype)
        return x, gamma, beta

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 88:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3939')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/normalization_tests/test_decorrelated_batch_normalization.py: 43-59
</a>
<div class="mid" id="frag3939" style="display:none"><pre>
def _calc_projection_1group(x, mean, eps):
    spatial_ndim = len(x.shape[2:])
    spatial_axis = tuple(range(2, 2 + spatial_ndim))
    b, C = x.shape[:2]
    m = b
    for i in spatial_axis:
        m *= x.shape[i]

    x_hat = x.transpose((1, 0) + spatial_axis).reshape(C, -1)
    mean = x_hat.mean(axis=1)
    x_hat = x_hat - mean[:, None]
    cov = x_hat.dot(x_hat.T) / m + eps * numpy.eye(C, dtype=x.dtype)
    eigvals, eigvectors = numpy.linalg.eigh(cov)
    projection = eigvectors.dot(numpy.diag(eigvals ** -0.5)).dot(eigvectors.T)
    return projection


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6717')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/normalization_tests/test_decorrelated_batch_normalization.py: 37-53
</a>
<div class="mid" id="frag6717" style="display:none"><pre>
def _calc_projection_1group(x, mean, eps):
    spatial_ndim = len(x.shape[2:])
    spatial_axis = tuple(range(2, 2 + spatial_ndim))
    b, C = x.shape[:2]
    m = b
    for i in spatial_axis:
        m *= x.shape[i]

    x_hat = x.transpose((1, 0) + spatial_axis).reshape(C, -1)
    mean = x_hat.mean(axis=1)
    x_hat = x_hat - mean[:, None]
    cov = x_hat.dot(x_hat.T) / m + eps * numpy.eye(C, dtype=x.dtype)
    eigvals, eigvectors = numpy.linalg.eigh(cov)
    projection = eigvectors.dot(numpy.diag(eigvals ** -0.5)).dot(eigvectors.T)
    return projection


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 89:</b> &nbsp; 2 fragments, nominal size 22 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3944')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/normalization_tests/test_decorrelated_batch_normalization.py: 121-156
</a>
<div class="mid" id="frag3944" style="display:none"><pre>
    def generate_inputs(self):
        dtype = self.dtype
        ndim = self.ndim
        shape = (5, self.n_channels) + (2,) * ndim
        m = 5 * 2 ** ndim

        # NOTE(kataoka): The current implementation uses linalg.eigh. Small
        # eigenvalues of the correlation matrix, which can be as small as
        # eps=2e-5, cannot be computed with good *relative* accuracy, but
        # the eigenvalues are used later as `eigvals ** -0.5`. Require the
        # following is sufficiently large:
        # min(eigvals[:k]) == min(singular_vals ** 2 / m + eps)
        min_singular_value = 0.1
        # NOTE(kataoka): Decorrelated batch normalization should be free from
        # "stochastic axis swapping". Requiring a gap between singular values
        # just hides mistakes in implementations.
        min_singular_value_gap = 0.001
        g = self.groups
        zca_shape = g, self.n_channels // g, m
        x = numpy.random.uniform(-1, 1, zca_shape)
        mean = x.mean(axis=2, keepdims=True)
        a = x - mean
        u, s, vh = numpy.linalg.svd(a, full_matrices=False)
        # Decrement the latter dim because of the constraint `sum(_) == 0`
        k = min(zca_shape[1], zca_shape[2] - 1)
        s[:, :k] += (
            min_singular_value
            + min_singular_value_gap * numpy.arange(k)
        )[::-1]
        a = numpy.einsum('bij,bj,bjk-&gt;bik', u, s, vh)
        x = a + mean

        x = x.reshape((self.n_channels, shape[0]) + shape[2:]).swapaxes(0, 1)
        x = x.astype(dtype)
        return x,

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6720')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/normalization_tests/test_decorrelated_batch_normalization.py: 101-136
</a>
<div class="mid" id="frag6720" style="display:none"><pre>
    def generate_inputs(self):
        dtype = self.dtype
        ndim = self.ndim
        shape = (5, self.n_channels) + (2,) * ndim
        m = 5 * 2 ** ndim

        # NOTE(kataoka): The current implementation uses linalg.eigh. Small
        # eigenvalues of the correlation matrix, which can be as small as
        # eps=2e-5, cannot be computed with good *relative* accuracy, but
        # the eigenvalues are used later as `eigvals ** -0.5`. Require the
        # following is sufficiently large:
        # min(eigvals[:k]) == min(singular_vals ** 2 / m + eps)
        min_singular_value = 0.1
        # NOTE(kataoka): Decorrelated batch normalization should be free from
        # "stochastic axis swapping". Requiring a gap between singular values
        # just hides mistakes in implementations.
        min_singular_value_gap = 0.001
        g = self.groups
        zca_shape = g, self.n_channels // g, m
        x = numpy.random.uniform(-1, 1, zca_shape)
        mean = x.mean(axis=2, keepdims=True)
        a = x - mean
        u, s, vh = numpy.linalg.svd(a, full_matrices=False)
        # Decrement the latter dim because of the constraint `sum(_) == 0`
        k = min(zca_shape[1], zca_shape[2] - 1)
        s[:, :k] += (
            min_singular_value
            + min_singular_value_gap * numpy.arange(k)
        )[::-1]
        a = numpy.einsum('bij,bj,bjk-&gt;bik', u, s, vh)
        x = a + mean

        x = x.reshape((self.n_channels, shape[0]) + shape[2:]).swapaxes(0, 1)
        x = x.astype(dtype)
        return x,

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 90:</b> &nbsp; 3 fragments, nominal size 21 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4001')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/model_tests/test_vision.py: 74-96
</a>
<div class="mid" id="frag4001" style="display:none"><pre>
    def test_prepare(self):
        x1 = numpy.random.uniform(0, 255, (320, 240, 3)).astype(numpy.uint8)
        x2 = numpy.random.uniform(0, 255, (320, 240)).astype(numpy.uint8)
        x3 = numpy.random.uniform(0, 255, (160, 120, 3)).astype(self.dtype)
        x4 = numpy.random.uniform(0, 255, (1, 160, 120)).astype(self.dtype)
        x5 = numpy.random.uniform(0, 255, (3, 160, 120)).astype(numpy.uint8)

        y1 = resnet.prepare(x1)
        assert y1.shape == (3, 224, 224)
        assert y1.dtype == self.dtype
        y2 = resnet.prepare(x2)
        assert y2.shape == (3, 224, 224)
        assert y2.dtype == self.dtype
        y3 = resnet.prepare(x3, size=None)
        assert y3.shape == (3, 160, 120)
        assert y3.dtype == self.dtype
        y4 = resnet.prepare(x4)
        assert y4.shape == (3, 224, 224)
        assert y4.dtype == self.dtype
        y5 = resnet.prepare(x5, size=None)
        assert y5.shape == (3, 160, 120)
        assert y5.dtype == self.dtype

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4017')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/model_tests/test_vision.py: 216-238
</a>
<div class="mid" id="frag4017" style="display:none"><pre>
    def test_prepare(self):
        x1 = numpy.random.uniform(0, 255, (320, 240, 3)).astype(numpy.uint8)
        x2 = numpy.random.uniform(0, 255, (320, 240)).astype(numpy.uint8)
        x3 = numpy.random.uniform(0, 255, (160, 120, 3)).astype(self.dtype)
        x4 = numpy.random.uniform(0, 255, (1, 160, 120)).astype(self.dtype)
        x5 = numpy.random.uniform(0, 255, (3, 160, 120)).astype(numpy.uint8)

        y1 = vgg.prepare(x1)
        assert y1.shape == (3, 224, 224)
        assert y1.dtype == self.dtype
        y2 = vgg.prepare(x2)
        assert y2.shape == (3, 224, 224)
        assert y2.dtype == self.dtype
        y3 = vgg.prepare(x3, size=None)
        assert y3.shape == (3, 160, 120)
        assert y3.dtype == self.dtype
        y4 = vgg.prepare(x4)
        assert y4.shape == (3, 224, 224)
        assert y4.dtype == self.dtype
        y5 = vgg.prepare(x5, size=None)
        assert y5.shape == (3, 160, 120)
        assert y5.dtype == self.dtype

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4035')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/model_tests/test_vision.py: 360-382
</a>
<div class="mid" id="frag4035" style="display:none"><pre>
    def test_prepare(self):
        x1 = numpy.random.uniform(0, 255, (320, 240, 3)).astype(numpy.uint8)
        x2 = numpy.random.uniform(0, 255, (320, 240)).astype(numpy.uint8)
        x3 = numpy.random.uniform(0, 255, (160, 120, 3)).astype(self.dtype)
        x4 = numpy.random.uniform(0, 255, (1, 160, 120)).astype(self.dtype)
        x5 = numpy.random.uniform(0, 255, (3, 160, 120)).astype(numpy.uint8)

        y1 = googlenet.prepare(x1)
        assert y1.shape == (3, 224, 224)
        assert y1.dtype, self.dtype
        y2 = googlenet.prepare(x2)
        assert y2.shape == (3, 224, 224)
        assert y2.dtype, self.dtype
        y3 = googlenet.prepare(x3, size=None)
        assert y3.shape == (3, 160, 120)
        assert y3.dtype, self.dtype
        y4 = googlenet.prepare(x4)
        assert y4.shape == (3, 224, 224)
        assert y4.dtype, self.dtype
        y5 = googlenet.prepare(x5, size=None)
        assert y5.shape == (3, 160, 120)
        assert y5.dtype, self.dtype

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 91:</b> &nbsp; 3 fragments, nominal size 18 lines, similarity 94%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4002')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/model_tests/test_vision.py: 97-117
</a>
<div class="mid" id="frag4002" style="display:none"><pre>
    def check_extract(self):
        x1 = numpy.random.uniform(0, 255, (320, 240, 3)).astype(numpy.uint8)
        x2 = numpy.random.uniform(0, 255, (320, 240)).astype(numpy.uint8)

        with numpy.errstate(divide='ignore'):
            result = self.link.extract([x1, x2], layers=['res3', 'pool5'])
            assert len(result) == 2
            y1 = cuda.to_cpu(result['res3'].data)
            assert y1.shape == (2, 512, 28, 28)
            assert y1.dtype == self.dtype
            y2 = cuda.to_cpu(result['pool5'].data)
            assert y2.shape == (2, 2048)
            assert y2.dtype == self.dtype

            x3 = numpy.random.uniform(0, 255, (80, 60)).astype(numpy.uint8)
            result = self.link.extract([x3], layers=['res2'], size=None)
            assert len(result) == 1
            y3 = cuda.to_cpu(result['res2'].data)
            assert y3.shape == (1, 256, 20, 15)
            assert y3.dtype == self.dtype

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4036')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/model_tests/test_vision.py: 383-402
</a>
<div class="mid" id="frag4036" style="display:none"><pre>
    def check_extract(self):
        x1 = numpy.random.uniform(0, 255, (320, 240, 3)).astype(numpy.uint8)
        x2 = numpy.random.uniform(0, 255, (320, 240)).astype(numpy.uint8)

        result = self.link.extract([x1, x2], layers=['pool5', 'loss3_fc'])
        assert len(result) == 2
        y1 = cuda.to_cpu(result['pool5'].data)
        assert y1.shape == (2, 1024, 1, 1)
        assert y1.dtype == self.dtype
        y2 = cuda.to_cpu(result['loss3_fc'].data)
        assert y2.shape == (2, 1000)
        assert y2.dtype == self.dtype

        x3 = numpy.random.uniform(0, 255, (80, 60)).astype(numpy.uint8)
        result = self.link.extract([x3], layers=['pool1'], size=None)
        assert len(result) == 1
        y3 = cuda.to_cpu(result['pool1'].data)
        assert y3.shape == (1, 64, 20, 15)
        assert y3.dtype == self.dtype

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4018')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/model_tests/test_vision.py: 239-257
</a>
<div class="mid" id="frag4018" style="display:none"><pre>
    def check_extract(self):
        x1 = numpy.random.uniform(0, 255, (320, 240, 3)).astype(numpy.uint8)
        x2 = numpy.random.uniform(0, 255, (320, 240)).astype(numpy.uint8)
        result = self.link.extract([x1, x2], layers=['pool3', 'fc7'])
        assert len(result) == 2
        y1 = cuda.to_cpu(result['pool3'].data)
        assert y1.shape == (2, 256, 28, 28)
        assert y1.dtype == self.dtype
        y2 = cuda.to_cpu(result['fc7'].data)
        assert y2.shape == (2, 4096)
        assert y2.dtype == self.dtype

        x3 = numpy.random.uniform(0, 255, (80, 60)).astype(numpy.uint8)
        result = self.link.extract([x3], layers=['pool1'], size=None)
        assert len(result) == 1
        y3 = cuda.to_cpu(result['pool1'].data)
        assert y3.shape == (1, 64, 40, 30)
        assert y3.dtype == self.dtype

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 92:</b> &nbsp; 3 fragments, nominal size 12 lines, similarity 91%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4005')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/model_tests/test_vision.py: 129-142
</a>
<div class="mid" id="frag4005" style="display:none"><pre>
    def check_predict(self):
        x1 = numpy.random.uniform(0, 255, (320, 240, 3)).astype(numpy.uint8)
        x2 = numpy.random.uniform(0, 255, (320, 240)).astype(numpy.uint8)

        with numpy.errstate(divide='ignore'):
            result = self.link.predict([x1, x2], oversample=False)
            y = cuda.to_cpu(result.data)
            assert y.shape == (2, 1000)
            assert y.dtype == self.dtype
            result = self.link.predict([x1, x2], oversample=True)
            y = cuda.to_cpu(result.data)
            assert y.shape == (2, 1000)
            assert y.dtype == self.dtype

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4039')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/model_tests/test_vision.py: 414-426
</a>
<div class="mid" id="frag4039" style="display:none"><pre>
    def check_predict(self):
        x1 = numpy.random.uniform(0, 255, (320, 240, 3)).astype(numpy.uint8)
        x2 = numpy.random.uniform(0, 255, (320, 240)).astype(numpy.uint8)

        result = self.link.predict([x1, x2], oversample=False)
        y = cuda.to_cpu(result.data)
        assert y.shape == (2, 1000)
        assert y.dtype == self.dtype
        result = self.link.predict([x1, x2], oversample=True)
        y = cuda.to_cpu(result.data)
        assert y.shape == (2, 1000)
        assert y.dtype == self.dtype

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4021')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/model_tests/test_vision.py: 267-278
</a>
<div class="mid" id="frag4021" style="display:none"><pre>
    def check_predict(self):
        x1 = numpy.random.uniform(0, 255, (320, 240, 3)).astype(numpy.uint8)
        x2 = numpy.random.uniform(0, 255, (320, 240)).astype(numpy.uint8)
        result = self.link.predict([x1, x2], oversample=False)
        y = cuda.to_cpu(result.data)
        assert y.shape == (2, 1000)
        assert y.dtype == self.dtype
        result = self.link.predict([x1, x2], oversample=True)
        y = cuda.to_cpu(result.data)
        assert y.shape == (2, 1000)
        assert y.dtype == self.dtype

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 93:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4114')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_mlp_convolution_2d.py: 154-168
</a>
<div class="mid" id="frag4114" style="display:none"><pre>
    def test_valid_instantiation_ksize_is_none(self):
        l = links.MLPConvolution2D(self.out_channels, self.ksize, None,
                                   self.stride, self.pad, functions.relu,
                                   conv_init=None, bias_init=None)
        x = numpy.random.uniform(
            -1, 1, (10, self.in_channels, 10, 10)).astype(numpy.float32)
        l(x)  # create weight tensors of convolutions by initialization

        self.assertEqual(len(l), 2)
        self.assertEqual(l[0].W.shape,
                         (self.out_channels[0], self.in_channels,
                          self.ksize, self.ksize))
        self.assertEqual(l[1].W.shape,
                         (self.out_channels[1], self.out_channels[0], 1, 1))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4115')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_mlp_convolution_2d.py: 169-183
</a>
<div class="mid" id="frag4115" style="display:none"><pre>
    def test_valid_instantiation_in_channels_is_omitted(self):
        l = links.MLPConvolution2D(
            self.out_channels, self.ksize, stride=self.stride, pad=self.pad,
            activation=functions.relu, conv_init=None, bias_init=None)
        x = numpy.random.uniform(
            -1, 1, (10, self.in_channels, 10, 10)).astype(numpy.float32)
        l(x)  # create weight tensors of convolutions by initialization

        self.assertEqual(len(l), 2)
        self.assertEqual(l[0].W.shape,
                         (self.out_channels[0], self.in_channels,
                          self.ksize, self.ksize))
        self.assertEqual(l[1].W.shape,
                         (self.out_channels[1], self.out_channels[0], 1, 1))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 94:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4136')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_linear.py: 146-160
</a>
<div class="mid" id="frag4136" style="display:none"><pre>
    def test_serialization(self):
        lin1 = links.Linear(self.out_size)
        x = chainer.Variable(self.x)
        # Must call the link to initialize weights.
        lin1(x)
        w1 = lin1.W.data
        fd, temp_file_path = tempfile.mkstemp()
        os.close(fd)
        npz.save_npz(temp_file_path, lin1)
        lin2 = links.Linear(self.out_size)
        npz.load_npz(temp_file_path, lin2)
        w2 = lin2.W.data
        self.assertEqual((w1 == w2).all(), True)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4357')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/activation_tests/test_simplified_dropconnect.py: 180-194
</a>
<div class="mid" id="frag4357" style="display:none"><pre>
    def test_serialization(self):
        lin1 = links.SimplifiedDropconnect(None, self.out_size)
        x = chainer.Variable(self.x)
        # Must call the link to initialize weights.
        lin1(x)
        w1 = lin1.W.data
        fd, temp_file_path = tempfile.mkstemp()
        os.close(fd)
        npz.save_npz(temp_file_path, lin1)
        lin2 = links.SimplifiedDropconnect(None, self.out_size)
        npz.load_npz(temp_file_path, lin2)
        w2 = lin2.W.data
        self.assertEqual((w1 == w2).all(), True)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 95:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4194')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_scale.py: 100-111
</a>
<div class="mid" id="frag4194" style="display:none"><pre>
    def test_backward_gpu(self):
        with testing.assert_warns(DeprecationWarning):
            self.link.to_gpu()
        x = cuda.to_gpu(self.x)
        if self.learn_W:
            W = None
        else:
            W = cuda.to_gpu(self.W)
        gy = cuda.to_gpu(self.gy)
        self.check_backward(x, W, gy)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4318')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_bias.py: 86-97
</a>
<div class="mid" id="frag4318" style="display:none"><pre>
    def test_backward_gpu(self):
        with testing.assert_warns(DeprecationWarning):
            self.link.to_gpu()
        x = cuda.to_gpu(self.x)
        if self.learn_b:
            b = None
        else:
            b = cuda.to_gpu(self.b)
        gy = cuda.to_gpu(self.gy)
        self.check_backward(x, b, gy)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 96:</b> &nbsp; 3 fragments, nominal size 17 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4205')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_convolution_2d.py: 94-115
</a>
<div class="mid" id="frag4205" style="display:none"><pre>
    def test_pickling(self, backend_config):
        x_data, = self.generate_inputs()

        link = self.create_link(self.generate_params())
        link.to_device(backend_config.device)

        x = chainer.Variable(x_data)
        x.to_device(backend_config.device)

        y = link(x)
        y_data1 = y.data
        del x, y
        pickled = pickle.dumps(link, -1)
        del link
        link = pickle.loads(pickled)
        x = chainer.Variable(x_data)
        x.to_device(backend_config.device)
        y = link(x)
        y_data2 = y.data

        testing.assert_allclose(y_data1, y_data2, atol=0, rtol=0)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4215')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_convolution_2d.py: 213-235
</a>
<div class="mid" id="frag4215" style="display:none"><pre>
    def test_pickling(self, backend_config):
        x_data, = self.generate_inputs()

        link = self.create_link(self.generate_params())
        link.to_device(backend_config.device)

        x = chainer.Variable(x_data)
        x.to_device(backend_config.device)

        y = link(x)
        y_data1 = y.data
        del x, y
        pickled = pickle.dumps(link, -1)
        del link
        link = pickle.loads(pickled)
        x = chainer.Variable(x_data)
        x.to_device(backend_config.device)
        y = link(x)
        y_data2 = y.data

        testing.assert_allclose(y_data1, y_data2, atol=0, rtol=0)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4267')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_convolution_nd.py: 106-127
</a>
<div class="mid" id="frag4267" style="display:none"><pre>
    def test_pickling(self, backend_config):
        x_data, = self.generate_inputs()

        link = self.create_link(self.generate_params())
        link.to_device(backend_config.device)

        x = chainer.Variable(x_data)
        x.to_device(backend_config.device)

        y = link(x)
        y_data1 = y.data
        del x, y
        pickled = pickle.dumps(link, -1)
        del link
        link = pickle.loads(pickled)
        x = chainer.Variable(x_data)
        x.to_device(backend_config.device)
        y = link(x)
        y_data2 = y.data

        testing.assert_allclose(y_data1, y_data2, atol=0, rtol=0)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 97:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 77%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4206')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_convolution_2d.py: 116-132
</a>
<div class="mid" id="frag4206" style="display:none"><pre>
    def test_from_params(self, backend_config):
        if (
                (backend_config.use_cuda and
                 backend_config.cuda_device == 1) or
                (backend_config.use_chainerx and
                 'cuda' in backend_config.chainerx_device)):
            raise unittest.SkipTest()
        link1 = self.create_link(self.generate_params())
        link1.to_device(backend_config.device)
        link2 = links.Convolution2D.from_params(
            link1.W, link1.b, stride=self.stride, pad=self.pad)
        assert link2.W.shape == link1.W.shape
        assert link2.b.shape == link2.b.shape
        assert link2.stride == link1.stride
        assert link2.pad == link1.pad


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4268')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_convolution_nd.py: 128-149
</a>
<div class="mid" id="frag4268" style="display:none"><pre>
    def test_from_params(self, backend_config):
        if (
                (backend_config.use_cuda and
                 backend_config.cuda_device == 1) or
                (backend_config.use_chainerx and
                 'cuda' in backend_config.chainerx_device)):
            raise unittest.SkipTest()
        link1 = self.create_link(self.generate_params())
        link1.to_device(backend_config.device)

        if self.in_channels in (None, 'omit'):
            link1._initialize_params(self.x_shape[1])

        link2 = convolution_nd.ConvolutionND.from_params(
            link1.W, link1.b,
            stride=self.stride, pad=self.pad, groups=self.groups)
        assert link2.W.shape == link1.W.shape
        assert link2.b.shape == link1.b.shape
        assert link2.stride == link1.stride
        assert link2.pad == link1.pad


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 98:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4225')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_dilated_convolution_2d.py: 45-57
</a>
<div class="mid" id="frag4225" style="display:none"><pre>
    def check_forward_consistency(self):
        x_cpu = chainer.Variable(self.x)
        y_cpu = self.link(x_cpu)
        self.assertEqual(y_cpu.data.dtype, numpy.float32)

        with testing.assert_warns(DeprecationWarning):
            self.link.to_gpu()
        x_gpu = chainer.Variable(cuda.to_gpu(self.x))
        y_gpu = self.link(x_gpu)
        self.assertEqual(y_gpu.data.dtype, numpy.float32)

        testing.assert_allclose(y_cpu.data, y_gpu.data.get())

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4238')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_dilated_convolution_2d.py: 147-159
</a>
<div class="mid" id="frag4238" style="display:none"><pre>
    def check_forward_consistency(self):
        x_cpu = chainer.Variable(self.x)
        y_cpu = self.link(x_cpu)
        self.assertEqual(y_cpu.data.dtype, numpy.float32)

        with testing.assert_warns(DeprecationWarning):
            self.link.to_gpu()
        x_gpu = chainer.Variable(cuda.to_gpu(self.x))
        y_gpu = self.link(x_gpu)
        self.assertEqual(y_gpu.data.dtype, numpy.float32)

        testing.assert_allclose(y_cpu.data, y_gpu.data.get())

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 99:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4232')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_dilated_convolution_2d.py: 88-104
</a>
<div class="mid" id="frag4232" style="display:none"><pre>
    def check_pickling(self, x_data):
        x = chainer.Variable(x_data)
        y = self.link(x)
        y_data1 = y.data

        del x, y

        pickled = pickle.dumps(self.link, -1)
        del self.link
        self.link = pickle.loads(pickled)

        x = chainer.Variable(x_data)
        y = self.link(x)
        y_data2 = y.data

        testing.assert_allclose(y_data1, y_data2, atol=0, rtol=0)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4245')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_dilated_convolution_2d.py: 190-206
</a>
<div class="mid" id="frag4245" style="display:none"><pre>
    def check_pickling(self, x_data):
        x = chainer.Variable(x_data)
        y = self.link(x)
        y_data1 = y.data

        del x, y

        pickled = pickle.dumps(self.link, -1)
        del self.link
        self.link = pickle.loads(pickled)

        x = chainer.Variable(x_data)
        y = self.link(x)
        y_data2 = y.data

        testing.assert_allclose(y_data1, y_data2, atol=0, rtol=0)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 100:</b> &nbsp; 3 fragments, nominal size 11 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4235')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_dilated_convolution_2d.py: 120-130
</a>
<div class="mid" id="frag4235" style="display:none"><pre>
    def setUp(self):
        self.link = links.DilatedConvolution2D(*self.args, **self.kwargs)
        self.x = numpy.random.uniform(-1, 1,
                                      (2, 3, 4, 3)).astype(numpy.float32)
        self.link(chainer.Variable(self.x))
        b = self.link.b.data
        b[...] = numpy.random.uniform(-1, 1, b.shape)
        self.link.cleargrads()
        self.gy = numpy.random.uniform(-1, 1,
                                       (2, 2, 2, 2)).astype(numpy.float32)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4307')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_local_convolution_2d.py: 55-67
</a>
<div class="mid" id="frag4307" style="display:none"><pre>
    def setUp(self):
        in_channels = None
        self.link = links.LocalConvolution2D(in_channels, 2, ksize=3,
                                             stride=1)
        self.x = numpy.random.uniform(-1, 1,
                                      (2, 3, 4, 4)).astype(numpy.float32)
        self.link(chainer.Variable(self.x))
        b = self.link.b.data
        b[...] = numpy.random.uniform(-1, 1, b.shape)
        self.link.cleargrads()
        self.gy = numpy.random.uniform(-1, 1,
                                       (2, 2, 2, 2)).astype(numpy.float32)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4326')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_depthwise_convolution_2d.py: 55-67
</a>
<div class="mid" id="frag4326" style="display:none"><pre>
    def setUp(self):
        in_channels = None
        self.link = links.DepthwiseConvolution2D(in_channels, 2, 3,
                                                 stride=2, pad=1)
        self.x = numpy.random.uniform(-1, 1,
                                      (2, 3, 4, 3)).astype(numpy.float32)
        self.link(chainer.Variable(self.x))
        b = self.link.b.data
        b[...] = numpy.random.uniform(-1, 1, b.shape)
        self.link.cleargrads()
        self.gy = numpy.random.uniform(-1, 1,
                                       (2, 6, 2, 2)).astype(numpy.float32)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 101:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 92%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4303')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_local_convolution_2d.py: 21-35
</a>
<div class="mid" id="frag4303" style="display:none"><pre>
    def setUp(self):
        self.link = links.LocalConvolution2D(
            3, 2, in_size=4, ksize=3, stride=1,
            initialW=chainer.initializers.Normal(1, self.W_dtype),
            initial_bias=chainer.initializers.Normal(1, self.x_dtype))
        self.link.cleargrads()

        self.x = numpy.random.uniform(-1, 1,
                                      (2, 3, 4, 4)).astype(self.x_dtype)
        self.gy = numpy.random.uniform(-1, 1,
                                       (2, 2, 2, 2)).astype(self.x_dtype)
        self.check_backward_options = {}
        if self.x_dtype == numpy.float16 or self.W_dtype == numpy.float16:
            self.check_backward_options = {'atol': 3e-2, 'rtol': 5e-2}

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4322')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_depthwise_convolution_2d.py: 21-35
</a>
<div class="mid" id="frag4322" style="display:none"><pre>
    def setUp(self):
        self.link = links.DepthwiseConvolution2D(
            3, 2, 3, stride=2, pad=1,
            initialW=chainer.initializers.Normal(1, self.W_dtype),
            initial_bias=chainer.initializers.Normal(1, self.x_dtype))
        self.link.cleargrads()

        self.x = numpy.random.uniform(-1, 1,
                                      (2, 3, 4, 3)).astype(self.x_dtype)
        self.gy = numpy.random.uniform(-1, 1,
                                       (2, 6, 2, 2)).astype(self.x_dtype)
        self.check_backward_options = {}
        if self.x_dtype == numpy.float16 or self.W_dtype == numpy.float16:
            self.check_backward_options = {'atol': 3e-2, 'rtol': 5e-2}

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 102:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4559')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_backprop.py: 20-33
</a>
<div class="mid" id="frag4559" style="display:none"><pre>
    def check_multiple_output_1arg(self, xp, skip_retain_grad_test=False):
        x = chainer.Variable(xp.array([1, 2], np.float32))
        h = x * 2
        y0 = h * 3
        y1 = h * 4
        y0.grad = xp.array([1, 10], np.float32)
        y1.grad = xp.array([100, 1000], np.float32)
        chainer.backward([y0, y1])
        testing.assert_allclose(x.grad, np.array([806, 8060], np.float32))
        if skip_retain_grad_test:
            return
        assert y0.grad is None
        assert y1.grad is None

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4560')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_backprop.py: 34-47
</a>
<div class="mid" id="frag4560" style="display:none"><pre>
    def check_multiple_output_2args(self, xp, skip_retain_grad_test=False):
        x = chainer.Variable(xp.array([1, 2], np.float32))
        h = x * 2
        y0 = h * 3
        y1 = h * 4
        gy0 = chainer.Variable(xp.array([1, 10], np.float32))
        gy1 = chainer.Variable(xp.array([100, 1000], np.float32))
        chainer.backward([y0, y1], [gy0, gy1])
        testing.assert_allclose(x.grad, np.array([806, 8060], np.float32))
        if skip_retain_grad_test:
            return
        assert y0.grad is None
        assert y1.grad is None

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 103:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4660')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_variable.py: 704-722
</a>
<div class="mid" id="frag4660" style="display:none"><pre>
    def test_copydata_to_uninitialized_parameter(
            self, src_backend_config, dst_backend_config):
        shape = self.shape
        dtype = np.float32
        src_arr_numpy = np.asarray(np.random.randn(*shape), dtype)
        src_arr = src_backend_config.get_array(src_arr_numpy.copy())
        dst_var = chainer.Parameter()
        dst_var.to_device(dst_backend_config.device)
        src_var = chainer.Parameter(src_arr)
        src_arr_prev = src_var.array

        dst_var.copydata(src_var)

        assert src_var.array is src_arr_prev
        assert src_var.device == src_backend_config.device
        assert dst_var.device == dst_backend_config.device
        np.testing.assert_array_equal(
            _numpy_device.send(dst_var.data), src_arr_numpy)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4661')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_variable.py: 723-743
</a>
<div class="mid" id="frag4661" style="display:none"><pre>
    def test_copydata_from_uninitialized_parameter(
            self, src_backend_config, dst_backend_config):
        shape = self.shape
        dtype = np.float32
        dst_arr_numpy = np.asarray(np.random.randn(*shape), dtype)
        dst_arr = dst_backend_config.get_array(dst_arr_numpy.copy())
        initializer = initializers.Zero()
        dst_var = chainer.Parameter(dst_arr)
        src_var = chainer.Parameter(initializer)
        src_var.to_device(src_backend_config.device)
        dst_arr_prev = dst_var.array

        dst_var.copydata(src_var)

        assert src_var.device == src_backend_config.device
        assert dst_var.device == dst_backend_config.device
        assert dst_var.array is dst_arr_prev
        np.testing.assert_array_equal(
            _numpy_device.send(dst_var.array),
            _numpy_device.send(src_var.array))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 104:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4663')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_variable.py: 768-788
</a>
<div class="mid" id="frag4663" style="display:none"><pre>
    def test_grad(self, backend_config):
        x = backend_config.get_array(
            np.random.uniform(-1, 1, self.shape).astype(np.float32))
        g = backend_config.get_array(
            np.random.uniform(0.1, 10, self.shape).astype(np.float32))
        v = chainer.Variable(x, requires_grad=self.requires_grad)
        expected_error = (
            backend_config.xp is chainerx
            and not self.requires_grad)

        if expected_error:
            with pytest.raises(Exception):
                v.grad = g
        else:
            v.grad = g

            assert v.grad_var.requires_grad is True
            assert v.grad is not None
            assert v.requires_grad == self.requires_grad
            backend_config.xp.testing.assert_array_equal(v.grad, g)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4664')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_variable.py: 789-811
</a>
<div class="mid" id="frag4664" style="display:none"><pre>
    def check_grad_var(self, backend_config, grad_var_requires_grad):
        x = backend_config.get_array(
            np.random.uniform(-1, 1, self.shape).astype(np.float32))
        g = backend_config.get_array(
            np.random.uniform(0.1, 10, self.shape).astype(np.float32))
        v = chainer.Variable(x, requires_grad=self.requires_grad)
        gv = chainer.Variable(g, requires_grad=grad_var_requires_grad)
        expected_error = (
            backend_config.xp is chainerx
            and not self.requires_grad)

        if expected_error:
            with pytest.raises(Exception):
                v.grad_var = gv
        else:
            v.grad_var = gv

            assert v.requires_grad == self.requires_grad
            backend_config.xp.testing.assert_array_equal(v.grad, g)

            # Same instance should be returned each time.
            assert v.grad_var is gv

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 105:</b> &nbsp; 4 fragments, nominal size 11 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4787')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_variable.py: 1914-1925
</a>
<div class="mid" id="frag4787" style="display:none"><pre>
    def test_addgrad_to_uninitialized_parameter_cpu_to_gpu(self):
        x = chainer.Parameter()
        y = chainer.Parameter(self.a)
        y.grad = self.b
        x.to_gpu()
        x.cleargrad()
        x.addgrad(y)
        cp = cuda.cupy
        assert isinstance(x.data, cp.ndarray)
        assert isinstance(x.grad, cp.ndarray)
        cp.testing.assert_array_equal(x.grad, self.b)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4788')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_variable.py: 1927-1937
</a>
<div class="mid" id="frag4788" style="display:none"><pre>
    def test_addgrad_to_uninitialized_parameter_gpu_to_cpu(self):
        x = chainer.Parameter()
        y = chainer.Parameter(self.a)
        y.grad = self.b
        y.to_gpu()
        x.cleargrad()
        x.addgrad(y)
        assert isinstance(x.data, np.ndarray)
        assert isinstance(x.grad, np.ndarray)
        np.testing.assert_array_equal(x.grad, self.b)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4789')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_variable.py: 1939-1951
</a>
<div class="mid" id="frag4789" style="display:none"><pre>
    def test_addgrad_to_uninitialized_parameter_gpu_to_gpu(self):
        x = chainer.Parameter()
        y = chainer.Parameter(self.a)
        y.grad = self.b
        x.to_gpu()
        y.to_gpu()
        x.cleargrad()
        x.addgrad(y)
        cp = cuda.cupy
        assert isinstance(x.data, cp.ndarray)
        assert isinstance(x.grad, cp.ndarray)
        cp.testing.assert_array_equal(x.grad, self.b)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4790')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_variable.py: 1953-1967
</a>
<div class="mid" id="frag4790" style="display:none"><pre>
    def test_addgrad_to_uninitialized_parameter_gpu_to_another_gpu(self):
        x = chainer.Parameter()
        y = chainer.Parameter(self.a)
        y.grad = self.b
        x.to_gpu(1)
        y.to_gpu(0)
        x.cleargrad()
        x.addgrad(y)
        cp = cuda.cupy
        assert isinstance(x.data, cp.ndarray)
        assert isinstance(x.grad, cp.ndarray)
        assert int(x.data.device) == 1
        assert int(x.grad.device) == 1
        cp.testing.assert_array_equal(x.grad, self.b)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 106:</b> &nbsp; 3 fragments, nominal size 14 lines, similarity 92%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4806')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_variable.py: 2084-2102
</a>
<div class="mid" id="frag4806" style="display:none"><pre>
    def check_type_mismatch(self, x_data, retain):
        xp = backend.get_array_module(x_data)

        class DummyFunction(chainer.Function):
            label = 'dummy_function'

            def forward(self, inputs):
                if not retain:
                    self.retain_inputs(())
                return xp.array(1, np.float32),

            def backward(self, inputs, grads):
                return [1]

        x = chainer.Variable(x_data)
        y = DummyFunction()(x)
        with six.assertRaisesRegex(self, TypeError, 'dummy_function'):
            y.backward()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4813')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_variable.py: 2117-2135
</a>
<div class="mid" id="frag4813" style="display:none"><pre>
    def check_dtype_mismatch(self, x_data, retain):
        xp = backend.get_array_module(x_data)

        class DummyFunction(chainer.Function):
            label = 'dummy_function'

            def forward(self, inputs):
                if not retain:
                    self.retain_inputs(())
                return xp.array(1, np.float32),

            def backward(self, inputs, grads):
                return xp.array([1], np.int32),

        x = chainer.Variable(x_data)
        y = DummyFunction()(x)
        with six.assertRaisesRegex(self, TypeError, 'dummy_function'):
            y.backward()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4820')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_variable.py: 2150-2168
</a>
<div class="mid" id="frag4820" style="display:none"><pre>
    def check_shape_mismatch(self, x_data, retain):
        xp = backend.get_array_module(x_data)

        class DummyFunction(chainer.Function):
            label = 'dummy_function'

            def forward(self, inputs):
                if not retain:
                    self.retain_inputs(())
                return xp.array(1, np.float32),

            def backward(self, inputs, grads):
                return xp.array([1, 2], np.float32),

        x = chainer.Variable(x_data)
        y = DummyFunction()(x)
        with six.assertRaisesRegex(self, ValueError, 'dummy_function'):
            y.backward()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 107:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4871')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_variable.py: 2501-2511
</a>
<div class="mid" id="frag4871" style="display:none"><pre>
    def setUp(self):
        x = np.empty((0, 0))
        x = x.astype(np.float32)
        self.x = chainer.Variable(x)
        if (sys.version_info &lt; (3,) and sys.maxsize &gt; 2**32 and
                platform.system() == 'Windows'):
            self.repr = 'variable([], shape=(0L, 0L))'
        else:
            self.repr = 'variable([], shape=(0, 0))'
        self.str = 'variable([])'

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4881')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_variable.py: 2591-2601
</a>
<div class="mid" id="frag4881" style="display:none"><pre>
    def setUp(self):
        x = np.empty((0, 0))
        x = x.astype(np.float32)
        self.x = chainer.Variable(x, name='x')
        if (sys.version_info &lt; (3,) and sys.maxsize &gt; 2**32 and
                platform.system() == 'Windows'):
            self.repr = 'variable x([], shape=(0L, 0L))'
        else:
            self.repr = 'variable x([], shape=(0, 0))'
        self.str = 'variable x([])'

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 108:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4893')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_variable.py: 2674-2685
</a>
<div class="mid" id="frag4893" style="display:none"><pre>
    def test_default_backward(self):
        x = chainer.Variable(np.array([42], np.float32))
        y = x * 2  # x.grad_var will be different from y.grad_var
        with testing.assert_warns(DeprecationWarning):
            y.backward(retain_grad=True)
        assert x.grad_var.creator is None
        with warnings.catch_warnings():
            # ok to be warned that x.grad_var is old-styled scalar
            warnings.simplefilter('ignore', DeprecationWarning)
            x.grad_var.backward()
        assert y.grad_var.grad_var is None

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4895')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_variable.py: 2697-2708
</a>
<div class="mid" id="frag4895" style="display:none"><pre>
    def test_raise_double_backprop_2(self):
        x = chainer.Variable(np.array([42], np.float32))
        z = F.identity(x)  # new style
        y = IdentityFunction()(z)  # old style
        with testing.assert_warns(DeprecationWarning):
            y.backward(enable_double_backprop=True)
        with pytest.raises(RuntimeError):
            with warnings.catch_warnings():
                # ok to be warned that x.grad_var is old-styled scalar
                warnings.simplefilter('ignore', DeprecationWarning)
                x.grad_var.backward()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 109:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5134')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_gradient_check.py: 594-615
</a>
<div class="mid" id="frag5134" style="display:none"><pre>
    def test_no_grads_option(self, backend_config):
        if backend_config.use_chainerx:
            raise unittest.SkipTest(
                'gradient_check does not support no_grad option for ChainerX')
        x1 = backend_config.get_array(numpy.array([2], dtype='f'))
        # grad check for this is skipped
        x2 = backend_config.get_array(numpy.array([3], dtype='f'))
        g1 = backend_config.get_array(numpy.array([5], dtype='f'))

        def f(x, y):
            y_array = y.array
            if (backend_config.xp is chainerx
                    and isinstance(y_array, chainerx.ndarray)):
                y_array = y_array.as_grad_stopped()
            s = x + y_array
            return s,

        self.assertRaises(
            RuntimeError,  # backward computes x1.grad
            gradient_check.check_backward,
            f, (x1, x2), g1, no_grads=[True, True])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5136')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_gradient_check.py: 616-634
</a>
<div class="mid" id="frag5136" style="display:none"><pre>
    def test_const_input(self, backend_config):
        x1 = backend_config.get_array(numpy.array([2], dtype='f'))
        # grad check for this is skipped
        x2 = backend_config.get_array(numpy.array([3], dtype='f'))
        g1 = backend_config.get_array(numpy.array([5], dtype='f'))

        def f(x, y):
            y_array = y.array
            if (backend_config.xp is chainerx
                    and isinstance(y_array, chainerx.ndarray)):
                y_array = y_array.as_grad_stopped()
            s = x + y_array
            return s,

        self.assertRaises(
            AssertionError,  # numerical backward to x2 is nonzero
            gradient_check.check_backward,
            f, (x1, x2), g1, no_grads=[False, False])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 110:</b> &nbsp; 3 fragments, nominal size 11 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5144')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_gradient_check.py: 699-711
</a>
<div class="mid" id="frag5144" style="display:none"><pre>
    def _broken_func_1(self):
        class Broken(chainer.Function):
            def forward(self, inputs):
                x, = inputs
                return (x * x),

            def backward(self, inputs, grad_outputs):
                x, = inputs
                gy, = grad_outputs
                return 3 * x * gy,

        return Broken()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5147')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_gradient_check.py: 712-725
</a>
<div class="mid" id="frag5147" style="display:none"><pre>
    def _broken_func_2(self):
        class Broken(chainer.FunctionNode):
            def forward(self, inputs):
                x, = inputs
                self.retain_inputs((0,))
                return (x * x),

            def backward(self, indexes, grad_outputs):
                x, = self.get_retained_inputs()
                gy, = grad_outputs
                return 3 * x * gy,

        return Broken()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5150')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_gradient_check.py: 726-741
</a>
<div class="mid" id="frag5150" style="display:none"><pre>
    def _broken_func_3(self):
        class Broken(chainer.FunctionNode):
            def forward(self, inputs):
                x, = inputs
                self.retain_inputs((0,))
                return (x * x),

            def backward(self, indexes, grad_outputs):
                x, = self.get_retained_inputs()
                gy, = grad_outputs
                gx1 = 2 * x * gy
                gx2 = 3 * x * gy
                return (gx1, gx2)

        return Broken()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 111:</b> &nbsp; 3 fragments, nominal size 13 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5174')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/utils_tests/test_cache.py: 50-64
</a>
<div class="mid" id="frag5174" style="display:none"><pre>
    def test3(self):
        obj = MockDistribution(chainer.Variable(numpy.array([1.])))
        h0 = obj.h
        with chainer.no_backprop_mode():
            h1 = obj.h
        h2 = obj.h
        with chainer.no_backprop_mode():
            h3 = obj.h
        assert obj.h_call_count &lt;= 2
        assert h0 is h2
        assert h0 is not h1
        assert h1 is h3
        numpy.testing.assert_allclose(h0.array, 2.)
        numpy.testing.assert_allclose(h1.array, 2.)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5175')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/utils_tests/test_cache.py: 65-77
</a>
<div class="mid" id="frag5175" style="display:none"><pre>
    def test_attrs1(self):
        obj = MockDistribution(chainer.Variable(numpy.array([1.])))
        h0 = obj.h
        y0 = obj.y
        h1 = obj.h
        y1 = obj.y
        assert obj.h_call_count == 1
        assert obj.y_call_count == 1
        assert h0 is h1
        assert y0 is y1
        numpy.testing.assert_allclose(h0.array, 2.)
        numpy.testing.assert_allclose(y0.array, 6.)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5176')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/utils_tests/test_cache.py: 78-92
</a>
<div class="mid" id="frag5176" style="display:none"><pre>
    def test_objs1(self):
        obj0 = MockDistribution(chainer.Variable(numpy.array([1.])))
        obj1 = MockDistribution(chainer.Variable(numpy.array([10.])))
        y00 = obj0.y
        y10 = obj1.y
        y01 = obj0.y
        y11 = obj1.y
        assert obj0.y_call_count == 1
        assert obj1.y_call_count == 1
        assert y00 is y01
        assert y10 is y11
        numpy.testing.assert_allclose(y00.array, 6.)
        numpy.testing.assert_allclose(y10.array, 60.)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 112:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 87%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5358')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/optimizers_tests/test_optimizers.py: 179-195
</a>
<div class="mid" id="frag5358" style="display:none"><pre>
    def test_hooks(self):
        w_pre = np.copy(self.target.w.data)
        h_pre = WeightSaveHook()
        h_post = WeightSaveHook()
        self.create()
        self.optimizer.add_hook(h_pre, timing='pre')
        self.optimizer.add_hook(h_post, name='WeightSaveHookPost',
                                timing='post')

        x = chainer.Variable(np.array(5., dtype=np.float32))
        self.optimizer.update(self.target, x)
        w_post = np.copy(self.target.w.data)

        self.assertEqual(w_pre, h_pre.value)
        self.assertEqual(w_post, h_post.value)
        self.assertNotEqual(h_pre.value, h_post.value)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5359')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/optimizers_tests/test_optimizers.py: 196-215
</a>
<div class="mid" id="frag5359" style="display:none"><pre>
    def test_hooks_auto(self):
        w_pre = np.copy(self.target.w.data)
        h_pre = WeightSaveHook()
        h_pre.timing = 'pre'
        h_post = WeightSaveHook()
        h_post.timing = 'post'
        self.create()
        self.optimizer.add_hook(h_pre, timing='auto')
        self.optimizer.add_hook(h_post, name='WeightSaveHookPost',
                                timing='auto')

        x = chainer.Variable(np.array(5., dtype=np.float32))
        self.optimizer.update(self.target, x)
        w_post = np.copy(self.target.w.data)

        self.assertEqual(w_pre, h_pre.value)
        self.assertEqual(w_post, h_post.value)
        self.assertNotEqual(h_pre.value, h_post.value)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 113:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5363')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/optimizers_tests/test_optimizers.py: 229-242
</a>
<div class="mid" id="frag5363" style="display:none"><pre>
    def test_new_pickle(self):
        self.create()
        pickled_opt = pickle.dumps(self.optimizer)

        x = chainer.Variable(np.array(5., dtype=np.float32))
        self.optimizer.update(self.target, x)
        w_post = np.copy(self.target.w.data)
        # Pickle has saved a copy of the target
        opt = pickle.loads(pickled_opt)
        opt.update(opt.target, x)
        pickled_w_post = np.copy(opt.target.w.data)

        self.assertEqual(w_post, pickled_w_post)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5364')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/optimizers_tests/test_optimizers.py: 243-259
</a>
<div class="mid" id="frag5364" style="display:none"><pre>
    def test_updated_pickle(self):
        self.create()

        x = chainer.Variable(np.array(5., dtype=np.float32))
        self.optimizer.update(self.target, x)
        pickled_opt = pickle.dumps(self.optimizer)

        self.optimizer.update(self.target, x)
        w_post = np.copy(self.target.w.data)
        # Pickle has saved a copy of the target
        opt = pickle.loads(pickled_opt)
        opt.update(opt.target, x)
        pickled_w_post = np.copy(opt.target.w.data)

        self.assertEqual(w_post, pickled_w_post)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 114:</b> &nbsp; 3 fragments, nominal size 16 lines, similarity 77%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5393')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 244-259
</a>
<div class="mid" id="frag5393" style="display:none"><pre>

    def test_copy_with_share_mode(self):
        link = self.link.copy(mode='share')
        self.assertIsInstance(link._params, set)
        self.assertIsInstance(link._persistent, set)
        self.assertTrue(hasattr(link, 'x'))
        self.assertTrue(hasattr(link, 'y'))
        self.assertTrue(hasattr(link, 'u'))
        self.assertTrue(hasattr(link, 'p'))
        self.assertIsNot(link.x, self.link.x)
        self.assertIs(link.x.array, self.link.x.array)
        self.assertIsNot(link.y, self.link.y)
        self.assertIs(link.y.array, self.link.y.array)
        self.assertIsNone(link.u.array)
        self.assertIs(link.p, self.link.p)
        self.assertIs(link.name, None)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5395')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 276-294
</a>
<div class="mid" id="frag5395" style="display:none"><pre>

    def test_copy_with_init_mode(self):
        self.link.u.initializer = initializers.Normal(
            dtype=self.link.u.initializer.dtype)
        self.link.u.initialize((2, 3))
        link = self.link.copy(mode='init')
        self.assertFalse(numpy.array_equal(self.link.u.array, link.u.array))
        self.assertIsInstance(link._params, set)
        self.assertIsInstance(link._persistent, set)
        self.assertTrue(hasattr(link, 'x'))
        self.assertTrue(hasattr(link, 'y'))
        self.assertTrue(hasattr(link, 'u'))
        self.assertTrue(hasattr(link, 'p'))
        self.assertIsNot(link.x, self.link.x)
        self.assertIsNot(link.x.array, self.link.x.array)
        self.assertIsNot(link.y, self.link.y)
        self.assertIsNot(link.y.array, self.link.y.array)
        self.assertIsNot(link.p, self.link.p)
        self.assertIsNot(link.name, None)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5394')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 260-275
</a>
<div class="mid" id="frag5394" style="display:none"><pre>

    def test_copy_with_copy_mode(self):
        link = self.link.copy(mode='copy')
        self.assertIsInstance(link._params, set)
        self.assertIsInstance(link._persistent, set)
        self.assertTrue(hasattr(link, 'x'))
        self.assertTrue(hasattr(link, 'y'))
        self.assertTrue(hasattr(link, 'u'))
        self.assertTrue(hasattr(link, 'p'))
        self.assertIsNot(link.x, self.link.x)
        self.assertIsNot(link.x.array, self.link.x.array)
        self.assertIsNot(link.y, self.link.y)
        self.assertIsNot(link.y.array, self.link.y.array)
        self.assertIsNone(link.u.array)
        self.assertIsNot(link.p, self.link.p)
        self.assertIsNot(link.name, None)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 115:</b> &nbsp; 3 fragments, nominal size 15 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5402')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 378-395
</a>
<div class="mid" id="frag5402" style="display:none"><pre>

    def test_to_cpu_on_cpu(self):
        x = self.link.x.data
        gx = self.link.x.grad
        y = self.link.y.data
        gy = self.link.y.grad
        p = self.link.p
        with testing.assert_warns(DeprecationWarning):
            self.link.to_cpu()
        self.assertIs(self.link.x.data, x)
        self.assertIs(self.link.x.grad, gx)
        self.assertIs(self.link.y.data, y)
        self.assertIs(self.link.y.grad, gy)
        self.assertIsNone(self.link.u.data)
        u = self.link.u
        with pytest.raises(RuntimeError):
            u.grad
        self.assertIs(self.link.p, p)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5467')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 1274-1290
</a>
<div class="mid" id="frag5467" style="display:none"><pre>

    def test_to_cpu_on_cpu(self):
        x1 = self.l1.x.data
        gx1 = self.l1.x.grad
        x2 = self.l2.x.data
        gx2 = self.l2.x.grad
        x3 = self.l3.x.data

        with testing.assert_warns(DeprecationWarning):
            self.c2.to_cpu()
        self.assertIs(self.l1.x.data, x1)
        self.assertIs(self.l1.x.grad, gx1)
        self.assertIs(self.l2.x.data, x2)
        self.assertIs(self.l2.x.grad, gx2)
        self.assertIs(self.l3.x.data, x3)
        with pytest.raises(RuntimeError):
            self.l3.x.grad
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5517')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 1911-1928
</a>
<div class="mid" id="frag5517" style="display:none"><pre>

    def test_to_cpu_on_cpu(self):
        x1 = self.l1.x.data
        gx1 = self.l1.x.grad
        x2 = self.l2.x.data
        gx2 = self.l2.x.grad
        x3 = self.l3.x.data
        gx3 = self.l3.x.grad

        with testing.assert_warns(DeprecationWarning):
            self.c2.to_cpu()

        self.assertIs(self.l1.x.data, x1)
        self.assertIs(self.l1.x.grad, gx1)
        self.assertIs(self.l2.x.data, x2)
        self.assertIs(self.l2.x.grad, gx2)
        self.assertIs(self.l3.x.data, x3)
        self.assertIs(self.l3.x.grad, gx3)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 116:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 88%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5403')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 397-415
</a>
<div class="mid" id="frag5403" style="display:none"><pre>
    @attr.gpu
    def test_to_cpu(self):
        with testing.assert_warns(DeprecationWarning):
            self.link.to_gpu()
        with testing.assert_warns(DeprecationWarning):
            self.link.to_cpu()
        self.link.v.initialize((2, 3))
        self.assertIs(self.link.xp, numpy)
        self.assertIsInstance(self.link.x.data, numpy.ndarray)
        self.assertIsInstance(self.link.x.grad, numpy.ndarray)
        self.assertIsInstance(self.link.y.data, numpy.ndarray)
        self.assertIsInstance(self.link.y.grad, numpy.ndarray)
        self.assertIsNone(self.link.u.data)
        u = self.link.u
        with pytest.raises(RuntimeError):
            u.grad
        self.assertIsInstance(self.link.v.data, numpy.ndarray)
        self.assertIsInstance(self.link.v.grad, numpy.ndarray)
        self.assertIsInstance(self.link.p, numpy.ndarray)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5404')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 417-434
</a>
<div class="mid" id="frag5404" style="display:none"><pre>
    @attr.gpu
    def test_to_gpu(self):
        cupy = cuda.cupy
        with testing.assert_warns(DeprecationWarning):
            self.link.to_gpu()
        self.link.v.initialize((2, 3))
        self.assertIs(self.link.xp, cupy)
        self.assertIsInstance(self.link.x.data, cupy.ndarray)
        self.assertIsInstance(self.link.x.grad, cupy.ndarray)
        self.assertIsInstance(self.link.y.data, cupy.ndarray)
        self.assertIsInstance(self.link.y.grad, cupy.ndarray)
        self.assertIsNone(self.link.u.data)
        u = self.link.u
        with pytest.raises(RuntimeError):
            u.grad
        self.assertIsInstance(self.link.v.data, cupy.ndarray)
        self.assertIsInstance(self.link.v.grad, cupy.ndarray)
        self.assertIsInstance(self.link.p, cupy.ndarray)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 117:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5425')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 636-650
</a>
<div class="mid" id="frag5425" style="display:none"><pre>

    def test_count_params(self):
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            assert self.link.count_params() == 8
        assert len(w) == 2
        assert w[0].category is UserWarning

        self.link.u.initialize((2, 3))
        self.link.v.initialize((2, 3))
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            self.link.count_params()
        assert not w

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5484')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 1508-1523
</a>
<div class="mid" id="frag5484" style="display:none"><pre>

    def test_count_params(self):
        assert self.c1.count_params() == 8

        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            self.c2.count_params()
        assert len(w) == 1
        assert w[0].category is UserWarning

        self.c2.l3.x.initialize((3,))
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            self.c2.count_params()
        assert not w

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 118:</b> &nbsp; 3 fragments, nominal size 28 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5426')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 661-695
</a>
<div class="mid" id="frag5426" style="display:none"><pre>

    def test_serialize(self, backend_config):
        call_record = []

        def serializer(key, value):
            call_record.append((key, value))
            return value

        l = chainer.Link()
        with l.init_scope():
            l.x = chainer.Parameter(shape=self.shape_x)
            l.y = chainer.Parameter(shape=self.shape_y)
        l.add_persistent('z', 1)
        l.to_device(backend_config.device)

        old_x_data = l.x.array
        old_y_data = l.y.array
        old_z = l.z

        l.serialize(serializer)

        # Link data are not modified
        self.assertIs(l.x.array, old_x_data)
        self.assertIs(l.y.array, old_y_data)
        self.assertEqual(l.z, old_z)

        # Check inputs to the serializer
        self.assertEqual(len(call_record), 3)
        call_record = sorted(call_record)
        self.assertEqual(call_record[0][0], 'x')
        self.assertIs(call_record[0][1], l.x.array)
        self.assertEqual(call_record[1][0], 'y')
        self.assertIs(call_record[1][1], l.y.array)
        self.assertEqual(call_record[2][0], 'z')
        self.assertEqual(call_record[2][1], old_z)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5428')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 696-741
</a>
<div class="mid" id="frag5428" style="display:none"><pre>

    def test_deserialize(self, backend_config):
        call_record = []

        state = {
            'x': _shaped_random(self.shape_x, 'float32'),
            'y': _shaped_random(self.shape_y, 'float32'),
            'z': numpy.random.randn(),
        }

        def deserializer(key, value):
            call_record.append((key, value))
            if key == 'z':
                return state[key]  # scalar
            value[...] = backend_config.device.send(state[key])
            return value

        l = chainer.Link()
        with l.init_scope():
            l.x = chainer.Parameter(shape=self.shape_x)
            l.y = chainer.Parameter(shape=self.shape_y)
        l.add_persistent('z', 1)
        l.to_device(backend_config.device)

        old_x_data = l.x.array
        old_y_data = l.y.array
        old_z = l.z

        l.serialize(deserializer)

        # Check link data
        self.assertIs(l.x.array, old_x_data)
        self.assertIs(l.y.array, old_y_data)
        _assert_arrays_equal(l.x.array, state['x'])
        _assert_arrays_equal(l.y.array, state['y'])
        self.assertEqual(l.z, state['z'])

        # Check inputs to the deserializer
        self.assertEqual(len(call_record), 3)
        call_record = sorted(call_record)
        self.assertEqual(call_record[0][0], 'x')
        self.assertIs(call_record[0][1], l.x.array)
        self.assertEqual(call_record[1][0], 'y')
        self.assertIs(call_record[1][1], l.y.array)
        self.assertEqual(call_record[2][0], 'z')
        self.assertEqual(call_record[2][1], old_z)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5430')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 742-773
</a>
<div class="mid" id="frag5430" style="display:none"><pre>

    def test_deserialize_uninitialized1(self, backend_config):
        # Deserializes uninitialized parameters into initialized ones.
        # TODO(niboshi): Currently the existing initialized parameters are
        # untouched, but maybe uninitialized state should be restored? (#7916)
        call_record = []

        def deserializer(key, value):
            call_record.append((key, value))
            return None  # to be uninitialized

        l = chainer.Link()
        with l.init_scope():
            l.x = chainer.Parameter(shape=self.shape_x)  # initialized
            l.y = chainer.Parameter(shape=self.shape_y)  # initialized
        l.to_device(backend_config.device)

        old_x_data = l.x.array
        old_y_data = l.y.array

        l.serialize(deserializer)

        # Link is kept untouched
        self.assertIs(l.x.array, old_x_data)
        self.assertIs(l.y.array, old_y_data)

        # Check inputs to the deserializer
        self.assertEqual(len(call_record), 2)
        call_record = sorted(call_record)
        self.assertEqual(call_record[0][0], 'x')
        self.assertIs(call_record[0][1], l.x.array)
        self.assertEqual(call_record[1][0], 'y')
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 119:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5434')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 815-836
</a>
<div class="mid" id="frag5434" style="display:none"><pre>

    def test_serialize(self, backend_config):
        call_record = []

        def serializer(key, value):
            call_record.append((key, value))
            return value

        l = chainer.Link()
        with l.init_scope():
            l.x = chainer.Parameter()  # uninitialized
        l.to_device(backend_config.device)

        l.serialize(serializer)

        # Link is kept uninitialized
        self.assertIsNone(l.x.array)

        # Check inputs to the serializer
        self.assertEqual(len(call_record), 1)
        self.assertEqual(call_record[0][0], 'x')
        self.assertIs(call_record[0][1], None)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5436')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 837-860
</a>
<div class="mid" id="frag5436" style="display:none"><pre>

    def test_deserialize(self, backend_config):
        # Deserializes uninitialized parameters into uninitialied ones.
        call_record = []

        def serializer(key, value):
            call_record.append((key, value))
            return None  # to be uninitialized

        l = chainer.Link()
        with l.init_scope():
            l.x = chainer.Parameter()  # uninitialized
        l.to_device(backend_config.device)

        l.serialize(serializer)

        # Link is kept uninitialized
        self.assertIsNone(l.x.array)

        # Check inputs to the serializer
        self.assertEqual(len(call_record), 1)
        self.assertEqual(call_record[0][0], 'x')
        self.assertIs(call_record[0][1], None)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 120:</b> &nbsp; 3 fragments, nominal size 12 lines, similarity 84%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5451')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 976-992
</a>
<div class="mid" id="frag5451" style="display:none"><pre>

    def test_repeat_with_init(self):
        ret = self.link.repeat(2, mode='init')
        self.assertEqual(len(ret), 2)
        # Both should be different objects from the original link
        self.assertIsNot(ret[0], self.link)
        self.assertIsNot(ret[1], self.link)
        # Object IDs of elements should be different
        self.assertIsNot(ret[0], ret[1])
        self.assertIsNot(ret[0].x, ret[1].x)
        # But shape and type of paratmeres shuld be same
        self.assertEqual(ret[0].x.shape, self.link.x.shape)
        self.assertEqual(ret[0].x.dtype, self.link.x.dtype)
        self.assertEqual(ret[0].x.shape, ret[1].x.shape)
        self.assertEqual(ret[0].x.dtype, ret[1].x.dtype)
        # Parameters are re-initialized, so the values should be different
        self.assertFalse(numpy.all(ret[0].x.array == ret[1].x.array))
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5452')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 993-1008
</a>
<div class="mid" id="frag5452" style="display:none"><pre>

    def test_repeat_with_copy(self):
        ret = self.link.repeat(2, mode='copy')
        self.assertEqual(len(ret), 2)
        # Both should be different objects from the original link
        self.assertIsNot(ret[0], self.link)
        self.assertIsNot(ret[1], self.link)
        # Object IDs of elements should be different
        self.assertIsNot(ret[0], ret[1])
        self.assertIsNot(ret[0].x, ret[1].x)
        # But shape, type, and value of paratmeres shuld be same
        self.assertEqual(ret[0].x.shape, self.link.x.shape)
        self.assertEqual(ret[0].x.dtype, self.link.x.dtype)
        self.assertEqual(ret[0].x.shape, ret[1].x.shape)
        self.assertEqual(ret[0].x.dtype, ret[1].x.dtype)
        numpy.testing.assert_array_equal(ret[0].x.array, ret[1].x.array)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5453')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 1009-1027
</a>
<div class="mid" id="frag5453" style="display:none"><pre>

    def test_repeat_with_share(self):
        ret = self.link.repeat(2, mode='share')
        self.assertEqual(len(ret), 2)
        # Both should be different objects from the original link
        self.assertIsNot(ret[0], self.link)
        self.assertIsNot(ret[1], self.link)
        # Object IDs of elements should be different
        self.assertIsNot(ret[0], ret[1])
        self.assertIsNot(ret[0].x, ret[1].x)
        # But the array objects should be the same
        self.assertIs(ret[0].x.array, ret[1].x.array)
        # But shape, type, and value of paratmeres shuld be same
        self.assertEqual(ret[0].x.shape, self.link.x.shape)
        self.assertEqual(ret[0].x.dtype, self.link.x.dtype)
        self.assertEqual(ret[0].x.shape, ret[1].x.shape)
        self.assertEqual(ret[0].x.dtype, ret[1].x.dtype)
        numpy.testing.assert_array_equal(ret[0].x.array, ret[1].x.array)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 121:</b> &nbsp; 3 fragments, nominal size 40 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5464')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 1130-1171
</a>
<div class="mid" id="frag5464" style="display:none"><pre>

    def test_copy_with_share_mode(self):
        self.l1.x.initializer = initializers.Normal(
            dtype=self.l1.x.initializer.dtype)
        self.l1.x.initialize(self.l1.x.shape)
        self.l2.x.initializer = initializers.Normal(
            dtype=self.l2.x.initializer.dtype)
        self.l2.x.initialize(self.l2.x.shape)
        self.x.initializer = initializers.Normal(
            dtype=self.x.initializer.dtype)
        self.x.initialize(self.x.shape)

        c2 = self.c2.copy(mode='share')
        self.assertIs(c2.name, None)
        self.assertIsInstance(c2._children, set)
        self.assertTrue(hasattr(c2, 'x'))
        self.assertIsNot(c2.x, self.x)
        self.assertIs(c2.x.data, self.x.data)

        self.assertTrue(hasattr(c2, 'c1'))
        self.assertEqual(c2.c1.name, 'c1')
        self.assertIsInstance(c2.c1._children, set)
        self.assertIsNot(c2.c1, self.c1)
        self.assertEqual(c2.c1.l1.name, 'l1')
        self.assertIsNot(c2.c1.l1, self.l1)
        self.assertIsNot(c2.c1.l1.x, self.l1.x)
        self.assertIs(c2.c1.l1.x.data, self.l1.x.data)
        self.assertIs(c2.c1.l1.x.grad, None)

        self.assertTrue(hasattr(c2.c1, 'l2'))
        self.assertEqual(c2.c1.l2.name, 'l2')
        self.assertIsNot(c2.c1.l2, self.l2)
        self.assertIsNot(c2.c1.l2.x, self.l2.x)
        self.assertIs(c2.c1.l2.x.data, self.l2.x.data)
        self.assertIs(c2.c1.l2.x.grad, None)

        self.assertTrue(hasattr(c2, 'l3'))
        self.assertEqual(c2.l3.name, 'l3')
        self.assertIsNot(c2.l3, self.l3)
        self.assertIsNot(c2.l3.x, self.l3.x)
        self.assertIs(c2.l3.x.data, self.l3.x.data)
        self.assertIs(c2.l3.x.grad, None)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5466')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 1219-1273
</a>
<div class="mid" id="frag5466" style="display:none"><pre>

    def test_copy_with_init_mode(self):
        self.l1.x.initializer = initializers.Normal(
            dtype=self.l1.x.initializer.dtype)
        self.l1.x.initialize(self.l1.x.shape)
        self.l2.x.initializer = initializers.Normal(
            dtype=self.l2.x.initializer.dtype)
        self.l2.x.initialize(self.l2.x.shape)
        self.x.initializer = initializers.Normal(
            dtype=self.x.initializer.dtype)
        self.c2.x.initialize(self.x.shape)

        c2 = self.c2.copy(mode='init')
        self.assertIs(c2.name, None)
        self.assertIsInstance(c2._children, set)
        self.assertTrue(hasattr(c2, 'x'))
        self.assertIsNot(c2.x, self.x)
        self.assertIsNot(c2.x.data, self.x.data)
        self.assertFalse(numpy.array_equal(c2.x.data, self.x.data))
        # _grad_initializer attribute in a copied Parameter has constant.NaN
        # after calling initilize() method
        self.assertTrue(numpy.isnan(c2.x.grad).all())

        self.assertTrue(hasattr(c2, 'c1'))
        self.assertEqual(c2.c1.name, 'c1')
        self.assertIsInstance(c2.c1._children, set)
        self.assertIsNot(c2.c1, self.c1)
        self.assertEqual(c2.c1.l1.name, 'l1')
        self.assertIsNot(c2.c1.l1, self.l1)
        self.assertIsNot(c2.c1.l1.x, self.l1.x)
        self.assertIsNot(c2.c1.l1.x.data, self.l1.x.data)
        self.assertFalse(numpy.array_equal(c2.c1.l1.x.data, self.l1.x.data))
        # _grad_initializer attribute in a copied Parameter has constant.NaN
        # after calling initilize() method
        self.assertTrue(numpy.isnan(c2.c1.l1.x.grad).all())

        self.assertTrue(hasattr(c2.c1, 'l2'))
        self.assertEqual(c2.c1.l2.name, 'l2')
        self.assertIsNot(c2.c1.l2, self.l2)
        self.assertIsNot(c2.c1.l2.x, self.l2.x)
        self.assertIsNot(c2.c1.l2.x.data, self.l2.x.data)
        self.assertFalse(numpy.array_equal(c2.c1.l2.x.data, self.l2.x.data))
        # _grad_initializer attribute in a copied Parameter has constant.NaN
        # after calling initilize() method
        self.assertTrue(numpy.isnan(c2.c1.l2.x.grad).all())

        self.assertTrue(hasattr(c2, 'l3'))
        self.assertEqual(c2.l3.name, 'l3')
        self.assertIsNot(c2.l3, self.l3)
        self.assertIsNot(c2.l3.x, self.l3.x)
        self.assertIs(c2.l3.x.data, self.l3.x.data)
        # A Parameter constructed with shape argument but not initialized
        # has invalid grad
        with pytest.raises(RuntimeError):
            c2.l3.x.grad
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5465')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 1172-1218
</a>
<div class="mid" id="frag5465" style="display:none"><pre>

    def test_copy_with_copy_mode(self):
        self.l1.x.initializer = initializers.Normal(
            dtype=self.l1.x.initializer.dtype)
        self.l1.x.initialize(self.l1.x.shape)
        self.l2.x.initializer = initializers.Normal(
            dtype=self.l2.x.initializer.dtype)
        self.l2.x.initialize(self.l2.x.shape)
        self.x.initializer = initializers.Normal(
            dtype=self.x.initializer.dtype)
        self.x.initialize(self.x.shape)

        c2 = self.c2.copy(mode='copy')
        self.assertIs(c2.name, None)
        self.assertIsInstance(c2._children, set)
        self.assertTrue(hasattr(c2, 'x'))
        self.assertIsNot(c2.x, self.x)
        self.assertIsNot(c2.x.data, self.x.data)
        self.assertTrue(numpy.array_equal(c2.x.data, self.x.data))

        self.assertTrue(hasattr(c2, 'c1'))
        self.assertEqual(c2.c1.name, 'c1')
        self.assertIsInstance(c2.c1._children, set)
        self.assertIsNot(c2.c1, self.c1)
        self.assertEqual(c2.c1.l1.name, 'l1')
        self.assertIsNot(c2.c1.l1, self.l1)
        self.assertIsNot(c2.c1.l1.x, self.l1.x)
        self.assertIsNot(c2.c1.l1.x.data, self.l1.x.data)
        self.assertTrue(numpy.array_equal(c2.c1.l1.x.data, self.l1.x.data))
        self.assertIs(c2.c1.l1.x.grad, None)

        self.assertTrue(hasattr(c2.c1, 'l2'))
        self.assertEqual(c2.c1.l2.name, 'l2')
        self.assertIsNot(c2.c1.l2, self.l2)
        self.assertIsNot(c2.c1.l2.x, self.l2.x)
        self.assertIsNot(c2.c1.l2.x.data, self.l2.x.data)
        self.assertTrue(numpy.array_equal(c2.c1.l2.x.data, self.l2.x.data))
        self.assertIs(c2.c1.l2.x.grad, None)

        self.assertTrue(hasattr(c2, 'l3'))
        self.assertEqual(c2.l3.name, 'l3')
        self.assertIsNot(c2.l3, self.l3)
        self.assertIsNot(c2.l3.x, self.l3.x)
        self.assertIs(c2.l3.x.data, self.l3.x.data)
        x = c2.l3.x
        with pytest.raises(RuntimeError):
            x.grad
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 122:</b> &nbsp; 6 fragments, nominal size 18 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5468')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 1292-1313
</a>
<div class="mid" id="frag5468" style="display:none"><pre>
    @attr.gpu
    def test_to_cpu(self):
        self.set_count_parameters()
        with testing.assert_warns(DeprecationWarning):
            self.c2.to_gpu()
        with testing.assert_warns(DeprecationWarning):
            self.c2.to_cpu()
        self.assertIs(self.c2.xp, numpy)
        self.assertIs(self.c1.xp, numpy)
        self.assertIs(self.l1.xp, numpy)
        self.assertIs(self.l2.xp, numpy)
        self.assertIs(self.l3.xp, numpy)
        self.assertIsInstance(self.l1.x.data, numpy.ndarray)
        self.assertIsInstance(self.l1.x.grad, numpy.ndarray)
        self.assertIsInstance(self.l2.x.data, numpy.ndarray)
        self.assertIsInstance(self.l2.x.grad, numpy.ndarray)
        self.assertIsNone(self.l3.x.data)
        self.assertIsNone(self.l3.x.grad)

        self.l3.x.initialize(3)
        self.assertIsInstance(self.l3.x.data, numpy.ndarray)
        self.assertIsInstance(self.l3.x.grad, numpy.ndarray)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5518')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 1930-1946
</a>
<div class="mid" id="frag5518" style="display:none"><pre>
    @attr.gpu
    def test_to_cpu(self):
        with testing.assert_warns(DeprecationWarning):
            self.c2.to_gpu()
        with testing.assert_warns(DeprecationWarning):
            self.c2.to_cpu()
        self.assertIs(self.c2.xp, numpy)
        self.assertIs(self.c1.xp, numpy)
        self.assertIs(self.l1.xp, numpy)
        self.assertIs(self.l2.xp, numpy)
        self.assertIs(self.l3.xp, numpy)
        self.assertIsInstance(self.l1.x.data, numpy.ndarray)
        self.assertIsInstance(self.l1.x.grad, numpy.ndarray)
        self.assertIsInstance(self.l2.x.data, numpy.ndarray)
        self.assertIsInstance(self.l2.x.grad, numpy.ndarray)
        self.assertIsInstance(self.l3.x.data, numpy.ndarray)
        self.assertIsInstance(self.l3.x.grad, numpy.ndarray)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5469')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 1315-1335
</a>
<div class="mid" id="frag5469" style="display:none"><pre>
    @attr.gpu
    def test_to_gpu(self):
        self.set_count_parameters()
        cupy = cuda.cupy
        with testing.assert_warns(DeprecationWarning):
            self.c2.to_gpu()
        self.assertIs(self.c2.xp, cupy)
        self.assertIs(self.c1.xp, cupy)
        self.assertIs(self.l1.xp, cupy)
        self.assertIs(self.l2.xp, cupy)
        self.assertIs(self.l3.xp, cupy)
        self.assertIsInstance(self.l1.x.data, cupy.ndarray)
        self.assertIsInstance(self.l1.x.grad, cupy.ndarray)
        self.assertIsInstance(self.l2.x.data, cupy.ndarray)
        self.assertIsInstance(self.l2.x.grad, cupy.ndarray)
        self.assertIsNone(self.l3.x.data)
        self.assertIsNone(self.l3.x.grad)

        self.l3.x.initialize(3)
        self.assertIsInstance(self.l3.x.data, cupy.ndarray)
        self.assertIsInstance(self.l3.x.grad, cupy.ndarray)
</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5519')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 1948-1963
</a>
<div class="mid" id="frag5519" style="display:none"><pre>
    @attr.gpu
    def test_to_gpu(self):
        cupy = cuda.cupy
        with testing.assert_warns(DeprecationWarning):
            self.c2.to_gpu()
        self.assertIs(self.c2.xp, cupy)
        self.assertIs(self.c1.xp, cupy)
        self.assertIs(self.l1.xp, cupy)
        self.assertIs(self.l2.xp, cupy)
        self.assertIs(self.l3.xp, cupy)
        self.assertIsInstance(self.l1.x.data, cupy.ndarray)
        self.assertIsInstance(self.l1.x.grad, cupy.ndarray)
        self.assertIsInstance(self.l2.x.data, cupy.ndarray)
        self.assertIsInstance(self.l2.x.grad, cupy.ndarray)
        self.assertIsInstance(self.l3.x.data, cupy.ndarray)
        self.assertIsInstance(self.l3.x.grad, cupy.ndarray)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5470')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 1336-1354
</a>
<div class="mid" id="frag5470" style="display:none"><pre>

    def test_to_device(self):
        self.set_count_parameters()
        device = backend.CpuDevice()
        self.c2.to_device(device)
        self.assertIs(self.c2.xp, numpy)
        self.assertIs(self.c1.xp, numpy)
        self.assertIs(self.l1.xp, numpy)
        self.assertIs(self.l2.xp, numpy)
        self.assertIs(self.l3.xp, numpy)
        self.assertIsInstance(self.l1.x.data, numpy.ndarray)
        self.assertIsInstance(self.l1.x.grad, numpy.ndarray)
        self.assertIsInstance(self.l2.x.data, numpy.ndarray)
        self.assertIsInstance(self.l2.x.grad, numpy.ndarray)
        self.assertIsNone(self.l3.x.data)

        self.l3.x.initialize((3,))
        self.assertIsInstance(self.l3.x.data, numpy.ndarray)
        self.assertIsInstance(self.l3.x.grad, numpy.ndarray)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5521')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 1987-2001
</a>
<div class="mid" id="frag5521" style="display:none"><pre>

    def test_to_device(self):
        device = backend.CpuDevice()
        self.c2.to_device(device)
        self.assertIs(self.c2.xp, numpy)
        self.assertIs(self.c1.xp, numpy)
        self.assertIs(self.l1.xp, numpy)
        self.assertIs(self.l2.xp, numpy)
        self.assertIs(self.l3.xp, numpy)
        self.assertIsInstance(self.l1.x.data, numpy.ndarray)
        self.assertIsInstance(self.l1.x.grad, numpy.ndarray)
        self.assertIsInstance(self.l2.x.data, numpy.ndarray)
        self.assertIsInstance(self.l2.x.grad, numpy.ndarray)
        self.assertIsInstance(self.l3.x.data, numpy.ndarray)
        self.assertIsInstance(self.l3.x.grad, numpy.ndarray)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 123:</b> &nbsp; 2 fragments, nominal size 30 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5480')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 1415-1445
</a>
<div class="mid" id="frag5480" style="display:none"><pre>

    def test_copyparams(self):
        l1 = chainer.Link()
        with l1.init_scope():
            l1.x = chainer.Parameter(shape=(2, 3))
        l2 = chainer.Link()
        with l2.init_scope():
            l2.x = chainer.Parameter(shape=2)
        l3 = chainer.Link()
        with l3.init_scope():
            l3.x = chainer.Parameter(shape=3)
        c1 = chainer.Chain()
        with c1.init_scope():
            c1.l1 = l1
            c1.l2 = l2
        c2 = chainer.Chain()
        with c2.init_scope():
            c2.c1 = c1
            c2.l3 = l3
            c2.x = chainer.Parameter(shape=2)
        l1.x.data.fill(0)
        l2.x.data.fill(1)
        l3.x.data.fill(2)
        c2.x.data.fill(3)

        self.c2.copyparams(c2)

        numpy.testing.assert_array_equal(self.l1.x.data, l1.x.data)
        numpy.testing.assert_array_equal(self.l2.x.data, l2.x.data)
        numpy.testing.assert_array_equal(self.l3.x.data, l3.x.data)
        numpy.testing.assert_array_equal(self.c2.x.data, c2.x.data)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5482')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 1459-1493
</a>
<div class="mid" id="frag5482" style="display:none"><pre>

    def test_addgrads(self):
        l1 = chainer.Link()
        with l1.init_scope():
            l1.x = chainer.Parameter(shape=(2, 3))
        l2 = chainer.Link()
        with l2.init_scope():
            l2.x = chainer.Parameter(shape=2)
        l3 = chainer.Link()
        with l3.init_scope():
            l3.x = chainer.Parameter(shape=3)
        c1 = chainer.Chain()
        with c1.init_scope():
            c1.l1 = l1
            c1.l2 = l2
        c2 = chainer.Chain()
        with c2.init_scope():
            c2.c1 = c1
            c2.l3 = l3
            c2.x = chainer.Parameter(shape=2)
        l1.x.grad.fill(1)
        l2.x.grad.fill(2)
        l3.x.grad.fill(3)
        c2.x.grad.fill(4)

        self.l1.x.grad.fill(-1)
        self.l2.x.grad.fill(-2)
        self.c2.x.grad.fill(-3)
        self.l3.cleargrads()

        self.c2.addgrads(c2)
        numpy.testing.assert_array_equal(self.l1.x.grad, numpy.zeros((2, 3)))
        numpy.testing.assert_array_equal(self.l2.x.grad, numpy.zeros(2))
        numpy.testing.assert_array_equal(self.l3.x.grad, numpy.full(3, 3.))
        numpy.testing.assert_array_equal(self.c2.x.grad, numpy.ones(2))
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 124:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5487')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 1551-1563
</a>
<div class="mid" id="frag5487" style="display:none"><pre>

    def test_to_chx(self, backend_config):
        self.set_count_parameters()
        self.c2.to_device(backend_config.device)
        self.c2.to_chx()

        src_device = backend_config.device
        if src_device.xp is chainerx:
            expected_device = src_device
        else:
            expected_device = (
                backend.ChainerxDevice.from_fallback_device(src_device))
        self.check_expected_device(expected_device)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5488')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 1564-1576
</a>
<div class="mid" id="frag5488" style="display:none"><pre>

    def test_from_chx(self, backend_config):
        self.set_count_parameters()
        self.c2.to_device(backend_config.device)
        self.c2.from_chx()

        src_device = backend_config.device
        if src_device.xp is chainerx:
            expected_device = src_device.fallback_device
        else:
            expected_device = src_device
        self.check_expected_device(expected_device)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 125:</b> &nbsp; 2 fragments, nominal size 23 lines, similarity 86%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5494')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 1618-1641
</a>
<div class="mid" id="frag5494" style="display:none"><pre>

    def test_repeat_with_copy_mode(self):
        ret = self.chain.repeat(2, mode='copy')
        self.assertEqual(len(ret), 2)
        self.assertIsNot(ret[0], self.chain)
        self.assertIsNot(ret[1], self.chain)
        self.assertIsNot(ret[0], ret[1])
        self.assertIsNot(ret[0].link, self.chain.link)
        self.assertIsNot(ret[1].link, self.chain.link)
        self.assertIsNot(ret[0].link, ret[1].link)
        self.assertIsNot(ret[0].link.x, self.link.x)
        self.assertIsNot(ret[1].link.x, self.link.x)
        self.assertIsNot(ret[0].link.x, ret[1].link.x)
        self.assertIsNot(ret[0].link.x.data, self.chain.link.x.data)
        self.assertIsNot(ret[1].link.x.data, self.chain.link.x.data)
        self.assertIsNot(ret[0].link.x.data, ret[1].link.x.data)
        self.assertTrue(numpy.array_equal(
            ret[0].link.x.data, self.chain.link.x.data))
        self.assertTrue(numpy.array_equal(
            ret[0].link.x.data, ret[1].link.x.data))
        self.assertEqual(ret[0].link.x.shape, self.chain.link.x.shape)
        self.assertEqual(ret[0].link.x.shape, ret[1].link.x.shape)
        self.assertEqual(ret[0].link.x.dtype, self.chain.link.x.dtype)
        self.assertEqual(ret[0].link.x.dtype, ret[1].link.x.dtype)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5495')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 1642-1665
</a>
<div class="mid" id="frag5495" style="display:none"><pre>

    def test_repeat_with_init_mode(self):
        ret = self.chain.repeat(2, mode='init')
        self.assertEqual(len(ret), 2)
        self.assertIsNot(ret[0], self.chain)
        self.assertIsNot(ret[1], self.chain)
        self.assertIsNot(ret[0], ret[1])
        self.assertIsNot(ret[0].link, self.chain.link)
        self.assertIsNot(ret[1].link, self.chain.link)
        self.assertIsNot(ret[0].link.x, ret[1].link.x)
        self.assertIsNot(ret[0].link.x.data, self.chain.link.x.data)
        self.assertIsNot(ret[1].link.x.data, self.chain.link.x.data)
        self.assertIsNot(ret[0].link.x.data, ret[1].link.x.data)
        self.assertFalse(numpy.array_equal(
            ret[0].link.x.data, self.chain.link.x.data))
        self.assertFalse(numpy.array_equal(
            ret[1].link.x.data, self.chain.link.x.data))
        self.assertFalse(numpy.array_equal(
            ret[0].link.x.data, ret[1].link.x.data))
        self.assertEqual(ret[0].link.x.shape, self.chain.link.x.shape)
        self.assertEqual(ret[0].link.x.shape, ret[1].link.x.shape)
        self.assertEqual(ret[0].link.x.dtype, self.chain.link.x.dtype)
        self.assertEqual(ret[0].link.x.dtype, ret[1].link.x.dtype)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 126:</b> &nbsp; 3 fragments, nominal size 29 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5511')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 1776-1807
</a>
<div class="mid" id="frag5511" style="display:none"><pre>

    def test_copy_with_share_mode(self):
        c2 = self.c2.copy(mode='share')
        self.l1.x.initializer = initializers.Normal(
            dtype=self.l1.x.initializer.dtype)
        self.l1.x.initialize(self.l1.x.shape)
        self.l2.x.initializer = initializers.Normal(
            dtype=self.l2.x.initializer.dtype)
        self.l2.x.initialize(self.l2.x.shape)

        self.assertIs(c2.name, None)
        self.assertIsInstance(c2._children, list)
        self.assertIsNot(c2[0], self.c1)
        self.assertEqual(c2[0].name, '0')
        self.assertIsInstance(c2[0]._children, list)
        self.assertIsNot(c2[0][0], self.l1)
        self.assertEqual(c2[0][0].name, '0')
        self.assertIsNot(c2[0][0].x, self.l1.x)
        self.assertIs(c2[0][0].x.data, self.l1.x.data)
        self.assertIs(c2[0][0].x.grad, None)

        self.assertIsNot(c2[0][1], self.l2)
        self.assertEqual(c2[0][1].name, '1')
        self.assertIsNot(c2[0][1].x, self.l2.x)
        self.assertIs(c2[0][1].x.data, self.l2.x.data)
        self.assertIs(c2[0][1].x.grad, None)

        self.assertIsNot(c2[1], self.l3)
        self.assertEqual(c2[1].name, '1')
        self.assertIsNot(c2[1].x, self.l3.x)
        self.assertIs(c2[1].x.data, self.l3.x.data)
        self.assertIs(c2[1].x.grad, None)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5513')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 1842-1878
</a>
<div class="mid" id="frag5513" style="display:none"><pre>

    def test_copy_with_init_mode(self):
        self.l1.x.initializer = initializers.Normal(
            dtype=self.l1.x.initializer.dtype)
        self.l1.x.initialize(self.l1.x.shape)
        self.l2.x.initializer = initializers.Normal(
            dtype=self.l2.x.initializer.dtype)
        self.l2.x.initialize(self.l2.x.shape)

        c2 = self.c2.copy(mode='init')
        self.assertIs(c2.name, None)
        self.assertIsInstance(c2._children, list)
        self.assertEqual(c2[0].name, '0')
        self.assertIsInstance(c2[0]._children, list)
        self.assertIsNot(c2[0][0], self.l1)
        self.assertEqual(c2[0][0].name, '0')
        self.assertIsNot(c2[0][0].x, self.l1.x)
        self.assertIsNot(c2[0][0].x.data, self.l1.x.data)
        self.assertFalse(numpy.array_equal(c2[0][0].x.data, self.l1.x.data))
        # _grad_initializer attribute in a copied Parameter has constant.NaN
        # after calling initilize() method
        self.assertTrue(numpy.isnan(c2[0][0].x.grad).all())

        self.assertIsNot(c2[0][1], self.l2)
        self.assertEqual(c2[0][1].name, '1')
        self.assertIsNot(c2[0][1].x, self.l2.x)
        self.assertIsNot(c2[0][1].x.data, self.l2.x.data)
        self.assertFalse(numpy.array_equal(c2[0][1].x.data, self.l2.x.data))
        # _grad_initializer attribute in a copied Parameter has constant.NaN
        # after calling initilize() method
        self.assertTrue(numpy.isnan(c2[0][1].x.grad).all())

        self.assertIsNot(c2[1], self.l3)
        self.assertEqual(c2[1].name, '1')
        self.assertIsNot(c2[1].x, self.l3.x)
        self.assertTrue(numpy.isnan(c2[1].x.data).all())
        self.assertTrue(numpy.isnan(c2[1].x.grad).all())
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5512')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 1808-1841
</a>
<div class="mid" id="frag5512" style="display:none"><pre>

    def test_copy_with_copy_mode(self):
        self.l1.x.initializer = initializers.Normal(
            dtype=self.l1.x.initializer.dtype)
        self.l1.x.initialize(self.l1.x.shape)
        self.l2.x.initializer = initializers.Normal(
            dtype=self.l2.x.initializer.dtype)
        self.l2.x.initialize(self.l2.x.shape)

        c2 = self.c2.copy(mode='copy')
        self.assertIs(c2.name, None)
        self.assertIsInstance(c2._children, list)
        self.assertEqual(c2[0].name, '0')
        self.assertIsInstance(c2[0]._children, list)
        self.assertIsNot(c2[0][0], self.l1)
        self.assertEqual(c2[0][0].name, '0')
        self.assertIsNot(c2[0][0].x, self.l1.x)
        self.assertIsNot(c2[0][0].x.data, self.l1.x.data)
        self.assertTrue(numpy.array_equal(c2[0][0].x.data, self.l1.x.data))
        self.assertIs(c2[0][0].x.grad, None)

        self.assertIsNot(c2[0][1], self.l2)
        self.assertEqual(c2[0][1].name, '1')
        self.assertIsNot(c2[0][1].x, self.l2.x)
        self.assertIsNot(c2[0][1].x.data, self.l2.x.data)
        self.assertTrue(numpy.array_equal(c2[0][1].x.data, self.l2.x.data))
        self.assertIs(c2[0][1].x.grad, None)

        self.assertIsNot(c2[1], self.l3)
        self.assertEqual(c2[1].name, '1')
        self.assertIsNot(c2[1].x, self.l3.x)
        self.assertIsNot(c2[1].x.data, self.l3.x.data)
        # l3 is constructed with shape argument but not initialized
        self.assertTrue(numpy.isnan(c2[1].x.grad).all())
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 127:</b> &nbsp; 3 fragments, nominal size 21 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5541')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 2199-2217
</a>
<div class="mid" id="frag5541" style="display:none"><pre>

    def test_repeat_with_share_mode(self):
        ret = self.chainlist.repeat(2, mode='share')
        self.assertEqual(len(ret), 2)
        self.assertIsNot(ret[0], self.chainlist)
        self.assertIsNot(ret[1], self.chainlist)
        self.assertIsNot(ret[0], ret[1])
        self.assertIsNot(ret[0][0], self.chainlist[0])
        self.assertIsNot(ret[1][0], self.chainlist[0])
        self.assertIsNot(ret[0][0], ret[1][0])
        self.assertIsNot(ret[0][0].x, self.chainlist[0].x)
        self.assertIsNot(ret[1][0].x, self.chainlist[0].x)
        self.assertIsNot(ret[0][0].x, ret[1][0].x)
        self.assertIs(ret[0][0].x.data, self.chainlist[0].x.data)
        self.assertIs(ret[0][0].x.data, ret[1][0].x.data)
        self.assertEqual(ret[0][0].x.shape, self.chainlist[0].x.shape)
        self.assertEqual(ret[0][0].x.shape, ret[1][0].x.shape)
        self.assertEqual(ret[0][0].x.dtype, self.chainlist[0].x.dtype)
        self.assertEqual(ret[0][0].x.dtype, ret[1][0].x.dtype)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5542')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 2218-2241
</a>
<div class="mid" id="frag5542" style="display:none"><pre>

    def test_repeat_with_copy_mode(self):
        ret = self.chainlist.repeat(2, mode='copy')
        self.assertEqual(len(ret), 2)
        self.assertIsNot(ret[0], self.chainlist)
        self.assertIsNot(ret[1], self.chainlist)
        self.assertIsNot(ret[0], ret[1])
        self.assertIsNot(ret[0][0], self.chainlist[0])
        self.assertIsNot(ret[1][0], self.chainlist[0])
        self.assertIsNot(ret[0][0], ret[1][0])
        self.assertIsNot(ret[0][0].x, self.chainlist[0].x)
        self.assertIsNot(ret[1][0].x, self.chainlist[0].x)
        self.assertIsNot(ret[0][0].x, ret[1][0].x)
        self.assertIsNot(ret[0][0].x.data, self.chainlist[0].x.data)
        self.assertIsNot(ret[1][0].x.data, self.chainlist[0].x.data)
        self.assertIsNot(ret[0][0].x.data, ret[1][0].x.data)
        self.assertTrue(numpy.array_equal(
            ret[0][0].x.data, self.chainlist[0].x.data))
        self.assertTrue(numpy.array_equal(
            ret[0][0].x.data, ret[1][0].x.data))
        self.assertEqual(ret[0][0].x.shape, self.chainlist[0].x.shape)
        self.assertEqual(ret[0][0].x.shape, ret[1][0].x.shape)
        self.assertEqual(ret[0][0].x.dtype, self.chainlist[0].x.dtype)
        self.assertEqual(ret[0][0].x.dtype, ret[1][0].x.dtype)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5543')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 2242-2268
</a>
<div class="mid" id="frag5543" style="display:none"><pre>

    def test_repeat_with_init_mode(self):
        ret = self.chainlist.repeat(2, mode='init')
        self.assertEqual(len(ret), 2)
        self.assertIsNot(ret[0], self.chainlist)
        self.assertIsNot(ret[1], self.chainlist)
        self.assertIsNot(ret[0], ret[1])
        self.assertIsNot(ret[0][0], self.chainlist[0])
        self.assertIsNot(ret[1][0], self.chainlist[0])
        self.assertIsNot(ret[0][0], ret[1][0])
        self.assertIsNot(ret[0][0].x, self.chainlist[0].x)
        self.assertIsNot(ret[1][0].x, self.chainlist[0].x)
        self.assertIsNot(ret[0][0].x, ret[1][0].x)
        self.assertIsNot(ret[0][0].x.data, self.chainlist[0].x.data)
        self.assertIsNot(ret[1][0].x.data, self.chainlist[0].x.data)
        self.assertIsNot(ret[0][0].x.data, ret[1][0].x.data)
        self.assertFalse(numpy.array_equal(
            ret[0][0].x.data, self.chainlist[0].x.data))
        self.assertFalse(numpy.array_equal(
            ret[1][0].x.data, self.chainlist[0].x.data))
        self.assertFalse(numpy.array_equal(
            ret[0][0].x.data, ret[1][0].x.data))
        self.assertEqual(ret[0][0].x.shape, self.chainlist[0].x.shape)
        self.assertEqual(ret[0][0].x.shape, ret[1][0].x.shape)
        self.assertEqual(ret[0][0].x.dtype, self.chainlist[0].x.dtype)
        self.assertEqual(ret[0][0].x.dtype, ret[1][0].x.dtype)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 128:</b> &nbsp; 3 fragments, nominal size 15 lines, similarity 88%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5544')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 2272-2292
</a>
<div class="mid" id="frag5544" style="display:none"><pre>

    def setUp(self):
        self.link = chainer.Link()
        shape = (2, 2)
        dtype = numpy.float32
        y_array = numpy.random.rand(*shape).astype(dtype)
        pa_array = numpy.random.rand(*shape).astype(dtype)
        ps_scalar = 2.4

        with self.link.init_scope():
            # Initialized parameter
            self.link.y = chainer.Parameter(y_array)
            # Uninitialized parameter
            self.link.v = chainer.Parameter()
            # Persistent ndarray
            self.link.add_persistent('pa', pa_array)
            # Persistent scalar
            self.link.add_persistent('ps', ps_scalar)
        self.y_array = y_array
        self.pa_array = pa_array
        self.ps_scalar = ps_scalar
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5552')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 2426-2446
</a>
<div class="mid" id="frag5552" style="display:none"><pre>

    def setUp(self):
        self.link = chainer.Link()
        shape = (2, 2)
        dtype = numpy.float32
        y_array = numpy.random.rand(*shape).astype(dtype)
        pa_array = numpy.random.rand(*shape).astype(dtype)
        ps_scalar = 2.4

        with self.link.init_scope():
            # Initialized parameter
            self.link.y = chainer.Parameter(y_array)
            # Uninitialized parameter
            self.link.v = chainer.Parameter()
            # Persistent ndarray
            self.link.add_persistent('pa', pa_array)
            # Persistent scalar
            self.link.add_persistent('ps', ps_scalar)
        self.y_array = y_array
        self.pa_array = pa_array
        self.ps_scalar = ps_scalar
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5556')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 2513-2536
</a>
<div class="mid" id="frag5556" style="display:none"><pre>
class TestToDevice(unittest.TestCase):
    def setUp(self):
        self.link = chainer.Link()
        shape = (2, 2)
        dtype = numpy.float32
        y_array = numpy.random.rand(*shape).astype(dtype)
        pa_array = numpy.random.rand(*shape).astype(dtype)
        ps_scalar = 2.4

        with self.link.init_scope():
            # Initialized parameter
            self.link.y = chainer.Parameter(y_array)
            # Uninitialized parameter
            self.link.v = chainer.Parameter()
            # Persistent ndarray
            self.link.add_persistent('pa', pa_array)
            # Persistent scalar
            self.link.add_persistent('ps', ps_scalar)
        self.y_array = y_array
        self.pa_array = pa_array
        self.ps_scalar = ps_scalar

        if cuda.available:
            self.current_device_id = cuda.cupy.cuda.get_device_id()
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 129:</b> &nbsp; 6 fragments, nominal size 13 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5545')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 2293-2311
</a>
<div class="mid" id="frag5545" style="display:none"><pre>

    def test_cpu_to_intel64(self):
        link = self.link
        with testing.assert_warns(DeprecationWarning):
            link.to_intel64()
        assert isinstance(link.device, backend.Intel64Device)

        # Arrays should be converted to ideep.mdarray

        # Initialized parameter
        assert isinstance(link.y.data, intel64.ideep.mdarray)
        _assert_variable_array_equal(link.y, self.y_array)
        # Uninitialized parameter
        assert link.v.data is None
        # Persistent ndarray
        assert isinstance(link.pa, intel64.ideep.mdarray)
        _assert_arrays_equal(link.pa, self.pa_array)
        # Persistent scalar
        assert link.ps == self.ps_scalar
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5547')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 2336-2357
</a>
<div class="mid" id="frag5547" style="display:none"><pre>
    @attr.gpu
    def test_gpu_to_intel64(self):
        link = self.link
        with testing.assert_warns(DeprecationWarning):
            link.to_gpu()
        assert link.device.device == cuda.Device(0)
        with testing.assert_warns(DeprecationWarning):
            link.to_intel64()
        assert isinstance(link.device, backend.Intel64Device)

        # Arrays should be converted to ideep.mdarray

        # Initialized parameter
        assert isinstance(link.y.data, intel64.ideep.mdarray)
        _assert_variable_array_equal(link.y, self.y_array)
        # Uninitialized parameter
        assert link.v.data is None
        # Persistent ndarray
        assert isinstance(link.pa, intel64.ideep.mdarray)
        _assert_arrays_equal(link.pa, self.pa_array)
        # Persistent scalar
        assert link.ps == self.ps_scalar
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5548')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 2359-2380
</a>
<div class="mid" id="frag5548" style="display:none"><pre>
    @attr.gpu
    def test_intel64_to_gpu(self):
        link = self.link
        with testing.assert_warns(DeprecationWarning):
            link.to_intel64()
        assert isinstance(link.device, backend.Intel64Device)
        with testing.assert_warns(DeprecationWarning):
            link.to_gpu()
        assert link.device.device == cuda.Device(0)

        # Arrays should be converted to cupy.ndarray

        # Initialized parameter
        assert isinstance(link.y.data, cuda.cupy.ndarray)
        _assert_variable_array_equal(link.y, self.y_array)
        # Uninitialized parameter
        assert link.v.data is None
        # Persistent ndarray
        assert isinstance(link.pa, cuda.ndarray)
        _assert_arrays_equal(link.pa, self.pa_array)
        # Persistent scalar
        assert link.ps == self.ps_scalar
</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5549')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 2381-2402
</a>
<div class="mid" id="frag5549" style="display:none"><pre>

    def test_intel64_to_cpu(self):
        link = self.link
        with testing.assert_warns(DeprecationWarning):
            link.to_intel64()
        assert isinstance(link.device, backend.Intel64Device)
        with testing.assert_warns(DeprecationWarning):
            link.to_cpu()
        assert isinstance(link.device, backend.CpuDevice)

        # Arrays should be converted to numpy.ndarray

        # Initialized parameter
        assert isinstance(link.y.data, numpy.ndarray)
        _assert_variable_array_equal(link.y, self.y_array)
        # Uninitialized parameter
        assert link.v.data is None
        # Persistent ndarray
        assert isinstance(link.pa, numpy.ndarray)
        _assert_arrays_equal(link.pa, self.pa_array)
        # Persistent scalar
        assert link.ps == self.ps_scalar
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5555')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 2486-2509
</a>
<div class="mid" id="frag5555" style="display:none"><pre>
    @attr.gpu
    def test_gpu_to_chx(self):
        link = self.link
        with testing.assert_warns(DeprecationWarning):
            link.to_gpu()
        assert link.device.device == cuda.Device(0)
        link.to_chx()
        assert link.device.device == chainerx.get_device('cuda:0')

        # Arrays should be converted to chainerx.ndarray

        # Initialized parameter
        assert isinstance(link.y.data, chainerx.ndarray)
        assert link.y.data.device.backend.name == 'cuda'
        assert link.y.data.device.index == 0
        _assert_variable_array_equal(link.y, self.y_array)
        # Uninitialized parameter
        assert link.v.data is None
        # Persistent ndarray
        assert isinstance(link.pa, chainerx.ndarray)
        _assert_arrays_equal(link.pa, self.pa_array)
        # Persistent scalar
        assert link.ps == self.ps_scalar

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5554')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 2468-2484
</a>
<div class="mid" id="frag5554" style="display:none"><pre>

    def test_cpu_to_chx(self):
        link = self.link
        link.to_chx()

        # Initialized parameter
        assert isinstance(link.y.data, chainerx.ndarray)
        assert link.y.data.device.backend.name == 'native'
        assert link.y.data.device.index == 0
        _assert_variable_array_equal(link.y, self.y_array)
        # Uninitialized parameter
        assert link.v.data is None
        # Persistent ndarray
        assert isinstance(link.pa, chainerx.ndarray)
        _assert_arrays_equal(link.pa, self.pa_array)
        # Persistent scalar
        assert link.ps == self.ps_scalar
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 130:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5546')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 2312-2334
</a>
<div class="mid" id="frag5546" style="display:none"><pre>

    def test_intel64_to_intel64(self):
        link = self.link
        with testing.assert_warns(DeprecationWarning):
            link.to_intel64()
        prev_y = link.y
        prev_v = link.v
        prev_pa = link.pa
        prev_ps = link.ps
        with testing.assert_warns(DeprecationWarning):
            link.to_intel64()
        assert isinstance(link.device, backend.Intel64Device)

        # Everything should be left untouched

        # Initialized parameter
        assert link.y is prev_y
        # Uninitialized parameter
        assert link.v is prev_v
        # Persistent ndarray
        assert link.pa is prev_pa
        # Persistent scalar
        assert link.ps is prev_ps
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5553')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_link.py: 2447-2467
</a>
<div class="mid" id="frag5553" style="display:none"><pre>

    def test_chainerx_to_chx(self):
        link = self.link
        link.to_chx()
        prev_y = link.y
        prev_v = link.v
        prev_pa = link.pa
        prev_ps = link.ps
        link.to_chx()
        assert link.device.device == chainerx.get_device('native:0')

        # Everything should be left untouched

        # Initialized parameter
        assert link.y is prev_y
        # Uninitialized parameter
        assert link.v is prev_v
        # Persistent ndarray
        assert link.pa is prev_pa
        # Persistent scalar
        assert link.ps is prev_ps
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 131:</b> &nbsp; 2 fragments, nominal size 26 lines, similarity 88%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5635')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/initializer_tests/test_normal.py: 94-122
</a>
<div class="mid" id="frag5635" style="display:none"><pre>
    def check_initializer_statistics(self, backend_config, n):
        from scipy import stats

        xp = backend_config.xp
        ws = numpy.empty((n,) + self.shape, dtype=self.dtype)
        ws = backend_config.get_array(ws)
        for i in range(n):
            initializer = self.target(**self.target_kwargs)
            initializer(xp.squeeze(ws[i:i+1], axis=0))

        fan = self.fan_option or default_fan.get(self.target)
        expected_std = self.scale or default_scale.get(self.target) or 1.
        expected_std *= default_coeff.get(self.target) or 1.
        if fan is not None:
            if fan == 'fan_in':
                expected_std *= math.sqrt(1. / self.fans[0])
            elif fan == 'fan_out':
                expected_std *= math.sqrt(1. / self.fans[1])
            elif fan == 'fan_avg':
                expected_std *= math.sqrt(2. / sum(self.fans))
            else:
                assert False

        sampless = cuda.to_cpu(ws.reshape(n, -1).T)
        alpha = 0.01 / len(sampless)
        for samples in sampless:
            _, p = stats.kstest(samples, stats.norm(0, expected_std).cdf)
            assert p &gt;= alpha

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5658')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/initializer_tests/test_uniform.py: 95-126
</a>
<div class="mid" id="frag5658" style="display:none"><pre>
    def check_initializer_statistics(self, backend_config, n):
        from scipy import stats

        xp = backend_config.xp
        ws = numpy.empty((n,) + self.shape, dtype=self.dtype)
        ws = backend_config.get_array(ws)
        for i in range(n):
            initializer = self.target(**self.target_kwargs)
            initializer(xp.squeeze(ws[i:i+1], axis=0))

        fan = self.fan_option or default_fan.get(self.target)
        expected_max = self.scale or default_scale.get(self.target) or 1.
        expected_max *= default_coeff.get(self.target) or 1.
        if fan is not None:
            if fan == 'fan_in':
                expected_max *= math.sqrt(1. / self.fans[0])
            elif fan == 'fan_out':
                expected_max *= math.sqrt(1. / self.fans[1])
            elif fan == 'fan_avg':
                expected_max *= math.sqrt(2. / sum(self.fans))
            else:
                assert False

        sampless = cuda.to_cpu(ws.reshape(n, -1).T)
        alpha = 0.01 / len(sampless)
        for samples in sampless:
            _, p = stats.kstest(
                samples,
                stats.uniform(-expected_max, 2*expected_max).cdf
            )
            assert p &gt;= alpha

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 132:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5664')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/initializer_tests/test_constant.py: 49-60
</a>
<div class="mid" id="frag5664" style="display:none"><pre>
    def check_shaped_initializer(self, backend_config):
        initializer = initializers.Identity(
            scale=self.scale, dtype=self.dtype)
        xp = backend_config.xp
        w = initializers.generate_array(initializer, self.shape, xp)
        self.assertIs(backend.get_array_module(w), xp)
        self.assertTupleEqual(w.shape, self.shape)
        self.assertEqual(w.dtype, self.dtype)
        testing.assert_allclose(
            w, self.scale * numpy.identity(len(self.shape)),
            **self.check_options)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5671')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/initializer_tests/test_constant.py: 120-131
</a>
<div class="mid" id="frag5671" style="display:none"><pre>
    def check_shaped_initializer(self, backend_config):
        initializer = initializers.Constant(
            fill_value=self.fill_value, dtype=self.dtype)
        xp = backend_config.xp
        w = initializers.generate_array(initializer, self.shape, xp)
        self.assertIs(backend.get_array_module(w), xp)
        self.assertTupleEqual(w.shape, self.shape)
        self.assertEqual(w.dtype, self.dtype)
        testing.assert_allclose(
            w, numpy.full(self.shape, self.fill_value),
            **self.check_options)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 133:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5726')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_function.py: 114-127
</a>
<div class="mid" id="frag5726" style="display:none"><pre>
    def check_check_type_forward(self):
        self.assertEqual(self.f.check_type_forward.call_count, 1)
        ts = self.f.check_type_forward.call_args[0][0]
        self.assertIsInstance(ts, type_check.LightTypeInfoTuple)
        self.assertEqual(len(ts), 2)

        t1 = ts[0]
        assert t1.shape == self.x_shape
        assert t1.dtype == numpy.float32

        t2 = ts[1]
        assert t2.shape == self.x_shape
        assert t2.dtype == numpy.int32

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8740')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_function_node.py: 116-129
</a>
<div class="mid" id="frag8740" style="display:none"><pre>
    def check_check_type_forward(self):
        self.assertEqual(self.f.check_type_forward.call_count, 1)
        ts = self.f.check_type_forward.call_args[0][0]
        self.assertIsInstance(ts, type_check.LightTypeInfoTuple)
        self.assertEqual(len(ts), 2)

        t1 = ts[0]
        assert t1.shape == self.x_shape
        assert t1.dtype == numpy.float32

        t2 = ts[1]
        assert t2.shape == self.x_shape
        assert t2.dtype == numpy.int32

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 134:</b> &nbsp; 7 fragments, nominal size 11 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5733')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_function.py: 182-196
</a>
<div class="mid" id="frag5733" style="display:none"><pre>
    def check_call_all_ndarray(self):
        x1 = self.x1
        x2 = self.x2
        ys = self.f(x1, x2)

        self.assertEqual(len(ys), 2)
        self.check_check_type_forward()

        xp = backend.get_array_module(x1)

        for y in ys:
            self.assertIsInstance(y, chainer.Variable)
            self.assertIsInstance(y.data, xp.ndarray)
            self.assertFalse(y.requires_grad)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8754')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_function_node.py: 243-256
</a>
<div class="mid" id="frag8754" style="display:none"><pre>
    def check_apply_ndarray_chainerx(self):
        x1 = chainer.Variable(self.x1)
        x2 = self.x2
        ys = self.f.apply((x1, x2))

        self.assertEqual(len(ys), 2)
        self.check_check_type_forward()

        for y in ys:
            self.assertIsInstance(y, chainer.Variable)
            self.assertIsInstance(y.data, chainerx.ndarray)
            self.assertIs(y.data.device, self.x1.device)
            self.assertTrue(y.requires_grad)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8748')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_function_node.py: 190-204
</a>
<div class="mid" id="frag8748" style="display:none"><pre>
    def check_apply_all_ndarray(self):
        x1 = self.x1
        x2 = self.x2
        ys = self.f.apply((x1, x2))

        self.assertEqual(len(ys), 2)
        self.check_check_type_forward()

        xp = backend.get_array_module(x1)

        for y in ys:
            self.assertIsInstance(y, chainer.Variable)
            self.assertIsInstance(y.data, xp.ndarray)
            self.assertFalse(y.requires_grad)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag8753')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_function_node.py: 225-242
</a>
<div class="mid" id="frag8753" style="display:none"><pre>
    def check_apply_ndarray(self):
        x1 = chainer.Variable(self.x1)
        x2 = self.x2
        x1._node._rank = 1
        ys = self.f.apply((x1, x2))

        self.assertEqual(len(ys), 2)
        self.check_check_type_forward()

        for y in ys:
            self.assertIsInstance(y, chainer.Variable)
            # rank is (maximum rank in xs) + 1
            self.assertEqual(y.rank, 2)
            self.assertIs(y.creator_node, self.f)
            self.assertTrue(y.requires_grad)

        self.assertIsInstance(y.creator_node.outputs, tuple)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8742')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_function_node.py: 149-163
</a>
<div class="mid" id="frag8742" style="display:none"><pre>
    def check_apply_chainerx(self):
        x1 = chainer.Variable(self.x1)
        # TODO(sonots): ChainerX does not support computing gradients for int32
        x2 = chainer.Variable(self.x2, requires_grad=False)
        ys = self.f.apply((x1, x2))

        self.assertEqual(len(ys), 2)
        self.check_check_type_forward()

        for y in ys:
            self.assertIsInstance(y, chainer.Variable)
            self.assertIsInstance(y.data, chainerx.ndarray)
            self.assertIs(y.data.device, self.x1.device)
            self.assertTrue(y.requires_grad)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5736')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_function.py: 205-222
</a>
<div class="mid" id="frag5736" style="display:none"><pre>
    def check_call_ndarray(self):
        x1 = chainer.Variable(self.x1)
        x2 = self.x2
        x1._node._rank = 1
        ys = self.f(x1, x2)

        self.assertEqual(len(ys), 2)
        self.check_check_type_forward()

        for y in ys:
            self.assertIsInstance(y, chainer.Variable)
            # rank is (maximum rank in xs) + 1
            self.assertEqual(y.rank, 2)
            self.assertIs(y.creator, self.f)
            self.assertTrue(y.requires_grad)

        self.assertIsInstance(y.creator.outputs, tuple)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag8741')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_function_node.py: 130-148
</a>
<div class="mid" id="frag8741" style="display:none"><pre>
    def check_apply(self):
        x1 = chainer.Variable(self.x1)
        x2 = chainer.Variable(self.x2)
        x1._node._rank = 1
        x2._node._rank = 3
        ys = self.f.apply((x1, x2))

        self.assertEqual(len(ys), 2)
        self.check_check_type_forward()

        for y in ys:
            self.assertIsInstance(y, chainer.Variable)
            # rank is (maximum rank in xs) + 1
            self.assertEqual(y.rank, 4)
            self.assertIs(y.creator_node, self.f)
            self.assertTrue(y.requires_grad)

        self.assertIsInstance(y.creator_node.outputs, tuple)

</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 135:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5743')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_function.py: 257-271
</a>
<div class="mid" id="frag5743" style="display:none"><pre>
    def test_unchain(self):
        f, _x1, _y1 = self._get_f()
        y1, y2 = f.outputs
        f.unchain()

        # As _y1 is alive, this weak ref is also alive
        y1_ref = y1()
        self.assertIsNotNone(y1_ref)
        self.assertIsNone(y1_ref.creator)
        # This weak ref is dead by unchain
        y2_ref = y2()
        self.assertIsNone(y2_ref)

        self.assertIsNone(f.inputs)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8766')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_function_node.py: 328-343
</a>
<div class="mid" id="frag8766" style="display:none"><pre>
    def test_unchain(self):
        self.setup_cpu()
        f, _x1, _y1 = self._get_f()
        y1, y2 = f.outputs
        f.unchain()

        # As _y1 is alive, this weak ref is also alive
        y1_ref = y1()
        self.assertIsNotNone(y1_ref)
        self.assertIsNone(y1_ref.creator)
        # This weak ref is dead by unchain
        y2_ref = y2()
        self.assertIsNone(y2_ref)

        self.assertIsNone(f.inputs)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 136:</b> &nbsp; 2 fragments, nominal size 24 lines, similarity 87%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5746')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_function.py: 297-342
</a>
<div class="mid" id="frag5746" style="display:none"><pre>
    def test_forward_invalid1(self):
        class Function(chainer.Function):

            def check_type_forward(self, in_types):
                x_type, = in_types
                type_check.expect(
                    x_type.dtype == numpy.float32,
                    x_type.ndim &gt;= 2,
                )

            def forward(self, inputs):
                return inputs

        f = Function()

        # OK
        v = chainer.Variable(numpy.random.randn(1, 5).astype(numpy.float32))
        result = f(v)
        assert isinstance(result, chainer.Variable)

        # Incorrect dtype
        # in py3, numpy dtypes are represented as class
        msg = """\
Invalid operation is performed in: Function \\(Forward\\)

Expect: in_types\\[0\\]\\.dtype == &lt;(type|class) 'numpy\\.float32'&gt;
Actual: float64 \\!= &lt;(type|class) 'numpy\\.float32'&gt;"""

        v = chainer.Variable(numpy.random.randn(1, 5))
        with six.assertRaisesRegex(self, chainer.utils.type_check.InvalidType,
                                   msg):
            f(v)

        # Incorrect dim
        msg = """\
Invalid operation is performed in: Function \\(Forward\\)

Expect: in_types\\[0\\]\\.ndim &gt;= 2
Actual: 1 &lt; 2"""

        v = chainer.Variable(numpy.random.randn(5).astype(numpy.float32))
        with six.assertRaisesRegex(self, chainer.utils.type_check.InvalidType,
                                   msg):
            f(v)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8773')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_function_node.py: 381-426
</a>
<div class="mid" id="frag8773" style="display:none"><pre>
    def test_forward_invalid1(self):
        class FunctionNode(chainer.FunctionNode):

            def check_type_forward(self, in_types):
                x_type, = in_types
                type_check.expect(
                    x_type.dtype == numpy.float32,
                    x_type.ndim &gt;= 2,
                )

            def forward(self, inputs):
                return inputs

        f = FunctionNode()

        # OK
        v = chainer.Variable(numpy.random.randn(1, 5).astype(numpy.float32))
        result, = f.apply((v,))
        assert isinstance(result, chainer.Variable)

        # Incorrect dtype
        # in py3, numpy dtypes are represented as class
        msg = """\
Invalid operation is performed in: FunctionNode \\(Forward\\)

Expect: in_types\\[0\\]\\.dtype == &lt;(type|class) 'numpy\\.float32'&gt;
Actual: float64 \\!= &lt;(type|class) 'numpy\\.float32'&gt;"""

        v = chainer.Variable(numpy.random.randn(1, 5))
        with six.assertRaisesRegex(self, chainer.utils.type_check.InvalidType,
                                   msg):
            f.apply((v,))

        # Incorrect dim
        msg = """\
Invalid operation is performed in: FunctionNode \\(Forward\\)

Expect: in_types\\[0\\]\\.ndim &gt;= 2
Actual: 1 &lt; 2"""

        v = chainer.Variable(numpy.random.randn(5).astype(numpy.float32))
        with six.assertRaisesRegex(self, chainer.utils.type_check.InvalidType,
                                   msg):
            f.apply((v,))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 137:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5761')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_function.py: 439-452
</a>
<div class="mid" id="frag5761" style="display:none"><pre>
    def test_force_backprop_mode(self):
        with chainer.no_backprop_mode():
            with chainer.force_backprop_mode():
                y = self.x + 1
        self.assertTrue(y.creator_node is not None)

        y = self.x + 1
        self.assertTrue(y.creator_node is not None)

        with chainer.force_backprop_mode():
            y = self.x + 1
        self.assertTrue(y.creator_node is not None)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8805')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_function_node.py: 680-692
</a>
<div class="mid" id="frag8805" style="display:none"><pre>
    def test_force_backprop_mode(self):
        with chainer.no_backprop_mode():
            with chainer.force_backprop_mode():
                y = self.x + 1
        self.assertTrue(y.creator_node is not None)

        y = self.x + 1
        self.assertTrue(y.creator_node is not None)

        with chainer.force_backprop_mode():
            y = self.x + 1
        self.assertTrue(y.creator_node is not None)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 138:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5799')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/dataset_tests/tabular_tests/test_asmode.py: 15-26
</a>
<div class="mid" id="frag5799" style="display:none"><pre>
    def test_astuple(self):
        dataset = dummy_dataset.DummyDataset(mode=self.mode, convert=True)
        view = dataset.astuple()
        self.assertIsInstance(view, chainer.dataset.TabularDataset)
        self.assertEqual(len(view), len(dataset))
        self.assertEqual(view.keys, dataset.keys)
        self.assertEqual(view.mode, tuple)
        self.assertEqual(
            view.get_examples(None, None), dataset.get_examples(None, None))
        self.assertEqual(view.convert(view.fetch()), 'converted')


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5800')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/dataset_tests/tabular_tests/test_asmode.py: 34-45
</a>
<div class="mid" id="frag5800" style="display:none"><pre>
    def test_asdict(self):
        dataset = dummy_dataset.DummyDataset(mode=self.mode, convert=True)
        view = dataset.asdict()
        self.assertIsInstance(view, chainer.dataset.TabularDataset)
        self.assertEqual(len(view), len(dataset))
        self.assertEqual(view.keys, dataset.keys)
        self.assertEqual(view.mode, dict)
        self.assertEqual(
            view.get_examples(None, None), dataset.get_examples(None, None))
        self.assertEqual(view.convert(view.fetch()), 'converted')


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 139:</b> &nbsp; 2 fragments, nominal size 49 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5859')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_function_and_function_node.py: 48-119
</a>
<div class="mid" id="frag5859" style="display:none"><pre>
    def call_func_function(self, backend_config, x1, x2, x3):
        forward_xp, backward_xp = _get_expected_xp(backend_config, True)

        class Func(chainer.Function):
            def __init__(self):
                self.array_init = backend_config.device.send(
                    numpy.array([3], numpy.float32))

            def forward(self, inputs):
                # Inputs
                assert isinstance(inputs, tuple)
                # x1, x3: float32
                # x2: int32
                x1, x2, x3 = inputs
                assert isinstance(x1, forward_xp.ndarray)
                assert isinstance(x2, forward_xp.ndarray)
                assert isinstance(x3, forward_xp.ndarray)

                # attribute fallback
                assert isinstance(self.array_init, forward_xp.ndarray)
                self.array_forward = forward_xp.array([2], numpy.float32)
                assert isinstance(self.array_forward, forward_xp.ndarray)

                y1 = x2 - 1  # int32
                y2 = x1 * x3 + x2.astype(x1.dtype)
                y3 = x1 + x3
                self.retain_inputs((0, 2))
                self.retain_outputs((0, 1,))
                return y1, y2, y3

            def backward(self, inputs, grad_outputs):

                # Retained inputs
                assert isinstance(inputs, tuple)
                x1, x2, x3 = inputs
                assert isinstance(x1, backward_xp.ndarray)
                assert x2 is None  # not retained
                assert isinstance(x3, backward_xp.ndarray)

                # Output gradients
                assert isinstance(grad_outputs, tuple)
                gy1, gy2, gy3 = grad_outputs
                assert gy1 is None  # y1 is int32
                # y3 is disconnected
                # TODO(niboshi): Expression after "or" is workaround for
                # chainerx. ChainerX backward should return None for
                # disconnected output and this workaround should be removed.
                assert (gy3 is None
                        or (float(gy3.max()) == 0
                            and float((-gy3).max()) == 0))

                # Retained outputs
                output_data = self.output_data
                assert isinstance(output_data, tuple)
                y1, y2, y3 = output_data
                assert isinstance(y1, backward_xp.ndarray)
                assert isinstance(y2, backward_xp.ndarray)
                assert y3 is None

                # attribute fallback
                assert isinstance(self.array_init, backward_xp.ndarray)
                assert isinstance(self.array_forward, backward_xp.ndarray)
                self.array_backward = backward_xp.array([4], numpy.float32)
                assert isinstance(self.array_backward, backward_xp.ndarray)

                gx1 = x3 * gy2  # + gy3
                gx2 = None
                gx3 = x1 * gy2  # + gy3
                return gx1, gx2, gx3

        return Func()(x1, x2, x3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5863')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_function_and_function_node.py: 120-195
</a>
<div class="mid" id="frag5863" style="display:none"><pre>
    def call_func_function_node(self, backend_config, x1, x2, x3):
        forward_xp, backward_xp = _get_expected_xp(backend_config, False)

        class Func(chainer.FunctionNode):
            def __init__(self):
                self.array_init = backend_config.device.send(
                    numpy.array([3], numpy.float32))

            def forward(self, inputs):

                # Inputs
                # x1, x3: float32
                # x2: int32
                x1, x2, x3 = inputs
                assert isinstance(x1, forward_xp.ndarray)
                assert isinstance(x2, forward_xp.ndarray)
                assert isinstance(x3, forward_xp.ndarray)

                # attribute fallback
                assert isinstance(self.array_init, forward_xp.ndarray)
                self.array_forward = forward_xp.array([2], numpy.float32)
                assert isinstance(self.array_forward, forward_xp.ndarray)

                y1 = x2 - 1  # int32
                y2 = x1 * x3 + x2.astype(x1.dtype)
                y3 = x1 + x3
                self.retain_inputs((0, 2))
                self.retain_outputs((0, 1,))
                return y1, y2, y3

            def backward(self, input_indexes, grad_outputs):

                # Input indexes
                assert isinstance(input_indexes, tuple)
                assert input_indexes == (0, 2)

                # Retained inputs
                retained_inputs = self.get_retained_inputs()
                assert isinstance(retained_inputs, tuple)
                x1, x3 = retained_inputs
                assert isinstance(x1.array, backward_xp.ndarray)
                assert isinstance(x3.array, backward_xp.ndarray)

                # Output gradients
                assert isinstance(grad_outputs, tuple)
                gy1, gy2, gy3 = grad_outputs
                assert gy1 is None  # y1 is int32
                assert isinstance(gy2.array, backward_xp.ndarray)
                # y3 is disconnected
                # TODO(niboshi): Expression after "or" is workaround for
                # chainerx. ChainerX backward should return None for
                # disconnected output and this workaround should be removed.
                assert (gy3 is None
                        or (float(gy3.array.max()) == 0
                            and float((-gy3.array).max()) == 0))

                # Retained outputs
                retained_outputs = self.get_retained_outputs()
                assert isinstance(retained_outputs, tuple)
                y1, y2, = retained_outputs
                assert isinstance(y1.array, backward_xp.ndarray)
                assert isinstance(y2.array, backward_xp.ndarray)

                # attribute fallback
                assert isinstance(self.array_init, backward_xp.ndarray)
                assert isinstance(self.array_forward, backward_xp.ndarray)
                self.array_backward = backward_xp.array([4], numpy.float32)
                assert isinstance(self.array_backward, backward_xp.ndarray)

                gx1 = x3 * gy2  # + gy3
                gx2 = None
                gx3 = x1 * gy2  # + gy3
                return gx1, gx2, gx3

        return Func().apply((x1, x2, x3))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 140:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5876')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_function_and_function_node.py: 342-361
</a>
<div class="mid" id="frag5876" style="display:none"><pre>
    def test_backprop(self, backend_config):

        x2_arr = numpy.array([2, 3], numpy.float32)
        gy1_arr = numpy.array([2, 4], numpy.float32)
        x2_arr, gy1_arr = backend_config.get_array((x2_arr, gy1_arr))

        x2 = chainer.Variable(x2_arr, requires_grad=True)

        # Forward
        y1, = self.call_func(backend_config, x2)

        assert isinstance(y1.array, backend_config.xp.ndarray)

        # Backward
        y1.grad = gy1_arr
        y1.backward()

        assert isinstance(x2.grad, backend_config.xp.ndarray)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5884')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/test_function_and_function_node.py: 474-495
</a>
<div class="mid" id="frag5884" style="display:none"><pre>
    def test_backprop(self, backend_config):

        x1_arr = numpy.array([2, 3], numpy.float32)
        gy2_arr = numpy.array([2, 4], numpy.float32)
        x1_arr, gy2_arr = backend_config.get_array((x1_arr, gy2_arr))

        x1 = chainer.Variable(x1_arr, requires_grad=True)

        # Forward
        y1, y2, y3 = self.call_func(backend_config, x1)

        assert y1.array is None
        assert isinstance(y2.array, backend_config.xp.ndarray)
        assert y3.array is None

        # Backward
        y2.grad = gy2_arr
        y2.backward()

        assert isinstance(x1.grad, backend_config.xp.ndarray)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 141:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5970')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/datasets_tests/test_sub_dataset.py: 39-49
</a>
<div class="mid" id="frag5970" style="display:none"><pre>
    def test_split_dataset(self):
        original = [1, 2, 3, 4, 5]
        subset1, subset2 = datasets.split_dataset(original, 2)
        self.assertEqual(len(subset1), 2)
        self.assertEqual(subset1[0], 1)
        self.assertEqual(subset1[1], 2)
        self.assertEqual(len(subset2), 3)
        self.assertEqual(subset2[0], 3)
        self.assertEqual(subset2[1], 4)
        self.assertEqual(subset2[2], 5)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5975')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/datasets_tests/test_sub_dataset.py: 74-84
</a>
<div class="mid" id="frag5975" style="display:none"><pre>
    def test_permuted_split_dataset(self):
        original = [1, 2, 3, 4, 5]
        subset1, subset2 = datasets.split_dataset(original, 2, [2, 0, 3, 1, 4])
        self.assertEqual(len(subset1), 2)
        self.assertEqual(subset1[0], 3)
        self.assertEqual(subset1[1], 1)
        self.assertEqual(len(subset2), 3)
        self.assertEqual(subset2[0], 4)
        self.assertEqual(subset2[1], 2)
        self.assertEqual(subset2[2], 5)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 142:</b> &nbsp; 2 fragments, nominal size 32 lines, similarity 87%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5980')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/datasets_tests/test_sub_dataset.py: 153-186
</a>
<div class="mid" id="frag5980" style="display:none"><pre>
    def test_get_cross_validation_datasets(self):
        original = [1, 2, 3, 4, 5, 6]
        cv1, cv2, cv3 = datasets.get_cross_validation_datasets(original, 3)

        tr1, te1 = cv1
        self.assertEqual(len(tr1), 4)
        self.assertEqual(tr1[0], 1)
        self.assertEqual(tr1[1], 2)
        self.assertEqual(tr1[2], 3)
        self.assertEqual(tr1[3], 4)
        self.assertEqual(len(te1), 2)
        self.assertEqual(te1[0], 5)
        self.assertEqual(te1[1], 6)

        tr2, te2 = cv2
        self.assertEqual(len(tr2), 4)
        self.assertEqual(tr2[0], 5)
        self.assertEqual(tr2[1], 6)
        self.assertEqual(tr2[2], 1)
        self.assertEqual(tr2[3], 2)
        self.assertEqual(len(te2), 2)
        self.assertEqual(te2[0], 3)
        self.assertEqual(te2[1], 4)

        tr3, te3 = cv3
        self.assertEqual(len(tr3), 4)
        self.assertEqual(tr3[0], 3)
        self.assertEqual(tr3[1], 4)
        self.assertEqual(tr3[2], 5)
        self.assertEqual(tr3[3], 6)
        self.assertEqual(len(te3), 2)
        self.assertEqual(te3[0], 1)
        self.assertEqual(te3[1], 2)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5981')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/datasets_tests/test_sub_dataset.py: 187-223
</a>
<div class="mid" id="frag5981" style="display:none"><pre>
    def test_get_cross_validation_datasets_2(self):
        original = [1, 2, 3, 4, 5, 6, 7]
        cv1, cv2, cv3 = datasets.get_cross_validation_datasets(original, 3)

        tr1, te1 = cv1
        self.assertEqual(len(tr1), 4)
        self.assertEqual(tr1[0], 1)
        self.assertEqual(tr1[1], 2)
        self.assertEqual(tr1[2], 3)
        self.assertEqual(tr1[3], 4)
        self.assertEqual(len(te1), 3)
        self.assertEqual(te1[0], 5)
        self.assertEqual(te1[1], 6)
        self.assertEqual(te1[2], 7)

        tr2, te2 = cv2
        self.assertEqual(len(tr2), 5)
        self.assertEqual(tr2[0], 5)
        self.assertEqual(tr2[1], 6)
        self.assertEqual(tr2[2], 7)
        self.assertEqual(tr2[3], 1)
        self.assertEqual(tr2[4], 2)
        self.assertEqual(len(te2), 2)
        self.assertEqual(te2[0], 3)
        self.assertEqual(te2[1], 4)

        tr3, te3 = cv3
        self.assertEqual(len(tr3), 5)
        self.assertEqual(tr3[0], 3)
        self.assertEqual(tr3[1], 4)
        self.assertEqual(tr3[2], 5)
        self.assertEqual(tr3[3], 6)
        self.assertEqual(tr3[4], 7)
        self.assertEqual(len(te3), 2)
        self.assertEqual(te3[0], 1)
        self.assertEqual(te3[1], 2)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 143:</b> &nbsp; 2 fragments, nominal size 21 lines, similarity 85%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6116')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/backends_tests/test_cuda.py: 216-238
</a>
<div class="mid" id="frag6116" style="display:none"><pre>
    def _check_list_tuple(self, typ):
        assert typ in (list, tuple)
        a = numpy.random.uniform(-1, 1, (0,))
        b = numpy.random.uniform(-1, 1, (2, 3))
        c = cuda.cupy.random.uniform(-1, 1, (0,))
        d = cuda.cupy.random.uniform(-1, 1, (2, 2))
        xs = typ([a, b, c, d, None, a, b, None, c, d])
        xs_cpu = cuda.to_cpu(xs)

        assert isinstance(xs_cpu, typ)
        assert len(xs) == len(xs_cpu)
        for i in (0, 1, 2, 3, 5, 6, 8, 9):
            assert isinstance(xs_cpu[i], numpy.ndarray)
            cuda.cupy.testing.assert_array_equal(xs[i], xs_cpu[i])
        assert xs_cpu[0] is a
        assert xs_cpu[1] is b
        assert xs_cpu[2] is xs_cpu[8]
        assert xs_cpu[3] is xs_cpu[9]
        assert xs_cpu[4] is None
        assert xs_cpu[5] is a
        assert xs_cpu[6] is b
        assert xs_cpu[7] is None

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6135')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/backends_tests/test_cuda.py: 403-425
</a>
<div class="mid" id="frag6135" style="display:none"><pre>
    def _check_list_tuple(self, typ):
        assert typ in (list, tuple)
        a = numpy.random.uniform(-1, 1, (0,))
        b = numpy.random.uniform(-1, 1, (2, 3))
        c = cuda.cupy.random.uniform(-1, 1, (0,))
        d = cuda.cupy.random.uniform(-1, 1, (2, 2))
        xs = typ([a, b, c, d, None, a, b, None, c, d])
        xs_gpu = cuda.to_gpu(xs)

        assert isinstance(xs_gpu, typ)
        assert len(xs) == len(xs_gpu)
        for i in (0, 1, 2, 3, 5, 6, 8, 9):
            assert isinstance(xs_gpu[i], cuda.cupy.ndarray)
            cuda.cupy.testing.assert_array_equal(xs[i], xs_gpu[i])
        assert xs_gpu[0] is xs_gpu[5]
        assert xs_gpu[1] is xs_gpu[6]
        assert xs_gpu[2] is c
        assert xs_gpu[3] is d
        assert xs_gpu[4] is None
        assert xs_gpu[7] is None
        assert xs_gpu[8] is c
        assert xs_gpu[9] is d

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 144:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6120')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/backends_tests/test_cuda.py: 262-281
</a>
<div class="mid" id="frag6120" style="display:none"><pre>
    def test_numpy_scalar(self):
        dtype = self.dtype
        if dtype is numpy.bool_:
            x = dtype(True)
        elif issubclass(dtype, numpy.complex_):
            x = dtype(3.2 - 2.4j)
        elif issubclass(dtype, numpy.integer):
            x = dtype(3)
        elif issubclass(dtype, numpy.floating):
            x = dtype(3.2)
        else:
            assert False

        y = cuda.to_cpu(x)
        assert isinstance(y, numpy.ndarray)
        assert y.shape == ()
        assert y.dtype == dtype
        assert y == x


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6139')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/backends_tests/test_cuda.py: 451-470
</a>
<div class="mid" id="frag6139" style="display:none"><pre>
    def test_numpy_scalar(self):
        dtype = self.dtype
        if dtype is numpy.bool_:
            x = dtype(True)
        elif issubclass(dtype, numpy.complex_):
            x = dtype(3.2 - 2.4j)
        elif issubclass(dtype, numpy.integer):
            x = dtype(3)
        elif issubclass(dtype, numpy.floating):
            x = dtype(3.2)
        else:
            assert False

        y = cuda.to_gpu(x)
        assert isinstance(y, cuda.ndarray)
        assert y.shape == ()
        assert y.dtype == dtype
        assert y == x


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 145:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6143')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/backends_tests/test_cuda.py: 507-523
</a>
<div class="mid" id="frag6143" style="display:none"><pre>
    def test_get_device_from_array(self, backend_config):
        with cuda.Device(backend_config.cuda_device):
            arr = cuda.ndarray((), numpy.float32)
        # Test precondition check
        assert arr.device.id == backend_config.cuda_device

        expected_device = backend_config.device

        device = backend.GpuDevice.from_array(arr)
        self.check_device(device, backend_config)
        assert device == expected_device

        device = backend.get_device_from_array(arr)
        self.check_device(device, backend_config)
        assert device == expected_device


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6165')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/backends_tests/test_chainerx.py: 51-67
</a>
<div class="mid" id="frag6165" style="display:none"><pre>
    def test_from_array(self, backend_config):
        arr = backend_config.get_array(numpy.ndarray((2,), numpy.float32))
        # Test precondition check
        assert arr.device.name == backend_config.chainerx_device

        expected_device = backend_config.device

        # ChainerxDevice.from_array
        device = backend.ChainerxDevice.from_array(arr)
        self.check_device(device, backend_config)
        assert device == expected_device

        # backend.get_device_from_array
        device = backend.get_device_from_array(arr)
        self.check_device(device, backend_config)
        assert device == expected_device

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 146:</b> &nbsp; 5 fragments, nominal size 12 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6174')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/distributions_tests/test_beta.py: 18-33
</a>
<div class="mid" id="frag6174" style="display:none"><pre>
    def setUp_configure(self):
        from scipy import stats
        self.dist = distributions.Beta
        self.scipy_dist = stats.beta

        self.test_targets = set([
            'batch_shape', 'entropy', 'event_shape', 'log_prob', 'mean',
            'sample', 'support', 'variance'])

        a = numpy.random.uniform(0, 10, self.shape).astype(numpy.float32)
        b = numpy.random.uniform(0, 10, self.shape).astype(numpy.float32)
        self.params = {'a': a, 'b': b}
        self.scipy_params = {'a': a, 'b': b}

        self.support = '[0, 1]'

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6244')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/distributions_tests/test_chisquare.py: 18-32
</a>
<div class="mid" id="frag6244" style="display:none"><pre>
    def setUp_configure(self):
        from scipy import stats
        self.dist = distributions.Chisquare
        self.scipy_dist = stats.chi2

        self.test_targets = set([
            'batch_shape', 'entropy', 'event_shape', 'log_prob', 'mean',
            'sample', 'support', 'variance'])

        k = numpy.random.randint(1, 10, self.shape).astype(numpy.float32)
        self.params = {'k': k}
        self.scipy_params = {'df': k}

        self.support = 'positive'

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6207')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/distributions_tests/test_poisson.py: 18-33
</a>
<div class="mid" id="frag6207" style="display:none"><pre>
    def setUp_configure(self):
        from scipy import stats
        self.dist = distributions.Poisson
        self.scipy_dist = stats.poisson

        self.test_targets = set([
            'batch_shape', 'event_shape', 'log_prob', 'mean', 'sample',
            'support', 'variance'])

        lam = numpy.random.uniform(0.1, 10, self.shape).astype(numpy.float32)
        self.params = {'lam': lam}
        self.scipy_params = {'mu': lam}

        self.continuous = False
        self.support = 'non negative integer'

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6178')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/distributions_tests/test_geometric.py: 18-33
</a>
<div class="mid" id="frag6178" style="display:none"><pre>
    def setUp_configure(self):
        from scipy import stats
        self.dist = distributions.Geometric
        self.scipy_dist = stats.geom

        self.test_targets = set([
            'batch_shape', 'event_shape', 'log_prob', 'mean', 'sample',
            'support', 'variance'])

        p = numpy.random.uniform(0, 1, self.shape).astype(numpy.float32)
        self.params = {'p': p}
        self.scipy_params = {'p': p}

        self.support = 'positive integer'
        self.continuous = False

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6246')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/distributions_tests/test_dirichlet.py: 18-33
</a>
<div class="mid" id="frag6246" style="display:none"><pre>
    def setUp_configure(self):
        from scipy import stats
        self.dist = distributions.Dirichlet
        self.scipy_dist = stats.dirichlet

        self.test_targets = set([
            'batch_shape', 'entropy', 'event_shape', 'mean', 'sample',
            'support', 'variance'])

        alpha = numpy.random.uniform(
            0, 10, self.shape + (3,)).astype(numpy.float32)
        self.params = {'alpha': alpha}
        self.scipy_params = {'alpha': alpha}
        self.support = '[0, 1]'
        self.event_shape = (3,)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 147:</b> &nbsp; 4 fragments, nominal size 13 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6192')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/distributions_tests/test_gumbel.py: 19-34
</a>
<div class="mid" id="frag6192" style="display:none"><pre>
    def setUp_configure(self):
        from scipy import stats
        self.dist = distributions.Gumbel
        self.scipy_dist = stats.gumbel_r

        self.test_targets = set([
            'batch_shape', 'entropy', 'event_shape', 'log_prob', 'mean',
            'sample', 'support', 'variance'])

        loc = utils.force_array(
            numpy.random.uniform(-1, 1, self.shape).astype(numpy.float32))
        scale = utils.force_array(numpy.exp(
            numpy.random.uniform(-1, 1, self.shape)).astype(numpy.float32))
        self.params = {'loc': loc, 'scale': scale}
        self.scipy_params = {'loc': loc, 'scale': scale}

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6248')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/distributions_tests/test_cauchy.py: 22-37
</a>
<div class="mid" id="frag6248" style="display:none"><pre>
    def setUp_configure(self):
        from scipy import stats
        self.dist = distributions.Cauchy
        self.scipy_dist = stats.cauchy

        self.test_targets = set(['batch_shape', 'cdf', 'entropy',
                                 'event_shape', 'icdf', 'log_prob',
                                 'support'])

        loc = utils.force_array(
            numpy.random.uniform(-1, 1, self.shape).astype(numpy.float32))
        scale = utils.force_array(numpy.exp(
            numpy.random.uniform(-1, 1, self.shape)).astype(numpy.float32))
        self.params = {'loc': loc, 'scale': scale}
        self.scipy_params = {'loc': loc, 'scale': scale}

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6288')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/distributions_tests/test_log_normal.py: 19-36
</a>
<div class="mid" id="frag6288" style="display:none"><pre>
    def setUp_configure(self):
        from scipy import stats
        self.dist = distributions.LogNormal
        self.scipy_dist = stats.lognorm

        self.test_targets = set([
            'batch_shape', 'entropy', 'event_shape', 'log_prob', 'mean',
            'sample', 'support', 'variance'])

        mu = utils.force_array(
            numpy.random.uniform(-1, 1, self.shape).astype(numpy.float32))
        sigma = utils.force_array(numpy.exp(numpy.random.uniform(
            -1, 0, self.shape)).astype(numpy.float32))
        self.params = {'mu': mu, 'sigma': sigma}
        self.scipy_params = {'s': sigma, 'scale': numpy.exp(mu)}

        self.support = 'positive'

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6290')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/distributions_tests/test_laplace.py: 24-39
</a>
<div class="mid" id="frag6290" style="display:none"><pre>
    def setUp_configure(self):
        from scipy import stats
        self.dist = distributions.Laplace
        self.scipy_dist = stats.laplace

        self.test_targets = set([
            'batch_shape', 'cdf', 'entropy', 'event_shape', 'icdf', 'log_prob',
            'mean', 'prob', 'sample', 'stddev', 'support', 'variance'])

        loc = utils.force_array(
            numpy.random.uniform(-1, 1, self.shape).astype(numpy.float32))
        scale = utils.force_array(numpy.exp(
            numpy.random.uniform(-1, 1, self.shape)).astype(numpy.float32))
        self.params = {'loc': loc, 'scale': scale}
        self.scipy_params = {'loc': loc, 'scale': scale}

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 148:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6225')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/distributions_tests/test_bernoulli.py: 66-77
</a>
<div class="mid" id="frag6225" style="display:none"><pre>
    def check_log_prob_binary_check(self, is_gpu):
        smp = self.sample_for_binary_check_test()
        if is_gpu:
            log_prob = self.gpu_dist.log_prob(cuda.to_gpu(smp)).data
        else:
            log_prob = self.cpu_dist.log_prob(smp).data
        xp = backend.get_array_module(log_prob)
        if self.binary_check:
            self.assertTrue(xp.all(log_prob == -xp.inf))
        else:
            self.assertTrue(xp.all(xp.isfinite(log_prob)))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6228')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/distributions_tests/test_bernoulli.py: 85-96
</a>
<div class="mid" id="frag6228" style="display:none"><pre>
    def check_prob_binary_check(self, is_gpu):
        smp = self.sample_for_binary_check_test()
        if is_gpu:
            prob = self.gpu_dist.prob(cuda.to_gpu(smp)).data
        else:
            prob = self.cpu_dist.prob(smp).data
        xp = backend.get_array_module(prob)
        if self.binary_check:
            self.assertTrue(xp.all(prob == 0))
        else:
            self.assertTrue(xp.all(prob &gt; 0))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 149:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6250')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/distributions_tests/test_cauchy.py: 43-59
</a>
<div class="mid" id="frag6250" style="display:none"><pre>
    def check_mean(self, is_gpu):
        with testing.assert_warns(RuntimeWarning):
            if is_gpu:
                mean1 = self.gpu_dist.mean.data
            else:
                mean1 = self.cpu_dist.mean.data

        if self.scipy_onebyone:
            mean2 = []
            for one_params in self.scipy_onebyone_params_iter():
                mean2.append(self.scipy_dist.mean(**one_params))
            mean2 = numpy.vstack(mean2).reshape(
                self.shape + self.cpu_dist.event_shape)
        else:
            mean2 = self.scipy_dist.mean(**self.scipy_params)
        array.assert_allclose(mean1, mean2)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6256')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/distributions_tests/test_cauchy.py: 88-104
</a>
<div class="mid" id="frag6256" style="display:none"><pre>
    def check_variance(self, is_gpu):
        with testing.assert_warns(RuntimeWarning):
            if is_gpu:
                variance1 = self.gpu_dist.variance.data
            else:
                variance1 = self.cpu_dist.variance.data

        if self.scipy_onebyone:
            variance2 = []
            for one_params in self.scipy_onebyone_params_iter():
                variance2.append(self.scipy_dist.var(**one_params))
            variance2 = numpy.vstack(variance2).reshape(
                self.shape + self.cpu_dist.event_shape)
        else:
            variance2 = self.scipy_dist.var(**self.scipy_params)
        array.assert_allclose(variance1, variance2)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 150:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6253')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/distributions_tests/test_cauchy.py: 67-80
</a>
<div class="mid" id="frag6253" style="display:none"><pre>
    def check_sample(self, is_gpu):
        if is_gpu:
            smp1 = self.gpu_dist.sample(
                sample_shape=(100000,)+self.sample_shape).data
            smp1 = cuda.to_cpu(smp1)
        else:
            smp1 = self.cpu_dist.sample(
                sample_shape=(100000,)+self.sample_shape).data
        smp2 = self.scipy_dist.rvs(
            size=(100000,)+self.sample_shape+self.shape, **self.scipy_params)
        testing.assert_allclose(numpy.median(smp1, axis=0),
                                numpy.median(smp2, axis=0),
                                atol=3e-2, rtol=3e-2)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6261')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/distributions_tests/test_pareto.py: 43-56
</a>
<div class="mid" id="frag6261" style="display:none"><pre>
    def check_sample(self, is_gpu):
        if is_gpu:
            smp1 = self.gpu_dist.sample(
                sample_shape=(100000,)+self.sample_shape).data
            smp1 = cuda.to_cpu(smp1)
        else:
            smp1 = self.cpu_dist.sample(
                sample_shape=(100000,)+self.sample_shape).data
        smp2 = self.scipy_dist.rvs(
            size=(100000,)+self.sample_shape+self.shape, **self.scipy_params)
        testing.assert_allclose(numpy.median(smp1, axis=0),
                                numpy.median(smp2, axis=0),
                                atol=3e-2, rtol=3e-2)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 151:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6349')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/training_tests/extensions_tests/test_evaluator.py: 69-82
</a>
<div class="mid" id="frag6349" style="display:none"><pre>
    def setUp(self):
        self.data = [
            numpy.random.uniform(-1, 1, (3, 4)).astype('f') for _ in range(2)]
        self.batches = [
            numpy.random.uniform(-1, 1, (2, 3, 4)).astype('f')
            for _ in range(2)]

        self.iterator = DummyIterator(self.data)
        self.converter = DummyConverter(self.batches)
        self.target = DummyModel(self)
        self.evaluator = extensions.Evaluator(
            self.iterator, self.target, converter=self.converter)
        self.expect_mean = numpy.mean([numpy.sum(x) for x in self.batches])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6359')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/training_tests/extensions_tests/test_evaluator.py: 232-245
</a>
<div class="mid" id="frag6359" style="display:none"><pre>
    def setUp(self):
        self.data = [
            numpy.random.uniform(-1, 1, (3, 4)).astype('f') for _ in range(2)]
        self.batches = [
            numpy.random.uniform(-1, 1, (2, 3, 4)).astype('f')
            for _ in range(2)]

        self.iterator = DummyIterator(self.data)
        self.converter = DummyConverter(self.batches)
        self.target = DummyModel(self)
        self.evaluator = extensions.Evaluator(
            self.iterator, {}, converter=self.converter,
            eval_func=self.target)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 152:</b> &nbsp; 3 fragments, nominal size 10 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6389')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/training_tests/extensions_tests/test_polynomial_shift.py: 28-40
</a>
<div class="mid" id="frag6389" style="display:none"><pre>
    def setUp(self):
        self.optimizer = mock.MagicMock()
        self.extension = extensions.PolynomialShift(
            'x', self.rate, self.max_count, self.init, self.target,
            self.optimizer)

        self.interval = 4
        self.expect = [e for e in self.expect for _ in range(self.interval)]
        self.trigger = util.get_trigger((self.interval, 'iteration'))

        self.trainer = testing.get_trainer_with_mock_updater(self.trigger)
        self.trainer.updater.get_optimizer.return_value = self.optimizer

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6412')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/training_tests/extensions_tests/test_inverse_shift.py: 26-38
</a>
<div class="mid" id="frag6412" style="display:none"><pre>
    def setUp(self):
        self.optimizer = mock.MagicMock()
        self.extension = extensions.InverseShift(
            'x', self.gamma, self.power, self.init, self.target,
            self.optimizer)

        self.interval = 4
        self.expect = [e for e in self.expect for _ in range(self.interval)]
        self.trigger = util.get_trigger((self.interval, 'iteration'))

        self.trainer = testing.get_trainer_with_mock_updater(self.trigger)
        self.trainer.updater.get_optimizer.return_value = self.optimizer

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6462')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/training_tests/extensions_tests/test_warmup_shift.py: 24-36
</a>
<div class="mid" id="frag6462" style="display:none"><pre>
    def setUp(self):
        self.optimizer = mock.MagicMock()
        self.extension = extensions.WarmupShift(
            'x', self.warmup_start, self.warmup_iter,
            self.init, self.optimizer)

        self.interval = 1
        self.expect = [e for e in self.expect for _ in range(self.interval)]
        self.trigger = util.get_trigger((self.interval, 'iteration'))

        self.trainer = testing.get_trainer_with_mock_updater(self.trigger)
        self.trainer.updater.get_optimizer.return_value = self.optimizer

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 153:</b> &nbsp; 5 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6390')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/training_tests/extensions_tests/test_polynomial_shift.py: 41-54
</a>
<div class="mid" id="frag6390" style="display:none"><pre>
    def _run_trainer(self, extension, expect, optimizer=None):
        if optimizer is None:
            optimizer = self.optimizer
        extension.initialize(self.trainer)

        actual = []
        for _ in expect:
            self.trainer.updater.update()
            actual.append(optimizer.x)
            if self.trigger(self.trainer):
                extension(self.trainer)

        self.assertEqual(actual, expect)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6413')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/training_tests/extensions_tests/test_inverse_shift.py: 39-52
</a>
<div class="mid" id="frag6413" style="display:none"><pre>
    def _run_trainer(self, extension, expect, optimizer=None):
        if optimizer is None:
            optimizer = self.optimizer
        extension.initialize(self.trainer)

        actual = []
        for _ in expect:
            self.trainer.updater.update()
            actual.append(optimizer.x)
            if self.trigger(self.trainer):
                extension(self.trainer)

        self.assertEqual(actual, expect)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6427')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/training_tests/extensions_tests/test_linear_shift.py: 27-40
</a>
<div class="mid" id="frag6427" style="display:none"><pre>
    def _run_trainer(self, extension, expect, optimizer=None):
        if optimizer is None:
            optimizer = self.optimizer
        extension.initialize(self.trainer)

        actual = []
        for _ in expect:
            self.trainer.updater.update()
            actual.append(optimizer.x)
            if self.trigger(self.trainer):
                extension(self.trainer)

        self.assertEqual(actual, expect)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6457')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/training_tests/extensions_tests/test_step_shift.py: 38-51
</a>
<div class="mid" id="frag6457" style="display:none"><pre>
    def _run_trainer(self, extension, expect, optimizer=None):
        if optimizer is None:
            optimizer = self.optimizer
        extension.initialize(self.trainer)

        actual = []
        for _ in expect:
            self.trainer.updater.update()
            actual.append(optimizer.x)
            if self.trigger(self.trainer):
                extension(self.trainer)

        self.assertEqual(actual, expect)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6402')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/training_tests/extensions_tests/test_exponential_shift.py: 32-45
</a>
<div class="mid" id="frag6402" style="display:none"><pre>
    def _run_trainer(self, extension, expect, optimizer=None):
        if optimizer is None:
            optimizer = self.optimizer
        extension.initialize(self.trainer)

        actual = []
        for _ in expect:
            self.trainer.updater.update()
            actual.append(optimizer.x)
            if self.trigger(self.trainer):
                extension(self.trainer)

        self.assertEqual(actual, expect)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 154:</b> &nbsp; 5 fragments, nominal size 13 lines, similarity 84%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6394')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/training_tests/extensions_tests/test_polynomial_shift.py: 75-92
</a>
<div class="mid" id="frag6394" style="display:none"><pre>
    def test_resume(self):
        new_optimizer = mock.Mock()
        new_extension = extensions.PolynomialShift(
            'x', self.rate, self.max_count, self.init, self.target,
            new_optimizer)

        self.trainer.extend(self.extension)
        self.trainer.run()

        new_trainer = testing.get_trainer_with_mock_updater((3, 'iteration'))
        new_trainer.extend(new_extension)
        testing.save_and_load_npz(self.trainer, new_trainer)

        new_extension.initialize(new_trainer)
        self.assertEqual(new_optimizer.x, self.optimizer.x)
        self.assertIsInstance(new_optimizer.x, float)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6406')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/training_tests/extensions_tests/test_exponential_shift.py: 66-82
</a>
<div class="mid" id="frag6406" style="display:none"><pre>
    def test_resume(self):
        new_optimizer = mock.Mock()
        new_extension = extensions.ExponentialShift(
            'x', self.rate, self.init, self.target, new_optimizer)

        self.trainer.extend(self.extension)
        self.trainer.run()

        new_trainer = testing.get_trainer_with_mock_updater((3, 'iteration'))
        new_trainer.extend(new_extension)
        testing.save_and_load_npz(self.trainer, new_trainer)

        new_extension.initialize(new_trainer)
        self.assertEqual(new_optimizer.x, self.optimizer.x)
        self.assertIsInstance(new_optimizer.x, float)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6417')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/training_tests/extensions_tests/test_inverse_shift.py: 73-89
</a>
<div class="mid" id="frag6417" style="display:none"><pre>
    def test_resume(self):
        new_optimizer = mock.Mock()
        new_extension = extensions.InverseShift(
            'x', self.gamma, self.power, self.init, self.target, new_optimizer)

        self.trainer.extend(self.extension)
        self.trainer.run()

        new_trainer = testing.get_trainer_with_mock_updater((3, 'iteration'))
        new_trainer.extend(new_extension)
        testing.save_and_load_npz(self.trainer, new_trainer)

        new_extension.initialize(new_trainer)
        self.assertEqual(new_optimizer.x, self.optimizer.x)
        self.assertIsInstance(new_optimizer.x, float)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6430')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/training_tests/extensions_tests/test_linear_shift.py: 54-70
</a>
<div class="mid" id="frag6430" style="display:none"><pre>
    def test_resume(self):
        new_optimizer = mock.Mock()
        new_extension = extensions.LinearShift(
            'x', self.value_range, self.time_range, new_optimizer)

        self.trainer.extend(self.extension)
        self.trainer.run()

        new_trainer = testing.get_trainer_with_mock_updater((5, 'iteration'))
        new_trainer.extend(new_extension)
        testing.save_and_load_npz(self.trainer, new_trainer)

        new_extension.initialize(new_trainer)
        self.assertEqual(new_optimizer.x, self.optimizer.x)
        self.assertIsInstance(new_optimizer.x, float)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6461')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/training_tests/extensions_tests/test_step_shift.py: 71-87
</a>
<div class="mid" id="frag6461" style="display:none"><pre>
    def test_resume(self):
        new_optimizer = mock.Mock()
        new_extension = extensions.StepShift(
            'x', self.gamma, self.step, self.init, self.target, new_optimizer)

        self.trainer.extend(self.extension)
        self.trainer.run()

        new_trainer = testing.get_trainer_with_mock_updater((5, 'iteration'))
        new_trainer.extend(new_extension)
        testing.save_and_load_npz(self.trainer, new_trainer)

        new_extension.initialize(new_trainer)
        self.assertEqual(new_optimizer.x, self.optimizer.x)
        self.assertIsInstance(new_optimizer.x, float)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 155:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6499')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/training_tests/test_trainer.py: 183-198
</a>
<div class="mid" id="frag6499" style="display:none"><pre>
    def test_add_two_extensions_default_priority(self):
        self.called_order = []

        @training.make_extension(trigger=(1, 'epoch'))
        def dummy_extension_1(trainer):
            self.called_order.append(1)

        @training.make_extension(trigger=(1, 'epoch'))
        def dummy_extension_2(trainer):
            self.called_order.append(2)

        self.trainer.extend(dummy_extension_1)
        self.trainer.extend(dummy_extension_2)
        self.trainer.run()
        self.assertEqual(self.called_order, [1, 2])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6502')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/training_tests/test_trainer.py: 199-214
</a>
<div class="mid" id="frag6502" style="display:none"><pre>
    def test_add_two_extensions_specific_priority(self):
        self.called_order = []

        @training.make_extension(trigger=(1, 'epoch'), priority=50)
        def dummy_extension_1(trainer):
            self.called_order.append(1)

        @training.make_extension(trigger=(1, 'epoch'), priority=100)
        def dummy_extension_2(trainer):
            self.called_order.append(2)

        self.trainer.extend(dummy_extension_1)
        self.trainer.extend(dummy_extension_2)
        self.trainer.run()
        self.assertEqual(self.called_order, [2, 1])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 156:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 84%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6505')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/training_tests/test_trainer.py: 215-241
</a>
<div class="mid" id="frag6505" style="display:none"><pre>
    def test_exception_handler(self):

        ext = ErrorHandlingExtension()
        self.trainer.extend(ext, trigger=(1, 'iteration'), priority=1)
        self.assertFalse(ext.is_error_handled)

        d = {}

        def exception_handler(trainer, exp, tb):
            d['called'] = True

        @training.make_extension(trigger=(1, 'iteration'), priority=100,
                                 on_error=exception_handler)
        def exception_raiser(trainer):
            raise TheOnlyError()
        self.trainer.extend(exception_raiser)

        dummy_extension = DummyExtension(self)
        self.trainer.extend(dummy_extension)

        with self.assertRaises(TheOnlyError):
            self.trainer.run()

        self.assertTrue(d['called'])
        self.assertTrue(ext.is_error_handled)
        self.assertTrue(dummy_extension.is_finalized)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6508')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/training_tests/test_trainer.py: 242-266
</a>
<div class="mid" id="frag6508" style="display:none"><pre>
    def test_exception_in_exception_handler(self):

        ext = ErrorHandlingExtension()
        self.trainer.extend(ext, trigger=(1, 'iteration'), priority=1)
        self.assertFalse(ext.is_error_handled)

        def exception_handler(trainer, exp, tb):
            raise ValueError('hogehoge from exception handler')

        @training.make_extension(trigger=(1, 'iteration'), priority=100,
                                 on_error=exception_handler)
        def exception_raiser(trainer):
            raise TheOnlyError()
        self.trainer.extend(exception_raiser)

        dummy_extension = DummyExtension(self)
        self.trainer.extend(dummy_extension)

        with self.assertRaises(TheOnlyError):
            self.trainer.run()

        self.assertTrue(ext.is_error_handled)
        self.assertTrue(dummy_extension.is_finalized)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 157:</b> &nbsp; 7 fragments, nominal size 21 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6523')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/training_tests/triggers_tests/test_once_trigger.py: 56-75
</a>
<div class="mid" id="frag6523" style="display:none"><pre>
    def test_resumed_trigger(self):
        trainer = testing.get_trainer_with_mock_updater(
            stop_trigger=None, iter_per_epoch=self.iter_per_epoch)
        with tempfile.NamedTemporaryFile(delete=False) as f:
            trigger = training.triggers.OnceTrigger(self.call_on_resume)
            for expected, finished in zip(self.resumed_expected[:self.resume],
                                          self.resumed_finished[:self.resume]):
                trainer.updater.update()
                self.assertEqual(trigger.finished, finished)
                self.assertEqual(trigger(trainer), expected)
            serializers.save_npz(f.name, trigger)

            trigger = training.triggers.OnceTrigger(self.call_on_resume)
            serializers.load_npz(f.name, trigger)
            for expected, finished in zip(self.resumed_expected[self.resume:],
                                          self.resumed_finished[self.resume:]):
                trainer.updater.update()
                self.assertEqual(trigger.finished, finished)
                self.assertEqual(trigger(trainer), expected)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6529')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/training_tests/triggers_tests/test_manual_schedule_trigger.py: 83-102
</a>
<div class="mid" id="frag6529" style="display:none"><pre>
    def test_resumed_trigger(self):
        trainer = testing.get_trainer_with_mock_updater(
            stop_trigger=None, iter_per_epoch=self.iter_per_epoch)
        with tempfile.NamedTemporaryFile(delete=False) as f:
            trigger = training.triggers.ManualScheduleTrigger(*self.schedule)
            for expected, finished in zip(self.expected[:self.resume],
                                          self.finished[:self.resume]):
                trainer.updater.update()
                self.assertEqual(trigger(trainer), expected)
                self.assertEqual(trigger.finished, finished)
            serializers.save_npz(f.name, trigger)

            trigger = training.triggers.ManualScheduleTrigger(*self.schedule)
            serializers.load_npz(f.name, trigger)
            for expected, finished in zip(self.expected[self.resume:],
                                          self.finished[self.resume:]):
                trainer.updater.update()
                self.assertEqual(trigger(trainer), expected)
                self.assertEqual(trigger.finished, finished)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6532')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/training_tests/triggers_tests/test_manual_schedule_trigger.py: 145-167
</a>
<div class="mid" id="frag6532" style="display:none"><pre>
    def test_resumed_trigger_backward_compat(self):
        trainer = testing.get_trainer_with_mock_updater(
            stop_trigger=None, iter_per_epoch=self.iter_per_epoch)
        with tempfile.NamedTemporaryFile(delete=False) as f:
            trigger = training.triggers.ManualScheduleTrigger(*self.schedule)
            for expected, finished in zip(self.expected[:self.resume],
                                          self.finished[:self.resume]):
                trainer.updater.update()
                self.assertEqual(trigger(trainer), expected)
                self.assertEqual(trigger.finished, finished)
            # old version does not save anything
            np.savez(f, dummy=0)

            trigger = training.triggers.ManualScheduleTrigger(*self.schedule)
            with testing.assert_warns(UserWarning):
                serializers.load_npz(f.name, trigger)
            for expected, finished in zip(self.expected[self.resume:],
                                          self.finished[self.resume:]):
                trainer.updater.update()
                self.assertEqual(trigger(trainer), expected)
                self.assertEqual(trigger.finished, finished)


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6526')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/training_tests/triggers_tests/test_once_trigger.py: 126-148
</a>
<div class="mid" id="frag6526" style="display:none"><pre>
    def test_resumed_trigger_backward_compat(self):
        trainer = testing.get_trainer_with_mock_updater(
            stop_trigger=None, iter_per_epoch=self.iter_per_epoch)
        with tempfile.NamedTemporaryFile(delete=False) as f:
            trigger = training.triggers.OnceTrigger(self.call_on_resume)
            for expected, finished in zip(self.resumed_expected[:self.resume],
                                          self.resumed_finished[:self.resume]):
                trainer.updater.update()
                self.assertEqual(trigger.finished, finished)
                self.assertEqual(trigger(trainer), expected)
            # old version does not save anything
            np.savez(f, dummy=0)

            trigger = training.triggers.OnceTrigger(self.call_on_resume)
            with testing.assert_warns(UserWarning):
                serializers.load_npz(f.name, trigger)
            for expected, finished in zip(self.resumed_expected[self.resume:],
                                          self.resumed_finished[self.resume:]):
                trainer.updater.update()
                self.assertEqual(trigger.finished, finished)
                self.assertEqual(trigger(trainer), expected)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6531')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/training_tests/triggers_tests/test_manual_schedule_trigger.py: 118-144
</a>
<div class="mid" id="frag6531" style="display:none"><pre>
    def test_resumed_trigger_sparse_call(self):
        trainer = testing.get_trainer_with_mock_updater(
            stop_trigger=None, iter_per_epoch=self.iter_per_epoch)
        accumulated = False
        with tempfile.NamedTemporaryFile(delete=False) as f:
            trigger = training.triggers.ManualScheduleTrigger(*self.schedule)
            for expected, finished in zip(self.expected[:self.resume],
                                          self.finished[:self.resume]):
                trainer.updater.update()
                accumulated = accumulated or expected
                if random.randrange(2):
                    self.assertEqual(trigger(trainer), accumulated)
                    self.assertEqual(trigger.finished, finished)
                    accumulated = False
            serializers.save_npz(f.name, trigger)

            trigger = training.triggers.ManualScheduleTrigger(*self.schedule)
            serializers.load_npz(f.name, trigger)
            for expected, finished in zip(self.expected[self.resume:],
                                          self.finished[self.resume:]):
                trainer.updater.update()
                accumulated = accumulated or expected
                if random.randrange(2):
                    self.assertEqual(trigger(trainer), accumulated)
                    self.assertEqual(trigger.finished, finished)
                    accumulated = False

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6525')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/training_tests/triggers_tests/test_once_trigger.py: 94-125
</a>
<div class="mid" id="frag6525" style="display:none"><pre>
    def test_resumed_trigger_sparse_call(self):
        trainer = testing.get_trainer_with_mock_updater(
            stop_trigger=None, iter_per_epoch=self.iter_per_epoch)
        accumulated = False
        accumulated_finished = True
        with tempfile.NamedTemporaryFile(delete=False) as f:
            trigger = training.triggers.OnceTrigger(self.call_on_resume)
            for expected, finished in zip(self.resumed_expected[:self.resume],
                                          self.resumed_finished[:self.resume]):
                trainer.updater.update()
                accumulated = accumulated or expected
                accumulated_finished = accumulated_finished and finished
                if random.randrange(2):
                    self.assertEqual(trigger.finished, accumulated_finished)
                    self.assertEqual(trigger(trainer), accumulated)
                    accumulated = False
                    accumulated_finished = True
            serializers.save_npz(f.name, trigger)

            trigger = training.triggers.OnceTrigger(self.call_on_resume)
            serializers.load_npz(f.name, trigger)
            for expected, finished in zip(self.resumed_expected[self.resume:],
                                          self.resumed_finished[self.resume:]):
                trainer.updater.update()
                accumulated = accumulated or expected
                accumulated_finished = accumulated_finished and finished
                if random.randrange(2):
                    self.assertEqual(trigger.finished, accumulated_finished)
                    self.assertEqual(trigger(trainer), accumulated)
                    accumulated = False
                    accumulated_finished = True

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6537')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/training_tests/triggers_tests/test_interval_trigger.py: 79-101
</a>
<div class="mid" id="frag6537" style="display:none"><pre>
    def test_resumed_trigger_sparse_call(self):
        trainer = testing.get_trainer_with_mock_updater(
            stop_trigger=None, iter_per_epoch=self.iter_per_epoch)
        accumulated = False
        with tempfile.NamedTemporaryFile(delete=False) as f:
            trigger = training.triggers.IntervalTrigger(*self.interval)
            for expected in self.expected[:self.resume]:
                trainer.updater.update()
                accumulated = accumulated or expected
                if random.randrange(2):
                    self.assertEqual(trigger(trainer), accumulated)
                    accumulated = False
            serializers.save_npz(f.name, trigger)

            trigger = training.triggers.IntervalTrigger(*self.interval)
            serializers.load_npz(f.name, trigger)
            for expected in self.expected[self.resume:]:
                trainer.updater.update()
                accumulated = accumulated or expected
                if random.randrange(2):
                    self.assertEqual(trigger(trainer), accumulated)
                    accumulated = False

</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 158:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6530')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/training_tests/triggers_tests/test_manual_schedule_trigger.py: 104-116
</a>
<div class="mid" id="frag6530" style="display:none"><pre>
    def test_trigger_sparse_call(self):
        trainer = testing.get_trainer_with_mock_updater(
            stop_trigger=None, iter_per_epoch=self.iter_per_epoch)
        trigger = training.triggers.ManualScheduleTrigger(*self.schedule)
        accumulated = False
        for expected, finished in zip(self.expected, self.finished):
            trainer.updater.update()
            accumulated = accumulated or expected
            if random.randrange(2):
                self.assertEqual(trigger(trainer), accumulated)
                self.assertEqual(trigger.finished, finished)
                accumulated = False

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6536')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/training_tests/triggers_tests/test_interval_trigger.py: 65-77
</a>
<div class="mid" id="frag6536" style="display:none"><pre>
    def test_trigger_sparse_call(self):
        trainer = testing.get_trainer_with_mock_updater(
            stop_trigger=None, iter_per_epoch=self.iter_per_epoch)
        trigger = training.triggers.IntervalTrigger(*self.interval)
        accumulated = False
        # before the first iteration, trigger should be False
        for expected in [False] + self.expected:
            accumulated = accumulated or expected
            if random.randrange(2):
                self.assertEqual(trigger(trainer), accumulated)
                accumulated = False
            trainer.updater.update()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 159:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 86%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6535')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/training_tests/triggers_tests/test_interval_trigger.py: 48-63
</a>
<div class="mid" id="frag6535" style="display:none"><pre>
    def test_resumed_trigger(self):
        trainer = testing.get_trainer_with_mock_updater(
            stop_trigger=None, iter_per_epoch=self.iter_per_epoch)
        with tempfile.NamedTemporaryFile(delete=False) as f:
            trigger = training.triggers.IntervalTrigger(*self.interval)
            for expected in self.expected[:self.resume]:
                trainer.updater.update()
                self.assertEqual(trigger(trainer), expected)
            serializers.save_npz(f.name, trigger)

            trigger = training.triggers.IntervalTrigger(*self.interval)
            serializers.load_npz(f.name, trigger)
            for expected in self.expected[self.resume:]:
                trainer.updater.update()
                self.assertEqual(trigger(trainer), expected)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6538')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/training_tests/triggers_tests/test_interval_trigger.py: 102-119
</a>
<div class="mid" id="frag6538" style="display:none"><pre>
    def test_resumed_trigger_backward_compat(self):
        trainer = testing.get_trainer_with_mock_updater(
            stop_trigger=None, iter_per_epoch=self.iter_per_epoch)
        with tempfile.NamedTemporaryFile(delete=False) as f:
            trigger = training.triggers.IntervalTrigger(*self.interval)
            for expected in self.expected[:self.resume]:
                trainer.updater.update()
                self.assertEqual(trigger(trainer), expected)
            # old version does not save anything
            np.savez(f, dummy=0)

            trigger = training.triggers.IntervalTrigger(*self.interval)
            with testing.assert_warns(UserWarning):
                serializers.load_npz(f.name, trigger)
            for expected in self.expected[self.resume:]:
                trainer.updater.update()
                self.assertEqual(trigger(trainer), expected)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 160:</b> &nbsp; 4 fragments, nominal size 11 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6542')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/training_tests/triggers_tests/test_early_stopping_trigger.py: 24-38
</a>
<div class="mid" id="frag6542" style="display:none"><pre>
    def test_early_stopping_trigger_with_accuracy(self):
        key = 'main/accuracy'
        trigger = triggers.EarlyStoppingTrigger(monitor=key, patience=3,
                                                check_trigger=(1, 'epoch'),
                                                verbose=False)
        trigger = util.get_trigger(trigger)

        accuracies = [0.5, 0.5, 0.6, 0.7, 0.6, 0.4, 0.3, 0.2]
        accuracies = numpy.asarray([
            chainer.Variable(numpy.asarray(acc, dtype=numpy.float32))
            for acc in accuracies])

        expected = [False, False, False, False, False, False, True, True]
        _test_trigger(self, trigger, key, accuracies, expected)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6544')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/training_tests/triggers_tests/test_early_stopping_trigger.py: 53-67
</a>
<div class="mid" id="frag6544" style="display:none"><pre>
    def test_early_stopping_trigger_with_max_epoch(self):
        key = 'main/loss'
        trigger = triggers.EarlyStoppingTrigger(monitor=key, patience=3,
                                                check_trigger=(1, 'epoch'),
                                                max_trigger=(3, 'epoch'))
        trigger = util.get_trigger(trigger)

        accuracies = [100, 80, 30]
        accuracies = numpy.asarray([
            chainer.Variable(numpy.asarray(acc, dtype=numpy.float32))
            for acc in accuracies])

        expected = [False, False, True]
        _test_trigger(self, trigger, key, accuracies, expected)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6543')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/training_tests/triggers_tests/test_early_stopping_trigger.py: 39-52
</a>
<div class="mid" id="frag6543" style="display:none"><pre>
    def test_early_stopping_trigger_with_loss(self):
        key = 'main/loss'
        trigger = triggers.EarlyStoppingTrigger(monitor=key, patience=3,
                                                check_trigger=(1, 'epoch'))
        trigger = util.get_trigger(trigger)

        accuracies = [100, 80, 30, 10, 20, 24, 30, 35]
        accuracies = numpy.asarray([
            chainer.Variable(numpy.asarray(acc, dtype=numpy.float32))
            for acc in accuracies])

        expected = [False, False, False, False, False, False, True, True]
        _test_trigger(self, trigger, key, accuracies, expected)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6545')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/training_tests/triggers_tests/test_early_stopping_trigger.py: 68-83
</a>
<div class="mid" id="frag6545" style="display:none"><pre>
    def test_early_stopping_trigger_with_max_iteration(self):
        key = 'main/loss'
        trigger = triggers.EarlyStoppingTrigger(monitor=key, patience=3,
                                                check_trigger=(1, 'epoch'),
                                                max_trigger=(3, 'iteration'))
        trigger = util.get_trigger(trigger)

        accuracies = [100, 80, 30]
        accuracies = numpy.asarray([
            chainer.Variable(numpy.asarray(acc, dtype=numpy.float32))
            for acc in accuracies])

        expected = [False, False, True]
        _test_trigger(self, trigger, key, accuracies, expected)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 161:</b> &nbsp; 3 fragments, nominal size 15 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6555')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_lstm.py: 77-94
</a>
<div class="mid" id="frag6555" style="display:none"><pre>
    def generate_grad_outputs(self, outputs_template):
        grad_out = []
        c = outputs_template[0]
        h = outputs_template[1]

        c_shape = c.shape
        h_shape = h.shape
        if self.grad_outputs[0] is True:
            grad_out.append(_shaped_random(c_shape, c.dtype))
        else:
            grad_out.append(None)

        if self.grad_outputs[1] is True:
            grad_out.append(_shaped_random(h_shape, h.dtype))
        else:
            grad_out.append(None)
        return tuple(grad_out)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6609')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_slstm.py: 123-143
</a>
<div class="mid" id="frag6609" style="display:none"><pre>
        h_expect = _sigmoid(o1_in + o2_in) * numpy.tanh(c_expect)
        return c_expect, h_expect

    def generate_grad_outputs(self, outputs_template):
        grad_out = []
        c = outputs_template[0]
        h = outputs_template[1]

        c_shape = c.shape
        h_shape = h.shape
        if self.grad_outputs[0] is True:
            grad_out.append(numpy.random.uniform(-1, 1,
                                                 h_shape).astype(h.dtype))
        else:
            grad_out.append(None)

        if self.grad_outputs[1] is True:
            grad_out.append(numpy.random.uniform(-1, 1,
                                                 c_shape).astype(c.dtype))
        else:
            grad_out.append(None)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6556')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_lstm.py: 95-113
</a>
<div class="mid" id="frag6556" style="display:none"><pre>
    def generate_grad_grad_inputs(self, inputs_template):
        grad_grad_in = []
        c = inputs_template[0]
        x = inputs_template[1]

        c_shape = c.shape
        x_shape = x.shape
        if self.grad_grad_inputs[0] is True:
            grad_grad_in.append(_shaped_random(c_shape, c.dtype))
        else:
            grad_grad_in.append(None)

        if self.grad_grad_inputs[1] is True:
            grad_grad_in.append(_shaped_random(x_shape, x.dtype))
        else:
            grad_grad_in.append(None)
        return tuple(grad_grad_in)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 162:</b> &nbsp; 11 fragments, nominal size 23 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6567')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_lstm.py: 114-140
</a>
<div class="mid" id="frag6567" style="display:none"><pre>

    def generate_inputs(self):
        h_shape = (self.n_layers, self.batches[0], self.hidden_size)
        dtype = numpy.float32

        h = array(h_shape, dtype)
        c = array(h_shape, dtype)
        in_size = self.input_size
        out_size = self.hidden_size
        xs = []
        for b in range(len(self.batches)):
            xs.append(array((self.batches[b], in_size), dtype))

        def w_in(i, j):
            return in_size if i == 0 and j &lt; 4 else out_size

        inputs = []
        inputs.append(h)
        inputs.append(c)
        for i in range(len(self.batches)):
            inputs.append(xs[i])
        for n in range(self.n_layers):
            for i in range(8):
                inputs.append(array((out_size, w_in(n, i)), dtype))
            for i in range(8):
                inputs.append(array((out_size,), dtype))
        return tuple(inputs)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6616')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_rnn.py: 75-98
</a>
<div class="mid" id="frag6616" style="display:none"><pre>
    def generate_inputs(self):
        h_shape = (self.n_layers, self.batches[0], self.hidden_size)
        dtype = self.dtype

        h = array(h_shape, dtype)
        in_size = self.input_size
        out_size = self.hidden_size
        xs = [array((self.batches[b], in_size), dtype)
              for b in range(len(self.batches))]

        def w_in(i, j):
            return in_size if i == 0 and j &lt; 1 else out_size

        inputs = []
        inputs.append(h)
        for i in range(len(self.batches)):
            inputs.append(xs[i])
        for n in range(self.n_layers):
            for i in range(2):
                inputs.append(array((out_size, w_in(n, i)), dtype))
            for i in range(2):
                inputs.append(array((out_size,), dtype))
        return tuple(inputs)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6584')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_gru.py: 62-85
</a>
<div class="mid" id="frag6584" style="display:none"><pre>
    def generate_inputs(self):
        h_shape = (self.n_layers, self.batches[0], self.hidden_size)
        dtype = numpy.float32

        h = array(h_shape, dtype)
        in_size = self.input_size
        out_size = self.hidden_size
        xs = [array((self.batches[b], in_size), dtype)
              for b in range(len(self.batches))]

        def w_in(i, j):
            return in_size if i == 0 and j &lt; 3 else out_size

        inputs = []
        inputs.append(h)
        for i in range(len(self.batches)):
            inputs.append(xs[i])
        for n in range(self.n_layers):
            for i in range(6):
                inputs.append(array((out_size, w_in(n, i)), dtype))
            for i in range(6):
                inputs.append(array((out_size,), dtype))
        return tuple(inputs)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6573')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_lstm.py: 246-278
</a>
<div class="mid" id="frag6573" style="display:none"><pre>

    def generate_inputs(self):
        h_shape = (self.n_layers * 2, self.batches[0], self.hidden_size)
        dtype = numpy.float32

        h = array(h_shape, dtype)
        c = array(h_shape, dtype)
        in_size = self.input_size
        out_size = self.hidden_size
        xs = []
        for b in range(len(self.batches)):
            xs.append(array((self.batches[b], in_size), dtype))

        def w_in(i, j):
            if i == 0 and j &lt; 4:
                return in_size
            elif i &gt; 0 and j &lt; 4:
                return out_size * 2
            else:
                return out_size

        inputs = []
        inputs.append(h)
        inputs.append(c)
        for i in range(len(self.batches)):
            inputs.append(xs[i])
        for n in range(self.n_layers):
            for direction in (0, 1):
                for i in range(8):
                    inputs.append(array((out_size, w_in(n, i)), dtype))
                for i in range(8):
                    inputs.append(array((out_size,), dtype))
        return tuple(inputs)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9668')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py: 479-505
</a>
<div class="mid" id="frag9668" style="display:none"><pre>
    def generate_inputs(self):
        h_shape = (self.n_layers, self.batches[0], self.hidden_size)
        dtype = self.in_dtypes[0]

        h = array_utils.uniform(h_shape, dtype)

        in_size = self.input_size
        out_size = self.hidden_size
        xs = [array_utils.uniform((self.batches[b], in_size), dtype)
              for b in range(len(self.batches))]

        def w_in(i, j):
            return in_size if i == 0 and j &lt; 1 else out_size

        inputs = []
        inputs.append(h)
        for i in range(len(self.batches)):
            inputs.append(xs[i])

        for n in range(self.n_layers):
            for i in range(2):
                inputs.append(array_utils.uniform(
                    (out_size, w_in(n, i)), dtype))
            for i in range(2):
                inputs.append(array_utils.uniform((out_size,), dtype))
        return tuple(inputs)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9656')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py: 267-293
</a>
<div class="mid" id="frag9656" style="display:none"><pre>
    def generate_inputs(self):
        h_shape = (self.n_layers, self.batches[0], self.hidden_size)
        dtype = self.in_dtypes[0]

        h = array_utils.uniform(h_shape, dtype)

        in_size = self.input_size
        out_size = self.hidden_size
        xs = [array_utils.uniform((self.batches[b], in_size), dtype)
              for b in range(len(self.batches))]

        def w_in(i, j):
            return in_size if i == 0 and j &lt; 3 else out_size

        inputs = []
        inputs.append(h)
        for i in range(len(self.batches)):
            inputs.append(xs[i])

        for n in range(self.n_layers):
            for i in range(6):
                inputs.append(array_utils.uniform(
                    (out_size, w_in(n, i)), dtype))
            for i in range(6):
                inputs.append(array_utils.uniform((out_size,), dtype))
        return tuple(inputs)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9650')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py: 156-189
</a>
<div class="mid" id="frag9650" style="display:none"><pre>
    def generate_inputs(self):
        h_shape = (self.n_layers * 2, self.batches[0], self.hidden_size)
        dtype = self.in_dtypes[0]

        h = array_utils.uniform(h_shape, dtype)
        c = array_utils.uniform(h_shape, dtype)
        in_size = self.input_size
        out_size = self.hidden_size
        xs = [array_utils.uniform((self.batches[b], in_size), dtype)
              for b in range(len(self.batches))]

        def w_in(i, j):
            if i == 0 and j &lt; 4:
                return in_size
            elif i &gt; 0 and j &lt; 4:
                return out_size * 2
            else:
                return out_size

        inputs = []
        inputs.append(h)
        inputs.append(c)
        for i in range(len(self.batches)):
            inputs.append(xs[i])

        for n in range(self.n_layers):
            for direction in (0, 1):
                for i in range(8):
                    inputs.append(array_utils.uniform(
                        (out_size, w_in(n, i)), dtype))
                for i in range(8):
                    inputs.append(array_utils.uniform((out_size,), dtype))
        return tuple(inputs)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6622')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_rnn.py: 200-230
</a>
<div class="mid" id="frag6622" style="display:none"><pre>
    def generate_inputs(self):
        h_shape = (self.n_layers * 2, self.batches[0], self.hidden_size)
        dtype = self.dtype

        h = array(h_shape, dtype)
        in_size = self.input_size
        out_size = self.hidden_size
        xs = [array((self.batches[b], in_size), dtype)
              for b in range(len(self.batches))]

        def w_in(i, j):
            if i == 0 and j &lt; 1:
                return in_size
            elif i &gt; 0 and j &lt; 1:
                return out_size * 2
            else:
                return out_size

        inputs = []
        inputs.append(h)
        for i in range(len(self.batches)):
            inputs.append(xs[i])

        for n in range(self.n_layers):
            for direction in (0, 1):
                for i in range(2):
                    inputs.append(array((out_size, w_in(n, i)), dtype))
                for i in range(2):
                    inputs.append(array((out_size,), dtype))
        return tuple(inputs)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6590')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_gru.py: 188-218
</a>
<div class="mid" id="frag6590" style="display:none"><pre>
    def generate_inputs(self):
        h_shape = (self.n_layers * 2, self.batches[0], self.hidden_size)
        dtype = numpy.float32

        h = array(h_shape, dtype)
        in_size = self.input_size
        out_size = self.hidden_size
        xs = [array((self.batches[b], in_size), dtype)
              for b in range(len(self.batches))]

        def w_in(i, j):
            if i == 0 and j &lt; 3:
                return in_size
            elif i &gt; 0 and j &lt; 3:
                return out_size * 2
            else:
                return out_size

        inputs = []
        inputs.append(h)
        for i in range(len(self.batches)):
            inputs.append(xs[i])

        for n in range(self.n_layers):
            for direction in (0, 1):
                for i in range(6):
                    inputs.append(array((out_size, w_in(n, i)), dtype))
                for i in range(6):
                    inputs.append(array((out_size,), dtype))
        return tuple(inputs)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9644')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py: 54-81
</a>
<div class="mid" id="frag9644" style="display:none"><pre>
    def generate_inputs(self):
        h_shape = (self.n_layers, self.batches[0], self.hidden_size)
        dtype = self.in_dtypes[0]

        h = array_utils.uniform(h_shape, dtype)
        c = array_utils.uniform(h_shape, dtype)
        in_size = self.input_size
        out_size = self.hidden_size
        xs = [array_utils.uniform((self.batches[b], in_size), dtype)
              for b in range(len(self.batches))]

        def w_in(i, j):
            return in_size if i == 0 and j &lt; 4 else out_size

        inputs = []
        inputs.append(h)
        inputs.append(c)
        for i in range(len(self.batches)):
            inputs.append(xs[i])

        for n in range(self.n_layers):
            for i in range(8):
                inputs.append(array_utils.uniform(
                    (out_size, w_in(n, i)), dtype))
            for i in range(8):
                inputs.append(array_utils.uniform((out_size,), dtype))
        return tuple(inputs)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9662')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py: 365-396
</a>
<div class="mid" id="frag9662" style="display:none"><pre>
    def generate_inputs(self):
        h_shape = (self.n_layers * 2, self.batches[0], self.hidden_size)
        dtype = self.in_dtypes[0]

        h = array_utils.uniform(h_shape, dtype)
        in_size = self.input_size
        out_size = self.hidden_size
        xs = [array_utils.uniform((self.batches[b], in_size), dtype)
              for b in range(len(self.batches))]

        def w_in(i, j):
            if i == 0 and j &lt; 3:
                return in_size
            elif i &gt; 0 and j &lt; 3:
                return out_size * 2
            else:
                return out_size

        inputs = []
        inputs.append(h)
        for i in range(len(self.batches)):
            inputs.append(xs[i])

        for n in range(self.n_layers):
            for direction in (0, 1):
                for i in range(6):
                    inputs.append(array_utils.uniform(
                        (out_size, w_in(n, i)), dtype))
                for i in range(6):
                    inputs.append(array_utils.uniform((out_size,), dtype))
        return tuple(inputs)

</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 163:</b> &nbsp; 12 fragments, nominal size 12 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6569')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_lstm.py: 141-153
</a>
<div class="mid" id="frag6569" style="display:none"><pre>

    def process_inputs(self, inputs):
        h = inputs[0]
        c = inputs[1]
        xs = inputs[2: 2 + len(self.batches)]
        ws = []
        bs = []
        index = 2 + len(self.batches)
        for n in range(self.n_layers):
            ws.append(inputs[index: index + 8])
            bs.append(inputs[index + 8: index + 16])
            index += 16
        return h, c, ws, bs, xs
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6618')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_rnn.py: 99-112
</a>
<div class="mid" id="frag6618" style="display:none"><pre>
    def process_inputs(self, inputs):
        h = inputs[0]

        xs = inputs[1: 1 + len(self.batches)]
        ws = []
        bs = []
        index = 1 + len(self.batches)
        for n in range(self.n_layers):
            ws.append(inputs[index: index + 2])
            bs.append(inputs[index + 2: index + 4])
            index += 4

        return h, ws, bs, xs

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9664')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py: 397-410
</a>
<div class="mid" id="frag9664" style="display:none"><pre>
    def process_input(self, inputs):
        h = inputs[0]
        xs = inputs[1:1 + len(self.batches)]
        ws = []
        bs = []
        index = 1 + len(self.batches)
        for n in range(self.n_layers):
            ws.append(inputs[index: index + 6])
            bs.append(inputs[index + 6: index + 12])
            ws.append(inputs[index + 12: index + 18])
            bs.append(inputs[index + 18: index + 24])
            index += 24
        return h, ws, bs, xs

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9646')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py: 82-94
</a>
<div class="mid" id="frag9646" style="display:none"><pre>
    def process_input(self, inputs):
        h = inputs[0]
        c = inputs[1]
        xs = inputs[2:2 + len(self.batches)]
        ws = []
        bs = []
        index = 2 + len(self.batches)
        for n in range(self.n_layers):
            ws.append(inputs[index: index + 8])
            bs.append(inputs[index + 8: index + 16])
            index += 16
        return h, c, ws, bs, xs

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6586')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_gru.py: 86-99
</a>
<div class="mid" id="frag6586" style="display:none"><pre>
    def process_inputs(self, inputs):
        h = inputs[0]

        xs = inputs[1:1 + len(self.batches)]
        ws = []
        bs = []
        index = 1 + len(self.batches)
        for n in range(self.n_layers):
            ws.append(inputs[index: index + 6])
            bs.append(inputs[index + 6: index + 12])
            index += 12

        return h, ws, bs, xs

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6575')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_lstm.py: 279-293
</a>
<div class="mid" id="frag6575" style="display:none"><pre>

    def process_inputs(self, inputs):
        h = inputs[0]
        c = inputs[1]
        xs = inputs[2:2 + len(self.batches)]
        ws = []
        bs = []
        index = 2 + len(self.batches)
        for n in range(self.n_layers):
            ws.append(inputs[index: index + 8])
            bs.append(inputs[index + 8: index + 16])
            ws.append(inputs[index + 16: index + 24])
            bs.append(inputs[index + 24: index + 32])
            index += 32
        return h, c, ws, bs, xs
</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9676')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py: 625-638
</a>
<div class="mid" id="frag9676" style="display:none"><pre>
    def process_input(self, inputs):
        h = inputs[0]
        xs = inputs[1:1 + len(self.batches)]
        ws = []
        bs = []
        index = 1 + len(self.batches)
        for n in range(self.n_layers):
            ws.append(inputs[index: index + 2])
            bs.append(inputs[index + 2: index + 4])
            ws.append(inputs[index + 4: index + 6])
            bs.append(inputs[index + 6: index + 8])
            index += 8
        return h, ws, bs, xs

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9658')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py: 294-305
</a>
<div class="mid" id="frag9658" style="display:none"><pre>
    def process_input(self, inputs):
        h = inputs[0]
        xs = inputs[1:1 + len(self.batches)]
        ws = []
        bs = []
        index = 1 + len(self.batches)
        for n in range(self.n_layers):
            ws.append(inputs[index: index + 6])
            bs.append(inputs[index + 6: index + 12])
            index += 12
        return h, ws, bs, xs

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9670')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py: 506-517
</a>
<div class="mid" id="frag9670" style="display:none"><pre>
    def process_input(self, inputs):
        h = inputs[0]
        xs = inputs[1:1 + len(self.batches)]
        ws = []
        bs = []
        index = 1 + len(self.batches)
        for n in range(self.n_layers):
            ws.append(inputs[index: index + 2])
            bs.append(inputs[index + 2: index + 4])
            index += 4
        return h, ws, bs, xs

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9652')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py: 190-204
</a>
<div class="mid" id="frag9652" style="display:none"><pre>
    def process_input(self, inputs):
        h = inputs[0]
        c = inputs[1]
        xs = inputs[2:2 + len(self.batches)]
        ws = []
        bs = []
        index = 2 + len(self.batches)
        for n in range(self.n_layers):
            ws.append(inputs[index: index + 8])
            bs.append(inputs[index + 8: index + 16])
            ws.append(inputs[index + 16: index + 24])
            bs.append(inputs[index + 24: index + 32])
            index += 32
        return h, c, ws, bs, xs

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6624')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_rnn.py: 231-244
</a>
<div class="mid" id="frag6624" style="display:none"><pre>
    def process_inputs(self, inputs):
        h = inputs[0]
        xs = inputs[1: 1 + len(self.batches)]
        ws = []
        bs = []
        index = 1 + len(self.batches)
        for n in range(self.n_layers):
            ws.append(inputs[index: index + 2])
            bs.append(inputs[index + 2: index + 4])
            ws.append(inputs[index + 4: index + 6])
            bs.append(inputs[index + 6: index + 8])
            index += 8
        return h, ws, bs, xs

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6592')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_gru.py: 219-232
</a>
<div class="mid" id="frag6592" style="display:none"><pre>
    def process_inputs(self, inputs):
        h = inputs[0]
        xs = inputs[1:1 + len(self.batches)]
        ws = []
        bs = []
        index = 1 + len(self.batches)
        for n in range(self.n_layers):
            ws.append(inputs[index: index + 6])
            bs.append(inputs[index + 6: index + 12])
            ws.append(inputs[index + 12: index + 18])
            bs.append(inputs[index + 18: index + 24])
            index += 24
        return h, ws, bs, xs

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 164:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6570')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_lstm.py: 154-167
</a>
<div class="mid" id="frag6570" style="display:none"><pre>

    def forward(self, inputs, device):
        h, c, ws, bs, xs = self.process_inputs(inputs)
        if h.array.dtype == numpy.float64:
            with chainer.using_config('use_cudnn', 'never'):
                out = F.n_step_lstm(self.n_layers, 0.0, h, c, ws, bs, xs)
        else:
            out = F.n_step_lstm(self.n_layers, 0.0, h, c, ws, bs, xs)
        rets = []
        rets.append(out[0])
        rets.append(out[1])
        for i in range(len(out[2])):
            rets.append(out[2][i])
        return tuple(rets)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6576')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_lstm.py: 294-308
</a>
<div class="mid" id="frag6576" style="display:none"><pre>

    def forward(self, inputs, device):
        h, c, ws, bs, xs = self.process_inputs(inputs)
        if h.array.dtype == numpy.float64:
            with chainer.using_config('use_cudnn', 'never'):
                out = F.n_step_bilstm(self.n_layers, 0.0, h, c, ws, bs, xs)
        else:
            out = F.n_step_bilstm(self.n_layers, 0.0, h, c, ws, bs, xs)

        rets = []
        rets.append(out[0])
        rets.append(out[1])
        for i in range(len(out[2])):
            rets.append(out[2][i])
        return tuple(rets)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 165:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6587')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_gru.py: 100-113
</a>
<div class="mid" id="frag6587" style="display:none"><pre>
    def forward(self, inputs, device):
        h, ws, bs, xs = self.process_inputs(inputs)
        if h.array.dtype == numpy.float64:
            with chainer.using_config('use_cudnn', 'never'):
                out = F.n_step_gru(self.n_layers, 0.0, h, ws, bs, xs)
        else:
            out = F.n_step_gru(self.n_layers, 0.0, h, ws, bs, xs)

        rets = []
        rets.append(out[0])
        for i in range(len(out[1])):
            rets.append(out[1][i])
        return tuple(rets)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6593')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_gru.py: 233-245
</a>
<div class="mid" id="frag6593" style="display:none"><pre>
    def forward(self, inputs, device):
        h, ws, bs, xs = self.process_inputs(inputs)
        if h.array.dtype == numpy.float64:
            with chainer.using_config('use_cudnn', 'never'):
                out = F.n_step_bigru(self.n_layers, 0.0, h, ws, bs, xs)
        else:
            out = F.n_step_bigru(self.n_layers, 0.0, h, ws, bs, xs)
        rets = []
        rets.append(out[0])
        for i in range(len(out[1])):
            rets.append(out[1][i])
        return tuple(rets)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 166:</b> &nbsp; 2 fragments, nominal size 25 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6588')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_gru.py: 114-143
</a>
<div class="mid" id="frag6588" style="display:none"><pre>
    def forward_expected(self, inputs):
        h, ws, bs, xs = self.process_inputs(inputs)
        e_hy = h.copy()
        ys = []
        for ind in range(len(xs)):
            x = xs[ind]
            batch = x.shape[0]
            for layer in range(self.n_layers):
                w = ws[layer]
                b = bs[layer]
                h_prev = e_hy[layer, :batch]

                # GRU
                z = sigmoid(x.dot(w[1].T) + h_prev.dot(w[4].T) + b[1] + b[4])
                r = sigmoid(x.dot(w[0].T) + h_prev.dot(w[3].T) + b[0] + b[3])
                h_bar = numpy.tanh(x.dot(w[2].T) +
                                   r *
                                   ((h_prev).dot(w[5].T) + b[5]) + b[2])
                e_h = (1 - z) * h_bar + z * h_prev
                e_hy[layer, :batch] = e_h

                x = e_h
            ys.append(x)
        rets = []
        rets.append(e_hy)
        for i in range(len(ys)):
            rets.append(ys[i])
        return tuple(rets)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6620')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_rnn.py: 123-152
</a>
<div class="mid" id="frag6620" style="display:none"><pre>
    def forward_expected(self, inputs):
        h, ws, bs, xs = self.process_inputs(inputs)

        e_hy = h.copy()
        ys = []
        for ind in range(len(xs)):
            x = xs[ind]
            batch = x.shape[0]
            for layer in range(self.n_layers):
                w = ws[layer]
                b = bs[layer]
                h_prev = e_hy[layer, :batch]
                if self.activation == 'tanh':
                    e_h = numpy.tanh(x.dot(w[0].T) +
                                     h_prev.dot(w[1].T) + b[0] + b[1])
                elif self.activation == 'relu':
                    e_h = _relu(x.dot(w[0].T) +
                                h_prev.dot(w[1].T) + b[0] + b[1])

                e_hy[layer, :batch] = e_h

                x = e_h
            ys.append(x)
        rets = []
        rets.append(e_hy)
        for i in range(len(ys)):
            rets.append(ys[i])
        return tuple(rets)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 167:</b> &nbsp; 2 fragments, nominal size 47 lines, similarity 74%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6594')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_gru.py: 246-300
</a>
<div class="mid" id="frag6594" style="display:none"><pre>
    def forward_expected(self, inputs):
        h, ws, bs, xs = self.process_inputs(inputs)
        xs_next = xs
        e_hy = h.copy()
        for layer in range(self.n_layers):
            # forward
            di = 0
            xf = []
            layer_idx = layer * 2 + di
            w = ws[layer_idx]
            b = bs[layer_idx]
            for ind in range(len(xs)):
                x = xs_next[ind]
                batch = x.shape[0]
                h_prev = e_hy[layer_idx, :batch]
                # GRU
                z = sigmoid(x.dot(w[1].T) + h_prev.dot(w[4].T) + b[1] + b[4])
                r = sigmoid(x.dot(w[0].T) + h_prev.dot(w[3].T) + b[0] + b[3])
                h_bar = numpy.tanh(x.dot(w[2].T) +
                                   r *
                                   ((h_prev).dot(w[5].T) + b[5]) + b[2])
                e_h = (1 - z) * h_bar + z * h_prev
                e_hy[layer_idx, :batch] = e_h
                xf.append(e_h)

            # backward
            di = 1
            xb = []
            layer_idx = layer * 2 + di
            w = ws[layer_idx]
            b = bs[layer_idx]
            for ind in reversed(range(len(xs))):
                x = xs_next[ind]
                batch = x.shape[0]
                h_prev = e_hy[layer_idx, :batch]
                # GRU
                z = sigmoid(x.dot(w[1].T) + h_prev.dot(w[4].T) + b[1] + b[4])
                r = sigmoid(x.dot(w[0].T) + h_prev.dot(w[3].T) + b[0] + b[3])
                h_bar = numpy.tanh(x.dot(w[2].T) +
                                   r *
                                   ((h_prev).dot(w[5].T) + b[5]) + b[2])
                e_h = (1 - z) * h_bar + z * h_prev
                e_hy[layer_idx, :batch] = e_h
                xb.append(e_h)
            xb.reverse()
            xs_next = [numpy.concatenate([hfi, hbi], axis=1) for (hfi, hbi) in
                       zip(xf, xb)]

        rets = []
        rets.append(e_hy)
        for x in xs_next:
            rets.append(x)
        return tuple(rets)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6626')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_rnn.py: 255-308
</a>
<div class="mid" id="frag6626" style="display:none"><pre>
    def forward_expected(self, inputs):
        h, ws, bs, xs = self.process_inputs(inputs)
        xs_next = xs
        e_hy = h.copy()
        for layer in range(self.n_layers):
            # forward
            di = 0
            xf = []
            layer_idx = layer * 2 + di
            w = ws[layer_idx]
            b = bs[layer_idx]
            for ind in range(len(xs)):
                x = xs_next[ind]
                batch = x.shape[0]
                h_prev = e_hy[layer_idx, :batch]
                if self.activation == 'tanh':
                    e_h = numpy.tanh(x.dot(w[0].T) +
                                     h_prev.dot(w[1].T) + b[0] + b[1])
                elif self.activation == 'relu':
                    e_h = _relu(x.dot(w[0].T) +
                                h_prev.dot(w[1].T) + b[0] + b[1])

                e_hy[layer_idx, :batch] = e_h
                xf.append(e_h)

            # backward
            di = 1
            xb = []
            layer_idx = layer * 2 + di
            w = ws[layer_idx]
            b = bs[layer_idx]
            for ind in reversed(range(len(xs))):
                x = xs_next[ind]
                batch = x.shape[0]
                h_prev = e_hy[layer_idx, :batch]
                if self.activation == 'tanh':
                    e_h = numpy.tanh(x.dot(w[0].T) +
                                     h_prev.dot(w[1].T) + b[0] + b[1])
                elif self.activation == 'relu':
                    e_h = _relu(x.dot(w[0].T) +
                                h_prev.dot(w[1].T) + b[0] + b[1])

                e_hy[layer_idx, :batch] = e_h
                xb.append(e_h)
            xb.reverse()
            xs_next = [numpy.concatenate([hfi, hbi], axis=1)
                       for (hfi, hbi) in zip(xf, xb)]
        rets = []
        rets.append(e_hy)
        for x in xs_next:
            rets.append(x)
        return tuple(rets)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 168:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 86%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6673')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/normalization_tests/test_batch_normalization.py: 221-238
</a>
<div class="mid" id="frag6673" style="display:none"><pre>
    def check_backward(self, inputs, grad_outputs, backend_config):
        inputs = backend_config.get_array(inputs)
        grad_outputs = backend_config.get_array(grad_outputs)
        if not self.c_contiguous:
            with backend_config:
                inputs = _as_noncontiguous_array(inputs)
                grad_outputs = _as_noncontiguous_array(grad_outputs)

        def f(*inputs):
            y = functions.batch_normalization(
                *inputs, **self.bn_options)
            return y,

        with backend_config:
            gradient_check.check_backward(
                f, inputs, grad_outputs,
                **self.check_backward_options)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6684')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/normalization_tests/test_batch_normalization.py: 360-376
</a>
<div class="mid" id="frag6684" style="display:none"><pre>
    def check_backward(self, inputs, grad_outputs, backend_config):
        inputs = backend_config.get_array(inputs)
        grad_outputs = backend_config.get_array(grad_outputs)
        if not self.c_contiguous:
            with backend_config:
                inputs = _as_noncontiguous_array(inputs)
                grad_outputs = _as_noncontiguous_array(grad_outputs)

        def f(*inputs):
            y = functions.fixed_batch_normalization(*inputs, eps=self.eps)
            return y,

        with backend_config:
            gradient_check.check_backward(
                f, inputs, grad_outputs,
                **self.check_backward_options)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 169:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 88%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6676')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/normalization_tests/test_batch_normalization.py: 242-261
</a>
<div class="mid" id="frag6676" style="display:none"><pre>
    def check_double_backward(
            self, inputs, grad_outputs, grad_grad_inputs, backend_config):
        inputs = backend_config.get_array(inputs)
        grad_outputs = backend_config.get_array(grad_outputs)
        grad_grad_inputs = backend_config.get_array(grad_grad_inputs)
        if not self.c_contiguous:
            with backend_config:
                inputs = _as_noncontiguous_array(inputs)
                grad_outputs = _as_noncontiguous_array(grad_outputs)
                grad_grad_inputs = _as_noncontiguous_array(grad_grad_inputs)

        def f(*inputs):
            return functions.batch_normalization(
                *inputs, **self.bn_options)

        with backend_config:
            gradient_check.check_double_backward(
                f, inputs, grad_outputs, grad_grad_inputs,
                **self.check_double_backward_options)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6687')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/normalization_tests/test_batch_normalization.py: 380-398
</a>
<div class="mid" id="frag6687" style="display:none"><pre>
    def check_double_backward(
            self, inputs, grad_outputs, grad_grad_inputs, backend_config):
        inputs = backend_config.get_array(inputs)
        grad_outputs = backend_config.get_array(grad_outputs)
        grad_grad_inputs = backend_config.get_array(grad_grad_inputs)
        if not self.c_contiguous:
            with backend_config:
                inputs = _as_noncontiguous_array(inputs)
                grad_outputs = _as_noncontiguous_array(grad_outputs)
                grad_grad_inputs = _as_noncontiguous_array(grad_grad_inputs)

        def f(*inputs):
            return functions.fixed_batch_normalization(*inputs, eps=self.eps)

        with backend_config:
            gradient_check.check_double_backward(
                f, inputs, grad_outputs, grad_grad_inputs,
                **self.check_double_backward_options)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 170:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6775')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/math_tests/test_sparse_matmul.py: 114-125
</a>
<div class="mid" id="frag6775" style="display:none"><pre>
    def check_SPDN_backward(self, a_data, b_data, c_grad, atol, rtol):
        sp_a = utils.to_coo(a_data)
        func = F.math.sparse_matmul.CooMatMul(
            sp_a.row, sp_a.col, sp_a.shape, sp_a.order,
            transa=self.transa, transb=self.transb, transc=False)

        def op(a, b):
            return func.apply((a, b))[0]
        gradient_check.check_backward(
            op, (sp_a.data.data, b_data), c_grad, atol=atol, rtol=rtol,
            dtype=numpy.float32)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6786')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/math_tests/test_sparse_matmul.py: 194-205
</a>
<div class="mid" id="frag6786" style="display:none"><pre>
    def check_DNSP_backward(self, a_data, b_data, c_grad, atol, rtol):
        sp_b = utils.to_coo(b_data)
        func = F.math.sparse_matmul.CooMatMul(
            sp_b.row, sp_b.col, sp_b.shape, sp_b.order,
            transa=not self.transb, transb=not self.transa, transc=True)

        def op(b, a):
            return func.apply((b, a))[0]
        gradient_check.check_backward(
            op, (sp_b.data.data, a_data), c_grad, atol=atol, rtol=rtol,
            dtype=numpy.float32)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 171:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 92%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6779')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/math_tests/test_sparse_matmul.py: 138-153
</a>
<div class="mid" id="frag6779" style="display:none"><pre>
    def check_SPDN_double_backward(
            self, a_data, b_data, c_grad, a_grad_grad, b_grad_grad,
            atol, rtol):
        sp_a = utils.to_coo(a_data)
        sp_gga = utils.to_coo(a_grad_grad)
        func = F.math.sparse_matmul.CooMatMul(
            sp_a.row, sp_a.col, sp_a.shape, sp_a.order,
            transa=self.transa, transb=self.transb, transc=False)

        def op(a, b):
            return func.apply((a, b))[0]
        gradient_check.check_double_backward(
            op, (sp_a.data.data, b_data),
            c_grad, (sp_gga.data.data, b_grad_grad),
            atol=atol, rtol=rtol, dtype=numpy.float32)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag6790')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/math_tests/test_sparse_matmul.py: 218-233
</a>
<div class="mid" id="frag6790" style="display:none"><pre>
    def check_DNSP_double_backward(
            self, a_data, b_data, c_grad, a_grad_grad, b_grad_grad,
            atol, rtol):
        sp_b = utils.to_coo(b_data)
        sp_ggb = utils.to_coo(b_grad_grad)
        func = F.math.sparse_matmul.CooMatMul(
            sp_b.row, sp_b.col, sp_b.shape, sp_b.order,
            transa=not self.transb, transb=not self.transa, transc=True)

        def op(b, a):
            return func.apply((b, a))[0]
        gradient_check.check_double_backward(
            op, (sp_b.data.data, a_data),
            c_grad, (sp_ggb.data.data, a_grad_grad),
            atol=atol, rtol=rtol, dtype=numpy.float32)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 172:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7370')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/math_tests/test_minimum.py: 43-55
</a>
<div class="mid" id="frag7370" style="display:none"><pre>
    def setUp(self):
        if self.dtype == numpy.float16:
            eps = 1e-2
            self.check_forward_options.update({'atol': 1e-4, 'rtol': 1e-3})
            self.check_backward_options.update({
                'atol': 1e-2, 'rtol': 1e-2})
            self.check_double_backward_options.update({
                'atol': 1e-2, 'rtol': 1e-2})
        else:
            eps = 1e-3
        self.check_backward_options['eps'] = eps
        self.check_double_backward_options['eps'] = eps

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7405')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/math_tests/test_maximum.py: 43-55
</a>
<div class="mid" id="frag7405" style="display:none"><pre>
    def setUp(self):
        if self.dtype == numpy.float16:
            eps = 1e-2
            self.check_forward_options.update({'atol': 1e-4, 'rtol': 1e-3})
            self.check_backward_options.update({
                'atol': 1e-2, 'rtol': 1e-2})
            self.check_double_backward_options.update({
                'atol': 1e-2, 'rtol': 1e-2})
        else:
            eps = 1e-3
        self.check_backward_options['eps'] = eps
        self.check_double_backward_options['eps'] = eps

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 173:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7444')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/math_tests/test_det.py: 263-275
</a>
<div class="mid" id="frag7444" style="display:none"><pre>
    def test_answer_gpu_cpu(self):
        x = cuda.to_gpu(self.x)
        y = F.det(chainer.Variable(x))
        gpu = cuda.to_cpu(y.data)
        if self.dtype == numpy.float16:
            cpu = numpy.linalg.det(
                self.x.astype(numpy.float32)).astype(numpy.float16)
            testing.assert_allclose(gpu, cpu, atol=5e-3, rtol=5e-3)
        else:
            cpu = numpy.linalg.det(self.x)
            testing.assert_allclose(gpu, cpu)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7446')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/math_tests/test_det.py: 287-299
</a>
<div class="mid" id="frag7446" style="display:none"><pre>
    def test_answer_gpu_cpu(self):
        x = cuda.to_gpu(self.x)
        y = F.batch_det(chainer.Variable(x))
        gpu = cuda.to_cpu(y.data)
        if self.dtype == numpy.float16:
            cpu = numpy.linalg.det(
                self.x.astype(numpy.float32)).astype(numpy.float16)
            testing.assert_allclose(gpu, cpu, atol=5e-3, rtol=5e-3)
        else:
            cpu = numpy.linalg.det(self.x)
            testing.assert_allclose(gpu, cpu)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 174:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7468')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/math_tests/test_inv.py: 52-65
</a>
<div class="mid" id="frag7468" style="display:none"><pre>
    def setUp(self):
        if self.dtype == numpy.float16:
            self.check_forward_dtype = numpy.float32
            self.check_forward_options.update({'atol': 1e-3, 'rtol': 1e-3})
            self.check_backward_options.update({'atol': 1e-3, 'rtol': 1e-3})
            self.check_double_backward_options.update({
                'atol': 5e-3, 'rtol': 5e-3})
        else:
            self.check_forward_dtype = self.dtype
            self.check_forward_options.update({'atol': 1e-4, 'rtol': 1e-4})
            self.check_backward_options.update({'atol': 5e-4, 'rtol': 5e-4})
            self.check_double_backward_options.update({
                'atol': 5e-4, 'rtol': 5e-4})

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7473')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/math_tests/test_inv.py: 112-125
</a>
<div class="mid" id="frag7473" style="display:none"><pre>
    def setUp(self):
        if self.dtype == numpy.float16:
            self.check_forward_dtype = numpy.float32
            self.check_forward_options.update({'atol': 1e-3, 'rtol': 1e-3})
            self.check_backward_options.update({'atol': 2e-3, 'rtol': 2e-3})
            self.check_double_backward_options.update({
                'atol': 5e-3, 'rtol': 5e-3})
        else:
            self.check_forward_dtype = self.dtype
            self.check_forward_options.update({'atol': 1e-4, 'rtol': 1e-4})
            self.check_backward_options.update({'atol': 5e-4, 'rtol': 5e-4})
            self.check_double_backward_options.update({
                'atol': 1e-3, 'rtol': 1e-3})

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 175:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 85%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7770')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/array_tests/test_copy.py: 160-175
</a>
<div class="mid" id="frag7770" style="display:none"><pre>
    def test_forward_int(self, src_backend_config, dst_backend_config):
        assert dst_backend_config.xp is not chainerx
        src_device = src_backend_config.device
        dst_device = dst_backend_config.device
        if dst_device.xp is numpy:
            dst_device_spec = -1
        elif dst_device.xp is chainer.backends.cuda.cupy:
            dst_device_spec = dst_device.device.id
        else:
            assert False, dst_device

        self.check_forward(
            dst_device_spec,
            src_device,
            dst_device)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7771')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/array_tests/test_copy.py: 176-192
</a>
<div class="mid" id="frag7771" style="display:none"><pre>
    def test_forward_str(self, src_backend_config, dst_backend_config):
        assert dst_backend_config.xp is not chainerx
        src_device = src_backend_config.device
        dst_device = dst_backend_config.device
        if dst_device.xp is numpy:
            dst_device_spec = '@numpy'
        elif dst_device.xp is chainer.backends.cuda.cupy:
            dst_device_spec = '@cupy:{}'.format(dst_device.device.id)
        else:
            assert False, dst_device

        self.check_forward(
            dst_device_spec,
            src_device,
            dst_device)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 176:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 86%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7788')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/array_tests/test_spatial_transformer_sampler.py: 132-147
</a>
<div class="mid" id="frag7788" style="display:none"><pre>
    def test_consistency_with_cudnn_cpu(self):
        with chainer.using_config('use_cudnn', 'never'):
            x_cpu, grid_cpu, y_cpu = self._apply_backward(
                self.x, self.grid, self.grads)
        with chainer.using_config('use_cudnn', 'always'):
            x_cudnn, grid_cudnn, y_cudnn = self._apply_backward(
                cuda.to_gpu(self.x), cuda.to_gpu(self.grid),
                cuda.to_gpu(self.grads))

        testing.assert_allclose(
            y_cpu.data, y_cudnn.data, **self.assert_options)
        testing.assert_allclose(
            x_cpu.grad, x_cudnn.grad, **self.assert_options)
        testing.assert_allclose(
            grid_cpu.grad, grid_cudnn.grad, **self.assert_options)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7789')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/array_tests/test_spatial_transformer_sampler.py: 150-167
</a>
<div class="mid" id="frag7789" style="display:none"><pre>
    def test_consistency_with_cudnn_gpu(self):
        with chainer.using_config('use_cudnn', 'never'):
            x_gpu, grid_gpu, y_gpu = self._apply_backward(
                cuda.to_gpu(self.x), cuda.to_gpu(self.grid),
                cuda.to_gpu(self.grads))
        with chainer.using_config('use_cudnn', 'always'):
            x_cudnn, grid_cudnn, y_cudnn = self._apply_backward(
                cuda.to_gpu(self.x), cuda.to_gpu(self.grid),
                cuda.to_gpu(self.grads))

        testing.assert_allclose(
            y_gpu.data, y_cudnn.data, **self.assert_options)
        testing.assert_allclose(
            x_gpu.grad, x_cudnn.grad, **self.assert_options)
        testing.assert_allclose(
            grid_gpu.grad, grid_cudnn.grad, **self.assert_options)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 177:</b> &nbsp; 2 fragments, nominal size 35 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag7856')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/array_tests/test_space_2_depth.py: 18-54
</a>
<div class="mid" id="frag7856" style="display:none"><pre>
    def setUp(self):
        self.depth = numpy.arange(96).reshape(2, 8, 3, 2).astype(self.dtype)
        self.space = numpy.array([[[[0.,  12.,   1.,  13.],
                                    [24.,  36.,  25.,  37.],
                                    [2.,  14.,   3.,  15.],
                                    [26.,  38.,  27.,  39.],
                                    [4.,  16.,   5.,  17.],
                                    [28.,  40.,  29.,  41.]],
                                   [[6.,  18.,   7.,  19.],
                                    [30.,  42.,  31.,  43.],
                                    [8.,  20.,   9.,  21.],
                                    [32.,  44.,  33.,  45.],
                                    [10.,  22.,  11.,  23.],
                                    [34.,  46.,  35.,  47.]]],
                                  [[[48.,  60.,  49.,  61.],
                                    [72.,  84.,  73.,  85.],
                                    [50.,  62.,  51.,  63.],
                                    [74.,  86.,  75.,  87.],
                                    [52.,  64.,  53.,  65.],
                                    [76.,  88.,  77.,  89.]],
                                   [[54.,  66.,  55.,  67.],
                                    [78.,  90.,  79.,  91.],
                                    [56.,  68.,  57.,  69.],
                                    [80.,  92.,  81.,  93.],
                                    [58.,  70.,  59.,  71.],
                                    [82.,  94.,  83.,  95.]]]]
                                 ).astype(self.dtype)
        self.x = numpy.random.randn(2, 2, 6, 4).astype(self.dtype)
        self.gy = numpy.random.randn(2, 8, 3, 2).astype(self.dtype)
        self.ggx = numpy.random.randn(2, 2, 6, 4).astype(self.dtype)
        self.r = 2
        self.check_backward_options = {}
        self.check_double_backward_options = {}
        if self.dtype == numpy.float16:
            self.check_backward_options = {'atol': 5e-4, 'rtol': 5e-3}
            self.check_double_backward_options = {'atol': 5e-3, 'rtol': 5e-2}

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag7897')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/array_tests/test_depth_2_space.py: 18-54
</a>
<div class="mid" id="frag7897" style="display:none"><pre>
    def setUp(self):
        self.depth = numpy.arange(96).reshape(2, 8, 3, 2).astype(self.dtype)
        self.space = numpy.array([[[[0.,  12.,   1.,  13.],
                                    [24.,  36.,  25.,  37.],
                                    [2.,  14.,   3.,  15.],
                                    [26.,  38.,  27.,  39.],
                                    [4.,  16.,   5.,  17.],
                                    [28.,  40.,  29.,  41.]],
                                   [[6.,  18.,   7.,  19.],
                                    [30.,  42.,  31.,  43.],
                                    [8.,  20.,   9.,  21.],
                                    [32.,  44.,  33.,  45.],
                                    [10.,  22.,  11.,  23.],
                                    [34.,  46.,  35.,  47.]]],
                                  [[[48.,  60.,  49.,  61.],
                                    [72.,  84.,  73.,  85.],
                                    [50.,  62.,  51.,  63.],
                                    [74.,  86.,  75.,  87.],
                                    [52.,  64.,  53.,  65.],
                                    [76.,  88.,  77.,  89.]],
                                   [[54.,  66.,  55.,  67.],
                                    [78.,  90.,  79.,  91.],
                                    [56.,  68.,  57.,  69.],
                                    [80.,  92.,  81.,  93.],
                                    [58.,  70.,  59.,  71.],
                                    [82.,  94.,  83.,  95.]]]]
                                 ).astype(self.dtype)
        self.x = numpy.random.randn(2, 8, 3, 2).astype(self.dtype)
        self.gy = numpy.random.randn(2, 2, 6, 4).astype(self.dtype)
        self.ggx = numpy.random.randn(2, 8, 3, 2).astype(self.dtype)
        self.r = 2
        self.check_backward_options = {}
        self.check_double_backward_options = {}
        if self.dtype == numpy.float16:
            self.check_backward_options = {'atol': 5e-4, 'rtol': 5e-3}
            self.check_double_backward_options = {'atol': 5e-3, 'rtol': 5e-2}

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 178:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag8201')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/loss_tests/test_softmax_cross_entropy.py: 400-416
</a>
<div class="mid" id="frag8201" style="display:none"><pre>
    def check_backward(self, xp):
        x = xp.asarray(self.x)
        t = xp.asarray(self.t)
        gy = xp.asarray(self.gy)
        if self.class_weight is not None:
            class_weight = xp.asarray(self.class_weight)
        else:
            class_weight = None

        def f(x_, t_):
            return functions.softmax_cross_entropy(
                x_, t_, class_weight=class_weight, reduce=self.reduce,
                ignore_label=self.ignore_label,
                enable_double_backprop=self.enable_double_backprop)

        gradient_check.check_backward(f, (x, t), gy)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8205')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/loss_tests/test_softmax_cross_entropy.py: 424-441
</a>
<div class="mid" id="frag8205" style="display:none"><pre>
    def check_double_backward(self, xp):
        x = xp.asarray(self.x)
        t = xp.asarray(self.t)
        gy = xp.asarray(self.gy)
        ggx = xp.asarray(self.ggx)
        if self.class_weight is not None:
            class_weight = xp.asarray(self.class_weight)
        else:
            class_weight = None

        def f(x_):
            return functions.softmax_cross_entropy(
                x_, t, class_weight=class_weight, reduce=self.reduce,
                ignore_label=self.ignore_label,
                enable_double_backprop=True)

        gradient_check.check_double_backward(f, x, gy, ggx)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 179:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag8249')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/loss_tests/test_negative_sampling.py: 138-152
</a>
<div class="mid" id="frag8249" style="display:none"><pre>
    def test_backward(self, backend_config):
        sampler = make_sampler(backend_config, self.label_size)
        x_data = backend_config.get_array(self.x)
        t_data = backend_config.get_array(self.t)
        w_data = backend_config.get_array(self.w)
        y_grad = backend_config.get_array(self.gy)

        def f(x, w):
            return functions.negative_sampling(
                x, t_data, w, sampler, self.sample_size, reduce=self.reduce)

        with backend_config:
            gradient_check.check_backward(
                f, (x_data, w_data), y_grad, **self.check_backward_options)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8251')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/loss_tests/test_negative_sampling.py: 153-171
</a>
<div class="mid" id="frag8251" style="display:none"><pre>
    def test_double_backward(self, backend_config):
        sampler = make_sampler(backend_config, self.label_size)
        x_data = backend_config.get_array(self.x)
        t_data = backend_config.get_array(self.t)
        w_data = backend_config.get_array(self.w)
        y_grad = backend_config.get_array(self.gy)
        x_grad_grad = backend_config.get_array(self.ggx)
        w_grad_grad = backend_config.get_array(self.ggw)

        def f(x, w):
            return functions.negative_sampling(
                x, t_data, w, sampler, self.sample_size, reduce=self.reduce)

        with backend_config:
            gradient_check.check_double_backward(
                f, (x_data, w_data), y_grad, (x_grad_grad, w_grad_grad),
                **self.check_double_backward_options)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 180:</b> &nbsp; 6 fragments, nominal size 13 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag8279')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/connection_tests/test_convolution_2d.py: 124-144
</a>
<div class="mid" id="frag8279" style="display:none"><pre>
    def forward_expected(self, inputs):
        """
        Current forward_expected implementation depends on
        F.convolution_2d itself and thus it's only capable
        of checking consistency between backends, not absolute
        correctness of computations
        """
        if self.nobias:
            x, W = inputs
            b = None
        else:
            x, W, b = inputs
        with chainer.using_config('use_ideep', 'never'):
            y_expected = F.convolution_2d(
                x, W, b, stride=self.stride, pad=self.pad,
                cover_all=self.cover_all, dilate=self.dilate,
                groups=self.groups)
        if self.old_numpy_fp16:
            return y_expected.array*0,
        return y_expected.array,

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8280')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/connection_tests/test_convolution_2d.py: 145-158
</a>
<div class="mid" id="frag8280" style="display:none"><pre>
    def forward(self, inputs, device):
        if self.nobias:
            x, W = inputs
            b = None
        else:
            x, W, b = inputs
        out = F.convolution_2d(
            x, W, b, stride=self.stride, pad=self.pad,
            cover_all=self.cover_all, dilate=self.dilate,
            groups=self.groups)
        if self.old_numpy_fp16:
            return out*0,
        return out,

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8360')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/connection_tests/test_deconvolution_nd.py: 129-146
</a>
<div class="mid" id="frag8360" style="display:none"><pre>
    def forward_expected(self, inputs):
        """
        Current forward_expected implementation depends on
        F.deconvolution_nd itself and thus it's only capable
        of checking consistency between backends, not absolute
        correctness of computations
        """
        if self.nobias:
            x, W = inputs
            b = None
        else:
            x, W, b = inputs
        y_expected = F.deconvolution_nd(
            x, W, b, stride=self.stride, pad=self.pad,
            outsize=self.outsize, dilate=self.dilate,
            groups=self.groups)
        return y_expected.array,

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag8321')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/connection_tests/test_convolution_nd.py: 110-127
</a>
<div class="mid" id="frag8321" style="display:none"><pre>
    def forward_expected(self, inputs):
        """
        Current forward_expected implementation depends on
        F.convolution_nd itself and thus it's only capable
        of checking consistency between backends, not absolute
        correctness of computations
        """
        if self.nobias:
            x, W = inputs
            b = None
        else:
            x, W, b = inputs
        y_expected = F.convolution_nd(
            x, W, b, stride=self.stride, pad=self.pad,
            cover_all=self.cover_all, dilate=self.dilate,
            groups=self.groups)
        return y_expected.array,

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8322')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/connection_tests/test_convolution_nd.py: 128-139
</a>
<div class="mid" id="frag8322" style="display:none"><pre>
    def forward(self, inputs, device):
        if self.nobias:
            x, W = inputs
            b = None
        else:
            x, W, b = inputs
        y = F.convolution_nd(
            x, W, b, stride=self.stride, pad=self.pad,
            cover_all=self.cover_all, dilate=self.dilate,
            groups=self.groups)
        return y,

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8361')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/connection_tests/test_deconvolution_nd.py: 147-158
</a>
<div class="mid" id="frag8361" style="display:none"><pre>
    def forward(self, inputs, device):
        if self.nobias:
            x, W = inputs
            b = None
        else:
            x, W, b = inputs
        y = F.deconvolution_nd(
            x, W, b, stride=self.stride, pad=self.pad,
            outsize=self.outsize, dilate=self.dilate,
            groups=self.groups)
        return y,

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 181:</b> &nbsp; 3 fragments, nominal size 11 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag8291')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/connection_tests/test_convolution_2d.py: 321-333
</a>
<div class="mid" id="frag8291" style="display:none"><pre>
    def test_1(self):
        n_batches = 2
        in_channels = 3
        out_channels = 1  # important
        x_shape = (n_batches, in_channels, 10, 10)
        w_shape = (out_channels, in_channels, 3, 3)
        x = numpy.ones(x_shape, numpy.float32)
        w = numpy.ones(w_shape, numpy.float32)
        y = F.convolution_2d(x, chainer.Variable(w))
        z = F.sum(y)
        z.backward()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8333')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/connection_tests/test_convolution_nd.py: 263-274
</a>
<div class="mid" id="frag8333" style="display:none"><pre>
    def test_1(self):
        n_batches = 2
        in_channels = 3
        out_channels = 1  # important
        x_shape = (n_batches, in_channels, 4)
        w_shape = (out_channels, in_channels, 3)
        x = numpy.ones(x_shape, numpy.float32)
        w = numpy.ones(w_shape, numpy.float32)
        y = F.convolution_nd(chainer.Variable(x), w)
        z = F.sum(y)
        z.backward()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8334')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/connection_tests/test_convolution_nd.py: 275-287
</a>
<div class="mid" id="frag8334" style="display:none"><pre>
    def test_2(self):
        n_batches = 2
        in_channels = 3
        out_channels = 1  # important
        x_shape = (n_batches, in_channels, 4)
        w_shape = (out_channels, in_channels, 3)
        x = numpy.ones(x_shape, numpy.float32)
        w = numpy.ones(w_shape, numpy.float32)
        y = F.convolution_nd(x, chainer.Variable(w))
        z = F.sum(y)
        z.backward()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 182:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag8320')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/connection_tests/test_convolution_nd.py: 99-109
</a>
<div class="mid" id="frag8320" style="display:none"><pre>
    def generate_inputs(self):
        W = numpy.random.normal(
            0, self.W_scale, self.W_shape).astype(self.W_dtype)
        x = numpy.random.uniform(-1, 1, self.x_shape).astype(self.x_dtype)
        if self.nobias:
            return x, W
        else:
            b = numpy.random.uniform(
                -1, 1, self.out_channels).astype(self.x_dtype)
            return x, W, b

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8359')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/connection_tests/test_deconvolution_nd.py: 118-128
</a>
<div class="mid" id="frag8359" style="display:none"><pre>
    def generate_inputs(self):
        W = numpy.random.normal(
            0, self.W_scale, self.W_shape).astype(self.W_dtype)
        x = numpy.random.uniform(-1, 1, self.x_shape).astype(self.x_dtype)
        if self.nobias:
            return x, W
        else:
            b = numpy.random.uniform(
                -1, 1, self.out_channels).astype(self.x_dtype)
            return x, W, b

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 183:</b> &nbsp; 2 fragments, nominal size 22 lines, similarity 77%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag8323')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/connection_tests/test_convolution_nd.py: 140-164
</a>
<div class="mid" id="frag8323" style="display:none"><pre>
    def check_forward_consistency_regression(self, backend_config):
        inputs = self.generate_inputs()
        if self.nobias:
            x, W = inputs
            b = None
        else:
            x, W, b = inputs
        x = chainer.Variable(backend_config.get_array(x))
        W = chainer.Variable(backend_config.get_array(W))
        if b is not None:
            b = chainer.Variable(backend_config.get_array(b))

        with chainer.using_config('use_cudnn', 'never'):
            y_nd = F.convolution_nd(
                x, W, b, stride=self.stride, pad=self.pad,
                cover_all=self.cover_all, dilate=self.dilate,
                groups=self.groups)
            y_2d = F.convolution_2d(
                x, W, b, stride=self.stride, pad=self.pad,
                cover_all=self.cover_all, dilate=self.dilate,
                groups=self.groups)

        testing.assert_allclose(
            y_nd.array, y_2d.array, **self.check_forward_options)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8362')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/connection_tests/test_deconvolution_nd.py: 159-183
</a>
<div class="mid" id="frag8362" style="display:none"><pre>
    def check_forward_consistency_regression(self, backend_config):
        inputs = self.generate_inputs()
        if self.nobias:
            x, W = inputs
            b = None
        else:
            x, W, b = inputs
        x = chainer.Variable(backend_config.get_array(x))
        W = chainer.Variable(backend_config.get_array(W))
        if b is not None:
            b = chainer.Variable(backend_config.get_array(b))

        use_cudnn = backend_config.use_cudnn

        with chainer.using_config('use_cudnn', use_cudnn):
            y_nd = F.deconvolution_nd(x, W, b, stride=self.stride,
                                      pad=self.pad, outsize=self.outsize,
                                      dilate=self.dilate)
            y_2d = F.deconvolution_2d(x, W, b, stride=self.stride,
                                      pad=self.pad, outsize=self.outsize,
                                      dilate=self.dilate)

        testing.assert_allclose(
            y_nd.array, y_2d.array, **self.check_forward_options)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 184:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag8329')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/connection_tests/test_convolution_nd.py: 223-234
</a>
<div class="mid" id="frag8329" style="display:none"><pre>
    def setUp(self):
        N = 2
        in_channels = 3
        out_channels = 2
        dtype = numpy.float32

        x_shape = (N, in_channels, 3, 3, 3)
        self.x_data = numpy.random.uniform(-1, 1, x_shape).astype(dtype)
        W_shape = (out_channels, in_channels, 1, 1, 1)
        self.W_data = numpy.random.uniform(-1, 1, W_shape).astype(dtype)
        self.b_data = numpy.random.uniform(-1, 1, out_channels).astype(dtype)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8368')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/connection_tests/test_deconvolution_nd.py: 243-254
</a>
<div class="mid" id="frag8368" style="display:none"><pre>
    def setUp(self):
        N = 2
        in_channels = 3
        out_channels = 2
        dtype = numpy.float32

        x_shape = (N, in_channels, 3, 3, 3)
        self.x_data = numpy.random.uniform(-1, 1, x_shape).astype(dtype)
        W_shape = (in_channels, out_channels, 1, 1, 1)
        self.W_data = numpy.random.uniform(-1, 1, W_shape).astype(dtype)
        self.b_data = numpy.random.uniform(-1, 1, out_channels).astype(dtype)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 185:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag8335')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/connection_tests/test_convolution_nd.py: 290-302
</a>
<div class="mid" id="frag8335" style="display:none"><pre>
    def _get_data(self, ndim):
        in_channels = 3
        out_channels = 2
        dtype = numpy.float32

        x_shape = (2, in_channels) + (3,) * ndim
        x = numpy.random.uniform(-1, 1, x_shape).astype(dtype)
        W_shape = (out_channels, in_channels) + (1,) * ndim
        W = numpy.random.uniform(-1, 1, W_shape).astype(dtype)
        b = numpy.random.uniform(-1, 1, out_channels).astype(dtype)

        return x, W, b

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8377')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/connection_tests/test_deconvolution_nd.py: 349-361
</a>
<div class="mid" id="frag8377" style="display:none"><pre>
    def _get_data(self, ndim):
        in_channels = 3
        out_channels = 2
        dtype = numpy.float32

        x_shape = (2, in_channels) + (3,) * ndim
        x = numpy.random.uniform(-1, 1, x_shape).astype(dtype)
        W_shape = (in_channels, out_channels) + (1,) * ndim
        W = numpy.random.uniform(-1, 1, W_shape).astype(dtype)
        b = numpy.random.uniform(-1, 1, out_channels).astype(dtype)

        return x, W, b

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 186:</b> &nbsp; 2 fragments, nominal size 19 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag8340')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/connection_tests/test_deformable_convolution_2d_sampler.py: 28-52
</a>
<div class="mid" id="frag8340" style="display:none"><pre>
    def setUp(self):
        in_channels = 3
        out_channels = 2
        batch_size = 2
        h = 9
        w = 9

        kh, kw, sy, sx, ph, pw = self.params

        self.stride = (sy, sx)
        self.pad = (ph, pw)

        self.W = numpy.random.normal(
            size=(out_channels, in_channels, kh, kw)).astype(numpy.float32)
        self.b = numpy.random.uniform(
            size=(out_channels,)).astype(numpy.float32)

        self.x = numpy.random.uniform(
            size=(batch_size, in_channels, h, w)).astype(numpy.float32)

        out_h = utils.conv.get_conv_outsize(h, kh, sy, ph)
        out_w = utils.conv.get_conv_outsize(w, kw, sx, pw)
        self.offset = numpy.zeros(
            (batch_size, 2 * kh * kw, out_h, out_w), dtype=numpy.float32)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8344')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/connection_tests/test_deformable_convolution_2d_sampler.py: 91-115
</a>
<div class="mid" id="frag8344" style="display:none"><pre>
    def setUp(self):
        in_channels = 3
        out_channels = 2
        batch_size = 2
        h = 9
        w = 9

        kh, kw, sy, sx, ph, pw = self.params

        self.stride = (sy, sx)
        self.pad = (ph, pw)

        self.W = numpy.random.normal(
            size=(out_channels, in_channels, kh, kw)).astype(numpy.float32)
        self.b = numpy.random.uniform(
            size=(out_channels,)).astype(numpy.float32)

        self.x = numpy.random.uniform(
            size=(batch_size, in_channels, h, w)).astype(numpy.float32)

        out_h = utils.conv.get_conv_outsize(h, kh, sy, ph)
        out_w = utils.conv.get_conv_outsize(w, kw, sx, pw)
        self.offset = numpy.zeros(
            (batch_size, 2 * kh * kw, out_h, out_w), dtype=numpy.float32)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 187:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 88%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag8421')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_unpooling_2d.py: 36-54
</a>
<div class="mid" id="frag8421" style="display:none"><pre>
    def setUp(self):
        self.N = 2
        self.n_channels = 3
        inh, inw = 2, 1
        self.x = pooling_nd_helper.shuffled_linspace(
            (self.N, self.n_channels, inh, inw), self.dtype)

        self.ksize = 2
        outh, outw = self.outsize or self.expected_outsize
        self.gy = numpy.random.uniform(
            -1, 1, (self.N, self.n_channels, outh, outw)).astype(self.dtype)
        self.check_backward_options = {'atol': 1e-4, 'rtol': 1e-3}
        self.check_double_backward_options = {}
        if self.dtype == numpy.float16:
            self.check_backward_options = {'atol': 2e-3, 'rtol': 2e-2}
            self.check_double_backward_options = {'atol': 3e-3, 'rtol': 3e-2}
        self.ggx = numpy.random.uniform(
            -1, 1, self.x.shape).astype(self.dtype)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8435')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_unpooling_2d.py: 156-173
</a>
<div class="mid" id="frag8435" style="display:none"><pre>
    def setUp(self):
        self.N = 2
        self.n_channels = 3
        inh, inw = self.insize
        self.x = pooling_nd_helper.shuffled_linspace(
            (self.N, self.n_channels, inh, inw), self.dtype)

        outh, outw = self.outsize or self.expected_outsize
        self.gy = numpy.random.uniform(
            -1, 1, (self.N, self.n_channels, outh, outw)).astype(self.dtype)
        self.check_backward_options = {'atol': 1e-4, 'rtol': 1e-3}
        self.check_double_backward_options = {}
        if self.dtype == numpy.float16:
            self.check_backward_options = {'atol': 2e-3, 'rtol': 2e-2}
            self.check_double_backward_options = {'atol': 3e-3, 'rtol': 3e-2}
        self.ggx = numpy.random.uniform(
            -1, 1, self.x.shape).astype(self.dtype)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 188:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag8449')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_unpooling_2d.py: 277-289
</a>
<div class="mid" id="frag8449" style="display:none"><pre>
    def check_left_inverse(self, xp, use_cudnn='never'):
        x = xp.arange(self.h * self.h).reshape(
            (1, 1, self.h, self.h)).astype(self.dtype)
        with chainer.using_config('use_cudnn', use_cudnn):
            y = chainer.functions.unpooling_2d(
                x, self.k, self.s, self.p, None, self.cover_all)
            x_ = chainer.functions.max_pooling_2d(
                y, self.k, self.s, self.p, self.cover_all).data

        self.assertEqual(x.shape, x_.shape)
        self.assertEqual(x.dtype, x_.dtype)
        chainer.testing.assert_allclose(x, x_)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8453')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_unpooling_2d.py: 311-325
</a>
<div class="mid" id="frag8453" style="display:none"><pre>
    def check_left_inverse(self, xp, use_cudnn='never'):
        x = xp.arange(self.h * self.h).reshape(
            (1, 1, self.h, self.h)).astype(self.dtype)
        with chainer.using_config('use_cudnn', use_cudnn):
            # average_pooling_2d does not have cover_all option
            # as max_pooling_2d has.
            y = chainer.functions.unpooling_2d(
                x, self.k, self.s, self.p, None, False)
            x_ = chainer.functions.average_pooling_2d(
                y, self.k, self.s, self.p).data

        self.assertEqual(x.shape, x_.shape)
        self.assertEqual(x.dtype, x_.dtype)
        chainer.testing.assert_allclose(x, x_)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 189:</b> &nbsp; 3 fragments, nominal size 13 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag8461')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_roi_pooling_2d.py: 61-76
</a>
<div class="mid" id="frag8461" style="display:none"><pre>
    def test_forward_cpu_gpu_equal(self):
        # cpu
        x_cpu = chainer.Variable(self.x)
        rois_cpu = chainer.Variable(self.rois)
        y_cpu = functions.roi_pooling_2d(
            x_cpu, rois_cpu, outh=self.outh, outw=self.outw,
            spatial_scale=self.spatial_scale)

        # gpu
        x_gpu = chainer.Variable(cuda.to_gpu(self.x))
        rois_gpu = chainer.Variable(cuda.to_gpu(self.rois))
        y_gpu = functions.roi_pooling_2d(
            x_gpu, rois_gpu, outh=self.outh, outw=self.outw,
            spatial_scale=self.spatial_scale)
        testing.assert_allclose(y_cpu.data, cuda.to_cpu(y_gpu.data))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8471')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_roi_max_pooling_2d.py: 75-92
</a>
<div class="mid" id="frag8471" style="display:none"><pre>
    def test_forward_cpu_gpu_equal(self):
        # cpu
        x_cpu = chainer.Variable(self.x)
        rois_cpu = chainer.Variable(self.rois)
        roi_indices_cpu = chainer.Variable(self.roi_indices)
        y_cpu = functions.roi_max_pooling_2d(
            x_cpu, rois_cpu, roi_indices_cpu, outsize=self.outsize,
            spatial_scale=self.spatial_scale)

        # gpu
        x_gpu = chainer.Variable(cuda.to_gpu(self.x))
        rois_gpu = chainer.Variable(cuda.to_gpu(self.rois))
        roi_indices_gpu = chainer.Variable(cuda.to_gpu(self.roi_indices))
        y_gpu = functions.roi_max_pooling_2d(
            x_gpu, rois_gpu, roi_indices_gpu, outsize=self.outsize,
            spatial_scale=self.spatial_scale)
        testing.assert_allclose(y_cpu.data, cuda.to_cpu(y_gpu.data))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8494')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_roi_average_pooling_2d.py: 75-92
</a>
<div class="mid" id="frag8494" style="display:none"><pre>
    def test_forward_cpu_gpu_equal(self):
        # cpu
        x_cpu = chainer.Variable(self.x)
        rois_cpu = chainer.Variable(self.rois)
        roi_indices_cpu = chainer.Variable(self.roi_indices)
        y_cpu = functions.roi_average_pooling_2d(
            x_cpu, rois_cpu, roi_indices_cpu, outsize=self.outsize,
            spatial_scale=self.spatial_scale)

        # gpu
        x_gpu = chainer.Variable(cuda.to_gpu(self.x))
        rois_gpu = chainer.Variable(cuda.to_gpu(self.rois))
        roi_indices_gpu = chainer.Variable(cuda.to_gpu(self.roi_indices))
        y_gpu = functions.roi_average_pooling_2d(
            x_gpu, rois_gpu, roi_indices_gpu, outsize=self.outsize,
            spatial_scale=self.spatial_scale)
        testing.assert_allclose(y_cpu.data, cuda.to_cpu(y_gpu.data))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 190:</b> &nbsp; 4 fragments, nominal size 20 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag8467')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_roi_max_pooling_2d.py: 30-52
</a>
<div class="mid" id="frag8467" style="display:none"><pre>
    def setUp(self):
        N = 3
        n_channels = 3
        self.x = pooling_nd_helper.shuffled_linspace(
            (N, n_channels, 12, 8), self.dtype)
        self.rois = numpy.array([
            [1, 1, 7, 7],
            [2, 6, 12, 8],
            [1, 3, 11, 6],
            [3, 3, 4, 4]
        ], dtype=self.dtype)
        self.roi_indices = numpy.array([0, 2, 1, 0], dtype=numpy.int32)
        n_rois = self.rois.shape[0]
        outsize = _pair(self.outsize)
        self.gy = numpy.random.uniform(
            -1, 1, (n_rois, n_channels,
                    outsize[0], outsize[1])).astype(self.dtype)
        if self.dtype == numpy.float16:
            self.check_backward_options = {
                'dtype': numpy.float64, 'atol': 1e-2, 'rtol': 1e-2}
        else:
            self.check_backward_options = {'atol': 1e-3, 'rtol': 1e-2}

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8490')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_roi_average_pooling_2d.py: 30-52
</a>
<div class="mid" id="frag8490" style="display:none"><pre>
    def setUp(self):
        N = 3
        n_channels = 3
        self.x = pooling_nd_helper.shuffled_linspace(
            (N, n_channels, 12, 8), self.dtype)
        self.rois = numpy.array([
            [1, 1, 7, 7],
            [2, 6, 12, 8],
            [1, 3, 11, 6],
            [3, 3, 4, 4]
        ], dtype=self.dtype)
        self.roi_indices = numpy.array([0, 2, 1, 0], dtype=numpy.int32)
        n_rois = self.rois.shape[0]
        outsize = _pair(self.outsize)
        self.gy = numpy.random.uniform(
            -1, 1, (n_rois, n_channels,
                    outsize[0], outsize[1])).astype(self.dtype)
        if self.dtype == numpy.float16:
            self.check_backward_options = {
                'dtype': numpy.float64, 'atol': 1e-2, 'rtol': 1e-2}
        else:
            self.check_backward_options = {'atol': 1e-3, 'rtol': 1e-2}

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8522')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_roi_average_align_2d.py: 36-55
</a>
<div class="mid" id="frag8522" style="display:none"><pre>
    def setUp(self):
        N = 3
        n_channels = 3
        self.x = pooling_nd_helper.shuffled_linspace(
            (N, n_channels, 12, 8), numpy.float32)
        self.rois = numpy.array([
            [1, 1, 6, 6],
            [2, 6, 11, 7],
            [1, 3, 10, 5],
            [3, 3, 3, 3],
            [1.1, 2.2, 3.3, 4.4],
        ], dtype=numpy.float32)
        self.roi_indices = numpy.array([0, 2, 1, 0, 2], dtype=numpy.int32)
        n_rois = self.rois.shape[0]
        outsize = _pair(self.outsize)
        self.gy = numpy.random.uniform(
            -1, 1, (n_rois, n_channels,
                    outsize[0], outsize[1])).astype(numpy.float32)
        self.check_backward_options = {'atol': 5e-4, 'rtol': 5e-3}

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8532')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_roi_max_align_2d.py: 36-55
</a>
<div class="mid" id="frag8532" style="display:none"><pre>
    def setUp(self):
        N = 3
        n_channels = 3
        self.x = pooling_nd_helper.shuffled_linspace(
            (N, n_channels, 12, 8), numpy.float32)
        self.rois = numpy.array([
            [1, 1, 6, 6],
            [6, 2, 7, 11],
            [3, 1, 5, 10],
            [3, 3, 3, 3],
            [1.1, 2.2, 3.3, 4.4],
        ], dtype=numpy.float32)
        self.roi_indices = numpy.array([0, 2, 1, 0, 2], dtype=numpy.int32)
        n_rois = self.rois.shape[0]
        outsize = _pair(self.outsize)
        self.gy = numpy.random.uniform(
            -1, 1, (n_rois, n_channels,
                    outsize[0], outsize[1])).astype(numpy.float32)
        self.check_backward_options = {'atol': 5e-4, 'rtol': 5e-3}

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 191:</b> &nbsp; 4 fragments, nominal size 10 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag8468')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_roi_max_pooling_2d.py: 53-64
</a>
<div class="mid" id="frag8468" style="display:none"><pre>
    def check_forward(self, x_data, roi_data, roi_index_data):
        x = chainer.Variable(x_data)
        rois = chainer.Variable(roi_data)
        roi_indices = chainer.Variable(roi_index_data)
        y = functions.roi_max_pooling_2d(
            x, rois, roi_indices, outsize=self.outsize,
            spatial_scale=self.spatial_scale)
        self.assertEqual(y.data.dtype, self.dtype)
        y_data = cuda.to_cpu(y.data)

        self.assertEqual(self.gy.shape, y_data.shape)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8533')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_roi_max_align_2d.py: 56-69
</a>
<div class="mid" id="frag8533" style="display:none"><pre>
    def check_forward(self, x_data, roi_data, roi_index_data):
        x = chainer.Variable(x_data)
        rois = chainer.Variable(roi_data)
        roi_indices = chainer.Variable(roi_index_data)
        y = functions.roi_max_align_2d(
            x, rois, roi_indices, outsize=self.outsize,
            spatial_scale=self.spatial_scale,
            sampling_ratio=self.sampling_ratio,
        )
        self.assertEqual(y.data.dtype, numpy.float32)
        y_data = cuda.to_cpu(y.data)

        self.assertEqual(self.gy.shape, y_data.shape)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8491')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_roi_average_pooling_2d.py: 53-64
</a>
<div class="mid" id="frag8491" style="display:none"><pre>
    def check_forward(self, x_data, roi_data, roi_index_data):
        x = chainer.Variable(x_data)
        rois = chainer.Variable(roi_data)
        roi_indices = chainer.Variable(roi_index_data)
        y = functions.roi_average_pooling_2d(
            x, rois, roi_indices, outsize=self.outsize,
            spatial_scale=self.spatial_scale)
        self.assertEqual(y.data.dtype, self.dtype)
        y_data = cuda.to_cpu(y.data)

        self.assertEqual(self.gy.shape, y_data.shape)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8523')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_roi_average_align_2d.py: 56-69
</a>
<div class="mid" id="frag8523" style="display:none"><pre>
    def check_forward(self, x_data, roi_data, roi_index_data):
        x = chainer.Variable(x_data)
        rois = chainer.Variable(roi_data)
        roi_indices = chainer.Variable(roi_index_data)
        y = functions.roi_average_align_2d(
            x, rois, roi_indices, outsize=self.outsize,
            spatial_scale=self.spatial_scale,
            sampling_ratio=self.sampling_ratio,
        )
        self.assertEqual(y.data.dtype, numpy.float32)
        y_data = cuda.to_cpu(y.data)

        self.assertEqual(self.gy.shape, y_data.shape)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 192:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 84%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag8472')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_roi_max_pooling_2d.py: 93-107
</a>
<div class="mid" id="frag8472" style="display:none"><pre>
    def check_backward(self, x_data, roi_data, roi_index_data, y_grad):
        def f(x, rois, roi_indices):
            y = functions.roi_max_pooling_2d(
                x, rois, roi_indices, outsize=self.outsize,
                spatial_scale=self.spatial_scale)
            xp = cuda.get_array_module(y)
            # replace -inf with zero for gradient_check
            y = functions.where(
                xp.isinf(y.array), xp.zeros(y.shape, dtype=y.dtype), y)
            return y

        gradient_check.check_backward(
            f, (x_data, roi_data, roi_index_data), y_grad,
            no_grads=[False, True, True], **self.check_backward_options)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8537')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_roi_max_align_2d.py: 105-119
</a>
<div class="mid" id="frag8537" style="display:none"><pre>
    def check_backward(self, x_data, roi_data, roi_index_data, y_grad):
        def f(x, rois, roi_indices):
            y = functions.roi_max_align_2d(
                x, rois, roi_indices, outsize=self.outsize,
                spatial_scale=self.spatial_scale,
                sampling_ratio=self.sampling_ratio)
            xp = chainer.backend.get_array_module(y)
            y = functions.where(
                xp.isinf(y.array), xp.zeros(y.shape, dtype=y.dtype), y)
            return y

        gradient_check.check_backward(
            f, (x_data, roi_data, roi_index_data), y_grad,
            no_grads=[False, True, True], **self.check_backward_options)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 193:</b> &nbsp; 2 fragments, nominal size 20 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag8486')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_max_pooling_2d.py: 140-162
</a>
<div class="mid" id="frag8486" style="display:none"><pre>
    def _check(self, x):
        out, indices = functions.max_pooling_2d(
            x, 2, cover_all=False, return_indices=True)
        assert isinstance(out, chainer.Variable)
        assert isinstance(out.array, type(x))
        assert isinstance(indices, type(x))
        assert indices.shape == out.array.shape

        # Calculate expected indices.
        expect = numpy.zeros(indices.shape, dtype=indices.dtype)
        for i in six.moves.range(2):
            for c in six.moves.range(3):
                xx = x[i, c]
                expect[i, c] = numpy.array([
                    [xx[0:2, 0:2].ravel().argmax(),
                     xx[0:2, 2:4].ravel().argmax()],
                    [xx[2:4, 0:2].ravel().argmax(),
                     xx[2:4, 2:4].ravel().argmax()],
                ])
        if out.xp is cuda.cupy:
            expect = cuda.to_gpu(expect)
        assert (expect == indices).all()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8585')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_max_pooling_nd.py: 187-209
</a>
<div class="mid" id="frag8585" style="display:none"><pre>
    def _check(self, x):
        out, indices = functions.max_pooling_nd(
            x, 2, cover_all=False, return_indices=True)
        assert isinstance(out, chainer.Variable)
        assert isinstance(out.array, type(x))
        assert isinstance(indices, type(x))
        assert indices.shape == out.array.shape

        # Calculate expected indices.
        expect = numpy.zeros(indices.shape, dtype=indices.dtype)
        for i in six.moves.range(2):
            for c in six.moves.range(3):
                xx = x[i, c]
                expect[i, c] = numpy.array([
                    [xx[0:2, 0:2].ravel().argmax(),
                     xx[0:2, 2:4].ravel().argmax()],
                    [xx[2:4, 0:2].ravel().argmax(),
                     xx[2:4, 2:4].ravel().argmax()],
                ])
        if out.xp is cuda.cupy:
            expect = cuda.to_gpu(expect)
        assert (expect == indices).all()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 194:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 77%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag8526')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_roi_average_align_2d.py: 83-104
</a>
<div class="mid" id="frag8526" style="display:none"><pre>
    def test_forward_cpu_gpu_equal(self):
        # cpu
        x_cpu = chainer.Variable(self.x)
        rois_cpu = chainer.Variable(self.rois)
        roi_indices_cpu = chainer.Variable(self.roi_indices)
        y_cpu = functions.roi_average_align_2d(
            x_cpu, rois_cpu, roi_indices_cpu, outsize=self.outsize,
            spatial_scale=self.spatial_scale,
            sampling_ratio=self.sampling_ratio,
        )

        # gpu
        x_gpu = chainer.Variable(cuda.to_gpu(self.x))
        rois_gpu = chainer.Variable(cuda.to_gpu(self.rois))
        roi_indices_gpu = chainer.Variable(cuda.to_gpu(self.roi_indices))
        y_gpu = functions.roi_average_align_2d(
            x_gpu, rois_gpu, roi_indices_gpu, outsize=self.outsize,
            spatial_scale=self.spatial_scale,
            sampling_ratio=self.sampling_ratio,
        )
        testing.assert_allclose(y_cpu.data, cuda.to_cpu(y_gpu.data))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8536')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_roi_max_align_2d.py: 83-104
</a>
<div class="mid" id="frag8536" style="display:none"><pre>
    def test_forward_cpu_gpu_equal(self):
        # cpu
        x_cpu = chainer.Variable(self.x)
        rois_cpu = chainer.Variable(self.rois)
        roi_index_cpu = chainer.Variable(self.roi_indices)
        y_cpu = functions.roi_max_align_2d(
            x_cpu, rois_cpu, roi_index_cpu,
            outsize=self.outsize, spatial_scale=self.spatial_scale,
            sampling_ratio=self.sampling_ratio,
        )

        # gpu
        x_gpu = chainer.Variable(cuda.to_gpu(self.x))
        rois_gpu = chainer.Variable(cuda.to_gpu(self.rois))
        roi_index_gpu = chainer.Variable(cuda.to_gpu(self.roi_indices))
        y_gpu = functions.roi_max_align_2d(
            x_gpu, rois_gpu, roi_index_gpu,
            outsize=self.outsize, spatial_scale=self.spatial_scale,
            sampling_ratio=self.sampling_ratio,
        )
        testing.assert_allclose(y_cpu.data, cuda.to_cpu(y_gpu.data))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 195:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 91%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag8559')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_average_pooling_nd.py: 195-208
</a>
<div class="mid" id="frag8559" style="display:none"><pre>
    def setUp(self):
        self.ndim = len(self.dims)
        self.ksize = (3,) * self.ndim
        self.stride = (2,) * self.ndim
        self.pad = (1,) * self.ndim
        x_shape = (2, 3) + self.dims
        self.x = cuda.cupy.arange(functools.reduce(operator.mul, x_shape),
                                  dtype=self.dtype).reshape(x_shape)
        gy_shape = (2, 3) + tuple(
            conv.get_conv_outsize(d, k, s, p)
            for (d, k, s, p)
            in six.moves.zip(self.dims, self.ksize, self.stride, self.pad))
        self.gy = cuda.cupy.random.uniform(-1, 1, gy_shape).astype(self.dtype)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8575')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_max_pooling_nd.py: 110-123
</a>
<div class="mid" id="frag8575" style="display:none"><pre>
    def setUp(self):
        self.ndim = len(self.dims)
        self.ksize = (3,) * self.ndim
        self.stride = (2,) * self.ndim
        self.pad = (1,) * self.ndim
        x_shape = (2, 3) + self.dims
        self.x = cuda.cupy.arange(functools.reduce(mul, x_shape),
                                  dtype=self.dtype).reshape(x_shape)
        gy_shape = (2, 3) + tuple(
            conv.get_conv_outsize(d, k, s, p)
            for (d, k, s, p)
            in six.moves.zip(self.dims, self.ksize, self.stride, self.pad))
        self.gy = cuda.cupy.random.uniform(-1, 1, gy_shape).astype(self.dtype)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 196:</b> &nbsp; 6 fragments, nominal size 10 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag8889')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/link_hooks_tests/test_weight_standardization.py: 135-146
</a>
<div class="mid" id="frag8889" style="display:none"><pre>
    def setUp(self):
        self.in_channels, self.out_channels = 3, 10
        in_channels = None if self.lazy_init else self.in_channels
        conv_init_args = {'ksize': 3, 'stride': 1, 'pad': 1}
        self.layer = self.link(
            in_channels, self.out_channels, **conv_init_args)
        self.x = numpy.random.normal(
            size=(5, self.in_channels, 4)).astype(numpy.float32)
        self.hook = WeightStandardization()
        self.out_size = self.out_channels  # For compatibility


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8891')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/link_hooks_tests/test_weight_standardization.py: 171-182
</a>
<div class="mid" id="frag8891" style="display:none"><pre>
    def setUp(self):
        self.in_channels, self.out_channels = 3, 10
        in_channels = None if self.lazy_init else self.in_channels
        conv_init_args = {'ksize': 3, 'stride': 1, 'pad': 1}
        self.layer = self.link(
            in_channels, self.out_channels, **conv_init_args)
        self.x = numpy.random.normal(
            size=(5, self.in_channels, 4, 4, 4)).astype(numpy.float32)
        self.hook = WeightStandardization()
        self.out_size = self.out_channels  # For compatibility


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8918')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/link_hooks_tests/test_spectral_normalization.py: 365-376
</a>
<div class="mid" id="frag8918" style="display:none"><pre>
    def setUp(self):
        self.in_channels, self.out_channels = 3, 10
        in_channels = None if self.lazy_init else self.in_channels
        conv_init_args = {'ksize': 3, 'stride': 1, 'pad': 1}
        self.layer = self.link(
            in_channels, self.out_channels, **conv_init_args)
        self.x = numpy.random.normal(
            size=(5, self.in_channels, 4, 4, 4)).astype(numpy.float32)
        self.hook = SpectralNormalization(use_gamma=self.use_gamma)
        self.out_size = self.out_channels  # For compatibility


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag8917')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/link_hooks_tests/test_spectral_normalization.py: 345-356
</a>
<div class="mid" id="frag8917" style="display:none"><pre>
    def setUp(self):
        self.in_channels, self.out_channels = 3, 10
        in_channels = None if self.lazy_init else self.in_channels
        conv_init_args = {'ksize': 3, 'stride': 1, 'pad': 1}
        self.layer = self.link(
            in_channels, self.out_channels, **conv_init_args)
        self.x = numpy.random.normal(
            size=(5, self.in_channels, 4, 4)).astype(numpy.float32)
        self.hook = SpectralNormalization(use_gamma=self.use_gamma)
        self.out_size = self.out_channels  # For compatibility


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8916')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/link_hooks_tests/test_spectral_normalization.py: 325-336
</a>
<div class="mid" id="frag8916" style="display:none"><pre>
    def setUp(self):
        self.in_channels, self.out_channels = 3, 10
        in_channels = None if self.lazy_init else self.in_channels
        conv_init_args = {'ksize': 3, 'stride': 1, 'pad': 1}
        self.layer = self.link(
            in_channels, self.out_channels, **conv_init_args)
        self.x = numpy.random.normal(
            size=(5, self.in_channels, 4)).astype(numpy.float32)
        self.hook = SpectralNormalization(use_gamma=self.use_gamma)
        self.out_size = self.out_channels  # For compatibility


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8890')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/link_hooks_tests/test_weight_standardization.py: 153-164
</a>
<div class="mid" id="frag8890" style="display:none"><pre>
    def setUp(self):
        self.in_channels, self.out_channels = 3, 10
        in_channels = None if self.lazy_init else self.in_channels
        conv_init_args = {'ksize': 3, 'stride': 1, 'pad': 1}
        self.layer = self.link(
            in_channels, self.out_channels, **conv_init_args)
        self.x = numpy.random.normal(
            size=(5, self.in_channels, 4, 4)).astype(numpy.float32)
        self.hook = WeightStandardization()
        self.out_size = self.out_channels  # For compatibility


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 197:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 88%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag8896')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/link_hooks_tests/test_spectral_normalization.py: 43-59
</a>
<div class="mid" id="frag8896" style="display:none"><pre>
    def test_add_sn_hook(self):
        layer, hook = self.layer, self.hook
        layer.add_hook(hook)
        if self.lazy_init:
            assert not hasattr(layer, hook.vector_name)
            if self.use_gamma:
                assert not hasattr(layer, 'gamma')
            with chainer.using_config('train', False):
                layer(self.x)
        assert hasattr(layer, hook.vector_name)
        assert (self.out_size,) == getattr(layer, hook.vector_name).shape
        if not self.use_gamma:
            assert not hasattr(layer, 'gamma')
        else:  # Use gamma parameter
            assert hasattr(layer, 'gamma')
            assert layer.gamma.ndim == 0 and layer.gamma.size == 1

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8914')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/link_hooks_tests/test_spectral_normalization.py: 281-299
</a>
<div class="mid" id="frag8914" style="display:none"><pre>
    def test_add_sn_hook(self):
        hook = SpectralNormalization(use_gamma=self.use_gamma)
        layer = self.layer
        layer.add_hook(hook)
        if self.lazy_init:
            assert not hasattr(layer, hook.vector_name)
            if self.use_gamma:
                assert not hasattr(layer, 'gamma')
            with chainer.using_config('train', False):
                layer(self.x)
        assert hasattr(layer, hook.vector_name)
        assert (self.in_size,) == getattr(layer, hook.vector_name).shape
        if not self.use_gamma:
            assert not hasattr(layer, 'gamma')
        else:  # Use gamma parameter
            assert hasattr(layer, 'gamma')
            assert layer.gamma.ndim == 0 and layer.gamma.size == 1


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 198:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 88%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag8900')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/link_hooks_tests/test_spectral_normalization.py: 82-101
</a>
<div class="mid" id="frag8900" style="display:none"><pre>
    def check_in_recomputing(self, backend_config):
        layer, hook = self.layer, self.hook
        layer.add_hook(hook)
        layer.to_device(backend_config.device)
        x = backend_config.get_array(self.x)

        y1 = layer(x).array
        u1 = getattr(layer, hook.vector_name).copy()
        v1 = hook.v.copy()
        with chainer.using_config('in_recomputing', True):
            y2 = layer(x).array
        u2 = getattr(layer, hook.vector_name)
        v2 = hook.v

        u1, u2 = _cpu._to_cpu(u1), _cpu._to_cpu(u2)
        v1, v2 = _cpu._to_cpu(v1), _cpu._to_cpu(v2)
        numpy.testing.assert_array_equal(u1, u2)
        numpy.testing.assert_array_equal(v1, v2)
        testing.assert_allclose(y1, y2)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8906')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/link_hooks_tests/test_spectral_normalization.py: 145-164
</a>
<div class="mid" id="frag8906" style="display:none"><pre>
    def check_u_not_updated_in_test(self, backend_config):
        layer, hook = self.layer, self.hook
        layer.add_hook(hook)
        layer.to_device(backend_config.device)
        x = backend_config.get_array(self.x)

        with chainer.using_config('train', False):
            y1 = layer(x).array
            u1 = getattr(layer, hook.vector_name).copy()
            v1 = hook.v.copy()
            y2 = layer(x).array
            u2 = getattr(layer, hook.vector_name)
            v2 = hook.v.copy()

        u1, u2 = _cpu._to_cpu(u1), _cpu._to_cpu(u2)
        v1, v2 = _cpu._to_cpu(v1), _cpu._to_cpu(v2)
        numpy.testing.assert_array_equal(u1, u2)
        numpy.testing.assert_array_equal(v1, v2)
        testing.assert_allclose(y1, y2)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 199:</b> &nbsp; 6 fragments, nominal size 29 lines, similarity 86%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag8920')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multiprocess_iterator.py: 39-68
</a>
<div class="mid" id="frag8920" style="display:none"><pre>
    def test_iterator_repeat(self):
        dataset = [1, 2, 3, 4, 5, 6]
        it = iterators.MultiprocessIterator(dataset, 2, **self.options)
        for i in range(3):
            self.assertEqual(it.epoch, i)
            self.assertAlmostEqual(it.epoch_detail, i + 0 / 6)
            if i == 0:
                self.assertIsNone(it.previous_epoch_detail)
            else:
                self.assertAlmostEqual(it.previous_epoch_detail, i - 2 / 6)
            batch1 = it.next()
            self.assertEqual(len(batch1), 2)
            self.assertIsInstance(batch1, list)
            self.assertFalse(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 2 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 0 / 6)
            batch2 = it.next()
            self.assertEqual(len(batch2), 2)
            self.assertIsInstance(batch2, list)
            self.assertFalse(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 4 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 2 / 6)
            batch3 = it.next()
            self.assertEqual(len(batch3), 2)
            self.assertIsInstance(batch3, list)
            self.assertTrue(it.is_new_epoch)
            self.assertEqual(sorted(batch1 + batch2 + batch3), dataset)
            self.assertAlmostEqual(it.epoch_detail, i + 6 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 4 / 6)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8995')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/iterators_tests/test_serial_iterator.py: 114-141
</a>
<div class="mid" id="frag8995" style="display:none"><pre>
    def test_iterator_repeat(self):
        dataset = [1, 2, 3, 4, 5, 6]
        it = iterators.SerialIterator(dataset, 2, shuffle=self.shuffle,
                                      order_sampler=self.order_sampler)
        for i in range(3):
            self.assertEqual(it.epoch, i)
            self.assertAlmostEqual(it.epoch_detail, i + 0 / 6)
            if i == 0:
                self.assertIsNone(it.previous_epoch_detail)
            else:
                self.assertAlmostEqual(it.previous_epoch_detail, i - 2 / 6)
            batch1 = it.next()
            self.assertEqual(len(batch1), 2)
            self.assertFalse(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 2 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 0 / 6)
            batch2 = it.next()
            self.assertEqual(len(batch2), 2)
            self.assertFalse(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 4 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 2 / 6)
            batch3 = it.next()
            self.assertEqual(len(batch3), 2)
            self.assertTrue(it.is_new_epoch)
            self.assertEqual(sorted(batch1 + batch2 + batch3), dataset)
            self.assertAlmostEqual(it.epoch_detail, i + 6 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 4 / 6)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8943')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multiprocess_iterator.py: 523-555
</a>
<div class="mid" id="frag8943" style="display:none"><pre>
    def test_iterator_repeat(self):
        dataset = [1, 2, 3]
        it = iterators.MultiprocessIterator(dataset, 2, **self.options)
        for i in range(3):
            self.assertEqual(it.epoch, i)
            self.assertAlmostEqual(it.epoch_detail, i + 0 / 6)
            if i == 0:
                self.assertIsNone(it.previous_epoch_detail)
            else:
                self.assertAlmostEqual(it.previous_epoch_detail, i - 2 / 6)
            batch1 = it.next()
            self.assertEqual(len(batch1), 2)
            self.assertIsInstance(batch1, list)
            self.assertFalse(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 2 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 0 / 6)
            batch2 = it.next()
            self.assertEqual(len(batch2), 2)
            self.assertIsInstance(batch2, list)
            self.assertFalse(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 4 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 2 / 6)
            batch3 = it.next()
            self.assertEqual(len(batch3), 2)
            self.assertIsInstance(batch3, list)
            self.assertTrue(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 6 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 4 / 6)

            self.assertEqual(
                sorted(batch1 + batch2 + batch3), [1, 1, 2, 2, 3, 3])


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag8966')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multithread_iterator.py: 24-53
</a>
<div class="mid" id="frag8966" style="display:none"><pre>
    def test_iterator_repeat(self):
        dataset = [1, 2, 3, 4, 5, 6]
        it = iterators.MultithreadIterator(dataset, 2, **self.options)
        for i in range(3):
            self.assertEqual(it.epoch, i)
            self.assertAlmostEqual(it.epoch_detail, i + 0 / 6)
            if i == 0:
                self.assertIsNone(it.previous_epoch_detail)
            else:
                self.assertAlmostEqual(it.previous_epoch_detail, i - 2 / 6)
            batch1 = it.next()
            self.assertEqual(len(batch1), 2)
            self.assertIsInstance(batch1, list)
            self.assertFalse(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 2 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 0 / 6)
            batch2 = it.next()
            self.assertEqual(len(batch2), 2)
            self.assertIsInstance(batch2, list)
            self.assertFalse(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 4 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 2 / 6)
            batch3 = it.next()
            self.assertEqual(len(batch3), 2)
            self.assertIsInstance(batch3, list)
            self.assertTrue(it.is_new_epoch)
            self.assertEqual(sorted(batch1 + batch2 + batch3), dataset)
            self.assertAlmostEqual(it.epoch_detail, i + 6 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 4 / 6)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9006')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/iterators_tests/test_serial_iterator.py: 306-338
</a>
<div class="mid" id="frag9006" style="display:none"><pre>
    def test_iterator_repeat(self):
        dataset = [1, 2, 3]
        it = iterators.SerialIterator(dataset, 2, **self.options)
        for i in range(3):
            self.assertEqual(it.epoch, i)
            self.assertAlmostEqual(it.epoch_detail, i + 0 / 6)
            if i == 0:
                self.assertIsNone(it.previous_epoch_detail)
            else:
                self.assertAlmostEqual(it.previous_epoch_detail, i - 2 / 6)
            batch1 = it.next()
            self.assertEqual(len(batch1), 2)
            self.assertIsInstance(batch1, list)
            self.assertFalse(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 2 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 0 / 6)
            batch2 = it.next()
            self.assertEqual(len(batch2), 2)
            self.assertIsInstance(batch2, list)
            self.assertFalse(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 4 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 2 / 6)
            batch3 = it.next()
            self.assertEqual(len(batch3), 2)
            self.assertIsInstance(batch3, list)
            self.assertTrue(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 6 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 4 / 6)

            self.assertEqual(
                sorted(batch1 + batch2 + batch3), [1, 1, 2, 2, 3, 3])


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8984')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multithread_iterator.py: 310-342
</a>
<div class="mid" id="frag8984" style="display:none"><pre>
    def test_iterator_repeat(self):
        dataset = [1, 2, 3]
        it = iterators.MultithreadIterator(dataset, 2, **self.options)
        for i in range(3):
            self.assertEqual(it.epoch, i)
            self.assertAlmostEqual(it.epoch_detail, i + 0 / 6)
            if i == 0:
                self.assertIsNone(it.previous_epoch_detail)
            else:
                self.assertAlmostEqual(it.previous_epoch_detail, i - 2 / 6)
            batch1 = it.next()
            self.assertEqual(len(batch1), 2)
            self.assertIsInstance(batch1, list)
            self.assertFalse(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 2 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 0 / 6)
            batch2 = it.next()
            self.assertEqual(len(batch2), 2)
            self.assertIsInstance(batch2, list)
            self.assertFalse(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 4 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 2 / 6)
            batch3 = it.next()
            self.assertEqual(len(batch3), 2)
            self.assertIsInstance(batch3, list)
            self.assertTrue(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 6 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 4 / 6)

            self.assertEqual(
                sorted(batch1 + batch2 + batch3), [1, 1, 2, 2, 3, 3])


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 200:</b> &nbsp; 6 fragments, nominal size 29 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag8921')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multiprocess_iterator.py: 72-102
</a>
<div class="mid" id="frag8921" style="display:none"><pre>
    def test_iterator_list_type(self):
        dataset = [[i, numpy.zeros((10,)) + i] for i in range(6)]
        it = iterators.MultiprocessIterator(dataset, 2, **self.options)
        for i in range(3):
            self.assertEqual(it.epoch, i)
            self.assertAlmostEqual(it.epoch_detail, i)
            if i == 0:
                self.assertIsNone(it.previous_epoch_detail)
            else:
                self.assertAlmostEqual(it.previous_epoch_detail, i - 2 / 6)
            batches = {}
            for j in range(3):
                batch = it.next()
                self.assertEqual(len(batch), 2)
                if j != 2:
                    self.assertFalse(it.is_new_epoch)
                else:
                    self.assertTrue(it.is_new_epoch)
                self.assertAlmostEqual(
                    it.epoch_detail, (3 * i + j + 1) * 2 / 6)
                self.assertAlmostEqual(
                    it.previous_epoch_detail, (3 * i + j) * 2 / 6)
                for x in batch:
                    self.assertIsInstance(x, list)
                    self.assertIsInstance(x[1], numpy.ndarray)
                    batches[x[0]] = x[1]

            self.assertEqual(len(batches), len(dataset))
            for k, v in six.iteritems(batches):
                numpy.testing.assert_allclose(dataset[k][1], v)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8922')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multiprocess_iterator.py: 106-136
</a>
<div class="mid" id="frag8922" style="display:none"><pre>
    def test_iterator_tuple_type(self):
        dataset = [(i, numpy.zeros((10,)) + i) for i in range(6)]
        it = iterators.MultiprocessIterator(dataset, 2, **self.options)
        for i in range(3):
            self.assertEqual(it.epoch, i)
            self.assertAlmostEqual(it.epoch_detail, i)
            if i == 0:
                self.assertIsNone(it.previous_epoch_detail)
            else:
                self.assertAlmostEqual(it.previous_epoch_detail, i - 2 / 6)
            batches = {}
            for j in range(3):
                batch = it.next()
                self.assertEqual(len(batch), 2)
                if j != 2:
                    self.assertFalse(it.is_new_epoch)
                else:
                    self.assertTrue(it.is_new_epoch)
                self.assertAlmostEqual(
                    it.epoch_detail, (3 * i + j + 1) * 2 / 6)
                self.assertAlmostEqual(
                    it.previous_epoch_detail, (3 * i + j) * 2 / 6)
                for x in batch:
                    self.assertIsInstance(x, tuple)
                    self.assertIsInstance(x[1], numpy.ndarray)
                    batches[x[0]] = x[1]

            self.assertEqual(len(batches), len(dataset))
            for k, v in six.iteritems(batches):
                numpy.testing.assert_allclose(dataset[k][1], v)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8923')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multiprocess_iterator.py: 140-173
</a>
<div class="mid" id="frag8923" style="display:none"><pre>
    def test_iterator_dict_type(self):
        dataset = [{i: numpy.zeros((10,)) + i} for i in range(6)]
        it = iterators.MultiprocessIterator(dataset, 2, **self.options)
        for i in range(3):
            self.assertEqual(it.epoch, i)
            self.assertAlmostEqual(it.epoch_detail, i)
            if i == 0:
                self.assertIsNone(it.previous_epoch_detail)
            else:
                self.assertAlmostEqual(it.previous_epoch_detail, i - 2 / 6)
            batches = {}
            for j in range(3):
                batch = it.next()
                self.assertEqual(len(batch), 2)
                if j != 2:
                    self.assertFalse(it.is_new_epoch)
                else:
                    self.assertTrue(it.is_new_epoch)
                self.assertAlmostEqual(
                    it.epoch_detail, (3 * i + j + 1) * 2 / 6)
                self.assertAlmostEqual(
                    it.previous_epoch_detail, (3 * i + j) * 2 / 6)
                for x in batch:
                    self.assertIsInstance(x, dict)
                    k = tuple(x)[0]
                    v = x[k]
                    self.assertIsInstance(v, numpy.ndarray)
                    batches[k] = v

            self.assertEqual(len(batches), len(dataset))
            for k, v in six.iteritems(batches):
                x = dataset[k][tuple(dataset[k])[0]]
                numpy.testing.assert_allclose(x, v)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag8969')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multithread_iterator.py: 116-149
</a>
<div class="mid" id="frag8969" style="display:none"><pre>
    def test_iterator_dict_type(self):
        dataset = [{i: numpy.zeros((10,)) + i} for i in range(6)]
        it = iterators.MultithreadIterator(dataset, 2, **self.options)
        for i in range(3):
            self.assertEqual(it.epoch, i)
            self.assertAlmostEqual(it.epoch_detail, i)
            if i == 0:
                self.assertIsNone(it.previous_epoch_detail)
            else:
                self.assertAlmostEqual(it.previous_epoch_detail, i - 2 / 6)
            batches = {}
            for j in range(3):
                batch = it.next()
                self.assertEqual(len(batch), 2)
                if j != 2:
                    self.assertFalse(it.is_new_epoch)
                else:
                    self.assertTrue(it.is_new_epoch)
                self.assertAlmostEqual(
                    it.epoch_detail, (3 * i + j + 1) * 2 / 6)
                self.assertAlmostEqual(
                    it.previous_epoch_detail, (3 * i + j) * 2 / 6)
                for x in batch:
                    self.assertIsInstance(x, dict)
                    k = tuple(x)[0]
                    v = x[k]
                    self.assertIsInstance(v, numpy.ndarray)
                    batches[k] = v

            self.assertEqual(len(batches), len(dataset))
            for k, v in six.iteritems(batches):
                x = dataset[k][tuple(dataset[k])[0]]
                numpy.testing.assert_allclose(x, v)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8967')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multithread_iterator.py: 54-84
</a>
<div class="mid" id="frag8967" style="display:none"><pre>
    def test_iterator_list_type(self):
        dataset = [[i, numpy.zeros((10,)) + i] for i in range(6)]
        it = iterators.MultithreadIterator(dataset, 2, **self.options)
        for i in range(3):
            self.assertEqual(it.epoch, i)
            self.assertAlmostEqual(it.epoch_detail, i)
            if i == 0:
                self.assertIsNone(it.previous_epoch_detail)
            else:
                self.assertAlmostEqual(it.previous_epoch_detail, i - 2 / 6)
            batches = {}
            for j in range(3):
                batch = it.next()
                self.assertEqual(len(batch), 2)
                if j != 2:
                    self.assertFalse(it.is_new_epoch)
                else:
                    self.assertTrue(it.is_new_epoch)
                self.assertAlmostEqual(
                    it.epoch_detail, (3 * i + j + 1) * 2 / 6)
                self.assertAlmostEqual(
                    it.previous_epoch_detail, (3 * i + j) * 2 / 6)
                for x in batch:
                    self.assertIsInstance(x, list)
                    self.assertIsInstance(x[1], numpy.ndarray)
                    batches[x[0]] = x[1]

            self.assertEqual(len(batches), len(dataset))
            for k, v in six.iteritems(batches):
                numpy.testing.assert_allclose(dataset[k][1], v)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8968')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multithread_iterator.py: 85-115
</a>
<div class="mid" id="frag8968" style="display:none"><pre>
    def test_iterator_tuple_type(self):
        dataset = [(i, numpy.zeros((10,)) + i) for i in range(6)]
        it = iterators.MultithreadIterator(dataset, 2, **self.options)
        for i in range(3):
            self.assertEqual(it.epoch, i)
            self.assertAlmostEqual(it.epoch_detail, i)
            if i == 0:
                self.assertIsNone(it.previous_epoch_detail)
            else:
                self.assertAlmostEqual(it.previous_epoch_detail, i - 2 / 6)
            batches = {}
            for j in range(3):
                batch = it.next()
                self.assertEqual(len(batch), 2)
                if j != 2:
                    self.assertFalse(it.is_new_epoch)
                else:
                    self.assertTrue(it.is_new_epoch)
                self.assertAlmostEqual(
                    it.epoch_detail, (3 * i + j + 1) * 2 / 6)
                self.assertAlmostEqual(
                    it.previous_epoch_detail, (3 * i + j) * 2 / 6)
                for x in batch:
                    self.assertIsInstance(x, tuple)
                    self.assertIsInstance(x[1], numpy.ndarray)
                    batches[x[0]] = x[1]

            self.assertEqual(len(batches), len(dataset))
            for k, v in six.iteritems(batches):
                numpy.testing.assert_allclose(dataset[k][1], v)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 201:</b> &nbsp; 3 fragments, nominal size 18 lines, similarity 84%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag8926')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multiprocess_iterator.py: 200-220
</a>
<div class="mid" id="frag8926" style="display:none"><pre>
    def test_iterator_not_repeat_not_even(self):
        dataset = [1, 2, 3, 4, 5]
        it = iterators.MultiprocessIterator(
            dataset, 2, repeat=False, **self.options)

        self.assertAlmostEqual(it.epoch_detail, 0 / 5)
        self.assertIsNone(it.previous_epoch_detail)
        batch1 = it.next()
        self.assertAlmostEqual(it.epoch_detail, 2 / 5)
        self.assertAlmostEqual(it.previous_epoch_detail, 0 / 5)
        batch2 = it.next()
        self.assertAlmostEqual(it.epoch_detail, 4 / 5)
        self.assertAlmostEqual(it.previous_epoch_detail, 2 / 5)
        batch3 = it.next()
        self.assertAlmostEqual(it.epoch_detail, 5 / 5)
        self.assertAlmostEqual(it.previous_epoch_detail, 4 / 5)
        self.assertRaises(StopIteration, it.next)

        self.assertEqual(len(batch3), 1)
        self.assertEqual(sorted(batch1 + batch2 + batch3), dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8972')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multithread_iterator.py: 167-187
</a>
<div class="mid" id="frag8972" style="display:none"><pre>
    def test_iterator_not_repeat_not_even(self):
        dataset = [1, 2, 3, 4, 5]
        it = iterators.MultithreadIterator(
            dataset, 2, repeat=False, **self.options)

        self.assertAlmostEqual(it.epoch_detail, 0 / 5)
        self.assertIsNone(it.previous_epoch_detail)
        batch1 = it.next()
        self.assertAlmostEqual(it.epoch_detail, 2 / 5)
        self.assertAlmostEqual(it.previous_epoch_detail, 0 / 5)
        batch2 = it.next()
        self.assertAlmostEqual(it.epoch_detail, 4 / 5)
        self.assertAlmostEqual(it.previous_epoch_detail, 2 / 5)
        batch3 = it.next()
        self.assertAlmostEqual(it.epoch_detail, 5 / 5)
        self.assertAlmostEqual(it.previous_epoch_detail, 4 / 5)
        self.assertRaises(StopIteration, it.next)

        self.assertEqual(len(batch3), 1)
        self.assertEqual(sorted(batch1 + batch2 + batch3), dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8998')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/iterators_tests/test_serial_iterator.py: 162-183
</a>
<div class="mid" id="frag8998" style="display:none"><pre>
    def test_iterator_not_repeat_not_even(self):
        dataset = [1, 2, 3, 4, 5]
        it = iterators.SerialIterator(dataset, 2, repeat=False,
                                      shuffle=self.shuffle,
                                      order_sampler=self.order_sampler)

        self.assertAlmostEqual(it.epoch_detail, 0 / 5)
        self.assertIsNone(it.previous_epoch_detail)
        batch1 = it.next()
        self.assertAlmostEqual(it.epoch_detail, 2 / 5)
        self.assertAlmostEqual(it.previous_epoch_detail, 0 / 5)
        batch2 = it.next()
        self.assertAlmostEqual(it.epoch_detail, 4 / 5)
        self.assertAlmostEqual(it.previous_epoch_detail, 2 / 5)
        batch3 = it.next()
        self.assertAlmostEqual(it.epoch_detail, 5 / 5)
        self.assertAlmostEqual(it.previous_epoch_detail, 4 / 5)
        self.assertRaises(StopIteration, it.next)

        self.assertEqual(len(batch3), 1)
        self.assertEqual(sorted(batch1 + batch2 + batch3), dataset)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 202:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag8929')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multiprocess_iterator.py: 243-258
</a>
<div class="mid" id="frag8929" style="display:none"><pre>
    def test_copy_not_repeat(self):
        dataset = [1, 2, 3, 4, 5]
        it = iterators.MultiprocessIterator(
            dataset, 2, repeat=False, **self.options)
        copy_it = copy.copy(it)
        batches = sum([it.next() for _ in range(3)], [])
        self.assertEqual(sorted(batches), dataset)
        for _ in range(2):
            self.assertRaises(StopIteration, it.next)
        it = None

        batches = sum([copy_it.next() for _ in range(3)], [])
        self.assertEqual(sorted(batches), dataset)
        for _ in range(2):
            self.assertRaises(StopIteration, copy_it.next)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8975')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multithread_iterator.py: 201-216
</a>
<div class="mid" id="frag8975" style="display:none"><pre>
    def test_copy_not_repeat(self):
        dataset = [1, 2, 3, 4, 5]
        it = iterators.MultithreadIterator(
            dataset, 2, repeat=False, **self.options)
        copy_it = copy.copy(it)
        batches = sum([it.next() for _ in range(3)], [])
        self.assertEqual(sorted(batches), dataset)
        for _ in range(2):
            self.assertRaises(StopIteration, it.next)
        it = None

        batches = sum([copy_it.next() for _ in range(3)], [])
        self.assertEqual(sorted(batches), dataset)
        for _ in range(2):
            self.assertRaises(StopIteration, copy_it.next)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 203:</b> &nbsp; 4 fragments, nominal size 10 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag8930')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multiprocess_iterator.py: 262-273
</a>
<div class="mid" id="frag8930" style="display:none"><pre>
    def test_reset(self):
        dataset = [1, 2, 3, 4, 5]
        it = iterators.MultiprocessIterator(
            dataset, 2, repeat=False, **self.options)

        for trial in range(4):
            batches = sum([it.next() for _ in range(3)], [])
            self.assertEqual(sorted(batches), dataset)
            for _ in range(2):
                self.assertRaises(StopIteration, it.next)
            it.reset()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9001')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/iterators_tests/test_serial_iterator.py: 196-209
</a>
<div class="mid" id="frag9001" style="display:none"><pre>
    def test_reset(self):
        dataset = [1, 2, 3, 4, 5]
        it = iterators.SerialIterator(dataset, 2, repeat=False,
                                      shuffle=self.shuffle,
                                      order_sampler=self.order_sampler)

        for trial in range(4):
            batches = sum([it.next() for _ in range(3)], [])
            self.assertEqual(sorted(batches), dataset)
            for _ in range(2):
                self.assertRaises(StopIteration, it.next)
            it.reset()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8976')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multithread_iterator.py: 217-228
</a>
<div class="mid" id="frag8976" style="display:none"><pre>
    def test_reset(self):
        dataset = [1, 2, 3, 4, 5]
        it = iterators.MultithreadIterator(
            dataset, 2, repeat=False, **self.options)

        for trial in range(4):
            batches = sum([it.next() for _ in range(3)], [])
            self.assertEqual(sorted(batches), dataset)
            for _ in range(2):
                self.assertRaises(StopIteration, it.next)
            it.reset()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8931')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multiprocess_iterator.py: 277-290
</a>
<div class="mid" id="frag8931" style="display:none"><pre>
    def test_reset_middle(self):
        dataset = [1, 2, 3, 4, 5]
        it = iterators.MultiprocessIterator(
            dataset, 2, repeat=False, **self.options)

        for trial in range(4):
            it.next()
            it.reset()
            batches = sum([it.next() for _ in range(3)], [])
            self.assertEqual(sorted(batches), dataset)
            for _ in range(2):
                self.assertRaises(StopIteration, it.next)
            it.reset()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 204:</b> &nbsp; 7 fragments, nominal size 31 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag8937')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multiprocess_iterator.py: 377-412
</a>
<div class="mid" id="frag8937" style="display:none"><pre>
    def test_iterator_pickle_after_init(self):
        dataset = [1, 2, 3, 4, 5, 6]
        it = iterators.MultiprocessIterator(dataset, 2, **self.options)

        self.assertEqual(it.epoch, 0)
        self.assertAlmostEqual(it.epoch_detail, 0 / 6)
        self.assertIsNone(it.previous_epoch_detail)
        batch1 = it.next()
        self.assertEqual(len(batch1), 2)
        self.assertIsInstance(batch1, list)
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 2 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 0 / 6)
        batch2 = it.next()
        self.assertEqual(len(batch2), 2)
        self.assertIsInstance(batch2, list)
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 4 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 2 / 6)

        pickled_it = pickle.dumps(it)
        it = pickle.loads(pickled_it)

        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 4 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 2 / 6)

        batch3 = it.next()
        self.assertEqual(len(batch3), 2)
        self.assertIsInstance(batch3, list)
        self.assertTrue(it.is_new_epoch)
        self.assertEqual(sorted(batch1 + batch2 + batch3), dataset)
        self.assertAlmostEqual(it.epoch_detail, 6 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 4 / 6)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8981')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multithread_iterator.py: 265-302
</a>
<div class="mid" id="frag8981" style="display:none"><pre>
    def test_iterator_serialize(self):
        dataset = [1, 2, 3, 4, 5, 6]
        it = iterators.MultithreadIterator(dataset, 2, **self.options)

        self.assertEqual(it.epoch, 0)
        self.assertAlmostEqual(it.epoch_detail, 0 / 6)
        self.assertIsNone(it.previous_epoch_detail)
        batch1 = it.next()
        self.assertEqual(len(batch1), 2)
        self.assertIsInstance(batch1, list)
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 2 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 0 / 6)
        batch2 = it.next()
        self.assertEqual(len(batch2), 2)
        self.assertIsInstance(batch2, list)
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 4 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 2 / 6)

        target = dict()
        it.serialize(serializers.DictionarySerializer(target))

        it = iterators.MultithreadIterator(dataset, 2, **self.options)
        it.serialize(serializers.NpzDeserializer(target))
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 4 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 2 / 6)

        batch3 = it.next()
        self.assertEqual(len(batch3), 2)
        self.assertIsInstance(batch3, list)
        self.assertTrue(it.is_new_epoch)
        self.assertEqual(sorted(batch1 + batch2 + batch3), dataset)
        self.assertAlmostEqual(it.epoch_detail, 6 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 4 / 6)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9002')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/iterators_tests/test_serial_iterator.py: 217-254
</a>
<div class="mid" id="frag9002" style="display:none"><pre>
    def test_iterator_serialize(self):
        dataset = [1, 2, 3, 4, 5, 6]
        it = iterators.SerialIterator(dataset, 2, shuffle=self.shuffle,
                                      order_sampler=self.order_sampler)

        self.assertEqual(it.epoch, 0)
        self.assertAlmostEqual(it.epoch_detail, 0 / 6)
        self.assertIsNone(it.previous_epoch_detail)
        batch1 = it.next()
        self.assertEqual(len(batch1), 2)
        self.assertIsInstance(batch1, list)
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 2 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 0 / 6)
        batch2 = it.next()
        self.assertEqual(len(batch2), 2)
        self.assertIsInstance(batch2, list)
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 4 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 2 / 6)

        target = dict()
        it.serialize(serializers.DictionarySerializer(target))

        it = iterators.SerialIterator(dataset, 2)
        it.serialize(serializers.NpzDeserializer(target))
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 4 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 2 / 6)

        batch3 = it.next()
        self.assertEqual(len(batch3), 2)
        self.assertIsInstance(batch3, list)
        self.assertTrue(it.is_new_epoch)
        self.assertEqual(sorted(batch1 + batch2 + batch3), dataset)
        self.assertAlmostEqual(it.epoch_detail, 6 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 4 / 6)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag8940')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multiprocess_iterator.py: 467-506
</a>
<div class="mid" id="frag8940" style="display:none"><pre>
    def test_iterator_serialize_backward_compat(self):
        dataset = [1, 2, 3, 4, 5, 6]
        it = iterators.MultiprocessIterator(dataset, 2, **self.options)

        self.assertEqual(it.epoch, 0)
        self.assertAlmostEqual(it.epoch_detail, 0 / 6)
        self.assertIsNone(it.previous_epoch_detail)
        batch1 = it.next()
        self.assertEqual(len(batch1), 2)
        self.assertIsInstance(batch1, list)
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 2 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 0 / 6)
        batch2 = it.next()
        self.assertEqual(len(batch2), 2)
        self.assertIsInstance(batch2, list)
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 4 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 2 / 6)

        target = dict()
        it.serialize(serializers.DictionarySerializer(target))
        # older version does not have previous_epoch_detail
        del target['previous_epoch_detail']

        it = iterators.MultiprocessIterator(dataset, 2, **self.options)
        it.serialize(serializers.NpzDeserializer(target))
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 4 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 2 / 6)

        batch3 = it.next()
        self.assertEqual(len(batch3), 2)
        self.assertIsInstance(batch3, list)
        self.assertTrue(it.is_new_epoch)
        self.assertEqual(sorted(batch1 + batch2 + batch3), dataset)
        self.assertAlmostEqual(it.epoch_detail, 6 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 4 / 6)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9003')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/iterators_tests/test_serial_iterator.py: 255-298
</a>
<div class="mid" id="frag9003" style="display:none"><pre>
    def test_iterator_serialize_backward_compat(self):
        dataset = [1, 2, 3, 4, 5, 6]
        it = iterators.SerialIterator(dataset, 2, shuffle=self.shuffle,
                                      order_sampler=self.order_sampler)

        self.assertEqual(it.epoch, 0)
        self.assertAlmostEqual(it.epoch_detail, 0 / 6)
        self.assertIsNone(it.previous_epoch_detail)
        batch1 = it.next()
        self.assertEqual(len(batch1), 2)
        self.assertIsInstance(batch1, list)
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 2 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 0 / 6)
        batch2 = it.next()
        self.assertEqual(len(batch2), 2)
        self.assertIsInstance(batch2, list)
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 4 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 2 / 6)

        target = dict()
        it.serialize(serializers.DictionarySerializer(target))
        # older version uses '_order'
        target['_order'] = target['order']
        del target['order']
        # older version does not have previous_epoch_detail
        del target['previous_epoch_detail']

        it = iterators.SerialIterator(dataset, 2)
        it.serialize(serializers.NpzDeserializer(target))
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 4 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 2 / 6)

        batch3 = it.next()
        self.assertEqual(len(batch3), 2)
        self.assertIsInstance(batch3, list)
        self.assertTrue(it.is_new_epoch)
        self.assertEqual(sorted(batch1 + batch2 + batch3), dataset)
        self.assertAlmostEqual(it.epoch_detail, 6 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 4 / 6)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8939')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multiprocess_iterator.py: 430-466
</a>
<div class="mid" id="frag8939" style="display:none"><pre>
    def test_iterator_serialize(self):
        dataset = [1, 2, 3, 4, 5, 6]
        it = iterators.MultiprocessIterator(dataset, 2, **self.options)

        self.assertEqual(it.epoch, 0)
        self.assertAlmostEqual(it.epoch_detail, 0 / 6)
        self.assertIsNone(it.previous_epoch_detail)
        batch1 = it.next()
        self.assertEqual(len(batch1), 2)
        self.assertIsInstance(batch1, list)
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 2 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 0 / 6)
        batch2 = it.next()
        self.assertEqual(len(batch2), 2)
        self.assertIsInstance(batch2, list)
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 4 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 2 / 6)

        target = dict()
        it.serialize(serializers.DictionarySerializer(target))

        it = iterators.MultiprocessIterator(dataset, 2, **self.options)
        it.serialize(serializers.NpzDeserializer(target))
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 4 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 2 / 6)

        batch3 = it.next()
        self.assertEqual(len(batch3), 2)
        self.assertIsInstance(batch3, list)
        self.assertTrue(it.is_new_epoch)
        self.assertEqual(sorted(batch1 + batch2 + batch3), dataset)
        self.assertAlmostEqual(it.epoch_detail, 6 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 4 / 6)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9014')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/iterators_tests/test_iterator_compatibility.py: 22-61
</a>
<div class="mid" id="frag9014" style="display:none"><pre>
    def test_iterator_compatibilty(self):
        dataset = [1, 2, 3, 4, 5, 6]

        iters = (
            lambda: iterators.SerialIterator(dataset, 2),
            lambda: iterators.MultiprocessIterator(dataset, 2, **self.options),
        )

        for it_before, it_after in itertools.permutations(iters, 2):
            it = it_before()

            self.assertEqual(it.epoch, 0)
            self.assertAlmostEqual(it.epoch_detail, 0 / 6)
            batch1 = it.next()
            self.assertEqual(len(batch1), 2)
            self.assertIsInstance(batch1, list)
            self.assertFalse(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, 2 / 6)
            batch2 = it.next()
            self.assertEqual(len(batch2), 2)
            self.assertIsInstance(batch2, list)
            self.assertFalse(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, 4 / 6)

            target = dict()
            it.serialize(serializers.DictionarySerializer(target))

            it = it_after()
            it.serialize(serializers.NpzDeserializer(target))
            self.assertFalse(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, 4 / 6)

            batch3 = it.next()
            self.assertEqual(len(batch3), 2)
            self.assertIsInstance(batch3, list)
            self.assertTrue(it.is_new_epoch)
            self.assertEqual(sorted(batch1 + batch2 + batch3), dataset)
            self.assertAlmostEqual(it.epoch_detail, 6 / 6)


</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 205:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag8993')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/iterators_tests/test_serial_iterator.py: 67-86
</a>
<div class="mid" id="frag8993" style="display:none"><pre>
    def test_iterator_not_repeat(self):
        dataset = [1, 2, 3, 4, 5, 6]
        it = iterators.SerialIterator(dataset, 2, repeat=False, shuffle=False)

        self.assertAlmostEqual(it.epoch_detail, 0 / 6)
        self.assertIsNone(it.previous_epoch_detail)
        self.assertEqual(it.next(), [1, 2])
        self.assertAlmostEqual(it.epoch_detail, 2 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 0 / 6)
        self.assertEqual(it.next(), [3, 4])
        self.assertAlmostEqual(it.epoch_detail, 4 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 2 / 6)
        self.assertEqual(it.next(), [5, 6])
        self.assertTrue(it.is_new_epoch)
        self.assertEqual(it.epoch, 1)
        self.assertAlmostEqual(it.epoch_detail, 6 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 4 / 6)
        for i in range(2):
            self.assertRaises(StopIteration, it.next)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag8994')" href="javascript:;">
chainer-7.2.0/tests/chainer_tests/iterators_tests/test_serial_iterator.py: 87-106
</a>
<div class="mid" id="frag8994" style="display:none"><pre>
    def test_iterator_not_repeat_not_even(self):
        dataset = [1, 2, 3, 4, 5]
        it = iterators.SerialIterator(dataset, 2, repeat=False, shuffle=False)

        self.assertAlmostEqual(it.epoch_detail, 0 / 5)
        self.assertIsNone(it.previous_epoch_detail)
        self.assertEqual(it.next(), [1, 2])
        self.assertAlmostEqual(it.epoch_detail, 2 / 5)
        self.assertAlmostEqual(it.previous_epoch_detail, 0 / 5)
        self.assertEqual(it.next(), [3, 4])
        self.assertAlmostEqual(it.epoch_detail, 4 / 5)
        self.assertAlmostEqual(it.previous_epoch_detail, 2 / 5)
        self.assertEqual(it.next(), [5])
        self.assertTrue(it.is_new_epoch)
        self.assertEqual(it.epoch, 1)
        self.assertAlmostEqual(it.epoch_detail, 5 / 5)
        self.assertAlmostEqual(it.previous_epoch_detail, 4 / 5)
        self.assertRaises(StopIteration, it.next)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 206:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 88%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9016')" href="javascript:;">
chainer-7.2.0/tests/chainermn_tests/optimizer_tests/test_double_buffering_optimizer.py: 25-42
</a>
<div class="mid" id="frag9016" style="display:none"><pre>
    def setup(self, batched_copy):
        if nccl.get_build_version() &lt; 2000:
            pytest.skip('This test requires NCCL version &gt;= 2.0')
        self.comm = chainermn.create_communicator('pure_nccl',
                                                  batched_copy=batched_copy)
        device = self.comm.intra_rank
        chainer.cuda.get_device_from_id(device).use()
        self.target = ExampleModel()
        self.target.to_device(cupy.cuda.Device())
        self.target.a.W.data[:] = self.comm.rank
        self.target.b.W.data[:] = self.comm.rank + 1
        self.target.c.W.data[:] = self.comm.rank + 2
        self.target.a.W.grad[:] = 0
        self.target.b.W.grad[:] = 0
        self.target.c.W.grad[:] = 0
        self.actual_optimizer = chainer.GradientMethod()
        self.actual_optimizer.create_update_rule = mock.MagicMock

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9021')" href="javascript:;">
chainer-7.2.0/tests/chainermn_tests/optimizer_tests/test_double_buffering_optimizer.py: 112-127
</a>
<div class="mid" id="frag9021" style="display:none"><pre>
    def setup(self, batched_copy):
        if nccl.get_build_version() &lt; 2000:
            pytest.skip('This test requires NCCL version &gt;= 2.0')
        self.comm = chainermn.create_communicator('pure_nccl',
                                                  batched_copy=batched_copy)
        device = self.comm.intra_rank
        chainer.cuda.get_device_from_id(device).use()
        self.target = DynamicExampleModel()
        self.target.to_device(cupy.cuda.Device())
        self.target.a.W.data[:] = self.comm.rank
        self.target.b.W.data[:] = self.comm.rank + 1
        self.target.a.W.grad[:] = 0
        self.target.b.W.grad[:] = 0
        self.actual_optimizer = chainer.GradientMethod()
        self.actual_optimizer.create_update_rule = mock.MagicMock

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 207:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 86%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9027')" href="javascript:;">
chainer-7.2.0/tests/chainermn_tests/optimizer_tests/test_multi_node_optimizer.py: 34-50
</a>
<div class="mid" id="frag9027" style="display:none"><pre>
    def setup_gpu(self, use_chx=False):
        self.comm = chainermn.create_communicator('flat')
        self.target = ExampleModel()
        self.device = chainermn.testing.get_device(self.comm.intra_rank,
                                                   use_chx)
        chainer.cuda.get_device_from_id(self.comm.intra_rank).use()
        self.target.to_device(self.device)

        self.target.a.W.data[:] = self.comm.rank
        self.target.b.W.data[:] = self.comm.rank + 1
        self.target.c.W.data[:] = self.comm.rank + 2
        self.target.a.W.grad[:] = 0
        self.target.b.W.grad[:] = 0
        self.target.c.W.grad[:] = 0
        self.actual_optimizer = chainer.GradientMethod()
        self.actual_optimizer.create_update_rule = mock.MagicMock

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9032')" href="javascript:;">
chainer-7.2.0/tests/chainermn_tests/optimizer_tests/test_multi_node_optimizer.py: 134-148
</a>
<div class="mid" id="frag9032" style="display:none"><pre>
    def setup_gpu(self, use_chx=False):
        self.comm = chainermn.create_communicator('flat')
        self.target = DynamicExampleModel()
        self.device = chainermn.testing.get_device(self.comm.intra_rank,
                                                   use_chx)
        chainer.cuda.get_device_from_id(self.comm.intra_rank).use()
        self.target.to_device(self.device)

        self.target.a.W.data[:] = self.comm.rank
        self.target.b.W.data[:] = self.comm.rank + 1
        self.target.a.W.grad[:] = 0
        self.target.b.W.grad[:] = 0
        self.actual_optimizer = chainer.GradientMethod()
        self.actual_optimizer.create_update_rule = mock.MagicMock

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 208:</b> &nbsp; 4 fragments, nominal size 32 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9028')" href="javascript:;">
chainer-7.2.0/tests/chainermn_tests/optimizer_tests/test_multi_node_optimizer.py: 51-79
</a>
<div class="mid" id="frag9028" style="display:none"><pre>
    def test_update_with_cpu(self):
        self.setup_cpu()
        self.optimizer = chainermn.create_multi_node_optimizer(
            self.actual_optimizer, self.comm)
        opt = self.optimizer.setup(self.target)
        assert opt is self.optimizer
        self.optimizer.update()
        assert self.actual_optimizer.t == 0
        self.optimizer.target.a.W.grad[:] = self.comm.rank
        self.optimizer.target.b.W.grad[:] = self.comm.rank + 1
        self.optimizer.target.c.W.grad[:] = self.comm.rank + 2

        self.optimizer.update()
        assert self.actual_optimizer.t == 1
        self.optimizer.target.a.W.update_rule.update.assert_called_once_with(
            self.optimizer.target.a.W)
        self.optimizer.target.b.W.update_rule.update.assert_called_once_with(
            self.optimizer.target.b.W)
        self.optimizer.target.c.W.update_rule.update.assert_called_once_with(
            self.optimizer.target.c.W)

        base = (self.comm.size - 1.0) / 2
        chainer.testing.assert_allclose(self.optimizer.target.a.W.grad,
                                        (base + 0) * np.ones((3, 2)))
        chainer.testing.assert_allclose(self.optimizer.target.b.W.grad,
                                        (base + 1) * np.ones((4, 3)))
        chainer.testing.assert_allclose(self.optimizer.target.c.W.grad,
                                        (base + 2) * np.ones((5, 4)))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9033')" href="javascript:;">
chainer-7.2.0/tests/chainermn_tests/optimizer_tests/test_multi_node_optimizer.py: 149-190
</a>
<div class="mid" id="frag9033" style="display:none"><pre>
    def test_update_with_cpu(self):
        self.setup_cpu()
        self.optimizer = chainermn.create_multi_node_optimizer(
            self.actual_optimizer, self.comm)
        opt = self.optimizer.setup(self.target)
        assert opt is self.optimizer
        self.optimizer.update()
        assert self.actual_optimizer.t == 0

        with self.target.init_scope():
            self.target.c = chainer.links.Linear(4, 4)
        if self.comm.rank == 0:
            self.target.c.W.data[:] = self.comm.rank + 2
        self.optimizer.setup(self.target)
        self.optimizer.update()
        assert self.actual_optimizer.t == 0

        send_buf = chainer.cuda.to_cpu(self.optimizer.target.c.W.data)
        recv_buf = self.comm.mpi_comm.allgather(send_buf)
        for i in range(1, self.comm.size):
            chainer.testing.assert_allclose(recv_buf[0], recv_buf[i])

        self.optimizer.target.a.W.grad[:] = self.comm.rank
        self.optimizer.target.b.W.grad[:] = self.comm.rank + 1
        self.optimizer.target.c.W.grad[:] = self.comm.rank + 2
        self.optimizer.update()
        assert self.actual_optimizer.t == 1
        self.optimizer.target.a.W.update_rule.update.assert_called_once_with(
            self.optimizer.target.a.W)
        self.optimizer.target.b.W.update_rule.update.assert_called_once_with(
            self.optimizer.target.b.W)
        self.optimizer.target.c.W.update_rule.update.assert_called_once_with(
            self.optimizer.target.c.W)

        base = (self.comm.size - 1.0) / 2
        chainer.testing.assert_allclose(self.optimizer.target.a.W.grad,
                                        (base + 0) * np.ones((3, 2)))
        chainer.testing.assert_allclose(self.optimizer.target.b.W.grad,
                                        (base + 1) * np.ones((4, 3)))
        chainer.testing.assert_allclose(self.optimizer.target.c.W.grad,
                                        (base + 2) * np.ones((4, 4)))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9029')" href="javascript:;">
chainer-7.2.0/tests/chainermn_tests/optimizer_tests/test_multi_node_optimizer.py: 82-112
</a>
<div class="mid" id="frag9029" style="display:none"><pre>
    def test_update_with_gpu(self, use_chx):
        self.setup_gpu(use_chx)
        self.optimizer = chainermn.create_multi_node_optimizer(
            self.actual_optimizer, self.comm)
        opt = self.optimizer.setup(self.target)
        assert opt is self.optimizer
        self.optimizer.update()
        assert self.actual_optimizer.t == 0

        self.optimizer.target.a.W.grad[:] = self.comm.rank
        self.optimizer.target.b.W.grad[:] = self.comm.rank + 1
        self.optimizer.target.c.W.grad[:] = self.comm.rank + 2

        self.optimizer.update()
        assert self.actual_optimizer.t == 1
        self.optimizer.target.a.W.update_rule.update.assert_called_once_with(
            self.optimizer.target.a.W)
        self.optimizer.target.b.W.update_rule.update.assert_called_once_with(
            self.optimizer.target.b.W)
        self.optimizer.target.c.W.update_rule.update.assert_called_once_with(
            self.optimizer.target.c.W)

        base = (self.comm.size - 1.0) / 2
        chainer.testing.assert_allclose(self.optimizer.target.a.W.grad,
                                        (base + 0) * np.ones((3, 2)))
        chainer.testing.assert_allclose(self.optimizer.target.b.W.grad,
                                        (base + 1) * np.ones((4, 3)))
        chainer.testing.assert_allclose(self.optimizer.target.c.W.grad,
                                        (base + 2) * np.ones((5, 4)))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9034')" href="javascript:;">
chainer-7.2.0/tests/chainermn_tests/optimizer_tests/test_multi_node_optimizer.py: 193-235
</a>
<div class="mid" id="frag9034" style="display:none"><pre>
    def test_update_with_gpu(self, use_chx):
        self.setup_gpu(use_chx)
        self.optimizer = chainermn.create_multi_node_optimizer(
            self.actual_optimizer, self.comm)
        opt = self.optimizer.setup(self.target)
        assert opt is self.optimizer
        self.optimizer.update()
        assert self.actual_optimizer.t == 0

        with self.target.init_scope():
            c = chainer.links.Linear(4, 4)
            c.to_device(self.device)
            self.target.c = c
        if self.comm.rank == 0:
            self.target.c.W.data[:] = self.comm.rank + 2
        self.optimizer.setup(self.target)
        self.optimizer.update()
        assert self.actual_optimizer.t == 0

        send_buf = chainer.cuda.to_cpu(self.optimizer.target.c.W.data)
        recv_buf = self.comm.mpi_comm.allgather(send_buf)
        for i in range(1, self.comm.size):
            chainer.testing.assert_allclose(recv_buf[0], recv_buf[i])

        self.optimizer.target.a.W.grad[:] = self.comm.rank
        self.optimizer.target.b.W.grad[:] = self.comm.rank + 1
        self.optimizer.target.c.W.grad[:] = self.comm.rank + 2
        self.optimizer.update()
        assert self.actual_optimizer.t == 1
        self.optimizer.target.a.W.update_rule.update.assert_called_once_with(
            self.optimizer.target.a.W)
        self.optimizer.target.b.W.update_rule.update.assert_called_once_with(
            self.optimizer.target.b.W)
        self.optimizer.target.c.W.update_rule.update.assert_called_once_with(
            self.optimizer.target.c.W)

        base = (self.comm.size - 1.0) / 2
        chainer.testing.assert_allclose(self.optimizer.target.a.W.grad,
                                        (base + 0) * np.ones((3, 2)))
        chainer.testing.assert_allclose(self.optimizer.target.b.W.grad,
                                        (base + 1) * np.ones((4, 3)))
        chainer.testing.assert_allclose(self.optimizer.target.c.W.grad,
                                        (base + 2) * np.ones((4, 4)))
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 209:</b> &nbsp; 2 fragments, nominal size 21 lines, similarity 77%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9070')" href="javascript:;">
chainer-7.2.0/tests/chainermn_tests/links_tests/test_multi_node_chain_list.py: 280-304
</a>
<div class="mid" id="frag9070" style="display:none"><pre>
def check_crossing_model(gpu, param):
    communicator, rank_next, rank_prev = create_communicator(gpu)

    n, d = 100, 10
    X = np.random.randn(n, d).astype(param.dtype)
    Y = (np.random.rand(n) * 2).astype(np.int32)

    with chainer.using_config('dtype', param.dtype):
        if communicator.rank == 0:
            model = L.Classifier(Cross0(
                d, communicator, rank_next, rank_prev))
        else:
            model = L.Classifier(Cross1(
                d, communicator, rank_next, rank_prev))

        if gpu:
            model.to_device(cupy.cuda.Device())
            X = chainer.backends.cuda.to_gpu(X)
            Y = chainer.backends.cuda.to_gpu(Y)

        for i in range(n):
            err = model(X[i:i + 1], Y[i:i + 1])
            err.backward()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9077')" href="javascript:;">
chainer-7.2.0/tests/chainermn_tests/links_tests/test_multi_node_chain_list.py: 371-398
</a>
<div class="mid" id="frag9077" style="display:none"><pre>
def check_twisting_model(gpu, param):
    communicator, rank_next, rank_prev = create_communicator(gpu)

    n, d = 100, 10
    X = np.random.randn(n, d).astype(param.dtype)
    Y = (np.random.rand(n) * 2).astype(np.int32)

    with chainer.using_config('dtype', param.dtype):
        if communicator.rank == 0:
            model = L.Classifier(
                TwistFirst(d, communicator, rank_next))
        elif communicator.rank == communicator.size - 1:
            model = L.Classifier(
                TwistLast(d, communicator, rank_prev))
        else:
            model = L.Classifier(Twist(
                d, communicator, rank_prev, rank_next))

        if gpu:
            model.to_device(cupy.cuda.Device())
            X = chainer.backends.cuda.to_gpu(X)
            Y = chainer.backends.cuda.to_gpu(Y)

        for i in range(n):
            err = model(X[i:i + 1], Y[i:i + 1])
            err.backward()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 210:</b> &nbsp; 2 fragments, nominal size 20 lines, similarity 85%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9086')" href="javascript:;">
chainer-7.2.0/tests/chainermn_tests/links_tests/test_n_step_rnn.py: 65-93
</a>
<div class="mid" id="frag9086" style="display:none"><pre>
def check_homogeneous_rnn(gpu, dtype):
    communicator, rank_prev, rank_next = setup_communicator(gpu=gpu)

    n, n_vocab, l = 100, 8, 10
    # Number of model parameters are same among processes.
    n_hid = 2
    with chainer.using_config('dtype', dtype):
        X = [np.random.randint(
            0, n_vocab, size=np.random.randint(l // 2, l + 1),
            dtype=np.int32)
            for _ in range(n)]
        Y = (np.random.rand(n) * 2).astype(dtype)
        model = Model(
            n_vocab, n_hid, communicator, rank_next,
            rank_prev)

        if gpu:
            model.to_device(cupy.cuda.Device())
            X = [chainer.backends.cuda.to_gpu(x) for x in X]
            Y = chainer.backends.cuda.to_gpu(Y)

        for i in range(n):
            err = model(X[i:i + 1], Y[i:i + 1])
            err.backward()

        # Check if backprop finishes without deadlock.
        assert True


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9089')" href="javascript:;">
chainer-7.2.0/tests/chainermn_tests/links_tests/test_n_step_rnn.py: 105-134
</a>
<div class="mid" id="frag9089" style="display:none"><pre>
def check_heterogeneous_rnn(gpu, dtype):
    communicator, rank_prev, rank_next = setup_communicator(gpu)

    with chainer.using_config('dtype', dtype):
        n, n_vocab, l = 100, 8, 10
        # Number of model parameters are different among processes.
        n_hid = (communicator.rank + 1) * 10

        X = [np.random.randint(
            0, n_vocab, size=np.random.randint(l // 2, l + 1),
            dtype=np.int32)
            for _ in range(n)]
        Y = (np.random.rand(n) * 2).astype(dtype)
        model = Model(
            n_vocab, n_hid, communicator, rank_next,
            rank_prev)

        if gpu:
            model.to_device(cupy.cuda.Device())
            X = [chainer.backends.cuda.to_gpu(x) for x in X]
            Y = chainer.backends.cuda.to_gpu(Y)

        for i in range(n):
            err = model(X[i:i + 1], Y[i:i + 1])
            err.backward()

        # Check if backprop finishes without deadlock.
        assert True


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 211:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 86%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9097')" href="javascript:;">
chainer-7.2.0/tests/chainermn_tests/links_tests/test_create_mnbn_model.py: 45-61
</a>
<div class="mid" id="frag9097" style="display:none"><pre>
    def check_create_mnbn_model_chain(self, use_gpu, use_chx):
        model = BnChain(3)
        mnbn_model = chainermn.links.create_mnbn_model(model,
                                                       self.communicator)
        self.assertTrue(isinstance(mnbn_model.conv,
                                   chainer.links.Convolution2D))
        self.assertTrue(
            isinstance(mnbn_model.bn,
                       chainermn.links.MultiNodeBatchNormalization))
        device = get_device(self.communicator.intra_rank if use_gpu else None,
                            use_chx)
        mnbn_model.to_device(device)

        with chainer.using_device(mnbn_model.device):
            x = mnbn_model.xp.zeros((1, 1, 1, 1))
            mnbn_model(x)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9098')" href="javascript:;">
chainer-7.2.0/tests/chainermn_tests/links_tests/test_create_mnbn_model.py: 62-78
</a>
<div class="mid" id="frag9098" style="display:none"><pre>
    def check_create_mnbn_model_chain_list(self, use_gpu, use_chx):
        model = BnChainList(3)
        mnbn_model = chainermn.links.create_mnbn_model(model,
                                                       self.communicator)
        self.assertTrue(isinstance(mnbn_model[0],
                                   chainer.links.Convolution2D))
        self.assertTrue(
            isinstance(mnbn_model[1],
                       chainermn.links.MultiNodeBatchNormalization))
        device = get_device(self.communicator.intra_rank if use_gpu else None,
                            use_chx)
        mnbn_model.to_device(device)

        with chainer.using_device(mnbn_model.device):
            x = mnbn_model.xp.zeros((1, 1, 1, 1))
            mnbn_model(x)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 212:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9194')" href="javascript:;">
chainer-7.2.0/tests/chainermn_tests/functions_tests/test_point_to_point_communication.py: 35-48
</a>
<div class="mid" id="frag9194" style="display:none"><pre>
def create_communicator(gpu, param):
    if gpu:
        communicator = chainermn.create_communicator('flat')
        device = communicator.intra_rank
        chainer.cuda.get_device_from_id(device).use()
    else:
        communicator = chainermn.create_communicator('naive')

    if communicator.size &lt; 2:
        pytest.skip('This test is for multinode')

    return communicator


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9209')" href="javascript:;">
chainer-7.2.0/tests/chainermn_tests/functions_tests/test_collective_communication.py: 27-42
</a>
<div class="mid" id="frag9209" style="display:none"><pre>
def get_communicator(gpu):
    numpy.random.seed(42)

    if gpu:
        communicator = chainermn.create_communicator('flat')
        device = communicator.intra_rank
        chainer.cuda.get_device_from_id(device).use()
    else:
        communicator = chainermn.create_communicator('naive')

    if communicator.size &lt; 2:
        pytest.skip('This test is for multinode')

    return communicator


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 213:</b> &nbsp; 4 fragments, nominal size 17 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9237')" href="javascript:;">
chainer-7.2.0/tests/chainermn_tests/iterators_tests/test_synchronized_iterator.py: 20-41
</a>
<div class="mid" id="frag9237" style="display:none"><pre>
    def test_sync(self):
        # test the case when datasize is a multiple of batchsize
        iterator = chainermn.iterators.create_synchronized_iterator(
            chainer.iterators.SerialIterator(
                self.dataset, batch_size=4, shuffle=True),
            self.communicator)

        for e in range(3):
            self.assertEqual(e, iterator.epoch)

            while True:
                batch = np.array(iterator.next(), dtype=np.float32)
                if self.communicator.rank == 0:
                    for rank_from in range(1, self.communicator.size):
                        _batch = self.communicator.recv(rank_from, tag=0)
                        chainer.testing.assert_allclose(batch, _batch)
                else:
                    self.communicator.send(batch, dest=0, tag=0)

                if iterator.is_new_epoch:
                    break

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9238')" href="javascript:;">
chainer-7.2.0/tests/chainermn_tests/iterators_tests/test_synchronized_iterator.py: 42-63
</a>
<div class="mid" id="frag9238" style="display:none"><pre>
    def test_sync_frag(self):
        # test the case when datasize is not a multiple of batchsize
        iterator = chainermn.iterators.create_synchronized_iterator(
            chainer.iterators.SerialIterator(
                self.dataset, batch_size=7, shuffle=True),
            self.communicator)

        for e in range(3):
            self.assertEqual(e, iterator.epoch)

            while True:
                batch = np.array(iterator.next(), dtype=np.float32)
                if self.communicator.rank == 0:
                    for rank_from in range(1, self.communicator.size):
                        _batch = self.communicator.recv(rank_from, tag=0)
                        chainer.testing.assert_allclose(batch, _batch)
                else:
                    self.communicator.send(batch, dest=0, tag=0)

                if iterator.is_new_epoch:
                    break

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9239')" href="javascript:;">
chainer-7.2.0/tests/chainermn_tests/iterators_tests/test_synchronized_iterator.py: 64-82
</a>
<div class="mid" id="frag9239" style="display:none"><pre>
    def test_sync_no_repeat(self):
        iterator = chainermn.iterators.create_synchronized_iterator(
            chainer.iterators.SerialIterator(
                self.dataset, batch_size=4, shuffle=True, repeat=False),
            self.communicator)

        for e in range(3):
            try:
                while True:
                    batch = np.array(iterator.next(), dtype=np.float32)
                    if self.communicator.rank == 0:
                        for rank_from in range(1, self.communicator.size):
                            _batch = self.communicator.recv(rank_from, tag=0)
                            chainer.testing.assert_allclose(batch, _batch)
                    else:
                        self.communicator.send(batch, dest=0, tag=0)
            except StopIteration:
                iterator.reset()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9240')" href="javascript:;">
chainer-7.2.0/tests/chainermn_tests/iterators_tests/test_synchronized_iterator.py: 83-100
</a>
<div class="mid" id="frag9240" style="display:none"><pre>
    def test_sync_no_repeat_frag(self):
        iterator = chainermn.iterators.create_synchronized_iterator(
            chainer.iterators.SerialIterator(
                self.dataset, batch_size=7, shuffle=True, repeat=False),
            self.communicator)

        for e in range(3):
            try:
                while True:
                    batch = np.array(iterator.next(), dtype=np.float32)
                    if self.communicator.rank == 0:
                        for rank_from in range(1, self.communicator.size):
                            _batch = self.communicator.recv(rank_from, tag=0)
                            chainer.testing.assert_allclose(batch, _batch)
                    else:
                        self.communicator.send(batch, dest=0, tag=0)
            except StopIteration:
                iterator.reset()
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 214:</b> &nbsp; 3 fragments, nominal size 17 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9248')" href="javascript:;">
chainer-7.2.0/tests/chainermn_tests/iterators_tests/test_multi_node_iterator.py: 72-90
</a>
<div class="mid" id="frag9248" style="display:none"><pre>

    def test_mn_iterator(self):
        # Datasize is a multiple of batchsize.
        bs = 4
        iterator = chainermn.iterators.create_multi_node_iterator(
            self.iterator_class(
                self.dataset, batch_size=bs, shuffle=True),
            self.communicator)

        for e in range(3):
            for i in range(100):
                batch = iterator.next()
                if self.communicator.rank == 0:
                    for rank_from in range(1, self.communicator.size):
                        _batch = self.communicator.mpi_comm.recv(
                            source=rank_from)
                        self.assertEqual(batch, _batch)
                else:
                    self.communicator.mpi_comm.ssend(batch, dest=0)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9251')" href="javascript:;">
chainer-7.2.0/tests/chainermn_tests/iterators_tests/test_multi_node_iterator.py: 132-153
</a>
<div class="mid" id="frag9251" style="display:none"><pre>

    def test_mn_iterator_no_repeat(self):
        # Do not repeat iterator to test if we can catch StopIteration.
        bs = 4
        iterator = chainermn.iterators.create_multi_node_iterator(
            self.iterator_class(
                self.dataset, batch_size=bs, shuffle=True, repeat=False),
            self.communicator)

        for e in range(3):
            try:
                while True:
                    batch = iterator.next()
                    if self.communicator.rank == 0:
                        for rank_from in range(1, self.communicator.size):
                            _batch = self.communicator.mpi_comm.recv(
                                source=rank_from)
                            self.assertEqual(batch, _batch)
                    else:
                        self.communicator.mpi_comm.ssend(batch, dest=0)
            except StopIteration:
                continue
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9249')" href="javascript:;">
chainer-7.2.0/tests/chainermn_tests/iterators_tests/test_multi_node_iterator.py: 91-109
</a>
<div class="mid" id="frag9249" style="display:none"><pre>

    def test_mn_iterator_frag(self):
        # Batasize is not a multiple of batchsize.
        bs = 7
        iterator = chainermn.iterators.create_multi_node_iterator(
            self.iterator_class(
                self.dataset, batch_size=bs, shuffle=True),
            self.communicator)

        for e in range(3):
            for i in range(100):
                batch = iterator.next()
                if self.communicator.rank == 0:
                    for rank_from in range(1, self.communicator.size):
                        _batch = self.communicator.mpi_comm.recv(
                            source=rank_from)
                        self.assertEqual(batch, _batch)
                else:
                    self.communicator.mpi_comm.ssend(batch, dest=0)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 215:</b> &nbsp; 2 fragments, nominal size 38 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9263')" href="javascript:;">
chainer-7.2.0/tests/chainermn_tests/iterators_tests/test_iterator_compatibility.py: 72-117
</a>
<div class="mid" id="frag9263" style="display:none"><pre>

    def test_multi_node_iterator_compatibility(self):
        iters = (
            lambda: chainermn.iterators.create_multi_node_iterator(
                self.iterator_class(
                    self.dataset, batch_size=self.bs),
                self.communicator),
            lambda: self.iterator_class(
                self.dataset, batch_size=self.bs),
        )

        bs_n_ratio = 1. * self.bs / self.N

        it_before, it_after = iters

        it = it_before()

        self.assertEqual(it.epoch, 0)
        self.assertAlmostEqual(it.epoch_detail, 0 * bs_n_ratio)
        batch1 = it.next()
        self.assertEqual(len(batch1), self.bs)
        self.assertIsInstance(batch1, list)
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 1 * bs_n_ratio)
        batch2 = it.next()
        self.assertEqual(len(batch2), self.bs)
        self.assertIsInstance(batch2, list)
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 2 * bs_n_ratio)

        target = dict()
        it.serialize(DummySerializer(target))

        it = it_after()
        it.serialize(DummyDeserializer(target))
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 2 * bs_n_ratio)

        batch3 = it.next()
        self.assertEqual(len(batch3), self.bs)
        self.assertIsInstance(batch3, list)
        self.assertTrue(it.is_new_epoch)
        self.assertEqual(
            sorted(batch1 + batch2 + batch3),
            self.dataset.tolist())
        self.assertAlmostEqual(it.epoch_detail, 3 * bs_n_ratio)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9264')" href="javascript:;">
chainer-7.2.0/tests/chainermn_tests/iterators_tests/test_iterator_compatibility.py: 118-168
</a>
<div class="mid" id="frag9264" style="display:none"><pre>

    def test_synchronized_iterator_compatibility(self):
        """
        Do not use `chainer.testing.parameterize` to share the code with
        `test_multi_node_iterator_compatibility` because pytest cannot
        guarantee the execution order of tests produced by `parameterize`,
        which causes unexpected behaviors with MPI programs.
        """
        iters = (
            lambda: chainermn.iterators.create_synchronized_iterator(
                self.iterator_class(
                    self.dataset, batch_size=self.bs),
                self.communicator),
            lambda: self.iterator_class(
                self.dataset, batch_size=self.bs),
        )

        bs_n_ratio = 1. * self.bs / self.N

        it_before, it_after = iters

        it = it_before()

        self.assertEqual(it.epoch, 0)
        self.assertAlmostEqual(it.epoch_detail, 0 * bs_n_ratio)
        batch1 = it.next()
        self.assertEqual(len(batch1), self.bs)
        self.assertIsInstance(batch1, list)
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 1 * bs_n_ratio)
        batch2 = it.next()
        self.assertEqual(len(batch2), self.bs)
        self.assertIsInstance(batch2, list)
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 2 * bs_n_ratio)

        target = dict()
        it.serialize(DummySerializer(target))

        it = it_after()
        it.serialize(DummyDeserializer(target))
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 2 * bs_n_ratio)

        batch3 = it.next()
        self.assertEqual(len(batch3), self.bs)
        self.assertIsInstance(batch3, list)
        self.assertTrue(it.is_new_epoch)
        self.assertEqual(
            sorted(batch1 + batch2 + batch3),
            self.dataset.tolist())
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 216:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9332')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/testing_tests/test_array.py: 247-260
</a>
<div class="mid" id="frag9332" style="display:none"><pre>
def test_assert_array_equal_ex_fail_dtype():
    shape = (3, 2)
    dtype1 = numpy.float32
    dtype2 = numpy.int64
    a = numpy.arange(2, 2 + numpy.prod(shape)).astype(dtype1).reshape(shape)
    b = a.astype(dtype2)
    with pytest.raises(AssertionError):
        chainerx.testing.assert_array_equal_ex(a, b)
    with pytest.raises(AssertionError):
        # strides_check does not affect dtype_check
        chainerx.testing.assert_array_equal_ex(a, b, strides_check=False)
    chainerx.testing.assert_array_equal_ex(a, b, dtype_check=False)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9333')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/testing_tests/test_array.py: 261-271
</a>
<div class="mid" id="frag9333" style="display:none"><pre>
def test_assert_array_equal_ex_fail_strides():
    shape = (3, 2)
    dtype = numpy.float32
    a = numpy.arange(2, 2 + numpy.prod(shape)).astype(dtype).reshape(shape)
    b = numpy.empty(a.T.shape, dtype).T
    b[:] = a
    with pytest.raises(AssertionError):
        chainerx.testing.assert_array_equal_ex(a, b)
    chainerx.testing.assert_array_equal_ex(a, b, strides_check=False)
    # dtype_check=False implies strides_check=False
    chainerx.testing.assert_array_equal_ex(a, b, dtype_check=False)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 217:</b> &nbsp; 5 fragments, nominal size 14 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9355')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/test_cupy_interop.py: 50-68
</a>
<div class="mid" id="frag9355" style="display:none"><pre>
def test_cupy_to_chainerx_delete_cupy_first():
    dtype = numpy.float32
    a_cupy = cupy.arange(6, dtype=dtype).reshape((2, 3))
    a_chx = _fromrawpointer(
        a_cupy.data.mem.ptr,
        a_cupy.shape,
        a_cupy.dtype,
        a_cupy.strides,
        'cuda:0',
        0,
        a_cupy)

    del a_cupy

    a_chx += 1
    chainerx.testing.assert_array_equal_ex(
        a_chx, numpy.array([[1, 2, 3], [4, 5, 6]], dtype))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9356')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/test_cupy_interop.py: 70-88
</a>
<div class="mid" id="frag9356" style="display:none"><pre>
def test_cupy_to_chainerx_delete_chainerx_first():
    dtype = numpy.float32
    a_cupy = cupy.arange(6, dtype=dtype).reshape((2, 3))
    a_chx = _fromrawpointer(
        a_cupy.data.mem.ptr,
        a_cupy.shape,
        a_cupy.dtype,
        a_cupy.strides,
        'cuda:0',
        0,
        a_cupy)

    del a_chx

    a_cupy += 1
    chainerx.testing.assert_array_equal_ex(
        a_cupy.get(), numpy.array([[1, 2, 3], [4, 5, 6]], dtype))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9360')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/test_cupy_interop.py: 166-182
</a>
<div class="mid" id="frag9360" style="display:none"><pre>
def test_cupy_to_chainerx_nondefault_device():
    dtype = numpy.float32
    with cupy.cuda.Device(1):
        a_cupy = cupy.arange(6, dtype=dtype).reshape((2, 3))
    a_chx = _fromrawpointer(
        a_cupy.data.mem.ptr,
        a_cupy.shape,
        a_cupy.dtype,
        a_cupy.strides,
        'cuda:1',
        0,
        a_cupy)

    assert a_chx.device.name == 'cuda:1'
    chainerx.testing.assert_array_equal_ex(a_chx, a_cupy.get())


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9361')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/test_cupy_interop.py: 184-198
</a>
<div class="mid" id="frag9361" style="display:none"><pre>
def test_cupy_to_chainerx_invalid_device():
    dtype = numpy.float32
    with cupy.cuda.Device(1):
        a_cupy = cupy.arange(6, dtype=dtype).reshape((2, 3))
    with pytest.raises(chainerx.ChainerxError):
        _fromrawpointer(
            a_cupy.data.mem.ptr,
            a_cupy.shape,
            a_cupy.dtype,
            a_cupy.strides,
            'cuda:0',
            0,
            a_cupy)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9357')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/test_cupy_interop.py: 90-103
</a>
<div class="mid" id="frag9357" style="display:none"><pre>
def test_cupy_to_chainerx_from_invalid_pointer():
    dtype = numpy.float32
    a_numpy = numpy.arange(6, dtype=dtype).reshape((2, 3))
    with pytest.raises(chainerx.ChainerxError):
        _fromrawpointer(
            a_numpy.ctypes.data,
            a_numpy.shape,
            a_numpy.dtype,
            a_numpy.strides,
            'cuda:0',
            0,
            a_numpy)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 218:</b> &nbsp; 2 fragments, nominal size 21 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9358')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/test_cupy_interop.py: 105-133
</a>
<div class="mid" id="frag9358" style="display:none"><pre>
def test_cupy_to_chainerx_noncontiguous_with_offset():
    dtype = numpy.float32
    a_cupy = cupy.arange(12, dtype=dtype).reshape((2, 6))[::-1, ::2]
    offset = a_cupy.data.ptr - a_cupy.data.mem.ptr

    # test preconditions
    assert offset &gt; 0
    assert not a_cupy.flags.c_contiguous

    a_chx = _fromrawpointer(
        a_cupy.data.mem.ptr,
        a_cupy.shape,
        a_cupy.dtype,
        a_cupy.strides,
        'cuda:0',
        offset,
        a_cupy)

    assert a_chx.strides == a_cupy.strides
    chainerx.testing.assert_array_equal_ex(
        a_chx, a_cupy.get(), strides_check=False)

    a_cupy[1, 1] = 53

    assert a_chx.strides == a_cupy.strides
    chainerx.testing.assert_array_equal_ex(
        a_chx, a_cupy.get(), strides_check=False)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9359')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/test_cupy_interop.py: 135-164
</a>
<div class="mid" id="frag9359" style="display:none"><pre>
def test_cupy_to_chainerx_noncontiguous_without_offset():
    # This test includes access to address before the given pointer (because of
    # a negative stride).
    dtype = numpy.float32
    a_cupy = cupy.arange(12, dtype=dtype).reshape((2, 6))[::-1, ::2]

    # test preconditons
    assert a_cupy.data.mem.ptr &lt; a_cupy.data.ptr
    assert not a_cupy.flags.c_contiguous

    a_chx = _fromrawpointer(
        a_cupy.data.ptr,
        a_cupy.shape,
        a_cupy.dtype,
        a_cupy.strides,
        'cuda:0',
        0,
        a_cupy)

    assert a_chx.strides == a_cupy.strides
    chainerx.testing.assert_array_equal_ex(
        a_chx, a_cupy.get(), strides_check=False)

    a_cupy[1, 1] = 53

    assert a_chx.strides == a_cupy.strides
    chainerx.testing.assert_array_equal_ex(
        a_chx, a_cupy.get(), strides_check=False)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 219:</b> &nbsp; 4 fragments, nominal size 18 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9362')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/test_cupy_interop.py: 200-227
</a>
<div class="mid" id="frag9362" style="display:none"><pre>
def test_chainerx_to_cupy_contiguous():
    dtype = 'float32'
    a_chx = chainerx.arange(6, dtype=dtype, device='cuda:0').reshape((2, 3))
    a_cupy = cupy.ndarray(
        a_chx.shape,
        cupy.dtype(a_chx.dtype.name),
        cupy.cuda.MemoryPointer(cupy.cuda.UnownedMemory(
            a_chx.data_ptr + a_chx.offset,
            a_chx.data_size,
            a_chx,
            0), 0),
        strides=a_chx.strides,
    )

    assert a_cupy.device.id == 0
    chainerx.testing.assert_array_equal_ex(a_chx, a_cupy.get())

    # Write to a_cupy
    a_cupy[0, 1] = 8
    chainerx.testing.assert_array_equal_ex(
        a_chx, numpy.array([[0, 8, 2], [3, 4, 5]], dtype))

    # Write to a_chx
    a_chx += 1
    chainerx.testing.assert_array_equal_ex(
        a_cupy.get(), numpy.array([[1, 9, 3], [4, 5, 6]], dtype))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9363')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/test_cupy_interop.py: 229-249
</a>
<div class="mid" id="frag9363" style="display:none"><pre>
def test_chainerx_to_cupy_delete_cupy_first():
    dtype = 'float32'
    a_chx = chainerx.arange(6, dtype=dtype, device='cuda:0').reshape((2, 3))
    a_cupy = cupy.ndarray(
        a_chx.shape,
        cupy.dtype(a_chx.dtype.name),
        cupy.cuda.MemoryPointer(cupy.cuda.UnownedMemory(
            a_chx.data_ptr + a_chx.offset,
            a_chx.data_size,
            a_chx,
            0), 0),
        strides=a_chx.strides,
    )

    del a_cupy

    a_chx += 1
    chainerx.testing.assert_array_equal_ex(
        a_chx, numpy.array([[1, 2, 3], [4, 5, 6]], dtype))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9364')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/test_cupy_interop.py: 251-271
</a>
<div class="mid" id="frag9364" style="display:none"><pre>
def test_chainerx_to_cupy_delete_chainerx_first():
    dtype = 'float32'
    a_chx = chainerx.arange(6, dtype=dtype, device='cuda:0').reshape((2, 3))
    a_cupy = cupy.ndarray(
        a_chx.shape,
        cupy.dtype(a_chx.dtype.name),
        cupy.cuda.MemoryPointer(cupy.cuda.UnownedMemory(
            a_chx.data_ptr + a_chx.offset,
            a_chx.data_size,
            a_chx,
            0), 0),
        strides=a_chx.strides,
    )

    del a_chx

    a_cupy += 1
    chainerx.testing.assert_array_equal_ex(
        a_cupy.get(), numpy.array([[1, 2, 3], [4, 5, 6]], dtype))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9366')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/test_cupy_interop.py: 306-321
</a>
<div class="mid" id="frag9366" style="display:none"><pre>
def test_chainerx_to_cupy_nondefault_device():
    dtype = 'float32'
    a_chx = chainerx.arange(6, dtype=dtype, device='cuda:1').reshape((2, 3))
    a_cupy = cupy.ndarray(
        a_chx.shape,
        cupy.dtype(a_chx.dtype.name),
        cupy.cuda.MemoryPointer(cupy.cuda.UnownedMemory(
            a_chx.data_ptr + a_chx.offset,
            a_chx.data_size,
            a_chx,
            -1), 0),
        strides=a_chx.strides,
    )

    assert a_cupy.device.id == 1
    chainerx.testing.assert_array_equal_ex(a_chx, a_cupy.get())
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 220:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9399')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_creation.py: 106-121
</a>
<div class="mid" id="frag9399" style="display:none"><pre>
def test_array_from_numpy_array(xp, shape, dtype, device):
    a_np = array_utils.create_dummy_ndarray(numpy, shape, dtype)
    a_xp = xp.array(a_np)

    if xp is chainerx:
        _check_array_from_numpy_array(a_xp, a_np, device)

        # test possibly freed memory
        a_np_copy = a_np.copy()
        del a_np
        chainerx.testing.assert_array_equal_ex(
            a_xp, a_np_copy, strides_check=False)

    return a_xp


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9400')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_creation.py: 124-139
</a>
<div class="mid" id="frag9400" style="display:none"><pre>
def test_array_from_numpy_non_contiguous_array(xp, shape, dtype, device):
    a_np = array_utils.create_dummy_ndarray(numpy, shape, dtype, padding=True)
    a_xp = xp.array(a_np)

    if xp is chainerx:
        _check_array_from_numpy_array(a_xp, a_np, device)

        # test possibly freed memory
        a_np_copy = a_np.copy()
        del a_np
        chainerx.testing.assert_array_equal_ex(
            a_xp, a_np_copy, strides_check=False)

    return a_xp


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 221:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 77%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9410')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_creation.py: 253-274
</a>
<div class="mid" id="frag9410" style="display:none"><pre>
def test_array_from_chainerx_array_with_device(
        src_dtype, dst_dtype, copy, device, dst_device_spec):
    t = array_utils.create_dummy_ndarray(
        chainerx, (2,), src_dtype, device=device)
    a = chainerx.array(t, dtype=dst_dtype, copy=copy, device=dst_device_spec)

    dst_device = chainerx.get_device(dst_device_spec)

    if (not copy
            and (dst_dtype is None or src_dtype == dst_dtype)
            and (dst_device_spec is None or device is dst_device)):
        assert t is a
    else:
        assert t is not a
        if dst_dtype is None:
            dst_dtype = t.dtype
        chainerx.testing.assert_array_equal_ex(
            a, t.to_device(dst_device).astype(dst_dtype))
        assert a.dtype == chainerx.dtype(dst_dtype)
        assert a.device is dst_device


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9416')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_creation.py: 351-371
</a>
<div class="mid" id="frag9416" style="display:none"><pre>
def test_asarray_from_chainerx_array_with_device(
        src_dtype, dst_dtype, device, dst_device_spec):
    t = array_utils.create_dummy_ndarray(
        chainerx, (2,), src_dtype, device=device)
    a = chainerx.asarray(t, dtype=dst_dtype, device=dst_device_spec)

    dst_device = chainerx.get_device(dst_device_spec)

    if ((dst_dtype is None or src_dtype == dst_dtype)
            and (dst_device_spec is None or device is dst_device)):
        assert t is a
    else:
        assert t is not a
        if dst_dtype is None:
            dst_dtype = t.dtype
        chainerx.testing.assert_array_equal_ex(
            a, t.to_device(dst_device).astype(dst_dtype))
        assert a.dtype == chainerx.dtype(dst_dtype)
        assert a.device is dst_device


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 222:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9414')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_creation.py: 324-335
</a>
<div class="mid" id="frag9414" style="display:none"><pre>
def test_asarray_from_chainerx_array(dtype):
    obj = array_utils.create_dummy_ndarray(chainerx, (2, 3), 'int32')
    a = chainerx.asarray(obj, dtype=dtype)
    if a.dtype == obj.dtype:
        assert a is obj
    else:
        assert a is not obj
    e = chainerx.array(obj, dtype=dtype, copy=False)
    chainerx.testing.assert_array_equal_ex(e, a)
    assert e.device is a.device


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9423')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_creation.py: 440-451
</a>
<div class="mid" id="frag9423" style="display:none"><pre>
def test_asanyarray_from_chainerx_array(dtype):
    obj = array_utils.create_dummy_ndarray(chainerx, (2, 3), 'int32')
    a = chainerx.asanyarray(obj, dtype=dtype)
    if a.dtype == obj.dtype:
        assert a is obj
    else:
        assert a is not obj
    e = chainerx.array(obj, dtype=dtype, copy=False)
    chainerx.testing.assert_array_equal_ex(e, a)
    assert e.device is a.device


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 223:</b> &nbsp; 3 fragments, nominal size 11 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9425')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_creation.py: 465-477
</a>
<div class="mid" id="frag9425" style="display:none"><pre>
def test_empty(xp, shape_as_sequence_or_int, dtype_spec, device):
    if xp is numpy and isinstance(dtype_spec, chainerx.dtype):
        dtype_spec = dtype_spec.name
    if dtype_spec is Unspecified:
        a = xp.empty(shape_as_sequence_or_int)
    else:
        a = xp.empty(shape_as_sequence_or_int, dtype_spec)
    a.fill(0)
    if dtype_spec in (None, Unspecified):
        a = dtype_utils.cast_if_numpy_array(xp, a, 'float32')
    return a


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9433')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_creation.py: 554-565
</a>
<div class="mid" id="frag9433" style="display:none"><pre>
def test_ones(xp, shape_as_sequence_or_int, dtype_spec, device):
    if xp is numpy and isinstance(dtype_spec, chainerx.dtype):
        dtype_spec = dtype_spec.name
    if dtype_spec is Unspecified:
        out = xp.ones(shape_as_sequence_or_int)
    else:
        out = xp.ones(shape_as_sequence_or_int, dtype_spec)
    if dtype_spec in (None, Unspecified):
        out = dtype_utils.cast_if_numpy_array(xp, out, 'float32')
    return out


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9429')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_creation.py: 512-523
</a>
<div class="mid" id="frag9429" style="display:none"><pre>
def test_zeros(xp, shape_as_sequence_or_int, dtype_spec, device):
    if xp is numpy and isinstance(dtype_spec, chainerx.dtype):
        dtype_spec = dtype_spec.name
    if dtype_spec is Unspecified:
        out = xp.zeros(shape_as_sequence_or_int)
    else:
        out = xp.zeros(shape_as_sequence_or_int, dtype_spec)
    if dtype_spec in (None, Unspecified):
        out = dtype_utils.cast_if_numpy_array(xp, out, 'float32')
    return out


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 224:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9444')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_creation.py: 685-702
</a>
<div class="mid" id="frag9444" style="display:none"><pre>
def test_arange_start_stop(xp, start, stop, dtype_spec, device):
    if xp is numpy and isinstance(dtype_spec, chainerx.dtype):
        dtype_spec = dtype_spec.name
    # Checked in test_invalid_arange_too_long_bool
    if _is_bool_spec(dtype_spec) and abs(stop - start) &gt; 2:
        return chainerx.testing.ignore()
    if ((isinstance(start, bool)
            or isinstance(stop, bool))
            and dtype_spec is None):
        # TODO(niboshi): This pattern needs dtype promotion.
        return chainerx.testing.ignore()
    out = xp.arange(start, stop, dtype=dtype_spec)
    if dtype_spec in (None, Unspecified):
        expected_dtype = _get_default_dtype(stop)
        out = dtype_utils.cast_if_numpy_array(xp, out, expected_dtype)
    return out


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9445')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_creation.py: 719-737
</a>
<div class="mid" id="frag9445" style="display:none"><pre>
def test_arange_start_stop_step(xp, start, stop, step, dtype_spec, device):
    if xp is numpy and isinstance(dtype_spec, chainerx.dtype):
        dtype_spec = dtype_spec.name
    # Checked in test_invalid_arange_too_long_bool
    if _is_bool_spec(dtype_spec) and abs((stop - start) / step) &gt; 2:
        return chainerx.testing.ignore()
    if ((isinstance(start, bool)
            or isinstance(stop, bool)
            or isinstance(step, bool))
            and dtype_spec is None):
        # TODO(niboshi): This pattern needs dtype promotion.
        return chainerx.testing.ignore()
    out = xp.arange(start, stop, step, dtype=dtype_spec)
    if dtype_spec in (None, Unspecified):
        expected_dtype = _get_default_dtype(step)
        out = dtype_utils.cast_if_numpy_array(xp, out, expected_dtype)
    return out


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 225:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9457')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_creation.py: 852-864
</a>
<div class="mid" id="frag9457" style="display:none"><pre>
def test_eye_with_default(xp, N, M, k, dtype_spec, device):
    if xp is numpy and isinstance(dtype_spec, chainerx.dtype):
        dtype_spec = dtype_spec.name

    if M is None and k is None:
        return xp.eye(N, dtype=dtype_spec)
    elif M is None:
        return xp.eye(N, k=k, dtype=dtype_spec)
    elif k is None:
        return xp.eye(N, M=M, dtype=dtype_spec)
    assert False


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9485')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_creation.py: 1213-1225
</a>
<div class="mid" id="frag9485" style="display:none"><pre>
def test_tri_with_default(xp, N, M, k, dtype_spec, device):
    if xp is numpy and isinstance(dtype_spec, chainerx.dtype):
        dtype_spec = dtype_spec.name

    if M is None and k is None:
        return xp.tri(N, dtype=dtype_spec)
    elif M is None:
        return xp.tri(N, k=k, dtype=dtype_spec)
    elif k is None:
        return xp.tri(N, M=M, dtype=dtype_spec)
    assert False


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 226:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9512')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_normalization.py: 162-177
</a>
<div class="mid" id="frag9512" style="display:none"><pre>
    def forward_chainerx(self, inputs):
        x, gamma, beta = inputs

        running_mean = chainerx.array(self.running_mean, copy=True)
        running_var = chainerx.array(self.running_var, copy=True)

        y = chainerx.batch_norm(
            x, gamma, beta, running_mean=running_mean, running_var=running_var,
            **self.optional_args)

        # Record running values for later checks.
        self.running_mean_chx = running_mean
        self.running_var_chx = running_var

        return y,

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9513')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_normalization.py: 178-193
</a>
<div class="mid" id="frag9513" style="display:none"><pre>
    def forward_chainer(self, inputs):
        x, gamma, beta = inputs

        running_mean = self.running_mean.copy()
        running_var = self.running_var.copy()

        y = chainer.functions.batch_normalization(
            x, gamma, beta, running_mean=running_mean, running_var=running_var,
            **self.optional_args)

        # Record running values for later checks.
        self.running_mean_ch = running_mean
        self.running_var_ch = running_var

        return y,

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 227:</b> &nbsp; 3 fragments, nominal size 15 lines, similarity 93%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9540')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_connection.py: 136-152
</a>
<div class="mid" id="frag9540" style="display:none"><pre>
    def generate_inputs(self):
        x_shape = self.x_shape
        w_shape = self.w_shape
        b_shape = self.b_shape
        if len(self.in_dtypes) == 3:
            x_dtype, w_dtype, b_dtype = self.in_dtypes
        else:
            (x_dtype, w_dtype), b_dtype = self.in_dtypes, None
        x = array_utils.uniform(x_shape, x_dtype)
        w = array_utils.uniform(w_shape, w_dtype)

        if b_shape is None:
            return x, w
        else:
            b = array_utils.uniform(b_shape, b_dtype)
            return x, w, b

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9547')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_connection.py: 323-339
</a>
<div class="mid" id="frag9547" style="display:none"><pre>
    def generate_inputs(self):
        x_shape = self.x_shape
        w_shape = self.w_shape
        b_shape = self.b_shape
        if len(self.in_dtypes) == 3:
            x_dtype, w_dtype, b_dtype = self.in_dtypes
        else:
            (x_dtype, w_dtype), b_dtype = self.in_dtypes, None
        x = array_utils.uniform(x_shape, x_dtype)
        w = array_utils.uniform(w_shape, w_dtype)

        if b_shape is None:
            return x, w
        else:
            b = array_utils.uniform(b_shape, b_dtype)
            return x, w, b

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9554')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_connection.py: 531-546
</a>
<div class="mid" id="frag9554" style="display:none"><pre>
    def generate_inputs(self):
        x_shape = self.x_shape
        w_shape = self.w_shape
        b_shape = self.b_shape
        if len(self.in_dtypes) == 3:
            x_dtype, w_dtype, b_dtype = self.in_dtypes
        else:
            (x_dtype, w_dtype), b_dtype = self.in_dtypes, None
        x = array_utils.uniform(x_shape, x_dtype)
        w = array_utils.uniform(w_shape, w_dtype)
        if b_shape in (None, Unspecified):
            return x, w
        else:
            b = array_utils.uniform(b_shape, b_dtype)
            return x, w, b

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 228:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9542')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_connection.py: 161-176
</a>
<div class="mid" id="frag9542" style="display:none"><pre>
    def forward_chainer(self, inputs):
        if len(inputs) == 2:
            (x, w), b = inputs, None
        else:
            x, w, b = inputs
        if x.dtype.kind != 'f':
            x = F.cast(x, 'float64')
        if w.dtype.kind != 'f':
            w = F.cast(w, 'float64')
        if b is not None and b.dtype.kind != 'f':
            b = F.cast(b, 'float64')
        y = F.convolution_nd(
            x, w, b, self.stride, self.pad, self.cover_all)
        y = F.cast(y, self.out_dtype)
        return y,

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9549')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_connection.py: 349-364
</a>
<div class="mid" id="frag9549" style="display:none"><pre>
    def forward_chainer(self, inputs):
        if len(inputs) == 3:
            x, w, b = inputs
        else:
            (x, w), b = inputs, None
        if x.dtype.kind != 'f':
            x = F.cast(x, 'float64')
        if w.dtype.kind != 'f':
            w = F.cast(w, 'float64')
        if b is not None and b.dtype.kind != 'f':
            b = F.cast(b, 'float64')
        y = chainer.functions.deconvolution_nd(
            x, w, b, self.stride, self.pad, self.outsize)
        y = F.cast(y, self.out_dtype)
        return y,

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 229:</b> &nbsp; 7 fragments, nominal size 11 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9620')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_binary.py: 555-567
</a>
<div class="mid" id="frag9620" style="display:none"><pre>
    def func_scalar(self, xp, a, scalar):
        if self.is_module:
            if self.is_scalar_rhs:
                return a &amp; scalar
            else:
                return scalar &amp; a
        else:
            if self.is_scalar_rhs:
                return xp.bitwise_and(a, scalar)
            else:
                return xp.bitwise_and(scalar, a)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9622')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_binary.py: 591-603
</a>
<div class="mid" id="frag9622" style="display:none"><pre>
    def func_scalar(self, xp, a, scalar):
        if self.is_module:
            if self.is_scalar_rhs:
                return a ^ scalar
            else:
                return scalar ^ a
        else:
            if self.is_scalar_rhs:
                return xp.bitwise_xor(a, scalar)
            else:
                return xp.bitwise_xor(scalar, a)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9621')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_binary.py: 573-585
</a>
<div class="mid" id="frag9621" style="display:none"><pre>
    def func_scalar(self, xp, a, scalar):
        if self.is_module:
            if self.is_scalar_rhs:
                return a | scalar
            else:
                return scalar | a
        else:
            if self.is_scalar_rhs:
                return xp.bitwise_or(a, scalar)
            else:
                return xp.bitwise_or(scalar, a)


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9686')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_arithmetic.py: 328-340
</a>
<div class="mid" id="frag9686" style="display:none"><pre>
    def func_scalar(self, xp, a, scalar):
        if self.is_module:
            if self.is_scalar_rhs:
                return a + scalar
            else:
                return scalar + a
        else:
            if self.is_scalar_rhs:
                return xp.add(a, scalar)
            else:
                return xp.add(scalar, a)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9731')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_arithmetic.py: 1626-1638
</a>
<div class="mid" id="frag9731" style="display:none"><pre>
    def func_scalar(self, xp, a, scalar):
        if self.is_module:
            if self.is_scalar_rhs:
                return a % scalar
            else:
                return scalar % a
        else:
            if self.is_scalar_rhs:
                return xp.remainder(a, scalar)
            else:
                return xp.remainder(scalar, a)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9696')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_arithmetic.py: 719-731
</a>
<div class="mid" id="frag9696" style="display:none"><pre>
    def func_scalar(self, xp, a, scalar):
        if self.is_module:
            if self.is_scalar_rhs:
                return a * scalar
            else:
                return scalar * a
        else:
            if self.is_scalar_rhs:
                return xp.multiply(a, scalar)
            else:
                return xp.multiply(scalar, a)


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9692')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_arithmetic.py: 534-546
</a>
<div class="mid" id="frag9692" style="display:none"><pre>
    def func_scalar(self, xp, a, scalar):
        if self.is_module:
            if self.is_scalar_rhs:
                return a - scalar
            else:
                return scalar - a
        else:
            if self.is_scalar_rhs:
                return xp.subtract(a, scalar)
            else:
                return xp.subtract(scalar, a)


</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 230:</b> &nbsp; 6 fragments, nominal size 19 lines, similarity 95%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9643')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py: 34-53
</a>
<div class="mid" id="frag9643" style="display:none"><pre>
    def setup(self):
        self.check_forward_options.update({
            'rtol': 1e-2, 'atol': 1e-2})
        self.check_backward_options.update({
            'rtol': 1e-2, 'atol': 1e-2})
        self.check_double_backward_options.update({
            'rtol': 5e-3, 'atol': 5e-2})
        if self.in_dtypes[0] == 'float16':
            self.check_forward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
            self.check_backward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
            self.check_double_backward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
        device = chainerx.get_default_device()
        if device.backend.name == 'cuda':
            if self.in_dtypes[0] != 'float32':
                self.skip_backward_test = True
            self.skip_double_backward_test = True

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9661')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py: 344-364
</a>
<div class="mid" id="frag9661" style="display:none"><pre>
    def setup(self):

        self.check_forward_options.update({
            'rtol': 1e-2, 'atol': 1e-2})
        self.check_backward_options.update({
            'rtol': 1e-2, 'atol': 1e-2})
        self.check_double_backward_options.update({
            'rtol': 5e-2, 'atol': 5e-2})
        if self.in_dtypes[0] == 'float16':
            self.check_forward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
            self.check_backward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
            self.check_double_backward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
        device = chainerx.get_default_device()
        if device.backend.name == 'cuda':
            if self.in_dtypes[0] != 'float32':
                self.skip_backward_test = True
            self.skip_double_backward_test = True

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9655')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py: 246-266
</a>
<div class="mid" id="frag9655" style="display:none"><pre>
    def setup(self):
        self.check_forward_options.update({
            'rtol': 1e-2, 'atol': 1e-2})
        self.check_backward_options.update({
            'rtol': 1e-2, 'atol': 1e-2})
        self.check_double_backward_options.update({
            'rtol': 5e-3, 'atol': 5e-2})
        if self.in_dtypes[0] == 'float16':
            self.check_forward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
            self.check_backward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
            self.check_double_backward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
        device = chainerx.get_default_device()
        if device.backend.name == 'cuda':
            if self.in_dtypes[0] != 'float32':

                self.skip_backward_test = True
            self.skip_double_backward_test = True

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9667')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py: 458-478
</a>
<div class="mid" id="frag9667" style="display:none"><pre>
    def setup(self):
        self.check_forward_options.update({
            'rtol': 1e-2, 'atol': 1e-2})
        self.check_backward_options.update({
            'rtol': 1e-2, 'atol': 1e-2})
        self.check_double_backward_options.update({
            'rtol': 5e-2, 'atol': 5e-2})
        if self.in_dtypes[0] == 'float16':
            self.check_forward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
            self.check_backward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
            self.check_double_backward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
        device = chainerx.get_default_device()
        if device.backend.name == 'cuda':
            if self.in_dtypes[0] != 'float32':
                self.skip_forward_test = True
                self.skip_backward_test = True
            self.skip_double_backward_test = True

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9673')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py: 564-585
</a>
<div class="mid" id="frag9673" style="display:none"><pre>
    def setup(self):

        self.check_forward_options.update({
            'rtol': 1e-2, 'atol': 1e-2})
        self.check_backward_options.update({
            'rtol': 1e-2, 'atol': 1e-2})
        self.check_double_backward_options.update({
            'rtol': 5e-2, 'atol': 5e-2})
        if self.in_dtypes[0] == 'float16':
            self.check_forward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
            self.check_backward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
            self.check_double_backward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
        device = chainerx.get_default_device()
        if device.backend.name == 'cuda':
            if self.in_dtypes[0] != 'float32':
                self.skip_forward_test = True
                self.skip_backward_test = True
            self.skip_double_backward_test = True

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9649')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py: 135-155
</a>
<div class="mid" id="frag9649" style="display:none"><pre>
    def setup(self):

        self.check_forward_options.update({
            'rtol': 1e-2, 'atol': 1e-2})
        self.check_backward_options.update({
            'rtol': 1e-2, 'atol': 1e-2})
        self.check_double_backward_options.update({
            'rtol': 5e-3, 'atol': 5e-2})
        if self.in_dtypes[0] == 'float16':
            self.check_forward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
            self.check_backward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
            self.check_double_backward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
        device = chainerx.get_default_device()
        if device.backend.name == 'cuda':
            if self.in_dtypes[0] != 'float32':
                self.skip_backward_test = True
            self.skip_double_backward_test = True

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 231:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9648')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py: 105-117
</a>
<div class="mid" id="frag9648" style="display:none"><pre>
    def forward_chainer(self, inputs):
        h, c, ws, bs, xs = self.process_input(inputs)
        out = chainer.functions.n_step_lstm(
            self.n_layers, 0.0, h, c, ws, bs, xs)
        rets = []
        rets.append(out[0])
        rets.append(out[1])
        for i in range(len(out[2])):
            rets.append(out[2][i])

        return tuple(rets)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9654')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py: 215-227
</a>
<div class="mid" id="frag9654" style="display:none"><pre>
    def forward_chainer(self, inputs):
        h, c, ws, bs, xs = self.process_input(inputs)
        out = chainer.functions.n_step_bilstm(
            self.n_layers, 0.0, h, c, ws, bs, xs)
        rets = []
        rets.append(out[0])
        rets.append(out[1])
        for i in range(len(out[2])):
            rets.append(out[2][i])

        return tuple(rets)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 232:</b> &nbsp; 5 fragments, nominal size 10 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9683')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_arithmetic.py: 224-235
</a>
<div class="mid" id="frag9683" style="display:none"><pre>
def test_add_invalid_dtypes(device, dtypes, is_module):
    (in_dtype1, in_dtype2), _ = dtypes
    shape = (2, 3)
    a = chainerx.array(array_utils.uniform(shape, in_dtype1))
    b = chainerx.array(array_utils.uniform(shape, in_dtype2))
    with pytest.raises(chainerx.DtypeError):
        if is_module:
            a + b
        else:
            chainerx.add(a, b)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9709')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_arithmetic.py: 1050-1061
</a>
<div class="mid" id="frag9709" style="display:none"><pre>
def test_truediv_invalid_dtypes(device, dtypes, is_module):
    (in_dtype1, in_dtype2), _ = dtypes
    shape = (2, 3)
    a = chainerx.array(array_utils.uniform(shape, in_dtype1))
    b = chainerx.array(array_utils.uniform(shape, in_dtype2))
    with pytest.raises(chainerx.DtypeError):
        if is_module:
            a / b
        else:
            chainerx.true_divide(a, b)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9725')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_arithmetic.py: 1490-1501
</a>
<div class="mid" id="frag9725" style="display:none"><pre>
def test_remainder_invalid_dtypes(device, dtypes, is_module):
    (in_dtype1, in_dtype2), _ = dtypes
    shape = (2, 3)
    a = chainerx.array(array_utils.uniform(shape, in_dtype1))
    b = chainerx.array(array_utils.uniform(shape, in_dtype2))
    with pytest.raises(chainerx.DtypeError):
        if is_module:
            a % b
        else:
            chainerx.remainder(a, b)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9689')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_arithmetic.py: 430-441
</a>
<div class="mid" id="frag9689" style="display:none"><pre>
def test_sub_invalid_dtypes(device, dtypes, is_module):
    (in_dtype1, in_dtype2), _ = dtypes
    shape = (2, 3)
    a = chainerx.array(array_utils.uniform(shape, in_dtype1))
    b = chainerx.array(array_utils.uniform(shape, in_dtype2))
    with pytest.raises(chainerx.DtypeError):
        if is_module:
            a - b
        else:
            chainerx.subtract(a, b)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9703')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_arithmetic.py: 864-878
</a>
<div class="mid" id="frag9703" style="display:none"><pre>
def test_floordiv_invalid_dtypes(device, dtypes, is_module):
    (in_dtype1, in_dtype2), _ = dtypes
    shape = (2, 3)
    a = chainerx.array(array_utils.uniform(shape, in_dtype1))
    b = chainerx.array(array_utils.uniform(shape, in_dtype2))
    with pytest.raises(chainerx.DtypeError):
        if is_module:
            a // b
        else:
            chainerx.floor_divide(a, b)


# TODO(imanishi): Support and test zero division and mixed dtypes.
# TODO(imanishi): Support and test chainerx.Scalar // chainerx.ndarray.
# TODO(imanishi): Support and test bool dtype.
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 233:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9702')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_arithmetic.py: 849-861
</a>
<div class="mid" id="frag9702" style="display:none"><pre>
    def func_scalar(self, xp, a, scalar):
        if self.is_module:
            if self.is_scalar_rhs:
                return xp.floor_divide(a, scalar)
            else:
                return xp.floor_divide(scalar, a)
        else:
            if self.is_scalar_rhs:
                return a // scalar
            else:
                return scalar // a


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9713')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_arithmetic.py: 1160-1172
</a>
<div class="mid" id="frag9713" style="display:none"><pre>
    def func_scalar(self, xp, a, scalar):
        if self.is_module:
            if self.is_scalar_rhs:
                return xp.divide(a, scalar)
            else:
                return xp.divide(scalar, a)
        else:
            if self.is_scalar_rhs:
                return a / scalar
            else:
                return scalar / a


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 234:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9723')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_arithmetic.py: 1469-1480
</a>
<div class="mid" id="frag9723" style="display:none"><pre>
    def generate_inputs(self):
        dtype1, dtype2 = self.in_dtypes
        shape1, shape2 = self.in_shapes
        low1 = -5 if numpy.dtype(dtype1).kind != 'u' else 2
        low2 = -5 if numpy.dtype(dtype2).kind != 'u' else 2
        high = 5
        a = array_utils.uniform(shape1, dtype1, low=low1, high=high)
        b = array_utils.uniform(shape2, dtype2, low=low2, high=high)
        a[numpy.logical_and(-0.5 &lt; a, a &lt; 0.5)] = 1
        b[numpy.logical_and(-0.5 &lt; b, b &lt; 0.5)] = 1
        return a, b

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9727')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_arithmetic.py: 1543-1554
</a>
<div class="mid" id="frag9727" style="display:none"><pre>
    def generate_inputs(self):
        dtype1, dtype2 = self.in_dtypes
        shape1, shape2 = self.in_shapes
        low1 = -5 if numpy.dtype(dtype1).kind != 'u' else 2
        low2 = -5 if numpy.dtype(dtype2).kind != 'u' else 2
        high = 5
        a = array_utils.uniform(shape1, dtype1, low=low1, high=high)
        b = array_utils.uniform(shape2, dtype2, low=low2, high=high)
        a[numpy.logical_and(-0.5 &lt; a, a &lt; 0.5)] = 1
        b[numpy.logical_and(-0.5 &lt; b, b &lt; 0.5)] = 1
        return a, b

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 235:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9785')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_activation.py: 230-249
</a>
<div class="mid" id="frag9785" style="display:none"><pre>
    def setup(self):
        in_dtype, = self.in_dtypes
        if isinstance(self.alpha_range, tuple):
            l, u = self.alpha_range
            self.alpha = random.uniform(l, u)
        elif self.alpha_range is Unspecified:
            self.alpha = 1.0
        else:
            self.alpha = self.alpha_range

        if numpy.dtype(in_dtype).kind != 'f':
            self.skip_backward_test = True
            self.skip_double_backward_test = True

        if in_dtype == 'float16':
            self.check_forward_options.update({'rtol': 1e-3, 'atol': 1e-3})
            self.check_backward_options.update({'rtol': 2e-3, 'atol': 2e-3})
            self.check_double_backward_options.update(
                {'rtol': 1e-2, 'atol': 1e-2})

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9790')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_activation.py: 367-386
</a>
<div class="mid" id="frag9790" style="display:none"><pre>
    def setup(self):
        in_dtype, = self.in_dtypes
        if isinstance(self.beta_range, tuple):
            l, u = self.beta_range
            self.beta = random.uniform(l, u)
        elif self.beta_range is Unspecified:
            self.beta = 1.0
        else:
            self.beta = self.beta_range

        if numpy.dtype(in_dtype).kind != 'f':
            self.skip_backward_test = True
            self.skip_double_backward_test = True

        if in_dtype == 'float16':
            self.check_forward_options.update({'rtol': 2e-3, 'atol': 2e-3})
            self.check_backward_options.update({'rtol': 2e-3, 'atol': 2e-3})
            self.check_double_backward_options.update(
                {'rtol': 1e-2, 'atol': 1e-2})

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 236:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9796')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_logic.py: 132-148
</a>
<div class="mid" id="frag9796" style="display:none"><pre>
def test_cmp_invalid_shapes(cmp_func, a_shape, b_shape):
    cmp_op = _cmp_funcs[cmp_func]
    chx_cmp = getattr(chainerx, cmp_func)

    def check(x, y):
        with pytest.raises(chainerx.DimensionError):
            cmp_op(x, y)

        with pytest.raises(chainerx.DimensionError):
            chx_cmp(x, y)

    a = array_utils.create_dummy_ndarray(chainerx, a_shape, 'float32')
    b = array_utils.create_dummy_ndarray(chainerx, b_shape, 'float32')
    check(a, b)
    check(b, a)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9798')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_logic.py: 150-166
</a>
<div class="mid" id="frag9798" style="display:none"><pre>
def test_cmp_invalid_dtypes(cmp_func, numeric_dtype):
    cmp_op = _cmp_funcs[cmp_func]
    chx_cmp = getattr(chainerx, cmp_func)

    def check(x, y):
        with pytest.raises(chainerx.DtypeError):
            cmp_op(x, y)

        with pytest.raises(chainerx.DtypeError):
            chx_cmp(x, y)

    a = array_utils.create_dummy_ndarray(chainerx, (2, 3), 'bool_')
    b = array_utils.create_dummy_ndarray(chainerx, (2, 3), numeric_dtype)
    check(a, b)
    check(b, a)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 237:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9858')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/op_utils.py: 234-255
</a>
<div class="mid" id="frag9858" style="display:none"><pre>
    def forward_chainerx(self, inputs):
        # Computes the forward pass in ChainerX.
        #
        # In case of an acceptable error, the error is stored and a dummy
        # output array is returned.
        #
        # The detected errors are checked in `check_forward_outputs`, and
        # also in `_create_test_entry_function` to skip
        # backward/double-backward tests.
        accept_errors = self.__get_accept_errors()
        try:
            outputs = self.forward_xp(inputs, chainerx)
            self.__forward_error_chainerx = 'ok'
        except accept_errors as e:
            # Keep detected error
            self.__forward_error_chainerx = e
            # A dummy output array is returned
            y = chainerx.zeros((0,), 'float32')
            outputs = y,

        return outputs

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9859')" href="javascript:;">
chainer-7.2.0/tests/chainerx_tests/op_utils.py: 256-271
</a>
<div class="mid" id="frag9859" style="display:none"><pre>
    def forward_expected(self, inputs):
        # Computes the forward pass in NumPy.
        # Also see comments in `forward_chainerx`.
        accept_errors = self.__get_accept_errors()
        try:
            outputs = self.forward_xp(inputs, numpy)
            self.__forward_error_expected = 'ok'
        except accept_errors as e:
            # Keep detected error
            self.__forward_error_expected = e
            # A dummy output array is returned
            y = numpy.zeros((0,), 'float32')
            outputs = y,

        return tuple([numpy.asarray(y) for y in outputs])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 238:</b> &nbsp; 4 fragments, nominal size 10 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9892')" href="javascript:;">
chainer-7.2.0/onnx_chainer/functions/math.py: 18-30
</a>
<div class="mid" id="frag9892" style="display:none"><pre>
def convert_AddConstant(
        func, opset_version, input_names, output_names, context):
    value_name = context.add_const(
        np.array(func.value, dtype=func.inputs[0].dtype), 'value')
    input_names.append(value_name)

    if opset_version == 1:
        return onnx_helper.make_node(
            'Add', input_names, output_names, consumed_inputs=[1, 1]),
    elif opset_version == 6 or opset_version == 7:
        return onnx_helper.make_node('Add', input_names, output_names),


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9896')" href="javascript:;">
chainer-7.2.0/onnx_chainer/functions/math.py: 64-76
</a>
<div class="mid" id="frag9896" style="display:none"><pre>
def convert_MulConstant(
        func, opset_version, input_names, output_names, context):
    value_name = context.add_const(
        np.array(func.value, dtype=func.inputs[0].dtype), 'value')
    input_names.append(value_name)

    if opset_version == 1:
        return onnx_helper.make_node(
            'Mul', input_names, output_names, consumed_inputs=[1, 1]),
    elif opset_version == 6 or opset_version == 7:
        return onnx_helper.make_node('Mul', input_names, output_names),


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9899')" href="javascript:;">
chainer-7.2.0/onnx_chainer/functions/math.py: 96-108
</a>
<div class="mid" id="frag9899" style="display:none"><pre>
def convert_DivFromConstant(
        func, opset_version, input_names, output_names, context):
    value_name = context.add_const(
        np.array(func.value, dtype=func.inputs[0].dtype), 'value')
    input_names[:0] = [value_name]

    if opset_version == 1:
        return onnx_helper.make_node(
            'Div', input_names, output_names, consumed_inputs=[1, 1]),
    elif opset_version == 6 or opset_version == 7:
        return onnx_helper.make_node('Div', input_names, output_names),


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9894')" href="javascript:;">
chainer-7.2.0/onnx_chainer/functions/math.py: 41-53
</a>
<div class="mid" id="frag9894" style="display:none"><pre>
def convert_SubFromConstant(
        func, opset_version, input_names, output_names, context):
    value_name = context.add_const(
        np.array(func.value, dtype=func.inputs[0].dtype), 'value')
    input_names[:0] = [value_name]

    if opset_version == 1:
        return onnx_helper.make_node(
            'Sub', input_names, output_names, consumed_inputs=[1, 1]),
    elif opset_version == 6 or opset_version == 7:
        return onnx_helper.make_node('Sub', input_names, output_names),


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 239:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 94%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9935')" href="javascript:;">
chainer-7.2.0/onnx_chainer/functions/connection.py: 31-49
</a>
<div class="mid" id="frag9935" style="display:none"><pre>
def convert_ConvolutionND(
        func, opset_version, input_names, output_names, context):
    pad = []
    x_ndim = len(func.inputs[0].shape)
    w_ndim = len(func.inputs[1].shape)
    for _ in range(x_ndim - w_ndim):
        pad.append(0)
    for p in func.pad:
        pad.append(p)
    pad = pad * 2

    return onnx_helper.make_node(
        'Conv', input_names, output_names,
        kernel_shape=func.inputs[1].shape[2:],
        pads=pad,
        strides=func.stride,
        group=func.groups,
    ),

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9937')" href="javascript:;">
chainer-7.2.0/onnx_chainer/functions/connection.py: 64-83
</a>
<div class="mid" id="frag9937" style="display:none"><pre>
def convert_DeconvolutionND(
        func, opset_version, input_names, output_names, context):
    pad = []
    x_ndim = len(func.inputs[0].shape)
    w_ndim = len(func.inputs[1].shape)
    for _ in range(x_ndim - w_ndim):
        pad.append(0)
    for p in func.pad:
        pad.append(p)
    pad = pad * 2

    return onnx_helper.make_node(
        'ConvTranspose', input_names, output_names,
        kernel_shape=func.inputs[1].shape[2:],
        output_shape=func.outs,
        pads=pad,
        strides=func.stride,
        group=func.groups,
    ),

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 240:</b> &nbsp; 2 fragments, nominal size 19 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9940')" href="javascript:;">
chainer-7.2.0/onnx_chainer/functions/pooling.py: 11-36
</a>
<div class="mid" id="frag9940" style="display:none"><pre>
def convert_AveragePooling2D(
        func, opset_version, input_names, output_names, context):
    pad = [func.ph, func.pw]
    stride = [func.sy, func.sx]
    ksize = [func.kh, func.kw]
    if func.cover_all:
        # Supports cover_all by setting extra padding
        # NOTE: onnxruntime may not run when "k &lt;= p + s - 1".
        pad.extend([p + s - 1 for p, s in zip(pad, stride)])
    else:
        pad = pad * 2

    if opset_version == 1:
        raise ValueError(
            'AveragePooling2D is not compatible with ONNX\'s AveragePool-1. '
            'Use operation set version &gt;= 7.')
    elif opset_version == 7:
        return onnx_helper.make_node(
            'AveragePool', input_names, output_names,
            kernel_shape=ksize,
            pads=pad,
            strides=stride,
            count_include_pad=1,
        ),


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9941')" href="javascript:;">
chainer-7.2.0/onnx_chainer/functions/pooling.py: 38-61
</a>
<div class="mid" id="frag9941" style="display:none"><pre>
def convert_AveragePoolingND(
        func, opset_version, input_names, output_names, context):
    pad = list(func.pad[:])
    if func.cover_all:
        # Supports cover_all by setting extra padding
        # NOTE: onnxruntime may not run when "k &lt;= p + s - 1".
        pad.extend([p + s - 1 for p, s in zip(pad, func.stride)])
    else:
        pad = pad * 2

    if opset_version == 1:
        raise ValueError(
            'AveragePoolingND is not compatible with ONNX\'s AveragePool-1. '
            'Use operation set version &gt;= 7.')
    elif opset_version == 7:
        return onnx_helper.make_node(
            'AveragePool', input_names, output_names,
            kernel_shape=func.ksize,
            pads=pad,
            strides=func.stride,
            count_include_pad=1,
        ),


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 241:</b> &nbsp; 2 fragments, nominal size 28 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9942')" href="javascript:;">
chainer-7.2.0/onnx_chainer/functions/pooling.py: 63-97
</a>
<div class="mid" id="frag9942" style="display:none"><pre>
def convert_MaxPooling2D(
        func, opset_version, input_names, output_names, context):
    pad = [func.ph, func.pw]
    stride = [func.sy, func.sx]
    ksize = [func.kh, func.kw]
    attrs = {}
    if func.cover_all:
        if opset_version &lt; 11:
            # Supports cover_all by setting extra padding
            # NOTE: onnxruntime may not run when "k &lt;= p + s - 1".
            pad.extend([p + s - 1 for p, s in zip(pad, func.stride)])
        else:
            pad = pad * 2
            attrs['ceil_mode'] = 1
    else:
        pad = pad * 2

    if opset_version == 1:
        return onnx_helper.make_node(
            'MaxPool', input_names, output_names,
            kernel_shape=ksize,
            pads=pad,
            strides=stride
        ),
    elif opset_version &gt;= 8:
        return onnx_helper.make_node(
            'MaxPool', input_names, output_names,
            kernel_shape=ksize,
            pads=pad,
            strides=stride,
            storage_order=0,  # row major
            **attrs,
        ),


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9943')" href="javascript:;">
chainer-7.2.0/onnx_chainer/functions/pooling.py: 99-131
</a>
<div class="mid" id="frag9943" style="display:none"><pre>
def convert_MaxPoolingND(
        func, opset_version, input_names, output_names, context):
    pad = list(func.pad[:])
    attrs = {}
    if func.cover_all:
        if opset_version &lt; 11:
            # Supports cover_all by setting extra padding
            # NOTE: onnxruntime may not run when "k &lt;= p + s - 1".
            pad.extend([p + s - 1 for p, s in zip(pad, func.stride)])
        else:
            pad = pad * 2
            attrs['ceil_mode'] = 1
    else:
        pad = pad * 2

    if opset_version == 1:
        return onnx_helper.make_node(
            'MaxPool', input_names, output_names,
            kernel_shape=func.ksize,
            pads=pad,
            strides=func.stride
        ),
    elif opset_version &gt;= 8:
        return onnx_helper.make_node(
            'MaxPool', input_names, output_names,
            kernel_shape=func.ksize,
            pads=pad,
            strides=func.stride,
            storage_order=0,  # row major
            **attrs,
        ),


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 242:</b> &nbsp; 3 fragments, nominal size 10 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9949')" href="javascript:;">
chainer-7.2.0/onnx_chainer/functions/activation.py: 53-65
</a>
<div class="mid" id="frag9949" style="display:none"><pre>
def convert_ELU(func, opset_version, input_names, output_names, context):
    if opset_version == 1:
        return onnx_helper.make_node(
            'Elu', input_names, output_names,
            alpha=func.alpha,
        ),
    elif opset_version == 6:
        return onnx_helper.make_node(
            'Elu', input_names, output_names,
            alpha=func.alpha
        ),


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9951')" href="javascript:;">
chainer-7.2.0/onnx_chainer/functions/activation.py: 85-98
</a>
<div class="mid" id="frag9951" style="display:none"><pre>
def convert_LeakyReLU(func, opset_version, input_names, output_names, context):
    if opset_version == 1:
        return onnx_helper.make_node(
            'LeakyRelu', input_names, output_names,
            alpha=func.slope,
            consumed_inputs=[1],
        ),
    elif opset_version == 6:
        return onnx_helper.make_node(
            'LeakyRelu', input_names, output_names,
            alpha=func.slope
        ),


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9962')" href="javascript:;">
chainer-7.2.0/onnx_chainer/functions/array.py: 33-46
</a>
<div class="mid" id="frag9962" style="display:none"><pre>
def convert_Cast(func, opset_version, input_names, output_names, context):
    typ = func.type if isinstance(func.type, np.dtype) else np.dtype(func.type)
    if opset_version == 1:
        return onnx_helper.make_node(
            'Cast', input_names, output_names,
            to=TENSOR_TYPE_TO_NAME[NP_TYPE_TO_TENSOR_TYPE[typ]]
        ),
    elif opset_version == 6:
        return onnx_helper.make_node(
            'Cast', input_names, output_names,
            to=NP_TYPE_TO_TENSOR_TYPE[typ]
        ),


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 243:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag9985')" href="javascript:;">
chainer-7.2.0/onnx_chainer/functions/array.py: 576-588
</a>
<div class="mid" id="frag9985" style="display:none"><pre>

def convert_Vstack(func, opset_version, input_names, output_names, context):
    gb = onnx_helper.GraphBuilder()
    input0_ndim = len(func.inputs[0].shape)
    inputs = input_names
    if input0_ndim == 0:
        inputs = [gb.op('Unsqueeze', [name], axes=[0, 1]) for
                  name in input_names]
    elif input0_ndim == 1:
        inputs = [gb.op('Unsqueeze', [name], axes=[0]) for name in input_names]
    gb.op_output_named('Concat', inputs, output_names, axis=0)
    return gb.nodes()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag9986')" href="javascript:;">
chainer-7.2.0/onnx_chainer/functions/array.py: 589-604
</a>
<div class="mid" id="frag9986" style="display:none"><pre>

def convert_Dstack(func, opset_version, input_names, output_names, context):
    gb = onnx_helper.GraphBuilder()
    input0_ndim = len(func.inputs[0].shape)
    inputs = input_names
    if input0_ndim == 0:
        inputs = [gb.op('Unsqueeze', [name], axes=[0, 1, 2]) for
                  name in input_names]
    elif input0_ndim == 1:
        inputs = [gb.op('Unsqueeze', [name], axes=[0, 2]) for
                  name in input_names]
    elif input0_ndim == 2:
        inputs = [gb.op('Unsqueeze', [name], axes=[2]) for name in input_names]
    gb.op_output_named('Concat', inputs, output_names, axis=2)
    return gb.nodes()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 244:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10157')" href="javascript:;">
chainer-7.2.0/chainermn/communicators/_memory_utility.py: 109-123
</a>
<div class="mid" id="frag10157" style="display:none"><pre>
    def from_device(self, src, size, offset=0, stream=None):
        dst = self.memory + offset
        xp = chainer.backend.get_array_module(src)
        if xp == cp:
            src_data = src.data
        elif xp == chx:
            src_data = _get_memory_pointer_from_chainerx(src)
        else:
            raise ValueError(
                '{} is from an unsupported array module'.format(type(src)))
        if stream is None:
            dst.copy_from_device(src_data, size)
        else:
            dst.copy_from_device_async(src_data, size, stream)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10158')" href="javascript:;">
chainer-7.2.0/chainermn/communicators/_memory_utility.py: 124-138
</a>
<div class="mid" id="frag10158" style="display:none"><pre>
    def to_device(self, dst, size, offset=0, stream=None):
        src = self.memory + offset
        xp = chainer.backend.get_array_module(dst)
        if xp == cp:
            dst_data = dst.data
        elif xp == chx:
            dst_data = _get_memory_pointer_from_chainerx(dst)
        else:
            raise ValueError(
                '{} is from an unsupported array module'.format(type(dst)))
        if stream is None:
            dst_data.copy_from_device(src, size)
        else:
            dst_data.copy_from_device_async(src, size, stream)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 245:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10169')" href="javascript:;">
chainer-7.2.0/chainermn/communicators/_memory_utility.py: 253-270
</a>
<div class="mid" id="frag10169" style="display:none"><pre>
def _batched_pack_params(params_data, buffer, dtype, stream=None):
    n_params = params_data.n_params
    n_elems = params_data.n_elems
    params_dptr = params_data.dptr
    params_dtype = params_data.dtype
    params_size_csum = params_data.size_csum
    buf_dtype = _communication_utility._get_nccl_type_id(dtype)
    n_threads = 128
    n_blocks = (n_elems + n_threads - 1) // n_threads
    if stream is None:
        stream = cp.cuda.get_current_stream()
    with stream:
        _cupy_batched_pack_params()(
            (n_blocks, ), (n_threads, ),
            (buffer.memory.ptr, buf_dtype, n_elems,
             params_dptr, params_dtype, params_size_csum, n_params))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10170')" href="javascript:;">
chainer-7.2.0/chainermn/communicators/_memory_utility.py: 271-288
</a>
<div class="mid" id="frag10170" style="display:none"><pre>
def _batched_unpack_params(params_data, buffer, dtype, stream=None):
    n_params = params_data.n_params
    n_elems = params_data.n_elems
    params_dptr = params_data.dptr
    params_dtype = params_data.dtype
    params_size_csum = params_data.size_csum
    buf_dtype = _communication_utility._get_nccl_type_id(dtype)
    n_threads = 128
    n_blocks = (n_elems + n_threads - 1) // n_threads
    if stream is None:
        stream = cp.cuda.get_current_stream()
    with stream:
        _cupy_batched_unpack_params()(
            (n_blocks, ), (n_threads, ),
            (buffer.memory.ptr, buf_dtype, n_elems,
             params_dptr, params_dtype, params_size_csum, n_params))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 246:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10178')" href="javascript:;">
chainer-7.2.0/chainermn/functions/batch_normalization.py: 11-22
</a>
<div class="mid" id="frag10178" style="display:none"><pre>
    def get_mean_and_var(self, axis, gamma, x, xp, interm_dtype):
        tmp = xp.empty(gamma.size * 2, dtype=gamma.dtype)
        x.mean(axis=axis, out=tmp[:gamma.size], dtype=gamma.dtype)
        xp.square(x).mean(axis=axis, out=tmp[gamma.size:], dtype=gamma.dtype)
        if xp is cuda.cupy:
            chainer.cuda.Stream.null.synchronize()
        self.comm._multi_node_mean(None, tmp)
        mean = tmp[:gamma.size]
        sqmean = tmp[gamma.size:]
        var = sqmean - xp.square(mean)
        return mean, var

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10179')" href="javascript:;">
chainer-7.2.0/chainermn/functions/batch_normalization.py: 23-34
</a>
<div class="mid" id="frag10179" style="display:none"><pre>
    def get_ggamma_and_gbeta(self, axis, gamma, gy, x_hat, xp):
        tmp = xp.empty(gamma.size * 2, dtype=gamma.dtype)
        gy.sum(axis=axis, out=tmp[:gamma.size], dtype=gamma.dtype)
        (gy * x_hat).sum(axis=axis, out=tmp[gamma.size:], dtype=gamma.dtype)
        if xp is cuda.cupy:
            chainer.cuda.Stream.null.synchronize()
        self.comm._multi_node_mean(None, tmp)
        gbeta = tmp[:gamma.size]
        ggamma = tmp[gamma.size:]
        return gbeta, ggamma


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 247:</b> &nbsp; 2 fragments, nominal size 24 lines, similarity 91%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10181')" href="javascript:;">
chainer-7.2.0/chainermn/functions/batch_normalization.py: 44-69
</a>
<div class="mid" id="frag10181" style="display:none"><pre>
    def get_mean_and_var(self, axis, gamma, x, xp, interm_dtype):
        gpu_buffer_n_elems = gamma.size * 2
        gpu_buffer_size = gamma.dtype.itemsize * gpu_buffer_n_elems
        gpu_buffer_a = self.memory_utility_module.DeviceMemory()
        gpu_buffer_b = self.memory_utility_module.DeviceMemory()
        gpu_buffer_a.assign(gpu_buffer_size)
        gpu_buffer_b.assign(gpu_buffer_size)
        gpu_buffer_a_array = gpu_buffer_a.array(
            gpu_buffer_n_elems, dtype=gamma.dtype)
        x.mean(axis=axis, out=gpu_buffer_a_array[:gamma.size],
               dtype=gamma.dtype)
        xp.square(x).mean(axis=axis, out=gpu_buffer_a_array[gamma.size:],
                          dtype=gamma.dtype)
        self.comm._multi_node_mean_nccl(gpu_buffer_a,
                                        gpu_buffer_b,
                                        gpu_buffer_n_elems,
                                        gamma.dtype)
        gpu_buffer_a_array = gpu_buffer_b.array(
            gpu_buffer_n_elems,
            dtype=gamma.dtype)

        mean = gpu_buffer_a_array[:gamma.size]
        sqmean = gpu_buffer_a_array[gamma.size:]
        var = sqmean - xp.square(mean)
        return mean, var

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10182')" href="javascript:;">
chainer-7.2.0/chainermn/functions/batch_normalization.py: 70-95
</a>
<div class="mid" id="frag10182" style="display:none"><pre>
    def get_ggamma_and_gbeta(self, axis, gamma, gy, x_hat, xp):
        gpu_buffer_n_elems = gamma.size * 2
        gpu_buffer_size = gamma.dtype.itemsize * gpu_buffer_n_elems
        gpu_buffer_a = self.memory_utility_module.DeviceMemory()
        gpu_buffer_b = self.memory_utility_module.DeviceMemory()
        gpu_buffer_a.assign(gpu_buffer_size)
        gpu_buffer_b.assign(gpu_buffer_size)
        gpu_buffer_a_array = gpu_buffer_a.array(
            gpu_buffer_n_elems, dtype=gamma.dtype)
        gy.sum(axis=axis, out=gpu_buffer_a_array[:gamma.size],
               dtype=gamma.dtype)
        (gy * x_hat).sum(axis=axis, out=gpu_buffer_a_array[gamma.size:],
                         dtype=gamma.dtype)
        self.comm._multi_node_mean_nccl(gpu_buffer_a,
                                        gpu_buffer_b,
                                        gpu_buffer_n_elems,
                                        gamma.dtype)
        gpu_buffer_a_array = gpu_buffer_b.array(
            gpu_buffer_n_elems,
            dtype=gamma.dtype)

        gbeta = gpu_buffer_a_array[:gamma.size]
        ggamma = gpu_buffer_a_array[gamma.size:]
        return gbeta, ggamma


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 248:</b> &nbsp; 2 fragments, nominal size 89 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10261')" href="javascript:;">
chainer-7.2.0/examples/pix2pix/train_facade.py: 26-137
</a>
<div class="mid" id="frag10261" style="display:none"><pre>
def main():
    parser = argparse.ArgumentParser(
        description='chainer implementation of pix2pix')
    parser.add_argument('--batchsize', '-b', type=int, default=1,
                        help='Number of images in each mini-batch')
    parser.add_argument('--epoch', '-e', type=int, default=200,
                        help='Number of sweeps over the dataset to train')
    parser.add_argument('--device', '-d', type=str, default='-1',
                        help='Device specifier. Either ChainerX device '
                        'specifier or an integer. If non-negative integer, '
                        'CuPy arrays with specified device id are used. If '
                        'negative integer, NumPy arrays are used')
    parser.add_argument('--dataset', '-i', default='./facade/base',
                        help='Directory of image files.')
    parser.add_argument('--out', '-o', default='result',
                        help='Directory to output the result')
    parser.add_argument('--resume', '-r', type=str,
                        help='Resume the training from snapshot')
    parser.add_argument('--seed', type=int, default=0,
                        help='Random seed')
    parser.add_argument('--snapshot_interval', type=int, default=1000,
                        help='Interval of snapshot')
    parser.add_argument('--display_interval', type=int, default=100,
                        help='Interval of displaying log to console')
    group = parser.add_argument_group('deprecated arguments')
    group.add_argument('--gpu', '-g', dest='device',
                       type=int, nargs='?', const=0,
                       help='GPU ID (negative value indicates CPU)')
    args = parser.parse_args()

    if chainer.get_dtype() == numpy.float16:
        warnings.warn(
            'This example may cause NaN in FP16 mode.', RuntimeWarning)

    device = chainer.get_device(args.device)
    if device.xp is chainerx:
        sys.stderr.write('This example does not support ChainerX devices.\n')
        sys.exit(1)

    print('Device: {}'.format(device))
    print('# Minibatch-size: {}'.format(args.batchsize))
    print('# epoch: {}'.format(args.epoch))
    print('')

    device.use()

    # Set up a neural network to train
    enc = Encoder(in_ch=12)
    dec = Decoder(out_ch=3)
    dis = Discriminator(in_ch=12, out_ch=3)

    enc.to_device(device)
    dec.to_device(device)
    dis.to_device(device)

    # Setup an optimizer
    def make_optimizer(model, alpha=0.0002, beta1=0.5):
        optimizer = chainer.optimizers.Adam(alpha=alpha, beta1=beta1)
        optimizer.setup(model)
        optimizer.add_hook(chainer.optimizer.WeightDecay(0.00001), 'hook_dec')
        return optimizer
    opt_enc = make_optimizer(enc)
    opt_dec = make_optimizer(dec)
    opt_dis = make_optimizer(dis)

    train_d = FacadeDataset(args.dataset, data_range=(1, 300))
    test_d = FacadeDataset(args.dataset, data_range=(300, 379))
    train_iter = chainer.iterators.SerialIterator(train_d, args.batchsize)
    test_iter = chainer.iterators.SerialIterator(test_d, args.batchsize)

    # Set up a trainer
    updater = FacadeUpdater(
        models=(enc, dec, dis),
        iterator={
            'main': train_iter,
            'test': test_iter},
        optimizer={
            'enc': opt_enc, 'dec': opt_dec,
            'dis': opt_dis},
        device=device)
    trainer = training.Trainer(updater, (args.epoch, 'epoch'), out=args.out)

    snapshot_interval = (args.snapshot_interval, 'iteration')
    display_interval = (args.display_interval, 'iteration')
    trainer.extend(extensions.snapshot(
        filename='snapshot_iter_{.updater.iteration}.npz'),
        trigger=snapshot_interval)
    trainer.extend(extensions.snapshot_object(
        enc, 'enc_iter_{.updater.iteration}.npz'), trigger=snapshot_interval)
    trainer.extend(extensions.snapshot_object(
        dec, 'dec_iter_{.updater.iteration}.npz'), trigger=snapshot_interval)
    trainer.extend(extensions.snapshot_object(
        dis, 'dis_iter_{.updater.iteration}.npz'), trigger=snapshot_interval)
    trainer.extend(extensions.LogReport(trigger=display_interval))
    trainer.extend(extensions.PrintReport([
        'epoch', 'iteration', 'enc/loss', 'dec/loss', 'dis/loss',
    ]), trigger=display_interval)
    trainer.extend(extensions.ProgressBar(update_interval=10))
    trainer.extend(
        out_image(
            updater, enc, dec,
            5, 5, args.seed, args.out),
        trigger=snapshot_interval)

    if args.resume is not None:
        # Resume from a snapshot
        chainer.serializers.load_npz(args.resume, trainer)

    # Run the training
    trainer.run()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10594')" href="javascript:;">
chainer-7.2.0/examples/dcgan/train_dcgan.py: 18-131
</a>
<div class="mid" id="frag10594" style="display:none"><pre>
def main():
    parser = argparse.ArgumentParser(description='Chainer example: DCGAN')
    parser.add_argument('--batchsize', '-b', type=int, default=50,
                        help='Number of images in each mini-batch')
    parser.add_argument('--epoch', '-e', type=int, default=1000,
                        help='Number of sweeps over the dataset to train')
    parser.add_argument('--device', '-d', type=str, default='-1',
                        help='Device specifier. Either ChainerX device '
                        'specifier or an integer. If non-negative integer, '
                        'CuPy arrays with specified device id are used. If '
                        'negative integer, NumPy arrays are used')
    parser.add_argument('--dataset', '-i', default='',
                        help='Directory of image files.  Default is cifar-10.')
    parser.add_argument('--out', '-o', default='result',
                        help='Directory to output the result')
    parser.add_argument('--resume', '-r', type=str,
                        help='Resume the training from snapshot')
    parser.add_argument('--n_hidden', '-n', type=int, default=100,
                        help='Number of hidden units (z)')
    parser.add_argument('--seed', type=int, default=0,
                        help='Random seed of z at visualization stage')
    parser.add_argument('--snapshot_interval', type=int, default=1000,
                        help='Interval of snapshot')
    parser.add_argument('--display_interval', type=int, default=100,
                        help='Interval of displaying log to console')
    group = parser.add_argument_group('deprecated arguments')
    group.add_argument('--gpu', '-g', dest='device',
                       type=int, nargs='?', const=0,
                       help='GPU ID (negative value indicates CPU)')
    args = parser.parse_args()

    if chainer.get_dtype() == numpy.float16:
        warnings.warn(
            'This example may cause NaN in FP16 mode.', RuntimeWarning)

    device = chainer.get_device(args.device)
    device.use()

    print('Device: {}'.format(device))
    print('# Minibatch-size: {}'.format(args.batchsize))
    print('# n_hidden: {}'.format(args.n_hidden))
    print('# epoch: {}'.format(args.epoch))
    print('')

    # Set up a neural network to train
    gen = Generator(n_hidden=args.n_hidden)
    dis = Discriminator()

    gen.to_device(device)  # Copy the model to the device
    dis.to_device(device)

    # Setup an optimizer
    def make_optimizer(model, alpha=0.0002, beta1=0.5):
        optimizer = chainer.optimizers.Adam(alpha=alpha, beta1=beta1)
        optimizer.setup(model)
        optimizer.add_hook(
            chainer.optimizer_hooks.WeightDecay(0.0001), 'hook_dec')
        return optimizer

    opt_gen = make_optimizer(gen)
    opt_dis = make_optimizer(dis)

    if args.dataset == '':
        # Load the CIFAR10 dataset if args.dataset is not specified
        train, _ = chainer.datasets.get_cifar10(withlabel=False, scale=255.)
    else:
        all_files = os.listdir(args.dataset)
        image_files = [f for f in all_files if ('png' in f or 'jpg' in f)]
        print('{} contains {} image files'
              .format(args.dataset, len(image_files)))
        train = chainer.datasets\
            .ImageDataset(paths=image_files, root=args.dataset)

    # Setup an iterator
    train_iter = chainer.iterators.SerialIterator(train, args.batchsize)

    # Setup an updater
    updater = DCGANUpdater(
        models=(gen, dis),
        iterator=train_iter,
        optimizer={
            'gen': opt_gen, 'dis': opt_dis},
        device=device)

    # Setup a trainer
    trainer = training.Trainer(updater, (args.epoch, 'epoch'), out=args.out)

    snapshot_interval = (args.snapshot_interval, 'iteration')
    display_interval = (args.display_interval, 'iteration')
    trainer.extend(
        extensions.snapshot(filename='snapshot_iter_{.updater.iteration}.npz'),
        trigger=snapshot_interval)
    trainer.extend(extensions.snapshot_object(
        gen, 'gen_iter_{.updater.iteration}.npz'), trigger=snapshot_interval)
    trainer.extend(extensions.snapshot_object(
        dis, 'dis_iter_{.updater.iteration}.npz'), trigger=snapshot_interval)
    trainer.extend(extensions.LogReport(trigger=display_interval))
    trainer.extend(extensions.PrintReport([
        'epoch', 'iteration', 'gen/loss', 'dis/loss',
    ]), trigger=display_interval)
    trainer.extend(extensions.ProgressBar(update_interval=10))
    trainer.extend(
        out_generated_image(
            gen, dis,
            10, 10, args.seed, args.out),
        trigger=snapshot_interval)

    if args.resume is not None:
        # Resume from a snapshot
        chainer.serializers.load_npz(args.resume, trainer)

    # Run the training
    trainer.run()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 249:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10315')" href="javascript:;">
chainer-7.2.0/examples/ptb/train_ptb.py: 82-105
</a>
<div class="mid" id="frag10315" style="display:none"><pre>
    def __next__(self):
        # This iterator returns a list representing a mini-batch. Each item
        # indicates a different position in the original sequence. Each item is
        # represented by a pair of two word IDs. The first word is at the
        # "current" position, while the second word at the next position.
        # At each iteration, the iteration count is incremented, which pushes
        # forward the "current" position.
        length = len(self.dataset)
        if not self.repeat and self.iteration * self.batch_size &gt;= length:
            # If not self.repeat, this iterator stops at the end of the first
            # epoch (i.e., when all words are visited once).
            raise StopIteration
        cur_words = self.get_words()
        self._previous_epoch_detail = self.epoch_detail
        self.iteration += 1
        next_words = self.get_words()

        epoch = self.iteration * self.batch_size // length
        self.is_new_epoch = self.epoch &lt; epoch
        if self.is_new_epoch:
            self.epoch = epoch

        return list(zip(cur_words, next_words))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10491')" href="javascript:;">
chainer-7.2.0/examples/static_graph_optimizations/ptb/train_ptb_custom_loop.py: 120-143
</a>
<div class="mid" id="frag10491" style="display:none"><pre>
    def __next__(self):
        # This iterator returns a list representing a mini-batch. Each item
        # indicates a different position in the original sequence. Each item is
        # represented by a pair of two word IDs. The first word is at the
        # "current" position, while the second word at the next position.
        # At each iteration, the iteration count is incremented, which pushes
        # forward the "current" position.
        length = len(self.dataset)
        if not self.repeat and self.iteration * self.batch_size &gt;= length:
            # If not self.repeat, this iterator stops at the end of the first
            # epoch (i.e., when all words are visited once).
            raise StopIteration
        cur_words = self.get_words()
        self._previous_epoch_detail = self.epoch_detail
        self.iteration += 1
        next_words = self.get_words()

        epoch = self.iteration * self.batch_size // length
        self.is_new_epoch = self.epoch &lt; epoch
        if self.is_new_epoch:
            self.epoch = epoch

        return list(zip(cur_words, next_words))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 250:</b> &nbsp; 2 fragments, nominal size 56 lines, similarity 87%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10326')" href="javascript:;">
chainer-7.2.0/examples/mnist/train_mnist_data_parallel.py: 13-96
</a>
<div class="mid" id="frag10326" style="display:none"><pre>
def main():
    # This script is almost identical to train_mnist.py. The only difference is
    # that this script uses data-parallel computation on two GPUs.
    # See train_mnist.py for more details.
    parser = argparse.ArgumentParser(description='Chainer example: MNIST')
    parser.add_argument('--batchsize', '-b', type=int, default=400,
                        help='Number of images in each mini-batch')
    parser.add_argument('--epoch', '-e', type=int, default=20,
                        help='Number of sweeps over the dataset to train')
    parser.add_argument('--out', '-o', default='result_data_parallel',
                        help='Directory to output the result')
    parser.add_argument('--resume', '-r', default='',
                        help='Resume the training from snapshot')
    parser.add_argument('--unit', '-u', type=int, default=1000,
                        help='Number of units')
    parser.add_argument('--device0', '-d', type=str, default='0',
                        help='Device specifier of the first device. '
                        'Either ChainerX device '
                        'specifier or an integer. If non-negative integer, '
                        'CuPy arrays with specified device id are used. If '
                        'negative integer, NumPy arrays are used')
    parser.add_argument('--device1', '-D', type=str, default='1',
                        help='Device specifier of the second device. '
                        'Either ChainerX device '
                        'specifier or an integer. If non-negative integer, '
                        'CuPy arrays with specified device id are used. If '
                        'negative integer, NumPy arrays are used')
    group = parser.add_argument_group('deprecated arguments')
    group.add_argument('--gpu0', '-g', dest='device0', type=int, nargs='?',
                       const=0,
                       help='First GPU ID')
    group.add_argument('--gpu1', '-G', dest='device1', type=int, nargs='?',
                       const=1,
                       help='Second GPU ID')
    args = parser.parse_args()
    device0 = chainer.get_device(args.device0)
    device1 = chainer.get_device(args.device1)

    print('Devices: {}, {}'.format(device0, device1))
    print('# unit: {}'.format(args.unit))
    print('# Minibatch-size: {}'.format(args.batchsize))
    print('# epoch: {}'.format(args.epoch))
    print('')

    device0.use()

    model = L.Classifier(train_mnist.MLP(args.unit, 10))
    optimizer = chainer.optimizers.Adam()
    optimizer.setup(model)

    train, test = chainer.datasets.get_mnist()
    train_iter = chainer.iterators.SerialIterator(train, args.batchsize)
    test_iter = chainer.iterators.SerialIterator(test, args.batchsize,
                                                 repeat=False, shuffle=False)

    # ParallelUpdater implements the data-parallel gradient computation on
    # multiple devices. It accepts "devices" argument that specifies which
    # device to use.
    updater = training.updaters.ParallelUpdater(
        train_iter,
        optimizer,
        # The device of the name 'main' is used as a "master", while others are
        # used as slaves. Names other than 'main' are arbitrary.
        devices={'main': device0, 'second': device1},
    )
    trainer = training.Trainer(updater, (args.epoch, 'epoch'), out=args.out)

    trainer.extend(extensions.Evaluator(test_iter, model, device=device0))
    # TODO(niboshi): Temporarily disabled for chainerx. Fix it.
    if device0.xp is not chainerx:
        trainer.extend(extensions.DumpGraph('main/loss'))
    trainer.extend(extensions.snapshot(), trigger=(args.epoch, 'epoch'))
    trainer.extend(extensions.LogReport())
    trainer.extend(extensions.PrintReport(
        ['epoch', 'main/loss', 'validation/main/loss',
         'main/accuracy', 'validation/main/accuracy', 'elapsed_time']))
    trainer.extend(extensions.ProgressBar())

    if args.resume:
        chainer.serializers.load_npz(args.resume, trainer)

    trainer.run()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10329')" href="javascript:;">
chainer-7.2.0/examples/mnist/train_mnist_model_parallel.py: 64-139
</a>
<div class="mid" id="frag10329" style="display:none"><pre>
def main():
    parser = argparse.ArgumentParser(description='Chainer example: MNIST')
    parser.add_argument('--batchsize', '-b', type=int, default=100,
                        help='Number of images in each mini-batch')
    parser.add_argument('--epoch', '-e', default=20, type=int,
                        help='Number of sweeps over the dataset to train')
    parser.add_argument('--out', '-o', default='result_model_parallel',
                        help='Directory to output the result')
    parser.add_argument('--resume', '-r', default='',
                        help='Resume the training from snapshot')
    parser.add_argument('--unit', '-u', default=1000, type=int,
                        help='Number of units')
    parser.add_argument('--device0', '-d', type=str, default='0',
                        help='Device specifier of the first device. '
                        'Either ChainerX device '
                        'specifier or an integer. If non-negative integer, '
                        'CuPy arrays with specified device id are used. If '
                        'negative integer, NumPy arrays are used')
    parser.add_argument('--device1', '-D', type=str, default='1',
                        help='Device specifier of the second device. '
                        'Either ChainerX device '
                        'specifier or an integer. If non-negative integer, '
                        'CuPy arrays with specified device id are used. If '
                        'negative integer, NumPy arrays are used')
    group = parser.add_argument_group('deprecated arguments')
    group.add_argument('--gpu0', '-g', dest='device0', type=int, nargs='?',
                       const=0,
                       help='First GPU ID')
    group.add_argument('--gpu1', '-G', dest='device1', type=int, nargs='?',
                       const=1,
                       help='Second GPU ID')
    args = parser.parse_args()
    device0 = chainer.get_device(args.device0)
    device1 = chainer.get_device(args.device1)

    print('Devices: {}, {}'.format(device0, device1))
    print('# unit: {}'.format(args.unit))
    print('# Minibatch-size: {}'.format(args.batchsize))
    print('# epoch: {}'.format(args.epoch))
    print('')

    # See train_mnist.py for the meaning of these lines

    model = L.Classifier(ParallelMLP(args.unit, 10, device0, device1))
    device0.use()

    optimizer = chainer.optimizers.Adam()
    optimizer.setup(model)

    train, test = chainer.datasets.get_mnist()

    train_iter = chainer.iterators.SerialIterator(train, args.batchsize)
    test_iter = chainer.iterators.SerialIterator(test, args.batchsize,
                                                 repeat=False, shuffle=False)

    updater = training.updaters.StandardUpdater(
        train_iter, optimizer, input_device=device0)
    trainer = training.Trainer(updater, (args.epoch, 'epoch'), out=args.out)

    trainer.extend(extensions.Evaluator(test_iter, model, device=device0))
    # TODO(niboshi): Temporarily disabled for chainerx. Fix it.
    if device0.xp is not chainerx:
        trainer.extend(extensions.DumpGraph('main/loss'))
    trainer.extend(extensions.snapshot(), trigger=(args.epoch, 'epoch'))
    trainer.extend(extensions.LogReport())
    trainer.extend(extensions.PrintReport(
        ['epoch', 'main/loss', 'validation/main/loss',
         'main/accuracy', 'validation/main/accuracy', 'elapsed_time']))
    trainer.extend(extensions.ProgressBar())

    if args.resume:
        chainer.serializers.load_npz(args.resume, trainer)

    trainer.run()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 251:</b> &nbsp; 2 fragments, nominal size 67 lines, similarity 98%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10333')" href="javascript:;">
chainer-7.2.0/examples/mnist/.testdata/replacements/train_mnist.py: 32-148
</a>
<div class="mid" id="frag10333" style="display:none"><pre>
def main():
    parser = argparse.ArgumentParser(description='Chainer example: MNIST')
    parser.add_argument('--batchsize', '-b', type=int, default=100,
                        help='Number of images in each mini-batch')
    parser.add_argument('--epoch', '-e', type=int, default=20,
                        help='Number of sweeps over the dataset to train')
    parser.add_argument('--frequency', '-f', type=int, default=-1,
                        help='Frequency of taking a snapshot')
    parser.add_argument('--device', '-d', type=str, default='-1',
                        help='Device specifier. Either ChainerX device '
                        'specifier or an integer. If non-negative integer, '
                        'CuPy arrays with specified device id are used. If '
                        'negative integer, NumPy arrays are used')
    parser.add_argument('--out', '-o', default='result',
                        help='Directory to output the result')
    parser.add_argument('--resume', '-r', type=str,
                        help='Resume the training from snapshot')
    parser.add_argument('--autoload', action='store_true',
                        help='Automatically load trainer snapshots in case'
                        ' of preemption or other temporary system failure')
    parser.add_argument('--unit', '-u', type=int, default=1000,
                        help='Number of units')
    group = parser.add_argument_group('deprecated arguments')
    group.add_argument('--gpu', '-g', dest='device',
                       type=int, nargs='?', const=0,
                       help='GPU ID (negative value indicates CPU)')
    args = parser.parse_args()

    device = chainer.get_device(args.device)

    print('Device: {}'.format(device))
    print('# unit: {}'.format(args.unit))
    print('# Minibatch-size: {}'.format(args.batchsize))
    print('# epoch: {}'.format(args.epoch))
    print('')

    # Set up a neural network to train
    # Classifier reports softmax cross entropy loss and accuracy at every
    # iteration, which will be used by the PrintReport extension below.
    model = L.Classifier(MLP(args.unit, 10))
    model.to_device(device)
    device.use()

    # Setup an optimizer
    optimizer = chainer.optimizers.Adam()
    optimizer.setup(model)

    # Load the MNIST dataset
    train, test = chainer.datasets.get_mnist()

    train_iter = chainer.iterators.SerialIterator(train, args.batchsize)
    test_iter = chainer.iterators.SerialIterator(test, args.batchsize,
                                                 repeat=False, shuffle=False)

    # Set up a trainer
    updater = training.updaters.StandardUpdater(
        train_iter, optimizer, device=device)
    trainer = training.Trainer(updater, (args.epoch, 'epoch'), out=args.out)

    # Evaluate the model with the test dataset for each epoch
    trainer.extend(extensions.Evaluator(test_iter, model, device=device),
                   call_before_training=True)

    # Dump a computational graph from 'loss' variable at the first iteration
    # The "main" refers to the target link of the "main" optimizer.
    # TODO(niboshi): Temporarily disabled for chainerx. Fix it.
    if device.xp is not chainerx:
        trainer.extend(extensions.DumpGraph('main/loss'))

    # Take a snapshot for each specified epoch
    frequency = args.epoch if args.frequency == -1 else max(1, args.frequency)
    # Take a snapshot each ``frequency`` epoch, delete old stale
    # snapshots and automatically load from snapshot files if any
    # files are already resident at result directory.
    trainer.extend(extensions.snapshot(n_retains=1, autoload=args.autoload),
                   trigger=(frequency, 'epoch'))

    # Write a log of evaluation statistics for each epoch
    trainer.extend(extensions.LogReport(), call_before_training=True)

    # Save two plot images to the result dir
    trainer.extend(
        extensions.PlotReport(['main/loss', 'validation/main/loss'],
                              'epoch', file_name='loss.png'),
        call_before_training=True)
    trainer.extend(
        extensions.PlotReport(
            ['main/accuracy', 'validation/main/accuracy'],
            'epoch', file_name='accuracy.png'),
        call_before_training=True)

    # Print selected entries of the log to stdout
    # Here "main" refers to the target link of the "main" optimizer again, and
    # "validation" refers to the default name of the Evaluator extension.
    # Entries other than 'epoch' are reported by the Classifier link, called by
    # either the updater or the evaluator.
    trainer.extend(extensions.PrintReport(
        ['epoch', 'main/loss', 'validation/main/loss',
         'main/accuracy', 'validation/main/accuracy', 'elapsed_time']),
        call_before_training=True)

    # Print a progress bar to stdout
    trainer.extend(extensions.ProgressBar())

    if args.resume is not None:
        # Resume from a snapshot (Note: this loaded model is to be
        # overwritten by --autoload option, autoloading snapshots, if
        # any snapshots exist in output directory)
        chainer.serializers.load_npz(args.resume, trainer)

    # BEGIN ADDITIONAL TEST CODE
    del trainer._extensions['ProgressBar']
    # END ADDITIONAL TEST CODE
    # Run the training
    trainer.run()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10336')" href="javascript:;">
chainer-7.2.0/examples/mnist/train_mnist.py: 32-145
</a>
<div class="mid" id="frag10336" style="display:none"><pre>
def main():
    parser = argparse.ArgumentParser(description='Chainer example: MNIST')
    parser.add_argument('--batchsize', '-b', type=int, default=100,
                        help='Number of images in each mini-batch')
    parser.add_argument('--epoch', '-e', type=int, default=20,
                        help='Number of sweeps over the dataset to train')
    parser.add_argument('--frequency', '-f', type=int, default=-1,
                        help='Frequency of taking a snapshot')
    parser.add_argument('--device', '-d', type=str, default='-1',
                        help='Device specifier. Either ChainerX device '
                        'specifier or an integer. If non-negative integer, '
                        'CuPy arrays with specified device id are used. If '
                        'negative integer, NumPy arrays are used')
    parser.add_argument('--out', '-o', default='result',
                        help='Directory to output the result')
    parser.add_argument('--resume', '-r', type=str,
                        help='Resume the training from snapshot')
    parser.add_argument('--autoload', action='store_true',
                        help='Automatically load trainer snapshots in case'
                        ' of preemption or other temporary system failure')
    parser.add_argument('--unit', '-u', type=int, default=1000,
                        help='Number of units')
    group = parser.add_argument_group('deprecated arguments')
    group.add_argument('--gpu', '-g', dest='device',
                       type=int, nargs='?', const=0,
                       help='GPU ID (negative value indicates CPU)')
    args = parser.parse_args()

    device = chainer.get_device(args.device)

    print('Device: {}'.format(device))
    print('# unit: {}'.format(args.unit))
    print('# Minibatch-size: {}'.format(args.batchsize))
    print('# epoch: {}'.format(args.epoch))
    print('')

    # Set up a neural network to train
    # Classifier reports softmax cross entropy loss and accuracy at every
    # iteration, which will be used by the PrintReport extension below.
    model = L.Classifier(MLP(args.unit, 10))
    model.to_device(device)
    device.use()

    # Setup an optimizer
    optimizer = chainer.optimizers.Adam()
    optimizer.setup(model)

    # Load the MNIST dataset
    train, test = chainer.datasets.get_mnist()

    train_iter = chainer.iterators.SerialIterator(train, args.batchsize)
    test_iter = chainer.iterators.SerialIterator(test, args.batchsize,
                                                 repeat=False, shuffle=False)

    # Set up a trainer
    updater = training.updaters.StandardUpdater(
        train_iter, optimizer, device=device)
    trainer = training.Trainer(updater, (args.epoch, 'epoch'), out=args.out)

    # Evaluate the model with the test dataset for each epoch
    trainer.extend(extensions.Evaluator(test_iter, model, device=device),
                   call_before_training=True)

    # Dump a computational graph from 'loss' variable at the first iteration
    # The "main" refers to the target link of the "main" optimizer.
    # TODO(niboshi): Temporarily disabled for chainerx. Fix it.
    if device.xp is not chainerx:
        trainer.extend(extensions.DumpGraph('main/loss'))

    # Take a snapshot for each specified epoch
    frequency = args.epoch if args.frequency == -1 else max(1, args.frequency)
    # Take a snapshot each ``frequency`` epoch, delete old stale
    # snapshots and automatically load from snapshot files if any
    # files are already resident at result directory.
    trainer.extend(extensions.snapshot(n_retains=1, autoload=args.autoload),
                   trigger=(frequency, 'epoch'))

    # Write a log of evaluation statistics for each epoch
    trainer.extend(extensions.LogReport(), call_before_training=True)

    # Save two plot images to the result dir
    trainer.extend(
        extensions.PlotReport(['main/loss', 'validation/main/loss'],
                              'epoch', file_name='loss.png'),
        call_before_training=True)
    trainer.extend(
        extensions.PlotReport(
            ['main/accuracy', 'validation/main/accuracy'],
            'epoch', file_name='accuracy.png'),
        call_before_training=True)

    # Print selected entries of the log to stdout
    # Here "main" refers to the target link of the "main" optimizer again, and
    # "validation" refers to the default name of the Evaluator extension.
    # Entries other than 'epoch' are reported by the Classifier link, called by
    # either the updater or the evaluator.
    trainer.extend(extensions.PrintReport(
        ['epoch', 'main/loss', 'validation/main/loss',
         'main/accuracy', 'validation/main/accuracy', 'elapsed_time']),
        call_before_training=True)

    # Print a progress bar to stdout
    trainer.extend(extensions.ProgressBar())

    if args.resume is not None:
        # Resume from a snapshot (Note: this loaded model is to be
        # overwritten by --autoload option, autoloading snapshots, if
        # any snapshots exist in output directory)
        chainer.serializers.load_npz(args.resume, trainer)

    # Run the training
    trainer.run()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 252:</b> &nbsp; 2 fragments, nominal size 89 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10337')" href="javascript:;">
chainer-7.2.0/examples/mnist/train_mnist_custom_loop.py: 22-130
</a>
<div class="mid" id="frag10337" style="display:none"><pre>
def main():
    parser = argparse.ArgumentParser(description='Chainer example: MNIST')
    parser.add_argument('--batchsize', '-b', type=int, default=100,
                        help='Number of images in each mini-batch')
    parser.add_argument('--epoch', '-e', type=int, default=20,
                        help='Number of sweeps over the dataset to train')
    parser.add_argument('--device', '-d', type=str, default='-1',
                        help='Device specifier. Either ChainerX device '
                        'specifier or an integer. If non-negative integer, '
                        'CuPy arrays with specified device id are used. If '
                        'negative integer, NumPy arrays are used')
    parser.add_argument('--out', '-o', default='result',
                        help='Directory to output the result')
    parser.add_argument('--resume', '-r', type=str,
                        help='Resume the training from snapshot using model '
                             'and state files in the specified directory')
    parser.add_argument('--unit', '-u', type=int, default=1000,
                        help='Number of units')
    group = parser.add_argument_group('deprecated arguments')
    group.add_argument('--gpu', '-g', dest='device',
                       type=int, nargs='?', const=0,
                       help='GPU ID (negative value indicates CPU)')
    args = parser.parse_args()

    device = chainer.get_device(args.device)

    print('Device: {}'.format(device))
    print('# unit: {}'.format(args.unit))
    print('# Minibatch-size: {}'.format(args.batchsize))
    print('# epoch: {}'.format(args.epoch))
    print('')

    # Set up a neural network to train
    model = L.Classifier(train_mnist.MLP(args.unit, 10))
    model.to_device(device)
    device.use()

    # Setup an optimizer
    optimizer = chainer.optimizers.Adam()
    optimizer.setup(model)

    if args.resume is not None:
        # Resume from a snapshot
        resume = args.resume
        if os.path.exists(resume):
            serializers.load_npz(os.path.join(resume, 'mlp.model'), model)
            serializers.load_npz(os.path.join(resume, 'mlp.state'), optimizer)
        else:
            raise ValueError(
                '`args.resume` ("{}") is specified,'
                ' but it does not exist'.format(resume)
            )

    # Load the MNIST dataset
    train, test = chainer.datasets.get_mnist()

    test_count = len(test)

    train_iter = chainer.iterators.SerialIterator(train, args.batchsize)
    test_iter = chainer.iterators.SerialIterator(test, args.batchsize,
                                                 repeat=False, shuffle=False)

    train_count = 0
    sum_accuracy = 0
    sum_loss = 0

    while train_iter.epoch &lt; args.epoch:
        batch = train_iter.next()
        x, t = convert.concat_examples(batch, device)
        optimizer.update(model, x, t)
        train_count += len(t)
        sum_loss += float(model.loss.array) * len(t)
        sum_accuracy += float(model.accuracy.array) * len(t)

        if train_iter.is_new_epoch:
            print('epoch: {}'.format(train_iter.epoch))
            print('train mean loss: {}, accuracy: {}'.format(
                sum_loss / train_count, sum_accuracy / train_count))
            # evaluation
            train_count = 0
            sum_accuracy = 0
            sum_loss = 0
            # Enable evaluation mode.
            with configuration.using_config('train', False):
                # This is optional but can reduce computational overhead.
                with chainer.using_config('enable_backprop', False):
                    for batch in test_iter:
                        x, t = convert.concat_examples(batch, device)
                        loss = model(x, t)
                        sum_loss += float(loss.array) * len(t)
                        sum_accuracy += float(
                            model.accuracy.array) * len(t)

            test_iter.reset()
            print('test mean  loss: {}, accuracy: {}'.format(
                sum_loss / test_count, sum_accuracy / test_count))
            sum_accuracy = 0
            sum_loss = 0

    # Save the model and the optimizer
    out = args.out
    if not os.path.isdir(out):
        os.makedirs(out)
    print('save the model')
    serializers.save_npz(os.path.join(out, 'mlp.model'), model)
    print('save the optimizer')
    serializers.save_npz(os.path.join(out, 'mlp.state'), optimizer)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10357')" href="javascript:;">
chainer-7.2.0/examples/cifar/train_cifar_custom_loop.py: 24-150
</a>
<div class="mid" id="frag10357" style="display:none"><pre>
def main():
    parser = argparse.ArgumentParser(description='Chainer CIFAR example:')
    parser.add_argument('--dataset', default='cifar10',
                        help='The dataset to use: cifar10 or cifar100')
    parser.add_argument('--batchsize', '-b', type=int, default=64,
                        help='Number of images in each mini-batch')
    parser.add_argument('--learnrate', '-l', type=float, default=0.05,
                        help='Learning rate for SGD')
    parser.add_argument('--epoch', '-e', type=int, default=300,
                        help='Number of sweeps over the dataset to train')
    parser.add_argument('--device', '-d', type=str, default='-1',
                        help='Device specifier. Either ChainerX device '
                        'specifier or an integer. If non-negative integer, '
                        'CuPy arrays with specified device id are used. If '
                        'negative integer, NumPy arrays are used')
    parser.add_argument('--out', '-o', default='result',
                        help='Directory to output the result')
    parser.add_argument('--test', action='store_true',
                        help='Use tiny datasets for quick tests')
    parser.add_argument('--resume', '-r', type=str,
                        help='Directory that has `vgg.model` and `vgg.state`')
    group = parser.add_argument_group('deprecated arguments')
    group.add_argument('--gpu', '-g', dest='device',
                       type=int, nargs='?', const=0,
                       help='GPU ID (negative value indicates CPU)')
    args = parser.parse_args()

    device = chainer.get_device(args.device)
    device.use()

    print('Device: {}'.format(device))
    print('# Minibatch-size: {}'.format(args.batchsize))
    print('# epoch: {}'.format(args.epoch))
    print('')

    # Set up a neural network to train.
    # Classifier reports softmax cross entropy loss and accuracy at every
    # iteration, which will be used by the PrintReport extension below.
    if args.dataset == 'cifar10':
        print('Using CIFAR10 dataset.')
        class_labels = 10
        train, test = get_cifar10()
    elif args.dataset == 'cifar100':
        print('Using CIFAR100 dataset.')
        class_labels = 100
        train, test = get_cifar100()
    else:
        raise RuntimeError('Invalid dataset choice.')

    if args.test:
        train = train[:200]
        test = test[:200]

    test_count = len(test)

    model = L.Classifier(models.VGG.VGG(class_labels))
    model.to_device(device)

    optimizer = chainer.optimizers.MomentumSGD(args.learnrate)
    optimizer.setup(model)
    optimizer.add_hook(chainer.optimizer.WeightDecay(5e-4))

    if args.resume is not None:
        resume = args.resume
        if os.path.exists(resume):
            serializers.load_npz(os.path.join(resume, 'vgg.model'), model)
            serializers.load_npz(os.path.join(resume, 'vgg.state'), optimizer)
        else:
            raise ValueError(
                '`args.resume` ("{}") is specified,'
                ' but it does not exist.'.format(resume)
            )

    train_iter = chainer.iterators.SerialIterator(train, args.batchsize)
    test_iter = chainer.iterators.SerialIterator(test, args.batchsize,
                                                 repeat=False, shuffle=False)

    train_count = 0
    sum_acc = 0
    sum_loss = 0

    while train_iter.epoch &lt; args.epoch:
        batch = train_iter.next()
        # Reduce learning rate by 0.5 every 25 epochs.
        if train_iter.epoch % 25 == 0 and train_iter.is_new_epoch:
            optimizer.lr *= 0.5
            print('Reducing learning rate to: {}'.format(optimizer.lr))

        x, t = convert.concat_examples(batch, device)
        optimizer.update(model, x, t)
        train_count += len(t)
        sum_loss += float(model.loss.array) * len(t)
        sum_acc += float(model.accuracy.array) * len(t)

        if train_iter.is_new_epoch:
            print('epoch: {}'.format(train_iter.epoch))
            print('train mean loss: {}, accuracy: {}'.format(
                sum_loss / train_count, sum_acc / train_count))
            train_count = 0
            sum_acc = 0
            sum_loss = 0
            # Enable evaluation mode.
            with configuration.using_config('train', False):
                # This is optional but can reduce computational overhead.
                with chainer.using_config('enable_backprop', False):
                    for batch in test_iter:
                        x, t = convert.concat_examples(batch, device)
                        loss = model(x, t)
                        sum_loss += float(loss.array) * len(t)
                        sum_acc += float(model.accuracy.array) * len(t)

            test_iter.reset()
            print('test mean  loss: {}, accuracy: {}'.format(
                sum_loss / test_count, sum_acc / test_count))
            sum_acc = 0
            sum_loss = 0

    # Save the model and the optimizer
    out = args.out
    if not os.path.exists(out):
        os.makedirs(out)
    print('save the model')
    serializers.save_npz(os.path.join(out, 'vgg.model'), model)
    print('save the optimizer')
    serializers.save_npz(os.path.join(out, 'vgg.state'), optimizer)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 253:</b> &nbsp; 2 fragments, nominal size 74 lines, similarity 84%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10356')" href="javascript:;">
chainer-7.2.0/examples/cifar/train_cifar.py: 16-126
</a>
<div class="mid" id="frag10356" style="display:none"><pre>
def main():
    parser = argparse.ArgumentParser(description='Chainer CIFAR example:')
    parser.add_argument('--dataset', default='cifar10',
                        help='The dataset to use: cifar10 or cifar100')
    parser.add_argument('--batchsize', '-b', type=int, default=64,
                        help='Number of images in each mini-batch')
    parser.add_argument('--learnrate', '-l', type=float, default=0.05,
                        help='Learning rate for SGD')
    parser.add_argument('--epoch', '-e', type=int, default=300,
                        help='Number of sweeps over the dataset to train')
    parser.add_argument('--device', '-d', type=str, default='-1',
                        help='Device specifier. Either ChainerX device '
                        'specifier or an integer. If non-negative integer, '
                        'CuPy arrays with specified device id are used. If '
                        'negative integer, NumPy arrays are used')
    parser.add_argument('--out', '-o', default='result',
                        help='Directory to output the result')
    parser.add_argument('--resume', '-r', default='',
                        help='Resume the training from snapshot')
    parser.add_argument('--early-stopping', type=str,
                        help='Metric to watch for early stopping')
    group = parser.add_argument_group('deprecated arguments')
    group.add_argument('--gpu', '-g', dest='device',
                       type=int, nargs='?', const=0,
                       help='GPU ID (negative value indicates CPU)')
    args = parser.parse_args()

    device = chainer.get_device(args.device)
    device.use()

    print('Device: {}'.format(device))
    print('# Minibatch-size: {}'.format(args.batchsize))
    print('# epoch: {}'.format(args.epoch))
    print('')

    # Set up a neural network to train.
    # Classifier reports softmax cross entropy loss and accuracy at every
    # iteration, which will be used by the PrintReport extension below.
    if args.dataset == 'cifar10':
        print('Using CIFAR10 dataset.')
        class_labels = 10
        train, test = get_cifar10()
    elif args.dataset == 'cifar100':
        print('Using CIFAR100 dataset.')
        class_labels = 100
        train, test = get_cifar100()
    else:
        raise RuntimeError('Invalid dataset choice.')
    model = L.Classifier(models.VGG.VGG(class_labels))
    model.to_device(device)

    optimizer = chainer.optimizers.MomentumSGD(args.learnrate)
    optimizer.setup(model)
    optimizer.add_hook(chainer.optimizer_hooks.WeightDecay(5e-4))

    train_iter = chainer.iterators.SerialIterator(train, args.batchsize)
    test_iter = chainer.iterators.SerialIterator(test, args.batchsize,
                                                 repeat=False, shuffle=False)

    stop_trigger = (args.epoch, 'epoch')
    # Early stopping option
    if args.early_stopping:
        stop_trigger = triggers.EarlyStoppingTrigger(
            monitor=args.early_stopping, verbose=True,
            max_trigger=(args.epoch, 'epoch'))

    # Set up a trainer
    updater = training.updaters.StandardUpdater(
        train_iter, optimizer, device=device)
    trainer = training.Trainer(updater, stop_trigger, out=args.out)

    # Evaluate the model with the test dataset for each epoch
    trainer.extend(extensions.Evaluator(test_iter, model, device=device))

    # Reduce the learning rate by half every 25 epochs.
    trainer.extend(extensions.ExponentialShift('lr', 0.5),
                   trigger=(25, 'epoch'))

    # Dump a computational graph from 'loss' variable at the first iteration
    # The "main" refers to the target link of the "main" optimizer.
    # TODO(imanishi): Support for ChainerX
    if not isinstance(device, backend.ChainerxDevice):
        trainer.extend(extensions.DumpGraph('main/loss'))

    # Take a snapshot at each epoch
    trainer.extend(extensions.snapshot(
        filename='snaphot_epoch_{.updater.epoch}'))

    # Write a log of evaluation statistics for each epoch
    trainer.extend(extensions.LogReport())

    # Print selected entries of the log to stdout
    # Here "main" refers to the target link of the "main" optimizer again, and
    # "validation" refers to the default name of the Evaluator extension.
    # Entries other than 'epoch' are reported by the Classifier link, called by
    # either the updater or the evaluator.
    trainer.extend(extensions.PrintReport(
        ['epoch', 'main/loss', 'validation/main/loss',
         'main/accuracy', 'validation/main/accuracy', 'elapsed_time']))

    # Print a progress bar to stdout
    trainer.extend(extensions.ProgressBar())

    if args.resume:
        # Resume from a snapshot
        chainer.serializers.load_npz(args.resume, trainer)

    # Run the training
    trainer.run()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10506')" href="javascript:;">
chainer-7.2.0/examples/static_graph_optimizations/cifar/train_cifar.py: 26-147
</a>
<div class="mid" id="frag10506" style="display:none"><pre>
def main():
    parser = argparse.ArgumentParser(description='Chainer CIFAR example:')
    parser.add_argument('--dataset', default='cifar10',
                        help='The dataset to use: cifar10 or cifar100')
    parser.add_argument('--batchsize', '-b', type=int, default=64,
                        help='Number of images in each mini-batch')
    parser.add_argument('--learnrate', '-l', type=float, default=0.05,
                        help='Learning rate for SGD')
    parser.add_argument('--epoch', '-e', type=int, default=300,
                        help='Number of sweeps over the dataset to train')
    parser.add_argument('--device', '-d', type=str, default='0',
                        help='Device specifier. Either ChainerX device '
                        'specifier or an integer. If non-negative integer, '
                        'CuPy arrays with specified device id are used. If '
                        'negative integer, NumPy arrays are used')
    parser.add_argument('--out', '-o', default='result',
                        help='Directory to output the result')
    parser.add_argument('--resume', '-r', default='',
                        help='Resume the training from snapshot')
    parser.add_argument('--early-stopping', type=str,
                        help='Metric to watch for early stopping')
    group = parser.add_argument_group('deprecated arguments')
    group.add_argument('--gpu', '-g', dest='device',
                       type=int, nargs='?', const=0,
                       help='GPU ID (negative value indicates CPU)')
    args = parser.parse_args()

    if chainer.get_dtype() == numpy.float16:
        warnings.warn(
            'This example may cause NaN in FP16 mode.', RuntimeWarning)

    device = chainer.get_device(args.device)

    print('Device: {}'.format(device))
    print('# Minibatch-size: {}'.format(args.batchsize))
    print('# epoch: {}'.format(args.epoch))
    print('')

    device.use()

    # Set up a neural network to train.
    # Classifier reports softmax cross entropy loss and accuracy at every
    # iteration, which will be used by the PrintReport extension below.
    if args.dataset == 'cifar10':
        print('Using CIFAR10 dataset.')
        class_labels = 10
        train, test = get_cifar10()
    elif args.dataset == 'cifar100':
        print('Using CIFAR100 dataset.')
        class_labels = 100
        train, test = get_cifar100()
    else:
        raise RuntimeError('Invalid dataset choice.')
    model = L.Classifier(models.VGG.VGG(class_labels))
    model.to_device(device)

    optimizer = chainer.optimizers.MomentumSGD(args.learnrate)
    optimizer.setup(model)
    optimizer.add_hook(chainer.optimizer_hooks.WeightDecay(5e-4))

    train_iter = chainer.iterators.SerialIterator(train, args.batchsize)
    test_iter = chainer.iterators.SerialIterator(test, args.batchsize,
                                                 repeat=False, shuffle=False)

    stop_trigger = (args.epoch, 'epoch')
    # Early stopping option
    if args.early_stopping:
        stop_trigger = triggers.EarlyStoppingTrigger(
            monitor=args.early_stopping, verbose=True,
            max_trigger=(args.epoch, 'epoch'))

    # Set up a trainer
    updater = training.updaters.StandardUpdater(
        train_iter, optimizer, device=device)
    trainer = training.Trainer(updater, stop_trigger, out=args.out)

    # Evaluate the model with the test dataset for each epoch
    trainer.extend(extensions.Evaluator(test_iter, model, device=device))

    # Reduce the learning rate by half every 25 epochs.
    trainer.extend(extensions.ExponentialShift('lr', 0.5),
                   trigger=(25, 'epoch'))

    # Dump a computational graph from 'loss' variable at the first iteration
    # The "main" refers to the target link of the "main" optimizer.
    # TODO(hvy): Support ChainerX
    if device.xp is not chainerx:
        trainer.extend(extensions.DumpGraph('main/loss'))

    # Take a snapshot at each epoch
    trainer.extend(extensions.snapshot(), trigger=(args.epoch, 'epoch'))

    # Write a log of evaluation statistics for each epoch
    trainer.extend(extensions.LogReport())

    # Print selected entries of the log to stdout
    # Here "main" refers to the target link of the "main" optimizer again, and
    # "validation" refers to the default name of the Evaluator extension.
    # Entries other than 'epoch' are reported by the Classifier link, called by
    # either the updater or the evaluator.
    trainer.extend(extensions.PrintReport(
        ['epoch', 'main/loss', 'validation/main/loss',
         'main/accuracy', 'validation/main/accuracy', 'elapsed_time']))

    # Print a progress bar to stdout
    trainer.extend(extensions.ProgressBar())

    if args.resume:
        # Resume from a snapshot
        chainer.serializers.load_npz(args.resume, trainer)

    # Run the training
    if device.xp is not chainerx:
        trainer.run()
    else:
        warnings.warn(
            'Static subgraph optimization does not support ChainerX and will'
            ' be disabled.', UserWarning)
        with chainer.using_config('use_static_graph', False):
            trainer.run()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 254:</b> &nbsp; 2 fragments, nominal size 19 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10360')" href="javascript:;">
chainer-7.2.0/examples/cifar/models/VGG.py: 60-79
</a>
<div class="mid" id="frag10360" style="display:none"><pre>
    def __init__(self, class_labels=10):
        super(VGG, self).__init__()
        with self.init_scope():
            self.block1_1 = Block(64, 3)
            self.block1_2 = Block(64, 3)
            self.block2_1 = Block(128, 3)
            self.block2_2 = Block(128, 3)
            self.block3_1 = Block(256, 3)
            self.block3_2 = Block(256, 3)
            self.block3_3 = Block(256, 3)
            self.block4_1 = Block(512, 3)
            self.block4_2 = Block(512, 3)
            self.block4_3 = Block(512, 3)
            self.block5_1 = Block(512, 3)
            self.block5_2 = Block(512, 3)
            self.block5_3 = Block(512, 3)
            self.fc1 = L.Linear(None, 512, nobias=True)
            self.bn_fc1 = L.BatchNormalization(512)
            self.fc2 = L.Linear(None, class_labels, nobias=True)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10511')" href="javascript:;">
chainer-7.2.0/examples/static_graph_optimizations/cifar/models/VGG.py: 61-80
</a>
<div class="mid" id="frag10511" style="display:none"><pre>
    def __init__(self, class_labels=10):
        super(VGG, self).__init__()
        with self.init_scope():
            self.block1_1 = Block(64, 3)
            self.block1_2 = Block(64, 3)
            self.block2_1 = Block(128, 3)
            self.block2_2 = Block(128, 3)
            self.block3_1 = Block(256, 3)
            self.block3_2 = Block(256, 3)
            self.block3_3 = Block(256, 3)
            self.block4_1 = Block(512, 3)
            self.block4_2 = Block(512, 3)
            self.block4_3 = Block(512, 3)
            self.block5_1 = Block(512, 3)
            self.block5_2 = Block(512, 3)
            self.block5_3 = Block(512, 3)
            self.fc1 = L.Linear(None, 512, nobias=True)
            self.bn_fc1 = L.BatchNormalization(512)
            self.fc2 = L.Linear(None, class_labels, nobias=True)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 255:</b> &nbsp; 4 fragments, nominal size 33 lines, similarity 94%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10361')" href="javascript:;">
chainer-7.2.0/examples/cifar/models/VGG.py: 80-122
</a>
<div class="mid" id="frag10361" style="display:none"><pre>
    def forward(self, x):
        # 64 channel blocks:
        h = self.block1_1(x)
        h = F.dropout(h, ratio=0.3)
        h = self.block1_2(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 128 channel blocks:
        h = self.block2_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block2_2(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 256 channel blocks:
        h = self.block3_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block3_2(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block3_3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 512 channel blocks:
        h = self.block4_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block4_2(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block4_3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 512 channel blocks:
        h = self.block5_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block5_2(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block5_3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        h = F.dropout(h, ratio=0.5)
        h = self.fc1(h)
        h = self.bn_fc1(h)
        h = F.relu(h)
        h = F.dropout(h, ratio=0.5)
        return self.fc2(h)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10433')" href="javascript:;">
chainer-7.2.0/examples/chainermn/parallel_convolution/VGG.py: 133-177
</a>
<div class="mid" id="frag10433" style="display:none"><pre>
    def __call__(self, x):
        # 64 channel blocks:
        h = self.block1_1(x)
        h = F.dropout(h, ratio=0.3)
        h = self.block1_2(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 128 channel blocks:
        h = self.block2_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block2_2(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 256 channel blocks:
        h = self.block3_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block3_2(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block3_3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 512 channel blocks:
        h = self.block4_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block4_2(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block4_3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 512 channel blocks:
        h = self.block5_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block5_2(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block5_3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        h = F.dropout(h, ratio=0.5)
        h = self.fc1(h)
        h = self.bn_fc1(h)
        h = F.relu(h)
        h = F.dropout(h, ratio=0.5)
        h = self.fc2(h)

        return h
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10422')" href="javascript:;">
chainer-7.2.0/examples/chainermn/cifar/models/VGG.py: 93-135
</a>
<div class="mid" id="frag10422" style="display:none"><pre>
    def forward(self, x):
        # 64 channel blocks:
        h = self.block1_1(x)
        h = F.dropout(h, ratio=0.3)
        h = self.block1_2(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 128 channel blocks:
        h = self.block2_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block2_2(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 256 channel blocks:
        h = self.block3_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block3_2(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block3_3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 512 channel blocks:
        h = self.block4_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block4_2(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block4_3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 512 channel blocks:
        h = self.block5_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block5_2(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block5_3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        h = F.dropout(h, ratio=0.5)
        h = self.fc1(h)
        h = self.bn_fc1(h)
        h = F.relu(h)
        h = F.dropout(h, ratio=0.5)
        return self.fc2(h)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10512')" href="javascript:;">
chainer-7.2.0/examples/static_graph_optimizations/cifar/models/VGG.py: 82-124
</a>
<div class="mid" id="frag10512" style="display:none"><pre>
    def __call__(self, x):
        # 64 channel blocks:
        h = self.block1_1(x)
        h = F.dropout(h, ratio=0.3)
        h = self.block1_2(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 128 channel blocks:
        h = self.block2_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block2_2(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 256 channel blocks:
        h = self.block3_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block3_2(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block3_3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 512 channel blocks:
        h = self.block4_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block4_2(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block4_3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 512 channel blocks:
        h = self.block5_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block5_2(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block5_3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        h = F.dropout(h, ratio=0.5)
        h = self.fc1(h)
        h = self.bn_fc1(h)
        h = F.relu(h)
        h = F.dropout(h, ratio=0.5)
        return self.fc2(h)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 256:</b> &nbsp; 2 fragments, nominal size 66 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10405')" href="javascript:;">
chainer-7.2.0/examples/chainermn/mnist/train_mnist_model_parallel.py: 64-138
</a>
<div class="mid" id="frag10405" style="display:none"><pre>
def main():
    parser = argparse.ArgumentParser(
        description='ChainerMN example: pipelined neural network')
    parser.add_argument('--batchsize', '-b', type=int, default=100,
                        help='Number of images in each mini-batch')
    parser.add_argument('--epoch', '-e', type=int, default=20,
                        help='Number of sweeps over the dataset to train')
    parser.add_argument('--gpu', '-g', action='store_true',
                        help='Use GPU')
    parser.add_argument('--out', '-o', default='result',
                        help='Directory to output the result')
    parser.add_argument('--unit', '-u', type=int, default=1000,
                        help='Number of units')
    args = parser.parse_args()

    # Prepare ChainerMN communicator.
    if args.gpu:
        comm = chainermn.create_communicator('pure_nccl')
        device = comm.intra_rank
    else:
        comm = chainermn.create_communicator('naive')
        device = -1

    if comm.size != 2:
        raise ValueError(
            'This example can only be executed on exactly 2 processes.')

    if comm.rank == 0:
        print('==========================================')
        if args.gpu:
            print('Using GPUs')
        print('Num unit: {}'.format(args.unit))
        print('Num Minibatch-size: {}'.format(args.batchsize))
        print('Num epoch: {}'.format(args.epoch))
        print('==========================================')

    if comm.rank == 0:
        model = L.Classifier(MLP0(comm, args.unit))
    elif comm.rank == 1:
        model = MLP1(comm, args.unit, 10)

    if device &gt;= 0:
        chainer.cuda.get_device_from_id(device).use()
        model.to_gpu()

    optimizer = chainer.optimizers.Adam()
    optimizer.setup(model)

    # Iterate dataset only on worker 0.
    train, test = chainer.datasets.get_mnist()
    if comm.rank == 1:
        train = chainermn.datasets.create_empty_dataset(train)
        test = chainermn.datasets.create_empty_dataset(test)

    train_iter = chainer.iterators.SerialIterator(
        train, args.batchsize, shuffle=False)
    test_iter = chainer.iterators.SerialIterator(
        test, args.batchsize, repeat=False, shuffle=False)

    updater = training.StandardUpdater(train_iter, optimizer, device=device)
    trainer = training.Trainer(updater, (args.epoch, 'epoch'), out=args.out)
    trainer.extend(extensions.Evaluator(test_iter, model, device=device))

    # Some display and output extentions are necessary only for worker 0.
    if comm.rank == 0:
        trainer.extend(extensions.DumpGraph('main/loss'))
        trainer.extend(extensions.LogReport())
        trainer.extend(extensions.PrintReport(
            ['epoch', 'main/loss', 'validation/main/loss',
             'main/accuracy', 'validation/main/accuracy', 'elapsed_time']))
        trainer.extend(extensions.ProgressBar())

    trainer.run()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10417')" href="javascript:;">
chainer-7.2.0/examples/chainermn/mnist/train_mnist_dual_parallel.py: 64-154
</a>
<div class="mid" id="frag10417" style="display:none"><pre>
def main():
    parser = argparse.ArgumentParser(
        description='ChainerMN example: pipelined neural network')
    parser.add_argument('--batchsize', '-b', type=int, default=100,
                        help='Number of images in each mini-batch')
    parser.add_argument('--epoch', '-e', type=int, default=20,
                        help='Number of sweeps over the dataset to train')
    parser.add_argument('--gpu', '-g', action='store_true',
                        help='Use GPU')
    parser.add_argument('--out', '-o', default='result',
                        help='Directory to output the result')
    parser.add_argument('--unit', '-u', type=int, default=1000,
                        help='Number of units')
    args = parser.parse_args()

    # Prepare ChainerMN communicator.
    if args.gpu:
        comm = chainermn.create_communicator('pure_nccl')
        data_axis, model_axis = comm.rank % 2, comm.rank // 2
        data_comm = comm.split(data_axis, comm.rank)
        model_comm = comm.split(model_axis, comm.rank)
        device = comm.intra_rank
    else:
        comm = chainermn.create_communicator('naive')
        data_axis, model_axis = comm.rank % 2, comm.rank // 2
        data_comm = comm.split(data_axis, comm.rank)
        model_comm = comm.split(model_axis, comm.rank)
        device = -1

    if model_comm.size != 2:
        raise ValueError(
            'This example can only be executed on the even number '
            'of processes.')

    if comm.rank == 0:
        print('==========================================')
        if args.gpu:
            print('Using GPUs')
        print('Num unit: {}'.format(args.unit))
        print('Num Minibatch-size: {}'.format(args.batchsize))
        print('Num epoch: {}'.format(args.epoch))
        print('==========================================')

    if data_axis == 0:
        model = L.Classifier(MLP0(model_comm, args.unit))
    elif data_axis == 1:
        model = MLP1(model_comm, args.unit, 10)

    if device &gt;= 0:
        chainer.cuda.get_device_from_id(device).use()
        model.to_gpu()

    optimizer = chainermn.create_multi_node_optimizer(
        chainer.optimizers.Adam(), data_comm)
    optimizer.setup(model)

    # Original dataset on worker 0 and 1.
    # Datasets of worker 0 and 1 are split and distributed to all workers.
    if model_axis == 0:
        train, test = chainer.datasets.get_mnist()
        if data_axis == 1:
            train = chainermn.datasets.create_empty_dataset(train)
            test = chainermn.datasets.create_empty_dataset(test)
    else:
        train, test = None, None
    train = chainermn.scatter_dataset(train, data_comm, shuffle=True)
    test = chainermn.scatter_dataset(test, data_comm, shuffle=True)

    train_iter = chainer.iterators.SerialIterator(
        train, args.batchsize, shuffle=False)
    test_iter = chainer.iterators.SerialIterator(
        test, args.batchsize, repeat=False, shuffle=False)

    updater = training.StandardUpdater(train_iter, optimizer, device=device)
    trainer = training.Trainer(updater, (args.epoch, 'epoch'), out=args.out)
    evaluator = extensions.Evaluator(test_iter, model, device=device)
    evaluator = chainermn.create_multi_node_evaluator(evaluator, data_comm)
    trainer.extend(evaluator)

    # Some display and output extensions are necessary only for worker 0.
    if comm.rank == 0:
        trainer.extend(extensions.DumpGraph('main/loss'))
        trainer.extend(extensions.LogReport())
        trainer.extend(extensions.PrintReport(
            ['epoch', 'main/loss', 'validation/main/loss',
             'main/accuracy', 'validation/main/accuracy', 'elapsed_time']))
        trainer.extend(extensions.ProgressBar())

    trainer.run()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 257:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10439')" href="javascript:;">
chainer-7.2.0/examples/chainermn/imagenet/models/nin.py: 13-26
</a>
<div class="mid" id="frag10439" style="display:none"><pre>
    def __init__(self):
        super(NIN, self).__init__()
        conv_init = I.HeNormal()  # MSRA scaling

        with self.init_scope():
            self.mlpconv1 = L.MLPConvolution2D(
                None, (96, 96, 96), 11, stride=4, conv_init=conv_init)
            self.mlpconv2 = L.MLPConvolution2D(
                None, (256, 256, 256), 5, pad=2, conv_init=conv_init)
            self.mlpconv3 = L.MLPConvolution2D(
                None, (384, 384, 384), 3, pad=1, conv_init=conv_init)
            self.mlpconv4 = L.MLPConvolution2D(
                None, (1024, 1024, 1000), 3, pad=1, conv_init=conv_init)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10552')" href="javascript:;">
chainer-7.2.0/examples/imagenet/nin.py: 13-26
</a>
<div class="mid" id="frag10552" style="display:none"><pre>
    def __init__(self):
        super(NIN, self).__init__()
        conv_init = I.HeNormal()  # MSRA scaling

        with self.init_scope():
            self.mlpconv1 = L.MLPConvolution2D(
                None, (96, 96, 96), 11, stride=4, conv_init=conv_init)
            self.mlpconv2 = L.MLPConvolution2D(
                None, (256, 256, 256), 5, pad=2, conv_init=conv_init)
            self.mlpconv3 = L.MLPConvolution2D(
                None, (384, 384, 384), 3, pad=1, conv_init=conv_init)
            self.mlpconv4 = L.MLPConvolution2D(
                None, (1024, 1024, 1000), 3, pad=1, conv_init=conv_init)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 258:</b> &nbsp; 2 fragments, nominal size 22 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10441')" href="javascript:;">
chainer-7.2.0/examples/chainermn/imagenet/models/googlenet.py: 10-34
</a>
<div class="mid" id="frag10441" style="display:none"><pre>
    def __init__(self):
        super(GoogLeNet, self).__init__()
        with self.init_scope():
            self.conv1 = L.Convolution2D(None, 64, 7, stride=2, pad=3)
            self.conv2_reduce = L.Convolution2D(None, 64, 1)
            self.conv2 = L.Convolution2D(None, 192, 3, stride=1, pad=1)
            self.inc3a = L.Inception(None, 64, 96, 128, 16, 32, 32)
            self.inc3b = L.Inception(None, 128, 128, 192, 32, 96, 64)
            self.inc4a = L.Inception(None, 192, 96, 208, 16, 48, 64)
            self.inc4b = L.Inception(None, 160, 112, 224, 24, 64, 64)
            self.inc4c = L.Inception(None, 128, 128, 256, 24, 64, 64)
            self.inc4d = L.Inception(None, 112, 144, 288, 32, 64, 64)
            self.inc4e = L.Inception(None, 256, 160, 320, 32, 128, 128)
            self.inc5a = L.Inception(None, 256, 160, 320, 32, 128, 128)
            self.inc5b = L.Inception(None, 384, 192, 384, 48, 128, 128)
            self.loss3_fc = L.Linear(None, 1000)

            self.loss1_conv = L.Convolution2D(None, 128, 1)
            self.loss1_fc1 = L.Linear(None, 1024)
            self.loss1_fc2 = L.Linear(None, 1000)

            self.loss2_conv = L.Convolution2D(None, 128, 1)
            self.loss2_fc1 = L.Linear(None, 1024)
            self.loss2_fc2 = L.Linear(None, 1000)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10572')" href="javascript:;">
chainer-7.2.0/examples/imagenet/googlenet.py: 10-34
</a>
<div class="mid" id="frag10572" style="display:none"><pre>
    def __init__(self):
        super(GoogLeNet, self).__init__()
        with self.init_scope():
            self.conv1 = L.Convolution2D(None,  64, 7, stride=2, pad=3)
            self.conv2_reduce = L.Convolution2D(None,  64, 1)
            self.conv2 = L.Convolution2D(None, 192, 3, stride=1, pad=1)
            self.inc3a = L.Inception(None,  64,  96, 128, 16,  32,  32)
            self.inc3b = L.Inception(None, 128, 128, 192, 32,  96,  64)
            self.inc4a = L.Inception(None, 192,  96, 208, 16,  48,  64)
            self.inc4b = L.Inception(None, 160, 112, 224, 24,  64,  64)
            self.inc4c = L.Inception(None, 128, 128, 256, 24,  64,  64)
            self.inc4d = L.Inception(None, 112, 144, 288, 32,  64,  64)
            self.inc4e = L.Inception(None, 256, 160, 320, 32, 128, 128)
            self.inc5a = L.Inception(None, 256, 160, 320, 32, 128, 128)
            self.inc5b = L.Inception(None, 384, 192, 384, 48, 128, 128)
            self.loss3_fc = L.Linear(None, 1000)

            self.loss1_conv = L.Convolution2D(None, 128, 1)
            self.loss1_fc1 = L.Linear(None, 1024)
            self.loss1_fc2 = L.Linear(None, 1000)

            self.loss2_conv = L.Convolution2D(None, 128, 1)
            self.loss2_fc1 = L.Linear(None, 1024)
            self.loss2_fc2 = L.Linear(None, 1000)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 259:</b> &nbsp; 2 fragments, nominal size 41 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10442')" href="javascript:;">
chainer-7.2.0/examples/chainermn/imagenet/models/googlenet.py: 35-84
</a>
<div class="mid" id="frag10442" style="display:none"><pre>
    def __call__(self, x, t):
        h = F.relu(self.conv1(x))
        h = F.local_response_normalization(
            F.max_pooling_2d(h, 3, stride=2), n=5)
        h = F.relu(self.conv2_reduce(h))
        h = F.relu(self.conv2(h))
        h = F.max_pooling_2d(
            F.local_response_normalization(h, n=5), 3, stride=2)

        h = self.inc3a(h)
        h = self.inc3b(h)
        h = F.max_pooling_2d(h, 3, stride=2)
        h = self.inc4a(h)

        l = F.average_pooling_2d(h, 5, stride=3)
        l = F.relu(self.loss1_conv(l))
        l = F.relu(self.loss1_fc1(l))
        l = self.loss1_fc2(l)
        loss1 = F.softmax_cross_entropy(l, t)

        h = self.inc4b(h)
        h = self.inc4c(h)
        h = self.inc4d(h)

        l = F.average_pooling_2d(h, 5, stride=3)
        l = F.relu(self.loss2_conv(l))
        l = F.relu(self.loss2_fc1(l))
        l = self.loss2_fc2(l)
        loss2 = F.softmax_cross_entropy(l, t)

        h = self.inc4e(h)
        h = F.max_pooling_2d(h, 3, stride=2)
        h = self.inc5a(h)
        h = self.inc5b(h)

        h = F.average_pooling_2d(h, 7, stride=1)
        h = self.loss3_fc(F.dropout(h, 0.4))
        loss3 = F.softmax_cross_entropy(h, t)

        loss = 0.3 * (loss1 + loss2) + loss3
        accuracy = F.accuracy(h, t)

        chainer.report({
            'loss': loss,
            'loss1': loss1,
            'loss2': loss2,
            'loss3': loss3,
            'accuracy': accuracy
        }, self)
        return loss
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10573')" href="javascript:;">
chainer-7.2.0/examples/imagenet/googlenet.py: 35-84
</a>
<div class="mid" id="frag10573" style="display:none"><pre>
    def forward(self, x, t):
        h = F.relu(self.conv1(x))
        h = F.local_response_normalization(
            F.max_pooling_2d(h, 3, stride=2), n=5)
        h = F.relu(self.conv2_reduce(h))
        h = F.relu(self.conv2(h))
        h = F.max_pooling_2d(
            F.local_response_normalization(h, n=5), 3, stride=2)

        h = self.inc3a(h)
        h = self.inc3b(h)
        h = F.max_pooling_2d(h, 3, stride=2)
        h = self.inc4a(h)

        l = F.average_pooling_2d(h, 5, stride=3)
        l = F.relu(self.loss1_conv(l))
        l = F.relu(self.loss1_fc1(l))
        l = self.loss1_fc2(l)
        loss1 = F.softmax_cross_entropy(l, t)

        h = self.inc4b(h)
        h = self.inc4c(h)
        h = self.inc4d(h)

        l = F.average_pooling_2d(h, 5, stride=3)
        l = F.relu(self.loss2_conv(l))
        l = F.relu(self.loss2_fc1(l))
        l = self.loss2_fc2(l)
        loss2 = F.softmax_cross_entropy(l, t)

        h = self.inc4e(h)
        h = F.max_pooling_2d(h, 3, stride=2)
        h = self.inc5a(h)
        h = self.inc5b(h)

        h = F.average_pooling_2d(h, 7, stride=1)
        h = self.loss3_fc(F.dropout(h, 0.4))
        loss3 = F.softmax_cross_entropy(h, t)

        loss = 0.3 * (loss1 + loss2) + loss3
        accuracy = F.accuracy(h, t)

        chainer.report({
            'loss': loss,
            'loss1': loss1,
            'loss2': loss2,
            'loss3': loss3,
            'accuracy': accuracy
        }, self)
        return loss
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 260:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10443')" href="javascript:;">
chainer-7.2.0/examples/chainermn/imagenet/models/alex.py: 12-23
</a>
<div class="mid" id="frag10443" style="display:none"><pre>
    def __init__(self):
        super(Alex, self).__init__()
        with self.init_scope():
            self.conv1 = L.Convolution2D(None, 96, 11, stride=4)
            self.conv2 = L.Convolution2D(None, 256, 5, pad=2)
            self.conv3 = L.Convolution2D(None, 384, 3, pad=1)
            self.conv4 = L.Convolution2D(None, 384, 3, pad=1)
            self.conv5 = L.Convolution2D(None, 256, 3, pad=1)
            self.fc6 = L.Linear(None, 4096)
            self.fc7 = L.Linear(None, 4096)
            self.fc8 = L.Linear(None, 1000)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10577')" href="javascript:;">
chainer-7.2.0/examples/imagenet/alex.py: 12-23
</a>
<div class="mid" id="frag10577" style="display:none"><pre>
    def __init__(self):
        super(Alex, self).__init__()
        with self.init_scope():
            self.conv1 = L.Convolution2D(None,  96, 11, stride=4)
            self.conv2 = L.Convolution2D(None, 256,  5, pad=2)
            self.conv3 = L.Convolution2D(None, 384,  3, pad=1)
            self.conv4 = L.Convolution2D(None, 384,  3, pad=1)
            self.conv5 = L.Convolution2D(None, 256,  3, pad=1)
            self.fc6 = L.Linear(None, 4096)
            self.fc7 = L.Linear(None, 4096)
            self.fc8 = L.Linear(None, 1000)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 261:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10444')" href="javascript:;">
chainer-7.2.0/examples/chainermn/imagenet/models/alex.py: 24-38
</a>
<div class="mid" id="frag10444" style="display:none"><pre>
    def __call__(self, x, t):
        h = F.max_pooling_2d(F.local_response_normalization(
            F.relu(self.conv1(x))), 3, stride=2)
        h = F.max_pooling_2d(F.local_response_normalization(
            F.relu(self.conv2(h))), 3, stride=2)
        h = F.relu(self.conv3(h))
        h = F.relu(self.conv4(h))
        h = F.max_pooling_2d(F.relu(self.conv5(h)), 3, stride=2)
        h = F.dropout(F.relu(self.fc6(h)))
        h = F.dropout(F.relu(self.fc7(h)))
        h = self.fc8(h)

        loss = F.softmax_cross_entropy(h, t)
        chainer.report({'loss': loss, 'accuracy': F.accuracy(h, t)}, self)
        return loss
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10578')" href="javascript:;">
chainer-7.2.0/examples/imagenet/alex.py: 24-38
</a>
<div class="mid" id="frag10578" style="display:none"><pre>
    def forward(self, x, t):
        h = F.max_pooling_2d(F.local_response_normalization(
            F.relu(self.conv1(x))), 3, stride=2)
        h = F.max_pooling_2d(F.local_response_normalization(
            F.relu(self.conv2(h))), 3, stride=2)
        h = F.relu(self.conv3(h))
        h = F.relu(self.conv4(h))
        h = F.max_pooling_2d(F.relu(self.conv5(h)), 3, stride=2)
        h = F.dropout(F.relu(self.fc6(h)))
        h = F.dropout(F.relu(self.fc7(h)))
        h = self.fc8(h)

        loss = F.softmax_cross_entropy(h, t)
        chainer.report({'loss': loss, 'accuracy': F.accuracy(h, t)}, self)
        return loss
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 262:</b> &nbsp; 3 fragments, nominal size 18 lines, similarity 77%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10445')" href="javascript:;">
chainer-7.2.0/examples/chainermn/imagenet/models/resnet50.py: 12-31
</a>
<div class="mid" id="frag10445" style="display:none"><pre>
    def __init__(self, in_size, ch, out_size, stride=2):
        super(BottleNeckA, self).__init__()
        initialW = initializers.HeNormal()

        with self.init_scope():
            self.conv1 = L.Convolution2D(
                in_size, ch, 1, stride, 0, initialW=initialW, nobias=True)
            self.bn1 = L.BatchNormalization(ch)
            self.conv2 = L.Convolution2D(
                ch, ch, 3, 1, 1, initialW=initialW, nobias=True)
            self.bn2 = L.BatchNormalization(ch)
            self.conv3 = L.Convolution2D(
                ch, out_size, 1, 1, 0, initialW=initialW, nobias=True)
            self.bn3 = L.BatchNormalization(out_size)

            self.conv4 = L.Convolution2D(
                in_size, out_size, 1, stride, 0,
                initialW=initialW, nobias=True)
            self.bn4 = L.BatchNormalization(out_size)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10564')" href="javascript:;">
chainer-7.2.0/examples/imagenet/resnext50.py: 9-29
</a>
<div class="mid" id="frag10564" style="display:none"><pre>
    def __init__(self, in_size, ch, out_size, stride=2, groups=1):
        super(BottleNeckA, self).__init__()
        initialW = initializers.HeNormal()

        with self.init_scope():
            self.conv1 = L.Convolution2D(
                in_size, ch, 1, 1, 0, initialW=initialW, nobias=True)
            self.bn1 = L.BatchNormalization(ch)
            self.conv2 = L.Convolution2D(
                ch, ch, 3, stride, 1, initialW=initialW, nobias=True,
                groups=groups)
            self.bn2 = L.BatchNormalization(ch)
            self.conv3 = L.Convolution2D(
                ch, out_size, 1, 1, 0, initialW=initialW, nobias=True)
            self.bn3 = L.BatchNormalization(out_size)

            self.conv4 = L.Convolution2D(
                in_size, out_size, 1, stride, 0,
                initialW=initialW, nobias=True)
            self.bn4 = L.BatchNormalization(out_size)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10579')" href="javascript:;">
chainer-7.2.0/examples/imagenet/resnet50.py: 12-32
</a>
<div class="mid" id="frag10579" style="display:none"><pre>
    def __init__(self, in_size, ch, out_size, stride=2, groups=1):
        super(BottleNeckA, self).__init__()
        initialW = initializers.HeNormal()

        with self.init_scope():
            self.conv1 = L.Convolution2D(
                in_size, ch, 1, stride, 0, initialW=initialW, nobias=True)
            self.bn1 = L.BatchNormalization(ch)
            self.conv2 = L.Convolution2D(
                ch, ch, 3, 1, 1, initialW=initialW, nobias=True,
                groups=groups)
            self.bn2 = L.BatchNormalization(ch)
            self.conv3 = L.Convolution2D(
                ch, out_size, 1, 1, 0, initialW=initialW, nobias=True)
            self.bn3 = L.BatchNormalization(out_size)

            self.conv4 = L.Convolution2D(
                in_size, out_size, 1, stride, 0,
                initialW=initialW, nobias=True)
            self.bn4 = L.BatchNormalization(out_size)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 263:</b> &nbsp; 3 fragments, nominal size 14 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10447')" href="javascript:;">
chainer-7.2.0/examples/chainermn/imagenet/models/resnet50.py: 43-57
</a>
<div class="mid" id="frag10447" style="display:none"><pre>
    def __init__(self, in_size, ch):
        super(BottleNeckB, self).__init__()
        initialW = initializers.HeNormal()

        with self.init_scope():
            self.conv1 = L.Convolution2D(
                in_size, ch, 1, 1, 0, initialW=initialW, nobias=True)
            self.bn1 = L.BatchNormalization(ch)
            self.conv2 = L.Convolution2D(
                ch, ch, 3, 1, 1, initialW=initialW, nobias=True)
            self.bn2 = L.BatchNormalization(ch)
            self.conv3 = L.Convolution2D(
                ch, in_size, 1, 1, 0, initialW=initialW, nobias=True)
            self.bn3 = L.BatchNormalization(in_size)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10581')" href="javascript:;">
chainer-7.2.0/examples/imagenet/resnet50.py: 44-59
</a>
<div class="mid" id="frag10581" style="display:none"><pre>
    def __init__(self, in_size, ch, groups=1):
        super(BottleNeckB, self).__init__()
        initialW = initializers.HeNormal()

        with self.init_scope():
            self.conv1 = L.Convolution2D(
                in_size, ch, 1, 1, 0, initialW=initialW, nobias=True)
            self.bn1 = L.BatchNormalization(ch)
            self.conv2 = L.Convolution2D(
                ch, ch, 3, 1, 1, initialW=initialW, nobias=True,
                groups=groups)
            self.bn2 = L.BatchNormalization(ch)
            self.conv3 = L.Convolution2D(
                ch, in_size, 1, 1, 0, initialW=initialW, nobias=True)
            self.bn3 = L.BatchNormalization(in_size)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10566')" href="javascript:;">
chainer-7.2.0/examples/imagenet/resnext50.py: 41-56
</a>
<div class="mid" id="frag10566" style="display:none"><pre>
    def __init__(self, in_size, ch, groups=1):
        super(BottleNeckB, self).__init__()
        initialW = initializers.HeNormal()

        with self.init_scope():
            self.conv1 = L.Convolution2D(
                in_size, ch, 1, 1, 0, initialW=initialW, nobias=True)
            self.bn1 = L.BatchNormalization(ch)
            self.conv2 = L.Convolution2D(
                ch, ch, 3, 1, 1, initialW=initialW, nobias=True,
                groups=groups)
            self.bn2 = L.BatchNormalization(ch)
            self.conv3 = L.Convolution2D(
                ch, in_size, 1, 1, 0, initialW=initialW, nobias=True)
            self.bn3 = L.BatchNormalization(in_size)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 264:</b> &nbsp; 3 fragments, nominal size 11 lines, similarity 91%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10451')" href="javascript:;">
chainer-7.2.0/examples/chainermn/imagenet/models/resnet50.py: 84-95
</a>
<div class="mid" id="frag10451" style="display:none"><pre>
    def __init__(self):
        super(ResNet50, self).__init__()
        with self.init_scope():
            self.conv1 = L.Convolution2D(
                3, 64, 7, 2, 3, initialW=initializers.HeNormal())
            self.bn1 = L.BatchNormalization(64)
            self.res2 = Block(3, 64, 64, 256, 1)
            self.res3 = Block(4, 256, 128, 512)
            self.res4 = Block(6, 512, 256, 1024)
            self.res5 = Block(3, 1024, 512, 2048)
            self.fc = L.Linear(2048, 1000)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10585')" href="javascript:;">
chainer-7.2.0/examples/imagenet/resnet50.py: 86-97
</a>
<div class="mid" id="frag10585" style="display:none"><pre>
    def __init__(self):
        super(ResNet50, self).__init__()
        with self.init_scope():
            self.conv1 = L.Convolution2D(
                3, 64, 7, 2, 3, initialW=initializers.HeNormal())
            self.bn1 = L.BatchNormalization(64)
            self.res2 = Block(3, 64, 64, 256, 1)
            self.res3 = Block(4, 256, 128, 512)
            self.res4 = Block(6, 512, 256, 1024)
            self.res5 = Block(3, 1024, 512, 2048)
            self.fc = L.Linear(2048, 1000)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10587')" href="javascript:;">
chainer-7.2.0/examples/imagenet/resnet50.py: 117-129
</a>
<div class="mid" id="frag10587" style="display:none"><pre>
    def __init__(self):
        super(ResNet50_Nhwc, self).__init__()
        with self.init_scope():
            self.conv1 = L.Convolution2D(
                3, 64, 7, 2, 3, initialW=initializers.HeNormal())
            self.bn1 = L.BatchNormalization(64)
            with chainer.using_config('compute_mode', 'cudnn_fast'):
                self.res2 = Block(3, 64, 64, 256, 1)
                self.res3 = Block(4, 256, 128, 512)
                self.res4 = Block(6, 512, 256, 1024)
                self.res5 = Block(3, 1024, 512, 2048)
            self.fc = L.Linear(2048, 1000)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 265:</b> &nbsp; 4 fragments, nominal size 12 lines, similarity 85%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10452')" href="javascript:;">
chainer-7.2.0/examples/chainermn/imagenet/models/resnet50.py: 96-108
</a>
<div class="mid" id="frag10452" style="display:none"><pre>
    def __call__(self, x, t):
        h = self.bn1(self.conv1(x))
        h = F.max_pooling_2d(F.relu(h), 3, stride=2)
        h = self.res2(h)
        h = self.res3(h)
        h = self.res4(h)
        h = self.res5(h)
        h = F.average_pooling_2d(h, 7, stride=1)
        h = self.fc(h)

        loss = F.softmax_cross_entropy(h, t)
        chainer.report({'loss': loss, 'accuracy': F.accuracy(h, t)}, self)
        return loss
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10588')" href="javascript:;">
chainer-7.2.0/examples/imagenet/resnet50.py: 130-144
</a>
<div class="mid" id="frag10588" style="display:none"><pre>
    def forward(self, x, t):
        h = self.bn1(self.conv1(x))
        h = F.max_pooling_2d(F.relu(h), 3, stride=2)
        h = h.as_layout(chainer.memory_layouts.CUDNN_CHANNEL_LAST_X)
        h = self.res2(h)
        h = self.res3(h)
        h = self.res4(h)
        h = self.res5(h)
        h = h.as_layout(None)
        h = F.average_pooling_2d(h, 7, stride=1)
        h = self.fc(h)

        loss = F.softmax_cross_entropy(h, t)
        chainer.report({'loss': loss, 'accuracy': F.accuracy(h, t)}, self)
        return loss
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10586')" href="javascript:;">
chainer-7.2.0/examples/imagenet/resnet50.py: 98-112
</a>
<div class="mid" id="frag10586" style="display:none"><pre>
    def forward(self, x, t):
        h = self.bn1(self.conv1(x))
        h = F.max_pooling_2d(F.relu(h), 3, stride=2)
        h = self.res2(h)
        h = self.res3(h)
        h = self.res4(h)
        h = self.res5(h)
        h = F.average_pooling_2d(h, 7, stride=1)
        h = self.fc(h)

        loss = F.softmax_cross_entropy(h, t)
        chainer.report({'loss': loss, 'accuracy': F.accuracy(h, t)}, self)
        return loss


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10571')" href="javascript:;">
chainer-7.2.0/examples/imagenet/resnext50.py: 95-107
</a>
<div class="mid" id="frag10571" style="display:none"><pre>
    def forward(self, x, t):
        h = self.bn1(self.conv1(x))
        h = F.max_pooling_2d(F.relu(h), 3, stride=2)
        h = self.res2(h)
        h = self.res3(h)
        h = self.res4(h)
        h = self.res5(h)
        h = F.average_pooling_2d(h, 7, stride=1)
        h = self.fc(h)

        loss = F.softmax_cross_entropy(h, t)
        chainer.report({'loss': loss, 'accuracy': F.accuracy(h, t)}, self)
        return loss
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 266:</b> &nbsp; 2 fragments, nominal size 39 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10453')" href="javascript:;">
chainer-7.2.0/examples/chainermn/imagenet/models/googlenetbn.py: 12-53
</a>
<div class="mid" id="frag10453" style="display:none"><pre>
    def __init__(self):
        super(GoogLeNetBN, self).__init__()
        with self.init_scope():
            self.conv1 = L.Convolution2D(
                None, 64, 7, stride=2, pad=3, nobias=True)
            self.norm1 = L.BatchNormalization(64)
            self.conv2 = L.Convolution2D(None, 192, 3, pad=1, nobias=True)
            self.norm2 = L.BatchNormalization(192)
            self.inc3a = L.InceptionBN(
                None, 64, 64, 64, 64, 96, 'avg', 32)
            self.inc3b = L.InceptionBN(
                None, 64, 64, 96, 64, 96, 'avg', 64)
            self.inc3c = L.InceptionBN(
                None, 0, 128, 160, 64, 96, 'max', stride=2)
            self.inc4a = L.InceptionBN(
                None, 224, 64, 96, 96, 128, 'avg', 128)
            self.inc4b = L.InceptionBN(
                None, 192, 96, 128, 96, 128, 'avg', 128)
            self.inc4c = L.InceptionBN(
                None, 128, 128, 160, 128, 160, 'avg', 128)
            self.inc4d = L.InceptionBN(
                None, 64, 128, 192, 160, 192, 'avg', 128)
            self.inc4e = L.InceptionBN(
                None, 0, 128, 192, 192, 256, 'max', stride=2)
            self.inc5a = L.InceptionBN(
                None, 352, 192, 320, 160, 224, 'avg', 128)
            self.inc5b = L.InceptionBN(
                None, 352, 192, 320, 192, 224, 'max', 128)
            self.out = L.Linear(None, 1000)

            self.conva = L.Convolution2D(None, 128, 1, nobias=True)
            self.norma = L.BatchNormalization(128)
            self.lina = L.Linear(None, 1024, nobias=True)
            self.norma2 = L.BatchNormalization(1024)
            self.outa = L.Linear(None, 1000)

            self.convb = L.Convolution2D(None, 128, 1, nobias=True)
            self.normb = L.BatchNormalization(128)
            self.linb = L.Linear(None, 1024, nobias=True)
            self.normb2 = L.BatchNormalization(1024)
            self.outb = L.Linear(None, 1000)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10589')" href="javascript:;">
chainer-7.2.0/examples/imagenet/googlenetbn.py: 12-53
</a>
<div class="mid" id="frag10589" style="display:none"><pre>
    def __init__(self):
        super(GoogLeNetBN, self).__init__()
        with self.init_scope():
            self.conv1 = L.Convolution2D(
                None, 64, 7, stride=2, pad=3, nobias=True)
            self.norm1 = L.BatchNormalization(64)
            self.conv2 = L.Convolution2D(None, 192, 3, pad=1, nobias=True)
            self.norm2 = L.BatchNormalization(192)
            self.inc3a = L.InceptionBN(
                None, 64, 64, 64, 64, 96, 'avg', 32)
            self.inc3b = L.InceptionBN(
                None, 64, 64, 96, 64, 96, 'avg', 64)
            self.inc3c = L.InceptionBN(
                None, 0, 128, 160, 64, 96, 'max', stride=2)
            self.inc4a = L.InceptionBN(
                None, 224, 64, 96, 96, 128, 'avg', 128)
            self.inc4b = L.InceptionBN(
                None, 192, 96, 128, 96, 128, 'avg', 128)
            self.inc4c = L.InceptionBN(
                None, 160, 128, 160, 128, 160, 'avg', 128)
            self.inc4d = L.InceptionBN(
                None, 96, 128, 192, 160, 192, 'avg', 128)
            self.inc4e = L.InceptionBN(
                None, 0, 128, 192, 192, 256, 'max', stride=2)
            self.inc5a = L.InceptionBN(
                None, 352, 192, 320, 160, 224, 'avg', 128)
            self.inc5b = L.InceptionBN(
                None, 352, 192, 320, 192, 224, 'max', 128)
            self.out = L.Linear(None, 1000)

            self.conva = L.Convolution2D(None, 128, 1, nobias=True)
            self.norma = L.BatchNormalization(128)
            self.lina = L.Linear(None, 1024, nobias=True)
            self.norma2 = L.BatchNormalization(1024)
            self.outa = L.Linear(None, 1000)

            self.convb = L.Convolution2D(None, 128, 1, nobias=True)
            self.normb = L.BatchNormalization(128)
            self.linb = L.Linear(None, 1024, nobias=True)
            self.normb2 = L.BatchNormalization(1024)
            self.outb = L.Linear(None, 1000)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 267:</b> &nbsp; 2 fragments, nominal size 37 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10454')" href="javascript:;">
chainer-7.2.0/examples/chainermn/imagenet/models/googlenetbn.py: 54-97
</a>
<div class="mid" id="frag10454" style="display:none"><pre>
    def __call__(self, x, t):
        h = F.max_pooling_2d(
            F.relu(self.norm1(self.conv1(x))), 3, stride=2, pad=1)
        h = F.max_pooling_2d(
            F.relu(self.norm2(self.conv2(h))), 3, stride=2, pad=1)

        h = self.inc3a(h)
        h = self.inc3b(h)
        h = self.inc3c(h)
        h = self.inc4a(h)

        a = F.average_pooling_2d(h, 5, stride=3)
        a = F.relu(self.norma(self.conva(a)))
        a = F.relu(self.norma2(self.lina(a)))
        a = self.outa(a)
        loss1 = F.softmax_cross_entropy(a, t)

        h = self.inc4b(h)
        h = self.inc4c(h)
        h = self.inc4d(h)

        b = F.average_pooling_2d(h, 5, stride=3)
        b = F.relu(self.normb(self.convb(b)))
        b = F.relu(self.normb2(self.linb(b)))
        b = self.outb(b)
        loss2 = F.softmax_cross_entropy(b, t)

        h = self.inc4e(h)
        h = self.inc5a(h)
        h = F.average_pooling_2d(self.inc5b(h), 7)
        h = self.out(h)
        loss3 = F.softmax_cross_entropy(h, t)

        loss = 0.3 * (loss1 + loss2) + loss3
        accuracy = F.accuracy(h, t)

        chainer.report({
            'loss': loss,
            'loss1': loss1,
            'loss2': loss2,
            'loss3': loss3,
            'accuracy': accuracy,
        }, self)
        return loss
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10590')" href="javascript:;">
chainer-7.2.0/examples/imagenet/googlenetbn.py: 54-97
</a>
<div class="mid" id="frag10590" style="display:none"><pre>
    def forward(self, x, t):
        h = F.max_pooling_2d(
            F.relu(self.norm1(self.conv1(x))),  3, stride=2, pad=1)
        h = F.max_pooling_2d(
            F.relu(self.norm2(self.conv2(h))), 3, stride=2, pad=1)

        h = self.inc3a(h)
        h = self.inc3b(h)
        h = self.inc3c(h)
        h = self.inc4a(h)

        a = F.average_pooling_2d(h, 5, stride=3)
        a = F.relu(self.norma(self.conva(a)))
        a = F.relu(self.norma2(self.lina(a)))
        a = self.outa(a)
        loss1 = F.softmax_cross_entropy(a, t)

        h = self.inc4b(h)
        h = self.inc4c(h)
        h = self.inc4d(h)

        b = F.average_pooling_2d(h, 5, stride=3)
        b = F.relu(self.normb(self.convb(b)))
        b = F.relu(self.normb2(self.linb(b)))
        b = self.outb(b)
        loss2 = F.softmax_cross_entropy(b, t)

        h = self.inc4e(h)
        h = self.inc5a(h)
        h = F.average_pooling_2d(self.inc5b(h), 7)
        h = self.out(h)
        loss3 = F.softmax_cross_entropy(h, t)

        loss = 0.3 * (loss1 + loss2) + loss3
        accuracy = F.accuracy(h, t)

        chainer.report({
            'loss': loss,
            'loss1': loss1,
            'loss2': loss2,
            'loss3': loss3,
            'accuracy': accuracy,
        }, self)
        return loss
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 268:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10455')" href="javascript:;">
chainer-7.2.0/examples/chainermn/imagenet/compute_mean.py: 10-21
</a>
<div class="mid" id="frag10455" style="display:none"><pre>
def compute_mean(dataset):
    print('compute mean image')
    sum_image = 0
    N = len(dataset)
    for i, (image, _) in enumerate(dataset):
        sum_image += image
        sys.stderr.write('{} / {}\r'.format(i, N))
        sys.stderr.flush()
    sys.stderr.write('\n')
    return sum_image / N


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10592')" href="javascript:;">
chainer-7.2.0/examples/imagenet/compute_mean.py: 10-21
</a>
<div class="mid" id="frag10592" style="display:none"><pre>
def compute_mean(dataset):
    print('compute mean image')
    sum_image = 0
    N = len(dataset)
    for i, (image, _) in enumerate(dataset):
        sum_image += image
        sys.stderr.write('{} / {}\r'.format(i, N))
        sys.stderr.flush()
    sys.stderr.write('\n')
    return sum_image / N


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 269:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10456')" href="javascript:;">
chainer-7.2.0/examples/chainermn/imagenet/compute_mean.py: 22-36
</a>
<div class="mid" id="frag10456" style="display:none"><pre>
def main():
    parser = argparse.ArgumentParser(description='Compute images mean array')
    parser.add_argument('dataset',
                        help='Path to training image-label list file')
    parser.add_argument('--root', '-R', default='.',
                        help='Root directory path of image files')
    parser.add_argument('--output', '-o', default='mean.npy',
                        help='path to output mean array')
    args = parser.parse_args()

    dataset = chainer.datasets.LabeledImageDataset(args.dataset, args.root)
    mean = compute_mean(dataset)
    np.save(args.output, mean)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10593')" href="javascript:;">
chainer-7.2.0/examples/imagenet/compute_mean.py: 22-36
</a>
<div class="mid" id="frag10593" style="display:none"><pre>
def main():
    parser = argparse.ArgumentParser(description='Compute images mean array')
    parser.add_argument('dataset',
                        help='Path to training image-label list file')
    parser.add_argument('--root', '-R', default='.',
                        help='Root directory path of image files')
    parser.add_argument('--output', '-o', default='mean.npy',
                        help='path to output mean array')
    args = parser.parse_args()

    dataset = chainer.datasets.LabeledImageDataset(args.dataset, args.root)
    mean = compute_mean(dataset)
    np.save(args.output, mean)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 270:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10460')" href="javascript:;">
chainer-7.2.0/examples/chainermn/dcgan/net.py: 23-41
</a>
<div class="mid" id="frag10460" style="display:none"><pre>
    def __init__(self, n_hidden, bottom_width=4, ch=512, wscale=0.02):
        super(Generator, self).__init__()
        self.n_hidden = n_hidden
        self.ch = ch
        self.bottom_width = bottom_width

        with self.init_scope():
            w = chainer.initializers.Normal(wscale)
            self.l0 = L.Linear(self.n_hidden, bottom_width * bottom_width * ch,
                               initialW=w)
            self.dc1 = L.Deconvolution2D(ch, ch // 2, 4, 2, 1, initialW=w)
            self.dc2 = L.Deconvolution2D(ch // 2, ch // 4, 4, 2, 1, initialW=w)
            self.dc3 = L.Deconvolution2D(ch // 4, ch // 8, 4, 2, 1, initialW=w)
            self.dc4 = L.Deconvolution2D(ch // 8, 3, 3, 1, 1, initialW=w)
            self.bn0 = L.BatchNormalization(bottom_width * bottom_width * ch)
            self.bn1 = L.BatchNormalization(ch // 2)
            self.bn2 = L.BatchNormalization(ch // 4)
            self.bn3 = L.BatchNormalization(ch // 8)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10597')" href="javascript:;">
chainer-7.2.0/examples/dcgan/net.py: 27-45
</a>
<div class="mid" id="frag10597" style="display:none"><pre>
    def __init__(self, n_hidden, bottom_width=4, ch=512, wscale=0.02):
        super(Generator, self).__init__()
        self.n_hidden = n_hidden
        self.ch = ch
        self.bottom_width = bottom_width

        with self.init_scope():
            w = chainer.initializers.Normal(wscale)
            self.l0 = L.Linear(self.n_hidden, bottom_width * bottom_width * ch,
                               initialW=w)
            self.dc1 = L.Deconvolution2D(ch, ch // 2, 4, 2, 1, initialW=w)
            self.dc2 = L.Deconvolution2D(ch // 2, ch // 4, 4, 2, 1, initialW=w)
            self.dc3 = L.Deconvolution2D(ch // 4, ch // 8, 4, 2, 1, initialW=w)
            self.dc4 = L.Deconvolution2D(ch // 8, 3, 3, 1, 1, initialW=w)
            self.bn0 = L.BatchNormalization(bottom_width * bottom_width * ch)
            self.bn1 = L.BatchNormalization(ch // 2)
            self.bn2 = L.BatchNormalization(ch // 4)
            self.bn3 = L.BatchNormalization(ch // 8)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 271:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10463')" href="javascript:;">
chainer-7.2.0/examples/chainermn/dcgan/net.py: 57-75
</a>
<div class="mid" id="frag10463" style="display:none"><pre>

    def __init__(self, bottom_width=4, ch=512, wscale=0.02):
        w = chainer.initializers.Normal(wscale)
        super(Discriminator, self).__init__()
        with self.init_scope():
            self.c0_0 = L.Convolution2D(3, ch // 8, 3, 1, 1, initialW=w)
            self.c0_1 = L.Convolution2D(ch // 8, ch // 4, 4, 2, 1, initialW=w)
            self.c1_0 = L.Convolution2D(ch // 4, ch // 4, 3, 1, 1, initialW=w)
            self.c1_1 = L.Convolution2D(ch // 4, ch // 2, 4, 2, 1, initialW=w)
            self.c2_0 = L.Convolution2D(ch // 2, ch // 2, 3, 1, 1, initialW=w)
            self.c2_1 = L.Convolution2D(ch // 2, ch // 1, 4, 2, 1, initialW=w)
            self.c3_0 = L.Convolution2D(ch // 1, ch // 1, 3, 1, 1, initialW=w)
            self.l4 = L.Linear(bottom_width * bottom_width * ch, 1, initialW=w)
            self.bn0_1 = L.BatchNormalization(ch // 4, use_gamma=False)
            self.bn1_0 = L.BatchNormalization(ch // 4, use_gamma=False)
            self.bn1_1 = L.BatchNormalization(ch // 2, use_gamma=False)
            self.bn2_0 = L.BatchNormalization(ch // 2, use_gamma=False)
            self.bn2_1 = L.BatchNormalization(ch // 1, use_gamma=False)
            self.bn3_0 = L.BatchNormalization(ch // 1, use_gamma=False)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10600')" href="javascript:;">
chainer-7.2.0/examples/dcgan/net.py: 62-80
</a>
<div class="mid" id="frag10600" style="display:none"><pre>

    def __init__(self, bottom_width=4, ch=512, wscale=0.02):
        w = chainer.initializers.Normal(wscale)
        super(Discriminator, self).__init__()
        with self.init_scope():
            self.c0_0 = L.Convolution2D(3, ch // 8, 3, 1, 1, initialW=w)
            self.c0_1 = L.Convolution2D(ch // 8, ch // 4, 4, 2, 1, initialW=w)
            self.c1_0 = L.Convolution2D(ch // 4, ch // 4, 3, 1, 1, initialW=w)
            self.c1_1 = L.Convolution2D(ch // 4, ch // 2, 4, 2, 1, initialW=w)
            self.c2_0 = L.Convolution2D(ch // 2, ch // 2, 3, 1, 1, initialW=w)
            self.c2_1 = L.Convolution2D(ch // 2, ch // 1, 4, 2, 1, initialW=w)
            self.c3_0 = L.Convolution2D(ch // 1, ch // 1, 3, 1, 1, initialW=w)
            self.l4 = L.Linear(bottom_width * bottom_width * ch, 1, initialW=w)
            self.bn0_1 = L.BatchNormalization(ch // 4, use_gamma=False)
            self.bn1_0 = L.BatchNormalization(ch // 4, use_gamma=False)
            self.bn1_1 = L.BatchNormalization(ch // 2, use_gamma=False)
            self.bn2_0 = L.BatchNormalization(ch // 2, use_gamma=False)
            self.bn2_1 = L.BatchNormalization(ch // 1, use_gamma=False)
            self.bn3_0 = L.BatchNormalization(ch // 1, use_gamma=False)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 272:</b> &nbsp; 2 fragments, nominal size 22 lines, similarity 95%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10465')" href="javascript:;">
chainer-7.2.0/examples/chainermn/dcgan/visualize.py: 13-36
</a>
<div class="mid" id="frag10465" style="display:none"><pre>
def out_generated_image(gen, dis, rows, cols, seed, dst):
    @chainer.training.make_extension()
    def make_image(trainer):
        np.random.seed(seed)
        n_images = rows * cols
        xp = gen.xp
        z = Variable(xp.asarray(gen.make_hidden(n_images)))
        with chainer.using_config('train', False):
            x = gen(z)
        x = chainer.cuda.to_cpu(x.array)
        np.random.seed()

        x = np.asarray(np.clip(x * 255, 0.0, 255.0), dtype=np.uint8)
        _, _, H, W = x.shape
        x = x.reshape((rows, cols, 3, H, W))
        x = x.transpose(0, 3, 1, 4, 2)
        x = x.reshape((rows * H, cols * W, 3))

        preview_dir = '{}/preview'.format(dst)
        preview_path = preview_dir +\
            '/image{:0&gt;8}.png'.format(trainer.updater.iteration)
        if not os.path.exists(preview_dir):
            os.makedirs(preview_dir)
        Image.fromarray(x).save(preview_path)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10602')" href="javascript:;">
chainer-7.2.0/examples/dcgan/visualize.py: 13-36
</a>
<div class="mid" id="frag10602" style="display:none"><pre>
def out_generated_image(gen, dis, rows, cols, seed, dst):
    @chainer.training.make_extension()
    def make_image(trainer):
        np.random.seed(seed)
        n_images = rows * cols
        xp = gen.xp
        z = Variable(xp.asarray(gen.make_hidden(n_images)))
        with chainer.using_config('train', False):
            x = gen(z)
        x = chainer.backends.cuda.to_cpu(x.array)
        np.random.seed()

        x = np.asarray(np.clip(x * 255, 0.0, 255.0), dtype=np.uint8)
        _, _, H, W = x.shape
        x = x.reshape((rows, cols, 3, H, W))
        x = x.transpose(0, 3, 1, 4, 2)
        x = x.reshape((rows * H, cols * W, 3))

        preview_dir = '{}/preview'.format(dst)
        preview_path = preview_dir +\
            '/image{:0&gt;8}.png'.format(trainer.updater.iteration)
        if not os.path.exists(preview_dir):
            os.makedirs(preview_dir)
        Image.fromarray(x).save(preview_path)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 273:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10470')" href="javascript:;">
chainer-7.2.0/examples/chainermn/dcgan/updater.py: 30-48
</a>
<div class="mid" id="frag10470" style="display:none"><pre>
    def update_core(self):
        gen_optimizer = self.get_optimizer('gen')
        dis_optimizer = self.get_optimizer('dis')

        batch = self.get_iterator('main').next()
        x_real = Variable(self.converter(batch, self.device)) / 255.
        xp = chainer.backend.get_array_module(x_real.array)

        gen, dis = self.gen, self.dis
        batchsize = len(batch)

        y_real = dis(x_real)

        z = Variable(xp.asarray(gen.make_hidden(batchsize)))
        x_fake = gen(z)
        y_fake = dis(x_fake)

        dis_optimizer.update(self.loss_dis, dis, y_fake, y_real)
        gen_optimizer.update(self.loss_gen, gen, y_fake)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10607')" href="javascript:;">
chainer-7.2.0/examples/dcgan/updater.py: 27-45
</a>
<div class="mid" id="frag10607" style="display:none"><pre>
    def update_core(self):
        gen_optimizer = self.get_optimizer('gen')
        dis_optimizer = self.get_optimizer('dis')

        batch = self.get_iterator('main').next()
        device = self.device
        x_real = Variable(self.converter(batch, device)) / 255.

        gen, dis = self.gen, self.dis
        batchsize = len(batch)

        y_real = dis(x_real)

        z = Variable(device.xp.asarray(gen.make_hidden(batchsize)))
        x_fake = gen(z)
        y_fake = dis(x_fake)

        dis_optimizer.update(self.loss_dis, dis, y_fake, y_real)
        gen_optimizer.update(self.loss_gen, gen, y_fake)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 274:</b> &nbsp; 2 fragments, nominal size 39 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10504')" href="javascript:;">
chainer-7.2.0/examples/static_graph_optimizations/mnist/train_mnist_custom_loop.py: 30-72
</a>
<div class="mid" id="frag10504" style="display:none"><pre>
def run_train_loop(
        optimizer, train_iter, test_iter, test_count, epoch, device):
    model = optimizer.target

    train_count = 0
    sum_accuracy = 0
    sum_loss = 0
    while train_iter.epoch &lt; epoch:
        batch = train_iter.next()
        x_array, t_array = convert.concat_examples(batch, device)
        x = chainer.Variable(x_array)
        t = chainer.Variable(t_array, requires_grad=False)
        optimizer.update(model, x, t)
        train_count += len(t)
        sum_loss += float(model.loss.array) * len(t)
        sum_accuracy += float(model.accuracy.array) * len(t)

        if train_iter.is_new_epoch:
            print('epoch: ', train_iter.epoch)
            print('train mean loss: {}, accuracy: {}'.format(
                sum_loss / train_count, sum_accuracy / train_count))
            # evaluation
            train_count = 0
            sum_accuracy = 0
            sum_loss = 0
            # It is good practice to turn off train mode during evaluation.
            with configuration.using_config('train', False):
                for batch in test_iter:
                    x_array, t_array = convert.concat_examples(
                        batch, device)
                    x = chainer.Variable(x_array)
                    t = chainer.Variable(t_array, requires_grad=False)
                    loss = model(x, t)
                    sum_loss += float(loss.array) * len(t)
                    sum_accuracy += float(model.accuracy.array) * len(t)

            test_iter.reset()
            print('test mean  loss: {}, accuracy: {}'.format(
                sum_loss / test_count, sum_accuracy / test_count))
            sum_accuracy = 0
            sum_loss = 0


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10507')" href="javascript:;">
chainer-7.2.0/examples/static_graph_optimizations/cifar/train_cifar_custom_loop.py: 31-81
</a>
<div class="mid" id="frag10507" style="display:none"><pre>
def run_train_loop(
        optimizer, train_iter, test_iter, test_count, epoch,
        device):
    model = optimizer.target

    train_count = 0
    sum_accuracy = 0
    sum_loss = 0
    while train_iter.epoch &lt; epoch:
        batch = train_iter.next()
        # Reduce learning rate by 0.5 every 25 epochs.
        if train_iter.epoch % 25 == 0 and train_iter.is_new_epoch:
            optimizer.lr *= 0.5
            print('Reducing learning rate to: {}'.format(optimizer.lr))

        x_array, t_array = convert.concat_examples(batch, device)
        x = chainer.Variable(x_array)
        t = chainer.Variable(t_array, requires_grad=False)
        optimizer.update(model, x, t)
        train_count += len(t)
        sum_loss += float(model.loss.array) * len(t)
        sum_accuracy += float(model.accuracy.array) * len(t)

        if train_iter.is_new_epoch:
            print('epoch: {}'.format(train_iter.epoch))
            print('train mean loss: {}, accuracy: {}'.format(
                sum_loss / train_count, sum_accuracy / train_count))
            # evaluation
            train_count = 0
            sum_accuracy = 0
            sum_loss = 0
            model.predictor.train = False
            # It is good practice to turn off train mode during evaluation.
            with configuration.using_config('train', False):
                for batch in test_iter:
                    x_array, t_array = convert.concat_examples(
                        batch, device)
                    x = chainer.Variable(x_array)
                    t = chainer.Variable(t_array, requires_grad=False)
                    loss = model(x, t)
                    sum_loss += float(loss.array) * len(t)
                    sum_accuracy += float(model.accuracy.array) * len(t)

            test_iter.reset()
            model.predictor.train = True
            print('test mean  loss: {}, accuracy: {}'.format(
                sum_loss / test_count, sum_accuracy / test_count))
            sum_accuracy = 0
            sum_loss = 0


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 275:</b> &nbsp; 2 fragments, nominal size 63 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10505')" href="javascript:;">
chainer-7.2.0/examples/static_graph_optimizations/mnist/train_mnist_custom_loop.py: 73-150
</a>
<div class="mid" id="frag10505" style="display:none"><pre>
def main():
    parser = argparse.ArgumentParser(description='Chainer example: MNIST')
    parser.add_argument('--batchsize', '-b', type=int, default=100,
                        help='Number of images in each mini-batch')
    parser.add_argument('--epoch', '-e', type=int, default=20,
                        help='Number of sweeps over the dataset to train')
    parser.add_argument('--device', '-d', type=str, default='-1',
                        help='Device specifier. Either ChainerX device '
                        'specifier or an integer. If non-negative integer, '
                        'CuPy arrays with specified device id are used. If '
                        'negative integer, NumPy arrays are used')
    parser.add_argument('--out', '-o', default='result',
                        help='Directory to output the result')
    parser.add_argument('--model', '-m', default='MLP',
                        help='Choose the model: MLP or MLPSideEffect')
    parser.add_argument('--resume', '-r', default='',
                        help='Resume the training from snapshot')
    parser.add_argument('--unit', '-u', type=int, default=1000,
                        help='Number of units')
    group = parser.add_argument_group('deprecated arguments')
    group.add_argument('--gpu', '-g', dest='device',
                       type=int, nargs='?', const=0,
                       help='GPU ID (negative value indicates CPU)')
    args = parser.parse_args()

    if chainer.get_dtype() == numpy.float16:
        warnings.warn(
            'This example may cause NaN in FP16 mode.', RuntimeWarning)

    device = chainer.get_device(args.device)

    print('Device: {}'.format(device))
    print('# unit: {}'.format(args.unit))
    print('# Minibatch-size: {}'.format(args.batchsize))
    print('# epoch: {}'.format(args.epoch))
    print('')

    device.use()

    # Set up a neural network to train
    if args.model == 'MLP':
        model = L.Classifier(train_mnist.MLP(args.unit, 10))
    elif args.model == 'MLPSideEffect':
        model = L.Classifier(train_mnist.MLPSideEffect(args.unit, 10))
    model.to_device(device)

    # Setup an optimizer
    optimizer = chainer.optimizers.Adam()
    optimizer.setup(model)

    # Load the MNIST dataset
    train, test = chainer.datasets.get_mnist()

    test_count = len(test)

    train_iter = chainer.iterators.SerialIterator(train, args.batchsize)
    test_iter = chainer.iterators.SerialIterator(
        test, args.batchsize, repeat=False, shuffle=False)

    if device.xp is not chainerx:
        run_train_loop(
            optimizer, train_iter, test_iter, test_count, args.epoch, device)
    else:
        warnings.warn(
            'Static subgraph optimization does not support ChainerX and will'
            ' be disabled.', UserWarning)
        with chainer.using_config('use_static_graph', False):
            run_train_loop(
                optimizer, train_iter, test_iter, test_count, args.epoch,
                device)

    # Save the model and the optimizer
    print('save the model')
    serializers.save_npz('mlp.model', model)
    print('save the optimizer')
    serializers.save_npz('mlp.state', optimizer)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10508')" href="javascript:;">
chainer-7.2.0/examples/static_graph_optimizations/cifar/train_cifar_custom_loop.py: 82-171
</a>
<div class="mid" id="frag10508" style="display:none"><pre>
def main():
    parser = argparse.ArgumentParser(description='Chainer CIFAR example:')
    parser.add_argument('--dataset', default='cifar10',
                        help='The dataset to use: cifar10 or cifar100')
    parser.add_argument('--batchsize', '-b', type=int, default=64,
                        help='Number of images in each mini-batch')
    parser.add_argument('--learnrate', '-l', type=float, default=0.05,
                        help='Learning rate for SGD')
    parser.add_argument('--epoch', '-e', type=int, default=300,
                        help='Number of sweeps over the dataset to train')
    parser.add_argument('--device', '-d', type=str, default='0',
                        help='Device specifier. Either ChainerX device '
                        'specifier or an integer. If non-negative integer, '
                        'CuPy arrays with specified device id are used. If '
                        'negative integer, NumPy arrays are used')
    parser.add_argument('--out', '-o', default='result',
                        help='Directory to output the result')
    parser.add_argument('--test', action='store_true',
                        help='Use tiny datasets for quick tests')
    parser.add_argument('--resume', '-r', default='',
                        help='Resume the training from snapshot')
    group = parser.add_argument_group('deprecated arguments')
    group.add_argument('--gpu', '-g', dest='device',
                       type=int, nargs='?', const=0,
                       help='GPU ID (negative value indicates CPU)')
    args = parser.parse_args()

    if chainer.get_dtype() == numpy.float16:
        warnings.warn(
            'This example may cause NaN in FP16 mode.', RuntimeWarning)

    device = chainer.get_device(args.device)

    print('Device: {}'.format(device))
    print('# Minibatch-size: {}'.format(args.batchsize))
    print('# epoch: {}'.format(args.epoch))
    print('')

    device.use()

    # Set up a neural network to train.
    # Classifier reports softmax cross entropy loss and accuracy at every
    # iteration, which will be used by the PrintReport extension below.
    if args.dataset == 'cifar10':
        print('Using CIFAR10 dataset.')
        class_labels = 10
        train, test = get_cifar10()
    elif args.dataset == 'cifar100':
        print('Using CIFAR100 dataset.')
        class_labels = 100
        train, test = get_cifar100()
    else:
        raise RuntimeError('Invalid dataset choice.')

    if args.test:
        train = train[:200]
        test = test[:200]

    test_count = len(test)

    model = L.Classifier(models.VGG.VGG(class_labels))
    model.to_device(device)

    optimizer = chainer.optimizers.MomentumSGD(args.learnrate)
    optimizer.setup(model)
    optimizer.add_hook(chainer.optimizer.WeightDecay(5e-4))

    train_iter = chainer.iterators.SerialIterator(train, args.batchsize)
    test_iter = chainer.iterators.SerialIterator(
        test, args.batchsize, repeat=False, shuffle=False)

    if device.xp is not chainerx:
        run_train_loop(
            optimizer, train_iter, test_iter, test_count, args.epoch, device)
    else:
        warnings.warn(
            'Static subgraph optimization does not support ChainerX and will'
            ' be disabled.', UserWarning)
        with chainer.using_config('use_static_graph', False):
            run_train_loop(
                optimizer, train_iter, test_iter, test_count, args.epoch,
                device)

    # Save the model and the optimizer
    print('save the model')
    serializers.save_npz('mlp.model', model)
    print('save the optimizer')
    serializers.save_npz('mlp.state', optimizer)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 276:</b> &nbsp; 2 fragments, nominal size 94 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10540')" href="javascript:;">
chainer-7.2.0/examples/reinforcement_learning/dqn_cartpole.py: 80-211
</a>
<div class="mid" id="frag10540" style="display:none"><pre>
def main():

    parser = argparse.ArgumentParser(description='Chainer example: DQN')
    parser.add_argument('--env', type=str, default='CartPole-v0',
                        help='Name of the OpenAI Gym environment')
    parser.add_argument('--batch-size', '-b', type=int, default=64,
                        help='Number of transitions in each mini-batch')
    parser.add_argument('--episodes', '-e', type=int, default=1000,
                        help='Number of episodes to run')
    parser.add_argument('--device', '-d', type=str, default='-1',
                        help='Device specifier. Either ChainerX device '
                        'specifier or an integer. If non-negative integer, '
                        'CuPy arrays with specified device id are used. If '
                        'negative integer, NumPy arrays are used')
    parser.add_argument('--out', '-o', default='dqn_result',
                        help='Directory to output the result')
    parser.add_argument('--unit', '-u', type=int, default=100,
                        help='Number of units')
    parser.add_argument('--target-type', type=str, default='dqn',
                        help='Target type', choices=['dqn', 'double_dqn'])
    parser.add_argument('--reward-scale', type=float, default=1e-2,
                        help='Reward scale factor')
    parser.add_argument('--replay-start-size', type=int, default=500,
                        help=('Number of iterations after which replay is '
                              'started'))
    parser.add_argument('--iterations-to-decay-epsilon', type=int,
                        default=5000,
                        help='Number of steps used to linearly decay epsilon')
    parser.add_argument('--min-epsilon', type=float, default=0.01,
                        help='Minimum value of epsilon')
    parser.add_argument('--target-update-freq', type=int, default=100,
                        help='Frequency of target network update')
    parser.add_argument('--record', action='store_true', default=True,
                        help='Record performance')
    parser.add_argument('--no-record', action='store_false', dest='record')
    group = parser.add_argument_group('deprecated arguments')
    group.add_argument('--gpu', '-g', dest='device',
                       type=int, nargs='?', const=0,
                       help='GPU ID (negative value indicates CPU)')
    args = parser.parse_args()

    if chainer.get_dtype() == np.float16:
        warnings.warn(
            'This example may cause NaN in FP16 mode.', RuntimeWarning)

    device = chainer.get_device(args.device)
    device.use()

    # Initialize an environment
    env = gym.make(args.env)
    assert isinstance(env.observation_space, gym.spaces.Box)
    assert isinstance(env.action_space, gym.spaces.Discrete)
    obs_size = env.observation_space.low.size
    n_actions = env.action_space.n
    if args.record:
        env = gym.wrappers.Monitor(env, args.out, force=True)
    reward_threshold = env.spec.reward_threshold
    if reward_threshold is not None:
        print('{} defines "solving" as getting average reward of {} over 100 '
              'consecutive trials.'.format(args.env, reward_threshold))
    else:
        print('{} is an unsolved environment, which means it does not have a '
              'specified reward threshold at which it\'s considered '
              'solved.'.format(args.env))

    # Initialize variables
    D = collections.deque(maxlen=10 ** 6)  # Replay buffer
    Rs = collections.deque(maxlen=100)  # History of returns
    iteration = 0

    # Initialize a model and its optimizer
    Q = QFunction(obs_size, n_actions, n_units=args.unit)
    Q.to_device(device)
    target_Q = copy.deepcopy(Q)
    opt = optimizers.Adam(eps=1e-2)
    opt.setup(Q)

    for episode in range(args.episodes):

        obs = env.reset()
        done = False
        R = 0.0  # Return (sum of rewards obtained in an episode)
        timestep = 0

        while not done and timestep &lt; env.spec.timestep_limit:

            # Epsilon is linearly decayed
            epsilon = 1.0 if len(D) &lt; args.replay_start_size else \
                max(args.min_epsilon,
                    np.interp(
                        iteration,
                        [0, args.iterations_to_decay_epsilon],
                        [1.0, args.min_epsilon]))

            # Select an action epsilon-greedily
            if np.random.rand() &lt; epsilon:
                action = env.action_space.sample()
            else:
                action = get_greedy_action(Q, obs)

            # Execute an action
            new_obs, reward, done, _ = env.step(action)
            R += reward

            # Store a transition
            D.append((obs, action, reward * args.reward_scale, done, new_obs))
            obs = new_obs

            # Sample a random minibatch of transitions and replay
            if len(D) &gt;= args.replay_start_size:
                sample_indices = random.sample(range(len(D)), args.batch_size)
                samples = [D[i] for i in sample_indices]
                update(Q, target_Q, opt, samples, target_type=args.target_type)

            # Update the target network
            if iteration % args.target_update_freq == 0:
                target_Q = copy.deepcopy(Q)

            iteration += 1
            timestep += 1

        Rs.append(R)
        average_R = np.mean(Rs)
        print('episode: {} iteration: {} R: {} average_R: {}'.format(
              episode, iteration, R, average_R))

        if reward_threshold is not None and average_R &gt;= reward_threshold:
            print('Solved {} by getting average reward of '
                  '{} &gt;= {} over 100 consecutive episodes.'.format(
                      args.env, average_R, reward_threshold))
            break

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10551')" href="javascript:;">
chainer-7.2.0/examples/reinforcement_learning/ddpg_pendulum.py: 130-256
</a>
<div class="mid" id="frag10551" style="display:none"><pre>
def main():

    parser = argparse.ArgumentParser(description='Chainer example: DDPG')
    parser.add_argument('--env', type=str, default='Pendulum-v0',
                        help='Name of the OpenAI Gym environment')
    parser.add_argument('--batch-size', '-b', type=int, default=64,
                        help='Number of transitions in each mini-batch')
    parser.add_argument('--episodes', '-e', type=int, default=1000,
                        help='Number of episodes to run')
    parser.add_argument('--device', '-d', type=str, default='-1',
                        help='Device specifier. Either ChainerX device '
                        'specifier or an integer. If non-negative integer, '
                        'CuPy arrays with specified device id are used. If '
                        'negative integer, NumPy arrays are used')
    parser.add_argument('--out', '-o', default='ddpg_result',
                        help='Directory to output the result')
    parser.add_argument('--unit', '-u', type=int, default=100,
                        help='Number of units')
    parser.add_argument('--reward-scale', type=float, default=1e-3,
                        help='Reward scale factor')
    parser.add_argument('--replay-start-size', type=int, default=500,
                        help=('Number of iterations after which replay is '
                              'started'))
    parser.add_argument('--tau', type=float, default=1e-2,
                        help='Softness of soft target update (0, 1]')
    parser.add_argument('--noise-scale', type=float, default=0.4,
                        help='Scale of additive Gaussian noises')
    parser.add_argument('--record', action='store_true', default=True,
                        help='Record performance')
    parser.add_argument('--no-record', action='store_false', dest='record')
    group = parser.add_argument_group('deprecated arguments')
    group.add_argument('--gpu', '-g', dest='device',
                       type=int, nargs='?', const=0,
                       help='GPU ID (negative value indicates CPU)')
    args = parser.parse_args()

    if chainer.get_dtype() == np.float16:
        warnings.warn(
            'This example may cause NaN in FP16 mode.', RuntimeWarning)

    device = chainer.get_device(args.device)
    device.use()

    # Initialize an environment
    env = gym.make(args.env)
    assert isinstance(env.observation_space, gym.spaces.Box)
    assert isinstance(env.action_space, gym.spaces.Box)
    obs_size = env.observation_space.low.size
    action_size = env.action_space.low.size
    if args.record:
        env = gym.wrappers.Monitor(env, args.out, force=True)
    reward_threshold = env.spec.reward_threshold
    if reward_threshold is not None:
        print('{} defines "solving" as getting average reward of {} over 100 '
              'consecutive trials.'.format(args.env, reward_threshold))
    else:
        print('{} is an unsolved environment, which means it does not have a '
              'specified reward threshold at which it\'s considered '
              'solved.'.format(args.env))

    # Initialize variables
    D = collections.deque(maxlen=10 ** 6)  # Replay buffer
    Rs = collections.deque(maxlen=100)  # History of returns
    iteration = 0

    # Initialize models and optimizers
    Q = QFunction(obs_size, action_size, n_units=args.unit)
    policy = Policy(obs_size, action_size,
                    env.action_space.low, env.action_space.high,
                    n_units=args.unit)
    Q.to_device(device)
    policy.to_device(device)
    target_Q = copy.deepcopy(Q)
    target_policy = copy.deepcopy(policy)
    opt_Q = optimizers.Adam(eps=1e-5)  # Use larger eps in case of FP16 mode
    opt_Q.setup(Q)
    opt_policy = optimizers.Adam(alpha=1e-4)
    opt_policy.setup(policy)

    for episode in range(args.episodes):

        obs = env.reset()
        done = False
        R = 0.0  # Return (sum of rewards obtained in an episode)
        timestep = 0

        while not done and timestep &lt; env.spec.timestep_limit:

            # Select an action with additive noises for exploration
            action = (get_action(policy, obs) +
                      np.random.normal(scale=args.noise_scale))

            # Execute an action
            new_obs, reward, done, _ = env.step(
                np.clip(action, env.action_space.low, env.action_space.high))
            R += reward

            # Store a transition
            D.append((obs, action, reward * args.reward_scale, done, new_obs))
            obs = new_obs

            # Sample a random minibatch of transitions and replay
            if len(D) &gt;= args.replay_start_size:
                sample_indices = random.sample(range(len(D)), args.batch_size)
                samples = [D[i] for i in sample_indices]
                update(Q, target_Q, policy, target_policy,
                       opt_Q, opt_policy, samples)

            # Soft update of the target networks
            soft_copy_params(Q, target_Q, args.tau)
            soft_copy_params(policy, target_policy, args.tau)

            iteration += 1
            timestep += 1

        Rs.append(R)
        average_R = np.mean(Rs)
        print('episode: {} iteration: {} R:{} average_R:{}'.format(
              episode, iteration, R, average_R))

        if reward_threshold is not None and average_R &gt;= reward_threshold:
            print('Solved {} by getting average reward of '
                  '{} &gt;= {} over 100 consecutive episodes.'.format(
                      args.env, average_R, reward_threshold))
            break


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 277:</b> &nbsp; 2 fragments, nominal size 26 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10556')" href="javascript:;">
chainer-7.2.0/examples/imagenet/dali_util.py: 28-62
</a>
<div class="mid" id="frag10556" style="display:none"><pre>
    def __init__(self, file_list, file_root, crop_size,
                 batch_size, n_threads, device_id,
                 random_shuffle=True, seed=-1, mean=None, std=None,
                 n_samples=None):
        super(DaliPipelineTrain, self).__init__(batch_size, n_threads,
                                                device_id, seed=seed)
        crop_size = _pair(crop_size)
        if mean is None:
            mean = [0.485 * 255, 0.456 * 255, 0.406 * 255]
        if std is None:
            std = [0.229 * 255, 0.224 * 255, 0.225 * 255]
        if n_samples is None:
            initial_fill = 4096
        else:
            initial_fill = min(4096, n_samples)
        self.loader = ops.FileReader(file_root=file_root, file_list=file_list,
                                     random_shuffle=random_shuffle,
                                     initial_fill=initial_fill)
        self.decode = ops.HostDecoder()
        self.resize = ops.Resize(device='gpu', resize_x=256, resize_y=256)
        # self.hue = ops.Hue(device="gpu")
        # self.bright = ops.Brightness(device="gpu")
        # self.cntrst = ops.Contrast(device="gpu")
        # self.rotate = ops.Rotate(device="gpu")
        # self.jitter = ops.Jitter(device="gpu")
        random_area = (crop_size[0] / 256.0) * (crop_size[1] / 256.0)
        random_area = _pair(random_area)
        random_aspect_ratio = _pair(1.0)
        self.rrcrop = ops.RandomResizedCrop(
            device='gpu', size=crop_size, random_area=random_area,
            random_aspect_ratio=random_aspect_ratio)
        self.cmnorm = ops.CropMirrorNormalize(
            device='gpu', crop=list(crop_size), mean=mean, std=std)
        self.coin = ops.CoinFlip(probability=0.5)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10558')" href="javascript:;">
chainer-7.2.0/examples/imagenet/dali_util.py: 82-104
</a>
<div class="mid" id="frag10558" style="display:none"><pre>
    def __init__(self, file_list, file_root, crop_size,
                 batch_size, n_threads, device_id,
                 random_shuffle=False, seed=-1, mean=None, std=None,
                 n_samples=None):
        super(DaliPipelineVal, self).__init__(batch_size, n_threads,
                                              device_id, seed=seed)
        crop_size = _pair(crop_size)
        if mean is None:
            mean = [0.485 * 255, 0.456 * 255, 0.406 * 255]
        if std is None:
            std = [0.229 * 255, 0.224 * 255, 0.225 * 255]
        if n_samples is None:
            initial_fill = 512
        else:
            initial_fill = min(512, n_samples)
        self.loader = ops.FileReader(file_root=file_root, file_list=file_list,
                                     random_shuffle=random_shuffle,
                                     initial_fill=initial_fill)
        self.decode = ops.HostDecoder()
        self.resize = ops.Resize(device='gpu', resize_x=256, resize_y=256)
        self.cmnorm = ops.CropMirrorNormalize(
            device='gpu', crop=list(crop_size), mean=mean, std=std)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 278:</b> &nbsp; 2 fragments, nominal size 25 lines, similarity 77%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10561')" href="javascript:;">
chainer-7.2.0/examples/imagenet/dali_util.py: 125-159
</a>
<div class="mid" id="frag10561" style="display:none"><pre>
    def __call__(self, inputs, device=None):
        """Convert DALI arrays to Numpy/CuPy arrays"""

        xp = chainer.backend.get_array_module(self.perturbation)
        if xp is not cuda.cupy:
            self.perturbation = cuda.to_gpu(self.perturbation, device)

        outputs = []
        for i in range(len(inputs)):
            x = inputs[i].as_tensor()
            if (isinstance(x, dali.backend_impl.TensorCPU)):
                x = np.array(x)
                if x.ndim == 2 and x.shape[1] == 1:
                    x = x.squeeze(axis=1)
                if device is not None and device &gt;= 0:
                    x = cuda.to_gpu(x, device)
            elif (isinstance(x, dali.backend_impl.TensorGPU)):
                x_cupy = cuda.cupy.empty(shape=x.shape(), dtype=x.dtype())
                # Synchronization is necessary here to avoid data corruption
                # because DALI and CuPy will use different CUDA streams.
                cuda.cupy.cuda.runtime.deviceSynchronize()
                # copy data from DALI array to CuPy array
                x.copy_to_external(ctypes.c_void_p(x_cupy.data.ptr))
                cuda.cupy.cuda.runtime.deviceSynchronize()
                x = x_cupy.astype(chainer.get_dtype())
                if self.perturbation is not None:
                    x = x - self.perturbation
                if device is not None and device &lt; 0:
                    x = cuda.to_cpu(x)
            else:
                raise ValueError('Unexpected object')
            outputs.append(x)
        return tuple(outputs)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10562')" href="javascript:;">
chainer-7.2.0/examples/imagenet/dali_util.py: 160-186
</a>
<div class="mid" id="frag10562" style="display:none"><pre>
def dali_converter(inputs, device=None):
    """Convert DALI arrays to Numpy/CuPy arrays"""

    outputs = []
    for i in range(len(inputs)):
        x = inputs[i].as_tensor()
        if (isinstance(x, dali.backend_impl.TensorCPU)):
            x = np.array(x)
            if x.ndim == 2 and x.shape[1] == 1:
                x = x.squeeze(axis=1)
            if device is not None and device &gt;= 0:
                x = cuda.to_gpu(x, device)
        elif (isinstance(x, dali.backend_impl.TensorGPU)):
            x_cupy = cuda.cupy.empty(shape=x.shape(), dtype=x.dtype())
            # Synchronization is necessary here to avoid data corruption
            # because DALI and CuPy will use different CUDA streams.
            cuda.cupy.cuda.runtime.deviceSynchronize()
            # copy data from DALI array to CuPy array
            x.copy_to_external(ctypes.c_void_p(x_cupy.data.ptr))
            cuda.cupy.cuda.runtime.deviceSynchronize()
            x = x_cupy.astype(chainer.get_dtype())
            if device is not None and device &lt; 0:
                x = cuda.to_cpu(x)
        else:
            raise ValueError('Unexpected object')
        outputs.append(x)
    return tuple(outputs)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 279:</b> &nbsp; 2 fragments, nominal size 121 lines, similarity 98%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10563')" href="javascript:;">
chainer-7.2.0/examples/imagenet/train_imagenet.py: 32-185
</a>
<div class="mid" id="frag10563" style="display:none"><pre>
def main():
    archs = {
        'alex': alex.Alex,
        'googlenet': googlenet.GoogLeNet,
        'googlenetbn': googlenetbn.GoogLeNetBN,
        'nin': nin.NIN,
        'resnet50': resnet50.ResNet50,
        'resnext50': resnext50.ResNeXt50,
        'resnet50_nhwc': resnet50.ResNet50_Nhwc,
    }

    dtypes = {
        'float16': np.float16,
        'float32': np.float32,
        'float64': np.float64,
    }

    parser = argparse.ArgumentParser(
        description='Learning convnet from ILSVRC2012 dataset')
    parser.add_argument('train', help='Path to training image-label list file')
    parser.add_argument('val', help='Path to validation image-label list file')
    parser.add_argument('--arch', '-a', choices=archs.keys(), default='nin',
                        help='Convnet architecture')
    parser.add_argument('--batchsize', '-B', type=int, default=32,
                        help='Learning minibatch size')
    parser.add_argument('--dtype', choices=dtypes, help='Specify the dtype '
                        'used. If not supplied, the default dtype is used')
    parser.add_argument('--epoch', '-E', type=int, default=10,
                        help='Number of epochs to train')
    parser.add_argument('--device', '-d', type=str, default='-1',
                        help='Device specifier. Either ChainerX device '
                        'specifier or an integer. If non-negative integer, '
                        'CuPy arrays with specified device id are used. If '
                        'negative integer, NumPy arrays are used')
    parser.add_argument('--initmodel',
                        help='Initialize the model from given file')
    parser.add_argument('--loaderjob', '-j', type=int,
                        help='Number of parallel data loading processes')
    parser.add_argument('--mean', '-m', default='mean.npy',
                        help='Mean file (computed by compute_mean.py)')
    parser.add_argument('--resume', '-r', default='',
                        help='Initialize the trainer from given file')
    parser.add_argument('--out', '-o', default='result',
                        help='Output directory')
    parser.add_argument('--root', '-R', default='.',
                        help='Root directory path of image files')
    parser.add_argument('--val_batchsize', '-b', type=int, default=250,
                        help='Validation minibatch size')
    parser.add_argument('--test', action='store_true')
    parser.set_defaults(test=False)
    parser.add_argument('--dali', action='store_true')
    parser.set_defaults(dali=False)
    group = parser.add_argument_group('deprecated arguments')
    group.add_argument('--gpu', '-g', dest='device',
                       type=int, nargs='?', const=0,
                       help='GPU ID (negative value indicates CPU)')
    args = parser.parse_args()

    device = chainer.get_device(args.device)

    # Set the dtype if supplied.
    if args.dtype is not None:
        chainer.config.dtype = args.dtype

    print('Device: {}'.format(device))
    print('Dtype: {}'.format(chainer.config.dtype))
    print('# Minibatch-size: {}'.format(args.batchsize))
    print('# epoch: {}'.format(args.epoch))
    print('')

    # Initialize the model to train
    model = archs[args.arch]()
    if args.initmodel:
        print('Load model from {}'.format(args.initmodel))
        chainer.serializers.load_npz(args.initmodel, model)
    model.to_device(device)
    device.use()

    # Load the mean file
    mean = np.load(args.mean)
    if args.dali:
        if not dali_util._dali_available:
            raise RuntimeError('DALI seems not available on your system.')
        if device.xp is not chainer.backend.cuda.cupy:
            raise RuntimeError('Using DALI requires GPU device. Please '
                               'specify it with --device option.')
        n_threads = args.loaderjob
        if n_threads is None or n_threads &lt;= 0:
            n_threads = 1
        ch_mean = list(np.average(mean, axis=(1, 2)))
        ch_std = [255.0, 255.0, 255.0]
        # Setup DALI pipelines
        train_pipe = dali_util.DaliPipelineTrain(
            args.train, args.root, model.insize, args.batchsize,
            n_threads, device.device.id, True, mean=ch_mean, std=ch_std)
        val_pipe = dali_util.DaliPipelineVal(
            args.val, args.root, model.insize, args.val_batchsize,
            n_threads, device.device.id, False, mean=ch_mean, std=ch_std)
        train_iter = chainer.iterators.DaliIterator(train_pipe)
        val_iter = chainer.iterators.DaliIterator(val_pipe, repeat=False)
        # converter = dali_converter
        converter = dali_util.DaliConverter(mean=mean, crop_size=model.insize)
    else:
        # Load the dataset files
        train = PreprocessedDataset(args.train, args.root, mean, model.insize)
        val = PreprocessedDataset(args.val, args.root, mean, model.insize,
                                  False)
        # These iterators load the images with subprocesses running in parallel
        # to the training/validation.
        train_iter = chainer.iterators.MultiprocessIterator(
            train, args.batchsize, n_processes=args.loaderjob)
        val_iter = chainer.iterators.MultiprocessIterator(
            val, args.val_batchsize, repeat=False, n_processes=args.loaderjob)
        converter = dataset.concat_examples

    # Set up an optimizer
    optimizer = chainer.optimizers.MomentumSGD(lr=0.01, momentum=0.9)
    optimizer.setup(model)

    # Set up a trainer
    updater = training.updaters.StandardUpdater(
        train_iter, optimizer, converter=converter, device=device)
    trainer = training.Trainer(updater, (args.epoch, 'epoch'), args.out)

    val_interval = (100000, 'iteration')
    log_interval = (1000, 'iteration')
    if args.test:
        val_interval = (1, 'iteration')
        log_interval = (1, 'iteration')

    trainer.extend(extensions.Evaluator(val_iter, model, converter=converter,
                                        device=device), trigger=val_interval)
    # TODO(sonots): Temporarily disabled for chainerx. Fix it.
    if device.xp is not chainerx:
        trainer.extend(extensions.DumpGraph('main/loss'))
    trainer.extend(extensions.snapshot(), trigger=val_interval)
    trainer.extend(extensions.snapshot_object(
        model, 'model_iter_{.updater.iteration}'), trigger=val_interval)
    # Be careful to pass the interval directly to LogReport
    # (it determines when to emit log rather than when to read observations)
    trainer.extend(extensions.LogReport(trigger=log_interval))
    trainer.extend(extensions.observe_lr(), trigger=log_interval)
    trainer.extend(extensions.PrintReport([
        'epoch', 'iteration', 'main/loss', 'validation/main/loss',
        'main/accuracy', 'validation/main/accuracy', 'lr'
    ]), trigger=log_interval)
    trainer.extend(extensions.ProgressBar(update_interval=10))

    if args.resume:
        chainer.serializers.load_npz(args.resume, trainer)

    trainer.run()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10591')" href="javascript:;">
chainer-7.2.0/examples/imagenet/.testdata/replacements/train_imagenet.py: 32-189
</a>
<div class="mid" id="frag10591" style="display:none"><pre>
def main():
    archs = {
        'alex': alex.Alex,
        'googlenet': googlenet.GoogLeNet,
        'googlenetbn': googlenetbn.GoogLeNetBN,
        'nin': nin.NIN,
        'resnet50': resnet50.ResNet50,
        'resnext50': resnext50.ResNeXt50,
        'resnet50_nhwc': resnet50.ResNet50_Nhwc,
    }

    dtypes = {
        'float16': np.float16,
        'float32': np.float32,
        'float64': np.float64,
    }

    parser = argparse.ArgumentParser(
        description='Learning convnet from ILSVRC2012 dataset')
    parser.add_argument('train', help='Path to training image-label list file')
    parser.add_argument('val', help='Path to validation image-label list file')
    parser.add_argument('--arch', '-a', choices=archs.keys(), default='nin',
                        help='Convnet architecture')
    parser.add_argument('--batchsize', '-B', type=int, default=32,
                        help='Learning minibatch size')
    parser.add_argument('--dtype', choices=dtypes, help='Specify the dtype '
                        'used. If not supplied, the default dtype is used')
    parser.add_argument('--epoch', '-E', type=int, default=10,
                        help='Number of epochs to train')
    parser.add_argument('--device', '-d', type=str, default='-1',
                        help='Device specifier. Either ChainerX device '
                        'specifier or an integer. If non-negative integer, '
                        'CuPy arrays with specified device id are used. If '
                        'negative integer, NumPy arrays are used')
    parser.add_argument('--initmodel',
                        help='Initialize the model from given file')
    parser.add_argument('--loaderjob', '-j', type=int,
                        help='Number of parallel data loading processes')
    parser.add_argument('--mean', '-m', default='mean.npy',
                        help='Mean file (computed by compute_mean.py)')
    parser.add_argument('--resume', '-r', default='',
                        help='Initialize the trainer from given file')
    parser.add_argument('--out', '-o', default='result',
                        help='Output directory')
    parser.add_argument('--root', '-R', default='.',
                        help='Root directory path of image files')
    parser.add_argument('--val_batchsize', '-b', type=int, default=250,
                        help='Validation minibatch size')
    parser.add_argument('--test', action='store_true')
    parser.set_defaults(test=False)
    parser.add_argument('--dali', action='store_true')
    parser.set_defaults(dali=False)
    group = parser.add_argument_group('deprecated arguments')
    group.add_argument('--gpu', '-g', dest='device',
                       type=int, nargs='?', const=0,
                       help='GPU ID (negative value indicates CPU)')
    args = parser.parse_args()

    device = chainer.get_device(args.device)

    # Set the dtype if supplied.
    if args.dtype is not None:
        chainer.config.dtype = args.dtype

    print('Device: {}'.format(device))
    print('Dtype: {}'.format(chainer.config.dtype))
    print('# Minibatch-size: {}'.format(args.batchsize))
    print('# epoch: {}'.format(args.epoch))
    print('')

    # Initialize the model to train
    model = archs[args.arch]()
    if args.initmodel:
        print('Load model from {}'.format(args.initmodel))
        chainer.serializers.load_npz(args.initmodel, model)
    model.to_device(device)
    device.use()

    # Load the mean file
    mean = np.load(args.mean)
    if args.dali:
        if not dali_util._dali_available:
            raise RuntimeError('DALI seems not available on your system.')
        if device.xp is not chainer.backend.cuda.cupy:
            raise RuntimeError('Using DALI requires GPU device. Please '
                               'specify it with --device option.')
        n_threads = args.loaderjob
        if n_threads is None or n_threads &lt;= 0:
            n_threads = 1
        ch_mean = list(np.average(mean, axis=(1, 2)))
        ch_std = [255.0, 255.0, 255.0]
        # Setup DALI pipelines
        train_pipe = dali_util.DaliPipelineTrain(
            args.train, args.root, model.insize, args.batchsize,
            n_threads, device.device.id, True, mean=ch_mean, std=ch_std)
        val_pipe = dali_util.DaliPipelineVal(
            args.val, args.root, model.insize, args.val_batchsize,
            n_threads, device.device.id, False, mean=ch_mean, std=ch_std)
        train_iter = chainer.iterators.DaliIterator(train_pipe)
        val_iter = chainer.iterators.DaliIterator(val_pipe, repeat=False)
        # converter = dali_converter
        converter = dali_util.DaliConverter(mean=mean, crop_size=model.insize)
    else:
        # Load the dataset files
        train = PreprocessedDataset(args.train, args.root, mean, model.insize)
        val = PreprocessedDataset(args.val, args.root, mean, model.insize,
                                  False)
        # These iterators load the images with subprocesses running in parallel
        # to the training/validation.
        train_iter = chainer.iterators.MultiprocessIterator(
            train, args.batchsize, n_processes=args.loaderjob)
        val_iter = chainer.iterators.MultiprocessIterator(
            val, args.val_batchsize, repeat=False, n_processes=args.loaderjob)
        converter = dataset.concat_examples

    # Set up an optimizer
    optimizer = chainer.optimizers.MomentumSGD(lr=0.01, momentum=0.9)
    optimizer.setup(model)

    # Set up a trainer
    updater = training.updaters.StandardUpdater(
        train_iter, optimizer, converter=converter, device=device)
    trainer = training.Trainer(updater, (args.epoch, 'epoch'), args.out)

    val_interval = (100000, 'iteration')
    log_interval = (1000, 'iteration')
    if args.test:
        val_interval = (1, 'iteration')
        log_interval = (1, 'iteration')
    # BEGIN ADDITIONAL TEST CODE
    val_interval = (1, 'iteration')
    log_interval = (1, 'iteration')
    # END ADDITIONAL TEST CODE

    trainer.extend(extensions.Evaluator(val_iter, model, converter=converter,
                                        device=device), trigger=val_interval)
    # TODO(sonots): Temporarily disabled for chainerx. Fix it.
    if device.xp is not chainerx:
        trainer.extend(extensions.DumpGraph('main/loss'))
    trainer.extend(extensions.snapshot(), trigger=val_interval)
    trainer.extend(extensions.snapshot_object(
        model, 'model_iter_{.updater.iteration}'), trigger=val_interval)
    # Be careful to pass the interval directly to LogReport
    # (it determines when to emit log rather than when to read observations)
    trainer.extend(extensions.LogReport(trigger=log_interval))
    trainer.extend(extensions.observe_lr(), trigger=log_interval)
    trainer.extend(extensions.PrintReport([
        'epoch', 'iteration', 'main/loss', 'validation/main/loss',
        'main/accuracy', 'validation/main/accuracy', 'lr'
    ]), trigger=log_interval)
    trainer.extend(extensions.ProgressBar(update_interval=10))

    if args.resume:
        chainer.serializers.load_npz(args.resume, trainer)

    trainer.run()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 280:</b> &nbsp; 9 fragments, nominal size 17 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10688')" href="javascript:;">
chainer-7.2.0/chainerx/_docs/device.py: 5-53
</a>
<div class="mid" id="frag10688" style="display:none"><pre>
def _set_docs_device():
    Device = chainerx.Device

    _docs.set_doc(
        Device,
        """Represents a physical computing unit.
""")

    _docs.set_doc(
        Device.synchronize,
        """Synchronizes the device.
""")

    _docs.set_doc(
        Device.name,
        """Device name.

It is the backend name and the device index concatenated with a colon, e.g.
``native:0``.

Returns:
    str: Device name.
""")

    _docs.set_doc(
        Device.backend,
        """Backend to which this device belongs.

Returns:
    ~chainerx.Backend: Backend object.
""")

    _docs.set_doc(
        Device.context,
        """Context to which this device belongs.

Returns:
    ~chainerx.Context: Context object.
""")

    _docs.set_doc(
        Device.index,
        """Index of this device.

Returns:
    int: Index of this device.
""")


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10691')" href="javascript:;">
chainer-7.2.0/chainerx/_docs/backend.py: 5-54
</a>
<div class="mid" id="frag10691" style="display:none"><pre>
def _set_docs_backend():
    Backend = chainerx.Backend

    _docs.set_doc(
        Backend,
        """Pluggable entity that abstracts various computing platforms.

A backend holds one or more :class:`~chainerx.Device`\\ s, each of which
represents a physical computing unit.
""")

    _docs.set_doc(
        Backend.name,
        """Backend name.

Returns:
    str: Backend name.
""")

    _docs.set_doc(
        Backend.context,
        """Context to which this backend belongs.

Returns:
    ~chainerx.Context: Context object.

""")

    _docs.set_doc(
        Backend.get_device,
        """get_device(index)
Returns a device specified by the given index.

Args:
    index (int): Device index.

Returns:
    ~chainerx.Device: Device object.
""")

    _docs.set_doc(
        Backend.get_device_count,
        """get_device_count()
Returns the number of devices available in this backend.

Returns:
    int: Number of devices.
""")


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10693')" href="javascript:;">
chainer-7.2.0/chainerx/_docs/backprop.py: 5-147
</a>
<div class="mid" id="frag10693" style="display:none"><pre>
def set_docs():
    _docs.set_doc(
        chainerx.backward,
        """backward(outputs, *, enable_double_backprop=False)
Runs backpropagation.

On backpropagation (a.k.a. backprop),
the computational graph is traversed backward starting from the output arrays,
up until the root arrays on which :func:`ndarray.require_grad()` have been
called.

Backpropagation uses :data:`ndarray.grad &lt;chainerx.ndarray.grad&gt;` held by
the output arrays as the initial gradients.
You can manually assign them before calling this function.
Otherwise, they are assumed to be 1.

To enable higher order differentiation, pass ``enable_double_backprop=True``
so that you can further run backpropagation from the resulting gradient arrays.
Note that enabling it results in larger memory consumption needed to store the
gradients w.r.t intermediate arrays that are required for the second gradient
computation.

Note:
    The whole process of backpropagation is executed in C++, except those
    operations whose backward computation falls back to the corresponding
    Python implementation. Currently this function does not release the GIL at
    all.

Args:
    outputs (~chainerx.ndarray or list of ndarrays):
        Output arrays from which backpropagation starts.
    enable_double_backprop (bool): If ``True``,
        a computational trace of the whole backpropagation procedure is
        recorded to the computational graph so that one can further do
        backpropagation from the resulting gradients.

.. seealso::
    * :meth:`chainerx.ndarray.backward`
""")

    _docs.set_doc(
        chainerx.grad,
        """grad(outputs, inputs, *, enable_double_backprop=False)
Computes and returns the gradients of the outputs w.r.t. the inputs.

This function differs from :func:`chainerx.backward` in the sense that
gradients are returned instead of being added to the gradients held by the
inputs. Gradients held by the inputs are not modified. Also, instead of
traversing through the whole graph starting from the outputs, a sub-graph is
extracted for computation. This means that is is more efficient, especially
for larger computational graphs.

Args:
    outputs (list of ndarrays):
        Output arrays from which backpropagation starts.
    inputs (list of ndarrays):
        Input arrays of which this function computes the gradients w.r.t.
    enable_double_backprop (bool): If ``True``,
        a computational trace of the whole backpropagation procedure is
        recorded to the computational graph so that one can further do
        backpropagation from the resulting gradients.

Returns:
    list of :class:`~chainerx.ndarray`\\ s:
        A list of gradients. The list always has the same length as the number
        of inputs.

.. seealso::
    * :func:`chainerx.backward`
    * :func:`chainer.grad`
""")

    _docs.set_doc(
        chainerx.no_backprop_mode,
        """no_backprop_mode()
Creates a context manager which temporarily disables backpropagation.

Within this context, no computational graph will be formed unless
:meth:`~chainerx.force_backprop_mode` is used.

Arrays resulting from operations enclosed with this context will be
disconnected from the computational graph. Trying to perform backpropagation
from such arrays would result in an error.

.. code-block:: py

    x = chainerx.array([4, 3], numpy.float32)
    x.require_grad()

    with chainerx.no_backprop_mode():
        y = 2 * x + 1

    y.backward()  # ! error

Benefits of ``no_backprop_mode`` include reduced CPU overhead of building
computational graphs, and reduced consumption of device memory that
would be otherwise retained for backward propagation.

.. seealso::
    * :func:`chainerx.force_backprop_mode`
    * :func:`chainerx.is_backprop_required`
    * :func:`chainer.no_backprop_mode`
""")

    _docs.set_doc(
        chainerx.force_backprop_mode,
        """force_backprop_mode()
Creates a context manager which temporarily enables backpropagation.

This context re-enables backpropagation that is disabled by
any surrounding :func:`~chainerx.no_backprop_mode` context.

.. code-block:: py

    x = chainerx.array([4, 3], numpy.float32)
    x.require_grad()

    with chainerx.no_backprop_mode():
        with chainerx.force_backprop_mode():
            y = 2 * x + 1

    y.backward()
    x.grad
    # array([2., 2.], shape=(2,), dtype=float32, device='native:0')

.. seealso::
    * :func:`chainerx.no_backprop_mode`
    * :func:`chainerx.is_backprop_required`
    * :func:`chainer.force_backprop_mode`
""")

    _docs.set_doc(
        chainerx.is_backprop_required,
        """is_backprop_required()
Returns whether the backpropagation is enabled in the current thread.

The result is affect by :func:`chainerx.no_backprop_mode` and
:func:`chainerx.force_backprop_mode`.

.. seealso::
    * :func:`chainerx.no_backprop_mode`
    * :func:`chainerx.force_backprop_mode`
""")
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10700')" href="javascript:;">
chainer-7.2.0/chainerx/_docs/routines.py: 1119-1273
</a>
<div class="mid" id="frag10700" style="display:none"><pre>
def _docs_loss():
    _docs.set_doc(
        chainerx.absolute_error,
        """Element-wise absolute error function.

Computes the element-wise absolute error :math:`L` between two inputs
:math:`x_1` and :math:`x_2` defined as follows.

.. math::
    L = |x_1 - x_2|

Args:
    x1 (~chainerx.ndarray): Input variable.
    x2 (~chainerx.ndarray): Input variable.

Returns:
    :class:`~chainerx.ndarray`: A variable holding an array representing
    the absolute error of two inputs.

.. seealso:: :func:`chainer.functions.absolute_error`
""")

    _docs.set_doc(
        chainerx.squared_error,
        """Element-wise squared error function.

Computes the element-wise squared error :math:`L` between two inputs
:math:`x_1` and :math:`x_2` defined as follows.

.. math::
    L = (x_1 - x_2)^2

Can be used to compute mean squared error by just calling `mean()`
on the output array.

Args:
    x0 (~chainerx.ndarray): Input variable.
    x1 (~chainerx.ndarray): Input variable.

Returns:
    :class:`~chainerx.ndarray`: A variable holding an array representing
    the squared error of two inputs.

.. seealso:: :func:`chainer.functions.squared_error`
""")

    _docs.set_doc(
        chainerx.huber_loss,
        """Element-wise Huber loss.

The Huber loss is similar to the squared error but is less sensitive to
outliers in the data. It is defined as

.. math::

    L_{\\delta}(a) = \\left \\{ \\begin{array}{cc}
    \\frac{1}{2} a^2 &amp; {\\rm if~|a| \\leq \\delta} \\\\
    \\delta (|a| - \\frac{1}{2} \\delta) &amp; {\\rm otherwise,}
    \\end{array} \\right.

where :math:`a = x - t` is the difference between the input :math:`x`
and the target :math:`t`.

See: `Huber loss - Wikipedia &lt;https://en.wikipedia.org/wiki/Huber_loss&gt;`_.

Args:
    x (~chainerx.ndarray): Input variable.
    t (~chainerx.ndarray): Target variable for regression.
    delta (float): Constant variable for Huber loss function as used in
        definition.

Returns:
    :class:`~chainerx.ndarray`:
        A variable object holding an array representing the Huber loss
        :math:`L_{\\delta}` of the two inputs.

.. seealso:: :func:`chainer.functions.huber_loss`
""")

    _docs.set_doc(
        chainerx.gaussian_kl_divergence,
        """Element-wise KL-divergence of Gaussian variables from the standard one.

Given two variable ``mean`` representing :math:`\\mu` and ``ln_var``
representing :math:`\\log(\\sigma^2)`, this function calculates
the element-wise KL-divergence between the given multi-dimensional
Gaussian :math:`N(\\mu, S)` and the standard Gaussian :math:`N(0, I)`

.. math::

   D_{\\mathbf{KL}}(N(\\mu, S) \\| N(0, I)),

where :math:`S` is a diagonal matrix such that :math:`S_{ii} = \\sigma_i^2`
and :math:`I` is an identity matrix.

Args:
    mean (~chainerx.ndarray):
        A variable representing mean of given
        gaussian distribution, :math:`\\mu`.
    ln_var (~chainerx.ndarray):
        A variable representing logarithm of
        variance of given gaussian distribution, :math:`\\log(\\sigma^2)`.

Returns:
    :class:`~chainerx.ndarray`:
        A variable representing KL-divergence between
        given gaussian distribution and the standard gaussian.

.. seealso:: :func:`chainer.functions.gaussian_kl_divergence`
""")

    _docs.set_doc(
        chainerx.sigmoid_cross_entropy,
        """sigmoid_cross_entropy(x1, x2)

Element-wise cross entropy loss for pre-sigmoid activations.

Args:
    x1 (~chainerx.ndarray): An array whose (i, j)-th element indicates the
        unnormalized log probability of the j-th unit at the i-th example.
    x2 (~chainerx.ndarray): An array whose (i, j)-th element indicates a signed
        integer vector of ground truth labels 0 or 1. If ``x2[i, j] == -1``,
        corresponding ``x1[i, j]`` is ignored. Loss is zero if all ground truth
        labels are -1.

Returns:
    :class:`~chainerx.ndarray`: An array of the cross entropy.

Note:
    During backpropagation, this function propagates the gradient of the output
    array to the input array ``x1`` only.
""")

    _docs.set_doc(
        chainerx.softmax_cross_entropy,
        """softmax_cross_entropy(x1, x2)

Element-wise cross entropy loss for pre-softmax activations.

Args:
    x1 (~chainerx.ndarray): An array whose element indicates unnormalized log
        probability: the first axis of the array represents the number of
        samples, and the second axis represents the number of classes.
    x2 (~chainerx.ndarray): A signed integer vector of ground truth labels. If
        ``x2[i] == -1``, corresponding ``x1[i]`` is ignored.

Returns:
    :class:`~chainerx.ndarray`: An array of the cross entropy.

Note:
    During backpropagation, this function propagates the gradient of the output
    array to the input array ``x1`` only.
""")


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10708')" href="javascript:;">
chainer-7.2.0/chainerx/_docs/routines.py: 3362-3945
</a>
<div class="mid" id="frag10708" style="display:none"><pre>
def _docs_rnn():
    _docs.set_doc(
        chainerx.n_step_lstm,
        """n_step_lstm(n_layers, hx, cx, ws, bs, xs)
    Stacked Uni-directional Long Short-Term Memory function.

This function calculates stacked Uni-directional LSTM with sequences.
This function gets an initial hidden state :math:`h_0`, an initial cell
state :math:`c_0`, an input sequence :math:`x`, weight matrices :math:`W`,
and bias vectors :math:`b`.
This function calculates hidden states :math:`h_t` and :math:`c_t` for each
time :math:`t` from input :math:`x_t`.

.. math::
   i_t &amp;= \\sigma(W_0 x_t + W_4 h_{t-1} + b_0 + b_4) \\\\
   f_t &amp;= \\sigma(W_1 x_t + W_5 h_{t-1} + b_1 + b_5) \\\\
   o_t &amp;= \\sigma(W_2 x_t + W_6 h_{t-1} + b_2 + b_6) \\\\
   a_t &amp;= \\tanh(W_3 x_t + W_7 h_{t-1} + b_3 + b_7) \\\\
   c_t &amp;= f_t \\cdot c_{t-1} + i_t \\cdot a_t \\\\
   h_t &amp;= o_t \\cdot \\tanh(c_t)

As the function accepts a sequence, it calculates :math:`h_t` for all
:math:`t` with one call. Eight weight matrices and eight bias vectors are
required for each layer. So, when :math:`S` layers exist, you need to
prepare :math:`8S` weight matrices and :math:`8S` bias vectors.
If the number of layers ``n_layers`` is greater than :math:`1`, the input
of the ``k``-th layer is the hidden state ``h_t`` of the ``k-1``-th layer.
Note that all input variables except the first layer may have different
shape from the first layer.

Args:
    n_layers(int): The number of layers.
    hx (:class:`~chainerx.array`):
        Variable holding stacked hidden states.
        Its shape is ``(S, B, N)`` where ``S`` is the number of layers and
        is equal to ``n_layers``, ``B`` is the mini-batch size, and ``N``
        is the dimension of the hidden units.
    cx (:class:`~chainerx.array`): Variable holding stacked cell states.
        It has the same shape as ``hx``.
    ws (list of list of :class:`~chainerx.array`): Weight matrices.
        ``ws[i]`` represents the weights for the i-th layer.
        Each ``ws[i]`` is a list containing eight matrices.
        ``ws[i][j]`` corresponds to :math:`W_j` in the equation.
        Only ``ws[0][j]`` where ``0 &lt;= j &lt; 4`` are ``(N, I)``-shaped as
        they are multiplied with input variables, where ``I`` is the size
        of the input and ``N`` is the dimension of the hidden units. All
        other matrices are ``(N, N)``-shaped.
    bs (list of list of :class:`~chainerx.array`): Bias vectors.
        ``bs[i]`` represents the biases for the i-th layer.
        Each ``bs[i]`` is a list containing eight vectors.
        ``bs[i][j]`` corresponds to :math:`b_j` in the equation.
        The shape of each matrix is ``(N,)`` where ``N`` is the dimension
        of the hidden units.
    xs (list of :class:`~chainerx.array`):
        A list of :class:`~chainerx.array`
        holding input values. Each element ``xs[t]`` holds input value
        for time ``t``. Its shape is ``(B_t, I)``, where ``B_t`` is the
        mini-batch size for time ``t``.
        When sequences has different lengths, they must be
        sorted in descending order of their lengths.
        So ``xs`` needs to satisfy
        ``xs[t].shape[0] &gt;= xs[t + 1].shape[0]``.

Returns:
    tuple: This function returns a tuple containing three elements,
    ``hy``, ``cy`` and ``ys``.

    - ``hy`` is an updated hidden states whose shape is the same as
      ``hx``.
    - ``cy`` is an updated cell states whose shape is the same as
      ``cx``.
    - ``ys`` is a list of :class:`~chainerx.array` . Each element
      ``ys[t]`` holds hidden states of the last layer corresponding
      to an input ``xs[t]``. Its shape is ``(B_t, N)`` where ``B_t`` is
      the mini-batch size for time ``t``, and ``N`` is size of hidden
      units. Note that ``B_t`` is the same value as ``xs[t]``.

.. note::
   The dimension of hidden units is limited to only one size ``N``. If you
   want to use variable dimension of hidden units, please use
   :class:`chainerx.lstm`.

.. seealso::
   :func:`chainerx.lstm`

.. admonition:: Example

    &gt;&gt;&gt; import chainerx as chx
    &gt;&gt;&gt; batchs = [3, 2, 1]  # support variable length sequences
    &gt;&gt;&gt; in_size, out_size, n_layers = 3, 2, 2
    &gt;&gt;&gt; xs = [chx.ones((b, in_size)).astype(chx.float32) for b in batchs]
    &gt;&gt;&gt; [x.shape for x in xs]
    [(3, 3), (2, 3), (1, 3)]
    &gt;&gt;&gt; h_shape = (n_layers, batchs[0], out_size)
    &gt;&gt;&gt; hx = chx.ones(h_shape).astype(chx.float32)
    &gt;&gt;&gt; cx = chx.ones(h_shape).astype(chx.float32)
    &gt;&gt;&gt; w_in = lambda i, j: in_size if i == 0 and j &lt; 4 else out_size
    &gt;&gt;&gt; ws = []
    &gt;&gt;&gt; bs = []
    &gt;&gt;&gt; for n in range(n_layers):
    ...     ws.append([chx.ones((out_size, w_in(n, i))).\
astype(np.float32) for i in range(8)])
    ...     bs.append([chx.ones((out_size,)).astype(chx.float32) \
for _ in range(8)])
    ...
    &gt;&gt;&gt; ws[0][0].shape  # ws[0][:4].shape are (out_size, in_size)
    (2, 3)
    &gt;&gt;&gt; ws[1][0].shape  # others are (out_size, out_size)
    (2, 2)
    &gt;&gt;&gt; bs[0][0].shape
    (2,)
    &gt;&gt;&gt; hy, cy, ys = chx.n_step_lstm(
    ...     n_layers, hx, cx, ws, bs, xs)
    &gt;&gt;&gt; hy.shape
    (2, 3, 2)
    &gt;&gt;&gt; cy.shape
    (2, 3, 2)
    &gt;&gt;&gt; [y.shape for y in ys]
    [(3, 2), (2, 2), (1, 2)]
""")

    _docs.set_doc(
        chainerx.n_step_bilstm,
        """n_step_bilstm(n_layers, hx, cx, ws, bs, xs)
Stacked Bi-directional Long Short-Term Memory function.
This function calculates stacked Bi-directional LSTM with sequences.
This function gets an initial hidden state :math:`h_0`, an initial cell
state :math:`c_0`, an input sequence :math:`x`, weight matrices :math:`W`,
and bias vectors :math:`b`.
This function calculates hidden states :math:`h_t` and :math:`c_t` for each
time :math:`t` from input :math:`x_t`.

.. math::
    i^{f}_t &amp;=&amp; \\sigma(W^{f}_0 x_t + W^{f}_4 h_{t-1} + b^{f}_0 + b^{f}_4),
    \\\\
    f^{f}_t &amp;=&amp; \\sigma(W^{f}_1 x_t + W^{f}_5 h_{t-1} + b^{f}_1 + b^{f}_5),
    \\\\
    o^{f}_t &amp;=&amp; \\sigma(W^{f}_2 x_t + W^{f}_6 h_{t-1} + b^{f}_2 + b^{f}_6),
    \\\\
    a^{f}_t &amp;=&amp; \\tanh(W^{f}_3 x_t + W^{f}_7 h_{t-1} + b^{f}_3 + b^{f}_7),
    \\\\
    c^{f}_t &amp;=&amp; f^{f}_t \\cdot c^{f}_{t-1} + i^{f}_t \\cdot a^{f}_t,
    \\\\
    h^{f}_t &amp;=&amp; o^{f}_t \\cdot \\tanh(c^{f}_t),
    \\\\
    i^{b}_t &amp;=&amp; \\sigma(W^{b}_0 x_t + W^{b}_4 h_{t-1} + b^{b}_0 + b^{b}_4),
    \\\\
    f^{b}_t &amp;=&amp; \\sigma(W^{b}_1 x_t + W^{b}_5 h_{t-1} + b^{b}_1 + b^{b}_5),
    \\\\
    o^{b}_t &amp;=&amp; \\sigma(W^{b}_2 x_t + W^{b}_6 h_{t-1} + b^{b}_2 + b^{b}_6),
    \\\\
    a^{b}_t &amp;=&amp; \\tanh(W^{b}_3 x_t + W^{b}_7 h_{t-1} + b^{b}_3 + b^{b}_7),
    \\\\
    c^{b}_t &amp;=&amp; f^{b}_t \\cdot c^{b}_{t-1} + i^{b}_t \\cdot a^{b}_t, \\\\
    h^{b}_t &amp;=&amp; o^{b}_t \\cdot \\tanh(c^{b}_t), \\\\
    h_t &amp;=&amp; [h^{f}_t; h^{b}_t]

where :math:`W^{f}` is the weight matrices for forward-LSTM, :math:`W^{b}`
is weight matrices for backward-LSTM.
As the function accepts a sequence, it calculates :math:`h_t` for all
:math:`t` with one call. Eight weight matrices and eight bias vectors are
required for each layer of each direction. So, when :math:`S` layers
exist, you need to prepare :math:`16S` weight matrices and :math:`16S`
bias vectors.
If the number of layers ``n_layers`` is greater than :math:`1`, the input
of the ``k``-th layer is the hidden state ``h_t`` of the ``k-1``-th layer.
Note that all input variables except the first layer may have different
shape from the first layer.

Args:
    n_layers(int): The number of layers.
    hx (:class:`~chainerx.array`):
        Variable holding stacked hidden states.
        Its shape is ``(2S, B, N)`` where ``S`` is the number of layers and
        is equal to ``n_layers``, ``B`` is the mini-batch size, and ``N``
        is the dimension of the hidden units. Because of bi-direction, the
        first dimension length is ``2S``.
    cx (:class:`~chainerx.array`): Variable holding stacked cell states.
        It has the same shape as ``hx``.
    ws (list of list of :class:`~chainerx.array`): Weight matrices.
        ``ws[2 * l + m]`` represents the weights for the l-th layer of
        the m-th direction. (``m == 0`` means the forward direction and
        ``m == 1`` means the backward direction.) Each ``ws[i]`` is a
        list containing eight matrices. ``ws[i][j]`` corresponds to
        :math:`W_j` in the equation. ``ws[0][j]`` and ``ws[1][j]`` where
        ``0 &lt;= j &lt; 4`` are ``(N, I)``-shaped because they are multiplied
        with input variables, where ``I`` is the size of the input.
        ``ws[i][j]`` where ``2 &lt;= i`` and ``0 &lt;= j &lt; 4`` are
        ``(N, 2N)``-shaped because they are multiplied with two hidden
        layers :math:`h_t = [h^{f}_t; h^{b}_t]`. All other matrices are
        ``(N, N)``-shaped.
    bs (list of list of :class:`~chainerx.array`): Bias vectors.
        ``bs[2 * l + m]`` represents the weights for the l-th layer of
        m-th direction. (``m == 0`` means the forward direction and
        ``m == 1`` means the backward direction.)
        Each ``bs[i]`` is a list containing eight vectors.
        ``bs[i][j]`` corresponds to :math:`b_j` in the equation.
        The shape of each matrix is ``(N,)``.
    xs (list of :class:`~chainerx.array`):
        A list of :class:`~chainerx.array`
        holding input values. Each element ``xs[t]`` holds input value
        for time ``t``. Its shape is ``(B_t, I)``, where ``B_t`` is the
        mini-batch size for time ``t``.
        When sequences has different lengths, they must be
        sorted in descending order of their lengths.
        So ``xs`` needs to satisfy
        ``xs[t].shape[0] &gt;= xs[t + 1].shape[0]``.

Returns:
    tuple: This function returns a tuple containing three elements,
    ``hy``, ``cy`` and ``ys``.

    - ``hy`` is an updated hidden states whose shape is the same as
      ``hx``.
    - ``cy`` is an updated cell states whose shape is the same as
      ``cx``.
    - ``ys`` is a list of :class:`~chainer.array` . Each element
      ``ys[t]`` holds hidden states of the last layer corresponding
      to an input ``xs[t]``. Its shape is ``(B_t, 2N)`` where ``B_t``
      is the mini-batch size for time ``t``, and ``N`` is size of
      hidden units. Note that ``B_t`` is the same value as ``xs[t]``.

.. admonition:: Example

    &gt;&gt;&gt; import chainerx as chx
    &gt;&gt;&gt; batchs = [3, 2, 1]  # support variable length sequences
    &gt;&gt;&gt; in_size, out_size, n_layers = 3, 2, 2
    &gt;&gt;&gt; dropout_ratio = 0.0
    &gt;&gt;&gt; xs = [chx.ones((b, in_size)).astype(chx.float32) for b in batchs]
    &gt;&gt;&gt; [x.shape for x in xs]
    [(3, 3), (2, 3), (1, 3)]
    &gt;&gt;&gt; h_shape = (n_layers * 2, batchs[0], out_size)
    &gt;&gt;&gt; hx = chx.ones(h_shape).astype(chx.float32)
    &gt;&gt;&gt; cx = chx.ones(h_shape).astype(chx.float32)
    &gt;&gt;&gt; def w_in(i, j):
    ...     if i == 0 and j &lt; 4:
    ...         return in_size
    ...     elif i &gt; 0 and j &lt; 4:
    ...         return out_size * 2
    ...     else:
    ...         return out_size
    ...
    &gt;&gt;&gt; ws = []
    &gt;&gt;&gt; bs = []
    &gt;&gt;&gt; for n in range(n_layers):
    ...     for direction in (0, 1):
    ...         ws.append([chx.ones((out_size, w_in(n, i))).\
astype(np.float32) for i in range(8)])
    ...         bs.append([chx.ones((out_size,)).astype(chx.float32) \
for _ in range(8)])
    ...
    &gt;&gt;&gt; ws[0][0].shape  # ws[0:2][:4].shape are (out_size, in_size)
    (2, 3)
    &gt;&gt;&gt; ws[2][0].shape  # ws[2:][:4].shape are (out_size, 2 * out_size)
    (2, 4)
    &gt;&gt;&gt; ws[0][4].shape  # others are (out_size, out_size)
    (2, 2)
    &gt;&gt;&gt; bs[0][0].shape
    (2,)
    &gt;&gt;&gt; hy, cy, ys = chx.n_step_bilstm(
    ...     n_layers, hx, cx, ws, bs, xs)
    &gt;&gt;&gt; hy.shape
    (4, 3, 2)
    &gt;&gt;&gt; cy.shape
    (4, 3, 2)
    &gt;&gt;&gt; [y.shape for y in ys]
    [(3, 4), (2, 4), (1, 4)]
    """)

    _docs.set_doc(
        chainerx.n_step_gru,
        """n_step_gru(n_layers, hx, ws, bs, xs)
Stacked Uni-directional Gated Recurrent Unit function.
This function calculates stacked Uni-directional GRU with sequences.
This function gets an initial hidden state :math:`h_0`, an input
sequence :math:`x`, weight matrices :math:`W`, and bias vectors :math:`b`.
This function calculates hidden states :math:`h_t` for each time :math:`t`
from input :math:`x_t`.

.. math::
   r_t &amp;= \\sigma(W_0 x_t + W_3 h_{t-1} + b_0 + b_3) \\\\
   z_t &amp;= \\sigma(W_1 x_t + W_4 h_{t-1} + b_1 + b_4) \\\\
   h'_t &amp;= \\tanh(W_2 x_t + b_2 + r_t \\cdot (W_5 h_{t-1} + b_5)) \\\\
   h_t &amp;= (1 - z_t) \\cdot h'_t + z_t \\cdot h_{t-1}

As the function accepts a sequence, it calculates :math:`h_t` for all
:math:`t` with one call. Six weight matrices and six bias vectors are
required for each layers. So, when :math:`S` layers exists, you need to
prepare :math:`6S` weight matrices and :math:`6S` bias vectors.
If the number of layers ``n_layers`` is greather than :math:`1`, input
of ``k``-th layer is hidden state ``h_t`` of ``k-1``-th layer.
Note that all input variables except first layer may have different shape
from the first layer.

Args:
    n_layers(int): Number of layers.
    hx (~chainerx.array):
        Variable holding stacked hidden states.
        Its shape is ``(S, B, N)`` where ``S`` is number of layers and is
        equal to ``n_layers``, ``B`` is mini-batch size, and ``N`` is
        dimension of hidden units.
    ws (list of list of :class:`~chainerx.array`): Weight matrices.
        ``ws[i]`` represents weights for i-th layer.
        Each ``ws[i]`` is a list containing six matrices.
        ``ws[i][j]`` is corresponding with ``W_j`` in the equation.
        Only ``ws[0][j]`` where ``0 &lt;= j &lt; 3`` is ``(N, I)`` shape as they
        are multiplied with input variables. All other matrices has
        ``(N, N)`` shape.
    bs (list of list of :class:`~chainerx.array`): Bias vectors.
        ``bs[i]`` represnents biases for i-th layer.
        Each ``bs[i]`` is a list containing six vectors.
        ``bs[i][j]`` is corresponding with ``b_j`` in the equation.
        Shape of each matrix is ``(N,)`` where ``N`` is dimension of
        hidden units.
    xs (list of :class:`~chainerx.array`):
        A list of :class:`~chainerx.array`
        holding input values. Each element ``xs[t]`` holds input value
        for time ``t``. Its shape is ``(B_t, I)``, where ``B_t`` is
        mini-batch size for time ``t``, and ``I`` is size of input units.
        Note that this function supports variable length sequences.
        When sequneces has different lengths, sort sequences in descending
        order by length.
        So ``xs`` needs to satisfy
        ``xs[t].shape[0] &gt;= xs[t + 1].shape[0]``.

Returns:
    tuple: This function returns a tuple containing two elements,
    ``hy`` and ``ys``.

    - ``hy`` is an updated hidden states whose shape is same as ``hx``.
    - ``ys`` is a list of :class:`~chainerx.array` . Each element
      ``ys[t]`` holds hidden states of the last layer corresponding
      to an input ``xs[t]``. Its shape is ``(B_t, N)`` where ``B_t`` is
      mini-batch size for time ``t``, and ``N`` is size of hidden
      units. Note that ``B_t`` is the same value as ``xs[t]``
    """)

    _docs.set_doc(
        chainerx.n_step_bigru,
        """n_step_bigru(n_layers, hx, ws, bs, xs)
Stacked Bi-directional Gated Recurrent Unit function.
This function calculates stacked Bi-directional GRU with sequences.
This function gets an initial hidden state :math:`h_0`, an input
sequence :math:`x`, weight matrices :math:`W`, and bias vectors :math:`b`.
This function calculates hidden states :math:`h_t` for each time :math:`t`
from input :math:`x_t`.

.. math::
   r^{f}_t &amp;= \\sigma(W^{f}_0 x_t + W^{f}_3 h_{t-1} + b^{f}_0 + b^{f}_3)
   \\\\
   z^{f}_t &amp;= \\sigma(W^{f}_1 x_t + W^{f}_4 h_{t-1} + b^{f}_1 + b^{f}_4)
   \\\\
   h^{f'}_t &amp;= \\tanh(W^{f}_2 x_t + b^{f}_2 + r^{f}_t \\cdot (W^{f}_5
   h_{t-1} + b^{f}_5)) \\\\
   h^{f}_t &amp;= (1 - z^{f}_t) \\cdot h^{f'}_t + z^{f}_t \\cdot h_{t-1}
   \\\\
   r^{b}_t &amp;= \\sigma(W^{b}_0 x_t + W^{b}_3 h_{t-1} + b^{b}_0 + b^{b}_3)
   \\\\
   z^{b}_t &amp;= \\sigma(W^{b}_1 x_t + W^{b}_4 h_{t-1} + b^{b}_1 + b^{b}_4)
   \\\\
   h^{b'}_t &amp;= \\tanh(W^{b}_2 x_t + b^{b}_2 + r^{b}_t \\cdot (W^{b}_5
   h_{t-1} + b^{b}_5)) \\\\
   h^{b}_t &amp;= (1 - z^{b}_t) \\cdot h^{b'}_t + z^{b}_t \\cdot h_{t-1}
   \\\\
   h_t  &amp;= [h^{f}_t; h^{b}_t] \\\\

where :math:`W^{f}` is weight matrices for forward-GRU, :math:`W^{b}` is
weight matrices for backward-GRU.
As the function accepts a sequence, it calculates :math:`h_t` for all
:math:`t` with one call. Six weight matrices and six bias vectors are
required for each layers. So, when :math:`S` layers exists, you need to
prepare :math:`6S` weight matrices and :math:`6S` bias vectors.
If the number of layers ``n_layers`` is greather than :math:`1`, input
of ``k``-th layer is hidden state ``h_t`` of ``k-1``-th layer.
Note that all input variables except first layer may have different shape
from the first layer.

Args:
    n_layers(int): Number of layers.
    hx (:class:`~chainerx.array`):
        Variable holding stacked hidden states.
        Its shape is ``(2S, B, N)`` where ``S`` is number of layers and is
        equal to ``n_layers``, ``B`` is mini-batch size, and ``N`` is
        dimension of hidden units.
    ws (list of list of :class:`~chainerx.array`): Weight matrices.
        ``ws[i]`` represents weights for i-th layer.
        Each ``ws[i]`` is a list containing six matrices.
        ``ws[i][j]`` is corresponding with ``W_j`` in the equation.
        Only ``ws[0][j]`` where ``0 &lt;= j &lt; 3`` is ``(N, I)`` shape as they
        are multiplied with input variables. All other matrices has
        ``(N, N)`` shape.
    bs (list of list of :class:`~chainerx.array`): Bias vectors.
        ``bs[i]`` represnents biases for i-th layer.
        Each ``bs[i]`` is a list containing six vectors.
        ``bs[i][j]`` is corresponding with ``b_j`` in the equation.
        Shape of each matrix is ``(N,)`` where ``N`` is dimension of
        hidden units.
    xs (list of :class:`~chainerx.array`):
        A list of :class:`~chainerx.array` holding input values.
        Each element ``xs[t]`` holds input value
        for time ``t``. Its shape is ``(B_t, I)``, where ``B_t`` is
        mini-batch size for time ``t``, and ``I`` is size of input units.
        Note that this function supports variable length sequences.
        When sequneces has different lengths, sort sequences in descending
        order by length.
        So ``xs`` needs to satisfy
        ``xs[t].shape[0] &gt;= xs[t + 1].shape[0]``.

Returns:
    tuple: This function returns a tuple containing two elements,
    ``hy`` and ``ys``.

    - ``hy`` is an updated hidden states whose shape is same as ``hx``.
    - ``ys`` is a list of :class:`~chainerx.array` . Each element
      ``ys[t]`` holds hidden states of the last layer corresponding
      to an input ``xs[t]``. Its shape is ``(B_t, N)`` where ``B_t`` is
      mini-batch size for time ``t``, and ``N`` is size of hidden
      units. Note that ``B_t`` is the same value as ``xs[t]``.
    """)

    _docs.set_doc(
        chainerx.n_step_rnn,
        """n_step_rnn(n_layers, hx, ws, bs, xs, activation='tanh')
Stacked Uni-directional RNN function for sequence inputs.
This function calculates stacked Uni-directional RNN with sequences.
This function gets an initial hidden state :math:`h_0`,
an initial cell state :math:`c_0`, an input sequence :math:`x`,
weight matrices :math:`W`, and bias vectors :math:`b`.
This function calculates hidden states :math:`h_t` and :math:`c_t` for each
time :math:`t` from input :math:`x_t`.

.. math::
   h_t = f(W_0 x_t + W_1 h_{t-1} + b_0 + b_1)

where :math:`f` is an activation function.
Weight matrices :math:`W` contains two matrices :math:`W_0` and
:math:`W_1`. :math:`W_0` is a parameter for an input sequence.
:math:`W_1` is a parameter for a hidden state.
Bias matrices :math:`b` contains two matrices :math:`b_0` and :math:`b_1`.
:math:`b_0` is a parameter for an input sequence.
:math:`b_1` is a parameter for a hidden state.
As the function accepts a sequence, it calculates :math:`h_t` for all
:math:`t` with one call. Two weight matrices and two bias vectors are
required for each layer. So, when :math:`S` layers exist, you need to
prepare :math:`2S` weight matrices and :math:`2S` bias vectors.
If the number of layers ``n_layers`` is greather than :math:`1`, input
of ``k``-th layer is hidden state ``h_t`` of ``k-1``-th layer.
Note that all input variables except first layer may have different shape
from the first layer.

Args:
    n_layers(int): Number of layers.
    hx (:class:`~chainerx.array`):
        Variable holding stacked hidden states.
        Its shape is ``(S, B, N)`` where ``S`` is number of layers and is
        equal to ``n_layers``, ``B`` is mini-batch size, and ``N`` is
        dimension of hidden units.
    ws (list of list of :class:`~chainerx.array`): Weight matrices.
        ``ws[i]`` represents weights for i-th layer.
        Each ``ws[i]`` is a list containing two matrices.
        ``ws[i][j]`` is corresponding with ``W_j`` in the equation.
        Only ``ws[0][j]`` where ``0 &lt;= j &lt; 1`` is ``(N, I)`` shape as they
        are multiplied with input variables. All other matrices has
        ``(N, N)`` shape.
    bs (list of list of :class:`~chainerx.array`): Bias vectors.
        ``bs[i]`` represnents biases for i-th layer.
        Each ``bs[i]`` is a list containing two vectors.
        ``bs[i][j]`` is corresponding with ``b_j`` in the equation.
        Shape of each matrix is ``(N,)`` where ``N`` is dimension of
        hidden units.
    xs (list of :class:`~chainerx.array`):
        A list of :class:`~chainerx.array` holding input values.
        Each element ``xs[t]`` holds input value for time ``t``.
        Its shape is ``(B_t, I)``, where ``B_t`` is
        mini-batch size for time ``t``, and ``I`` is size of input units.
        Note that this function supports variable length sequences.
        When sequneces has different lengths, sort sequences in descending
        order by length.
        So ``xs`` needs to satisfy
        ``xs[t].shape[0] &gt;= xs[t + 1].shape[0]``.
    activation (str): Activation function name.
        Please select ``tanh`` or ``relu``.

Returns:
    tuple: This function returns a tuple containing two elements,
    ``hy`` and ``ys``.

    - ``hy`` is an updated hidden states whose shape is same as ``hx``.
    - ``ys`` is a list of :class:`~chainerx.array` . Each element
      ``ys[t]`` holds hidden states of the last layer corresponding
      to an input ``xs[t]``. Its shape is ``(B_t, N)`` where ``B_t`` is
      mini-batch size for time ``t``, and ``N`` is size of hidden
      units. Note that ``B_t`` is the same value as ``xs[t]``.
    """)

    _docs.set_doc(
        chainerx.n_step_birnn,
        """n_step_birnn(n_layers, hx, ws, bs, xs, activation='tanh')
Stacked Bi-directional RNN function for sequence inputs.
This function calculates stacked Bi-directional RNN with sequences.
This function gets an initial hidden state :math:`h_0`, an initial
cell state :math:`c_0`, an input sequence :math:`x`,
weight matrices :math:`W`, and bias vectors :math:`b`.
This function calculates hidden states :math:`h_t` and :math:`c_t` for each
time :math:`t` from input :math:`x_t`.

.. math::
    h^{f}_t &amp;=&amp; f(W^{f}_0 x_t + W^{f}_1 h_{t-1} + b^{f}_0 + b^{f}_1), \\\\
    h^{b}_t &amp;=&amp; f(W^{b}_0 x_t + W^{b}_1 h_{t-1} + b^{b}_0 + b^{b}_1), \\\\
    h_t  &amp;=&amp; [h^{f}_t; h^{f}_t], \\\\

where :math:`f` is an activation function.
Weight matrices :math:`W` contains two matrices :math:`W^{f}` and
:math:`W^{b}`. :math:`W^{f}` is weight matrices for forward directional
RNN. :math:`W^{b}` is weight matrices for backward directional RNN.
:math:`W^{f}` contains :math:`W^{f}_0` for an input sequence and
:math:`W^{f}_1` for a hidden state.
:math:`W^{b}` contains :math:`W^{b}_0` for an input sequence and
:math:`W^{b}_1` for a hidden state.
Bias matrices :math:`b` contains two matrices :math:`b^{f}` and
:math:`b^{f}`. :math:`b^{f}` contains :math:`b^{f}_0` for an input sequence
and :math:`b^{f}_1` for a hidden state.
:math:`b^{b}` contains :math:`b^{b}_0` for an input sequence and
:math:`b^{b}_1` for a hidden state.
As the function accepts a sequence, it calculates :math:`h_t` for all
:math:`t` with one call. Two weight matrices and two bias vectors are
required for each layer. So, when :math:`S` layers exist, you need to
prepare :math:`2S` weight matrices and :math:`2S` bias vectors.
If the number of layers ``n_layers`` is greather than :math:`1`, input
of ``k``-th layer is hidden state ``h_t`` of ``k-1``-th layer.
Note that all input variables except first layer may have different shape
from the first layer.

Args:
    n_layers(int): Number of layers.
    hx (:class:`~chainerx.array`):
        Variable holding stacked hidden states.
        Its shape is ``(2S, B, N)`` where ``S`` is number of layers and is
        equal to ``n_layers``, ``B`` is mini-batch size, and ``N`` is
        dimension of hidden units. Because of bi-direction, the
        first dimension length is ``2S``.
    ws (list of list of :class:`~chainerx.array`): Weight matrices.
        ``ws[i + di]`` represents weights for i-th layer.
        Note that ``di = 0`` for forward-RNN and ``di = 1`` for
        backward-RNN.
        Each ``ws[i + di]`` is a list containing two matrices.
        ``ws[i + di][j]`` is corresponding with ``W^{f}_j`` if ``di = 0``
        and corresponding with ``W^{b}_j`` if ``di = 1`` in the equation.
        Only ``ws[0][j]`` and ``ws[1][j]`` where ``0 &lt;= j &lt; 1`` are
        ``(I, N)`` shape as they are multiplied with input variables.
        All other matrices has ``(N, N)`` shape.
    bs (list of list of :class:`~chainerx.array`): Bias vectors.
        ``bs[i + di]`` represnents biases for i-th layer.
        Note that ``di = 0`` for forward-RNN and ``di = 1`` for
        backward-RNN.
        Each ``bs[i + di]`` is a list containing two vectors.
        ``bs[i + di][j]`` is corresponding with ``b^{f}_j`` if ``di = 0``
        and corresponding with ``b^{b}_j`` if ``di = 1`` in the equation.
        Shape of each matrix is ``(N,)`` where ``N`` is dimension of
        hidden units.
    xs (list of :class:`~chainerx.array`):
        A list of :class:`~chainerx.array` holding input values.
        Each element ``xs[t]`` holds input value
        for time ``t``. Its shape is ``(B_t, I)``, where ``B_t`` is
        mini-batch size for time ``t``, and ``I`` is size of input units.
        Note that this function supports variable length sequences.
        When sequneces has different lengths, sort sequences in descending
        order by length.
        So ``xs`` needs to satisfy
        ``xs[t].shape[0] &gt;= xs[t + 1].shape[0]``.
    activation (str): Activation function name.
        Please select ``tanh`` or ``relu``.

Returns:
    tuple: This function returns a tuple containing two elements,
    ``hy`` and ``ys``.

    - ``hy`` is an updated hidden states whose shape is same as ``hx``.
    - ``ys`` is a list of :class:`~chainerx.array` . Each element
      ``ys[t]`` holds hidden states of the last layer corresponding
      to an input ``xs[t]``. Its shape is ``(B_t, N)`` where ``B_t``
      is mini-batch size for time ``t``, and ``N`` is size of hidden
      units. Note that ``B_t`` is the same value as ``xs[t]``.
    """)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10705')" href="javascript:;">
chainer-7.2.0/chainerx/_docs/routines.py: 2853-3224
</a>
<div class="mid" id="frag10705" style="display:none"><pre>
def _docs_connection():
    _docs.set_doc(
        chainerx.conv,
        """conv(x, w, b=None, stride=1, pad=0, cover_all=False)
N-dimensional convolution.

This is an implementation of N-dimensional convolution which is generalized
two-dimensional convolution in ConvNets. It takes three arrays: the
input ``x``, the filter weight ``w`` and the bias vector ``b``.

Notation: here is a notation for dimensionalities.

- :math:`N` is the number of spatial dimensions.
- :math:`n` is the batch size.
- :math:`c_I` and :math:`c_O` are the number of the input and output
  channels, respectively.
- :math:`d_1, d_2, ..., d_N` are the size of each axis of the input's
  spatial dimensions, respectively.
- :math:`k_1, k_2, ..., k_N` are the size of each axis of the filters,
  respectively.
- :math:`l_1, l_2, ..., l_N` are the size of each axis of the output's
  spatial dimensions, respectively.
- :math:`p_1, p_2, ..., p_N` are the size of each axis of the spatial
  padding size, respectively.

Then the ``conv`` function computes correlations between filters
and patches of size :math:`(k_1, k_2, ..., k_N)` in ``x``.
Note that correlation here is equivalent to the inner product between
expanded tensors.
Patches are extracted at positions shifted by multiples of ``stride`` from
the first position ``(-p_1, -p_2, ..., -p_N)`` for each spatial axis.

Let :math:`(s_1, s_2, ..., s_N)` be the stride of filter application.
Then, the output size :math:`(l_1, l_2, ..., l_N)` is determined by the
following equations:

.. math::

   l_n = (d_n + 2p_n - k_n) / s_n + 1 \\ \\ (n = 1, ..., N)

If ``cover_all`` option is ``True``, the filter will cover the all
spatial locations. So, if the last stride of filter does not cover the
end of spatial locations, an additional stride will be applied to the end
part of spatial locations. In this case, the output size is determined by
the following equations:

.. math::

   l_n = (d_n + 2p_n - k_n + s_n - 1) / s_n + 1 \\ \\ (n = 1, ..., N)

Args:
    x (:class:`~chainerx.ndarray`):
        Input array of shape :math:`(n, c_I, d_1, d_2, ..., d_N)`.
    w (:class:`~chainerx.ndarray`):
        Weight array of shape :math:`(c_O, c_I, k_1, k_2, ..., k_N)`.
    b (None or :class:`~chainerx.ndarray`):
        One-dimensional bias array with length :math:`c_O` (optional).
    stride (:class:`int` or :class:`tuple` of :class:`int` s):
        Stride of filter applications :math:`(s_1, s_2, ..., s_N)`.
        ``stride=s`` is equivalent to ``(s, s, ..., s)``.
    pad (:class:`int` or :class:`tuple` of :class:`int` s):
        Spatial padding width for input arrays
        :math:`(p_1, p_2, ..., p_N)`. ``pad=p`` is equivalent to
        ``(p, p, ..., p)``.
    cover_all (bool): If ``True``, all spatial locations are convoluted
        into some output pixels. It may make the output size larger.
        `cover_all` needs to be ``False`` if you want to use ``cuda`` backend.

Returns:
    ~chainerx.ndarray:
        Output array of shape :math:`(n, c_O, l_1, l_2, ..., l_N)`.

Note:

    In ``cuda`` backend, this function uses cuDNN implementation for its
    forward and backward computation.

Note:

    In ``cuda`` backend, this function has following limitations yet:

    - The ``cover_all=True`` option is not supported yet.
    - The ``dtype`` must be ``float32`` or ``float64`` (``float16`` is not
      supported yet.)

Note:

    During backpropagation, this function propagates the gradient of the
    output array to input arrays ``x``, ``w``, and ``b``.

.. seealso:: :func:`chainer.functions.convolution_nd`

.. admonition:: Example

    &gt;&gt;&gt; n = 10
    &gt;&gt;&gt; c_i, c_o = 3, 1
    &gt;&gt;&gt; d1, d2, d3 = 30, 40, 50
    &gt;&gt;&gt; k1, k2, k3 = 10, 10, 10
    &gt;&gt;&gt; p1, p2, p3 = 5, 5, 5
    &gt;&gt;&gt; x = chainerx.random.uniform(0, 1, (n, c_i, d1, d2, d3)).\
astype(np.float32)
    &gt;&gt;&gt; x.shape
    (10, 3, 30, 40, 50)
    &gt;&gt;&gt; w = chainerx.random.uniform(0, 1, (c_o, c_i, k1, k2, k3)).\
astype(np.float32)
    &gt;&gt;&gt; w.shape
    (1, 3, 10, 10, 10)
    &gt;&gt;&gt; b = chainerx.random.uniform(0, 1, (c_o)).astype(np.float32)
    &gt;&gt;&gt; b.shape
    (1,)
    &gt;&gt;&gt; s1, s2, s3 = 2, 4, 6
    &gt;&gt;&gt; y = chainerx.conv(x, w, b, stride=(s1, s2, s3),\
 pad=(p1, p2, p3))
    &gt;&gt;&gt; y.shape
    (10, 1, 16, 11, 9)
    &gt;&gt;&gt; l1 = int((d1 + 2 * p1 - k1) / s1 + 1)
    &gt;&gt;&gt; l2 = int((d2 + 2 * p2 - k2) / s2 + 1)
    &gt;&gt;&gt; l3 = int((d3 + 2 * p3 - k3) / s3 + 1)
    &gt;&gt;&gt; y.shape == (n, c_o, l1, l2, l3)
    True
    &gt;&gt;&gt; y = chainerx.conv(x, w, b, stride=(s1, s2, s3),\
 pad=(p1, p2, p3), cover_all=True)
    &gt;&gt;&gt; y.shape == (n, c_o, l1, l2, l3 + 1)
    True
""")

    _docs.set_doc(
        chainerx.conv_transpose,
        """conv_transpose(x, w, b=None, stride=1, pad=0, outsize=None)
N-dimensional transposed convolution.

This is an implementation of N-dimensional transposed convolution, which is
previously known as **deconvolution** in Chainer.

.. _Deconvolutional Networks: \
://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf

It takes three arrays: the input ``x``, the filter weight ``w``, and the
bias vector ``b``.

Notation: here is a notation for dimensionalities.

- :math:`N` is the number of spatial dimensions.
- :math:`n` is the batch size.
- :math:`c_I` and :math:`c_O` are the number of the input and output
  channels, respectively.
- :math:`d_1, d_2, ..., d_N` are the size of each axis of the input's
  spatial dimensions, respectively.
- :math:`k_1, k_2, ..., k_N` are the size of each axis of the filters,
  respectively.
- :math:`p_1, p_2, ..., p_N` are the size of each axis of the spatial
  padding size, respectively.
- :math:`s_1, s_2, ..., s_N` are the stride of each axis of filter
  application, respectively.

If ``outsize`` option is ``None``, the output size
:math:`(l_1, l_2, ..., l_N)` is determined by the following equations with
the items in the above list:

.. math::

   l_n = s_n (d_n - 1)  + k_n - 2 p_n \\ \\ (n = 1, ..., N)

If ``outsize`` option is given, the output size is determined by
``outsize``. In this case, the ``outsize`` :math:`(l_1, l_2, ..., l_N)`
must satisfy the following equations:

.. math::

   d_n = \\lfloor (l_n + 2p_n - k_n) / s_n \\rfloor + 1 \\ \\ \
   (n = 1, ..., N)

Args:
    x (:class:`~chainerx.ndarray`):
        Input array of shape :math:`(n, c_I, d_1, d_2, ..., d_N)`.
    w (:class:`~chainerx.ndarray`):
        Weight array of shape :math:`(c_I, c_O, k_1, k_2, ..., k_N)`.
    b (None or :class:`~chainerx.ndarray`):
        One-dimensional bias array with length :math:`c_O` (optional).
    stride (:class:`int` or :class:`tuple` of :class:`int` s):
        Stride of filter applications :math:`(s_1, s_2, ..., s_N)`.
        ``stride=s`` is equivalent to ``(s, s, ..., s)``.
    pad (:class:`int` or :class:`tuple` of :class:`int` s):
        Spatial padding width for input arrays
        :math:`(p_1, p_2, ..., p_N)`. ``pad=p`` is equivalent to
        ``(p, p, ..., p)``.
    outsize (None or :class:`tuple` of :class:`int` s):
        Expected output size of deconvolutional operation. It should be a
        tuple of ints :math:`(l_1, l_2, ..., l_N)`. Default value is
        ``None`` and the outsize is estimated by input size, stride and
        pad.

Returns:
    ~chainerx.ndarray:
        Output array of shape :math:`(n, c_O, l_1, l_2, ..., l_N)`.

Note:

    During backpropagation, this function propagates the gradient of the
    output array to input arrays ``x``, ``w``, and ``b``.

.. seealso:: :func:`chainer.functions.deconvolution_nd`

.. admonition:: Example

    **Example1**: the case when ``outsize`` is not given.

    &gt;&gt;&gt; n = 10
    &gt;&gt;&gt; c_i, c_o = 3, 1
    &gt;&gt;&gt; d1, d2, d3 = 5, 10, 15
    &gt;&gt;&gt; k1, k2, k3 = 10, 10, 10
    &gt;&gt;&gt; p1, p2, p3 = 5, 5, 5
    &gt;&gt;&gt; x = chainerx.random.uniform(0, 1, (n, c_i, d1, d2, d3)).\
astype(np.float32)
    &gt;&gt;&gt; x.shape
    (10, 3, 5, 10, 15)
    &gt;&gt;&gt; w = chainerx.random.uniform(0, 1, (c_i, c_o, k1, k2, k3)).\
astype(np.float32)
    &gt;&gt;&gt; w.shape
    (3, 1, 10, 10, 10)
    &gt;&gt;&gt; b = chainerx.random.uniform(0, 1, (c_o)).astype(np.float32)
    &gt;&gt;&gt; b.shape
    (1,)
    &gt;&gt;&gt; s1, s2, s3 = 2, 4, 6
    &gt;&gt;&gt; y = chainerx.conv_transpose(x, w, b, stride=(s1, s2, s3), \
pad=(p1, p2, p3))
    &gt;&gt;&gt; y.shape
    (10, 1, 8, 36, 84)
    &gt;&gt;&gt; l1 = s1 * (d1 - 1) + k1 - 2 * p1
    &gt;&gt;&gt; l2 = s2 * (d2 - 1) + k2 - 2 * p2
    &gt;&gt;&gt; l3 = s3 * (d3 - 1) + k3 - 2 * p3
    &gt;&gt;&gt; y.shape == (n, c_o, l1, l2, l3)
    True

    **Example2**: the case when ``outsize`` is given.

    &gt;&gt;&gt; n = 10
    &gt;&gt;&gt; c_i, c_o = 3, 1
    &gt;&gt;&gt; d1, d2, d3 = 5, 10, 15
    &gt;&gt;&gt; k1, k2, k3 = 10, 10, 10
    &gt;&gt;&gt; p1, p2, p3 = 5, 5, 5
    &gt;&gt;&gt; x = chainerx.array(np.random.uniform(0, 1, (n, c_i, d1, d2, d3)).\
astype(np.float32))
    &gt;&gt;&gt; x.shape
    (10, 3, 5, 10, 15)
    &gt;&gt;&gt; w = chainerx.array(np.random.uniform(0, 1, (c_i, c_o, k1, k2, k3)).\
astype(np.float32))
    &gt;&gt;&gt; w.shape
    (3, 1, 10, 10, 10)
    &gt;&gt;&gt; b = chainerx.array(np.random.uniform(0, 1, (c_o)).astype(np.float32))
    &gt;&gt;&gt; b.shape
    (1,)
    &gt;&gt;&gt; s1, s2, s3 = 2, 4, 6
    &gt;&gt;&gt; l1, l2, l3 = 9, 38, 87
    &gt;&gt;&gt; d1 == int((l1 + 2 * p1 - k1) / s1) + 1
    True
    &gt;&gt;&gt; d2 == int((l2 + 2 * p2 - k2) / s2) + 1
    True
    &gt;&gt;&gt; d3 == int((l3 + 2 * p3 - k3) / s3) + 1
    True
    &gt;&gt;&gt; y = chainerx.conv_transpose(x, w, b, stride=(s1, s2, s3), \
pad=(p1, p2, p3), outsize=(l1, l2, l3))
    &gt;&gt;&gt; y.shape
    (10, 1, 9, 38, 87)
    &gt;&gt;&gt; y.shape == (n, c_o, l1, l2, l3)
    True
""")

    _docs.set_doc(
        chainerx.linear,
        """linear(x, W, b=None, n_batch_axis=1)
Linear function, or affine transformation.

It accepts two or three arguments: an input minibatch ``x``, a weight
matrix ``W``, and optionally a bias vector ``b``. It computes

.. math:: Y = xW^\\top + b.

Args:
    x (~chainerx.ndarray):
        Input array, which is a :math:`(s_1, s_2, ..., s_n)`-shaped array.
    W (~chainerx.ndarray):
        Weight variable of shape :math:`(M, N)`,
        where :math:`(N = s_{\\rm n\\_batch\\_axes} * ... * s_n)`.
    b (~chainerx.ndarray):
        Bias variable (optional) of shape :math:`(M,)`.
    n_batch_axes (int):
        The number of batch axes. The default is 1. The input variable is
        reshaped into (:math:`{\\rm n\\_batch\\_axes} + 1`)-dimensional
        tensor. This should be greater than 0.

Returns:
    :class:`~chainerx.ndarray`:
        Output array with shape of
        :math:`(s_1, ..., s_{\\rm n\\_batch\\_axes}, M)`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to input arrays ``x``, ``W`` and ``b``.
""")

    _docs.set_doc(
        chainerx.lstm,
        """lstm(c_prev, x)
Long Short-Term Memory units as an activation function.

This function implements LSTM units with forget gates. Let the previous
cell state ``c_prev`` and the input array ``x``.
First, the input array ``x`` is split into four arrays
:math:`a, i, f, o` of the same shapes along the second axis. It means that
``x`` 's second axis must have 4 times the ``c_prev`` 's second axis.
The split input arrays are corresponding to:

    - :math:`a` : sources of cell input
    - :math:`i` : sources of input gate
    - :math:`f` : sources of forget gate
    - :math:`o` : sources of output gate

Second, it computes the updated cell state ``c`` and the outgoing signal
``h`` as

.. math::
    c &amp;= \\tanh(a) \\sigma(i)
       + c_{\\text{prev}} \\sigma(f), \\\\
    h &amp;= \\tanh(c) \\sigma(o),

where :math:`\\sigma` is the elementwise sigmoid function.
These are returned as a tuple of two variables.
This function supports variable length inputs. The mini-batch size of
the current input must be equal to or smaller than that of the previous
one. When mini-batch size of ``x`` is smaller than that of ``c``, this
function only updates ``c[0:len(x)]`` and doesn't change the rest of ``c``,
``c[len(x):]``. So,
please sort input sequences in descending order of lengths before
applying the function.

Args:
    c_prev (:class:`~chainerx.array`):
        Variable that holds the previous cell state. The cell state
        should be a zero array or the output of the previous call of LSTM.
    x (:class:`~chainer.array`):
        Variable that holds the sources of cell input, input gate, forget
        gate and output gate. It must have the second dimension whose size
        is four times of that of the cell state.

Returns:
    tuple: Two :class:`~chainerx.array` objects ``c`` and ``h``.
    ``c`` is the updated cell state. ``h`` indicates the outgoing signal.

See the original paper proposing LSTM with forget gates:
`Long Short-Term Memory in Recurrent Neural Networks
&lt;http://www.felixgers.de/papers/phd.pdf&gt;`_.

.. admonition:: Example

    Assuming ``y`` is the current incoming signal, ``c`` is the previous
    cell state, and ``h`` is the previous outgoing signal from an ``lstm``
    function. Each of ``y``, ``c`` and ``h`` has ``n_units`` channels.
    Most typical preparation of ``x`` is

    &gt;&gt;&gt; n_units = 100
    &gt;&gt;&gt; c_prev = chainerx.zeros((1, n_units), chainerx.float32)
    &gt;&gt;&gt; x = chainerx.zeros((1, 4 * n_units), chainerx.float32)
    &gt;&gt;&gt; c, h = chainerx.lstm(c_prev, x)

    It corresponds to calculate the input array ``x``, or the input
    sources :math:`a, i, f, o`, from the current incoming signal ``y`` and
    the previous outgoing signal ``h``. Different parameters are used for
    different kind of input sources.
""")


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10704')" href="javascript:;">
chainer-7.2.0/chainerx/_docs/routines.py: 2751-2852
</a>
<div class="mid" id="frag10704" style="display:none"><pre>
def _docs_statistics():
    _docs.set_doc(
        chainerx.amax,
        """amax(a, axis=None, keepdims=False)
Returns the maximum of an array or the maximum along an axis.

Note:
    When at least one element is NaN, the corresponding max value will be NaN.

Args:
    a (~chainerx.ndarray): Array to take the maximum.
    axis (None or int or tuple of ints): Along which axis to take the maximum.
        The flattened array is used by default.
        If this is a tuple of ints, the maximum is selected over multiple
        axes, instead of a single axis or all the axes.
    keepdims (bool): If ``True``, the axis is remained as an axis of size one.

Returns:
    :class:`~chainerx.ndarray`: The maximum of ``a``, along the axis if
    specified.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``a``.

.. seealso:: :func:`numpy.amax`
""")

    _docs.set_doc(
        chainerx.amin,
        """amin(a, axis=None, keepdims=False)
Returns the minimum of an array or the minimum along an axis.

Note:
    When at least one element is NaN, the corresponding min value will be NaN.

Args:
    a (~chainerx.ndarray): Array to take the minimum.
    axis (None or int or tuple of ints): Along which axis to take the minimum.
        The flattened array is used by default.
        If this is a tuple of ints, the minimum is selected over multiple
        axes, instead of a single axis or all the axes.
    keepdims (bool): If ``True``, the axis is remained as an axis of size one.

Returns:
    :class:`~chainerx.ndarray`: The minimum of ``a``, along the axis if
    specified.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``a``.

.. seealso:: :func:`numpy.amin`
""")

    _docs.set_doc(
        chainerx.mean,
        """mean(a, axis=None, keepdims=False)
Compute the arithmetic mean along the specified axis.

Returns the average of the array elements. The average is taken over the
flattened array by default, otherwise over the specified axis.

Args:
    a (~chainerx.ndarray): Array to take the mean of.
    axis (None or int or tuple of ints): Along which axis or axes to compute
    the mean. The flattened array is used by default.
    keepdims (bool): If this is set to True, the axes which are reduced are
    left in the result as dimensions with size one. With this option,
    the result will broadcast correctly against the input array.

Returns:
    :class:`~chainerx.ndarray`: The mean of ``a``, along the axis or axes if
    specified.

.. seealso:: :func:`numpy.mean`
""")

    _docs.set_doc(
        chainerx.var,
        """var(a, axis=None, keepdims=False)
Compute the arithmetic var along the specified axis.

Returns the var of the array elements. The var is taken over the flattened
array by default, otherwise over the specified axis.

Args:
    a (~chainerx.ndarray): Array to take the var of.
    axis (None or int or tuple of ints): Along which axis or axes to compute
    the var. The flattened array is used by default.
    keepdims (bool): If this is set to True, the axes which are reduced are
    left in the result as dimensions with size one. With this option,
    the result will broadcast correctly against the input array.

Returns:
    :class:`~chainerx.ndarray`: The var of ``a``, along the axis or axes if
    specified.

.. seealso:: :func:`numpy.var`
""")


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10689')" href="javascript:;">
chainer-7.2.0/chainerx/_docs/device.py: 54-123
</a>
<div class="mid" id="frag10689" style="display:none"><pre>
def set_docs():
    _set_docs_device()

    _docs.set_doc(
        chainerx.get_device,
        """get_device(*device)
Returns a device specified by the arguments.

If the argument is a single :class:`~chainerx.Device` instance, it's simply
returned.

Otherwise, there are three ways to specify a device:


.. testcode::

    # Specify a backend name and a device index separately.
    chainerx.get_device('native', 0)

    # Specify a backend name and a device index in a single string.
    chainerx.get_device('native:0')

    # Specify only a backend name. In this case device index 0 is chosen.
    chainerx.get_device('native')

Returns:
    ~chainerx.Device: Device object.
""")

    _docs.set_doc(
        chainerx.get_default_device,
        """get_default_device()
Returns the default device associated with the current thread.

Returns:
    ~chainerx.Device: The default device.

.. seealso::
    * :func:`chainerx.set_default_device`
    * :func:`chainerx.using_device`
""")

    _docs.set_doc(
        chainerx.set_default_device,
        """set_default_device(device)
Sets the given device as the default device of the current thread.

Args:
    device (~chainerx.Device or str): Device object or device name to set as
        the default device.

.. seealso::
    * :func:`chainerx.get_default_device`
    * :func:`chainerx.using_device`
""")

    _docs.set_doc(
        chainerx.using_device,
        """using_device(device)
Creates a context manager to temporarily set the default device.

Args:
    device (~chainerx.Device or str): Device object or device name to set as
        the default device during the context. See :data:`chainerx.Device.name`
        for the specification of device names.

.. seealso::
    * :func:`chainerx.get_default_device`
    * :func:`chainerx.set_default_device`
""")
</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10697')" href="javascript:;">
chainer-7.2.0/chainerx/_docs/routines.py: 558-632
</a>
<div class="mid" id="frag10697" style="display:none"><pre>
def _docs_indexing():
    _docs.set_doc(
        chainerx.take,
        """take(a, indices, axis)
Takes elements from an array along an axis.

Args:
    a (~chainerx.ndarray): Source array.
    indices (~chainerx.ndarray):
        The indices of the values to extract. When indices are out of bounds,
        they are wrapped around.
    axis (int): The axis over which to select values.
    mode (str): Specifies how out-of-bounds indices will behave.
        'raise' - raise an error
        'wrap' - wrap around
        'clip' - clip to the range

Returns:
    :func:`~chainerx.ndarray`: Output array.

Note:
    This function currently does not support ``axis=None``

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``a``.

Note:
   The default mode for the native backend is 'raise', while for the cuda
   backend is 'wrap' in order to prevent device synchronization.
   'raise' mode is currently not supported in the CUDA backend.

.. seealso:: :func:`numpy.take`
""")

    _docs.set_doc(
        chainerx.where,
        """where(condition, x, y)
Return elements chosen from ``x`` or ``y`` depending on condition.

Args:
    condition (~chainerx.ndarray): Where True, yield ``x``, otherwise
    yield ``y``.
    x (~chainerx.ndarray): Values from which to choose.
    y (~chainerx.ndarray): Values from which to choose.

Returns:
    :func:`~chainerx.ndarray`: An array with elements
    from ``x`` where condition is True, and elements from ``y`` elsewhere.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x`` and ``y``.

.. seealso:: :func:`numpy.where`
""")

    _docs.set_doc(
        chainerx.nonzero,
        """nonzero(a)
Return the indices of the elements that are non-zero.

Args:
    a (~chainerx.ndarray): Input array.

Returns:
    tuple of :func:`~chainerx.ndarray`: Indices of elements that are non-zero.

Note:
    During backpropagation, this function does not propagate gradients.

.. seealso:: :func:`numpy.nonzero`
""")


</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 281:</b> &nbsp; 2 fragments, nominal size 70 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10695')" href="javascript:;">
chainer-7.2.0/chainerx/_docs/routines.py: 22-508
</a>
<div class="mid" id="frag10695" style="display:none"><pre>
def _docs_creation():
    _docs.set_doc(
        chainerx.empty,
        """empty(shape, dtype, device=None)
Returns an array without initializing the elements.

Args:
    shape (tuple of ints): Shape of the array.
    dtype: Data type of the array.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device &lt;chainerx_device&gt;` is chosen.

Returns:
    :class:`~chainerx.ndarray`: New array with elements not initialized.

.. seealso:: :func:`numpy.empty`
""")

    _docs.set_doc(
        chainerx.empty_like,
        """empty_like(a, device=None)
Returns a new array with same shape and dtype of a given array.

Args:
    a (~chainerx.ndarray): Prototype array.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device &lt;chainerx_device&gt;` is chosen.

Returns:
    :class:`~chainerx.ndarray`: New array with same shape and dtype as ``a`` \
with elements not initialized.

Warning:
    If ``device`` argument is omitted, the new array is created on the default
    device, not the device of the prototype array.

.. seealso:: :func:`numpy.empty_like`
""")

    _docs.set_doc(
        chainerx.eye,
        """eye(N, M=None, k=0, dtype=float64, device=None)
Returns a 2-D array with ones on the diagonals and zeros elsewhere.

Args:
    N (int): Number of rows.
    M (int): Number of columns. M == N by default.
    k (int): Index of the diagonal. Zero indicates the main diagonal,
        a positive index an upper diagonal, and a negative index a lower
        diagonal.
    dtype: Data type.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device &lt;chainerx_device&gt;` is chosen.

Returns:
    ~chainerx.ndarray: A 2-D array with given diagonals filled with ones and
    zeros elsewhere.

.. seealso:: :func:`numpy.eye`
""")

    _docs.set_doc(
        chainerx.tri,
        """tri(N, M=None, k=0, dtype=float32, device=None)
Returns a 2-D array with ones at and below the given diagonal
and zeros elsewhere.

Args:
    N (int): Number of rows.
    M (int): Number of columns. M == N by default.
    k (int): Index of the diagonal. Zero indicates the main diagonal,
        a positive index an upper diagonal, and a negative index a lower
        diagonal.
    dtype: Data type.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device &lt;chainerx_device&gt;` is chosen.

Returns:
    ~chainerx.ndarray: A 2-D array with given diagonals filled ones at and
    below the given diagonal and zeros elsewhere.

.. seealso:: :func:`numpy.tri`
""")

    _docs.set_doc(
        chainerx.tril,
        """tril(m, k=0)
Lower triangle of an array.

Returns a copy of an array with elements above the k-th diagonal zeroed.

Args:
    m (~chainerx.ndarray): Input array.
    k (int): Index of the diagonal. Zero indicates the main diagonal,
        a positive index an upper diagonal, and a negative index a lower
        diagonal.

Returns:
    ~chainerx.ndarray: Lower triangle of ``m``.

.. seealso:: :func:`numpy.tril`
""")

    _docs.set_doc(
        chainerx.triu,
        """triu(m, k=0)
Upper triangle of an array.

Returns a copy of an array with elements below the k-th diagonal zeroed.

Args:
    m (~chainerx.ndarray): Input array.
    k (int): Index of the diagonal. Zero indicates the main diagonal,
        a positive index an upper diagonal, and a negative index a lower
        diagonal.

Returns:
    ~chainerx.ndarray: Upper triangle of ``m``.

.. seealso:: :func:`numpy.triu`
""")

    _docs.set_doc(
        chainerx.identity,
        """identity(n, dtype=None, device=None)
Returns a 2-D identity array.

It is equivalent to ``eye(n, n, dtype)``.

Args:
    n (int): Number of rows and columns.
    dtype: Data type.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device &lt;chainerx_device&gt;` is chosen.

Returns:
    ~chainerx.ndarray: A 2-D identity array.

.. seealso:: :func:`numpy.identity`
""")

    _docs.set_doc(
        chainerx.ones,
        """ones(shape, dtype, device=None)
Returns a new array of given shape and dtype, filled with ones.

Args:
    shape (tuple of ints): Shape of the array.
    dtype: Data type.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device &lt;chainerx_device&gt;` is chosen.

Returns:
    ~chainerx.ndarray: New array.

.. seealso:: :func:`numpy.ones`
""")

    _docs.set_doc(
        chainerx.ones_like,
        """ones_like(a, device=None)
Returns an array of ones with same shape and dtype as a given array.

Args:
    a (~chainerx.ndarray): Prototype array.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device &lt;chainerx_device&gt;` is chosen.

Returns:
    ~chainerx.ndarray: New array.

Warning:
    If ``device`` argument is omitted, the new array is created on the default
    device, not the device of the prototype array.

.. seealso:: :func:`numpy.ones_like`
""")

    _docs.set_doc(
        chainerx.zeros,
        """zeros(shape, dtype, device=None)
Returns a new array of given shape and dtype, filled with zeros.

Args:
    shape (tuple of ints): Shape of the array.
    dtype: Data type.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device &lt;chainerx_device&gt;` is chosen.

Returns:
    ~chainerx.ndarray: New array.

.. seealso:: :func:`numpy.zeros`
""")

    _docs.set_doc(
        chainerx.zeros_like,
        """zeros_like(a, device=None)
Returns an array of zeros with same shape and dtype as a given array.

Args:
    a (~chainerx.ndarray): Prototype array.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device &lt;chainerx_device&gt;` is chosen.

Returns:
    ~chainerx.ndarray: New array.

Warning:
    If ``device`` argument is omitted, the new array is created on the default
    device, not the device of the prototype array.

.. seealso:: :func:`numpy.zeros_like`
""")

    _docs.set_doc(
        chainerx.full,
        """full(shape, fill_value, dtype, device=None)
Returns a new array of given shape and dtype, filled with a given value.

Args:
    shape (tuple of ints): Shape of the array.
    dtype: Data type.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device &lt;chainerx_device&gt;` is chosen.

Returns:
    ~chainerx.ndarray: New array.

.. seealso:: :func:`numpy.full`
""")

    _docs.set_doc(
        chainerx.full_like,
        """full_like(a, fill_value, dtype=None, device=None)
Returns a full array with same shape and dtype as a given array.

Args:
    a (~chainerx.ndarray): Prototype array.
    dtype: Data type.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device &lt;chainerx_device&gt;` is chosen.

Returns:
    ~chainerx.ndarray: New array.

Warning:
    If ``device`` argument is omitted, the new array is created on the default
    device, not the device of the prototype array.

.. seealso:: :func:`numpy.full_like`
""")

    _docs.set_doc(
        chainerx.array,
        """array(object, dtype=None, copy=True, device=None)
Creates an array.

Args:
    object: A :class:`~chainerx.ndarray` object or any other object that can be
        passed to :func:`numpy.array`.
    dtype: Data type. If omitted, it's inferred from the input.
    copy (bool): If ``True``, the object is always copied. Otherwise, a copy
        will only be made if it is needed to satisfy any of the other
        requirements (dtype, device, etc.).
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device &lt;chainerx_device&gt;` is chosen.

Returns:
    ~chainerx.ndarray: New array.

Warning:
    If ``device`` argument is omitted, the new array is created on the default
    device, not the device of the input array.

.. seealso:: :func:`numpy.array`
""")

    _docs.set_doc(
        chainerx.asarray,
        """asarray(a, dtype=None, device=None)
Converts an object to an array.

Args:
    a: The source object.
    dtype: Data type. If omitted, it's inferred from the input.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device &lt;chainerx_device&gt;` is chosen.

Returns:
    ~chainerx.ndarray: Array interpretation of ``a``. If ``a`` is already an \
ndarray on the given device with matching dtype, no copy is performed.

Warning:
    If ``device`` argument is omitted, the new array is created on the default
    device, not the device of the input array.

.. seealso:: :func:`numpy.asarray`
""")

    _docs.set_doc(
        chainerx.ascontiguousarray,
        """ascontiguousarray(a, dtype=None, device=None)
Returns a C-contiguous array.

Args:
    a (~chainerx.ndarray): Source array.
    dtype: Data type.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device &lt;chainerx_device&gt;` is chosen.

Returns:
    ~chainerx.ndarray: C-contiguous array. A copy will be made only if needed.

Warning:
    If ``device`` argument is omitted, the new array is created on the default
    device, not the device of the input array.

.. seealso:: :func:`numpy.ascontiguousarray`
""")

    _docs.set_doc(
        chainerx.copy,
        """copy(a)
Creates a copy of a given array.

Args:
    a (~chainerx.ndarray): Source array.

Returns:
    ~chainerx.ndarray: A copy array on the same device as ``a``.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``a``.

.. seealso:: :func:`numpy.copy`
""")

    _docs.set_doc(
        chainerx.frombuffer,
        """frombuffer(buffer, dtype=float, count=-1, offset=0, device=None)
Returns a 1-D array interpretation of a buffer.

The given ``buffer`` memory must be usable on the given device, otherwise,
an error is raised.

Note:
    The ``native`` backend requires a buffer of main memory, and
    the ``cuda`` backend requires a buffer of CUDA memory.
    No copy is performed.

Args:
    buffer: An object that exposes the buffer interface.
    dtype: Data type of the returned array.
    count (int): Number of items to read. -1 means all data in the buffer.
    offset (int): Start reading the buffer from this offset (in bytes).
    device (~chainerx.Device): Device of the returned array.
        If omitted, :ref:`the default device &lt;chainerx_device&gt;` is chosen.

Returns:
    ~chainerx.ndarray: 1-D array interpretation of ``buffer``.

.. seealso:: :func:`numpy.frombuffer`
""")

    _docs.set_doc(
        chainerx.arange,
        """arange([start=0, ]stop, [step=1, ]dtype=None, device=None)
Returns an array with  evenly spaced values within a given interval.

Values are generated within the half-open interval [``start``, ``stop``).
The first three arguments are mapped like the ``range`` built-in function,
i.e. ``start`` and ``step`` are optional.

Args:
    start: Start of the interval.
    stop: End of the interval.
    step: Step width between each pair of consecutive values.
    dtype: Data type specifier. It is inferred from other arguments by
        default.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device &lt;chainerx_device&gt;` is chosen.

Returns:
    ~chainerx.ndarray: The 1-D array of range values.

.. seealso:: :func:`numpy.arange`
""")

    _docs.set_doc(
        chainerx.linspace,
        """linspace(start, stop, num=50, endpoint=True, dtype=None, device=None)
Returns an array with evenly spaced numbers over a specified interval.

Instead of specifying the step width like :func:`chainerx.arange()`,
this function requires the total number of elements specified.

Args:
    start: Start of the interval.
    stop: End of the interval.
    num: Number of elements.
    endpoint (bool): If ``True``, the stop value is included as the last
        element. Otherwise, the stop value is omitted.
    dtype: Data type specifier. It is inferred from the start and stop
        arguments by default.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device &lt;chainerx_device&gt;` is chosen.

Returns:
    ~chainerx.ndarray: The 1-D array of ranged values.

.. seealso:: :func:`numpy.linspace`
""")  # NOQA

    _docs.set_doc(
        chainerx.diag,
        """diag(v, k=0, device=None)
Returns a diagonal or a diagonal array.

Args:
    v (~chainerx.ndarray): Array object.
    k (int): Index of diagonals. Zero indicates the main diagonal, a
        positive value an upper diagonal, and a negative value a lower
        diagonal.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device &lt;chainerx_device&gt;` is chosen.

Returns:
    ~chainerx.ndarray: If ``v`` is a 1-D array, then it returns a 2-D
    array with the specified diagonal filled by ``v``. If ``v`` is a
    2-D array, then it returns the specified diagonal of ``v``. In latter
    case, if ``v`` is a :class:`chainerx.ndarray` object, then its view is
    returned.

Note:
    The argument ``v`` does not support array-like objects yet.

.. seealso:: :func:`numpy.diag`
""")

    _docs.set_doc(
        chainerx.diagflat,
        """diagflat(v, k=0, device=None)
Creates a diagonal array from the flattened input.

Args:
    v (~chainerx.ndarray): Array object.
    k (int): Index of diagonals. See :func:`chainerx.diag`.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device &lt;chainerx_device&gt;` is chosen.

Returns:
    ~chainerx.ndarray: A 2-D diagonal array with the diagonal copied
    from ``v``.

Note:
    The argument ``v`` does not support array-like objects yet.

.. seealso:: :func:`numpy.diagflat`
""")

    _docs.set_doc(
        chainerx.meshgrid,
        """meshgrid(xi, indexing='xy')
Returns coordinate matrices from coordinate vectors.

Make N-D coordinate arrays for vectorized evaluations of N-D scalar/vector
fields over N-D grids, given one-dimensional coordinate arrays x1, x2,, xn.

Args:
    xi (sequence of :class:`~chainerx.ndarray`\\ s): 1-D arrays
        representing the coordinates of a grid.
    indexing (str): {xy, ij}, optional
        Cartesian (xy, default) or matrix (ij) indexing of output.

Returns:
    list of :class:`~chainerx.ndarray`\\ s: For vectors x1, x2,, xn with
    lengths Ni=len(xi), return (N1, N2, N3,...Nn) shaped arrays if
    indexing=ij or (N2, N1, N3,...Nn) shaped arrays if indexing=xy
    with the elements of xi repeated to fill the matrix along the first
    dimension for x1, the second for x2 and so on.

.. seealso:: :func:`numpy.meshgrid`
""")


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10701')" href="javascript:;">
chainer-7.2.0/chainerx/_docs/routines.py: 1274-1770
</a>
<div class="mid" id="frag10701" style="display:none"><pre>
def _docs_manipulation():
    _docs.set_doc(
        chainerx.reshape,
        """reshape(a, newshape)
Returns a reshaped array.

Args:
    a (~chainerx.ndarray): Array to be reshaped.
    newshape (int or tuple of ints): The new shape of the array to return.
        If it is an integer, then it is treated as a tuple of length one.
        It should be compatible with ``a.size``. One of the elements can be
        -1, which is automatically replaced with the appropriate value to
        make the shape compatible with ``a.size``.

Returns:
    :class:`~chainerx.ndarray`: A reshaped view of ``a`` if possible,
    otherwise a copy.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``a``.

.. seealso:: :func:`numpy.reshape`
""")

    _docs.set_doc(
        chainerx.ravel,
        """ravel(a)
Returns a flattened array.

Args:
    a (~chainerx.ndarray): Array to be flattened.

Returns:
    :class:`~chainerx.ndarray`: A flattened view of ``a`` if possible,
    otherwise a copy.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``a``.

.. seealso:: :func:`numpy.ravel`
""")

    _docs.set_doc(
        chainerx.transpose,
        """transpose(a, axes=None)
Permutes the dimensions of an array.

Args:
    a (~chainerx.ndarray): Array to permute the dimensions.
    axes (tuple of ints): Permutation of the dimensions. This function reverses
        the shape by default.

Returns:
    ~chainerx.ndarray: A view of ``a`` with the dimensions permuted.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``a``.

.. seealso:: :func:`numpy.transpose`
""")

    _docs.set_doc(
        chainerx.broadcast_to,
        """broadcast_to(array, shape)
Broadcasts an array to a given shape.

Args:
    array (~chainerx.ndarray): Array to broadcast.
    shape (tuple of ints): The shape of the desired array.

Returns:
    ~chainerx.ndarray: Broadcasted view.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``array``.

.. seealso:: :func:`numpy.broadcast_to`
""")

    _docs.set_doc(
        chainerx.squeeze,
        """squeeze(a, axis=None)
Removes size-one axes from the shape of an array.

Args:
    a (~chainerx.ndarray): Array to be reshaped.
    axis (int or tuple of ints): Axes to be removed. This function removes all
        size-one axes by default. If one of the specified axes is not of size
        one, an exception is raised.

Returns:
    ~chainerx.ndarray: An array without (specified) size-one axes.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``a``.

.. seealso:: :func:`numpy.squeeze`
""")

    _docs.set_doc(
        chainerx.concatenate,
        """concatenate(arrays, axis=0)
Joins arrays along an axis.

Args:
    arrays (sequence of :class:`~chainerx.ndarray`\\ s): Arrays to be joined.
        All of these should have the same dimensionalities except the specified
        axis.
    axis (int): The axis to join arrays along.


Returns:
    ~chainerx.ndarray: Joined array.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input arrays in ``arrays``.

.. seealso:: :func:`numpy.concatenate`
""")

    _docs.set_doc(
        chainerx.stack,
        """stack(arrays, axis=0)
Stacks arrays along a new axis.

Args:
    arrays (sequence of :class:`~chainerx.ndarray`\\ s): Arrays to be stacked.
    axis (int): Axis along which the arrays are stacked.

Returns:
    ~chainerx.ndarray: Stacked array.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input arrays in ``arrays``.

.. seealso:: :func:`numpy.stack`
""")

    _docs.set_doc(
        chainerx.hstack,
        """hstack(arrays)
Stack arrays in sequence horizontally (column wise).

Args:
    arrays (sequence of :class:`~chainerx.ndarray`\\ s): Arrays to be stacked.

Returns:
    ~chainerx.ndarray: Stacked array.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input arrays in ``arrays``.

.. seealso:: :func:`numpy.hstack`
""")

    _docs.set_doc(
        chainerx.vstack,
        """vstack(arrays)
Stack arrays in sequence vertically (row wise).

Args:
    arrays (sequence of :class:`~chainerx.ndarray`\\ s): Arrays to be stacked.

Returns:
    ~chainerx.ndarray: Stacked array.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input arrays in ``arrays``.

.. seealso:: :func:`numpy.vstack`
""")

    _docs.set_doc(
        chainerx.dstack,
        """dstack(arrays)
Stack arrays in sequence depth wise (along third axis).

Args:
    arrays (sequence of :class:`~chainerx.ndarray`\\ s): Arrays to be stacked.

Returns:
    ~chainerx.ndarray: Stacked array.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input arrays in ``arrays``.

.. seealso:: :func:`numpy.dstack`
""")

    _docs.set_doc(
        chainerx.atleast_2d,
        """atleast_2d(a)
View inputs as arrays with at least two dimensions.

Args:
    a (~chainerx.ndarray): Array.

Returns:
    ~chainerx.ndarray: An array with a.ndim &gt;= 2.
    Copies are avoided where possible, and views with
    two or more dimensions are returned.

Note:
    * Arrays that already have two or more dimensions are preserved.
    * During backpropagation, this function propagates the gradient of the
      output array to the input arrays in ``a``.

.. seealso:: :func:`numpy.atleast_2d`
""")

    _docs.set_doc(
        chainerx.atleast_3d,
        """atleast_3d(a)
View inputs as arrays with at least three dimensions.

Args:
    a (~chainerx.ndarray): Array.

Returns:
    ~chainerx.ndarray: An array with a.ndim &gt;= 3.
    Copies are avoided where possible, and views with
    three or more dimensions are returned.

Note:
    * Arrays that already have three or more dimensions are preserved.
    * During backpropagation, this function propagates the gradient of the
      output array to the input arrays in ``a``.

.. seealso:: :func:`numpy.atleast_3d`
""")

    _docs.set_doc(
        chainerx.split,
        """split(ary, indices_or_sections, axis=0)
Splits an array into multiple sub arrays along a given axis.

Args:
    ary (~chainerx.ndarray): Array to split.
    indices_or_sections (int or sequence of ints): A value indicating how to
        divide the axis. If it is an integer, then is treated as the number of
        sections, and the axis is evenly divided. Otherwise, the integers
        indicate indices to split at. Note that a sequence on the device
        memory is not allowed.
    axis (int): Axis along which the array is split.

Returns:
    list of :class:`~chainerx.ndarray`\\ s: A list of sub arrays. Each array \
is a partial view of the input array.

Note:
    During backpropagation, this function propagates the gradients of the
    output arrays to the input array ``ary``.

.. seealso:: :func:`numpy.split`
""")

    _docs.set_doc(
        chainerx.dsplit,
        """dsplit(ary, indices_or_sections)
Split array into multiple sub-arrays along the 3rd axis (depth).

Args:
    ary (~chainerx.ndarray): Array to split.
    indices_or_sections (int or sequence of ints): A value indicating how to
        divide the axis. If it is an integer, then is treated as the number of
        sections, and the axis is evenly divided. Otherwise, the integers
        indicate indices to split at. Note that a sequence on the device
        memory is not allowed.

Returns:
    list of :class:`~chainerx.ndarray`\\ s: A list of sub arrays. Each array \
is a partial view of the input array.

Note:
    During backpropagation, this function propagates the gradients of the
    output arrays to the input array ``ary``.

.. seealso:: :func:`numpy.dsplit`
""")

    _docs.set_doc(
        chainerx.vsplit,
        """vsplit(ary, indices_or_sections)
Splits an array into multiple sub-arrays vertically (row-wise).

Args:
    ary (~chainerx.ndarray): Array to split.
    indices_or_sections (int or sequence of ints): A value indicating how to
        divide the axis. If it is an integer, then is treated as the number of
        sections, and the axis is evenly divided. Otherwise, the integers
        indicate indices to split at. Note that a sequence on the device
        memory is not allowed.

Returns:
    list of :class:`~chainerx.ndarray`\\ s: A list of sub arrays. Each array \
is a partial view of the input array.

Note:
    During backpropagation, this function propagates the gradients of the
    output arrays to the input array ``ary``.

.. seealso:: :func:`numpy.vsplit`
""")

    _docs.set_doc(
        chainerx.hsplit,
        """hsplit(ary, indices_or_sections)
Split an array into multiple sub-arrays horizontally (column-wise).

Args:
    ary (~chainerx.ndarray): Array to split.
    indices_or_sections (int or sequence of ints): A value indicating how to
        divide the axis. If it is an integer, then is treated as the number of
        sections, and the axis is evenly divided. Otherwise, the integers
        indicate indices to split at. Note that a sequence on the device
        memory is not allowed.

Returns:
    list of :class:`~chainerx.ndarray`\\ s: A list of sub arrays. Each array \
is a partial view of the input array.

Note:
    During backpropagation, this function propagates the gradients of the
    output arrays to the input array ``ary``.

.. seealso:: :func:`numpy.hsplit`
""")

    _docs.set_doc(
        chainerx.swapaxes,
        """swapaxes(a, axis1, axis2)
Interchange two axes of an array.

Args:
    a (~chainerx.ndarray): Array to swapaxes.
    axis1 (int): First Axis
    axis2 (int): Second Axis

Returns:
    ~chainerx.ndarray: Swaped array.

Note:
    * Output array is a view of the input array.
    * During backpropagation, this function propagates the gradients of the
      output arrays to the input array ``a``.


.. seealso:: :func:`numpy.swapaxes`
""")

    _docs.set_doc(
        chainerx.repeat,
        """repeat(a, repeats, axis=None)
Constructs an array by repeating a given array.

Args:
    a (~chainerx.ndarray): Array to repeat.
    repeats (int or tuple of ints): The number of times which each
        element of a is repeated.
    axis (int): The axis along which to repeat values.

Returns:
    ~chainerx.ndarray: The repeated output array.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``a``.

.. seealso:: :func:`numpy.repeat`
""")

    _docs.set_doc(
        chainerx.expand_dims,
        """expand_dims(a, axis)
Expand the shape of an array.

Args:
    a (~chainerx.ndarray): Input Array.
    axis (int): Position in the expanded axes where the new axis is placed.

Returns:
    ~chainerx.ndarray: Output array.

Note:
    * Output array may or may not be a view of the input array.
    * During backpropagation, this function propagates the gradients of the
      output arrays to the input array ``a``.


.. seealso:: :func:`numpy.expand_dims`
""")

    _docs.set_doc(
        chainerx.flip,
        """flip(m, axis)
Reverse the order of elements in an array along the given axis.

Args:
    m (~chainerx.ndarray): Input Array.
    axis (int or tuple of ints): Axis or axes along which to flip over.
    The default, axis=None, will flip over all of the axes of the input array.
    If axis is negative it counts from the last to the first axis.
    If axis is a tuple of ints, flipping is performed on all of the
    axes specified in the tuple.

Returns:
    ~chainerx.ndarray: A view of m with the entries of axis reversed.
    Since a view is returned, this operation is done in constant time.

Note:
    * Output array is a view of the input array.
    * During backpropagation, this function propagates the gradients of the
      output arrays to the input array ``m``.


.. seealso:: :func:`numpy.flip`
""")

    _docs.set_doc(
        chainerx.fliplr,
        """fliplr(m)
Flip array in the left/right direction.

Args:
    m (~chainerx.ndarray): Input Array.

Returns:
    ~chainerx.ndarray: A view of m with the columns reversed.
    Since a view is returned, this operation is done in constant time.

Note:
    * Output array is a view of the input array.
    * During backpropagation, this function propagates the gradients of the
      output arrays to the input array ``m``.


.. seealso:: :func:`numpy.fliplr`
""")

    _docs.set_doc(
        chainerx.flipud,
        """flipud(m)
Flip array in the up/down direction.

Args:
    m (~chainerx.ndarray): Input Array.

Returns:
    ~chainerx.ndarray: A view of m with the rows reversed.
    Since a view is returned, this operation is done in constant time.

Note:
    * Output array is a view of the input array.
    * During backpropagation, this function propagates the gradients of the
      output arrays to the input array ``m``.


.. seealso:: :func:`numpy.flipud`
""")

    _docs.set_doc(
        chainerx.moveaxis,
        """moveaxis(a, source, destination)
Move axes of an array to new positions.

Other axes remain in their original order.

Args:
    a (~chainerx.ndarray): Input Array.
    source (int or tuple of ints): Original positions of the axes to move.
    These must be unique.
    destintation (int or tuple of ints): Destination positions for each of
    the original axes. These must also be unique.

Returns:
    ~chainerx.ndarray: Array with moved axes. This array is a view of the
    input array.

Note:
    * During backpropagation, this function propagates the gradients of the
      output arrays to the input array ``a``.


.. seealso:: :func:`numpy.moveaxis`
""")


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 282:</b> &nbsp; 2 fragments, nominal size 146 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10702')" href="javascript:;">
chainer-7.2.0/chainerx/_docs/routines.py: 1771-2714
</a>
<div class="mid" id="frag10702" style="display:none"><pre>
def _docs_math():
    _docs.set_doc(
        chainerx.negative,
        """negative(x)
Numerical negative, element-wise.

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = -x`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.

.. seealso:: :data:`numpy.negative`
""")

    _docs.set_doc(
        chainerx.add,
        """add(x1, x2)
Add arguments, element-wise.

Args:
    x1 (~chainerx.ndarray or scalar): Input array.
    x2 (~chainerx.ndarray or scalar): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = x_1 + x_2`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input arrays ``x1`` and ``x2``.

.. seealso:: :data:`numpy.add`
""")

    _docs.set_doc(
        chainerx.subtract,
        """subtract(x1, x2)
Subtract arguments, element-wise.

Args:
    x1 (~chainerx.ndarray or scalar): Input array.
    x2 (~chainerx.ndarray or scalar): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = x_1 - x_2`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input arrays ``x1`` and ``x2``.

.. seealso:: :data:`numpy.subtract`
""")

    _docs.set_doc(
        chainerx.multiply,
        """multiply(x1, x2)
Multiply arguments, element-wise.

Args:
    x1 (~chainerx.ndarray or scalar): Input array.
    x2 (~chainerx.ndarray or scalar): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = x_1 \\times x_2`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input arrays ``x1`` and ``x2``.

.. seealso:: :data:`numpy.multiply`
""")

    _docs.set_doc(
        chainerx.divide,
        """divide(x1, x2)
Divide arguments, element-wise.

Args:
    x1 (~chainerx.ndarray or scalar): Input array.
    x2 (~chainerx.ndarray or scalar): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = \\frac{x_1}{x_2}`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input arrays ``x1`` and ``x2``.

.. seealso:: :data:`numpy.divide`
""")

    _docs.set_doc(
        chainerx.sum,
        """sum(a, axis=None, keepdims=False)
Sum of array elements over a given axis.

Args:
    a (~chainerx.ndarray): Input array.
    axis (None or int or tuple of ints):
        Axis or axes along which a sum is performed.
        The flattened array is used by default.
    keepdims (bool):
        If this is set to ``True``, the reduced axes are left in the result
        as dimensions with size one.

Returns:
    :class:`~chainerx.ndarray`: The sum of input elements over a given axis.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``a``.

.. seealso:: :func:`numpy.sum`
""")

    _docs.set_doc(
        chainerx.maximum,
        """maximum(x1, x2)
Maximum arguments, element-wise.

Args:
    x1 (~chainerx.ndarray or scalar): Input array.
    x2 (~chainerx.ndarray or scalar): Input array.

Returns:
    :class:`~chainerx.ndarray`:
        Returned array: :math:`y = max(\\{x_1, x_2\\})`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input arrays ``x1`` and ``x2``.

.. seealso:: :data:`numpy.maximum`
""")

    _docs.set_doc(
        chainerx.minimum,
        """minimum(x1, x2)
Minimum arguments, element-wise.

Args:
    x1 (~chainerx.ndarray or scalar): Input array.
    x2 (~chainerx.ndarray or scalar): Input array.

Returns:
    :class:`~chainerx.ndarray`:
        Returned array: :math:`y = min(\\{x_1, x_2\\})`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input arrays ``x1`` and ``x2``.

.. seealso:: :data:`numpy.minimum`
""")

    _docs.set_doc(
        chainerx.remainder,
        """remainder(x1, x2)
Return element-wise remainder of division.

Args:
    x1 (~chainerx.ndarray or scalar): Input array.
    x2 (~chainerx.ndarray or scalar): Input array.

Returns:
    :class:`~chainerx.ndarray`:
        Returned array: The element-wise remainder of
        the quotient ``floor_divide(x1, x2)``.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input arrays ``x1`` and ``x2``.

.. seealso:: :data:`numpy.remainder`
""")

    _docs.set_doc(
        chainerx.exp,
        """exp(x)
Numerical exponential, element-wise.

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = \\exp x`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.

.. seealso:: :data:`numpy.exp`
""")

    _docs.set_doc(
        chainerx.log,
        """log(x)
Natural logarithm, element-wise.

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = \\ln x`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.

.. seealso:: :data:`numpy.log`
""")

    _docs.set_doc(
        chainerx.log10,
        """log10(x)
Base 10 logarithm, element-wise.

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = \\log_{10} x`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.

.. seealso:: :data:`numpy.log10`
""")

    _docs.set_doc(
        chainerx.log2,
        """log2(x)
Base 2 logarithm, element-wise.

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = \\log_{2} x`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.

.. seealso:: :data:`numpy.log2`
""")

    _docs.set_doc(
        chainerx.log1p,
        """log1p(x)
Natural logarithm of one plus the input, element-wise.

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = \\log(1 + x)`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.

.. seealso:: :data:`numpy.log1p`
""")

    _docs.set_doc(
        chainerx.logsumexp,
        """logsumexp(x, axis=None, keepdims=False)
The log of the sum of exponentials of input array.

Args:
    x (~chainerx.ndarray): Input array.
    axis (None or int or tuple of ints):
        Axis or axes along which a sum is performed.
        The flattened array is used by default.
    keepdims (bool):
        If this is set to ``True``, the reduced axes are left in the result
        as dimensions with size one.

Returns:
    :class:`~chainerx.ndarray`: The log of the sum of exponentials of
    input elements over a given axis.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.
""")

    _docs.set_doc(
        chainerx.log_softmax,
        """log_softmax(x, axis=None)
The log of the softmax of input array.

Args:
    x (~chainerx.ndarray): Input array.
    axis (None or int or tuple of ints):
        Axis or axes along which a sum is performed.
        The flattened array is used by default.

Returns:
    :class:`~chainerx.ndarray`: The log of the softmax of input elements
    over a given axis.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.
""")

    _docs.set_doc(
        chainerx.square,
        """square(x)
Returns the element-wise square of the input.

Args:
    x (~chainerx.ndarray or scalar): Input data

Returns:
    ~chainerx.ndarray: Returned array: :math:`y = x * x`.
    A scalar is returned if ``x`` is a scalar.

Note:
    During backpropagation, this function propagates the gradient
    of the output array to the input array ``x``.

.. seealso:: :data:`numpy.square`
""")

    _docs.set_doc(
        chainerx.sqrt,
        """sqrt(x)
Non-negative square-root, element-wise

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = \\sqrt x`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.

.. seealso:: :data:`numpy.sqrt`
""")

    _docs.set_doc(
        chainerx.sinh,
        """sinh(x)
Hyperbolic Sine, element-wise

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = \\sinh x`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.

.. seealso:: :data:`numpy.sinh`
""")

    _docs.set_doc(
        chainerx.cosh,
        """cosh(x)
Hyperbolic Cosine, element-wise

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = \\cosh x`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.

.. seealso:: :data:`numpy.cosh`
""")

    _docs.set_doc(
        chainerx.tanh,
        """tanh(x)
Element-wise hyperbolic tangent function.

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = \\tanh x`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.

.. seealso:: :data:`numpy.tanh`
""")

    _docs.set_doc(
        chainerx.sigmoid,
        """sigmoid(x)
Element-wise sigmoid logistic function.

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array:
    :math:`f(x) = (1 + \\exp(-x))^{-1}`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.

.. seealso:: :func:`chainer.functions.sigmoid`
""")

    _docs.set_doc(
        chainerx.sin,
        """sin(x)
Sine, element-wise

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = \\sin x`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.

.. seealso:: :data:`numpy.sin`
""")

    _docs.set_doc(
        chainerx.cos,
        """cos(x)
Cosine, element-wise

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = \\cos x`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.

.. seealso:: :data:`numpy.cos`
""")

    _docs.set_doc(
        chainerx.ceil,
        """ceil(x)
Return the ceiling of the input, element-wise..

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: The ceiling of each element in array.

.. seealso:: :data:`numpy.ceil`
""")

    _docs.set_doc(
        chainerx.tan,
        """tan(x)
Tangent, element-wise

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = \\tan x`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.

.. seealso:: :data:`numpy.tan`
""")

    _docs.set_doc(
        chainerx.relu,
        """Rectified Linear Unit function.
Args:
    x (~chainerx.ndarray): Input array.
Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = \\max (0, x)`.
Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.
""")

    _docs.set_doc(
        chainerx.tree_lstm,
        """tree_lstm(*inputs)
TreeLSTM unit as an activation function.

This function implements TreeLSTM units both for
N-ary TreeLSTM and Child-Sum TreeLSTM.
Let the children cell states
:math:`c_{\\text{1}}, c_{\\text{2}}, \\dots, c_{\\text{N}}`,
and the incoming signal :math:`x`.
First, the incoming signal :math:`x` is split into (3 + N) arrays
:math:`a, i, o, f_{\\text{1}}, f_{\\text{2}}, ..., f_{\\text{N}}`
of the same shapes along the second axis.
It means that :math:`x` 's second axis must have (3 + N) times
of the length of each :math:`c_{n}`.
The splitted input signals are corresponding to

    - :math:`a` : sources of cell input
    - :math:`i` : sources of input gate
    - :math:`o` : sources of output gate
    - :math:`f_{n}` : sources of forget gate for n-th ary

Second, it computes outputs as

.. math::
    c &amp;= \\tanh(a) \\text{sigmoid}(i) \\\\
      &amp; + c_{\\text{1}} \\text{sigmoid}(f_{\\text{1}}), \\\\
      &amp; + c_{\\text{2}} \\text{sigmoid}(f_{\\text{2}}), \\\\
      &amp; + ..., \\\\
      &amp; + c_{\\text{N}} \\text{sigmoid}(f_{\\text{N}}), \\\\
    h &amp;= \\tanh(c) \\text{sigmoid}(o).

These are returned as a tuple of (N + 1) variables.

Args:
    inputs (list of :class:`~chainerx.array`): Variable arguments which
        include all cell vectors from child-nodes, and an input vector.
        Each of the cell vectors and the input vector is
        :class:`~chainerx.array`.
        The input vector must have the second dimension whose size
        is (N + 3) times of that of each cell,
        where N denotes the total number of cells.

Returns:
    tuple: Two :class:`~chainerx.array` objects ``c`` and ``h``. ``c`` is
    the updated cell state. ``h`` indicates the outgoing signal.

See the papers for details: `Improved Semantic Representations From
Tree-Structured Long Short-Term Memory Networks
&lt;https://www.aclweb.org/anthology/P15-1150&gt;`_ and
`A Fast Unified Model for Parsing and Sentence Understanding
&lt;https://arxiv.org/pdf/1603.06021.pdf&gt;`_.
Tai et al.'s N-Ary TreeLSTM is little extended in
Bowman et al., and this link is based on
the variant by Bowman et al.
Specifically, eq. 10 in Tai et al. only has one :math:`W` matrix
to be applied to :math:`x`, consistently for all children.
On the other hand, Bowman et al.'s model has multiple matrices,
each of which affects the forget gate for each child's cell individually.

.. admonition:: Example

    Assuming ``y`` is the current input signal, ``c`` is the previous cell
    state, and ``h`` is the previous output signal from an
    :meth:`~chainerx.tree_lstm` function.
    Each of ``y``, ``c`` and ``h`` has ``n_units`` channels.
    Using 2-ary (binary) TreeLSTM,

    most typical preparation of ``x`` is

    &gt;&gt;&gt; c1 = chainerx.ones((4, 10), dtype = chainerx.float32)
    &gt;&gt;&gt; c2 = chainerx.ones((4, 10), dtype = chainerx.float32)
    &gt;&gt;&gt; x = chainerx.ones((4, 50), dtype = chainerx.float32)
    &gt;&gt;&gt; c, h = chainerx.tree_lstm(c1, c2, x)
    """)

    _docs.set_doc(
        chainerx.slstm,
        """slstm(c_prev1, c_prev2, x1, x2)
S-LSTM units as an activation function.

This function implements S-LSTM unit. It is an extension of LSTM unit
applied to tree structures.
The function is applied to binary trees. Each node has two child nodes.
It gets four arguments, previous cell states ``c_prev1`` and ``c_prev2``,
and input arrays ``x1`` and ``x2``.
First both input arrays ``x1`` and ``x2`` are split into eight arrays
:math:`a_1, i_1, f_1, o_1`, and :math:`a_2, i_2, f_2, o_2`. They have the
same shape along the second axis.
It means that ``x1`` and ``x2`` 's second axis must have 4 times
the length of ``c_prev1`` and ``c_prev2``.
The split input arrays are corresponding to

    - :math:`a_i` : sources of cell input
    - :math:`i_i` : sources of input gate
    - :math:`f_i` : sources of forget gate
    - :math:`o_i` : sources of output gate

It computes the updated cell state ``c`` and the outgoing signal
``h`` as.

.. math::
    c &amp;= \\tanh(a_1 + a_2) \\sigma(i_1 + i_2)
       + c_{\\text{prev}1} \\sigma(f_1)
       + c_{\\text{prev}2} \\sigma(f_2), \\\\
    h &amp;= \\tanh(c) \\sigma(o_1 + o_2),

where :math:`\\sigma` is the elementwise sigmoid function.
The function returns ``c`` and ``h`` as a tuple.

Args:
    c_prev1 (:class:`~chainerx.array`):
        Variable that holds the previous cell state of the first child
        node. The cell state should be a zero array or the output of
        the previous call of LSTM.
    c_prev2 (:class:`~chainerx.array`):
        Variable that holds the previous cell state of the second child
        node.
    x1 (:class:`~chainerx.array`):
        Variable that holds the sources of cell input, input gate, forget
        gate and output gate from the first child node. It must have the
        second dimension whose size is four times of that of the cell
        state.
    x2 (:class:`~chainerx.array`):
        Variable that holds the input sources from the second child node.

Returns:
    tuple: Two :class:`~chainerx.array` objects ``c`` and ``h``. ``c`` is
    the cell state. ``h`` indicates the outgoing signal.

See detail in paper: `Long Short-Term Memory Over Tree Structures
&lt;https://arxiv.org/abs/1503.04881&gt;`_.

.. admonition:: Example

    Assuming ``c1``, ``c2`` is the previous cell state of children,
    and ``h1``, ``h2`` is the previous outgoing signal from children.
    Each of ``c1``, ``c2``, ``h1`` and ``h2`` has ``n_units`` channels.
    Most typical preparation of ``x1``, ``x2`` is:

    &gt;&gt;&gt; n_units = 100
    &gt;&gt;&gt; c1 = chainerx.ones((1, n_units), np.float32)
    &gt;&gt;&gt; c2 = chainerx.ones((1, n_units), np.float32)
    &gt;&gt;&gt; x1 = chainerx.ones((1, 4 * n_units), chainerx.float32)
    &gt;&gt;&gt; x2 = chainerx.ones((1, 4 * n_units), chainerx.float32)
    &gt;&gt;&gt; c, h = chainerx.slstm(c1, c2, x1, x2)
    """)

    _docs.set_doc(
        chainerx.arcsin,
        """arcsin(x)
Inverse sine, element-wise

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = \\arcsin x`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.

.. seealso:: :data:`numpy.arcsin`
""")

    _docs.set_doc(
        chainerx.arccos,
        """arccos(x)
Trigonometric inverse cosine, element-wise

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = \\arccos x`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.

.. seealso:: :data:`numpy.arccos`
""")

    _docs.set_doc(
        chainerx.arctan,
        """arctan(x)
Trigonometric inverse tangent, element-wise

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = \\arctan x`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.

.. seealso:: :data:`numpy.arctan`
""")

    _docs.set_doc(
        chainerx.arctan2,
        """arctan2(x1, x2)
Element-wise arc tangent of :math:`\\frac{x_1}{x_2}` choosing the quadrant
correctly.

Args:
    x1 (~chainerx.ndarray): Input array.
    x2 (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returns an array where each element
    represents :math:`\\theta` in the range :math:`[-\\pi, \\pi]`, such
    that :math:`x_1 = r \\sin(\\theta)` and :math:`x_2 = r \\cos(\\theta)`
    for some :math:`r &gt; 0`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x1`` and/or ``x2``.

.. seealso:: :data:`numpy.arctan2`
""")

    _docs.set_doc(
        chainerx.arcsinh,
        """arcsinh(x)
Inverse hyperbolic sine, element-wise

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = \\arcsinh x`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.

.. seealso:: :data:`numpy.arcsinh`
""")

    _docs.set_doc(
        chainerx.arccosh,
        """arccosh(x)
Inverse hypberbolic inverse cosine, element-wise

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = \\arccosh x`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.

.. seealso:: :data:`numpy.arccosh`
""")

    _docs.set_doc(
        chainerx.fabs,
        """fabs(x)
Compute the absolute values element-wise.
Args:
    x (~chainerx.ndarray): Input array.
Returns:
    :class:`~chainerx.ndarray`: The absolute values of x, the returned values
    are always floats.
.. seealso:: :data:`numpy.fabs`
""")

    _docs.set_doc(
        chainerx.sign,
        """sign(x)
Returns an element-wise indication of the sign of a number.
The sign function returns :math:`-1 if x &lt; 0, 0 if x==0, 1 if x &gt; 0`.
``nan`` is returned for ``nan`` inputs.
Args:
    x (~chainerx.ndarray): Input array.
Returns:
    :class:`~chainerx.ndarray`: The sign of x.
.. seealso:: :data:`numpy.sign`
""")

    _docs.set_doc(
        chainerx.floor,
        """floor(x)
Return the floor of the input, element-wise.
Args:
    x (~chainerx.ndarray): Input array.
Returns:
    :class:`~chainerx.ndarray`: The floor of each element in array.
.. seealso:: :data:`numpy.floor`
""")

    _docs.set_doc(
        chainerx.isnan,
        """isnan(x)
Test element-wise for NaN and return result as a boolean array.

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: True where ``x`` is NaN, false otherwise

Note:
    During backpropagation, this function does not propagate gradients.

.. seealso:: :data:`numpy.isnan`
""")

    _docs.set_doc(
        chainerx.isfinite,
        """isfinite(x)
Test element-wise for finiteness (not infinity or not Not a Number).

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: True where x is not positive infinity,
    negative infinity, or NaN; false otherwise.

Note:
    During backpropagation, this function does not propagate gradients.

.. seealso:: :data:`numpy.isfinite`
""")

    _docs.set_doc(
        chainerx.isinf,
        """isinf(x)
Test element-wise for positive or negative infinity.

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: True where ``x`` is positive or negative
    infinity, false otherwise.

Note:
    During backpropagation, this function does not propagate gradients.

.. seealso:: :data:`numpy.isinf`
""")

    _docs.set_doc(
        chainerx.bitwise_and,
        """bitwise_and(x1, x2)
Compute the bit-wise AND of two arrays element-wise.

Args:
    x1 (~chainerx.ndarray or scalar): Input array of integers.
    x2 (~chainerx.ndarray or scalar): Input array of integers.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = x_1 \\&amp; x_2`

Note:
    During backpropagation, this function does not propagate gradients.

.. seealso:: :data:`numpy.bitwise_and`
""")

    _docs.set_doc(
        chainerx.bitwise_or,
        """bitwise_or(x1, x2)
Compute the bit-wise OR of two arrays element-wise.

Args:
    x1 (~chainerx.ndarray or scalar): Input array of integers.
    x2 (~chainerx.ndarray or scalar): Input array of integers.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = x_1 | x_2`

Note:
    During backpropagation, this function does not propagate gradients.

.. seealso:: :data:`numpy.bitwise_or`
""")

    _docs.set_doc(
        chainerx.bitwise_xor,
        """bitwise_xor(x1, x2)
Compute the bit-wise XOR of two arrays element-wise.

Args:
    x1 (~chainerx.ndarray or scalar): Input array of integers.
    x2 (~chainerx.ndarray or scalar): Input array of integers.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = x_1 \\oplus x_2`

Note:
    During backpropagation, this function does not propagate gradients.

.. seealso:: :data:`numpy.bitwise_xor`
""")

    _docs.set_doc(
        chainerx.left_shift,
        """left_shift(x1, x2)
Shift the bits of an integer to the left.

Args:
    x1 (~chainerx.ndarray or scalar): Input array of integers.
    x2 (~chainerx.ndarray or scalar): Input array of integers.

Returns:
    :class:`~chainerx.ndarray`: Return `x1` with bits shifted `x2` times to the left.

Note:
    During backpropagation, this function does not propagate gradients.

.. seealso:: :data:`numpy.left_shift`
""")  # NOQA

    _docs.set_doc(
        chainerx.right_shift,
        """right_shift(x1, x2)
Shift the bits of an integer to the right.

Args:
    x1 (~chainerx.ndarray or scalar): Input array of integers.
    x2 (~chainerx.ndarray or scalar): Input array of integers.

Returns:
    :class:`~chainerx.ndarray`: Return `x1` with bits shifted `x2` times to the right.

Note:
    During backpropagation, this function does not propagate gradients.

.. seealso:: :data:`numpy.right_shift`
""")  # NOQA


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag10709')" href="javascript:;">
chainer-7.2.0/chainerx/_docs/array.py: 5-422
</a>
<div class="mid" id="frag10709" style="display:none"><pre>
def set_docs():
    ndarray = chainerx.ndarray

    _docs.set_doc(
        ndarray,
        """ndarray(shape, dtype, device=None)
Multi-dimensional array, the central data structure of ChainerX.

This class, along with other APIs in the :mod:`chainerx` module, provides a
subset of NumPy APIs. This class works similar to :class:`numpy.ndarray`,
except for some differences including the following noticeable points:

- :class:`chainerx.ndarray` has a :attr:`device` attribute. It indicates on
  which device the array is allocated.
- :class:`chainerx.ndarray` supports :ref:`Define-by-Run &lt;define_by_run&gt;`
  backpropagation. Once you call :meth:`require_grad`, the array starts
  recording the operations applied to it recursively. Gradient of the result
  with respect to the original array can be computed then with the
  :meth:`backward` method or the :func:`chainerx.backward` function.

Args:
    shape (tuple of ints): Shape of the new array.
    dtype: Data type.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device &lt;chainerx_device&gt;` is chosen.

.. seealso:: :class:`numpy.ndarray`
""")

    _docs.set_doc(
        ndarray.data_ptr,
        """int: Address of the underlying memory allocation.

The meaning of the address is device-dependent.
""")

    _docs.set_doc(
        ndarray.data_size,
        'int: Total size of the underlying memory allocation.')

    _docs.set_doc(
        ndarray.device, '~chainerx.Device: Device on which the data exists.')

    _docs.set_doc(ndarray.dtype, 'Data type of the array.')

    # TODO(beam2d): Write about backprop id.
    _docs.set_doc(
        ndarray.grad,
        """~chainerx.ndarray: Gradient held by the array.

It is ``None`` if the gradient is not available.
Setter of this property overwrites the gradient.
""")

    _docs.set_doc(
        ndarray.is_contiguous,
        'bool: ``True`` iff the array is stored in the C-contiguous order.')

    _docs.set_doc(ndarray.itemsize, 'int: Size of each element in bytes.')

    _docs.set_doc(
        ndarray.nbytes,
        """int: Total size of all elements in bytes.

It does not count skips between elements.""")

    _docs.set_doc(ndarray.ndim, 'int: Number of dimensions.')

    _docs.set_doc(
        ndarray.offset,
        'int: Offset of the first element from the memory allocation in bytes.'
    )

    _docs.set_doc(
        ndarray.shape,
        """tuple of int: Lengths of axes.

.. note::
    Currently, this property does not support setter.""")

    _docs.set_doc(ndarray.size, 'int: Number of elements in the array.')

    _docs.set_doc(ndarray.strides, 'tuple of int: Strides of axes in bytes.')

    _docs.set_doc(
        ndarray.T,
        """~chainerx.ndarray: Shape-reversed view of the array.

New array is created at every access to this property.
``x.T`` is just a shorthand of ``x.transpose()``.
""")

    _docs.set_doc(
        ndarray.__getitem__,
        """___getitem__(self, key)
Returns self[key].

.. note::
    Currently, only basic indexing is supported not advanced indexing.
""")

    def unary_op(name, s):
        _docs.set_doc(getattr(ndarray, name), '{}()\n{}'.format(name, s))

    unary_op('__bool__', 'Casts a size-one array into a :class:`bool` value.')
    unary_op('__float__',
             'Casts a size-one array into a :class:`float` value.')
    unary_op('__int__', 'Casts a size-one array into :class:`int` value.')
    unary_op('__len__', 'Returns the length of the first axis.')
    unary_op('__neg__', 'Computes ``-x`` elementwise.')

    def binary_op(name, s):
        _docs.set_doc(getattr(ndarray, name), '{}(other)\n{}'.format(name, s))

    binary_op('__eq__', 'Computes ``x == y`` elementwise.')
    binary_op('__ne__', 'Computes ``x != y`` elementwise.')
    binary_op('__lt__', 'Computes ``x &lt; y`` elementwise.')
    binary_op('__le__', 'Computes ``x &lt;= y`` elementwise.')
    binary_op('__ge__', 'Computes ``x &gt;= y`` elementwise.')
    binary_op('__gt__', 'Computes ``x &gt; y`` elementwise.')

    binary_op('__iadd__', 'Computes ``x += y`` elementwise.')
    binary_op('__isub__', 'Computes ``x -= y`` elementwise.')
    binary_op('__imul__', 'Computes ``x *= y`` elementwise.')
    binary_op('__itruediv__', 'Computes ``x /= y`` elementwise.')
    binary_op('__iand__', 'Computes ``x &amp;= y`` elementwise.')
    binary_op('__ior__', 'Computes ``x |= y`` elementwise.')
    binary_op('__ixor__', 'Computes ``x ^= y`` elementwise.')

    binary_op('__add__', 'Computes ``x + y`` elementwise.')
    binary_op('__sub__', 'Computes ``x - y`` elementwise.')
    binary_op('__mul__', 'Computes ``x * y`` elementwise.')
    binary_op('__truediv__', 'Computes ``x / y`` elementwise.')

    binary_op('__and__', 'Computes ``x &amp; y`` elementwise.')
    binary_op('__or__', 'Computes ``x | y`` elementwise.')
    binary_op('__xor__', 'Computes ``x ^ y`` elementwise.')

    binary_op('__radd__', 'Computes ``y + x`` elementwise.')
    binary_op('__rsub__', 'Computes ``y - x`` elementwise.')
    binary_op('__rmul__', 'Computes ``y * x`` elementwise.')
    binary_op('__rand__', 'Computes ``y &amp; x`` elementwise.')
    binary_op('__ror__', 'Computes ``y | x`` elementwise.')
    binary_op('__rxor__', 'Computes ``y ^ x`` elementwise.')

    # TODO(beam2d): Write about as_grad_stopped(backprop_ids, copy) overload.
    _docs.set_doc(
        ndarray.as_grad_stopped,
        """as_grad_stopped(copy=False)
Creates a view or a copy of the array that stops gradient propagation.

This method behaves similar to :meth:`view` and :meth:`copy`, except that
the gradient is not propagated through this operation (internally, this
method creates a copy or view of the array without connecting the computational
graph for backprop).

Args:
    copy (bool): If ``True``, it copies the array. Otherwise, it returns a view
        of the original array.

Returns:
    ~chainerx.ndarray:
        A view or a copy of the array without propagating the  gradient on
        backprop.
""")

    _docs.set_doc(
        ndarray.argmax,
        """argmax(axis=None)
Returns the indices of the maximum elements along a given axis.

See :func:`chainerx.argmax` for the full documentation.
""")

    _docs.set_doc(
        ndarray.argmin,
        """argmin(axis=None)
Returns the indices of the minimum elements along a given axis.

See :func:`chainerx.argmin` for the full documentation.
""")

    _docs.set_doc(
        ndarray.astype,
        """astype(dtype, copy=True)
Casts each element to the specified data type.

Args:
    dtype: Data type of the new array.
    copy (bool): If ``True``, this method always copies the data. Otherwise,
        it creates a view of the array if possible.

Returns:
    ~chainerx.ndarray: An array with the specified dtype.
""")

    _docs.set_doc(
        ndarray.backward,
        """backward(backprop_id=None, enable_double_backprop=False)
Performs backpropagation starting from this array.

This method is equivalent to ``chainerx.backward([self], *args)``.
See :func:`chainerx.backward` for the full documentation.
""")

    # TODO(beam2d): Write about backprop id.
    _docs.set_doc(
        ndarray.cleargrad,
        """cleargrad()
Clears the gradient held by this array.
""")

    _docs.set_doc(
        ndarray.copy,
        """copy()
Creates an array and copies all the elements to it.

The copied array is allocated on the same device as ``self``.

.. seealso:: :func:`chainerx.copy`
""")

    _docs.set_doc(
        ndarray.dot,
        """dot(b)
Returns the dot product with a given array.

See :func:`chainerx.dot` for the full documentation.
""")

    _docs.set_doc(
        ndarray.fill,
        """fill(value)
Fills the array with a scalar value in place.

Args:
    value: Scalar value with which the array will be filled.
""")

    # TODO(beam2d): Write about backprop_id argument.
    _docs.set_doc(
        ndarray.get_grad,
        """get_grad()
Returns the gradient held by the array.

If the gradient is not available, it returns ``None``.
""")

    # TODO(beam2d): Write about backprop_id argument.
    _docs.set_doc(
        ndarray.is_backprop_required,
        """is_backprop_required()
Returns ``True`` if gradient propagates through this array on backprop.

See the note on :meth:`require_grad` for details.
""")

    # TODO(beam2d): Write about backprop_id argument.
    _docs.set_doc(
        ndarray.is_grad_required,
        """is_grad_required()
Returns ``True`` if the gradient will be set after backprop.

See the note on :meth:`require_grad` for details.
""")

    _docs.set_doc(
        ndarray.item,
        """item()
Copies an element of an array to a standard Python scalar and returns it.

Returns:
    z:
        A copy of the specified element of the array as a suitable Python
        scalar.

.. seealso:: :func:`numpy.item`
""")

    _docs.set_doc(
        ndarray.max,
        """max(axis=None, keepdims=False)
Returns the maximum along a given axis.

See :func:`chainerx.amax` for the full documentation.
""")

    _docs.set_doc(
        ndarray.min,
        """min(axis=None, keepdims=False)
Returns the minimum along a given axis.

See :func:`chainerx.amin` for the full documentation.
""")

    # TODO(beam2d): Write about backprop_id argument.
    _docs.set_doc(
        ndarray.require_grad,
        """require_grad()
Declares that a gradient for this array will be made available after backprop.

Once calling this method, any operations applied to this array are recorded for
later backprop. After backprop, the :attr:`grad` attribute holds the gradient
array.

.. note::
    ChainerX distinguishes *gradient requirements* and *backprop requirements*
    strictly. They are strongly related, but different concepts as follows.

    - *Gradient requirement* indicates that the gradient array should be made
      available after backprop. This attribute **is not propagated** through
      any operations. It implicates the backprop requirement.
    - *Backprop requirement* indicates that the gradient should be propagated
      through the array during backprop. This attribute **is propagated**
      through differentiable operations.

    :meth:`require_grad` sets the gradient requirement flag. If you need to
    extract the gradient after backprop, you have to call :meth:`require_grad`
    on the array even if the array is an intermediate result of differentiable
    computations.

Returns:
    ~chainerx.ndarray: ``self``
""")

    _docs.set_doc(
        ndarray.reshape,
        """reshape(newshape)
Creates an array with a new shape and the same data.

See :func:`chainerx.reshape` for the full documentation.
""")

    _docs.set_doc(
        ndarray.set_grad,
        """set_grad(grad)
Sets a gradient to the array.

This method overwrites the gradient with a given array.

Args:
    grad (~chainerx.ndarray): New gradient array.
""")

    _docs.set_doc(
        ndarray.squeeze,
        """squeeze(axis=None)
Removes size-one axes from an array.

See :func:`chainerx.squeeze` for the full documentation.
""")

    _docs.set_doc(
        ndarray.swapaxes,
        """swapaxes(axis1, axis2)
Interchange two axes of an array..

See :func:`chainerx.swapaxes` for the full documentation.
""")

    _docs.set_doc(
        ndarray.repeat,
        """repeat(repeats, axis=None)
Constructs an array by repeating a given array.

See :func:`chainerx.repeats` for the full documentation.
""")

    _docs.set_doc(
        ndarray.sum,
        """sum(axis=None, keepdims=False)
Returns the sum of an array along given axes.

See :func:`chainerx.sum` for the full documentation.
""")

    _docs.set_doc(
        ndarray.take,
        """take(indices, axis)
Takes elements from the array along an axis.

See :func:`chainerx.take` for the full documentation.
""")

    _docs.set_doc(
        ndarray.to_device,
        """to_device(device, index=None)
Transfers the array to the specified device.

Args:
    device (~chainerx.Device or str): Device to which the array is transferred,
        or a backend name. If it is a backend name, ``index`` should also be
        specified.
    index (int): Index of the device for the backend specified by ``device``.

Returns:
    ~chainerx.ndarray:
        An array on the target device.
        If the original array is already on the device, it is a view of that.
        Otherwise, it is a copy of the array on the target device.
""")

    _docs.set_doc(
        ndarray.transpose,
        """transpose(axes=None)
Creates a view of an array with permutated axes.

See :func:`chainerx.transpose` for the full documentation.
""")

    _docs.set_doc(
        ndarray.view,
        """view()
Returns a view of the array.

The returned array shares the underlying buffer, though it has a different
identity as a Python object.
""")
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<script language="JavaScript">
function ShowHide(divId) { 
    if(document.getElementById(divId).style.display == 'none') {
        document.getElementById(divId).style.display='block';
    } else { 
        document.getElementById(divId).style.display = 'none';
    } 
}
</script>
</body>
</html>

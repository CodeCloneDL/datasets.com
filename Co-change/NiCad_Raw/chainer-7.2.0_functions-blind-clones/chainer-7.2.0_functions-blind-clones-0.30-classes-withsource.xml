<clones>
<systeminfo processor="nicad6" system="chainer-7.2.0" granularity="functions-blind" threshold="30%" minlines="10" maxlines="2500"/>
<cloneinfo npcs="10723" npairs="784"/>
<runinfo ncompares="1508116" cputime="510590"/>
<classinfo nclasses="282"/>

<class classid="1" nclones="7" nlines="10" similarity="70">
<source file="systems/chainer-7.2.0/chainer/distributions/geometric.py" startline="54" endline="65" pcid="156">
    def sample_n(self, n):
        xp = chainer.backend.get_array_module(self.p)
        if xp is cuda.cupy:
            eps = xp.random.geometric(
                self.p.data,
                size=(n,)+self.batch_shape, dtype=self.p.dtype)
        else:
            eps = xp.random.geometric(
                self.p.data,
                size=(n,)+self.batch_shape).astype(self.p.dtype)
        return chainer.Variable(eps)

</source>
<source file="systems/chainer-7.2.0/chainer/distributions/gumbel.py" startline="78" endline="88" pcid="348">
    def sample_n(self, n):
        xp = chainer.backend.get_array_module(self.loc)
        if xp is cuda.cupy:
            eps = xp.random.gumbel(
                size=(n,)+self.batch_shape, dtype=self.loc.dtype)
        else:
            eps = xp.random.gumbel(
                size=(n,)+self.batch_shape).astype(self.loc.dtype)
        noise = self.scale * eps + self.loc
        return noise

</source>
<source file="systems/chainer-7.2.0/chainer/distributions/gamma.py" startline="76" endline="86" pcid="362">
    def sample_n(self, n):
        xp = chainer.backend.get_array_module(self.k)
        if xp is cuda.cupy:
            eps = xp.random.gamma(
                self.k.data, size=(n,) + self.batch_shape, dtype=self.k.dtype)
        else:
            eps = xp.random.gamma(
                self.k.data, size=(n,) + self.batch_shape).astype(self.k.dtype)
        noise = broadcast.broadcast_to(self.theta, eps.shape) * eps
        return noise

</source>
<source file="systems/chainer-7.2.0/chainer/distributions/chisquare.py" startline="69" endline="79" pcid="173">
    def sample_n(self, n):
        xp = chainer.backend.get_array_module(self.k)
        if xp is cuda.cupy:
            eps = xp.random.chisquare(
                self.k.data, (n,)+self.k.shape, dtype=self.k.dtype)
        else:
            eps = xp.random.chisquare(
                self.k.data, (n,)+self.k.shape).astype(self.k.dtype)
        noise = chainer.Variable(eps)
        return noise

</source>
<source file="systems/chainer-7.2.0/chainer/distributions/exponential.py" startline="76" endline="86" pcid="251">
    def sample_n(self, n):
        xp = chainer.backend.get_array_module(self.lam)
        if xp is cuda.cupy:
            eps = xp.random.standard_exponential(
                (n,)+self.lam.shape, dtype=self.lam.dtype)
        else:
            eps = xp.random.standard_exponential(
                (n,)+self.lam.shape).astype(self.lam.dtype)
        noise = eps / self.lam
        return noise

</source>
<source file="systems/chainer-7.2.0/chainer/distributions/cauchy.py" startline="91" endline="103" pcid="189">
    def sample_n(self, n):
        xp = chainer.backend.get_array_module(self.loc)
        if xp is cuda.cupy:
            eps = xp.random.standard_cauchy(
                (n,)+self.loc.shape, dtype=self.loc.dtype)
        else:
            eps = xp.random.standard_cauchy(
                (n,)+self.loc.shape).astype(self.loc.dtype)

        noise = self.scale * eps + self.loc

        return noise

</source>
<source file="systems/chainer-7.2.0/chainer/distributions/pareto.py" startline="90" endline="102" pcid="225">
    def sample_n(self, n):
        xp = chainer.backend.get_array_module(self.scale)
        if xp is cuda.cupy:
            eps = xp.random.pareto(
                self.alpha.data, (n,)+self.batch_shape, dtype=self.alpha.dtype)
        else:
            eps = xp.random.pareto(
                self.alpha.data, (n,)+self.batch_shape
            ).astype(self.alpha.dtype)

        noise = self.scale * (eps + 1)
        return noise

</source>
</class>

<class classid="2" nclones="4" nlines="21" similarity="86">
<source file="systems/chainer-7.2.0/chainer/function_node.py" startline="1309" endline="1346" pcid="447">

def _extract_apply_in_data(inputs):
    # Extracts arrays from FunctionNode.apply() inputs.
    #
    # A flag that indicates whether inputs are chainerx arrays is also
    # returned.
    #
    # Each object in `inputs` may be `Variable` or an array.
    # If it's a `Variable` and its underlying array is a chainerx array,
    # `Variable._data[0]` (which is backproppable in contrast to
    # `Variable.array`) is returned.
    #
    # If at least one of the arrays is a ChainerX array, all other
    # arrays need to be ChainerX arrays.
    if not inputs:
        return False, ()

    if chainerx.is_available():
        has_chainerx_array = False

        # Unwrap arrays
        arrays = []
        for x in inputs:
            if isinstance(x, variable.Variable):
                arrays.append(x._data[0])
                if x._has_chainerx_array:
                    has_chainerx_array = True
            else:  # x is ndarray
                arrays.append(x)
                if not has_chainerx_array:
                    if isinstance(x, chainerx.ndarray):
                        has_chainerx_array = True
        return has_chainerx_array, tuple(arrays)
    else:
        return False, tuple([
            x.raw_array if isinstance(x, variable.Variable) else x
            for x in inputs])

</source>
<source file="systems/chainer-7.2.0/chainer/functions/rnn/n_step_gru.py" startline="21" endline="48" pcid="2425">
def _extract_apply_in_data(inputs):
    if not inputs:
        return False, ()

    if chainerx.is_available():
        has_chainerx_array = False

        # Unwrap arrays
        arrays = []
        for x in inputs:
            if isinstance(x, variable.Variable):
                if x._has_chainerx_array:
                    arrays.append(x._data[0])
                    has_chainerx_array = True
                else:
                    arrays.append(x.array)
            else:  # x is ndarray
                arrays.append(x)
                if not has_chainerx_array:
                    if isinstance(x, chainerx.ndarray):
                        has_chainerx_array = True
        return has_chainerx_array, tuple(arrays)
    else:
        return False, tuple([
            x.array if isinstance(x, variable.Variable) else x
            for x in inputs])


</source>
<source file="systems/chainer-7.2.0/chainer/functions/rnn/n_step_lstm.py" startline="20" endline="47" pcid="2443">
def _extract_apply_in_data(inputs):
    if not inputs:
        return False, ()

    if chainerx.is_available():
        has_chainerx_array = False

        # Unwrap arrays
        arrays = []
        for x in inputs:
            if isinstance(x, variable.Variable):
                if x._has_chainerx_array:
                    arrays.append(x._data[0])
                    has_chainerx_array = True
                else:
                    arrays.append(x.array)
            else:  # x is ndarray
                arrays.append(x)
                if not has_chainerx_array:
                    if isinstance(x, chainerx.ndarray):
                        has_chainerx_array = True
        return has_chainerx_array, tuple(arrays)
    else:
        return False, tuple([
            x.array if isinstance(x, variable.Variable) else x
            for x in inputs])


</source>
<source file="systems/chainer-7.2.0/chainer/functions/rnn/n_step_rnn.py" startline="63" endline="90" pcid="2453">
def _extract_apply_in_data(inputs):
    if not inputs:
        return False, ()

    if chainerx.is_available():
        has_chainerx_array = False

        # Unwrap arrays
        arrays = []
        for x in inputs:
            if isinstance(x, variable.Variable):
                if x._has_chainerx_array:
                    arrays.append(x._data[0])
                    has_chainerx_array = True
                else:
                    arrays.append(x.array)
            else:  # x is ndarray
                arrays.append(x)
                if not has_chainerx_array:
                    if isinstance(x, chainerx.ndarray):
                        has_chainerx_array = True
        return has_chainerx_array, tuple(arrays)
    else:
        return False, tuple([
            x.array if isinstance(x, variable.Variable) else x
            for x in inputs])


</source>
</class>

<class classid="3" nclones="7" nlines="10" similarity="70">
<source file="systems/chainer-7.2.0/chainer/training/extensions/inverse_shift.py" startline="36" endline="48" pcid="496">
    def __init__(self, attr, gamma, power,
                 init=None, target=None, optimizer=None):
        self._attr = attr
        if gamma < 0:
            raise ValueError('InverseShift does not support negative gamma')
        self._gamma = gamma
        self._power = power
        self._init = init
        self._target = target
        self._optimizer = optimizer
        self._t = 0
        self._last_value = None

</source>
<source file="systems/chainer-7.2.0/chainer/training/extensions/exponential_shift.py" startline="33" endline="43" pcid="527">
    def __init__(self, attr, rate, init=None, target=None, optimizer=None):
        self._attr = attr
        if rate < 0:
            raise ValueError('ExponentialShift does not support negative rate')
        self._rate = rate
        self._init = init
        self._target = target
        self._optimizer = optimizer
        self._t = 0
        self._last_value = None

</source>
<source file="systems/chainer-7.2.0/chainer/training/extensions/polynomial_shift.py" startline="39" endline="49" pcid="604">
    def __init__(self, attr, rate, max_count, init=None, target=None,
                 optimizer=None):
        self._attr = attr
        self._rate = rate
        self._init = init
        self._target = target
        self._optimizer = optimizer
        self._t = 0
        self._max_count = max_count
        self._last_value = None

</source>
<source file="systems/chainer-7.2.0/chainer/training/extensions/step_shift.py" startline="38" endline="48" pcid="571">
    def __init__(self, attr, gamma, step, init=None, target=None,
                 optimizer=None):
        self._attr = attr
        self._gamma = gamma
        self._step = step
        self._init = init
        self._target = target
        self._optimizer = optimizer
        self._t = 0
        self._last_value = None

</source>
<source file="systems/chainer-7.2.0/chainer/functions/normalization/batch_renormalization.py" startline="21" endline="32" pcid="908">
    def __init__(self, eps=2e-5, mean=None, var=None, decay=0.9,
                 rmax=1, dmax=0, update_statistics=True):
        self._running_mean = mean
        self._running_var = var
        self.rmax = rmax
        self.dmax = dmax
        self.r = None
        self.update_statistics = update_statistics

        self.eps = eps
        self.decay = decay

</source>
<source file="systems/chainer-7.2.0/chainer/functions/normalization/local_response_normalization.py" startline="125" endline="136" pcid="944">
    def __init__(self, n, k, alpha, beta, use_ideep,
                 scale=None, indexes=None, unit_scale=None):
        self.n = n
        self.k = k
        self.alpha = alpha
        self.beta = beta
        self._use_ideep = use_ideep

        self.scale = scale
        self.indexes = indexes
        self.unit_scale = unit_scale

</source>
<source file="systems/chainer-7.2.0/chainer/functions/normalization/batch_normalization.py" startline="473" endline="484" pcid="970">
    def __init__(self, eps, expander, axis, mean, var,
                 inv_std, key_axis, impl, forward_data):
        self.eps = eps
        self.expander = expander
        self.axis = axis
        self.mean = mean
        self.var = var  # Only used in iDeep implementation
        self.inv_std = inv_std
        self.key_axis = key_axis
        self._impl = impl
        self.forward_data = forward_data

</source>
</class>

<class classid="4" nclones="3" nlines="12" similarity="83">
<source file="systems/chainer-7.2.0/chainer/training/extensions/inverse_shift.py" startline="60" endline="76" pcid="498">
    def __call__(self, trainer):
        self._t += 1

        optimizer = self._get_optimizer(trainer)
        value = self._init * (1 + self._gamma * self._t) ** (-self._power)
        if self._target is not None:
            if self._power < 0:
                # almost same as value = min(value, self._target), but this
                # line supports negative values, too
                if value / self._target > 1:
                    value = self._target
            else:
                # ditto
                if value / self._target < 1:
                    value = self._target
        self._update_value(optimizer, value)

</source>
<source file="systems/chainer-7.2.0/chainer/training/extensions/exponential_shift.py" startline="55" endline="71" pcid="529">
    def __call__(self, trainer):
        self._t += 1

        optimizer = self._get_optimizer(trainer)
        value = self._init * (self._rate ** self._t)
        if self._target is not None:
            if self._rate > 1:
                # almost same as value = min(value, self._target), but this
                # line supports negative values, too
                if value / self._target > 1:
                    value = self._target
            else:
                # ditto
                if value / self._target < 1:
                    value = self._target
        self._update_value(optimizer, value)

</source>
<source file="systems/chainer-7.2.0/chainer/training/extensions/step_shift.py" startline="60" endline="75" pcid="573">
    def __call__(self, trainer):
        self._t += 1
        optimizer = self._get_optimizer(trainer)
        value = self._init * self._gamma ** numpy.floor(self._t / self._step)
        if self._target is not None:
            if self._gamma > 1:
                # almost same as value = min(value, self._target), but this
                # line supports negative values, too
                if value / self._target > 1:
                    value = self._target
            else:
                # ditto
                if value / self._target < 1:
                    value = self._target
        self._update_value(optimizer, value)

</source>
</class>

<class classid="5" nclones="2" nlines="26" similarity="75">
<source file="systems/chainer-7.2.0/chainer/training/updaters/multiprocess_parallel_updater.py" startline="334" endline="362" pcid="641">
def _gather(link, target):
    size, num = size_num_grads(link)

    ptrs = numpy.empty(num, dtype=numpy.uint64)
    dtypes = numpy.empty(num, dtype=numpy.int8)
    info = numpy.empty(num + 1, dtype=numpy.int32)
    info[0] = 0
    i = 0
    for _, param in sorted(link.namedparams()):
        if param.size == 0:
            continue
        ptrs[i] = 0  # NULL pointer
        d = getattr(param, target)
        if d is not None:
            ptrs[i] = d.data.ptr
        dtypes[i] = 0  # fp32
        if param.dtype == numpy.float16:
            dtypes[i] = 1  # fp16
        info[i + 1] = info[i] + param.size
        i += 1
    info[0] = num

    ptrs = cuda.to_gpu(ptrs)
    dtypes = cuda.to_gpu(dtypes)
    info = cuda.to_gpu(info)

    return _memcpy_gather()(ptrs, dtypes, info, size=size)


</source>
<source file="systems/chainer-7.2.0/chainer/training/updaters/multiprocess_parallel_updater.py" startline="426" endline="458" pcid="645">
def _scatter(link, array, target):
    size, num = size_num_grads(link)

    ptrs = numpy.zeros(num, dtype=numpy.uint64)
    dtypes = numpy.zeros(num, dtype=numpy.int8)
    info = numpy.zeros(num + 1, dtype=numpy.int32)
    info[0] = 0
    i = 0
    for _, param in sorted(link.namedparams()):
        if param.size == 0:
            continue
        ptrs[i] = 0  # NULL pointer
        d = getattr(param, target)
        if d is None:
            d = cuda.cupy.zeros(param.shape, dtype=param.dtype)
            setattr(param, target, d)
        ptrs[i] = d.data.ptr
        dtypes[i] = 0  # fp32
        if param.dtype == numpy.float16:
            dtypes[i] = 1  # fp16
        info[i + 1] = info[i] + param.size
        i += 1
    if i != num:
        raise()
    info[0] = num

    ptrs = cuda.to_gpu(ptrs)
    dtypes = cuda.to_gpu(dtypes)
    info = cuda.to_gpu(info)

    return _memcpy_scatter()(ptrs, dtypes, info, array, size=size)


</source>
</class>

<class classid="6" nclones="2" nlines="14" similarity="92">
<source file="systems/chainer-7.2.0/chainer/graph_optimizations/static_graph.py" startline="1424" endline="1439" pcid="713">
        return (xs,), inds, 0
    for x in xs:
        if isinstance(x, (list, tuple)):
            x, sub_inds, total = _flatten_args(x, )
            inds.append(('i', i, i+total, sub_inds))
            i += total
        else:
            x = [x]
            inds.append(('f', i))
            i += 1
        ys.extend([y for y in x])
    return tuple(ys), inds, i


# todo: this only outputs tuples of tuples. Any list in the original input
# will be converted to a tuple, changing the types of the input arguments
</source>
<source file="systems/chainer-7.2.0/chainer/graph_optimizations/static_graph.py" startline="1440" endline="1453" pcid="714">
# to the static chain.
def _unflatten_args(xs, inds):
    ys = []
    for ind in inds:
        code = ind[0]
        if code == 's':
            return xs[0]
        elif code == 'i':
            i_start, i_end, sub_inds = ind[1:]
            y = _unflatten_args(xs[i_start:i_end], sub_inds)
        else:
            i = ind[1]
            y = xs[i]
        ys.append(y)
</source>
</class>

<class classid="7" nclones="2" nlines="11" similarity="81">
<source file="systems/chainer-7.2.0/chainer/link_hooks/timer.py" startline="64" endline="75" pcid="815">
    def _preprocess(self):
        if self.xp is numpy:
            start = _get_time()
            self._running_stack.append(start)
        else:
            assert self.xp is cuda.cupy
            start = cuda.Event()
            stop = cuda.Event()
            start.record()
            self._running_stack.append((start, stop))
        self._depth += 1

</source>
<source file="systems/chainer-7.2.0/chainer/function_hooks/timer.py" startline="60" endline="70" pcid="3097">
    def _preprocess(self):
        if self.xp == numpy:
            start = _get_time()
            self._running_stack.append(start)
        else:
            start = cuda.Event()
            stop = cuda.Event()
            start.record()
            self._running_stack.append((start, stop))
        self._depth += 1

</source>
</class>

<class classid="8" nclones="2" nlines="17" similarity="82">
<source file="systems/chainer-7.2.0/chainer/link_hooks/timer.py" startline="80" endline="99" pcid="817">
    def _postprocess(self, link):
        if self.xp is numpy:
            start = self._running_stack.pop()
            stop = _get_time()
            elapsed_time = stop - start
        else:
            assert self.xp is cuda.cupy
            start, stop = self._running_stack.pop()
            stop.record()
            stop.synchronize()
            # Note that `get_elapsed_time` returns result in milliseconds
            elapsed_time = cuda.cupy.cuda.get_elapsed_time(
                start, stop) / 1000
        self.call_history.append((link.__class__.__name__, elapsed_time))

        assert self._depth > 0
        self._depth -= 1
        if self._depth == 0:
            self._total_time += elapsed_time

</source>
<source file="systems/chainer-7.2.0/chainer/function_hooks/timer.py" startline="79" endline="97" pcid="3100">
    def _postprocess(self, function):
        if self.xp == numpy:
            start = self._running_stack.pop()
            stop = _get_time()
            elapsed_time = stop - start
        else:
            start, stop = self._running_stack.pop()
            stop.record()
            stop.synchronize()
            # Note that `get_elapsed_time` returns result in milliseconds
            elapsed_time = cuda.cupy.cuda.get_elapsed_time(
                start, stop) / 1000
        self.call_history.append((function._impl_name, elapsed_time))

        assert self._depth > 0
        self._depth -= 1
        if self._depth == 0:
            self._total_time += elapsed_time

</source>
</class>

<class classid="9" nclones="2" nlines="27" similarity="92">
<source file="systems/chainer-7.2.0/chainer/link_hooks/timer.py" startline="135" endline="172" pcid="822">
    def print_report(self, unit='auto', file=sys.stdout):
        """Prints a summary report of time profiling in links.

        Args:
            unit (str): Supplementary units used for computational times.
                `sec`, `ms`, `us`, `ns`, `auto`(default) and `auto_foreach`
                are supported. If `auto`, units of times are aligned to the
                largest, and if `auto_foreach`, units of times are adjusted for
                each element.
        """
        entries = [['LinkName', 'ElapsedTime', 'Occurrence']]
        auto_foreach = (unit == 'auto_foreach')
        if unit == 'auto':
            max_time = max(
                record['elapsed_time'] for record in self.summary().values())
            factor, unit = self._choose_unit(max_time)
        elif not auto_foreach:
            factor = self.table[unit]
        for link_name, record in self.summary().items():
            second = record['elapsed_time']
            if auto_foreach:
                factor, unit = self._choose_unit(second)
            elapsed_time = '%3.2f%s' % (second * factor, unit)
            occurrence = str(record['occurrence'])
            entries.append([link_name, elapsed_time, occurrence])
        entry_widths = []
        entry_widths.append(max(len(f) for f, _, _ in entries))
        entry_widths.append(max(len(e) for _, e, _ in entries))
        entry_widths.append(max(len(o) for _, _, o in entries))
        template = '  '.join('{:>%d}' % w for w in entry_widths)
        for link_name, elapsed_time, occurrence in entries:
            line = template.format(link_name, elapsed_time, occurrence)
            file.write(line)
            file.write('\n')
        file.flush()

    # TODO(crcrpar): Support backward pre/post process.
    # See https://github.com/chainer/chainer/issues/5197
</source>
<source file="systems/chainer-7.2.0/chainer/function_hooks/timer.py" startline="137" endline="172" pcid="3106">
    def print_report(self, unit='auto', file=sys.stdout):
        """Prints a summary report of time profiling in functions.

        Args:
            unit (str): Supplementary units used for computational times.
                `sec`, `ms`, `us`, `ns`, `auto`(default) and `auto_foreach`
                are supported. If `auto`, units of times are aligned to the
                largest, and if `auto_foreach`, units of times are adjusted for
                each element.
        """
        entries = [['FunctionName', 'ElapsedTime', 'Occurrence']]
        auto_foreach = (unit == 'auto_foreach')
        if unit == 'auto':
            max_time = max(
                record['elapsed_time'] for record in self.summary().values())
            factor, unit = self._choose_unit(max_time)
        elif unit != 'auto_foreach':
            factor = self.table[unit]
        for function_name, record in self.summary().items():
            second = record['elapsed_time']
            if auto_foreach:
                factor, unit = self._choose_unit(second)
            elapsed_time = '%3.2f%s' % (second * factor, unit)
            occurrence = str(record['occurrence'])
            entries.append([function_name, elapsed_time, occurrence])
        entry_widths = []
        entry_widths.append(max(len(f) for f, _, _ in entries))
        entry_widths.append(max(len(e) for _, e, _ in entries))
        entry_widths.append(max(len(o) for _, _, o in entries))
        template = '  '.join('{:>%d}' % w for w in entry_widths)
        for function_name, elapsed_time, occurrence in entries:
            line = template.format(function_name, elapsed_time, occurrence)
            file.write(line)
            file.write('\n')
        if hasattr(file, 'flush'):
            file.flush()
</source>
</class>

<class classid="10" nclones="2" nlines="12" similarity="76">
<source file="systems/chainer-7.2.0/chainer/link_hook.py" startline="10" endline="21" pcid="873">
    def __init__(
            self,
            link: 'chainer.link.Link',
            forward_name: str,
            args: tp.Tuple[tp.Any, ...],
            kwargs: tp.Dict[str, tp.Any]
    ) -> None:
        self.link = link
        self.forward_name = forward_name
        self.args = args
        self.kwargs = kwargs

</source>
<source file="systems/chainer-7.2.0/chainer/link_hook.py" startline="31" endline="44" pcid="875">
    def __init__(
            self,
            link: 'chainer.link.Link',
            forward_name: str,
            args: tp.Tuple[tp.Any, ...],
            kwargs: tp.Dict[str, tp.Any],
            out: tp.Any
    ) -> None:
        self.link = link
        self.forward_name = forward_name
        self.args = args
        self.kwargs = kwargs
        self.out = out

</source>
</class>

<class classid="11" nclones="8" nlines="13" similarity="71">
<source file="systems/chainer-7.2.0/chainer/functions/normalization/group_normalization.py" startline="28" endline="41" pcid="889">
    def check_type_forward(self, in_types):
        type_check.expect(in_types.size() == 3)
        x_type, gamma_type, beta_type = in_types
        type_check.expect(
            x_type.dtype.kind == 'f',
            x_type.ndim >= 2,
            gamma_type.ndim == 1,
            beta_type.ndim == 1,
            gamma_type.dtype.kind == 'f',
            gamma_type.dtype == beta_type.dtype,
            x_type.shape[1] == gamma_type.shape[0],
            gamma_type.shape == beta_type.shape,
        )

</source>
<source file="systems/chainer-7.2.0/chainer/links/loss/hierarchical_softmax.py" startline="107" endline="122" pcid="3328">
    def check_type_forward(self, in_types):
        type_check.expect(in_types.size() == 3)
        x_type, t_type, w_type = in_types

        type_check.expect(
            x_type.dtype.kind == 'f',
            x_type.ndim == 2,
            t_type.dtype == numpy.int32,
            t_type.ndim == 1,
            x_type.shape[0] == t_type.shape[0],
            w_type.dtype == x_type.dtype,
            w_type.ndim == 2,
            w_type.shape[0] == self.parser_size,
            w_type.shape[1] == x_type.shape[1],
        )

</source>
<source file="systems/chainer-7.2.0/chainer/functions/pooling/roi_max_pooling_2d.py" startline="74" endline="88" pcid="1401">
    def check_type_forward(self, in_types):
        type_check.expect(in_types.size() == 3)

        x_type, roi_type, roi_index_type = in_types
        type_check.expect(
            x_type.dtype.kind == 'f',
            x_type.ndim == 4,
            x_type.dtype == roi_type.dtype,
            roi_type.ndim == 2,
            roi_type.shape[1] == 4,
            roi_index_type.dtype == numpy.int32,
            roi_index_type.ndim == 1,
            roi_type.shape[0] == roi_index_type.shape[0],
        )

</source>
<source file="systems/chainer-7.2.0/chainer/functions/pooling/roi_average_pooling_2d.py" startline="74" endline="88" pcid="1365">
    def check_type_forward(self, in_types):
        type_check.expect(in_types.size() == 3)

        x_type, roi_type, roi_index_type = in_types
        type_check.expect(
            x_type.dtype.kind == 'f',
            x_type.ndim == 4,
            x_type.dtype == roi_type.dtype,
            roi_type.ndim == 2,
            roi_type.shape[1] == 4,
            roi_index_type.dtype == numpy.int32,
            roi_index_type.ndim == 1,
            roi_type.shape[0] == roi_index_type.shape[0],
        )

</source>
<source file="systems/chainer-7.2.0/chainer/functions/pooling/roi_average_align_2d.py" startline="133" endline="147" pcid="1503">
    def check_type_forward(self, in_types):
        type_check.expect(in_types.size() == 3)

        x_type, roi_type, roi_index_type = in_types
        type_check.expect(
            x_type.dtype == numpy.float32,
            x_type.ndim == 4,
            roi_type.dtype == numpy.float32,
            roi_type.ndim == 2,
            roi_type.shape[1] == 4,
            roi_index_type.dtype == numpy.int32,
            roi_index_type.ndim == 1,
            roi_type.shape[0] == roi_index_type.shape[0],
        )

</source>
<source file="systems/chainer-7.2.0/chainer/functions/loss/negative_sampling.py" startline="43" endline="57" pcid="2197">
    def check_type_forward(self, in_types):
        type_check._argname(in_types, ('x', 't', 'W'))
        x_type, t_type, w_type = in_types

        type_check.expect(
            x_type.dtype.kind == 'f',
            x_type.ndim == 2,
            t_type.dtype == numpy.int32,
            t_type.ndim == 1,
            x_type.shape[0] == t_type.shape[0],
            w_type.dtype == x_type.dtype,
            w_type.ndim == 2,
        )

    # Avoid fp16 computation to keep the precision in reduction operations.
</source>
<source file="systems/chainer-7.2.0/chainer/functions/pooling/roi_max_align_2d.py" startline="70" endline="84" pcid="1393">
        self.sampling_ratio = sampling_ratio

    def check_type_forward(self, in_types):
        type_check.expect(in_types.size() == 3)

        x_type, roi_type, roi_index_type = in_types
        type_check.expect(
            x_type.dtype == numpy.float32,
            x_type.ndim == 4,
            roi_type.dtype == numpy.float32,
            roi_type.ndim == 2,
            roi_type.shape[1] == 4,
            roi_index_type.dtype == numpy.int32,
            roi_index_type.ndim == 1,
            roi_type.shape[0] == roi_index_type.shape[0],
</source>
<source file="systems/chainer-7.2.0/examples/sentiment/thin_stack.py" startline="10" endline="24" pcid="10375">
    def check_type_forward(self, in_types):
        type_check.expect(in_types.size() == 3)
        s_type, i_type, v_type = in_types
        type_check.expect(
            s_type.dtype.kind == 'f',
            i_type.dtype.kind == 'i',
            s_type.dtype == v_type.dtype,
            s_type.ndim == 3,
            i_type.ndim == 1,
            v_type.ndim == 2,
            s_type.shape[0] >= i_type.shape[0],
            i_type.shape[0] == v_type.shape[0],
            s_type.shape[2] == v_type.shape[1],
        )

</source>
</class>

<class classid="12" nclones="2" nlines="11" similarity="72">
<source file="systems/chainer-7.2.0/chainer/functions/normalization/decorrelated_batch_normalization.py" startline="70" endline="80" pcid="922">
    def check_type_forward(self, in_types):
        type_check.expect(in_types.size() == 1)
        x_type = in_types[0]
        type_check.expect(
            x_type.dtype.kind == 'f',
            x_type.shape[1] % self.groups == 0,
        )
        type_check.expect(
            x_type.ndim >= 2,
        )

</source>
<source file="systems/chainer-7.2.0/chainer/functions/normalization/decorrelated_batch_normalization.py" startline="206" endline="217" pcid="929">
    def check_type_forward(self, in_types):
        type_check.expect(in_types.size() == 3)
        x_type, mean_type, var_type = in_types
        type_check.expect(
            x_type.dtype.kind == 'f',
            mean_type.dtype == x_type.dtype,
            var_type.dtype == x_type.dtype,
        )
        type_check.expect(
            x_type.ndim >= 2,
        )

</source>
</class>

<class classid="13" nclones="2" nlines="19" similarity="71">
<source file="systems/chainer-7.2.0/chainer/functions/normalization/batch_normalization.py" startline="300" endline="328" pcid="964">
    def __init__(self, eps=2e-5, mean=None, var=None, decay=0.9, axis=None,
                 impl_selector=_impl_selector):
        self.running_mean = mean
        self.running_var = var

        # Note: cuDNN requires that eps be greater than or equals to
        # CUDNN_BN_MIN_EPSILON. Otherwise, an error will occur.
        # See CUDNN_BN_MIN_EPSILON value in cudnn.h to verify minimum allowable
        # value.
        self.eps = eps
        if chainer.should_use_cudnn('>=auto'):
            if eps < libcudnn.CUDNN_BN_MIN_EPSILON:
                raise RuntimeError(
                    'cuDNN does not allow an eps value '
                    'less than {}.'.format(libcudnn.CUDNN_BN_MIN_EPSILON))
        self.decay = decay
        if isinstance(axis, collections_abc.Sequence):
            for i in range(1, len(axis)):
                if axis[i - 1] >= axis[i]:
                    msg = 'numbers in axis must be sorted in ascending order'
                    raise RuntimeError(msg)
        elif isinstance(axis, six.integer_types):
            axis = axis,
        elif axis is not None:
            raise RuntimeError('axis must be int, tuple of int or None')
        self.axis = axis

        self._impl_selector = impl_selector

</source>
<source file="systems/chainer-7.2.0/chainer/functions/normalization/batch_normalization.py" startline="560" endline="581" pcid="974">
    def __init__(self, eps=2e-5, axis=None):
        # Note: cuDNN requires that eps be greater than or equals to
        # CUDNN_BN_MIN_EPSILON. Otherwise, an error will occur.
        # See CUDNN_BN_MIN_EPSILON value in cudnn.h to verify minimum allowable
        # value.
        self.eps = eps
        if chainer.should_use_cudnn('>=auto'):
            if eps < libcudnn.CUDNN_BN_MIN_EPSILON:
                raise RuntimeError(
                    'cuDNN does not allow an eps value '
                    'less than {}.'.format(libcudnn.CUDNN_BN_MIN_EPSILON))
        if isinstance(axis, collections_abc.Sequence):
            for i in range(1, len(axis)):
                if axis[i - 1] >= axis[i]:
                    msg = 'numbers in axis must be sorted in ascending order'
                    raise RuntimeError(msg)
        elif isinstance(axis, six.integer_types):
            axis = axis,
        elif axis is not None:
            raise RuntimeError('axis must be int, tuple of int or None')
        self.axis = axis

</source>
</class>

<class classid="14" nclones="2" nlines="25" similarity="81">
<source file="systems/chainer-7.2.0/chainer/functions/normalization/batch_normalization.py" startline="329" endline="352" pcid="965">
    def check_type_forward(self, in_types):
        type_check.expect(in_types.size() == 3)
        x_type, gamma_type, beta_type = in_types
        type_check.expect(
            x_type.dtype.kind == 'f',
            gamma_type.dtype.kind == 'f',
            gamma_type.dtype == beta_type.dtype,
            gamma_type.shape == beta_type.shape,
        )
        _x_ndim = type_check.eval(x_type.ndim)
        _gamma_ndim = type_check.eval(gamma_type.ndim)
        _axis = _compute_axis(_x_ndim, _gamma_ndim, self.axis)
        type_check.expect(
            x_type.ndim >= len(_axis),
        )
        _key_axis = _compute_key_axis(_x_ndim, _gamma_ndim, _axis)
        type_check.expect(
            gamma_type.ndim == len(_key_axis),
        )
        for i in range(len(_key_axis)):
            type_check.expect(
                x_type.shape[_key_axis[i]] == gamma_type.shape[i],
            )

</source>
<source file="systems/chainer-7.2.0/chainer/functions/normalization/batch_normalization.py" startline="582" endline="610" pcid="975">
    def check_type_forward(self, in_types):
        type_check.expect(in_types.size() == 5)
        x_type, gamma_type, beta_type, mean_type, var_type = in_types
        type_check.expect(
            x_type.dtype.kind == 'f',
            # TODO(beam2d): Check shape
            gamma_type.dtype.kind == 'f',
            beta_type.dtype == gamma_type.dtype,
            mean_type.dtype == gamma_type.dtype,
            var_type.dtype == gamma_type.dtype,
            beta_type.shape == gamma_type.shape,
            mean_type.shape == gamma_type.shape,
            var_type.shape == gamma_type.shape,
        )
        _x_ndim = type_check.eval(x_type.ndim)
        _gamma_ndim = type_check.eval(gamma_type.ndim)
        _axis = _compute_axis(_x_ndim, _gamma_ndim, self.axis)
        type_check.expect(
            x_type.ndim >= len(_axis),
        )
        _key_axis = _compute_key_axis(_x_ndim, _gamma_ndim, _axis)
        type_check.expect(
            gamma_type.ndim == len(_key_axis),
        )
        for i in range(len(_key_axis)):
            type_check.expect(
                x_type.shape[_key_axis[i]] == gamma_type.shape[i],
            )

</source>
</class>

<class classid="15" nclones="2" nlines="20" similarity="80">
<source file="systems/chainer-7.2.0/chainer/functions/connection/deconvolution_nd.py" startline="213" endline="235" pcid="1037">

    def backward(self, indexes, grad_outputs):
        x, W = self.get_retained_inputs()
        gy, = grad_outputs

        ret = []
        if 0 in indexes:
            gx = chainer.functions.convolution_nd(
                gy, W, stride=self.stride, pad=self.pad,
                cover_all=self.cover_all, dilate=self.dilate,
                groups=self.groups)
            ret.append(gx)
        if 1 in indexes:
            gW, = convolution_nd.ConvolutionNDGradW(self).apply((gy, x))
            ret.append(gW)
        if 2 in indexes:
            axis = (0,) + tuple(moves.range(2, gy.ndim))
            gb = chainer.functions.sum(gy, axis=axis)
            if gb.dtype != self.inputs[2].dtype:
                gb = chainer.functions.cast(gb, self.inputs[2].dtype)
            ret.append(gb)

        return ret
</source>
<source file="systems/chainer-7.2.0/chainer/functions/connection/convolution_nd.py" startline="192" endline="215" pcid="1081">
    def backward(self, indexes, grad_outputs):
        x, W = self.get_retained_inputs()
        gy, = grad_outputs

        ret = []
        if 0 in indexes:
            x_shape = x.shape[2:]
            gx = chainer.functions.deconvolution_nd(
                gy, W, stride=self.stride, pad=self.pad, outsize=x_shape,
                dilate=self.dilate, groups=self.groups)
            ret.append(gx)
        if 1 in indexes:
            gW, = ConvolutionNDGradW(self).apply((x, gy))
            ret.append(gW)
        if 2 in indexes:
            axis = (0,) + tuple(moves.range(2, gy.ndim))
            gb = chainer.functions.sum(gy, axis=axis)
            if gb.dtype != self.inputs[2].dtype:
                gb = chainer.functions.cast(gb, self.inputs[2].dtype)
            ret.append(gb)

        return ret


</source>
</class>

<class classid="16" nclones="4" nlines="19" similarity="73">
<source file="systems/chainer-7.2.0/chainer/functions/connection/convolution_2d.py" startline="62" endline="83" pcid="1048">
    def check_type_forward(self, in_types):
        n_in = in_types.size()
        type_check.expect(2 <= n_in, n_in <= 3)

        x_type = in_types[0]
        w_type = in_types[1]
        type_check.expect(
            x_type.dtype.kind == 'f',
            w_type.dtype.kind == 'f',
            x_type.ndim == 4,
            w_type.ndim == 4,
            x_type.shape[1] == w_type.shape[1] * self.groups,
        )

        if type_check.eval(n_in) == 3:
            b_type = in_types[2]
            type_check.expect(
                b_type.dtype == x_type.dtype,
                b_type.ndim == 1,
                b_type.shape[0] == w_type.shape[0],
            )

</source>
<source file="systems/chainer-7.2.0/chainer/functions/connection/convolution_nd.py" startline="28" endline="50" pcid="1073">
    def check_type_forward(self, in_types):
        n_in = in_types.size()
        type_check.expect(2 <= n_in, n_in <= 3)

        x_type = in_types[0]
        w_type = in_types[1]
        type_check.expect(
            x_type.dtype.kind == 'f',
            w_type.dtype.kind == 'f',
            x_type.ndim == self.ndim + 2,
            w_type.ndim == self.ndim + 2,
            # Need to consider the case that group count > 1.
            # x_type.shape[1] == w_type.shape[1],
        )

        if type_check.eval(n_in) == 3:
            b_type = in_types[2]
            type_check.expect(
                b_type.dtype.kind == 'f',
                b_type.ndim == 1,
                b_type.shape[0] == w_type.shape[0],
            )

</source>
<source file="systems/chainer-7.2.0/chainer/functions/connection/local_convolution_2d.py" startline="21" endline="40" pcid="1110">
    def check_type_forward(self, in_types):
        n_in = in_types.size()
        type_check.expect(2 <= n_in, n_in <= 3)
        x_type, w_type = in_types[:2]

        type_check.expect(
            x_type.dtype.kind == 'f',
            w_type.dtype.kind == 'f',
            x_type.ndim == 4,
            w_type.ndim == 6,
            x_type.shape[1] == w_type.shape[3],
        )
        if type_check.eval(n_in) == 3:
            b_type = in_types[2]
            type_check.expect(
                b_type.dtype == x_type.dtype,
                b_type.ndim == 3,
                b_type.shape == w_type.shape[:3]
            )

</source>
<source file="systems/chainer-7.2.0/chainer/functions/connection/linear.py" startline="18" endline="39" pcid="1093">
    def check_type_forward(self, in_types):
        n_in = in_types.size()
        type_check.expect(2 <= n_in, n_in <= 3)
        x_type, w_type = in_types[:2]
        type_check._argname((x_type, w_type), ('x', 'W'))

        type_check.expect(
            x_type.dtype.kind == 'f',
            w_type.dtype.kind == 'f',
            x_type.ndim == 2,
            w_type.ndim == 2,
            x_type.shape[1] == w_type.shape[1],
        )
        if type_check.eval(n_in) == 3:
            b_type = in_types[2]
            type_check._argname((b_type,), ('b',))
            type_check.expect(
                b_type.dtype == x_type.dtype,
                b_type.ndim == 1,
                b_type.shape[0] == w_type.shape[0],
            )

</source>
</class>

<class classid="17" nclones="2" nlines="20" similarity="70">
<source file="systems/chainer-7.2.0/chainer/functions/connection/convolution_2d.py" startline="176" endline="197" pcid="1055">
    def _forward_ideep(self, x, W, b):
        out_c, input_c, kh, kw = W.shape
        n, c, h, w = x.shape

        out_h, out_w = self._get_out_size(x.shape, W.shape)
        pd = (self.sy * (out_h - 1)
              + (kh + (kh - 1) * (self.dy - 1)) - h - self.ph)
        pr = (self.sx * (out_w - 1)
              + (kw + (kw - 1) * (self.dx - 1)) - w - self.pw)
        param = intel64.ideep.convolution2DParam(
            (n, out_c, out_h, out_w),
            self.dy, self.dx,
            self.sy, self.sx,
            self.ph, self.pw,
            pd, pr)
        y = intel64.ideep.convolution2D.Forward(
            intel64.ideep.array(x),
            intel64.ideep.array(W),
            intel64.ideep.array(b) if b is not None else None,
            param)
        return y,

</source>
<source file="systems/chainer-7.2.0/chainer/functions/connection/convolution_2d.py" startline="384" endline="405" pcid="1065">
    def _forward_ideep(self, x, gy):
        n, input_c, h, w = x.shape
        n, out_c, out_h, out_w = gy.shape
        pd = (self.sy * (out_h - 1)
              + (self.kh + (self.kh - 1) * (self.dy - 1))
              - h - self.ph)
        pr = (self.sx * (out_w - 1)
              + (self.kw + (self.kw - 1) * (self.dx - 1))
              - w - self.pw)

        param = intel64.ideep.convolution2DParam(
            (out_c, input_c, self.kh, self.kw),
            self.dy, self.dx,
            self.sy, self.sx,
            self.ph, self.pw,
            pd, pr)
        gW = intel64.ideep.convolution2D.BackwardWeights(
            intel64.ideep.array(x),
            intel64.ideep.array(gy),
            param)
        return gW,

</source>
</class>

<class classid="18" nclones="2" nlines="18" similarity="72">
<source file="systems/chainer-7.2.0/chainer/functions/connection/convolution_2d.py" startline="253" endline="283" pcid="1058">
    def _forward_grouped_convolution(self, x, W, b):
        # G: group count
        # N: batch size
        # kH, kW: kernel height, kernel width
        # iC, iH, iW: input channels, input height, input width
        # oC, oH, oW: output channels, output height, output width
        G = self.groups
        N, iC, iH, iW = x.shape
        oC, _, kH, kW = W.shape  # _ == iCg
        iCg = iC // G
        oCg = oC // G

        # (N, iC, kW, kW, oH, oW)
        x = conv.im2col(x, kH, kW, self.sy, self.sx, self.ph, self.pw,
                        cover_all=self.cover_all, dy=self.dy, dx=self.dx)
        oH, oW = x.shape[-2:]

        x = x.transpose(1, 2, 3, 0, 4, 5)  # (iC, kH, kW, N, oH, oW)
        x = x.reshape(G, iCg * kH * kW, N * oH * oW)

        W = W.reshape(G, oCg, iCg * kH * kW)

        # (G, oCg, N*oH*oW) = (G, oCg, iCg*kH*kW) @ (G, iCg*kH*kW, N*oH*oW)
        y = _matmul(W, x).astype(x.dtype, copy=False)
        y = y.reshape(oC, N, oH, oW)
        y = y.transpose(1, 0, 2, 3)  # (N, oC, oH, oW)
        if b is not None:
            y += b.reshape(1, b.size, 1, 1)

        return y,

</source>
<source file="systems/chainer-7.2.0/chainer/functions/connection/convolution_2d.py" startline="438" endline="468" pcid="1068">
    def _forward_grouped_convolution(self, x, gy):
        # G: group count
        # N: batch size
        # kH, kW: kernel height, kernel width
        # iC, iH, iW: input channels, input height, input width
        # oC, oH, oW: output channels, output height, output width
        G = self.groups
        N, iC, iH, iW = x.shape
        _, oC, oH, oW = gy.shape  # _ == N
        kH = self.kh
        kW = self.kw
        iCg = iC // G
        oCg = oC // G

        # (N, iC, kH, kW, oH, oW)
        x = conv.im2col(x, kH, kW, self.sy, self.sx, self.ph, self.pw,
                        cover_all=self.cover_all, dy=self.dy, dx=self.dx)

        x = x.transpose(1, 2, 3, 0, 4, 5)  # (iC, kH, kW, N, oH, oW)
        x = x.reshape(G, iCg * kH * kW, N * oH * oW)
        x = x.transpose(0, 2, 1)  # (G, N*oH*oW, iCg*kH*kW)

        gy = gy.transpose(1, 0, 2, 3)  # (oC, N, oH, oW)
        gy = gy.reshape(G, oCg, N * oH * oW)

        # (G, oCg, iCg*kH*kW) = (G, oCg, N*oH*oW) @ (G, N*oH*oW, iCg*kH*kW)
        gW = _matmul(gy, x).astype(self.W_dtype, copy=False)
        gW = gW.reshape(oC, iCg, kH, kW)

        return gW,

</source>
</class>

<class classid="19" nclones="2" nlines="11" similarity="90">
<source file="systems/chainer-7.2.0/chainer/functions/connection/convolution_nd.py" startline="69" endline="81" pcid="1075">
    def _use_cudnn(self, x, W):
        if cuda._cudnn_version < 6000 and any(d != 1 for d in self.dilate):
            # cuDNN < 6.0 does not support dilated convolutions
            return False
        if cuda._cudnn_version < 7000 and 1 < self.groups:
            # cuDNN < 7.0 does not support grouped convolutions
            return False
        return (
            chainer.should_use_cudnn('>=auto')
            and not self.cover_all
            and x.dtype == W.dtype
            and self.ndim > 1)

</source>
<source file="systems/chainer-7.2.0/chainer/functions/connection/convolution_nd.py" startline="229" endline="242" pcid="1083">
    def _use_cudnn(self, x, gy):
        if cuda._cudnn_version < 6000 and any(d != 1 for d in self.dilate):
            # cuDNN < 6.0 does not support dilated convolutions
            return False
        if cuda._cudnn_version < 7000 and 1 < self.groups:
            # cuDNN < 7.0 does not support grouped convolutions
            return False
        return (
            chainer.should_use_cudnn('>=auto')
            and not self.cover_all
            and x.dtype == self.W_dtype
            and gy.dtype == self.W_dtype
            and self.ndim > 1)

</source>
</class>

<class classid="20" nclones="4" nlines="11" similarity="72">
<source file="systems/chainer-7.2.0/chainer/functions/connection/convolution_nd.py" startline="218" endline="228" pcid="1082">
    def __init__(self, convnd):
        W_node = convnd.inputs[1]
        self.ndim = convnd.ndim
        self.ksize = W_node.shape[2:]
        self.stride = convnd.stride
        self.pad = convnd.pad
        self.cover_all = convnd.cover_all
        self.dilate = convnd.dilate
        self.groups = convnd.groups
        self.W_dtype = W_node.dtype

</source>
<source file="systems/chainer-7.2.0/chainer/functions/pooling/unpooling_2d.py" startline="91" endline="102" pcid="1424">
    def __init__(self, unpooling2d):
        self.kh = unpooling2d.kh
        self.kw = unpooling2d.kw
        self.sy = unpooling2d.sy
        self.sx = unpooling2d.sx
        self.ph = unpooling2d.ph
        self.pw = unpooling2d.pw
        self.outh = unpooling2d.outh
        self.outw = unpooling2d.outw
        self.cover_all = unpooling2d.cover_all
        self._use_int_scale_forward = unpooling2d._use_int_scale_forward

</source>
<source file="systems/chainer-7.2.0/chainer/functions/pooling/upsampling_2d.py" startline="109" endline="121" pcid="1386">
    def __init__(self, upsampling2d):
        self.kh = upsampling2d.kh
        self.kw = upsampling2d.kw
        self.sy = upsampling2d.sy
        self.sx = upsampling2d.sx
        self.ph = upsampling2d.ph
        self.pw = upsampling2d.pw
        self.outh = upsampling2d.outh
        self.outw = upsampling2d.outw
        self.cover_all = upsampling2d.cover_all
        self.indexes = upsampling2d.indexes
        self._in_dtype = upsampling2d._in_dtype

</source>
<source file="systems/chainer-7.2.0/chainer/functions/pooling/average_pooling_2d.py" startline="105" endline="117" pcid="1412">
    def __init__(self, apool2d):
        self.kh = apool2d.kh
        self.kw = apool2d.kw
        self.sy = apool2d.sy
        self.sx = apool2d.sx
        self.ph = apool2d.ph
        self.pw = apool2d.pw
        self._used_cudnn = apool2d._used_cudnn
        if not self._used_cudnn:
            self._in_shape = apool2d._in_shape
            self._in_dtype = apool2d._in_dtype
        self.apool2d = apool2d

</source>
</class>

<class classid="21" nclones="3" nlines="13" similarity="73">
<source file="systems/chainer-7.2.0/chainer/functions/connection/linear.py" startline="137" endline="154" pcid="1099">
    def backward(self, indexes, grad_outputs):
        x, W = self.get_retained_inputs()
        gy, = grad_outputs
        ret = []
        with chainer.using_config('use_ideep', self._config_use_ideep):
            if 0 in indexes:
                gx, = LinearGradData().apply((W, gy))
                ret.append(chainer.functions.cast(gx, x.dtype))
            if 1 in indexes:
                gW, = LinearGradWeight(W.dtype).apply((x, gy))
                ret.append(chainer.functions.cast(gW, W.dtype))
            if 2 in indexes:
                gb = chainer.functions.sum(gy, axis=0)
                ret.append(gb)

        return ret


</source>
<source file="systems/chainer-7.2.0/chainer/functions/connection/linear.py" startline="236" endline="250" pcid="1106">
    def backward(self, indexes, grad_outputs):
        x, gy = self.get_retained_inputs()
        ggW, = grad_outputs

        ret = []
        with chainer.using_config('use_ideep', self._config_use_ideep):
            if 0 in indexes:
                gx, = LinearGradData().apply((ggW, gy))
                ret.append(chainer.functions.cast(gx, x.dtype))
            if 1 in indexes:
                ggy = linear(x, ggW)
                ret.append(chainer.functions.cast(ggy, gy.dtype))
        return ret


</source>
<source file="systems/chainer-7.2.0/chainer/functions/connection/linear.py" startline="186" endline="200" pcid="1102">
    def backward(self, indexes, grad_outputs):
        W, gy = self.get_retained_inputs()
        ggx, = grad_outputs

        ret = []
        with chainer.using_config('use_ideep', self._config_use_ideep):
            if 0 in indexes:
                gw, = LinearGradWeight(W.dtype).apply((ggx, gy))
                ret.append(chainer.functions.cast(gw, W.dtype))
            if 1 in indexes:
                ggy = linear(ggx, W)
                ret.append(chainer.functions.cast(ggy, gy.dtype))
        return ret


</source>
</class>

<class classid="22" nclones="2" nlines="14" similarity="85">
<source file="systems/chainer-7.2.0/chainer/functions/connection/linear.py" startline="159" endline="177" pcid="1100">
    def forward(self, inputs):
        self._config_use_ideep = chainer.config.use_ideep
        if (intel64.should_use_ideep('>=auto')
                and intel64.inputs_all_ready(inputs)):
            # iDeep implementation
            return self._forward_ideep(inputs)

        # Generic implementation
        self.retain_inputs((0, 1))
        W, gy = inputs

        if (isinstance(gy, numpy.ndarray) and
                not (gy.flags.c_contiguous or gy.flags.f_contiguous) and
                1 in gy.shape):
            gy = numpy.ascontiguousarray(gy)

        gx = gy.dot(W).astype(gy.dtype, copy=False)
        return gx,

</source>
<source file="systems/chainer-7.2.0/chainer/functions/connection/linear.py" startline="208" endline="227" pcid="1104">
    def forward(self, inputs):
        self._config_use_ideep = chainer.config.use_ideep
        if (intel64.should_use_ideep('>=auto')
                and self._w_dtype == numpy.float32
                and intel64.inputs_all_ready(inputs)):
            # iDeep implementation
            return self._forward_ideep(inputs)

        # Generic implementation
        self.retain_inputs((0, 1))
        x, gy = inputs

        if (isinstance(gy, numpy.ndarray) and
                not (gy.flags.c_contiguous or gy.flags.f_contiguous) and
                1 in gy.shape):
            gy = numpy.ascontiguousarray(gy)

        gW = gy.T.dot(x).astype(self._w_dtype, copy=False)
        return gW,

</source>
</class>

<class classid="23" nclones="2" nlines="16" similarity="70">
<source file="systems/chainer-7.2.0/chainer/functions/array/spatial_transformer_grid.py" startline="57" endline="74" pcid="1135">

    def _forward(self, inputs):
        theta, = inputs
        H, W = self.output_shape
        B, _, _ = theta.shape
        xp = backend.get_array_module(theta)

        ys, xs = xp.meshgrid(
            xp.linspace(-1, 1, H, dtype=theta.dtype),
            xp.linspace(-1, 1, W, dtype=theta.dtype), indexing='ij',
            copy=False
        )

        coords = xp.concatenate(
            [xs[None], ys[None], xp.ones((1, H, W), dtype=theta.dtype)],
            axis=0)
        grid = theta.dot(coords.reshape(3, H * W)).reshape(B, 2, H, W)
        return grid,
</source>
<source file="systems/chainer-7.2.0/chainer/functions/array/spatial_transformer_grid.py" startline="92" endline="112" pcid="1138">

    def _backward(self, inputs, grad_outputs):
        theta, = inputs
        ggrid, = grad_outputs
        H, W = self.output_shape
        B, _, _ = theta.shape
        xp = backend.get_array_module(theta)

        ys, xs = xp.meshgrid(
            xp.linspace(-1, 1, H, dtype=theta.dtype),
            xp.linspace(-1, 1, W, dtype=theta.dtype), indexing='ij',
            copy=False
        )

        coords = xp.concatenate(
            [xs[None], ys[None], xp.ones((1, H, W), dtype=theta.dtype)],
            axis=0)
        coords_T = coords.reshape(3, H * W).transpose(1, 0)
        ggrid = ggrid.reshape(B, 2, H * W)
        gtheta = ggrid.dot(coords_T).reshape(B, 2, 3)
        return gtheta,
</source>
</class>

<class classid="24" nclones="3" nlines="16" similarity="75">
<source file="systems/chainer-7.2.0/chainer/functions/array/hstack.py" startline="14" endline="31" pcid="1160">
    def check_type_forward(self, in_types):
        type_check.expect(in_types.size() > 0)
        type_check._argname((in_types[0],), ('x0',))

        ndim = type_check.eval(in_types[0].ndim)
        for i in six.moves.range(1, type_check.eval(in_types.size())):
            type_check._argname((in_types[i],), ('x{}'.format(i),))
            type_check.expect(
                in_types[0].dtype == in_types[i].dtype,
                in_types[0].ndim == in_types[i].ndim,
            )
            if ndim <= 1:
                continue
            for d in six.moves.range(0, ndim):
                if d == 1:
                    continue
                type_check.expect(in_types[0].shape[d] == in_types[i].shape[d])

</source>
<source file="systems/chainer-7.2.0/chainer/functions/array/dstack.py" startline="14" endline="32" pcid="1179">
    def check_type_forward(self, in_types):
        type_check.expect(in_types.size() > 0)
        type_check._argname((in_types[0],), ('x0',))

        ndim = type_check.eval(in_types[0].ndim)
        for i in six.moves.range(1, type_check.eval(in_types.size())):
            type_check._argname((in_types[i],), ('x{}'.format(i),))
            type_check.expect(
                in_types[0].dtype == in_types[i].dtype,
                in_types[0].ndim == in_types[i].ndim,
            )
            if ndim <= 2:
                type_check.expect(in_types[0].shape == in_types[i].shape)
                continue
            for d in six.moves.range(0, ndim):
                if d == 2:
                    continue
                type_check.expect(in_types[0].shape[d] == in_types[i].shape[d])

</source>
<source file="systems/chainer-7.2.0/chainer/functions/array/vstack.py" startline="14" endline="28" pcid="1305">
    def check_type_forward(self, in_types):
        type_check.expect(in_types.size() > 0)

        ndim = type_check.eval(in_types[0].ndim)
        for i in six.moves.range(1, type_check.eval(in_types.size())):
            type_check.expect(
                in_types[0].dtype == in_types[i].dtype,
                in_types[0].ndim == in_types[i].ndim,
            )
            if ndim <= 1:
                type_check.expect(in_types[0].shape == in_types[i].shape)
                continue
            for d in six.moves.range(1, ndim):
                type_check.expect(in_types[0].shape[d] == in_types[i].shape[d])

</source>
</class>

<class classid="25" nclones="2" nlines="12" similarity="83">
<source file="systems/chainer-7.2.0/chainer/functions/array/dstack.py" startline="37" endline="52" pcid="1181">
    def backward(self, indexes, grad_outputs):
        gy, = grad_outputs
        ndim = len(self.inputs[0].shape)
        if len(self.inputs) == 1:
            if ndim <= 2:
                return gy.reshape(self.inputs[0].shape),
            return gy,

        if ndim <= 2:
            gxs = chainer.functions.split_axis(gy, len(self.inputs), axis=2)
            return [gx.reshape(self.inputs[0].shape) for gx in gxs]

        sizes = numpy.array([x.shape[2] for x in self.inputs[:-1]]).cumsum()
        return chainer.functions.split_axis(gy, sizes, axis=2)


</source>
<source file="systems/chainer-7.2.0/chainer/functions/array/vstack.py" startline="33" endline="48" pcid="1307">
    def backward(self, indexes, grad_outputs):
        gy, = grad_outputs
        ndim = len(self.inputs[0].shape)
        if len(self.inputs) == 1:
            if ndim <= 1:
                return gy.reshape(self.inputs[0].shape),
            return gy,

        if ndim <= 1:
            gxs = chainer.functions.split_axis(gy, len(self.inputs), 0)
            return [gx.reshape(self.inputs[0].shape) for gx in gxs]

        sizes = numpy.array([x.shape[0] for x in self.inputs[:-1]]).cumsum()
        return chainer.functions.split_axis(gy, sizes, 0)


</source>
</class>

<class classid="26" nclones="2" nlines="15" similarity="100">
<source file="systems/chainer-7.2.0/chainer/functions/array/scatter_add.py" startline="13" endline="30" pcid="1223">
    def __init__(self, slices):
        if isinstance(slices, list):
            if all([isinstance(s, int) for s in slices]):
                slices = slices,
            slices = tuple(slices)
        elif not isinstance(slices, tuple):
            slices = slices,

        if chainer.is_debug():
            n_ellipses = 0
            for s in slices:
                if s is Ellipsis:
                    n_ellipses += 1
            if n_ellipses > 1:
                raise ValueError('Only one Ellipsis is allowed')

        self.slices = slices

</source>
<source file="systems/chainer-7.2.0/chainer/functions/array/get_item.py" startline="19" endline="36" pcid="1316">

    def __init__(self, slices):
        if isinstance(slices, list):
            if all([isinstance(s, int) for s in slices]):
                slices = slices,
            slices = tuple(slices)
        elif not isinstance(slices, tuple):
            slices = slices,

        if chainer.is_debug():
            n_ellipses = 0
            for s in slices:
                if s is Ellipsis:
                    n_ellipses += 1
            if n_ellipses > 1:
                raise ValueError('Only one Ellipsis is allowed')

        self.slices = slices
</source>
</class>

<class classid="27" nclones="6" nlines="13" similarity="76">
<source file="systems/chainer-7.2.0/chainer/functions/array/squeeze.py" startline="22" endline="33" pcid="1299">
    def __init__(self, axis=None):

        if axis is None:
            self.axis = None
        elif isinstance(axis, six.integer_types):
            self.axis = (axis,)
        elif isinstance(axis, tuple) and all(
                isinstance(x, six.integer_types) for x in axis):
            self.axis = axis
        else:
            raise TypeError('axis must be None, int or tuple of ints')

</source>
<source file="systems/chainer-7.2.0/chainer/functions/math/logsumexp.py" startline="13" endline="26" pcid="1710">
    def __init__(self, axis=None):
        if axis is None:
            self.axis = None
        elif isinstance(axis, six.integer_types):
            self.axis = (axis,)
        elif isinstance(axis, tuple) and all(
                isinstance(a, six.integer_types) for a in axis):
            if len(set(axis)) != len(axis):
                raise ValueError('duplicate value in axis: ({})'.format(
                    ', '.join(map(str, axis))))
            self.axis = axis
        else:
            raise TypeError('None, int or tuple of int are required')

</source>
<source file="systems/chainer-7.2.0/chainer/functions/math/average.py" startline="15" endline="30" pcid="1636">
    def __init__(self, axis, keepdims):
        if axis is None:
            self.axis = None
        elif isinstance(axis, six.integer_types):
            self.axis = (axis,)
        elif isinstance(axis, tuple) and all(
                isinstance(a, six.integer_types) for a in axis):
            if len(set(axis)) != len(axis):
                raise ValueError('duplicate value in axis: ({})'.format(
                    ', '.join(map(str, axis))))
            self.axis = axis
        else:
            raise TypeError('None, int or tuple of int are required')

        self.keepdims = keepdims

</source>
<source file="systems/chainer-7.2.0/chainer/functions/math/prod.py" startline="15" endline="30" pcid="1734">
    def __init__(self, axis=None, keepdims=False):
        if axis is None:
            self.axis = None
        elif isinstance(axis, six.integer_types):
            self.axis = (axis,)
        elif isinstance(axis, tuple) and all(
                isinstance(a, six.integer_types) for a in axis):
            if len(set(axis)) != len(axis):
                raise ValueError('duplicate value in axis: ({})'.format(
                    ', '.join(map(str, axis))))
            self.axis = axis
        else:
            raise TypeError('None, int or tuple of int are required')

        self.keepdims = keepdims

</source>
<source file="systems/chainer-7.2.0/chainer/functions/math/minmax.py" startline="15" endline="29" pcid="1924">
    def __init__(self, axis=None, keepdims=False):
        self.keepdims = keepdims
        if axis is None:
            self.axis = None
        elif isinstance(axis, six.integer_types):
            self.axis = (axis,)
        elif isinstance(axis, tuple) and all(
                isinstance(a, six.integer_types) for a in axis):
            if len(set(axis)) != len(axis):
                raise ValueError('duplicate value in axis: ({})'.format(
                    ', '.join(map(str, axis))))
            self.axis = axis
        else:
            raise TypeError('None, int or tuple of int are required')

</source>
<source file="systems/chainer-7.2.0/chainer/functions/math/sum.py" startline="17" endline="32" pcid="1895">
    def __init__(self, axis=None, keepdims=False):
        if axis is None:
            self.axis = None
        elif isinstance(axis, six.integer_types):
            self.axis = (axis,)
        elif isinstance(axis, tuple) and all(
                isinstance(a, six.integer_types) for a in axis):
            if len(set(axis)) != len(axis):
                raise ValueError('duplicate value in axis: ({})'.format(
                    ', '.join(map(str, axis))))
            self.axis = axis
        else:
            raise TypeError('None, int or tuple of int are required')

        self.keepdims = keepdims

</source>
</class>

<class classid="28" nclones="2" nlines="19" similarity="100">
<source file="systems/chainer-7.2.0/chainer/functions/pooling/roi_average_pooling_2d.py" startline="53" endline="73" pcid="1364">
    def __init__(self, outsize, spatial_scale):
        outh, outw = _pair(outsize)
        if not (isinstance(outh, numbers.Integral) and outh > 0):
            raise TypeError(
                'outsize[0] must be positive integer: {}, {}'
                .format(type(outh), outh))
        if not (isinstance(outw, numbers.Integral) and outw > 0):
            raise TypeError(
                'outsize[1] must be positive integer: {}, {}'
                .format(type(outw), outw))
        if isinstance(spatial_scale, numbers.Integral):
            spatial_scale = float(spatial_scale)
        if not (isinstance(spatial_scale, numbers.Real) and
                spatial_scale > 0):
            raise TypeError(
                'spatial_scale must be a positive float number: {}, {}'
                .format(type(spatial_scale), spatial_scale))

        self.outh, self.outw = outh, outw
        self.spatial_scale = spatial_scale

</source>
<source file="systems/chainer-7.2.0/chainer/functions/pooling/roi_max_pooling_2d.py" startline="54" endline="73" pcid="1400">
    def __init__(self, outsize, spatial_scale):
        outh, outw = _pair(outsize)
        if not (isinstance(outh, numbers.Integral) and outh > 0):
            raise TypeError(
                'outsize[0] must be positive integer: {}, {}'
                .format(type(outh), outh))
        if not (isinstance(outw, numbers.Integral) and outw > 0):
            raise TypeError(
                'outsize[1] must be positive integer: {}, {}'
                .format(type(outw), outw))
        if isinstance(spatial_scale, numbers.Integral):
            spatial_scale = float(spatial_scale)
        if not (isinstance(spatial_scale, numbers.Real) and
                spatial_scale > 0):
            raise TypeError(
                'spatial_scale must be a positive float number: {}, {}'
                .format(type(spatial_scale), spatial_scale))
        self.outh, self.outw = outh, outw
        self.spatial_scale = spatial_scale

</source>
</class>

<class classid="29" nclones="4" nlines="35" similarity="71">
<source file="systems/chainer-7.2.0/chainer/functions/pooling/roi_average_pooling_2d.py" startline="89" endline="125" pcid="1366">
    def forward_cpu(self, inputs):
        self.retain_inputs((1, 2))
        self._bottom_data_shape = inputs[0].shape

        bottom_data, bottom_rois, bottom_roi_indices = inputs
        channels, height, width = bottom_data.shape[1:]
        n_rois = bottom_rois.shape[0]
        top_data = numpy.zeros((n_rois, channels, self.outh, self.outw),
                               dtype=bottom_data.dtype)

        for i_roi in six.moves.range(n_rois):
            idx = bottom_roi_indices[i_roi]
            ymin, xmin, ymax, xmax = bottom_rois[i_roi]
            ymin = int(round(ymin * self.spatial_scale))
            xmin = int(round(xmin * self.spatial_scale))
            ymax = int(round(ymax * self.spatial_scale))
            xmax = int(round(xmax * self.spatial_scale))
            roi_height = max(ymax - ymin, 1)
            roi_width = max(xmax - xmin, 1)
            strideh = 1. * roi_height / self.outh
            stridew = 1. * roi_width / self.outw

            for outh in six.moves.range(self.outh):
                sliceh, lenh = _roi_pooling_slice(
                    outh, strideh, height, ymin)
                if sliceh.stop <= sliceh.start:
                    continue
                for outw in six.moves.range(self.outw):
                    slicew, lenw = _roi_pooling_slice(
                        outw, stridew, width, xmin)
                    if slicew.stop <= slicew.start:
                        continue
                    roi_data = bottom_data[int(idx), :, sliceh, slicew]\
                        .reshape(channels, -1)
                    top_data[i_roi, :, outh, outw] =\
                        numpy.average(roi_data, axis=1)

</source>
<source file="systems/chainer-7.2.0/chainer/functions/pooling/roi_max_pooling_2d.py" startline="89" endline="134" pcid="1402">
    def forward_cpu(self, inputs):
        self.retain_inputs((1, 2))
        self._bottom_data_shape = inputs[0].shape

        bottom_data, bottom_rois, bottom_roi_indices = inputs
        channels, height, width = bottom_data.shape[1:]
        n_rois = bottom_rois.shape[0]
        top_data = numpy.full(
            (n_rois, channels, self.outh, self.outw),
            - numpy.inf, dtype=bottom_data.dtype)
        self.argmax_data = - numpy.ones(top_data.shape, numpy.int32)

        for i_roi in six.moves.range(n_rois):
            idx = bottom_roi_indices[i_roi]
            ymin, xmin, ymax, xmax = bottom_rois[i_roi]
            ymin = int(round(ymin * self.spatial_scale))
            xmin = int(round(xmin * self.spatial_scale))
            ymax = int(round(ymax * self.spatial_scale))
            xmax = int(round(xmax * self.spatial_scale))
            roi_height = max(ymax - ymin, 1)
            roi_width = max(xmax - xmin, 1)
            strideh = 1. * roi_height / self.outh
            stridew = 1. * roi_width / self.outw

            for outh in six.moves.range(self.outh):
                sliceh, lenh = _roi_pooling_slice(
                    outh, strideh, height, ymin)
                if sliceh.stop <= sliceh.start:
                    continue
                for outw in six.moves.range(self.outw):
                    slicew, lenw = _roi_pooling_slice(
                        outw, stridew, width, xmin)
                    if slicew.stop <= slicew.start:
                        continue
                    roi_data = bottom_data[int(idx), :, sliceh, slicew]\
                        .reshape(channels, -1)
                    top_data[i_roi, :, outh, outw] =\
                        numpy.max(roi_data, axis=1)

                    # get the max idx respect to feature_maps coordinates
                    max_idx_slice = numpy.unravel_index(
                        numpy.argmax(roi_data, axis=1), (lenh, lenw))
                    max_idx_slice_h = max_idx_slice[0] + sliceh.start
                    max_idx_slice_w = max_idx_slice[1] + slicew.start
                    max_idx_slice = max_idx_slice_h * width + max_idx_slice_w
                    self.argmax_data[i_roi, :, outh, outw] = max_idx_slice
</source>
<source file="systems/chainer-7.2.0/chainer/functions/pooling/roi_average_pooling_2d.py" startline="196" endline="229" pcid="1368">
        return top_data,

    def backward_cpu(self, inputs, gy):
        bottom_rois, bottom_roi_indices = inputs[1:]
        channels, height, width = self._bottom_data_shape[1:]
        n_rois = bottom_rois.shape[0]
        bottom_diff = numpy.zeros(self._bottom_data_shape, gy[0].dtype)

        for i_roi in six.moves.range(n_rois):
            idx = bottom_roi_indices[i_roi]
            ymin, xmin, ymax, xmax = bottom_rois[i_roi]
            ymin = int(round(ymin * self.spatial_scale))
            xmin = int(round(xmin * self.spatial_scale))
            ymax = int(round(ymax * self.spatial_scale))
            xmax = int(round(xmax * self.spatial_scale))
            roi_height = max(ymax - ymin, 1)
            roi_width = max(xmax - xmin, 1)
            strideh = 1. * roi_height / self.outh
            stridew = 1. * roi_width / self.outw

            for outh in six.moves.range(self.outh):
                sliceh, lenh = _roi_pooling_slice(
                    outh, strideh, height, ymin)
                if sliceh.stop <= sliceh.start:
                    continue
                for outw in six.moves.range(self.outw):
                    slicew, lenw = _roi_pooling_slice(
                        outw, stridew, width, xmin)
                    if slicew.stop <= slicew.start:
                        continue
                    diff_val = gy[0][i_roi, :, outh, outw]\
                        .reshape(channels, 1, 1)
                    diff_val = diff_val / lenh / lenw
                    bottom_diff[int(idx), :, sliceh, slicew] \
</source>
<source file="systems/chainer-7.2.0/chainer/functions/pooling/roi_pooling_2d.py" startline="69" endline="114" pcid="1447">
    def forward_cpu(self, inputs):
        self.retain_inputs((1,))
        self._bottom_data_shape = inputs[0].shape

        bottom_data, bottom_rois = inputs
        channels, height, width = bottom_data.shape[1:]
        n_rois = bottom_rois.shape[0]
        # `numpy.zeros` needs to be used because the arrays can be
        # returned without having some of its values updated.
        top_data = numpy.zeros((n_rois, channels, self.outh, self.outw),
                               dtype=bottom_data.dtype)
        self.argmax_data = numpy.zeros(top_data.shape, numpy.int32)

        for i_roi in six.moves.range(n_rois):
            idx, xmin, ymin, xmax, ymax = bottom_rois[i_roi]
            xmin = int(round(xmin * self.spatial_scale))
            xmax = int(round(xmax * self.spatial_scale))
            ymin = int(round(ymin * self.spatial_scale))
            ymax = int(round(ymax * self.spatial_scale))
            roi_width = max(xmax - xmin + 1, 1)
            roi_height = max(ymax - ymin + 1, 1)
            strideh = 1. * roi_height / self.outh
            stridew = 1. * roi_width / self.outw

            for outh in six.moves.range(self.outh):
                sliceh, lenh = _roi_pooling_slice(
                    outh, strideh, height, ymin)
                if sliceh.stop <= sliceh.start:
                    continue
                for outw in six.moves.range(self.outw):
                    slicew, lenw = _roi_pooling_slice(
                        outw, stridew, width, xmin)
                    if slicew.stop <= slicew.start:
                        continue
                    roi_data = bottom_data[int(idx), :, sliceh, slicew]\
                        .reshape(channels, -1)
                    top_data[i_roi, :, outh, outw] =\
                        numpy.max(roi_data, axis=1)

                    # get the max idx respect to feature_maps coordinates
                    max_idx_slice = numpy.unravel_index(
                        numpy.argmax(roi_data, axis=1), (lenh, lenw))
                    max_idx_slice_h = max_idx_slice[0] + sliceh.start
                    max_idx_slice_w = max_idx_slice[1] + slicew.start
                    max_idx_slice = max_idx_slice_h * width + max_idx_slice_w
                    self.argmax_data[i_roi, :, outh, outw] = max_idx_slice
</source>
</class>

<class classid="30" nclones="2" nlines="15" similarity="75">
<source file="systems/chainer-7.2.0/chainer/functions/pooling/roi_average_pooling_2d.py" startline="126" endline="195" pcid="1367">
        return top_data,

    def forward_gpu(self, inputs):
        self.retain_inputs((1, 2))
        self._bottom_data_shape = inputs[0].shape

        bottom_data, bottom_rois, bottom_roi_indices = inputs
        channels, height, width = bottom_data.shape[1:]
        n_rois = bottom_rois.shape[0]
        top_data = cuda.cupy.empty((n_rois, channels, self.outh,
                                    self.outw), dtype=bottom_data.dtype)
        cuda.elementwise(
            '''
            raw T bottom_data, raw T bottom_rois, raw int32 bottom_roi_indices,
            T spatial_scale, int32 channels, int32 height, int32 width,
            int32 pooled_height, int32 pooled_width
            ''',
            'T top_data',
            '''
            // pos in output filter
            int pw = i % pooled_width;
            int ph = (i / pooled_width) % pooled_height;
            int c = (i / pooled_width / pooled_height) % channels;
            int n = i / pooled_width / pooled_height / channels;

            int roi_batch_ind = bottom_roi_indices[n];
            int roi_start_h = round(bottom_rois[n * 4 + 0] * spatial_scale);
            int roi_start_w = round(bottom_rois[n * 4 + 1] * spatial_scale);
            int roi_end_h = round(bottom_rois[n * 4 + 2] * spatial_scale);
            int roi_end_w = round(bottom_rois[n * 4 + 3] * spatial_scale);

            // Force malformed ROIs to be 1x1
            int roi_height = max(roi_end_h - roi_start_h, 1);
            int roi_width = max(roi_end_w - roi_start_w, 1);
            T bin_size_h = static_cast<T>(roi_height)
                           / static_cast<T>(pooled_height);
            T bin_size_w = static_cast<T>(roi_width)
                           / static_cast<T>(pooled_width);

            int hstart = static_cast<int>(floor(static_cast<T>(ph)
                                          * bin_size_h));
            int wstart = static_cast<int>(floor(static_cast<T>(pw)
                                          * bin_size_w));
            int hend = static_cast<int>(ceil(static_cast<T>(ph + 1)
                                        * bin_size_h));
            int wend = static_cast<int>(ceil(static_cast<T>(pw + 1)
                                        * bin_size_w));

            // Add roi offsets and clip to input boundaries
            hstart = min(max(hstart + roi_start_h, 0), height);
            hend = min(max(hend + roi_start_h, 0), height);
            wstart = min(max(wstart + roi_start_w, 0), width);
            wend = min(max(wend + roi_start_w, 0), width);
            bool is_empty = (hend <= hstart) || (wend <= wstart);

            // Define an empty pooling region to be zero
            T sumval = 0.;
            T count = (hend - hstart) * (wend - wstart);
            int data_offset = (roi_batch_ind * channels + c) * height * width;
            for (int h = hstart; h < hend; ++h) {
                for (int w = wstart; w < wend; ++w) {
                    int bottom_index = h * width + w;
                    sumval += bottom_data[data_offset + bottom_index];
                }
            }
            top_data = is_empty ? 0. : sumval / count;
            ''', 'roi_average_pooling_2d_fwd'
        )(bottom_data, bottom_rois, bottom_roi_indices, self.spatial_scale,
          channels, height, width, self.outh, self.outw, top_data)

</source>
<source file="systems/chainer-7.2.0/chainer/functions/pooling/roi_max_pooling_2d.py" startline="135" endline="210" pcid="1403">
        return top_data,

    def forward_gpu(self, inputs):
        self.retain_inputs((1, 2))
        self._bottom_data_shape = inputs[0].shape

        bottom_data, bottom_rois, bottom_roi_indices = inputs
        channels, height, width = bottom_data.shape[1:]
        n_rois = bottom_rois.shape[0]
        top_data = cuda.cupy.empty((n_rois, channels, self.outh,
                                    self.outw), dtype=bottom_data.dtype)
        self.argmax_data = cuda.cupy.empty(top_data.shape, numpy.int32)
        cuda.elementwise(
            '''
            raw T bottom_data, raw T bottom_rois, raw int32 bottom_roi_indices,
            T spatial_scale, int32 channels, int32 height, int32 width,
            int32 pooled_height, int32 pooled_width
            ''',
            'T top_data, int32 argmax_data',
            '''
            // pos in output filter
            int pw = i % pooled_width;
            int ph = (i / pooled_width) % pooled_height;
            int c = (i / pooled_width / pooled_height) % channels;
            int n = i / pooled_width / pooled_height / channels;

            int roi_batch_ind = bottom_roi_indices[n];
            int roi_start_h = round(bottom_rois[n * 4 + 0] * spatial_scale);
            int roi_start_w = round(bottom_rois[n * 4 + 1] * spatial_scale);
            int roi_end_h = round(bottom_rois[n * 4 + 2] * spatial_scale);
            int roi_end_w = round(bottom_rois[n * 4 + 3] * spatial_scale);

            // Force malformed ROIs to be 1x1
            int roi_height = max(roi_end_h - roi_start_h , 1);
            int roi_width = max(roi_end_w - roi_start_w, 1);
            T bin_size_h = static_cast<T>(roi_height)
                           / static_cast<T>(pooled_height);
            T bin_size_w = static_cast<T>(roi_width)
                           / static_cast<T>(pooled_width);

            int hstart = static_cast<int>(floor(static_cast<T>(ph)
                                          * bin_size_h));
            int wstart = static_cast<int>(floor(static_cast<T>(pw)
                                          * bin_size_w));
            int hend = static_cast<int>(ceil(static_cast<T>(ph + 1)
                                        * bin_size_h));
            int wend = static_cast<int>(ceil(static_cast<T>(pw + 1)
                                        * bin_size_w));

            // Add roi offsets and clip to input boundaries
            hstart = min(max(hstart + roi_start_h, 0), height);
            hend = min(max(hend + roi_start_h, 0), height);
            wstart = min(max(wstart + roi_start_w, 0), width);
            wend = min(max(wend + roi_start_w, 0), width);

            // Define an empty pooling region to be zero
            T maxval = - (T) (1.0 / 0.0);
            // If nothing is pooled, argmax=-1 causes nothing to be backprop'd
            int maxidx = -1;
            int data_offset = (roi_batch_ind * channels + c) * height * width;
            for (int h = hstart; h < hend; ++h) {
                for (int w = wstart; w < wend; ++w) {
                    int bottom_index = h * width + w;
                    if (bottom_data[data_offset + bottom_index] > maxval) {
                        maxval = bottom_data[data_offset + bottom_index];
                        maxidx = bottom_index;
                    }
                }
            }
            top_data = maxval;
            argmax_data = maxidx;
            ''', 'roi_max_pooling_2d_fwd'
        )(bottom_data, bottom_rois, bottom_roi_indices,
          self.spatial_scale, channels, height, width,
          self.outh, self.outw, top_data, self.argmax_data)

</source>
</class>

<class classid="31" nclones="2" nlines="17" similarity="94">
<source file="systems/chainer-7.2.0/chainer/functions/pooling/upsampling_2d.py" startline="20" endline="39" pcid="1382">
    def check_type_forward(self, in_types):
        n_in = in_types.size()
        type_check.expect(n_in == 1)
        x_type = in_types[0]

        type_check.expect(
            x_type.dtype.kind == 'f',
            x_type.ndim == 4,
            x_type.shape == self.indexes.shape,
        )

        if self.outh is not None:
            expected_h = conv.get_conv_outsize(
                self.outh, self.kh, self.sy, self.ph, cover_all=self.cover_all)
            type_check.expect(x_type.shape[2] == expected_h)
        if self.outw is not None:
            expected_w = conv.get_conv_outsize(
                self.outw, self.kw, self.sx, self.pw, cover_all=self.cover_all)
            type_check.expect(x_type.shape[3] == expected_w)

</source>
<source file="systems/chainer-7.2.0/chainer/functions/pooling/unpooling_2d.py" startline="26" endline="44" pcid="1420">
    def check_type_forward(self, in_types):
        n_in = in_types.size()
        type_check.expect(n_in == 1)
        x_type = in_types[0]

        type_check.expect(
            x_type.dtype.kind == 'f',
            x_type.ndim == 4,
        )

        if self.outh is not None:
            expected_h = conv.get_conv_outsize(
                self.outh, self.kh, self.sy, self.ph, cover_all=self.cover_all)
            type_check.expect(x_type.shape[2] == expected_h)
        if self.outw is not None:
            expected_w = conv.get_conv_outsize(
                self.outw, self.kw, self.sx, self.pw, cover_all=self.cover_all)
            type_check.expect(x_type.shape[3] == expected_w)

</source>
</class>

<class classid="32" nclones="2" nlines="26" similarity="100">
<source file="systems/chainer-7.2.0/chainer/functions/pooling/roi_max_align_2d.py" startline="42" endline="69" pcid="1392">
    """ROI max align over a set of 2d planes."""

    def __init__(self, outsize, spatial_scale, sampling_ratio=None):
        outh, outw = _pair(outsize)
        if not (isinstance(outh, numbers.Integral) and outh > 0):
            raise TypeError(
                'outsize[0] must be positive integer: {}, {}'
                .format(type(outh), outh))
        if not (isinstance(outw, numbers.Integral) and outw > 0):
            raise TypeError(
                'outsize[1] must be positive integer: {}, {}'
                .format(type(outw), outw))
        if isinstance(spatial_scale, numbers.Integral):
            spatial_scale = float(spatial_scale)
        if not (isinstance(spatial_scale, numbers.Real) and
                spatial_scale > 0):
            raise TypeError(
                'spatial_scale must be a positive float number: {}, {}'
                .format(type(spatial_scale), spatial_scale))
        sampling_ratio = _pair(sampling_ratio)
        if not all((isinstance(s, numbers.Integral) and s >= 1) or
                   s is None for s in sampling_ratio):
            raise TypeError(
                'sampling_ratio must be integer >= 1 or a pair of it: {}'
                .format(sampling_ratio))

        self.outh, self.outw = outh, outw
        self.spatial_scale = spatial_scale
</source>
<source file="systems/chainer-7.2.0/chainer/functions/pooling/roi_average_align_2d.py" startline="105" endline="132" pcid="1502">
    def __init__(self, outsize, spatial_scale, sampling_ratio=None):
        outh, outw = _pair(outsize)
        if not (isinstance(outh, numbers.Integral) and outh > 0):
            raise TypeError(
                'outsize[0] must be positive integer: {}, {}'
                .format(type(outh), outh))
        if not (isinstance(outw, numbers.Integral) and outw > 0):
            raise TypeError(
                'outsize[1] must be positive integer: {}, {}'
                .format(type(outw), outw))
        if isinstance(spatial_scale, numbers.Integral):
            spatial_scale = float(spatial_scale)
        if not (isinstance(spatial_scale, numbers.Real) and
                spatial_scale > 0):
            raise TypeError(
                'spatial_scale must be a positive float number: {}, {}'
                .format(type(spatial_scale), spatial_scale))
        sampling_ratio = _pair(sampling_ratio)
        if not all((isinstance(s, numbers.Integral) and s >= 1) or
                   s is None for s in sampling_ratio):
            raise TypeError(
                'sampling_ratio must be integer >= 1 or a pair of it: {}'
                .format(sampling_ratio))

        self.outh, self.outw = outh, outw
        self.spatial_scale = spatial_scale
        self.sampling_ratio = sampling_ratio

</source>
</class>

<class classid="33" nclones="2" nlines="64" similarity="85">
<source file="systems/chainer-7.2.0/chainer/functions/pooling/roi_max_align_2d.py" startline="85" endline="169" pcid="1394">
        )

    def forward_cpu(self, inputs):
        self.retain_inputs((1, 2))
        self._bottom_data_shape = inputs[0].shape

        bottom_data, bottom_rois, bottom_roi_indices = inputs
        channels, height, width = bottom_data.shape[1:]
        n_rois = bottom_rois.shape[0]
        top_data = numpy.empty((n_rois, channels, self.outh,
                                self.outw), dtype=bottom_data.dtype)
        self.argmax_data = numpy.empty(top_data.shape, numpy.int32)

        pooled_width, pooled_height = self.outw, self.outh
        spatial_scale = self.spatial_scale

        for i in six.moves.range(top_data.size):
            pw = i % pooled_width
            ph = int(i / pooled_width) % pooled_height
            c = int(i / pooled_width / pooled_height) % channels
            n = int(i / pooled_width / pooled_height / channels)

            roi_batch_ind = bottom_roi_indices[n]
            roi_start_h = bottom_rois[n, 0] * spatial_scale
            roi_start_w = bottom_rois[n, 1] * spatial_scale
            roi_end_h = bottom_rois[n, 2] * spatial_scale
            roi_end_w = bottom_rois[n, 3] * spatial_scale

            roi_width = max(roi_end_w - roi_start_w, 1.)
            roi_height = max(roi_end_h - roi_start_h, 1.)
            bin_size_h = roi_height / pooled_height
            bin_size_w = roi_width / pooled_width

            if self.sampling_ratio[0] is None:
                roi_bin_grid_h = int(numpy.ceil(roi_height / pooled_height))
            else:
                roi_bin_grid_h = self.sampling_ratio[0]
            if self.sampling_ratio[1] is None:
                roi_bin_grid_w = int(numpy.ceil(roi_width / pooled_width))
            else:
                roi_bin_grid_w = self.sampling_ratio[1]

            max_val = - numpy.inf
            max_index = -1
            for iy in six.moves.range(roi_bin_grid_h):
                y = roi_start_h + ph * bin_size_h + \
                    (iy + .5) * bin_size_h / roi_bin_grid_h
                y, y_low, y_high = _get_bounds(y, height)
                if y is None or y_low is None or y_high is None:
                    continue
                for ix in six.moves.range(roi_bin_grid_w):
                    x = roi_start_w + pw * bin_size_w + \
                        (ix + .5) * bin_size_w / roi_bin_grid_w
                    x, x_low, x_high = _get_bounds(x, width)
                    if x is None or x_low is None or x_high is None:
                        continue
                    # bilinear interpolation {{

                    w1, w2, w3, w4 = _get_bilinear_interp_params(
                        y, x, y_low, x_low, y_high, x_high)

                    tmp_val = 0.
                    if w1 > 0 and y_low >= 0 and x_low >= 0:
                        v1 = bottom_data[roi_batch_ind, c, y_low, x_low]
                        tmp_val += w1 * v1

                    if w2 > 0 and y_low >= 0 and x_high <= width - 1:
                        v2 = bottom_data[roi_batch_ind, c, y_low, x_high]
                        tmp_val += w2 * v2

                    if w3 > 0 and y_high <= height - 1 and x_low >= 0:
                        v3 = bottom_data[roi_batch_ind, c, y_high, x_low]
                        tmp_val += w3 * v3

                    if w4 > 0 and y_high <= height - 1 and x_high <= width - 1:
                        v4 = bottom_data[roi_batch_ind, c, y_high, x_high]
                        tmp_val += w4 * v4

                    tmp_index = iy * roi_bin_grid_w + ix
                    if tmp_val > max_val:
                        max_val = tmp_val
                        max_index = tmp_index

                    # }}
            top_data[n, c, ph, pw] = max_val
</source>
<source file="systems/chainer-7.2.0/chainer/functions/pooling/roi_average_align_2d.py" startline="148" endline="227" pcid="1504">
    def forward_cpu(self, inputs):
        self.retain_inputs((1, 2))
        self._bottom_data_shape = inputs[0].shape

        bottom_data, bottom_rois, bottom_roi_indices = inputs
        channels, height, width = bottom_data.shape[1:]
        n_rois = bottom_rois.shape[0]
        top_data = numpy.empty((n_rois, channels, self.outh,
                                self.outw), dtype=bottom_data.dtype)

        pooled_width, pooled_height = self.outw, self.outh
        spatial_scale = self.spatial_scale

        for i in six.moves.range(top_data.size):
            pw = i % pooled_width
            ph = int(i / pooled_width) % pooled_height
            c = int(i / pooled_width / pooled_height) % channels
            n = int(i / pooled_width / pooled_height / channels)

            roi_batch_ind = int(bottom_roi_indices[n])
            roi_start_h = bottom_rois[n, 0] * spatial_scale
            roi_start_w = bottom_rois[n, 1] * spatial_scale
            roi_end_h = bottom_rois[n, 2] * spatial_scale
            roi_end_w = bottom_rois[n, 3] * spatial_scale

            roi_height = max(roi_end_h - roi_start_h, 1.)
            roi_width = max(roi_end_w - roi_start_w, 1.)
            bin_size_h = roi_height / pooled_height
            bin_size_w = roi_width / pooled_width

            if self.sampling_ratio[0] is None:
                roi_bin_grid_h = int(numpy.ceil(roi_height / pooled_height))
            else:
                roi_bin_grid_h = self.sampling_ratio[0]
            if self.sampling_ratio[1] is None:
                roi_bin_grid_w = int(numpy.ceil(roi_width / pooled_width))
            else:
                roi_bin_grid_w = self.sampling_ratio[1]

            count = roi_bin_grid_h * roi_bin_grid_w

            output_val = 0.
            for iy in six.moves.range(roi_bin_grid_h):
                y = roi_start_h + ph * bin_size_h + \
                    (iy + .5) * bin_size_h / roi_bin_grid_h
                y, y_low, y_high = _get_bounds(y, height)
                if y is None or y_low is None or y_high is None:
                    continue
                for ix in six.moves.range(roi_bin_grid_w):
                    x = roi_start_w + pw * bin_size_w + \
                        (ix + .5) * bin_size_w / roi_bin_grid_w
                    x, x_low, x_high = _get_bounds(x, width)
                    if x is None or x_low is None or x_high is None:
                        continue
                    # bilinear interpolation {{

                    w1, w2, w3, w4 = _get_bilinear_interp_params(
                        y, x, y_low, x_low, y_high, x_high)

                    if w1 > 0 and y_low >= 0 and x_low >= 0:
                        v1 = bottom_data[roi_batch_ind, c, y_low, x_low]
                        output_val += w1 * v1

                    if w2 > 0 and y_low >= 0 and x_high <= width - 1:
                        v2 = bottom_data[roi_batch_ind, c, y_low, x_high]
                        output_val += w2 * v2

                    if w3 > 0 and y_high <= height - 1 and x_low >= 0:
                        v3 = bottom_data[roi_batch_ind, c, y_high, x_low]
                        output_val += w3 * v3

                    if w4 > 0 and y_high <= height - 1 and x_high <= width - 1:
                        v4 = bottom_data[roi_batch_ind, c, y_high, x_high]
                        output_val += w4 * v4

                    # }}

            output_val /= count
            top_data[n, c, ph, pw] = output_val

</source>
</class>

<class classid="34" nclones="2" nlines="27" similarity="92">
<source file="systems/chainer-7.2.0/chainer/functions/pooling/roi_max_align_2d.py" startline="170" endline="295" pcid="1395">
            self.argmax_data[n, c, ph, pw] = max_index

        return top_data,

    def forward_gpu(self, inputs):
        self.retain_inputs((1, 2))
        self._bottom_data_shape = inputs[0].shape

        bottom_data, bottom_rois, bottom_roi_indices = inputs
        channels, height, width = bottom_data.shape[1:]
        n_rois = bottom_rois.shape[0]
        top_data = cuda.cupy.empty((n_rois, channels, self.outh,
                                    self.outw), dtype=bottom_data.dtype)
        self.argmax_data = cuda.cupy.empty(top_data.shape, numpy.int32)

        if self.sampling_ratio[0] is None:
            sampling_ratio_h = 0
        else:
            sampling_ratio_h = self.sampling_ratio[0]
        if self.sampling_ratio[1] is None:
            sampling_ratio_w = 0
        else:
            sampling_ratio_w = self.sampling_ratio[1]
        cuda.elementwise(
            '''
            raw T bottom_data, T spatial_scale, int32 channels,
            int32 height, int32 width, int32 pooled_height, int32 pooled_width,
            int32 sampling_ratio_h, int32 sampling_ratio_w,
            raw T bottom_rois, raw int32 bottom_roi_indices
            ''',
            'T top_data, int32 argmax_data',
            '''
            int pw = i % pooled_width;
            int ph = (i / pooled_width) % pooled_height;
            int c = (i / pooled_width / pooled_height) % channels;
            int n = i / pooled_width / pooled_height / channels;

            int roi_batch_ind = bottom_roi_indices[n];

            T roi_start_h = bottom_rois[n * 4 + 0] * spatial_scale;
            T roi_start_w = bottom_rois[n * 4 + 1] * spatial_scale;
            T roi_end_h = bottom_rois[n * 4 + 2] * spatial_scale;
            T roi_end_w = bottom_rois[n * 4 + 3] * spatial_scale;

            // Force malformed ROIs to be 1x1
            T roi_width = max(roi_end_w - roi_start_w, (T)1.);
            T roi_height = max(roi_end_h - roi_start_h, (T)1.);
            T bin_size_h = static_cast<T>(roi_height)
                            / static_cast<T>(pooled_height);
            T bin_size_w = static_cast<T>(roi_width)
                            / static_cast<T>(pooled_width);

            int bottom_data_offset =
                (roi_batch_ind * channels + c) * height * width;

            // We use roi_bin_grid to sample the grid and mimic integral
            int roi_bin_grid_h = (sampling_ratio_h > 0)
                ? sampling_ratio_h
                : ceil(roi_height / pooled_height);  // e.g. = 2
            int roi_bin_grid_w = (sampling_ratio_w > 0)
                ? sampling_ratio_w
                : ceil(roi_width / pooled_width);

            T max_val = - (T) (1.0 / 0.0);
            int max_index = -1;
            for (int iy = 0; iy < roi_bin_grid_h; iy++)  // e.g. iy = 0, 1
            {
                T y = roi_start_h + ph * bin_size_h +
                    static_cast<T>(iy + .5f) * bin_size_h /
                        static_cast<T>(roi_bin_grid_h);  // e.g. 0.5, 1.5
                int y_low, y_high;
                bool y_ret = get_bounds(y, height, y_low, y_high);
                if (!y_ret) continue;
                for (int ix = 0; ix < roi_bin_grid_w; ix++) {
                    T x = roi_start_w + pw * bin_size_w +
                        static_cast<T>(ix + .5f) * bin_size_w /
                            static_cast<T>(roi_bin_grid_w);

                    int x_low, x_high;
                    bool x_ret = get_bounds(x, width, x_low, x_high);
                    if (!x_ret) continue;
                    // bilinear_interpolation {{
                    T w1, w2, w3, w4;
                    get_bilinear_interp_params(
                        y, x, y_low, x_low, y_high, x_high, w1, w2, w3, w4);

                    T tmp_val = 0.;
                    if (w1 > 0 && y_low >= 0 && x_low >= 0) {
                        T v1 = bottom_data[
                            bottom_data_offset + y_low * width + x_low];
                        tmp_val += w1 * v1;
                    }
                    if (w2 > 0 && y_low >= 0 && x_high <= width - 1) {
                        T v2 = bottom_data[
                            bottom_data_offset + y_low * width + x_high];
                        tmp_val += w2 * v2;
                    }
                    if (w3 > 0 && y_high <= height - 1 && x_low >= 0) {
                        T v3 = bottom_data[
                            bottom_data_offset + y_high * width + x_low];
                        tmp_val += w3 * v3;
                    }
                    if (w4 > 0 && y_high <= height - 1 &&
                            x_high <= width - 1) {
                        T v4 = bottom_data[
                            bottom_data_offset + y_high * width + x_high];
                        tmp_val += w4 * v4;
                    }

                    int tmp_index = iy * roi_bin_grid_w + ix;
                    if (tmp_val > max_val) {
                        max_val = tmp_val;
                        max_index = tmp_index;
                    }

                    // }}
                }
            }

            top_data = max_val;
            argmax_data = max_index;
            ''',
            'roi_max_align_2d_fwd',
            preamble=_GET_BILINEAR_INTERP_KERNEL,
        )(bottom_data, self.spatial_scale, channels, height, width,
          self.outh, self.outw, sampling_ratio_h, sampling_ratio_w,
</source>
<source file="systems/chainer-7.2.0/chainer/functions/pooling/roi_average_align_2d.py" startline="228" endline="346" pcid="1505">
        return top_data,

    def forward_gpu(self, inputs):
        self.retain_inputs((1, 2))
        self._bottom_data_shape = inputs[0].shape

        bottom_data, bottom_rois, bottom_roi_indices = inputs
        channels, height, width = bottom_data.shape[1:]
        n_rois = bottom_rois.shape[0]
        top_data = cuda.cupy.empty((n_rois, channels, self.outh,
                                    self.outw), dtype=bottom_data.dtype)
        if self.sampling_ratio[0] is None:
            sampling_ratio_h = 0
        else:
            sampling_ratio_h = self.sampling_ratio[0]
        if self.sampling_ratio[1] is None:
            sampling_ratio_w = 0
        else:
            sampling_ratio_w = self.sampling_ratio[1]
        cuda.elementwise(
            '''
            raw T bottom_data, T spatial_scale, int32 channels,
            int32 height, int32 width, int32 pooled_height, int32 pooled_width,
            int32 sampling_ratio_h, int32 sampling_ratio_w,
            raw T bottom_rois, raw int32 bottom_roi_indices
            ''',
            'T top_data',
            '''
            int pw = i % pooled_width;
            int ph = (i / pooled_width) % pooled_height;
            int c = (i / pooled_width / pooled_height) % channels;
            int n = i / pooled_width / pooled_height / channels;

            int roi_batch_ind = bottom_roi_indices[n];

            T roi_start_h = bottom_rois[n * 4 + 0] * spatial_scale;
            T roi_start_w = bottom_rois[n * 4 + 1] * spatial_scale;
            T roi_end_h = bottom_rois[n * 4 + 2] * spatial_scale;
            T roi_end_w = bottom_rois[n * 4 + 3] * spatial_scale;

            // Force malformed ROIs to be 1x1
            T roi_height = max(roi_end_h - roi_start_h, (T)1.);
            T roi_width = max(roi_end_w - roi_start_w, (T)1.);
            T bin_size_h = static_cast<T>(roi_height)
                            / static_cast<T>(pooled_height);
            T bin_size_w = static_cast<T>(roi_width)
                            / static_cast<T>(pooled_width);

            int bottom_data_offset =
                (roi_batch_ind * channels + c) * height * width;

            // We use roi_bin_grid to sample the grid and mimic integral
            int roi_bin_grid_h = (sampling_ratio_h > 0)
                ? sampling_ratio_h
                : ceil(roi_height / pooled_height);  // e.g. = 2
            int roi_bin_grid_w = (sampling_ratio_w > 0)
                ? sampling_ratio_w
                : ceil(roi_width / pooled_width);

            // We do average (integral) pooling inside a bin
            T count = roi_bin_grid_h * roi_bin_grid_w;  // e.g. = 4

            T output_val = 0.;
            for (int iy = 0; iy < roi_bin_grid_h; iy++)  // e.g. iy = 0, 1
            {
                T y = roi_start_h + ph * bin_size_h +
                    static_cast<T>(iy + .5f) * bin_size_h /
                        static_cast<T>(roi_bin_grid_h);  // e.g. 0.5, 1.5
                int y_low, y_high;
                bool y_ret = get_bounds(y, height, y_low, y_high);
                if (!y_ret) continue;
                for (int ix = 0; ix < roi_bin_grid_w; ix++) {
                    T x = roi_start_w + pw * bin_size_w +
                        static_cast<T>(ix + .5f) * bin_size_w /
                            static_cast<T>(roi_bin_grid_w);

                    int x_low, x_high;
                    bool x_ret = get_bounds(x, width, x_low, x_high);
                    if (!x_ret) continue;
                    // bilinear_interpolation_gradient {{
                    T w1, w2, w3, w4;
                    get_bilinear_interp_params(
                        y, x, y_low, x_low, y_high, x_high, w1, w2, w3, w4);

                    if (w1 > 0 && y_low >= 0 && x_low >= 0) {
                        T v1 = bottom_data[
                            bottom_data_offset + y_low * width + x_low];
                        output_val += w1 * v1;
                    }
                    if (w2 > 0 && y_low >= 0 && x_high <= width - 1) {
                        T v2 = bottom_data[
                            bottom_data_offset + y_low * width + x_high];
                        output_val += w2 * v2;
                    }
                    if (w3 > 0 && y_high <= height - 1 && x_low >= 0) {
                        T v3 = bottom_data[
                            bottom_data_offset + y_high * width + x_low];
                        output_val += w3 * v3;
                    }
                    if (w4 > 0 && y_high <= height - 1 &&
                            x_high <= width - 1) {
                        T v4 = bottom_data[
                            bottom_data_offset + y_high * width + x_high];
                        output_val += w4 * v4;
                    }

                    // }}
                }
            }
            output_val /= count;

            top_data = output_val;
            ''',
            'roi_average_align_2d_fwd',
            preamble=_GET_BILINEAR_INTERP_KERNEL,
        )(bottom_data, self.spatial_scale, channels, height, width,
          self.outh, self.outw, sampling_ratio_h, sampling_ratio_w,
          bottom_rois, bottom_roi_indices, top_data)

</source>
</class>

<class classid="35" nclones="2" nlines="58" similarity="79">
<source file="systems/chainer-7.2.0/chainer/functions/pooling/roi_max_align_2d.py" startline="296" endline="372" pcid="1396">
          bottom_rois, bottom_roi_indices, top_data, self.argmax_data)

        return top_data,

    def backward_cpu(self, inputs, gy):
        bottom_rois, bottom_roi_indices = inputs[1:]
        channels, height, width = self._bottom_data_shape[1:]
        bottom_diff = numpy.zeros(self._bottom_data_shape, gy[0].dtype)

        spatial_scale = self.spatial_scale
        pooled_height = self.outh
        pooled_width = self.outw
        top_diff = gy[0]

        for i in six.moves.range(top_diff.size):
            pw = i % pooled_width
            ph = int(i / pooled_width) % pooled_height
            c = int(i / pooled_width / pooled_height) % channels
            n = int(i / pooled_width / pooled_height / channels)

            roi_batch_ind = bottom_roi_indices[n]
            roi_start_h = bottom_rois[n, 0] * spatial_scale
            roi_start_w = bottom_rois[n, 1] * spatial_scale
            roi_end_h = bottom_rois[n, 2] * spatial_scale
            roi_end_w = bottom_rois[n, 3] * spatial_scale

            roi_height = max(roi_end_h - roi_start_h, 1.)
            roi_width = max(roi_end_w - roi_start_w, 1.)
            bin_size_h = roi_height / pooled_height
            bin_size_w = roi_width / pooled_width

            top_diff_this_bin = top_diff[n, c, ph, pw]
            max_index = self.argmax_data[n, c, ph, pw]

            if max_index != -1:
                if self.sampling_ratio[0] is None:
                    roi_bin_grid_h = numpy.ceil(roi_height / pooled_height)
                else:
                    roi_bin_grid_h = self.sampling_ratio[0]
                if self.sampling_ratio[1] is None:
                    roi_bin_grid_w = numpy.ceil(roi_width / pooled_width)
                else:
                    roi_bin_grid_w = self.sampling_ratio[1]

                iy = int(max_index / roi_bin_grid_w)
                ix = max_index % roi_bin_grid_w

                y = roi_start_h + ph * bin_size_h + \
                    (iy + .5) * bin_size_h / roi_bin_grid_h
                x = roi_start_w + pw * bin_size_w + \
                    (ix + .5) * bin_size_w / roi_bin_grid_w

                # bilinear_interpolation_gradient {{

                y, y_low, y_high = _get_bounds(y, height)
                if y is None or y_low is None or y_high is None:
                    continue
                x, x_low, x_high = _get_bounds(x, width)
                if x is None or x_low is None or x_high is None:
                    continue
                w1, w2, w3, w4 = _get_bilinear_interp_params(
                    y, x, y_low, x_low, y_high, x_high)

                if w1 > 0 and y_low >= 0 and x_low >= 0:
                    g1 = top_diff_this_bin * w1
                    bottom_diff[roi_batch_ind, c, y_low, x_low] += g1

                if w2 > 0 and y_low >= 0 and x_high <= width - 1:
                    g2 = top_diff_this_bin * w2
                    bottom_diff[roi_batch_ind, c, y_low, x_high] += g2

                if w3 > 0 and y_high <= height - 1 and x_low >= 0:
                    g3 = top_diff_this_bin * w3
                    bottom_diff[roi_batch_ind, c, y_high, x_low] += g3

                if w4 > 0 and y_high <= height - 1 and x_high <= width - 1:
                    g4 = top_diff_this_bin * w4
</source>
<source file="systems/chainer-7.2.0/chainer/functions/pooling/roi_average_align_2d.py" startline="347" endline="421" pcid="1506">
        return top_data,

    def backward_cpu(self, inputs, gy):
        bottom_rois, bottom_roi_indices = inputs[1:]
        channels, height, width = self._bottom_data_shape[1:]
        bottom_diff = numpy.zeros(self._bottom_data_shape, gy[0].dtype)

        spatial_scale = self.spatial_scale
        pooled_height = self.outh
        pooled_width = self.outw
        top_diff = gy[0]

        for i in six.moves.range(top_diff.size):
            pw = i % pooled_width
            ph = int(i / pooled_width) % pooled_height
            c = int(i / pooled_width / pooled_height) % channels
            n = int(i / pooled_width / pooled_height / channels)

            roi_batch_ind = int(bottom_roi_indices[n])
            roi_start_h = bottom_rois[n, 0] * spatial_scale
            roi_start_w = bottom_rois[n, 1] * spatial_scale
            roi_end_h = bottom_rois[n, 2] * spatial_scale
            roi_end_w = bottom_rois[n, 3] * spatial_scale

            roi_height = max(roi_end_h - roi_start_h, 1.)
            roi_width = max(roi_end_w - roi_start_w, 1.)
            bin_size_h = roi_height / pooled_height
            bin_size_w = roi_width / pooled_width

            top_diff_this_bin = top_diff[n, c, ph, pw]

            if self.sampling_ratio[0] is None:
                roi_bin_grid_h = int(numpy.ceil(roi_height / pooled_height))
            else:
                roi_bin_grid_h = self.sampling_ratio[0]
            if self.sampling_ratio[1] is None:
                roi_bin_grid_w = int(numpy.ceil(roi_width / pooled_width))
            else:
                roi_bin_grid_w = self.sampling_ratio[1]

            count = roi_bin_grid_h * roi_bin_grid_w

            for iy in six.moves.range(roi_bin_grid_h):
                y = roi_start_h + ph * bin_size_h + \
                    (iy + .5) * bin_size_h / roi_bin_grid_h
                y, y_low, y_high = _get_bounds(y, height)
                if y is None or y_low is None or y_high is None:
                    continue
                for ix in six.moves.range(roi_bin_grid_w):
                    x = roi_start_w + pw * bin_size_w + \
                        (ix + .5) * bin_size_w / roi_bin_grid_w
                    x, x_low, x_high = _get_bounds(x, width)
                    if x is None or x_low is None or x_high is None:
                        continue
                    # bilinear_interpolation_gradient {{

                    w1, w2, w3, w4 = _get_bilinear_interp_params(
                        y, x, y_low, x_low, y_high, x_high)

                    if w1 > 0 and y_low >= 0 and x_low >= 0:
                        g1 = top_diff_this_bin * w1 / count
                        bottom_diff[roi_batch_ind, c, y_low, x_low] += g1

                    if w2 > 0 and y_low >= 0 and x_high <= width - 1:
                        g2 = top_diff_this_bin * w2 / count
                        bottom_diff[roi_batch_ind, c, y_low, x_high] += g2

                    if w3 > 0 and y_high <= height - 1 and x_low >= 0:
                        g3 = top_diff_this_bin * w3 / count
                        bottom_diff[roi_batch_ind, c, y_high, x_low] += g3

                    if w4 > 0 and y_high <= height - 1 and x_high <= width - 1:
                        g4 = top_diff_this_bin * w4 / count
                        bottom_diff[roi_batch_ind, c, y_high, x_high] += g4

</source>
</class>

<class classid="36" nclones="2" nlines="24" similarity="83">
<source file="systems/chainer-7.2.0/chainer/functions/pooling/roi_max_align_2d.py" startline="373" endline="488" pcid="1397">
                    bottom_diff[roi_batch_ind, c, y_high, x_high] += g4

                # }}

        return bottom_diff, None, None

    def backward_gpu(self, inputs, gy):
        utils.nondeterministic('atomicAdd')
        bottom_rois, bottom_roi_indices = inputs[1:]
        channels, height, width = self._bottom_data_shape[1:]
        bottom_diff = cuda.cupy.zeros(self._bottom_data_shape, gy[0].dtype)

        if self.sampling_ratio[0] is None:
            sampling_ratio_h = 0
        else:
            sampling_ratio_h = self.sampling_ratio[0]
        if self.sampling_ratio[1] is None:
            sampling_ratio_w = 0
        else:
            sampling_ratio_w = self.sampling_ratio[1]

        cuda.elementwise(
            '''
            raw T top_diff, T spatial_scale,
            int32 channels, int32 height, int32 width,
            int32 pooled_height, int32 pooled_width,
            int32 sampling_ratio_h, int32 sampling_ratio_w,
            raw T bottom_rois, raw int32 bottom_roi_indices
            ''',
            'raw T bottom_diff, raw int32 argmax_data',
            '''
            // (n, c, h, w) coords in bottom data
            int pw = i % pooled_width;
            int ph = (i / pooled_width) % pooled_height;
            int c = (i / pooled_width / pooled_height) % channels;
            int n = i / pooled_width / pooled_height / channels;

            // Do not using rounding; this implementation detail is critical
            int roi_batch_ind = bottom_roi_indices[n];
            T roi_start_h = bottom_rois[n * 4 + 0] * spatial_scale;
            T roi_start_w = bottom_rois[n * 4 + 1] * spatial_scale;
            T roi_end_h = bottom_rois[n * 4 + 2] * spatial_scale;
            T roi_end_w = bottom_rois[n * 4 + 3] * spatial_scale;

            // Force malformed ROIs to be 1x1
            T roi_width = max(roi_end_w - roi_start_w, (T)1.);
            T roi_height = max(roi_end_h - roi_start_h, (T)1.);
            T bin_size_h = static_cast<T>(roi_height) /
                static_cast<T>(pooled_height);
            T bin_size_w = static_cast<T>(roi_width) /
                static_cast<T>(pooled_width);

            int bottom_diff_offset =
                (roi_batch_ind * channels + c) * height * width;

            int top_offset = (n * channels + c) * pooled_height * pooled_width;
            int max_index = argmax_data[top_offset + ph * pooled_width + pw];

            if (max_index != -1) {
                T top_diff_this_bin =
                    top_diff[top_offset + ph * pooled_width + pw];

                // We use roi_bin_grid to sample the grid and mimic integral
                int roi_bin_grid_h = (sampling_ratio_h > 0)
                    ? sampling_ratio_h
                    : ceil(roi_height / pooled_height); // e.g. = 2
                int roi_bin_grid_w = (sampling_ratio_w > 0)
                    ? sampling_ratio_w
                    : ceil(roi_width / pooled_width);

                int iy = max_index / roi_bin_grid_w;
                int ix = max_index % roi_bin_grid_w;

                T y = roi_start_h + ph * bin_size_h +
                    static_cast<T>(iy + .5f) * bin_size_h /
                        static_cast<T>(roi_bin_grid_h);  // e.g. 0.5, 1.5
                T x = roi_start_w + pw * bin_size_w +
                    static_cast<T>(ix + .5f) * bin_size_w /
                        static_cast<T>(roi_bin_grid_w);

                // bilinear_interpolation_gradient {{
                int y_low, x_low, y_high, x_high;
                T w1, w2, w3, w4;
                bool y_ret = get_bounds(y, height, y_low, y_high);
                bool x_ret = get_bounds(x, width, x_low, x_high);
                if (!x_ret || !y_ret) continue;
                get_bilinear_interp_params(
                    y, x, y_low, x_low, y_high, x_high, w1, w2, w3, w4);

                if (w1 > 0 && y_low >= 0 && x_low >= 0) {
                    T g1 = top_diff_this_bin * w1;
                    atomicAdd(&bottom_diff[
                        bottom_diff_offset + y_low * width + x_low], g1);
                }
                if (w2 > 0 && y_low >= 0 && x_high <= width - 1) {
                    T g2 = top_diff_this_bin * w2;
                    atomicAdd(&bottom_diff[
                        bottom_diff_offset + y_low * width + x_high], g2);
                }
                if (w3 > 0 && y_high <= height - 1 && x_low >= 0) {
                    T g3 = top_diff_this_bin * w3;
                    atomicAdd(&bottom_diff[
                        bottom_diff_offset + y_high * width + x_low], g3);
                }
                if (w4 > 0 && y_high <= height - 1 && x_high <= width - 1) {
                    T g4 = top_diff_this_bin * w4;
                    atomicAdd(&bottom_diff[
                        bottom_diff_offset + y_high * width + x_high], g4);
                }
            }
            // }}
            ''',
            'roi_max_align_2d_bwd',
            preamble=_GET_BILINEAR_INTERP_KERNEL,
        )(gy[0], self.spatial_scale, channels, height, width,
          self.outh, self.outw, sampling_ratio_h, sampling_ratio_w,
</source>
<source file="systems/chainer-7.2.0/chainer/functions/pooling/roi_average_align_2d.py" startline="422" endline="541" pcid="1507">
                    # }}

        return bottom_diff, None, None

    def backward_gpu(self, inputs, gy):
        utils.nondeterministic('atomicAdd')
        bottom_rois, bottom_roi_indices = inputs[1:]
        channels, height, width = self._bottom_data_shape[1:]
        bottom_diff = cuda.cupy.zeros(self._bottom_data_shape, gy[0].dtype)

        if self.sampling_ratio[0] is None:
            sampling_ratio_h = 0
        else:
            sampling_ratio_h = self.sampling_ratio[0]
        if self.sampling_ratio[1] is None:
            sampling_ratio_w = 0
        else:
            sampling_ratio_w = self.sampling_ratio[1]
        cuda.elementwise(
            '''
            raw T top_diff,
            int32 num_rois, T spatial_scale,
            int32 channels, int32 height, int32 width,
            int32 pooled_height, int32 pooled_width,
            int32 sampling_ratio_h, int32 sampling_ratio_w,
            raw T bottom_rois, raw int32 bottom_roi_indices
            ''',
            'raw T bottom_diff',
            '''
            // (n, c, h, w) coords in bottom data
            int pw = i % pooled_width;
            int ph = (i / pooled_width) % pooled_height;
            int c = (i / pooled_width / pooled_height) % channels;
            int n = i / pooled_width / pooled_height / channels;

            // Do not using rounding; this implementation detail is critical
            int roi_batch_ind = bottom_roi_indices[n];
            T roi_start_h = bottom_rois[n * 4 + 0] * spatial_scale;
            T roi_start_w = bottom_rois[n * 4 + 1] * spatial_scale;
            T roi_end_h = bottom_rois[n * 4 + 2] * spatial_scale;
            T roi_end_w = bottom_rois[n * 4 + 3] * spatial_scale;

            // Force malformed ROIs to be 1x1
            T roi_width = max(roi_end_w - roi_start_w, (T)1.);
            T roi_height = max(roi_end_h - roi_start_h, (T)1.);
            T bin_size_h = static_cast<T>(roi_height) /
                static_cast<T>(pooled_height);
            T bin_size_w = static_cast<T>(roi_width) /
                static_cast<T>(pooled_width);

            int bottom_diff_offset =
                (roi_batch_ind * channels + c) * height * width;

            int top_offset = (n * channels + c) * pooled_height * pooled_width;
            T top_diff_this_bin =
                top_diff[top_offset + ph * pooled_width + pw];

            // We use roi_bin_grid to sample the grid and mimic integral
            int roi_bin_grid_h = (sampling_ratio_h > 0)
                ? sampling_ratio_h
                : ceil(roi_height / pooled_height); // e.g. = 2
            int roi_bin_grid_w = (sampling_ratio_w > 0)
                ? sampling_ratio_w
                : ceil(roi_width / pooled_width);

            // We do average (integral) pooling inside a bin
            T count = roi_bin_grid_h * roi_bin_grid_w;  // e.g. = 4

            for (int iy = 0; iy < roi_bin_grid_h; iy++) {
                T y = roi_start_h + ph * bin_size_h +
                    static_cast<T>(iy + .5f) * bin_size_h /
                        static_cast<T>(roi_bin_grid_h);  // e.g. 0.5, 1.5
                int y_low, y_high;
                bool y_ret = get_bounds(y, height, y_low, y_high);
                if (!y_ret) continue;
                for (int ix = 0; ix < roi_bin_grid_w; ix++) {
                    T x = roi_start_w + pw * bin_size_w +
                        static_cast<T>(ix + .5f) * bin_size_w /
                            static_cast<T>(roi_bin_grid_w);

                    int x_low, x_high;
                    bool x_ret = get_bounds(x, width, x_low, x_high);
                    if (!x_ret) continue;
                    // bilinear_interpolation_gradient {{
                    T w1, w2, w3, w4;
                    get_bilinear_interp_params(
                        y, x, y_low, x_low, y_high, x_high, w1, w2, w3, w4);

                    if (w1 > 0 && y_low >= 0 && x_low >= 0) {
                        T g1 = top_diff_this_bin * w1 / count;
                        atomicAdd(&bottom_diff[
                            bottom_diff_offset + y_low * width + x_low], g1);
                    }
                    if (w2 > 0 && y_low >= 0 && x_high <= width - 1) {
                        T g2 = top_diff_this_bin * w2 / count;
                        atomicAdd(&bottom_diff[
                            bottom_diff_offset + y_low * width + x_high], g2);
                    }
                    if (w3 > 0 && y_high <= height - 1 && x_low >= 0) {
                        T g3 = top_diff_this_bin * w3 / count;
                        atomicAdd(&bottom_diff[
                            bottom_diff_offset + y_high * width + x_low], g3);
                    }
                    if (w4 > 0 && y_high <= height - 1 &&
                            x_high <= width - 1) {
                        T g4 = top_diff_this_bin * w4 / count;
                        atomicAdd(&bottom_diff[
                            bottom_diff_offset + y_high * width + x_high], g4);
                    }

                    // }}
                }
            }
            ''',
            'roi_average_align_2d_bwd',
            preamble=_GET_BILINEAR_INTERP_KERNEL,
        )(gy[0], bottom_rois.shape[0],
          self.spatial_scale, channels, height, width, self.outh, self.outw,
          sampling_ratio_h, sampling_ratio_w, bottom_rois, bottom_roi_indices,
          bottom_diff, size=gy[0].size)
</source>
</class>

<class classid="37" nclones="3" nlines="10" similarity="70">
<source file="systems/chainer-7.2.0/chainer/functions/pooling/roi_pooling_2d.py" startline="57" endline="68" pcid="1446">
    def check_type_forward(self, in_types):
        type_check.expect(in_types.size() == 2)

        x_type, roi_type = in_types
        type_check.expect(
            x_type.dtype.kind == 'f',
            x_type.ndim == 4,
            x_type.dtype == roi_type.dtype,
            roi_type.ndim == 2,
            roi_type.shape[1] == 5,
        )

</source>
<source file="systems/chainer-7.2.0/examples/sentiment/thin_stack.py" startline="46" endline="56" pcid="10379">
    def check_type_forward(self, in_types):
        type_check.expect(in_types.size() == 2)
        s_type, i_type = in_types
        type_check.expect(
            s_type.dtype.kind == 'f',
            i_type.dtype.kind == 'i',
            s_type.ndim == 3,
            i_type.ndim == 1,
            s_type.shape[0] >= i_type.shape[0],
        )

</source>
<source file="systems/chainer-7.2.0/chainer/functions/loss/hinge.py" startline="33" endline="44" pcid="2145">
    def check_type_forward(self, in_types):
        type_check._argname(in_types, ('x', 't'))

        x_type, t_type = in_types
        type_check.expect(
            x_type.dtype.kind == 'f',
            t_type.dtype.kind == 'i',
            x_type.ndim == 2,
            t_type.ndim == 1,
            x_type.shape[0] == t_type.shape[0],
        )

</source>
</class>

<class classid="38" nclones="2" nlines="25" similarity="81">
<source file="systems/chainer-7.2.0/chainer/functions/pooling/max_pooling_nd.py" startline="309" endline="345" pcid="1487">
        return col.reshape((n, c) + outs),

    def forward_gpu(self, inputs):
        func = self.func

        if func.is_cudnn_used:
            x = func.get_retained_inputs()[0].array
            return self._forward_gpu_compute_indexes_again((x, inputs[0]))

        ndim = func.ndim
        ksize = func.ksize
        stride = func.stride
        pad = func.pad
        cover_all = func.cover_all
        indexes = backend.from_chx(func.indexes)

        x, = inputs
        in_shape = x.shape
        in_dtype = x.dtype

        n, c = in_shape[:2]
        dims = in_shape[2:]

        ys = tuple(conv_nd.get_conv_outsize(d, k, s, p, cover_all)
                   for (d, k, s, p) in six.moves.zip(dims, ksize, stride, pad))
        # (n, c, y_1, y_2, ..., y_N)
        y_shape = (n, c) + ys
        y = cuda.cupy.empty(y_shape, dtype=x.dtype)

        cls = max_pooling_nd_kernel.MaxPoolingNDKernelForwardWithIndexes
        in_params, out_params, operation, name = cls.generate(ndim)
        cuda.elementwise(in_params, out_params, operation, name)(
            x.reduced_view(),
            *(dims + ys + ksize + stride + pad + (indexes.reduced_view(), y)))

        self._in_shape = in_shape
        self._in_dtype = in_dtype
</source>
<source file="systems/chainer-7.2.0/chainer/functions/pooling/max_pooling_nd.py" startline="346" endline="376" pcid="1488">
        return y,

    def _forward_gpu_compute_indexes_again(self, inputs):
        func = self.func
        ndim = func.ndim
        ksize = func.ksize
        stride = func.stride
        pad = func.pad
        cover_all = func.cover_all

        x, ggx = inputs
        in_shape = x.shape
        in_dtype = x.dtype

        n, c = in_shape[:2]
        dims = in_shape[2:]

        ys = tuple(conv_nd.get_conv_outsize(d, k, s, p, cover_all)
                   for (d, k, s, p) in six.moves.zip(dims, ksize, stride, pad))
        # (n, c, y_1, y_2, ..., y_N)
        y_shape = (n, c) + ys
        y = cuda.cupy.empty(y_shape, dtype=x.dtype)

        cls = max_pooling_nd_kernel.MaxPoolingNDKernelForwardWithIndexes1
        in_params, out_params, operation, name = cls.generate(ndim)
        cuda.elementwise(in_params, out_params, operation, name)(
            x.reduced_view(),
            *(dims + ys + ksize + stride + pad + (ggx.reduced_view(), y)))

        self._in_shape = in_shape
        self._in_dtype = in_dtype
</source>
</class>

<class classid="39" nclones="2" nlines="12" similarity="83">
<source file="systems/chainer-7.2.0/chainer/functions/pooling/max_pooling_nd_kernel.py" startline="136" endline="149" pcid="1527">
    def _compile_out(self):
        def aux(offset, d_val, max_val, offset1):
            return 'int {} = {} * ({} + {});'.format(
                offset, d_val, max_val, offset1)

        d_vals = conv_nd_kernel.vars('d', self.ndim)[1:] + [1]
        max_vals = conv_nd_kernel.vars('max', self.ndim)
        offsets = conv_nd_kernel.vars('offset', self.ndim)
        offsets1 = ['d_0 * c0'] + offsets[:-1]
        offset_strs = conv_nd_kernel.map_(
            aux, offsets, d_vals, max_vals, offsets1)
        offset_strs.append('out = in[offset_{}];'.format(self.ndim - 1))
        return offset_strs

</source>
<source file="systems/chainer-7.2.0/chainer/functions/pooling/max_pooling_nd_kernel.py" startline="191" endline="206" pcid="1533">
    def after(self, out_xs):
        # 2D: int offset_0 = d_1 * (argmax_0 + d_0 * c0);
        #     int offset_1 = 1 * (argmax_1 + offset_0);
        #     out = ggx[offset_1];
        def aux(offset, d_val, max_val, offset1):
            return 'int {} = {} * ({} + {});'.format(
                offset, d_val, max_val, offset1)

        d_vals = conv_nd_kernel.vars('d', self.ndim)[1:] + [1]
        max_vals = conv_nd_kernel.vars('argmax', self.ndim)
        offsets = conv_nd_kernel.vars('offset', self.ndim)
        offsets1 = ['d_0 * c0'] + offsets[:-1]
        offset_strs = conv_nd_kernel.map_(
            aux, offsets, d_vals, max_vals, offsets1)
        offset_strs.append('out = ggx[offset_{}];'.format(self.ndim - 1))
        return '\n'.join(offset_strs)
</source>
</class>

<class classid="40" nclones="2" nlines="12" similarity="100">
<source file="systems/chainer-7.2.0/chainer/functions/pooling/pooling_nd_kernel.py" startline="32" endline="45" pcid="1542">
    def _generate(self, ndim):
        self.ndim = ndim
        self.ds = conv_nd_kernel.vars('d', ndim)
        self.outs = conv_nd_kernel.vars('out', ndim)
        self.ks = conv_nd_kernel.vars('k', ndim)
        self.ss = conv_nd_kernel.vars('s', ndim)
        self.ps = conv_nd_kernel.vars('p', ndim)

        in_params = self._in_params()
        out_params = self._out_params()
        operation = self._operation()
        name = '{}_pool_{}d_fwd'.format(self.name(), self.ndim)
        return in_params, out_params, operation, name

</source>
<source file="systems/chainer-7.2.0/chainer/functions/pooling/pooling_nd_kernel.py" startline="181" endline="194" pcid="1562">
    def _generate(self, ndim):
        self.ndim = ndim
        self.ds = conv_nd_kernel.vars('d', ndim)
        self.outs = conv_nd_kernel.vars('out', ndim)
        self.ks = conv_nd_kernel.vars('k', ndim)
        self.ss = conv_nd_kernel.vars('s', ndim)
        self.ps = conv_nd_kernel.vars('p', ndim)

        in_params = self._in_params()
        out_params = self._out_params()
        operation = self._operation()
        name = '{}_pool_{}d_bwd'.format(self.name(), self.ndim)
        return in_params, out_params, operation, name

</source>
</class>

<class classid="41" nclones="2" nlines="12" similarity="100">
<source file="systems/chainer-7.2.0/chainer/functions/pooling/pooling_nd_kernel.py" startline="46" endline="61" pcid="1543">
    def _in_params(self):
        # 2D: raw T in, int32 d_0, int32 d_1, int32 out_0, int32 out_1,
        #     int32 k_0, int32 k_1, int32 s_0, int32 s_1, int32 p_0,
        #     int32 p_1, ...
        def aux(x):
            return 'int32 {}'.format(x)
        in_params = self.in_params()
        if type(in_params) is tuple:
            raws = in_params[0]
            in_params = in_params[1]
        else:
            raws = []
        vars = self.ds + self.outs + self.ks + self.ss + self.ps
        return ', '.join(
            ['raw T in'] + raws + conv_nd_kernel.map_(aux, vars) + in_params)

</source>
<source file="systems/chainer-7.2.0/chainer/functions/pooling/pooling_nd_kernel.py" startline="195" endline="210" pcid="1563">
    def _in_params(self):
        # 2D: raw T gy, int32 d_0, int32 d_1, int32 out_0, int32 out_1,
        #     int32 k_0, int32 k_1, int32 s_0, int32 s_1, int32 p_0,
        #     int32 p_1, ...
        def aux(x):
            return 'int32 {}'.format(x)
        in_params = self.in_params()
        if type(in_params) is tuple:
            raws = in_params[0]
            in_params = in_params[1]
        else:
            raws = []
        vars = self.ds + self.outs + self.ks + self.ss + self.ps
        return ', '.join(
            ['raw T gy'] + raws + conv_nd_kernel.map_(aux, vars) + in_params)

</source>
</class>

<class classid="42" nclones="2" nlines="30" similarity="73">
<source file="systems/chainer-7.2.0/chainer/functions/pooling/pooling_nd_kernel.py" startline="87" endline="139" pcid="1549">
    def _compile_loop(self, out_xs):
        # 2D: int in_x0_0 = max(0,   out_x_0 * s_0       - p_0);
        #     int in_x1_0 = min(d_0, out_x_0 * s_0 + k_0 - p_0);
        #     int in_x0_1 = max(0,   out_x_1 * s_1       - p_1);
        #     int in_x1_1 = min(d_1, out_x_1 * s_1 + k_1 - p_1);
        #     ... Before-part here ...
        #     for (int x_0 = in_x0_0; x_0 < in_x1_0; ++x_0) {
        #       int offset_0 = d_1 * (x_0 + d_0 * c0);
        #       for (int x_1 = in_x0_1; x_1 < in_x1_1; ++x_1) {
        #         int offset_1 = 1 * (x_1 + offset_0);
        #         ... Main-part here ...
        #       }
        #     }
        #     ... After-part here ...
        def aux(in_x0, in_x1, d, out, k, s, p):
            return [
                'int {} = max(0, {} * {} - {});'.format(in_x0, out, s, p),
                'int {} = min({}, {} * {} + {} - {});'.format(
                    in_x1, d, out, s, k, p)]
        in_x0s = conv_nd_kernel.vars('in_x0', self.ndim)
        in_x1s = conv_nd_kernel.vars('in_x1', self.ndim)
        bounds = sum(conv_nd_kernel.map_(
            aux, in_x0s, in_x1s, self.ds, out_xs, self.ks, self.ss, self.ps
        ), [])

        def _loop_main(main):
            w = conv_nd_kernel.Writer()

            # Loop openings.
            xs = conv_nd_kernel.vars('x', self.ndim)
            offsets = conv_nd_kernel.vars('offset', self.ndim)
            ds1 = self.ds[1:] + [1]
            offsets1 = ['d_0 * c0'] + offsets[:-1]
            for x, in_x0, in_x1, offset, offset1, d1 in moves.zip(
                    xs, in_x0s, in_x1s, offsets, offsets1, ds1):
                w.write('for (int {} = {}; {} < {}; ++{}) {{'.format(
                    x, in_x0, x, in_x1, x), 'inc')
                w.write(
                    'int {} = {} * ({} + {});'.format(offset, d1, x, offset1))

            # Write main-part.
            offset = offsets[-1]
            for l in main(offset, xs).split('\n'):
                w.write(l)

            # Loop closings.
            for _ in xs:
                w.write('}', 'dec')

            return [w.get()]

        return bounds, _loop_main

</source>
<source file="systems/chainer-7.2.0/chainer/functions/pooling/pooling_nd_kernel.py" startline="236" endline="288" pcid="1569">
    def _compile_loop(self, xs):
        # 2D: int out_x0_0 = max(0,     (x_0 - k_0 + s_0) / s_0);
        #     int out_x1_0 = min(out_0, (x_0       + s_0) / s_0);
        #     int out_x0_1 = max(0,     (x_1 - k_1 + s_1) / s_1);
        #     int out_x1_1 = min(out_1, (x_1       + s_1) / s_1);
        #     ... Before-part here ...
        #     for (int out_x_0 = out_x0_0; out_x_0 < out_x1_0; ++out_x_0) {
        #       int offset_0 = out_1 * (out_x_0 + out_0 * c0);
        #       for (int out_x_1 = out_x0_1; out_x_1 < out_x1_1; ++out_x_1) {
        #         int offset_1 = 1 * (out_x_1 + offset_0);
        #         ... Main-part here ...
        #       }
        #     }
        #     ... After-part here ...
        def aux(out_x0, out_x1, x, out, k, s):
            return [
                'int {} = max(0, ({} - {} + {}) / {});'.format(
                    out_x0, x, k, s, s),
                'int {} = min({}, ({} + {}) / {});'.format(
                    out_x1, out, x, s, s)]
        out_x0s = conv_nd_kernel.vars('out_x0', self.ndim)
        out_x1s = conv_nd_kernel.vars('out_x1', self.ndim)
        bounds = sum(conv_nd_kernel.map_(
            aux, out_x0s, out_x1s, xs, self.outs, self.ks, self.ss), [])

        def _loop_main(main):
            w = conv_nd_kernel.Writer()

            # Loop openings.
            out_xs = conv_nd_kernel.vars('out_x', self.ndim)
            offsets = conv_nd_kernel.vars('offset', self.ndim)
            outs1 = self.outs[1:] + [1]
            offsets1 = ['out_0 * c0'] + offsets[:-1]
            for out_x, out_x0, out_x1, offset, offset1, out1 in moves.zip(
                    out_xs, out_x0s, out_x1s, offsets, offsets1, outs1):
                w.write('for (int {} = {}; {} < {}; ++{}) {{'.format(
                    out_x, out_x0, out_x, out_x1, out_x), 'inc')
                w.write('int {} = {} * ({} + {});'.format(
                    offset, out1, out_x, offset1))

            # Write main-part.
            offset = offsets[-1]
            for l in main(offset, xs, out_xs).split('\n'):
                w.write(l)

            # Loop closings.
            for _ in out_xs:
                w.write('}', 'dec')

            return [w.get()]

        return bounds, _loop_main

</source>
</class>

<class classid="43" nclones="3" nlines="13" similarity="92">
<source file="systems/chainer-7.2.0/chainer/functions/math/erf.py" startline="26" endline="40" pcid="1577">
    def forward_cpu(self, x):
        global _erf_cpu
        if _erf_cpu is None:
            try:
                from scipy import special
                _erf_cpu = special.erf
            except ImportError:
                warnings.warn(
                    'SciPy is not available. Forward computation of erf in CPU'
                    ' can be slow without SciPy.',
                    chainer.warnings.PerformanceWarning)
                _erf_cpu = numpy.vectorize(math.erf)
        self.retain_inputs((0,))
        return utils.force_array(_erf_cpu(x[0]), dtype=x[0].dtype),

</source>
<source file="systems/chainer-7.2.0/chainer/functions/math/ndtr.py" startline="31" endline="45" pcid="2068">
    def forward_cpu(self, x):
        global _ndtr_cpu
        if _ndtr_cpu is None:
            try:
                from scipy import special
                _ndtr_cpu = special.ndtr
            except ImportError:
                warnings.warn(
                    'SciPy is not available. Forward computation of ndtr in'
                    ' CPU can be slow without SciPy.',
                    chainer.warnings.PerformanceWarning)
                _ndtr_cpu = numpy.vectorize(_slow_ndtr_cpu)
        self.retain_inputs((0,))
        return utils.force_array(_ndtr_cpu(x[0]), dtype=x[0].dtype),

</source>
<source file="systems/chainer-7.2.0/chainer/functions/math/erfc.py" startline="26" endline="40" pcid="2009">
    def forward_cpu(self, x):
        global _erfc_cpu
        if _erfc_cpu is None:
            try:
                from scipy import special
                _erfc_cpu = special.erfc
            except ImportError:
                warnings.warn(
                    'SciPy is not available. Forward computation of erfc in'
                    ' CPU can be slow without SciPy.',
                    chainer.warnings.PerformanceWarning)
                _erfc_cpu = numpy.vectorize(math.erfc)
        self.retain_inputs((0,))
        return utils.force_array(_erfc_cpu(x[0]), dtype=x[0].dtype),

</source>
</class>

<class classid="44" nclones="5" nlines="10" similarity="72">
<source file="systems/chainer-7.2.0/chainer/functions/math/log_ndtr.py" startline="23" endline="35" pcid="1583">
    def forward_cpu(self, x):
        global _log_ndtr_cpu
        if _log_ndtr_cpu is None:
            try:
                from scipy import special
                _log_ndtr_cpu = special.log_ndtr
            except ImportError:
                raise ImportError('SciPy is not available. Forward computation'
                                  ' of log_ndtr can not be done.')

        self.retain_inputs((0,))
        return utils.force_array(_log_ndtr_cpu(x[0]), dtype=x[0].dtype),

</source>
<source file="systems/chainer-7.2.0/chainer/functions/math/zeta.py" startline="26" endline="38" pcid="2075">
    def forward_cpu(self, inputs):
        q, = inputs
        global _zeta_cpu
        if _zeta_cpu is None:
            try:
                from scipy import special
                _zeta_cpu = special.zeta
            except ImportError:
                raise ImportError('Scipy is not available. Forward computation'
                                  ' of zeta cannot be done.')
        self.retain_inputs((0,))
        return utils.force_array(_zeta_cpu(self._x, q), dtype=q.dtype),

</source>
<source file="systems/chainer-7.2.0/chainer/functions/math/digamma.py" startline="22" endline="33" pcid="1759">
    def forward_cpu(self, x):
        global _digamma_cpu
        if _digamma_cpu is None:
            try:
                from scipy import special
                _digamma_cpu = special.digamma
            except ImportError:
                raise ImportError('SciPy is not available. Forward computation'
                                  ' of digamma can not be done.')
        self.retain_inputs((0,))
        return utils.force_array(_digamma_cpu(x[0]), dtype=x[0].dtype),

</source>
<source file="systems/chainer-7.2.0/chainer/functions/math/polygamma.py" startline="25" endline="37" pcid="1648">
    def forward_cpu(self, inputs):
        n, x = inputs
        global _polygamma_cpu
        if _polygamma_cpu is None:
            try:
                from scipy import special
                _polygamma_cpu = special.polygamma
            except ImportError:
                raise ImportError('SciPy is not available. Forward computation'
                                  ' of polygamma can not be done.')
        self.retain_inputs((0, 1))
        return utils.force_array(_polygamma_cpu(n, x), dtype=x.dtype),

</source>
<source file="systems/chainer-7.2.0/chainer/functions/math/lgamma.py" startline="20" endline="31" pcid="1632">
    def forward_cpu(self, x):
        global _lgamma_cpu
        if _lgamma_cpu is None:
            try:
                from scipy import special
                _lgamma_cpu = special.gammaln
            except ImportError:
                raise ImportError('SciPy is not available. Forward computation'
                                  ' of lgamma can not be done.')
        self.retain_inputs((0,))
        return utils.force_array(_lgamma_cpu(x[0]), dtype=x[0].dtype),

</source>
</class>

<class classid="45" nclones="5" nlines="13" similarity="100">
<source file="systems/chainer-7.2.0/chainer/functions/math/average.py" startline="31" endline="48" pcid="1637">
    def check_type_forward(self, in_types):
        type_check._argname(in_types, ('x',))
        type_check.expect(in_types[0].dtype.kind == 'f')

        if self.axis is not None:
            for axis in self.axis:
                if axis >= 0:
                    type_check.expect(
                        axis < in_types[0].ndim,
                    )
                else:
                    type_check.expect(
                        -axis - 1 < in_types[0].ndim,
                    )

    # TODO(kataoka): override `forward_chainerx` if `chainerx.mean` does not
    # overflow for large float16 inputs

</source>
<source file="systems/chainer-7.2.0/chainer/functions/math/prod.py" startline="31" endline="45" pcid="1735">
    def check_type_forward(self, in_types):
        type_check._argname(in_types, ('x',))
        type_check.expect(in_types[0].dtype.kind == 'f')

        if self.axis is not None:
            for axis in self.axis:
                if axis >= 0:
                    type_check.expect(
                        axis < in_types[0].ndim,
                    )
                else:
                    type_check.expect(
                        -axis - 1 < in_types[0].ndim,
                    )

</source>
<source file="systems/chainer-7.2.0/chainer/functions/math/sum.py" startline="33" endline="47" pcid="1896">
    def check_type_forward(self, in_types):
        type_check._argname(in_types, ('x',))
        type_check.expect(in_types[0].dtype.kind == 'f')

        if self.axis is not None:
            for axis in self.axis:
                if axis >= 0:
                    type_check.expect(
                        axis < in_types[0].ndim,
                    )
                else:
                    type_check.expect(
                        -axis - 1 < in_types[0].ndim,
                    )

</source>
<source file="systems/chainer-7.2.0/chainer/functions/math/logsumexp.py" startline="27" endline="41" pcid="1711">
    def check_type_forward(self, in_types):
        type_check._argname(in_types, ('x',))
        type_check.expect(in_types[0].dtype.kind == 'f')

        if self.axis is not None:
            for axis in self.axis:
                if axis >= 0:
                    type_check.expect(
                        axis < in_types[0].ndim,
                    )
                else:
                    type_check.expect(
                        -axis - 1 < in_types[0].ndim,
                    )

</source>
<source file="systems/chainer-7.2.0/chainer/functions/math/minmax.py" startline="33" endline="47" pcid="1926">
    def check_type_forward(self, in_types):
        type_check._argname(in_types, ('x',))
        type_check.expect(in_types[0].dtype.kind == 'f')

        if self.axis is not None:
            for axis in self.axis:
                if axis >= 0:
                    type_check.expect(
                        axis < in_types[0].ndim,
                    )
                else:
                    type_check.expect(
                        -axis - 1 < in_types[0].ndim,
                    )

</source>
</class>

<class classid="46" nclones="2" nlines="12" similarity="91">
<source file="systems/chainer-7.2.0/chainer/functions/math/average.py" startline="62" endline="75" pcid="1639">
    def backward(self, indexes, grad_outputs):
        gy, = grad_outputs
        gy *= self.multiplier
        ndim = len(self.inputs[0].shape)
        if not (ndim == 0 or self.axis is None or self.keepdims):
            actual_axis = [
                axis if axis >= 0 else axis + ndim
                for axis in self.axis]
            shape = list(gy.shape)
            for axis in sorted(actual_axis):
                shape.insert(axis, 1)
            gy = chainer.functions.reshape(gy, shape)
        return chainer.functions.broadcast_to(gy, self.inputs[0].shape),

</source>
<source file="systems/chainer-7.2.0/chainer/functions/math/sum.py" startline="59" endline="71" pcid="1899">
    def backward(self, indexes, grad_outputs):
        gy, = grad_outputs
        ndim = len(self.inputs[0].shape)
        if not (ndim == 0 or self.axis is None or self.keepdims):
            actual_axis = [
                axis if axis >= 0 else axis + ndim
                for axis in self.axis]
            shape = list(gy.shape)
            for axis in sorted(actual_axis):
                shape.insert(axis, 1)
            gy = chainer.functions.reshape(gy, shape)
        return chainer.functions.broadcast_to(gy, self.inputs[0].shape),

</source>
</class>

<class classid="47" nclones="2" nlines="11" similarity="90">
<source file="systems/chainer-7.2.0/chainer/functions/math/trigonometric.py" startline="184" endline="194" pcid="1682">
    def forward_cpu(self, inputs):
        self.retain_inputs((0, 1))
        x, gy = inputs
        gx = utils.force_array(numpy.square(x))
        numpy.negative(gx, out=gx)
        gx += 1
        numpy.sqrt(gx, out=gx)
        numpy.reciprocal(gx, out=gx)
        gx *= gy
        return gx,

</source>
<source file="systems/chainer-7.2.0/chainer/functions/math/trigonometric.py" startline="252" endline="263" pcid="1690">
    def forward_cpu(self, inputs):
        self.retain_inputs((0, 1))
        x, gy = inputs
        gx = utils.force_array(numpy.square(x))
        numpy.negative(gx, out=gx)
        gx += 1
        numpy.sqrt(gx, out=gx)
        numpy.reciprocal(gx, out=gx)
        numpy.negative(gx, out=gx)
        gx *= gy
        return gx,

</source>
</class>

<class classid="48" nclones="2" nlines="10" similarity="90">
<source file="systems/chainer-7.2.0/chainer/functions/math/bias.py" startline="6" endline="48" pcid="1733">
def bias(x, y, axis=1):
    """Elementwise summation with broadcasting.

    Computes a elementwise summation of two input variables, with the shape of
    the latter variable broadcasted to match the shape of the former. ``axis``
    is the first axis of the first variable along which the second variable is
    applied.

    The term "broadcasting" here comes from Caffe's bias layer so the
    "broadcasting" with the following arguments::

           x : 100 x 3 x 40 x 5 x 6
           y : 3 x 40
        axis : 1

    is equivalent to the following numpy broadcasting::

        x : 100 x  3 x 40 x 5 x 6
        y :  (1 x) 3 x 40 x 1 x 1

    Note that the axis of ``x`` to which we apply ``y`` is specified by the
    argument ``axis``, whose meaning is different from numpy's ``axis``.

    Args:
        x (:class:`~chainer.Variable` or :ref:`ndarray`):
            Input variable to be summed.
        y (:class:`~chainer.Variable` or :ref:`ndarray`):
            Input variable to sum, broadcasted.
        axis (int): The first axis of ``x`` along which ``y`` is applied.

    Returns:
        ~chainer.Variable: Output variable.

    """
    x_shape = x.shape
    y_shape = y.shape
    if chainer.is_debug():
        assert x_shape[axis:axis + len(y_shape)] == y_shape
    y1_shape = tuple([1] * axis + list(y_shape) +
                     [1] * (len(x_shape) - axis - len(y_shape)))
    y1 = reshape.reshape(y, y1_shape)
    y2 = broadcast.broadcast_to(y1, x_shape)
    return x + y2
</source>
<source file="systems/chainer-7.2.0/chainer/functions/math/scale.py" startline="6" endline="48" pcid="2006">
def scale(x, y, axis=1):
    """Elementwise product with broadcasting.

    Computes a elementwise product of two input variables, with the shape of
    the latter variable broadcasted to match the shape of the former. ``axis``
    is the first axis of the first variable along which the second variable is
    applied.

    The term "broadcasting" here comes from Caffe's scale layer so the
    "broadcasting" with the following arguments::

           x : 100 x 3 x 40 x 5 x 6
           y : 3 x 40
        axis : 1

    is equivalent to the following numpy broadcasting::

        x : 100 x  3 x 40 x 5 x 6
        y :  (1 x) 3 x 40 x 1 x 1

    Note that the axis of ``x`` to which we apply ``y`` is specified by the
    argument ``axis``, whose meaning is different from numpy's ``axis``.

    Args:
        x (:class:`~chainer.Variable` or :ref:`ndarray`):
            Input variable to be scaled.
        y (:class:`~chainer.Variable` or :ref:`ndarray`):
            Input variable to scale, broadcasted.
        axis (int): The first axis of ``x`` along which ``y`` is applied.

    Returns:
        ~chainer.Variable: Output variable.

    """
    x_shape = x.shape
    y_shape = y.shape
    if chainer.is_debug():
        assert x_shape[axis:axis + len(y_shape)] == y_shape
    y1_shape = tuple([1] * axis + list(y_shape) +
                     [1] * (len(x_shape) - axis - len(y_shape)))
    y1 = reshape.reshape(y, y1_shape)
    y2 = broadcast.broadcast_to(y1, x_shape)
    return x * y2
</source>
</class>

<class classid="49" nclones="2" nlines="10" similarity="70">
<source file="systems/chainer-7.2.0/chainer/functions/math/linear_interpolate.py" startline="9" endline="20" pcid="1740">
    def check_type_forward(self, in_types):
        type_check._argname(in_types, ('p', 'x', 'y'))
        p_type, x_type, y_type = in_types

        type_check.expect(
            p_type.dtype.kind == 'f',
            x_type.dtype == p_type.dtype,
            y_type.dtype == p_type.dtype,
            p_type.shape == x_type.shape,
            p_type.shape == y_type.shape,
        )

</source>
<source file="systems/chainer-7.2.0/chainer/functions/activation/prelu.py" startline="70" endline="81" pcid="2342">
    def check_type_forward(self, in_types):
        type_check._argname(in_types, ('x', 'W', 'gy'))
        x_type, W_type, gy_type = in_types
        type_check.expect(
            x_type.dtype.kind == 'f',
            W_type.dtype == x_type.dtype,
            gy_type.dtype == x_type.dtype,
            x_type.ndim >= W_type.ndim + 1,
            x_type.shape[1:1 + type_check.eval(W_type.ndim)] == W_type.shape,
            gy_type.shape == x_type.shape
        )

</source>
</class>

<class classid="50" nclones="2" nlines="21" similarity="90">
<source file="systems/chainer-7.2.0/chainer/functions/math/sparse_matmul.py" startline="165" endline="185" pcid="2040">
class CooMatMul(function_node.FunctionNode):

    def __init__(self, sp_row, sp_col, sp_shape, sp_order='other',
                 transa=False, transb=False, transc=False, dtype=None):
        if sp_row.ndim != sp_col.ndim:
            raise ValueError('ndim of sp_row and sp_col must be the same.')
        if sp_row.ndim != 1 and sp_row.ndim != 2:
            raise ValueError('ndim of sp_row and sp_col must be one or two.')
        for i in range(sp_row.ndim):
            if sp_row.shape[i] != sp_col.shape[i]:
                msg = 'shape of sp_row and sp_col must be the same.'
                raise ValueError(msg)
        if len(sp_shape) != 2:
            raise ValueError('len(sp_shape) must be two.')
        self.sp_row = sp_row  # ((nb,) ldnz)
        self.sp_col = sp_col  # ((nb,) ldnz)
        self.sp_shape = sp_shape  # (_m, _k) when transa is False
        self.sp_order = sp_order
        self.transa = transa
        self.transb = transb
        self.transc = transc
</source>
<source file="systems/chainer-7.2.0/chainer/functions/math/sparse_matmul.py" startline="353" endline="374" pcid="2048">
class CooMatMulGradSP(function_node.FunctionNode):

    def __init__(self, sp_row, sp_col, sp_shape, sp_order='other',
                 transa=False, transb=False, transc=False,
                 dtype=None):
        if sp_row.ndim != sp_col.ndim:
            raise ValueError('ndim of sp_row and sp_col must be the same.')
        if sp_row.ndim != 1 and sp_row.ndim != 2:
            raise ValueError('ndim of sp_row and sp_col must be one or two.')
        for i in range(sp_row.ndim):
            if sp_row.shape[i] != sp_col.shape[i]:
                msg = 'shape of sp_row and sp_col must be the same.'
                raise ValueError(msg)
        if len(sp_shape) != 2:
            raise ValueError('len(sp_shape) must be two.')
        self.sp_row = sp_row  # ((nb,) ldnz)
        self.sp_col = sp_col  # ((nb,) ldnz)
        self.sp_shape = sp_shape  # (_m, _n) when transc is False
        self.sp_order = sp_order
        self.transa = transa
        self.transb = transb
        self.transc = transc
</source>
</class>

<class classid="51" nclones="2" nlines="17" similarity="94">
<source file="systems/chainer-7.2.0/chainer/functions/math/sparse_matmul.py" startline="221" endline="239" pcid="2043">
        return utils.force_array(c, self.dtype),

    def backward(self, indexes, grad_outputs):
        sp, dn = self.get_retained_inputs()
        g_c, = grad_outputs
        ret = []
        if 0 in indexes:
            g_sp = CooMatMulGradSP(self.sp_row, self.sp_col, self.sp_shape,
                                   self.sp_order,
                                   self.transc, not self.transb, self.transa,
                                   dtype=sp.dtype).apply((g_c, dn))[0]
            ret.append(g_sp)
        if 1 in indexes:
            g_dn = CooMatMul(self.sp_row, self.sp_col, self.sp_shape,
                             self.sp_order,
                             not self.transa, self.transc, self.transb,
                             dtype=dn.dtype).apply((sp, g_c))[0]
            ret.append(g_dn)
        return ret
</source>
<source file="systems/chainer-7.2.0/chainer/functions/math/sparse_matmul.py" startline="414" endline="432" pcid="2051">
        return utils.force_array(c),

    def backward(self, indexes, grad_outputs):
        a, b = self.get_retained_inputs()
        g_sp, = grad_outputs
        ret = []
        if 0 in indexes:
            g_a = CooMatMul(self.sp_row, self.sp_col, self.sp_shape,
                            self.sp_order,
                            self.transc, not self.transb, self.transa,
                            dtype=a.dtype).apply((g_sp, b))[0]
            ret.append(g_a)
        if 1 in indexes:
            g_b = CooMatMul(self.sp_row, self.sp_col, self.sp_shape,
                            self.sp_order,
                            not self.transc, self.transa, not self.transb,
                            dtype=b.dtype).apply((g_sp, a))[0]
            ret.append(g_b)
        return ret
</source>
</class>

<class classid="52" nclones="2" nlines="15" similarity="70">
<source file="systems/chainer-7.2.0/chainer/functions/noise/dropout.py" startline="130" endline="213" pcid="2104">
def dropout(x, ratio=.5, **kwargs):
    """dropout(x, ratio=.5, *, mask=None, return_mask=False)

    Drops elements of input variable randomly.

    This function drops input elements randomly with probability ``ratio`` and
    scales the remaining elements by factor ``1 / (1 - ratio)``. In testing
    mode (i.e., ``chainer.config.train`` is set to ``False``), it does nothing
    and just returns ``x``.

    Args:
        x (:class:`~chainer.Variable` or :ref:`ndarray`):
            Input variable. A :math:`(s_1, s_2, ..., s_N)` -shaped float array.
        ratio (float):
            Dropout ratio. The ``ratio`` must be ``0.0 <= ratio < 1.0``.
        mask (:ref:`ndarray` or None):
            The mask to be used for dropout.
            You do not have to specify this value, unless you need to make
            results deterministic.
            If ``mask`` is not specified or set to ``None``, a mask will be
            generated randomly according to the given ``ratio``.
            If ``mask`` is specified, ``ratio`` will be ignored.
            The shape and dtype must be the same as ``x`` and should be on the
            same device.
            Note that iDeep and cuDNN will not be used for this function if
            mask is specified, as iDeep and cuDNN do not support it.
        return_mask (bool):
            If ``True``, the mask used for dropout is returned together with
            the output variable.
            The returned mask can later be reused by passing it to ``mask``
            argument.

    Returns:
        ~chainer.Variable or tuple:
            When ``return_mask`` is ``False`` (default), returns the output
            variable.
            When ``True``, returns the tuple of the output variable and
            mask (:ref:`ndarray`). The mask will be on the same device as the
            input. The mask will become ``None`` when ``chainer.config.train``
            is set to ``False``.

    See the paper by G. Hinton: `Improving neural networks by preventing
    co-adaptation of feature detectors <https://arxiv.org/abs/1207.0580>`_.

    .. admonition:: Example

        >>> x = np.array([[-1, 0], [2, -3], [-2, 1]], np.float32)
        >>> with chainer.using_config('train', True):
        ...     y = F.dropout(x)
        >>> y.array
        array([[-2.,  0.],
               [ 4., -6.],
               [-0.,  2.]], dtype=float32)
        >>> with chainer.using_config('train', True):
        ...     y = F.dropout(x, ratio=0.0) \
# dropout returns original input if ratio=0.0
        >>> (x == y.array).all()
        True
        >>> with chainer.using_config('train', False):
        ...     y = F.dropout(x) \
# dropout in test mode returns original input
        >>> (x == y.array).all()
        True

    """
    mask = None
    return_mask = False
    if kwargs:
        mask, return_mask = argument.parse_kwargs(
            kwargs, ('mask', mask), ('return_mask', return_mask),
            train='train argument is not supported anymore. '
                  'Use chainer.using_config')

    if configuration.config.train:
        func = Dropout(ratio, mask, return_mask)
        out, = func.apply((x,))
        mask = func.mask
    else:
        out = chainer.as_variable(x)
        mask = None

    if return_mask:
        return out, mask
    return out
</source>
<source file="systems/chainer-7.2.0/chainer/functions/activation/rrelu.py" startline="92" endline="161" pcid="2321">
def rrelu(x, l=1. / 8, u=1. / 3, **kwargs):
    """rrelu(x, l=1. / 8, u=1. / 3, *, r=None, return_r=False)

    Randomized Leaky Rectified Liner Unit function.

    This function is expressed as

    .. math:: f(x)=\\max(x, rx),

    where :math:`r` is a random number sampled from a uniform distribution
    :math:`U(l, u)`.

    .. note::

        The :math:`r` corresponds to :math:`a` in the original
        paper (https://arxiv.org/pdf/1505.00853.pdf).

    Args:
        x (:class:`~chainer.Variable` or :ref:`ndarray`):
            Input variable. A :math:`(s_1, s_2, ..., s_N)`-shaped float array.
        l (float): The lower bound of the uniform distribution.
        u (float): The upper bound of the uniform distribution.
        r (:ref:`ndarray` or None):
            The r to be used for rrelu.
            The shape and dtype must be the same as ``x[0]`` and should be on
            the same device.
            If ``r``  is not specified or set to ``None``, an ``r`` will be
            generated randomly according to the given ``l`` and ``u``.
            If ``r`` is specified, ``l`` and ``u`` will be ignored.
        return_r (bool):
            If ``True``, the r used for rrelu is returned altogether with
            the output variable.
            The returned ``r`` can latter be reused by passing it to ``r``
            argument.

    Returns:
        ~chainer.Variable or tuple:
            When ``return_r`` is ``False`` (default), return the output
            variable. Otherwise returnes the tuple of the output variable and
            ``r`` (:ref:`ndarray`). The ``r`` will be on the same device as
            the input.
            A :math:`(s_1, s_2, ..., s_N)`-shaped float array.

    .. admonition:: Example

        >>> x = np.array([[-1, 0], [2, -3], [-2, 1]], np.float32)
        >>> x
        array([[-1.,  0.],
               [ 2., -3.],
               [-2.,  1.]], dtype=float32)
        >>> F.rrelu(x).array # doctest: +SKIP
        array([[-0.24850948,  0.        ],
               [ 2.        , -0.50844127],
               [-0.598535  ,  1.        ]], dtype=float32)
    """
    r = None
    return_r = False
    if kwargs:
        r, return_r = argument.parse_kwargs(
            kwargs, ('r', r), ('return_r', r),
            train='train argument is not supported anymore. '
                  'Use chainer.using_config')

    func = RReLU(l, u, r)
    out, = func.apply((x,))
    r = func.r

    if return_r:
        return out, r
    return out
</source>
</class>

<class classid="53" nclones="2" nlines="13" similarity="84">
<source file="systems/chainer-7.2.0/chainer/functions/loss/hinge.py" startline="81" endline="96" pcid="2148">
    def backward_cpu(self, inputs, grad_outputs):
        t, gloss = inputs[1], grad_outputs[0]

        if self.reduce == 'mean':
            gloss /= len(t)

        self.bottom_diff[numpy.arange(len(t)), t] *= -1
        if self.norm == 'L1':
            gx = gloss * numpy.sign(self.bottom_diff)
        elif self.norm == 'L2':
            gx = 2 * gloss * self.bottom_diff
        else:
            raise NotImplementedError()

        return gx, None

</source>
<source file="systems/chainer-7.2.0/chainer/functions/loss/hinge.py" startline="97" endline="114" pcid="2149">
    def backward_gpu(self, inputs, grad_outputs):
        xp = backend.get_array_module(*inputs)
        t, gloss = inputs[1], grad_outputs[0]

        if self.reduce == 'mean':
            gloss /= len(t)

        self.bottom_diff = _hinge_fwd_kernel()(t, self.bottom_diff)
        if self.norm == 'L1':
            gx = gloss * xp.sign(self.bottom_diff)
        elif self.norm == 'L2':
            gx = 2 * gloss * self.bottom_diff
        else:
            raise NotImplementedError()

        return gx, None


</source>
</class>

<class classid="54" nclones="2" nlines="12" similarity="76">
<source file="systems/chainer-7.2.0/chainer/functions/loss/vae.py" startline="9" endline="65" pcid="2206">
def gaussian_kl_divergence(mean, ln_var, reduce='sum'):
    """Computes the KL-divergence of Gaussian variables from the standard one.

    Given two variable ``mean`` representing :math:`\\mu` and ``ln_var``
    representing :math:`\\log(\\sigma^2)`, this function calculates
    the KL-divergence in elementwise manner between the given multi-dimensional
    Gaussian :math:`N(\\mu, S)` and the standard Gaussian :math:`N(0, I)`

    .. math::

       D_{\\mathbf{KL}}(N(\\mu, S) \\| N(0, I)),

    where :math:`S` is a diagonal matrix such that :math:`S_{ii} = \\sigma_i^2`
    and :math:`I` is an identity matrix.

    The output is a variable whose value depends on the value of
    the option ``reduce``. If it is ``'no'``, it holds the elementwise
    loss values. If it is ``'sum'`` or ``'mean'``, loss values are summed up
    or averaged respectively.

    Args:
        mean (:class:`~chainer.Variable` or :ref:`ndarray`):
            A variable representing mean of given
            gaussian distribution, :math:`\\mu`.
        ln_var (:class:`~chainer.Variable` or :ref:`ndarray`):
            A variable representing logarithm of
            variance of given gaussian distribution, :math:`\\log(\\sigma^2)`.
        reduce (str): Reduction option. Its value must be either
            ``'sum'``, ``'mean'`` or ``'no'``. Otherwise, :class:`ValueError`
            is raised.

    Returns:
        ~chainer.Variable:
            A variable representing KL-divergence between
            given gaussian distribution and the standard gaussian.
            If ``reduce`` is ``'no'``, the output variable holds array
            whose shape is same as one of (hence both of) input variables.
            If it is ``'sum'`` or ``'mean'``, the output variable holds a
            scalar value.

    """
    if reduce not in ('sum', 'mean', 'no'):
        raise ValueError(
            'only \'sum\', \'mean\' and \'no\' are valid for \'reduce\', but '
            '\'%s\' is given' % reduce)

    var = exponential.exp(ln_var)
    mean_square = mean * mean
    loss = (mean_square + var - ln_var - 1) * 0.5
    if reduce == 'sum':
        return sum.sum(loss)
    elif reduce == 'mean':
        return average.average(loss)
    else:
        return loss


</source>
<source file="systems/chainer-7.2.0/chainer/functions/loss/vae.py" startline="66" endline="122" pcid="2207">
def bernoulli_nll(x, y, reduce='sum'):
    """Computes the negative log-likelihood of a Bernoulli distribution.

    This function calculates the negative log-likelihood of a Bernoulli
    distribution.

    .. math::

        -\\log B(x; p) = -\\sum_i \\{x_i \\log(p_i) + \
        (1 - x_i)\\log(1 - p_i)\\},

    where :math:`p = \\sigma(y)`, :math:`\\sigma(\\cdot)` is a sigmoid
    function, and :math:`B(x; p)` is a Bernoulli distribution.


    The output is a variable whose value depends on the value of
    the option ``reduce``. If it is ``'no'``, it holds the elementwise
    loss values. If it is ``'sum'`` or ``'mean'``, loss values are summed up
    or averaged respectively.

    .. note::

       As this function uses a sigmoid function, you can pass a result of
       fully-connected layer (that means :class:`Linear`) to this function
       directly.

    Args:
        x (:class:`~chainer.Variable` or :ref:`ndarray`): Input variable.
        y (:class:`~chainer.Variable` or :ref:`ndarray`): A variable
            representing the parameter of Bernoulli distribution.
        reduce (str): Reduction option. Its value must be either
            ``'sum'``, ``'mean'`` or ``'no'``. Otherwise, :class:`ValueError`
            is raised.

    Returns:
        ~chainer.Variable:
            A variable representing the negative log-likelihood.
            If ``reduce`` is ``'no'``, the output variable holds array
            whose shape is same as one of (hence both of) input variables.
            If it is ``'sum'`` or ``'mean'``, the output variable holds a
            scalar value.

    """
    if reduce not in ('sum', 'mean', 'no'):
        raise ValueError(
            'only \'sum\', \'mean\' and \'no\' are valid for \'reduce\', but '
            '\'%s\' is given' % reduce)

    loss = softplus.softplus(y) - x * y
    if reduce == 'sum':
        return sum.sum(loss)
    elif reduce == 'mean':
        return average.average(loss)
    else:
        return loss


</source>
</class>

<class classid="55" nclones="2" nlines="12" similarity="83">
<source file="systems/chainer-7.2.0/chainer/functions/activation/tanh.py" startline="71" endline="83" pcid="2257">
    def forward_gpu(self, inputs):
        self.retain_inputs((0, 1))
        y, gy = inputs
        if (chainer.should_use_cudnn('==always') and
                self.x is not None and gy.flags.c_contiguous):
            gx = cudnn.activation_backward(self.x, y, gy, _mode)
        else:
            gx = cuda.elementwise(
                'T y, T gy', 'T gx',
                'gx = gy * (1 - y * y)',
                'tanh_bwd')(y, gy)
        return gx,

</source>
<source file="systems/chainer-7.2.0/chainer/functions/activation/sigmoid.py" startline="74" endline="86" pcid="2364">
    def forward_gpu(self, inputs):
        self.retain_inputs((0, 1))
        y, gy = inputs
        if (chainer.should_use_cudnn('==always') and gy.flags.c_contiguous and
                self.x is not None and self.x.flags.c_contiguous):
            gx = cudnn.activation_backward(self.x, y, gy, _mode)
        else:
            gx = cuda.elementwise(
                'T y, T gy', 'T gx',
                'gx = gy * y * (1 - y)',
                'sigmoid_bwd')(y, gy)
        return gx,

</source>
</class>

<class classid="56" nclones="2" nlines="13" similarity="84">
<source file="systems/chainer-7.2.0/chainer/functions/activation/rrelu.py" startline="38" endline="51" pcid="2314">
    def forward_cpu(self, inputs):
        x, = inputs
        if chainer.config.train:
            if self.r is None:
                self.r = np.random.uniform(
                    self.lower, self.upper, x.shape
                ).astype(x.dtype, copy=False)
        else:
            self.r = np.full(
                x.shape, (self.lower + self.upper) / 2, dtype=x.dtype)
        y = np.where(x >= 0, x, x * self.r)
        self.retain_outputs((0,))
        return y,

</source>
<source file="systems/chainer-7.2.0/chainer/functions/activation/rrelu.py" startline="52" endline="66" pcid="2315">
    def forward_gpu(self, inputs):
        x, = inputs
        xp = cuda.cupy
        if chainer.config.train:
            if self.r is None:
                self.r = xp.random.uniform(
                    self.lower, self.upper, x.shape
                ).astype(x.dtype, copy=False)
        else:
            self.r = xp.full(
                x.shape, (self.lower + self.upper) / 2, dtype=x.dtype)
        y = _kern()(x, x, self.r)
        self.retain_outputs((0,))
        return y,

</source>
</class>

<class classid="57" nclones="2" nlines="13" similarity="100">
<source file="systems/chainer-7.2.0/chainer/functions/evaluation/classification_summary.py" startline="26" endline="43" pcid="2376">
    def check_type_forward(self, in_types):
        type_check._argname(in_types, ('x', 't'))
        x_type, t_type = in_types

        type_check.expect(
            x_type.dtype.kind == 'f',
            t_type.dtype.kind == 'i'
        )

        t_ndim = type_check.eval(t_type.ndim)
        type_check.expect(
            x_type.ndim >= t_type.ndim,
            x_type.shape[0] == t_type.shape[0],
            x_type.shape[2: t_ndim + 1] == t_type.shape[1:]
        )
        for i in six.moves.range(t_ndim + 1, type_check.eval(x_type.ndim)):
            type_check.expect(x_type.shape[i] == 1)

</source>
<source file="systems/chainer-7.2.0/chainer/functions/evaluation/accuracy.py" startline="15" endline="32" pcid="2388">
    def check_type_forward(self, in_types):
        type_check._argname(in_types, ('x', 't'))
        x_type, t_type = in_types

        type_check.expect(
            x_type.dtype.kind == 'f',
            t_type.dtype.kind == 'i'
        )

        t_ndim = type_check.eval(t_type.ndim)
        type_check.expect(
            x_type.ndim >= t_type.ndim,
            x_type.shape[0] == t_type.shape[0],
            x_type.shape[2: t_ndim + 1] == t_type.shape[1:]
        )
        for i in six.moves.range(t_ndim + 1, type_check.eval(x_type.ndim)):
            type_check.expect(x_type.shape[i] == 1)

</source>
</class>

<class classid="58" nclones="3" nlines="13" similarity="85">
<source file="systems/chainer-7.2.0/chainer/functions/rnn/n_step_gru.py" startline="49" endline="65" pcid="2426">
def _combine_inputs(hx, ws, bs, xs, num_layers, directions):
    combined = []
    combined.append(hx)
    for x in xs:
        combined.append(x)

    for n in range(num_layers):
        for direction in range(directions):
            idx = directions * n + direction

            for i in range(6):
                combined.append(ws[idx][i])
            for i in range(6):
                combined.append(bs[idx][i])
    return combined


</source>
<source file="systems/chainer-7.2.0/chainer/functions/rnn/n_step_rnn.py" startline="91" endline="107" pcid="2454">
def _combine_inputs(hx, ws, bs, xs, num_layers, directions):
    combined = []
    combined.append(hx)
    for x in xs:
        combined.append(x)

    for n in range(num_layers):
        for direction in range(directions):
            idx = directions * n + direction

            for i in range(2):
                combined.append(ws[idx][i])
            for i in range(2):
                combined.append(bs[idx][i])
    return combined


</source>
<source file="systems/chainer-7.2.0/chainer/functions/rnn/n_step_lstm.py" startline="48" endline="65" pcid="2444">
def _combine_inputs(hx, cx, ws, bs, xs, num_layers, directions):
    combined = []
    combined.append(hx)
    combined.append(cx)
    for x in xs:
        combined.append(x)

    for n in range(num_layers):
        for direction in range(directions):
            idx = directions * n + direction

            for i in range(8):
                combined.append(ws[idx][i])
            for i in range(8):
                combined.append(bs[idx][i])
    return combined


</source>
</class>

<class classid="59" nclones="3" nlines="15" similarity="87">
<source file="systems/chainer-7.2.0/chainer/functions/rnn/n_step_gru.py" startline="66" endline="82" pcid="2427">
def _seperate_inputs(combined, num_layers, seq_length, directions):
    hx = combined[0]
    xs = combined[1: 1 + seq_length]
    ws = []
    bs = []
    index = 1 + seq_length
    for n in range(num_layers):
        ws.append(combined[index: index + 6])
        bs.append(combined[index + 6: index + 12])
        index += 12
        if directions == 2:
            ws.append(combined[index: index + 6])
            bs.append(combined[index + 6: index + 12])
            index += 12
    return hx, ws, bs, xs


</source>
<source file="systems/chainer-7.2.0/chainer/functions/rnn/n_step_rnn.py" startline="108" endline="124" pcid="2455">
def _seperate_inputs(combined, num_layers, seq_length, directions):
    hx = combined[0]
    xs = combined[1: 1 + seq_length]
    ws = []
    bs = []
    index = 1 + seq_length
    for n in range(num_layers):
        ws.append(combined[index: index + 2])
        bs.append(combined[index + 2: index + 4])
        index += 4
        if directions == 2:
            ws.append(combined[index: index + 2])
            bs.append(combined[index + 2: index + 4])
            index += 4
    return hx, ws, bs, xs


</source>
<source file="systems/chainer-7.2.0/chainer/functions/rnn/n_step_lstm.py" startline="66" endline="83" pcid="2445">
def _seperate_inputs(combined, num_layers, seq_length, directions):
    hx = combined[0]
    cx = combined[1]
    xs = combined[2: 2 + seq_length]
    ws = []
    bs = []
    index = 2 + seq_length
    for n in range(num_layers):
        ws.append(combined[index: index + 8])
        bs.append(combined[index + 8: index + 16])
        index += 16
        if directions == 2:
            ws.append(combined[index: index + 8])
            bs.append(combined[index + 8: index + 16])
            index += 16
    return hx, cx, ws, bs, xs


</source>
</class>

<class classid="60" nclones="2" nlines="12" similarity="100">
<source file="systems/chainer-7.2.0/chainer/utils/conv_nd_kernel.py" startline="98" endline="112" pcid="2519">
    def _compile_out_x(self, ndim, outs):
        # 2D: int out_x0 = i / (out_1) % out_0;
        #     int out_x1 = i % out_1;
        def aux(out_x, xs):
            head = xs[0]
            tail = xs[1:]
            if tail:
                return 'int {} = i / ({}) % {};'.format(
                    out_x, mulexp(tail), head)
            else:
                return 'int {} = i % {};'.format(out_x, head)
        out_xs = vars('out_x', ndim)
        out_x_decls = map_(aux, out_xs, succ_sublists(outs))
        return out_x_decls, out_xs

</source>
<source file="systems/chainer-7.2.0/chainer/utils/conv_nd_kernel.py" startline="199" endline="213" pcid="2530">
    def _compile_x(self, ndim, ds):
        # 2D: int x_0 = i / (d_1) % d_0;
        #     int x_1 = i % d_1;
        def aux(x, ds):
            head = ds[0]
            tail = ds[1:]
            if tail:
                return 'int {} = i / ({}) % {};'.format(
                    x, mulexp(tail), head)
            else:
                return 'int {} = i % {};'.format(x, head)
        xs = vars('x', ndim)
        x_decls = map_(aux, xs, succ_sublists(ds))
        return x_decls, xs

</source>
</class>

<class classid="61" nclones="2" nlines="12" similarity="91">
<source file="systems/chainer-7.2.0/chainer/utils/conv_nd_kernel.py" startline="155" endline="168" pcid="2524">
    def _generate(self, ndim):
        ds = vars('d', ndim)
        outs = vars('out', ndim)
        ks = vars('k', ndim)
        ss = vars('s', ndim)
        ps = vars('p', ndim)
        dilate = vars('di', ndim)

        in_params = self._in_params(ds, outs, ks, ss, ps, dilate)
        out_params = self._out_params()
        operation = self._operation(ndim, ds, outs, ks, ss, ps, dilate)
        name = name = 'im2col_{}d'.format(ndim)
        return in_params, out_params, operation, name

</source>
<source file="systems/chainer-7.2.0/chainer/utils/conv_nd_kernel.py" startline="277" endline="290" pcid="2537">
    def _generate(self, ndim):
        ds = vars('d', ndim)
        outs = vars('out', ndim)
        ks = vars('k', ndim)
        ss = vars('s', ndim)
        ps = vars('p', ndim)
        dilate = vars('di', ndim)

        in_params = self._in_params(ds, outs, ks, ss, ps, dilate)
        out_params = self._out_params()
        operation = self._operation(ndim, ds, outs, ks, ss, ps, dilate)
        name = 'col2im_{}d'.format(ndim)
        return in_params, out_params, operation, name

</source>
</class>

<class classid="62" nclones="3" nlines="10" similarity="90">
<source file="systems/chainer-7.2.0/chainer/datasets/kuzushiji_mnist.py" startline="30" endline="79" pcid="2582">
def get_kuzushiji_mnist(withlabel=True, ndim=1, scale=1., dtype=None,
                        label_dtype=numpy.int32, rgb_format=False):
    """Gets the Kuzushiji-MNIST dataset.

    `Kuzushiji-MNIST (KMNIST) <http://codh.rois.ac.jp/kmnist/>`_ is a set of
    hand-written Japanese characters represented by grey-scale 28x28 images.
    In the original images, each pixel is represented by one-byte unsigned
    integer. This function scales the pixels to floating point values in the
    interval ``[0, scale]``.

    This function returns the training set and the test set of the official
    KMNIST dataset. If ``withlabel`` is ``True``, each dataset consists of
    tuples of images and labels, otherwise it only consists of images.

    Args:
        withlabel (bool): If ``True``, it returns datasets with labels. In this
            case, each example is a tuple of an image and a label. Otherwise,
            the datasets only contain images.
        ndim (int): Number of dimensions of each image. The shape of each image
            is determined depending on ``ndim`` as follows:

            - ``ndim == 1``: the shape is ``(784,)``
            - ``ndim == 2``: the shape is ``(28, 28)``
            - ``ndim == 3``: the shape is ``(1, 28, 28)``

        scale (float): Pixel value scale. If it is 1 (default), pixels are
            scaled to the interval ``[0, 1]``.
        dtype: Data type of resulting image arrays. ``chainer.config.dtype`` is
            used by default (see :ref:`configuration`).
        label_dtype: Data type of the labels.
        rgb_format (bool): if ``ndim == 3`` and ``rgb_format`` is ``True``, the
            image will be converted to rgb format by duplicating the channels
            so the image shape is (3, 28, 28). Default is ``False``.

    Returns:
        A tuple of two datasets. If ``withlabel`` is ``True``, both datasets
        are :class:`~chainer.datasets.TupleDataset` instances. Otherwise, both
        datasets are arrays of images.

    """
    dtype = chainer.get_dtype(dtype)
    train_raw = _retrieve_kuzushiji_mnist_training()
    train = preprocess_mnist(train_raw, withlabel, ndim, scale, dtype,
                             label_dtype, rgb_format)
    test_raw = _retrieve_kuzushiji_mnist_test()
    test = preprocess_mnist(test_raw, withlabel, ndim, scale, dtype,
                            label_dtype, rgb_format)
    return train, test


</source>
<source file="systems/chainer-7.2.0/chainer/datasets/mnist.py" startline="11" endline="59" pcid="2636">
def get_mnist(withlabel=True, ndim=1, scale=1., dtype=None,
              label_dtype=numpy.int32, rgb_format=False):
    """Gets the MNIST dataset.

    `MNIST <http://yann.lecun.com/exdb/mnist/>`_ is a set of hand-written
    digits represented by grey-scale 28x28 images. In the original images, each
    pixel is represented by one-byte unsigned integer. This function
    scales the pixels to floating point values in the interval ``[0, scale]``.

    This function returns the training set and the test set of the official
    MNIST dataset. If ``withlabel`` is ``True``, each dataset consists of
    tuples of images and labels, otherwise it only consists of images.

    Args:
        withlabel (bool): If ``True``, it returns datasets with labels. In this
            case, each example is a tuple of an image and a label. Otherwise,
            the datasets only contain images.
        ndim (int): Number of dimensions of each image. The shape of each image
            is determined depending on ``ndim`` as follows:

            - ``ndim == 1``: the shape is ``(784,)``
            - ``ndim == 2``: the shape is ``(28, 28)``
            - ``ndim == 3``: the shape is ``(1, 28, 28)``

        scale (float): Pixel value scale. If it is 1 (default), pixels are
            scaled to the interval ``[0, 1]``.
        dtype: Data type of resulting image arrays. ``chainer.config.dtype`` is
            used by default (see :ref:`configuration`).
        label_dtype: Data type of the labels.
        rgb_format (bool): if ``ndim == 3`` and ``rgb_format`` is ``True``, the
            image will be converted to rgb format by duplicating the channels
            so the image shape is (3, 28, 28). Default is ``False``.

    Returns:
        A tuple of two datasets. If ``withlabel`` is ``True``, both datasets
        are :class:`~chainer.datasets.TupleDataset` instances. Otherwise, both
        datasets are arrays of images.

    """
    dtype = chainer.get_dtype(dtype)
    train_raw = _retrieve_mnist_training()
    train = preprocess_mnist(train_raw, withlabel, ndim, scale, dtype,
                             label_dtype, rgb_format)
    test_raw = _retrieve_mnist_test()
    test = preprocess_mnist(test_raw, withlabel, ndim, scale, dtype,
                            label_dtype, rgb_format)
    return train, test


</source>
<source file="systems/chainer-7.2.0/chainer/datasets/fashion_mnist.py" startline="25" endline="75" pcid="2648">
def get_fashion_mnist(withlabel=True, ndim=1, scale=1., dtype=None,
                      label_dtype=numpy.int32, rgb_format=False):
    """Gets the Fashion-MNIST dataset.

    `Fashion-MNIST <https://github.com/zalandoresearch/fashion-mnist/>`_ is a
    set of fashion articles represented by grey-scale 28x28 images. In the
    original images, each pixel is represented by one-byte unsigned integer.
    This function scales the pixels to floating point values in the interval
    ``[0, scale]``.

    This function returns the training set and the test set of the official
    Fashion-MNIST dataset. If ``withlabel`` is ``True``, each dataset consists
    of tuples of images and labels, otherwise it only consists of images.

    Args:
        withlabel (bool): If ``True``, it returns datasets with labels. In this
            case, each example is a tuple of an image and a label. Otherwise,
            the datasets only contain images.
        ndim (int): Number of dimensions of each image. The shape of each image
            is determined depending on ``ndim`` as follows:

            - ``ndim == 1``: the shape is ``(784,)``
            - ``ndim == 2``: the shape is ``(28, 28)``
            - ``ndim == 3``: the shape is ``(1, 28, 28)``

        scale (float): Pixel value scale. If it is 1 (default), pixels are
            scaled to the interval ``[0, 1]``.
        dtype: Data type of resulting image arrays. ``chainer.config.dtype`` is
            used by default (see :ref:`configuration`).
        label_dtype: Data type of the labels.
        rgb_format (bool): if ``ndim == 3`` and ``rgb_format`` is ``True``, the
            image will be converted to rgb format by duplicating the channels
            so the image shape is (3, 28, 28). Default is ``False``.

    Returns:
        A tuple of two datasets. If ``withlabel`` is ``True``, both datasets
        are :class:`~chainer.datasets.TupleDataset` instances. Otherwise, both
        datasets are arrays of images.

    """
    train_raw = _retrieve_fashion_mnist_training()
    dtype = chainer.get_dtype(dtype)

    train = preprocess_mnist(train_raw, withlabel, ndim, scale, dtype,
                             label_dtype, rgb_format)
    test_raw = _retrieve_fashion_mnist_test()
    test = preprocess_mnist(test_raw, withlabel, ndim, scale, dtype,
                            label_dtype, rgb_format)
    return train, test


</source>
</class>

<class classid="63" nclones="3" nlines="11" similarity="81">
<source file="systems/chainer-7.2.0/chainer/optimizers/adam.py" startline="480" endline="491" pcid="2734">
    def __init__(self,
                 alpha=_default_hyperparam.alpha,
                 beta1=_default_hyperparam.beta1,
                 beta2=_default_hyperparam.beta2,
                 eps=_default_hyperparam.eps,
                 eta=_default_hyperparam.eta,
                 weight_decay_rate=_default_hyperparam.weight_decay_rate):
        super(AdamW, self).__init__(
            alpha=alpha, beta1=beta1, beta2=beta2, eps=eps, eta=eta,
            weight_decay_rate=weight_decay_rate)


</source>
<source file="systems/chainer-7.2.0/chainer/optimizers/adam.py" startline="539" endline="551" pcid="2736">
    def __init__(self,
                 alpha=_default_hyperparam.alpha,
                 beta1=_default_hyperparam.beta1,
                 beta2=_default_hyperparam.beta2,
                 final_lr=_default_hyperparam.final_lr,
                 gamma=_default_hyperparam.gamma,
                 eps=_default_hyperparam.eps,
                 eta=_default_hyperparam.eta):
        super(AdaBound, self).__init__(
            alpha=alpha, beta1=beta1, beta2=beta2, eps=eps, eta=eta,
            amsgrad=False, adabound=True, final_lr=final_lr, gamma=gamma)


</source>
<source file="systems/chainer-7.2.0/chainer/optimizers/adam.py" startline="571" endline="581" pcid="2737">
    def __init__(self,
                 alpha=_default_hyperparam.alpha,
                 beta1=_default_hyperparam.beta1,
                 beta2=_default_hyperparam.beta2,
                 final_lr=_default_hyperparam.final_lr,
                 gamma=_default_hyperparam.gamma,
                 eps=_default_hyperparam.eps,
                 eta=_default_hyperparam.eta):
        super(AMSBound, self).__init__(
            alpha=alpha, beta1=beta1, beta2=beta2, eps=eps, eta=eta,
            amsgrad=True, adabound=True, final_lr=final_lr, gamma=gamma)
</source>
</class>

<class classid="64" nclones="5" nlines="13" similarity="71">
<source file="systems/chainer-7.2.0/chainer/optimizers/ada_grad.py" startline="68" endline="82" pcid="2741">
    def update_core_gpu(self, param):
        grad = param.grad
        if grad is None:
            return
        if AdaGradRule._kernel is None:
            AdaGradRule._kernel = cuda.elementwise(
                'T grad, T lr, T eps',
                'T param, T h',
                '''h += grad * grad;
                   param -= lr * grad / (sqrt(h) + eps);''',
                'adagrad')
        AdaGradRule._kernel(grad, self.hyperparam.lr, self.hyperparam.eps,
                            param.data, self.state['h'])


</source>
<source file="systems/chainer-7.2.0/chainer/optimizers/nesterov_ag.py" startline="67" endline="84" pcid="2776">
    def update_core_gpu(self, param):
        grad = param.grad
        if grad is None:
            return
        if NesterovAGRule._kernel is None:
            NesterovAGRule._kernel = cuda.elementwise(
                'T grad, T lr, T momentum',
                'T param, T v',
                '''
                v = v * momentum - lr * grad;
                param += momentum * momentum * v - (1 + momentum) * lr * grad;
                ''',
                'nesterov_ag')
        NesterovAGRule._kernel(
            grad, self.hyperparam.lr, self.hyperparam.momentum,
            param.data, self.state['v'])


</source>
<source file="systems/chainer-7.2.0/chainer/optimizers/smorms3.py" startline="74" endline="95" pcid="2782">

    def update_core_gpu(self, param):
        grad = param.grad
        if grad is None:
            return
        if SMORMS3Rule._kernel is None:
            SMORMS3Rule._kernel = cuda.elementwise(
                'T grad, T lr, T eps',
                'T param, T mem, T g, T g2',
                '''T r, x;
                   r = 1 / (mem + 1);
                   g = (1 - r) * g + r * grad;
                   g2 = (1 - r) * g2 + r * grad * grad;
                   x = g * g / (g2 + eps);
                   param -= grad * min(lr, x) / (sqrt(g2) + eps);
                   mem = 1 + mem * (1 - x)
                   ''',
                'smorms3')
        SMORMS3Rule._kernel(
            grad, self.hyperparam.lr, self.hyperparam.eps, param.data,
            self.state['mem'], self.state['g'], self.state['g2'])

</source>
<source file="systems/chainer-7.2.0/chainer/optimizers/rmsprop_graves.py" startline="87" endline="106" pcid="2753">
    def update_core_gpu(self, param):
        grad = param.grad
        if grad is None:
            return
        hp = self.hyperparam
        if RMSpropGravesRule._kernel is None:
            RMSpropGravesRule._kernel = cuda.elementwise(
                'T grad, T lr, T alpha, T momentum, T eps',
                'T param, T avg_n, T avg_g, T delta',
                '''avg_n = alpha * avg_n + (1 - alpha) * grad * grad;
                   avg_g = alpha * avg_g + (1 - alpha) * grad;
                   delta = delta * momentum -
                       lr * grad * rsqrt(avg_n - avg_g * avg_g + eps);
                   param += delta;''',
                'rmsprop_graves')
        RMSpropGravesRule._kernel(
            grad, hp.lr, hp.alpha, hp.momentum, hp.eps, param.data,
            self.state['n'], self.state['g'], self.state['delta'])


</source>
<source file="systems/chainer-7.2.0/chainer/optimizers/ada_delta.py" startline="74" endline="91" pcid="2770">
    def update_core_gpu(self, param):
        grad = param.grad
        if grad is None:
            return
        if AdaDeltaRule._kernel is None:
            AdaDeltaRule._kernel = cuda.elementwise(
                'T grad, T one_minus_rho, T eps',
                'T param, T msg, T msdx',
                '''msg   = msg + one_minus_rho * (grad * grad - msg);
                   T dx  = sqrt((msdx + eps) / (msg + eps)) * grad;
                   msdx  += one_minus_rho * (dx * dx - msdx);
                   param -= dx;''',
                'adadelta')
        AdaDeltaRule._kernel(
            grad, 1 - self.hyperparam.rho, self.hyperparam.eps, param.data,
            self.state['msg'], self.state['msdx'])


</source>
</class>

<class classid="65" nclones="3" nlines="12" similarity="78">
<source file="systems/chainer-7.2.0/chainer/optimizers/rmsprop_graves.py" startline="52" endline="64" pcid="2750">
    def __init__(self, parent_hyperparam=None,
                 lr=None, alpha=None, momentum=None, eps=None):
        super(RMSpropGravesRule, self).__init__(
            parent_hyperparam or _default_hyperparam)
        if lr is not None:
            self.hyperparam.lr = lr
        if alpha is not None:
            self.hyperparam.alpha = alpha
        if momentum is not None:
            self.hyperparam.momentum = momentum
        if eps is not None:
            self.hyperparam.eps = eps

</source>
<source file="systems/chainer-7.2.0/chainer/optimizers/msvag.py" startline="59" endline="74" pcid="2785">
    def __init__(self, parent_hyperparam=None,
                 lr=None, beta=None,
                 eta=None, weight_decay_rate=None):
        super(MSVAGRule, self).__init__(
            parent_hyperparam or _default_hyperparam)
        if lr is not None:
            self.hyperparam.lr = lr
        if beta is not None:
            self.hyperparam.beta = beta
        if eta is not None:
            self.hyperparam.eta = eta
        if weight_decay_rate is not None:
            self.hyperparam.weight_decay_rate = weight_decay_rate

        self.beta_power = self.hyperparam.beta

</source>
<source file="systems/chainer-7.2.0/chainer/optimizers/rmsprop.py" startline="55" endline="67" pcid="2756">
    def __init__(self, parent_hyperparam=None, lr=None, alpha=None, eps=None,
                 eps_inside_sqrt=None):
        super(RMSpropRule, self).__init__(
            parent_hyperparam or _default_hyperparam)
        if lr is not None:
            self.hyperparam.lr = lr
        if alpha is not None:
            self.hyperparam.alpha = alpha
        if eps is not None:
            self.hyperparam.eps = eps
        if eps_inside_sqrt is not None:
            self.hyperparam.eps_inside_sqrt = eps_inside_sqrt

</source>
</class>

<class classid="66" nclones="2" nlines="18" similarity="78">
<source file="systems/chainer-7.2.0/chainer/iterators/multithread_iterator.py" startline="45" endline="70" pcid="2821">
    def __init__(self, dataset, batch_size, repeat=True, shuffle=None,
                 n_threads=1, order_sampler=None):
        self.dataset = dataset
        self.batch_size = batch_size
        self._repeat = repeat
        self._shuffle = shuffle

        if self._shuffle is not None:
            if order_sampler is not None:
                raise ValueError('`shuffle` is not `None` and a custom '
                                 '`order_sampler` is set. Please set '
                                 '`shuffle` to `None` to use the custom '
                                 'order sampler.')
            else:
                if self._shuffle:
                    order_sampler = ShuffleOrderSampler()
        else:
            if order_sampler is None:
                order_sampler = ShuffleOrderSampler()
        self.order_sampler = order_sampler

        self.n_threads = n_threads
        self._pool = None

        self.reset()

</source>
<source file="systems/chainer-7.2.0/chainer/iterators/serial_iterator.py" startline="46" endline="68" pcid="2890">
    def __init__(self, dataset, batch_size,
                 repeat=True, shuffle=None, order_sampler=None):
        self.dataset = dataset
        self.batch_size = batch_size
        self._repeat = repeat
        self._shuffle = shuffle

        if self._shuffle is not None:
            if order_sampler is not None:
                raise ValueError('`shuffle` is not `None` and a custom '
                                 '`order_sampler` is set. Please set '
                                 '`shuffle` to `None` to use the custom '
                                 'order sampler.')
            else:
                if self._shuffle:
                    order_sampler = ShuffleOrderSampler()
        else:
            if order_sampler is None:
                order_sampler = ShuffleOrderSampler()
        self.order_sampler = order_sampler

        self.reset()

</source>
</class>

<class classid="67" nclones="3" nlines="14" similarity="80">
<source file="systems/chainer-7.2.0/chainer/iterators/dali_iterator.py" startline="69" endline="85" pcid="2841">
    def serialize(self, serializer):
        self.current_position = serializer('current_position',
                                           self.current_position)
        self.epoch = serializer('epoch', self.epoch)
        self.is_new_epoch = serializer('is_new_epoch', self.is_new_epoch)
        try:
            self._previous_epoch_detail = serializer(
                'previous_epoch_detail', self._previous_epoch_detail)
        except KeyError:
            # guess previous_epoch_detail for older version
            self._previous_epoch_detail = self.epoch + \
                (self.current_position - self.batch_size) / self.epoch_size
            if self.epoch_detail > 0:
                self._previous_epoch_detail = max(
                    self._previous_epoch_detail, 0.)
            else:
                self._previous_epoch_detail = -1.
</source>
<source file="systems/chainer-7.2.0/examples/static_graph_optimizations/ptb/train_ptb_custom_loop.py" startline="160" endline="176" pcid="10495">
    def serialize(self, serializer):
        # It is important to serialize the state to be recovered on resume.
        self.iteration = serializer('iteration', self.iteration)
        self.epoch = serializer('epoch', self.epoch)
        try:
            self._previous_epoch_detail = serializer(
                'previous_epoch_detail', self._previous_epoch_detail)
        except KeyError:
            # guess previous_epoch_detail for older version
            self._previous_epoch_detail = self.epoch + \
                (self.current_position - self.batch_size) / len(self.dataset)
            if self.epoch_detail > 0:
                self._previous_epoch_detail = max(
                    self._previous_epoch_detail, 0.)
            else:
                self._previous_epoch_detail = -1.

</source>
<source file="systems/chainer-7.2.0/examples/ptb/train_ptb.py" startline="122" endline="139" pcid="10319">
    def serialize(self, serializer):
        # It is important to serialize the state to be recovered on resume.
        self.iteration = serializer('iteration', self.iteration)
        self.epoch = serializer('epoch', self.epoch)
        try:
            self._previous_epoch_detail = serializer(
                'previous_epoch_detail', self._previous_epoch_detail)
        except KeyError:
            # guess previous_epoch_detail for older version
            self._previous_epoch_detail = self.epoch + \
                (self.current_position - self.batch_size) / len(self.dataset)
            if self.epoch_detail > 0:
                self._previous_epoch_detail = max(
                    self._previous_epoch_detail, 0.)
            else:
                self._previous_epoch_detail = -1.


</source>
</class>

<class classid="68" nclones="2" nlines="22" similarity="82">
<source file="systems/chainer-7.2.0/chainer/iterators/multiprocess_iterator.py" startline="216" endline="238" pcid="2855">
    def serialize(self, serializer):
        current_position = serializer('current_position',
                                      self.current_position)
        epoch = serializer('epoch', self.epoch)
        is_new_epoch = serializer('is_new_epoch', self.is_new_epoch)
        order = self._state.order.copy()
        try:
            serializer('order', order)
        except KeyError:
            serializer('_order', order)
        self._reset_state(current_position, epoch, is_new_epoch, order)
        try:
            self._previous_epoch_detail = serializer(
                'previous_epoch_detail', self._previous_epoch_detail)
        except KeyError:
            # guess previous_epoch_detail for older version
            self._previous_epoch_detail = self.epoch + \
                (self.current_position - self.batch_size) / self._epoch_size
            if self.epoch_detail > 0:
                self._previous_epoch_detail = max(
                    self._previous_epoch_detail, 0.)
            else:
                self._previous_epoch_detail = -1.
</source>
<source file="systems/chainer-7.2.0/chainer/iterators/serial_iterator.py" startline="105" endline="129" pcid="2897">
    def serialize(self, serializer):
        current_position = serializer('current_position',
                                      self.current_position)
        epoch = serializer('epoch', self.epoch)
        is_new_epoch = serializer('is_new_epoch', self.is_new_epoch)
        order = self._state.order
        if order is not None:
            try:
                serializer('order', order)
            except KeyError:
                serializer('_order', order)
        self._state = _statemachine.IteratorState(
            current_position, epoch, is_new_epoch, order)
        try:
            self._previous_epoch_detail = serializer(
                'previous_epoch_detail', self._previous_epoch_detail)
        except KeyError:
            # guess previous_epoch_detail for older version
            self._previous_epoch_detail = self.epoch + \
                (self.current_position - self.batch_size) / self._epoch_size
            if self.epoch_detail > 0:
                self._previous_epoch_detail = max(
                    self._previous_epoch_detail, 0.)
            else:
                self._previous_epoch_detail = -1.
</source>
</class>

<class classid="69" nclones="2" nlines="38" similarity="76">
<source file="systems/chainer-7.2.0/chainer/testing/function_link.py" startline="224" endline="267" pcid="3021">
    def run_test_backward(self, backend_config):
        # Runs the backward test.

        if self.skip_backward_test:
            raise unittest.SkipTest('skip_backward_test is set')

        # avoid cyclic import
        from chainer import gradient_check

        self.backend_config = backend_config
        self.test_name = 'test_backward'
        self.before_test(self.test_name)

        def f(*args):
            return self._forward(args, backend_config)

        def do_check():
            inputs = self._generate_inputs()
            outputs = self._forward_expected(inputs)
            grad_outputs = self._generate_grad_outputs(outputs)

            inputs = backend_config.get_array(inputs)
            grad_outputs = backend_config.get_array(grad_outputs)
            inputs = self._to_noncontiguous_as_needed(inputs)
            grad_outputs = self._to_noncontiguous_as_needed(grad_outputs)

            with FunctionTestError.raise_if_fail(
                    'backward is not implemented correctly'):
                gradient_check.check_backward(
                    f, inputs, grad_outputs, dtype=self.numerical_grad_dtype,
                    detect_nondifferentiable=self.dodge_nondifferentiable,
                    **self.check_backward_options)

        if self.dodge_nondifferentiable:
            while True:
                try:
                    do_check()
                except gradient_check.NondifferentiableError:
                    continue
                else:
                    break
        else:
            do_check()

</source>
<source file="systems/chainer-7.2.0/chainer/testing/function_link.py" startline="268" endline="326" pcid="3024">
    def run_test_double_backward(self, backend_config):
        # Runs the double-backward test.

        if self.skip_double_backward_test:
            raise unittest.SkipTest('skip_double_backward_test is set')

        # avoid cyclic import
        from chainer import gradient_check

        self.backend_config = backend_config
        self.test_name = 'test_double_backward'
        self.before_test(self.test_name)

        def f(*args):
            return self._forward(args, backend_config)

        def do_check():
            inputs = self._generate_inputs()
            outputs = self._forward_expected(inputs)
            grad_outputs = self._generate_grad_outputs(outputs)
            grad_grad_inputs = self._generate_grad_grad_inputs(inputs)

            # Drop ggx corresponding to non-differentiable inputs.
            # Generated `grad_grad_inputs`, the upstream gradients for the
            # double backward test, may contain `None` for omitted gradients.
            # These must be propagated to the gradient check.
            grad_grad_inputs = [
                ggx for ggx in grad_grad_inputs
                if (ggx is None or ggx.dtype.kind == 'f')]

            inputs = backend_config.get_array(inputs)
            grad_outputs = backend_config.get_array(grad_outputs)
            grad_grad_inputs = backend_config.get_array(grad_grad_inputs)
            inputs = self._to_noncontiguous_as_needed(inputs)
            grad_outputs = self._to_noncontiguous_as_needed(grad_outputs)
            grad_grad_inputs = (
                self._to_noncontiguous_as_needed(grad_grad_inputs))

            with backend_config:
                with FunctionTestError.raise_if_fail(
                        'double backward is not implemented correctly'):
                    gradient_check.check_double_backward(
                        f, inputs, grad_outputs, grad_grad_inputs,
                        dtype=self.numerical_grad_dtype,
                        detect_nondifferentiable=self.dodge_nondifferentiable,
                        **self.check_double_backward_options)

        if self.dodge_nondifferentiable:
            while True:
                try:
                    do_check()
                except gradient_check.NondifferentiableError:
                    continue
                else:
                    break
        else:
            do_check()


</source>
</class>

<class classid="70" nclones="2" nlines="26" similarity="70">
<source file="systems/chainer-7.2.0/chainer/links/connection/dilated_convolution_2d.py" startline="94" endline="120" pcid="3268">
    def __init__(self, in_channels, out_channels, ksize=None, stride=1, pad=0,
                 dilate=1, nobias=False, initialW=None, initial_bias=None):
        super(DilatedConvolution2D, self).__init__()

        if ksize is None:
            out_channels, ksize, in_channels = in_channels, out_channels, None

        self.ksize = ksize
        self.stride = _pair(stride)
        self.pad = _pair(pad)
        self.dilate = _pair(dilate)
        self.out_channels = out_channels

        with self.init_scope():
            W_initializer = initializers._get_initializer(initialW)
            self.W = variable.Parameter(W_initializer)
            if in_channels is not None:
                self._initialize_params(in_channels)

            if nobias:
                self.b = None
            else:
                if initial_bias is None:
                    initial_bias = 0
                initial_bias = initializers._get_initializer(initial_bias)
                self.b = variable.Parameter(initial_bias, out_channels)

</source>
<source file="systems/chainer-7.2.0/chainer/links/connection/deconvolution_2d.py" startline="129" endline="166" pcid="3307">
    def __init__(self, in_channels, out_channels, ksize=None, stride=1, pad=0,
                 nobias=False, outsize=None, initialW=None, initial_bias=None,
                 **kwargs):
        super(Deconvolution2D, self).__init__()

        dilate, groups, = argument.parse_kwargs(
            kwargs, ('dilate', 1), ('groups', 1),
            deterministic='deterministic argument is not supported anymore. '
            'Use chainer.using_config(\'cudnn_deterministic\', value) '
            'context where value is either `True` or `False`.')

        if ksize is None:
            out_channels, ksize, in_channels = in_channels, out_channels, None

        self.ksize = ksize
        self.stride = _pair(stride)
        self.pad = _pair(pad)
        self.dilate = _pair(dilate)
        self.outsize = (None, None) if outsize is None else outsize
        self.out_channels = out_channels
        self.groups = int(groups)

        with self.init_scope():
            W_initializer = initializers._get_initializer(initialW)
            self.W = variable.Parameter(W_initializer)
            if in_channels is not None:
                self._initialize_params(in_channels)

            if nobias:
                self.b = None
            else:
                if isinstance(initial_bias, (numpy.ndarray, cuda.ndarray)):
                    assert initial_bias.shape == (out_channels,)
                if initial_bias is None:
                    initial_bias = 0
                bias_initializer = initializers._get_initializer(initial_bias)
                self.b = variable.Parameter(bias_initializer, out_channels)

</source>
</class>

<class classid="71" nclones="2" nlines="22" similarity="95">
<source file="systems/chainer-7.2.0/chainer/links/connection/deformable_convolution_2d.py" startline="88" endline="115" pcid="3281">
    def __init__(self, in_channels, out_channels, ksize, stride=1, pad=0,
                 nobias=False, initialW=None, initial_bias=None):
        super(DeformableConvolution2DSampler, self).__init__()

        self.ksize = ksize
        self.stride = _pair(stride)
        self.pad = _pair(pad)
        self.out_channels = out_channels
        self.initialW = initialW

        if initialW is None:
            initialW = constant.Zero()

        with self.init_scope():
            W_initializer = initializers._get_initializer(initialW)
            self.W = variable.Parameter(W_initializer)

            if nobias:
                self.b = None
            else:
                if initial_bias is None:
                    initial_bias = initializers.Constant(0)
                bias_initializer = initializers._get_initializer(initial_bias)
                self.b = variable.Parameter(bias_initializer)

        if in_channels is not None:
            self._initialize_params(in_channels)

</source>
<source file="systems/chainer-7.2.0/chainer/links/connection/depthwise_convolution_2d.py" startline="45" endline="71" pcid="3311">
    def __init__(self, in_channels, channel_multiplier, ksize, stride=1, pad=0,
                 nobias=False, initialW=None, initial_bias=None):
        super(DepthwiseConvolution2D, self).__init__()
        self.ksize = ksize
        self.stride = _pair(stride)
        self.pad = _pair(pad)
        self.channel_multiplier = channel_multiplier
        self.nobias = nobias

        if initialW is None:
            initialW = initializers.HeNormal(1. / numpy.sqrt(2))

        with self.init_scope():
            W_initializer = initializers._get_initializer(initialW)
            self.W = variable.Parameter(W_initializer)

            if nobias:
                self.b = None
            else:
                if initial_bias is None:
                    initial_bias = initializers.Constant(0)
                bias_initializer = initializers._get_initializer(initial_bias)
                self.b = variable.Parameter(bias_initializer)

        if in_channels is not None:
            self._initialize_params(in_channels)

</source>
</class>

<class classid="72" nclones="2" nlines="19" similarity="94">
<source file="systems/chainer-7.2.0/chainer/links/caffe/caffe_function.py" startline="243" endline="265" pcid="3366">
    def _setup_convolution(self, layer):
        blobs = layer.blobs
        param = layer.convolution_param
        ksize = _get_ksize(param)
        stride = _get_stride(param)
        pad = _get_pad(param)
        num = _get_num(blobs[0])
        channels = _get_channels(blobs[0])
        bias_term = param.bias_term

        n_in = channels * param.group
        n_out = num

        func = convolution_2d.Convolution2D(
            n_in, n_out, ksize, stride, pad, nobias=not bias_term,
            initialW=_ConvolutionBlob(blobs[0], param.group),
            initial_bias=_Blob(blobs[1]) if bias_term else None)

        with self.init_scope():
            setattr(self, layer.name, func)
        self.forwards[layer.name] = _CallChildLink(self, layer.name)
        self._add_layer(layer)

</source>
<source file="systems/chainer-7.2.0/chainer/links/caffe/caffe_function.py" startline="267" endline="289" pcid="3367">
    def _setup_deconvolution(self, layer):
        blobs = layer.blobs
        param = layer.convolution_param
        ksize = _get_ksize(param)
        stride = _get_stride(param)
        pad = _get_pad(param)
        num = _get_num(blobs[0])
        channels = _get_channels(blobs[0])
        bias_term = param.bias_term

        n_in = num
        n_out = channels * param.group

        func = deconvolution_2d.Deconvolution2D(
            n_in, n_out, ksize, stride, pad, nobias=not bias_term,
            initialW=_ConvolutionBlob(blobs[0], param.group),
            initial_bias=_Blob(blobs[1]) if bias_term else None)

        with self.init_scope():
            setattr(self, layer.name, func)
        self.forwards[layer.name] = _CallChildLink(self, layer.name)
        self._add_layer(layer)

</source>
</class>

<class classid="73" nclones="2" nlines="11" similarity="90">
<source file="systems/chainer-7.2.0/chainer/links/caffe/caffe_function.py" startline="541" endline="553" pcid="3385">
def _get_stride(param):
    if param.stride_h > 0:
        return param.stride_h, param.stride_w
    elif type(param.stride) == int:
        return param.stride
    elif len(param.stride) == 0:
        return 1
    elif len(param.stride) == 1:
        return param.stride[0]
    else:
        return param.stride


</source>
<source file="systems/chainer-7.2.0/chainer/links/caffe/caffe_function.py" startline="554" endline="566" pcid="3386">
def _get_pad(param):
    if param.pad_h > 0 or param.pad_w > 0:
        return param.pad_h, param.pad_w
    elif type(param.pad) == int:
        return param.pad
    elif len(param.pad) == 0:
        return 0
    elif len(param.pad) == 1:
        return param.pad[0]
    else:
        return param.pad


</source>
</class>

<class classid="74" nclones="2" nlines="11" similarity="100">
<source file="systems/chainer-7.2.0/chainer/links/caffe/caffe_function.py" startline="581" endline="593" pcid="3389">
def _get_height(blob):
    if blob.height > 0:
        return blob.height
    elif len(blob.shape.dim) == 2:
        return blob.shape.dim[0]
    elif len(blob.shape.dim) == 4:
        return blob.shape.dim[2]
    else:
        raise RuntimeError(
            '{}-dimensional array is not supported'.format(
                len(blob.shape.dim)))


</source>
<source file="systems/chainer-7.2.0/chainer/links/caffe/caffe_function.py" startline="594" endline="609" pcid="3390">
def _get_width(blob):
    if blob.width > 0:
        return blob.width
    elif len(blob.shape.dim) == 2:
        return blob.shape.dim[1]
    elif len(blob.shape.dim) == 4:
        return blob.shape.dim[3]
    else:
        raise RuntimeError(
            '{}-dimensional array is not supported'.format(
                len(blob.shape.dim)))


# Internal class
# __call__ must return Variable or tuple

</source>
</class>

<class classid="75" nclones="2" nlines="18" similarity="94">
<source file="systems/chainer-7.2.0/chainerx_cc/examples/mnist_py/train_mnist.py" startline="51" endline="74" pcid="3455">
def evaluate(model, X_test, Y_test, eval_size, batch_size):
    N_test = X_test.shape[0] if eval_size is None else eval_size

    if N_test > X_test.shape[0]:
        raise ValueError(
            'Test size can be no larger than {}'.format(X_test.shape[0]))

    with chx.no_backprop_mode():
        total_loss = chx.array(0, dtype=chx.float32)
        num_correct = chx.array(0, dtype=chx.int64)
        for i in range(0, N_test, batch_size):
            x = X_test[i:min(i + batch_size, N_test)]
            t = Y_test[i:min(i + batch_size, N_test)]

            y = model.forward(x)
            total_loss += chx.softmax_cross_entropy(y, t).sum()
            num_correct += (y.argmax(axis=1).astype(t.dtype)
                            == t).astype(chx.int32).sum()

    mean_loss = float(total_loss) / N_test
    accuracy = int(num_correct) / N_test
    return mean_loss, accuracy


</source>
<source file="systems/chainer-7.2.0/chainerx_cc/examples/imagenet_py/train_imagenet.py" startline="29" endline="52" pcid="3459">
def evaluate(model, X_test, Y_test, eval_size, batch_size):
    N_test = X_test.shape[0] if eval_size is None else eval_size

    if N_test > X_test.shape[0]:
        raise ValueError(
            'Test size can be no larger than {}'.format(X_test.shape[0]))

    with chx.no_backprop_mode():
        total_loss = chx.array(0, dtype=chx.float32)
        num_correct = chx.array(0, dtype=chx.int64)
        for i in range(0, N_test, batch_size):
            x = X_test[i:min(i + batch_size, N_test)]
            t = Y_test[i:min(i + batch_size, N_test)]

            y = model(x)
            total_loss += chx.softmax_cross_entropy(y, t).sum()
            num_correct += (y.argmax(axis=1).astype(t.dtype)
                            == t).astype(chx.int32).sum()

    mean_loss = float(total_loss) / N_test
    accuracy = int(num_correct) / N_test
    return mean_loss, accuracy


</source>
</class>

<class classid="76" nclones="3" nlines="18" similarity="100">
<source file="systems/chainer-7.2.0/chainerx_cc/examples/imagenet_py/image_dataset.py" startline="18" endline="45" pcid="3498">
    def get_example(self, i):
        # It reads the i-th image/label pair and return a preprocessed image.
        # It applies following preprocesses:
        #     - Cropping (random or center rectangular)
        #     - Random flip
        #     - Scaling to [0, 1] value
        crop_size = self.crop_size

        image, label = self.base[i]
        _, h, w = image.shape

        if self.random:
            # Randomly crop a region and flip the image
            top = random.randint(0, h - crop_size - 1)
            left = random.randint(0, w - crop_size - 1)
            if random.randint(0, 1):
                image = image[:, :, ::-1]
        else:
            # Crop the center
            top = (h - crop_size) // 2
            left = (w - crop_size) // 2
        bottom = top + crop_size
        right = left + crop_size

        image = image[:, top:bottom, left:right]
        image -= self.mean[:, top:bottom, left:right]
        image *= (1.0 / 255.0)  # Scale to [0, 1]
        return image, label
</source>
<source file="systems/chainer-7.2.0/examples/chainermn/imagenet/train_imagenet.py" startline="49" endline="80" pcid="10436">
    def get_example(self, i):
        # It reads the i-th image/label pair and return a preprocessed image.
        # It applies following preprocesses:
        #     - Cropping (random or center rectangular)
        #     - Random flip
        #     - Scaling to [0, 1] value
        crop_size = self.crop_size

        image, label = self.base[i]
        _, h, w = image.shape

        if self.random:
            # Randomly crop a region and flip the image
            top = random.randint(0, h - crop_size - 1)
            left = random.randint(0, w - crop_size - 1)
            if random.randint(0, 1):
                image = image[:, :, ::-1]
        else:
            # Crop the center
            top = (h - crop_size) // 2
            left = (w - crop_size) // 2
        bottom = top + crop_size
        right = left + crop_size

        image = image[:, top:bottom, left:right]
        image -= self.mean[:, top:bottom, left:right]
        image *= (1.0 / 255.0)  # Scale to [0, 1]
        return image, label


# chainermn.create_multi_node_evaluator can be also used with user customized
# evaluator classes that inherit chainer.training.extensions.Evaluator.
</source>
<source file="systems/chainer-7.2.0/examples/imagenet/dataset_util.py" startline="19" endline="46" pcid="10576">
    def get_example(self, i):
        # It reads the i-th image/label pair and return a preprocessed image.
        # It applies following preprocesses:
        #     - Cropping (random or center rectangular)
        #     - Random flip
        #     - Scaling to [0, 1] value
        crop_size = self.crop_size

        image, label = self.base[i]
        _, h, w = image.shape

        if self.random:
            # Randomly crop a region and flip the image
            top = random.randint(0, h - crop_size - 1)
            left = random.randint(0, w - crop_size - 1)
            if random.randint(0, 1):
                image = image[:, :, ::-1]
        else:
            # Crop the center
            top = (h - crop_size) // 2
            left = (w - crop_size) // 2
        bottom = top + crop_size
        right = left + crop_size

        image = image[:, top:bottom, left:right]
        image -= self.mean[:, top:bottom, left:right]
        image *= (1.0 / 255.0)  # Scale to [0, 1]
        return image, label
</source>
</class>

<class classid="77" nclones="2" nlines="13" similarity="76">
<source file="systems/chainer-7.2.0/tests/onnx_chainer_tests/functions_tests/test_normalizations.py" startline="28" endline="45" pcid="3553">
    def setUp(self):

        class Model(chainer.Chain):

            def __init__(self, ops, args, input_argname):
                super(Model, self).__init__()
                self.ops = ops
                self.args = args
                self.input_argname = input_argname

            def __call__(self, x):
                self.args[self.input_argname] = x
                return self.ops(**self.args)

        ops = getattr(F, self.name)
        self.model = Model(ops, self.args, self.input_argname)
        self.x = input_generator.increasing(2, 5, 3, 3)

</source>
<source file="systems/chainer-7.2.0/tests/onnx_chainer_tests/functions_tests/test_arrays.py" startline="349" endline="369" pcid="3594">
    def setUp(self):

        class Model(chainer.Chain):

            def __init__(self, ops, args, input_argname):
                super(Model, self).__init__()
                self.ops = ops
                self.args = args
                self.input_argname = input_argname

            def __call__(self, x):
                self.args[self.input_argname] = x
                return self.ops(**self.args)

        # (batch, channel, height, width) = (1, 1, 2, 2)
        self.x = np.array([[[[64, 32], [64, 32]]]], np.float32)

        # 2x upsampling
        args = {'output_shape': (4, 4)}
        self.model = Model(F.resize_images, args, 'x')

</source>
</class>

<class classid="78" nclones="2" nlines="10" similarity="70">
<source file="systems/chainer-7.2.0/tests/onnx_chainer_tests/functions_tests/test_normalizations.py" startline="64" endline="78" pcid="3557">
    def setUp(self):

        class Model(chainer.Chain):

            def __init__(self, **kwargs):
                super(Model, self).__init__()
                with self.init_scope():
                    self.bn = L.BatchNormalization(5, **kwargs)

            def __call__(self, x):
                return self.bn(x)

        self.model = Model(**self.kwargs)
        self.x = input_generator.increasing(2, 5)

</source>
<source file="systems/chainer-7.2.0/tests/onnx_chainer_tests/functions_tests/test_activations.py" startline="59" endline="73" pcid="3622">
    def setUp(self):

        class Model(chainer.Chain):

            def __init__(self):
                super(Model, self).__init__()
                with self.init_scope():
                    self.prelu = L.PReLU()

            def __call__(self, x):
                return self.prelu(x)

        self.model = Model()
        self.x = input_generator.increasing(2, 5)

</source>
</class>

<class classid="79" nclones="2" nlines="10" similarity="80">
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link_hook.py" startline="188" endline="202" pcid="3655">
    def test_global_hook_delete(self):
        # Deleted hook should not be called

        model, x, dot = self._create_model_and_data()
        hook = MyLinkHook()

        with hook:
            pass
        model(chainer.Variable(x), 'foo', test2='bar')

        assert len(hook.added_args) == 1
        assert len(hook.deleted_args) == 1
        assert len(hook.forward_preprocess_args) == 0
        assert len(hook.forward_postprocess_args) == 0

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link_hook.py" startline="203" endline="218" pcid="3656">
    def test_local_hook_delete(self):
        # Deleted hook should not be called

        model, x, dot = self._create_model_and_data()
        hook = MyLinkHook()

        model.add_hook(hook)
        model.delete_hook('MyLinkHook')
        model(chainer.Variable(x), 'foo', test2='bar')

        assert len(hook.added_args) == 1
        assert len(hook.deleted_args) == 1
        assert len(hook.forward_preprocess_args) == 0
        assert len(hook.forward_postprocess_args) == 0


</source>
</class>

<class classid="80" nclones="3" nlines="16" similarity="72">
<source file="systems/chainer-7.2.0/tests/chainer_tests/optimizer_hooks_tests/test_weight_decay.py" startline="37" endline="58" pcid="3663">
    def check_weight_decay(self, backend_configs):
        target = self.target
        assert len(backend_configs) == len(list(target.params()))
        devices = [bc.device for bc in backend_configs]

        decay = 0.2

        # Compute expected
        expects = []
        for param, device in zip(target.params(), devices):
            expects.append(param.array - param.grad - decay * param.array)
            param.to_device(device)

        opt = optimizers.SGD(lr=1)
        opt.setup(self.target)
        opt.add_hook(optimizer_hooks.WeightDecay(decay))
        opt.update()

        # Validate
        for expect, param in zip(expects, target.params()):
            testing.assert_allclose(expect, param.array)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/optimizer_hooks_tests/test_gradient_hard_clipping.py" startline="35" endline="59" pcid="3680">
    def check_hardclipping(self, backend_configs):
        target = self.target
        assert len(backend_configs) == len(list(target.params()))
        devices = [bc.device for bc in backend_configs]

        lower_bound = -0.9
        upper_bound = 1.1
        expects = []
        # Compute expected
        for param, device in zip(target.params(), devices):
            expects.append(param.array - np.clip(param.grad,
                                                 lower_bound, upper_bound))
            param.to_device(device)

        # Apply optimizer_hook
        opt = optimizers.SGD(lr=1)
        opt.setup(self.target)
        opt.add_hook(
            optimizer_hooks.GradientHardClipping(lower_bound, upper_bound))
        opt.update()

        # Validate
        for expect, param in zip(expects, target.params()):
            testing.assert_allclose(expect, param.array)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/optimizer_hooks_tests/test_lasso.py" startline="34" endline="57" pcid="3677">
    def check_lasso(self, backend_configs):
        target = self.target
        assert len(backend_configs) == len(list(target.params()))
        devices = [bc.device for bc in backend_configs]

        decay = 0.2

        expects = []
        # Compute expected
        for param, device in zip(target.params(), devices):
            expects.append(param.array - param.grad -
                           decay * np.sign(param.array))
            param.to_device(device)

        # Compute using optimizer_hook
        opt = optimizers.SGD(lr=1)
        opt.setup(target)
        opt.add_hook(optimizer_hooks.Lasso(decay))
        opt.update()

        # Validate
        for expect, param in zip(expects, target.params()):
            testing.assert_allclose(expect, param.array)

</source>
</class>

<class classid="81" nclones="2" nlines="15" similarity="100">
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/caffe_tests/test_caffe_function.py" startline="166" endline="184" pcid="3691">
    def test_convolution(self):
        self.init_func()
        self.assertEqual(len(self.func.layers), 1)
        f = self.func.l1
        self.assertIsInstance(f, links.Convolution2D)
        for i in range(3):  # 3 == group
            in_slice = slice(i * 4, (i + 1) * 4)  # 4 == channels
            out_slice = slice(i * 2, (i + 1) * 2)  # 2 == num / group
            w = f.W.data[out_slice, in_slice]
            numpy.testing.assert_array_equal(
                w.flatten(), range(i * 32, (i + 1) * 32))

        numpy.testing.assert_array_equal(
            f.b.data, range(6))

        self.call(['x'], ['y'])
        self.mock.assert_called_once_with(self.inputs[0])


</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/caffe_tests/test_caffe_function.py" startline="219" endline="237" pcid="3692">
    def test_deconvolution(self):
        self.init_func()
        self.assertEqual(len(self.func.layers), 1)
        f = self.func.l1
        self.assertIsInstance(f, links.Deconvolution2D)
        for i in range(3):  # 3 == group
            in_slice = slice(i * 4, (i + 1) * 4)  # 4 == channels
            out_slice = slice(i * 2, (i + 1) * 2)  # 2 == num / group
            w = f.W.data[out_slice, in_slice]
            numpy.testing.assert_array_equal(
                w.flatten(), range(i * 32, (i + 1) * 32))

        numpy.testing.assert_array_equal(
            f.b.data, range(12))

        self.call(['x'], ['y'])
        self.mock.assert_called_once_with(self.inputs[0])


</source>
</class>

<class classid="82" nclones="2" nlines="13" similarity="100">
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/rnn_tests/test_link_peephole.py" startline="171" endline="184" pcid="3777">
    def check_to_cpu(self, c, h):
        self.link.c = c
        self.link.h = h
        with testing.assert_warns(DeprecationWarning):
            self.link.to_cpu()
        self.assertIs(self.link.xp, numpy)
        self.assertIsInstance(self.link.c.data, self.link.xp.ndarray)
        self.assertIsInstance(self.link.h.data, self.link.xp.ndarray)
        with testing.assert_warns(DeprecationWarning):
            self.link.to_cpu()
        self.assertIs(self.link.xp, numpy)
        self.assertIsInstance(self.link.c.data, self.link.xp.ndarray)
        self.assertIsInstance(self.link.h.data, self.link.xp.ndarray)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_zoneoutlstm.py" startline="174" endline="187" pcid="4093">

    def check_to_cpu(self, c, h):
        self.link.c = c
        self.link.h = h
        with testing.assert_warns(DeprecationWarning):
            self.link.to_cpu()
        self.assertIs(self.link.xp, numpy)
        self.assertIsInstance(self.link.c.data, self.link.xp.ndarray)
        self.assertIsInstance(self.link.h.data, self.link.xp.ndarray)
        with testing.assert_warns(DeprecationWarning):
            self.link.to_cpu()
        self.assertIs(self.link.xp, numpy)
        self.assertIsInstance(self.link.c.data, self.link.xp.ndarray)
        self.assertIsInstance(self.link.h.data, self.link.xp.ndarray)
</source>
</class>

<class classid="83" nclones="2" nlines="23" similarity="100">
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/rnn_tests/test_link_peephole.py" startline="194" endline="217" pcid="3780">
    def check_to_cpu_to_gpu(self, c, h):
        self.link.c = c
        self.link.h = h
        with testing.assert_warns(DeprecationWarning):
            self.link.to_gpu()
        self.assertIs(self.link.xp, cuda.cupy)
        self.assertIsInstance(self.link.c.data, self.link.xp.ndarray)
        self.assertIsInstance(self.link.h.data, self.link.xp.ndarray)
        with testing.assert_warns(DeprecationWarning):
            self.link.to_gpu()
        self.assertIs(self.link.xp, cuda.cupy)
        self.assertIsInstance(self.link.c.data, self.link.xp.ndarray)
        self.assertIsInstance(self.link.h.data, self.link.xp.ndarray)
        with testing.assert_warns(DeprecationWarning):
            self.link.to_cpu()
        self.assertIs(self.link.xp, numpy)
        self.assertIsInstance(self.link.c.data, self.link.xp.ndarray)
        self.assertIsInstance(self.link.h.data, self.link.xp.ndarray)
        with testing.assert_warns(DeprecationWarning):
            self.link.to_gpu()
        self.assertIs(self.link.xp, cuda.cupy)
        self.assertIsInstance(self.link.c.data, self.link.xp.ndarray)
        self.assertIsInstance(self.link.h.data, self.link.xp.ndarray)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_zoneoutlstm.py" startline="197" endline="220" pcid="4096">

    def check_to_cpu_to_gpu(self, c, h):
        self.link.c = c
        self.link.h = h
        with testing.assert_warns(DeprecationWarning):
            self.link.to_gpu()
        self.assertIs(self.link.xp, cuda.cupy)
        self.assertIsInstance(self.link.c.data, self.link.xp.ndarray)
        self.assertIsInstance(self.link.h.data, self.link.xp.ndarray)
        with testing.assert_warns(DeprecationWarning):
            self.link.to_gpu()
        self.assertIs(self.link.xp, cuda.cupy)
        self.assertIsInstance(self.link.c.data, self.link.xp.ndarray)
        self.assertIsInstance(self.link.h.data, self.link.xp.ndarray)
        with testing.assert_warns(DeprecationWarning):
            self.link.to_cpu()
        self.assertIs(self.link.xp, numpy)
        self.assertIsInstance(self.link.c.data, self.link.xp.ndarray)
        self.assertIsInstance(self.link.h.data, self.link.xp.ndarray)
        with testing.assert_warns(DeprecationWarning):
            self.link.to_gpu()
        self.assertIs(self.link.xp, cuda.cupy)
        self.assertIsInstance(self.link.c.data, self.link.xp.ndarray)
        self.assertIsInstance(self.link.h.data, self.link.xp.ndarray)
</source>
</class>

<class classid="84" nclones="2" nlines="13" similarity="73">
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/rnn_tests/test_link_lstm.py" startline="31" endline="46" pcid="3793">
    def setUp(self):
        if self.input_none:
            if self.input_omit:
                self.link = links.LSTM(self.out_size)
            else:
                self.link = links.LSTM(None, self.out_size)
        else:
            self.link = links.LSTM(self.in_size, self.out_size)
        self.link.cleargrads()
        x1_shape = (4, self.in_size)
        self.x1 = numpy.random.uniform(-1, 1, x1_shape).astype(numpy.float32)
        x2_shape = (3, self.in_size)
        self.x2 = numpy.random.uniform(-1, 1, x2_shape).astype(numpy.float32)
        x3_shape = (0, self.in_size)
        self.x3 = numpy.random.uniform(-1, 1, x3_shape).astype(numpy.float32)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/rnn_tests/test_link_lstm.py" startline="294" endline="306" pcid="3822">
class TestStatelessLSTM(unittest.TestCase):

    def setUp(self):
        if self.input_none:
            if self.input_omit:
                self.link = links.StatelessLSTM(self.out_size)
            else:
                self.link = links.StatelessLSTM(None, self.out_size)
        else:
            self.link = links.StatelessLSTM(self.in_size, self.out_size)
        self.link.cleargrads()

        x_shape = (4, self.in_size)
</source>
</class>

<class classid="85" nclones="2" nlines="11" similarity="81">
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/normalization_tests/test_batch_renormalization.py" startline="159" endline="177" pcid="3852">
    def check_statistics2(self, x, y):
        x = chainer.Variable(x)
        y = chainer.Variable(y)
        self.link(x, finetune=True)
        self.link(y, finetune=True)
        mean = (self.x.sum(axis=0) + self.y.sum(axis=0)) / (self.nx + self.ny)
        var = (self.x.var(axis=0) * self.nx +
               self.y.var(axis=0) * self.ny) / (self.nx + self.ny)

        # TODO(Kenta Oono)
        # Fix the estimate of the unbiased variance.
        # Unbiased variance should be (nx + ny) / (nx + ny - 1) times of
        # the variance.
        # But the multiplier is ny / (ny - 1) in current implementation
        # these two values are different when nx is not equal to ny.
        unbiased_var = var * self.ny / (self.ny - 1)
        testing.assert_allclose(self.link.avg_mean, mean)
        testing.assert_allclose(self.link.avg_var, unbiased_var)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/normalization_tests/test_batch_normalization.py" startline="307" endline="325" pcid="3871">
    def check_statistics2(self, x, y):
        x = chainer.Variable(x)
        y = chainer.Variable(y)
        self.link(x, finetune=True)
        self.link(y, finetune=True)
        mean = (self.x.sum(axis=0) + self.y.sum(axis=0)) / (self.nx + self.ny)
        var = (self.x.var(axis=0) * self.nx +
               self.y.var(axis=0) * self.ny) / (self.nx + self.ny)

        # TODO(Kenta Oono)
        # Fix the estimate of the unbiased variance.
        # Unbiased variance should be (nx + ny) / (nx + ny - 1) times of
        # the variance.
        # But the multiplier is ny / (ny - 1) in current implementation
        # these two values are different when nx is not equal to ny.
        unbiased_var = var * self.ny / (self.ny - 1)
        testing.assert_allclose(mean, self.link.avg_mean)
        testing.assert_allclose(unbiased_var, self.link.avg_var)

</source>
</class>

<class classid="86" nclones="2" nlines="17" similarity="82">
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/normalization_tests/test_batch_normalization.py" startline="753" endline="773" pcid="3918">
    def test_forward(self, backend_config):
        with chainer.using_config('compute_mode', 'cudnn_fast'):
            link = self.create_link()
        link.to_device(backend_config.device)

        x = self.create_input_array(backend_config.xp)
        x = chainer.Variable(x, layout=memory_layouts.CUDNN_CHANNEL_LAST_X)
        x.to_device(backend_config.device)
        with backend_config:
            y = link(x)

        assert link.gamma.device == backend_config.device
        assert link.beta.device == backend_config.device
        assert y.layout == memory_layouts.CUDNN_CHANNEL_LAST_X
        assert y.shape == (
            self.batch,
            self.channels,
            self.height,
            self.width)


</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_convolution_2d.py" startline="285" endline="304" pcid="4220">
    def test_forward(self, backend_config):
        with chainer.using_config('compute_mode', 'cudnn_fast'):
            link = self.create_link()
        link.to_device(backend_config.device)

        x = self.create_input_array(backend_config.xp)
        x = chainer.Variable(x, layout=memory_layouts.CUDNN_CHANNEL_LAST_X)
        x.to_device(backend_config.device)
        with backend_config:
            y = link(x)

        assert link.W.device == backend_config.device
        assert y.layout == memory_layouts.CUDNN_CHANNEL_LAST_X
        assert y.shape == (
            self.batch,
            self.out_channels,
            (self.height - self.kernel_height + 1) // self.strides_height,
            (self.width - self.kernel_width + 1) // self.strides_width)


</source>
</class>

<class classid="87" nclones="2" nlines="13" similarity="71">
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/normalization_tests/test_group_normalization.py" startline="63" endline="78" pcid="3922">
    def generate_inputs(self):
        shape = self.shape

        # sample x such that x.std >= min_std
        min_std = 0.02
        retry = 0
        while True:
            x = numpy.random.uniform(-1, 1, shape).astype(self.dtype)
            x_groups = x.reshape(shape[0], self.groups, -1)
            if x_groups.std(axis=2).min() >= min_std:
                break
            retry += 1
            assert retry <= 20, 'Too many retries to generate inputs'

        return x,

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/normalization_tests/test_group_normalization.py" startline="64" endline="81" pcid="6708">
    def generate_inputs(self):
        shape = self.shape

        # sample x such that x.std >= min_std
        min_std = 0.2 if self.dtype == numpy.float16 else 0.02
        retry = 0
        while True:
            x = numpy.random.uniform(-1, 1, shape).astype(self.dtype)
            x_groups = x.reshape(shape[0], self.groups, -1)
            if x_groups.std(axis=2).min() >= min_std:
                break
            retry += 1
            assert retry <= 20, 'Too many retries to generate inputs'

        gamma = numpy.random.uniform(-1, 1, shape[1]).astype(self.dtype)
        beta = numpy.random.uniform(-1, 1, shape[1]).astype(self.dtype)
        return x, gamma, beta

</source>
</class>

<class classid="88" nclones="2" nlines="14" similarity="100">
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/normalization_tests/test_decorrelated_batch_normalization.py" startline="43" endline="59" pcid="3939">
def _calc_projection_1group(x, mean, eps):
    spatial_ndim = len(x.shape[2:])
    spatial_axis = tuple(range(2, 2 + spatial_ndim))
    b, C = x.shape[:2]
    m = b
    for i in spatial_axis:
        m *= x.shape[i]

    x_hat = x.transpose((1, 0) + spatial_axis).reshape(C, -1)
    mean = x_hat.mean(axis=1)
    x_hat = x_hat - mean[:, None]
    cov = x_hat.dot(x_hat.T) / m + eps * numpy.eye(C, dtype=x.dtype)
    eigvals, eigvectors = numpy.linalg.eigh(cov)
    projection = eigvectors.dot(numpy.diag(eigvals ** -0.5)).dot(eigvectors.T)
    return projection


</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/normalization_tests/test_decorrelated_batch_normalization.py" startline="37" endline="53" pcid="6717">
def _calc_projection_1group(x, mean, eps):
    spatial_ndim = len(x.shape[2:])
    spatial_axis = tuple(range(2, 2 + spatial_ndim))
    b, C = x.shape[:2]
    m = b
    for i in spatial_axis:
        m *= x.shape[i]

    x_hat = x.transpose((1, 0) + spatial_axis).reshape(C, -1)
    mean = x_hat.mean(axis=1)
    x_hat = x_hat - mean[:, None]
    cov = x_hat.dot(x_hat.T) / m + eps * numpy.eye(C, dtype=x.dtype)
    eigvals, eigvectors = numpy.linalg.eigh(cov)
    projection = eigvectors.dot(numpy.diag(eigvals ** -0.5)).dot(eigvectors.T)
    return projection


</source>
</class>

<class classid="89" nclones="2" nlines="22" similarity="100">
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/normalization_tests/test_decorrelated_batch_normalization.py" startline="121" endline="156" pcid="3944">
    def generate_inputs(self):
        dtype = self.dtype
        ndim = self.ndim
        shape = (5, self.n_channels) + (2,) * ndim
        m = 5 * 2 ** ndim

        # NOTE(kataoka): The current implementation uses linalg.eigh. Small
        # eigenvalues of the correlation matrix, which can be as small as
        # eps=2e-5, cannot be computed with good *relative* accuracy, but
        # the eigenvalues are used later as `eigvals ** -0.5`. Require the
        # following is sufficiently large:
        # min(eigvals[:k]) == min(singular_vals ** 2 / m + eps)
        min_singular_value = 0.1
        # NOTE(kataoka): Decorrelated batch normalization should be free from
        # "stochastic axis swapping". Requiring a gap between singular values
        # just hides mistakes in implementations.
        min_singular_value_gap = 0.001
        g = self.groups
        zca_shape = g, self.n_channels // g, m
        x = numpy.random.uniform(-1, 1, zca_shape)
        mean = x.mean(axis=2, keepdims=True)
        a = x - mean
        u, s, vh = numpy.linalg.svd(a, full_matrices=False)
        # Decrement the latter dim because of the constraint `sum(_) == 0`
        k = min(zca_shape[1], zca_shape[2] - 1)
        s[:, :k] += (
            min_singular_value
            + min_singular_value_gap * numpy.arange(k)
        )[::-1]
        a = numpy.einsum('bij,bj,bjk->bik', u, s, vh)
        x = a + mean

        x = x.reshape((self.n_channels, shape[0]) + shape[2:]).swapaxes(0, 1)
        x = x.astype(dtype)
        return x,

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/normalization_tests/test_decorrelated_batch_normalization.py" startline="101" endline="136" pcid="6720">
    def generate_inputs(self):
        dtype = self.dtype
        ndim = self.ndim
        shape = (5, self.n_channels) + (2,) * ndim
        m = 5 * 2 ** ndim

        # NOTE(kataoka): The current implementation uses linalg.eigh. Small
        # eigenvalues of the correlation matrix, which can be as small as
        # eps=2e-5, cannot be computed with good *relative* accuracy, but
        # the eigenvalues are used later as `eigvals ** -0.5`. Require the
        # following is sufficiently large:
        # min(eigvals[:k]) == min(singular_vals ** 2 / m + eps)
        min_singular_value = 0.1
        # NOTE(kataoka): Decorrelated batch normalization should be free from
        # "stochastic axis swapping". Requiring a gap between singular values
        # just hides mistakes in implementations.
        min_singular_value_gap = 0.001
        g = self.groups
        zca_shape = g, self.n_channels // g, m
        x = numpy.random.uniform(-1, 1, zca_shape)
        mean = x.mean(axis=2, keepdims=True)
        a = x - mean
        u, s, vh = numpy.linalg.svd(a, full_matrices=False)
        # Decrement the latter dim because of the constraint `sum(_) == 0`
        k = min(zca_shape[1], zca_shape[2] - 1)
        s[:, :k] += (
            min_singular_value
            + min_singular_value_gap * numpy.arange(k)
        )[::-1]
        a = numpy.einsum('bij,bj,bjk->bik', u, s, vh)
        x = a + mean

        x = x.reshape((self.n_channels, shape[0]) + shape[2:]).swapaxes(0, 1)
        x = x.astype(dtype)
        return x,

</source>
</class>

<class classid="90" nclones="3" nlines="21" similarity="76">
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/model_tests/test_vision.py" startline="74" endline="96" pcid="4001">
    def test_prepare(self):
        x1 = numpy.random.uniform(0, 255, (320, 240, 3)).astype(numpy.uint8)
        x2 = numpy.random.uniform(0, 255, (320, 240)).astype(numpy.uint8)
        x3 = numpy.random.uniform(0, 255, (160, 120, 3)).astype(self.dtype)
        x4 = numpy.random.uniform(0, 255, (1, 160, 120)).astype(self.dtype)
        x5 = numpy.random.uniform(0, 255, (3, 160, 120)).astype(numpy.uint8)

        y1 = resnet.prepare(x1)
        assert y1.shape == (3, 224, 224)
        assert y1.dtype == self.dtype
        y2 = resnet.prepare(x2)
        assert y2.shape == (3, 224, 224)
        assert y2.dtype == self.dtype
        y3 = resnet.prepare(x3, size=None)
        assert y3.shape == (3, 160, 120)
        assert y3.dtype == self.dtype
        y4 = resnet.prepare(x4)
        assert y4.shape == (3, 224, 224)
        assert y4.dtype == self.dtype
        y5 = resnet.prepare(x5, size=None)
        assert y5.shape == (3, 160, 120)
        assert y5.dtype == self.dtype

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/model_tests/test_vision.py" startline="216" endline="238" pcid="4017">
    def test_prepare(self):
        x1 = numpy.random.uniform(0, 255, (320, 240, 3)).astype(numpy.uint8)
        x2 = numpy.random.uniform(0, 255, (320, 240)).astype(numpy.uint8)
        x3 = numpy.random.uniform(0, 255, (160, 120, 3)).astype(self.dtype)
        x4 = numpy.random.uniform(0, 255, (1, 160, 120)).astype(self.dtype)
        x5 = numpy.random.uniform(0, 255, (3, 160, 120)).astype(numpy.uint8)

        y1 = vgg.prepare(x1)
        assert y1.shape == (3, 224, 224)
        assert y1.dtype == self.dtype
        y2 = vgg.prepare(x2)
        assert y2.shape == (3, 224, 224)
        assert y2.dtype == self.dtype
        y3 = vgg.prepare(x3, size=None)
        assert y3.shape == (3, 160, 120)
        assert y3.dtype == self.dtype
        y4 = vgg.prepare(x4)
        assert y4.shape == (3, 224, 224)
        assert y4.dtype == self.dtype
        y5 = vgg.prepare(x5, size=None)
        assert y5.shape == (3, 160, 120)
        assert y5.dtype == self.dtype

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/model_tests/test_vision.py" startline="360" endline="382" pcid="4035">
    def test_prepare(self):
        x1 = numpy.random.uniform(0, 255, (320, 240, 3)).astype(numpy.uint8)
        x2 = numpy.random.uniform(0, 255, (320, 240)).astype(numpy.uint8)
        x3 = numpy.random.uniform(0, 255, (160, 120, 3)).astype(self.dtype)
        x4 = numpy.random.uniform(0, 255, (1, 160, 120)).astype(self.dtype)
        x5 = numpy.random.uniform(0, 255, (3, 160, 120)).astype(numpy.uint8)

        y1 = googlenet.prepare(x1)
        assert y1.shape == (3, 224, 224)
        assert y1.dtype, self.dtype
        y2 = googlenet.prepare(x2)
        assert y2.shape == (3, 224, 224)
        assert y2.dtype, self.dtype
        y3 = googlenet.prepare(x3, size=None)
        assert y3.shape == (3, 160, 120)
        assert y3.dtype, self.dtype
        y4 = googlenet.prepare(x4)
        assert y4.shape == (3, 224, 224)
        assert y4.dtype, self.dtype
        y5 = googlenet.prepare(x5, size=None)
        assert y5.shape == (3, 160, 120)
        assert y5.dtype, self.dtype

</source>
</class>

<class classid="91" nclones="3" nlines="18" similarity="94">
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/model_tests/test_vision.py" startline="97" endline="117" pcid="4002">
    def check_extract(self):
        x1 = numpy.random.uniform(0, 255, (320, 240, 3)).astype(numpy.uint8)
        x2 = numpy.random.uniform(0, 255, (320, 240)).astype(numpy.uint8)

        with numpy.errstate(divide='ignore'):
            result = self.link.extract([x1, x2], layers=['res3', 'pool5'])
            assert len(result) == 2
            y1 = cuda.to_cpu(result['res3'].data)
            assert y1.shape == (2, 512, 28, 28)
            assert y1.dtype == self.dtype
            y2 = cuda.to_cpu(result['pool5'].data)
            assert y2.shape == (2, 2048)
            assert y2.dtype == self.dtype

            x3 = numpy.random.uniform(0, 255, (80, 60)).astype(numpy.uint8)
            result = self.link.extract([x3], layers=['res2'], size=None)
            assert len(result) == 1
            y3 = cuda.to_cpu(result['res2'].data)
            assert y3.shape == (1, 256, 20, 15)
            assert y3.dtype == self.dtype

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/model_tests/test_vision.py" startline="383" endline="402" pcid="4036">
    def check_extract(self):
        x1 = numpy.random.uniform(0, 255, (320, 240, 3)).astype(numpy.uint8)
        x2 = numpy.random.uniform(0, 255, (320, 240)).astype(numpy.uint8)

        result = self.link.extract([x1, x2], layers=['pool5', 'loss3_fc'])
        assert len(result) == 2
        y1 = cuda.to_cpu(result['pool5'].data)
        assert y1.shape == (2, 1024, 1, 1)
        assert y1.dtype == self.dtype
        y2 = cuda.to_cpu(result['loss3_fc'].data)
        assert y2.shape == (2, 1000)
        assert y2.dtype == self.dtype

        x3 = numpy.random.uniform(0, 255, (80, 60)).astype(numpy.uint8)
        result = self.link.extract([x3], layers=['pool1'], size=None)
        assert len(result) == 1
        y3 = cuda.to_cpu(result['pool1'].data)
        assert y3.shape == (1, 64, 20, 15)
        assert y3.dtype == self.dtype

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/model_tests/test_vision.py" startline="239" endline="257" pcid="4018">
    def check_extract(self):
        x1 = numpy.random.uniform(0, 255, (320, 240, 3)).astype(numpy.uint8)
        x2 = numpy.random.uniform(0, 255, (320, 240)).astype(numpy.uint8)
        result = self.link.extract([x1, x2], layers=['pool3', 'fc7'])
        assert len(result) == 2
        y1 = cuda.to_cpu(result['pool3'].data)
        assert y1.shape == (2, 256, 28, 28)
        assert y1.dtype == self.dtype
        y2 = cuda.to_cpu(result['fc7'].data)
        assert y2.shape == (2, 4096)
        assert y2.dtype == self.dtype

        x3 = numpy.random.uniform(0, 255, (80, 60)).astype(numpy.uint8)
        result = self.link.extract([x3], layers=['pool1'], size=None)
        assert len(result) == 1
        y3 = cuda.to_cpu(result['pool1'].data)
        assert y3.shape == (1, 64, 40, 30)
        assert y3.dtype == self.dtype

</source>
</class>

<class classid="92" nclones="3" nlines="12" similarity="91">
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/model_tests/test_vision.py" startline="129" endline="142" pcid="4005">
    def check_predict(self):
        x1 = numpy.random.uniform(0, 255, (320, 240, 3)).astype(numpy.uint8)
        x2 = numpy.random.uniform(0, 255, (320, 240)).astype(numpy.uint8)

        with numpy.errstate(divide='ignore'):
            result = self.link.predict([x1, x2], oversample=False)
            y = cuda.to_cpu(result.data)
            assert y.shape == (2, 1000)
            assert y.dtype == self.dtype
            result = self.link.predict([x1, x2], oversample=True)
            y = cuda.to_cpu(result.data)
            assert y.shape == (2, 1000)
            assert y.dtype == self.dtype

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/model_tests/test_vision.py" startline="414" endline="426" pcid="4039">
    def check_predict(self):
        x1 = numpy.random.uniform(0, 255, (320, 240, 3)).astype(numpy.uint8)
        x2 = numpy.random.uniform(0, 255, (320, 240)).astype(numpy.uint8)

        result = self.link.predict([x1, x2], oversample=False)
        y = cuda.to_cpu(result.data)
        assert y.shape == (2, 1000)
        assert y.dtype == self.dtype
        result = self.link.predict([x1, x2], oversample=True)
        y = cuda.to_cpu(result.data)
        assert y.shape == (2, 1000)
        assert y.dtype == self.dtype

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/model_tests/test_vision.py" startline="267" endline="278" pcid="4021">
    def check_predict(self):
        x1 = numpy.random.uniform(0, 255, (320, 240, 3)).astype(numpy.uint8)
        x2 = numpy.random.uniform(0, 255, (320, 240)).astype(numpy.uint8)
        result = self.link.predict([x1, x2], oversample=False)
        y = cuda.to_cpu(result.data)
        assert y.shape == (2, 1000)
        assert y.dtype == self.dtype
        result = self.link.predict([x1, x2], oversample=True)
        y = cuda.to_cpu(result.data)
        assert y.shape == (2, 1000)
        assert y.dtype == self.dtype

</source>
</class>

<class classid="93" nclones="2" nlines="13" similarity="76">
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_mlp_convolution_2d.py" startline="154" endline="168" pcid="4114">
    def test_valid_instantiation_ksize_is_none(self):
        l = links.MLPConvolution2D(self.out_channels, self.ksize, None,
                                   self.stride, self.pad, functions.relu,
                                   conv_init=None, bias_init=None)
        x = numpy.random.uniform(
            -1, 1, (10, self.in_channels, 10, 10)).astype(numpy.float32)
        l(x)  # create weight tensors of convolutions by initialization

        self.assertEqual(len(l), 2)
        self.assertEqual(l[0].W.shape,
                         (self.out_channels[0], self.in_channels,
                          self.ksize, self.ksize))
        self.assertEqual(l[1].W.shape,
                         (self.out_channels[1], self.out_channels[0], 1, 1))

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_mlp_convolution_2d.py" startline="169" endline="183" pcid="4115">
    def test_valid_instantiation_in_channels_is_omitted(self):
        l = links.MLPConvolution2D(
            self.out_channels, self.ksize, stride=self.stride, pad=self.pad,
            activation=functions.relu, conv_init=None, bias_init=None)
        x = numpy.random.uniform(
            -1, 1, (10, self.in_channels, 10, 10)).astype(numpy.float32)
        l(x)  # create weight tensors of convolutions by initialization

        self.assertEqual(len(l), 2)
        self.assertEqual(l[0].W.shape,
                         (self.out_channels[0], self.in_channels,
                          self.ksize, self.ksize))
        self.assertEqual(l[1].W.shape,
                         (self.out_channels[1], self.out_channels[0], 1, 1))

</source>
</class>

<class classid="94" nclones="2" nlines="12" similarity="83">
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_linear.py" startline="146" endline="160" pcid="4136">
    def test_serialization(self):
        lin1 = links.Linear(self.out_size)
        x = chainer.Variable(self.x)
        # Must call the link to initialize weights.
        lin1(x)
        w1 = lin1.W.data
        fd, temp_file_path = tempfile.mkstemp()
        os.close(fd)
        npz.save_npz(temp_file_path, lin1)
        lin2 = links.Linear(self.out_size)
        npz.load_npz(temp_file_path, lin2)
        w2 = lin2.W.data
        self.assertEqual((w1 == w2).all(), True)


</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/activation_tests/test_simplified_dropconnect.py" startline="180" endline="194" pcid="4357">
    def test_serialization(self):
        lin1 = links.SimplifiedDropconnect(None, self.out_size)
        x = chainer.Variable(self.x)
        # Must call the link to initialize weights.
        lin1(x)
        w1 = lin1.W.data
        fd, temp_file_path = tempfile.mkstemp()
        os.close(fd)
        npz.save_npz(temp_file_path, lin1)
        lin2 = links.SimplifiedDropconnect(None, self.out_size)
        npz.load_npz(temp_file_path, lin2)
        w2 = lin2.W.data
        self.assertEqual((w1 == w2).all(), True)


</source>
</class>

<class classid="95" nclones="2" nlines="10" similarity="100">
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_scale.py" startline="100" endline="111" pcid="4194">
    def test_backward_gpu(self):
        with testing.assert_warns(DeprecationWarning):
            self.link.to_gpu()
        x = cuda.to_gpu(self.x)
        if self.learn_W:
            W = None
        else:
            W = cuda.to_gpu(self.W)
        gy = cuda.to_gpu(self.gy)
        self.check_backward(x, W, gy)


</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_bias.py" startline="86" endline="97" pcid="4318">
    def test_backward_gpu(self):
        with testing.assert_warns(DeprecationWarning):
            self.link.to_gpu()
        x = cuda.to_gpu(self.x)
        if self.learn_b:
            b = None
        else:
            b = cuda.to_gpu(self.b)
        gy = cuda.to_gpu(self.gy)
        self.check_backward(x, b, gy)


</source>
</class>

<class classid="96" nclones="3" nlines="17" similarity="100">
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_convolution_2d.py" startline="94" endline="115" pcid="4205">
    def test_pickling(self, backend_config):
        x_data, = self.generate_inputs()

        link = self.create_link(self.generate_params())
        link.to_device(backend_config.device)

        x = chainer.Variable(x_data)
        x.to_device(backend_config.device)

        y = link(x)
        y_data1 = y.data
        del x, y
        pickled = pickle.dumps(link, -1)
        del link
        link = pickle.loads(pickled)
        x = chainer.Variable(x_data)
        x.to_device(backend_config.device)
        y = link(x)
        y_data2 = y.data

        testing.assert_allclose(y_data1, y_data2, atol=0, rtol=0)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_convolution_2d.py" startline="213" endline="235" pcid="4215">
    def test_pickling(self, backend_config):
        x_data, = self.generate_inputs()

        link = self.create_link(self.generate_params())
        link.to_device(backend_config.device)

        x = chainer.Variable(x_data)
        x.to_device(backend_config.device)

        y = link(x)
        y_data1 = y.data
        del x, y
        pickled = pickle.dumps(link, -1)
        del link
        link = pickle.loads(pickled)
        x = chainer.Variable(x_data)
        x.to_device(backend_config.device)
        y = link(x)
        y_data2 = y.data

        testing.assert_allclose(y_data1, y_data2, atol=0, rtol=0)


</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_convolution_nd.py" startline="106" endline="127" pcid="4267">
    def test_pickling(self, backend_config):
        x_data, = self.generate_inputs()

        link = self.create_link(self.generate_params())
        link.to_device(backend_config.device)

        x = chainer.Variable(x_data)
        x.to_device(backend_config.device)

        y = link(x)
        y_data1 = y.data
        del x, y
        pickled = pickle.dumps(link, -1)
        del link
        link = pickle.loads(pickled)
        x = chainer.Variable(x_data)
        x.to_device(backend_config.device)
        y = link(x)
        y_data2 = y.data

        testing.assert_allclose(y_data1, y_data2, atol=0, rtol=0)

</source>
</class>

<class classid="97" nclones="2" nlines="17" similarity="77">
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_convolution_2d.py" startline="116" endline="132" pcid="4206">
    def test_from_params(self, backend_config):
        if (
                (backend_config.use_cuda and
                 backend_config.cuda_device == 1) or
                (backend_config.use_chainerx and
                 'cuda' in backend_config.chainerx_device)):
            raise unittest.SkipTest()
        link1 = self.create_link(self.generate_params())
        link1.to_device(backend_config.device)
        link2 = links.Convolution2D.from_params(
            link1.W, link1.b, stride=self.stride, pad=self.pad)
        assert link2.W.shape == link1.W.shape
        assert link2.b.shape == link2.b.shape
        assert link2.stride == link1.stride
        assert link2.pad == link1.pad


</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_convolution_nd.py" startline="128" endline="149" pcid="4268">
    def test_from_params(self, backend_config):
        if (
                (backend_config.use_cuda and
                 backend_config.cuda_device == 1) or
                (backend_config.use_chainerx and
                 'cuda' in backend_config.chainerx_device)):
            raise unittest.SkipTest()
        link1 = self.create_link(self.generate_params())
        link1.to_device(backend_config.device)

        if self.in_channels in (None, 'omit'):
            link1._initialize_params(self.x_shape[1])

        link2 = convolution_nd.ConvolutionND.from_params(
            link1.W, link1.b,
            stride=self.stride, pad=self.pad, groups=self.groups)
        assert link2.W.shape == link1.W.shape
        assert link2.b.shape == link1.b.shape
        assert link2.stride == link1.stride
        assert link2.pad == link1.pad


</source>
</class>

<class classid="98" nclones="2" nlines="10" similarity="100">
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_dilated_convolution_2d.py" startline="45" endline="57" pcid="4225">
    def check_forward_consistency(self):
        x_cpu = chainer.Variable(self.x)
        y_cpu = self.link(x_cpu)
        self.assertEqual(y_cpu.data.dtype, numpy.float32)

        with testing.assert_warns(DeprecationWarning):
            self.link.to_gpu()
        x_gpu = chainer.Variable(cuda.to_gpu(self.x))
        y_gpu = self.link(x_gpu)
        self.assertEqual(y_gpu.data.dtype, numpy.float32)

        testing.assert_allclose(y_cpu.data, y_gpu.data.get())

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_dilated_convolution_2d.py" startline="147" endline="159" pcid="4238">
    def check_forward_consistency(self):
        x_cpu = chainer.Variable(self.x)
        y_cpu = self.link(x_cpu)
        self.assertEqual(y_cpu.data.dtype, numpy.float32)

        with testing.assert_warns(DeprecationWarning):
            self.link.to_gpu()
        x_gpu = chainer.Variable(cuda.to_gpu(self.x))
        y_gpu = self.link(x_gpu)
        self.assertEqual(y_gpu.data.dtype, numpy.float32)

        testing.assert_allclose(y_cpu.data, y_gpu.data.get())

</source>
</class>

<class classid="99" nclones="2" nlines="12" similarity="100">
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_dilated_convolution_2d.py" startline="88" endline="104" pcid="4232">
    def check_pickling(self, x_data):
        x = chainer.Variable(x_data)
        y = self.link(x)
        y_data1 = y.data

        del x, y

        pickled = pickle.dumps(self.link, -1)
        del self.link
        self.link = pickle.loads(pickled)

        x = chainer.Variable(x_data)
        y = self.link(x)
        y_data2 = y.data

        testing.assert_allclose(y_data1, y_data2, atol=0, rtol=0)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_dilated_convolution_2d.py" startline="190" endline="206" pcid="4245">
    def check_pickling(self, x_data):
        x = chainer.Variable(x_data)
        y = self.link(x)
        y_data1 = y.data

        del x, y

        pickled = pickle.dumps(self.link, -1)
        del self.link
        self.link = pickle.loads(pickled)

        x = chainer.Variable(x_data)
        y = self.link(x)
        y_data2 = y.data

        testing.assert_allclose(y_data1, y_data2, atol=0, rtol=0)

</source>
</class>

<class classid="100" nclones="3" nlines="11" similarity="75">
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_dilated_convolution_2d.py" startline="120" endline="130" pcid="4235">
    def setUp(self):
        self.link = links.DilatedConvolution2D(*self.args, **self.kwargs)
        self.x = numpy.random.uniform(-1, 1,
                                      (2, 3, 4, 3)).astype(numpy.float32)
        self.link(chainer.Variable(self.x))
        b = self.link.b.data
        b[...] = numpy.random.uniform(-1, 1, b.shape)
        self.link.cleargrads()
        self.gy = numpy.random.uniform(-1, 1,
                                       (2, 2, 2, 2)).astype(numpy.float32)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_local_convolution_2d.py" startline="55" endline="67" pcid="4307">
    def setUp(self):
        in_channels = None
        self.link = links.LocalConvolution2D(in_channels, 2, ksize=3,
                                             stride=1)
        self.x = numpy.random.uniform(-1, 1,
                                      (2, 3, 4, 4)).astype(numpy.float32)
        self.link(chainer.Variable(self.x))
        b = self.link.b.data
        b[...] = numpy.random.uniform(-1, 1, b.shape)
        self.link.cleargrads()
        self.gy = numpy.random.uniform(-1, 1,
                                       (2, 2, 2, 2)).astype(numpy.float32)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_depthwise_convolution_2d.py" startline="55" endline="67" pcid="4326">
    def setUp(self):
        in_channels = None
        self.link = links.DepthwiseConvolution2D(in_channels, 2, 3,
                                                 stride=2, pad=1)
        self.x = numpy.random.uniform(-1, 1,
                                      (2, 3, 4, 3)).astype(numpy.float32)
        self.link(chainer.Variable(self.x))
        b = self.link.b.data
        b[...] = numpy.random.uniform(-1, 1, b.shape)
        self.link.cleargrads()
        self.gy = numpy.random.uniform(-1, 1,
                                       (2, 6, 2, 2)).astype(numpy.float32)

</source>
</class>

<class classid="101" nclones="2" nlines="13" similarity="92">
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_local_convolution_2d.py" startline="21" endline="35" pcid="4303">
    def setUp(self):
        self.link = links.LocalConvolution2D(
            3, 2, in_size=4, ksize=3, stride=1,
            initialW=chainer.initializers.Normal(1, self.W_dtype),
            initial_bias=chainer.initializers.Normal(1, self.x_dtype))
        self.link.cleargrads()

        self.x = numpy.random.uniform(-1, 1,
                                      (2, 3, 4, 4)).astype(self.x_dtype)
        self.gy = numpy.random.uniform(-1, 1,
                                       (2, 2, 2, 2)).astype(self.x_dtype)
        self.check_backward_options = {}
        if self.x_dtype == numpy.float16 or self.W_dtype == numpy.float16:
            self.check_backward_options = {'atol': 3e-2, 'rtol': 5e-2}

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/links_tests/connection_tests/test_depthwise_convolution_2d.py" startline="21" endline="35" pcid="4322">
    def setUp(self):
        self.link = links.DepthwiseConvolution2D(
            3, 2, 3, stride=2, pad=1,
            initialW=chainer.initializers.Normal(1, self.W_dtype),
            initial_bias=chainer.initializers.Normal(1, self.x_dtype))
        self.link.cleargrads()

        self.x = numpy.random.uniform(-1, 1,
                                      (2, 3, 4, 3)).astype(self.x_dtype)
        self.gy = numpy.random.uniform(-1, 1,
                                       (2, 6, 2, 2)).astype(self.x_dtype)
        self.check_backward_options = {}
        if self.x_dtype == numpy.float16 or self.W_dtype == numpy.float16:
            self.check_backward_options = {'atol': 3e-2, 'rtol': 5e-2}

</source>
</class>

<class classid="102" nclones="2" nlines="13" similarity="76">
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_backprop.py" startline="20" endline="33" pcid="4559">
    def check_multiple_output_1arg(self, xp, skip_retain_grad_test=False):
        x = chainer.Variable(xp.array([1, 2], np.float32))
        h = x * 2
        y0 = h * 3
        y1 = h * 4
        y0.grad = xp.array([1, 10], np.float32)
        y1.grad = xp.array([100, 1000], np.float32)
        chainer.backward([y0, y1])
        testing.assert_allclose(x.grad, np.array([806, 8060], np.float32))
        if skip_retain_grad_test:
            return
        assert y0.grad is None
        assert y1.grad is None

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_backprop.py" startline="34" endline="47" pcid="4560">
    def check_multiple_output_2args(self, xp, skip_retain_grad_test=False):
        x = chainer.Variable(xp.array([1, 2], np.float32))
        h = x * 2
        y0 = h * 3
        y1 = h * 4
        gy0 = chainer.Variable(xp.array([1, 10], np.float32))
        gy1 = chainer.Variable(xp.array([100, 1000], np.float32))
        chainer.backward([y0, y1], [gy0, gy1])
        testing.assert_allclose(x.grad, np.array([806, 8060], np.float32))
        if skip_retain_grad_test:
            return
        assert y0.grad is None
        assert y1.grad is None

</source>
</class>

<class classid="103" nclones="2" nlines="17" similarity="72">
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_variable.py" startline="704" endline="722" pcid="4660">
    def test_copydata_to_uninitialized_parameter(
            self, src_backend_config, dst_backend_config):
        shape = self.shape
        dtype = np.float32
        src_arr_numpy = np.asarray(np.random.randn(*shape), dtype)
        src_arr = src_backend_config.get_array(src_arr_numpy.copy())
        dst_var = chainer.Parameter()
        dst_var.to_device(dst_backend_config.device)
        src_var = chainer.Parameter(src_arr)
        src_arr_prev = src_var.array

        dst_var.copydata(src_var)

        assert src_var.array is src_arr_prev
        assert src_var.device == src_backend_config.device
        assert dst_var.device == dst_backend_config.device
        np.testing.assert_array_equal(
            _numpy_device.send(dst_var.data), src_arr_numpy)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_variable.py" startline="723" endline="743" pcid="4661">
    def test_copydata_from_uninitialized_parameter(
            self, src_backend_config, dst_backend_config):
        shape = self.shape
        dtype = np.float32
        dst_arr_numpy = np.asarray(np.random.randn(*shape), dtype)
        dst_arr = dst_backend_config.get_array(dst_arr_numpy.copy())
        initializer = initializers.Zero()
        dst_var = chainer.Parameter(dst_arr)
        src_var = chainer.Parameter(initializer)
        src_var.to_device(src_backend_config.device)
        dst_arr_prev = dst_var.array

        dst_var.copydata(src_var)

        assert src_var.device == src_backend_config.device
        assert dst_var.device == dst_backend_config.device
        assert dst_var.array is dst_arr_prev
        np.testing.assert_array_equal(
            _numpy_device.send(dst_var.array),
            _numpy_device.send(src_var.array))

</source>
</class>

<class classid="104" nclones="2" nlines="18" similarity="83">
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_variable.py" startline="768" endline="788" pcid="4663">
    def test_grad(self, backend_config):
        x = backend_config.get_array(
            np.random.uniform(-1, 1, self.shape).astype(np.float32))
        g = backend_config.get_array(
            np.random.uniform(0.1, 10, self.shape).astype(np.float32))
        v = chainer.Variable(x, requires_grad=self.requires_grad)
        expected_error = (
            backend_config.xp is chainerx
            and not self.requires_grad)

        if expected_error:
            with pytest.raises(Exception):
                v.grad = g
        else:
            v.grad = g

            assert v.grad_var.requires_grad is True
            assert v.grad is not None
            assert v.requires_grad == self.requires_grad
            backend_config.xp.testing.assert_array_equal(v.grad, g)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_variable.py" startline="789" endline="811" pcid="4664">
    def check_grad_var(self, backend_config, grad_var_requires_grad):
        x = backend_config.get_array(
            np.random.uniform(-1, 1, self.shape).astype(np.float32))
        g = backend_config.get_array(
            np.random.uniform(0.1, 10, self.shape).astype(np.float32))
        v = chainer.Variable(x, requires_grad=self.requires_grad)
        gv = chainer.Variable(g, requires_grad=grad_var_requires_grad)
        expected_error = (
            backend_config.xp is chainerx
            and not self.requires_grad)

        if expected_error:
            with pytest.raises(Exception):
                v.grad_var = gv
        else:
            v.grad_var = gv

            assert v.requires_grad == self.requires_grad
            backend_config.xp.testing.assert_array_equal(v.grad, g)

            # Same instance should be returned each time.
            assert v.grad_var is gv

</source>
</class>

<class classid="105" nclones="4" nlines="11" similarity="71">
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_variable.py" startline="1914" endline="1925" pcid="4787">
    def test_addgrad_to_uninitialized_parameter_cpu_to_gpu(self):
        x = chainer.Parameter()
        y = chainer.Parameter(self.a)
        y.grad = self.b
        x.to_gpu()
        x.cleargrad()
        x.addgrad(y)
        cp = cuda.cupy
        assert isinstance(x.data, cp.ndarray)
        assert isinstance(x.grad, cp.ndarray)
        cp.testing.assert_array_equal(x.grad, self.b)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_variable.py" startline="1927" endline="1937" pcid="4788">
    def test_addgrad_to_uninitialized_parameter_gpu_to_cpu(self):
        x = chainer.Parameter()
        y = chainer.Parameter(self.a)
        y.grad = self.b
        y.to_gpu()
        x.cleargrad()
        x.addgrad(y)
        assert isinstance(x.data, np.ndarray)
        assert isinstance(x.grad, np.ndarray)
        np.testing.assert_array_equal(x.grad, self.b)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_variable.py" startline="1939" endline="1951" pcid="4789">
    def test_addgrad_to_uninitialized_parameter_gpu_to_gpu(self):
        x = chainer.Parameter()
        y = chainer.Parameter(self.a)
        y.grad = self.b
        x.to_gpu()
        y.to_gpu()
        x.cleargrad()
        x.addgrad(y)
        cp = cuda.cupy
        assert isinstance(x.data, cp.ndarray)
        assert isinstance(x.grad, cp.ndarray)
        cp.testing.assert_array_equal(x.grad, self.b)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_variable.py" startline="1953" endline="1967" pcid="4790">
    def test_addgrad_to_uninitialized_parameter_gpu_to_another_gpu(self):
        x = chainer.Parameter()
        y = chainer.Parameter(self.a)
        y.grad = self.b
        x.to_gpu(1)
        y.to_gpu(0)
        x.cleargrad()
        x.addgrad(y)
        cp = cuda.cupy
        assert isinstance(x.data, cp.ndarray)
        assert isinstance(x.grad, cp.ndarray)
        assert int(x.data.device) == 1
        assert int(x.grad.device) == 1
        cp.testing.assert_array_equal(x.grad, self.b)

</source>
</class>

<class classid="106" nclones="3" nlines="14" similarity="92">
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_variable.py" startline="2084" endline="2102" pcid="4806">
    def check_type_mismatch(self, x_data, retain):
        xp = backend.get_array_module(x_data)

        class DummyFunction(chainer.Function):
            label = 'dummy_function'

            def forward(self, inputs):
                if not retain:
                    self.retain_inputs(())
                return xp.array(1, np.float32),

            def backward(self, inputs, grads):
                return [1]

        x = chainer.Variable(x_data)
        y = DummyFunction()(x)
        with six.assertRaisesRegex(self, TypeError, 'dummy_function'):
            y.backward()

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_variable.py" startline="2117" endline="2135" pcid="4813">
    def check_dtype_mismatch(self, x_data, retain):
        xp = backend.get_array_module(x_data)

        class DummyFunction(chainer.Function):
            label = 'dummy_function'

            def forward(self, inputs):
                if not retain:
                    self.retain_inputs(())
                return xp.array(1, np.float32),

            def backward(self, inputs, grads):
                return xp.array([1], np.int32),

        x = chainer.Variable(x_data)
        y = DummyFunction()(x)
        with six.assertRaisesRegex(self, TypeError, 'dummy_function'):
            y.backward()

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_variable.py" startline="2150" endline="2168" pcid="4820">
    def check_shape_mismatch(self, x_data, retain):
        xp = backend.get_array_module(x_data)

        class DummyFunction(chainer.Function):
            label = 'dummy_function'

            def forward(self, inputs):
                if not retain:
                    self.retain_inputs(())
                return xp.array(1, np.float32),

            def backward(self, inputs, grads):
                return xp.array([1, 2], np.float32),

        x = chainer.Variable(x_data)
        y = DummyFunction()(x)
        with six.assertRaisesRegex(self, ValueError, 'dummy_function'):
            y.backward()

</source>
</class>

<class classid="107" nclones="2" nlines="10" similarity="90">
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_variable.py" startline="2501" endline="2511" pcid="4871">
    def setUp(self):
        x = np.empty((0, 0))
        x = x.astype(np.float32)
        self.x = chainer.Variable(x)
        if (sys.version_info < (3,) and sys.maxsize > 2**32 and
                platform.system() == 'Windows'):
            self.repr = 'variable([], shape=(0L, 0L))'
        else:
            self.repr = 'variable([], shape=(0, 0))'
        self.str = 'variable([])'

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_variable.py" startline="2591" endline="2601" pcid="4881">
    def setUp(self):
        x = np.empty((0, 0))
        x = x.astype(np.float32)
        self.x = chainer.Variable(x, name='x')
        if (sys.version_info < (3,) and sys.maxsize > 2**32 and
                platform.system() == 'Windows'):
            self.repr = 'variable x([], shape=(0L, 0L))'
        else:
            self.repr = 'variable x([], shape=(0, 0))'
        self.str = 'variable x([])'

</source>
</class>

<class classid="108" nclones="2" nlines="10" similarity="70">
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_variable.py" startline="2674" endline="2685" pcid="4893">
    def test_default_backward(self):
        x = chainer.Variable(np.array([42], np.float32))
        y = x * 2  # x.grad_var will be different from y.grad_var
        with testing.assert_warns(DeprecationWarning):
            y.backward(retain_grad=True)
        assert x.grad_var.creator is None
        with warnings.catch_warnings():
            # ok to be warned that x.grad_var is old-styled scalar
            warnings.simplefilter('ignore', DeprecationWarning)
            x.grad_var.backward()
        assert y.grad_var.grad_var is None

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_variable.py" startline="2697" endline="2708" pcid="4895">
    def test_raise_double_backprop_2(self):
        x = chainer.Variable(np.array([42], np.float32))
        z = F.identity(x)  # new style
        y = IdentityFunction()(z)  # old style
        with testing.assert_warns(DeprecationWarning):
            y.backward(enable_double_backprop=True)
        with pytest.raises(RuntimeError):
            with warnings.catch_warnings():
                # ok to be warned that x.grad_var is old-styled scalar
                warnings.simplefilter('ignore', DeprecationWarning)
                x.grad_var.backward()

</source>
</class>

<class classid="109" nclones="2" nlines="17" similarity="83">
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_gradient_check.py" startline="594" endline="615" pcid="5134">
    def test_no_grads_option(self, backend_config):
        if backend_config.use_chainerx:
            raise unittest.SkipTest(
                'gradient_check does not support no_grad option for ChainerX')
        x1 = backend_config.get_array(numpy.array([2], dtype='f'))
        # grad check for this is skipped
        x2 = backend_config.get_array(numpy.array([3], dtype='f'))
        g1 = backend_config.get_array(numpy.array([5], dtype='f'))

        def f(x, y):
            y_array = y.array
            if (backend_config.xp is chainerx
                    and isinstance(y_array, chainerx.ndarray)):
                y_array = y_array.as_grad_stopped()
            s = x + y_array
            return s,

        self.assertRaises(
            RuntimeError,  # backward computes x1.grad
            gradient_check.check_backward,
            f, (x1, x2), g1, no_grads=[True, True])

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_gradient_check.py" startline="616" endline="634" pcid="5136">
    def test_const_input(self, backend_config):
        x1 = backend_config.get_array(numpy.array([2], dtype='f'))
        # grad check for this is skipped
        x2 = backend_config.get_array(numpy.array([3], dtype='f'))
        g1 = backend_config.get_array(numpy.array([5], dtype='f'))

        def f(x, y):
            y_array = y.array
            if (backend_config.xp is chainerx
                    and isinstance(y_array, chainerx.ndarray)):
                y_array = y_array.as_grad_stopped()
            s = x + y_array
            return s,

        self.assertRaises(
            AssertionError,  # numerical backward to x2 is nonzero
            gradient_check.check_backward,
            f, (x1, x2), g1, no_grads=[False, False])

</source>
</class>

<class classid="110" nclones="3" nlines="11" similarity="76">
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_gradient_check.py" startline="699" endline="711" pcid="5144">
    def _broken_func_1(self):
        class Broken(chainer.Function):
            def forward(self, inputs):
                x, = inputs
                return (x * x),

            def backward(self, inputs, grad_outputs):
                x, = inputs
                gy, = grad_outputs
                return 3 * x * gy,

        return Broken()

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_gradient_check.py" startline="712" endline="725" pcid="5147">
    def _broken_func_2(self):
        class Broken(chainer.FunctionNode):
            def forward(self, inputs):
                x, = inputs
                self.retain_inputs((0,))
                return (x * x),

            def backward(self, indexes, grad_outputs):
                x, = self.get_retained_inputs()
                gy, = grad_outputs
                return 3 * x * gy,

        return Broken()

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_gradient_check.py" startline="726" endline="741" pcid="5150">
    def _broken_func_3(self):
        class Broken(chainer.FunctionNode):
            def forward(self, inputs):
                x, = inputs
                self.retain_inputs((0,))
                return (x * x),

            def backward(self, indexes, grad_outputs):
                x, = self.get_retained_inputs()
                gy, = grad_outputs
                gx1 = 2 * x * gy
                gx2 = 3 * x * gy
                return (gx1, gx2)

        return Broken()

</source>
</class>

<class classid="111" nclones="3" nlines="13" similarity="71">
<source file="systems/chainer-7.2.0/tests/chainer_tests/utils_tests/test_cache.py" startline="50" endline="64" pcid="5174">
    def test3(self):
        obj = MockDistribution(chainer.Variable(numpy.array([1.])))
        h0 = obj.h
        with chainer.no_backprop_mode():
            h1 = obj.h
        h2 = obj.h
        with chainer.no_backprop_mode():
            h3 = obj.h
        assert obj.h_call_count <= 2
        assert h0 is h2
        assert h0 is not h1
        assert h1 is h3
        numpy.testing.assert_allclose(h0.array, 2.)
        numpy.testing.assert_allclose(h1.array, 2.)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/utils_tests/test_cache.py" startline="65" endline="77" pcid="5175">
    def test_attrs1(self):
        obj = MockDistribution(chainer.Variable(numpy.array([1.])))
        h0 = obj.h
        y0 = obj.y
        h1 = obj.h
        y1 = obj.y
        assert obj.h_call_count == 1
        assert obj.y_call_count == 1
        assert h0 is h1
        assert y0 is y1
        numpy.testing.assert_allclose(h0.array, 2.)
        numpy.testing.assert_allclose(y0.array, 6.)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/utils_tests/test_cache.py" startline="78" endline="92" pcid="5176">
    def test_objs1(self):
        obj0 = MockDistribution(chainer.Variable(numpy.array([1.])))
        obj1 = MockDistribution(chainer.Variable(numpy.array([10.])))
        y00 = obj0.y
        y10 = obj1.y
        y01 = obj0.y
        y11 = obj1.y
        assert obj0.y_call_count == 1
        assert obj1.y_call_count == 1
        assert y00 is y01
        assert y10 is y11
        numpy.testing.assert_allclose(y00.array, 6.)
        numpy.testing.assert_allclose(y10.array, 60.)


</source>
</class>

<class classid="112" nclones="2" nlines="15" similarity="87">
<source file="systems/chainer-7.2.0/tests/chainer_tests/optimizers_tests/test_optimizers.py" startline="179" endline="195" pcid="5358">
    def test_hooks(self):
        w_pre = np.copy(self.target.w.data)
        h_pre = WeightSaveHook()
        h_post = WeightSaveHook()
        self.create()
        self.optimizer.add_hook(h_pre, timing='pre')
        self.optimizer.add_hook(h_post, name='WeightSaveHookPost',
                                timing='post')

        x = chainer.Variable(np.array(5., dtype=np.float32))
        self.optimizer.update(self.target, x)
        w_post = np.copy(self.target.w.data)

        self.assertEqual(w_pre, h_pre.value)
        self.assertEqual(w_post, h_post.value)
        self.assertNotEqual(h_pre.value, h_post.value)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/optimizers_tests/test_optimizers.py" startline="196" endline="215" pcid="5359">
    def test_hooks_auto(self):
        w_pre = np.copy(self.target.w.data)
        h_pre = WeightSaveHook()
        h_pre.timing = 'pre'
        h_post = WeightSaveHook()
        h_post.timing = 'post'
        self.create()
        self.optimizer.add_hook(h_pre, timing='auto')
        self.optimizer.add_hook(h_post, name='WeightSaveHookPost',
                                timing='auto')

        x = chainer.Variable(np.array(5., dtype=np.float32))
        self.optimizer.update(self.target, x)
        w_post = np.copy(self.target.w.data)

        self.assertEqual(w_pre, h_pre.value)
        self.assertEqual(w_post, h_post.value)
        self.assertNotEqual(h_pre.value, h_post.value)


</source>
</class>

<class classid="113" nclones="2" nlines="11" similarity="81">
<source file="systems/chainer-7.2.0/tests/chainer_tests/optimizers_tests/test_optimizers.py" startline="229" endline="242" pcid="5363">
    def test_new_pickle(self):
        self.create()
        pickled_opt = pickle.dumps(self.optimizer)

        x = chainer.Variable(np.array(5., dtype=np.float32))
        self.optimizer.update(self.target, x)
        w_post = np.copy(self.target.w.data)
        # Pickle has saved a copy of the target
        opt = pickle.loads(pickled_opt)
        opt.update(opt.target, x)
        pickled_w_post = np.copy(opt.target.w.data)

        self.assertEqual(w_post, pickled_w_post)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/optimizers_tests/test_optimizers.py" startline="243" endline="259" pcid="5364">
    def test_updated_pickle(self):
        self.create()

        x = chainer.Variable(np.array(5., dtype=np.float32))
        self.optimizer.update(self.target, x)
        pickled_opt = pickle.dumps(self.optimizer)

        self.optimizer.update(self.target, x)
        w_post = np.copy(self.target.w.data)
        # Pickle has saved a copy of the target
        opt = pickle.loads(pickled_opt)
        opt.update(opt.target, x)
        pickled_w_post = np.copy(opt.target.w.data)

        self.assertEqual(w_post, pickled_w_post)


</source>
</class>

<class classid="114" nclones="3" nlines="16" similarity="77">
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="244" endline="259" pcid="5393">

    def test_copy_with_share_mode(self):
        link = self.link.copy(mode='share')
        self.assertIsInstance(link._params, set)
        self.assertIsInstance(link._persistent, set)
        self.assertTrue(hasattr(link, 'x'))
        self.assertTrue(hasattr(link, 'y'))
        self.assertTrue(hasattr(link, 'u'))
        self.assertTrue(hasattr(link, 'p'))
        self.assertIsNot(link.x, self.link.x)
        self.assertIs(link.x.array, self.link.x.array)
        self.assertIsNot(link.y, self.link.y)
        self.assertIs(link.y.array, self.link.y.array)
        self.assertIsNone(link.u.array)
        self.assertIs(link.p, self.link.p)
        self.assertIs(link.name, None)
</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="276" endline="294" pcid="5395">

    def test_copy_with_init_mode(self):
        self.link.u.initializer = initializers.Normal(
            dtype=self.link.u.initializer.dtype)
        self.link.u.initialize((2, 3))
        link = self.link.copy(mode='init')
        self.assertFalse(numpy.array_equal(self.link.u.array, link.u.array))
        self.assertIsInstance(link._params, set)
        self.assertIsInstance(link._persistent, set)
        self.assertTrue(hasattr(link, 'x'))
        self.assertTrue(hasattr(link, 'y'))
        self.assertTrue(hasattr(link, 'u'))
        self.assertTrue(hasattr(link, 'p'))
        self.assertIsNot(link.x, self.link.x)
        self.assertIsNot(link.x.array, self.link.x.array)
        self.assertIsNot(link.y, self.link.y)
        self.assertIsNot(link.y.array, self.link.y.array)
        self.assertIsNot(link.p, self.link.p)
        self.assertIsNot(link.name, None)
</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="260" endline="275" pcid="5394">

    def test_copy_with_copy_mode(self):
        link = self.link.copy(mode='copy')
        self.assertIsInstance(link._params, set)
        self.assertIsInstance(link._persistent, set)
        self.assertTrue(hasattr(link, 'x'))
        self.assertTrue(hasattr(link, 'y'))
        self.assertTrue(hasattr(link, 'u'))
        self.assertTrue(hasattr(link, 'p'))
        self.assertIsNot(link.x, self.link.x)
        self.assertIsNot(link.x.array, self.link.x.array)
        self.assertIsNot(link.y, self.link.y)
        self.assertIsNot(link.y.array, self.link.y.array)
        self.assertIsNone(link.u.array)
        self.assertIsNot(link.p, self.link.p)
        self.assertIsNot(link.name, None)
</source>
</class>

<class classid="115" nclones="3" nlines="15" similarity="70">
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="378" endline="395" pcid="5402">

    def test_to_cpu_on_cpu(self):
        x = self.link.x.data
        gx = self.link.x.grad
        y = self.link.y.data
        gy = self.link.y.grad
        p = self.link.p
        with testing.assert_warns(DeprecationWarning):
            self.link.to_cpu()
        self.assertIs(self.link.x.data, x)
        self.assertIs(self.link.x.grad, gx)
        self.assertIs(self.link.y.data, y)
        self.assertIs(self.link.y.grad, gy)
        self.assertIsNone(self.link.u.data)
        u = self.link.u
        with pytest.raises(RuntimeError):
            u.grad
        self.assertIs(self.link.p, p)
</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="1274" endline="1290" pcid="5467">

    def test_to_cpu_on_cpu(self):
        x1 = self.l1.x.data
        gx1 = self.l1.x.grad
        x2 = self.l2.x.data
        gx2 = self.l2.x.grad
        x3 = self.l3.x.data

        with testing.assert_warns(DeprecationWarning):
            self.c2.to_cpu()
        self.assertIs(self.l1.x.data, x1)
        self.assertIs(self.l1.x.grad, gx1)
        self.assertIs(self.l2.x.data, x2)
        self.assertIs(self.l2.x.grad, gx2)
        self.assertIs(self.l3.x.data, x3)
        with pytest.raises(RuntimeError):
            self.l3.x.grad
</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="1911" endline="1928" pcid="5517">

    def test_to_cpu_on_cpu(self):
        x1 = self.l1.x.data
        gx1 = self.l1.x.grad
        x2 = self.l2.x.data
        gx2 = self.l2.x.grad
        x3 = self.l3.x.data
        gx3 = self.l3.x.grad

        with testing.assert_warns(DeprecationWarning):
            self.c2.to_cpu()

        self.assertIs(self.l1.x.data, x1)
        self.assertIs(self.l1.x.grad, gx1)
        self.assertIs(self.l2.x.data, x2)
        self.assertIs(self.l2.x.grad, gx2)
        self.assertIs(self.l3.x.data, x3)
        self.assertIs(self.l3.x.grad, gx3)
</source>
</class>

<class classid="116" nclones="2" nlines="18" similarity="88">
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="397" endline="415" pcid="5403">
    @attr.gpu
    def test_to_cpu(self):
        with testing.assert_warns(DeprecationWarning):
            self.link.to_gpu()
        with testing.assert_warns(DeprecationWarning):
            self.link.to_cpu()
        self.link.v.initialize((2, 3))
        self.assertIs(self.link.xp, numpy)
        self.assertIsInstance(self.link.x.data, numpy.ndarray)
        self.assertIsInstance(self.link.x.grad, numpy.ndarray)
        self.assertIsInstance(self.link.y.data, numpy.ndarray)
        self.assertIsInstance(self.link.y.grad, numpy.ndarray)
        self.assertIsNone(self.link.u.data)
        u = self.link.u
        with pytest.raises(RuntimeError):
            u.grad
        self.assertIsInstance(self.link.v.data, numpy.ndarray)
        self.assertIsInstance(self.link.v.grad, numpy.ndarray)
        self.assertIsInstance(self.link.p, numpy.ndarray)
</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="417" endline="434" pcid="5404">
    @attr.gpu
    def test_to_gpu(self):
        cupy = cuda.cupy
        with testing.assert_warns(DeprecationWarning):
            self.link.to_gpu()
        self.link.v.initialize((2, 3))
        self.assertIs(self.link.xp, cupy)
        self.assertIsInstance(self.link.x.data, cupy.ndarray)
        self.assertIsInstance(self.link.x.grad, cupy.ndarray)
        self.assertIsInstance(self.link.y.data, cupy.ndarray)
        self.assertIsInstance(self.link.y.grad, cupy.ndarray)
        self.assertIsNone(self.link.u.data)
        u = self.link.u
        with pytest.raises(RuntimeError):
            u.grad
        self.assertIsInstance(self.link.v.data, cupy.ndarray)
        self.assertIsInstance(self.link.v.grad, cupy.ndarray)
        self.assertIsInstance(self.link.p, cupy.ndarray)
</source>
</class>

<class classid="117" nclones="2" nlines="12" similarity="75">
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="636" endline="650" pcid="5425">

    def test_count_params(self):
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            assert self.link.count_params() == 8
        assert len(w) == 2
        assert w[0].category is UserWarning

        self.link.u.initialize((2, 3))
        self.link.v.initialize((2, 3))
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            self.link.count_params()
        assert not w

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="1508" endline="1523" pcid="5484">

    def test_count_params(self):
        assert self.c1.count_params() == 8

        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            self.c2.count_params()
        assert len(w) == 1
        assert w[0].category is UserWarning

        self.c2.l3.x.initialize((3,))
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            self.c2.count_params()
        assert not w

</source>
</class>

<class classid="118" nclones="3" nlines="28" similarity="71">
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="661" endline="695" pcid="5426">

    def test_serialize(self, backend_config):
        call_record = []

        def serializer(key, value):
            call_record.append((key, value))
            return value

        l = chainer.Link()
        with l.init_scope():
            l.x = chainer.Parameter(shape=self.shape_x)
            l.y = chainer.Parameter(shape=self.shape_y)
        l.add_persistent('z', 1)
        l.to_device(backend_config.device)

        old_x_data = l.x.array
        old_y_data = l.y.array
        old_z = l.z

        l.serialize(serializer)

        # Link data are not modified
        self.assertIs(l.x.array, old_x_data)
        self.assertIs(l.y.array, old_y_data)
        self.assertEqual(l.z, old_z)

        # Check inputs to the serializer
        self.assertEqual(len(call_record), 3)
        call_record = sorted(call_record)
        self.assertEqual(call_record[0][0], 'x')
        self.assertIs(call_record[0][1], l.x.array)
        self.assertEqual(call_record[1][0], 'y')
        self.assertIs(call_record[1][1], l.y.array)
        self.assertEqual(call_record[2][0], 'z')
        self.assertEqual(call_record[2][1], old_z)
</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="696" endline="741" pcid="5428">

    def test_deserialize(self, backend_config):
        call_record = []

        state = {
            'x': _shaped_random(self.shape_x, 'float32'),
            'y': _shaped_random(self.shape_y, 'float32'),
            'z': numpy.random.randn(),
        }

        def deserializer(key, value):
            call_record.append((key, value))
            if key == 'z':
                return state[key]  # scalar
            value[...] = backend_config.device.send(state[key])
            return value

        l = chainer.Link()
        with l.init_scope():
            l.x = chainer.Parameter(shape=self.shape_x)
            l.y = chainer.Parameter(shape=self.shape_y)
        l.add_persistent('z', 1)
        l.to_device(backend_config.device)

        old_x_data = l.x.array
        old_y_data = l.y.array
        old_z = l.z

        l.serialize(deserializer)

        # Check link data
        self.assertIs(l.x.array, old_x_data)
        self.assertIs(l.y.array, old_y_data)
        _assert_arrays_equal(l.x.array, state['x'])
        _assert_arrays_equal(l.y.array, state['y'])
        self.assertEqual(l.z, state['z'])

        # Check inputs to the deserializer
        self.assertEqual(len(call_record), 3)
        call_record = sorted(call_record)
        self.assertEqual(call_record[0][0], 'x')
        self.assertIs(call_record[0][1], l.x.array)
        self.assertEqual(call_record[1][0], 'y')
        self.assertIs(call_record[1][1], l.y.array)
        self.assertEqual(call_record[2][0], 'z')
        self.assertEqual(call_record[2][1], old_z)
</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="742" endline="773" pcid="5430">

    def test_deserialize_uninitialized1(self, backend_config):
        # Deserializes uninitialized parameters into initialized ones.
        # TODO(niboshi): Currently the existing initialized parameters are
        # untouched, but maybe uninitialized state should be restored? (#7916)
        call_record = []

        def deserializer(key, value):
            call_record.append((key, value))
            return None  # to be uninitialized

        l = chainer.Link()
        with l.init_scope():
            l.x = chainer.Parameter(shape=self.shape_x)  # initialized
            l.y = chainer.Parameter(shape=self.shape_y)  # initialized
        l.to_device(backend_config.device)

        old_x_data = l.x.array
        old_y_data = l.y.array

        l.serialize(deserializer)

        # Link is kept untouched
        self.assertIs(l.x.array, old_x_data)
        self.assertIs(l.y.array, old_y_data)

        # Check inputs to the deserializer
        self.assertEqual(len(call_record), 2)
        call_record = sorted(call_record)
        self.assertEqual(call_record[0][0], 'x')
        self.assertIs(call_record[0][1], l.x.array)
        self.assertEqual(call_record[1][0], 'y')
</source>
</class>

<class classid="119" nclones="2" nlines="14" similarity="100">
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="815" endline="836" pcid="5434">

    def test_serialize(self, backend_config):
        call_record = []

        def serializer(key, value):
            call_record.append((key, value))
            return value

        l = chainer.Link()
        with l.init_scope():
            l.x = chainer.Parameter()  # uninitialized
        l.to_device(backend_config.device)

        l.serialize(serializer)

        # Link is kept uninitialized
        self.assertIsNone(l.x.array)

        # Check inputs to the serializer
        self.assertEqual(len(call_record), 1)
        self.assertEqual(call_record[0][0], 'x')
        self.assertIs(call_record[0][1], None)
</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="837" endline="860" pcid="5436">

    def test_deserialize(self, backend_config):
        # Deserializes uninitialized parameters into uninitialied ones.
        call_record = []

        def serializer(key, value):
            call_record.append((key, value))
            return None  # to be uninitialized

        l = chainer.Link()
        with l.init_scope():
            l.x = chainer.Parameter()  # uninitialized
        l.to_device(backend_config.device)

        l.serialize(serializer)

        # Link is kept uninitialized
        self.assertIsNone(l.x.array)

        # Check inputs to the serializer
        self.assertEqual(len(call_record), 1)
        self.assertEqual(call_record[0][0], 'x')
        self.assertIs(call_record[0][1], None)

</source>
</class>

<class classid="120" nclones="3" nlines="12" similarity="84">
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="976" endline="992" pcid="5451">

    def test_repeat_with_init(self):
        ret = self.link.repeat(2, mode='init')
        self.assertEqual(len(ret), 2)
        # Both should be different objects from the original link
        self.assertIsNot(ret[0], self.link)
        self.assertIsNot(ret[1], self.link)
        # Object IDs of elements should be different
        self.assertIsNot(ret[0], ret[1])
        self.assertIsNot(ret[0].x, ret[1].x)
        # But shape and type of paratmeres shuld be same
        self.assertEqual(ret[0].x.shape, self.link.x.shape)
        self.assertEqual(ret[0].x.dtype, self.link.x.dtype)
        self.assertEqual(ret[0].x.shape, ret[1].x.shape)
        self.assertEqual(ret[0].x.dtype, ret[1].x.dtype)
        # Parameters are re-initialized, so the values should be different
        self.assertFalse(numpy.all(ret[0].x.array == ret[1].x.array))
</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="993" endline="1008" pcid="5452">

    def test_repeat_with_copy(self):
        ret = self.link.repeat(2, mode='copy')
        self.assertEqual(len(ret), 2)
        # Both should be different objects from the original link
        self.assertIsNot(ret[0], self.link)
        self.assertIsNot(ret[1], self.link)
        # Object IDs of elements should be different
        self.assertIsNot(ret[0], ret[1])
        self.assertIsNot(ret[0].x, ret[1].x)
        # But shape, type, and value of paratmeres shuld be same
        self.assertEqual(ret[0].x.shape, self.link.x.shape)
        self.assertEqual(ret[0].x.dtype, self.link.x.dtype)
        self.assertEqual(ret[0].x.shape, ret[1].x.shape)
        self.assertEqual(ret[0].x.dtype, ret[1].x.dtype)
        numpy.testing.assert_array_equal(ret[0].x.array, ret[1].x.array)
</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="1009" endline="1027" pcid="5453">

    def test_repeat_with_share(self):
        ret = self.link.repeat(2, mode='share')
        self.assertEqual(len(ret), 2)
        # Both should be different objects from the original link
        self.assertIsNot(ret[0], self.link)
        self.assertIsNot(ret[1], self.link)
        # Object IDs of elements should be different
        self.assertIsNot(ret[0], ret[1])
        self.assertIsNot(ret[0].x, ret[1].x)
        # But the array objects should be the same
        self.assertIs(ret[0].x.array, ret[1].x.array)
        # But shape, type, and value of paratmeres shuld be same
        self.assertEqual(ret[0].x.shape, self.link.x.shape)
        self.assertEqual(ret[0].x.dtype, self.link.x.dtype)
        self.assertEqual(ret[0].x.shape, ret[1].x.shape)
        self.assertEqual(ret[0].x.dtype, ret[1].x.dtype)
        numpy.testing.assert_array_equal(ret[0].x.array, ret[1].x.array)

</source>
</class>

<class classid="121" nclones="3" nlines="40" similarity="78">
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="1130" endline="1171" pcid="5464">

    def test_copy_with_share_mode(self):
        self.l1.x.initializer = initializers.Normal(
            dtype=self.l1.x.initializer.dtype)
        self.l1.x.initialize(self.l1.x.shape)
        self.l2.x.initializer = initializers.Normal(
            dtype=self.l2.x.initializer.dtype)
        self.l2.x.initialize(self.l2.x.shape)
        self.x.initializer = initializers.Normal(
            dtype=self.x.initializer.dtype)
        self.x.initialize(self.x.shape)

        c2 = self.c2.copy(mode='share')
        self.assertIs(c2.name, None)
        self.assertIsInstance(c2._children, set)
        self.assertTrue(hasattr(c2, 'x'))
        self.assertIsNot(c2.x, self.x)
        self.assertIs(c2.x.data, self.x.data)

        self.assertTrue(hasattr(c2, 'c1'))
        self.assertEqual(c2.c1.name, 'c1')
        self.assertIsInstance(c2.c1._children, set)
        self.assertIsNot(c2.c1, self.c1)
        self.assertEqual(c2.c1.l1.name, 'l1')
        self.assertIsNot(c2.c1.l1, self.l1)
        self.assertIsNot(c2.c1.l1.x, self.l1.x)
        self.assertIs(c2.c1.l1.x.data, self.l1.x.data)
        self.assertIs(c2.c1.l1.x.grad, None)

        self.assertTrue(hasattr(c2.c1, 'l2'))
        self.assertEqual(c2.c1.l2.name, 'l2')
        self.assertIsNot(c2.c1.l2, self.l2)
        self.assertIsNot(c2.c1.l2.x, self.l2.x)
        self.assertIs(c2.c1.l2.x.data, self.l2.x.data)
        self.assertIs(c2.c1.l2.x.grad, None)

        self.assertTrue(hasattr(c2, 'l3'))
        self.assertEqual(c2.l3.name, 'l3')
        self.assertIsNot(c2.l3, self.l3)
        self.assertIsNot(c2.l3.x, self.l3.x)
        self.assertIs(c2.l3.x.data, self.l3.x.data)
        self.assertIs(c2.l3.x.grad, None)
</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="1219" endline="1273" pcid="5466">

    def test_copy_with_init_mode(self):
        self.l1.x.initializer = initializers.Normal(
            dtype=self.l1.x.initializer.dtype)
        self.l1.x.initialize(self.l1.x.shape)
        self.l2.x.initializer = initializers.Normal(
            dtype=self.l2.x.initializer.dtype)
        self.l2.x.initialize(self.l2.x.shape)
        self.x.initializer = initializers.Normal(
            dtype=self.x.initializer.dtype)
        self.c2.x.initialize(self.x.shape)

        c2 = self.c2.copy(mode='init')
        self.assertIs(c2.name, None)
        self.assertIsInstance(c2._children, set)
        self.assertTrue(hasattr(c2, 'x'))
        self.assertIsNot(c2.x, self.x)
        self.assertIsNot(c2.x.data, self.x.data)
        self.assertFalse(numpy.array_equal(c2.x.data, self.x.data))
        # _grad_initializer attribute in a copied Parameter has constant.NaN
        # after calling initilize() method
        self.assertTrue(numpy.isnan(c2.x.grad).all())

        self.assertTrue(hasattr(c2, 'c1'))
        self.assertEqual(c2.c1.name, 'c1')
        self.assertIsInstance(c2.c1._children, set)
        self.assertIsNot(c2.c1, self.c1)
        self.assertEqual(c2.c1.l1.name, 'l1')
        self.assertIsNot(c2.c1.l1, self.l1)
        self.assertIsNot(c2.c1.l1.x, self.l1.x)
        self.assertIsNot(c2.c1.l1.x.data, self.l1.x.data)
        self.assertFalse(numpy.array_equal(c2.c1.l1.x.data, self.l1.x.data))
        # _grad_initializer attribute in a copied Parameter has constant.NaN
        # after calling initilize() method
        self.assertTrue(numpy.isnan(c2.c1.l1.x.grad).all())

        self.assertTrue(hasattr(c2.c1, 'l2'))
        self.assertEqual(c2.c1.l2.name, 'l2')
        self.assertIsNot(c2.c1.l2, self.l2)
        self.assertIsNot(c2.c1.l2.x, self.l2.x)
        self.assertIsNot(c2.c1.l2.x.data, self.l2.x.data)
        self.assertFalse(numpy.array_equal(c2.c1.l2.x.data, self.l2.x.data))
        # _grad_initializer attribute in a copied Parameter has constant.NaN
        # after calling initilize() method
        self.assertTrue(numpy.isnan(c2.c1.l2.x.grad).all())

        self.assertTrue(hasattr(c2, 'l3'))
        self.assertEqual(c2.l3.name, 'l3')
        self.assertIsNot(c2.l3, self.l3)
        self.assertIsNot(c2.l3.x, self.l3.x)
        self.assertIs(c2.l3.x.data, self.l3.x.data)
        # A Parameter constructed with shape argument but not initialized
        # has invalid grad
        with pytest.raises(RuntimeError):
            c2.l3.x.grad
</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="1172" endline="1218" pcid="5465">

    def test_copy_with_copy_mode(self):
        self.l1.x.initializer = initializers.Normal(
            dtype=self.l1.x.initializer.dtype)
        self.l1.x.initialize(self.l1.x.shape)
        self.l2.x.initializer = initializers.Normal(
            dtype=self.l2.x.initializer.dtype)
        self.l2.x.initialize(self.l2.x.shape)
        self.x.initializer = initializers.Normal(
            dtype=self.x.initializer.dtype)
        self.x.initialize(self.x.shape)

        c2 = self.c2.copy(mode='copy')
        self.assertIs(c2.name, None)
        self.assertIsInstance(c2._children, set)
        self.assertTrue(hasattr(c2, 'x'))
        self.assertIsNot(c2.x, self.x)
        self.assertIsNot(c2.x.data, self.x.data)
        self.assertTrue(numpy.array_equal(c2.x.data, self.x.data))

        self.assertTrue(hasattr(c2, 'c1'))
        self.assertEqual(c2.c1.name, 'c1')
        self.assertIsInstance(c2.c1._children, set)
        self.assertIsNot(c2.c1, self.c1)
        self.assertEqual(c2.c1.l1.name, 'l1')
        self.assertIsNot(c2.c1.l1, self.l1)
        self.assertIsNot(c2.c1.l1.x, self.l1.x)
        self.assertIsNot(c2.c1.l1.x.data, self.l1.x.data)
        self.assertTrue(numpy.array_equal(c2.c1.l1.x.data, self.l1.x.data))
        self.assertIs(c2.c1.l1.x.grad, None)

        self.assertTrue(hasattr(c2.c1, 'l2'))
        self.assertEqual(c2.c1.l2.name, 'l2')
        self.assertIsNot(c2.c1.l2, self.l2)
        self.assertIsNot(c2.c1.l2.x, self.l2.x)
        self.assertIsNot(c2.c1.l2.x.data, self.l2.x.data)
        self.assertTrue(numpy.array_equal(c2.c1.l2.x.data, self.l2.x.data))
        self.assertIs(c2.c1.l2.x.grad, None)

        self.assertTrue(hasattr(c2, 'l3'))
        self.assertEqual(c2.l3.name, 'l3')
        self.assertIsNot(c2.l3, self.l3)
        self.assertIsNot(c2.l3.x, self.l3.x)
        self.assertIs(c2.l3.x.data, self.l3.x.data)
        x = c2.l3.x
        with pytest.raises(RuntimeError):
            x.grad
</source>
</class>

<class classid="122" nclones="6" nlines="18" similarity="70">
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="1292" endline="1313" pcid="5468">
    @attr.gpu
    def test_to_cpu(self):
        self.set_count_parameters()
        with testing.assert_warns(DeprecationWarning):
            self.c2.to_gpu()
        with testing.assert_warns(DeprecationWarning):
            self.c2.to_cpu()
        self.assertIs(self.c2.xp, numpy)
        self.assertIs(self.c1.xp, numpy)
        self.assertIs(self.l1.xp, numpy)
        self.assertIs(self.l2.xp, numpy)
        self.assertIs(self.l3.xp, numpy)
        self.assertIsInstance(self.l1.x.data, numpy.ndarray)
        self.assertIsInstance(self.l1.x.grad, numpy.ndarray)
        self.assertIsInstance(self.l2.x.data, numpy.ndarray)
        self.assertIsInstance(self.l2.x.grad, numpy.ndarray)
        self.assertIsNone(self.l3.x.data)
        self.assertIsNone(self.l3.x.grad)

        self.l3.x.initialize(3)
        self.assertIsInstance(self.l3.x.data, numpy.ndarray)
        self.assertIsInstance(self.l3.x.grad, numpy.ndarray)
</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="1930" endline="1946" pcid="5518">
    @attr.gpu
    def test_to_cpu(self):
        with testing.assert_warns(DeprecationWarning):
            self.c2.to_gpu()
        with testing.assert_warns(DeprecationWarning):
            self.c2.to_cpu()
        self.assertIs(self.c2.xp, numpy)
        self.assertIs(self.c1.xp, numpy)
        self.assertIs(self.l1.xp, numpy)
        self.assertIs(self.l2.xp, numpy)
        self.assertIs(self.l3.xp, numpy)
        self.assertIsInstance(self.l1.x.data, numpy.ndarray)
        self.assertIsInstance(self.l1.x.grad, numpy.ndarray)
        self.assertIsInstance(self.l2.x.data, numpy.ndarray)
        self.assertIsInstance(self.l2.x.grad, numpy.ndarray)
        self.assertIsInstance(self.l3.x.data, numpy.ndarray)
        self.assertIsInstance(self.l3.x.grad, numpy.ndarray)
</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="1315" endline="1335" pcid="5469">
    @attr.gpu
    def test_to_gpu(self):
        self.set_count_parameters()
        cupy = cuda.cupy
        with testing.assert_warns(DeprecationWarning):
            self.c2.to_gpu()
        self.assertIs(self.c2.xp, cupy)
        self.assertIs(self.c1.xp, cupy)
        self.assertIs(self.l1.xp, cupy)
        self.assertIs(self.l2.xp, cupy)
        self.assertIs(self.l3.xp, cupy)
        self.assertIsInstance(self.l1.x.data, cupy.ndarray)
        self.assertIsInstance(self.l1.x.grad, cupy.ndarray)
        self.assertIsInstance(self.l2.x.data, cupy.ndarray)
        self.assertIsInstance(self.l2.x.grad, cupy.ndarray)
        self.assertIsNone(self.l3.x.data)
        self.assertIsNone(self.l3.x.grad)

        self.l3.x.initialize(3)
        self.assertIsInstance(self.l3.x.data, cupy.ndarray)
        self.assertIsInstance(self.l3.x.grad, cupy.ndarray)
</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="1948" endline="1963" pcid="5519">
    @attr.gpu
    def test_to_gpu(self):
        cupy = cuda.cupy
        with testing.assert_warns(DeprecationWarning):
            self.c2.to_gpu()
        self.assertIs(self.c2.xp, cupy)
        self.assertIs(self.c1.xp, cupy)
        self.assertIs(self.l1.xp, cupy)
        self.assertIs(self.l2.xp, cupy)
        self.assertIs(self.l3.xp, cupy)
        self.assertIsInstance(self.l1.x.data, cupy.ndarray)
        self.assertIsInstance(self.l1.x.grad, cupy.ndarray)
        self.assertIsInstance(self.l2.x.data, cupy.ndarray)
        self.assertIsInstance(self.l2.x.grad, cupy.ndarray)
        self.assertIsInstance(self.l3.x.data, cupy.ndarray)
        self.assertIsInstance(self.l3.x.grad, cupy.ndarray)
</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="1336" endline="1354" pcid="5470">

    def test_to_device(self):
        self.set_count_parameters()
        device = backend.CpuDevice()
        self.c2.to_device(device)
        self.assertIs(self.c2.xp, numpy)
        self.assertIs(self.c1.xp, numpy)
        self.assertIs(self.l1.xp, numpy)
        self.assertIs(self.l2.xp, numpy)
        self.assertIs(self.l3.xp, numpy)
        self.assertIsInstance(self.l1.x.data, numpy.ndarray)
        self.assertIsInstance(self.l1.x.grad, numpy.ndarray)
        self.assertIsInstance(self.l2.x.data, numpy.ndarray)
        self.assertIsInstance(self.l2.x.grad, numpy.ndarray)
        self.assertIsNone(self.l3.x.data)

        self.l3.x.initialize((3,))
        self.assertIsInstance(self.l3.x.data, numpy.ndarray)
        self.assertIsInstance(self.l3.x.grad, numpy.ndarray)
</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="1987" endline="2001" pcid="5521">

    def test_to_device(self):
        device = backend.CpuDevice()
        self.c2.to_device(device)
        self.assertIs(self.c2.xp, numpy)
        self.assertIs(self.c1.xp, numpy)
        self.assertIs(self.l1.xp, numpy)
        self.assertIs(self.l2.xp, numpy)
        self.assertIs(self.l3.xp, numpy)
        self.assertIsInstance(self.l1.x.data, numpy.ndarray)
        self.assertIsInstance(self.l1.x.grad, numpy.ndarray)
        self.assertIsInstance(self.l2.x.data, numpy.ndarray)
        self.assertIsInstance(self.l2.x.grad, numpy.ndarray)
        self.assertIsInstance(self.l3.x.data, numpy.ndarray)
        self.assertIsInstance(self.l3.x.grad, numpy.ndarray)
</source>
</class>

<class classid="123" nclones="2" nlines="30" similarity="75">
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="1415" endline="1445" pcid="5480">

    def test_copyparams(self):
        l1 = chainer.Link()
        with l1.init_scope():
            l1.x = chainer.Parameter(shape=(2, 3))
        l2 = chainer.Link()
        with l2.init_scope():
            l2.x = chainer.Parameter(shape=2)
        l3 = chainer.Link()
        with l3.init_scope():
            l3.x = chainer.Parameter(shape=3)
        c1 = chainer.Chain()
        with c1.init_scope():
            c1.l1 = l1
            c1.l2 = l2
        c2 = chainer.Chain()
        with c2.init_scope():
            c2.c1 = c1
            c2.l3 = l3
            c2.x = chainer.Parameter(shape=2)
        l1.x.data.fill(0)
        l2.x.data.fill(1)
        l3.x.data.fill(2)
        c2.x.data.fill(3)

        self.c2.copyparams(c2)

        numpy.testing.assert_array_equal(self.l1.x.data, l1.x.data)
        numpy.testing.assert_array_equal(self.l2.x.data, l2.x.data)
        numpy.testing.assert_array_equal(self.l3.x.data, l3.x.data)
        numpy.testing.assert_array_equal(self.c2.x.data, c2.x.data)
</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="1459" endline="1493" pcid="5482">

    def test_addgrads(self):
        l1 = chainer.Link()
        with l1.init_scope():
            l1.x = chainer.Parameter(shape=(2, 3))
        l2 = chainer.Link()
        with l2.init_scope():
            l2.x = chainer.Parameter(shape=2)
        l3 = chainer.Link()
        with l3.init_scope():
            l3.x = chainer.Parameter(shape=3)
        c1 = chainer.Chain()
        with c1.init_scope():
            c1.l1 = l1
            c1.l2 = l2
        c2 = chainer.Chain()
        with c2.init_scope():
            c2.c1 = c1
            c2.l3 = l3
            c2.x = chainer.Parameter(shape=2)
        l1.x.grad.fill(1)
        l2.x.grad.fill(2)
        l3.x.grad.fill(3)
        c2.x.grad.fill(4)

        self.l1.x.grad.fill(-1)
        self.l2.x.grad.fill(-2)
        self.c2.x.grad.fill(-3)
        self.l3.cleargrads()

        self.c2.addgrads(c2)
        numpy.testing.assert_array_equal(self.l1.x.grad, numpy.zeros((2, 3)))
        numpy.testing.assert_array_equal(self.l2.x.grad, numpy.zeros(2))
        numpy.testing.assert_array_equal(self.l3.x.grad, numpy.full(3, 3.))
        numpy.testing.assert_array_equal(self.c2.x.grad, numpy.ones(2))
</source>
</class>

<class classid="124" nclones="2" nlines="11" similarity="72">
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="1551" endline="1563" pcid="5487">

    def test_to_chx(self, backend_config):
        self.set_count_parameters()
        self.c2.to_device(backend_config.device)
        self.c2.to_chx()

        src_device = backend_config.device
        if src_device.xp is chainerx:
            expected_device = src_device
        else:
            expected_device = (
                backend.ChainerxDevice.from_fallback_device(src_device))
        self.check_expected_device(expected_device)
</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="1564" endline="1576" pcid="5488">

    def test_from_chx(self, backend_config):
        self.set_count_parameters()
        self.c2.to_device(backend_config.device)
        self.c2.from_chx()

        src_device = backend_config.device
        if src_device.xp is chainerx:
            expected_device = src_device.fallback_device
        else:
            expected_device = src_device
        self.check_expected_device(expected_device)

</source>
</class>

<class classid="125" nclones="2" nlines="23" similarity="86">
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="1618" endline="1641" pcid="5494">

    def test_repeat_with_copy_mode(self):
        ret = self.chain.repeat(2, mode='copy')
        self.assertEqual(len(ret), 2)
        self.assertIsNot(ret[0], self.chain)
        self.assertIsNot(ret[1], self.chain)
        self.assertIsNot(ret[0], ret[1])
        self.assertIsNot(ret[0].link, self.chain.link)
        self.assertIsNot(ret[1].link, self.chain.link)
        self.assertIsNot(ret[0].link, ret[1].link)
        self.assertIsNot(ret[0].link.x, self.link.x)
        self.assertIsNot(ret[1].link.x, self.link.x)
        self.assertIsNot(ret[0].link.x, ret[1].link.x)
        self.assertIsNot(ret[0].link.x.data, self.chain.link.x.data)
        self.assertIsNot(ret[1].link.x.data, self.chain.link.x.data)
        self.assertIsNot(ret[0].link.x.data, ret[1].link.x.data)
        self.assertTrue(numpy.array_equal(
            ret[0].link.x.data, self.chain.link.x.data))
        self.assertTrue(numpy.array_equal(
            ret[0].link.x.data, ret[1].link.x.data))
        self.assertEqual(ret[0].link.x.shape, self.chain.link.x.shape)
        self.assertEqual(ret[0].link.x.shape, ret[1].link.x.shape)
        self.assertEqual(ret[0].link.x.dtype, self.chain.link.x.dtype)
        self.assertEqual(ret[0].link.x.dtype, ret[1].link.x.dtype)
</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="1642" endline="1665" pcid="5495">

    def test_repeat_with_init_mode(self):
        ret = self.chain.repeat(2, mode='init')
        self.assertEqual(len(ret), 2)
        self.assertIsNot(ret[0], self.chain)
        self.assertIsNot(ret[1], self.chain)
        self.assertIsNot(ret[0], ret[1])
        self.assertIsNot(ret[0].link, self.chain.link)
        self.assertIsNot(ret[1].link, self.chain.link)
        self.assertIsNot(ret[0].link.x, ret[1].link.x)
        self.assertIsNot(ret[0].link.x.data, self.chain.link.x.data)
        self.assertIsNot(ret[1].link.x.data, self.chain.link.x.data)
        self.assertIsNot(ret[0].link.x.data, ret[1].link.x.data)
        self.assertFalse(numpy.array_equal(
            ret[0].link.x.data, self.chain.link.x.data))
        self.assertFalse(numpy.array_equal(
            ret[1].link.x.data, self.chain.link.x.data))
        self.assertFalse(numpy.array_equal(
            ret[0].link.x.data, ret[1].link.x.data))
        self.assertEqual(ret[0].link.x.shape, self.chain.link.x.shape)
        self.assertEqual(ret[0].link.x.shape, ret[1].link.x.shape)
        self.assertEqual(ret[0].link.x.dtype, self.chain.link.x.dtype)
        self.assertEqual(ret[0].link.x.dtype, ret[1].link.x.dtype)

</source>
</class>

<class classid="126" nclones="3" nlines="29" similarity="75">
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="1776" endline="1807" pcid="5511">

    def test_copy_with_share_mode(self):
        c2 = self.c2.copy(mode='share')
        self.l1.x.initializer = initializers.Normal(
            dtype=self.l1.x.initializer.dtype)
        self.l1.x.initialize(self.l1.x.shape)
        self.l2.x.initializer = initializers.Normal(
            dtype=self.l2.x.initializer.dtype)
        self.l2.x.initialize(self.l2.x.shape)

        self.assertIs(c2.name, None)
        self.assertIsInstance(c2._children, list)
        self.assertIsNot(c2[0], self.c1)
        self.assertEqual(c2[0].name, '0')
        self.assertIsInstance(c2[0]._children, list)
        self.assertIsNot(c2[0][0], self.l1)
        self.assertEqual(c2[0][0].name, '0')
        self.assertIsNot(c2[0][0].x, self.l1.x)
        self.assertIs(c2[0][0].x.data, self.l1.x.data)
        self.assertIs(c2[0][0].x.grad, None)

        self.assertIsNot(c2[0][1], self.l2)
        self.assertEqual(c2[0][1].name, '1')
        self.assertIsNot(c2[0][1].x, self.l2.x)
        self.assertIs(c2[0][1].x.data, self.l2.x.data)
        self.assertIs(c2[0][1].x.grad, None)

        self.assertIsNot(c2[1], self.l3)
        self.assertEqual(c2[1].name, '1')
        self.assertIsNot(c2[1].x, self.l3.x)
        self.assertIs(c2[1].x.data, self.l3.x.data)
        self.assertIs(c2[1].x.grad, None)
</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="1842" endline="1878" pcid="5513">

    def test_copy_with_init_mode(self):
        self.l1.x.initializer = initializers.Normal(
            dtype=self.l1.x.initializer.dtype)
        self.l1.x.initialize(self.l1.x.shape)
        self.l2.x.initializer = initializers.Normal(
            dtype=self.l2.x.initializer.dtype)
        self.l2.x.initialize(self.l2.x.shape)

        c2 = self.c2.copy(mode='init')
        self.assertIs(c2.name, None)
        self.assertIsInstance(c2._children, list)
        self.assertEqual(c2[0].name, '0')
        self.assertIsInstance(c2[0]._children, list)
        self.assertIsNot(c2[0][0], self.l1)
        self.assertEqual(c2[0][0].name, '0')
        self.assertIsNot(c2[0][0].x, self.l1.x)
        self.assertIsNot(c2[0][0].x.data, self.l1.x.data)
        self.assertFalse(numpy.array_equal(c2[0][0].x.data, self.l1.x.data))
        # _grad_initializer attribute in a copied Parameter has constant.NaN
        # after calling initilize() method
        self.assertTrue(numpy.isnan(c2[0][0].x.grad).all())

        self.assertIsNot(c2[0][1], self.l2)
        self.assertEqual(c2[0][1].name, '1')
        self.assertIsNot(c2[0][1].x, self.l2.x)
        self.assertIsNot(c2[0][1].x.data, self.l2.x.data)
        self.assertFalse(numpy.array_equal(c2[0][1].x.data, self.l2.x.data))
        # _grad_initializer attribute in a copied Parameter has constant.NaN
        # after calling initilize() method
        self.assertTrue(numpy.isnan(c2[0][1].x.grad).all())

        self.assertIsNot(c2[1], self.l3)
        self.assertEqual(c2[1].name, '1')
        self.assertIsNot(c2[1].x, self.l3.x)
        self.assertTrue(numpy.isnan(c2[1].x.data).all())
        self.assertTrue(numpy.isnan(c2[1].x.grad).all())
</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="1808" endline="1841" pcid="5512">

    def test_copy_with_copy_mode(self):
        self.l1.x.initializer = initializers.Normal(
            dtype=self.l1.x.initializer.dtype)
        self.l1.x.initialize(self.l1.x.shape)
        self.l2.x.initializer = initializers.Normal(
            dtype=self.l2.x.initializer.dtype)
        self.l2.x.initialize(self.l2.x.shape)

        c2 = self.c2.copy(mode='copy')
        self.assertIs(c2.name, None)
        self.assertIsInstance(c2._children, list)
        self.assertEqual(c2[0].name, '0')
        self.assertIsInstance(c2[0]._children, list)
        self.assertIsNot(c2[0][0], self.l1)
        self.assertEqual(c2[0][0].name, '0')
        self.assertIsNot(c2[0][0].x, self.l1.x)
        self.assertIsNot(c2[0][0].x.data, self.l1.x.data)
        self.assertTrue(numpy.array_equal(c2[0][0].x.data, self.l1.x.data))
        self.assertIs(c2[0][0].x.grad, None)

        self.assertIsNot(c2[0][1], self.l2)
        self.assertEqual(c2[0][1].name, '1')
        self.assertIsNot(c2[0][1].x, self.l2.x)
        self.assertIsNot(c2[0][1].x.data, self.l2.x.data)
        self.assertTrue(numpy.array_equal(c2[0][1].x.data, self.l2.x.data))
        self.assertIs(c2[0][1].x.grad, None)

        self.assertIsNot(c2[1], self.l3)
        self.assertEqual(c2[1].name, '1')
        self.assertIsNot(c2[1].x, self.l3.x)
        self.assertIsNot(c2[1].x.data, self.l3.x.data)
        # l3 is constructed with shape argument but not initialized
        self.assertTrue(numpy.isnan(c2[1].x.grad).all())
</source>
</class>

<class classid="127" nclones="3" nlines="21" similarity="72">
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="2199" endline="2217" pcid="5541">

    def test_repeat_with_share_mode(self):
        ret = self.chainlist.repeat(2, mode='share')
        self.assertEqual(len(ret), 2)
        self.assertIsNot(ret[0], self.chainlist)
        self.assertIsNot(ret[1], self.chainlist)
        self.assertIsNot(ret[0], ret[1])
        self.assertIsNot(ret[0][0], self.chainlist[0])
        self.assertIsNot(ret[1][0], self.chainlist[0])
        self.assertIsNot(ret[0][0], ret[1][0])
        self.assertIsNot(ret[0][0].x, self.chainlist[0].x)
        self.assertIsNot(ret[1][0].x, self.chainlist[0].x)
        self.assertIsNot(ret[0][0].x, ret[1][0].x)
        self.assertIs(ret[0][0].x.data, self.chainlist[0].x.data)
        self.assertIs(ret[0][0].x.data, ret[1][0].x.data)
        self.assertEqual(ret[0][0].x.shape, self.chainlist[0].x.shape)
        self.assertEqual(ret[0][0].x.shape, ret[1][0].x.shape)
        self.assertEqual(ret[0][0].x.dtype, self.chainlist[0].x.dtype)
        self.assertEqual(ret[0][0].x.dtype, ret[1][0].x.dtype)
</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="2218" endline="2241" pcid="5542">

    def test_repeat_with_copy_mode(self):
        ret = self.chainlist.repeat(2, mode='copy')
        self.assertEqual(len(ret), 2)
        self.assertIsNot(ret[0], self.chainlist)
        self.assertIsNot(ret[1], self.chainlist)
        self.assertIsNot(ret[0], ret[1])
        self.assertIsNot(ret[0][0], self.chainlist[0])
        self.assertIsNot(ret[1][0], self.chainlist[0])
        self.assertIsNot(ret[0][0], ret[1][0])
        self.assertIsNot(ret[0][0].x, self.chainlist[0].x)
        self.assertIsNot(ret[1][0].x, self.chainlist[0].x)
        self.assertIsNot(ret[0][0].x, ret[1][0].x)
        self.assertIsNot(ret[0][0].x.data, self.chainlist[0].x.data)
        self.assertIsNot(ret[1][0].x.data, self.chainlist[0].x.data)
        self.assertIsNot(ret[0][0].x.data, ret[1][0].x.data)
        self.assertTrue(numpy.array_equal(
            ret[0][0].x.data, self.chainlist[0].x.data))
        self.assertTrue(numpy.array_equal(
            ret[0][0].x.data, ret[1][0].x.data))
        self.assertEqual(ret[0][0].x.shape, self.chainlist[0].x.shape)
        self.assertEqual(ret[0][0].x.shape, ret[1][0].x.shape)
        self.assertEqual(ret[0][0].x.dtype, self.chainlist[0].x.dtype)
        self.assertEqual(ret[0][0].x.dtype, ret[1][0].x.dtype)
</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="2242" endline="2268" pcid="5543">

    def test_repeat_with_init_mode(self):
        ret = self.chainlist.repeat(2, mode='init')
        self.assertEqual(len(ret), 2)
        self.assertIsNot(ret[0], self.chainlist)
        self.assertIsNot(ret[1], self.chainlist)
        self.assertIsNot(ret[0], ret[1])
        self.assertIsNot(ret[0][0], self.chainlist[0])
        self.assertIsNot(ret[1][0], self.chainlist[0])
        self.assertIsNot(ret[0][0], ret[1][0])
        self.assertIsNot(ret[0][0].x, self.chainlist[0].x)
        self.assertIsNot(ret[1][0].x, self.chainlist[0].x)
        self.assertIsNot(ret[0][0].x, ret[1][0].x)
        self.assertIsNot(ret[0][0].x.data, self.chainlist[0].x.data)
        self.assertIsNot(ret[1][0].x.data, self.chainlist[0].x.data)
        self.assertIsNot(ret[0][0].x.data, ret[1][0].x.data)
        self.assertFalse(numpy.array_equal(
            ret[0][0].x.data, self.chainlist[0].x.data))
        self.assertFalse(numpy.array_equal(
            ret[1][0].x.data, self.chainlist[0].x.data))
        self.assertFalse(numpy.array_equal(
            ret[0][0].x.data, ret[1][0].x.data))
        self.assertEqual(ret[0][0].x.shape, self.chainlist[0].x.shape)
        self.assertEqual(ret[0][0].x.shape, ret[1][0].x.shape)
        self.assertEqual(ret[0][0].x.dtype, self.chainlist[0].x.dtype)
        self.assertEqual(ret[0][0].x.dtype, ret[1][0].x.dtype)

</source>
</class>

<class classid="128" nclones="3" nlines="15" similarity="88">
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="2272" endline="2292" pcid="5544">

    def setUp(self):
        self.link = chainer.Link()
        shape = (2, 2)
        dtype = numpy.float32
        y_array = numpy.random.rand(*shape).astype(dtype)
        pa_array = numpy.random.rand(*shape).astype(dtype)
        ps_scalar = 2.4

        with self.link.init_scope():
            # Initialized parameter
            self.link.y = chainer.Parameter(y_array)
            # Uninitialized parameter
            self.link.v = chainer.Parameter()
            # Persistent ndarray
            self.link.add_persistent('pa', pa_array)
            # Persistent scalar
            self.link.add_persistent('ps', ps_scalar)
        self.y_array = y_array
        self.pa_array = pa_array
        self.ps_scalar = ps_scalar
</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="2426" endline="2446" pcid="5552">

    def setUp(self):
        self.link = chainer.Link()
        shape = (2, 2)
        dtype = numpy.float32
        y_array = numpy.random.rand(*shape).astype(dtype)
        pa_array = numpy.random.rand(*shape).astype(dtype)
        ps_scalar = 2.4

        with self.link.init_scope():
            # Initialized parameter
            self.link.y = chainer.Parameter(y_array)
            # Uninitialized parameter
            self.link.v = chainer.Parameter()
            # Persistent ndarray
            self.link.add_persistent('pa', pa_array)
            # Persistent scalar
            self.link.add_persistent('ps', ps_scalar)
        self.y_array = y_array
        self.pa_array = pa_array
        self.ps_scalar = ps_scalar
</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="2513" endline="2536" pcid="5556">
class TestToDevice(unittest.TestCase):
    def setUp(self):
        self.link = chainer.Link()
        shape = (2, 2)
        dtype = numpy.float32
        y_array = numpy.random.rand(*shape).astype(dtype)
        pa_array = numpy.random.rand(*shape).astype(dtype)
        ps_scalar = 2.4

        with self.link.init_scope():
            # Initialized parameter
            self.link.y = chainer.Parameter(y_array)
            # Uninitialized parameter
            self.link.v = chainer.Parameter()
            # Persistent ndarray
            self.link.add_persistent('pa', pa_array)
            # Persistent scalar
            self.link.add_persistent('ps', ps_scalar)
        self.y_array = y_array
        self.pa_array = pa_array
        self.ps_scalar = ps_scalar

        if cuda.available:
            self.current_device_id = cuda.cupy.cuda.get_device_id()
</source>
</class>

<class classid="129" nclones="6" nlines="13" similarity="71">
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="2293" endline="2311" pcid="5545">

    def test_cpu_to_intel64(self):
        link = self.link
        with testing.assert_warns(DeprecationWarning):
            link.to_intel64()
        assert isinstance(link.device, backend.Intel64Device)

        # Arrays should be converted to ideep.mdarray

        # Initialized parameter
        assert isinstance(link.y.data, intel64.ideep.mdarray)
        _assert_variable_array_equal(link.y, self.y_array)
        # Uninitialized parameter
        assert link.v.data is None
        # Persistent ndarray
        assert isinstance(link.pa, intel64.ideep.mdarray)
        _assert_arrays_equal(link.pa, self.pa_array)
        # Persistent scalar
        assert link.ps == self.ps_scalar
</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="2336" endline="2357" pcid="5547">
    @attr.gpu
    def test_gpu_to_intel64(self):
        link = self.link
        with testing.assert_warns(DeprecationWarning):
            link.to_gpu()
        assert link.device.device == cuda.Device(0)
        with testing.assert_warns(DeprecationWarning):
            link.to_intel64()
        assert isinstance(link.device, backend.Intel64Device)

        # Arrays should be converted to ideep.mdarray

        # Initialized parameter
        assert isinstance(link.y.data, intel64.ideep.mdarray)
        _assert_variable_array_equal(link.y, self.y_array)
        # Uninitialized parameter
        assert link.v.data is None
        # Persistent ndarray
        assert isinstance(link.pa, intel64.ideep.mdarray)
        _assert_arrays_equal(link.pa, self.pa_array)
        # Persistent scalar
        assert link.ps == self.ps_scalar
</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="2359" endline="2380" pcid="5548">
    @attr.gpu
    def test_intel64_to_gpu(self):
        link = self.link
        with testing.assert_warns(DeprecationWarning):
            link.to_intel64()
        assert isinstance(link.device, backend.Intel64Device)
        with testing.assert_warns(DeprecationWarning):
            link.to_gpu()
        assert link.device.device == cuda.Device(0)

        # Arrays should be converted to cupy.ndarray

        # Initialized parameter
        assert isinstance(link.y.data, cuda.cupy.ndarray)
        _assert_variable_array_equal(link.y, self.y_array)
        # Uninitialized parameter
        assert link.v.data is None
        # Persistent ndarray
        assert isinstance(link.pa, cuda.ndarray)
        _assert_arrays_equal(link.pa, self.pa_array)
        # Persistent scalar
        assert link.ps == self.ps_scalar
</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="2381" endline="2402" pcid="5549">

    def test_intel64_to_cpu(self):
        link = self.link
        with testing.assert_warns(DeprecationWarning):
            link.to_intel64()
        assert isinstance(link.device, backend.Intel64Device)
        with testing.assert_warns(DeprecationWarning):
            link.to_cpu()
        assert isinstance(link.device, backend.CpuDevice)

        # Arrays should be converted to numpy.ndarray

        # Initialized parameter
        assert isinstance(link.y.data, numpy.ndarray)
        _assert_variable_array_equal(link.y, self.y_array)
        # Uninitialized parameter
        assert link.v.data is None
        # Persistent ndarray
        assert isinstance(link.pa, numpy.ndarray)
        _assert_arrays_equal(link.pa, self.pa_array)
        # Persistent scalar
        assert link.ps == self.ps_scalar
</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="2486" endline="2509" pcid="5555">
    @attr.gpu
    def test_gpu_to_chx(self):
        link = self.link
        with testing.assert_warns(DeprecationWarning):
            link.to_gpu()
        assert link.device.device == cuda.Device(0)
        link.to_chx()
        assert link.device.device == chainerx.get_device('cuda:0')

        # Arrays should be converted to chainerx.ndarray

        # Initialized parameter
        assert isinstance(link.y.data, chainerx.ndarray)
        assert link.y.data.device.backend.name == 'cuda'
        assert link.y.data.device.index == 0
        _assert_variable_array_equal(link.y, self.y_array)
        # Uninitialized parameter
        assert link.v.data is None
        # Persistent ndarray
        assert isinstance(link.pa, chainerx.ndarray)
        _assert_arrays_equal(link.pa, self.pa_array)
        # Persistent scalar
        assert link.ps == self.ps_scalar

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="2468" endline="2484" pcid="5554">

    def test_cpu_to_chx(self):
        link = self.link
        link.to_chx()

        # Initialized parameter
        assert isinstance(link.y.data, chainerx.ndarray)
        assert link.y.data.device.backend.name == 'native'
        assert link.y.data.device.index == 0
        _assert_variable_array_equal(link.y, self.y_array)
        # Uninitialized parameter
        assert link.v.data is None
        # Persistent ndarray
        assert isinstance(link.pa, chainerx.ndarray)
        _assert_arrays_equal(link.pa, self.pa_array)
        # Persistent scalar
        assert link.ps == self.ps_scalar
</source>
</class>

<class classid="130" nclones="2" nlines="14" similarity="80">
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="2312" endline="2334" pcid="5546">

    def test_intel64_to_intel64(self):
        link = self.link
        with testing.assert_warns(DeprecationWarning):
            link.to_intel64()
        prev_y = link.y
        prev_v = link.v
        prev_pa = link.pa
        prev_ps = link.ps
        with testing.assert_warns(DeprecationWarning):
            link.to_intel64()
        assert isinstance(link.device, backend.Intel64Device)

        # Everything should be left untouched

        # Initialized parameter
        assert link.y is prev_y
        # Uninitialized parameter
        assert link.v is prev_v
        # Persistent ndarray
        assert link.pa is prev_pa
        # Persistent scalar
        assert link.ps is prev_ps
</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_link.py" startline="2447" endline="2467" pcid="5553">

    def test_chainerx_to_chx(self):
        link = self.link
        link.to_chx()
        prev_y = link.y
        prev_v = link.v
        prev_pa = link.pa
        prev_ps = link.ps
        link.to_chx()
        assert link.device.device == chainerx.get_device('native:0')

        # Everything should be left untouched

        # Initialized parameter
        assert link.y is prev_y
        # Uninitialized parameter
        assert link.v is prev_v
        # Persistent ndarray
        assert link.pa is prev_pa
        # Persistent scalar
        assert link.ps is prev_ps
</source>
</class>

<class classid="131" nclones="2" nlines="26" similarity="88">
<source file="systems/chainer-7.2.0/tests/chainer_tests/initializer_tests/test_normal.py" startline="94" endline="122" pcid="5635">
    def check_initializer_statistics(self, backend_config, n):
        from scipy import stats

        xp = backend_config.xp
        ws = numpy.empty((n,) + self.shape, dtype=self.dtype)
        ws = backend_config.get_array(ws)
        for i in range(n):
            initializer = self.target(**self.target_kwargs)
            initializer(xp.squeeze(ws[i:i+1], axis=0))

        fan = self.fan_option or default_fan.get(self.target)
        expected_std = self.scale or default_scale.get(self.target) or 1.
        expected_std *= default_coeff.get(self.target) or 1.
        if fan is not None:
            if fan == 'fan_in':
                expected_std *= math.sqrt(1. / self.fans[0])
            elif fan == 'fan_out':
                expected_std *= math.sqrt(1. / self.fans[1])
            elif fan == 'fan_avg':
                expected_std *= math.sqrt(2. / sum(self.fans))
            else:
                assert False

        sampless = cuda.to_cpu(ws.reshape(n, -1).T)
        alpha = 0.01 / len(sampless)
        for samples in sampless:
            _, p = stats.kstest(samples, stats.norm(0, expected_std).cdf)
            assert p >= alpha

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/initializer_tests/test_uniform.py" startline="95" endline="126" pcid="5658">
    def check_initializer_statistics(self, backend_config, n):
        from scipy import stats

        xp = backend_config.xp
        ws = numpy.empty((n,) + self.shape, dtype=self.dtype)
        ws = backend_config.get_array(ws)
        for i in range(n):
            initializer = self.target(**self.target_kwargs)
            initializer(xp.squeeze(ws[i:i+1], axis=0))

        fan = self.fan_option or default_fan.get(self.target)
        expected_max = self.scale or default_scale.get(self.target) or 1.
        expected_max *= default_coeff.get(self.target) or 1.
        if fan is not None:
            if fan == 'fan_in':
                expected_max *= math.sqrt(1. / self.fans[0])
            elif fan == 'fan_out':
                expected_max *= math.sqrt(1. / self.fans[1])
            elif fan == 'fan_avg':
                expected_max *= math.sqrt(2. / sum(self.fans))
            else:
                assert False

        sampless = cuda.to_cpu(ws.reshape(n, -1).T)
        alpha = 0.01 / len(sampless)
        for samples in sampless:
            _, p = stats.kstest(
                samples,
                stats.uniform(-expected_max, 2*expected_max).cdf
            )
            assert p >= alpha

</source>
</class>

<class classid="132" nclones="2" nlines="11" similarity="90">
<source file="systems/chainer-7.2.0/tests/chainer_tests/initializer_tests/test_constant.py" startline="49" endline="60" pcid="5664">
    def check_shaped_initializer(self, backend_config):
        initializer = initializers.Identity(
            scale=self.scale, dtype=self.dtype)
        xp = backend_config.xp
        w = initializers.generate_array(initializer, self.shape, xp)
        self.assertIs(backend.get_array_module(w), xp)
        self.assertTupleEqual(w.shape, self.shape)
        self.assertEqual(w.dtype, self.dtype)
        testing.assert_allclose(
            w, self.scale * numpy.identity(len(self.shape)),
            **self.check_options)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/initializer_tests/test_constant.py" startline="120" endline="131" pcid="5671">
    def check_shaped_initializer(self, backend_config):
        initializer = initializers.Constant(
            fill_value=self.fill_value, dtype=self.dtype)
        xp = backend_config.xp
        w = initializers.generate_array(initializer, self.shape, xp)
        self.assertIs(backend.get_array_module(w), xp)
        self.assertTupleEqual(w.shape, self.shape)
        self.assertEqual(w.dtype, self.dtype)
        testing.assert_allclose(
            w, numpy.full(self.shape, self.fill_value),
            **self.check_options)

</source>
</class>

<class classid="133" nclones="2" nlines="11" similarity="100">
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_function.py" startline="114" endline="127" pcid="5726">
    def check_check_type_forward(self):
        self.assertEqual(self.f.check_type_forward.call_count, 1)
        ts = self.f.check_type_forward.call_args[0][0]
        self.assertIsInstance(ts, type_check.LightTypeInfoTuple)
        self.assertEqual(len(ts), 2)

        t1 = ts[0]
        assert t1.shape == self.x_shape
        assert t1.dtype == numpy.float32

        t2 = ts[1]
        assert t2.shape == self.x_shape
        assert t2.dtype == numpy.int32

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_function_node.py" startline="116" endline="129" pcid="8740">
    def check_check_type_forward(self):
        self.assertEqual(self.f.check_type_forward.call_count, 1)
        ts = self.f.check_type_forward.call_args[0][0]
        self.assertIsInstance(ts, type_check.LightTypeInfoTuple)
        self.assertEqual(len(ts), 2)

        t1 = ts[0]
        assert t1.shape == self.x_shape
        assert t1.dtype == numpy.float32

        t2 = ts[1]
        assert t2.shape == self.x_shape
        assert t2.dtype == numpy.int32

</source>
</class>

<class classid="134" nclones="7" nlines="11" similarity="72">
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_function.py" startline="182" endline="196" pcid="5733">
    def check_call_all_ndarray(self):
        x1 = self.x1
        x2 = self.x2
        ys = self.f(x1, x2)

        self.assertEqual(len(ys), 2)
        self.check_check_type_forward()

        xp = backend.get_array_module(x1)

        for y in ys:
            self.assertIsInstance(y, chainer.Variable)
            self.assertIsInstance(y.data, xp.ndarray)
            self.assertFalse(y.requires_grad)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_function_node.py" startline="243" endline="256" pcid="8754">
    def check_apply_ndarray_chainerx(self):
        x1 = chainer.Variable(self.x1)
        x2 = self.x2
        ys = self.f.apply((x1, x2))

        self.assertEqual(len(ys), 2)
        self.check_check_type_forward()

        for y in ys:
            self.assertIsInstance(y, chainer.Variable)
            self.assertIsInstance(y.data, chainerx.ndarray)
            self.assertIs(y.data.device, self.x1.device)
            self.assertTrue(y.requires_grad)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_function_node.py" startline="190" endline="204" pcid="8748">
    def check_apply_all_ndarray(self):
        x1 = self.x1
        x2 = self.x2
        ys = self.f.apply((x1, x2))

        self.assertEqual(len(ys), 2)
        self.check_check_type_forward()

        xp = backend.get_array_module(x1)

        for y in ys:
            self.assertIsInstance(y, chainer.Variable)
            self.assertIsInstance(y.data, xp.ndarray)
            self.assertFalse(y.requires_grad)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_function_node.py" startline="225" endline="242" pcid="8753">
    def check_apply_ndarray(self):
        x1 = chainer.Variable(self.x1)
        x2 = self.x2
        x1._node._rank = 1
        ys = self.f.apply((x1, x2))

        self.assertEqual(len(ys), 2)
        self.check_check_type_forward()

        for y in ys:
            self.assertIsInstance(y, chainer.Variable)
            # rank is (maximum rank in xs) + 1
            self.assertEqual(y.rank, 2)
            self.assertIs(y.creator_node, self.f)
            self.assertTrue(y.requires_grad)

        self.assertIsInstance(y.creator_node.outputs, tuple)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_function_node.py" startline="149" endline="163" pcid="8742">
    def check_apply_chainerx(self):
        x1 = chainer.Variable(self.x1)
        # TODO(sonots): ChainerX does not support computing gradients for int32
        x2 = chainer.Variable(self.x2, requires_grad=False)
        ys = self.f.apply((x1, x2))

        self.assertEqual(len(ys), 2)
        self.check_check_type_forward()

        for y in ys:
            self.assertIsInstance(y, chainer.Variable)
            self.assertIsInstance(y.data, chainerx.ndarray)
            self.assertIs(y.data.device, self.x1.device)
            self.assertTrue(y.requires_grad)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_function.py" startline="205" endline="222" pcid="5736">
    def check_call_ndarray(self):
        x1 = chainer.Variable(self.x1)
        x2 = self.x2
        x1._node._rank = 1
        ys = self.f(x1, x2)

        self.assertEqual(len(ys), 2)
        self.check_check_type_forward()

        for y in ys:
            self.assertIsInstance(y, chainer.Variable)
            # rank is (maximum rank in xs) + 1
            self.assertEqual(y.rank, 2)
            self.assertIs(y.creator, self.f)
            self.assertTrue(y.requires_grad)

        self.assertIsInstance(y.creator.outputs, tuple)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_function_node.py" startline="130" endline="148" pcid="8741">
    def check_apply(self):
        x1 = chainer.Variable(self.x1)
        x2 = chainer.Variable(self.x2)
        x1._node._rank = 1
        x2._node._rank = 3
        ys = self.f.apply((x1, x2))

        self.assertEqual(len(ys), 2)
        self.check_check_type_forward()

        for y in ys:
            self.assertIsInstance(y, chainer.Variable)
            # rank is (maximum rank in xs) + 1
            self.assertEqual(y.rank, 4)
            self.assertIs(y.creator_node, self.f)
            self.assertTrue(y.requires_grad)

        self.assertIsInstance(y.creator_node.outputs, tuple)

</source>
</class>

<class classid="135" nclones="2" nlines="11" similarity="90">
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_function.py" startline="257" endline="271" pcid="5743">
    def test_unchain(self):
        f, _x1, _y1 = self._get_f()
        y1, y2 = f.outputs
        f.unchain()

        # As _y1 is alive, this weak ref is also alive
        y1_ref = y1()
        self.assertIsNotNone(y1_ref)
        self.assertIsNone(y1_ref.creator)
        # This weak ref is dead by unchain
        y2_ref = y2()
        self.assertIsNone(y2_ref)

        self.assertIsNone(f.inputs)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_function_node.py" startline="328" endline="343" pcid="8766">
    def test_unchain(self):
        self.setup_cpu()
        f, _x1, _y1 = self._get_f()
        y1, y2 = f.outputs
        f.unchain()

        # As _y1 is alive, this weak ref is also alive
        y1_ref = y1()
        self.assertIsNotNone(y1_ref)
        self.assertIsNone(y1_ref.creator)
        # This weak ref is dead by unchain
        y2_ref = y2()
        self.assertIsNone(y2_ref)

        self.assertIsNone(f.inputs)

</source>
</class>

<class classid="136" nclones="2" nlines="24" similarity="87">
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_function.py" startline="297" endline="342" pcid="5746">
    def test_forward_invalid1(self):
        class Function(chainer.Function):

            def check_type_forward(self, in_types):
                x_type, = in_types
                type_check.expect(
                    x_type.dtype == numpy.float32,
                    x_type.ndim >= 2,
                )

            def forward(self, inputs):
                return inputs

        f = Function()

        # OK
        v = chainer.Variable(numpy.random.randn(1, 5).astype(numpy.float32))
        result = f(v)
        assert isinstance(result, chainer.Variable)

        # Incorrect dtype
        # in py3, numpy dtypes are represented as class
        msg = """\
Invalid operation is performed in: Function \\(Forward\\)

Expect: in_types\\[0\\]\\.dtype == <(type|class) 'numpy\\.float32'>
Actual: float64 \\!= <(type|class) 'numpy\\.float32'>"""

        v = chainer.Variable(numpy.random.randn(1, 5))
        with six.assertRaisesRegex(self, chainer.utils.type_check.InvalidType,
                                   msg):
            f(v)

        # Incorrect dim
        msg = """\
Invalid operation is performed in: Function \\(Forward\\)

Expect: in_types\\[0\\]\\.ndim >= 2
Actual: 1 < 2"""

        v = chainer.Variable(numpy.random.randn(5).astype(numpy.float32))
        with six.assertRaisesRegex(self, chainer.utils.type_check.InvalidType,
                                   msg):
            f(v)


</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_function_node.py" startline="381" endline="426" pcid="8773">
    def test_forward_invalid1(self):
        class FunctionNode(chainer.FunctionNode):

            def check_type_forward(self, in_types):
                x_type, = in_types
                type_check.expect(
                    x_type.dtype == numpy.float32,
                    x_type.ndim >= 2,
                )

            def forward(self, inputs):
                return inputs

        f = FunctionNode()

        # OK
        v = chainer.Variable(numpy.random.randn(1, 5).astype(numpy.float32))
        result, = f.apply((v,))
        assert isinstance(result, chainer.Variable)

        # Incorrect dtype
        # in py3, numpy dtypes are represented as class
        msg = """\
Invalid operation is performed in: FunctionNode \\(Forward\\)

Expect: in_types\\[0\\]\\.dtype == <(type|class) 'numpy\\.float32'>
Actual: float64 \\!= <(type|class) 'numpy\\.float32'>"""

        v = chainer.Variable(numpy.random.randn(1, 5))
        with six.assertRaisesRegex(self, chainer.utils.type_check.InvalidType,
                                   msg):
            f.apply((v,))

        # Incorrect dim
        msg = """\
Invalid operation is performed in: FunctionNode \\(Forward\\)

Expect: in_types\\[0\\]\\.ndim >= 2
Actual: 1 < 2"""

        v = chainer.Variable(numpy.random.randn(5).astype(numpy.float32))
        with six.assertRaisesRegex(self, chainer.utils.type_check.InvalidType,
                                   msg):
            f.apply((v,))


</source>
</class>

<class classid="137" nclones="2" nlines="10" similarity="100">
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_function.py" startline="439" endline="452" pcid="5761">
    def test_force_backprop_mode(self):
        with chainer.no_backprop_mode():
            with chainer.force_backprop_mode():
                y = self.x + 1
        self.assertTrue(y.creator_node is not None)

        y = self.x + 1
        self.assertTrue(y.creator_node is not None)

        with chainer.force_backprop_mode():
            y = self.x + 1
        self.assertTrue(y.creator_node is not None)


</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_function_node.py" startline="680" endline="692" pcid="8805">
    def test_force_backprop_mode(self):
        with chainer.no_backprop_mode():
            with chainer.force_backprop_mode():
                y = self.x + 1
        self.assertTrue(y.creator_node is not None)

        y = self.x + 1
        self.assertTrue(y.creator_node is not None)

        with chainer.force_backprop_mode():
            y = self.x + 1
        self.assertTrue(y.creator_node is not None)

</source>
</class>

<class classid="138" nclones="2" nlines="10" similarity="100">
<source file="systems/chainer-7.2.0/tests/chainer_tests/dataset_tests/tabular_tests/test_asmode.py" startline="15" endline="26" pcid="5799">
    def test_astuple(self):
        dataset = dummy_dataset.DummyDataset(mode=self.mode, convert=True)
        view = dataset.astuple()
        self.assertIsInstance(view, chainer.dataset.TabularDataset)
        self.assertEqual(len(view), len(dataset))
        self.assertEqual(view.keys, dataset.keys)
        self.assertEqual(view.mode, tuple)
        self.assertEqual(
            view.get_examples(None, None), dataset.get_examples(None, None))
        self.assertEqual(view.convert(view.fetch()), 'converted')


</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/dataset_tests/tabular_tests/test_asmode.py" startline="34" endline="45" pcid="5800">
    def test_asdict(self):
        dataset = dummy_dataset.DummyDataset(mode=self.mode, convert=True)
        view = dataset.asdict()
        self.assertIsInstance(view, chainer.dataset.TabularDataset)
        self.assertEqual(len(view), len(dataset))
        self.assertEqual(view.keys, dataset.keys)
        self.assertEqual(view.mode, dict)
        self.assertEqual(
            view.get_examples(None, None), dataset.get_examples(None, None))
        self.assertEqual(view.convert(view.fetch()), 'converted')


</source>
</class>

<class classid="139" nclones="2" nlines="49" similarity="71">
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_function_and_function_node.py" startline="48" endline="119" pcid="5859">
    def call_func_function(self, backend_config, x1, x2, x3):
        forward_xp, backward_xp = _get_expected_xp(backend_config, True)

        class Func(chainer.Function):
            def __init__(self):
                self.array_init = backend_config.device.send(
                    numpy.array([3], numpy.float32))

            def forward(self, inputs):
                # Inputs
                assert isinstance(inputs, tuple)
                # x1, x3: float32
                # x2: int32
                x1, x2, x3 = inputs
                assert isinstance(x1, forward_xp.ndarray)
                assert isinstance(x2, forward_xp.ndarray)
                assert isinstance(x3, forward_xp.ndarray)

                # attribute fallback
                assert isinstance(self.array_init, forward_xp.ndarray)
                self.array_forward = forward_xp.array([2], numpy.float32)
                assert isinstance(self.array_forward, forward_xp.ndarray)

                y1 = x2 - 1  # int32
                y2 = x1 * x3 + x2.astype(x1.dtype)
                y3 = x1 + x3
                self.retain_inputs((0, 2))
                self.retain_outputs((0, 1,))
                return y1, y2, y3

            def backward(self, inputs, grad_outputs):

                # Retained inputs
                assert isinstance(inputs, tuple)
                x1, x2, x3 = inputs
                assert isinstance(x1, backward_xp.ndarray)
                assert x2 is None  # not retained
                assert isinstance(x3, backward_xp.ndarray)

                # Output gradients
                assert isinstance(grad_outputs, tuple)
                gy1, gy2, gy3 = grad_outputs
                assert gy1 is None  # y1 is int32
                # y3 is disconnected
                # TODO(niboshi): Expression after "or" is workaround for
                # chainerx. ChainerX backward should return None for
                # disconnected output and this workaround should be removed.
                assert (gy3 is None
                        or (float(gy3.max()) == 0
                            and float((-gy3).max()) == 0))

                # Retained outputs
                output_data = self.output_data
                assert isinstance(output_data, tuple)
                y1, y2, y3 = output_data
                assert isinstance(y1, backward_xp.ndarray)
                assert isinstance(y2, backward_xp.ndarray)
                assert y3 is None

                # attribute fallback
                assert isinstance(self.array_init, backward_xp.ndarray)
                assert isinstance(self.array_forward, backward_xp.ndarray)
                self.array_backward = backward_xp.array([4], numpy.float32)
                assert isinstance(self.array_backward, backward_xp.ndarray)

                gx1 = x3 * gy2  # + gy3
                gx2 = None
                gx3 = x1 * gy2  # + gy3
                return gx1, gx2, gx3

        return Func()(x1, x2, x3)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_function_and_function_node.py" startline="120" endline="195" pcid="5863">
    def call_func_function_node(self, backend_config, x1, x2, x3):
        forward_xp, backward_xp = _get_expected_xp(backend_config, False)

        class Func(chainer.FunctionNode):
            def __init__(self):
                self.array_init = backend_config.device.send(
                    numpy.array([3], numpy.float32))

            def forward(self, inputs):

                # Inputs
                # x1, x3: float32
                # x2: int32
                x1, x2, x3 = inputs
                assert isinstance(x1, forward_xp.ndarray)
                assert isinstance(x2, forward_xp.ndarray)
                assert isinstance(x3, forward_xp.ndarray)

                # attribute fallback
                assert isinstance(self.array_init, forward_xp.ndarray)
                self.array_forward = forward_xp.array([2], numpy.float32)
                assert isinstance(self.array_forward, forward_xp.ndarray)

                y1 = x2 - 1  # int32
                y2 = x1 * x3 + x2.astype(x1.dtype)
                y3 = x1 + x3
                self.retain_inputs((0, 2))
                self.retain_outputs((0, 1,))
                return y1, y2, y3

            def backward(self, input_indexes, grad_outputs):

                # Input indexes
                assert isinstance(input_indexes, tuple)
                assert input_indexes == (0, 2)

                # Retained inputs
                retained_inputs = self.get_retained_inputs()
                assert isinstance(retained_inputs, tuple)
                x1, x3 = retained_inputs
                assert isinstance(x1.array, backward_xp.ndarray)
                assert isinstance(x3.array, backward_xp.ndarray)

                # Output gradients
                assert isinstance(grad_outputs, tuple)
                gy1, gy2, gy3 = grad_outputs
                assert gy1 is None  # y1 is int32
                assert isinstance(gy2.array, backward_xp.ndarray)
                # y3 is disconnected
                # TODO(niboshi): Expression after "or" is workaround for
                # chainerx. ChainerX backward should return None for
                # disconnected output and this workaround should be removed.
                assert (gy3 is None
                        or (float(gy3.array.max()) == 0
                            and float((-gy3.array).max()) == 0))

                # Retained outputs
                retained_outputs = self.get_retained_outputs()
                assert isinstance(retained_outputs, tuple)
                y1, y2, = retained_outputs
                assert isinstance(y1.array, backward_xp.ndarray)
                assert isinstance(y2.array, backward_xp.ndarray)

                # attribute fallback
                assert isinstance(self.array_init, backward_xp.ndarray)
                assert isinstance(self.array_forward, backward_xp.ndarray)
                self.array_backward = backward_xp.array([4], numpy.float32)
                assert isinstance(self.array_backward, backward_xp.ndarray)

                gx1 = x3 * gy2  # + gy3
                gx2 = None
                gx3 = x1 * gy2  # + gy3
                return gx1, gx2, gx3

        return Func().apply((x1, x2, x3))

</source>
</class>

<class classid="140" nclones="2" nlines="11" similarity="75">
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_function_and_function_node.py" startline="342" endline="361" pcid="5876">
    def test_backprop(self, backend_config):

        x2_arr = numpy.array([2, 3], numpy.float32)
        gy1_arr = numpy.array([2, 4], numpy.float32)
        x2_arr, gy1_arr = backend_config.get_array((x2_arr, gy1_arr))

        x2 = chainer.Variable(x2_arr, requires_grad=True)

        # Forward
        y1, = self.call_func(backend_config, x2)

        assert isinstance(y1.array, backend_config.xp.ndarray)

        # Backward
        y1.grad = gy1_arr
        y1.backward()

        assert isinstance(x2.grad, backend_config.xp.ndarray)


</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/test_function_and_function_node.py" startline="474" endline="495" pcid="5884">
    def test_backprop(self, backend_config):

        x1_arr = numpy.array([2, 3], numpy.float32)
        gy2_arr = numpy.array([2, 4], numpy.float32)
        x1_arr, gy2_arr = backend_config.get_array((x1_arr, gy2_arr))

        x1 = chainer.Variable(x1_arr, requires_grad=True)

        # Forward
        y1, y2, y3 = self.call_func(backend_config, x1)

        assert y1.array is None
        assert isinstance(y2.array, backend_config.xp.ndarray)
        assert y3.array is None

        # Backward
        y2.grad = gy2_arr
        y2.backward()

        assert isinstance(x1.grad, backend_config.xp.ndarray)


</source>
</class>

<class classid="141" nclones="2" nlines="10" similarity="90">
<source file="systems/chainer-7.2.0/tests/chainer_tests/datasets_tests/test_sub_dataset.py" startline="39" endline="49" pcid="5970">
    def test_split_dataset(self):
        original = [1, 2, 3, 4, 5]
        subset1, subset2 = datasets.split_dataset(original, 2)
        self.assertEqual(len(subset1), 2)
        self.assertEqual(subset1[0], 1)
        self.assertEqual(subset1[1], 2)
        self.assertEqual(len(subset2), 3)
        self.assertEqual(subset2[0], 3)
        self.assertEqual(subset2[1], 4)
        self.assertEqual(subset2[2], 5)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/datasets_tests/test_sub_dataset.py" startline="74" endline="84" pcid="5975">
    def test_permuted_split_dataset(self):
        original = [1, 2, 3, 4, 5]
        subset1, subset2 = datasets.split_dataset(original, 2, [2, 0, 3, 1, 4])
        self.assertEqual(len(subset1), 2)
        self.assertEqual(subset1[0], 3)
        self.assertEqual(subset1[1], 1)
        self.assertEqual(len(subset2), 3)
        self.assertEqual(subset2[0], 4)
        self.assertEqual(subset2[1], 2)
        self.assertEqual(subset2[2], 5)

</source>
</class>

<class classid="142" nclones="2" nlines="32" similarity="87">
<source file="systems/chainer-7.2.0/tests/chainer_tests/datasets_tests/test_sub_dataset.py" startline="153" endline="186" pcid="5980">
    def test_get_cross_validation_datasets(self):
        original = [1, 2, 3, 4, 5, 6]
        cv1, cv2, cv3 = datasets.get_cross_validation_datasets(original, 3)

        tr1, te1 = cv1
        self.assertEqual(len(tr1), 4)
        self.assertEqual(tr1[0], 1)
        self.assertEqual(tr1[1], 2)
        self.assertEqual(tr1[2], 3)
        self.assertEqual(tr1[3], 4)
        self.assertEqual(len(te1), 2)
        self.assertEqual(te1[0], 5)
        self.assertEqual(te1[1], 6)

        tr2, te2 = cv2
        self.assertEqual(len(tr2), 4)
        self.assertEqual(tr2[0], 5)
        self.assertEqual(tr2[1], 6)
        self.assertEqual(tr2[2], 1)
        self.assertEqual(tr2[3], 2)
        self.assertEqual(len(te2), 2)
        self.assertEqual(te2[0], 3)
        self.assertEqual(te2[1], 4)

        tr3, te3 = cv3
        self.assertEqual(len(tr3), 4)
        self.assertEqual(tr3[0], 3)
        self.assertEqual(tr3[1], 4)
        self.assertEqual(tr3[2], 5)
        self.assertEqual(tr3[3], 6)
        self.assertEqual(len(te3), 2)
        self.assertEqual(te3[0], 1)
        self.assertEqual(te3[1], 2)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/datasets_tests/test_sub_dataset.py" startline="187" endline="223" pcid="5981">
    def test_get_cross_validation_datasets_2(self):
        original = [1, 2, 3, 4, 5, 6, 7]
        cv1, cv2, cv3 = datasets.get_cross_validation_datasets(original, 3)

        tr1, te1 = cv1
        self.assertEqual(len(tr1), 4)
        self.assertEqual(tr1[0], 1)
        self.assertEqual(tr1[1], 2)
        self.assertEqual(tr1[2], 3)
        self.assertEqual(tr1[3], 4)
        self.assertEqual(len(te1), 3)
        self.assertEqual(te1[0], 5)
        self.assertEqual(te1[1], 6)
        self.assertEqual(te1[2], 7)

        tr2, te2 = cv2
        self.assertEqual(len(tr2), 5)
        self.assertEqual(tr2[0], 5)
        self.assertEqual(tr2[1], 6)
        self.assertEqual(tr2[2], 7)
        self.assertEqual(tr2[3], 1)
        self.assertEqual(tr2[4], 2)
        self.assertEqual(len(te2), 2)
        self.assertEqual(te2[0], 3)
        self.assertEqual(te2[1], 4)

        tr3, te3 = cv3
        self.assertEqual(len(tr3), 5)
        self.assertEqual(tr3[0], 3)
        self.assertEqual(tr3[1], 4)
        self.assertEqual(tr3[2], 5)
        self.assertEqual(tr3[3], 6)
        self.assertEqual(tr3[4], 7)
        self.assertEqual(len(te3), 2)
        self.assertEqual(te3[0], 1)
        self.assertEqual(te3[1], 2)

</source>
</class>

<class classid="143" nclones="2" nlines="21" similarity="85">
<source file="systems/chainer-7.2.0/tests/chainer_tests/backends_tests/test_cuda.py" startline="216" endline="238" pcid="6116">
    def _check_list_tuple(self, typ):
        assert typ in (list, tuple)
        a = numpy.random.uniform(-1, 1, (0,))
        b = numpy.random.uniform(-1, 1, (2, 3))
        c = cuda.cupy.random.uniform(-1, 1, (0,))
        d = cuda.cupy.random.uniform(-1, 1, (2, 2))
        xs = typ([a, b, c, d, None, a, b, None, c, d])
        xs_cpu = cuda.to_cpu(xs)

        assert isinstance(xs_cpu, typ)
        assert len(xs) == len(xs_cpu)
        for i in (0, 1, 2, 3, 5, 6, 8, 9):
            assert isinstance(xs_cpu[i], numpy.ndarray)
            cuda.cupy.testing.assert_array_equal(xs[i], xs_cpu[i])
        assert xs_cpu[0] is a
        assert xs_cpu[1] is b
        assert xs_cpu[2] is xs_cpu[8]
        assert xs_cpu[3] is xs_cpu[9]
        assert xs_cpu[4] is None
        assert xs_cpu[5] is a
        assert xs_cpu[6] is b
        assert xs_cpu[7] is None

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/backends_tests/test_cuda.py" startline="403" endline="425" pcid="6135">
    def _check_list_tuple(self, typ):
        assert typ in (list, tuple)
        a = numpy.random.uniform(-1, 1, (0,))
        b = numpy.random.uniform(-1, 1, (2, 3))
        c = cuda.cupy.random.uniform(-1, 1, (0,))
        d = cuda.cupy.random.uniform(-1, 1, (2, 2))
        xs = typ([a, b, c, d, None, a, b, None, c, d])
        xs_gpu = cuda.to_gpu(xs)

        assert isinstance(xs_gpu, typ)
        assert len(xs) == len(xs_gpu)
        for i in (0, 1, 2, 3, 5, 6, 8, 9):
            assert isinstance(xs_gpu[i], cuda.cupy.ndarray)
            cuda.cupy.testing.assert_array_equal(xs[i], xs_gpu[i])
        assert xs_gpu[0] is xs_gpu[5]
        assert xs_gpu[1] is xs_gpu[6]
        assert xs_gpu[2] is c
        assert xs_gpu[3] is d
        assert xs_gpu[4] is None
        assert xs_gpu[7] is None
        assert xs_gpu[8] is c
        assert xs_gpu[9] is d

</source>
</class>

<class classid="144" nclones="2" nlines="17" similarity="100">
<source file="systems/chainer-7.2.0/tests/chainer_tests/backends_tests/test_cuda.py" startline="262" endline="281" pcid="6120">
    def test_numpy_scalar(self):
        dtype = self.dtype
        if dtype is numpy.bool_:
            x = dtype(True)
        elif issubclass(dtype, numpy.complex_):
            x = dtype(3.2 - 2.4j)
        elif issubclass(dtype, numpy.integer):
            x = dtype(3)
        elif issubclass(dtype, numpy.floating):
            x = dtype(3.2)
        else:
            assert False

        y = cuda.to_cpu(x)
        assert isinstance(y, numpy.ndarray)
        assert y.shape == ()
        assert y.dtype == dtype
        assert y == x


</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/backends_tests/test_cuda.py" startline="451" endline="470" pcid="6139">
    def test_numpy_scalar(self):
        dtype = self.dtype
        if dtype is numpy.bool_:
            x = dtype(True)
        elif issubclass(dtype, numpy.complex_):
            x = dtype(3.2 - 2.4j)
        elif issubclass(dtype, numpy.integer):
            x = dtype(3)
        elif issubclass(dtype, numpy.floating):
            x = dtype(3.2)
        else:
            assert False

        y = cuda.to_gpu(x)
        assert isinstance(y, cuda.ndarray)
        assert y.shape == ()
        assert y.dtype == dtype
        assert y == x


</source>
</class>

<class classid="145" nclones="2" nlines="11" similarity="81">
<source file="systems/chainer-7.2.0/tests/chainer_tests/backends_tests/test_cuda.py" startline="507" endline="523" pcid="6143">
    def test_get_device_from_array(self, backend_config):
        with cuda.Device(backend_config.cuda_device):
            arr = cuda.ndarray((), numpy.float32)
        # Test precondition check
        assert arr.device.id == backend_config.cuda_device

        expected_device = backend_config.device

        device = backend.GpuDevice.from_array(arr)
        self.check_device(device, backend_config)
        assert device == expected_device

        device = backend.get_device_from_array(arr)
        self.check_device(device, backend_config)
        assert device == expected_device


</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/backends_tests/test_chainerx.py" startline="51" endline="67" pcid="6165">
    def test_from_array(self, backend_config):
        arr = backend_config.get_array(numpy.ndarray((2,), numpy.float32))
        # Test precondition check
        assert arr.device.name == backend_config.chainerx_device

        expected_device = backend_config.device

        # ChainerxDevice.from_array
        device = backend.ChainerxDevice.from_array(arr)
        self.check_device(device, backend_config)
        assert device == expected_device

        # backend.get_device_from_array
        device = backend.get_device_from_array(arr)
        self.check_device(device, backend_config)
        assert device == expected_device

</source>
</class>

<class classid="146" nclones="5" nlines="12" similarity="75">
<source file="systems/chainer-7.2.0/tests/chainer_tests/distributions_tests/test_beta.py" startline="18" endline="33" pcid="6174">
    def setUp_configure(self):
        from scipy import stats
        self.dist = distributions.Beta
        self.scipy_dist = stats.beta

        self.test_targets = set([
            'batch_shape', 'entropy', 'event_shape', 'log_prob', 'mean',
            'sample', 'support', 'variance'])

        a = numpy.random.uniform(0, 10, self.shape).astype(numpy.float32)
        b = numpy.random.uniform(0, 10, self.shape).astype(numpy.float32)
        self.params = {'a': a, 'b': b}
        self.scipy_params = {'a': a, 'b': b}

        self.support = '[0, 1]'

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/distributions_tests/test_chisquare.py" startline="18" endline="32" pcid="6244">
    def setUp_configure(self):
        from scipy import stats
        self.dist = distributions.Chisquare
        self.scipy_dist = stats.chi2

        self.test_targets = set([
            'batch_shape', 'entropy', 'event_shape', 'log_prob', 'mean',
            'sample', 'support', 'variance'])

        k = numpy.random.randint(1, 10, self.shape).astype(numpy.float32)
        self.params = {'k': k}
        self.scipy_params = {'df': k}

        self.support = 'positive'

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/distributions_tests/test_poisson.py" startline="18" endline="33" pcid="6207">
    def setUp_configure(self):
        from scipy import stats
        self.dist = distributions.Poisson
        self.scipy_dist = stats.poisson

        self.test_targets = set([
            'batch_shape', 'event_shape', 'log_prob', 'mean', 'sample',
            'support', 'variance'])

        lam = numpy.random.uniform(0.1, 10, self.shape).astype(numpy.float32)
        self.params = {'lam': lam}
        self.scipy_params = {'mu': lam}

        self.continuous = False
        self.support = 'non negative integer'

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/distributions_tests/test_geometric.py" startline="18" endline="33" pcid="6178">
    def setUp_configure(self):
        from scipy import stats
        self.dist = distributions.Geometric
        self.scipy_dist = stats.geom

        self.test_targets = set([
            'batch_shape', 'event_shape', 'log_prob', 'mean', 'sample',
            'support', 'variance'])

        p = numpy.random.uniform(0, 1, self.shape).astype(numpy.float32)
        self.params = {'p': p}
        self.scipy_params = {'p': p}

        self.support = 'positive integer'
        self.continuous = False

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/distributions_tests/test_dirichlet.py" startline="18" endline="33" pcid="6246">
    def setUp_configure(self):
        from scipy import stats
        self.dist = distributions.Dirichlet
        self.scipy_dist = stats.dirichlet

        self.test_targets = set([
            'batch_shape', 'entropy', 'event_shape', 'mean', 'sample',
            'support', 'variance'])

        alpha = numpy.random.uniform(
            0, 10, self.shape + (3,)).astype(numpy.float32)
        self.params = {'alpha': alpha}
        self.scipy_params = {'alpha': alpha}
        self.support = '[0, 1]'
        self.event_shape = (3,)

</source>
</class>

<class classid="147" nclones="4" nlines="13" similarity="71">
<source file="systems/chainer-7.2.0/tests/chainer_tests/distributions_tests/test_gumbel.py" startline="19" endline="34" pcid="6192">
    def setUp_configure(self):
        from scipy import stats
        self.dist = distributions.Gumbel
        self.scipy_dist = stats.gumbel_r

        self.test_targets = set([
            'batch_shape', 'entropy', 'event_shape', 'log_prob', 'mean',
            'sample', 'support', 'variance'])

        loc = utils.force_array(
            numpy.random.uniform(-1, 1, self.shape).astype(numpy.float32))
        scale = utils.force_array(numpy.exp(
            numpy.random.uniform(-1, 1, self.shape)).astype(numpy.float32))
        self.params = {'loc': loc, 'scale': scale}
        self.scipy_params = {'loc': loc, 'scale': scale}

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/distributions_tests/test_cauchy.py" startline="22" endline="37" pcid="6248">
    def setUp_configure(self):
        from scipy import stats
        self.dist = distributions.Cauchy
        self.scipy_dist = stats.cauchy

        self.test_targets = set(['batch_shape', 'cdf', 'entropy',
                                 'event_shape', 'icdf', 'log_prob',
                                 'support'])

        loc = utils.force_array(
            numpy.random.uniform(-1, 1, self.shape).astype(numpy.float32))
        scale = utils.force_array(numpy.exp(
            numpy.random.uniform(-1, 1, self.shape)).astype(numpy.float32))
        self.params = {'loc': loc, 'scale': scale}
        self.scipy_params = {'loc': loc, 'scale': scale}

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/distributions_tests/test_log_normal.py" startline="19" endline="36" pcid="6288">
    def setUp_configure(self):
        from scipy import stats
        self.dist = distributions.LogNormal
        self.scipy_dist = stats.lognorm

        self.test_targets = set([
            'batch_shape', 'entropy', 'event_shape', 'log_prob', 'mean',
            'sample', 'support', 'variance'])

        mu = utils.force_array(
            numpy.random.uniform(-1, 1, self.shape).astype(numpy.float32))
        sigma = utils.force_array(numpy.exp(numpy.random.uniform(
            -1, 0, self.shape)).astype(numpy.float32))
        self.params = {'mu': mu, 'sigma': sigma}
        self.scipy_params = {'s': sigma, 'scale': numpy.exp(mu)}

        self.support = 'positive'

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/distributions_tests/test_laplace.py" startline="24" endline="39" pcid="6290">
    def setUp_configure(self):
        from scipy import stats
        self.dist = distributions.Laplace
        self.scipy_dist = stats.laplace

        self.test_targets = set([
            'batch_shape', 'cdf', 'entropy', 'event_shape', 'icdf', 'log_prob',
            'mean', 'prob', 'sample', 'stddev', 'support', 'variance'])

        loc = utils.force_array(
            numpy.random.uniform(-1, 1, self.shape).astype(numpy.float32))
        scale = utils.force_array(numpy.exp(
            numpy.random.uniform(-1, 1, self.shape)).astype(numpy.float32))
        self.params = {'loc': loc, 'scale': scale}
        self.scipy_params = {'loc': loc, 'scale': scale}

</source>
</class>

<class classid="148" nclones="2" nlines="11" similarity="81">
<source file="systems/chainer-7.2.0/tests/chainer_tests/distributions_tests/test_bernoulli.py" startline="66" endline="77" pcid="6225">
    def check_log_prob_binary_check(self, is_gpu):
        smp = self.sample_for_binary_check_test()
        if is_gpu:
            log_prob = self.gpu_dist.log_prob(cuda.to_gpu(smp)).data
        else:
            log_prob = self.cpu_dist.log_prob(smp).data
        xp = backend.get_array_module(log_prob)
        if self.binary_check:
            self.assertTrue(xp.all(log_prob == -xp.inf))
        else:
            self.assertTrue(xp.all(xp.isfinite(log_prob)))

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/distributions_tests/test_bernoulli.py" startline="85" endline="96" pcid="6228">
    def check_prob_binary_check(self, is_gpu):
        smp = self.sample_for_binary_check_test()
        if is_gpu:
            prob = self.gpu_dist.prob(cuda.to_gpu(smp)).data
        else:
            prob = self.cpu_dist.prob(smp).data
        xp = backend.get_array_module(prob)
        if self.binary_check:
            self.assertTrue(xp.all(prob == 0))
        else:
            self.assertTrue(xp.all(prob > 0))

</source>
</class>

<class classid="149" nclones="2" nlines="15" similarity="100">
<source file="systems/chainer-7.2.0/tests/chainer_tests/distributions_tests/test_cauchy.py" startline="43" endline="59" pcid="6250">
    def check_mean(self, is_gpu):
        with testing.assert_warns(RuntimeWarning):
            if is_gpu:
                mean1 = self.gpu_dist.mean.data
            else:
                mean1 = self.cpu_dist.mean.data

        if self.scipy_onebyone:
            mean2 = []
            for one_params in self.scipy_onebyone_params_iter():
                mean2.append(self.scipy_dist.mean(**one_params))
            mean2 = numpy.vstack(mean2).reshape(
                self.shape + self.cpu_dist.event_shape)
        else:
            mean2 = self.scipy_dist.mean(**self.scipy_params)
        array.assert_allclose(mean1, mean2)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/distributions_tests/test_cauchy.py" startline="88" endline="104" pcid="6256">
    def check_variance(self, is_gpu):
        with testing.assert_warns(RuntimeWarning):
            if is_gpu:
                variance1 = self.gpu_dist.variance.data
            else:
                variance1 = self.cpu_dist.variance.data

        if self.scipy_onebyone:
            variance2 = []
            for one_params in self.scipy_onebyone_params_iter():
                variance2.append(self.scipy_dist.var(**one_params))
            variance2 = numpy.vstack(variance2).reshape(
                self.shape + self.cpu_dist.event_shape)
        else:
            variance2 = self.scipy_dist.var(**self.scipy_params)
        array.assert_allclose(variance1, variance2)

</source>
</class>

<class classid="150" nclones="2" nlines="13" similarity="100">
<source file="systems/chainer-7.2.0/tests/chainer_tests/distributions_tests/test_cauchy.py" startline="67" endline="80" pcid="6253">
    def check_sample(self, is_gpu):
        if is_gpu:
            smp1 = self.gpu_dist.sample(
                sample_shape=(100000,)+self.sample_shape).data
            smp1 = cuda.to_cpu(smp1)
        else:
            smp1 = self.cpu_dist.sample(
                sample_shape=(100000,)+self.sample_shape).data
        smp2 = self.scipy_dist.rvs(
            size=(100000,)+self.sample_shape+self.shape, **self.scipy_params)
        testing.assert_allclose(numpy.median(smp1, axis=0),
                                numpy.median(smp2, axis=0),
                                atol=3e-2, rtol=3e-2)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/distributions_tests/test_pareto.py" startline="43" endline="56" pcid="6261">
    def check_sample(self, is_gpu):
        if is_gpu:
            smp1 = self.gpu_dist.sample(
                sample_shape=(100000,)+self.sample_shape).data
            smp1 = cuda.to_cpu(smp1)
        else:
            smp1 = self.cpu_dist.sample(
                sample_shape=(100000,)+self.sample_shape).data
        smp2 = self.scipy_dist.rvs(
            size=(100000,)+self.sample_shape+self.shape, **self.scipy_params)
        testing.assert_allclose(numpy.median(smp1, axis=0),
                                numpy.median(smp2, axis=0),
                                atol=3e-2, rtol=3e-2)

</source>
</class>

<class classid="151" nclones="2" nlines="11" similarity="81">
<source file="systems/chainer-7.2.0/tests/chainer_tests/training_tests/extensions_tests/test_evaluator.py" startline="69" endline="82" pcid="6349">
    def setUp(self):
        self.data = [
            numpy.random.uniform(-1, 1, (3, 4)).astype('f') for _ in range(2)]
        self.batches = [
            numpy.random.uniform(-1, 1, (2, 3, 4)).astype('f')
            for _ in range(2)]

        self.iterator = DummyIterator(self.data)
        self.converter = DummyConverter(self.batches)
        self.target = DummyModel(self)
        self.evaluator = extensions.Evaluator(
            self.iterator, self.target, converter=self.converter)
        self.expect_mean = numpy.mean([numpy.sum(x) for x in self.batches])

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/training_tests/extensions_tests/test_evaluator.py" startline="232" endline="245" pcid="6359">
    def setUp(self):
        self.data = [
            numpy.random.uniform(-1, 1, (3, 4)).astype('f') for _ in range(2)]
        self.batches = [
            numpy.random.uniform(-1, 1, (2, 3, 4)).astype('f')
            for _ in range(2)]

        self.iterator = DummyIterator(self.data)
        self.converter = DummyConverter(self.batches)
        self.target = DummyModel(self)
        self.evaluator = extensions.Evaluator(
            self.iterator, {}, converter=self.converter,
            eval_func=self.target)

</source>
</class>

<class classid="152" nclones="3" nlines="10" similarity="80">
<source file="systems/chainer-7.2.0/tests/chainer_tests/training_tests/extensions_tests/test_polynomial_shift.py" startline="28" endline="40" pcid="6389">
    def setUp(self):
        self.optimizer = mock.MagicMock()
        self.extension = extensions.PolynomialShift(
            'x', self.rate, self.max_count, self.init, self.target,
            self.optimizer)

        self.interval = 4
        self.expect = [e for e in self.expect for _ in range(self.interval)]
        self.trigger = util.get_trigger((self.interval, 'iteration'))

        self.trainer = testing.get_trainer_with_mock_updater(self.trigger)
        self.trainer.updater.get_optimizer.return_value = self.optimizer

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/training_tests/extensions_tests/test_inverse_shift.py" startline="26" endline="38" pcid="6412">
    def setUp(self):
        self.optimizer = mock.MagicMock()
        self.extension = extensions.InverseShift(
            'x', self.gamma, self.power, self.init, self.target,
            self.optimizer)

        self.interval = 4
        self.expect = [e for e in self.expect for _ in range(self.interval)]
        self.trigger = util.get_trigger((self.interval, 'iteration'))

        self.trainer = testing.get_trainer_with_mock_updater(self.trigger)
        self.trainer.updater.get_optimizer.return_value = self.optimizer

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/training_tests/extensions_tests/test_warmup_shift.py" startline="24" endline="36" pcid="6462">
    def setUp(self):
        self.optimizer = mock.MagicMock()
        self.extension = extensions.WarmupShift(
            'x', self.warmup_start, self.warmup_iter,
            self.init, self.optimizer)

        self.interval = 1
        self.expect = [e for e in self.expect for _ in range(self.interval)]
        self.trigger = util.get_trigger((self.interval, 'iteration'))

        self.trainer = testing.get_trainer_with_mock_updater(self.trigger)
        self.trainer.updater.get_optimizer.return_value = self.optimizer

</source>
</class>

<class classid="153" nclones="5" nlines="11" similarity="100">
<source file="systems/chainer-7.2.0/tests/chainer_tests/training_tests/extensions_tests/test_polynomial_shift.py" startline="41" endline="54" pcid="6390">
    def _run_trainer(self, extension, expect, optimizer=None):
        if optimizer is None:
            optimizer = self.optimizer
        extension.initialize(self.trainer)

        actual = []
        for _ in expect:
            self.trainer.updater.update()
            actual.append(optimizer.x)
            if self.trigger(self.trainer):
                extension(self.trainer)

        self.assertEqual(actual, expect)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/training_tests/extensions_tests/test_inverse_shift.py" startline="39" endline="52" pcid="6413">
    def _run_trainer(self, extension, expect, optimizer=None):
        if optimizer is None:
            optimizer = self.optimizer
        extension.initialize(self.trainer)

        actual = []
        for _ in expect:
            self.trainer.updater.update()
            actual.append(optimizer.x)
            if self.trigger(self.trainer):
                extension(self.trainer)

        self.assertEqual(actual, expect)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/training_tests/extensions_tests/test_linear_shift.py" startline="27" endline="40" pcid="6427">
    def _run_trainer(self, extension, expect, optimizer=None):
        if optimizer is None:
            optimizer = self.optimizer
        extension.initialize(self.trainer)

        actual = []
        for _ in expect:
            self.trainer.updater.update()
            actual.append(optimizer.x)
            if self.trigger(self.trainer):
                extension(self.trainer)

        self.assertEqual(actual, expect)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/training_tests/extensions_tests/test_step_shift.py" startline="38" endline="51" pcid="6457">
    def _run_trainer(self, extension, expect, optimizer=None):
        if optimizer is None:
            optimizer = self.optimizer
        extension.initialize(self.trainer)

        actual = []
        for _ in expect:
            self.trainer.updater.update()
            actual.append(optimizer.x)
            if self.trigger(self.trainer):
                extension(self.trainer)

        self.assertEqual(actual, expect)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/training_tests/extensions_tests/test_exponential_shift.py" startline="32" endline="45" pcid="6402">
    def _run_trainer(self, extension, expect, optimizer=None):
        if optimizer is None:
            optimizer = self.optimizer
        extension.initialize(self.trainer)

        actual = []
        for _ in expect:
            self.trainer.updater.update()
            actual.append(optimizer.x)
            if self.trigger(self.trainer):
                extension(self.trainer)

        self.assertEqual(actual, expect)

</source>
</class>

<class classid="154" nclones="5" nlines="13" similarity="84">
<source file="systems/chainer-7.2.0/tests/chainer_tests/training_tests/extensions_tests/test_polynomial_shift.py" startline="75" endline="92" pcid="6394">
    def test_resume(self):
        new_optimizer = mock.Mock()
        new_extension = extensions.PolynomialShift(
            'x', self.rate, self.max_count, self.init, self.target,
            new_optimizer)

        self.trainer.extend(self.extension)
        self.trainer.run()

        new_trainer = testing.get_trainer_with_mock_updater((3, 'iteration'))
        new_trainer.extend(new_extension)
        testing.save_and_load_npz(self.trainer, new_trainer)

        new_extension.initialize(new_trainer)
        self.assertEqual(new_optimizer.x, self.optimizer.x)
        self.assertIsInstance(new_optimizer.x, float)


</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/training_tests/extensions_tests/test_exponential_shift.py" startline="66" endline="82" pcid="6406">
    def test_resume(self):
        new_optimizer = mock.Mock()
        new_extension = extensions.ExponentialShift(
            'x', self.rate, self.init, self.target, new_optimizer)

        self.trainer.extend(self.extension)
        self.trainer.run()

        new_trainer = testing.get_trainer_with_mock_updater((3, 'iteration'))
        new_trainer.extend(new_extension)
        testing.save_and_load_npz(self.trainer, new_trainer)

        new_extension.initialize(new_trainer)
        self.assertEqual(new_optimizer.x, self.optimizer.x)
        self.assertIsInstance(new_optimizer.x, float)


</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/training_tests/extensions_tests/test_inverse_shift.py" startline="73" endline="89" pcid="6417">
    def test_resume(self):
        new_optimizer = mock.Mock()
        new_extension = extensions.InverseShift(
            'x', self.gamma, self.power, self.init, self.target, new_optimizer)

        self.trainer.extend(self.extension)
        self.trainer.run()

        new_trainer = testing.get_trainer_with_mock_updater((3, 'iteration'))
        new_trainer.extend(new_extension)
        testing.save_and_load_npz(self.trainer, new_trainer)

        new_extension.initialize(new_trainer)
        self.assertEqual(new_optimizer.x, self.optimizer.x)
        self.assertIsInstance(new_optimizer.x, float)


</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/training_tests/extensions_tests/test_linear_shift.py" startline="54" endline="70" pcid="6430">
    def test_resume(self):
        new_optimizer = mock.Mock()
        new_extension = extensions.LinearShift(
            'x', self.value_range, self.time_range, new_optimizer)

        self.trainer.extend(self.extension)
        self.trainer.run()

        new_trainer = testing.get_trainer_with_mock_updater((5, 'iteration'))
        new_trainer.extend(new_extension)
        testing.save_and_load_npz(self.trainer, new_trainer)

        new_extension.initialize(new_trainer)
        self.assertEqual(new_optimizer.x, self.optimizer.x)
        self.assertIsInstance(new_optimizer.x, float)


</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/training_tests/extensions_tests/test_step_shift.py" startline="71" endline="87" pcid="6461">
    def test_resume(self):
        new_optimizer = mock.Mock()
        new_extension = extensions.StepShift(
            'x', self.gamma, self.step, self.init, self.target, new_optimizer)

        self.trainer.extend(self.extension)
        self.trainer.run()

        new_trainer = testing.get_trainer_with_mock_updater((5, 'iteration'))
        new_trainer.extend(new_extension)
        testing.save_and_load_npz(self.trainer, new_trainer)

        new_extension.initialize(new_trainer)
        self.assertEqual(new_optimizer.x, self.optimizer.x)
        self.assertIsInstance(new_optimizer.x, float)


</source>
</class>

<class classid="155" nclones="2" nlines="12" similarity="83">
<source file="systems/chainer-7.2.0/tests/chainer_tests/training_tests/test_trainer.py" startline="183" endline="198" pcid="6499">
    def test_add_two_extensions_default_priority(self):
        self.called_order = []

        @training.make_extension(trigger=(1, 'epoch'))
        def dummy_extension_1(trainer):
            self.called_order.append(1)

        @training.make_extension(trigger=(1, 'epoch'))
        def dummy_extension_2(trainer):
            self.called_order.append(2)

        self.trainer.extend(dummy_extension_1)
        self.trainer.extend(dummy_extension_2)
        self.trainer.run()
        self.assertEqual(self.called_order, [1, 2])

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/training_tests/test_trainer.py" startline="199" endline="214" pcid="6502">
    def test_add_two_extensions_specific_priority(self):
        self.called_order = []

        @training.make_extension(trigger=(1, 'epoch'), priority=50)
        def dummy_extension_1(trainer):
            self.called_order.append(1)

        @training.make_extension(trigger=(1, 'epoch'), priority=100)
        def dummy_extension_2(trainer):
            self.called_order.append(2)

        self.trainer.extend(dummy_extension_1)
        self.trainer.extend(dummy_extension_2)
        self.trainer.run()
        self.assertEqual(self.called_order, [2, 1])

</source>
</class>

<class classid="156" nclones="2" nlines="18" similarity="84">
<source file="systems/chainer-7.2.0/tests/chainer_tests/training_tests/test_trainer.py" startline="215" endline="241" pcid="6505">
    def test_exception_handler(self):

        ext = ErrorHandlingExtension()
        self.trainer.extend(ext, trigger=(1, 'iteration'), priority=1)
        self.assertFalse(ext.is_error_handled)

        d = {}

        def exception_handler(trainer, exp, tb):
            d['called'] = True

        @training.make_extension(trigger=(1, 'iteration'), priority=100,
                                 on_error=exception_handler)
        def exception_raiser(trainer):
            raise TheOnlyError()
        self.trainer.extend(exception_raiser)

        dummy_extension = DummyExtension(self)
        self.trainer.extend(dummy_extension)

        with self.assertRaises(TheOnlyError):
            self.trainer.run()

        self.assertTrue(d['called'])
        self.assertTrue(ext.is_error_handled)
        self.assertTrue(dummy_extension.is_finalized)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/training_tests/test_trainer.py" startline="242" endline="266" pcid="6508">
    def test_exception_in_exception_handler(self):

        ext = ErrorHandlingExtension()
        self.trainer.extend(ext, trigger=(1, 'iteration'), priority=1)
        self.assertFalse(ext.is_error_handled)

        def exception_handler(trainer, exp, tb):
            raise ValueError('hogehoge from exception handler')

        @training.make_extension(trigger=(1, 'iteration'), priority=100,
                                 on_error=exception_handler)
        def exception_raiser(trainer):
            raise TheOnlyError()
        self.trainer.extend(exception_raiser)

        dummy_extension = DummyExtension(self)
        self.trainer.extend(dummy_extension)

        with self.assertRaises(TheOnlyError):
            self.trainer.run()

        self.assertTrue(ext.is_error_handled)
        self.assertTrue(dummy_extension.is_finalized)


</source>
</class>

<class classid="157" nclones="7" nlines="21" similarity="70">
<source file="systems/chainer-7.2.0/tests/chainer_tests/training_tests/triggers_tests/test_once_trigger.py" startline="56" endline="75" pcid="6523">
    def test_resumed_trigger(self):
        trainer = testing.get_trainer_with_mock_updater(
            stop_trigger=None, iter_per_epoch=self.iter_per_epoch)
        with tempfile.NamedTemporaryFile(delete=False) as f:
            trigger = training.triggers.OnceTrigger(self.call_on_resume)
            for expected, finished in zip(self.resumed_expected[:self.resume],
                                          self.resumed_finished[:self.resume]):
                trainer.updater.update()
                self.assertEqual(trigger.finished, finished)
                self.assertEqual(trigger(trainer), expected)
            serializers.save_npz(f.name, trigger)

            trigger = training.triggers.OnceTrigger(self.call_on_resume)
            serializers.load_npz(f.name, trigger)
            for expected, finished in zip(self.resumed_expected[self.resume:],
                                          self.resumed_finished[self.resume:]):
                trainer.updater.update()
                self.assertEqual(trigger.finished, finished)
                self.assertEqual(trigger(trainer), expected)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/training_tests/triggers_tests/test_manual_schedule_trigger.py" startline="83" endline="102" pcid="6529">
    def test_resumed_trigger(self):
        trainer = testing.get_trainer_with_mock_updater(
            stop_trigger=None, iter_per_epoch=self.iter_per_epoch)
        with tempfile.NamedTemporaryFile(delete=False) as f:
            trigger = training.triggers.ManualScheduleTrigger(*self.schedule)
            for expected, finished in zip(self.expected[:self.resume],
                                          self.finished[:self.resume]):
                trainer.updater.update()
                self.assertEqual(trigger(trainer), expected)
                self.assertEqual(trigger.finished, finished)
            serializers.save_npz(f.name, trigger)

            trigger = training.triggers.ManualScheduleTrigger(*self.schedule)
            serializers.load_npz(f.name, trigger)
            for expected, finished in zip(self.expected[self.resume:],
                                          self.finished[self.resume:]):
                trainer.updater.update()
                self.assertEqual(trigger(trainer), expected)
                self.assertEqual(trigger.finished, finished)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/training_tests/triggers_tests/test_manual_schedule_trigger.py" startline="145" endline="167" pcid="6532">
    def test_resumed_trigger_backward_compat(self):
        trainer = testing.get_trainer_with_mock_updater(
            stop_trigger=None, iter_per_epoch=self.iter_per_epoch)
        with tempfile.NamedTemporaryFile(delete=False) as f:
            trigger = training.triggers.ManualScheduleTrigger(*self.schedule)
            for expected, finished in zip(self.expected[:self.resume],
                                          self.finished[:self.resume]):
                trainer.updater.update()
                self.assertEqual(trigger(trainer), expected)
                self.assertEqual(trigger.finished, finished)
            # old version does not save anything
            np.savez(f, dummy=0)

            trigger = training.triggers.ManualScheduleTrigger(*self.schedule)
            with testing.assert_warns(UserWarning):
                serializers.load_npz(f.name, trigger)
            for expected, finished in zip(self.expected[self.resume:],
                                          self.finished[self.resume:]):
                trainer.updater.update()
                self.assertEqual(trigger(trainer), expected)
                self.assertEqual(trigger.finished, finished)


</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/training_tests/triggers_tests/test_once_trigger.py" startline="126" endline="148" pcid="6526">
    def test_resumed_trigger_backward_compat(self):
        trainer = testing.get_trainer_with_mock_updater(
            stop_trigger=None, iter_per_epoch=self.iter_per_epoch)
        with tempfile.NamedTemporaryFile(delete=False) as f:
            trigger = training.triggers.OnceTrigger(self.call_on_resume)
            for expected, finished in zip(self.resumed_expected[:self.resume],
                                          self.resumed_finished[:self.resume]):
                trainer.updater.update()
                self.assertEqual(trigger.finished, finished)
                self.assertEqual(trigger(trainer), expected)
            # old version does not save anything
            np.savez(f, dummy=0)

            trigger = training.triggers.OnceTrigger(self.call_on_resume)
            with testing.assert_warns(UserWarning):
                serializers.load_npz(f.name, trigger)
            for expected, finished in zip(self.resumed_expected[self.resume:],
                                          self.resumed_finished[self.resume:]):
                trainer.updater.update()
                self.assertEqual(trigger.finished, finished)
                self.assertEqual(trigger(trainer), expected)


</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/training_tests/triggers_tests/test_manual_schedule_trigger.py" startline="118" endline="144" pcid="6531">
    def test_resumed_trigger_sparse_call(self):
        trainer = testing.get_trainer_with_mock_updater(
            stop_trigger=None, iter_per_epoch=self.iter_per_epoch)
        accumulated = False
        with tempfile.NamedTemporaryFile(delete=False) as f:
            trigger = training.triggers.ManualScheduleTrigger(*self.schedule)
            for expected, finished in zip(self.expected[:self.resume],
                                          self.finished[:self.resume]):
                trainer.updater.update()
                accumulated = accumulated or expected
                if random.randrange(2):
                    self.assertEqual(trigger(trainer), accumulated)
                    self.assertEqual(trigger.finished, finished)
                    accumulated = False
            serializers.save_npz(f.name, trigger)

            trigger = training.triggers.ManualScheduleTrigger(*self.schedule)
            serializers.load_npz(f.name, trigger)
            for expected, finished in zip(self.expected[self.resume:],
                                          self.finished[self.resume:]):
                trainer.updater.update()
                accumulated = accumulated or expected
                if random.randrange(2):
                    self.assertEqual(trigger(trainer), accumulated)
                    self.assertEqual(trigger.finished, finished)
                    accumulated = False

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/training_tests/triggers_tests/test_once_trigger.py" startline="94" endline="125" pcid="6525">
    def test_resumed_trigger_sparse_call(self):
        trainer = testing.get_trainer_with_mock_updater(
            stop_trigger=None, iter_per_epoch=self.iter_per_epoch)
        accumulated = False
        accumulated_finished = True
        with tempfile.NamedTemporaryFile(delete=False) as f:
            trigger = training.triggers.OnceTrigger(self.call_on_resume)
            for expected, finished in zip(self.resumed_expected[:self.resume],
                                          self.resumed_finished[:self.resume]):
                trainer.updater.update()
                accumulated = accumulated or expected
                accumulated_finished = accumulated_finished and finished
                if random.randrange(2):
                    self.assertEqual(trigger.finished, accumulated_finished)
                    self.assertEqual(trigger(trainer), accumulated)
                    accumulated = False
                    accumulated_finished = True
            serializers.save_npz(f.name, trigger)

            trigger = training.triggers.OnceTrigger(self.call_on_resume)
            serializers.load_npz(f.name, trigger)
            for expected, finished in zip(self.resumed_expected[self.resume:],
                                          self.resumed_finished[self.resume:]):
                trainer.updater.update()
                accumulated = accumulated or expected
                accumulated_finished = accumulated_finished and finished
                if random.randrange(2):
                    self.assertEqual(trigger.finished, accumulated_finished)
                    self.assertEqual(trigger(trainer), accumulated)
                    accumulated = False
                    accumulated_finished = True

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/training_tests/triggers_tests/test_interval_trigger.py" startline="79" endline="101" pcid="6537">
    def test_resumed_trigger_sparse_call(self):
        trainer = testing.get_trainer_with_mock_updater(
            stop_trigger=None, iter_per_epoch=self.iter_per_epoch)
        accumulated = False
        with tempfile.NamedTemporaryFile(delete=False) as f:
            trigger = training.triggers.IntervalTrigger(*self.interval)
            for expected in self.expected[:self.resume]:
                trainer.updater.update()
                accumulated = accumulated or expected
                if random.randrange(2):
                    self.assertEqual(trigger(trainer), accumulated)
                    accumulated = False
            serializers.save_npz(f.name, trigger)

            trigger = training.triggers.IntervalTrigger(*self.interval)
            serializers.load_npz(f.name, trigger)
            for expected in self.expected[self.resume:]:
                trainer.updater.update()
                accumulated = accumulated or expected
                if random.randrange(2):
                    self.assertEqual(trigger(trainer), accumulated)
                    accumulated = False

</source>
</class>

<class classid="158" nclones="2" nlines="12" similarity="75">
<source file="systems/chainer-7.2.0/tests/chainer_tests/training_tests/triggers_tests/test_manual_schedule_trigger.py" startline="104" endline="116" pcid="6530">
    def test_trigger_sparse_call(self):
        trainer = testing.get_trainer_with_mock_updater(
            stop_trigger=None, iter_per_epoch=self.iter_per_epoch)
        trigger = training.triggers.ManualScheduleTrigger(*self.schedule)
        accumulated = False
        for expected, finished in zip(self.expected, self.finished):
            trainer.updater.update()
            accumulated = accumulated or expected
            if random.randrange(2):
                self.assertEqual(trigger(trainer), accumulated)
                self.assertEqual(trigger.finished, finished)
                accumulated = False

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/training_tests/triggers_tests/test_interval_trigger.py" startline="65" endline="77" pcid="6536">
    def test_trigger_sparse_call(self):
        trainer = testing.get_trainer_with_mock_updater(
            stop_trigger=None, iter_per_epoch=self.iter_per_epoch)
        trigger = training.triggers.IntervalTrigger(*self.interval)
        accumulated = False
        # before the first iteration, trigger should be False
        for expected in [False] + self.expected:
            accumulated = accumulated or expected
            if random.randrange(2):
                self.assertEqual(trigger(trainer), accumulated)
                accumulated = False
            trainer.updater.update()

</source>
</class>

<class classid="159" nclones="2" nlines="15" similarity="86">
<source file="systems/chainer-7.2.0/tests/chainer_tests/training_tests/triggers_tests/test_interval_trigger.py" startline="48" endline="63" pcid="6535">
    def test_resumed_trigger(self):
        trainer = testing.get_trainer_with_mock_updater(
            stop_trigger=None, iter_per_epoch=self.iter_per_epoch)
        with tempfile.NamedTemporaryFile(delete=False) as f:
            trigger = training.triggers.IntervalTrigger(*self.interval)
            for expected in self.expected[:self.resume]:
                trainer.updater.update()
                self.assertEqual(trigger(trainer), expected)
            serializers.save_npz(f.name, trigger)

            trigger = training.triggers.IntervalTrigger(*self.interval)
            serializers.load_npz(f.name, trigger)
            for expected in self.expected[self.resume:]:
                trainer.updater.update()
                self.assertEqual(trigger(trainer), expected)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/training_tests/triggers_tests/test_interval_trigger.py" startline="102" endline="119" pcid="6538">
    def test_resumed_trigger_backward_compat(self):
        trainer = testing.get_trainer_with_mock_updater(
            stop_trigger=None, iter_per_epoch=self.iter_per_epoch)
        with tempfile.NamedTemporaryFile(delete=False) as f:
            trigger = training.triggers.IntervalTrigger(*self.interval)
            for expected in self.expected[:self.resume]:
                trainer.updater.update()
                self.assertEqual(trigger(trainer), expected)
            # old version does not save anything
            np.savez(f, dummy=0)

            trigger = training.triggers.IntervalTrigger(*self.interval)
            with testing.assert_warns(UserWarning):
                serializers.load_npz(f.name, trigger)
            for expected in self.expected[self.resume:]:
                trainer.updater.update()
                self.assertEqual(trigger(trainer), expected)

</source>
</class>

<class classid="160" nclones="4" nlines="11" similarity="72">
<source file="systems/chainer-7.2.0/tests/chainer_tests/training_tests/triggers_tests/test_early_stopping_trigger.py" startline="24" endline="38" pcid="6542">
    def test_early_stopping_trigger_with_accuracy(self):
        key = 'main/accuracy'
        trigger = triggers.EarlyStoppingTrigger(monitor=key, patience=3,
                                                check_trigger=(1, 'epoch'),
                                                verbose=False)
        trigger = util.get_trigger(trigger)

        accuracies = [0.5, 0.5, 0.6, 0.7, 0.6, 0.4, 0.3, 0.2]
        accuracies = numpy.asarray([
            chainer.Variable(numpy.asarray(acc, dtype=numpy.float32))
            for acc in accuracies])

        expected = [False, False, False, False, False, False, True, True]
        _test_trigger(self, trigger, key, accuracies, expected)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/training_tests/triggers_tests/test_early_stopping_trigger.py" startline="53" endline="67" pcid="6544">
    def test_early_stopping_trigger_with_max_epoch(self):
        key = 'main/loss'
        trigger = triggers.EarlyStoppingTrigger(monitor=key, patience=3,
                                                check_trigger=(1, 'epoch'),
                                                max_trigger=(3, 'epoch'))
        trigger = util.get_trigger(trigger)

        accuracies = [100, 80, 30]
        accuracies = numpy.asarray([
            chainer.Variable(numpy.asarray(acc, dtype=numpy.float32))
            for acc in accuracies])

        expected = [False, False, True]
        _test_trigger(self, trigger, key, accuracies, expected)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/training_tests/triggers_tests/test_early_stopping_trigger.py" startline="39" endline="52" pcid="6543">
    def test_early_stopping_trigger_with_loss(self):
        key = 'main/loss'
        trigger = triggers.EarlyStoppingTrigger(monitor=key, patience=3,
                                                check_trigger=(1, 'epoch'))
        trigger = util.get_trigger(trigger)

        accuracies = [100, 80, 30, 10, 20, 24, 30, 35]
        accuracies = numpy.asarray([
            chainer.Variable(numpy.asarray(acc, dtype=numpy.float32))
            for acc in accuracies])

        expected = [False, False, False, False, False, False, True, True]
        _test_trigger(self, trigger, key, accuracies, expected)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/training_tests/triggers_tests/test_early_stopping_trigger.py" startline="68" endline="83" pcid="6545">
    def test_early_stopping_trigger_with_max_iteration(self):
        key = 'main/loss'
        trigger = triggers.EarlyStoppingTrigger(monitor=key, patience=3,
                                                check_trigger=(1, 'epoch'),
                                                max_trigger=(3, 'iteration'))
        trigger = util.get_trigger(trigger)

        accuracies = [100, 80, 30]
        accuracies = numpy.asarray([
            chainer.Variable(numpy.asarray(acc, dtype=numpy.float32))
            for acc in accuracies])

        expected = [False, False, True]
        _test_trigger(self, trigger, key, accuracies, expected)


</source>
</class>

<class classid="161" nclones="3" nlines="15" similarity="76">
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_lstm.py" startline="77" endline="94" pcid="6555">
    def generate_grad_outputs(self, outputs_template):
        grad_out = []
        c = outputs_template[0]
        h = outputs_template[1]

        c_shape = c.shape
        h_shape = h.shape
        if self.grad_outputs[0] is True:
            grad_out.append(_shaped_random(c_shape, c.dtype))
        else:
            grad_out.append(None)

        if self.grad_outputs[1] is True:
            grad_out.append(_shaped_random(h_shape, h.dtype))
        else:
            grad_out.append(None)
        return tuple(grad_out)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_slstm.py" startline="123" endline="143" pcid="6609">
        h_expect = _sigmoid(o1_in + o2_in) * numpy.tanh(c_expect)
        return c_expect, h_expect

    def generate_grad_outputs(self, outputs_template):
        grad_out = []
        c = outputs_template[0]
        h = outputs_template[1]

        c_shape = c.shape
        h_shape = h.shape
        if self.grad_outputs[0] is True:
            grad_out.append(numpy.random.uniform(-1, 1,
                                                 h_shape).astype(h.dtype))
        else:
            grad_out.append(None)

        if self.grad_outputs[1] is True:
            grad_out.append(numpy.random.uniform(-1, 1,
                                                 c_shape).astype(c.dtype))
        else:
            grad_out.append(None)
</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_lstm.py" startline="95" endline="113" pcid="6556">
    def generate_grad_grad_inputs(self, inputs_template):
        grad_grad_in = []
        c = inputs_template[0]
        x = inputs_template[1]

        c_shape = c.shape
        x_shape = x.shape
        if self.grad_grad_inputs[0] is True:
            grad_grad_in.append(_shaped_random(c_shape, c.dtype))
        else:
            grad_grad_in.append(None)

        if self.grad_grad_inputs[1] is True:
            grad_grad_in.append(_shaped_random(x_shape, x.dtype))
        else:
            grad_grad_in.append(None)
        return tuple(grad_grad_in)


</source>
</class>

<class classid="162" nclones="11" nlines="23" similarity="70">
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_lstm.py" startline="114" endline="140" pcid="6567">

    def generate_inputs(self):
        h_shape = (self.n_layers, self.batches[0], self.hidden_size)
        dtype = numpy.float32

        h = array(h_shape, dtype)
        c = array(h_shape, dtype)
        in_size = self.input_size
        out_size = self.hidden_size
        xs = []
        for b in range(len(self.batches)):
            xs.append(array((self.batches[b], in_size), dtype))

        def w_in(i, j):
            return in_size if i == 0 and j < 4 else out_size

        inputs = []
        inputs.append(h)
        inputs.append(c)
        for i in range(len(self.batches)):
            inputs.append(xs[i])
        for n in range(self.n_layers):
            for i in range(8):
                inputs.append(array((out_size, w_in(n, i)), dtype))
            for i in range(8):
                inputs.append(array((out_size,), dtype))
        return tuple(inputs)
</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_rnn.py" startline="75" endline="98" pcid="6616">
    def generate_inputs(self):
        h_shape = (self.n_layers, self.batches[0], self.hidden_size)
        dtype = self.dtype

        h = array(h_shape, dtype)
        in_size = self.input_size
        out_size = self.hidden_size
        xs = [array((self.batches[b], in_size), dtype)
              for b in range(len(self.batches))]

        def w_in(i, j):
            return in_size if i == 0 and j < 1 else out_size

        inputs = []
        inputs.append(h)
        for i in range(len(self.batches)):
            inputs.append(xs[i])
        for n in range(self.n_layers):
            for i in range(2):
                inputs.append(array((out_size, w_in(n, i)), dtype))
            for i in range(2):
                inputs.append(array((out_size,), dtype))
        return tuple(inputs)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_gru.py" startline="62" endline="85" pcid="6584">
    def generate_inputs(self):
        h_shape = (self.n_layers, self.batches[0], self.hidden_size)
        dtype = numpy.float32

        h = array(h_shape, dtype)
        in_size = self.input_size
        out_size = self.hidden_size
        xs = [array((self.batches[b], in_size), dtype)
              for b in range(len(self.batches))]

        def w_in(i, j):
            return in_size if i == 0 and j < 3 else out_size

        inputs = []
        inputs.append(h)
        for i in range(len(self.batches)):
            inputs.append(xs[i])
        for n in range(self.n_layers):
            for i in range(6):
                inputs.append(array((out_size, w_in(n, i)), dtype))
            for i in range(6):
                inputs.append(array((out_size,), dtype))
        return tuple(inputs)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_lstm.py" startline="246" endline="278" pcid="6573">

    def generate_inputs(self):
        h_shape = (self.n_layers * 2, self.batches[0], self.hidden_size)
        dtype = numpy.float32

        h = array(h_shape, dtype)
        c = array(h_shape, dtype)
        in_size = self.input_size
        out_size = self.hidden_size
        xs = []
        for b in range(len(self.batches)):
            xs.append(array((self.batches[b], in_size), dtype))

        def w_in(i, j):
            if i == 0 and j < 4:
                return in_size
            elif i > 0 and j < 4:
                return out_size * 2
            else:
                return out_size

        inputs = []
        inputs.append(h)
        inputs.append(c)
        for i in range(len(self.batches)):
            inputs.append(xs[i])
        for n in range(self.n_layers):
            for direction in (0, 1):
                for i in range(8):
                    inputs.append(array((out_size, w_in(n, i)), dtype))
                for i in range(8):
                    inputs.append(array((out_size,), dtype))
        return tuple(inputs)
</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py" startline="479" endline="505" pcid="9668">
    def generate_inputs(self):
        h_shape = (self.n_layers, self.batches[0], self.hidden_size)
        dtype = self.in_dtypes[0]

        h = array_utils.uniform(h_shape, dtype)

        in_size = self.input_size
        out_size = self.hidden_size
        xs = [array_utils.uniform((self.batches[b], in_size), dtype)
              for b in range(len(self.batches))]

        def w_in(i, j):
            return in_size if i == 0 and j < 1 else out_size

        inputs = []
        inputs.append(h)
        for i in range(len(self.batches)):
            inputs.append(xs[i])

        for n in range(self.n_layers):
            for i in range(2):
                inputs.append(array_utils.uniform(
                    (out_size, w_in(n, i)), dtype))
            for i in range(2):
                inputs.append(array_utils.uniform((out_size,), dtype))
        return tuple(inputs)

</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py" startline="267" endline="293" pcid="9656">
    def generate_inputs(self):
        h_shape = (self.n_layers, self.batches[0], self.hidden_size)
        dtype = self.in_dtypes[0]

        h = array_utils.uniform(h_shape, dtype)

        in_size = self.input_size
        out_size = self.hidden_size
        xs = [array_utils.uniform((self.batches[b], in_size), dtype)
              for b in range(len(self.batches))]

        def w_in(i, j):
            return in_size if i == 0 and j < 3 else out_size

        inputs = []
        inputs.append(h)
        for i in range(len(self.batches)):
            inputs.append(xs[i])

        for n in range(self.n_layers):
            for i in range(6):
                inputs.append(array_utils.uniform(
                    (out_size, w_in(n, i)), dtype))
            for i in range(6):
                inputs.append(array_utils.uniform((out_size,), dtype))
        return tuple(inputs)

</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py" startline="156" endline="189" pcid="9650">
    def generate_inputs(self):
        h_shape = (self.n_layers * 2, self.batches[0], self.hidden_size)
        dtype = self.in_dtypes[0]

        h = array_utils.uniform(h_shape, dtype)
        c = array_utils.uniform(h_shape, dtype)
        in_size = self.input_size
        out_size = self.hidden_size
        xs = [array_utils.uniform((self.batches[b], in_size), dtype)
              for b in range(len(self.batches))]

        def w_in(i, j):
            if i == 0 and j < 4:
                return in_size
            elif i > 0 and j < 4:
                return out_size * 2
            else:
                return out_size

        inputs = []
        inputs.append(h)
        inputs.append(c)
        for i in range(len(self.batches)):
            inputs.append(xs[i])

        for n in range(self.n_layers):
            for direction in (0, 1):
                for i in range(8):
                    inputs.append(array_utils.uniform(
                        (out_size, w_in(n, i)), dtype))
                for i in range(8):
                    inputs.append(array_utils.uniform((out_size,), dtype))
        return tuple(inputs)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_rnn.py" startline="200" endline="230" pcid="6622">
    def generate_inputs(self):
        h_shape = (self.n_layers * 2, self.batches[0], self.hidden_size)
        dtype = self.dtype

        h = array(h_shape, dtype)
        in_size = self.input_size
        out_size = self.hidden_size
        xs = [array((self.batches[b], in_size), dtype)
              for b in range(len(self.batches))]

        def w_in(i, j):
            if i == 0 and j < 1:
                return in_size
            elif i > 0 and j < 1:
                return out_size * 2
            else:
                return out_size

        inputs = []
        inputs.append(h)
        for i in range(len(self.batches)):
            inputs.append(xs[i])

        for n in range(self.n_layers):
            for direction in (0, 1):
                for i in range(2):
                    inputs.append(array((out_size, w_in(n, i)), dtype))
                for i in range(2):
                    inputs.append(array((out_size,), dtype))
        return tuple(inputs)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_gru.py" startline="188" endline="218" pcid="6590">
    def generate_inputs(self):
        h_shape = (self.n_layers * 2, self.batches[0], self.hidden_size)
        dtype = numpy.float32

        h = array(h_shape, dtype)
        in_size = self.input_size
        out_size = self.hidden_size
        xs = [array((self.batches[b], in_size), dtype)
              for b in range(len(self.batches))]

        def w_in(i, j):
            if i == 0 and j < 3:
                return in_size
            elif i > 0 and j < 3:
                return out_size * 2
            else:
                return out_size

        inputs = []
        inputs.append(h)
        for i in range(len(self.batches)):
            inputs.append(xs[i])

        for n in range(self.n_layers):
            for direction in (0, 1):
                for i in range(6):
                    inputs.append(array((out_size, w_in(n, i)), dtype))
                for i in range(6):
                    inputs.append(array((out_size,), dtype))
        return tuple(inputs)

</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py" startline="54" endline="81" pcid="9644">
    def generate_inputs(self):
        h_shape = (self.n_layers, self.batches[0], self.hidden_size)
        dtype = self.in_dtypes[0]

        h = array_utils.uniform(h_shape, dtype)
        c = array_utils.uniform(h_shape, dtype)
        in_size = self.input_size
        out_size = self.hidden_size
        xs = [array_utils.uniform((self.batches[b], in_size), dtype)
              for b in range(len(self.batches))]

        def w_in(i, j):
            return in_size if i == 0 and j < 4 else out_size

        inputs = []
        inputs.append(h)
        inputs.append(c)
        for i in range(len(self.batches)):
            inputs.append(xs[i])

        for n in range(self.n_layers):
            for i in range(8):
                inputs.append(array_utils.uniform(
                    (out_size, w_in(n, i)), dtype))
            for i in range(8):
                inputs.append(array_utils.uniform((out_size,), dtype))
        return tuple(inputs)

</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py" startline="365" endline="396" pcid="9662">
    def generate_inputs(self):
        h_shape = (self.n_layers * 2, self.batches[0], self.hidden_size)
        dtype = self.in_dtypes[0]

        h = array_utils.uniform(h_shape, dtype)
        in_size = self.input_size
        out_size = self.hidden_size
        xs = [array_utils.uniform((self.batches[b], in_size), dtype)
              for b in range(len(self.batches))]

        def w_in(i, j):
            if i == 0 and j < 3:
                return in_size
            elif i > 0 and j < 3:
                return out_size * 2
            else:
                return out_size

        inputs = []
        inputs.append(h)
        for i in range(len(self.batches)):
            inputs.append(xs[i])

        for n in range(self.n_layers):
            for direction in (0, 1):
                for i in range(6):
                    inputs.append(array_utils.uniform(
                        (out_size, w_in(n, i)), dtype))
                for i in range(6):
                    inputs.append(array_utils.uniform((out_size,), dtype))
        return tuple(inputs)

</source>
</class>

<class classid="163" nclones="12" nlines="12" similarity="76">
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_lstm.py" startline="141" endline="153" pcid="6569">

    def process_inputs(self, inputs):
        h = inputs[0]
        c = inputs[1]
        xs = inputs[2: 2 + len(self.batches)]
        ws = []
        bs = []
        index = 2 + len(self.batches)
        for n in range(self.n_layers):
            ws.append(inputs[index: index + 8])
            bs.append(inputs[index + 8: index + 16])
            index += 16
        return h, c, ws, bs, xs
</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_rnn.py" startline="99" endline="112" pcid="6618">
    def process_inputs(self, inputs):
        h = inputs[0]

        xs = inputs[1: 1 + len(self.batches)]
        ws = []
        bs = []
        index = 1 + len(self.batches)
        for n in range(self.n_layers):
            ws.append(inputs[index: index + 2])
            bs.append(inputs[index + 2: index + 4])
            index += 4

        return h, ws, bs, xs

</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py" startline="397" endline="410" pcid="9664">
    def process_input(self, inputs):
        h = inputs[0]
        xs = inputs[1:1 + len(self.batches)]
        ws = []
        bs = []
        index = 1 + len(self.batches)
        for n in range(self.n_layers):
            ws.append(inputs[index: index + 6])
            bs.append(inputs[index + 6: index + 12])
            ws.append(inputs[index + 12: index + 18])
            bs.append(inputs[index + 18: index + 24])
            index += 24
        return h, ws, bs, xs

</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py" startline="82" endline="94" pcid="9646">
    def process_input(self, inputs):
        h = inputs[0]
        c = inputs[1]
        xs = inputs[2:2 + len(self.batches)]
        ws = []
        bs = []
        index = 2 + len(self.batches)
        for n in range(self.n_layers):
            ws.append(inputs[index: index + 8])
            bs.append(inputs[index + 8: index + 16])
            index += 16
        return h, c, ws, bs, xs

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_gru.py" startline="86" endline="99" pcid="6586">
    def process_inputs(self, inputs):
        h = inputs[0]

        xs = inputs[1:1 + len(self.batches)]
        ws = []
        bs = []
        index = 1 + len(self.batches)
        for n in range(self.n_layers):
            ws.append(inputs[index: index + 6])
            bs.append(inputs[index + 6: index + 12])
            index += 12

        return h, ws, bs, xs

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_lstm.py" startline="279" endline="293" pcid="6575">

    def process_inputs(self, inputs):
        h = inputs[0]
        c = inputs[1]
        xs = inputs[2:2 + len(self.batches)]
        ws = []
        bs = []
        index = 2 + len(self.batches)
        for n in range(self.n_layers):
            ws.append(inputs[index: index + 8])
            bs.append(inputs[index + 8: index + 16])
            ws.append(inputs[index + 16: index + 24])
            bs.append(inputs[index + 24: index + 32])
            index += 32
        return h, c, ws, bs, xs
</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py" startline="625" endline="638" pcid="9676">
    def process_input(self, inputs):
        h = inputs[0]
        xs = inputs[1:1 + len(self.batches)]
        ws = []
        bs = []
        index = 1 + len(self.batches)
        for n in range(self.n_layers):
            ws.append(inputs[index: index + 2])
            bs.append(inputs[index + 2: index + 4])
            ws.append(inputs[index + 4: index + 6])
            bs.append(inputs[index + 6: index + 8])
            index += 8
        return h, ws, bs, xs

</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py" startline="294" endline="305" pcid="9658">
    def process_input(self, inputs):
        h = inputs[0]
        xs = inputs[1:1 + len(self.batches)]
        ws = []
        bs = []
        index = 1 + len(self.batches)
        for n in range(self.n_layers):
            ws.append(inputs[index: index + 6])
            bs.append(inputs[index + 6: index + 12])
            index += 12
        return h, ws, bs, xs

</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py" startline="506" endline="517" pcid="9670">
    def process_input(self, inputs):
        h = inputs[0]
        xs = inputs[1:1 + len(self.batches)]
        ws = []
        bs = []
        index = 1 + len(self.batches)
        for n in range(self.n_layers):
            ws.append(inputs[index: index + 2])
            bs.append(inputs[index + 2: index + 4])
            index += 4
        return h, ws, bs, xs

</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py" startline="190" endline="204" pcid="9652">
    def process_input(self, inputs):
        h = inputs[0]
        c = inputs[1]
        xs = inputs[2:2 + len(self.batches)]
        ws = []
        bs = []
        index = 2 + len(self.batches)
        for n in range(self.n_layers):
            ws.append(inputs[index: index + 8])
            bs.append(inputs[index + 8: index + 16])
            ws.append(inputs[index + 16: index + 24])
            bs.append(inputs[index + 24: index + 32])
            index += 32
        return h, c, ws, bs, xs

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_rnn.py" startline="231" endline="244" pcid="6624">
    def process_inputs(self, inputs):
        h = inputs[0]
        xs = inputs[1: 1 + len(self.batches)]
        ws = []
        bs = []
        index = 1 + len(self.batches)
        for n in range(self.n_layers):
            ws.append(inputs[index: index + 2])
            bs.append(inputs[index + 2: index + 4])
            ws.append(inputs[index + 4: index + 6])
            bs.append(inputs[index + 6: index + 8])
            index += 8
        return h, ws, bs, xs

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_gru.py" startline="219" endline="232" pcid="6592">
    def process_inputs(self, inputs):
        h = inputs[0]
        xs = inputs[1:1 + len(self.batches)]
        ws = []
        bs = []
        index = 1 + len(self.batches)
        for n in range(self.n_layers):
            ws.append(inputs[index: index + 6])
            bs.append(inputs[index + 6: index + 12])
            ws.append(inputs[index + 12: index + 18])
            bs.append(inputs[index + 18: index + 24])
            index += 24
        return h, ws, bs, xs

</source>
</class>

<class classid="164" nclones="2" nlines="13" similarity="100">
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_lstm.py" startline="154" endline="167" pcid="6570">

    def forward(self, inputs, device):
        h, c, ws, bs, xs = self.process_inputs(inputs)
        if h.array.dtype == numpy.float64:
            with chainer.using_config('use_cudnn', 'never'):
                out = F.n_step_lstm(self.n_layers, 0.0, h, c, ws, bs, xs)
        else:
            out = F.n_step_lstm(self.n_layers, 0.0, h, c, ws, bs, xs)
        rets = []
        rets.append(out[0])
        rets.append(out[1])
        for i in range(len(out[2])):
            rets.append(out[2][i])
        return tuple(rets)
</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_lstm.py" startline="294" endline="308" pcid="6576">

    def forward(self, inputs, device):
        h, c, ws, bs, xs = self.process_inputs(inputs)
        if h.array.dtype == numpy.float64:
            with chainer.using_config('use_cudnn', 'never'):
                out = F.n_step_bilstm(self.n_layers, 0.0, h, c, ws, bs, xs)
        else:
            out = F.n_step_bilstm(self.n_layers, 0.0, h, c, ws, bs, xs)

        rets = []
        rets.append(out[0])
        rets.append(out[1])
        for i in range(len(out[2])):
            rets.append(out[2][i])
        return tuple(rets)
</source>
</class>

<class classid="165" nclones="2" nlines="12" similarity="100">
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_gru.py" startline="100" endline="113" pcid="6587">
    def forward(self, inputs, device):
        h, ws, bs, xs = self.process_inputs(inputs)
        if h.array.dtype == numpy.float64:
            with chainer.using_config('use_cudnn', 'never'):
                out = F.n_step_gru(self.n_layers, 0.0, h, ws, bs, xs)
        else:
            out = F.n_step_gru(self.n_layers, 0.0, h, ws, bs, xs)

        rets = []
        rets.append(out[0])
        for i in range(len(out[1])):
            rets.append(out[1][i])
        return tuple(rets)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_gru.py" startline="233" endline="245" pcid="6593">
    def forward(self, inputs, device):
        h, ws, bs, xs = self.process_inputs(inputs)
        if h.array.dtype == numpy.float64:
            with chainer.using_config('use_cudnn', 'never'):
                out = F.n_step_bigru(self.n_layers, 0.0, h, ws, bs, xs)
        else:
            out = F.n_step_bigru(self.n_layers, 0.0, h, ws, bs, xs)
        rets = []
        rets.append(out[0])
        for i in range(len(out[1])):
            rets.append(out[1][i])
        return tuple(rets)

</source>
</class>

<class classid="166" nclones="2" nlines="25" similarity="80">
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_gru.py" startline="114" endline="143" pcid="6588">
    def forward_expected(self, inputs):
        h, ws, bs, xs = self.process_inputs(inputs)
        e_hy = h.copy()
        ys = []
        for ind in range(len(xs)):
            x = xs[ind]
            batch = x.shape[0]
            for layer in range(self.n_layers):
                w = ws[layer]
                b = bs[layer]
                h_prev = e_hy[layer, :batch]

                # GRU
                z = sigmoid(x.dot(w[1].T) + h_prev.dot(w[4].T) + b[1] + b[4])
                r = sigmoid(x.dot(w[0].T) + h_prev.dot(w[3].T) + b[0] + b[3])
                h_bar = numpy.tanh(x.dot(w[2].T) +
                                   r *
                                   ((h_prev).dot(w[5].T) + b[5]) + b[2])
                e_h = (1 - z) * h_bar + z * h_prev
                e_hy[layer, :batch] = e_h

                x = e_h
            ys.append(x)
        rets = []
        rets.append(e_hy)
        for i in range(len(ys)):
            rets.append(ys[i])
        return tuple(rets)


</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_rnn.py" startline="123" endline="152" pcid="6620">
    def forward_expected(self, inputs):
        h, ws, bs, xs = self.process_inputs(inputs)

        e_hy = h.copy()
        ys = []
        for ind in range(len(xs)):
            x = xs[ind]
            batch = x.shape[0]
            for layer in range(self.n_layers):
                w = ws[layer]
                b = bs[layer]
                h_prev = e_hy[layer, :batch]
                if self.activation == 'tanh':
                    e_h = numpy.tanh(x.dot(w[0].T) +
                                     h_prev.dot(w[1].T) + b[0] + b[1])
                elif self.activation == 'relu':
                    e_h = _relu(x.dot(w[0].T) +
                                h_prev.dot(w[1].T) + b[0] + b[1])

                e_hy[layer, :batch] = e_h

                x = e_h
            ys.append(x)
        rets = []
        rets.append(e_hy)
        for i in range(len(ys)):
            rets.append(ys[i])
        return tuple(rets)


</source>
</class>

<class classid="167" nclones="2" nlines="47" similarity="74">
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_gru.py" startline="246" endline="300" pcid="6594">
    def forward_expected(self, inputs):
        h, ws, bs, xs = self.process_inputs(inputs)
        xs_next = xs
        e_hy = h.copy()
        for layer in range(self.n_layers):
            # forward
            di = 0
            xf = []
            layer_idx = layer * 2 + di
            w = ws[layer_idx]
            b = bs[layer_idx]
            for ind in range(len(xs)):
                x = xs_next[ind]
                batch = x.shape[0]
                h_prev = e_hy[layer_idx, :batch]
                # GRU
                z = sigmoid(x.dot(w[1].T) + h_prev.dot(w[4].T) + b[1] + b[4])
                r = sigmoid(x.dot(w[0].T) + h_prev.dot(w[3].T) + b[0] + b[3])
                h_bar = numpy.tanh(x.dot(w[2].T) +
                                   r *
                                   ((h_prev).dot(w[5].T) + b[5]) + b[2])
                e_h = (1 - z) * h_bar + z * h_prev
                e_hy[layer_idx, :batch] = e_h
                xf.append(e_h)

            # backward
            di = 1
            xb = []
            layer_idx = layer * 2 + di
            w = ws[layer_idx]
            b = bs[layer_idx]
            for ind in reversed(range(len(xs))):
                x = xs_next[ind]
                batch = x.shape[0]
                h_prev = e_hy[layer_idx, :batch]
                # GRU
                z = sigmoid(x.dot(w[1].T) + h_prev.dot(w[4].T) + b[1] + b[4])
                r = sigmoid(x.dot(w[0].T) + h_prev.dot(w[3].T) + b[0] + b[3])
                h_bar = numpy.tanh(x.dot(w[2].T) +
                                   r *
                                   ((h_prev).dot(w[5].T) + b[5]) + b[2])
                e_h = (1 - z) * h_bar + z * h_prev
                e_hy[layer_idx, :batch] = e_h
                xb.append(e_h)
            xb.reverse()
            xs_next = [numpy.concatenate([hfi, hbi], axis=1) for (hfi, hbi) in
                       zip(xf, xb)]

        rets = []
        rets.append(e_hy)
        for x in xs_next:
            rets.append(x)
        return tuple(rets)


</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_rnn.py" startline="255" endline="308" pcid="6626">
    def forward_expected(self, inputs):
        h, ws, bs, xs = self.process_inputs(inputs)
        xs_next = xs
        e_hy = h.copy()
        for layer in range(self.n_layers):
            # forward
            di = 0
            xf = []
            layer_idx = layer * 2 + di
            w = ws[layer_idx]
            b = bs[layer_idx]
            for ind in range(len(xs)):
                x = xs_next[ind]
                batch = x.shape[0]
                h_prev = e_hy[layer_idx, :batch]
                if self.activation == 'tanh':
                    e_h = numpy.tanh(x.dot(w[0].T) +
                                     h_prev.dot(w[1].T) + b[0] + b[1])
                elif self.activation == 'relu':
                    e_h = _relu(x.dot(w[0].T) +
                                h_prev.dot(w[1].T) + b[0] + b[1])

                e_hy[layer_idx, :batch] = e_h
                xf.append(e_h)

            # backward
            di = 1
            xb = []
            layer_idx = layer * 2 + di
            w = ws[layer_idx]
            b = bs[layer_idx]
            for ind in reversed(range(len(xs))):
                x = xs_next[ind]
                batch = x.shape[0]
                h_prev = e_hy[layer_idx, :batch]
                if self.activation == 'tanh':
                    e_h = numpy.tanh(x.dot(w[0].T) +
                                     h_prev.dot(w[1].T) + b[0] + b[1])
                elif self.activation == 'relu':
                    e_h = _relu(x.dot(w[0].T) +
                                h_prev.dot(w[1].T) + b[0] + b[1])

                e_hy[layer_idx, :batch] = e_h
                xb.append(e_h)
            xb.reverse()
            xs_next = [numpy.concatenate([hfi, hbi], axis=1)
                       for (hfi, hbi) in zip(xf, xb)]
        rets = []
        rets.append(e_hy)
        for x in xs_next:
            rets.append(x)
        return tuple(rets)


</source>
</class>

<class classid="168" nclones="2" nlines="15" similarity="86">
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/normalization_tests/test_batch_normalization.py" startline="221" endline="238" pcid="6673">
    def check_backward(self, inputs, grad_outputs, backend_config):
        inputs = backend_config.get_array(inputs)
        grad_outputs = backend_config.get_array(grad_outputs)
        if not self.c_contiguous:
            with backend_config:
                inputs = _as_noncontiguous_array(inputs)
                grad_outputs = _as_noncontiguous_array(grad_outputs)

        def f(*inputs):
            y = functions.batch_normalization(
                *inputs, **self.bn_options)
            return y,

        with backend_config:
            gradient_check.check_backward(
                f, inputs, grad_outputs,
                **self.check_backward_options)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/normalization_tests/test_batch_normalization.py" startline="360" endline="376" pcid="6684">
    def check_backward(self, inputs, grad_outputs, backend_config):
        inputs = backend_config.get_array(inputs)
        grad_outputs = backend_config.get_array(grad_outputs)
        if not self.c_contiguous:
            with backend_config:
                inputs = _as_noncontiguous_array(inputs)
                grad_outputs = _as_noncontiguous_array(grad_outputs)

        def f(*inputs):
            y = functions.fixed_batch_normalization(*inputs, eps=self.eps)
            return y,

        with backend_config:
            gradient_check.check_backward(
                f, inputs, grad_outputs,
                **self.check_backward_options)

</source>
</class>

<class classid="169" nclones="2" nlines="17" similarity="88">
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/normalization_tests/test_batch_normalization.py" startline="242" endline="261" pcid="6676">
    def check_double_backward(
            self, inputs, grad_outputs, grad_grad_inputs, backend_config):
        inputs = backend_config.get_array(inputs)
        grad_outputs = backend_config.get_array(grad_outputs)
        grad_grad_inputs = backend_config.get_array(grad_grad_inputs)
        if not self.c_contiguous:
            with backend_config:
                inputs = _as_noncontiguous_array(inputs)
                grad_outputs = _as_noncontiguous_array(grad_outputs)
                grad_grad_inputs = _as_noncontiguous_array(grad_grad_inputs)

        def f(*inputs):
            return functions.batch_normalization(
                *inputs, **self.bn_options)

        with backend_config:
            gradient_check.check_double_backward(
                f, inputs, grad_outputs, grad_grad_inputs,
                **self.check_double_backward_options)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/normalization_tests/test_batch_normalization.py" startline="380" endline="398" pcid="6687">
    def check_double_backward(
            self, inputs, grad_outputs, grad_grad_inputs, backend_config):
        inputs = backend_config.get_array(inputs)
        grad_outputs = backend_config.get_array(grad_outputs)
        grad_grad_inputs = backend_config.get_array(grad_grad_inputs)
        if not self.c_contiguous:
            with backend_config:
                inputs = _as_noncontiguous_array(inputs)
                grad_outputs = _as_noncontiguous_array(grad_outputs)
                grad_grad_inputs = _as_noncontiguous_array(grad_grad_inputs)

        def f(*inputs):
            return functions.fixed_batch_normalization(*inputs, eps=self.eps)

        with backend_config:
            gradient_check.check_double_backward(
                f, inputs, grad_outputs, grad_grad_inputs,
                **self.check_double_backward_options)

</source>
</class>

<class classid="170" nclones="2" nlines="10" similarity="90">
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/math_tests/test_sparse_matmul.py" startline="114" endline="125" pcid="6775">
    def check_SPDN_backward(self, a_data, b_data, c_grad, atol, rtol):
        sp_a = utils.to_coo(a_data)
        func = F.math.sparse_matmul.CooMatMul(
            sp_a.row, sp_a.col, sp_a.shape, sp_a.order,
            transa=self.transa, transb=self.transb, transc=False)

        def op(a, b):
            return func.apply((a, b))[0]
        gradient_check.check_backward(
            op, (sp_a.data.data, b_data), c_grad, atol=atol, rtol=rtol,
            dtype=numpy.float32)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/math_tests/test_sparse_matmul.py" startline="194" endline="205" pcid="6786">
    def check_DNSP_backward(self, a_data, b_data, c_grad, atol, rtol):
        sp_b = utils.to_coo(b_data)
        func = F.math.sparse_matmul.CooMatMul(
            sp_b.row, sp_b.col, sp_b.shape, sp_b.order,
            transa=not self.transb, transb=not self.transa, transc=True)

        def op(b, a):
            return func.apply((b, a))[0]
        gradient_check.check_backward(
            op, (sp_b.data.data, a_data), c_grad, atol=atol, rtol=rtol,
            dtype=numpy.float32)

</source>
</class>

<class classid="171" nclones="2" nlines="14" similarity="92">
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/math_tests/test_sparse_matmul.py" startline="138" endline="153" pcid="6779">
    def check_SPDN_double_backward(
            self, a_data, b_data, c_grad, a_grad_grad, b_grad_grad,
            atol, rtol):
        sp_a = utils.to_coo(a_data)
        sp_gga = utils.to_coo(a_grad_grad)
        func = F.math.sparse_matmul.CooMatMul(
            sp_a.row, sp_a.col, sp_a.shape, sp_a.order,
            transa=self.transa, transb=self.transb, transc=False)

        def op(a, b):
            return func.apply((a, b))[0]
        gradient_check.check_double_backward(
            op, (sp_a.data.data, b_data),
            c_grad, (sp_gga.data.data, b_grad_grad),
            atol=atol, rtol=rtol, dtype=numpy.float32)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/math_tests/test_sparse_matmul.py" startline="218" endline="233" pcid="6790">
    def check_DNSP_double_backward(
            self, a_data, b_data, c_grad, a_grad_grad, b_grad_grad,
            atol, rtol):
        sp_b = utils.to_coo(b_data)
        sp_ggb = utils.to_coo(b_grad_grad)
        func = F.math.sparse_matmul.CooMatMul(
            sp_b.row, sp_b.col, sp_b.shape, sp_b.order,
            transa=not self.transb, transb=not self.transa, transc=True)

        def op(b, a):
            return func.apply((b, a))[0]
        gradient_check.check_double_backward(
            op, (sp_b.data.data, a_data),
            c_grad, (sp_ggb.data.data, a_grad_grad),
            atol=atol, rtol=rtol, dtype=numpy.float32)

</source>
</class>

<class classid="172" nclones="2" nlines="12" similarity="100">
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/math_tests/test_minimum.py" startline="43" endline="55" pcid="7370">
    def setUp(self):
        if self.dtype == numpy.float16:
            eps = 1e-2
            self.check_forward_options.update({'atol': 1e-4, 'rtol': 1e-3})
            self.check_backward_options.update({
                'atol': 1e-2, 'rtol': 1e-2})
            self.check_double_backward_options.update({
                'atol': 1e-2, 'rtol': 1e-2})
        else:
            eps = 1e-3
        self.check_backward_options['eps'] = eps
        self.check_double_backward_options['eps'] = eps

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/math_tests/test_maximum.py" startline="43" endline="55" pcid="7405">
    def setUp(self):
        if self.dtype == numpy.float16:
            eps = 1e-2
            self.check_forward_options.update({'atol': 1e-4, 'rtol': 1e-3})
            self.check_backward_options.update({
                'atol': 1e-2, 'rtol': 1e-2})
            self.check_double_backward_options.update({
                'atol': 1e-2, 'rtol': 1e-2})
        else:
            eps = 1e-3
        self.check_backward_options['eps'] = eps
        self.check_double_backward_options['eps'] = eps

</source>
</class>

<class classid="173" nclones="2" nlines="11" similarity="100">
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/math_tests/test_det.py" startline="263" endline="275" pcid="7444">
    def test_answer_gpu_cpu(self):
        x = cuda.to_gpu(self.x)
        y = F.det(chainer.Variable(x))
        gpu = cuda.to_cpu(y.data)
        if self.dtype == numpy.float16:
            cpu = numpy.linalg.det(
                self.x.astype(numpy.float32)).astype(numpy.float16)
            testing.assert_allclose(gpu, cpu, atol=5e-3, rtol=5e-3)
        else:
            cpu = numpy.linalg.det(self.x)
            testing.assert_allclose(gpu, cpu)


</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/math_tests/test_det.py" startline="287" endline="299" pcid="7446">
    def test_answer_gpu_cpu(self):
        x = cuda.to_gpu(self.x)
        y = F.batch_det(chainer.Variable(x))
        gpu = cuda.to_cpu(y.data)
        if self.dtype == numpy.float16:
            cpu = numpy.linalg.det(
                self.x.astype(numpy.float32)).astype(numpy.float16)
            testing.assert_allclose(gpu, cpu, atol=5e-3, rtol=5e-3)
        else:
            cpu = numpy.linalg.det(self.x)
            testing.assert_allclose(gpu, cpu)


</source>
</class>

<class classid="174" nclones="2" nlines="13" similarity="100">
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/math_tests/test_inv.py" startline="52" endline="65" pcid="7468">
    def setUp(self):
        if self.dtype == numpy.float16:
            self.check_forward_dtype = numpy.float32
            self.check_forward_options.update({'atol': 1e-3, 'rtol': 1e-3})
            self.check_backward_options.update({'atol': 1e-3, 'rtol': 1e-3})
            self.check_double_backward_options.update({
                'atol': 5e-3, 'rtol': 5e-3})
        else:
            self.check_forward_dtype = self.dtype
            self.check_forward_options.update({'atol': 1e-4, 'rtol': 1e-4})
            self.check_backward_options.update({'atol': 5e-4, 'rtol': 5e-4})
            self.check_double_backward_options.update({
                'atol': 5e-4, 'rtol': 5e-4})

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/math_tests/test_inv.py" startline="112" endline="125" pcid="7473">
    def setUp(self):
        if self.dtype == numpy.float16:
            self.check_forward_dtype = numpy.float32
            self.check_forward_options.update({'atol': 1e-3, 'rtol': 1e-3})
            self.check_backward_options.update({'atol': 2e-3, 'rtol': 2e-3})
            self.check_double_backward_options.update({
                'atol': 5e-3, 'rtol': 5e-3})
        else:
            self.check_forward_dtype = self.dtype
            self.check_forward_options.update({'atol': 1e-4, 'rtol': 1e-4})
            self.check_backward_options.update({'atol': 5e-4, 'rtol': 5e-4})
            self.check_double_backward_options.update({
                'atol': 1e-3, 'rtol': 1e-3})

</source>
</class>

<class classid="175" nclones="2" nlines="14" similarity="85">
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/array_tests/test_copy.py" startline="160" endline="175" pcid="7770">
    def test_forward_int(self, src_backend_config, dst_backend_config):
        assert dst_backend_config.xp is not chainerx
        src_device = src_backend_config.device
        dst_device = dst_backend_config.device
        if dst_device.xp is numpy:
            dst_device_spec = -1
        elif dst_device.xp is chainer.backends.cuda.cupy:
            dst_device_spec = dst_device.device.id
        else:
            assert False, dst_device

        self.check_forward(
            dst_device_spec,
            src_device,
            dst_device)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/array_tests/test_copy.py" startline="176" endline="192" pcid="7771">
    def test_forward_str(self, src_backend_config, dst_backend_config):
        assert dst_backend_config.xp is not chainerx
        src_device = src_backend_config.device
        dst_device = dst_backend_config.device
        if dst_device.xp is numpy:
            dst_device_spec = '@numpy'
        elif dst_device.xp is chainer.backends.cuda.cupy:
            dst_device_spec = '@cupy:{}'.format(dst_device.device.id)
        else:
            assert False, dst_device

        self.check_forward(
            dst_device_spec,
            src_device,
            dst_device)


</source>
</class>

<class classid="176" nclones="2" nlines="15" similarity="86">
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/array_tests/test_spatial_transformer_sampler.py" startline="132" endline="147" pcid="7788">
    def test_consistency_with_cudnn_cpu(self):
        with chainer.using_config('use_cudnn', 'never'):
            x_cpu, grid_cpu, y_cpu = self._apply_backward(
                self.x, self.grid, self.grads)
        with chainer.using_config('use_cudnn', 'always'):
            x_cudnn, grid_cudnn, y_cudnn = self._apply_backward(
                cuda.to_gpu(self.x), cuda.to_gpu(self.grid),
                cuda.to_gpu(self.grads))

        testing.assert_allclose(
            y_cpu.data, y_cudnn.data, **self.assert_options)
        testing.assert_allclose(
            x_cpu.grad, x_cudnn.grad, **self.assert_options)
        testing.assert_allclose(
            grid_cpu.grad, grid_cudnn.grad, **self.assert_options)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/array_tests/test_spatial_transformer_sampler.py" startline="150" endline="167" pcid="7789">
    def test_consistency_with_cudnn_gpu(self):
        with chainer.using_config('use_cudnn', 'never'):
            x_gpu, grid_gpu, y_gpu = self._apply_backward(
                cuda.to_gpu(self.x), cuda.to_gpu(self.grid),
                cuda.to_gpu(self.grads))
        with chainer.using_config('use_cudnn', 'always'):
            x_cudnn, grid_cudnn, y_cudnn = self._apply_backward(
                cuda.to_gpu(self.x), cuda.to_gpu(self.grid),
                cuda.to_gpu(self.grads))

        testing.assert_allclose(
            y_gpu.data, y_cudnn.data, **self.assert_options)
        testing.assert_allclose(
            x_gpu.grad, x_cudnn.grad, **self.assert_options)
        testing.assert_allclose(
            grid_gpu.grad, grid_cudnn.grad, **self.assert_options)


</source>
</class>

<class classid="177" nclones="2" nlines="35" similarity="100">
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/array_tests/test_space_2_depth.py" startline="18" endline="54" pcid="7856">
    def setUp(self):
        self.depth = numpy.arange(96).reshape(2, 8, 3, 2).astype(self.dtype)
        self.space = numpy.array([[[[0.,  12.,   1.,  13.],
                                    [24.,  36.,  25.,  37.],
                                    [2.,  14.,   3.,  15.],
                                    [26.,  38.,  27.,  39.],
                                    [4.,  16.,   5.,  17.],
                                    [28.,  40.,  29.,  41.]],
                                   [[6.,  18.,   7.,  19.],
                                    [30.,  42.,  31.,  43.],
                                    [8.,  20.,   9.,  21.],
                                    [32.,  44.,  33.,  45.],
                                    [10.,  22.,  11.,  23.],
                                    [34.,  46.,  35.,  47.]]],
                                  [[[48.,  60.,  49.,  61.],
                                    [72.,  84.,  73.,  85.],
                                    [50.,  62.,  51.,  63.],
                                    [74.,  86.,  75.,  87.],
                                    [52.,  64.,  53.,  65.],
                                    [76.,  88.,  77.,  89.]],
                                   [[54.,  66.,  55.,  67.],
                                    [78.,  90.,  79.,  91.],
                                    [56.,  68.,  57.,  69.],
                                    [80.,  92.,  81.,  93.],
                                    [58.,  70.,  59.,  71.],
                                    [82.,  94.,  83.,  95.]]]]
                                 ).astype(self.dtype)
        self.x = numpy.random.randn(2, 2, 6, 4).astype(self.dtype)
        self.gy = numpy.random.randn(2, 8, 3, 2).astype(self.dtype)
        self.ggx = numpy.random.randn(2, 2, 6, 4).astype(self.dtype)
        self.r = 2
        self.check_backward_options = {}
        self.check_double_backward_options = {}
        if self.dtype == numpy.float16:
            self.check_backward_options = {'atol': 5e-4, 'rtol': 5e-3}
            self.check_double_backward_options = {'atol': 5e-3, 'rtol': 5e-2}

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/array_tests/test_depth_2_space.py" startline="18" endline="54" pcid="7897">
    def setUp(self):
        self.depth = numpy.arange(96).reshape(2, 8, 3, 2).astype(self.dtype)
        self.space = numpy.array([[[[0.,  12.,   1.,  13.],
                                    [24.,  36.,  25.,  37.],
                                    [2.,  14.,   3.,  15.],
                                    [26.,  38.,  27.,  39.],
                                    [4.,  16.,   5.,  17.],
                                    [28.,  40.,  29.,  41.]],
                                   [[6.,  18.,   7.,  19.],
                                    [30.,  42.,  31.,  43.],
                                    [8.,  20.,   9.,  21.],
                                    [32.,  44.,  33.,  45.],
                                    [10.,  22.,  11.,  23.],
                                    [34.,  46.,  35.,  47.]]],
                                  [[[48.,  60.,  49.,  61.],
                                    [72.,  84.,  73.,  85.],
                                    [50.,  62.,  51.,  63.],
                                    [74.,  86.,  75.,  87.],
                                    [52.,  64.,  53.,  65.],
                                    [76.,  88.,  77.,  89.]],
                                   [[54.,  66.,  55.,  67.],
                                    [78.,  90.,  79.,  91.],
                                    [56.,  68.,  57.,  69.],
                                    [80.,  92.,  81.,  93.],
                                    [58.,  70.,  59.,  71.],
                                    [82.,  94.,  83.,  95.]]]]
                                 ).astype(self.dtype)
        self.x = numpy.random.randn(2, 8, 3, 2).astype(self.dtype)
        self.gy = numpy.random.randn(2, 2, 6, 4).astype(self.dtype)
        self.ggx = numpy.random.randn(2, 8, 3, 2).astype(self.dtype)
        self.r = 2
        self.check_backward_options = {}
        self.check_double_backward_options = {}
        if self.dtype == numpy.float16:
            self.check_backward_options = {'atol': 5e-4, 'rtol': 5e-3}
            self.check_double_backward_options = {'atol': 5e-3, 'rtol': 5e-2}

</source>
</class>

<class classid="178" nclones="2" nlines="15" similarity="73">
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/loss_tests/test_softmax_cross_entropy.py" startline="400" endline="416" pcid="8201">
    def check_backward(self, xp):
        x = xp.asarray(self.x)
        t = xp.asarray(self.t)
        gy = xp.asarray(self.gy)
        if self.class_weight is not None:
            class_weight = xp.asarray(self.class_weight)
        else:
            class_weight = None

        def f(x_, t_):
            return functions.softmax_cross_entropy(
                x_, t_, class_weight=class_weight, reduce=self.reduce,
                ignore_label=self.ignore_label,
                enable_double_backprop=self.enable_double_backprop)

        gradient_check.check_backward(f, (x, t), gy)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/loss_tests/test_softmax_cross_entropy.py" startline="424" endline="441" pcid="8205">
    def check_double_backward(self, xp):
        x = xp.asarray(self.x)
        t = xp.asarray(self.t)
        gy = xp.asarray(self.gy)
        ggx = xp.asarray(self.ggx)
        if self.class_weight is not None:
            class_weight = xp.asarray(self.class_weight)
        else:
            class_weight = None

        def f(x_):
            return functions.softmax_cross_entropy(
                x_, t, class_weight=class_weight, reduce=self.reduce,
                ignore_label=self.ignore_label,
                enable_double_backprop=True)

        gradient_check.check_double_backward(f, x, gy, ggx)

</source>
</class>

<class classid="179" nclones="2" nlines="14" similarity="73">
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/loss_tests/test_negative_sampling.py" startline="138" endline="152" pcid="8249">
    def test_backward(self, backend_config):
        sampler = make_sampler(backend_config, self.label_size)
        x_data = backend_config.get_array(self.x)
        t_data = backend_config.get_array(self.t)
        w_data = backend_config.get_array(self.w)
        y_grad = backend_config.get_array(self.gy)

        def f(x, w):
            return functions.negative_sampling(
                x, t_data, w, sampler, self.sample_size, reduce=self.reduce)

        with backend_config:
            gradient_check.check_backward(
                f, (x_data, w_data), y_grad, **self.check_backward_options)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/loss_tests/test_negative_sampling.py" startline="153" endline="171" pcid="8251">
    def test_double_backward(self, backend_config):
        sampler = make_sampler(backend_config, self.label_size)
        x_data = backend_config.get_array(self.x)
        t_data = backend_config.get_array(self.t)
        w_data = backend_config.get_array(self.w)
        y_grad = backend_config.get_array(self.gy)
        x_grad_grad = backend_config.get_array(self.ggx)
        w_grad_grad = backend_config.get_array(self.ggw)

        def f(x, w):
            return functions.negative_sampling(
                x, t_data, w, sampler, self.sample_size, reduce=self.reduce)

        with backend_config:
            gradient_check.check_double_backward(
                f, (x_data, w_data), y_grad, (x_grad_grad, w_grad_grad),
                **self.check_double_backward_options)


</source>
</class>

<class classid="180" nclones="6" nlines="13" similarity="71">
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/connection_tests/test_convolution_2d.py" startline="124" endline="144" pcid="8279">
    def forward_expected(self, inputs):
        """
        Current forward_expected implementation depends on
        F.convolution_2d itself and thus it's only capable
        of checking consistency between backends, not absolute
        correctness of computations
        """
        if self.nobias:
            x, W = inputs
            b = None
        else:
            x, W, b = inputs
        with chainer.using_config('use_ideep', 'never'):
            y_expected = F.convolution_2d(
                x, W, b, stride=self.stride, pad=self.pad,
                cover_all=self.cover_all, dilate=self.dilate,
                groups=self.groups)
        if self.old_numpy_fp16:
            return y_expected.array*0,
        return y_expected.array,

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/connection_tests/test_convolution_2d.py" startline="145" endline="158" pcid="8280">
    def forward(self, inputs, device):
        if self.nobias:
            x, W = inputs
            b = None
        else:
            x, W, b = inputs
        out = F.convolution_2d(
            x, W, b, stride=self.stride, pad=self.pad,
            cover_all=self.cover_all, dilate=self.dilate,
            groups=self.groups)
        if self.old_numpy_fp16:
            return out*0,
        return out,

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/connection_tests/test_deconvolution_nd.py" startline="129" endline="146" pcid="8360">
    def forward_expected(self, inputs):
        """
        Current forward_expected implementation depends on
        F.deconvolution_nd itself and thus it's only capable
        of checking consistency between backends, not absolute
        correctness of computations
        """
        if self.nobias:
            x, W = inputs
            b = None
        else:
            x, W, b = inputs
        y_expected = F.deconvolution_nd(
            x, W, b, stride=self.stride, pad=self.pad,
            outsize=self.outsize, dilate=self.dilate,
            groups=self.groups)
        return y_expected.array,

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/connection_tests/test_convolution_nd.py" startline="110" endline="127" pcid="8321">
    def forward_expected(self, inputs):
        """
        Current forward_expected implementation depends on
        F.convolution_nd itself and thus it's only capable
        of checking consistency between backends, not absolute
        correctness of computations
        """
        if self.nobias:
            x, W = inputs
            b = None
        else:
            x, W, b = inputs
        y_expected = F.convolution_nd(
            x, W, b, stride=self.stride, pad=self.pad,
            cover_all=self.cover_all, dilate=self.dilate,
            groups=self.groups)
        return y_expected.array,

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/connection_tests/test_convolution_nd.py" startline="128" endline="139" pcid="8322">
    def forward(self, inputs, device):
        if self.nobias:
            x, W = inputs
            b = None
        else:
            x, W, b = inputs
        y = F.convolution_nd(
            x, W, b, stride=self.stride, pad=self.pad,
            cover_all=self.cover_all, dilate=self.dilate,
            groups=self.groups)
        return y,

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/connection_tests/test_deconvolution_nd.py" startline="147" endline="158" pcid="8361">
    def forward(self, inputs, device):
        if self.nobias:
            x, W = inputs
            b = None
        else:
            x, W, b = inputs
        y = F.deconvolution_nd(
            x, W, b, stride=self.stride, pad=self.pad,
            outsize=self.outsize, dilate=self.dilate,
            groups=self.groups)
        return y,

</source>
</class>

<class classid="181" nclones="3" nlines="11" similarity="72">
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/connection_tests/test_convolution_2d.py" startline="321" endline="333" pcid="8291">
    def test_1(self):
        n_batches = 2
        in_channels = 3
        out_channels = 1  # important
        x_shape = (n_batches, in_channels, 10, 10)
        w_shape = (out_channels, in_channels, 3, 3)
        x = numpy.ones(x_shape, numpy.float32)
        w = numpy.ones(w_shape, numpy.float32)
        y = F.convolution_2d(x, chainer.Variable(w))
        z = F.sum(y)
        z.backward()


</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/connection_tests/test_convolution_nd.py" startline="263" endline="274" pcid="8333">
    def test_1(self):
        n_batches = 2
        in_channels = 3
        out_channels = 1  # important
        x_shape = (n_batches, in_channels, 4)
        w_shape = (out_channels, in_channels, 3)
        x = numpy.ones(x_shape, numpy.float32)
        w = numpy.ones(w_shape, numpy.float32)
        y = F.convolution_nd(chainer.Variable(x), w)
        z = F.sum(y)
        z.backward()

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/connection_tests/test_convolution_nd.py" startline="275" endline="287" pcid="8334">
    def test_2(self):
        n_batches = 2
        in_channels = 3
        out_channels = 1  # important
        x_shape = (n_batches, in_channels, 4)
        w_shape = (out_channels, in_channels, 3)
        x = numpy.ones(x_shape, numpy.float32)
        w = numpy.ones(w_shape, numpy.float32)
        y = F.convolution_nd(x, chainer.Variable(w))
        z = F.sum(y)
        z.backward()


</source>
</class>

<class classid="182" nclones="2" nlines="10" similarity="100">
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/connection_tests/test_convolution_nd.py" startline="99" endline="109" pcid="8320">
    def generate_inputs(self):
        W = numpy.random.normal(
            0, self.W_scale, self.W_shape).astype(self.W_dtype)
        x = numpy.random.uniform(-1, 1, self.x_shape).astype(self.x_dtype)
        if self.nobias:
            return x, W
        else:
            b = numpy.random.uniform(
                -1, 1, self.out_channels).astype(self.x_dtype)
            return x, W, b

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/connection_tests/test_deconvolution_nd.py" startline="118" endline="128" pcid="8359">
    def generate_inputs(self):
        W = numpy.random.normal(
            0, self.W_scale, self.W_shape).astype(self.W_dtype)
        x = numpy.random.uniform(-1, 1, self.x_shape).astype(self.x_dtype)
        if self.nobias:
            return x, W
        else:
            b = numpy.random.uniform(
                -1, 1, self.out_channels).astype(self.x_dtype)
            return x, W, b

</source>
</class>

<class classid="183" nclones="2" nlines="22" similarity="77">
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/connection_tests/test_convolution_nd.py" startline="140" endline="164" pcid="8323">
    def check_forward_consistency_regression(self, backend_config):
        inputs = self.generate_inputs()
        if self.nobias:
            x, W = inputs
            b = None
        else:
            x, W, b = inputs
        x = chainer.Variable(backend_config.get_array(x))
        W = chainer.Variable(backend_config.get_array(W))
        if b is not None:
            b = chainer.Variable(backend_config.get_array(b))

        with chainer.using_config('use_cudnn', 'never'):
            y_nd = F.convolution_nd(
                x, W, b, stride=self.stride, pad=self.pad,
                cover_all=self.cover_all, dilate=self.dilate,
                groups=self.groups)
            y_2d = F.convolution_2d(
                x, W, b, stride=self.stride, pad=self.pad,
                cover_all=self.cover_all, dilate=self.dilate,
                groups=self.groups)

        testing.assert_allclose(
            y_nd.array, y_2d.array, **self.check_forward_options)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/connection_tests/test_deconvolution_nd.py" startline="159" endline="183" pcid="8362">
    def check_forward_consistency_regression(self, backend_config):
        inputs = self.generate_inputs()
        if self.nobias:
            x, W = inputs
            b = None
        else:
            x, W, b = inputs
        x = chainer.Variable(backend_config.get_array(x))
        W = chainer.Variable(backend_config.get_array(W))
        if b is not None:
            b = chainer.Variable(backend_config.get_array(b))

        use_cudnn = backend_config.use_cudnn

        with chainer.using_config('use_cudnn', use_cudnn):
            y_nd = F.deconvolution_nd(x, W, b, stride=self.stride,
                                      pad=self.pad, outsize=self.outsize,
                                      dilate=self.dilate)
            y_2d = F.deconvolution_2d(x, W, b, stride=self.stride,
                                      pad=self.pad, outsize=self.outsize,
                                      dilate=self.dilate)

        testing.assert_allclose(
            y_nd.array, y_2d.array, **self.check_forward_options)

</source>
</class>

<class classid="184" nclones="2" nlines="10" similarity="100">
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/connection_tests/test_convolution_nd.py" startline="223" endline="234" pcid="8329">
    def setUp(self):
        N = 2
        in_channels = 3
        out_channels = 2
        dtype = numpy.float32

        x_shape = (N, in_channels, 3, 3, 3)
        self.x_data = numpy.random.uniform(-1, 1, x_shape).astype(dtype)
        W_shape = (out_channels, in_channels, 1, 1, 1)
        self.W_data = numpy.random.uniform(-1, 1, W_shape).astype(dtype)
        self.b_data = numpy.random.uniform(-1, 1, out_channels).astype(dtype)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/connection_tests/test_deconvolution_nd.py" startline="243" endline="254" pcid="8368">
    def setUp(self):
        N = 2
        in_channels = 3
        out_channels = 2
        dtype = numpy.float32

        x_shape = (N, in_channels, 3, 3, 3)
        self.x_data = numpy.random.uniform(-1, 1, x_shape).astype(dtype)
        W_shape = (in_channels, out_channels, 1, 1, 1)
        self.W_data = numpy.random.uniform(-1, 1, W_shape).astype(dtype)
        self.b_data = numpy.random.uniform(-1, 1, out_channels).astype(dtype)

</source>
</class>

<class classid="185" nclones="2" nlines="10" similarity="100">
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/connection_tests/test_convolution_nd.py" startline="290" endline="302" pcid="8335">
    def _get_data(self, ndim):
        in_channels = 3
        out_channels = 2
        dtype = numpy.float32

        x_shape = (2, in_channels) + (3,) * ndim
        x = numpy.random.uniform(-1, 1, x_shape).astype(dtype)
        W_shape = (out_channels, in_channels) + (1,) * ndim
        W = numpy.random.uniform(-1, 1, W_shape).astype(dtype)
        b = numpy.random.uniform(-1, 1, out_channels).astype(dtype)

        return x, W, b

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/connection_tests/test_deconvolution_nd.py" startline="349" endline="361" pcid="8377">
    def _get_data(self, ndim):
        in_channels = 3
        out_channels = 2
        dtype = numpy.float32

        x_shape = (2, in_channels) + (3,) * ndim
        x = numpy.random.uniform(-1, 1, x_shape).astype(dtype)
        W_shape = (in_channels, out_channels) + (1,) * ndim
        W = numpy.random.uniform(-1, 1, W_shape).astype(dtype)
        b = numpy.random.uniform(-1, 1, out_channels).astype(dtype)

        return x, W, b

</source>
</class>

<class classid="186" nclones="2" nlines="19" similarity="100">
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/connection_tests/test_deformable_convolution_2d_sampler.py" startline="28" endline="52" pcid="8340">
    def setUp(self):
        in_channels = 3
        out_channels = 2
        batch_size = 2
        h = 9
        w = 9

        kh, kw, sy, sx, ph, pw = self.params

        self.stride = (sy, sx)
        self.pad = (ph, pw)

        self.W = numpy.random.normal(
            size=(out_channels, in_channels, kh, kw)).astype(numpy.float32)
        self.b = numpy.random.uniform(
            size=(out_channels,)).astype(numpy.float32)

        self.x = numpy.random.uniform(
            size=(batch_size, in_channels, h, w)).astype(numpy.float32)

        out_h = utils.conv.get_conv_outsize(h, kh, sy, ph)
        out_w = utils.conv.get_conv_outsize(w, kw, sx, pw)
        self.offset = numpy.zeros(
            (batch_size, 2 * kh * kw, out_h, out_w), dtype=numpy.float32)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/connection_tests/test_deformable_convolution_2d_sampler.py" startline="91" endline="115" pcid="8344">
    def setUp(self):
        in_channels = 3
        out_channels = 2
        batch_size = 2
        h = 9
        w = 9

        kh, kw, sy, sx, ph, pw = self.params

        self.stride = (sy, sx)
        self.pad = (ph, pw)

        self.W = numpy.random.normal(
            size=(out_channels, in_channels, kh, kw)).astype(numpy.float32)
        self.b = numpy.random.uniform(
            size=(out_channels,)).astype(numpy.float32)

        self.x = numpy.random.uniform(
            size=(batch_size, in_channels, h, w)).astype(numpy.float32)

        out_h = utils.conv.get_conv_outsize(h, kh, sy, ph)
        out_w = utils.conv.get_conv_outsize(w, kw, sx, pw)
        self.offset = numpy.zeros(
            (batch_size, 2 * kh * kw, out_h, out_w), dtype=numpy.float32)

</source>
</class>

<class classid="187" nclones="2" nlines="17" similarity="88">
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_unpooling_2d.py" startline="36" endline="54" pcid="8421">
    def setUp(self):
        self.N = 2
        self.n_channels = 3
        inh, inw = 2, 1
        self.x = pooling_nd_helper.shuffled_linspace(
            (self.N, self.n_channels, inh, inw), self.dtype)

        self.ksize = 2
        outh, outw = self.outsize or self.expected_outsize
        self.gy = numpy.random.uniform(
            -1, 1, (self.N, self.n_channels, outh, outw)).astype(self.dtype)
        self.check_backward_options = {'atol': 1e-4, 'rtol': 1e-3}
        self.check_double_backward_options = {}
        if self.dtype == numpy.float16:
            self.check_backward_options = {'atol': 2e-3, 'rtol': 2e-2}
            self.check_double_backward_options = {'atol': 3e-3, 'rtol': 3e-2}
        self.ggx = numpy.random.uniform(
            -1, 1, self.x.shape).astype(self.dtype)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_unpooling_2d.py" startline="156" endline="173" pcid="8435">
    def setUp(self):
        self.N = 2
        self.n_channels = 3
        inh, inw = self.insize
        self.x = pooling_nd_helper.shuffled_linspace(
            (self.N, self.n_channels, inh, inw), self.dtype)

        outh, outw = self.outsize or self.expected_outsize
        self.gy = numpy.random.uniform(
            -1, 1, (self.N, self.n_channels, outh, outw)).astype(self.dtype)
        self.check_backward_options = {'atol': 1e-4, 'rtol': 1e-3}
        self.check_double_backward_options = {}
        if self.dtype == numpy.float16:
            self.check_backward_options = {'atol': 2e-3, 'rtol': 2e-2}
            self.check_double_backward_options = {'atol': 3e-3, 'rtol': 3e-2}
        self.ggx = numpy.random.uniform(
            -1, 1, self.x.shape).astype(self.dtype)

</source>
</class>

<class classid="188" nclones="2" nlines="11" similarity="81">
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_unpooling_2d.py" startline="277" endline="289" pcid="8449">
    def check_left_inverse(self, xp, use_cudnn='never'):
        x = xp.arange(self.h * self.h).reshape(
            (1, 1, self.h, self.h)).astype(self.dtype)
        with chainer.using_config('use_cudnn', use_cudnn):
            y = chainer.functions.unpooling_2d(
                x, self.k, self.s, self.p, None, self.cover_all)
            x_ = chainer.functions.max_pooling_2d(
                y, self.k, self.s, self.p, self.cover_all).data

        self.assertEqual(x.shape, x_.shape)
        self.assertEqual(x.dtype, x_.dtype)
        chainer.testing.assert_allclose(x, x_)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_unpooling_2d.py" startline="311" endline="325" pcid="8453">
    def check_left_inverse(self, xp, use_cudnn='never'):
        x = xp.arange(self.h * self.h).reshape(
            (1, 1, self.h, self.h)).astype(self.dtype)
        with chainer.using_config('use_cudnn', use_cudnn):
            # average_pooling_2d does not have cover_all option
            # as max_pooling_2d has.
            y = chainer.functions.unpooling_2d(
                x, self.k, self.s, self.p, None, False)
            x_ = chainer.functions.average_pooling_2d(
                y, self.k, self.s, self.p).data

        self.assertEqual(x.shape, x_.shape)
        self.assertEqual(x.dtype, x_.dtype)
        chainer.testing.assert_allclose(x, x_)

</source>
</class>

<class classid="189" nclones="3" nlines="13" similarity="71">
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_roi_pooling_2d.py" startline="61" endline="76" pcid="8461">
    def test_forward_cpu_gpu_equal(self):
        # cpu
        x_cpu = chainer.Variable(self.x)
        rois_cpu = chainer.Variable(self.rois)
        y_cpu = functions.roi_pooling_2d(
            x_cpu, rois_cpu, outh=self.outh, outw=self.outw,
            spatial_scale=self.spatial_scale)

        # gpu
        x_gpu = chainer.Variable(cuda.to_gpu(self.x))
        rois_gpu = chainer.Variable(cuda.to_gpu(self.rois))
        y_gpu = functions.roi_pooling_2d(
            x_gpu, rois_gpu, outh=self.outh, outw=self.outw,
            spatial_scale=self.spatial_scale)
        testing.assert_allclose(y_cpu.data, cuda.to_cpu(y_gpu.data))

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_roi_max_pooling_2d.py" startline="75" endline="92" pcid="8471">
    def test_forward_cpu_gpu_equal(self):
        # cpu
        x_cpu = chainer.Variable(self.x)
        rois_cpu = chainer.Variable(self.rois)
        roi_indices_cpu = chainer.Variable(self.roi_indices)
        y_cpu = functions.roi_max_pooling_2d(
            x_cpu, rois_cpu, roi_indices_cpu, outsize=self.outsize,
            spatial_scale=self.spatial_scale)

        # gpu
        x_gpu = chainer.Variable(cuda.to_gpu(self.x))
        rois_gpu = chainer.Variable(cuda.to_gpu(self.rois))
        roi_indices_gpu = chainer.Variable(cuda.to_gpu(self.roi_indices))
        y_gpu = functions.roi_max_pooling_2d(
            x_gpu, rois_gpu, roi_indices_gpu, outsize=self.outsize,
            spatial_scale=self.spatial_scale)
        testing.assert_allclose(y_cpu.data, cuda.to_cpu(y_gpu.data))

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_roi_average_pooling_2d.py" startline="75" endline="92" pcid="8494">
    def test_forward_cpu_gpu_equal(self):
        # cpu
        x_cpu = chainer.Variable(self.x)
        rois_cpu = chainer.Variable(self.rois)
        roi_indices_cpu = chainer.Variable(self.roi_indices)
        y_cpu = functions.roi_average_pooling_2d(
            x_cpu, rois_cpu, roi_indices_cpu, outsize=self.outsize,
            spatial_scale=self.spatial_scale)

        # gpu
        x_gpu = chainer.Variable(cuda.to_gpu(self.x))
        rois_gpu = chainer.Variable(cuda.to_gpu(self.rois))
        roi_indices_gpu = chainer.Variable(cuda.to_gpu(self.roi_indices))
        y_gpu = functions.roi_average_pooling_2d(
            x_gpu, rois_gpu, roi_indices_gpu, outsize=self.outsize,
            spatial_scale=self.spatial_scale)
        testing.assert_allclose(y_cpu.data, cuda.to_cpu(y_gpu.data))

</source>
</class>

<class classid="190" nclones="4" nlines="20" similarity="71">
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_roi_max_pooling_2d.py" startline="30" endline="52" pcid="8467">
    def setUp(self):
        N = 3
        n_channels = 3
        self.x = pooling_nd_helper.shuffled_linspace(
            (N, n_channels, 12, 8), self.dtype)
        self.rois = numpy.array([
            [1, 1, 7, 7],
            [2, 6, 12, 8],
            [1, 3, 11, 6],
            [3, 3, 4, 4]
        ], dtype=self.dtype)
        self.roi_indices = numpy.array([0, 2, 1, 0], dtype=numpy.int32)
        n_rois = self.rois.shape[0]
        outsize = _pair(self.outsize)
        self.gy = numpy.random.uniform(
            -1, 1, (n_rois, n_channels,
                    outsize[0], outsize[1])).astype(self.dtype)
        if self.dtype == numpy.float16:
            self.check_backward_options = {
                'dtype': numpy.float64, 'atol': 1e-2, 'rtol': 1e-2}
        else:
            self.check_backward_options = {'atol': 1e-3, 'rtol': 1e-2}

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_roi_average_pooling_2d.py" startline="30" endline="52" pcid="8490">
    def setUp(self):
        N = 3
        n_channels = 3
        self.x = pooling_nd_helper.shuffled_linspace(
            (N, n_channels, 12, 8), self.dtype)
        self.rois = numpy.array([
            [1, 1, 7, 7],
            [2, 6, 12, 8],
            [1, 3, 11, 6],
            [3, 3, 4, 4]
        ], dtype=self.dtype)
        self.roi_indices = numpy.array([0, 2, 1, 0], dtype=numpy.int32)
        n_rois = self.rois.shape[0]
        outsize = _pair(self.outsize)
        self.gy = numpy.random.uniform(
            -1, 1, (n_rois, n_channels,
                    outsize[0], outsize[1])).astype(self.dtype)
        if self.dtype == numpy.float16:
            self.check_backward_options = {
                'dtype': numpy.float64, 'atol': 1e-2, 'rtol': 1e-2}
        else:
            self.check_backward_options = {'atol': 1e-3, 'rtol': 1e-2}

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_roi_average_align_2d.py" startline="36" endline="55" pcid="8522">
    def setUp(self):
        N = 3
        n_channels = 3
        self.x = pooling_nd_helper.shuffled_linspace(
            (N, n_channels, 12, 8), numpy.float32)
        self.rois = numpy.array([
            [1, 1, 6, 6],
            [2, 6, 11, 7],
            [1, 3, 10, 5],
            [3, 3, 3, 3],
            [1.1, 2.2, 3.3, 4.4],
        ], dtype=numpy.float32)
        self.roi_indices = numpy.array([0, 2, 1, 0, 2], dtype=numpy.int32)
        n_rois = self.rois.shape[0]
        outsize = _pair(self.outsize)
        self.gy = numpy.random.uniform(
            -1, 1, (n_rois, n_channels,
                    outsize[0], outsize[1])).astype(numpy.float32)
        self.check_backward_options = {'atol': 5e-4, 'rtol': 5e-3}

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_roi_max_align_2d.py" startline="36" endline="55" pcid="8532">
    def setUp(self):
        N = 3
        n_channels = 3
        self.x = pooling_nd_helper.shuffled_linspace(
            (N, n_channels, 12, 8), numpy.float32)
        self.rois = numpy.array([
            [1, 1, 6, 6],
            [6, 2, 7, 11],
            [3, 1, 5, 10],
            [3, 3, 3, 3],
            [1.1, 2.2, 3.3, 4.4],
        ], dtype=numpy.float32)
        self.roi_indices = numpy.array([0, 2, 1, 0, 2], dtype=numpy.int32)
        n_rois = self.rois.shape[0]
        outsize = _pair(self.outsize)
        self.gy = numpy.random.uniform(
            -1, 1, (n_rois, n_channels,
                    outsize[0], outsize[1])).astype(numpy.float32)
        self.check_backward_options = {'atol': 5e-4, 'rtol': 5e-3}

</source>
</class>

<class classid="191" nclones="4" nlines="10" similarity="75">
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_roi_max_pooling_2d.py" startline="53" endline="64" pcid="8468">
    def check_forward(self, x_data, roi_data, roi_index_data):
        x = chainer.Variable(x_data)
        rois = chainer.Variable(roi_data)
        roi_indices = chainer.Variable(roi_index_data)
        y = functions.roi_max_pooling_2d(
            x, rois, roi_indices, outsize=self.outsize,
            spatial_scale=self.spatial_scale)
        self.assertEqual(y.data.dtype, self.dtype)
        y_data = cuda.to_cpu(y.data)

        self.assertEqual(self.gy.shape, y_data.shape)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_roi_max_align_2d.py" startline="56" endline="69" pcid="8533">
    def check_forward(self, x_data, roi_data, roi_index_data):
        x = chainer.Variable(x_data)
        rois = chainer.Variable(roi_data)
        roi_indices = chainer.Variable(roi_index_data)
        y = functions.roi_max_align_2d(
            x, rois, roi_indices, outsize=self.outsize,
            spatial_scale=self.spatial_scale,
            sampling_ratio=self.sampling_ratio,
        )
        self.assertEqual(y.data.dtype, numpy.float32)
        y_data = cuda.to_cpu(y.data)

        self.assertEqual(self.gy.shape, y_data.shape)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_roi_average_pooling_2d.py" startline="53" endline="64" pcid="8491">
    def check_forward(self, x_data, roi_data, roi_index_data):
        x = chainer.Variable(x_data)
        rois = chainer.Variable(roi_data)
        roi_indices = chainer.Variable(roi_index_data)
        y = functions.roi_average_pooling_2d(
            x, rois, roi_indices, outsize=self.outsize,
            spatial_scale=self.spatial_scale)
        self.assertEqual(y.data.dtype, self.dtype)
        y_data = cuda.to_cpu(y.data)

        self.assertEqual(self.gy.shape, y_data.shape)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_roi_average_align_2d.py" startline="56" endline="69" pcid="8523">
    def check_forward(self, x_data, roi_data, roi_index_data):
        x = chainer.Variable(x_data)
        rois = chainer.Variable(roi_data)
        roi_indices = chainer.Variable(roi_index_data)
        y = functions.roi_average_align_2d(
            x, rois, roi_indices, outsize=self.outsize,
            spatial_scale=self.spatial_scale,
            sampling_ratio=self.sampling_ratio,
        )
        self.assertEqual(y.data.dtype, numpy.float32)
        y_data = cuda.to_cpu(y.data)

        self.assertEqual(self.gy.shape, y_data.shape)

</source>
</class>

<class classid="192" nclones="2" nlines="13" similarity="84">
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_roi_max_pooling_2d.py" startline="93" endline="107" pcid="8472">
    def check_backward(self, x_data, roi_data, roi_index_data, y_grad):
        def f(x, rois, roi_indices):
            y = functions.roi_max_pooling_2d(
                x, rois, roi_indices, outsize=self.outsize,
                spatial_scale=self.spatial_scale)
            xp = cuda.get_array_module(y)
            # replace -inf with zero for gradient_check
            y = functions.where(
                xp.isinf(y.array), xp.zeros(y.shape, dtype=y.dtype), y)
            return y

        gradient_check.check_backward(
            f, (x_data, roi_data, roi_index_data), y_grad,
            no_grads=[False, True, True], **self.check_backward_options)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_roi_max_align_2d.py" startline="105" endline="119" pcid="8537">
    def check_backward(self, x_data, roi_data, roi_index_data, y_grad):
        def f(x, rois, roi_indices):
            y = functions.roi_max_align_2d(
                x, rois, roi_indices, outsize=self.outsize,
                spatial_scale=self.spatial_scale,
                sampling_ratio=self.sampling_ratio)
            xp = chainer.backend.get_array_module(y)
            y = functions.where(
                xp.isinf(y.array), xp.zeros(y.shape, dtype=y.dtype), y)
            return y

        gradient_check.check_backward(
            f, (x_data, roi_data, roi_index_data), y_grad,
            no_grads=[False, True, True], **self.check_backward_options)

</source>
</class>

<class classid="193" nclones="2" nlines="20" similarity="100">
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_max_pooling_2d.py" startline="140" endline="162" pcid="8486">
    def _check(self, x):
        out, indices = functions.max_pooling_2d(
            x, 2, cover_all=False, return_indices=True)
        assert isinstance(out, chainer.Variable)
        assert isinstance(out.array, type(x))
        assert isinstance(indices, type(x))
        assert indices.shape == out.array.shape

        # Calculate expected indices.
        expect = numpy.zeros(indices.shape, dtype=indices.dtype)
        for i in six.moves.range(2):
            for c in six.moves.range(3):
                xx = x[i, c]
                expect[i, c] = numpy.array([
                    [xx[0:2, 0:2].ravel().argmax(),
                     xx[0:2, 2:4].ravel().argmax()],
                    [xx[2:4, 0:2].ravel().argmax(),
                     xx[2:4, 2:4].ravel().argmax()],
                ])
        if out.xp is cuda.cupy:
            expect = cuda.to_gpu(expect)
        assert (expect == indices).all()

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_max_pooling_nd.py" startline="187" endline="209" pcid="8585">
    def _check(self, x):
        out, indices = functions.max_pooling_nd(
            x, 2, cover_all=False, return_indices=True)
        assert isinstance(out, chainer.Variable)
        assert isinstance(out.array, type(x))
        assert isinstance(indices, type(x))
        assert indices.shape == out.array.shape

        # Calculate expected indices.
        expect = numpy.zeros(indices.shape, dtype=indices.dtype)
        for i in six.moves.range(2):
            for c in six.moves.range(3):
                xx = x[i, c]
                expect[i, c] = numpy.array([
                    [xx[0:2, 0:2].ravel().argmax(),
                     xx[0:2, 2:4].ravel().argmax()],
                    [xx[2:4, 0:2].ravel().argmax(),
                     xx[2:4, 2:4].ravel().argmax()],
                ])
        if out.xp is cuda.cupy:
            expect = cuda.to_gpu(expect)
        assert (expect == indices).all()

</source>
</class>

<class classid="194" nclones="2" nlines="18" similarity="77">
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_roi_average_align_2d.py" startline="83" endline="104" pcid="8526">
    def test_forward_cpu_gpu_equal(self):
        # cpu
        x_cpu = chainer.Variable(self.x)
        rois_cpu = chainer.Variable(self.rois)
        roi_indices_cpu = chainer.Variable(self.roi_indices)
        y_cpu = functions.roi_average_align_2d(
            x_cpu, rois_cpu, roi_indices_cpu, outsize=self.outsize,
            spatial_scale=self.spatial_scale,
            sampling_ratio=self.sampling_ratio,
        )

        # gpu
        x_gpu = chainer.Variable(cuda.to_gpu(self.x))
        rois_gpu = chainer.Variable(cuda.to_gpu(self.rois))
        roi_indices_gpu = chainer.Variable(cuda.to_gpu(self.roi_indices))
        y_gpu = functions.roi_average_align_2d(
            x_gpu, rois_gpu, roi_indices_gpu, outsize=self.outsize,
            spatial_scale=self.spatial_scale,
            sampling_ratio=self.sampling_ratio,
        )
        testing.assert_allclose(y_cpu.data, cuda.to_cpu(y_gpu.data))

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_roi_max_align_2d.py" startline="83" endline="104" pcid="8536">
    def test_forward_cpu_gpu_equal(self):
        # cpu
        x_cpu = chainer.Variable(self.x)
        rois_cpu = chainer.Variable(self.rois)
        roi_index_cpu = chainer.Variable(self.roi_indices)
        y_cpu = functions.roi_max_align_2d(
            x_cpu, rois_cpu, roi_index_cpu,
            outsize=self.outsize, spatial_scale=self.spatial_scale,
            sampling_ratio=self.sampling_ratio,
        )

        # gpu
        x_gpu = chainer.Variable(cuda.to_gpu(self.x))
        rois_gpu = chainer.Variable(cuda.to_gpu(self.rois))
        roi_index_gpu = chainer.Variable(cuda.to_gpu(self.roi_indices))
        y_gpu = functions.roi_max_align_2d(
            x_gpu, rois_gpu, roi_index_gpu,
            outsize=self.outsize, spatial_scale=self.spatial_scale,
            sampling_ratio=self.sampling_ratio,
        )
        testing.assert_allclose(y_cpu.data, cuda.to_cpu(y_gpu.data))

</source>
</class>

<class classid="195" nclones="2" nlines="12" similarity="91">
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_average_pooling_nd.py" startline="195" endline="208" pcid="8559">
    def setUp(self):
        self.ndim = len(self.dims)
        self.ksize = (3,) * self.ndim
        self.stride = (2,) * self.ndim
        self.pad = (1,) * self.ndim
        x_shape = (2, 3) + self.dims
        self.x = cuda.cupy.arange(functools.reduce(operator.mul, x_shape),
                                  dtype=self.dtype).reshape(x_shape)
        gy_shape = (2, 3) + tuple(
            conv.get_conv_outsize(d, k, s, p)
            for (d, k, s, p)
            in six.moves.zip(self.dims, self.ksize, self.stride, self.pad))
        self.gy = cuda.cupy.random.uniform(-1, 1, gy_shape).astype(self.dtype)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/functions_tests/pooling_tests/test_max_pooling_nd.py" startline="110" endline="123" pcid="8575">
    def setUp(self):
        self.ndim = len(self.dims)
        self.ksize = (3,) * self.ndim
        self.stride = (2,) * self.ndim
        self.pad = (1,) * self.ndim
        x_shape = (2, 3) + self.dims
        self.x = cuda.cupy.arange(functools.reduce(mul, x_shape),
                                  dtype=self.dtype).reshape(x_shape)
        gy_shape = (2, 3) + tuple(
            conv.get_conv_outsize(d, k, s, p)
            for (d, k, s, p)
            in six.moves.zip(self.dims, self.ksize, self.stride, self.pad))
        self.gy = cuda.cupy.random.uniform(-1, 1, gy_shape).astype(self.dtype)

</source>
</class>

<class classid="196" nclones="6" nlines="10" similarity="80">
<source file="systems/chainer-7.2.0/tests/chainer_tests/link_hooks_tests/test_weight_standardization.py" startline="135" endline="146" pcid="8889">
    def setUp(self):
        self.in_channels, self.out_channels = 3, 10
        in_channels = None if self.lazy_init else self.in_channels
        conv_init_args = {'ksize': 3, 'stride': 1, 'pad': 1}
        self.layer = self.link(
            in_channels, self.out_channels, **conv_init_args)
        self.x = numpy.random.normal(
            size=(5, self.in_channels, 4)).astype(numpy.float32)
        self.hook = WeightStandardization()
        self.out_size = self.out_channels  # For compatibility


</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/link_hooks_tests/test_weight_standardization.py" startline="171" endline="182" pcid="8891">
    def setUp(self):
        self.in_channels, self.out_channels = 3, 10
        in_channels = None if self.lazy_init else self.in_channels
        conv_init_args = {'ksize': 3, 'stride': 1, 'pad': 1}
        self.layer = self.link(
            in_channels, self.out_channels, **conv_init_args)
        self.x = numpy.random.normal(
            size=(5, self.in_channels, 4, 4, 4)).astype(numpy.float32)
        self.hook = WeightStandardization()
        self.out_size = self.out_channels  # For compatibility


</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/link_hooks_tests/test_spectral_normalization.py" startline="365" endline="376" pcid="8918">
    def setUp(self):
        self.in_channels, self.out_channels = 3, 10
        in_channels = None if self.lazy_init else self.in_channels
        conv_init_args = {'ksize': 3, 'stride': 1, 'pad': 1}
        self.layer = self.link(
            in_channels, self.out_channels, **conv_init_args)
        self.x = numpy.random.normal(
            size=(5, self.in_channels, 4, 4, 4)).astype(numpy.float32)
        self.hook = SpectralNormalization(use_gamma=self.use_gamma)
        self.out_size = self.out_channels  # For compatibility


</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/link_hooks_tests/test_spectral_normalization.py" startline="345" endline="356" pcid="8917">
    def setUp(self):
        self.in_channels, self.out_channels = 3, 10
        in_channels = None if self.lazy_init else self.in_channels
        conv_init_args = {'ksize': 3, 'stride': 1, 'pad': 1}
        self.layer = self.link(
            in_channels, self.out_channels, **conv_init_args)
        self.x = numpy.random.normal(
            size=(5, self.in_channels, 4, 4)).astype(numpy.float32)
        self.hook = SpectralNormalization(use_gamma=self.use_gamma)
        self.out_size = self.out_channels  # For compatibility


</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/link_hooks_tests/test_spectral_normalization.py" startline="325" endline="336" pcid="8916">
    def setUp(self):
        self.in_channels, self.out_channels = 3, 10
        in_channels = None if self.lazy_init else self.in_channels
        conv_init_args = {'ksize': 3, 'stride': 1, 'pad': 1}
        self.layer = self.link(
            in_channels, self.out_channels, **conv_init_args)
        self.x = numpy.random.normal(
            size=(5, self.in_channels, 4)).astype(numpy.float32)
        self.hook = SpectralNormalization(use_gamma=self.use_gamma)
        self.out_size = self.out_channels  # For compatibility


</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/link_hooks_tests/test_weight_standardization.py" startline="153" endline="164" pcid="8890">
    def setUp(self):
        self.in_channels, self.out_channels = 3, 10
        in_channels = None if self.lazy_init else self.in_channels
        conv_init_args = {'ksize': 3, 'stride': 1, 'pad': 1}
        self.layer = self.link(
            in_channels, self.out_channels, **conv_init_args)
        self.x = numpy.random.normal(
            size=(5, self.in_channels, 4, 4)).astype(numpy.float32)
        self.hook = WeightStandardization()
        self.out_size = self.out_channels  # For compatibility


</source>
</class>

<class classid="197" nclones="2" nlines="17" similarity="88">
<source file="systems/chainer-7.2.0/tests/chainer_tests/link_hooks_tests/test_spectral_normalization.py" startline="43" endline="59" pcid="8896">
    def test_add_sn_hook(self):
        layer, hook = self.layer, self.hook
        layer.add_hook(hook)
        if self.lazy_init:
            assert not hasattr(layer, hook.vector_name)
            if self.use_gamma:
                assert not hasattr(layer, 'gamma')
            with chainer.using_config('train', False):
                layer(self.x)
        assert hasattr(layer, hook.vector_name)
        assert (self.out_size,) == getattr(layer, hook.vector_name).shape
        if not self.use_gamma:
            assert not hasattr(layer, 'gamma')
        else:  # Use gamma parameter
            assert hasattr(layer, 'gamma')
            assert layer.gamma.ndim == 0 and layer.gamma.size == 1

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/link_hooks_tests/test_spectral_normalization.py" startline="281" endline="299" pcid="8914">
    def test_add_sn_hook(self):
        hook = SpectralNormalization(use_gamma=self.use_gamma)
        layer = self.layer
        layer.add_hook(hook)
        if self.lazy_init:
            assert not hasattr(layer, hook.vector_name)
            if self.use_gamma:
                assert not hasattr(layer, 'gamma')
            with chainer.using_config('train', False):
                layer(self.x)
        assert hasattr(layer, hook.vector_name)
        assert (self.in_size,) == getattr(layer, hook.vector_name).shape
        if not self.use_gamma:
            assert not hasattr(layer, 'gamma')
        else:  # Use gamma parameter
            assert hasattr(layer, 'gamma')
            assert layer.gamma.ndim == 0 and layer.gamma.size == 1


</source>
</class>

<class classid="198" nclones="2" nlines="17" similarity="88">
<source file="systems/chainer-7.2.0/tests/chainer_tests/link_hooks_tests/test_spectral_normalization.py" startline="82" endline="101" pcid="8900">
    def check_in_recomputing(self, backend_config):
        layer, hook = self.layer, self.hook
        layer.add_hook(hook)
        layer.to_device(backend_config.device)
        x = backend_config.get_array(self.x)

        y1 = layer(x).array
        u1 = getattr(layer, hook.vector_name).copy()
        v1 = hook.v.copy()
        with chainer.using_config('in_recomputing', True):
            y2 = layer(x).array
        u2 = getattr(layer, hook.vector_name)
        v2 = hook.v

        u1, u2 = _cpu._to_cpu(u1), _cpu._to_cpu(u2)
        v1, v2 = _cpu._to_cpu(v1), _cpu._to_cpu(v2)
        numpy.testing.assert_array_equal(u1, u2)
        numpy.testing.assert_array_equal(v1, v2)
        testing.assert_allclose(y1, y2)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/link_hooks_tests/test_spectral_normalization.py" startline="145" endline="164" pcid="8906">
    def check_u_not_updated_in_test(self, backend_config):
        layer, hook = self.layer, self.hook
        layer.add_hook(hook)
        layer.to_device(backend_config.device)
        x = backend_config.get_array(self.x)

        with chainer.using_config('train', False):
            y1 = layer(x).array
            u1 = getattr(layer, hook.vector_name).copy()
            v1 = hook.v.copy()
            y2 = layer(x).array
            u2 = getattr(layer, hook.vector_name)
            v2 = hook.v.copy()

        u1, u2 = _cpu._to_cpu(u1), _cpu._to_cpu(u2)
        v1, v2 = _cpu._to_cpu(v1), _cpu._to_cpu(v2)
        numpy.testing.assert_array_equal(u1, u2)
        numpy.testing.assert_array_equal(v1, v2)
        testing.assert_allclose(y1, y2)

</source>
</class>

<class classid="199" nclones="6" nlines="29" similarity="86">
<source file="systems/chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multiprocess_iterator.py" startline="39" endline="68" pcid="8920">
    def test_iterator_repeat(self):
        dataset = [1, 2, 3, 4, 5, 6]
        it = iterators.MultiprocessIterator(dataset, 2, **self.options)
        for i in range(3):
            self.assertEqual(it.epoch, i)
            self.assertAlmostEqual(it.epoch_detail, i + 0 / 6)
            if i == 0:
                self.assertIsNone(it.previous_epoch_detail)
            else:
                self.assertAlmostEqual(it.previous_epoch_detail, i - 2 / 6)
            batch1 = it.next()
            self.assertEqual(len(batch1), 2)
            self.assertIsInstance(batch1, list)
            self.assertFalse(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 2 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 0 / 6)
            batch2 = it.next()
            self.assertEqual(len(batch2), 2)
            self.assertIsInstance(batch2, list)
            self.assertFalse(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 4 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 2 / 6)
            batch3 = it.next()
            self.assertEqual(len(batch3), 2)
            self.assertIsInstance(batch3, list)
            self.assertTrue(it.is_new_epoch)
            self.assertEqual(sorted(batch1 + batch2 + batch3), dataset)
            self.assertAlmostEqual(it.epoch_detail, i + 6 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 4 / 6)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/iterators_tests/test_serial_iterator.py" startline="114" endline="141" pcid="8995">
    def test_iterator_repeat(self):
        dataset = [1, 2, 3, 4, 5, 6]
        it = iterators.SerialIterator(dataset, 2, shuffle=self.shuffle,
                                      order_sampler=self.order_sampler)
        for i in range(3):
            self.assertEqual(it.epoch, i)
            self.assertAlmostEqual(it.epoch_detail, i + 0 / 6)
            if i == 0:
                self.assertIsNone(it.previous_epoch_detail)
            else:
                self.assertAlmostEqual(it.previous_epoch_detail, i - 2 / 6)
            batch1 = it.next()
            self.assertEqual(len(batch1), 2)
            self.assertFalse(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 2 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 0 / 6)
            batch2 = it.next()
            self.assertEqual(len(batch2), 2)
            self.assertFalse(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 4 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 2 / 6)
            batch3 = it.next()
            self.assertEqual(len(batch3), 2)
            self.assertTrue(it.is_new_epoch)
            self.assertEqual(sorted(batch1 + batch2 + batch3), dataset)
            self.assertAlmostEqual(it.epoch_detail, i + 6 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 4 / 6)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multiprocess_iterator.py" startline="523" endline="555" pcid="8943">
    def test_iterator_repeat(self):
        dataset = [1, 2, 3]
        it = iterators.MultiprocessIterator(dataset, 2, **self.options)
        for i in range(3):
            self.assertEqual(it.epoch, i)
            self.assertAlmostEqual(it.epoch_detail, i + 0 / 6)
            if i == 0:
                self.assertIsNone(it.previous_epoch_detail)
            else:
                self.assertAlmostEqual(it.previous_epoch_detail, i - 2 / 6)
            batch1 = it.next()
            self.assertEqual(len(batch1), 2)
            self.assertIsInstance(batch1, list)
            self.assertFalse(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 2 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 0 / 6)
            batch2 = it.next()
            self.assertEqual(len(batch2), 2)
            self.assertIsInstance(batch2, list)
            self.assertFalse(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 4 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 2 / 6)
            batch3 = it.next()
            self.assertEqual(len(batch3), 2)
            self.assertIsInstance(batch3, list)
            self.assertTrue(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 6 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 4 / 6)

            self.assertEqual(
                sorted(batch1 + batch2 + batch3), [1, 1, 2, 2, 3, 3])


</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multithread_iterator.py" startline="24" endline="53" pcid="8966">
    def test_iterator_repeat(self):
        dataset = [1, 2, 3, 4, 5, 6]
        it = iterators.MultithreadIterator(dataset, 2, **self.options)
        for i in range(3):
            self.assertEqual(it.epoch, i)
            self.assertAlmostEqual(it.epoch_detail, i + 0 / 6)
            if i == 0:
                self.assertIsNone(it.previous_epoch_detail)
            else:
                self.assertAlmostEqual(it.previous_epoch_detail, i - 2 / 6)
            batch1 = it.next()
            self.assertEqual(len(batch1), 2)
            self.assertIsInstance(batch1, list)
            self.assertFalse(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 2 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 0 / 6)
            batch2 = it.next()
            self.assertEqual(len(batch2), 2)
            self.assertIsInstance(batch2, list)
            self.assertFalse(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 4 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 2 / 6)
            batch3 = it.next()
            self.assertEqual(len(batch3), 2)
            self.assertIsInstance(batch3, list)
            self.assertTrue(it.is_new_epoch)
            self.assertEqual(sorted(batch1 + batch2 + batch3), dataset)
            self.assertAlmostEqual(it.epoch_detail, i + 6 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 4 / 6)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/iterators_tests/test_serial_iterator.py" startline="306" endline="338" pcid="9006">
    def test_iterator_repeat(self):
        dataset = [1, 2, 3]
        it = iterators.SerialIterator(dataset, 2, **self.options)
        for i in range(3):
            self.assertEqual(it.epoch, i)
            self.assertAlmostEqual(it.epoch_detail, i + 0 / 6)
            if i == 0:
                self.assertIsNone(it.previous_epoch_detail)
            else:
                self.assertAlmostEqual(it.previous_epoch_detail, i - 2 / 6)
            batch1 = it.next()
            self.assertEqual(len(batch1), 2)
            self.assertIsInstance(batch1, list)
            self.assertFalse(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 2 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 0 / 6)
            batch2 = it.next()
            self.assertEqual(len(batch2), 2)
            self.assertIsInstance(batch2, list)
            self.assertFalse(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 4 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 2 / 6)
            batch3 = it.next()
            self.assertEqual(len(batch3), 2)
            self.assertIsInstance(batch3, list)
            self.assertTrue(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 6 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 4 / 6)

            self.assertEqual(
                sorted(batch1 + batch2 + batch3), [1, 1, 2, 2, 3, 3])


</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multithread_iterator.py" startline="310" endline="342" pcid="8984">
    def test_iterator_repeat(self):
        dataset = [1, 2, 3]
        it = iterators.MultithreadIterator(dataset, 2, **self.options)
        for i in range(3):
            self.assertEqual(it.epoch, i)
            self.assertAlmostEqual(it.epoch_detail, i + 0 / 6)
            if i == 0:
                self.assertIsNone(it.previous_epoch_detail)
            else:
                self.assertAlmostEqual(it.previous_epoch_detail, i - 2 / 6)
            batch1 = it.next()
            self.assertEqual(len(batch1), 2)
            self.assertIsInstance(batch1, list)
            self.assertFalse(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 2 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 0 / 6)
            batch2 = it.next()
            self.assertEqual(len(batch2), 2)
            self.assertIsInstance(batch2, list)
            self.assertFalse(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 4 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 2 / 6)
            batch3 = it.next()
            self.assertEqual(len(batch3), 2)
            self.assertIsInstance(batch3, list)
            self.assertTrue(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 6 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 4 / 6)

            self.assertEqual(
                sorted(batch1 + batch2 + batch3), [1, 1, 2, 2, 3, 3])


</source>
</class>

<class classid="200" nclones="6" nlines="29" similarity="78">
<source file="systems/chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multiprocess_iterator.py" startline="72" endline="102" pcid="8921">
    def test_iterator_list_type(self):
        dataset = [[i, numpy.zeros((10,)) + i] for i in range(6)]
        it = iterators.MultiprocessIterator(dataset, 2, **self.options)
        for i in range(3):
            self.assertEqual(it.epoch, i)
            self.assertAlmostEqual(it.epoch_detail, i)
            if i == 0:
                self.assertIsNone(it.previous_epoch_detail)
            else:
                self.assertAlmostEqual(it.previous_epoch_detail, i - 2 / 6)
            batches = {}
            for j in range(3):
                batch = it.next()
                self.assertEqual(len(batch), 2)
                if j != 2:
                    self.assertFalse(it.is_new_epoch)
                else:
                    self.assertTrue(it.is_new_epoch)
                self.assertAlmostEqual(
                    it.epoch_detail, (3 * i + j + 1) * 2 / 6)
                self.assertAlmostEqual(
                    it.previous_epoch_detail, (3 * i + j) * 2 / 6)
                for x in batch:
                    self.assertIsInstance(x, list)
                    self.assertIsInstance(x[1], numpy.ndarray)
                    batches[x[0]] = x[1]

            self.assertEqual(len(batches), len(dataset))
            for k, v in six.iteritems(batches):
                numpy.testing.assert_allclose(dataset[k][1], v)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multiprocess_iterator.py" startline="106" endline="136" pcid="8922">
    def test_iterator_tuple_type(self):
        dataset = [(i, numpy.zeros((10,)) + i) for i in range(6)]
        it = iterators.MultiprocessIterator(dataset, 2, **self.options)
        for i in range(3):
            self.assertEqual(it.epoch, i)
            self.assertAlmostEqual(it.epoch_detail, i)
            if i == 0:
                self.assertIsNone(it.previous_epoch_detail)
            else:
                self.assertAlmostEqual(it.previous_epoch_detail, i - 2 / 6)
            batches = {}
            for j in range(3):
                batch = it.next()
                self.assertEqual(len(batch), 2)
                if j != 2:
                    self.assertFalse(it.is_new_epoch)
                else:
                    self.assertTrue(it.is_new_epoch)
                self.assertAlmostEqual(
                    it.epoch_detail, (3 * i + j + 1) * 2 / 6)
                self.assertAlmostEqual(
                    it.previous_epoch_detail, (3 * i + j) * 2 / 6)
                for x in batch:
                    self.assertIsInstance(x, tuple)
                    self.assertIsInstance(x[1], numpy.ndarray)
                    batches[x[0]] = x[1]

            self.assertEqual(len(batches), len(dataset))
            for k, v in six.iteritems(batches):
                numpy.testing.assert_allclose(dataset[k][1], v)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multiprocess_iterator.py" startline="140" endline="173" pcid="8923">
    def test_iterator_dict_type(self):
        dataset = [{i: numpy.zeros((10,)) + i} for i in range(6)]
        it = iterators.MultiprocessIterator(dataset, 2, **self.options)
        for i in range(3):
            self.assertEqual(it.epoch, i)
            self.assertAlmostEqual(it.epoch_detail, i)
            if i == 0:
                self.assertIsNone(it.previous_epoch_detail)
            else:
                self.assertAlmostEqual(it.previous_epoch_detail, i - 2 / 6)
            batches = {}
            for j in range(3):
                batch = it.next()
                self.assertEqual(len(batch), 2)
                if j != 2:
                    self.assertFalse(it.is_new_epoch)
                else:
                    self.assertTrue(it.is_new_epoch)
                self.assertAlmostEqual(
                    it.epoch_detail, (3 * i + j + 1) * 2 / 6)
                self.assertAlmostEqual(
                    it.previous_epoch_detail, (3 * i + j) * 2 / 6)
                for x in batch:
                    self.assertIsInstance(x, dict)
                    k = tuple(x)[0]
                    v = x[k]
                    self.assertIsInstance(v, numpy.ndarray)
                    batches[k] = v

            self.assertEqual(len(batches), len(dataset))
            for k, v in six.iteritems(batches):
                x = dataset[k][tuple(dataset[k])[0]]
                numpy.testing.assert_allclose(x, v)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multithread_iterator.py" startline="116" endline="149" pcid="8969">
    def test_iterator_dict_type(self):
        dataset = [{i: numpy.zeros((10,)) + i} for i in range(6)]
        it = iterators.MultithreadIterator(dataset, 2, **self.options)
        for i in range(3):
            self.assertEqual(it.epoch, i)
            self.assertAlmostEqual(it.epoch_detail, i)
            if i == 0:
                self.assertIsNone(it.previous_epoch_detail)
            else:
                self.assertAlmostEqual(it.previous_epoch_detail, i - 2 / 6)
            batches = {}
            for j in range(3):
                batch = it.next()
                self.assertEqual(len(batch), 2)
                if j != 2:
                    self.assertFalse(it.is_new_epoch)
                else:
                    self.assertTrue(it.is_new_epoch)
                self.assertAlmostEqual(
                    it.epoch_detail, (3 * i + j + 1) * 2 / 6)
                self.assertAlmostEqual(
                    it.previous_epoch_detail, (3 * i + j) * 2 / 6)
                for x in batch:
                    self.assertIsInstance(x, dict)
                    k = tuple(x)[0]
                    v = x[k]
                    self.assertIsInstance(v, numpy.ndarray)
                    batches[k] = v

            self.assertEqual(len(batches), len(dataset))
            for k, v in six.iteritems(batches):
                x = dataset[k][tuple(dataset[k])[0]]
                numpy.testing.assert_allclose(x, v)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multithread_iterator.py" startline="54" endline="84" pcid="8967">
    def test_iterator_list_type(self):
        dataset = [[i, numpy.zeros((10,)) + i] for i in range(6)]
        it = iterators.MultithreadIterator(dataset, 2, **self.options)
        for i in range(3):
            self.assertEqual(it.epoch, i)
            self.assertAlmostEqual(it.epoch_detail, i)
            if i == 0:
                self.assertIsNone(it.previous_epoch_detail)
            else:
                self.assertAlmostEqual(it.previous_epoch_detail, i - 2 / 6)
            batches = {}
            for j in range(3):
                batch = it.next()
                self.assertEqual(len(batch), 2)
                if j != 2:
                    self.assertFalse(it.is_new_epoch)
                else:
                    self.assertTrue(it.is_new_epoch)
                self.assertAlmostEqual(
                    it.epoch_detail, (3 * i + j + 1) * 2 / 6)
                self.assertAlmostEqual(
                    it.previous_epoch_detail, (3 * i + j) * 2 / 6)
                for x in batch:
                    self.assertIsInstance(x, list)
                    self.assertIsInstance(x[1], numpy.ndarray)
                    batches[x[0]] = x[1]

            self.assertEqual(len(batches), len(dataset))
            for k, v in six.iteritems(batches):
                numpy.testing.assert_allclose(dataset[k][1], v)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multithread_iterator.py" startline="85" endline="115" pcid="8968">
    def test_iterator_tuple_type(self):
        dataset = [(i, numpy.zeros((10,)) + i) for i in range(6)]
        it = iterators.MultithreadIterator(dataset, 2, **self.options)
        for i in range(3):
            self.assertEqual(it.epoch, i)
            self.assertAlmostEqual(it.epoch_detail, i)
            if i == 0:
                self.assertIsNone(it.previous_epoch_detail)
            else:
                self.assertAlmostEqual(it.previous_epoch_detail, i - 2 / 6)
            batches = {}
            for j in range(3):
                batch = it.next()
                self.assertEqual(len(batch), 2)
                if j != 2:
                    self.assertFalse(it.is_new_epoch)
                else:
                    self.assertTrue(it.is_new_epoch)
                self.assertAlmostEqual(
                    it.epoch_detail, (3 * i + j + 1) * 2 / 6)
                self.assertAlmostEqual(
                    it.previous_epoch_detail, (3 * i + j) * 2 / 6)
                for x in batch:
                    self.assertIsInstance(x, tuple)
                    self.assertIsInstance(x[1], numpy.ndarray)
                    batches[x[0]] = x[1]

            self.assertEqual(len(batches), len(dataset))
            for k, v in six.iteritems(batches):
                numpy.testing.assert_allclose(dataset[k][1], v)

</source>
</class>

<class classid="201" nclones="3" nlines="18" similarity="84">
<source file="systems/chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multiprocess_iterator.py" startline="200" endline="220" pcid="8926">
    def test_iterator_not_repeat_not_even(self):
        dataset = [1, 2, 3, 4, 5]
        it = iterators.MultiprocessIterator(
            dataset, 2, repeat=False, **self.options)

        self.assertAlmostEqual(it.epoch_detail, 0 / 5)
        self.assertIsNone(it.previous_epoch_detail)
        batch1 = it.next()
        self.assertAlmostEqual(it.epoch_detail, 2 / 5)
        self.assertAlmostEqual(it.previous_epoch_detail, 0 / 5)
        batch2 = it.next()
        self.assertAlmostEqual(it.epoch_detail, 4 / 5)
        self.assertAlmostEqual(it.previous_epoch_detail, 2 / 5)
        batch3 = it.next()
        self.assertAlmostEqual(it.epoch_detail, 5 / 5)
        self.assertAlmostEqual(it.previous_epoch_detail, 4 / 5)
        self.assertRaises(StopIteration, it.next)

        self.assertEqual(len(batch3), 1)
        self.assertEqual(sorted(batch1 + batch2 + batch3), dataset)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multithread_iterator.py" startline="167" endline="187" pcid="8972">
    def test_iterator_not_repeat_not_even(self):
        dataset = [1, 2, 3, 4, 5]
        it = iterators.MultithreadIterator(
            dataset, 2, repeat=False, **self.options)

        self.assertAlmostEqual(it.epoch_detail, 0 / 5)
        self.assertIsNone(it.previous_epoch_detail)
        batch1 = it.next()
        self.assertAlmostEqual(it.epoch_detail, 2 / 5)
        self.assertAlmostEqual(it.previous_epoch_detail, 0 / 5)
        batch2 = it.next()
        self.assertAlmostEqual(it.epoch_detail, 4 / 5)
        self.assertAlmostEqual(it.previous_epoch_detail, 2 / 5)
        batch3 = it.next()
        self.assertAlmostEqual(it.epoch_detail, 5 / 5)
        self.assertAlmostEqual(it.previous_epoch_detail, 4 / 5)
        self.assertRaises(StopIteration, it.next)

        self.assertEqual(len(batch3), 1)
        self.assertEqual(sorted(batch1 + batch2 + batch3), dataset)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/iterators_tests/test_serial_iterator.py" startline="162" endline="183" pcid="8998">
    def test_iterator_not_repeat_not_even(self):
        dataset = [1, 2, 3, 4, 5]
        it = iterators.SerialIterator(dataset, 2, repeat=False,
                                      shuffle=self.shuffle,
                                      order_sampler=self.order_sampler)

        self.assertAlmostEqual(it.epoch_detail, 0 / 5)
        self.assertIsNone(it.previous_epoch_detail)
        batch1 = it.next()
        self.assertAlmostEqual(it.epoch_detail, 2 / 5)
        self.assertAlmostEqual(it.previous_epoch_detail, 0 / 5)
        batch2 = it.next()
        self.assertAlmostEqual(it.epoch_detail, 4 / 5)
        self.assertAlmostEqual(it.previous_epoch_detail, 2 / 5)
        batch3 = it.next()
        self.assertAlmostEqual(it.epoch_detail, 5 / 5)
        self.assertAlmostEqual(it.previous_epoch_detail, 4 / 5)
        self.assertRaises(StopIteration, it.next)

        self.assertEqual(len(batch3), 1)
        self.assertEqual(sorted(batch1 + batch2 + batch3), dataset)

</source>
</class>

<class classid="202" nclones="2" nlines="14" similarity="100">
<source file="systems/chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multiprocess_iterator.py" startline="243" endline="258" pcid="8929">
    def test_copy_not_repeat(self):
        dataset = [1, 2, 3, 4, 5]
        it = iterators.MultiprocessIterator(
            dataset, 2, repeat=False, **self.options)
        copy_it = copy.copy(it)
        batches = sum([it.next() for _ in range(3)], [])
        self.assertEqual(sorted(batches), dataset)
        for _ in range(2):
            self.assertRaises(StopIteration, it.next)
        it = None

        batches = sum([copy_it.next() for _ in range(3)], [])
        self.assertEqual(sorted(batches), dataset)
        for _ in range(2):
            self.assertRaises(StopIteration, copy_it.next)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multithread_iterator.py" startline="201" endline="216" pcid="8975">
    def test_copy_not_repeat(self):
        dataset = [1, 2, 3, 4, 5]
        it = iterators.MultithreadIterator(
            dataset, 2, repeat=False, **self.options)
        copy_it = copy.copy(it)
        batches = sum([it.next() for _ in range(3)], [])
        self.assertEqual(sorted(batches), dataset)
        for _ in range(2):
            self.assertRaises(StopIteration, it.next)
        it = None

        batches = sum([copy_it.next() for _ in range(3)], [])
        self.assertEqual(sorted(batches), dataset)
        for _ in range(2):
            self.assertRaises(StopIteration, copy_it.next)

</source>
</class>

<class classid="203" nclones="4" nlines="10" similarity="72">
<source file="systems/chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multiprocess_iterator.py" startline="262" endline="273" pcid="8930">
    def test_reset(self):
        dataset = [1, 2, 3, 4, 5]
        it = iterators.MultiprocessIterator(
            dataset, 2, repeat=False, **self.options)

        for trial in range(4):
            batches = sum([it.next() for _ in range(3)], [])
            self.assertEqual(sorted(batches), dataset)
            for _ in range(2):
                self.assertRaises(StopIteration, it.next)
            it.reset()

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/iterators_tests/test_serial_iterator.py" startline="196" endline="209" pcid="9001">
    def test_reset(self):
        dataset = [1, 2, 3, 4, 5]
        it = iterators.SerialIterator(dataset, 2, repeat=False,
                                      shuffle=self.shuffle,
                                      order_sampler=self.order_sampler)

        for trial in range(4):
            batches = sum([it.next() for _ in range(3)], [])
            self.assertEqual(sorted(batches), dataset)
            for _ in range(2):
                self.assertRaises(StopIteration, it.next)
            it.reset()


</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multithread_iterator.py" startline="217" endline="228" pcid="8976">
    def test_reset(self):
        dataset = [1, 2, 3, 4, 5]
        it = iterators.MultithreadIterator(
            dataset, 2, repeat=False, **self.options)

        for trial in range(4):
            batches = sum([it.next() for _ in range(3)], [])
            self.assertEqual(sorted(batches), dataset)
            for _ in range(2):
                self.assertRaises(StopIteration, it.next)
            it.reset()

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multiprocess_iterator.py" startline="277" endline="290" pcid="8931">
    def test_reset_middle(self):
        dataset = [1, 2, 3, 4, 5]
        it = iterators.MultiprocessIterator(
            dataset, 2, repeat=False, **self.options)

        for trial in range(4):
            it.next()
            it.reset()
            batches = sum([it.next() for _ in range(3)], [])
            self.assertEqual(sorted(batches), dataset)
            for _ in range(2):
                self.assertRaises(StopIteration, it.next)
            it.reset()

</source>
</class>

<class classid="204" nclones="7" nlines="31" similarity="75">
<source file="systems/chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multiprocess_iterator.py" startline="377" endline="412" pcid="8937">
    def test_iterator_pickle_after_init(self):
        dataset = [1, 2, 3, 4, 5, 6]
        it = iterators.MultiprocessIterator(dataset, 2, **self.options)

        self.assertEqual(it.epoch, 0)
        self.assertAlmostEqual(it.epoch_detail, 0 / 6)
        self.assertIsNone(it.previous_epoch_detail)
        batch1 = it.next()
        self.assertEqual(len(batch1), 2)
        self.assertIsInstance(batch1, list)
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 2 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 0 / 6)
        batch2 = it.next()
        self.assertEqual(len(batch2), 2)
        self.assertIsInstance(batch2, list)
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 4 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 2 / 6)

        pickled_it = pickle.dumps(it)
        it = pickle.loads(pickled_it)

        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 4 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 2 / 6)

        batch3 = it.next()
        self.assertEqual(len(batch3), 2)
        self.assertIsInstance(batch3, list)
        self.assertTrue(it.is_new_epoch)
        self.assertEqual(sorted(batch1 + batch2 + batch3), dataset)
        self.assertAlmostEqual(it.epoch_detail, 6 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 4 / 6)


</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multithread_iterator.py" startline="265" endline="302" pcid="8981">
    def test_iterator_serialize(self):
        dataset = [1, 2, 3, 4, 5, 6]
        it = iterators.MultithreadIterator(dataset, 2, **self.options)

        self.assertEqual(it.epoch, 0)
        self.assertAlmostEqual(it.epoch_detail, 0 / 6)
        self.assertIsNone(it.previous_epoch_detail)
        batch1 = it.next()
        self.assertEqual(len(batch1), 2)
        self.assertIsInstance(batch1, list)
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 2 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 0 / 6)
        batch2 = it.next()
        self.assertEqual(len(batch2), 2)
        self.assertIsInstance(batch2, list)
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 4 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 2 / 6)

        target = dict()
        it.serialize(serializers.DictionarySerializer(target))

        it = iterators.MultithreadIterator(dataset, 2, **self.options)
        it.serialize(serializers.NpzDeserializer(target))
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 4 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 2 / 6)

        batch3 = it.next()
        self.assertEqual(len(batch3), 2)
        self.assertIsInstance(batch3, list)
        self.assertTrue(it.is_new_epoch)
        self.assertEqual(sorted(batch1 + batch2 + batch3), dataset)
        self.assertAlmostEqual(it.epoch_detail, 6 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 4 / 6)


</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/iterators_tests/test_serial_iterator.py" startline="217" endline="254" pcid="9002">
    def test_iterator_serialize(self):
        dataset = [1, 2, 3, 4, 5, 6]
        it = iterators.SerialIterator(dataset, 2, shuffle=self.shuffle,
                                      order_sampler=self.order_sampler)

        self.assertEqual(it.epoch, 0)
        self.assertAlmostEqual(it.epoch_detail, 0 / 6)
        self.assertIsNone(it.previous_epoch_detail)
        batch1 = it.next()
        self.assertEqual(len(batch1), 2)
        self.assertIsInstance(batch1, list)
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 2 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 0 / 6)
        batch2 = it.next()
        self.assertEqual(len(batch2), 2)
        self.assertIsInstance(batch2, list)
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 4 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 2 / 6)

        target = dict()
        it.serialize(serializers.DictionarySerializer(target))

        it = iterators.SerialIterator(dataset, 2)
        it.serialize(serializers.NpzDeserializer(target))
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 4 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 2 / 6)

        batch3 = it.next()
        self.assertEqual(len(batch3), 2)
        self.assertIsInstance(batch3, list)
        self.assertTrue(it.is_new_epoch)
        self.assertEqual(sorted(batch1 + batch2 + batch3), dataset)
        self.assertAlmostEqual(it.epoch_detail, 6 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 4 / 6)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multiprocess_iterator.py" startline="467" endline="506" pcid="8940">
    def test_iterator_serialize_backward_compat(self):
        dataset = [1, 2, 3, 4, 5, 6]
        it = iterators.MultiprocessIterator(dataset, 2, **self.options)

        self.assertEqual(it.epoch, 0)
        self.assertAlmostEqual(it.epoch_detail, 0 / 6)
        self.assertIsNone(it.previous_epoch_detail)
        batch1 = it.next()
        self.assertEqual(len(batch1), 2)
        self.assertIsInstance(batch1, list)
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 2 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 0 / 6)
        batch2 = it.next()
        self.assertEqual(len(batch2), 2)
        self.assertIsInstance(batch2, list)
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 4 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 2 / 6)

        target = dict()
        it.serialize(serializers.DictionarySerializer(target))
        # older version does not have previous_epoch_detail
        del target['previous_epoch_detail']

        it = iterators.MultiprocessIterator(dataset, 2, **self.options)
        it.serialize(serializers.NpzDeserializer(target))
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 4 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 2 / 6)

        batch3 = it.next()
        self.assertEqual(len(batch3), 2)
        self.assertIsInstance(batch3, list)
        self.assertTrue(it.is_new_epoch)
        self.assertEqual(sorted(batch1 + batch2 + batch3), dataset)
        self.assertAlmostEqual(it.epoch_detail, 6 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 4 / 6)


</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/iterators_tests/test_serial_iterator.py" startline="255" endline="298" pcid="9003">
    def test_iterator_serialize_backward_compat(self):
        dataset = [1, 2, 3, 4, 5, 6]
        it = iterators.SerialIterator(dataset, 2, shuffle=self.shuffle,
                                      order_sampler=self.order_sampler)

        self.assertEqual(it.epoch, 0)
        self.assertAlmostEqual(it.epoch_detail, 0 / 6)
        self.assertIsNone(it.previous_epoch_detail)
        batch1 = it.next()
        self.assertEqual(len(batch1), 2)
        self.assertIsInstance(batch1, list)
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 2 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 0 / 6)
        batch2 = it.next()
        self.assertEqual(len(batch2), 2)
        self.assertIsInstance(batch2, list)
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 4 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 2 / 6)

        target = dict()
        it.serialize(serializers.DictionarySerializer(target))
        # older version uses '_order'
        target['_order'] = target['order']
        del target['order']
        # older version does not have previous_epoch_detail
        del target['previous_epoch_detail']

        it = iterators.SerialIterator(dataset, 2)
        it.serialize(serializers.NpzDeserializer(target))
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 4 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 2 / 6)

        batch3 = it.next()
        self.assertEqual(len(batch3), 2)
        self.assertIsInstance(batch3, list)
        self.assertTrue(it.is_new_epoch)
        self.assertEqual(sorted(batch1 + batch2 + batch3), dataset)
        self.assertAlmostEqual(it.epoch_detail, 6 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 4 / 6)


</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/iterators_tests/test_multiprocess_iterator.py" startline="430" endline="466" pcid="8939">
    def test_iterator_serialize(self):
        dataset = [1, 2, 3, 4, 5, 6]
        it = iterators.MultiprocessIterator(dataset, 2, **self.options)

        self.assertEqual(it.epoch, 0)
        self.assertAlmostEqual(it.epoch_detail, 0 / 6)
        self.assertIsNone(it.previous_epoch_detail)
        batch1 = it.next()
        self.assertEqual(len(batch1), 2)
        self.assertIsInstance(batch1, list)
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 2 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 0 / 6)
        batch2 = it.next()
        self.assertEqual(len(batch2), 2)
        self.assertIsInstance(batch2, list)
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 4 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 2 / 6)

        target = dict()
        it.serialize(serializers.DictionarySerializer(target))

        it = iterators.MultiprocessIterator(dataset, 2, **self.options)
        it.serialize(serializers.NpzDeserializer(target))
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 4 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 2 / 6)

        batch3 = it.next()
        self.assertEqual(len(batch3), 2)
        self.assertIsInstance(batch3, list)
        self.assertTrue(it.is_new_epoch)
        self.assertEqual(sorted(batch1 + batch2 + batch3), dataset)
        self.assertAlmostEqual(it.epoch_detail, 6 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 4 / 6)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/iterators_tests/test_iterator_compatibility.py" startline="22" endline="61" pcid="9014">
    def test_iterator_compatibilty(self):
        dataset = [1, 2, 3, 4, 5, 6]

        iters = (
            lambda: iterators.SerialIterator(dataset, 2),
            lambda: iterators.MultiprocessIterator(dataset, 2, **self.options),
        )

        for it_before, it_after in itertools.permutations(iters, 2):
            it = it_before()

            self.assertEqual(it.epoch, 0)
            self.assertAlmostEqual(it.epoch_detail, 0 / 6)
            batch1 = it.next()
            self.assertEqual(len(batch1), 2)
            self.assertIsInstance(batch1, list)
            self.assertFalse(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, 2 / 6)
            batch2 = it.next()
            self.assertEqual(len(batch2), 2)
            self.assertIsInstance(batch2, list)
            self.assertFalse(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, 4 / 6)

            target = dict()
            it.serialize(serializers.DictionarySerializer(target))

            it = it_after()
            it.serialize(serializers.NpzDeserializer(target))
            self.assertFalse(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, 4 / 6)

            batch3 = it.next()
            self.assertEqual(len(batch3), 2)
            self.assertIsInstance(batch3, list)
            self.assertTrue(it.is_new_epoch)
            self.assertEqual(sorted(batch1 + batch2 + batch3), dataset)
            self.assertAlmostEqual(it.epoch_detail, 6 / 6)


</source>
</class>

<class classid="205" nclones="2" nlines="18" similarity="83">
<source file="systems/chainer-7.2.0/tests/chainer_tests/iterators_tests/test_serial_iterator.py" startline="67" endline="86" pcid="8993">
    def test_iterator_not_repeat(self):
        dataset = [1, 2, 3, 4, 5, 6]
        it = iterators.SerialIterator(dataset, 2, repeat=False, shuffle=False)

        self.assertAlmostEqual(it.epoch_detail, 0 / 6)
        self.assertIsNone(it.previous_epoch_detail)
        self.assertEqual(it.next(), [1, 2])
        self.assertAlmostEqual(it.epoch_detail, 2 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 0 / 6)
        self.assertEqual(it.next(), [3, 4])
        self.assertAlmostEqual(it.epoch_detail, 4 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 2 / 6)
        self.assertEqual(it.next(), [5, 6])
        self.assertTrue(it.is_new_epoch)
        self.assertEqual(it.epoch, 1)
        self.assertAlmostEqual(it.epoch_detail, 6 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 4 / 6)
        for i in range(2):
            self.assertRaises(StopIteration, it.next)

</source>
<source file="systems/chainer-7.2.0/tests/chainer_tests/iterators_tests/test_serial_iterator.py" startline="87" endline="106" pcid="8994">
    def test_iterator_not_repeat_not_even(self):
        dataset = [1, 2, 3, 4, 5]
        it = iterators.SerialIterator(dataset, 2, repeat=False, shuffle=False)

        self.assertAlmostEqual(it.epoch_detail, 0 / 5)
        self.assertIsNone(it.previous_epoch_detail)
        self.assertEqual(it.next(), [1, 2])
        self.assertAlmostEqual(it.epoch_detail, 2 / 5)
        self.assertAlmostEqual(it.previous_epoch_detail, 0 / 5)
        self.assertEqual(it.next(), [3, 4])
        self.assertAlmostEqual(it.epoch_detail, 4 / 5)
        self.assertAlmostEqual(it.previous_epoch_detail, 2 / 5)
        self.assertEqual(it.next(), [5])
        self.assertTrue(it.is_new_epoch)
        self.assertEqual(it.epoch, 1)
        self.assertAlmostEqual(it.epoch_detail, 5 / 5)
        self.assertAlmostEqual(it.previous_epoch_detail, 4 / 5)
        self.assertRaises(StopIteration, it.next)


</source>
</class>

<class classid="206" nclones="2" nlines="16" similarity="88">
<source file="systems/chainer-7.2.0/tests/chainermn_tests/optimizer_tests/test_double_buffering_optimizer.py" startline="25" endline="42" pcid="9016">
    def setup(self, batched_copy):
        if nccl.get_build_version() < 2000:
            pytest.skip('This test requires NCCL version >= 2.0')
        self.comm = chainermn.create_communicator('pure_nccl',
                                                  batched_copy=batched_copy)
        device = self.comm.intra_rank
        chainer.cuda.get_device_from_id(device).use()
        self.target = ExampleModel()
        self.target.to_device(cupy.cuda.Device())
        self.target.a.W.data[:] = self.comm.rank
        self.target.b.W.data[:] = self.comm.rank + 1
        self.target.c.W.data[:] = self.comm.rank + 2
        self.target.a.W.grad[:] = 0
        self.target.b.W.grad[:] = 0
        self.target.c.W.grad[:] = 0
        self.actual_optimizer = chainer.GradientMethod()
        self.actual_optimizer.create_update_rule = mock.MagicMock

</source>
<source file="systems/chainer-7.2.0/tests/chainermn_tests/optimizer_tests/test_double_buffering_optimizer.py" startline="112" endline="127" pcid="9021">
    def setup(self, batched_copy):
        if nccl.get_build_version() < 2000:
            pytest.skip('This test requires NCCL version >= 2.0')
        self.comm = chainermn.create_communicator('pure_nccl',
                                                  batched_copy=batched_copy)
        device = self.comm.intra_rank
        chainer.cuda.get_device_from_id(device).use()
        self.target = DynamicExampleModel()
        self.target.to_device(cupy.cuda.Device())
        self.target.a.W.data[:] = self.comm.rank
        self.target.b.W.data[:] = self.comm.rank + 1
        self.target.a.W.grad[:] = 0
        self.target.b.W.grad[:] = 0
        self.actual_optimizer = chainer.GradientMethod()
        self.actual_optimizer.create_update_rule = mock.MagicMock

</source>
</class>

<class classid="207" nclones="2" nlines="14" similarity="86">
<source file="systems/chainer-7.2.0/tests/chainermn_tests/optimizer_tests/test_multi_node_optimizer.py" startline="34" endline="50" pcid="9027">
    def setup_gpu(self, use_chx=False):
        self.comm = chainermn.create_communicator('flat')
        self.target = ExampleModel()
        self.device = chainermn.testing.get_device(self.comm.intra_rank,
                                                   use_chx)
        chainer.cuda.get_device_from_id(self.comm.intra_rank).use()
        self.target.to_device(self.device)

        self.target.a.W.data[:] = self.comm.rank
        self.target.b.W.data[:] = self.comm.rank + 1
        self.target.c.W.data[:] = self.comm.rank + 2
        self.target.a.W.grad[:] = 0
        self.target.b.W.grad[:] = 0
        self.target.c.W.grad[:] = 0
        self.actual_optimizer = chainer.GradientMethod()
        self.actual_optimizer.create_update_rule = mock.MagicMock

</source>
<source file="systems/chainer-7.2.0/tests/chainermn_tests/optimizer_tests/test_multi_node_optimizer.py" startline="134" endline="148" pcid="9032">
    def setup_gpu(self, use_chx=False):
        self.comm = chainermn.create_communicator('flat')
        self.target = DynamicExampleModel()
        self.device = chainermn.testing.get_device(self.comm.intra_rank,
                                                   use_chx)
        chainer.cuda.get_device_from_id(self.comm.intra_rank).use()
        self.target.to_device(self.device)

        self.target.a.W.data[:] = self.comm.rank
        self.target.b.W.data[:] = self.comm.rank + 1
        self.target.a.W.grad[:] = 0
        self.target.b.W.grad[:] = 0
        self.actual_optimizer = chainer.GradientMethod()
        self.actual_optimizer.create_update_rule = mock.MagicMock

</source>
</class>

<class classid="208" nclones="4" nlines="32" similarity="70">
<source file="systems/chainer-7.2.0/tests/chainermn_tests/optimizer_tests/test_multi_node_optimizer.py" startline="51" endline="79" pcid="9028">
    def test_update_with_cpu(self):
        self.setup_cpu()
        self.optimizer = chainermn.create_multi_node_optimizer(
            self.actual_optimizer, self.comm)
        opt = self.optimizer.setup(self.target)
        assert opt is self.optimizer
        self.optimizer.update()
        assert self.actual_optimizer.t == 0
        self.optimizer.target.a.W.grad[:] = self.comm.rank
        self.optimizer.target.b.W.grad[:] = self.comm.rank + 1
        self.optimizer.target.c.W.grad[:] = self.comm.rank + 2

        self.optimizer.update()
        assert self.actual_optimizer.t == 1
        self.optimizer.target.a.W.update_rule.update.assert_called_once_with(
            self.optimizer.target.a.W)
        self.optimizer.target.b.W.update_rule.update.assert_called_once_with(
            self.optimizer.target.b.W)
        self.optimizer.target.c.W.update_rule.update.assert_called_once_with(
            self.optimizer.target.c.W)

        base = (self.comm.size - 1.0) / 2
        chainer.testing.assert_allclose(self.optimizer.target.a.W.grad,
                                        (base + 0) * np.ones((3, 2)))
        chainer.testing.assert_allclose(self.optimizer.target.b.W.grad,
                                        (base + 1) * np.ones((4, 3)))
        chainer.testing.assert_allclose(self.optimizer.target.c.W.grad,
                                        (base + 2) * np.ones((5, 4)))

</source>
<source file="systems/chainer-7.2.0/tests/chainermn_tests/optimizer_tests/test_multi_node_optimizer.py" startline="149" endline="190" pcid="9033">
    def test_update_with_cpu(self):
        self.setup_cpu()
        self.optimizer = chainermn.create_multi_node_optimizer(
            self.actual_optimizer, self.comm)
        opt = self.optimizer.setup(self.target)
        assert opt is self.optimizer
        self.optimizer.update()
        assert self.actual_optimizer.t == 0

        with self.target.init_scope():
            self.target.c = chainer.links.Linear(4, 4)
        if self.comm.rank == 0:
            self.target.c.W.data[:] = self.comm.rank + 2
        self.optimizer.setup(self.target)
        self.optimizer.update()
        assert self.actual_optimizer.t == 0

        send_buf = chainer.cuda.to_cpu(self.optimizer.target.c.W.data)
        recv_buf = self.comm.mpi_comm.allgather(send_buf)
        for i in range(1, self.comm.size):
            chainer.testing.assert_allclose(recv_buf[0], recv_buf[i])

        self.optimizer.target.a.W.grad[:] = self.comm.rank
        self.optimizer.target.b.W.grad[:] = self.comm.rank + 1
        self.optimizer.target.c.W.grad[:] = self.comm.rank + 2
        self.optimizer.update()
        assert self.actual_optimizer.t == 1
        self.optimizer.target.a.W.update_rule.update.assert_called_once_with(
            self.optimizer.target.a.W)
        self.optimizer.target.b.W.update_rule.update.assert_called_once_with(
            self.optimizer.target.b.W)
        self.optimizer.target.c.W.update_rule.update.assert_called_once_with(
            self.optimizer.target.c.W)

        base = (self.comm.size - 1.0) / 2
        chainer.testing.assert_allclose(self.optimizer.target.a.W.grad,
                                        (base + 0) * np.ones((3, 2)))
        chainer.testing.assert_allclose(self.optimizer.target.b.W.grad,
                                        (base + 1) * np.ones((4, 3)))
        chainer.testing.assert_allclose(self.optimizer.target.c.W.grad,
                                        (base + 2) * np.ones((4, 4)))

</source>
<source file="systems/chainer-7.2.0/tests/chainermn_tests/optimizer_tests/test_multi_node_optimizer.py" startline="82" endline="112" pcid="9029">
    def test_update_with_gpu(self, use_chx):
        self.setup_gpu(use_chx)
        self.optimizer = chainermn.create_multi_node_optimizer(
            self.actual_optimizer, self.comm)
        opt = self.optimizer.setup(self.target)
        assert opt is self.optimizer
        self.optimizer.update()
        assert self.actual_optimizer.t == 0

        self.optimizer.target.a.W.grad[:] = self.comm.rank
        self.optimizer.target.b.W.grad[:] = self.comm.rank + 1
        self.optimizer.target.c.W.grad[:] = self.comm.rank + 2

        self.optimizer.update()
        assert self.actual_optimizer.t == 1
        self.optimizer.target.a.W.update_rule.update.assert_called_once_with(
            self.optimizer.target.a.W)
        self.optimizer.target.b.W.update_rule.update.assert_called_once_with(
            self.optimizer.target.b.W)
        self.optimizer.target.c.W.update_rule.update.assert_called_once_with(
            self.optimizer.target.c.W)

        base = (self.comm.size - 1.0) / 2
        chainer.testing.assert_allclose(self.optimizer.target.a.W.grad,
                                        (base + 0) * np.ones((3, 2)))
        chainer.testing.assert_allclose(self.optimizer.target.b.W.grad,
                                        (base + 1) * np.ones((4, 3)))
        chainer.testing.assert_allclose(self.optimizer.target.c.W.grad,
                                        (base + 2) * np.ones((5, 4)))


</source>
<source file="systems/chainer-7.2.0/tests/chainermn_tests/optimizer_tests/test_multi_node_optimizer.py" startline="193" endline="235" pcid="9034">
    def test_update_with_gpu(self, use_chx):
        self.setup_gpu(use_chx)
        self.optimizer = chainermn.create_multi_node_optimizer(
            self.actual_optimizer, self.comm)
        opt = self.optimizer.setup(self.target)
        assert opt is self.optimizer
        self.optimizer.update()
        assert self.actual_optimizer.t == 0

        with self.target.init_scope():
            c = chainer.links.Linear(4, 4)
            c.to_device(self.device)
            self.target.c = c
        if self.comm.rank == 0:
            self.target.c.W.data[:] = self.comm.rank + 2
        self.optimizer.setup(self.target)
        self.optimizer.update()
        assert self.actual_optimizer.t == 0

        send_buf = chainer.cuda.to_cpu(self.optimizer.target.c.W.data)
        recv_buf = self.comm.mpi_comm.allgather(send_buf)
        for i in range(1, self.comm.size):
            chainer.testing.assert_allclose(recv_buf[0], recv_buf[i])

        self.optimizer.target.a.W.grad[:] = self.comm.rank
        self.optimizer.target.b.W.grad[:] = self.comm.rank + 1
        self.optimizer.target.c.W.grad[:] = self.comm.rank + 2
        self.optimizer.update()
        assert self.actual_optimizer.t == 1
        self.optimizer.target.a.W.update_rule.update.assert_called_once_with(
            self.optimizer.target.a.W)
        self.optimizer.target.b.W.update_rule.update.assert_called_once_with(
            self.optimizer.target.b.W)
        self.optimizer.target.c.W.update_rule.update.assert_called_once_with(
            self.optimizer.target.c.W)

        base = (self.comm.size - 1.0) / 2
        chainer.testing.assert_allclose(self.optimizer.target.a.W.grad,
                                        (base + 0) * np.ones((3, 2)))
        chainer.testing.assert_allclose(self.optimizer.target.b.W.grad,
                                        (base + 1) * np.ones((4, 3)))
        chainer.testing.assert_allclose(self.optimizer.target.c.W.grad,
                                        (base + 2) * np.ones((4, 4)))
</source>
</class>

<class classid="209" nclones="2" nlines="21" similarity="77">
<source file="systems/chainer-7.2.0/tests/chainermn_tests/links_tests/test_multi_node_chain_list.py" startline="280" endline="304" pcid="9070">
def check_crossing_model(gpu, param):
    communicator, rank_next, rank_prev = create_communicator(gpu)

    n, d = 100, 10
    X = np.random.randn(n, d).astype(param.dtype)
    Y = (np.random.rand(n) * 2).astype(np.int32)

    with chainer.using_config('dtype', param.dtype):
        if communicator.rank == 0:
            model = L.Classifier(Cross0(
                d, communicator, rank_next, rank_prev))
        else:
            model = L.Classifier(Cross1(
                d, communicator, rank_next, rank_prev))

        if gpu:
            model.to_device(cupy.cuda.Device())
            X = chainer.backends.cuda.to_gpu(X)
            Y = chainer.backends.cuda.to_gpu(Y)

        for i in range(n):
            err = model(X[i:i + 1], Y[i:i + 1])
            err.backward()


</source>
<source file="systems/chainer-7.2.0/tests/chainermn_tests/links_tests/test_multi_node_chain_list.py" startline="371" endline="398" pcid="9077">
def check_twisting_model(gpu, param):
    communicator, rank_next, rank_prev = create_communicator(gpu)

    n, d = 100, 10
    X = np.random.randn(n, d).astype(param.dtype)
    Y = (np.random.rand(n) * 2).astype(np.int32)

    with chainer.using_config('dtype', param.dtype):
        if communicator.rank == 0:
            model = L.Classifier(
                TwistFirst(d, communicator, rank_next))
        elif communicator.rank == communicator.size - 1:
            model = L.Classifier(
                TwistLast(d, communicator, rank_prev))
        else:
            model = L.Classifier(Twist(
                d, communicator, rank_prev, rank_next))

        if gpu:
            model.to_device(cupy.cuda.Device())
            X = chainer.backends.cuda.to_gpu(X)
            Y = chainer.backends.cuda.to_gpu(Y)

        for i in range(n):
            err = model(X[i:i + 1], Y[i:i + 1])
            err.backward()


</source>
</class>

<class classid="210" nclones="2" nlines="20" similarity="85">
<source file="systems/chainer-7.2.0/tests/chainermn_tests/links_tests/test_n_step_rnn.py" startline="65" endline="93" pcid="9086">
def check_homogeneous_rnn(gpu, dtype):
    communicator, rank_prev, rank_next = setup_communicator(gpu=gpu)

    n, n_vocab, l = 100, 8, 10
    # Number of model parameters are same among processes.
    n_hid = 2
    with chainer.using_config('dtype', dtype):
        X = [np.random.randint(
            0, n_vocab, size=np.random.randint(l // 2, l + 1),
            dtype=np.int32)
            for _ in range(n)]
        Y = (np.random.rand(n) * 2).astype(dtype)
        model = Model(
            n_vocab, n_hid, communicator, rank_next,
            rank_prev)

        if gpu:
            model.to_device(cupy.cuda.Device())
            X = [chainer.backends.cuda.to_gpu(x) for x in X]
            Y = chainer.backends.cuda.to_gpu(Y)

        for i in range(n):
            err = model(X[i:i + 1], Y[i:i + 1])
            err.backward()

        # Check if backprop finishes without deadlock.
        assert True


</source>
<source file="systems/chainer-7.2.0/tests/chainermn_tests/links_tests/test_n_step_rnn.py" startline="105" endline="134" pcid="9089">
def check_heterogeneous_rnn(gpu, dtype):
    communicator, rank_prev, rank_next = setup_communicator(gpu)

    with chainer.using_config('dtype', dtype):
        n, n_vocab, l = 100, 8, 10
        # Number of model parameters are different among processes.
        n_hid = (communicator.rank + 1) * 10

        X = [np.random.randint(
            0, n_vocab, size=np.random.randint(l // 2, l + 1),
            dtype=np.int32)
            for _ in range(n)]
        Y = (np.random.rand(n) * 2).astype(dtype)
        model = Model(
            n_vocab, n_hid, communicator, rank_next,
            rank_prev)

        if gpu:
            model.to_device(cupy.cuda.Device())
            X = [chainer.backends.cuda.to_gpu(x) for x in X]
            Y = chainer.backends.cuda.to_gpu(Y)

        for i in range(n):
            err = model(X[i:i + 1], Y[i:i + 1])
            err.backward()

        # Check if backprop finishes without deadlock.
        assert True


</source>
</class>

<class classid="211" nclones="2" nlines="15" similarity="86">
<source file="systems/chainer-7.2.0/tests/chainermn_tests/links_tests/test_create_mnbn_model.py" startline="45" endline="61" pcid="9097">
    def check_create_mnbn_model_chain(self, use_gpu, use_chx):
        model = BnChain(3)
        mnbn_model = chainermn.links.create_mnbn_model(model,
                                                       self.communicator)
        self.assertTrue(isinstance(mnbn_model.conv,
                                   chainer.links.Convolution2D))
        self.assertTrue(
            isinstance(mnbn_model.bn,
                       chainermn.links.MultiNodeBatchNormalization))
        device = get_device(self.communicator.intra_rank if use_gpu else None,
                            use_chx)
        mnbn_model.to_device(device)

        with chainer.using_device(mnbn_model.device):
            x = mnbn_model.xp.zeros((1, 1, 1, 1))
            mnbn_model(x)

</source>
<source file="systems/chainer-7.2.0/tests/chainermn_tests/links_tests/test_create_mnbn_model.py" startline="62" endline="78" pcid="9098">
    def check_create_mnbn_model_chain_list(self, use_gpu, use_chx):
        model = BnChainList(3)
        mnbn_model = chainermn.links.create_mnbn_model(model,
                                                       self.communicator)
        self.assertTrue(isinstance(mnbn_model[0],
                                   chainer.links.Convolution2D))
        self.assertTrue(
            isinstance(mnbn_model[1],
                       chainermn.links.MultiNodeBatchNormalization))
        device = get_device(self.communicator.intra_rank if use_gpu else None,
                            use_chx)
        mnbn_model.to_device(device)

        with chainer.using_device(mnbn_model.device):
            x = mnbn_model.xp.zeros((1, 1, 1, 1))
            mnbn_model(x)

</source>
</class>

<class classid="212" nclones="2" nlines="11" similarity="81">
<source file="systems/chainer-7.2.0/tests/chainermn_tests/functions_tests/test_point_to_point_communication.py" startline="35" endline="48" pcid="9194">
def create_communicator(gpu, param):
    if gpu:
        communicator = chainermn.create_communicator('flat')
        device = communicator.intra_rank
        chainer.cuda.get_device_from_id(device).use()
    else:
        communicator = chainermn.create_communicator('naive')

    if communicator.size < 2:
        pytest.skip('This test is for multinode')

    return communicator


</source>
<source file="systems/chainer-7.2.0/tests/chainermn_tests/functions_tests/test_collective_communication.py" startline="27" endline="42" pcid="9209">
def get_communicator(gpu):
    numpy.random.seed(42)

    if gpu:
        communicator = chainermn.create_communicator('flat')
        device = communicator.intra_rank
        chainer.cuda.get_device_from_id(device).use()
    else:
        communicator = chainermn.create_communicator('naive')

    if communicator.size < 2:
        pytest.skip('This test is for multinode')

    return communicator


</source>
</class>

<class classid="213" nclones="4" nlines="17" similarity="76">
<source file="systems/chainer-7.2.0/tests/chainermn_tests/iterators_tests/test_synchronized_iterator.py" startline="20" endline="41" pcid="9237">
    def test_sync(self):
        # test the case when datasize is a multiple of batchsize
        iterator = chainermn.iterators.create_synchronized_iterator(
            chainer.iterators.SerialIterator(
                self.dataset, batch_size=4, shuffle=True),
            self.communicator)

        for e in range(3):
            self.assertEqual(e, iterator.epoch)

            while True:
                batch = np.array(iterator.next(), dtype=np.float32)
                if self.communicator.rank == 0:
                    for rank_from in range(1, self.communicator.size):
                        _batch = self.communicator.recv(rank_from, tag=0)
                        chainer.testing.assert_allclose(batch, _batch)
                else:
                    self.communicator.send(batch, dest=0, tag=0)

                if iterator.is_new_epoch:
                    break

</source>
<source file="systems/chainer-7.2.0/tests/chainermn_tests/iterators_tests/test_synchronized_iterator.py" startline="42" endline="63" pcid="9238">
    def test_sync_frag(self):
        # test the case when datasize is not a multiple of batchsize
        iterator = chainermn.iterators.create_synchronized_iterator(
            chainer.iterators.SerialIterator(
                self.dataset, batch_size=7, shuffle=True),
            self.communicator)

        for e in range(3):
            self.assertEqual(e, iterator.epoch)

            while True:
                batch = np.array(iterator.next(), dtype=np.float32)
                if self.communicator.rank == 0:
                    for rank_from in range(1, self.communicator.size):
                        _batch = self.communicator.recv(rank_from, tag=0)
                        chainer.testing.assert_allclose(batch, _batch)
                else:
                    self.communicator.send(batch, dest=0, tag=0)

                if iterator.is_new_epoch:
                    break

</source>
<source file="systems/chainer-7.2.0/tests/chainermn_tests/iterators_tests/test_synchronized_iterator.py" startline="64" endline="82" pcid="9239">
    def test_sync_no_repeat(self):
        iterator = chainermn.iterators.create_synchronized_iterator(
            chainer.iterators.SerialIterator(
                self.dataset, batch_size=4, shuffle=True, repeat=False),
            self.communicator)

        for e in range(3):
            try:
                while True:
                    batch = np.array(iterator.next(), dtype=np.float32)
                    if self.communicator.rank == 0:
                        for rank_from in range(1, self.communicator.size):
                            _batch = self.communicator.recv(rank_from, tag=0)
                            chainer.testing.assert_allclose(batch, _batch)
                    else:
                        self.communicator.send(batch, dest=0, tag=0)
            except StopIteration:
                iterator.reset()

</source>
<source file="systems/chainer-7.2.0/tests/chainermn_tests/iterators_tests/test_synchronized_iterator.py" startline="83" endline="100" pcid="9240">
    def test_sync_no_repeat_frag(self):
        iterator = chainermn.iterators.create_synchronized_iterator(
            chainer.iterators.SerialIterator(
                self.dataset, batch_size=7, shuffle=True, repeat=False),
            self.communicator)

        for e in range(3):
            try:
                while True:
                    batch = np.array(iterator.next(), dtype=np.float32)
                    if self.communicator.rank == 0:
                        for rank_from in range(1, self.communicator.size):
                            _batch = self.communicator.recv(rank_from, tag=0)
                            chainer.testing.assert_allclose(batch, _batch)
                    else:
                        self.communicator.send(batch, dest=0, tag=0)
            except StopIteration:
                iterator.reset()
</source>
</class>

<class classid="214" nclones="3" nlines="17" similarity="73">
<source file="systems/chainer-7.2.0/tests/chainermn_tests/iterators_tests/test_multi_node_iterator.py" startline="72" endline="90" pcid="9248">

    def test_mn_iterator(self):
        # Datasize is a multiple of batchsize.
        bs = 4
        iterator = chainermn.iterators.create_multi_node_iterator(
            self.iterator_class(
                self.dataset, batch_size=bs, shuffle=True),
            self.communicator)

        for e in range(3):
            for i in range(100):
                batch = iterator.next()
                if self.communicator.rank == 0:
                    for rank_from in range(1, self.communicator.size):
                        _batch = self.communicator.mpi_comm.recv(
                            source=rank_from)
                        self.assertEqual(batch, _batch)
                else:
                    self.communicator.mpi_comm.ssend(batch, dest=0)
</source>
<source file="systems/chainer-7.2.0/tests/chainermn_tests/iterators_tests/test_multi_node_iterator.py" startline="132" endline="153" pcid="9251">

    def test_mn_iterator_no_repeat(self):
        # Do not repeat iterator to test if we can catch StopIteration.
        bs = 4
        iterator = chainermn.iterators.create_multi_node_iterator(
            self.iterator_class(
                self.dataset, batch_size=bs, shuffle=True, repeat=False),
            self.communicator)

        for e in range(3):
            try:
                while True:
                    batch = iterator.next()
                    if self.communicator.rank == 0:
                        for rank_from in range(1, self.communicator.size):
                            _batch = self.communicator.mpi_comm.recv(
                                source=rank_from)
                            self.assertEqual(batch, _batch)
                    else:
                        self.communicator.mpi_comm.ssend(batch, dest=0)
            except StopIteration:
                continue
</source>
<source file="systems/chainer-7.2.0/tests/chainermn_tests/iterators_tests/test_multi_node_iterator.py" startline="91" endline="109" pcid="9249">

    def test_mn_iterator_frag(self):
        # Batasize is not a multiple of batchsize.
        bs = 7
        iterator = chainermn.iterators.create_multi_node_iterator(
            self.iterator_class(
                self.dataset, batch_size=bs, shuffle=True),
            self.communicator)

        for e in range(3):
            for i in range(100):
                batch = iterator.next()
                if self.communicator.rank == 0:
                    for rank_from in range(1, self.communicator.size):
                        _batch = self.communicator.mpi_comm.recv(
                            source=rank_from)
                        self.assertEqual(batch, _batch)
                else:
                    self.communicator.mpi_comm.ssend(batch, dest=0)
</source>
</class>

<class classid="215" nclones="2" nlines="38" similarity="100">
<source file="systems/chainer-7.2.0/tests/chainermn_tests/iterators_tests/test_iterator_compatibility.py" startline="72" endline="117" pcid="9263">

    def test_multi_node_iterator_compatibility(self):
        iters = (
            lambda: chainermn.iterators.create_multi_node_iterator(
                self.iterator_class(
                    self.dataset, batch_size=self.bs),
                self.communicator),
            lambda: self.iterator_class(
                self.dataset, batch_size=self.bs),
        )

        bs_n_ratio = 1. * self.bs / self.N

        it_before, it_after = iters

        it = it_before()

        self.assertEqual(it.epoch, 0)
        self.assertAlmostEqual(it.epoch_detail, 0 * bs_n_ratio)
        batch1 = it.next()
        self.assertEqual(len(batch1), self.bs)
        self.assertIsInstance(batch1, list)
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 1 * bs_n_ratio)
        batch2 = it.next()
        self.assertEqual(len(batch2), self.bs)
        self.assertIsInstance(batch2, list)
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 2 * bs_n_ratio)

        target = dict()
        it.serialize(DummySerializer(target))

        it = it_after()
        it.serialize(DummyDeserializer(target))
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 2 * bs_n_ratio)

        batch3 = it.next()
        self.assertEqual(len(batch3), self.bs)
        self.assertIsInstance(batch3, list)
        self.assertTrue(it.is_new_epoch)
        self.assertEqual(
            sorted(batch1 + batch2 + batch3),
            self.dataset.tolist())
        self.assertAlmostEqual(it.epoch_detail, 3 * bs_n_ratio)
</source>
<source file="systems/chainer-7.2.0/tests/chainermn_tests/iterators_tests/test_iterator_compatibility.py" startline="118" endline="168" pcid="9264">

    def test_synchronized_iterator_compatibility(self):
        """
        Do not use `chainer.testing.parameterize` to share the code with
        `test_multi_node_iterator_compatibility` because pytest cannot
        guarantee the execution order of tests produced by `parameterize`,
        which causes unexpected behaviors with MPI programs.
        """
        iters = (
            lambda: chainermn.iterators.create_synchronized_iterator(
                self.iterator_class(
                    self.dataset, batch_size=self.bs),
                self.communicator),
            lambda: self.iterator_class(
                self.dataset, batch_size=self.bs),
        )

        bs_n_ratio = 1. * self.bs / self.N

        it_before, it_after = iters

        it = it_before()

        self.assertEqual(it.epoch, 0)
        self.assertAlmostEqual(it.epoch_detail, 0 * bs_n_ratio)
        batch1 = it.next()
        self.assertEqual(len(batch1), self.bs)
        self.assertIsInstance(batch1, list)
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 1 * bs_n_ratio)
        batch2 = it.next()
        self.assertEqual(len(batch2), self.bs)
        self.assertIsInstance(batch2, list)
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 2 * bs_n_ratio)

        target = dict()
        it.serialize(DummySerializer(target))

        it = it_after()
        it.serialize(DummyDeserializer(target))
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 2 * bs_n_ratio)

        batch3 = it.next()
        self.assertEqual(len(batch3), self.bs)
        self.assertIsInstance(batch3, list)
        self.assertTrue(it.is_new_epoch)
        self.assertEqual(
            sorted(batch1 + batch2 + batch3),
            self.dataset.tolist())
</source>
</class>

<class classid="216" nclones="2" nlines="11" similarity="72">
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/testing_tests/test_array.py" startline="247" endline="260" pcid="9332">
def test_assert_array_equal_ex_fail_dtype():
    shape = (3, 2)
    dtype1 = numpy.float32
    dtype2 = numpy.int64
    a = numpy.arange(2, 2 + numpy.prod(shape)).astype(dtype1).reshape(shape)
    b = a.astype(dtype2)
    with pytest.raises(AssertionError):
        chainerx.testing.assert_array_equal_ex(a, b)
    with pytest.raises(AssertionError):
        # strides_check does not affect dtype_check
        chainerx.testing.assert_array_equal_ex(a, b, strides_check=False)
    chainerx.testing.assert_array_equal_ex(a, b, dtype_check=False)


</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/testing_tests/test_array.py" startline="261" endline="271" pcid="9333">
def test_assert_array_equal_ex_fail_strides():
    shape = (3, 2)
    dtype = numpy.float32
    a = numpy.arange(2, 2 + numpy.prod(shape)).astype(dtype).reshape(shape)
    b = numpy.empty(a.T.shape, dtype).T
    b[:] = a
    with pytest.raises(AssertionError):
        chainerx.testing.assert_array_equal_ex(a, b)
    chainerx.testing.assert_array_equal_ex(a, b, strides_check=False)
    # dtype_check=False implies strides_check=False
    chainerx.testing.assert_array_equal_ex(a, b, dtype_check=False)
</source>
</class>

<class classid="217" nclones="5" nlines="14" similarity="73">
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/test_cupy_interop.py" startline="50" endline="68" pcid="9355">
def test_cupy_to_chainerx_delete_cupy_first():
    dtype = numpy.float32
    a_cupy = cupy.arange(6, dtype=dtype).reshape((2, 3))
    a_chx = _fromrawpointer(
        a_cupy.data.mem.ptr,
        a_cupy.shape,
        a_cupy.dtype,
        a_cupy.strides,
        'cuda:0',
        0,
        a_cupy)

    del a_cupy

    a_chx += 1
    chainerx.testing.assert_array_equal_ex(
        a_chx, numpy.array([[1, 2, 3], [4, 5, 6]], dtype))


</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/test_cupy_interop.py" startline="70" endline="88" pcid="9356">
def test_cupy_to_chainerx_delete_chainerx_first():
    dtype = numpy.float32
    a_cupy = cupy.arange(6, dtype=dtype).reshape((2, 3))
    a_chx = _fromrawpointer(
        a_cupy.data.mem.ptr,
        a_cupy.shape,
        a_cupy.dtype,
        a_cupy.strides,
        'cuda:0',
        0,
        a_cupy)

    del a_chx

    a_cupy += 1
    chainerx.testing.assert_array_equal_ex(
        a_cupy.get(), numpy.array([[1, 2, 3], [4, 5, 6]], dtype))


</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/test_cupy_interop.py" startline="166" endline="182" pcid="9360">
def test_cupy_to_chainerx_nondefault_device():
    dtype = numpy.float32
    with cupy.cuda.Device(1):
        a_cupy = cupy.arange(6, dtype=dtype).reshape((2, 3))
    a_chx = _fromrawpointer(
        a_cupy.data.mem.ptr,
        a_cupy.shape,
        a_cupy.dtype,
        a_cupy.strides,
        'cuda:1',
        0,
        a_cupy)

    assert a_chx.device.name == 'cuda:1'
    chainerx.testing.assert_array_equal_ex(a_chx, a_cupy.get())


</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/test_cupy_interop.py" startline="184" endline="198" pcid="9361">
def test_cupy_to_chainerx_invalid_device():
    dtype = numpy.float32
    with cupy.cuda.Device(1):
        a_cupy = cupy.arange(6, dtype=dtype).reshape((2, 3))
    with pytest.raises(chainerx.ChainerxError):
        _fromrawpointer(
            a_cupy.data.mem.ptr,
            a_cupy.shape,
            a_cupy.dtype,
            a_cupy.strides,
            'cuda:0',
            0,
            a_cupy)


</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/test_cupy_interop.py" startline="90" endline="103" pcid="9357">
def test_cupy_to_chainerx_from_invalid_pointer():
    dtype = numpy.float32
    a_numpy = numpy.arange(6, dtype=dtype).reshape((2, 3))
    with pytest.raises(chainerx.ChainerxError):
        _fromrawpointer(
            a_numpy.ctypes.data,
            a_numpy.shape,
            a_numpy.dtype,
            a_numpy.strides,
            'cuda:0',
            0,
            a_numpy)


</source>
</class>

<class classid="218" nclones="2" nlines="21" similarity="80">
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/test_cupy_interop.py" startline="105" endline="133" pcid="9358">
def test_cupy_to_chainerx_noncontiguous_with_offset():
    dtype = numpy.float32
    a_cupy = cupy.arange(12, dtype=dtype).reshape((2, 6))[::-1, ::2]
    offset = a_cupy.data.ptr - a_cupy.data.mem.ptr

    # test preconditions
    assert offset > 0
    assert not a_cupy.flags.c_contiguous

    a_chx = _fromrawpointer(
        a_cupy.data.mem.ptr,
        a_cupy.shape,
        a_cupy.dtype,
        a_cupy.strides,
        'cuda:0',
        offset,
        a_cupy)

    assert a_chx.strides == a_cupy.strides
    chainerx.testing.assert_array_equal_ex(
        a_chx, a_cupy.get(), strides_check=False)

    a_cupy[1, 1] = 53

    assert a_chx.strides == a_cupy.strides
    chainerx.testing.assert_array_equal_ex(
        a_chx, a_cupy.get(), strides_check=False)


</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/test_cupy_interop.py" startline="135" endline="164" pcid="9359">
def test_cupy_to_chainerx_noncontiguous_without_offset():
    # This test includes access to address before the given pointer (because of
    # a negative stride).
    dtype = numpy.float32
    a_cupy = cupy.arange(12, dtype=dtype).reshape((2, 6))[::-1, ::2]

    # test preconditons
    assert a_cupy.data.mem.ptr < a_cupy.data.ptr
    assert not a_cupy.flags.c_contiguous

    a_chx = _fromrawpointer(
        a_cupy.data.ptr,
        a_cupy.shape,
        a_cupy.dtype,
        a_cupy.strides,
        'cuda:0',
        0,
        a_cupy)

    assert a_chx.strides == a_cupy.strides
    chainerx.testing.assert_array_equal_ex(
        a_chx, a_cupy.get(), strides_check=False)

    a_cupy[1, 1] = 53

    assert a_chx.strides == a_cupy.strides
    chainerx.testing.assert_array_equal_ex(
        a_chx, a_cupy.get(), strides_check=False)


</source>
</class>

<class classid="219" nclones="4" nlines="18" similarity="70">
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/test_cupy_interop.py" startline="200" endline="227" pcid="9362">
def test_chainerx_to_cupy_contiguous():
    dtype = 'float32'
    a_chx = chainerx.arange(6, dtype=dtype, device='cuda:0').reshape((2, 3))
    a_cupy = cupy.ndarray(
        a_chx.shape,
        cupy.dtype(a_chx.dtype.name),
        cupy.cuda.MemoryPointer(cupy.cuda.UnownedMemory(
            a_chx.data_ptr + a_chx.offset,
            a_chx.data_size,
            a_chx,
            0), 0),
        strides=a_chx.strides,
    )

    assert a_cupy.device.id == 0
    chainerx.testing.assert_array_equal_ex(a_chx, a_cupy.get())

    # Write to a_cupy
    a_cupy[0, 1] = 8
    chainerx.testing.assert_array_equal_ex(
        a_chx, numpy.array([[0, 8, 2], [3, 4, 5]], dtype))

    # Write to a_chx
    a_chx += 1
    chainerx.testing.assert_array_equal_ex(
        a_cupy.get(), numpy.array([[1, 9, 3], [4, 5, 6]], dtype))


</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/test_cupy_interop.py" startline="229" endline="249" pcid="9363">
def test_chainerx_to_cupy_delete_cupy_first():
    dtype = 'float32'
    a_chx = chainerx.arange(6, dtype=dtype, device='cuda:0').reshape((2, 3))
    a_cupy = cupy.ndarray(
        a_chx.shape,
        cupy.dtype(a_chx.dtype.name),
        cupy.cuda.MemoryPointer(cupy.cuda.UnownedMemory(
            a_chx.data_ptr + a_chx.offset,
            a_chx.data_size,
            a_chx,
            0), 0),
        strides=a_chx.strides,
    )

    del a_cupy

    a_chx += 1
    chainerx.testing.assert_array_equal_ex(
        a_chx, numpy.array([[1, 2, 3], [4, 5, 6]], dtype))


</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/test_cupy_interop.py" startline="251" endline="271" pcid="9364">
def test_chainerx_to_cupy_delete_chainerx_first():
    dtype = 'float32'
    a_chx = chainerx.arange(6, dtype=dtype, device='cuda:0').reshape((2, 3))
    a_cupy = cupy.ndarray(
        a_chx.shape,
        cupy.dtype(a_chx.dtype.name),
        cupy.cuda.MemoryPointer(cupy.cuda.UnownedMemory(
            a_chx.data_ptr + a_chx.offset,
            a_chx.data_size,
            a_chx,
            0), 0),
        strides=a_chx.strides,
    )

    del a_chx

    a_cupy += 1
    chainerx.testing.assert_array_equal_ex(
        a_cupy.get(), numpy.array([[1, 2, 3], [4, 5, 6]], dtype))


</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/test_cupy_interop.py" startline="306" endline="321" pcid="9366">
def test_chainerx_to_cupy_nondefault_device():
    dtype = 'float32'
    a_chx = chainerx.arange(6, dtype=dtype, device='cuda:1').reshape((2, 3))
    a_cupy = cupy.ndarray(
        a_chx.shape,
        cupy.dtype(a_chx.dtype.name),
        cupy.cuda.MemoryPointer(cupy.cuda.UnownedMemory(
            a_chx.data_ptr + a_chx.offset,
            a_chx.data_size,
            a_chx,
            -1), 0),
        strides=a_chx.strides,
    )

    assert a_cupy.device.id == 1
    chainerx.testing.assert_array_equal_ex(a_chx, a_cupy.get())
</source>
</class>

<class classid="220" nclones="2" nlines="10" similarity="90">
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_creation.py" startline="106" endline="121" pcid="9399">
def test_array_from_numpy_array(xp, shape, dtype, device):
    a_np = array_utils.create_dummy_ndarray(numpy, shape, dtype)
    a_xp = xp.array(a_np)

    if xp is chainerx:
        _check_array_from_numpy_array(a_xp, a_np, device)

        # test possibly freed memory
        a_np_copy = a_np.copy()
        del a_np
        chainerx.testing.assert_array_equal_ex(
            a_xp, a_np_copy, strides_check=False)

    return a_xp


</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_creation.py" startline="124" endline="139" pcid="9400">
def test_array_from_numpy_non_contiguous_array(xp, shape, dtype, device):
    a_np = array_utils.create_dummy_ndarray(numpy, shape, dtype, padding=True)
    a_xp = xp.array(a_np)

    if xp is chainerx:
        _check_array_from_numpy_array(a_xp, a_np, device)

        # test possibly freed memory
        a_np_copy = a_np.copy()
        del a_np
        chainerx.testing.assert_array_equal_ex(
            a_xp, a_np_copy, strides_check=False)

    return a_xp


</source>
</class>

<class classid="221" nclones="2" nlines="18" similarity="77">
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_creation.py" startline="253" endline="274" pcid="9410">
def test_array_from_chainerx_array_with_device(
        src_dtype, dst_dtype, copy, device, dst_device_spec):
    t = array_utils.create_dummy_ndarray(
        chainerx, (2,), src_dtype, device=device)
    a = chainerx.array(t, dtype=dst_dtype, copy=copy, device=dst_device_spec)

    dst_device = chainerx.get_device(dst_device_spec)

    if (not copy
            and (dst_dtype is None or src_dtype == dst_dtype)
            and (dst_device_spec is None or device is dst_device)):
        assert t is a
    else:
        assert t is not a
        if dst_dtype is None:
            dst_dtype = t.dtype
        chainerx.testing.assert_array_equal_ex(
            a, t.to_device(dst_device).astype(dst_dtype))
        assert a.dtype == chainerx.dtype(dst_dtype)
        assert a.device is dst_device


</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_creation.py" startline="351" endline="371" pcid="9416">
def test_asarray_from_chainerx_array_with_device(
        src_dtype, dst_dtype, device, dst_device_spec):
    t = array_utils.create_dummy_ndarray(
        chainerx, (2,), src_dtype, device=device)
    a = chainerx.asarray(t, dtype=dst_dtype, device=dst_device_spec)

    dst_device = chainerx.get_device(dst_device_spec)

    if ((dst_dtype is None or src_dtype == dst_dtype)
            and (dst_device_spec is None or device is dst_device)):
        assert t is a
    else:
        assert t is not a
        if dst_dtype is None:
            dst_dtype = t.dtype
        chainerx.testing.assert_array_equal_ex(
            a, t.to_device(dst_device).astype(dst_dtype))
        assert a.dtype == chainerx.dtype(dst_dtype)
        assert a.device is dst_device


</source>
</class>

<class classid="222" nclones="2" nlines="10" similarity="100">
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_creation.py" startline="324" endline="335" pcid="9414">
def test_asarray_from_chainerx_array(dtype):
    obj = array_utils.create_dummy_ndarray(chainerx, (2, 3), 'int32')
    a = chainerx.asarray(obj, dtype=dtype)
    if a.dtype == obj.dtype:
        assert a is obj
    else:
        assert a is not obj
    e = chainerx.array(obj, dtype=dtype, copy=False)
    chainerx.testing.assert_array_equal_ex(e, a)
    assert e.device is a.device


</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_creation.py" startline="440" endline="451" pcid="9423">
def test_asanyarray_from_chainerx_array(dtype):
    obj = array_utils.create_dummy_ndarray(chainerx, (2, 3), 'int32')
    a = chainerx.asanyarray(obj, dtype=dtype)
    if a.dtype == obj.dtype:
        assert a is obj
    else:
        assert a is not obj
    e = chainerx.array(obj, dtype=dtype, copy=False)
    chainerx.testing.assert_array_equal_ex(e, a)
    assert e.device is a.device


</source>
</class>

<class classid="223" nclones="3" nlines="11" similarity="90">
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_creation.py" startline="465" endline="477" pcid="9425">
def test_empty(xp, shape_as_sequence_or_int, dtype_spec, device):
    if xp is numpy and isinstance(dtype_spec, chainerx.dtype):
        dtype_spec = dtype_spec.name
    if dtype_spec is Unspecified:
        a = xp.empty(shape_as_sequence_or_int)
    else:
        a = xp.empty(shape_as_sequence_or_int, dtype_spec)
    a.fill(0)
    if dtype_spec in (None, Unspecified):
        a = dtype_utils.cast_if_numpy_array(xp, a, 'float32')
    return a


</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_creation.py" startline="554" endline="565" pcid="9433">
def test_ones(xp, shape_as_sequence_or_int, dtype_spec, device):
    if xp is numpy and isinstance(dtype_spec, chainerx.dtype):
        dtype_spec = dtype_spec.name
    if dtype_spec is Unspecified:
        out = xp.ones(shape_as_sequence_or_int)
    else:
        out = xp.ones(shape_as_sequence_or_int, dtype_spec)
    if dtype_spec in (None, Unspecified):
        out = dtype_utils.cast_if_numpy_array(xp, out, 'float32')
    return out


</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_creation.py" startline="512" endline="523" pcid="9429">
def test_zeros(xp, shape_as_sequence_or_int, dtype_spec, device):
    if xp is numpy and isinstance(dtype_spec, chainerx.dtype):
        dtype_spec = dtype_spec.name
    if dtype_spec is Unspecified:
        out = xp.zeros(shape_as_sequence_or_int)
    else:
        out = xp.zeros(shape_as_sequence_or_int, dtype_spec)
    if dtype_spec in (None, Unspecified):
        out = dtype_utils.cast_if_numpy_array(xp, out, 'float32')
    return out


</source>
</class>

<class classid="224" nclones="2" nlines="15" similarity="73">
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_creation.py" startline="685" endline="702" pcid="9444">
def test_arange_start_stop(xp, start, stop, dtype_spec, device):
    if xp is numpy and isinstance(dtype_spec, chainerx.dtype):
        dtype_spec = dtype_spec.name
    # Checked in test_invalid_arange_too_long_bool
    if _is_bool_spec(dtype_spec) and abs(stop - start) > 2:
        return chainerx.testing.ignore()
    if ((isinstance(start, bool)
            or isinstance(stop, bool))
            and dtype_spec is None):
        # TODO(niboshi): This pattern needs dtype promotion.
        return chainerx.testing.ignore()
    out = xp.arange(start, stop, dtype=dtype_spec)
    if dtype_spec in (None, Unspecified):
        expected_dtype = _get_default_dtype(stop)
        out = dtype_utils.cast_if_numpy_array(xp, out, expected_dtype)
    return out


</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_creation.py" startline="719" endline="737" pcid="9445">
def test_arange_start_stop_step(xp, start, stop, step, dtype_spec, device):
    if xp is numpy and isinstance(dtype_spec, chainerx.dtype):
        dtype_spec = dtype_spec.name
    # Checked in test_invalid_arange_too_long_bool
    if _is_bool_spec(dtype_spec) and abs((stop - start) / step) > 2:
        return chainerx.testing.ignore()
    if ((isinstance(start, bool)
            or isinstance(stop, bool)
            or isinstance(step, bool))
            and dtype_spec is None):
        # TODO(niboshi): This pattern needs dtype promotion.
        return chainerx.testing.ignore()
    out = xp.arange(start, stop, step, dtype=dtype_spec)
    if dtype_spec in (None, Unspecified):
        expected_dtype = _get_default_dtype(step)
        out = dtype_utils.cast_if_numpy_array(xp, out, expected_dtype)
    return out


</source>
</class>

<class classid="225" nclones="2" nlines="10" similarity="100">
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_creation.py" startline="852" endline="864" pcid="9457">
def test_eye_with_default(xp, N, M, k, dtype_spec, device):
    if xp is numpy and isinstance(dtype_spec, chainerx.dtype):
        dtype_spec = dtype_spec.name

    if M is None and k is None:
        return xp.eye(N, dtype=dtype_spec)
    elif M is None:
        return xp.eye(N, k=k, dtype=dtype_spec)
    elif k is None:
        return xp.eye(N, M=M, dtype=dtype_spec)
    assert False


</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_creation.py" startline="1213" endline="1225" pcid="9485">
def test_tri_with_default(xp, N, M, k, dtype_spec, device):
    if xp is numpy and isinstance(dtype_spec, chainerx.dtype):
        dtype_spec = dtype_spec.name

    if M is None and k is None:
        return xp.tri(N, dtype=dtype_spec)
    elif M is None:
        return xp.tri(N, k=k, dtype=dtype_spec)
    elif k is None:
        return xp.tri(N, M=M, dtype=dtype_spec)
    assert False


</source>
</class>

<class classid="226" nclones="2" nlines="10" similarity="70">
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_normalization.py" startline="162" endline="177" pcid="9512">
    def forward_chainerx(self, inputs):
        x, gamma, beta = inputs

        running_mean = chainerx.array(self.running_mean, copy=True)
        running_var = chainerx.array(self.running_var, copy=True)

        y = chainerx.batch_norm(
            x, gamma, beta, running_mean=running_mean, running_var=running_var,
            **self.optional_args)

        # Record running values for later checks.
        self.running_mean_chx = running_mean
        self.running_var_chx = running_var

        return y,

</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_normalization.py" startline="178" endline="193" pcid="9513">
    def forward_chainer(self, inputs):
        x, gamma, beta = inputs

        running_mean = self.running_mean.copy()
        running_var = self.running_var.copy()

        y = chainer.functions.batch_normalization(
            x, gamma, beta, running_mean=running_mean, running_var=running_var,
            **self.optional_args)

        # Record running values for later checks.
        self.running_mean_ch = running_mean
        self.running_var_ch = running_var

        return y,

</source>
</class>

<class classid="227" nclones="3" nlines="15" similarity="93">
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_connection.py" startline="136" endline="152" pcid="9540">
    def generate_inputs(self):
        x_shape = self.x_shape
        w_shape = self.w_shape
        b_shape = self.b_shape
        if len(self.in_dtypes) == 3:
            x_dtype, w_dtype, b_dtype = self.in_dtypes
        else:
            (x_dtype, w_dtype), b_dtype = self.in_dtypes, None
        x = array_utils.uniform(x_shape, x_dtype)
        w = array_utils.uniform(w_shape, w_dtype)

        if b_shape is None:
            return x, w
        else:
            b = array_utils.uniform(b_shape, b_dtype)
            return x, w, b

</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_connection.py" startline="323" endline="339" pcid="9547">
    def generate_inputs(self):
        x_shape = self.x_shape
        w_shape = self.w_shape
        b_shape = self.b_shape
        if len(self.in_dtypes) == 3:
            x_dtype, w_dtype, b_dtype = self.in_dtypes
        else:
            (x_dtype, w_dtype), b_dtype = self.in_dtypes, None
        x = array_utils.uniform(x_shape, x_dtype)
        w = array_utils.uniform(w_shape, w_dtype)

        if b_shape is None:
            return x, w
        else:
            b = array_utils.uniform(b_shape, b_dtype)
            return x, w, b

</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_connection.py" startline="531" endline="546" pcid="9554">
    def generate_inputs(self):
        x_shape = self.x_shape
        w_shape = self.w_shape
        b_shape = self.b_shape
        if len(self.in_dtypes) == 3:
            x_dtype, w_dtype, b_dtype = self.in_dtypes
        else:
            (x_dtype, w_dtype), b_dtype = self.in_dtypes, None
        x = array_utils.uniform(x_shape, x_dtype)
        w = array_utils.uniform(w_shape, w_dtype)
        if b_shape in (None, Unspecified):
            return x, w
        else:
            b = array_utils.uniform(b_shape, b_dtype)
            return x, w, b

</source>
</class>

<class classid="228" nclones="2" nlines="15" similarity="80">
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_connection.py" startline="161" endline="176" pcid="9542">
    def forward_chainer(self, inputs):
        if len(inputs) == 2:
            (x, w), b = inputs, None
        else:
            x, w, b = inputs
        if x.dtype.kind != 'f':
            x = F.cast(x, 'float64')
        if w.dtype.kind != 'f':
            w = F.cast(w, 'float64')
        if b is not None and b.dtype.kind != 'f':
            b = F.cast(b, 'float64')
        y = F.convolution_nd(
            x, w, b, self.stride, self.pad, self.cover_all)
        y = F.cast(y, self.out_dtype)
        return y,

</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_connection.py" startline="349" endline="364" pcid="9549">
    def forward_chainer(self, inputs):
        if len(inputs) == 3:
            x, w, b = inputs
        else:
            (x, w), b = inputs, None
        if x.dtype.kind != 'f':
            x = F.cast(x, 'float64')
        if w.dtype.kind != 'f':
            w = F.cast(w, 'float64')
        if b is not None and b.dtype.kind != 'f':
            b = F.cast(b, 'float64')
        y = chainer.functions.deconvolution_nd(
            x, w, b, self.stride, self.pad, self.outsize)
        y = F.cast(y, self.out_dtype)
        return y,

</source>
</class>

<class classid="229" nclones="7" nlines="11" similarity="81">
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_binary.py" startline="555" endline="567" pcid="9620">
    def func_scalar(self, xp, a, scalar):
        if self.is_module:
            if self.is_scalar_rhs:
                return a & scalar
            else:
                return scalar & a
        else:
            if self.is_scalar_rhs:
                return xp.bitwise_and(a, scalar)
            else:
                return xp.bitwise_and(scalar, a)


</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_binary.py" startline="591" endline="603" pcid="9622">
    def func_scalar(self, xp, a, scalar):
        if self.is_module:
            if self.is_scalar_rhs:
                return a ^ scalar
            else:
                return scalar ^ a
        else:
            if self.is_scalar_rhs:
                return xp.bitwise_xor(a, scalar)
            else:
                return xp.bitwise_xor(scalar, a)


</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_binary.py" startline="573" endline="585" pcid="9621">
    def func_scalar(self, xp, a, scalar):
        if self.is_module:
            if self.is_scalar_rhs:
                return a | scalar
            else:
                return scalar | a
        else:
            if self.is_scalar_rhs:
                return xp.bitwise_or(a, scalar)
            else:
                return xp.bitwise_or(scalar, a)


</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_arithmetic.py" startline="328" endline="340" pcid="9686">
    def func_scalar(self, xp, a, scalar):
        if self.is_module:
            if self.is_scalar_rhs:
                return a + scalar
            else:
                return scalar + a
        else:
            if self.is_scalar_rhs:
                return xp.add(a, scalar)
            else:
                return xp.add(scalar, a)


</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_arithmetic.py" startline="1626" endline="1638" pcid="9731">
    def func_scalar(self, xp, a, scalar):
        if self.is_module:
            if self.is_scalar_rhs:
                return a % scalar
            else:
                return scalar % a
        else:
            if self.is_scalar_rhs:
                return xp.remainder(a, scalar)
            else:
                return xp.remainder(scalar, a)


</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_arithmetic.py" startline="719" endline="731" pcid="9696">
    def func_scalar(self, xp, a, scalar):
        if self.is_module:
            if self.is_scalar_rhs:
                return a * scalar
            else:
                return scalar * a
        else:
            if self.is_scalar_rhs:
                return xp.multiply(a, scalar)
            else:
                return xp.multiply(scalar, a)


</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_arithmetic.py" startline="534" endline="546" pcid="9692">
    def func_scalar(self, xp, a, scalar):
        if self.is_module:
            if self.is_scalar_rhs:
                return a - scalar
            else:
                return scalar - a
        else:
            if self.is_scalar_rhs:
                return xp.subtract(a, scalar)
            else:
                return xp.subtract(scalar, a)


</source>
</class>

<class classid="230" nclones="6" nlines="19" similarity="95">
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py" startline="34" endline="53" pcid="9643">
    def setup(self):
        self.check_forward_options.update({
            'rtol': 1e-2, 'atol': 1e-2})
        self.check_backward_options.update({
            'rtol': 1e-2, 'atol': 1e-2})
        self.check_double_backward_options.update({
            'rtol': 5e-3, 'atol': 5e-2})
        if self.in_dtypes[0] == 'float16':
            self.check_forward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
            self.check_backward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
            self.check_double_backward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
        device = chainerx.get_default_device()
        if device.backend.name == 'cuda':
            if self.in_dtypes[0] != 'float32':
                self.skip_backward_test = True
            self.skip_double_backward_test = True

</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py" startline="344" endline="364" pcid="9661">
    def setup(self):

        self.check_forward_options.update({
            'rtol': 1e-2, 'atol': 1e-2})
        self.check_backward_options.update({
            'rtol': 1e-2, 'atol': 1e-2})
        self.check_double_backward_options.update({
            'rtol': 5e-2, 'atol': 5e-2})
        if self.in_dtypes[0] == 'float16':
            self.check_forward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
            self.check_backward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
            self.check_double_backward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
        device = chainerx.get_default_device()
        if device.backend.name == 'cuda':
            if self.in_dtypes[0] != 'float32':
                self.skip_backward_test = True
            self.skip_double_backward_test = True

</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py" startline="246" endline="266" pcid="9655">
    def setup(self):
        self.check_forward_options.update({
            'rtol': 1e-2, 'atol': 1e-2})
        self.check_backward_options.update({
            'rtol': 1e-2, 'atol': 1e-2})
        self.check_double_backward_options.update({
            'rtol': 5e-3, 'atol': 5e-2})
        if self.in_dtypes[0] == 'float16':
            self.check_forward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
            self.check_backward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
            self.check_double_backward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
        device = chainerx.get_default_device()
        if device.backend.name == 'cuda':
            if self.in_dtypes[0] != 'float32':

                self.skip_backward_test = True
            self.skip_double_backward_test = True

</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py" startline="458" endline="478" pcid="9667">
    def setup(self):
        self.check_forward_options.update({
            'rtol': 1e-2, 'atol': 1e-2})
        self.check_backward_options.update({
            'rtol': 1e-2, 'atol': 1e-2})
        self.check_double_backward_options.update({
            'rtol': 5e-2, 'atol': 5e-2})
        if self.in_dtypes[0] == 'float16':
            self.check_forward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
            self.check_backward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
            self.check_double_backward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
        device = chainerx.get_default_device()
        if device.backend.name == 'cuda':
            if self.in_dtypes[0] != 'float32':
                self.skip_forward_test = True
                self.skip_backward_test = True
            self.skip_double_backward_test = True

</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py" startline="564" endline="585" pcid="9673">
    def setup(self):

        self.check_forward_options.update({
            'rtol': 1e-2, 'atol': 1e-2})
        self.check_backward_options.update({
            'rtol': 1e-2, 'atol': 1e-2})
        self.check_double_backward_options.update({
            'rtol': 5e-2, 'atol': 5e-2})
        if self.in_dtypes[0] == 'float16':
            self.check_forward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
            self.check_backward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
            self.check_double_backward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
        device = chainerx.get_default_device()
        if device.backend.name == 'cuda':
            if self.in_dtypes[0] != 'float32':
                self.skip_forward_test = True
                self.skip_backward_test = True
            self.skip_double_backward_test = True

</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py" startline="135" endline="155" pcid="9649">
    def setup(self):

        self.check_forward_options.update({
            'rtol': 1e-2, 'atol': 1e-2})
        self.check_backward_options.update({
            'rtol': 1e-2, 'atol': 1e-2})
        self.check_double_backward_options.update({
            'rtol': 5e-3, 'atol': 5e-2})
        if self.in_dtypes[0] == 'float16':
            self.check_forward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
            self.check_backward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
            self.check_double_backward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
        device = chainerx.get_default_device()
        if device.backend.name == 'cuda':
            if self.in_dtypes[0] != 'float32':
                self.skip_backward_test = True
            self.skip_double_backward_test = True

</source>
</class>

<class classid="231" nclones="2" nlines="10" similarity="100">
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py" startline="105" endline="117" pcid="9648">
    def forward_chainer(self, inputs):
        h, c, ws, bs, xs = self.process_input(inputs)
        out = chainer.functions.n_step_lstm(
            self.n_layers, 0.0, h, c, ws, bs, xs)
        rets = []
        rets.append(out[0])
        rets.append(out[1])
        for i in range(len(out[2])):
            rets.append(out[2][i])

        return tuple(rets)


</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py" startline="215" endline="227" pcid="9654">
    def forward_chainer(self, inputs):
        h, c, ws, bs, xs = self.process_input(inputs)
        out = chainer.functions.n_step_bilstm(
            self.n_layers, 0.0, h, c, ws, bs, xs)
        rets = []
        rets.append(out[0])
        rets.append(out[1])
        for i in range(len(out[2])):
            rets.append(out[2][i])

        return tuple(rets)


</source>
</class>

<class classid="232" nclones="5" nlines="10" similarity="90">
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_arithmetic.py" startline="224" endline="235" pcid="9683">
def test_add_invalid_dtypes(device, dtypes, is_module):
    (in_dtype1, in_dtype2), _ = dtypes
    shape = (2, 3)
    a = chainerx.array(array_utils.uniform(shape, in_dtype1))
    b = chainerx.array(array_utils.uniform(shape, in_dtype2))
    with pytest.raises(chainerx.DtypeError):
        if is_module:
            a + b
        else:
            chainerx.add(a, b)


</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_arithmetic.py" startline="1050" endline="1061" pcid="9709">
def test_truediv_invalid_dtypes(device, dtypes, is_module):
    (in_dtype1, in_dtype2), _ = dtypes
    shape = (2, 3)
    a = chainerx.array(array_utils.uniform(shape, in_dtype1))
    b = chainerx.array(array_utils.uniform(shape, in_dtype2))
    with pytest.raises(chainerx.DtypeError):
        if is_module:
            a / b
        else:
            chainerx.true_divide(a, b)


</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_arithmetic.py" startline="1490" endline="1501" pcid="9725">
def test_remainder_invalid_dtypes(device, dtypes, is_module):
    (in_dtype1, in_dtype2), _ = dtypes
    shape = (2, 3)
    a = chainerx.array(array_utils.uniform(shape, in_dtype1))
    b = chainerx.array(array_utils.uniform(shape, in_dtype2))
    with pytest.raises(chainerx.DtypeError):
        if is_module:
            a % b
        else:
            chainerx.remainder(a, b)


</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_arithmetic.py" startline="430" endline="441" pcid="9689">
def test_sub_invalid_dtypes(device, dtypes, is_module):
    (in_dtype1, in_dtype2), _ = dtypes
    shape = (2, 3)
    a = chainerx.array(array_utils.uniform(shape, in_dtype1))
    b = chainerx.array(array_utils.uniform(shape, in_dtype2))
    with pytest.raises(chainerx.DtypeError):
        if is_module:
            a - b
        else:
            chainerx.subtract(a, b)


</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_arithmetic.py" startline="864" endline="878" pcid="9703">
def test_floordiv_invalid_dtypes(device, dtypes, is_module):
    (in_dtype1, in_dtype2), _ = dtypes
    shape = (2, 3)
    a = chainerx.array(array_utils.uniform(shape, in_dtype1))
    b = chainerx.array(array_utils.uniform(shape, in_dtype2))
    with pytest.raises(chainerx.DtypeError):
        if is_module:
            a // b
        else:
            chainerx.floor_divide(a, b)


# TODO(imanishi): Support and test zero division and mixed dtypes.
# TODO(imanishi): Support and test chainerx.Scalar // chainerx.ndarray.
# TODO(imanishi): Support and test bool dtype.
</source>
</class>

<class classid="233" nclones="2" nlines="11" similarity="81">
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_arithmetic.py" startline="849" endline="861" pcid="9702">
    def func_scalar(self, xp, a, scalar):
        if self.is_module:
            if self.is_scalar_rhs:
                return xp.floor_divide(a, scalar)
            else:
                return xp.floor_divide(scalar, a)
        else:
            if self.is_scalar_rhs:
                return a // scalar
            else:
                return scalar // a


</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_arithmetic.py" startline="1160" endline="1172" pcid="9713">
    def func_scalar(self, xp, a, scalar):
        if self.is_module:
            if self.is_scalar_rhs:
                return xp.divide(a, scalar)
            else:
                return xp.divide(scalar, a)
        else:
            if self.is_scalar_rhs:
                return a / scalar
            else:
                return scalar / a


</source>
</class>

<class classid="234" nclones="2" nlines="11" similarity="100">
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_arithmetic.py" startline="1469" endline="1480" pcid="9723">
    def generate_inputs(self):
        dtype1, dtype2 = self.in_dtypes
        shape1, shape2 = self.in_shapes
        low1 = -5 if numpy.dtype(dtype1).kind != 'u' else 2
        low2 = -5 if numpy.dtype(dtype2).kind != 'u' else 2
        high = 5
        a = array_utils.uniform(shape1, dtype1, low=low1, high=high)
        b = array_utils.uniform(shape2, dtype2, low=low2, high=high)
        a[numpy.logical_and(-0.5 < a, a < 0.5)] = 1
        b[numpy.logical_and(-0.5 < b, b < 0.5)] = 1
        return a, b

</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_arithmetic.py" startline="1543" endline="1554" pcid="9727">
    def generate_inputs(self):
        dtype1, dtype2 = self.in_dtypes
        shape1, shape2 = self.in_shapes
        low1 = -5 if numpy.dtype(dtype1).kind != 'u' else 2
        low2 = -5 if numpy.dtype(dtype2).kind != 'u' else 2
        high = 5
        a = array_utils.uniform(shape1, dtype1, low=low1, high=high)
        b = array_utils.uniform(shape2, dtype2, low=low2, high=high)
        a[numpy.logical_and(-0.5 < a, a < 0.5)] = 1
        b[numpy.logical_and(-0.5 < b, b < 0.5)] = 1
        return a, b

</source>
</class>

<class classid="235" nclones="2" nlines="17" similarity="100">
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_activation.py" startline="230" endline="249" pcid="9785">
    def setup(self):
        in_dtype, = self.in_dtypes
        if isinstance(self.alpha_range, tuple):
            l, u = self.alpha_range
            self.alpha = random.uniform(l, u)
        elif self.alpha_range is Unspecified:
            self.alpha = 1.0
        else:
            self.alpha = self.alpha_range

        if numpy.dtype(in_dtype).kind != 'f':
            self.skip_backward_test = True
            self.skip_double_backward_test = True

        if in_dtype == 'float16':
            self.check_forward_options.update({'rtol': 1e-3, 'atol': 1e-3})
            self.check_backward_options.update({'rtol': 2e-3, 'atol': 2e-3})
            self.check_double_backward_options.update(
                {'rtol': 1e-2, 'atol': 1e-2})

</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_activation.py" startline="367" endline="386" pcid="9790">
    def setup(self):
        in_dtype, = self.in_dtypes
        if isinstance(self.beta_range, tuple):
            l, u = self.beta_range
            self.beta = random.uniform(l, u)
        elif self.beta_range is Unspecified:
            self.beta = 1.0
        else:
            self.beta = self.beta_range

        if numpy.dtype(in_dtype).kind != 'f':
            self.skip_backward_test = True
            self.skip_double_backward_test = True

        if in_dtype == 'float16':
            self.check_forward_options.update({'rtol': 2e-3, 'atol': 2e-3})
            self.check_backward_options.update({'rtol': 2e-3, 'atol': 2e-3})
            self.check_double_backward_options.update(
                {'rtol': 1e-2, 'atol': 1e-2})

</source>
</class>

<class classid="236" nclones="2" nlines="12" similarity="75">
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_logic.py" startline="132" endline="148" pcid="9796">
def test_cmp_invalid_shapes(cmp_func, a_shape, b_shape):
    cmp_op = _cmp_funcs[cmp_func]
    chx_cmp = getattr(chainerx, cmp_func)

    def check(x, y):
        with pytest.raises(chainerx.DimensionError):
            cmp_op(x, y)

        with pytest.raises(chainerx.DimensionError):
            chx_cmp(x, y)

    a = array_utils.create_dummy_ndarray(chainerx, a_shape, 'float32')
    b = array_utils.create_dummy_ndarray(chainerx, b_shape, 'float32')
    check(a, b)
    check(b, a)


</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/unit_tests/routines_tests/test_logic.py" startline="150" endline="166" pcid="9798">
def test_cmp_invalid_dtypes(cmp_func, numeric_dtype):
    cmp_op = _cmp_funcs[cmp_func]
    chx_cmp = getattr(chainerx, cmp_func)

    def check(x, y):
        with pytest.raises(chainerx.DtypeError):
            cmp_op(x, y)

        with pytest.raises(chainerx.DtypeError):
            chx_cmp(x, y)

    a = array_utils.create_dummy_ndarray(chainerx, (2, 3), 'bool_')
    b = array_utils.create_dummy_ndarray(chainerx, (2, 3), numeric_dtype)
    check(a, b)
    check(b, a)


</source>
</class>

<class classid="237" nclones="2" nlines="10" similarity="90">
<source file="systems/chainer-7.2.0/tests/chainerx_tests/op_utils.py" startline="234" endline="255" pcid="9858">
    def forward_chainerx(self, inputs):
        # Computes the forward pass in ChainerX.
        #
        # In case of an acceptable error, the error is stored and a dummy
        # output array is returned.
        #
        # The detected errors are checked in `check_forward_outputs`, and
        # also in `_create_test_entry_function` to skip
        # backward/double-backward tests.
        accept_errors = self.__get_accept_errors()
        try:
            outputs = self.forward_xp(inputs, chainerx)
            self.__forward_error_chainerx = 'ok'
        except accept_errors as e:
            # Keep detected error
            self.__forward_error_chainerx = e
            # A dummy output array is returned
            y = chainerx.zeros((0,), 'float32')
            outputs = y,

        return outputs

</source>
<source file="systems/chainer-7.2.0/tests/chainerx_tests/op_utils.py" startline="256" endline="271" pcid="9859">
    def forward_expected(self, inputs):
        # Computes the forward pass in NumPy.
        # Also see comments in `forward_chainerx`.
        accept_errors = self.__get_accept_errors()
        try:
            outputs = self.forward_xp(inputs, numpy)
            self.__forward_error_expected = 'ok'
        except accept_errors as e:
            # Keep detected error
            self.__forward_error_expected = e
            # A dummy output array is returned
            y = numpy.zeros((0,), 'float32')
            outputs = y,

        return tuple([numpy.asarray(y) for y in outputs])

</source>
</class>

<class classid="238" nclones="4" nlines="10" similarity="90">
<source file="systems/chainer-7.2.0/onnx_chainer/functions/math.py" startline="18" endline="30" pcid="9892">
def convert_AddConstant(
        func, opset_version, input_names, output_names, context):
    value_name = context.add_const(
        np.array(func.value, dtype=func.inputs[0].dtype), 'value')
    input_names.append(value_name)

    if opset_version == 1:
        return onnx_helper.make_node(
            'Add', input_names, output_names, consumed_inputs=[1, 1]),
    elif opset_version == 6 or opset_version == 7:
        return onnx_helper.make_node('Add', input_names, output_names),


</source>
<source file="systems/chainer-7.2.0/onnx_chainer/functions/math.py" startline="64" endline="76" pcid="9896">
def convert_MulConstant(
        func, opset_version, input_names, output_names, context):
    value_name = context.add_const(
        np.array(func.value, dtype=func.inputs[0].dtype), 'value')
    input_names.append(value_name)

    if opset_version == 1:
        return onnx_helper.make_node(
            'Mul', input_names, output_names, consumed_inputs=[1, 1]),
    elif opset_version == 6 or opset_version == 7:
        return onnx_helper.make_node('Mul', input_names, output_names),


</source>
<source file="systems/chainer-7.2.0/onnx_chainer/functions/math.py" startline="96" endline="108" pcid="9899">
def convert_DivFromConstant(
        func, opset_version, input_names, output_names, context):
    value_name = context.add_const(
        np.array(func.value, dtype=func.inputs[0].dtype), 'value')
    input_names[:0] = [value_name]

    if opset_version == 1:
        return onnx_helper.make_node(
            'Div', input_names, output_names, consumed_inputs=[1, 1]),
    elif opset_version == 6 or opset_version == 7:
        return onnx_helper.make_node('Div', input_names, output_names),


</source>
<source file="systems/chainer-7.2.0/onnx_chainer/functions/math.py" startline="41" endline="53" pcid="9894">
def convert_SubFromConstant(
        func, opset_version, input_names, output_names, context):
    value_name = context.add_const(
        np.array(func.value, dtype=func.inputs[0].dtype), 'value')
    input_names[:0] = [value_name]

    if opset_version == 1:
        return onnx_helper.make_node(
            'Sub', input_names, output_names, consumed_inputs=[1, 1]),
    elif opset_version == 6 or opset_version == 7:
        return onnx_helper.make_node('Sub', input_names, output_names),


</source>
</class>

<class classid="239" nclones="2" nlines="18" similarity="94">
<source file="systems/chainer-7.2.0/onnx_chainer/functions/connection.py" startline="31" endline="49" pcid="9935">
def convert_ConvolutionND(
        func, opset_version, input_names, output_names, context):
    pad = []
    x_ndim = len(func.inputs[0].shape)
    w_ndim = len(func.inputs[1].shape)
    for _ in range(x_ndim - w_ndim):
        pad.append(0)
    for p in func.pad:
        pad.append(p)
    pad = pad * 2

    return onnx_helper.make_node(
        'Conv', input_names, output_names,
        kernel_shape=func.inputs[1].shape[2:],
        pads=pad,
        strides=func.stride,
        group=func.groups,
    ),

</source>
<source file="systems/chainer-7.2.0/onnx_chainer/functions/connection.py" startline="64" endline="83" pcid="9937">
def convert_DeconvolutionND(
        func, opset_version, input_names, output_names, context):
    pad = []
    x_ndim = len(func.inputs[0].shape)
    w_ndim = len(func.inputs[1].shape)
    for _ in range(x_ndim - w_ndim):
        pad.append(0)
    for p in func.pad:
        pad.append(p)
    pad = pad * 2

    return onnx_helper.make_node(
        'ConvTranspose', input_names, output_names,
        kernel_shape=func.inputs[1].shape[2:],
        output_shape=func.outs,
        pads=pad,
        strides=func.stride,
        group=func.groups,
    ),

</source>
</class>

<class classid="240" nclones="2" nlines="19" similarity="70">
<source file="systems/chainer-7.2.0/onnx_chainer/functions/pooling.py" startline="11" endline="36" pcid="9940">
def convert_AveragePooling2D(
        func, opset_version, input_names, output_names, context):
    pad = [func.ph, func.pw]
    stride = [func.sy, func.sx]
    ksize = [func.kh, func.kw]
    if func.cover_all:
        # Supports cover_all by setting extra padding
        # NOTE: onnxruntime may not run when "k <= p + s - 1".
        pad.extend([p + s - 1 for p, s in zip(pad, stride)])
    else:
        pad = pad * 2

    if opset_version == 1:
        raise ValueError(
            'AveragePooling2D is not compatible with ONNX\'s AveragePool-1. '
            'Use operation set version >= 7.')
    elif opset_version == 7:
        return onnx_helper.make_node(
            'AveragePool', input_names, output_names,
            kernel_shape=ksize,
            pads=pad,
            strides=stride,
            count_include_pad=1,
        ),


</source>
<source file="systems/chainer-7.2.0/onnx_chainer/functions/pooling.py" startline="38" endline="61" pcid="9941">
def convert_AveragePoolingND(
        func, opset_version, input_names, output_names, context):
    pad = list(func.pad[:])
    if func.cover_all:
        # Supports cover_all by setting extra padding
        # NOTE: onnxruntime may not run when "k <= p + s - 1".
        pad.extend([p + s - 1 for p, s in zip(pad, func.stride)])
    else:
        pad = pad * 2

    if opset_version == 1:
        raise ValueError(
            'AveragePoolingND is not compatible with ONNX\'s AveragePool-1. '
            'Use operation set version >= 7.')
    elif opset_version == 7:
        return onnx_helper.make_node(
            'AveragePool', input_names, output_names,
            kernel_shape=func.ksize,
            pads=pad,
            strides=func.stride,
            count_include_pad=1,
        ),


</source>
</class>

<class classid="241" nclones="2" nlines="28" similarity="75">
<source file="systems/chainer-7.2.0/onnx_chainer/functions/pooling.py" startline="63" endline="97" pcid="9942">
def convert_MaxPooling2D(
        func, opset_version, input_names, output_names, context):
    pad = [func.ph, func.pw]
    stride = [func.sy, func.sx]
    ksize = [func.kh, func.kw]
    attrs = {}
    if func.cover_all:
        if opset_version < 11:
            # Supports cover_all by setting extra padding
            # NOTE: onnxruntime may not run when "k <= p + s - 1".
            pad.extend([p + s - 1 for p, s in zip(pad, func.stride)])
        else:
            pad = pad * 2
            attrs['ceil_mode'] = 1
    else:
        pad = pad * 2

    if opset_version == 1:
        return onnx_helper.make_node(
            'MaxPool', input_names, output_names,
            kernel_shape=ksize,
            pads=pad,
            strides=stride
        ),
    elif opset_version >= 8:
        return onnx_helper.make_node(
            'MaxPool', input_names, output_names,
            kernel_shape=ksize,
            pads=pad,
            strides=stride,
            storage_order=0,  # row major
            **attrs,
        ),


</source>
<source file="systems/chainer-7.2.0/onnx_chainer/functions/pooling.py" startline="99" endline="131" pcid="9943">
def convert_MaxPoolingND(
        func, opset_version, input_names, output_names, context):
    pad = list(func.pad[:])
    attrs = {}
    if func.cover_all:
        if opset_version < 11:
            # Supports cover_all by setting extra padding
            # NOTE: onnxruntime may not run when "k <= p + s - 1".
            pad.extend([p + s - 1 for p, s in zip(pad, func.stride)])
        else:
            pad = pad * 2
            attrs['ceil_mode'] = 1
    else:
        pad = pad * 2

    if opset_version == 1:
        return onnx_helper.make_node(
            'MaxPool', input_names, output_names,
            kernel_shape=func.ksize,
            pads=pad,
            strides=func.stride
        ),
    elif opset_version >= 8:
        return onnx_helper.make_node(
            'MaxPool', input_names, output_names,
            kernel_shape=func.ksize,
            pads=pad,
            strides=func.stride,
            storage_order=0,  # row major
            **attrs,
        ),


</source>
</class>

<class classid="242" nclones="3" nlines="10" similarity="70">
<source file="systems/chainer-7.2.0/onnx_chainer/functions/activation.py" startline="53" endline="65" pcid="9949">
def convert_ELU(func, opset_version, input_names, output_names, context):
    if opset_version == 1:
        return onnx_helper.make_node(
            'Elu', input_names, output_names,
            alpha=func.alpha,
        ),
    elif opset_version == 6:
        return onnx_helper.make_node(
            'Elu', input_names, output_names,
            alpha=func.alpha
        ),


</source>
<source file="systems/chainer-7.2.0/onnx_chainer/functions/activation.py" startline="85" endline="98" pcid="9951">
def convert_LeakyReLU(func, opset_version, input_names, output_names, context):
    if opset_version == 1:
        return onnx_helper.make_node(
            'LeakyRelu', input_names, output_names,
            alpha=func.slope,
            consumed_inputs=[1],
        ),
    elif opset_version == 6:
        return onnx_helper.make_node(
            'LeakyRelu', input_names, output_names,
            alpha=func.slope
        ),


</source>
<source file="systems/chainer-7.2.0/onnx_chainer/functions/array.py" startline="33" endline="46" pcid="9962">
def convert_Cast(func, opset_version, input_names, output_names, context):
    typ = func.type if isinstance(func.type, np.dtype) else np.dtype(func.type)
    if opset_version == 1:
        return onnx_helper.make_node(
            'Cast', input_names, output_names,
            to=TENSOR_TYPE_TO_NAME[NP_TYPE_TO_TENSOR_TYPE[typ]]
        ),
    elif opset_version == 6:
        return onnx_helper.make_node(
            'Cast', input_names, output_names,
            to=NP_TYPE_TO_TENSOR_TYPE[typ]
        ),


</source>
</class>

<class classid="243" nclones="2" nlines="13" similarity="78">
<source file="systems/chainer-7.2.0/onnx_chainer/functions/array.py" startline="576" endline="588" pcid="9985">

def convert_Vstack(func, opset_version, input_names, output_names, context):
    gb = onnx_helper.GraphBuilder()
    input0_ndim = len(func.inputs[0].shape)
    inputs = input_names
    if input0_ndim == 0:
        inputs = [gb.op('Unsqueeze', [name], axes=[0, 1]) for
                  name in input_names]
    elif input0_ndim == 1:
        inputs = [gb.op('Unsqueeze', [name], axes=[0]) for name in input_names]
    gb.op_output_named('Concat', inputs, output_names, axis=0)
    return gb.nodes()

</source>
<source file="systems/chainer-7.2.0/onnx_chainer/functions/array.py" startline="589" endline="604" pcid="9986">

def convert_Dstack(func, opset_version, input_names, output_names, context):
    gb = onnx_helper.GraphBuilder()
    input0_ndim = len(func.inputs[0].shape)
    inputs = input_names
    if input0_ndim == 0:
        inputs = [gb.op('Unsqueeze', [name], axes=[0, 1, 2]) for
                  name in input_names]
    elif input0_ndim == 1:
        inputs = [gb.op('Unsqueeze', [name], axes=[0, 2]) for
                  name in input_names]
    elif input0_ndim == 2:
        inputs = [gb.op('Unsqueeze', [name], axes=[2]) for name in input_names]
    gb.op_output_named('Concat', inputs, output_names, axis=2)
    return gb.nodes()

</source>
</class>

<class classid="244" nclones="2" nlines="14" similarity="100">
<source file="systems/chainer-7.2.0/chainermn/communicators/_memory_utility.py" startline="109" endline="123" pcid="10157">
    def from_device(self, src, size, offset=0, stream=None):
        dst = self.memory + offset
        xp = chainer.backend.get_array_module(src)
        if xp == cp:
            src_data = src.data
        elif xp == chx:
            src_data = _get_memory_pointer_from_chainerx(src)
        else:
            raise ValueError(
                '{} is from an unsupported array module'.format(type(src)))
        if stream is None:
            dst.copy_from_device(src_data, size)
        else:
            dst.copy_from_device_async(src_data, size, stream)

</source>
<source file="systems/chainer-7.2.0/chainermn/communicators/_memory_utility.py" startline="124" endline="138" pcid="10158">
    def to_device(self, dst, size, offset=0, stream=None):
        src = self.memory + offset
        xp = chainer.backend.get_array_module(dst)
        if xp == cp:
            dst_data = dst.data
        elif xp == chx:
            dst_data = _get_memory_pointer_from_chainerx(dst)
        else:
            raise ValueError(
                '{} is from an unsupported array module'.format(type(dst)))
        if stream is None:
            dst_data.copy_from_device(src, size)
        else:
            dst_data.copy_from_device_async(src, size, stream)

</source>
</class>

<class classid="245" nclones="2" nlines="16" similarity="100">
<source file="systems/chainer-7.2.0/chainermn/communicators/_memory_utility.py" startline="253" endline="270" pcid="10169">
def _batched_pack_params(params_data, buffer, dtype, stream=None):
    n_params = params_data.n_params
    n_elems = params_data.n_elems
    params_dptr = params_data.dptr
    params_dtype = params_data.dtype
    params_size_csum = params_data.size_csum
    buf_dtype = _communication_utility._get_nccl_type_id(dtype)
    n_threads = 128
    n_blocks = (n_elems + n_threads - 1) // n_threads
    if stream is None:
        stream = cp.cuda.get_current_stream()
    with stream:
        _cupy_batched_pack_params()(
            (n_blocks, ), (n_threads, ),
            (buffer.memory.ptr, buf_dtype, n_elems,
             params_dptr, params_dtype, params_size_csum, n_params))


</source>
<source file="systems/chainer-7.2.0/chainermn/communicators/_memory_utility.py" startline="271" endline="288" pcid="10170">
def _batched_unpack_params(params_data, buffer, dtype, stream=None):
    n_params = params_data.n_params
    n_elems = params_data.n_elems
    params_dptr = params_data.dptr
    params_dtype = params_data.dtype
    params_size_csum = params_data.size_csum
    buf_dtype = _communication_utility._get_nccl_type_id(dtype)
    n_threads = 128
    n_blocks = (n_elems + n_threads - 1) // n_threads
    if stream is None:
        stream = cp.cuda.get_current_stream()
    with stream:
        _cupy_batched_unpack_params()(
            (n_blocks, ), (n_threads, ),
            (buffer.memory.ptr, buf_dtype, n_elems,
             params_dptr, params_dtype, params_size_csum, n_params))


</source>
</class>

<class classid="246" nclones="2" nlines="11" similarity="81">
<source file="systems/chainer-7.2.0/chainermn/functions/batch_normalization.py" startline="11" endline="22" pcid="10178">
    def get_mean_and_var(self, axis, gamma, x, xp, interm_dtype):
        tmp = xp.empty(gamma.size * 2, dtype=gamma.dtype)
        x.mean(axis=axis, out=tmp[:gamma.size], dtype=gamma.dtype)
        xp.square(x).mean(axis=axis, out=tmp[gamma.size:], dtype=gamma.dtype)
        if xp is cuda.cupy:
            chainer.cuda.Stream.null.synchronize()
        self.comm._multi_node_mean(None, tmp)
        mean = tmp[:gamma.size]
        sqmean = tmp[gamma.size:]
        var = sqmean - xp.square(mean)
        return mean, var

</source>
<source file="systems/chainer-7.2.0/chainermn/functions/batch_normalization.py" startline="23" endline="34" pcid="10179">
    def get_ggamma_and_gbeta(self, axis, gamma, gy, x_hat, xp):
        tmp = xp.empty(gamma.size * 2, dtype=gamma.dtype)
        gy.sum(axis=axis, out=tmp[:gamma.size], dtype=gamma.dtype)
        (gy * x_hat).sum(axis=axis, out=tmp[gamma.size:], dtype=gamma.dtype)
        if xp is cuda.cupy:
            chainer.cuda.Stream.null.synchronize()
        self.comm._multi_node_mean(None, tmp)
        gbeta = tmp[:gamma.size]
        ggamma = tmp[gamma.size:]
        return gbeta, ggamma


</source>
</class>

<class classid="247" nclones="2" nlines="24" similarity="91">
<source file="systems/chainer-7.2.0/chainermn/functions/batch_normalization.py" startline="44" endline="69" pcid="10181">
    def get_mean_and_var(self, axis, gamma, x, xp, interm_dtype):
        gpu_buffer_n_elems = gamma.size * 2
        gpu_buffer_size = gamma.dtype.itemsize * gpu_buffer_n_elems
        gpu_buffer_a = self.memory_utility_module.DeviceMemory()
        gpu_buffer_b = self.memory_utility_module.DeviceMemory()
        gpu_buffer_a.assign(gpu_buffer_size)
        gpu_buffer_b.assign(gpu_buffer_size)
        gpu_buffer_a_array = gpu_buffer_a.array(
            gpu_buffer_n_elems, dtype=gamma.dtype)
        x.mean(axis=axis, out=gpu_buffer_a_array[:gamma.size],
               dtype=gamma.dtype)
        xp.square(x).mean(axis=axis, out=gpu_buffer_a_array[gamma.size:],
                          dtype=gamma.dtype)
        self.comm._multi_node_mean_nccl(gpu_buffer_a,
                                        gpu_buffer_b,
                                        gpu_buffer_n_elems,
                                        gamma.dtype)
        gpu_buffer_a_array = gpu_buffer_b.array(
            gpu_buffer_n_elems,
            dtype=gamma.dtype)

        mean = gpu_buffer_a_array[:gamma.size]
        sqmean = gpu_buffer_a_array[gamma.size:]
        var = sqmean - xp.square(mean)
        return mean, var

</source>
<source file="systems/chainer-7.2.0/chainermn/functions/batch_normalization.py" startline="70" endline="95" pcid="10182">
    def get_ggamma_and_gbeta(self, axis, gamma, gy, x_hat, xp):
        gpu_buffer_n_elems = gamma.size * 2
        gpu_buffer_size = gamma.dtype.itemsize * gpu_buffer_n_elems
        gpu_buffer_a = self.memory_utility_module.DeviceMemory()
        gpu_buffer_b = self.memory_utility_module.DeviceMemory()
        gpu_buffer_a.assign(gpu_buffer_size)
        gpu_buffer_b.assign(gpu_buffer_size)
        gpu_buffer_a_array = gpu_buffer_a.array(
            gpu_buffer_n_elems, dtype=gamma.dtype)
        gy.sum(axis=axis, out=gpu_buffer_a_array[:gamma.size],
               dtype=gamma.dtype)
        (gy * x_hat).sum(axis=axis, out=gpu_buffer_a_array[gamma.size:],
                         dtype=gamma.dtype)
        self.comm._multi_node_mean_nccl(gpu_buffer_a,
                                        gpu_buffer_b,
                                        gpu_buffer_n_elems,
                                        gamma.dtype)
        gpu_buffer_a_array = gpu_buffer_b.array(
            gpu_buffer_n_elems,
            dtype=gamma.dtype)

        gbeta = gpu_buffer_a_array[:gamma.size]
        ggamma = gpu_buffer_a_array[gamma.size:]
        return gbeta, ggamma


</source>
</class>

<class classid="248" nclones="2" nlines="89" similarity="70">
<source file="systems/chainer-7.2.0/examples/pix2pix/train_facade.py" startline="26" endline="137" pcid="10261">
def main():
    parser = argparse.ArgumentParser(
        description='chainer implementation of pix2pix')
    parser.add_argument('--batchsize', '-b', type=int, default=1,
                        help='Number of images in each mini-batch')
    parser.add_argument('--epoch', '-e', type=int, default=200,
                        help='Number of sweeps over the dataset to train')
    parser.add_argument('--device', '-d', type=str, default='-1',
                        help='Device specifier. Either ChainerX device '
                        'specifier or an integer. If non-negative integer, '
                        'CuPy arrays with specified device id are used. If '
                        'negative integer, NumPy arrays are used')
    parser.add_argument('--dataset', '-i', default='./facade/base',
                        help='Directory of image files.')
    parser.add_argument('--out', '-o', default='result',
                        help='Directory to output the result')
    parser.add_argument('--resume', '-r', type=str,
                        help='Resume the training from snapshot')
    parser.add_argument('--seed', type=int, default=0,
                        help='Random seed')
    parser.add_argument('--snapshot_interval', type=int, default=1000,
                        help='Interval of snapshot')
    parser.add_argument('--display_interval', type=int, default=100,
                        help='Interval of displaying log to console')
    group = parser.add_argument_group('deprecated arguments')
    group.add_argument('--gpu', '-g', dest='device',
                       type=int, nargs='?', const=0,
                       help='GPU ID (negative value indicates CPU)')
    args = parser.parse_args()

    if chainer.get_dtype() == numpy.float16:
        warnings.warn(
            'This example may cause NaN in FP16 mode.', RuntimeWarning)

    device = chainer.get_device(args.device)
    if device.xp is chainerx:
        sys.stderr.write('This example does not support ChainerX devices.\n')
        sys.exit(1)

    print('Device: {}'.format(device))
    print('# Minibatch-size: {}'.format(args.batchsize))
    print('# epoch: {}'.format(args.epoch))
    print('')

    device.use()

    # Set up a neural network to train
    enc = Encoder(in_ch=12)
    dec = Decoder(out_ch=3)
    dis = Discriminator(in_ch=12, out_ch=3)

    enc.to_device(device)
    dec.to_device(device)
    dis.to_device(device)

    # Setup an optimizer
    def make_optimizer(model, alpha=0.0002, beta1=0.5):
        optimizer = chainer.optimizers.Adam(alpha=alpha, beta1=beta1)
        optimizer.setup(model)
        optimizer.add_hook(chainer.optimizer.WeightDecay(0.00001), 'hook_dec')
        return optimizer
    opt_enc = make_optimizer(enc)
    opt_dec = make_optimizer(dec)
    opt_dis = make_optimizer(dis)

    train_d = FacadeDataset(args.dataset, data_range=(1, 300))
    test_d = FacadeDataset(args.dataset, data_range=(300, 379))
    train_iter = chainer.iterators.SerialIterator(train_d, args.batchsize)
    test_iter = chainer.iterators.SerialIterator(test_d, args.batchsize)

    # Set up a trainer
    updater = FacadeUpdater(
        models=(enc, dec, dis),
        iterator={
            'main': train_iter,
            'test': test_iter},
        optimizer={
            'enc': opt_enc, 'dec': opt_dec,
            'dis': opt_dis},
        device=device)
    trainer = training.Trainer(updater, (args.epoch, 'epoch'), out=args.out)

    snapshot_interval = (args.snapshot_interval, 'iteration')
    display_interval = (args.display_interval, 'iteration')
    trainer.extend(extensions.snapshot(
        filename='snapshot_iter_{.updater.iteration}.npz'),
        trigger=snapshot_interval)
    trainer.extend(extensions.snapshot_object(
        enc, 'enc_iter_{.updater.iteration}.npz'), trigger=snapshot_interval)
    trainer.extend(extensions.snapshot_object(
        dec, 'dec_iter_{.updater.iteration}.npz'), trigger=snapshot_interval)
    trainer.extend(extensions.snapshot_object(
        dis, 'dis_iter_{.updater.iteration}.npz'), trigger=snapshot_interval)
    trainer.extend(extensions.LogReport(trigger=display_interval))
    trainer.extend(extensions.PrintReport([
        'epoch', 'iteration', 'enc/loss', 'dec/loss', 'dis/loss',
    ]), trigger=display_interval)
    trainer.extend(extensions.ProgressBar(update_interval=10))
    trainer.extend(
        out_image(
            updater, enc, dec,
            5, 5, args.seed, args.out),
        trigger=snapshot_interval)

    if args.resume is not None:
        # Resume from a snapshot
        chainer.serializers.load_npz(args.resume, trainer)

    # Run the training
    trainer.run()


</source>
<source file="systems/chainer-7.2.0/examples/dcgan/train_dcgan.py" startline="18" endline="131" pcid="10594">
def main():
    parser = argparse.ArgumentParser(description='Chainer example: DCGAN')
    parser.add_argument('--batchsize', '-b', type=int, default=50,
                        help='Number of images in each mini-batch')
    parser.add_argument('--epoch', '-e', type=int, default=1000,
                        help='Number of sweeps over the dataset to train')
    parser.add_argument('--device', '-d', type=str, default='-1',
                        help='Device specifier. Either ChainerX device '
                        'specifier or an integer. If non-negative integer, '
                        'CuPy arrays with specified device id are used. If '
                        'negative integer, NumPy arrays are used')
    parser.add_argument('--dataset', '-i', default='',
                        help='Directory of image files.  Default is cifar-10.')
    parser.add_argument('--out', '-o', default='result',
                        help='Directory to output the result')
    parser.add_argument('--resume', '-r', type=str,
                        help='Resume the training from snapshot')
    parser.add_argument('--n_hidden', '-n', type=int, default=100,
                        help='Number of hidden units (z)')
    parser.add_argument('--seed', type=int, default=0,
                        help='Random seed of z at visualization stage')
    parser.add_argument('--snapshot_interval', type=int, default=1000,
                        help='Interval of snapshot')
    parser.add_argument('--display_interval', type=int, default=100,
                        help='Interval of displaying log to console')
    group = parser.add_argument_group('deprecated arguments')
    group.add_argument('--gpu', '-g', dest='device',
                       type=int, nargs='?', const=0,
                       help='GPU ID (negative value indicates CPU)')
    args = parser.parse_args()

    if chainer.get_dtype() == numpy.float16:
        warnings.warn(
            'This example may cause NaN in FP16 mode.', RuntimeWarning)

    device = chainer.get_device(args.device)
    device.use()

    print('Device: {}'.format(device))
    print('# Minibatch-size: {}'.format(args.batchsize))
    print('# n_hidden: {}'.format(args.n_hidden))
    print('# epoch: {}'.format(args.epoch))
    print('')

    # Set up a neural network to train
    gen = Generator(n_hidden=args.n_hidden)
    dis = Discriminator()

    gen.to_device(device)  # Copy the model to the device
    dis.to_device(device)

    # Setup an optimizer
    def make_optimizer(model, alpha=0.0002, beta1=0.5):
        optimizer = chainer.optimizers.Adam(alpha=alpha, beta1=beta1)
        optimizer.setup(model)
        optimizer.add_hook(
            chainer.optimizer_hooks.WeightDecay(0.0001), 'hook_dec')
        return optimizer

    opt_gen = make_optimizer(gen)
    opt_dis = make_optimizer(dis)

    if args.dataset == '':
        # Load the CIFAR10 dataset if args.dataset is not specified
        train, _ = chainer.datasets.get_cifar10(withlabel=False, scale=255.)
    else:
        all_files = os.listdir(args.dataset)
        image_files = [f for f in all_files if ('png' in f or 'jpg' in f)]
        print('{} contains {} image files'
              .format(args.dataset, len(image_files)))
        train = chainer.datasets\
            .ImageDataset(paths=image_files, root=args.dataset)

    # Setup an iterator
    train_iter = chainer.iterators.SerialIterator(train, args.batchsize)

    # Setup an updater
    updater = DCGANUpdater(
        models=(gen, dis),
        iterator=train_iter,
        optimizer={
            'gen': opt_gen, 'dis': opt_dis},
        device=device)

    # Setup a trainer
    trainer = training.Trainer(updater, (args.epoch, 'epoch'), out=args.out)

    snapshot_interval = (args.snapshot_interval, 'iteration')
    display_interval = (args.display_interval, 'iteration')
    trainer.extend(
        extensions.snapshot(filename='snapshot_iter_{.updater.iteration}.npz'),
        trigger=snapshot_interval)
    trainer.extend(extensions.snapshot_object(
        gen, 'gen_iter_{.updater.iteration}.npz'), trigger=snapshot_interval)
    trainer.extend(extensions.snapshot_object(
        dis, 'dis_iter_{.updater.iteration}.npz'), trigger=snapshot_interval)
    trainer.extend(extensions.LogReport(trigger=display_interval))
    trainer.extend(extensions.PrintReport([
        'epoch', 'iteration', 'gen/loss', 'dis/loss',
    ]), trigger=display_interval)
    trainer.extend(extensions.ProgressBar(update_interval=10))
    trainer.extend(
        out_generated_image(
            gen, dis,
            10, 10, args.seed, args.out),
        trigger=snapshot_interval)

    if args.resume is not None:
        # Resume from a snapshot
        chainer.serializers.load_npz(args.resume, trainer)

    # Run the training
    trainer.run()

</source>
</class>

<class classid="249" nclones="2" nlines="13" similarity="100">
<source file="systems/chainer-7.2.0/examples/ptb/train_ptb.py" startline="82" endline="105" pcid="10315">
    def __next__(self):
        # This iterator returns a list representing a mini-batch. Each item
        # indicates a different position in the original sequence. Each item is
        # represented by a pair of two word IDs. The first word is at the
        # "current" position, while the second word at the next position.
        # At each iteration, the iteration count is incremented, which pushes
        # forward the "current" position.
        length = len(self.dataset)
        if not self.repeat and self.iteration * self.batch_size >= length:
            # If not self.repeat, this iterator stops at the end of the first
            # epoch (i.e., when all words are visited once).
            raise StopIteration
        cur_words = self.get_words()
        self._previous_epoch_detail = self.epoch_detail
        self.iteration += 1
        next_words = self.get_words()

        epoch = self.iteration * self.batch_size // length
        self.is_new_epoch = self.epoch < epoch
        if self.is_new_epoch:
            self.epoch = epoch

        return list(zip(cur_words, next_words))

</source>
<source file="systems/chainer-7.2.0/examples/static_graph_optimizations/ptb/train_ptb_custom_loop.py" startline="120" endline="143" pcid="10491">
    def __next__(self):
        # This iterator returns a list representing a mini-batch. Each item
        # indicates a different position in the original sequence. Each item is
        # represented by a pair of two word IDs. The first word is at the
        # "current" position, while the second word at the next position.
        # At each iteration, the iteration count is incremented, which pushes
        # forward the "current" position.
        length = len(self.dataset)
        if not self.repeat and self.iteration * self.batch_size >= length:
            # If not self.repeat, this iterator stops at the end of the first
            # epoch (i.e., when all words are visited once).
            raise StopIteration
        cur_words = self.get_words()
        self._previous_epoch_detail = self.epoch_detail
        self.iteration += 1
        next_words = self.get_words()

        epoch = self.iteration * self.batch_size // length
        self.is_new_epoch = self.epoch < epoch
        if self.is_new_epoch:
            self.epoch = epoch

        return list(zip(cur_words, next_words))

</source>
</class>

<class classid="250" nclones="2" nlines="56" similarity="87">
<source file="systems/chainer-7.2.0/examples/mnist/train_mnist_data_parallel.py" startline="13" endline="96" pcid="10326">
def main():
    # This script is almost identical to train_mnist.py. The only difference is
    # that this script uses data-parallel computation on two GPUs.
    # See train_mnist.py for more details.
    parser = argparse.ArgumentParser(description='Chainer example: MNIST')
    parser.add_argument('--batchsize', '-b', type=int, default=400,
                        help='Number of images in each mini-batch')
    parser.add_argument('--epoch', '-e', type=int, default=20,
                        help='Number of sweeps over the dataset to train')
    parser.add_argument('--out', '-o', default='result_data_parallel',
                        help='Directory to output the result')
    parser.add_argument('--resume', '-r', default='',
                        help='Resume the training from snapshot')
    parser.add_argument('--unit', '-u', type=int, default=1000,
                        help='Number of units')
    parser.add_argument('--device0', '-d', type=str, default='0',
                        help='Device specifier of the first device. '
                        'Either ChainerX device '
                        'specifier or an integer. If non-negative integer, '
                        'CuPy arrays with specified device id are used. If '
                        'negative integer, NumPy arrays are used')
    parser.add_argument('--device1', '-D', type=str, default='1',
                        help='Device specifier of the second device. '
                        'Either ChainerX device '
                        'specifier or an integer. If non-negative integer, '
                        'CuPy arrays with specified device id are used. If '
                        'negative integer, NumPy arrays are used')
    group = parser.add_argument_group('deprecated arguments')
    group.add_argument('--gpu0', '-g', dest='device0', type=int, nargs='?',
                       const=0,
                       help='First GPU ID')
    group.add_argument('--gpu1', '-G', dest='device1', type=int, nargs='?',
                       const=1,
                       help='Second GPU ID')
    args = parser.parse_args()
    device0 = chainer.get_device(args.device0)
    device1 = chainer.get_device(args.device1)

    print('Devices: {}, {}'.format(device0, device1))
    print('# unit: {}'.format(args.unit))
    print('# Minibatch-size: {}'.format(args.batchsize))
    print('# epoch: {}'.format(args.epoch))
    print('')

    device0.use()

    model = L.Classifier(train_mnist.MLP(args.unit, 10))
    optimizer = chainer.optimizers.Adam()
    optimizer.setup(model)

    train, test = chainer.datasets.get_mnist()
    train_iter = chainer.iterators.SerialIterator(train, args.batchsize)
    test_iter = chainer.iterators.SerialIterator(test, args.batchsize,
                                                 repeat=False, shuffle=False)

    # ParallelUpdater implements the data-parallel gradient computation on
    # multiple devices. It accepts "devices" argument that specifies which
    # device to use.
    updater = training.updaters.ParallelUpdater(
        train_iter,
        optimizer,
        # The device of the name 'main' is used as a "master", while others are
        # used as slaves. Names other than 'main' are arbitrary.
        devices={'main': device0, 'second': device1},
    )
    trainer = training.Trainer(updater, (args.epoch, 'epoch'), out=args.out)

    trainer.extend(extensions.Evaluator(test_iter, model, device=device0))
    # TODO(niboshi): Temporarily disabled for chainerx. Fix it.
    if device0.xp is not chainerx:
        trainer.extend(extensions.DumpGraph('main/loss'))
    trainer.extend(extensions.snapshot(), trigger=(args.epoch, 'epoch'))
    trainer.extend(extensions.LogReport())
    trainer.extend(extensions.PrintReport(
        ['epoch', 'main/loss', 'validation/main/loss',
         'main/accuracy', 'validation/main/accuracy', 'elapsed_time']))
    trainer.extend(extensions.ProgressBar())

    if args.resume:
        chainer.serializers.load_npz(args.resume, trainer)

    trainer.run()


</source>
<source file="systems/chainer-7.2.0/examples/mnist/train_mnist_model_parallel.py" startline="64" endline="139" pcid="10329">
def main():
    parser = argparse.ArgumentParser(description='Chainer example: MNIST')
    parser.add_argument('--batchsize', '-b', type=int, default=100,
                        help='Number of images in each mini-batch')
    parser.add_argument('--epoch', '-e', default=20, type=int,
                        help='Number of sweeps over the dataset to train')
    parser.add_argument('--out', '-o', default='result_model_parallel',
                        help='Directory to output the result')
    parser.add_argument('--resume', '-r', default='',
                        help='Resume the training from snapshot')
    parser.add_argument('--unit', '-u', default=1000, type=int,
                        help='Number of units')
    parser.add_argument('--device0', '-d', type=str, default='0',
                        help='Device specifier of the first device. '
                        'Either ChainerX device '
                        'specifier or an integer. If non-negative integer, '
                        'CuPy arrays with specified device id are used. If '
                        'negative integer, NumPy arrays are used')
    parser.add_argument('--device1', '-D', type=str, default='1',
                        help='Device specifier of the second device. '
                        'Either ChainerX device '
                        'specifier or an integer. If non-negative integer, '
                        'CuPy arrays with specified device id are used. If '
                        'negative integer, NumPy arrays are used')
    group = parser.add_argument_group('deprecated arguments')
    group.add_argument('--gpu0', '-g', dest='device0', type=int, nargs='?',
                       const=0,
                       help='First GPU ID')
    group.add_argument('--gpu1', '-G', dest='device1', type=int, nargs='?',
                       const=1,
                       help='Second GPU ID')
    args = parser.parse_args()
    device0 = chainer.get_device(args.device0)
    device1 = chainer.get_device(args.device1)

    print('Devices: {}, {}'.format(device0, device1))
    print('# unit: {}'.format(args.unit))
    print('# Minibatch-size: {}'.format(args.batchsize))
    print('# epoch: {}'.format(args.epoch))
    print('')

    # See train_mnist.py for the meaning of these lines

    model = L.Classifier(ParallelMLP(args.unit, 10, device0, device1))
    device0.use()

    optimizer = chainer.optimizers.Adam()
    optimizer.setup(model)

    train, test = chainer.datasets.get_mnist()

    train_iter = chainer.iterators.SerialIterator(train, args.batchsize)
    test_iter = chainer.iterators.SerialIterator(test, args.batchsize,
                                                 repeat=False, shuffle=False)

    updater = training.updaters.StandardUpdater(
        train_iter, optimizer, input_device=device0)
    trainer = training.Trainer(updater, (args.epoch, 'epoch'), out=args.out)

    trainer.extend(extensions.Evaluator(test_iter, model, device=device0))
    # TODO(niboshi): Temporarily disabled for chainerx. Fix it.
    if device0.xp is not chainerx:
        trainer.extend(extensions.DumpGraph('main/loss'))
    trainer.extend(extensions.snapshot(), trigger=(args.epoch, 'epoch'))
    trainer.extend(extensions.LogReport())
    trainer.extend(extensions.PrintReport(
        ['epoch', 'main/loss', 'validation/main/loss',
         'main/accuracy', 'validation/main/accuracy', 'elapsed_time']))
    trainer.extend(extensions.ProgressBar())

    if args.resume:
        chainer.serializers.load_npz(args.resume, trainer)

    trainer.run()


</source>
</class>

<class classid="251" nclones="2" nlines="67" similarity="98">
<source file="systems/chainer-7.2.0/examples/mnist/.testdata/replacements/train_mnist.py" startline="32" endline="148" pcid="10333">
def main():
    parser = argparse.ArgumentParser(description='Chainer example: MNIST')
    parser.add_argument('--batchsize', '-b', type=int, default=100,
                        help='Number of images in each mini-batch')
    parser.add_argument('--epoch', '-e', type=int, default=20,
                        help='Number of sweeps over the dataset to train')
    parser.add_argument('--frequency', '-f', type=int, default=-1,
                        help='Frequency of taking a snapshot')
    parser.add_argument('--device', '-d', type=str, default='-1',
                        help='Device specifier. Either ChainerX device '
                        'specifier or an integer. If non-negative integer, '
                        'CuPy arrays with specified device id are used. If '
                        'negative integer, NumPy arrays are used')
    parser.add_argument('--out', '-o', default='result',
                        help='Directory to output the result')
    parser.add_argument('--resume', '-r', type=str,
                        help='Resume the training from snapshot')
    parser.add_argument('--autoload', action='store_true',
                        help='Automatically load trainer snapshots in case'
                        ' of preemption or other temporary system failure')
    parser.add_argument('--unit', '-u', type=int, default=1000,
                        help='Number of units')
    group = parser.add_argument_group('deprecated arguments')
    group.add_argument('--gpu', '-g', dest='device',
                       type=int, nargs='?', const=0,
                       help='GPU ID (negative value indicates CPU)')
    args = parser.parse_args()

    device = chainer.get_device(args.device)

    print('Device: {}'.format(device))
    print('# unit: {}'.format(args.unit))
    print('# Minibatch-size: {}'.format(args.batchsize))
    print('# epoch: {}'.format(args.epoch))
    print('')

    # Set up a neural network to train
    # Classifier reports softmax cross entropy loss and accuracy at every
    # iteration, which will be used by the PrintReport extension below.
    model = L.Classifier(MLP(args.unit, 10))
    model.to_device(device)
    device.use()

    # Setup an optimizer
    optimizer = chainer.optimizers.Adam()
    optimizer.setup(model)

    # Load the MNIST dataset
    train, test = chainer.datasets.get_mnist()

    train_iter = chainer.iterators.SerialIterator(train, args.batchsize)
    test_iter = chainer.iterators.SerialIterator(test, args.batchsize,
                                                 repeat=False, shuffle=False)

    # Set up a trainer
    updater = training.updaters.StandardUpdater(
        train_iter, optimizer, device=device)
    trainer = training.Trainer(updater, (args.epoch, 'epoch'), out=args.out)

    # Evaluate the model with the test dataset for each epoch
    trainer.extend(extensions.Evaluator(test_iter, model, device=device),
                   call_before_training=True)

    # Dump a computational graph from 'loss' variable at the first iteration
    # The "main" refers to the target link of the "main" optimizer.
    # TODO(niboshi): Temporarily disabled for chainerx. Fix it.
    if device.xp is not chainerx:
        trainer.extend(extensions.DumpGraph('main/loss'))

    # Take a snapshot for each specified epoch
    frequency = args.epoch if args.frequency == -1 else max(1, args.frequency)
    # Take a snapshot each ``frequency`` epoch, delete old stale
    # snapshots and automatically load from snapshot files if any
    # files are already resident at result directory.
    trainer.extend(extensions.snapshot(n_retains=1, autoload=args.autoload),
                   trigger=(frequency, 'epoch'))

    # Write a log of evaluation statistics for each epoch
    trainer.extend(extensions.LogReport(), call_before_training=True)

    # Save two plot images to the result dir
    trainer.extend(
        extensions.PlotReport(['main/loss', 'validation/main/loss'],
                              'epoch', file_name='loss.png'),
        call_before_training=True)
    trainer.extend(
        extensions.PlotReport(
            ['main/accuracy', 'validation/main/accuracy'],
            'epoch', file_name='accuracy.png'),
        call_before_training=True)

    # Print selected entries of the log to stdout
    # Here "main" refers to the target link of the "main" optimizer again, and
    # "validation" refers to the default name of the Evaluator extension.
    # Entries other than 'epoch' are reported by the Classifier link, called by
    # either the updater or the evaluator.
    trainer.extend(extensions.PrintReport(
        ['epoch', 'main/loss', 'validation/main/loss',
         'main/accuracy', 'validation/main/accuracy', 'elapsed_time']),
        call_before_training=True)

    # Print a progress bar to stdout
    trainer.extend(extensions.ProgressBar())

    if args.resume is not None:
        # Resume from a snapshot (Note: this loaded model is to be
        # overwritten by --autoload option, autoloading snapshots, if
        # any snapshots exist in output directory)
        chainer.serializers.load_npz(args.resume, trainer)

    # BEGIN ADDITIONAL TEST CODE
    del trainer._extensions['ProgressBar']
    # END ADDITIONAL TEST CODE
    # Run the training
    trainer.run()


</source>
<source file="systems/chainer-7.2.0/examples/mnist/train_mnist.py" startline="32" endline="145" pcid="10336">
def main():
    parser = argparse.ArgumentParser(description='Chainer example: MNIST')
    parser.add_argument('--batchsize', '-b', type=int, default=100,
                        help='Number of images in each mini-batch')
    parser.add_argument('--epoch', '-e', type=int, default=20,
                        help='Number of sweeps over the dataset to train')
    parser.add_argument('--frequency', '-f', type=int, default=-1,
                        help='Frequency of taking a snapshot')
    parser.add_argument('--device', '-d', type=str, default='-1',
                        help='Device specifier. Either ChainerX device '
                        'specifier or an integer. If non-negative integer, '
                        'CuPy arrays with specified device id are used. If '
                        'negative integer, NumPy arrays are used')
    parser.add_argument('--out', '-o', default='result',
                        help='Directory to output the result')
    parser.add_argument('--resume', '-r', type=str,
                        help='Resume the training from snapshot')
    parser.add_argument('--autoload', action='store_true',
                        help='Automatically load trainer snapshots in case'
                        ' of preemption or other temporary system failure')
    parser.add_argument('--unit', '-u', type=int, default=1000,
                        help='Number of units')
    group = parser.add_argument_group('deprecated arguments')
    group.add_argument('--gpu', '-g', dest='device',
                       type=int, nargs='?', const=0,
                       help='GPU ID (negative value indicates CPU)')
    args = parser.parse_args()

    device = chainer.get_device(args.device)

    print('Device: {}'.format(device))
    print('# unit: {}'.format(args.unit))
    print('# Minibatch-size: {}'.format(args.batchsize))
    print('# epoch: {}'.format(args.epoch))
    print('')

    # Set up a neural network to train
    # Classifier reports softmax cross entropy loss and accuracy at every
    # iteration, which will be used by the PrintReport extension below.
    model = L.Classifier(MLP(args.unit, 10))
    model.to_device(device)
    device.use()

    # Setup an optimizer
    optimizer = chainer.optimizers.Adam()
    optimizer.setup(model)

    # Load the MNIST dataset
    train, test = chainer.datasets.get_mnist()

    train_iter = chainer.iterators.SerialIterator(train, args.batchsize)
    test_iter = chainer.iterators.SerialIterator(test, args.batchsize,
                                                 repeat=False, shuffle=False)

    # Set up a trainer
    updater = training.updaters.StandardUpdater(
        train_iter, optimizer, device=device)
    trainer = training.Trainer(updater, (args.epoch, 'epoch'), out=args.out)

    # Evaluate the model with the test dataset for each epoch
    trainer.extend(extensions.Evaluator(test_iter, model, device=device),
                   call_before_training=True)

    # Dump a computational graph from 'loss' variable at the first iteration
    # The "main" refers to the target link of the "main" optimizer.
    # TODO(niboshi): Temporarily disabled for chainerx. Fix it.
    if device.xp is not chainerx:
        trainer.extend(extensions.DumpGraph('main/loss'))

    # Take a snapshot for each specified epoch
    frequency = args.epoch if args.frequency == -1 else max(1, args.frequency)
    # Take a snapshot each ``frequency`` epoch, delete old stale
    # snapshots and automatically load from snapshot files if any
    # files are already resident at result directory.
    trainer.extend(extensions.snapshot(n_retains=1, autoload=args.autoload),
                   trigger=(frequency, 'epoch'))

    # Write a log of evaluation statistics for each epoch
    trainer.extend(extensions.LogReport(), call_before_training=True)

    # Save two plot images to the result dir
    trainer.extend(
        extensions.PlotReport(['main/loss', 'validation/main/loss'],
                              'epoch', file_name='loss.png'),
        call_before_training=True)
    trainer.extend(
        extensions.PlotReport(
            ['main/accuracy', 'validation/main/accuracy'],
            'epoch', file_name='accuracy.png'),
        call_before_training=True)

    # Print selected entries of the log to stdout
    # Here "main" refers to the target link of the "main" optimizer again, and
    # "validation" refers to the default name of the Evaluator extension.
    # Entries other than 'epoch' are reported by the Classifier link, called by
    # either the updater or the evaluator.
    trainer.extend(extensions.PrintReport(
        ['epoch', 'main/loss', 'validation/main/loss',
         'main/accuracy', 'validation/main/accuracy', 'elapsed_time']),
        call_before_training=True)

    # Print a progress bar to stdout
    trainer.extend(extensions.ProgressBar())

    if args.resume is not None:
        # Resume from a snapshot (Note: this loaded model is to be
        # overwritten by --autoload option, autoloading snapshots, if
        # any snapshots exist in output directory)
        chainer.serializers.load_npz(args.resume, trainer)

    # Run the training
    trainer.run()


</source>
</class>

<class classid="252" nclones="2" nlines="89" similarity="71">
<source file="systems/chainer-7.2.0/examples/mnist/train_mnist_custom_loop.py" startline="22" endline="130" pcid="10337">
def main():
    parser = argparse.ArgumentParser(description='Chainer example: MNIST')
    parser.add_argument('--batchsize', '-b', type=int, default=100,
                        help='Number of images in each mini-batch')
    parser.add_argument('--epoch', '-e', type=int, default=20,
                        help='Number of sweeps over the dataset to train')
    parser.add_argument('--device', '-d', type=str, default='-1',
                        help='Device specifier. Either ChainerX device '
                        'specifier or an integer. If non-negative integer, '
                        'CuPy arrays with specified device id are used. If '
                        'negative integer, NumPy arrays are used')
    parser.add_argument('--out', '-o', default='result',
                        help='Directory to output the result')
    parser.add_argument('--resume', '-r', type=str,
                        help='Resume the training from snapshot using model '
                             'and state files in the specified directory')
    parser.add_argument('--unit', '-u', type=int, default=1000,
                        help='Number of units')
    group = parser.add_argument_group('deprecated arguments')
    group.add_argument('--gpu', '-g', dest='device',
                       type=int, nargs='?', const=0,
                       help='GPU ID (negative value indicates CPU)')
    args = parser.parse_args()

    device = chainer.get_device(args.device)

    print('Device: {}'.format(device))
    print('# unit: {}'.format(args.unit))
    print('# Minibatch-size: {}'.format(args.batchsize))
    print('# epoch: {}'.format(args.epoch))
    print('')

    # Set up a neural network to train
    model = L.Classifier(train_mnist.MLP(args.unit, 10))
    model.to_device(device)
    device.use()

    # Setup an optimizer
    optimizer = chainer.optimizers.Adam()
    optimizer.setup(model)

    if args.resume is not None:
        # Resume from a snapshot
        resume = args.resume
        if os.path.exists(resume):
            serializers.load_npz(os.path.join(resume, 'mlp.model'), model)
            serializers.load_npz(os.path.join(resume, 'mlp.state'), optimizer)
        else:
            raise ValueError(
                '`args.resume` ("{}") is specified,'
                ' but it does not exist'.format(resume)
            )

    # Load the MNIST dataset
    train, test = chainer.datasets.get_mnist()

    test_count = len(test)

    train_iter = chainer.iterators.SerialIterator(train, args.batchsize)
    test_iter = chainer.iterators.SerialIterator(test, args.batchsize,
                                                 repeat=False, shuffle=False)

    train_count = 0
    sum_accuracy = 0
    sum_loss = 0

    while train_iter.epoch < args.epoch:
        batch = train_iter.next()
        x, t = convert.concat_examples(batch, device)
        optimizer.update(model, x, t)
        train_count += len(t)
        sum_loss += float(model.loss.array) * len(t)
        sum_accuracy += float(model.accuracy.array) * len(t)

        if train_iter.is_new_epoch:
            print('epoch: {}'.format(train_iter.epoch))
            print('train mean loss: {}, accuracy: {}'.format(
                sum_loss / train_count, sum_accuracy / train_count))
            # evaluation
            train_count = 0
            sum_accuracy = 0
            sum_loss = 0
            # Enable evaluation mode.
            with configuration.using_config('train', False):
                # This is optional but can reduce computational overhead.
                with chainer.using_config('enable_backprop', False):
                    for batch in test_iter:
                        x, t = convert.concat_examples(batch, device)
                        loss = model(x, t)
                        sum_loss += float(loss.array) * len(t)
                        sum_accuracy += float(
                            model.accuracy.array) * len(t)

            test_iter.reset()
            print('test mean  loss: {}, accuracy: {}'.format(
                sum_loss / test_count, sum_accuracy / test_count))
            sum_accuracy = 0
            sum_loss = 0

    # Save the model and the optimizer
    out = args.out
    if not os.path.isdir(out):
        os.makedirs(out)
    print('save the model')
    serializers.save_npz(os.path.join(out, 'mlp.model'), model)
    print('save the optimizer')
    serializers.save_npz(os.path.join(out, 'mlp.state'), optimizer)


</source>
<source file="systems/chainer-7.2.0/examples/cifar/train_cifar_custom_loop.py" startline="24" endline="150" pcid="10357">
def main():
    parser = argparse.ArgumentParser(description='Chainer CIFAR example:')
    parser.add_argument('--dataset', default='cifar10',
                        help='The dataset to use: cifar10 or cifar100')
    parser.add_argument('--batchsize', '-b', type=int, default=64,
                        help='Number of images in each mini-batch')
    parser.add_argument('--learnrate', '-l', type=float, default=0.05,
                        help='Learning rate for SGD')
    parser.add_argument('--epoch', '-e', type=int, default=300,
                        help='Number of sweeps over the dataset to train')
    parser.add_argument('--device', '-d', type=str, default='-1',
                        help='Device specifier. Either ChainerX device '
                        'specifier or an integer. If non-negative integer, '
                        'CuPy arrays with specified device id are used. If '
                        'negative integer, NumPy arrays are used')
    parser.add_argument('--out', '-o', default='result',
                        help='Directory to output the result')
    parser.add_argument('--test', action='store_true',
                        help='Use tiny datasets for quick tests')
    parser.add_argument('--resume', '-r', type=str,
                        help='Directory that has `vgg.model` and `vgg.state`')
    group = parser.add_argument_group('deprecated arguments')
    group.add_argument('--gpu', '-g', dest='device',
                       type=int, nargs='?', const=0,
                       help='GPU ID (negative value indicates CPU)')
    args = parser.parse_args()

    device = chainer.get_device(args.device)
    device.use()

    print('Device: {}'.format(device))
    print('# Minibatch-size: {}'.format(args.batchsize))
    print('# epoch: {}'.format(args.epoch))
    print('')

    # Set up a neural network to train.
    # Classifier reports softmax cross entropy loss and accuracy at every
    # iteration, which will be used by the PrintReport extension below.
    if args.dataset == 'cifar10':
        print('Using CIFAR10 dataset.')
        class_labels = 10
        train, test = get_cifar10()
    elif args.dataset == 'cifar100':
        print('Using CIFAR100 dataset.')
        class_labels = 100
        train, test = get_cifar100()
    else:
        raise RuntimeError('Invalid dataset choice.')

    if args.test:
        train = train[:200]
        test = test[:200]

    test_count = len(test)

    model = L.Classifier(models.VGG.VGG(class_labels))
    model.to_device(device)

    optimizer = chainer.optimizers.MomentumSGD(args.learnrate)
    optimizer.setup(model)
    optimizer.add_hook(chainer.optimizer.WeightDecay(5e-4))

    if args.resume is not None:
        resume = args.resume
        if os.path.exists(resume):
            serializers.load_npz(os.path.join(resume, 'vgg.model'), model)
            serializers.load_npz(os.path.join(resume, 'vgg.state'), optimizer)
        else:
            raise ValueError(
                '`args.resume` ("{}") is specified,'
                ' but it does not exist.'.format(resume)
            )

    train_iter = chainer.iterators.SerialIterator(train, args.batchsize)
    test_iter = chainer.iterators.SerialIterator(test, args.batchsize,
                                                 repeat=False, shuffle=False)

    train_count = 0
    sum_acc = 0
    sum_loss = 0

    while train_iter.epoch < args.epoch:
        batch = train_iter.next()
        # Reduce learning rate by 0.5 every 25 epochs.
        if train_iter.epoch % 25 == 0 and train_iter.is_new_epoch:
            optimizer.lr *= 0.5
            print('Reducing learning rate to: {}'.format(optimizer.lr))

        x, t = convert.concat_examples(batch, device)
        optimizer.update(model, x, t)
        train_count += len(t)
        sum_loss += float(model.loss.array) * len(t)
        sum_acc += float(model.accuracy.array) * len(t)

        if train_iter.is_new_epoch:
            print('epoch: {}'.format(train_iter.epoch))
            print('train mean loss: {}, accuracy: {}'.format(
                sum_loss / train_count, sum_acc / train_count))
            train_count = 0
            sum_acc = 0
            sum_loss = 0
            # Enable evaluation mode.
            with configuration.using_config('train', False):
                # This is optional but can reduce computational overhead.
                with chainer.using_config('enable_backprop', False):
                    for batch in test_iter:
                        x, t = convert.concat_examples(batch, device)
                        loss = model(x, t)
                        sum_loss += float(loss.array) * len(t)
                        sum_acc += float(model.accuracy.array) * len(t)

            test_iter.reset()
            print('test mean  loss: {}, accuracy: {}'.format(
                sum_loss / test_count, sum_acc / test_count))
            sum_acc = 0
            sum_loss = 0

    # Save the model and the optimizer
    out = args.out
    if not os.path.exists(out):
        os.makedirs(out)
    print('save the model')
    serializers.save_npz(os.path.join(out, 'vgg.model'), model)
    print('save the optimizer')
    serializers.save_npz(os.path.join(out, 'vgg.state'), optimizer)


</source>
</class>

<class classid="253" nclones="2" nlines="74" similarity="84">
<source file="systems/chainer-7.2.0/examples/cifar/train_cifar.py" startline="16" endline="126" pcid="10356">
def main():
    parser = argparse.ArgumentParser(description='Chainer CIFAR example:')
    parser.add_argument('--dataset', default='cifar10',
                        help='The dataset to use: cifar10 or cifar100')
    parser.add_argument('--batchsize', '-b', type=int, default=64,
                        help='Number of images in each mini-batch')
    parser.add_argument('--learnrate', '-l', type=float, default=0.05,
                        help='Learning rate for SGD')
    parser.add_argument('--epoch', '-e', type=int, default=300,
                        help='Number of sweeps over the dataset to train')
    parser.add_argument('--device', '-d', type=str, default='-1',
                        help='Device specifier. Either ChainerX device '
                        'specifier or an integer. If non-negative integer, '
                        'CuPy arrays with specified device id are used. If '
                        'negative integer, NumPy arrays are used')
    parser.add_argument('--out', '-o', default='result',
                        help='Directory to output the result')
    parser.add_argument('--resume', '-r', default='',
                        help='Resume the training from snapshot')
    parser.add_argument('--early-stopping', type=str,
                        help='Metric to watch for early stopping')
    group = parser.add_argument_group('deprecated arguments')
    group.add_argument('--gpu', '-g', dest='device',
                       type=int, nargs='?', const=0,
                       help='GPU ID (negative value indicates CPU)')
    args = parser.parse_args()

    device = chainer.get_device(args.device)
    device.use()

    print('Device: {}'.format(device))
    print('# Minibatch-size: {}'.format(args.batchsize))
    print('# epoch: {}'.format(args.epoch))
    print('')

    # Set up a neural network to train.
    # Classifier reports softmax cross entropy loss and accuracy at every
    # iteration, which will be used by the PrintReport extension below.
    if args.dataset == 'cifar10':
        print('Using CIFAR10 dataset.')
        class_labels = 10
        train, test = get_cifar10()
    elif args.dataset == 'cifar100':
        print('Using CIFAR100 dataset.')
        class_labels = 100
        train, test = get_cifar100()
    else:
        raise RuntimeError('Invalid dataset choice.')
    model = L.Classifier(models.VGG.VGG(class_labels))
    model.to_device(device)

    optimizer = chainer.optimizers.MomentumSGD(args.learnrate)
    optimizer.setup(model)
    optimizer.add_hook(chainer.optimizer_hooks.WeightDecay(5e-4))

    train_iter = chainer.iterators.SerialIterator(train, args.batchsize)
    test_iter = chainer.iterators.SerialIterator(test, args.batchsize,
                                                 repeat=False, shuffle=False)

    stop_trigger = (args.epoch, 'epoch')
    # Early stopping option
    if args.early_stopping:
        stop_trigger = triggers.EarlyStoppingTrigger(
            monitor=args.early_stopping, verbose=True,
            max_trigger=(args.epoch, 'epoch'))

    # Set up a trainer
    updater = training.updaters.StandardUpdater(
        train_iter, optimizer, device=device)
    trainer = training.Trainer(updater, stop_trigger, out=args.out)

    # Evaluate the model with the test dataset for each epoch
    trainer.extend(extensions.Evaluator(test_iter, model, device=device))

    # Reduce the learning rate by half every 25 epochs.
    trainer.extend(extensions.ExponentialShift('lr', 0.5),
                   trigger=(25, 'epoch'))

    # Dump a computational graph from 'loss' variable at the first iteration
    # The "main" refers to the target link of the "main" optimizer.
    # TODO(imanishi): Support for ChainerX
    if not isinstance(device, backend.ChainerxDevice):
        trainer.extend(extensions.DumpGraph('main/loss'))

    # Take a snapshot at each epoch
    trainer.extend(extensions.snapshot(
        filename='snaphot_epoch_{.updater.epoch}'))

    # Write a log of evaluation statistics for each epoch
    trainer.extend(extensions.LogReport())

    # Print selected entries of the log to stdout
    # Here "main" refers to the target link of the "main" optimizer again, and
    # "validation" refers to the default name of the Evaluator extension.
    # Entries other than 'epoch' are reported by the Classifier link, called by
    # either the updater or the evaluator.
    trainer.extend(extensions.PrintReport(
        ['epoch', 'main/loss', 'validation/main/loss',
         'main/accuracy', 'validation/main/accuracy', 'elapsed_time']))

    # Print a progress bar to stdout
    trainer.extend(extensions.ProgressBar())

    if args.resume:
        # Resume from a snapshot
        chainer.serializers.load_npz(args.resume, trainer)

    # Run the training
    trainer.run()


</source>
<source file="systems/chainer-7.2.0/examples/static_graph_optimizations/cifar/train_cifar.py" startline="26" endline="147" pcid="10506">
def main():
    parser = argparse.ArgumentParser(description='Chainer CIFAR example:')
    parser.add_argument('--dataset', default='cifar10',
                        help='The dataset to use: cifar10 or cifar100')
    parser.add_argument('--batchsize', '-b', type=int, default=64,
                        help='Number of images in each mini-batch')
    parser.add_argument('--learnrate', '-l', type=float, default=0.05,
                        help='Learning rate for SGD')
    parser.add_argument('--epoch', '-e', type=int, default=300,
                        help='Number of sweeps over the dataset to train')
    parser.add_argument('--device', '-d', type=str, default='0',
                        help='Device specifier. Either ChainerX device '
                        'specifier or an integer. If non-negative integer, '
                        'CuPy arrays with specified device id are used. If '
                        'negative integer, NumPy arrays are used')
    parser.add_argument('--out', '-o', default='result',
                        help='Directory to output the result')
    parser.add_argument('--resume', '-r', default='',
                        help='Resume the training from snapshot')
    parser.add_argument('--early-stopping', type=str,
                        help='Metric to watch for early stopping')
    group = parser.add_argument_group('deprecated arguments')
    group.add_argument('--gpu', '-g', dest='device',
                       type=int, nargs='?', const=0,
                       help='GPU ID (negative value indicates CPU)')
    args = parser.parse_args()

    if chainer.get_dtype() == numpy.float16:
        warnings.warn(
            'This example may cause NaN in FP16 mode.', RuntimeWarning)

    device = chainer.get_device(args.device)

    print('Device: {}'.format(device))
    print('# Minibatch-size: {}'.format(args.batchsize))
    print('# epoch: {}'.format(args.epoch))
    print('')

    device.use()

    # Set up a neural network to train.
    # Classifier reports softmax cross entropy loss and accuracy at every
    # iteration, which will be used by the PrintReport extension below.
    if args.dataset == 'cifar10':
        print('Using CIFAR10 dataset.')
        class_labels = 10
        train, test = get_cifar10()
    elif args.dataset == 'cifar100':
        print('Using CIFAR100 dataset.')
        class_labels = 100
        train, test = get_cifar100()
    else:
        raise RuntimeError('Invalid dataset choice.')
    model = L.Classifier(models.VGG.VGG(class_labels))
    model.to_device(device)

    optimizer = chainer.optimizers.MomentumSGD(args.learnrate)
    optimizer.setup(model)
    optimizer.add_hook(chainer.optimizer_hooks.WeightDecay(5e-4))

    train_iter = chainer.iterators.SerialIterator(train, args.batchsize)
    test_iter = chainer.iterators.SerialIterator(test, args.batchsize,
                                                 repeat=False, shuffle=False)

    stop_trigger = (args.epoch, 'epoch')
    # Early stopping option
    if args.early_stopping:
        stop_trigger = triggers.EarlyStoppingTrigger(
            monitor=args.early_stopping, verbose=True,
            max_trigger=(args.epoch, 'epoch'))

    # Set up a trainer
    updater = training.updaters.StandardUpdater(
        train_iter, optimizer, device=device)
    trainer = training.Trainer(updater, stop_trigger, out=args.out)

    # Evaluate the model with the test dataset for each epoch
    trainer.extend(extensions.Evaluator(test_iter, model, device=device))

    # Reduce the learning rate by half every 25 epochs.
    trainer.extend(extensions.ExponentialShift('lr', 0.5),
                   trigger=(25, 'epoch'))

    # Dump a computational graph from 'loss' variable at the first iteration
    # The "main" refers to the target link of the "main" optimizer.
    # TODO(hvy): Support ChainerX
    if device.xp is not chainerx:
        trainer.extend(extensions.DumpGraph('main/loss'))

    # Take a snapshot at each epoch
    trainer.extend(extensions.snapshot(), trigger=(args.epoch, 'epoch'))

    # Write a log of evaluation statistics for each epoch
    trainer.extend(extensions.LogReport())

    # Print selected entries of the log to stdout
    # Here "main" refers to the target link of the "main" optimizer again, and
    # "validation" refers to the default name of the Evaluator extension.
    # Entries other than 'epoch' are reported by the Classifier link, called by
    # either the updater or the evaluator.
    trainer.extend(extensions.PrintReport(
        ['epoch', 'main/loss', 'validation/main/loss',
         'main/accuracy', 'validation/main/accuracy', 'elapsed_time']))

    # Print a progress bar to stdout
    trainer.extend(extensions.ProgressBar())

    if args.resume:
        # Resume from a snapshot
        chainer.serializers.load_npz(args.resume, trainer)

    # Run the training
    if device.xp is not chainerx:
        trainer.run()
    else:
        warnings.warn(
            'Static subgraph optimization does not support ChainerX and will'
            ' be disabled.', UserWarning)
        with chainer.using_config('use_static_graph', False):
            trainer.run()


</source>
</class>

<class classid="254" nclones="2" nlines="19" similarity="100">
<source file="systems/chainer-7.2.0/examples/cifar/models/VGG.py" startline="60" endline="79" pcid="10360">
    def __init__(self, class_labels=10):
        super(VGG, self).__init__()
        with self.init_scope():
            self.block1_1 = Block(64, 3)
            self.block1_2 = Block(64, 3)
            self.block2_1 = Block(128, 3)
            self.block2_2 = Block(128, 3)
            self.block3_1 = Block(256, 3)
            self.block3_2 = Block(256, 3)
            self.block3_3 = Block(256, 3)
            self.block4_1 = Block(512, 3)
            self.block4_2 = Block(512, 3)
            self.block4_3 = Block(512, 3)
            self.block5_1 = Block(512, 3)
            self.block5_2 = Block(512, 3)
            self.block5_3 = Block(512, 3)
            self.fc1 = L.Linear(None, 512, nobias=True)
            self.bn_fc1 = L.BatchNormalization(512)
            self.fc2 = L.Linear(None, class_labels, nobias=True)

</source>
<source file="systems/chainer-7.2.0/examples/static_graph_optimizations/cifar/models/VGG.py" startline="61" endline="80" pcid="10511">
    def __init__(self, class_labels=10):
        super(VGG, self).__init__()
        with self.init_scope():
            self.block1_1 = Block(64, 3)
            self.block1_2 = Block(64, 3)
            self.block2_1 = Block(128, 3)
            self.block2_2 = Block(128, 3)
            self.block3_1 = Block(256, 3)
            self.block3_2 = Block(256, 3)
            self.block3_3 = Block(256, 3)
            self.block4_1 = Block(512, 3)
            self.block4_2 = Block(512, 3)
            self.block4_3 = Block(512, 3)
            self.block5_1 = Block(512, 3)
            self.block5_2 = Block(512, 3)
            self.block5_3 = Block(512, 3)
            self.fc1 = L.Linear(None, 512, nobias=True)
            self.bn_fc1 = L.BatchNormalization(512)
            self.fc2 = L.Linear(None, class_labels, nobias=True)

</source>
</class>

<class classid="255" nclones="4" nlines="33" similarity="94">
<source file="systems/chainer-7.2.0/examples/cifar/models/VGG.py" startline="80" endline="122" pcid="10361">
    def forward(self, x):
        # 64 channel blocks:
        h = self.block1_1(x)
        h = F.dropout(h, ratio=0.3)
        h = self.block1_2(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 128 channel blocks:
        h = self.block2_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block2_2(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 256 channel blocks:
        h = self.block3_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block3_2(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block3_3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 512 channel blocks:
        h = self.block4_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block4_2(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block4_3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 512 channel blocks:
        h = self.block5_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block5_2(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block5_3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        h = F.dropout(h, ratio=0.5)
        h = self.fc1(h)
        h = self.bn_fc1(h)
        h = F.relu(h)
        h = F.dropout(h, ratio=0.5)
        return self.fc2(h)
</source>
<source file="systems/chainer-7.2.0/examples/chainermn/parallel_convolution/VGG.py" startline="133" endline="177" pcid="10433">
    def __call__(self, x):
        # 64 channel blocks:
        h = self.block1_1(x)
        h = F.dropout(h, ratio=0.3)
        h = self.block1_2(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 128 channel blocks:
        h = self.block2_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block2_2(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 256 channel blocks:
        h = self.block3_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block3_2(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block3_3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 512 channel blocks:
        h = self.block4_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block4_2(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block4_3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 512 channel blocks:
        h = self.block5_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block5_2(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block5_3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        h = F.dropout(h, ratio=0.5)
        h = self.fc1(h)
        h = self.bn_fc1(h)
        h = F.relu(h)
        h = F.dropout(h, ratio=0.5)
        h = self.fc2(h)

        return h
</source>
<source file="systems/chainer-7.2.0/examples/chainermn/cifar/models/VGG.py" startline="93" endline="135" pcid="10422">
    def forward(self, x):
        # 64 channel blocks:
        h = self.block1_1(x)
        h = F.dropout(h, ratio=0.3)
        h = self.block1_2(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 128 channel blocks:
        h = self.block2_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block2_2(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 256 channel blocks:
        h = self.block3_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block3_2(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block3_3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 512 channel blocks:
        h = self.block4_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block4_2(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block4_3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 512 channel blocks:
        h = self.block5_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block5_2(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block5_3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        h = F.dropout(h, ratio=0.5)
        h = self.fc1(h)
        h = self.bn_fc1(h)
        h = F.relu(h)
        h = F.dropout(h, ratio=0.5)
        return self.fc2(h)
</source>
<source file="systems/chainer-7.2.0/examples/static_graph_optimizations/cifar/models/VGG.py" startline="82" endline="124" pcid="10512">
    def __call__(self, x):
        # 64 channel blocks:
        h = self.block1_1(x)
        h = F.dropout(h, ratio=0.3)
        h = self.block1_2(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 128 channel blocks:
        h = self.block2_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block2_2(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 256 channel blocks:
        h = self.block3_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block3_2(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block3_3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 512 channel blocks:
        h = self.block4_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block4_2(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block4_3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 512 channel blocks:
        h = self.block5_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block5_2(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block5_3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        h = F.dropout(h, ratio=0.5)
        h = self.fc1(h)
        h = self.bn_fc1(h)
        h = F.relu(h)
        h = F.dropout(h, ratio=0.5)
        return self.fc2(h)
</source>
</class>

<class classid="256" nclones="2" nlines="66" similarity="73">
<source file="systems/chainer-7.2.0/examples/chainermn/mnist/train_mnist_model_parallel.py" startline="64" endline="138" pcid="10405">
def main():
    parser = argparse.ArgumentParser(
        description='ChainerMN example: pipelined neural network')
    parser.add_argument('--batchsize', '-b', type=int, default=100,
                        help='Number of images in each mini-batch')
    parser.add_argument('--epoch', '-e', type=int, default=20,
                        help='Number of sweeps over the dataset to train')
    parser.add_argument('--gpu', '-g', action='store_true',
                        help='Use GPU')
    parser.add_argument('--out', '-o', default='result',
                        help='Directory to output the result')
    parser.add_argument('--unit', '-u', type=int, default=1000,
                        help='Number of units')
    args = parser.parse_args()

    # Prepare ChainerMN communicator.
    if args.gpu:
        comm = chainermn.create_communicator('pure_nccl')
        device = comm.intra_rank
    else:
        comm = chainermn.create_communicator('naive')
        device = -1

    if comm.size != 2:
        raise ValueError(
            'This example can only be executed on exactly 2 processes.')

    if comm.rank == 0:
        print('==========================================')
        if args.gpu:
            print('Using GPUs')
        print('Num unit: {}'.format(args.unit))
        print('Num Minibatch-size: {}'.format(args.batchsize))
        print('Num epoch: {}'.format(args.epoch))
        print('==========================================')

    if comm.rank == 0:
        model = L.Classifier(MLP0(comm, args.unit))
    elif comm.rank == 1:
        model = MLP1(comm, args.unit, 10)

    if device >= 0:
        chainer.cuda.get_device_from_id(device).use()
        model.to_gpu()

    optimizer = chainer.optimizers.Adam()
    optimizer.setup(model)

    # Iterate dataset only on worker 0.
    train, test = chainer.datasets.get_mnist()
    if comm.rank == 1:
        train = chainermn.datasets.create_empty_dataset(train)
        test = chainermn.datasets.create_empty_dataset(test)

    train_iter = chainer.iterators.SerialIterator(
        train, args.batchsize, shuffle=False)
    test_iter = chainer.iterators.SerialIterator(
        test, args.batchsize, repeat=False, shuffle=False)

    updater = training.StandardUpdater(train_iter, optimizer, device=device)
    trainer = training.Trainer(updater, (args.epoch, 'epoch'), out=args.out)
    trainer.extend(extensions.Evaluator(test_iter, model, device=device))

    # Some display and output extentions are necessary only for worker 0.
    if comm.rank == 0:
        trainer.extend(extensions.DumpGraph('main/loss'))
        trainer.extend(extensions.LogReport())
        trainer.extend(extensions.PrintReport(
            ['epoch', 'main/loss', 'validation/main/loss',
             'main/accuracy', 'validation/main/accuracy', 'elapsed_time']))
        trainer.extend(extensions.ProgressBar())

    trainer.run()


</source>
<source file="systems/chainer-7.2.0/examples/chainermn/mnist/train_mnist_dual_parallel.py" startline="64" endline="154" pcid="10417">
def main():
    parser = argparse.ArgumentParser(
        description='ChainerMN example: pipelined neural network')
    parser.add_argument('--batchsize', '-b', type=int, default=100,
                        help='Number of images in each mini-batch')
    parser.add_argument('--epoch', '-e', type=int, default=20,
                        help='Number of sweeps over the dataset to train')
    parser.add_argument('--gpu', '-g', action='store_true',
                        help='Use GPU')
    parser.add_argument('--out', '-o', default='result',
                        help='Directory to output the result')
    parser.add_argument('--unit', '-u', type=int, default=1000,
                        help='Number of units')
    args = parser.parse_args()

    # Prepare ChainerMN communicator.
    if args.gpu:
        comm = chainermn.create_communicator('pure_nccl')
        data_axis, model_axis = comm.rank % 2, comm.rank // 2
        data_comm = comm.split(data_axis, comm.rank)
        model_comm = comm.split(model_axis, comm.rank)
        device = comm.intra_rank
    else:
        comm = chainermn.create_communicator('naive')
        data_axis, model_axis = comm.rank % 2, comm.rank // 2
        data_comm = comm.split(data_axis, comm.rank)
        model_comm = comm.split(model_axis, comm.rank)
        device = -1

    if model_comm.size != 2:
        raise ValueError(
            'This example can only be executed on the even number '
            'of processes.')

    if comm.rank == 0:
        print('==========================================')
        if args.gpu:
            print('Using GPUs')
        print('Num unit: {}'.format(args.unit))
        print('Num Minibatch-size: {}'.format(args.batchsize))
        print('Num epoch: {}'.format(args.epoch))
        print('==========================================')

    if data_axis == 0:
        model = L.Classifier(MLP0(model_comm, args.unit))
    elif data_axis == 1:
        model = MLP1(model_comm, args.unit, 10)

    if device >= 0:
        chainer.cuda.get_device_from_id(device).use()
        model.to_gpu()

    optimizer = chainermn.create_multi_node_optimizer(
        chainer.optimizers.Adam(), data_comm)
    optimizer.setup(model)

    # Original dataset on worker 0 and 1.
    # Datasets of worker 0 and 1 are split and distributed to all workers.
    if model_axis == 0:
        train, test = chainer.datasets.get_mnist()
        if data_axis == 1:
            train = chainermn.datasets.create_empty_dataset(train)
            test = chainermn.datasets.create_empty_dataset(test)
    else:
        train, test = None, None
    train = chainermn.scatter_dataset(train, data_comm, shuffle=True)
    test = chainermn.scatter_dataset(test, data_comm, shuffle=True)

    train_iter = chainer.iterators.SerialIterator(
        train, args.batchsize, shuffle=False)
    test_iter = chainer.iterators.SerialIterator(
        test, args.batchsize, repeat=False, shuffle=False)

    updater = training.StandardUpdater(train_iter, optimizer, device=device)
    trainer = training.Trainer(updater, (args.epoch, 'epoch'), out=args.out)
    evaluator = extensions.Evaluator(test_iter, model, device=device)
    evaluator = chainermn.create_multi_node_evaluator(evaluator, data_comm)
    trainer.extend(evaluator)

    # Some display and output extensions are necessary only for worker 0.
    if comm.rank == 0:
        trainer.extend(extensions.DumpGraph('main/loss'))
        trainer.extend(extensions.LogReport())
        trainer.extend(extensions.PrintReport(
            ['epoch', 'main/loss', 'validation/main/loss',
             'main/accuracy', 'validation/main/accuracy', 'elapsed_time']))
        trainer.extend(extensions.ProgressBar())

    trainer.run()


</source>
</class>

<class classid="257" nclones="2" nlines="12" similarity="100">
<source file="systems/chainer-7.2.0/examples/chainermn/imagenet/models/nin.py" startline="13" endline="26" pcid="10439">
    def __init__(self):
        super(NIN, self).__init__()
        conv_init = I.HeNormal()  # MSRA scaling

        with self.init_scope():
            self.mlpconv1 = L.MLPConvolution2D(
                None, (96, 96, 96), 11, stride=4, conv_init=conv_init)
            self.mlpconv2 = L.MLPConvolution2D(
                None, (256, 256, 256), 5, pad=2, conv_init=conv_init)
            self.mlpconv3 = L.MLPConvolution2D(
                None, (384, 384, 384), 3, pad=1, conv_init=conv_init)
            self.mlpconv4 = L.MLPConvolution2D(
                None, (1024, 1024, 1000), 3, pad=1, conv_init=conv_init)

</source>
<source file="systems/chainer-7.2.0/examples/imagenet/nin.py" startline="13" endline="26" pcid="10552">
    def __init__(self):
        super(NIN, self).__init__()
        conv_init = I.HeNormal()  # MSRA scaling

        with self.init_scope():
            self.mlpconv1 = L.MLPConvolution2D(
                None, (96, 96, 96), 11, stride=4, conv_init=conv_init)
            self.mlpconv2 = L.MLPConvolution2D(
                None, (256, 256, 256), 5, pad=2, conv_init=conv_init)
            self.mlpconv3 = L.MLPConvolution2D(
                None, (384, 384, 384), 3, pad=1, conv_init=conv_init)
            self.mlpconv4 = L.MLPConvolution2D(
                None, (1024, 1024, 1000), 3, pad=1, conv_init=conv_init)

</source>
</class>

<class classid="258" nclones="2" nlines="22" similarity="100">
<source file="systems/chainer-7.2.0/examples/chainermn/imagenet/models/googlenet.py" startline="10" endline="34" pcid="10441">
    def __init__(self):
        super(GoogLeNet, self).__init__()
        with self.init_scope():
            self.conv1 = L.Convolution2D(None, 64, 7, stride=2, pad=3)
            self.conv2_reduce = L.Convolution2D(None, 64, 1)
            self.conv2 = L.Convolution2D(None, 192, 3, stride=1, pad=1)
            self.inc3a = L.Inception(None, 64, 96, 128, 16, 32, 32)
            self.inc3b = L.Inception(None, 128, 128, 192, 32, 96, 64)
            self.inc4a = L.Inception(None, 192, 96, 208, 16, 48, 64)
            self.inc4b = L.Inception(None, 160, 112, 224, 24, 64, 64)
            self.inc4c = L.Inception(None, 128, 128, 256, 24, 64, 64)
            self.inc4d = L.Inception(None, 112, 144, 288, 32, 64, 64)
            self.inc4e = L.Inception(None, 256, 160, 320, 32, 128, 128)
            self.inc5a = L.Inception(None, 256, 160, 320, 32, 128, 128)
            self.inc5b = L.Inception(None, 384, 192, 384, 48, 128, 128)
            self.loss3_fc = L.Linear(None, 1000)

            self.loss1_conv = L.Convolution2D(None, 128, 1)
            self.loss1_fc1 = L.Linear(None, 1024)
            self.loss1_fc2 = L.Linear(None, 1000)

            self.loss2_conv = L.Convolution2D(None, 128, 1)
            self.loss2_fc1 = L.Linear(None, 1024)
            self.loss2_fc2 = L.Linear(None, 1000)

</source>
<source file="systems/chainer-7.2.0/examples/imagenet/googlenet.py" startline="10" endline="34" pcid="10572">
    def __init__(self):
        super(GoogLeNet, self).__init__()
        with self.init_scope():
            self.conv1 = L.Convolution2D(None,  64, 7, stride=2, pad=3)
            self.conv2_reduce = L.Convolution2D(None,  64, 1)
            self.conv2 = L.Convolution2D(None, 192, 3, stride=1, pad=1)
            self.inc3a = L.Inception(None,  64,  96, 128, 16,  32,  32)
            self.inc3b = L.Inception(None, 128, 128, 192, 32,  96,  64)
            self.inc4a = L.Inception(None, 192,  96, 208, 16,  48,  64)
            self.inc4b = L.Inception(None, 160, 112, 224, 24,  64,  64)
            self.inc4c = L.Inception(None, 128, 128, 256, 24,  64,  64)
            self.inc4d = L.Inception(None, 112, 144, 288, 32,  64,  64)
            self.inc4e = L.Inception(None, 256, 160, 320, 32, 128, 128)
            self.inc5a = L.Inception(None, 256, 160, 320, 32, 128, 128)
            self.inc5b = L.Inception(None, 384, 192, 384, 48, 128, 128)
            self.loss3_fc = L.Linear(None, 1000)

            self.loss1_conv = L.Convolution2D(None, 128, 1)
            self.loss1_fc1 = L.Linear(None, 1024)
            self.loss1_fc2 = L.Linear(None, 1000)

            self.loss2_conv = L.Convolution2D(None, 128, 1)
            self.loss2_fc1 = L.Linear(None, 1024)
            self.loss2_fc2 = L.Linear(None, 1000)

</source>
</class>

<class classid="259" nclones="2" nlines="41" similarity="100">
<source file="systems/chainer-7.2.0/examples/chainermn/imagenet/models/googlenet.py" startline="35" endline="84" pcid="10442">
    def __call__(self, x, t):
        h = F.relu(self.conv1(x))
        h = F.local_response_normalization(
            F.max_pooling_2d(h, 3, stride=2), n=5)
        h = F.relu(self.conv2_reduce(h))
        h = F.relu(self.conv2(h))
        h = F.max_pooling_2d(
            F.local_response_normalization(h, n=5), 3, stride=2)

        h = self.inc3a(h)
        h = self.inc3b(h)
        h = F.max_pooling_2d(h, 3, stride=2)
        h = self.inc4a(h)

        l = F.average_pooling_2d(h, 5, stride=3)
        l = F.relu(self.loss1_conv(l))
        l = F.relu(self.loss1_fc1(l))
        l = self.loss1_fc2(l)
        loss1 = F.softmax_cross_entropy(l, t)

        h = self.inc4b(h)
        h = self.inc4c(h)
        h = self.inc4d(h)

        l = F.average_pooling_2d(h, 5, stride=3)
        l = F.relu(self.loss2_conv(l))
        l = F.relu(self.loss2_fc1(l))
        l = self.loss2_fc2(l)
        loss2 = F.softmax_cross_entropy(l, t)

        h = self.inc4e(h)
        h = F.max_pooling_2d(h, 3, stride=2)
        h = self.inc5a(h)
        h = self.inc5b(h)

        h = F.average_pooling_2d(h, 7, stride=1)
        h = self.loss3_fc(F.dropout(h, 0.4))
        loss3 = F.softmax_cross_entropy(h, t)

        loss = 0.3 * (loss1 + loss2) + loss3
        accuracy = F.accuracy(h, t)

        chainer.report({
            'loss': loss,
            'loss1': loss1,
            'loss2': loss2,
            'loss3': loss3,
            'accuracy': accuracy
        }, self)
        return loss
</source>
<source file="systems/chainer-7.2.0/examples/imagenet/googlenet.py" startline="35" endline="84" pcid="10573">
    def forward(self, x, t):
        h = F.relu(self.conv1(x))
        h = F.local_response_normalization(
            F.max_pooling_2d(h, 3, stride=2), n=5)
        h = F.relu(self.conv2_reduce(h))
        h = F.relu(self.conv2(h))
        h = F.max_pooling_2d(
            F.local_response_normalization(h, n=5), 3, stride=2)

        h = self.inc3a(h)
        h = self.inc3b(h)
        h = F.max_pooling_2d(h, 3, stride=2)
        h = self.inc4a(h)

        l = F.average_pooling_2d(h, 5, stride=3)
        l = F.relu(self.loss1_conv(l))
        l = F.relu(self.loss1_fc1(l))
        l = self.loss1_fc2(l)
        loss1 = F.softmax_cross_entropy(l, t)

        h = self.inc4b(h)
        h = self.inc4c(h)
        h = self.inc4d(h)

        l = F.average_pooling_2d(h, 5, stride=3)
        l = F.relu(self.loss2_conv(l))
        l = F.relu(self.loss2_fc1(l))
        l = self.loss2_fc2(l)
        loss2 = F.softmax_cross_entropy(l, t)

        h = self.inc4e(h)
        h = F.max_pooling_2d(h, 3, stride=2)
        h = self.inc5a(h)
        h = self.inc5b(h)

        h = F.average_pooling_2d(h, 7, stride=1)
        h = self.loss3_fc(F.dropout(h, 0.4))
        loss3 = F.softmax_cross_entropy(h, t)

        loss = 0.3 * (loss1 + loss2) + loss3
        accuracy = F.accuracy(h, t)

        chainer.report({
            'loss': loss,
            'loss1': loss1,
            'loss2': loss2,
            'loss3': loss3,
            'accuracy': accuracy
        }, self)
        return loss
</source>
</class>

<class classid="260" nclones="2" nlines="11" similarity="100">
<source file="systems/chainer-7.2.0/examples/chainermn/imagenet/models/alex.py" startline="12" endline="23" pcid="10443">
    def __init__(self):
        super(Alex, self).__init__()
        with self.init_scope():
            self.conv1 = L.Convolution2D(None, 96, 11, stride=4)
            self.conv2 = L.Convolution2D(None, 256, 5, pad=2)
            self.conv3 = L.Convolution2D(None, 384, 3, pad=1)
            self.conv4 = L.Convolution2D(None, 384, 3, pad=1)
            self.conv5 = L.Convolution2D(None, 256, 3, pad=1)
            self.fc6 = L.Linear(None, 4096)
            self.fc7 = L.Linear(None, 4096)
            self.fc8 = L.Linear(None, 1000)

</source>
<source file="systems/chainer-7.2.0/examples/imagenet/alex.py" startline="12" endline="23" pcid="10577">
    def __init__(self):
        super(Alex, self).__init__()
        with self.init_scope():
            self.conv1 = L.Convolution2D(None,  96, 11, stride=4)
            self.conv2 = L.Convolution2D(None, 256,  5, pad=2)
            self.conv3 = L.Convolution2D(None, 384,  3, pad=1)
            self.conv4 = L.Convolution2D(None, 384,  3, pad=1)
            self.conv5 = L.Convolution2D(None, 256,  3, pad=1)
            self.fc6 = L.Linear(None, 4096)
            self.fc7 = L.Linear(None, 4096)
            self.fc8 = L.Linear(None, 1000)

</source>
</class>

<class classid="261" nclones="2" nlines="14" similarity="100">
<source file="systems/chainer-7.2.0/examples/chainermn/imagenet/models/alex.py" startline="24" endline="38" pcid="10444">
    def __call__(self, x, t):
        h = F.max_pooling_2d(F.local_response_normalization(
            F.relu(self.conv1(x))), 3, stride=2)
        h = F.max_pooling_2d(F.local_response_normalization(
            F.relu(self.conv2(h))), 3, stride=2)
        h = F.relu(self.conv3(h))
        h = F.relu(self.conv4(h))
        h = F.max_pooling_2d(F.relu(self.conv5(h)), 3, stride=2)
        h = F.dropout(F.relu(self.fc6(h)))
        h = F.dropout(F.relu(self.fc7(h)))
        h = self.fc8(h)

        loss = F.softmax_cross_entropy(h, t)
        chainer.report({'loss': loss, 'accuracy': F.accuracy(h, t)}, self)
        return loss
</source>
<source file="systems/chainer-7.2.0/examples/imagenet/alex.py" startline="24" endline="38" pcid="10578">
    def forward(self, x, t):
        h = F.max_pooling_2d(F.local_response_normalization(
            F.relu(self.conv1(x))), 3, stride=2)
        h = F.max_pooling_2d(F.local_response_normalization(
            F.relu(self.conv2(h))), 3, stride=2)
        h = F.relu(self.conv3(h))
        h = F.relu(self.conv4(h))
        h = F.max_pooling_2d(F.relu(self.conv5(h)), 3, stride=2)
        h = F.dropout(F.relu(self.fc6(h)))
        h = F.dropout(F.relu(self.fc7(h)))
        h = self.fc8(h)

        loss = F.softmax_cross_entropy(h, t)
        chainer.report({'loss': loss, 'accuracy': F.accuracy(h, t)}, self)
        return loss
</source>
</class>

<class classid="262" nclones="3" nlines="18" similarity="77">
<source file="systems/chainer-7.2.0/examples/chainermn/imagenet/models/resnet50.py" startline="12" endline="31" pcid="10445">
    def __init__(self, in_size, ch, out_size, stride=2):
        super(BottleNeckA, self).__init__()
        initialW = initializers.HeNormal()

        with self.init_scope():
            self.conv1 = L.Convolution2D(
                in_size, ch, 1, stride, 0, initialW=initialW, nobias=True)
            self.bn1 = L.BatchNormalization(ch)
            self.conv2 = L.Convolution2D(
                ch, ch, 3, 1, 1, initialW=initialW, nobias=True)
            self.bn2 = L.BatchNormalization(ch)
            self.conv3 = L.Convolution2D(
                ch, out_size, 1, 1, 0, initialW=initialW, nobias=True)
            self.bn3 = L.BatchNormalization(out_size)

            self.conv4 = L.Convolution2D(
                in_size, out_size, 1, stride, 0,
                initialW=initialW, nobias=True)
            self.bn4 = L.BatchNormalization(out_size)

</source>
<source file="systems/chainer-7.2.0/examples/imagenet/resnext50.py" startline="9" endline="29" pcid="10564">
    def __init__(self, in_size, ch, out_size, stride=2, groups=1):
        super(BottleNeckA, self).__init__()
        initialW = initializers.HeNormal()

        with self.init_scope():
            self.conv1 = L.Convolution2D(
                in_size, ch, 1, 1, 0, initialW=initialW, nobias=True)
            self.bn1 = L.BatchNormalization(ch)
            self.conv2 = L.Convolution2D(
                ch, ch, 3, stride, 1, initialW=initialW, nobias=True,
                groups=groups)
            self.bn2 = L.BatchNormalization(ch)
            self.conv3 = L.Convolution2D(
                ch, out_size, 1, 1, 0, initialW=initialW, nobias=True)
            self.bn3 = L.BatchNormalization(out_size)

            self.conv4 = L.Convolution2D(
                in_size, out_size, 1, stride, 0,
                initialW=initialW, nobias=True)
            self.bn4 = L.BatchNormalization(out_size)

</source>
<source file="systems/chainer-7.2.0/examples/imagenet/resnet50.py" startline="12" endline="32" pcid="10579">
    def __init__(self, in_size, ch, out_size, stride=2, groups=1):
        super(BottleNeckA, self).__init__()
        initialW = initializers.HeNormal()

        with self.init_scope():
            self.conv1 = L.Convolution2D(
                in_size, ch, 1, stride, 0, initialW=initialW, nobias=True)
            self.bn1 = L.BatchNormalization(ch)
            self.conv2 = L.Convolution2D(
                ch, ch, 3, 1, 1, initialW=initialW, nobias=True,
                groups=groups)
            self.bn2 = L.BatchNormalization(ch)
            self.conv3 = L.Convolution2D(
                ch, out_size, 1, 1, 0, initialW=initialW, nobias=True)
            self.bn3 = L.BatchNormalization(out_size)

            self.conv4 = L.Convolution2D(
                in_size, out_size, 1, stride, 0,
                initialW=initialW, nobias=True)
            self.bn4 = L.BatchNormalization(out_size)

</source>
</class>

<class classid="263" nclones="3" nlines="14" similarity="78">
<source file="systems/chainer-7.2.0/examples/chainermn/imagenet/models/resnet50.py" startline="43" endline="57" pcid="10447">
    def __init__(self, in_size, ch):
        super(BottleNeckB, self).__init__()
        initialW = initializers.HeNormal()

        with self.init_scope():
            self.conv1 = L.Convolution2D(
                in_size, ch, 1, 1, 0, initialW=initialW, nobias=True)
            self.bn1 = L.BatchNormalization(ch)
            self.conv2 = L.Convolution2D(
                ch, ch, 3, 1, 1, initialW=initialW, nobias=True)
            self.bn2 = L.BatchNormalization(ch)
            self.conv3 = L.Convolution2D(
                ch, in_size, 1, 1, 0, initialW=initialW, nobias=True)
            self.bn3 = L.BatchNormalization(in_size)

</source>
<source file="systems/chainer-7.2.0/examples/imagenet/resnet50.py" startline="44" endline="59" pcid="10581">
    def __init__(self, in_size, ch, groups=1):
        super(BottleNeckB, self).__init__()
        initialW = initializers.HeNormal()

        with self.init_scope():
            self.conv1 = L.Convolution2D(
                in_size, ch, 1, 1, 0, initialW=initialW, nobias=True)
            self.bn1 = L.BatchNormalization(ch)
            self.conv2 = L.Convolution2D(
                ch, ch, 3, 1, 1, initialW=initialW, nobias=True,
                groups=groups)
            self.bn2 = L.BatchNormalization(ch)
            self.conv3 = L.Convolution2D(
                ch, in_size, 1, 1, 0, initialW=initialW, nobias=True)
            self.bn3 = L.BatchNormalization(in_size)

</source>
<source file="systems/chainer-7.2.0/examples/imagenet/resnext50.py" startline="41" endline="56" pcid="10566">
    def __init__(self, in_size, ch, groups=1):
        super(BottleNeckB, self).__init__()
        initialW = initializers.HeNormal()

        with self.init_scope():
            self.conv1 = L.Convolution2D(
                in_size, ch, 1, 1, 0, initialW=initialW, nobias=True)
            self.bn1 = L.BatchNormalization(ch)
            self.conv2 = L.Convolution2D(
                ch, ch, 3, 1, 1, initialW=initialW, nobias=True,
                groups=groups)
            self.bn2 = L.BatchNormalization(ch)
            self.conv3 = L.Convolution2D(
                ch, in_size, 1, 1, 0, initialW=initialW, nobias=True)
            self.bn3 = L.BatchNormalization(in_size)

</source>
</class>

<class classid="264" nclones="3" nlines="11" similarity="91">
<source file="systems/chainer-7.2.0/examples/chainermn/imagenet/models/resnet50.py" startline="84" endline="95" pcid="10451">
    def __init__(self):
        super(ResNet50, self).__init__()
        with self.init_scope():
            self.conv1 = L.Convolution2D(
                3, 64, 7, 2, 3, initialW=initializers.HeNormal())
            self.bn1 = L.BatchNormalization(64)
            self.res2 = Block(3, 64, 64, 256, 1)
            self.res3 = Block(4, 256, 128, 512)
            self.res4 = Block(6, 512, 256, 1024)
            self.res5 = Block(3, 1024, 512, 2048)
            self.fc = L.Linear(2048, 1000)

</source>
<source file="systems/chainer-7.2.0/examples/imagenet/resnet50.py" startline="86" endline="97" pcid="10585">
    def __init__(self):
        super(ResNet50, self).__init__()
        with self.init_scope():
            self.conv1 = L.Convolution2D(
                3, 64, 7, 2, 3, initialW=initializers.HeNormal())
            self.bn1 = L.BatchNormalization(64)
            self.res2 = Block(3, 64, 64, 256, 1)
            self.res3 = Block(4, 256, 128, 512)
            self.res4 = Block(6, 512, 256, 1024)
            self.res5 = Block(3, 1024, 512, 2048)
            self.fc = L.Linear(2048, 1000)

</source>
<source file="systems/chainer-7.2.0/examples/imagenet/resnet50.py" startline="117" endline="129" pcid="10587">
    def __init__(self):
        super(ResNet50_Nhwc, self).__init__()
        with self.init_scope():
            self.conv1 = L.Convolution2D(
                3, 64, 7, 2, 3, initialW=initializers.HeNormal())
            self.bn1 = L.BatchNormalization(64)
            with chainer.using_config('compute_mode', 'cudnn_fast'):
                self.res2 = Block(3, 64, 64, 256, 1)
                self.res3 = Block(4, 256, 128, 512)
                self.res4 = Block(6, 512, 256, 1024)
                self.res5 = Block(3, 1024, 512, 2048)
            self.fc = L.Linear(2048, 1000)

</source>
</class>

<class classid="265" nclones="4" nlines="12" similarity="85">
<source file="systems/chainer-7.2.0/examples/chainermn/imagenet/models/resnet50.py" startline="96" endline="108" pcid="10452">
    def __call__(self, x, t):
        h = self.bn1(self.conv1(x))
        h = F.max_pooling_2d(F.relu(h), 3, stride=2)
        h = self.res2(h)
        h = self.res3(h)
        h = self.res4(h)
        h = self.res5(h)
        h = F.average_pooling_2d(h, 7, stride=1)
        h = self.fc(h)

        loss = F.softmax_cross_entropy(h, t)
        chainer.report({'loss': loss, 'accuracy': F.accuracy(h, t)}, self)
        return loss
</source>
<source file="systems/chainer-7.2.0/examples/imagenet/resnet50.py" startline="130" endline="144" pcid="10588">
    def forward(self, x, t):
        h = self.bn1(self.conv1(x))
        h = F.max_pooling_2d(F.relu(h), 3, stride=2)
        h = h.as_layout(chainer.memory_layouts.CUDNN_CHANNEL_LAST_X)
        h = self.res2(h)
        h = self.res3(h)
        h = self.res4(h)
        h = self.res5(h)
        h = h.as_layout(None)
        h = F.average_pooling_2d(h, 7, stride=1)
        h = self.fc(h)

        loss = F.softmax_cross_entropy(h, t)
        chainer.report({'loss': loss, 'accuracy': F.accuracy(h, t)}, self)
        return loss
</source>
<source file="systems/chainer-7.2.0/examples/imagenet/resnet50.py" startline="98" endline="112" pcid="10586">
    def forward(self, x, t):
        h = self.bn1(self.conv1(x))
        h = F.max_pooling_2d(F.relu(h), 3, stride=2)
        h = self.res2(h)
        h = self.res3(h)
        h = self.res4(h)
        h = self.res5(h)
        h = F.average_pooling_2d(h, 7, stride=1)
        h = self.fc(h)

        loss = F.softmax_cross_entropy(h, t)
        chainer.report({'loss': loss, 'accuracy': F.accuracy(h, t)}, self)
        return loss


</source>
<source file="systems/chainer-7.2.0/examples/imagenet/resnext50.py" startline="95" endline="107" pcid="10571">
    def forward(self, x, t):
        h = self.bn1(self.conv1(x))
        h = F.max_pooling_2d(F.relu(h), 3, stride=2)
        h = self.res2(h)
        h = self.res3(h)
        h = self.res4(h)
        h = self.res5(h)
        h = F.average_pooling_2d(h, 7, stride=1)
        h = self.fc(h)

        loss = F.softmax_cross_entropy(h, t)
        chainer.report({'loss': loss, 'accuracy': F.accuracy(h, t)}, self)
        return loss
</source>
</class>

<class classid="266" nclones="2" nlines="39" similarity="100">
<source file="systems/chainer-7.2.0/examples/chainermn/imagenet/models/googlenetbn.py" startline="12" endline="53" pcid="10453">
    def __init__(self):
        super(GoogLeNetBN, self).__init__()
        with self.init_scope():
            self.conv1 = L.Convolution2D(
                None, 64, 7, stride=2, pad=3, nobias=True)
            self.norm1 = L.BatchNormalization(64)
            self.conv2 = L.Convolution2D(None, 192, 3, pad=1, nobias=True)
            self.norm2 = L.BatchNormalization(192)
            self.inc3a = L.InceptionBN(
                None, 64, 64, 64, 64, 96, 'avg', 32)
            self.inc3b = L.InceptionBN(
                None, 64, 64, 96, 64, 96, 'avg', 64)
            self.inc3c = L.InceptionBN(
                None, 0, 128, 160, 64, 96, 'max', stride=2)
            self.inc4a = L.InceptionBN(
                None, 224, 64, 96, 96, 128, 'avg', 128)
            self.inc4b = L.InceptionBN(
                None, 192, 96, 128, 96, 128, 'avg', 128)
            self.inc4c = L.InceptionBN(
                None, 128, 128, 160, 128, 160, 'avg', 128)
            self.inc4d = L.InceptionBN(
                None, 64, 128, 192, 160, 192, 'avg', 128)
            self.inc4e = L.InceptionBN(
                None, 0, 128, 192, 192, 256, 'max', stride=2)
            self.inc5a = L.InceptionBN(
                None, 352, 192, 320, 160, 224, 'avg', 128)
            self.inc5b = L.InceptionBN(
                None, 352, 192, 320, 192, 224, 'max', 128)
            self.out = L.Linear(None, 1000)

            self.conva = L.Convolution2D(None, 128, 1, nobias=True)
            self.norma = L.BatchNormalization(128)
            self.lina = L.Linear(None, 1024, nobias=True)
            self.norma2 = L.BatchNormalization(1024)
            self.outa = L.Linear(None, 1000)

            self.convb = L.Convolution2D(None, 128, 1, nobias=True)
            self.normb = L.BatchNormalization(128)
            self.linb = L.Linear(None, 1024, nobias=True)
            self.normb2 = L.BatchNormalization(1024)
            self.outb = L.Linear(None, 1000)

</source>
<source file="systems/chainer-7.2.0/examples/imagenet/googlenetbn.py" startline="12" endline="53" pcid="10589">
    def __init__(self):
        super(GoogLeNetBN, self).__init__()
        with self.init_scope():
            self.conv1 = L.Convolution2D(
                None, 64, 7, stride=2, pad=3, nobias=True)
            self.norm1 = L.BatchNormalization(64)
            self.conv2 = L.Convolution2D(None, 192, 3, pad=1, nobias=True)
            self.norm2 = L.BatchNormalization(192)
            self.inc3a = L.InceptionBN(
                None, 64, 64, 64, 64, 96, 'avg', 32)
            self.inc3b = L.InceptionBN(
                None, 64, 64, 96, 64, 96, 'avg', 64)
            self.inc3c = L.InceptionBN(
                None, 0, 128, 160, 64, 96, 'max', stride=2)
            self.inc4a = L.InceptionBN(
                None, 224, 64, 96, 96, 128, 'avg', 128)
            self.inc4b = L.InceptionBN(
                None, 192, 96, 128, 96, 128, 'avg', 128)
            self.inc4c = L.InceptionBN(
                None, 160, 128, 160, 128, 160, 'avg', 128)
            self.inc4d = L.InceptionBN(
                None, 96, 128, 192, 160, 192, 'avg', 128)
            self.inc4e = L.InceptionBN(
                None, 0, 128, 192, 192, 256, 'max', stride=2)
            self.inc5a = L.InceptionBN(
                None, 352, 192, 320, 160, 224, 'avg', 128)
            self.inc5b = L.InceptionBN(
                None, 352, 192, 320, 192, 224, 'max', 128)
            self.out = L.Linear(None, 1000)

            self.conva = L.Convolution2D(None, 128, 1, nobias=True)
            self.norma = L.BatchNormalization(128)
            self.lina = L.Linear(None, 1024, nobias=True)
            self.norma2 = L.BatchNormalization(1024)
            self.outa = L.Linear(None, 1000)

            self.convb = L.Convolution2D(None, 128, 1, nobias=True)
            self.normb = L.BatchNormalization(128)
            self.linb = L.Linear(None, 1024, nobias=True)
            self.normb2 = L.BatchNormalization(1024)
            self.outb = L.Linear(None, 1000)

</source>
</class>

<class classid="267" nclones="2" nlines="37" similarity="100">
<source file="systems/chainer-7.2.0/examples/chainermn/imagenet/models/googlenetbn.py" startline="54" endline="97" pcid="10454">
    def __call__(self, x, t):
        h = F.max_pooling_2d(
            F.relu(self.norm1(self.conv1(x))), 3, stride=2, pad=1)
        h = F.max_pooling_2d(
            F.relu(self.norm2(self.conv2(h))), 3, stride=2, pad=1)

        h = self.inc3a(h)
        h = self.inc3b(h)
        h = self.inc3c(h)
        h = self.inc4a(h)

        a = F.average_pooling_2d(h, 5, stride=3)
        a = F.relu(self.norma(self.conva(a)))
        a = F.relu(self.norma2(self.lina(a)))
        a = self.outa(a)
        loss1 = F.softmax_cross_entropy(a, t)

        h = self.inc4b(h)
        h = self.inc4c(h)
        h = self.inc4d(h)

        b = F.average_pooling_2d(h, 5, stride=3)
        b = F.relu(self.normb(self.convb(b)))
        b = F.relu(self.normb2(self.linb(b)))
        b = self.outb(b)
        loss2 = F.softmax_cross_entropy(b, t)

        h = self.inc4e(h)
        h = self.inc5a(h)
        h = F.average_pooling_2d(self.inc5b(h), 7)
        h = self.out(h)
        loss3 = F.softmax_cross_entropy(h, t)

        loss = 0.3 * (loss1 + loss2) + loss3
        accuracy = F.accuracy(h, t)

        chainer.report({
            'loss': loss,
            'loss1': loss1,
            'loss2': loss2,
            'loss3': loss3,
            'accuracy': accuracy,
        }, self)
        return loss
</source>
<source file="systems/chainer-7.2.0/examples/imagenet/googlenetbn.py" startline="54" endline="97" pcid="10590">
    def forward(self, x, t):
        h = F.max_pooling_2d(
            F.relu(self.norm1(self.conv1(x))),  3, stride=2, pad=1)
        h = F.max_pooling_2d(
            F.relu(self.norm2(self.conv2(h))), 3, stride=2, pad=1)

        h = self.inc3a(h)
        h = self.inc3b(h)
        h = self.inc3c(h)
        h = self.inc4a(h)

        a = F.average_pooling_2d(h, 5, stride=3)
        a = F.relu(self.norma(self.conva(a)))
        a = F.relu(self.norma2(self.lina(a)))
        a = self.outa(a)
        loss1 = F.softmax_cross_entropy(a, t)

        h = self.inc4b(h)
        h = self.inc4c(h)
        h = self.inc4d(h)

        b = F.average_pooling_2d(h, 5, stride=3)
        b = F.relu(self.normb(self.convb(b)))
        b = F.relu(self.normb2(self.linb(b)))
        b = self.outb(b)
        loss2 = F.softmax_cross_entropy(b, t)

        h = self.inc4e(h)
        h = self.inc5a(h)
        h = F.average_pooling_2d(self.inc5b(h), 7)
        h = self.out(h)
        loss3 = F.softmax_cross_entropy(h, t)

        loss = 0.3 * (loss1 + loss2) + loss3
        accuracy = F.accuracy(h, t)

        chainer.report({
            'loss': loss,
            'loss1': loss1,
            'loss2': loss2,
            'loss3': loss3,
            'accuracy': accuracy,
        }, self)
        return loss
</source>
</class>

<class classid="268" nclones="2" nlines="10" similarity="100">
<source file="systems/chainer-7.2.0/examples/chainermn/imagenet/compute_mean.py" startline="10" endline="21" pcid="10455">
def compute_mean(dataset):
    print('compute mean image')
    sum_image = 0
    N = len(dataset)
    for i, (image, _) in enumerate(dataset):
        sum_image += image
        sys.stderr.write('{} / {}\r'.format(i, N))
        sys.stderr.flush()
    sys.stderr.write('\n')
    return sum_image / N


</source>
<source file="systems/chainer-7.2.0/examples/imagenet/compute_mean.py" startline="10" endline="21" pcid="10592">
def compute_mean(dataset):
    print('compute mean image')
    sum_image = 0
    N = len(dataset)
    for i, (image, _) in enumerate(dataset):
        sum_image += image
        sys.stderr.write('{} / {}\r'.format(i, N))
        sys.stderr.flush()
    sys.stderr.write('\n')
    return sum_image / N


</source>
</class>

<class classid="269" nclones="2" nlines="12" similarity="100">
<source file="systems/chainer-7.2.0/examples/chainermn/imagenet/compute_mean.py" startline="22" endline="36" pcid="10456">
def main():
    parser = argparse.ArgumentParser(description='Compute images mean array')
    parser.add_argument('dataset',
                        help='Path to training image-label list file')
    parser.add_argument('--root', '-R', default='.',
                        help='Root directory path of image files')
    parser.add_argument('--output', '-o', default='mean.npy',
                        help='path to output mean array')
    args = parser.parse_args()

    dataset = chainer.datasets.LabeledImageDataset(args.dataset, args.root)
    mean = compute_mean(dataset)
    np.save(args.output, mean)


</source>
<source file="systems/chainer-7.2.0/examples/imagenet/compute_mean.py" startline="22" endline="36" pcid="10593">
def main():
    parser = argparse.ArgumentParser(description='Compute images mean array')
    parser.add_argument('dataset',
                        help='Path to training image-label list file')
    parser.add_argument('--root', '-R', default='.',
                        help='Root directory path of image files')
    parser.add_argument('--output', '-o', default='mean.npy',
                        help='path to output mean array')
    args = parser.parse_args()

    dataset = chainer.datasets.LabeledImageDataset(args.dataset, args.root)
    mean = compute_mean(dataset)
    np.save(args.output, mean)


</source>
</class>

<class classid="270" nclones="2" nlines="17" similarity="100">
<source file="systems/chainer-7.2.0/examples/chainermn/dcgan/net.py" startline="23" endline="41" pcid="10460">
    def __init__(self, n_hidden, bottom_width=4, ch=512, wscale=0.02):
        super(Generator, self).__init__()
        self.n_hidden = n_hidden
        self.ch = ch
        self.bottom_width = bottom_width

        with self.init_scope():
            w = chainer.initializers.Normal(wscale)
            self.l0 = L.Linear(self.n_hidden, bottom_width * bottom_width * ch,
                               initialW=w)
            self.dc1 = L.Deconvolution2D(ch, ch // 2, 4, 2, 1, initialW=w)
            self.dc2 = L.Deconvolution2D(ch // 2, ch // 4, 4, 2, 1, initialW=w)
            self.dc3 = L.Deconvolution2D(ch // 4, ch // 8, 4, 2, 1, initialW=w)
            self.dc4 = L.Deconvolution2D(ch // 8, 3, 3, 1, 1, initialW=w)
            self.bn0 = L.BatchNormalization(bottom_width * bottom_width * ch)
            self.bn1 = L.BatchNormalization(ch // 2)
            self.bn2 = L.BatchNormalization(ch // 4)
            self.bn3 = L.BatchNormalization(ch // 8)

</source>
<source file="systems/chainer-7.2.0/examples/dcgan/net.py" startline="27" endline="45" pcid="10597">
    def __init__(self, n_hidden, bottom_width=4, ch=512, wscale=0.02):
        super(Generator, self).__init__()
        self.n_hidden = n_hidden
        self.ch = ch
        self.bottom_width = bottom_width

        with self.init_scope():
            w = chainer.initializers.Normal(wscale)
            self.l0 = L.Linear(self.n_hidden, bottom_width * bottom_width * ch,
                               initialW=w)
            self.dc1 = L.Deconvolution2D(ch, ch // 2, 4, 2, 1, initialW=w)
            self.dc2 = L.Deconvolution2D(ch // 2, ch // 4, 4, 2, 1, initialW=w)
            self.dc3 = L.Deconvolution2D(ch // 4, ch // 8, 4, 2, 1, initialW=w)
            self.dc4 = L.Deconvolution2D(ch // 8, 3, 3, 1, 1, initialW=w)
            self.bn0 = L.BatchNormalization(bottom_width * bottom_width * ch)
            self.bn1 = L.BatchNormalization(ch // 2)
            self.bn2 = L.BatchNormalization(ch // 4)
            self.bn3 = L.BatchNormalization(ch // 8)

</source>
</class>

<class classid="271" nclones="2" nlines="18" similarity="100">
<source file="systems/chainer-7.2.0/examples/chainermn/dcgan/net.py" startline="57" endline="75" pcid="10463">

    def __init__(self, bottom_width=4, ch=512, wscale=0.02):
        w = chainer.initializers.Normal(wscale)
        super(Discriminator, self).__init__()
        with self.init_scope():
            self.c0_0 = L.Convolution2D(3, ch // 8, 3, 1, 1, initialW=w)
            self.c0_1 = L.Convolution2D(ch // 8, ch // 4, 4, 2, 1, initialW=w)
            self.c1_0 = L.Convolution2D(ch // 4, ch // 4, 3, 1, 1, initialW=w)
            self.c1_1 = L.Convolution2D(ch // 4, ch // 2, 4, 2, 1, initialW=w)
            self.c2_0 = L.Convolution2D(ch // 2, ch // 2, 3, 1, 1, initialW=w)
            self.c2_1 = L.Convolution2D(ch // 2, ch // 1, 4, 2, 1, initialW=w)
            self.c3_0 = L.Convolution2D(ch // 1, ch // 1, 3, 1, 1, initialW=w)
            self.l4 = L.Linear(bottom_width * bottom_width * ch, 1, initialW=w)
            self.bn0_1 = L.BatchNormalization(ch // 4, use_gamma=False)
            self.bn1_0 = L.BatchNormalization(ch // 4, use_gamma=False)
            self.bn1_1 = L.BatchNormalization(ch // 2, use_gamma=False)
            self.bn2_0 = L.BatchNormalization(ch // 2, use_gamma=False)
            self.bn2_1 = L.BatchNormalization(ch // 1, use_gamma=False)
            self.bn3_0 = L.BatchNormalization(ch // 1, use_gamma=False)
</source>
<source file="systems/chainer-7.2.0/examples/dcgan/net.py" startline="62" endline="80" pcid="10600">

    def __init__(self, bottom_width=4, ch=512, wscale=0.02):
        w = chainer.initializers.Normal(wscale)
        super(Discriminator, self).__init__()
        with self.init_scope():
            self.c0_0 = L.Convolution2D(3, ch // 8, 3, 1, 1, initialW=w)
            self.c0_1 = L.Convolution2D(ch // 8, ch // 4, 4, 2, 1, initialW=w)
            self.c1_0 = L.Convolution2D(ch // 4, ch // 4, 3, 1, 1, initialW=w)
            self.c1_1 = L.Convolution2D(ch // 4, ch // 2, 4, 2, 1, initialW=w)
            self.c2_0 = L.Convolution2D(ch // 2, ch // 2, 3, 1, 1, initialW=w)
            self.c2_1 = L.Convolution2D(ch // 2, ch // 1, 4, 2, 1, initialW=w)
            self.c3_0 = L.Convolution2D(ch // 1, ch // 1, 3, 1, 1, initialW=w)
            self.l4 = L.Linear(bottom_width * bottom_width * ch, 1, initialW=w)
            self.bn0_1 = L.BatchNormalization(ch // 4, use_gamma=False)
            self.bn1_0 = L.BatchNormalization(ch // 4, use_gamma=False)
            self.bn1_1 = L.BatchNormalization(ch // 2, use_gamma=False)
            self.bn2_0 = L.BatchNormalization(ch // 2, use_gamma=False)
            self.bn2_1 = L.BatchNormalization(ch // 1, use_gamma=False)
            self.bn3_0 = L.BatchNormalization(ch // 1, use_gamma=False)
</source>
</class>

<class classid="272" nclones="2" nlines="22" similarity="95">
<source file="systems/chainer-7.2.0/examples/chainermn/dcgan/visualize.py" startline="13" endline="36" pcid="10465">
def out_generated_image(gen, dis, rows, cols, seed, dst):
    @chainer.training.make_extension()
    def make_image(trainer):
        np.random.seed(seed)
        n_images = rows * cols
        xp = gen.xp
        z = Variable(xp.asarray(gen.make_hidden(n_images)))
        with chainer.using_config('train', False):
            x = gen(z)
        x = chainer.cuda.to_cpu(x.array)
        np.random.seed()

        x = np.asarray(np.clip(x * 255, 0.0, 255.0), dtype=np.uint8)
        _, _, H, W = x.shape
        x = x.reshape((rows, cols, 3, H, W))
        x = x.transpose(0, 3, 1, 4, 2)
        x = x.reshape((rows * H, cols * W, 3))

        preview_dir = '{}/preview'.format(dst)
        preview_path = preview_dir +\
            '/image{:0>8}.png'.format(trainer.updater.iteration)
        if not os.path.exists(preview_dir):
            os.makedirs(preview_dir)
        Image.fromarray(x).save(preview_path)
</source>
<source file="systems/chainer-7.2.0/examples/dcgan/visualize.py" startline="13" endline="36" pcid="10602">
def out_generated_image(gen, dis, rows, cols, seed, dst):
    @chainer.training.make_extension()
    def make_image(trainer):
        np.random.seed(seed)
        n_images = rows * cols
        xp = gen.xp
        z = Variable(xp.asarray(gen.make_hidden(n_images)))
        with chainer.using_config('train', False):
            x = gen(z)
        x = chainer.backends.cuda.to_cpu(x.array)
        np.random.seed()

        x = np.asarray(np.clip(x * 255, 0.0, 255.0), dtype=np.uint8)
        _, _, H, W = x.shape
        x = x.reshape((rows, cols, 3, H, W))
        x = x.transpose(0, 3, 1, 4, 2)
        x = x.reshape((rows * H, cols * W, 3))

        preview_dir = '{}/preview'.format(dst)
        preview_path = preview_dir +\
            '/image{:0>8}.png'.format(trainer.updater.iteration)
        if not os.path.exists(preview_dir):
            os.makedirs(preview_dir)
        Image.fromarray(x).save(preview_path)
</source>
</class>

<class classid="273" nclones="2" nlines="14" similarity="78">
<source file="systems/chainer-7.2.0/examples/chainermn/dcgan/updater.py" startline="30" endline="48" pcid="10470">
    def update_core(self):
        gen_optimizer = self.get_optimizer('gen')
        dis_optimizer = self.get_optimizer('dis')

        batch = self.get_iterator('main').next()
        x_real = Variable(self.converter(batch, self.device)) / 255.
        xp = chainer.backend.get_array_module(x_real.array)

        gen, dis = self.gen, self.dis
        batchsize = len(batch)

        y_real = dis(x_real)

        z = Variable(xp.asarray(gen.make_hidden(batchsize)))
        x_fake = gen(z)
        y_fake = dis(x_fake)

        dis_optimizer.update(self.loss_dis, dis, y_fake, y_real)
        gen_optimizer.update(self.loss_gen, gen, y_fake)
</source>
<source file="systems/chainer-7.2.0/examples/dcgan/updater.py" startline="27" endline="45" pcid="10607">
    def update_core(self):
        gen_optimizer = self.get_optimizer('gen')
        dis_optimizer = self.get_optimizer('dis')

        batch = self.get_iterator('main').next()
        device = self.device
        x_real = Variable(self.converter(batch, device)) / 255.

        gen, dis = self.gen, self.dis
        batchsize = len(batch)

        y_real = dis(x_real)

        z = Variable(device.xp.asarray(gen.make_hidden(batchsize)))
        x_fake = gen(z)
        y_fake = dis(x_fake)

        dis_optimizer.update(self.loss_dis, dis, y_fake, y_real)
        gen_optimizer.update(self.loss_gen, gen, y_fake)
</source>
</class>

<class classid="274" nclones="2" nlines="39" similarity="80">
<source file="systems/chainer-7.2.0/examples/static_graph_optimizations/mnist/train_mnist_custom_loop.py" startline="30" endline="72" pcid="10504">
def run_train_loop(
        optimizer, train_iter, test_iter, test_count, epoch, device):
    model = optimizer.target

    train_count = 0
    sum_accuracy = 0
    sum_loss = 0
    while train_iter.epoch < epoch:
        batch = train_iter.next()
        x_array, t_array = convert.concat_examples(batch, device)
        x = chainer.Variable(x_array)
        t = chainer.Variable(t_array, requires_grad=False)
        optimizer.update(model, x, t)
        train_count += len(t)
        sum_loss += float(model.loss.array) * len(t)
        sum_accuracy += float(model.accuracy.array) * len(t)

        if train_iter.is_new_epoch:
            print('epoch: ', train_iter.epoch)
            print('train mean loss: {}, accuracy: {}'.format(
                sum_loss / train_count, sum_accuracy / train_count))
            # evaluation
            train_count = 0
            sum_accuracy = 0
            sum_loss = 0
            # It is good practice to turn off train mode during evaluation.
            with configuration.using_config('train', False):
                for batch in test_iter:
                    x_array, t_array = convert.concat_examples(
                        batch, device)
                    x = chainer.Variable(x_array)
                    t = chainer.Variable(t_array, requires_grad=False)
                    loss = model(x, t)
                    sum_loss += float(loss.array) * len(t)
                    sum_accuracy += float(model.accuracy.array) * len(t)

            test_iter.reset()
            print('test mean  loss: {}, accuracy: {}'.format(
                sum_loss / test_count, sum_accuracy / test_count))
            sum_accuracy = 0
            sum_loss = 0


</source>
<source file="systems/chainer-7.2.0/examples/static_graph_optimizations/cifar/train_cifar_custom_loop.py" startline="31" endline="81" pcid="10507">
def run_train_loop(
        optimizer, train_iter, test_iter, test_count, epoch,
        device):
    model = optimizer.target

    train_count = 0
    sum_accuracy = 0
    sum_loss = 0
    while train_iter.epoch < epoch:
        batch = train_iter.next()
        # Reduce learning rate by 0.5 every 25 epochs.
        if train_iter.epoch % 25 == 0 and train_iter.is_new_epoch:
            optimizer.lr *= 0.5
            print('Reducing learning rate to: {}'.format(optimizer.lr))

        x_array, t_array = convert.concat_examples(batch, device)
        x = chainer.Variable(x_array)
        t = chainer.Variable(t_array, requires_grad=False)
        optimizer.update(model, x, t)
        train_count += len(t)
        sum_loss += float(model.loss.array) * len(t)
        sum_accuracy += float(model.accuracy.array) * len(t)

        if train_iter.is_new_epoch:
            print('epoch: {}'.format(train_iter.epoch))
            print('train mean loss: {}, accuracy: {}'.format(
                sum_loss / train_count, sum_accuracy / train_count))
            # evaluation
            train_count = 0
            sum_accuracy = 0
            sum_loss = 0
            model.predictor.train = False
            # It is good practice to turn off train mode during evaluation.
            with configuration.using_config('train', False):
                for batch in test_iter:
                    x_array, t_array = convert.concat_examples(
                        batch, device)
                    x = chainer.Variable(x_array)
                    t = chainer.Variable(t_array, requires_grad=False)
                    loss = model(x, t)
                    sum_loss += float(loss.array) * len(t)
                    sum_accuracy += float(model.accuracy.array) * len(t)

            test_iter.reset()
            model.predictor.train = True
            print('test mean  loss: {}, accuracy: {}'.format(
                sum_loss / test_count, sum_accuracy / test_count))
            sum_accuracy = 0
            sum_loss = 0


</source>
</class>

<class classid="275" nclones="2" nlines="63" similarity="70">
<source file="systems/chainer-7.2.0/examples/static_graph_optimizations/mnist/train_mnist_custom_loop.py" startline="73" endline="150" pcid="10505">
def main():
    parser = argparse.ArgumentParser(description='Chainer example: MNIST')
    parser.add_argument('--batchsize', '-b', type=int, default=100,
                        help='Number of images in each mini-batch')
    parser.add_argument('--epoch', '-e', type=int, default=20,
                        help='Number of sweeps over the dataset to train')
    parser.add_argument('--device', '-d', type=str, default='-1',
                        help='Device specifier. Either ChainerX device '
                        'specifier or an integer. If non-negative integer, '
                        'CuPy arrays with specified device id are used. If '
                        'negative integer, NumPy arrays are used')
    parser.add_argument('--out', '-o', default='result',
                        help='Directory to output the result')
    parser.add_argument('--model', '-m', default='MLP',
                        help='Choose the model: MLP or MLPSideEffect')
    parser.add_argument('--resume', '-r', default='',
                        help='Resume the training from snapshot')
    parser.add_argument('--unit', '-u', type=int, default=1000,
                        help='Number of units')
    group = parser.add_argument_group('deprecated arguments')
    group.add_argument('--gpu', '-g', dest='device',
                       type=int, nargs='?', const=0,
                       help='GPU ID (negative value indicates CPU)')
    args = parser.parse_args()

    if chainer.get_dtype() == numpy.float16:
        warnings.warn(
            'This example may cause NaN in FP16 mode.', RuntimeWarning)

    device = chainer.get_device(args.device)

    print('Device: {}'.format(device))
    print('# unit: {}'.format(args.unit))
    print('# Minibatch-size: {}'.format(args.batchsize))
    print('# epoch: {}'.format(args.epoch))
    print('')

    device.use()

    # Set up a neural network to train
    if args.model == 'MLP':
        model = L.Classifier(train_mnist.MLP(args.unit, 10))
    elif args.model == 'MLPSideEffect':
        model = L.Classifier(train_mnist.MLPSideEffect(args.unit, 10))
    model.to_device(device)

    # Setup an optimizer
    optimizer = chainer.optimizers.Adam()
    optimizer.setup(model)

    # Load the MNIST dataset
    train, test = chainer.datasets.get_mnist()

    test_count = len(test)

    train_iter = chainer.iterators.SerialIterator(train, args.batchsize)
    test_iter = chainer.iterators.SerialIterator(
        test, args.batchsize, repeat=False, shuffle=False)

    if device.xp is not chainerx:
        run_train_loop(
            optimizer, train_iter, test_iter, test_count, args.epoch, device)
    else:
        warnings.warn(
            'Static subgraph optimization does not support ChainerX and will'
            ' be disabled.', UserWarning)
        with chainer.using_config('use_static_graph', False):
            run_train_loop(
                optimizer, train_iter, test_iter, test_count, args.epoch,
                device)

    # Save the model and the optimizer
    print('save the model')
    serializers.save_npz('mlp.model', model)
    print('save the optimizer')
    serializers.save_npz('mlp.state', optimizer)


</source>
<source file="systems/chainer-7.2.0/examples/static_graph_optimizations/cifar/train_cifar_custom_loop.py" startline="82" endline="171" pcid="10508">
def main():
    parser = argparse.ArgumentParser(description='Chainer CIFAR example:')
    parser.add_argument('--dataset', default='cifar10',
                        help='The dataset to use: cifar10 or cifar100')
    parser.add_argument('--batchsize', '-b', type=int, default=64,
                        help='Number of images in each mini-batch')
    parser.add_argument('--learnrate', '-l', type=float, default=0.05,
                        help='Learning rate for SGD')
    parser.add_argument('--epoch', '-e', type=int, default=300,
                        help='Number of sweeps over the dataset to train')
    parser.add_argument('--device', '-d', type=str, default='0',
                        help='Device specifier. Either ChainerX device '
                        'specifier or an integer. If non-negative integer, '
                        'CuPy arrays with specified device id are used. If '
                        'negative integer, NumPy arrays are used')
    parser.add_argument('--out', '-o', default='result',
                        help='Directory to output the result')
    parser.add_argument('--test', action='store_true',
                        help='Use tiny datasets for quick tests')
    parser.add_argument('--resume', '-r', default='',
                        help='Resume the training from snapshot')
    group = parser.add_argument_group('deprecated arguments')
    group.add_argument('--gpu', '-g', dest='device',
                       type=int, nargs='?', const=0,
                       help='GPU ID (negative value indicates CPU)')
    args = parser.parse_args()

    if chainer.get_dtype() == numpy.float16:
        warnings.warn(
            'This example may cause NaN in FP16 mode.', RuntimeWarning)

    device = chainer.get_device(args.device)

    print('Device: {}'.format(device))
    print('# Minibatch-size: {}'.format(args.batchsize))
    print('# epoch: {}'.format(args.epoch))
    print('')

    device.use()

    # Set up a neural network to train.
    # Classifier reports softmax cross entropy loss and accuracy at every
    # iteration, which will be used by the PrintReport extension below.
    if args.dataset == 'cifar10':
        print('Using CIFAR10 dataset.')
        class_labels = 10
        train, test = get_cifar10()
    elif args.dataset == 'cifar100':
        print('Using CIFAR100 dataset.')
        class_labels = 100
        train, test = get_cifar100()
    else:
        raise RuntimeError('Invalid dataset choice.')

    if args.test:
        train = train[:200]
        test = test[:200]

    test_count = len(test)

    model = L.Classifier(models.VGG.VGG(class_labels))
    model.to_device(device)

    optimizer = chainer.optimizers.MomentumSGD(args.learnrate)
    optimizer.setup(model)
    optimizer.add_hook(chainer.optimizer.WeightDecay(5e-4))

    train_iter = chainer.iterators.SerialIterator(train, args.batchsize)
    test_iter = chainer.iterators.SerialIterator(
        test, args.batchsize, repeat=False, shuffle=False)

    if device.xp is not chainerx:
        run_train_loop(
            optimizer, train_iter, test_iter, test_count, args.epoch, device)
    else:
        warnings.warn(
            'Static subgraph optimization does not support ChainerX and will'
            ' be disabled.', UserWarning)
        with chainer.using_config('use_static_graph', False):
            run_train_loop(
                optimizer, train_iter, test_iter, test_count, args.epoch,
                device)

    # Save the model and the optimizer
    print('save the model')
    serializers.save_npz('mlp.model', model)
    print('save the optimizer')
    serializers.save_npz('mlp.state', optimizer)


</source>
</class>

<class classid="276" nclones="2" nlines="94" similarity="78">
<source file="systems/chainer-7.2.0/examples/reinforcement_learning/dqn_cartpole.py" startline="80" endline="211" pcid="10540">
def main():

    parser = argparse.ArgumentParser(description='Chainer example: DQN')
    parser.add_argument('--env', type=str, default='CartPole-v0',
                        help='Name of the OpenAI Gym environment')
    parser.add_argument('--batch-size', '-b', type=int, default=64,
                        help='Number of transitions in each mini-batch')
    parser.add_argument('--episodes', '-e', type=int, default=1000,
                        help='Number of episodes to run')
    parser.add_argument('--device', '-d', type=str, default='-1',
                        help='Device specifier. Either ChainerX device '
                        'specifier or an integer. If non-negative integer, '
                        'CuPy arrays with specified device id are used. If '
                        'negative integer, NumPy arrays are used')
    parser.add_argument('--out', '-o', default='dqn_result',
                        help='Directory to output the result')
    parser.add_argument('--unit', '-u', type=int, default=100,
                        help='Number of units')
    parser.add_argument('--target-type', type=str, default='dqn',
                        help='Target type', choices=['dqn', 'double_dqn'])
    parser.add_argument('--reward-scale', type=float, default=1e-2,
                        help='Reward scale factor')
    parser.add_argument('--replay-start-size', type=int, default=500,
                        help=('Number of iterations after which replay is '
                              'started'))
    parser.add_argument('--iterations-to-decay-epsilon', type=int,
                        default=5000,
                        help='Number of steps used to linearly decay epsilon')
    parser.add_argument('--min-epsilon', type=float, default=0.01,
                        help='Minimum value of epsilon')
    parser.add_argument('--target-update-freq', type=int, default=100,
                        help='Frequency of target network update')
    parser.add_argument('--record', action='store_true', default=True,
                        help='Record performance')
    parser.add_argument('--no-record', action='store_false', dest='record')
    group = parser.add_argument_group('deprecated arguments')
    group.add_argument('--gpu', '-g', dest='device',
                       type=int, nargs='?', const=0,
                       help='GPU ID (negative value indicates CPU)')
    args = parser.parse_args()

    if chainer.get_dtype() == np.float16:
        warnings.warn(
            'This example may cause NaN in FP16 mode.', RuntimeWarning)

    device = chainer.get_device(args.device)
    device.use()

    # Initialize an environment
    env = gym.make(args.env)
    assert isinstance(env.observation_space, gym.spaces.Box)
    assert isinstance(env.action_space, gym.spaces.Discrete)
    obs_size = env.observation_space.low.size
    n_actions = env.action_space.n
    if args.record:
        env = gym.wrappers.Monitor(env, args.out, force=True)
    reward_threshold = env.spec.reward_threshold
    if reward_threshold is not None:
        print('{} defines "solving" as getting average reward of {} over 100 '
              'consecutive trials.'.format(args.env, reward_threshold))
    else:
        print('{} is an unsolved environment, which means it does not have a '
              'specified reward threshold at which it\'s considered '
              'solved.'.format(args.env))

    # Initialize variables
    D = collections.deque(maxlen=10 ** 6)  # Replay buffer
    Rs = collections.deque(maxlen=100)  # History of returns
    iteration = 0

    # Initialize a model and its optimizer
    Q = QFunction(obs_size, n_actions, n_units=args.unit)
    Q.to_device(device)
    target_Q = copy.deepcopy(Q)
    opt = optimizers.Adam(eps=1e-2)
    opt.setup(Q)

    for episode in range(args.episodes):

        obs = env.reset()
        done = False
        R = 0.0  # Return (sum of rewards obtained in an episode)
        timestep = 0

        while not done and timestep < env.spec.timestep_limit:

            # Epsilon is linearly decayed
            epsilon = 1.0 if len(D) < args.replay_start_size else \
                max(args.min_epsilon,
                    np.interp(
                        iteration,
                        [0, args.iterations_to_decay_epsilon],
                        [1.0, args.min_epsilon]))

            # Select an action epsilon-greedily
            if np.random.rand() < epsilon:
                action = env.action_space.sample()
            else:
                action = get_greedy_action(Q, obs)

            # Execute an action
            new_obs, reward, done, _ = env.step(action)
            R += reward

            # Store a transition
            D.append((obs, action, reward * args.reward_scale, done, new_obs))
            obs = new_obs

            # Sample a random minibatch of transitions and replay
            if len(D) >= args.replay_start_size:
                sample_indices = random.sample(range(len(D)), args.batch_size)
                samples = [D[i] for i in sample_indices]
                update(Q, target_Q, opt, samples, target_type=args.target_type)

            # Update the target network
            if iteration % args.target_update_freq == 0:
                target_Q = copy.deepcopy(Q)

            iteration += 1
            timestep += 1

        Rs.append(R)
        average_R = np.mean(Rs)
        print('episode: {} iteration: {} R: {} average_R: {}'.format(
              episode, iteration, R, average_R))

        if reward_threshold is not None and average_R >= reward_threshold:
            print('Solved {} by getting average reward of '
                  '{} >= {} over 100 consecutive episodes.'.format(
                      args.env, average_R, reward_threshold))
            break

</source>
<source file="systems/chainer-7.2.0/examples/reinforcement_learning/ddpg_pendulum.py" startline="130" endline="256" pcid="10551">
def main():

    parser = argparse.ArgumentParser(description='Chainer example: DDPG')
    parser.add_argument('--env', type=str, default='Pendulum-v0',
                        help='Name of the OpenAI Gym environment')
    parser.add_argument('--batch-size', '-b', type=int, default=64,
                        help='Number of transitions in each mini-batch')
    parser.add_argument('--episodes', '-e', type=int, default=1000,
                        help='Number of episodes to run')
    parser.add_argument('--device', '-d', type=str, default='-1',
                        help='Device specifier. Either ChainerX device '
                        'specifier or an integer. If non-negative integer, '
                        'CuPy arrays with specified device id are used. If '
                        'negative integer, NumPy arrays are used')
    parser.add_argument('--out', '-o', default='ddpg_result',
                        help='Directory to output the result')
    parser.add_argument('--unit', '-u', type=int, default=100,
                        help='Number of units')
    parser.add_argument('--reward-scale', type=float, default=1e-3,
                        help='Reward scale factor')
    parser.add_argument('--replay-start-size', type=int, default=500,
                        help=('Number of iterations after which replay is '
                              'started'))
    parser.add_argument('--tau', type=float, default=1e-2,
                        help='Softness of soft target update (0, 1]')
    parser.add_argument('--noise-scale', type=float, default=0.4,
                        help='Scale of additive Gaussian noises')
    parser.add_argument('--record', action='store_true', default=True,
                        help='Record performance')
    parser.add_argument('--no-record', action='store_false', dest='record')
    group = parser.add_argument_group('deprecated arguments')
    group.add_argument('--gpu', '-g', dest='device',
                       type=int, nargs='?', const=0,
                       help='GPU ID (negative value indicates CPU)')
    args = parser.parse_args()

    if chainer.get_dtype() == np.float16:
        warnings.warn(
            'This example may cause NaN in FP16 mode.', RuntimeWarning)

    device = chainer.get_device(args.device)
    device.use()

    # Initialize an environment
    env = gym.make(args.env)
    assert isinstance(env.observation_space, gym.spaces.Box)
    assert isinstance(env.action_space, gym.spaces.Box)
    obs_size = env.observation_space.low.size
    action_size = env.action_space.low.size
    if args.record:
        env = gym.wrappers.Monitor(env, args.out, force=True)
    reward_threshold = env.spec.reward_threshold
    if reward_threshold is not None:
        print('{} defines "solving" as getting average reward of {} over 100 '
              'consecutive trials.'.format(args.env, reward_threshold))
    else:
        print('{} is an unsolved environment, which means it does not have a '
              'specified reward threshold at which it\'s considered '
              'solved.'.format(args.env))

    # Initialize variables
    D = collections.deque(maxlen=10 ** 6)  # Replay buffer
    Rs = collections.deque(maxlen=100)  # History of returns
    iteration = 0

    # Initialize models and optimizers
    Q = QFunction(obs_size, action_size, n_units=args.unit)
    policy = Policy(obs_size, action_size,
                    env.action_space.low, env.action_space.high,
                    n_units=args.unit)
    Q.to_device(device)
    policy.to_device(device)
    target_Q = copy.deepcopy(Q)
    target_policy = copy.deepcopy(policy)
    opt_Q = optimizers.Adam(eps=1e-5)  # Use larger eps in case of FP16 mode
    opt_Q.setup(Q)
    opt_policy = optimizers.Adam(alpha=1e-4)
    opt_policy.setup(policy)

    for episode in range(args.episodes):

        obs = env.reset()
        done = False
        R = 0.0  # Return (sum of rewards obtained in an episode)
        timestep = 0

        while not done and timestep < env.spec.timestep_limit:

            # Select an action with additive noises for exploration
            action = (get_action(policy, obs) +
                      np.random.normal(scale=args.noise_scale))

            # Execute an action
            new_obs, reward, done, _ = env.step(
                np.clip(action, env.action_space.low, env.action_space.high))
            R += reward

            # Store a transition
            D.append((obs, action, reward * args.reward_scale, done, new_obs))
            obs = new_obs

            # Sample a random minibatch of transitions and replay
            if len(D) >= args.replay_start_size:
                sample_indices = random.sample(range(len(D)), args.batch_size)
                samples = [D[i] for i in sample_indices]
                update(Q, target_Q, policy, target_policy,
                       opt_Q, opt_policy, samples)

            # Soft update of the target networks
            soft_copy_params(Q, target_Q, args.tau)
            soft_copy_params(policy, target_policy, args.tau)

            iteration += 1
            timestep += 1

        Rs.append(R)
        average_R = np.mean(Rs)
        print('episode: {} iteration: {} R:{} average_R:{}'.format(
              episode, iteration, R, average_R))

        if reward_threshold is not None and average_R >= reward_threshold:
            print('Solved {} by getting average reward of '
                  '{} >= {} over 100 consecutive episodes.'.format(
                      args.env, average_R, reward_threshold))
            break


</source>
</class>

<class classid="277" nclones="2" nlines="26" similarity="75">
<source file="systems/chainer-7.2.0/examples/imagenet/dali_util.py" startline="28" endline="62" pcid="10556">
    def __init__(self, file_list, file_root, crop_size,
                 batch_size, n_threads, device_id,
                 random_shuffle=True, seed=-1, mean=None, std=None,
                 n_samples=None):
        super(DaliPipelineTrain, self).__init__(batch_size, n_threads,
                                                device_id, seed=seed)
        crop_size = _pair(crop_size)
        if mean is None:
            mean = [0.485 * 255, 0.456 * 255, 0.406 * 255]
        if std is None:
            std = [0.229 * 255, 0.224 * 255, 0.225 * 255]
        if n_samples is None:
            initial_fill = 4096
        else:
            initial_fill = min(4096, n_samples)
        self.loader = ops.FileReader(file_root=file_root, file_list=file_list,
                                     random_shuffle=random_shuffle,
                                     initial_fill=initial_fill)
        self.decode = ops.HostDecoder()
        self.resize = ops.Resize(device='gpu', resize_x=256, resize_y=256)
        # self.hue = ops.Hue(device="gpu")
        # self.bright = ops.Brightness(device="gpu")
        # self.cntrst = ops.Contrast(device="gpu")
        # self.rotate = ops.Rotate(device="gpu")
        # self.jitter = ops.Jitter(device="gpu")
        random_area = (crop_size[0] / 256.0) * (crop_size[1] / 256.0)
        random_area = _pair(random_area)
        random_aspect_ratio = _pair(1.0)
        self.rrcrop = ops.RandomResizedCrop(
            device='gpu', size=crop_size, random_area=random_area,
            random_aspect_ratio=random_aspect_ratio)
        self.cmnorm = ops.CropMirrorNormalize(
            device='gpu', crop=list(crop_size), mean=mean, std=std)
        self.coin = ops.CoinFlip(probability=0.5)

</source>
<source file="systems/chainer-7.2.0/examples/imagenet/dali_util.py" startline="82" endline="104" pcid="10558">
    def __init__(self, file_list, file_root, crop_size,
                 batch_size, n_threads, device_id,
                 random_shuffle=False, seed=-1, mean=None, std=None,
                 n_samples=None):
        super(DaliPipelineVal, self).__init__(batch_size, n_threads,
                                              device_id, seed=seed)
        crop_size = _pair(crop_size)
        if mean is None:
            mean = [0.485 * 255, 0.456 * 255, 0.406 * 255]
        if std is None:
            std = [0.229 * 255, 0.224 * 255, 0.225 * 255]
        if n_samples is None:
            initial_fill = 512
        else:
            initial_fill = min(512, n_samples)
        self.loader = ops.FileReader(file_root=file_root, file_list=file_list,
                                     random_shuffle=random_shuffle,
                                     initial_fill=initial_fill)
        self.decode = ops.HostDecoder()
        self.resize = ops.Resize(device='gpu', resize_x=256, resize_y=256)
        self.cmnorm = ops.CropMirrorNormalize(
            device='gpu', crop=list(crop_size), mean=mean, std=std)

</source>
</class>

<class classid="278" nclones="2" nlines="25" similarity="77">
<source file="systems/chainer-7.2.0/examples/imagenet/dali_util.py" startline="125" endline="159" pcid="10561">
    def __call__(self, inputs, device=None):
        """Convert DALI arrays to Numpy/CuPy arrays"""

        xp = chainer.backend.get_array_module(self.perturbation)
        if xp is not cuda.cupy:
            self.perturbation = cuda.to_gpu(self.perturbation, device)

        outputs = []
        for i in range(len(inputs)):
            x = inputs[i].as_tensor()
            if (isinstance(x, dali.backend_impl.TensorCPU)):
                x = np.array(x)
                if x.ndim == 2 and x.shape[1] == 1:
                    x = x.squeeze(axis=1)
                if device is not None and device >= 0:
                    x = cuda.to_gpu(x, device)
            elif (isinstance(x, dali.backend_impl.TensorGPU)):
                x_cupy = cuda.cupy.empty(shape=x.shape(), dtype=x.dtype())
                # Synchronization is necessary here to avoid data corruption
                # because DALI and CuPy will use different CUDA streams.
                cuda.cupy.cuda.runtime.deviceSynchronize()
                # copy data from DALI array to CuPy array
                x.copy_to_external(ctypes.c_void_p(x_cupy.data.ptr))
                cuda.cupy.cuda.runtime.deviceSynchronize()
                x = x_cupy.astype(chainer.get_dtype())
                if self.perturbation is not None:
                    x = x - self.perturbation
                if device is not None and device < 0:
                    x = cuda.to_cpu(x)
            else:
                raise ValueError('Unexpected object')
            outputs.append(x)
        return tuple(outputs)


</source>
<source file="systems/chainer-7.2.0/examples/imagenet/dali_util.py" startline="160" endline="186" pcid="10562">
def dali_converter(inputs, device=None):
    """Convert DALI arrays to Numpy/CuPy arrays"""

    outputs = []
    for i in range(len(inputs)):
        x = inputs[i].as_tensor()
        if (isinstance(x, dali.backend_impl.TensorCPU)):
            x = np.array(x)
            if x.ndim == 2 and x.shape[1] == 1:
                x = x.squeeze(axis=1)
            if device is not None and device >= 0:
                x = cuda.to_gpu(x, device)
        elif (isinstance(x, dali.backend_impl.TensorGPU)):
            x_cupy = cuda.cupy.empty(shape=x.shape(), dtype=x.dtype())
            # Synchronization is necessary here to avoid data corruption
            # because DALI and CuPy will use different CUDA streams.
            cuda.cupy.cuda.runtime.deviceSynchronize()
            # copy data from DALI array to CuPy array
            x.copy_to_external(ctypes.c_void_p(x_cupy.data.ptr))
            cuda.cupy.cuda.runtime.deviceSynchronize()
            x = x_cupy.astype(chainer.get_dtype())
            if device is not None and device < 0:
                x = cuda.to_cpu(x)
        else:
            raise ValueError('Unexpected object')
        outputs.append(x)
    return tuple(outputs)
</source>
</class>

<class classid="279" nclones="2" nlines="121" similarity="98">
<source file="systems/chainer-7.2.0/examples/imagenet/train_imagenet.py" startline="32" endline="185" pcid="10563">
def main():
    archs = {
        'alex': alex.Alex,
        'googlenet': googlenet.GoogLeNet,
        'googlenetbn': googlenetbn.GoogLeNetBN,
        'nin': nin.NIN,
        'resnet50': resnet50.ResNet50,
        'resnext50': resnext50.ResNeXt50,
        'resnet50_nhwc': resnet50.ResNet50_Nhwc,
    }

    dtypes = {
        'float16': np.float16,
        'float32': np.float32,
        'float64': np.float64,
    }

    parser = argparse.ArgumentParser(
        description='Learning convnet from ILSVRC2012 dataset')
    parser.add_argument('train', help='Path to training image-label list file')
    parser.add_argument('val', help='Path to validation image-label list file')
    parser.add_argument('--arch', '-a', choices=archs.keys(), default='nin',
                        help='Convnet architecture')
    parser.add_argument('--batchsize', '-B', type=int, default=32,
                        help='Learning minibatch size')
    parser.add_argument('--dtype', choices=dtypes, help='Specify the dtype '
                        'used. If not supplied, the default dtype is used')
    parser.add_argument('--epoch', '-E', type=int, default=10,
                        help='Number of epochs to train')
    parser.add_argument('--device', '-d', type=str, default='-1',
                        help='Device specifier. Either ChainerX device '
                        'specifier or an integer. If non-negative integer, '
                        'CuPy arrays with specified device id are used. If '
                        'negative integer, NumPy arrays are used')
    parser.add_argument('--initmodel',
                        help='Initialize the model from given file')
    parser.add_argument('--loaderjob', '-j', type=int,
                        help='Number of parallel data loading processes')
    parser.add_argument('--mean', '-m', default='mean.npy',
                        help='Mean file (computed by compute_mean.py)')
    parser.add_argument('--resume', '-r', default='',
                        help='Initialize the trainer from given file')
    parser.add_argument('--out', '-o', default='result',
                        help='Output directory')
    parser.add_argument('--root', '-R', default='.',
                        help='Root directory path of image files')
    parser.add_argument('--val_batchsize', '-b', type=int, default=250,
                        help='Validation minibatch size')
    parser.add_argument('--test', action='store_true')
    parser.set_defaults(test=False)
    parser.add_argument('--dali', action='store_true')
    parser.set_defaults(dali=False)
    group = parser.add_argument_group('deprecated arguments')
    group.add_argument('--gpu', '-g', dest='device',
                       type=int, nargs='?', const=0,
                       help='GPU ID (negative value indicates CPU)')
    args = parser.parse_args()

    device = chainer.get_device(args.device)

    # Set the dtype if supplied.
    if args.dtype is not None:
        chainer.config.dtype = args.dtype

    print('Device: {}'.format(device))
    print('Dtype: {}'.format(chainer.config.dtype))
    print('# Minibatch-size: {}'.format(args.batchsize))
    print('# epoch: {}'.format(args.epoch))
    print('')

    # Initialize the model to train
    model = archs[args.arch]()
    if args.initmodel:
        print('Load model from {}'.format(args.initmodel))
        chainer.serializers.load_npz(args.initmodel, model)
    model.to_device(device)
    device.use()

    # Load the mean file
    mean = np.load(args.mean)
    if args.dali:
        if not dali_util._dali_available:
            raise RuntimeError('DALI seems not available on your system.')
        if device.xp is not chainer.backend.cuda.cupy:
            raise RuntimeError('Using DALI requires GPU device. Please '
                               'specify it with --device option.')
        n_threads = args.loaderjob
        if n_threads is None or n_threads <= 0:
            n_threads = 1
        ch_mean = list(np.average(mean, axis=(1, 2)))
        ch_std = [255.0, 255.0, 255.0]
        # Setup DALI pipelines
        train_pipe = dali_util.DaliPipelineTrain(
            args.train, args.root, model.insize, args.batchsize,
            n_threads, device.device.id, True, mean=ch_mean, std=ch_std)
        val_pipe = dali_util.DaliPipelineVal(
            args.val, args.root, model.insize, args.val_batchsize,
            n_threads, device.device.id, False, mean=ch_mean, std=ch_std)
        train_iter = chainer.iterators.DaliIterator(train_pipe)
        val_iter = chainer.iterators.DaliIterator(val_pipe, repeat=False)
        # converter = dali_converter
        converter = dali_util.DaliConverter(mean=mean, crop_size=model.insize)
    else:
        # Load the dataset files
        train = PreprocessedDataset(args.train, args.root, mean, model.insize)
        val = PreprocessedDataset(args.val, args.root, mean, model.insize,
                                  False)
        # These iterators load the images with subprocesses running in parallel
        # to the training/validation.
        train_iter = chainer.iterators.MultiprocessIterator(
            train, args.batchsize, n_processes=args.loaderjob)
        val_iter = chainer.iterators.MultiprocessIterator(
            val, args.val_batchsize, repeat=False, n_processes=args.loaderjob)
        converter = dataset.concat_examples

    # Set up an optimizer
    optimizer = chainer.optimizers.MomentumSGD(lr=0.01, momentum=0.9)
    optimizer.setup(model)

    # Set up a trainer
    updater = training.updaters.StandardUpdater(
        train_iter, optimizer, converter=converter, device=device)
    trainer = training.Trainer(updater, (args.epoch, 'epoch'), args.out)

    val_interval = (100000, 'iteration')
    log_interval = (1000, 'iteration')
    if args.test:
        val_interval = (1, 'iteration')
        log_interval = (1, 'iteration')

    trainer.extend(extensions.Evaluator(val_iter, model, converter=converter,
                                        device=device), trigger=val_interval)
    # TODO(sonots): Temporarily disabled for chainerx. Fix it.
    if device.xp is not chainerx:
        trainer.extend(extensions.DumpGraph('main/loss'))
    trainer.extend(extensions.snapshot(), trigger=val_interval)
    trainer.extend(extensions.snapshot_object(
        model, 'model_iter_{.updater.iteration}'), trigger=val_interval)
    # Be careful to pass the interval directly to LogReport
    # (it determines when to emit log rather than when to read observations)
    trainer.extend(extensions.LogReport(trigger=log_interval))
    trainer.extend(extensions.observe_lr(), trigger=log_interval)
    trainer.extend(extensions.PrintReport([
        'epoch', 'iteration', 'main/loss', 'validation/main/loss',
        'main/accuracy', 'validation/main/accuracy', 'lr'
    ]), trigger=log_interval)
    trainer.extend(extensions.ProgressBar(update_interval=10))

    if args.resume:
        chainer.serializers.load_npz(args.resume, trainer)

    trainer.run()


</source>
<source file="systems/chainer-7.2.0/examples/imagenet/.testdata/replacements/train_imagenet.py" startline="32" endline="189" pcid="10591">
def main():
    archs = {
        'alex': alex.Alex,
        'googlenet': googlenet.GoogLeNet,
        'googlenetbn': googlenetbn.GoogLeNetBN,
        'nin': nin.NIN,
        'resnet50': resnet50.ResNet50,
        'resnext50': resnext50.ResNeXt50,
        'resnet50_nhwc': resnet50.ResNet50_Nhwc,
    }

    dtypes = {
        'float16': np.float16,
        'float32': np.float32,
        'float64': np.float64,
    }

    parser = argparse.ArgumentParser(
        description='Learning convnet from ILSVRC2012 dataset')
    parser.add_argument('train', help='Path to training image-label list file')
    parser.add_argument('val', help='Path to validation image-label list file')
    parser.add_argument('--arch', '-a', choices=archs.keys(), default='nin',
                        help='Convnet architecture')
    parser.add_argument('--batchsize', '-B', type=int, default=32,
                        help='Learning minibatch size')
    parser.add_argument('--dtype', choices=dtypes, help='Specify the dtype '
                        'used. If not supplied, the default dtype is used')
    parser.add_argument('--epoch', '-E', type=int, default=10,
                        help='Number of epochs to train')
    parser.add_argument('--device', '-d', type=str, default='-1',
                        help='Device specifier. Either ChainerX device '
                        'specifier or an integer. If non-negative integer, '
                        'CuPy arrays with specified device id are used. If '
                        'negative integer, NumPy arrays are used')
    parser.add_argument('--initmodel',
                        help='Initialize the model from given file')
    parser.add_argument('--loaderjob', '-j', type=int,
                        help='Number of parallel data loading processes')
    parser.add_argument('--mean', '-m', default='mean.npy',
                        help='Mean file (computed by compute_mean.py)')
    parser.add_argument('--resume', '-r', default='',
                        help='Initialize the trainer from given file')
    parser.add_argument('--out', '-o', default='result',
                        help='Output directory')
    parser.add_argument('--root', '-R', default='.',
                        help='Root directory path of image files')
    parser.add_argument('--val_batchsize', '-b', type=int, default=250,
                        help='Validation minibatch size')
    parser.add_argument('--test', action='store_true')
    parser.set_defaults(test=False)
    parser.add_argument('--dali', action='store_true')
    parser.set_defaults(dali=False)
    group = parser.add_argument_group('deprecated arguments')
    group.add_argument('--gpu', '-g', dest='device',
                       type=int, nargs='?', const=0,
                       help='GPU ID (negative value indicates CPU)')
    args = parser.parse_args()

    device = chainer.get_device(args.device)

    # Set the dtype if supplied.
    if args.dtype is not None:
        chainer.config.dtype = args.dtype

    print('Device: {}'.format(device))
    print('Dtype: {}'.format(chainer.config.dtype))
    print('# Minibatch-size: {}'.format(args.batchsize))
    print('# epoch: {}'.format(args.epoch))
    print('')

    # Initialize the model to train
    model = archs[args.arch]()
    if args.initmodel:
        print('Load model from {}'.format(args.initmodel))
        chainer.serializers.load_npz(args.initmodel, model)
    model.to_device(device)
    device.use()

    # Load the mean file
    mean = np.load(args.mean)
    if args.dali:
        if not dali_util._dali_available:
            raise RuntimeError('DALI seems not available on your system.')
        if device.xp is not chainer.backend.cuda.cupy:
            raise RuntimeError('Using DALI requires GPU device. Please '
                               'specify it with --device option.')
        n_threads = args.loaderjob
        if n_threads is None or n_threads <= 0:
            n_threads = 1
        ch_mean = list(np.average(mean, axis=(1, 2)))
        ch_std = [255.0, 255.0, 255.0]
        # Setup DALI pipelines
        train_pipe = dali_util.DaliPipelineTrain(
            args.train, args.root, model.insize, args.batchsize,
            n_threads, device.device.id, True, mean=ch_mean, std=ch_std)
        val_pipe = dali_util.DaliPipelineVal(
            args.val, args.root, model.insize, args.val_batchsize,
            n_threads, device.device.id, False, mean=ch_mean, std=ch_std)
        train_iter = chainer.iterators.DaliIterator(train_pipe)
        val_iter = chainer.iterators.DaliIterator(val_pipe, repeat=False)
        # converter = dali_converter
        converter = dali_util.DaliConverter(mean=mean, crop_size=model.insize)
    else:
        # Load the dataset files
        train = PreprocessedDataset(args.train, args.root, mean, model.insize)
        val = PreprocessedDataset(args.val, args.root, mean, model.insize,
                                  False)
        # These iterators load the images with subprocesses running in parallel
        # to the training/validation.
        train_iter = chainer.iterators.MultiprocessIterator(
            train, args.batchsize, n_processes=args.loaderjob)
        val_iter = chainer.iterators.MultiprocessIterator(
            val, args.val_batchsize, repeat=False, n_processes=args.loaderjob)
        converter = dataset.concat_examples

    # Set up an optimizer
    optimizer = chainer.optimizers.MomentumSGD(lr=0.01, momentum=0.9)
    optimizer.setup(model)

    # Set up a trainer
    updater = training.updaters.StandardUpdater(
        train_iter, optimizer, converter=converter, device=device)
    trainer = training.Trainer(updater, (args.epoch, 'epoch'), args.out)

    val_interval = (100000, 'iteration')
    log_interval = (1000, 'iteration')
    if args.test:
        val_interval = (1, 'iteration')
        log_interval = (1, 'iteration')
    # BEGIN ADDITIONAL TEST CODE
    val_interval = (1, 'iteration')
    log_interval = (1, 'iteration')
    # END ADDITIONAL TEST CODE

    trainer.extend(extensions.Evaluator(val_iter, model, converter=converter,
                                        device=device), trigger=val_interval)
    # TODO(sonots): Temporarily disabled for chainerx. Fix it.
    if device.xp is not chainerx:
        trainer.extend(extensions.DumpGraph('main/loss'))
    trainer.extend(extensions.snapshot(), trigger=val_interval)
    trainer.extend(extensions.snapshot_object(
        model, 'model_iter_{.updater.iteration}'), trigger=val_interval)
    # Be careful to pass the interval directly to LogReport
    # (it determines when to emit log rather than when to read observations)
    trainer.extend(extensions.LogReport(trigger=log_interval))
    trainer.extend(extensions.observe_lr(), trigger=log_interval)
    trainer.extend(extensions.PrintReport([
        'epoch', 'iteration', 'main/loss', 'validation/main/loss',
        'main/accuracy', 'validation/main/accuracy', 'lr'
    ]), trigger=log_interval)
    trainer.extend(extensions.ProgressBar(update_interval=10))

    if args.resume:
        chainer.serializers.load_npz(args.resume, trainer)

    trainer.run()


</source>
</class>

<class classid="280" nclones="9" nlines="17" similarity="76">
<source file="systems/chainer-7.2.0/chainerx/_docs/device.py" startline="5" endline="53" pcid="10688">
def _set_docs_device():
    Device = chainerx.Device

    _docs.set_doc(
        Device,
        """Represents a physical computing unit.
""")

    _docs.set_doc(
        Device.synchronize,
        """Synchronizes the device.
""")

    _docs.set_doc(
        Device.name,
        """Device name.

It is the backend name and the device index concatenated with a colon, e.g.
``native:0``.

Returns:
    str: Device name.
""")

    _docs.set_doc(
        Device.backend,
        """Backend to which this device belongs.

Returns:
    ~chainerx.Backend: Backend object.
""")

    _docs.set_doc(
        Device.context,
        """Context to which this device belongs.

Returns:
    ~chainerx.Context: Context object.
""")

    _docs.set_doc(
        Device.index,
        """Index of this device.

Returns:
    int: Index of this device.
""")


</source>
<source file="systems/chainer-7.2.0/chainerx/_docs/backend.py" startline="5" endline="54" pcid="10691">
def _set_docs_backend():
    Backend = chainerx.Backend

    _docs.set_doc(
        Backend,
        """Pluggable entity that abstracts various computing platforms.

A backend holds one or more :class:`~chainerx.Device`\\ s, each of which
represents a physical computing unit.
""")

    _docs.set_doc(
        Backend.name,
        """Backend name.

Returns:
    str: Backend name.
""")

    _docs.set_doc(
        Backend.context,
        """Context to which this backend belongs.

Returns:
    ~chainerx.Context: Context object.

""")

    _docs.set_doc(
        Backend.get_device,
        """get_device(index)
Returns a device specified by the given index.

Args:
    index (int): Device index.

Returns:
    ~chainerx.Device: Device object.
""")

    _docs.set_doc(
        Backend.get_device_count,
        """get_device_count()
Returns the number of devices available in this backend.

Returns:
    int: Number of devices.
""")


</source>
<source file="systems/chainer-7.2.0/chainerx/_docs/backprop.py" startline="5" endline="147" pcid="10693">
def set_docs():
    _docs.set_doc(
        chainerx.backward,
        """backward(outputs, *, enable_double_backprop=False)
Runs backpropagation.

On backpropagation (a.k.a. backprop),
the computational graph is traversed backward starting from the output arrays,
up until the root arrays on which :func:`ndarray.require_grad()` have been
called.

Backpropagation uses :data:`ndarray.grad <chainerx.ndarray.grad>` held by
the output arrays as the initial gradients.
You can manually assign them before calling this function.
Otherwise, they are assumed to be 1.

To enable higher order differentiation, pass ``enable_double_backprop=True``
so that you can further run backpropagation from the resulting gradient arrays.
Note that enabling it results in larger memory consumption needed to store the
gradients w.r.t intermediate arrays that are required for the second gradient
computation.

Note:
    The whole process of backpropagation is executed in C++, except those
    operations whose backward computation falls back to the corresponding
    Python implementation. Currently this function does not release the GIL at
    all.

Args:
    outputs (~chainerx.ndarray or list of ndarrays):
        Output arrays from which backpropagation starts.
    enable_double_backprop (bool): If ``True``,
        a computational trace of the whole backpropagation procedure is
        recorded to the computational graph so that one can further do
        backpropagation from the resulting gradients.

.. seealso::
    * :meth:`chainerx.ndarray.backward`
""")

    _docs.set_doc(
        chainerx.grad,
        """grad(outputs, inputs, *, enable_double_backprop=False)
Computes and returns the gradients of the outputs w.r.t. the inputs.

This function differs from :func:`chainerx.backward` in the sense that
gradients are returned instead of being added to the gradients held by the
inputs. Gradients held by the inputs are not modified. Also, instead of
traversing through the whole graph starting from the outputs, a sub-graph is
extracted for computation. This means that is is more efficient, especially
for larger computational graphs.

Args:
    outputs (list of ndarrays):
        Output arrays from which backpropagation starts.
    inputs (list of ndarrays):
        Input arrays of which this function computes the gradients w.r.t.
    enable_double_backprop (bool): If ``True``,
        a computational trace of the whole backpropagation procedure is
        recorded to the computational graph so that one can further do
        backpropagation from the resulting gradients.

Returns:
    list of :class:`~chainerx.ndarray`\\ s:
        A list of gradients. The list always has the same length as the number
        of inputs.

.. seealso::
    * :func:`chainerx.backward`
    * :func:`chainer.grad`
""")

    _docs.set_doc(
        chainerx.no_backprop_mode,
        """no_backprop_mode()
Creates a context manager which temporarily disables backpropagation.

Within this context, no computational graph will be formed unless
:meth:`~chainerx.force_backprop_mode` is used.

Arrays resulting from operations enclosed with this context will be
disconnected from the computational graph. Trying to perform backpropagation
from such arrays would result in an error.

.. code-block:: py

    x = chainerx.array([4, 3], numpy.float32)
    x.require_grad()

    with chainerx.no_backprop_mode():
        y = 2 * x + 1

    y.backward()  # ! error

Benefits of ``no_backprop_mode`` include reduced CPU overhead of building
computational graphs, and reduced consumption of device memory that
would be otherwise retained for backward propagation.

.. seealso::
    * :func:`chainerx.force_backprop_mode`
    * :func:`chainerx.is_backprop_required`
    * :func:`chainer.no_backprop_mode`
""")

    _docs.set_doc(
        chainerx.force_backprop_mode,
        """force_backprop_mode()
Creates a context manager which temporarily enables backpropagation.

This context re-enables backpropagation that is disabled by
any surrounding :func:`~chainerx.no_backprop_mode` context.

.. code-block:: py

    x = chainerx.array([4, 3], numpy.float32)
    x.require_grad()

    with chainerx.no_backprop_mode():
        with chainerx.force_backprop_mode():
            y = 2 * x + 1

    y.backward()
    x.grad
    # array([2., 2.], shape=(2,), dtype=float32, device='native:0')

.. seealso::
    * :func:`chainerx.no_backprop_mode`
    * :func:`chainerx.is_backprop_required`
    * :func:`chainer.force_backprop_mode`
""")

    _docs.set_doc(
        chainerx.is_backprop_required,
        """is_backprop_required()
Returns whether the backpropagation is enabled in the current thread.

The result is affect by :func:`chainerx.no_backprop_mode` and
:func:`chainerx.force_backprop_mode`.

.. seealso::
    * :func:`chainerx.no_backprop_mode`
    * :func:`chainerx.force_backprop_mode`
""")
</source>
<source file="systems/chainer-7.2.0/chainerx/_docs/routines.py" startline="1119" endline="1273" pcid="10700">
def _docs_loss():
    _docs.set_doc(
        chainerx.absolute_error,
        """Element-wise absolute error function.

Computes the element-wise absolute error :math:`L` between two inputs
:math:`x_1` and :math:`x_2` defined as follows.

.. math::
    L = |x_1 - x_2|

Args:
    x1 (~chainerx.ndarray): Input variable.
    x2 (~chainerx.ndarray): Input variable.

Returns:
    :class:`~chainerx.ndarray`: A variable holding an array representing
    the absolute error of two inputs.

.. seealso:: :func:`chainer.functions.absolute_error`
""")

    _docs.set_doc(
        chainerx.squared_error,
        """Element-wise squared error function.

Computes the element-wise squared error :math:`L` between two inputs
:math:`x_1` and :math:`x_2` defined as follows.

.. math::
    L = (x_1 - x_2)^2

Can be used to compute mean squared error by just calling `mean()`
on the output array.

Args:
    x0 (~chainerx.ndarray): Input variable.
    x1 (~chainerx.ndarray): Input variable.

Returns:
    :class:`~chainerx.ndarray`: A variable holding an array representing
    the squared error of two inputs.

.. seealso:: :func:`chainer.functions.squared_error`
""")

    _docs.set_doc(
        chainerx.huber_loss,
        """Element-wise Huber loss.

The Huber loss is similar to the squared error but is less sensitive to
outliers in the data. It is defined as

.. math::

    L_{\\delta}(a) = \\left \\{ \\begin{array}{cc}
    \\frac{1}{2} a^2 & {\\rm if~|a| \\leq \\delta} \\\\
    \\delta (|a| - \\frac{1}{2} \\delta) & {\\rm otherwise,}
    \\end{array} \\right.

where :math:`a = x - t` is the difference between the input :math:`x`
and the target :math:`t`.

See: `Huber loss - Wikipedia <https://en.wikipedia.org/wiki/Huber_loss>`_.

Args:
    x (~chainerx.ndarray): Input variable.
    t (~chainerx.ndarray): Target variable for regression.
    delta (float): Constant variable for Huber loss function as used in
        definition.

Returns:
    :class:`~chainerx.ndarray`:
        A variable object holding an array representing the Huber loss
        :math:`L_{\\delta}` of the two inputs.

.. seealso:: :func:`chainer.functions.huber_loss`
""")

    _docs.set_doc(
        chainerx.gaussian_kl_divergence,
        """Element-wise KL-divergence of Gaussian variables from the standard one.

Given two variable ``mean`` representing :math:`\\mu` and ``ln_var``
representing :math:`\\log(\\sigma^2)`, this function calculates
the element-wise KL-divergence between the given multi-dimensional
Gaussian :math:`N(\\mu, S)` and the standard Gaussian :math:`N(0, I)`

.. math::

   D_{\\mathbf{KL}}(N(\\mu, S) \\| N(0, I)),

where :math:`S` is a diagonal matrix such that :math:`S_{ii} = \\sigma_i^2`
and :math:`I` is an identity matrix.

Args:
    mean (~chainerx.ndarray):
        A variable representing mean of given
        gaussian distribution, :math:`\\mu`.
    ln_var (~chainerx.ndarray):
        A variable representing logarithm of
        variance of given gaussian distribution, :math:`\\log(\\sigma^2)`.

Returns:
    :class:`~chainerx.ndarray`:
        A variable representing KL-divergence between
        given gaussian distribution and the standard gaussian.

.. seealso:: :func:`chainer.functions.gaussian_kl_divergence`
""")

    _docs.set_doc(
        chainerx.sigmoid_cross_entropy,
        """sigmoid_cross_entropy(x1, x2)

Element-wise cross entropy loss for pre-sigmoid activations.

Args:
    x1 (~chainerx.ndarray): An array whose (i, j)-th element indicates the
        unnormalized log probability of the j-th unit at the i-th example.
    x2 (~chainerx.ndarray): An array whose (i, j)-th element indicates a signed
        integer vector of ground truth labels 0 or 1. If ``x2[i, j] == -1``,
        corresponding ``x1[i, j]`` is ignored. Loss is zero if all ground truth
        labels are -1.

Returns:
    :class:`~chainerx.ndarray`: An array of the cross entropy.

Note:
    During backpropagation, this function propagates the gradient of the output
    array to the input array ``x1`` only.
""")

    _docs.set_doc(
        chainerx.softmax_cross_entropy,
        """softmax_cross_entropy(x1, x2)

Element-wise cross entropy loss for pre-softmax activations.

Args:
    x1 (~chainerx.ndarray): An array whose element indicates unnormalized log
        probability: the first axis of the array represents the number of
        samples, and the second axis represents the number of classes.
    x2 (~chainerx.ndarray): A signed integer vector of ground truth labels. If
        ``x2[i] == -1``, corresponding ``x1[i]`` is ignored.

Returns:
    :class:`~chainerx.ndarray`: An array of the cross entropy.

Note:
    During backpropagation, this function propagates the gradient of the output
    array to the input array ``x1`` only.
""")


</source>
<source file="systems/chainer-7.2.0/chainerx/_docs/routines.py" startline="3362" endline="3945" pcid="10708">
def _docs_rnn():
    _docs.set_doc(
        chainerx.n_step_lstm,
        """n_step_lstm(n_layers, hx, cx, ws, bs, xs)
    Stacked Uni-directional Long Short-Term Memory function.

This function calculates stacked Uni-directional LSTM with sequences.
This function gets an initial hidden state :math:`h_0`, an initial cell
state :math:`c_0`, an input sequence :math:`x`, weight matrices :math:`W`,
and bias vectors :math:`b`.
This function calculates hidden states :math:`h_t` and :math:`c_t` for each
time :math:`t` from input :math:`x_t`.

.. math::
   i_t &= \\sigma(W_0 x_t + W_4 h_{t-1} + b_0 + b_4) \\\\
   f_t &= \\sigma(W_1 x_t + W_5 h_{t-1} + b_1 + b_5) \\\\
   o_t &= \\sigma(W_2 x_t + W_6 h_{t-1} + b_2 + b_6) \\\\
   a_t &= \\tanh(W_3 x_t + W_7 h_{t-1} + b_3 + b_7) \\\\
   c_t &= f_t \\cdot c_{t-1} + i_t \\cdot a_t \\\\
   h_t &= o_t \\cdot \\tanh(c_t)

As the function accepts a sequence, it calculates :math:`h_t` for all
:math:`t` with one call. Eight weight matrices and eight bias vectors are
required for each layer. So, when :math:`S` layers exist, you need to
prepare :math:`8S` weight matrices and :math:`8S` bias vectors.
If the number of layers ``n_layers`` is greater than :math:`1`, the input
of the ``k``-th layer is the hidden state ``h_t`` of the ``k-1``-th layer.
Note that all input variables except the first layer may have different
shape from the first layer.

Args:
    n_layers(int): The number of layers.
    hx (:class:`~chainerx.array`):
        Variable holding stacked hidden states.
        Its shape is ``(S, B, N)`` where ``S`` is the number of layers and
        is equal to ``n_layers``, ``B`` is the mini-batch size, and ``N``
        is the dimension of the hidden units.
    cx (:class:`~chainerx.array`): Variable holding stacked cell states.
        It has the same shape as ``hx``.
    ws (list of list of :class:`~chainerx.array`): Weight matrices.
        ``ws[i]`` represents the weights for the i-th layer.
        Each ``ws[i]`` is a list containing eight matrices.
        ``ws[i][j]`` corresponds to :math:`W_j` in the equation.
        Only ``ws[0][j]`` where ``0 <= j < 4`` are ``(N, I)``-shaped as
        they are multiplied with input variables, where ``I`` is the size
        of the input and ``N`` is the dimension of the hidden units. All
        other matrices are ``(N, N)``-shaped.
    bs (list of list of :class:`~chainerx.array`): Bias vectors.
        ``bs[i]`` represents the biases for the i-th layer.
        Each ``bs[i]`` is a list containing eight vectors.
        ``bs[i][j]`` corresponds to :math:`b_j` in the equation.
        The shape of each matrix is ``(N,)`` where ``N`` is the dimension
        of the hidden units.
    xs (list of :class:`~chainerx.array`):
        A list of :class:`~chainerx.array`
        holding input values. Each element ``xs[t]`` holds input value
        for time ``t``. Its shape is ``(B_t, I)``, where ``B_t`` is the
        mini-batch size for time ``t``.
        When sequences has different lengths, they must be
        sorted in descending order of their lengths.
        So ``xs`` needs to satisfy
        ``xs[t].shape[0] >= xs[t + 1].shape[0]``.

Returns:
    tuple: This function returns a tuple containing three elements,
    ``hy``, ``cy`` and ``ys``.

    - ``hy`` is an updated hidden states whose shape is the same as
      ``hx``.
    - ``cy`` is an updated cell states whose shape is the same as
      ``cx``.
    - ``ys`` is a list of :class:`~chainerx.array` . Each element
      ``ys[t]`` holds hidden states of the last layer corresponding
      to an input ``xs[t]``. Its shape is ``(B_t, N)`` where ``B_t`` is
      the mini-batch size for time ``t``, and ``N`` is size of hidden
      units. Note that ``B_t`` is the same value as ``xs[t]``.

.. note::
   The dimension of hidden units is limited to only one size ``N``. If you
   want to use variable dimension of hidden units, please use
   :class:`chainerx.lstm`.

.. seealso::
   :func:`chainerx.lstm`

.. admonition:: Example

    >>> import chainerx as chx
    >>> batchs = [3, 2, 1]  # support variable length sequences
    >>> in_size, out_size, n_layers = 3, 2, 2
    >>> xs = [chx.ones((b, in_size)).astype(chx.float32) for b in batchs]
    >>> [x.shape for x in xs]
    [(3, 3), (2, 3), (1, 3)]
    >>> h_shape = (n_layers, batchs[0], out_size)
    >>> hx = chx.ones(h_shape).astype(chx.float32)
    >>> cx = chx.ones(h_shape).astype(chx.float32)
    >>> w_in = lambda i, j: in_size if i == 0 and j < 4 else out_size
    >>> ws = []
    >>> bs = []
    >>> for n in range(n_layers):
    ...     ws.append([chx.ones((out_size, w_in(n, i))).\
astype(np.float32) for i in range(8)])
    ...     bs.append([chx.ones((out_size,)).astype(chx.float32) \
for _ in range(8)])
    ...
    >>> ws[0][0].shape  # ws[0][:4].shape are (out_size, in_size)
    (2, 3)
    >>> ws[1][0].shape  # others are (out_size, out_size)
    (2, 2)
    >>> bs[0][0].shape
    (2,)
    >>> hy, cy, ys = chx.n_step_lstm(
    ...     n_layers, hx, cx, ws, bs, xs)
    >>> hy.shape
    (2, 3, 2)
    >>> cy.shape
    (2, 3, 2)
    >>> [y.shape for y in ys]
    [(3, 2), (2, 2), (1, 2)]
""")

    _docs.set_doc(
        chainerx.n_step_bilstm,
        """n_step_bilstm(n_layers, hx, cx, ws, bs, xs)
Stacked Bi-directional Long Short-Term Memory function.
This function calculates stacked Bi-directional LSTM with sequences.
This function gets an initial hidden state :math:`h_0`, an initial cell
state :math:`c_0`, an input sequence :math:`x`, weight matrices :math:`W`,
and bias vectors :math:`b`.
This function calculates hidden states :math:`h_t` and :math:`c_t` for each
time :math:`t` from input :math:`x_t`.

.. math::
    i^{f}_t &=& \\sigma(W^{f}_0 x_t + W^{f}_4 h_{t-1} + b^{f}_0 + b^{f}_4),
    \\\\
    f^{f}_t &=& \\sigma(W^{f}_1 x_t + W^{f}_5 h_{t-1} + b^{f}_1 + b^{f}_5),
    \\\\
    o^{f}_t &=& \\sigma(W^{f}_2 x_t + W^{f}_6 h_{t-1} + b^{f}_2 + b^{f}_6),
    \\\\
    a^{f}_t &=& \\tanh(W^{f}_3 x_t + W^{f}_7 h_{t-1} + b^{f}_3 + b^{f}_7),
    \\\\
    c^{f}_t &=& f^{f}_t \\cdot c^{f}_{t-1} + i^{f}_t \\cdot a^{f}_t,
    \\\\
    h^{f}_t &=& o^{f}_t \\cdot \\tanh(c^{f}_t),
    \\\\
    i^{b}_t &=& \\sigma(W^{b}_0 x_t + W^{b}_4 h_{t-1} + b^{b}_0 + b^{b}_4),
    \\\\
    f^{b}_t &=& \\sigma(W^{b}_1 x_t + W^{b}_5 h_{t-1} + b^{b}_1 + b^{b}_5),
    \\\\
    o^{b}_t &=& \\sigma(W^{b}_2 x_t + W^{b}_6 h_{t-1} + b^{b}_2 + b^{b}_6),
    \\\\
    a^{b}_t &=& \\tanh(W^{b}_3 x_t + W^{b}_7 h_{t-1} + b^{b}_3 + b^{b}_7),
    \\\\
    c^{b}_t &=& f^{b}_t \\cdot c^{b}_{t-1} + i^{b}_t \\cdot a^{b}_t, \\\\
    h^{b}_t &=& o^{b}_t \\cdot \\tanh(c^{b}_t), \\\\
    h_t &=& [h^{f}_t; h^{b}_t]

where :math:`W^{f}` is the weight matrices for forward-LSTM, :math:`W^{b}`
is weight matrices for backward-LSTM.
As the function accepts a sequence, it calculates :math:`h_t` for all
:math:`t` with one call. Eight weight matrices and eight bias vectors are
required for each layer of each direction. So, when :math:`S` layers
exist, you need to prepare :math:`16S` weight matrices and :math:`16S`
bias vectors.
If the number of layers ``n_layers`` is greater than :math:`1`, the input
of the ``k``-th layer is the hidden state ``h_t`` of the ``k-1``-th layer.
Note that all input variables except the first layer may have different
shape from the first layer.

Args:
    n_layers(int): The number of layers.
    hx (:class:`~chainerx.array`):
        Variable holding stacked hidden states.
        Its shape is ``(2S, B, N)`` where ``S`` is the number of layers and
        is equal to ``n_layers``, ``B`` is the mini-batch size, and ``N``
        is the dimension of the hidden units. Because of bi-direction, the
        first dimension length is ``2S``.
    cx (:class:`~chainerx.array`): Variable holding stacked cell states.
        It has the same shape as ``hx``.
    ws (list of list of :class:`~chainerx.array`): Weight matrices.
        ``ws[2 * l + m]`` represents the weights for the l-th layer of
        the m-th direction. (``m == 0`` means the forward direction and
        ``m == 1`` means the backward direction.) Each ``ws[i]`` is a
        list containing eight matrices. ``ws[i][j]`` corresponds to
        :math:`W_j` in the equation. ``ws[0][j]`` and ``ws[1][j]`` where
        ``0 <= j < 4`` are ``(N, I)``-shaped because they are multiplied
        with input variables, where ``I`` is the size of the input.
        ``ws[i][j]`` where ``2 <= i`` and ``0 <= j < 4`` are
        ``(N, 2N)``-shaped because they are multiplied with two hidden
        layers :math:`h_t = [h^{f}_t; h^{b}_t]`. All other matrices are
        ``(N, N)``-shaped.
    bs (list of list of :class:`~chainerx.array`): Bias vectors.
        ``bs[2 * l + m]`` represents the weights for the l-th layer of
        m-th direction. (``m == 0`` means the forward direction and
        ``m == 1`` means the backward direction.)
        Each ``bs[i]`` is a list containing eight vectors.
        ``bs[i][j]`` corresponds to :math:`b_j` in the equation.
        The shape of each matrix is ``(N,)``.
    xs (list of :class:`~chainerx.array`):
        A list of :class:`~chainerx.array`
        holding input values. Each element ``xs[t]`` holds input value
        for time ``t``. Its shape is ``(B_t, I)``, where ``B_t`` is the
        mini-batch size for time ``t``.
        When sequences has different lengths, they must be
        sorted in descending order of their lengths.
        So ``xs`` needs to satisfy
        ``xs[t].shape[0] >= xs[t + 1].shape[0]``.

Returns:
    tuple: This function returns a tuple containing three elements,
    ``hy``, ``cy`` and ``ys``.

    - ``hy`` is an updated hidden states whose shape is the same as
      ``hx``.
    - ``cy`` is an updated cell states whose shape is the same as
      ``cx``.
    - ``ys`` is a list of :class:`~chainer.array` . Each element
      ``ys[t]`` holds hidden states of the last layer corresponding
      to an input ``xs[t]``. Its shape is ``(B_t, 2N)`` where ``B_t``
      is the mini-batch size for time ``t``, and ``N`` is size of
      hidden units. Note that ``B_t`` is the same value as ``xs[t]``.

.. admonition:: Example

    >>> import chainerx as chx
    >>> batchs = [3, 2, 1]  # support variable length sequences
    >>> in_size, out_size, n_layers = 3, 2, 2
    >>> dropout_ratio = 0.0
    >>> xs = [chx.ones((b, in_size)).astype(chx.float32) for b in batchs]
    >>> [x.shape for x in xs]
    [(3, 3), (2, 3), (1, 3)]
    >>> h_shape = (n_layers * 2, batchs[0], out_size)
    >>> hx = chx.ones(h_shape).astype(chx.float32)
    >>> cx = chx.ones(h_shape).astype(chx.float32)
    >>> def w_in(i, j):
    ...     if i == 0 and j < 4:
    ...         return in_size
    ...     elif i > 0 and j < 4:
    ...         return out_size * 2
    ...     else:
    ...         return out_size
    ...
    >>> ws = []
    >>> bs = []
    >>> for n in range(n_layers):
    ...     for direction in (0, 1):
    ...         ws.append([chx.ones((out_size, w_in(n, i))).\
astype(np.float32) for i in range(8)])
    ...         bs.append([chx.ones((out_size,)).astype(chx.float32) \
for _ in range(8)])
    ...
    >>> ws[0][0].shape  # ws[0:2][:4].shape are (out_size, in_size)
    (2, 3)
    >>> ws[2][0].shape  # ws[2:][:4].shape are (out_size, 2 * out_size)
    (2, 4)
    >>> ws[0][4].shape  # others are (out_size, out_size)
    (2, 2)
    >>> bs[0][0].shape
    (2,)
    >>> hy, cy, ys = chx.n_step_bilstm(
    ...     n_layers, hx, cx, ws, bs, xs)
    >>> hy.shape
    (4, 3, 2)
    >>> cy.shape
    (4, 3, 2)
    >>> [y.shape for y in ys]
    [(3, 4), (2, 4), (1, 4)]
    """)

    _docs.set_doc(
        chainerx.n_step_gru,
        """n_step_gru(n_layers, hx, ws, bs, xs)
Stacked Uni-directional Gated Recurrent Unit function.
This function calculates stacked Uni-directional GRU with sequences.
This function gets an initial hidden state :math:`h_0`, an input
sequence :math:`x`, weight matrices :math:`W`, and bias vectors :math:`b`.
This function calculates hidden states :math:`h_t` for each time :math:`t`
from input :math:`x_t`.

.. math::
   r_t &= \\sigma(W_0 x_t + W_3 h_{t-1} + b_0 + b_3) \\\\
   z_t &= \\sigma(W_1 x_t + W_4 h_{t-1} + b_1 + b_4) \\\\
   h'_t &= \\tanh(W_2 x_t + b_2 + r_t \\cdot (W_5 h_{t-1} + b_5)) \\\\
   h_t &= (1 - z_t) \\cdot h'_t + z_t \\cdot h_{t-1}

As the function accepts a sequence, it calculates :math:`h_t` for all
:math:`t` with one call. Six weight matrices and six bias vectors are
required for each layers. So, when :math:`S` layers exists, you need to
prepare :math:`6S` weight matrices and :math:`6S` bias vectors.
If the number of layers ``n_layers`` is greather than :math:`1`, input
of ``k``-th layer is hidden state ``h_t`` of ``k-1``-th layer.
Note that all input variables except first layer may have different shape
from the first layer.

Args:
    n_layers(int): Number of layers.
    hx (~chainerx.array):
        Variable holding stacked hidden states.
        Its shape is ``(S, B, N)`` where ``S`` is number of layers and is
        equal to ``n_layers``, ``B`` is mini-batch size, and ``N`` is
        dimension of hidden units.
    ws (list of list of :class:`~chainerx.array`): Weight matrices.
        ``ws[i]`` represents weights for i-th layer.
        Each ``ws[i]`` is a list containing six matrices.
        ``ws[i][j]`` is corresponding with ``W_j`` in the equation.
        Only ``ws[0][j]`` where ``0 <= j < 3`` is ``(N, I)`` shape as they
        are multiplied with input variables. All other matrices has
        ``(N, N)`` shape.
    bs (list of list of :class:`~chainerx.array`): Bias vectors.
        ``bs[i]`` represnents biases for i-th layer.
        Each ``bs[i]`` is a list containing six vectors.
        ``bs[i][j]`` is corresponding with ``b_j`` in the equation.
        Shape of each matrix is ``(N,)`` where ``N`` is dimension of
        hidden units.
    xs (list of :class:`~chainerx.array`):
        A list of :class:`~chainerx.array`
        holding input values. Each element ``xs[t]`` holds input value
        for time ``t``. Its shape is ``(B_t, I)``, where ``B_t`` is
        mini-batch size for time ``t``, and ``I`` is size of input units.
        Note that this function supports variable length sequences.
        When sequneces has different lengths, sort sequences in descending
        order by length.
        So ``xs`` needs to satisfy
        ``xs[t].shape[0] >= xs[t + 1].shape[0]``.

Returns:
    tuple: This function returns a tuple containing two elements,
    ``hy`` and ``ys``.

    - ``hy`` is an updated hidden states whose shape is same as ``hx``.
    - ``ys`` is a list of :class:`~chainerx.array` . Each element
      ``ys[t]`` holds hidden states of the last layer corresponding
      to an input ``xs[t]``. Its shape is ``(B_t, N)`` where ``B_t`` is
      mini-batch size for time ``t``, and ``N`` is size of hidden
      units. Note that ``B_t`` is the same value as ``xs[t]``
    """)

    _docs.set_doc(
        chainerx.n_step_bigru,
        """n_step_bigru(n_layers, hx, ws, bs, xs)
Stacked Bi-directional Gated Recurrent Unit function.
This function calculates stacked Bi-directional GRU with sequences.
This function gets an initial hidden state :math:`h_0`, an input
sequence :math:`x`, weight matrices :math:`W`, and bias vectors :math:`b`.
This function calculates hidden states :math:`h_t` for each time :math:`t`
from input :math:`x_t`.

.. math::
   r^{f}_t &= \\sigma(W^{f}_0 x_t + W^{f}_3 h_{t-1} + b^{f}_0 + b^{f}_3)
   \\\\
   z^{f}_t &= \\sigma(W^{f}_1 x_t + W^{f}_4 h_{t-1} + b^{f}_1 + b^{f}_4)
   \\\\
   h^{f'}_t &= \\tanh(W^{f}_2 x_t + b^{f}_2 + r^{f}_t \\cdot (W^{f}_5
   h_{t-1} + b^{f}_5)) \\\\
   h^{f}_t &= (1 - z^{f}_t) \\cdot h^{f'}_t + z^{f}_t \\cdot h_{t-1}
   \\\\
   r^{b}_t &= \\sigma(W^{b}_0 x_t + W^{b}_3 h_{t-1} + b^{b}_0 + b^{b}_3)
   \\\\
   z^{b}_t &= \\sigma(W^{b}_1 x_t + W^{b}_4 h_{t-1} + b^{b}_1 + b^{b}_4)
   \\\\
   h^{b'}_t &= \\tanh(W^{b}_2 x_t + b^{b}_2 + r^{b}_t \\cdot (W^{b}_5
   h_{t-1} + b^{b}_5)) \\\\
   h^{b}_t &= (1 - z^{b}_t) \\cdot h^{b'}_t + z^{b}_t \\cdot h_{t-1}
   \\\\
   h_t  &= [h^{f}_t; h^{b}_t] \\\\

where :math:`W^{f}` is weight matrices for forward-GRU, :math:`W^{b}` is
weight matrices for backward-GRU.
As the function accepts a sequence, it calculates :math:`h_t` for all
:math:`t` with one call. Six weight matrices and six bias vectors are
required for each layers. So, when :math:`S` layers exists, you need to
prepare :math:`6S` weight matrices and :math:`6S` bias vectors.
If the number of layers ``n_layers`` is greather than :math:`1`, input
of ``k``-th layer is hidden state ``h_t`` of ``k-1``-th layer.
Note that all input variables except first layer may have different shape
from the first layer.

Args:
    n_layers(int): Number of layers.
    hx (:class:`~chainerx.array`):
        Variable holding stacked hidden states.
        Its shape is ``(2S, B, N)`` where ``S`` is number of layers and is
        equal to ``n_layers``, ``B`` is mini-batch size, and ``N`` is
        dimension of hidden units.
    ws (list of list of :class:`~chainerx.array`): Weight matrices.
        ``ws[i]`` represents weights for i-th layer.
        Each ``ws[i]`` is a list containing six matrices.
        ``ws[i][j]`` is corresponding with ``W_j`` in the equation.
        Only ``ws[0][j]`` where ``0 <= j < 3`` is ``(N, I)`` shape as they
        are multiplied with input variables. All other matrices has
        ``(N, N)`` shape.
    bs (list of list of :class:`~chainerx.array`): Bias vectors.
        ``bs[i]`` represnents biases for i-th layer.
        Each ``bs[i]`` is a list containing six vectors.
        ``bs[i][j]`` is corresponding with ``b_j`` in the equation.
        Shape of each matrix is ``(N,)`` where ``N`` is dimension of
        hidden units.
    xs (list of :class:`~chainerx.array`):
        A list of :class:`~chainerx.array` holding input values.
        Each element ``xs[t]`` holds input value
        for time ``t``. Its shape is ``(B_t, I)``, where ``B_t`` is
        mini-batch size for time ``t``, and ``I`` is size of input units.
        Note that this function supports variable length sequences.
        When sequneces has different lengths, sort sequences in descending
        order by length.
        So ``xs`` needs to satisfy
        ``xs[t].shape[0] >= xs[t + 1].shape[0]``.

Returns:
    tuple: This function returns a tuple containing two elements,
    ``hy`` and ``ys``.

    - ``hy`` is an updated hidden states whose shape is same as ``hx``.
    - ``ys`` is a list of :class:`~chainerx.array` . Each element
      ``ys[t]`` holds hidden states of the last layer corresponding
      to an input ``xs[t]``. Its shape is ``(B_t, N)`` where ``B_t`` is
      mini-batch size for time ``t``, and ``N`` is size of hidden
      units. Note that ``B_t`` is the same value as ``xs[t]``.
    """)

    _docs.set_doc(
        chainerx.n_step_rnn,
        """n_step_rnn(n_layers, hx, ws, bs, xs, activation='tanh')
Stacked Uni-directional RNN function for sequence inputs.
This function calculates stacked Uni-directional RNN with sequences.
This function gets an initial hidden state :math:`h_0`,
an initial cell state :math:`c_0`, an input sequence :math:`x`,
weight matrices :math:`W`, and bias vectors :math:`b`.
This function calculates hidden states :math:`h_t` and :math:`c_t` for each
time :math:`t` from input :math:`x_t`.

.. math::
   h_t = f(W_0 x_t + W_1 h_{t-1} + b_0 + b_1)

where :math:`f` is an activation function.
Weight matrices :math:`W` contains two matrices :math:`W_0` and
:math:`W_1`. :math:`W_0` is a parameter for an input sequence.
:math:`W_1` is a parameter for a hidden state.
Bias matrices :math:`b` contains two matrices :math:`b_0` and :math:`b_1`.
:math:`b_0` is a parameter for an input sequence.
:math:`b_1` is a parameter for a hidden state.
As the function accepts a sequence, it calculates :math:`h_t` for all
:math:`t` with one call. Two weight matrices and two bias vectors are
required for each layer. So, when :math:`S` layers exist, you need to
prepare :math:`2S` weight matrices and :math:`2S` bias vectors.
If the number of layers ``n_layers`` is greather than :math:`1`, input
of ``k``-th layer is hidden state ``h_t`` of ``k-1``-th layer.
Note that all input variables except first layer may have different shape
from the first layer.

Args:
    n_layers(int): Number of layers.
    hx (:class:`~chainerx.array`):
        Variable holding stacked hidden states.
        Its shape is ``(S, B, N)`` where ``S`` is number of layers and is
        equal to ``n_layers``, ``B`` is mini-batch size, and ``N`` is
        dimension of hidden units.
    ws (list of list of :class:`~chainerx.array`): Weight matrices.
        ``ws[i]`` represents weights for i-th layer.
        Each ``ws[i]`` is a list containing two matrices.
        ``ws[i][j]`` is corresponding with ``W_j`` in the equation.
        Only ``ws[0][j]`` where ``0 <= j < 1`` is ``(N, I)`` shape as they
        are multiplied with input variables. All other matrices has
        ``(N, N)`` shape.
    bs (list of list of :class:`~chainerx.array`): Bias vectors.
        ``bs[i]`` represnents biases for i-th layer.
        Each ``bs[i]`` is a list containing two vectors.
        ``bs[i][j]`` is corresponding with ``b_j`` in the equation.
        Shape of each matrix is ``(N,)`` where ``N`` is dimension of
        hidden units.
    xs (list of :class:`~chainerx.array`):
        A list of :class:`~chainerx.array` holding input values.
        Each element ``xs[t]`` holds input value for time ``t``.
        Its shape is ``(B_t, I)``, where ``B_t`` is
        mini-batch size for time ``t``, and ``I`` is size of input units.
        Note that this function supports variable length sequences.
        When sequneces has different lengths, sort sequences in descending
        order by length.
        So ``xs`` needs to satisfy
        ``xs[t].shape[0] >= xs[t + 1].shape[0]``.
    activation (str): Activation function name.
        Please select ``tanh`` or ``relu``.

Returns:
    tuple: This function returns a tuple containing two elements,
    ``hy`` and ``ys``.

    - ``hy`` is an updated hidden states whose shape is same as ``hx``.
    - ``ys`` is a list of :class:`~chainerx.array` . Each element
      ``ys[t]`` holds hidden states of the last layer corresponding
      to an input ``xs[t]``. Its shape is ``(B_t, N)`` where ``B_t`` is
      mini-batch size for time ``t``, and ``N`` is size of hidden
      units. Note that ``B_t`` is the same value as ``xs[t]``.
    """)

    _docs.set_doc(
        chainerx.n_step_birnn,
        """n_step_birnn(n_layers, hx, ws, bs, xs, activation='tanh')
Stacked Bi-directional RNN function for sequence inputs.
This function calculates stacked Bi-directional RNN with sequences.
This function gets an initial hidden state :math:`h_0`, an initial
cell state :math:`c_0`, an input sequence :math:`x`,
weight matrices :math:`W`, and bias vectors :math:`b`.
This function calculates hidden states :math:`h_t` and :math:`c_t` for each
time :math:`t` from input :math:`x_t`.

.. math::
    h^{f}_t &=& f(W^{f}_0 x_t + W^{f}_1 h_{t-1} + b^{f}_0 + b^{f}_1), \\\\
    h^{b}_t &=& f(W^{b}_0 x_t + W^{b}_1 h_{t-1} + b^{b}_0 + b^{b}_1), \\\\
    h_t  &=& [h^{f}_t; h^{f}_t], \\\\

where :math:`f` is an activation function.
Weight matrices :math:`W` contains two matrices :math:`W^{f}` and
:math:`W^{b}`. :math:`W^{f}` is weight matrices for forward directional
RNN. :math:`W^{b}` is weight matrices for backward directional RNN.
:math:`W^{f}` contains :math:`W^{f}_0` for an input sequence and
:math:`W^{f}_1` for a hidden state.
:math:`W^{b}` contains :math:`W^{b}_0` for an input sequence and
:math:`W^{b}_1` for a hidden state.
Bias matrices :math:`b` contains two matrices :math:`b^{f}` and
:math:`b^{f}`. :math:`b^{f}` contains :math:`b^{f}_0` for an input sequence
and :math:`b^{f}_1` for a hidden state.
:math:`b^{b}` contains :math:`b^{b}_0` for an input sequence and
:math:`b^{b}_1` for a hidden state.
As the function accepts a sequence, it calculates :math:`h_t` for all
:math:`t` with one call. Two weight matrices and two bias vectors are
required for each layer. So, when :math:`S` layers exist, you need to
prepare :math:`2S` weight matrices and :math:`2S` bias vectors.
If the number of layers ``n_layers`` is greather than :math:`1`, input
of ``k``-th layer is hidden state ``h_t`` of ``k-1``-th layer.
Note that all input variables except first layer may have different shape
from the first layer.

Args:
    n_layers(int): Number of layers.
    hx (:class:`~chainerx.array`):
        Variable holding stacked hidden states.
        Its shape is ``(2S, B, N)`` where ``S`` is number of layers and is
        equal to ``n_layers``, ``B`` is mini-batch size, and ``N`` is
        dimension of hidden units. Because of bi-direction, the
        first dimension length is ``2S``.
    ws (list of list of :class:`~chainerx.array`): Weight matrices.
        ``ws[i + di]`` represents weights for i-th layer.
        Note that ``di = 0`` for forward-RNN and ``di = 1`` for
        backward-RNN.
        Each ``ws[i + di]`` is a list containing two matrices.
        ``ws[i + di][j]`` is corresponding with ``W^{f}_j`` if ``di = 0``
        and corresponding with ``W^{b}_j`` if ``di = 1`` in the equation.
        Only ``ws[0][j]`` and ``ws[1][j]`` where ``0 <= j < 1`` are
        ``(I, N)`` shape as they are multiplied with input variables.
        All other matrices has ``(N, N)`` shape.
    bs (list of list of :class:`~chainerx.array`): Bias vectors.
        ``bs[i + di]`` represnents biases for i-th layer.
        Note that ``di = 0`` for forward-RNN and ``di = 1`` for
        backward-RNN.
        Each ``bs[i + di]`` is a list containing two vectors.
        ``bs[i + di][j]`` is corresponding with ``b^{f}_j`` if ``di = 0``
        and corresponding with ``b^{b}_j`` if ``di = 1`` in the equation.
        Shape of each matrix is ``(N,)`` where ``N`` is dimension of
        hidden units.
    xs (list of :class:`~chainerx.array`):
        A list of :class:`~chainerx.array` holding input values.
        Each element ``xs[t]`` holds input value
        for time ``t``. Its shape is ``(B_t, I)``, where ``B_t`` is
        mini-batch size for time ``t``, and ``I`` is size of input units.
        Note that this function supports variable length sequences.
        When sequneces has different lengths, sort sequences in descending
        order by length.
        So ``xs`` needs to satisfy
        ``xs[t].shape[0] >= xs[t + 1].shape[0]``.
    activation (str): Activation function name.
        Please select ``tanh`` or ``relu``.

Returns:
    tuple: This function returns a tuple containing two elements,
    ``hy`` and ``ys``.

    - ``hy`` is an updated hidden states whose shape is same as ``hx``.
    - ``ys`` is a list of :class:`~chainerx.array` . Each element
      ``ys[t]`` holds hidden states of the last layer corresponding
      to an input ``xs[t]``. Its shape is ``(B_t, N)`` where ``B_t``
      is mini-batch size for time ``t``, and ``N`` is size of hidden
      units. Note that ``B_t`` is the same value as ``xs[t]``.
    """)
</source>
<source file="systems/chainer-7.2.0/chainerx/_docs/routines.py" startline="2853" endline="3224" pcid="10705">
def _docs_connection():
    _docs.set_doc(
        chainerx.conv,
        """conv(x, w, b=None, stride=1, pad=0, cover_all=False)
N-dimensional convolution.

This is an implementation of N-dimensional convolution which is generalized
two-dimensional convolution in ConvNets. It takes three arrays: the
input ``x``, the filter weight ``w`` and the bias vector ``b``.

Notation: here is a notation for dimensionalities.

- :math:`N` is the number of spatial dimensions.
- :math:`n` is the batch size.
- :math:`c_I` and :math:`c_O` are the number of the input and output
  channels, respectively.
- :math:`d_1, d_2, ..., d_N` are the size of each axis of the input's
  spatial dimensions, respectively.
- :math:`k_1, k_2, ..., k_N` are the size of each axis of the filters,
  respectively.
- :math:`l_1, l_2, ..., l_N` are the size of each axis of the output's
  spatial dimensions, respectively.
- :math:`p_1, p_2, ..., p_N` are the size of each axis of the spatial
  padding size, respectively.

Then the ``conv`` function computes correlations between filters
and patches of size :math:`(k_1, k_2, ..., k_N)` in ``x``.
Note that correlation here is equivalent to the inner product between
expanded tensors.
Patches are extracted at positions shifted by multiples of ``stride`` from
the first position ``(-p_1, -p_2, ..., -p_N)`` for each spatial axis.

Let :math:`(s_1, s_2, ..., s_N)` be the stride of filter application.
Then, the output size :math:`(l_1, l_2, ..., l_N)` is determined by the
following equations:

.. math::

   l_n = (d_n + 2p_n - k_n) / s_n + 1 \\ \\ (n = 1, ..., N)

If ``cover_all`` option is ``True``, the filter will cover the all
spatial locations. So, if the last stride of filter does not cover the
end of spatial locations, an additional stride will be applied to the end
part of spatial locations. In this case, the output size is determined by
the following equations:

.. math::

   l_n = (d_n + 2p_n - k_n + s_n - 1) / s_n + 1 \\ \\ (n = 1, ..., N)

Args:
    x (:class:`~chainerx.ndarray`):
        Input array of shape :math:`(n, c_I, d_1, d_2, ..., d_N)`.
    w (:class:`~chainerx.ndarray`):
        Weight array of shape :math:`(c_O, c_I, k_1, k_2, ..., k_N)`.
    b (None or :class:`~chainerx.ndarray`):
        One-dimensional bias array with length :math:`c_O` (optional).
    stride (:class:`int` or :class:`tuple` of :class:`int` s):
        Stride of filter applications :math:`(s_1, s_2, ..., s_N)`.
        ``stride=s`` is equivalent to ``(s, s, ..., s)``.
    pad (:class:`int` or :class:`tuple` of :class:`int` s):
        Spatial padding width for input arrays
        :math:`(p_1, p_2, ..., p_N)`. ``pad=p`` is equivalent to
        ``(p, p, ..., p)``.
    cover_all (bool): If ``True``, all spatial locations are convoluted
        into some output pixels. It may make the output size larger.
        `cover_all` needs to be ``False`` if you want to use ``cuda`` backend.

Returns:
    ~chainerx.ndarray:
        Output array of shape :math:`(n, c_O, l_1, l_2, ..., l_N)`.

Note:

    In ``cuda`` backend, this function uses cuDNN implementation for its
    forward and backward computation.

Note:

    In ``cuda`` backend, this function has following limitations yet:

    - The ``cover_all=True`` option is not supported yet.
    - The ``dtype`` must be ``float32`` or ``float64`` (``float16`` is not
      supported yet.)

Note:

    During backpropagation, this function propagates the gradient of the
    output array to input arrays ``x``, ``w``, and ``b``.

.. seealso:: :func:`chainer.functions.convolution_nd`

.. admonition:: Example

    >>> n = 10
    >>> c_i, c_o = 3, 1
    >>> d1, d2, d3 = 30, 40, 50
    >>> k1, k2, k3 = 10, 10, 10
    >>> p1, p2, p3 = 5, 5, 5
    >>> x = chainerx.random.uniform(0, 1, (n, c_i, d1, d2, d3)).\
astype(np.float32)
    >>> x.shape
    (10, 3, 30, 40, 50)
    >>> w = chainerx.random.uniform(0, 1, (c_o, c_i, k1, k2, k3)).\
astype(np.float32)
    >>> w.shape
    (1, 3, 10, 10, 10)
    >>> b = chainerx.random.uniform(0, 1, (c_o)).astype(np.float32)
    >>> b.shape
    (1,)
    >>> s1, s2, s3 = 2, 4, 6
    >>> y = chainerx.conv(x, w, b, stride=(s1, s2, s3),\
 pad=(p1, p2, p3))
    >>> y.shape
    (10, 1, 16, 11, 9)
    >>> l1 = int((d1 + 2 * p1 - k1) / s1 + 1)
    >>> l2 = int((d2 + 2 * p2 - k2) / s2 + 1)
    >>> l3 = int((d3 + 2 * p3 - k3) / s3 + 1)
    >>> y.shape == (n, c_o, l1, l2, l3)
    True
    >>> y = chainerx.conv(x, w, b, stride=(s1, s2, s3),\
 pad=(p1, p2, p3), cover_all=True)
    >>> y.shape == (n, c_o, l1, l2, l3 + 1)
    True
""")

    _docs.set_doc(
        chainerx.conv_transpose,
        """conv_transpose(x, w, b=None, stride=1, pad=0, outsize=None)
N-dimensional transposed convolution.

This is an implementation of N-dimensional transposed convolution, which is
previously known as **deconvolution** in Chainer.

.. _Deconvolutional Networks: \
://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf

It takes three arrays: the input ``x``, the filter weight ``w``, and the
bias vector ``b``.

Notation: here is a notation for dimensionalities.

- :math:`N` is the number of spatial dimensions.
- :math:`n` is the batch size.
- :math:`c_I` and :math:`c_O` are the number of the input and output
  channels, respectively.
- :math:`d_1, d_2, ..., d_N` are the size of each axis of the input's
  spatial dimensions, respectively.
- :math:`k_1, k_2, ..., k_N` are the size of each axis of the filters,
  respectively.
- :math:`p_1, p_2, ..., p_N` are the size of each axis of the spatial
  padding size, respectively.
- :math:`s_1, s_2, ..., s_N` are the stride of each axis of filter
  application, respectively.

If ``outsize`` option is ``None``, the output size
:math:`(l_1, l_2, ..., l_N)` is determined by the following equations with
the items in the above list:

.. math::

   l_n = s_n (d_n - 1)  + k_n - 2 p_n \\ \\ (n = 1, ..., N)

If ``outsize`` option is given, the output size is determined by
``outsize``. In this case, the ``outsize`` :math:`(l_1, l_2, ..., l_N)`
must satisfy the following equations:

.. math::

   d_n = \\lfloor (l_n + 2p_n - k_n) / s_n \\rfloor + 1 \\ \\ \
   (n = 1, ..., N)

Args:
    x (:class:`~chainerx.ndarray`):
        Input array of shape :math:`(n, c_I, d_1, d_2, ..., d_N)`.
    w (:class:`~chainerx.ndarray`):
        Weight array of shape :math:`(c_I, c_O, k_1, k_2, ..., k_N)`.
    b (None or :class:`~chainerx.ndarray`):
        One-dimensional bias array with length :math:`c_O` (optional).
    stride (:class:`int` or :class:`tuple` of :class:`int` s):
        Stride of filter applications :math:`(s_1, s_2, ..., s_N)`.
        ``stride=s`` is equivalent to ``(s, s, ..., s)``.
    pad (:class:`int` or :class:`tuple` of :class:`int` s):
        Spatial padding width for input arrays
        :math:`(p_1, p_2, ..., p_N)`. ``pad=p`` is equivalent to
        ``(p, p, ..., p)``.
    outsize (None or :class:`tuple` of :class:`int` s):
        Expected output size of deconvolutional operation. It should be a
        tuple of ints :math:`(l_1, l_2, ..., l_N)`. Default value is
        ``None`` and the outsize is estimated by input size, stride and
        pad.

Returns:
    ~chainerx.ndarray:
        Output array of shape :math:`(n, c_O, l_1, l_2, ..., l_N)`.

Note:

    During backpropagation, this function propagates the gradient of the
    output array to input arrays ``x``, ``w``, and ``b``.

.. seealso:: :func:`chainer.functions.deconvolution_nd`

.. admonition:: Example

    **Example1**: the case when ``outsize`` is not given.

    >>> n = 10
    >>> c_i, c_o = 3, 1
    >>> d1, d2, d3 = 5, 10, 15
    >>> k1, k2, k3 = 10, 10, 10
    >>> p1, p2, p3 = 5, 5, 5
    >>> x = chainerx.random.uniform(0, 1, (n, c_i, d1, d2, d3)).\
astype(np.float32)
    >>> x.shape
    (10, 3, 5, 10, 15)
    >>> w = chainerx.random.uniform(0, 1, (c_i, c_o, k1, k2, k3)).\
astype(np.float32)
    >>> w.shape
    (3, 1, 10, 10, 10)
    >>> b = chainerx.random.uniform(0, 1, (c_o)).astype(np.float32)
    >>> b.shape
    (1,)
    >>> s1, s2, s3 = 2, 4, 6
    >>> y = chainerx.conv_transpose(x, w, b, stride=(s1, s2, s3), \
pad=(p1, p2, p3))
    >>> y.shape
    (10, 1, 8, 36, 84)
    >>> l1 = s1 * (d1 - 1) + k1 - 2 * p1
    >>> l2 = s2 * (d2 - 1) + k2 - 2 * p2
    >>> l3 = s3 * (d3 - 1) + k3 - 2 * p3
    >>> y.shape == (n, c_o, l1, l2, l3)
    True

    **Example2**: the case when ``outsize`` is given.

    >>> n = 10
    >>> c_i, c_o = 3, 1
    >>> d1, d2, d3 = 5, 10, 15
    >>> k1, k2, k3 = 10, 10, 10
    >>> p1, p2, p3 = 5, 5, 5
    >>> x = chainerx.array(np.random.uniform(0, 1, (n, c_i, d1, d2, d3)).\
astype(np.float32))
    >>> x.shape
    (10, 3, 5, 10, 15)
    >>> w = chainerx.array(np.random.uniform(0, 1, (c_i, c_o, k1, k2, k3)).\
astype(np.float32))
    >>> w.shape
    (3, 1, 10, 10, 10)
    >>> b = chainerx.array(np.random.uniform(0, 1, (c_o)).astype(np.float32))
    >>> b.shape
    (1,)
    >>> s1, s2, s3 = 2, 4, 6
    >>> l1, l2, l3 = 9, 38, 87
    >>> d1 == int((l1 + 2 * p1 - k1) / s1) + 1
    True
    >>> d2 == int((l2 + 2 * p2 - k2) / s2) + 1
    True
    >>> d3 == int((l3 + 2 * p3 - k3) / s3) + 1
    True
    >>> y = chainerx.conv_transpose(x, w, b, stride=(s1, s2, s3), \
pad=(p1, p2, p3), outsize=(l1, l2, l3))
    >>> y.shape
    (10, 1, 9, 38, 87)
    >>> y.shape == (n, c_o, l1, l2, l3)
    True
""")

    _docs.set_doc(
        chainerx.linear,
        """linear(x, W, b=None, n_batch_axis=1)
Linear function, or affine transformation.

It accepts two or three arguments: an input minibatch ``x``, a weight
matrix ``W``, and optionally a bias vector ``b``. It computes

.. math:: Y = xW^\\top + b.

Args:
    x (~chainerx.ndarray):
        Input array, which is a :math:`(s_1, s_2, ..., s_n)`-shaped array.
    W (~chainerx.ndarray):
        Weight variable of shape :math:`(M, N)`,
        where :math:`(N = s_{\\rm n\\_batch\\_axes} * ... * s_n)`.
    b (~chainerx.ndarray):
        Bias variable (optional) of shape :math:`(M,)`.
    n_batch_axes (int):
        The number of batch axes. The default is 1. The input variable is
        reshaped into (:math:`{\\rm n\\_batch\\_axes} + 1`)-dimensional
        tensor. This should be greater than 0.

Returns:
    :class:`~chainerx.ndarray`:
        Output array with shape of
        :math:`(s_1, ..., s_{\\rm n\\_batch\\_axes}, M)`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to input arrays ``x``, ``W`` and ``b``.
""")

    _docs.set_doc(
        chainerx.lstm,
        """lstm(c_prev, x)
Long Short-Term Memory units as an activation function.

This function implements LSTM units with forget gates. Let the previous
cell state ``c_prev`` and the input array ``x``.
First, the input array ``x`` is split into four arrays
:math:`a, i, f, o` of the same shapes along the second axis. It means that
``x`` 's second axis must have 4 times the ``c_prev`` 's second axis.
The split input arrays are corresponding to:

    - :math:`a` : sources of cell input
    - :math:`i` : sources of input gate
    - :math:`f` : sources of forget gate
    - :math:`o` : sources of output gate

Second, it computes the updated cell state ``c`` and the outgoing signal
``h`` as

.. math::
    c &= \\tanh(a) \\sigma(i)
       + c_{\\text{prev}} \\sigma(f), \\\\
    h &= \\tanh(c) \\sigma(o),

where :math:`\\sigma` is the elementwise sigmoid function.
These are returned as a tuple of two variables.
This function supports variable length inputs. The mini-batch size of
the current input must be equal to or smaller than that of the previous
one. When mini-batch size of ``x`` is smaller than that of ``c``, this
function only updates ``c[0:len(x)]`` and doesn't change the rest of ``c``,
``c[len(x):]``. So,
please sort input sequences in descending order of lengths before
applying the function.

Args:
    c_prev (:class:`~chainerx.array`):
        Variable that holds the previous cell state. The cell state
        should be a zero array or the output of the previous call of LSTM.
    x (:class:`~chainer.array`):
        Variable that holds the sources of cell input, input gate, forget
        gate and output gate. It must have the second dimension whose size
        is four times of that of the cell state.

Returns:
    tuple: Two :class:`~chainerx.array` objects ``c`` and ``h``.
    ``c`` is the updated cell state. ``h`` indicates the outgoing signal.

See the original paper proposing LSTM with forget gates:
`Long Short-Term Memory in Recurrent Neural Networks
<http://www.felixgers.de/papers/phd.pdf>`_.

.. admonition:: Example

    Assuming ``y`` is the current incoming signal, ``c`` is the previous
    cell state, and ``h`` is the previous outgoing signal from an ``lstm``
    function. Each of ``y``, ``c`` and ``h`` has ``n_units`` channels.
    Most typical preparation of ``x`` is

    >>> n_units = 100
    >>> c_prev = chainerx.zeros((1, n_units), chainerx.float32)
    >>> x = chainerx.zeros((1, 4 * n_units), chainerx.float32)
    >>> c, h = chainerx.lstm(c_prev, x)

    It corresponds to calculate the input array ``x``, or the input
    sources :math:`a, i, f, o`, from the current incoming signal ``y`` and
    the previous outgoing signal ``h``. Different parameters are used for
    different kind of input sources.
""")


</source>
<source file="systems/chainer-7.2.0/chainerx/_docs/routines.py" startline="2751" endline="2852" pcid="10704">
def _docs_statistics():
    _docs.set_doc(
        chainerx.amax,
        """amax(a, axis=None, keepdims=False)
Returns the maximum of an array or the maximum along an axis.

Note:
    When at least one element is NaN, the corresponding max value will be NaN.

Args:
    a (~chainerx.ndarray): Array to take the maximum.
    axis (None or int or tuple of ints): Along which axis to take the maximum.
        The flattened array is used by default.
        If this is a tuple of ints, the maximum is selected over multiple
        axes, instead of a single axis or all the axes.
    keepdims (bool): If ``True``, the axis is remained as an axis of size one.

Returns:
    :class:`~chainerx.ndarray`: The maximum of ``a``, along the axis if
    specified.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``a``.

.. seealso:: :func:`numpy.amax`
""")

    _docs.set_doc(
        chainerx.amin,
        """amin(a, axis=None, keepdims=False)
Returns the minimum of an array or the minimum along an axis.

Note:
    When at least one element is NaN, the corresponding min value will be NaN.

Args:
    a (~chainerx.ndarray): Array to take the minimum.
    axis (None or int or tuple of ints): Along which axis to take the minimum.
        The flattened array is used by default.
        If this is a tuple of ints, the minimum is selected over multiple
        axes, instead of a single axis or all the axes.
    keepdims (bool): If ``True``, the axis is remained as an axis of size one.

Returns:
    :class:`~chainerx.ndarray`: The minimum of ``a``, along the axis if
    specified.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``a``.

.. seealso:: :func:`numpy.amin`
""")

    _docs.set_doc(
        chainerx.mean,
        """mean(a, axis=None, keepdims=False)
Compute the arithmetic mean along the specified axis.

Returns the average of the array elements. The average is taken over the
flattened array by default, otherwise over the specified axis.

Args:
    a (~chainerx.ndarray): Array to take the mean of.
    axis (None or int or tuple of ints): Along which axis or axes to compute
    the mean. The flattened array is used by default.
    keepdims (bool): If this is set to True, the axes which are reduced are
    left in the result as dimensions with size one. With this option,
    the result will broadcast correctly against the input array.

Returns:
    :class:`~chainerx.ndarray`: The mean of ``a``, along the axis or axes if
    specified.

.. seealso:: :func:`numpy.mean`
""")

    _docs.set_doc(
        chainerx.var,
        """var(a, axis=None, keepdims=False)
Compute the arithmetic var along the specified axis.

Returns the var of the array elements. The var is taken over the flattened
array by default, otherwise over the specified axis.

Args:
    a (~chainerx.ndarray): Array to take the var of.
    axis (None or int or tuple of ints): Along which axis or axes to compute
    the var. The flattened array is used by default.
    keepdims (bool): If this is set to True, the axes which are reduced are
    left in the result as dimensions with size one. With this option,
    the result will broadcast correctly against the input array.

Returns:
    :class:`~chainerx.ndarray`: The var of ``a``, along the axis or axes if
    specified.

.. seealso:: :func:`numpy.var`
""")


</source>
<source file="systems/chainer-7.2.0/chainerx/_docs/device.py" startline="54" endline="123" pcid="10689">
def set_docs():
    _set_docs_device()

    _docs.set_doc(
        chainerx.get_device,
        """get_device(*device)
Returns a device specified by the arguments.

If the argument is a single :class:`~chainerx.Device` instance, it's simply
returned.

Otherwise, there are three ways to specify a device:


.. testcode::

    # Specify a backend name and a device index separately.
    chainerx.get_device('native', 0)

    # Specify a backend name and a device index in a single string.
    chainerx.get_device('native:0')

    # Specify only a backend name. In this case device index 0 is chosen.
    chainerx.get_device('native')

Returns:
    ~chainerx.Device: Device object.
""")

    _docs.set_doc(
        chainerx.get_default_device,
        """get_default_device()
Returns the default device associated with the current thread.

Returns:
    ~chainerx.Device: The default device.

.. seealso::
    * :func:`chainerx.set_default_device`
    * :func:`chainerx.using_device`
""")

    _docs.set_doc(
        chainerx.set_default_device,
        """set_default_device(device)
Sets the given device as the default device of the current thread.

Args:
    device (~chainerx.Device or str): Device object or device name to set as
        the default device.

.. seealso::
    * :func:`chainerx.get_default_device`
    * :func:`chainerx.using_device`
""")

    _docs.set_doc(
        chainerx.using_device,
        """using_device(device)
Creates a context manager to temporarily set the default device.

Args:
    device (~chainerx.Device or str): Device object or device name to set as
        the default device during the context. See :data:`chainerx.Device.name`
        for the specification of device names.

.. seealso::
    * :func:`chainerx.get_default_device`
    * :func:`chainerx.set_default_device`
""")
</source>
<source file="systems/chainer-7.2.0/chainerx/_docs/routines.py" startline="558" endline="632" pcid="10697">
def _docs_indexing():
    _docs.set_doc(
        chainerx.take,
        """take(a, indices, axis)
Takes elements from an array along an axis.

Args:
    a (~chainerx.ndarray): Source array.
    indices (~chainerx.ndarray):
        The indices of the values to extract. When indices are out of bounds,
        they are wrapped around.
    axis (int): The axis over which to select values.
    mode (str): Specifies how out-of-bounds indices will behave.
        'raise' - raise an error
        'wrap' - wrap around
        'clip' - clip to the range

Returns:
    :func:`~chainerx.ndarray`: Output array.

Note:
    This function currently does not support ``axis=None``

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``a``.

Note:
   The default mode for the native backend is 'raise', while for the cuda
   backend is 'wrap' in order to prevent device synchronization.
   'raise' mode is currently not supported in the CUDA backend.

.. seealso:: :func:`numpy.take`
""")

    _docs.set_doc(
        chainerx.where,
        """where(condition, x, y)
Return elements chosen from ``x`` or ``y`` depending on condition.

Args:
    condition (~chainerx.ndarray): Where True, yield ``x``, otherwise
    yield ``y``.
    x (~chainerx.ndarray): Values from which to choose.
    y (~chainerx.ndarray): Values from which to choose.

Returns:
    :func:`~chainerx.ndarray`: An array with elements
    from ``x`` where condition is True, and elements from ``y`` elsewhere.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x`` and ``y``.

.. seealso:: :func:`numpy.where`
""")

    _docs.set_doc(
        chainerx.nonzero,
        """nonzero(a)
Return the indices of the elements that are non-zero.

Args:
    a (~chainerx.ndarray): Input array.

Returns:
    tuple of :func:`~chainerx.ndarray`: Indices of elements that are non-zero.

Note:
    During backpropagation, this function does not propagate gradients.

.. seealso:: :func:`numpy.nonzero`
""")


</source>
</class>

<class classid="281" nclones="2" nlines="70" similarity="100">
<source file="systems/chainer-7.2.0/chainerx/_docs/routines.py" startline="22" endline="508" pcid="10695">
def _docs_creation():
    _docs.set_doc(
        chainerx.empty,
        """empty(shape, dtype, device=None)
Returns an array without initializing the elements.

Args:
    shape (tuple of ints): Shape of the array.
    dtype: Data type of the array.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device <chainerx_device>` is chosen.

Returns:
    :class:`~chainerx.ndarray`: New array with elements not initialized.

.. seealso:: :func:`numpy.empty`
""")

    _docs.set_doc(
        chainerx.empty_like,
        """empty_like(a, device=None)
Returns a new array with same shape and dtype of a given array.

Args:
    a (~chainerx.ndarray): Prototype array.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device <chainerx_device>` is chosen.

Returns:
    :class:`~chainerx.ndarray`: New array with same shape and dtype as ``a`` \
with elements not initialized.

Warning:
    If ``device`` argument is omitted, the new array is created on the default
    device, not the device of the prototype array.

.. seealso:: :func:`numpy.empty_like`
""")

    _docs.set_doc(
        chainerx.eye,
        """eye(N, M=None, k=0, dtype=float64, device=None)
Returns a 2-D array with ones on the diagonals and zeros elsewhere.

Args:
    N (int): Number of rows.
    M (int): Number of columns. M == N by default.
    k (int): Index of the diagonal. Zero indicates the main diagonal,
        a positive index an upper diagonal, and a negative index a lower
        diagonal.
    dtype: Data type.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device <chainerx_device>` is chosen.

Returns:
    ~chainerx.ndarray: A 2-D array with given diagonals filled with ones and
    zeros elsewhere.

.. seealso:: :func:`numpy.eye`
""")

    _docs.set_doc(
        chainerx.tri,
        """tri(N, M=None, k=0, dtype=float32, device=None)
Returns a 2-D array with ones at and below the given diagonal
and zeros elsewhere.

Args:
    N (int): Number of rows.
    M (int): Number of columns. M == N by default.
    k (int): Index of the diagonal. Zero indicates the main diagonal,
        a positive index an upper diagonal, and a negative index a lower
        diagonal.
    dtype: Data type.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device <chainerx_device>` is chosen.

Returns:
    ~chainerx.ndarray: A 2-D array with given diagonals filled ones at and
    below the given diagonal and zeros elsewhere.

.. seealso:: :func:`numpy.tri`
""")

    _docs.set_doc(
        chainerx.tril,
        """tril(m, k=0)
Lower triangle of an array.

Returns a copy of an array with elements above the k-th diagonal zeroed.

Args:
    m (~chainerx.ndarray): Input array.
    k (int): Index of the diagonal. Zero indicates the main diagonal,
        a positive index an upper diagonal, and a negative index a lower
        diagonal.

Returns:
    ~chainerx.ndarray: Lower triangle of ``m``.

.. seealso:: :func:`numpy.tril`
""")

    _docs.set_doc(
        chainerx.triu,
        """triu(m, k=0)
Upper triangle of an array.

Returns a copy of an array with elements below the k-th diagonal zeroed.

Args:
    m (~chainerx.ndarray): Input array.
    k (int): Index of the diagonal. Zero indicates the main diagonal,
        a positive index an upper diagonal, and a negative index a lower
        diagonal.

Returns:
    ~chainerx.ndarray: Upper triangle of ``m``.

.. seealso:: :func:`numpy.triu`
""")

    _docs.set_doc(
        chainerx.identity,
        """identity(n, dtype=None, device=None)
Returns a 2-D identity array.

It is equivalent to ``eye(n, n, dtype)``.

Args:
    n (int): Number of rows and columns.
    dtype: Data type.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device <chainerx_device>` is chosen.

Returns:
    ~chainerx.ndarray: A 2-D identity array.

.. seealso:: :func:`numpy.identity`
""")

    _docs.set_doc(
        chainerx.ones,
        """ones(shape, dtype, device=None)
Returns a new array of given shape and dtype, filled with ones.

Args:
    shape (tuple of ints): Shape of the array.
    dtype: Data type.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device <chainerx_device>` is chosen.

Returns:
    ~chainerx.ndarray: New array.

.. seealso:: :func:`numpy.ones`
""")

    _docs.set_doc(
        chainerx.ones_like,
        """ones_like(a, device=None)
Returns an array of ones with same shape and dtype as a given array.

Args:
    a (~chainerx.ndarray): Prototype array.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device <chainerx_device>` is chosen.

Returns:
    ~chainerx.ndarray: New array.

Warning:
    If ``device`` argument is omitted, the new array is created on the default
    device, not the device of the prototype array.

.. seealso:: :func:`numpy.ones_like`
""")

    _docs.set_doc(
        chainerx.zeros,
        """zeros(shape, dtype, device=None)
Returns a new array of given shape and dtype, filled with zeros.

Args:
    shape (tuple of ints): Shape of the array.
    dtype: Data type.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device <chainerx_device>` is chosen.

Returns:
    ~chainerx.ndarray: New array.

.. seealso:: :func:`numpy.zeros`
""")

    _docs.set_doc(
        chainerx.zeros_like,
        """zeros_like(a, device=None)
Returns an array of zeros with same shape and dtype as a given array.

Args:
    a (~chainerx.ndarray): Prototype array.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device <chainerx_device>` is chosen.

Returns:
    ~chainerx.ndarray: New array.

Warning:
    If ``device`` argument is omitted, the new array is created on the default
    device, not the device of the prototype array.

.. seealso:: :func:`numpy.zeros_like`
""")

    _docs.set_doc(
        chainerx.full,
        """full(shape, fill_value, dtype, device=None)
Returns a new array of given shape and dtype, filled with a given value.

Args:
    shape (tuple of ints): Shape of the array.
    dtype: Data type.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device <chainerx_device>` is chosen.

Returns:
    ~chainerx.ndarray: New array.

.. seealso:: :func:`numpy.full`
""")

    _docs.set_doc(
        chainerx.full_like,
        """full_like(a, fill_value, dtype=None, device=None)
Returns a full array with same shape and dtype as a given array.

Args:
    a (~chainerx.ndarray): Prototype array.
    dtype: Data type.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device <chainerx_device>` is chosen.

Returns:
    ~chainerx.ndarray: New array.

Warning:
    If ``device`` argument is omitted, the new array is created on the default
    device, not the device of the prototype array.

.. seealso:: :func:`numpy.full_like`
""")

    _docs.set_doc(
        chainerx.array,
        """array(object, dtype=None, copy=True, device=None)
Creates an array.

Args:
    object: A :class:`~chainerx.ndarray` object or any other object that can be
        passed to :func:`numpy.array`.
    dtype: Data type. If omitted, it's inferred from the input.
    copy (bool): If ``True``, the object is always copied. Otherwise, a copy
        will only be made if it is needed to satisfy any of the other
        requirements (dtype, device, etc.).
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device <chainerx_device>` is chosen.

Returns:
    ~chainerx.ndarray: New array.

Warning:
    If ``device`` argument is omitted, the new array is created on the default
    device, not the device of the input array.

.. seealso:: :func:`numpy.array`
""")

    _docs.set_doc(
        chainerx.asarray,
        """asarray(a, dtype=None, device=None)
Converts an object to an array.

Args:
    a: The source object.
    dtype: Data type. If omitted, it's inferred from the input.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device <chainerx_device>` is chosen.

Returns:
    ~chainerx.ndarray: Array interpretation of ``a``. If ``a`` is already an \
ndarray on the given device with matching dtype, no copy is performed.

Warning:
    If ``device`` argument is omitted, the new array is created on the default
    device, not the device of the input array.

.. seealso:: :func:`numpy.asarray`
""")

    _docs.set_doc(
        chainerx.ascontiguousarray,
        """ascontiguousarray(a, dtype=None, device=None)
Returns a C-contiguous array.

Args:
    a (~chainerx.ndarray): Source array.
    dtype: Data type.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device <chainerx_device>` is chosen.

Returns:
    ~chainerx.ndarray: C-contiguous array. A copy will be made only if needed.

Warning:
    If ``device`` argument is omitted, the new array is created on the default
    device, not the device of the input array.

.. seealso:: :func:`numpy.ascontiguousarray`
""")

    _docs.set_doc(
        chainerx.copy,
        """copy(a)
Creates a copy of a given array.

Args:
    a (~chainerx.ndarray): Source array.

Returns:
    ~chainerx.ndarray: A copy array on the same device as ``a``.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``a``.

.. seealso:: :func:`numpy.copy`
""")

    _docs.set_doc(
        chainerx.frombuffer,
        """frombuffer(buffer, dtype=float, count=-1, offset=0, device=None)
Returns a 1-D array interpretation of a buffer.

The given ``buffer`` memory must be usable on the given device, otherwise,
an error is raised.

Note:
    The ``native`` backend requires a buffer of main memory, and
    the ``cuda`` backend requires a buffer of CUDA memory.
    No copy is performed.

Args:
    buffer: An object that exposes the buffer interface.
    dtype: Data type of the returned array.
    count (int): Number of items to read. -1 means all data in the buffer.
    offset (int): Start reading the buffer from this offset (in bytes).
    device (~chainerx.Device): Device of the returned array.
        If omitted, :ref:`the default device <chainerx_device>` is chosen.

Returns:
    ~chainerx.ndarray: 1-D array interpretation of ``buffer``.

.. seealso:: :func:`numpy.frombuffer`
""")

    _docs.set_doc(
        chainerx.arange,
        """arange([start=0, ]stop, [step=1, ]dtype=None, device=None)
Returns an array with  evenly spaced values within a given interval.

Values are generated within the half-open interval [``start``, ``stop``).
The first three arguments are mapped like the ``range`` built-in function,
i.e. ``start`` and ``step`` are optional.

Args:
    start: Start of the interval.
    stop: End of the interval.
    step: Step width between each pair of consecutive values.
    dtype: Data type specifier. It is inferred from other arguments by
        default.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device <chainerx_device>` is chosen.

Returns:
    ~chainerx.ndarray: The 1-D array of range values.

.. seealso:: :func:`numpy.arange`
""")

    _docs.set_doc(
        chainerx.linspace,
        """linspace(start, stop, num=50, endpoint=True, dtype=None, device=None)
Returns an array with evenly spaced numbers over a specified interval.

Instead of specifying the step width like :func:`chainerx.arange()`,
this function requires the total number of elements specified.

Args:
    start: Start of the interval.
    stop: End of the interval.
    num: Number of elements.
    endpoint (bool): If ``True``, the stop value is included as the last
        element. Otherwise, the stop value is omitted.
    dtype: Data type specifier. It is inferred from the start and stop
        arguments by default.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device <chainerx_device>` is chosen.

Returns:
    ~chainerx.ndarray: The 1-D array of ranged values.

.. seealso:: :func:`numpy.linspace`
""")  # NOQA

    _docs.set_doc(
        chainerx.diag,
        """diag(v, k=0, device=None)
Returns a diagonal or a diagonal array.

Args:
    v (~chainerx.ndarray): Array object.
    k (int): Index of diagonals. Zero indicates the main diagonal, a
        positive value an upper diagonal, and a negative value a lower
        diagonal.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device <chainerx_device>` is chosen.

Returns:
    ~chainerx.ndarray: If ``v`` is a 1-D array, then it returns a 2-D
    array with the specified diagonal filled by ``v``. If ``v`` is a
    2-D array, then it returns the specified diagonal of ``v``. In latter
    case, if ``v`` is a :class:`chainerx.ndarray` object, then its view is
    returned.

Note:
    The argument ``v`` does not support array-like objects yet.

.. seealso:: :func:`numpy.diag`
""")

    _docs.set_doc(
        chainerx.diagflat,
        """diagflat(v, k=0, device=None)
Creates a diagonal array from the flattened input.

Args:
    v (~chainerx.ndarray): Array object.
    k (int): Index of diagonals. See :func:`chainerx.diag`.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device <chainerx_device>` is chosen.

Returns:
    ~chainerx.ndarray: A 2-D diagonal array with the diagonal copied
    from ``v``.

Note:
    The argument ``v`` does not support array-like objects yet.

.. seealso:: :func:`numpy.diagflat`
""")

    _docs.set_doc(
        chainerx.meshgrid,
        """meshgrid(xi, indexing='xy')
Returns coordinate matrices from coordinate vectors.

Make N-D coordinate arrays for vectorized evaluations of N-D scalar/vector
fields over N-D grids, given one-dimensional coordinate arrays x1, x2,…, xn.

Args:
    xi (sequence of :class:`~chainerx.ndarray`\\ s): 1-D arrays
        representing the coordinates of a grid.
    indexing (str): {‘xy’, ‘ij’}, optional
        Cartesian (‘xy’, default) or matrix (‘ij’) indexing of output.

Returns:
    list of :class:`~chainerx.ndarray`\\ s: For vectors x1, x2,…, ‘xn’ with
    lengths Ni=len(xi), return (N1, N2, N3,...Nn) shaped arrays if
    indexing=’ij’ or (N2, N1, N3,...Nn) shaped arrays if indexing=’xy’
    with the elements of xi repeated to fill the matrix along the first
    dimension for x1, the second for x2 and so on.

.. seealso:: :func:`numpy.meshgrid`
""")


</source>
<source file="systems/chainer-7.2.0/chainerx/_docs/routines.py" startline="1274" endline="1770" pcid="10701">
def _docs_manipulation():
    _docs.set_doc(
        chainerx.reshape,
        """reshape(a, newshape)
Returns a reshaped array.

Args:
    a (~chainerx.ndarray): Array to be reshaped.
    newshape (int or tuple of ints): The new shape of the array to return.
        If it is an integer, then it is treated as a tuple of length one.
        It should be compatible with ``a.size``. One of the elements can be
        -1, which is automatically replaced with the appropriate value to
        make the shape compatible with ``a.size``.

Returns:
    :class:`~chainerx.ndarray`: A reshaped view of ``a`` if possible,
    otherwise a copy.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``a``.

.. seealso:: :func:`numpy.reshape`
""")

    _docs.set_doc(
        chainerx.ravel,
        """ravel(a)
Returns a flattened array.

Args:
    a (~chainerx.ndarray): Array to be flattened.

Returns:
    :class:`~chainerx.ndarray`: A flattened view of ``a`` if possible,
    otherwise a copy.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``a``.

.. seealso:: :func:`numpy.ravel`
""")

    _docs.set_doc(
        chainerx.transpose,
        """transpose(a, axes=None)
Permutes the dimensions of an array.

Args:
    a (~chainerx.ndarray): Array to permute the dimensions.
    axes (tuple of ints): Permutation of the dimensions. This function reverses
        the shape by default.

Returns:
    ~chainerx.ndarray: A view of ``a`` with the dimensions permuted.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``a``.

.. seealso:: :func:`numpy.transpose`
""")

    _docs.set_doc(
        chainerx.broadcast_to,
        """broadcast_to(array, shape)
Broadcasts an array to a given shape.

Args:
    array (~chainerx.ndarray): Array to broadcast.
    shape (tuple of ints): The shape of the desired array.

Returns:
    ~chainerx.ndarray: Broadcasted view.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``array``.

.. seealso:: :func:`numpy.broadcast_to`
""")

    _docs.set_doc(
        chainerx.squeeze,
        """squeeze(a, axis=None)
Removes size-one axes from the shape of an array.

Args:
    a (~chainerx.ndarray): Array to be reshaped.
    axis (int or tuple of ints): Axes to be removed. This function removes all
        size-one axes by default. If one of the specified axes is not of size
        one, an exception is raised.

Returns:
    ~chainerx.ndarray: An array without (specified) size-one axes.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``a``.

.. seealso:: :func:`numpy.squeeze`
""")

    _docs.set_doc(
        chainerx.concatenate,
        """concatenate(arrays, axis=0)
Joins arrays along an axis.

Args:
    arrays (sequence of :class:`~chainerx.ndarray`\\ s): Arrays to be joined.
        All of these should have the same dimensionalities except the specified
        axis.
    axis (int): The axis to join arrays along.


Returns:
    ~chainerx.ndarray: Joined array.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input arrays in ``arrays``.

.. seealso:: :func:`numpy.concatenate`
""")

    _docs.set_doc(
        chainerx.stack,
        """stack(arrays, axis=0)
Stacks arrays along a new axis.

Args:
    arrays (sequence of :class:`~chainerx.ndarray`\\ s): Arrays to be stacked.
    axis (int): Axis along which the arrays are stacked.

Returns:
    ~chainerx.ndarray: Stacked array.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input arrays in ``arrays``.

.. seealso:: :func:`numpy.stack`
""")

    _docs.set_doc(
        chainerx.hstack,
        """hstack(arrays)
Stack arrays in sequence horizontally (column wise).

Args:
    arrays (sequence of :class:`~chainerx.ndarray`\\ s): Arrays to be stacked.

Returns:
    ~chainerx.ndarray: Stacked array.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input arrays in ``arrays``.

.. seealso:: :func:`numpy.hstack`
""")

    _docs.set_doc(
        chainerx.vstack,
        """vstack(arrays)
Stack arrays in sequence vertically (row wise).

Args:
    arrays (sequence of :class:`~chainerx.ndarray`\\ s): Arrays to be stacked.

Returns:
    ~chainerx.ndarray: Stacked array.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input arrays in ``arrays``.

.. seealso:: :func:`numpy.vstack`
""")

    _docs.set_doc(
        chainerx.dstack,
        """dstack(arrays)
Stack arrays in sequence depth wise (along third axis).

Args:
    arrays (sequence of :class:`~chainerx.ndarray`\\ s): Arrays to be stacked.

Returns:
    ~chainerx.ndarray: Stacked array.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input arrays in ``arrays``.

.. seealso:: :func:`numpy.dstack`
""")

    _docs.set_doc(
        chainerx.atleast_2d,
        """atleast_2d(a)
View inputs as arrays with at least two dimensions.

Args:
    a (~chainerx.ndarray): Array.

Returns:
    ~chainerx.ndarray: An array with a.ndim >= 2.
    Copies are avoided where possible, and views with
    two or more dimensions are returned.

Note:
    * Arrays that already have two or more dimensions are preserved.
    * During backpropagation, this function propagates the gradient of the
      output array to the input arrays in ``a``.

.. seealso:: :func:`numpy.atleast_2d`
""")

    _docs.set_doc(
        chainerx.atleast_3d,
        """atleast_3d(a)
View inputs as arrays with at least three dimensions.

Args:
    a (~chainerx.ndarray): Array.

Returns:
    ~chainerx.ndarray: An array with a.ndim >= 3.
    Copies are avoided where possible, and views with
    three or more dimensions are returned.

Note:
    * Arrays that already have three or more dimensions are preserved.
    * During backpropagation, this function propagates the gradient of the
      output array to the input arrays in ``a``.

.. seealso:: :func:`numpy.atleast_3d`
""")

    _docs.set_doc(
        chainerx.split,
        """split(ary, indices_or_sections, axis=0)
Splits an array into multiple sub arrays along a given axis.

Args:
    ary (~chainerx.ndarray): Array to split.
    indices_or_sections (int or sequence of ints): A value indicating how to
        divide the axis. If it is an integer, then is treated as the number of
        sections, and the axis is evenly divided. Otherwise, the integers
        indicate indices to split at. Note that a sequence on the device
        memory is not allowed.
    axis (int): Axis along which the array is split.

Returns:
    list of :class:`~chainerx.ndarray`\\ s: A list of sub arrays. Each array \
is a partial view of the input array.

Note:
    During backpropagation, this function propagates the gradients of the
    output arrays to the input array ``ary``.

.. seealso:: :func:`numpy.split`
""")

    _docs.set_doc(
        chainerx.dsplit,
        """dsplit(ary, indices_or_sections)
Split array into multiple sub-arrays along the 3rd axis (depth).

Args:
    ary (~chainerx.ndarray): Array to split.
    indices_or_sections (int or sequence of ints): A value indicating how to
        divide the axis. If it is an integer, then is treated as the number of
        sections, and the axis is evenly divided. Otherwise, the integers
        indicate indices to split at. Note that a sequence on the device
        memory is not allowed.

Returns:
    list of :class:`~chainerx.ndarray`\\ s: A list of sub arrays. Each array \
is a partial view of the input array.

Note:
    During backpropagation, this function propagates the gradients of the
    output arrays to the input array ``ary``.

.. seealso:: :func:`numpy.dsplit`
""")

    _docs.set_doc(
        chainerx.vsplit,
        """vsplit(ary, indices_or_sections)
Splits an array into multiple sub-arrays vertically (row-wise).

Args:
    ary (~chainerx.ndarray): Array to split.
    indices_or_sections (int or sequence of ints): A value indicating how to
        divide the axis. If it is an integer, then is treated as the number of
        sections, and the axis is evenly divided. Otherwise, the integers
        indicate indices to split at. Note that a sequence on the device
        memory is not allowed.

Returns:
    list of :class:`~chainerx.ndarray`\\ s: A list of sub arrays. Each array \
is a partial view of the input array.

Note:
    During backpropagation, this function propagates the gradients of the
    output arrays to the input array ``ary``.

.. seealso:: :func:`numpy.vsplit`
""")

    _docs.set_doc(
        chainerx.hsplit,
        """hsplit(ary, indices_or_sections)
Split an array into multiple sub-arrays horizontally (column-wise).

Args:
    ary (~chainerx.ndarray): Array to split.
    indices_or_sections (int or sequence of ints): A value indicating how to
        divide the axis. If it is an integer, then is treated as the number of
        sections, and the axis is evenly divided. Otherwise, the integers
        indicate indices to split at. Note that a sequence on the device
        memory is not allowed.

Returns:
    list of :class:`~chainerx.ndarray`\\ s: A list of sub arrays. Each array \
is a partial view of the input array.

Note:
    During backpropagation, this function propagates the gradients of the
    output arrays to the input array ``ary``.

.. seealso:: :func:`numpy.hsplit`
""")

    _docs.set_doc(
        chainerx.swapaxes,
        """swapaxes(a, axis1, axis2)
Interchange two axes of an array.

Args:
    a (~chainerx.ndarray): Array to swapaxes.
    axis1 (int): First Axis
    axis2 (int): Second Axis

Returns:
    ~chainerx.ndarray: Swaped array.

Note:
    * Output array is a view of the input array.
    * During backpropagation, this function propagates the gradients of the
      output arrays to the input array ``a``.


.. seealso:: :func:`numpy.swapaxes`
""")

    _docs.set_doc(
        chainerx.repeat,
        """repeat(a, repeats, axis=None)
Constructs an array by repeating a given array.

Args:
    a (~chainerx.ndarray): Array to repeat.
    repeats (int or tuple of ints): The number of times which each
        element of a is repeated.
    axis (int): The axis along which to repeat values.

Returns:
    ~chainerx.ndarray: The repeated output array.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``a``.

.. seealso:: :func:`numpy.repeat`
""")

    _docs.set_doc(
        chainerx.expand_dims,
        """expand_dims(a, axis)
Expand the shape of an array.

Args:
    a (~chainerx.ndarray): Input Array.
    axis (int): Position in the expanded axes where the new axis is placed.

Returns:
    ~chainerx.ndarray: Output array.

Note:
    * Output array may or may not be a view of the input array.
    * During backpropagation, this function propagates the gradients of the
      output arrays to the input array ``a``.


.. seealso:: :func:`numpy.expand_dims`
""")

    _docs.set_doc(
        chainerx.flip,
        """flip(m, axis)
Reverse the order of elements in an array along the given axis.

Args:
    m (~chainerx.ndarray): Input Array.
    axis (int or tuple of ints): Axis or axes along which to flip over.
    The default, axis=None, will flip over all of the axes of the input array.
    If axis is negative it counts from the last to the first axis.
    If axis is a tuple of ints, flipping is performed on all of the
    axes specified in the tuple.

Returns:
    ~chainerx.ndarray: A view of m with the entries of axis reversed.
    Since a view is returned, this operation is done in constant time.

Note:
    * Output array is a view of the input array.
    * During backpropagation, this function propagates the gradients of the
      output arrays to the input array ``m``.


.. seealso:: :func:`numpy.flip`
""")

    _docs.set_doc(
        chainerx.fliplr,
        """fliplr(m)
Flip array in the left/right direction.

Args:
    m (~chainerx.ndarray): Input Array.

Returns:
    ~chainerx.ndarray: A view of m with the columns reversed.
    Since a view is returned, this operation is done in constant time.

Note:
    * Output array is a view of the input array.
    * During backpropagation, this function propagates the gradients of the
      output arrays to the input array ``m``.


.. seealso:: :func:`numpy.fliplr`
""")

    _docs.set_doc(
        chainerx.flipud,
        """flipud(m)
Flip array in the up/down direction.

Args:
    m (~chainerx.ndarray): Input Array.

Returns:
    ~chainerx.ndarray: A view of m with the rows reversed.
    Since a view is returned, this operation is done in constant time.

Note:
    * Output array is a view of the input array.
    * During backpropagation, this function propagates the gradients of the
      output arrays to the input array ``m``.


.. seealso:: :func:`numpy.flipud`
""")

    _docs.set_doc(
        chainerx.moveaxis,
        """moveaxis(a, source, destination)
Move axes of an array to new positions.

Other axes remain in their original order.

Args:
    a (~chainerx.ndarray): Input Array.
    source (int or tuple of ints): Original positions of the axes to move.
    These must be unique.
    destintation (int or tuple of ints): Destination positions for each of
    the original axes. These must also be unique.

Returns:
    ~chainerx.ndarray: Array with moved axes. This array is a view of the
    input array.

Note:
    * During backpropagation, this function propagates the gradients of the
      output arrays to the input array ``a``.


.. seealso:: :func:`numpy.moveaxis`
""")


</source>
</class>

<class classid="282" nclones="2" nlines="146" similarity="71">
<source file="systems/chainer-7.2.0/chainerx/_docs/routines.py" startline="1771" endline="2714" pcid="10702">
def _docs_math():
    _docs.set_doc(
        chainerx.negative,
        """negative(x)
Numerical negative, element-wise.

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = -x`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.

.. seealso:: :data:`numpy.negative`
""")

    _docs.set_doc(
        chainerx.add,
        """add(x1, x2)
Add arguments, element-wise.

Args:
    x1 (~chainerx.ndarray or scalar): Input array.
    x2 (~chainerx.ndarray or scalar): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = x_1 + x_2`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input arrays ``x1`` and ``x2``.

.. seealso:: :data:`numpy.add`
""")

    _docs.set_doc(
        chainerx.subtract,
        """subtract(x1, x2)
Subtract arguments, element-wise.

Args:
    x1 (~chainerx.ndarray or scalar): Input array.
    x2 (~chainerx.ndarray or scalar): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = x_1 - x_2`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input arrays ``x1`` and ``x2``.

.. seealso:: :data:`numpy.subtract`
""")

    _docs.set_doc(
        chainerx.multiply,
        """multiply(x1, x2)
Multiply arguments, element-wise.

Args:
    x1 (~chainerx.ndarray or scalar): Input array.
    x2 (~chainerx.ndarray or scalar): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = x_1 \\times x_2`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input arrays ``x1`` and ``x2``.

.. seealso:: :data:`numpy.multiply`
""")

    _docs.set_doc(
        chainerx.divide,
        """divide(x1, x2)
Divide arguments, element-wise.

Args:
    x1 (~chainerx.ndarray or scalar): Input array.
    x2 (~chainerx.ndarray or scalar): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = \\frac{x_1}{x_2}`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input arrays ``x1`` and ``x2``.

.. seealso:: :data:`numpy.divide`
""")

    _docs.set_doc(
        chainerx.sum,
        """sum(a, axis=None, keepdims=False)
Sum of array elements over a given axis.

Args:
    a (~chainerx.ndarray): Input array.
    axis (None or int or tuple of ints):
        Axis or axes along which a sum is performed.
        The flattened array is used by default.
    keepdims (bool):
        If this is set to ``True``, the reduced axes are left in the result
        as dimensions with size one.

Returns:
    :class:`~chainerx.ndarray`: The sum of input elements over a given axis.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``a``.

.. seealso:: :func:`numpy.sum`
""")

    _docs.set_doc(
        chainerx.maximum,
        """maximum(x1, x2)
Maximum arguments, element-wise.

Args:
    x1 (~chainerx.ndarray or scalar): Input array.
    x2 (~chainerx.ndarray or scalar): Input array.

Returns:
    :class:`~chainerx.ndarray`:
        Returned array: :math:`y = max(\\{x_1, x_2\\})`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input arrays ``x1`` and ``x2``.

.. seealso:: :data:`numpy.maximum`
""")

    _docs.set_doc(
        chainerx.minimum,
        """minimum(x1, x2)
Minimum arguments, element-wise.

Args:
    x1 (~chainerx.ndarray or scalar): Input array.
    x2 (~chainerx.ndarray or scalar): Input array.

Returns:
    :class:`~chainerx.ndarray`:
        Returned array: :math:`y = min(\\{x_1, x_2\\})`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input arrays ``x1`` and ``x2``.

.. seealso:: :data:`numpy.minimum`
""")

    _docs.set_doc(
        chainerx.remainder,
        """remainder(x1, x2)
Return element-wise remainder of division.

Args:
    x1 (~chainerx.ndarray or scalar): Input array.
    x2 (~chainerx.ndarray or scalar): Input array.

Returns:
    :class:`~chainerx.ndarray`:
        Returned array: The element-wise remainder of
        the quotient ``floor_divide(x1, x2)``.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input arrays ``x1`` and ``x2``.

.. seealso:: :data:`numpy.remainder`
""")

    _docs.set_doc(
        chainerx.exp,
        """exp(x)
Numerical exponential, element-wise.

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = \\exp x`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.

.. seealso:: :data:`numpy.exp`
""")

    _docs.set_doc(
        chainerx.log,
        """log(x)
Natural logarithm, element-wise.

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = \\ln x`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.

.. seealso:: :data:`numpy.log`
""")

    _docs.set_doc(
        chainerx.log10,
        """log10(x)
Base 10 logarithm, element-wise.

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = \\log_{10} x`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.

.. seealso:: :data:`numpy.log10`
""")

    _docs.set_doc(
        chainerx.log2,
        """log2(x)
Base 2 logarithm, element-wise.

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = \\log_{2} x`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.

.. seealso:: :data:`numpy.log2`
""")

    _docs.set_doc(
        chainerx.log1p,
        """log1p(x)
Natural logarithm of one plus the input, element-wise.

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = \\log(1 + x)`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.

.. seealso:: :data:`numpy.log1p`
""")

    _docs.set_doc(
        chainerx.logsumexp,
        """logsumexp(x, axis=None, keepdims=False)
The log of the sum of exponentials of input array.

Args:
    x (~chainerx.ndarray): Input array.
    axis (None or int or tuple of ints):
        Axis or axes along which a sum is performed.
        The flattened array is used by default.
    keepdims (bool):
        If this is set to ``True``, the reduced axes are left in the result
        as dimensions with size one.

Returns:
    :class:`~chainerx.ndarray`: The log of the sum of exponentials of
    input elements over a given axis.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.
""")

    _docs.set_doc(
        chainerx.log_softmax,
        """log_softmax(x, axis=None)
The log of the softmax of input array.

Args:
    x (~chainerx.ndarray): Input array.
    axis (None or int or tuple of ints):
        Axis or axes along which a sum is performed.
        The flattened array is used by default.

Returns:
    :class:`~chainerx.ndarray`: The log of the softmax of input elements
    over a given axis.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.
""")

    _docs.set_doc(
        chainerx.square,
        """square(x)
Returns the element-wise square of the input.

Args:
    x (~chainerx.ndarray or scalar): Input data

Returns:
    ~chainerx.ndarray: Returned array: :math:`y = x * x`.
    A scalar is returned if ``x`` is a scalar.

Note:
    During backpropagation, this function propagates the gradient
    of the output array to the input array ``x``.

.. seealso:: :data:`numpy.square`
""")

    _docs.set_doc(
        chainerx.sqrt,
        """sqrt(x)
Non-negative square-root, element-wise

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = \\sqrt x`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.

.. seealso:: :data:`numpy.sqrt`
""")

    _docs.set_doc(
        chainerx.sinh,
        """sinh(x)
Hyperbolic Sine, element-wise

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = \\sinh x`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.

.. seealso:: :data:`numpy.sinh`
""")

    _docs.set_doc(
        chainerx.cosh,
        """cosh(x)
Hyperbolic Cosine, element-wise

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = \\cosh x`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.

.. seealso:: :data:`numpy.cosh`
""")

    _docs.set_doc(
        chainerx.tanh,
        """tanh(x)
Element-wise hyperbolic tangent function.

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = \\tanh x`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.

.. seealso:: :data:`numpy.tanh`
""")

    _docs.set_doc(
        chainerx.sigmoid,
        """sigmoid(x)
Element-wise sigmoid logistic function.

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array:
    :math:`f(x) = (1 + \\exp(-x))^{-1}`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.

.. seealso:: :func:`chainer.functions.sigmoid`
""")

    _docs.set_doc(
        chainerx.sin,
        """sin(x)
Sine, element-wise

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = \\sin x`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.

.. seealso:: :data:`numpy.sin`
""")

    _docs.set_doc(
        chainerx.cos,
        """cos(x)
Cosine, element-wise

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = \\cos x`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.

.. seealso:: :data:`numpy.cos`
""")

    _docs.set_doc(
        chainerx.ceil,
        """ceil(x)
Return the ceiling of the input, element-wise..

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: The ceiling of each element in array.

.. seealso:: :data:`numpy.ceil`
""")

    _docs.set_doc(
        chainerx.tan,
        """tan(x)
Tangent, element-wise

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = \\tan x`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.

.. seealso:: :data:`numpy.tan`
""")

    _docs.set_doc(
        chainerx.relu,
        """Rectified Linear Unit function.
Args:
    x (~chainerx.ndarray): Input array.
Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = \\max (0, x)`.
Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.
""")

    _docs.set_doc(
        chainerx.tree_lstm,
        """tree_lstm(*inputs)
TreeLSTM unit as an activation function.

This function implements TreeLSTM units both for
N-ary TreeLSTM and Child-Sum TreeLSTM.
Let the children cell states
:math:`c_{\\text{1}}, c_{\\text{2}}, \\dots, c_{\\text{N}}`,
and the incoming signal :math:`x`.
First, the incoming signal :math:`x` is split into (3 + N) arrays
:math:`a, i, o, f_{\\text{1}}, f_{\\text{2}}, ..., f_{\\text{N}}`
of the same shapes along the second axis.
It means that :math:`x` 's second axis must have (3 + N) times
of the length of each :math:`c_{n}`.
The splitted input signals are corresponding to

    - :math:`a` : sources of cell input
    - :math:`i` : sources of input gate
    - :math:`o` : sources of output gate
    - :math:`f_{n}` : sources of forget gate for n-th ary

Second, it computes outputs as

.. math::
    c &= \\tanh(a) \\text{sigmoid}(i) \\\\
      & + c_{\\text{1}} \\text{sigmoid}(f_{\\text{1}}), \\\\
      & + c_{\\text{2}} \\text{sigmoid}(f_{\\text{2}}), \\\\
      & + ..., \\\\
      & + c_{\\text{N}} \\text{sigmoid}(f_{\\text{N}}), \\\\
    h &= \\tanh(c) \\text{sigmoid}(o).

These are returned as a tuple of (N + 1) variables.

Args:
    inputs (list of :class:`~chainerx.array`): Variable arguments which
        include all cell vectors from child-nodes, and an input vector.
        Each of the cell vectors and the input vector is
        :class:`~chainerx.array`.
        The input vector must have the second dimension whose size
        is (N + 3) times of that of each cell,
        where N denotes the total number of cells.

Returns:
    tuple: Two :class:`~chainerx.array` objects ``c`` and ``h``. ``c`` is
    the updated cell state. ``h`` indicates the outgoing signal.

See the papers for details: `Improved Semantic Representations From
Tree-Structured Long Short-Term Memory Networks
<https://www.aclweb.org/anthology/P15-1150>`_ and
`A Fast Unified Model for Parsing and Sentence Understanding
<https://arxiv.org/pdf/1603.06021.pdf>`_.
Tai et al.'s N-Ary TreeLSTM is little extended in
Bowman et al., and this link is based on
the variant by Bowman et al.
Specifically, eq. 10 in Tai et al. only has one :math:`W` matrix
to be applied to :math:`x`, consistently for all children.
On the other hand, Bowman et al.'s model has multiple matrices,
each of which affects the forget gate for each child's cell individually.

.. admonition:: Example

    Assuming ``y`` is the current input signal, ``c`` is the previous cell
    state, and ``h`` is the previous output signal from an
    :meth:`~chainerx.tree_lstm` function.
    Each of ``y``, ``c`` and ``h`` has ``n_units`` channels.
    Using 2-ary (binary) TreeLSTM,

    most typical preparation of ``x`` is

    >>> c1 = chainerx.ones((4, 10), dtype = chainerx.float32)
    >>> c2 = chainerx.ones((4, 10), dtype = chainerx.float32)
    >>> x = chainerx.ones((4, 50), dtype = chainerx.float32)
    >>> c, h = chainerx.tree_lstm(c1, c2, x)
    """)

    _docs.set_doc(
        chainerx.slstm,
        """slstm(c_prev1, c_prev2, x1, x2)
S-LSTM units as an activation function.

This function implements S-LSTM unit. It is an extension of LSTM unit
applied to tree structures.
The function is applied to binary trees. Each node has two child nodes.
It gets four arguments, previous cell states ``c_prev1`` and ``c_prev2``,
and input arrays ``x1`` and ``x2``.
First both input arrays ``x1`` and ``x2`` are split into eight arrays
:math:`a_1, i_1, f_1, o_1`, and :math:`a_2, i_2, f_2, o_2`. They have the
same shape along the second axis.
It means that ``x1`` and ``x2`` 's second axis must have 4 times
the length of ``c_prev1`` and ``c_prev2``.
The split input arrays are corresponding to

    - :math:`a_i` : sources of cell input
    - :math:`i_i` : sources of input gate
    - :math:`f_i` : sources of forget gate
    - :math:`o_i` : sources of output gate

It computes the updated cell state ``c`` and the outgoing signal
``h`` as.

.. math::
    c &= \\tanh(a_1 + a_2) \\sigma(i_1 + i_2)
       + c_{\\text{prev}1} \\sigma(f_1)
       + c_{\\text{prev}2} \\sigma(f_2), \\\\
    h &= \\tanh(c) \\sigma(o_1 + o_2),

where :math:`\\sigma` is the elementwise sigmoid function.
The function returns ``c`` and ``h`` as a tuple.

Args:
    c_prev1 (:class:`~chainerx.array`):
        Variable that holds the previous cell state of the first child
        node. The cell state should be a zero array or the output of
        the previous call of LSTM.
    c_prev2 (:class:`~chainerx.array`):
        Variable that holds the previous cell state of the second child
        node.
    x1 (:class:`~chainerx.array`):
        Variable that holds the sources of cell input, input gate, forget
        gate and output gate from the first child node. It must have the
        second dimension whose size is four times of that of the cell
        state.
    x2 (:class:`~chainerx.array`):
        Variable that holds the input sources from the second child node.

Returns:
    tuple: Two :class:`~chainerx.array` objects ``c`` and ``h``. ``c`` is
    the cell state. ``h`` indicates the outgoing signal.

See detail in paper: `Long Short-Term Memory Over Tree Structures
<https://arxiv.org/abs/1503.04881>`_.

.. admonition:: Example

    Assuming ``c1``, ``c2`` is the previous cell state of children,
    and ``h1``, ``h2`` is the previous outgoing signal from children.
    Each of ``c1``, ``c2``, ``h1`` and ``h2`` has ``n_units`` channels.
    Most typical preparation of ``x1``, ``x2`` is:

    >>> n_units = 100
    >>> c1 = chainerx.ones((1, n_units), np.float32)
    >>> c2 = chainerx.ones((1, n_units), np.float32)
    >>> x1 = chainerx.ones((1, 4 * n_units), chainerx.float32)
    >>> x2 = chainerx.ones((1, 4 * n_units), chainerx.float32)
    >>> c, h = chainerx.slstm(c1, c2, x1, x2)
    """)

    _docs.set_doc(
        chainerx.arcsin,
        """arcsin(x)
Inverse sine, element-wise

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = \\arcsin x`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.

.. seealso:: :data:`numpy.arcsin`
""")

    _docs.set_doc(
        chainerx.arccos,
        """arccos(x)
Trigonometric inverse cosine, element-wise

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = \\arccos x`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.

.. seealso:: :data:`numpy.arccos`
""")

    _docs.set_doc(
        chainerx.arctan,
        """arctan(x)
Trigonometric inverse tangent, element-wise

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = \\arctan x`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.

.. seealso:: :data:`numpy.arctan`
""")

    _docs.set_doc(
        chainerx.arctan2,
        """arctan2(x1, x2)
Element-wise arc tangent of :math:`\\frac{x_1}{x_2}` choosing the quadrant
correctly.

Args:
    x1 (~chainerx.ndarray): Input array.
    x2 (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returns an array where each element
    represents :math:`\\theta` in the range :math:`[-\\pi, \\pi]`, such
    that :math:`x_1 = r \\sin(\\theta)` and :math:`x_2 = r \\cos(\\theta)`
    for some :math:`r > 0`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x1`` and/or ``x2``.

.. seealso:: :data:`numpy.arctan2`
""")

    _docs.set_doc(
        chainerx.arcsinh,
        """arcsinh(x)
Inverse hyperbolic sine, element-wise

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = \\arcsinh x`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.

.. seealso:: :data:`numpy.arcsinh`
""")

    _docs.set_doc(
        chainerx.arccosh,
        """arccosh(x)
Inverse hypberbolic inverse cosine, element-wise

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = \\arccosh x`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``x``.

.. seealso:: :data:`numpy.arccosh`
""")

    _docs.set_doc(
        chainerx.fabs,
        """fabs(x)
Compute the absolute values element-wise.
Args:
    x (~chainerx.ndarray): Input array.
Returns:
    :class:`~chainerx.ndarray`: The absolute values of x, the returned values
    are always floats.
.. seealso:: :data:`numpy.fabs`
""")

    _docs.set_doc(
        chainerx.sign,
        """sign(x)
Returns an element-wise indication of the sign of a number.
The sign function returns :math:`-1 if x < 0, 0 if x==0, 1 if x > 0`.
``nan`` is returned for ``nan`` inputs.
Args:
    x (~chainerx.ndarray): Input array.
Returns:
    :class:`~chainerx.ndarray`: The sign of x.
.. seealso:: :data:`numpy.sign`
""")

    _docs.set_doc(
        chainerx.floor,
        """floor(x)
Return the floor of the input, element-wise.
Args:
    x (~chainerx.ndarray): Input array.
Returns:
    :class:`~chainerx.ndarray`: The floor of each element in array.
.. seealso:: :data:`numpy.floor`
""")

    _docs.set_doc(
        chainerx.isnan,
        """isnan(x)
Test element-wise for NaN and return result as a boolean array.

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: True where ``x`` is NaN, false otherwise

Note:
    During backpropagation, this function does not propagate gradients.

.. seealso:: :data:`numpy.isnan`
""")

    _docs.set_doc(
        chainerx.isfinite,
        """isfinite(x)
Test element-wise for finiteness (not infinity or not Not a Number).

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: True where x is not positive infinity,
    negative infinity, or NaN; false otherwise.

Note:
    During backpropagation, this function does not propagate gradients.

.. seealso:: :data:`numpy.isfinite`
""")

    _docs.set_doc(
        chainerx.isinf,
        """isinf(x)
Test element-wise for positive or negative infinity.

Args:
    x (~chainerx.ndarray): Input array.

Returns:
    :class:`~chainerx.ndarray`: True where ``x`` is positive or negative
    infinity, false otherwise.

Note:
    During backpropagation, this function does not propagate gradients.

.. seealso:: :data:`numpy.isinf`
""")

    _docs.set_doc(
        chainerx.bitwise_and,
        """bitwise_and(x1, x2)
Compute the bit-wise AND of two arrays element-wise.

Args:
    x1 (~chainerx.ndarray or scalar): Input array of integers.
    x2 (~chainerx.ndarray or scalar): Input array of integers.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = x_1 \\& x_2`

Note:
    During backpropagation, this function does not propagate gradients.

.. seealso:: :data:`numpy.bitwise_and`
""")

    _docs.set_doc(
        chainerx.bitwise_or,
        """bitwise_or(x1, x2)
Compute the bit-wise OR of two arrays element-wise.

Args:
    x1 (~chainerx.ndarray or scalar): Input array of integers.
    x2 (~chainerx.ndarray or scalar): Input array of integers.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = x_1 | x_2`

Note:
    During backpropagation, this function does not propagate gradients.

.. seealso:: :data:`numpy.bitwise_or`
""")

    _docs.set_doc(
        chainerx.bitwise_xor,
        """bitwise_xor(x1, x2)
Compute the bit-wise XOR of two arrays element-wise.

Args:
    x1 (~chainerx.ndarray or scalar): Input array of integers.
    x2 (~chainerx.ndarray or scalar): Input array of integers.

Returns:
    :class:`~chainerx.ndarray`: Returned array: :math:`y = x_1 \\oplus x_2`

Note:
    During backpropagation, this function does not propagate gradients.

.. seealso:: :data:`numpy.bitwise_xor`
""")

    _docs.set_doc(
        chainerx.left_shift,
        """left_shift(x1, x2)
Shift the bits of an integer to the left.

Args:
    x1 (~chainerx.ndarray or scalar): Input array of integers.
    x2 (~chainerx.ndarray or scalar): Input array of integers.

Returns:
    :class:`~chainerx.ndarray`: Return `x1` with bits shifted `x2` times to the left.

Note:
    During backpropagation, this function does not propagate gradients.

.. seealso:: :data:`numpy.left_shift`
""")  # NOQA

    _docs.set_doc(
        chainerx.right_shift,
        """right_shift(x1, x2)
Shift the bits of an integer to the right.

Args:
    x1 (~chainerx.ndarray or scalar): Input array of integers.
    x2 (~chainerx.ndarray or scalar): Input array of integers.

Returns:
    :class:`~chainerx.ndarray`: Return `x1` with bits shifted `x2` times to the right.

Note:
    During backpropagation, this function does not propagate gradients.

.. seealso:: :data:`numpy.right_shift`
""")  # NOQA


</source>
<source file="systems/chainer-7.2.0/chainerx/_docs/array.py" startline="5" endline="422" pcid="10709">
def set_docs():
    ndarray = chainerx.ndarray

    _docs.set_doc(
        ndarray,
        """ndarray(shape, dtype, device=None)
Multi-dimensional array, the central data structure of ChainerX.

This class, along with other APIs in the :mod:`chainerx` module, provides a
subset of NumPy APIs. This class works similar to :class:`numpy.ndarray`,
except for some differences including the following noticeable points:

- :class:`chainerx.ndarray` has a :attr:`device` attribute. It indicates on
  which device the array is allocated.
- :class:`chainerx.ndarray` supports :ref:`Define-by-Run <define_by_run>`
  backpropagation. Once you call :meth:`require_grad`, the array starts
  recording the operations applied to it recursively. Gradient of the result
  with respect to the original array can be computed then with the
  :meth:`backward` method or the :func:`chainerx.backward` function.

Args:
    shape (tuple of ints): Shape of the new array.
    dtype: Data type.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device <chainerx_device>` is chosen.

.. seealso:: :class:`numpy.ndarray`
""")

    _docs.set_doc(
        ndarray.data_ptr,
        """int: Address of the underlying memory allocation.

The meaning of the address is device-dependent.
""")

    _docs.set_doc(
        ndarray.data_size,
        'int: Total size of the underlying memory allocation.')

    _docs.set_doc(
        ndarray.device, '~chainerx.Device: Device on which the data exists.')

    _docs.set_doc(ndarray.dtype, 'Data type of the array.')

    # TODO(beam2d): Write about backprop id.
    _docs.set_doc(
        ndarray.grad,
        """~chainerx.ndarray: Gradient held by the array.

It is ``None`` if the gradient is not available.
Setter of this property overwrites the gradient.
""")

    _docs.set_doc(
        ndarray.is_contiguous,
        'bool: ``True`` iff the array is stored in the C-contiguous order.')

    _docs.set_doc(ndarray.itemsize, 'int: Size of each element in bytes.')

    _docs.set_doc(
        ndarray.nbytes,
        """int: Total size of all elements in bytes.

It does not count skips between elements.""")

    _docs.set_doc(ndarray.ndim, 'int: Number of dimensions.')

    _docs.set_doc(
        ndarray.offset,
        'int: Offset of the first element from the memory allocation in bytes.'
    )

    _docs.set_doc(
        ndarray.shape,
        """tuple of int: Lengths of axes.

.. note::
    Currently, this property does not support setter.""")

    _docs.set_doc(ndarray.size, 'int: Number of elements in the array.')

    _docs.set_doc(ndarray.strides, 'tuple of int: Strides of axes in bytes.')

    _docs.set_doc(
        ndarray.T,
        """~chainerx.ndarray: Shape-reversed view of the array.

New array is created at every access to this property.
``x.T`` is just a shorthand of ``x.transpose()``.
""")

    _docs.set_doc(
        ndarray.__getitem__,
        """___getitem__(self, key)
Returns self[key].

.. note::
    Currently, only basic indexing is supported not advanced indexing.
""")

    def unary_op(name, s):
        _docs.set_doc(getattr(ndarray, name), '{}()\n{}'.format(name, s))

    unary_op('__bool__', 'Casts a size-one array into a :class:`bool` value.')
    unary_op('__float__',
             'Casts a size-one array into a :class:`float` value.')
    unary_op('__int__', 'Casts a size-one array into :class:`int` value.')
    unary_op('__len__', 'Returns the length of the first axis.')
    unary_op('__neg__', 'Computes ``-x`` elementwise.')

    def binary_op(name, s):
        _docs.set_doc(getattr(ndarray, name), '{}(other)\n{}'.format(name, s))

    binary_op('__eq__', 'Computes ``x == y`` elementwise.')
    binary_op('__ne__', 'Computes ``x != y`` elementwise.')
    binary_op('__lt__', 'Computes ``x < y`` elementwise.')
    binary_op('__le__', 'Computes ``x <= y`` elementwise.')
    binary_op('__ge__', 'Computes ``x >= y`` elementwise.')
    binary_op('__gt__', 'Computes ``x > y`` elementwise.')

    binary_op('__iadd__', 'Computes ``x += y`` elementwise.')
    binary_op('__isub__', 'Computes ``x -= y`` elementwise.')
    binary_op('__imul__', 'Computes ``x *= y`` elementwise.')
    binary_op('__itruediv__', 'Computes ``x /= y`` elementwise.')
    binary_op('__iand__', 'Computes ``x &= y`` elementwise.')
    binary_op('__ior__', 'Computes ``x |= y`` elementwise.')
    binary_op('__ixor__', 'Computes ``x ^= y`` elementwise.')

    binary_op('__add__', 'Computes ``x + y`` elementwise.')
    binary_op('__sub__', 'Computes ``x - y`` elementwise.')
    binary_op('__mul__', 'Computes ``x * y`` elementwise.')
    binary_op('__truediv__', 'Computes ``x / y`` elementwise.')

    binary_op('__and__', 'Computes ``x & y`` elementwise.')
    binary_op('__or__', 'Computes ``x | y`` elementwise.')
    binary_op('__xor__', 'Computes ``x ^ y`` elementwise.')

    binary_op('__radd__', 'Computes ``y + x`` elementwise.')
    binary_op('__rsub__', 'Computes ``y - x`` elementwise.')
    binary_op('__rmul__', 'Computes ``y * x`` elementwise.')
    binary_op('__rand__', 'Computes ``y & x`` elementwise.')
    binary_op('__ror__', 'Computes ``y | x`` elementwise.')
    binary_op('__rxor__', 'Computes ``y ^ x`` elementwise.')

    # TODO(beam2d): Write about as_grad_stopped(backprop_ids, copy) overload.
    _docs.set_doc(
        ndarray.as_grad_stopped,
        """as_grad_stopped(copy=False)
Creates a view or a copy of the array that stops gradient propagation.

This method behaves similar to :meth:`view` and :meth:`copy`, except that
the gradient is not propagated through this operation (internally, this
method creates a copy or view of the array without connecting the computational
graph for backprop).

Args:
    copy (bool): If ``True``, it copies the array. Otherwise, it returns a view
        of the original array.

Returns:
    ~chainerx.ndarray:
        A view or a copy of the array without propagating the  gradient on
        backprop.
""")

    _docs.set_doc(
        ndarray.argmax,
        """argmax(axis=None)
Returns the indices of the maximum elements along a given axis.

See :func:`chainerx.argmax` for the full documentation.
""")

    _docs.set_doc(
        ndarray.argmin,
        """argmin(axis=None)
Returns the indices of the minimum elements along a given axis.

See :func:`chainerx.argmin` for the full documentation.
""")

    _docs.set_doc(
        ndarray.astype,
        """astype(dtype, copy=True)
Casts each element to the specified data type.

Args:
    dtype: Data type of the new array.
    copy (bool): If ``True``, this method always copies the data. Otherwise,
        it creates a view of the array if possible.

Returns:
    ~chainerx.ndarray: An array with the specified dtype.
""")

    _docs.set_doc(
        ndarray.backward,
        """backward(backprop_id=None, enable_double_backprop=False)
Performs backpropagation starting from this array.

This method is equivalent to ``chainerx.backward([self], *args)``.
See :func:`chainerx.backward` for the full documentation.
""")

    # TODO(beam2d): Write about backprop id.
    _docs.set_doc(
        ndarray.cleargrad,
        """cleargrad()
Clears the gradient held by this array.
""")

    _docs.set_doc(
        ndarray.copy,
        """copy()
Creates an array and copies all the elements to it.

The copied array is allocated on the same device as ``self``.

.. seealso:: :func:`chainerx.copy`
""")

    _docs.set_doc(
        ndarray.dot,
        """dot(b)
Returns the dot product with a given array.

See :func:`chainerx.dot` for the full documentation.
""")

    _docs.set_doc(
        ndarray.fill,
        """fill(value)
Fills the array with a scalar value in place.

Args:
    value: Scalar value with which the array will be filled.
""")

    # TODO(beam2d): Write about backprop_id argument.
    _docs.set_doc(
        ndarray.get_grad,
        """get_grad()
Returns the gradient held by the array.

If the gradient is not available, it returns ``None``.
""")

    # TODO(beam2d): Write about backprop_id argument.
    _docs.set_doc(
        ndarray.is_backprop_required,
        """is_backprop_required()
Returns ``True`` if gradient propagates through this array on backprop.

See the note on :meth:`require_grad` for details.
""")

    # TODO(beam2d): Write about backprop_id argument.
    _docs.set_doc(
        ndarray.is_grad_required,
        """is_grad_required()
Returns ``True`` if the gradient will be set after backprop.

See the note on :meth:`require_grad` for details.
""")

    _docs.set_doc(
        ndarray.item,
        """item()
Copies an element of an array to a standard Python scalar and returns it.

Returns:
    z:
        A copy of the specified element of the array as a suitable Python
        scalar.

.. seealso:: :func:`numpy.item`
""")

    _docs.set_doc(
        ndarray.max,
        """max(axis=None, keepdims=False)
Returns the maximum along a given axis.

See :func:`chainerx.amax` for the full documentation.
""")

    _docs.set_doc(
        ndarray.min,
        """min(axis=None, keepdims=False)
Returns the minimum along a given axis.

See :func:`chainerx.amin` for the full documentation.
""")

    # TODO(beam2d): Write about backprop_id argument.
    _docs.set_doc(
        ndarray.require_grad,
        """require_grad()
Declares that a gradient for this array will be made available after backprop.

Once calling this method, any operations applied to this array are recorded for
later backprop. After backprop, the :attr:`grad` attribute holds the gradient
array.

.. note::
    ChainerX distinguishes *gradient requirements* and *backprop requirements*
    strictly. They are strongly related, but different concepts as follows.

    - *Gradient requirement* indicates that the gradient array should be made
      available after backprop. This attribute **is not propagated** through
      any operations. It implicates the backprop requirement.
    - *Backprop requirement* indicates that the gradient should be propagated
      through the array during backprop. This attribute **is propagated**
      through differentiable operations.

    :meth:`require_grad` sets the gradient requirement flag. If you need to
    extract the gradient after backprop, you have to call :meth:`require_grad`
    on the array even if the array is an intermediate result of differentiable
    computations.

Returns:
    ~chainerx.ndarray: ``self``
""")

    _docs.set_doc(
        ndarray.reshape,
        """reshape(newshape)
Creates an array with a new shape and the same data.

See :func:`chainerx.reshape` for the full documentation.
""")

    _docs.set_doc(
        ndarray.set_grad,
        """set_grad(grad)
Sets a gradient to the array.

This method overwrites the gradient with a given array.

Args:
    grad (~chainerx.ndarray): New gradient array.
""")

    _docs.set_doc(
        ndarray.squeeze,
        """squeeze(axis=None)
Removes size-one axes from an array.

See :func:`chainerx.squeeze` for the full documentation.
""")

    _docs.set_doc(
        ndarray.swapaxes,
        """swapaxes(axis1, axis2)
Interchange two axes of an array..

See :func:`chainerx.swapaxes` for the full documentation.
""")

    _docs.set_doc(
        ndarray.repeat,
        """repeat(repeats, axis=None)
Constructs an array by repeating a given array.

See :func:`chainerx.repeats` for the full documentation.
""")

    _docs.set_doc(
        ndarray.sum,
        """sum(axis=None, keepdims=False)
Returns the sum of an array along given axes.

See :func:`chainerx.sum` for the full documentation.
""")

    _docs.set_doc(
        ndarray.take,
        """take(indices, axis)
Takes elements from the array along an axis.

See :func:`chainerx.take` for the full documentation.
""")

    _docs.set_doc(
        ndarray.to_device,
        """to_device(device, index=None)
Transfers the array to the specified device.

Args:
    device (~chainerx.Device or str): Device to which the array is transferred,
        or a backend name. If it is a backend name, ``index`` should also be
        specified.
    index (int): Index of the device for the backend specified by ``device``.

Returns:
    ~chainerx.ndarray:
        An array on the target device.
        If the original array is already on the device, it is a view of that.
        Otherwise, it is a copy of the array on the target device.
""")

    _docs.set_doc(
        ndarray.transpose,
        """transpose(axes=None)
Creates a view of an array with permutated axes.

See :func:`chainerx.transpose` for the full documentation.
""")

    _docs.set_doc(
        ndarray.view,
        """view()
Returns a view of the array.

The returned array shares the underlying buffer, though it has a different
identity as a Python object.
""")
</source>
</class>

</clones>
